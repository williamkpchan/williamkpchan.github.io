<base target="_blank"><html><head><title>Text Mining</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = false;
  var topicEnd = "<br>";
  var bookid = "Text Mining"
  var markerName = "h2, strong"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Practical Guide to Text Mining and Feature Engineering in R</h1>
<div id="toc"></div></center><br><br>
<a href="https://www.hackerearth.com/practice/" class="whitebut ">Programming Tutorials And Practice Problems</a>

<pre><br><br>

<h2>Introduction</h2>
The ability to deal with text data is one of the important skills a data scientist must posses. 
With advent of social media, forums, review sites, web page crawlers companies now have access to massive behavioural data of their customers. 

Yes, companies have more of textual data than numerical data. 
No doubt, this data will be messy. 
But, beneath it lives an enriching source of information, insights which can help companies to boost their businesses. 
That is the reason, why <b>natural language processing (NLP) a.k.a Text Mining</b> as a technique is growing rapidly and being extensively used by data scientists.
In the previous tutorial, we learnt about <a href="https://www.hackerearth.com/practice/machine-learning/advanced-techniques/regular-expressions-string-manipulation-r/tutorial/">regular expressions</a> in detail. 
Make sure you've read it.

In this tutorial, you'll about text mining from scratch. We'll follow a stepwise pedagogy to understand text mining concepts. 
Later, we'll work on a current kaggle competition data sets to gain practical experience, which is followed by two practice exercises. 

<h2>What is Text Mining (or Natural Language Processing) ?</h2>
Natural Language Processing (NLP) or Text mining helps computers to understand human language. 
It involves a set of techniques which automates text processing to derive useful insights from unstructured data. 
These techniques helps to transform messy text data sets into a structured form which can be used into machine learning. 

The resultant structured data sets are high dimensional i.e. 
large rows and columns. 
In a way, text expands the universe of data manifolds. 
Hence, to avoid long training time, you should be careful in choosing the ML algorithm for text data analysis. Generally, algorithms such as <b>naive bayes</b>, <b>glmnet</b>, <b>deep learning</b> tend to work well on text data. 
 
<h2>What are the steps involved in Text Mining ?</h2>
Let's say you are given a data set having product descriptions. 
And, you are asked to extract features from the given descriptions. 
How would you start to make sense out of it ? The raw text data (description) will be filtered through several cleaning phases to  get transformed into a tabular format for analysis. 
Let's look at some of the steps:
<strong>&emsp;Corpus Creation</strong> - It involves creating a matrix comprising of documents and terms (or tokens). 
A document can be understood as each row having product description and each column having terms. 
Terms refers to each word in the description. 
Usually, the number of documents in the corpus equals to number of rows in the given data. 

Let's say our document is "Free software comes with ABSOLUTELY NO certain WARRANTY","You are welcome to redistribute free software under certain conditions","Natural language support for software in an English locale","A collaborative project with many contributors". 
The image below shows the matrix format of this document where every column represents a term from the document.
<img src="http://blog.hackerearth.com/wp-content/uploads/2017/04/docterm_matrix.png">
<strong>&emsp;Text Cleaning</strong> - It involves cleaning the text in following ways:
Remove words - If the data is extracted using web scraping, you might want to remove html tags.
Remove stop words - Stop words are a set of words which helps in sentence construction and don't have any real information. 
Words such as a, an, the, they, where etc. are categorized as stop words.

Convert to lower - To maintain a standarization across all text and get rid of case differences and convert the entire text to lower.
Remove punctuation - We remove punctuation since they don't deliver any information.
Remove number - Similarly, we remove numerical figures from text
Remove whitespaces - Then, we remove the used spaces in the text.

Stemming &amp; Lemmatization - Finally, we convert the terms into their root form. 
For example: Words like playing, played, plays gets converted to the root word 'play'. 
It helps in capturing the intent of terms precisely.
<strong>&emsp;Feature Engineering</strong> -  To be explained in the following section
<strong>&emsp;Model Building</strong> - After the raw data is passed through all the above steps, it become ready for model building. 
As mentioned above, not all ML algorithms perform well on text data. 
Naive Bayes is popularly known to deliver high accuracy on text data. 
In addition, deep neural network models also perform fairly well.
<h2>What are Feature Engineering Techniques used in Text Mining ?</h2>
Do you know each word of this line you are reading can be converted into a feature ? Yes, you heard correctly. 
Text data offers a wide range of possibilities to generate new features. 
But sometimes, we end up generating lots of features, to an extent that processing them becomes a painful task. 
Hence we should meticulously analyze the extracted features. 
Don't worry! The methods explained below will also help in reducing the dimension of the resultant data set. 

Below is the list of popular feature engineering methods used: 
1. <strong>&emsp;n-grams</strong> : In the document corpus, 1 word (such as baby, play, drink) is known as 1-gram. 
Similarly, we can have 2-gram (baby toy, play station, diamond ring), 3-gram etc. 
The idea behind this technique is to explore the chances that when one or two or more words occurs together gives more information to the model. 

2. <strong>&emsp;TF - IDF</strong> : It is also known as Term Frequency - Inverse Document Frequency. 
This technique believes that, from a document corpus, a learning algorithm gets more information from the rarely occurring terms than frequently occurring terms. 
Using a weighted scheme, this technique helps to score the importance of terms. 
The terms occurring frequently are weighted lower and the terms occurring rarely get weighted higher.
*  TF is be calculated as: frequency of a term in a document / all the terms in the document.
*  IDF is calculated as: ratio of log (total documents in the corpus / number of documents with the 'term' in the corpus)
*  Finally, TF-IDF is calculated as: TF X IDF. 
Fortunately, R has packages which can do these calculations effort
3. <strong>&emsp;Cosine Similarity</strong> - This measure helps to find similar documents. 
It's one of the commonly used distance metric used in text analysis. 
For a given 2 vectors A and B of length n each, cosine similarity can be calculated as a dot product of two unit vectors:
<img src="https://alexn.org/assets/img/cosine-similarity-34eaf5ab.png">
4. <strong>&emsp;Jaccard Similarity</strong> - This is another distance metric used in text analysis. 
For a given two vectors (A and B), it can be calculated as ratio of (terms which are available in both vectors / terms which are available in either of the vectors). 
It's formula is: (A ∩ B)/(A U B). 
To create features using distance metrics, first we'll create cluster of similar documents and assign a unique label to each document in a new column. 

5. <strong>&emsp;Levenshtein Distance</strong> - We can also use levenshtein distance to create a new feature based on distance between two strings. 
We won't go into its complicated formula, but understand what it does: it finds the shorter string in longer texts and returns the maximum value as 1 if both the shorter string is found. 
For example: Calculating levenshtein distance for string "Alps Street 41" and "1st Block, Alps Street 41" will result in 1. 

6. <strong>&emsp;Feature Hashing</strong> - This technique implements the 'hashing trick' which helps in reducing the dimension of document matrix (lesser columns). 
It doesn't use the actual data, instead it uses the indexes[i,j] of the data, thus it processes data only when needed. 
And, that's why it takes lesser memory in computation. 
In addition, there are more techniques which we'll discover while modeling text data in the next section. 
 
<h2>Text Mining Practical - Predict the interest level</h2>
Since regular expressions help wonderfully in dealing with text data, make sure that you have referred to the regular expression tutorial as well. 
Instead of a dummy data, we'll get our hands on a real text mining problem. 
In this problem, we'll predict the popularity of apartment rental listing based on given features. 
The data set has been taken from currently running <a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries">two sigma rental listing</a> problem on Kaggle. 
Therefore, after you finish this tutorial, you can right away participate in it and try your luck. 
Since the focus of this tutorial is text mining, we'll work only on the text features available in the data. 
For this tutorial, you can download the <a href="https://s3-ap-southeast-1.amazonaws.com/he-public-data/text_dataee185a4.zip">data here</a>. (huge dataset)

Now, we'll load the data and useful libraries for solving this problem. 
Using map_at function from purrr package, we'll convert the json files into tabular data tables.
<code><span class="brown">#load libraries</span>
path &lt;- "/home/manish/Desktop/Data2017/February/twosigma/"
setwd(path)
library(data.table)
library(jsonlite)
library(purrr) <span class="brown"># big tool sets working with data wrangling.</span>
library(RecordLinkage) <span class="brown"># functions for data comparison and matching</span>
library(stringr) <span class="brown"># functions to make working with strings</span>
library(tm) <span class="brown"># for text mining applications</span>

<span class="brown">#load data</span>
traind &lt;- fromJSON("train.json")
test &lt;- fromJSON("test.json")</code>
<code><span class="brown">#convert json to data table</span>
vars &lt;- setdiff(names(traind),c("photos","features"))
train &lt;- map_at(traind, vars, unlist) %&gt;% as.data.table() <span class="brown"># Apply a function to each element of a vector conditionally</span>
test &lt;- map_at(test,vars,unlist) %&gt;% as.data.table()</code>
Since we are interested only in text variables, let's extract the text features:
<code>train &lt;- train[,.(listing_id,features, description,street_address,display_address,interest_level)]
test &lt;- test[,.(listing_id,features,street_address,display_address,description)]</code>
Let's quickly understand what the data is about: 
we can derive the following insights:
<code>dim(train)</code> #The train data has 49352 rows and 6 columns. 
<code>dim(test)</code> #The test data has 74659 rows and 5 columns.
<code>head(train)</code>
<code>head(test)</code>
<code>sapply(train,class)</code>
<code>sapply(test,class)</code>
interest_level is the dependent variable i.e. the variable to predict
listing_id variable has unique value for every listing. 
In other words, it is the identifier variable.
features comprises of a list of features for every listing_id
description refers to the description of a listing_id provided by the agent
Street address and display address refers to the address of the listed apartment.
Let's start with the analysis. 

We'll now join the data and create some new features. 

<code><span class="brown">#join data</span>
test[,interest_level := "None"]
tdata &lt;- rbindlist(list(train,test))</code>
<code><span class="brown">#fill empty values in the list</span>
tdata[,features := ifelse(map(features, is_empty),"aempty",features)]</code>
<code><span class="brown">#count number of features per listing</span>
tdata[,feature_count := unlist(lapply(features, length))]</code>
<code><span class="brown">#count number of words in description</span>
tdata[,desc_word_count := str_count(description,pattern = "\\w+")]</code>
<code><span class="brown">#count total length of description</span>
tdata[,desc_len := str_count(description)]</code>
 
<code><span class="brown">#similarity between address</span>
tdata[,lev_sim := levenshteinDist(street_address,display_address)]</code>
Feature engineering doesn't play by a fixed set of rules. 
With a different data set, you'll always discover new potential set of features. 
The best way to become at expert at feature engineering is solve different types of problems. 
Anyhow, for this problem I've created the following variables:
Count of number of features per listing
Count of number of words in the description
Count of total length of the description
Similarity between street and display address using levenshtein distance
<code>dim(tdata)</code>
Now, the data has 10 variables and 680961 observations. 
If you think these variables are too less for a machine learning algorithm to work best, hold tight. 
In a while, our data dimension is going to explode. 
We'll create more new features from the variable 'Features' and 'Description'. 

Let's take them up one by one. 
First, we'll transform the list 'Features' into one feature per row format. 

<code><span class="brown">#extract variables from features</span>
fdata &lt;- data.table(listing_id = rep(unlist(tdata$listing_id), lapply(tdata$features, length)), features = unlist(tdata$features))
head(fdata)</code>
<code><span class="brown">#convert features to lower</span>
fdata[,features := unlist(lapply(features, tolower))]</code>
Since not all the features will be useful, let's calculate the count of features and remove features which are occurring less than 100 times in the data. 

<code><span class="brown">#calculate count for every feature</span>
fdata[,count := .N, features]
fdata[order(count)][1:20]</code>
<code><span class="brown">#keep features which occur 100 or more times</span>
fdata &lt;- fdata[count &gt;= 100]</code>
Let's convert each feature into a separate column so that it can be used as a variable in model training. 
To accomplish that, we'll use the dcast function. 

<code><span class="brown">#convert columns into table</span>
fdata &lt;- dcast(data = fdata, formula = listing_id ~ features, fun.aggregate = length, value.var = "features")
dim(fdata)</code>
This has resulted in 96 new variables. 
We'll keep this new data set as is, and extract more features from the description variable. 
From here, we'll be using tm package.
<code><span class="brown">#create a corpus of descriptions</span>
text_corpus &lt;- Corpus(VectorSource(tdata$description))
<span class="brown">#check first 4 documents</span>
inspect(text_corpus[1:4])</code>
<code><span class="brown">#the corpus is a list object in R of type CORPUS</span>
print(lapply(text_corpus[1:2], as.character))</code>
As you can see, the word 'br' is just a noise and doesn't provide any useful information in the description. 
We'll remove it. 
Also, we'll perform the text mining steps to clean the data as explained in section above.
<code><span class="brown">#let's clean the data</span>
dropword &lt;- "br"</code>
<code><span class="brown">#remove br</span>
text_corpus &lt;- tm_map(text_corpus,removeWords,dropword)
print(as.character(text_corpus[[1]]))
<span class="brown">#tolower</span>
text_corpus &lt;- tm_map(text_corpus, tolower)
print(as.character(text_corpus[[1]]))
<span class="brown">#remove punctuation</span>
text_corpus &lt;- tm_map(text_corpus, removePunctuation)
print(as.character(text_corpus[[1]]))
<span class="brown">#remove number</span>
text_corpus &lt;- tm_map(text_corpus, removeNumbers)
print(as.character(text_corpus[[1]]))
<span class="brown">#remove whitespaces</span>
text_corpus &lt;- tm_map(text_corpus, stripWhitespace,lazy = T)
print(as.character(text_corpus[[1]]))
<span class="brown">#remove stopwords</span>
text_corpus &lt;- tm_map(text_corpus, removeWords, c(stopwords('english')))
print(as.character(text_corpus[[1]]))
<span class="brown">#convert to text document</span>
text_corpus &lt;- tm_map(text_corpus, PlainTextDocument)
<span class="brown">#perform stemming - this should always be performed after text doc conversion</span>
text_corpus &lt;- tm_map(text_corpus, stemDocument,language = "english")
print(as.character(text_corpus[[1]]))
text_corpus[[1]]$content</code>
After every cleaning step, we've printed the resultant corpus to help you understand the effect of each step on the corpus. 
Now, our corpus is ready to get converted into a matrix. 

<code><span class="brown">#convert to document term matrix</span>
docterm_corpus &lt;- DocumentTermMatrix(text_corpus)
dim(docterm_corpus)</code>
This matrix has resulted in 52647 features. 
Such matrices resulted from text data creates sparse matrices. 
'Sparse' means most of the rows have zeroes. 
In our matrix also, there could be columns which have &gt; 90% zeroes or we can say, those columns are &gt; 90% sparse. 
Also, training models on such ultra high data dimensional data would take weeks to process. 
Therefore, we'll remove the sparse terms. 

Let's remove the variables which are 95% or more sparse. 

<code>new_docterm_corpus &lt;- removeSparseTerms(docterm_corpus,sparse = 0.95)
dim(new_docterm_corpus)</code>
Now, our matrix looks more friendly with 87 features. 
But, which are these features ? It's equally important to explore and visualize the features to gain better understanding of the matrix. 
Let's calculate the most frequent and least frequently occurring set of features. 

<code><span class="brown">#find frequent terms</span>
colS &lt;- colSums(as.matrix(new_docterm_corpus))
length(colS)
doc_features &lt;- data.table(name = attributes(colS)$names, count = colS)</code>
<code><span class="brown">#most frequent and least frequent words</span>
doc_features[order(-count)][1:10] <span class="brown">#top 10 most frequent words</span>
doc_features[order(count)][1:10] <span class="brown">#least 10 frequent words</code></span>
We can also visualize this table using simple bar plots. 
To avoid overly populated bar chart, let's plot features occurring more than 20,000 times. 

<code>library(ggplot2)
library(ggthemes)
ggplot(doc_features[count&gt;20000],aes(name, count)) +</code> <code>geom_bar(stat = "identity",fill='lightblue',color='black')+</code> <code>theme(axis.text.x = element_text(angle = 45, hjust = 1))+</code> <code>theme_economist()+</code> <code>scale_color_economist()</code>
<img src="http://blog.hackerearth.com/wp-content/uploads/2017/04/doc_words.png">
<code><span class="brown">#check association of terms of top features</span>
findAssocs(new_docterm_corpus,"street",corlimit = 0.5)
findAssocs(new_docterm_corpus,"new",corlimit = 0.5)</code>
A better way to visualize text features is using a wordcloud. 
A wordcloud is composed of words where the size of each word is determined by its frequency in the data. 
In R, we'll use wordcloud package to do this task. 
Let's create two word clouds with different frequencies.
<code><span class="brown">#create wordcloud</span>
library(wordcloud)
wordcloud(names(colS), colS, min.freq = 100, scale = c(6,.1), colors = brewer.pal(6, 'Dark2'))</code>
<img src="http://blog.hackerearth.com/wp-content/uploads/2017/04/word_cloud.png">
<code>wordcloud(names(colS), colS, min.freq = 5000, scale = c(6,.1), colors = brewer.pal(6, 'Dark2'))</code>
<img src="http://blog.hackerearth.com/wp-content/uploads/2017/04/wordcloud-1.png">
Until here, we've chopped the description variable and extracted 87 features out of it. 
The final step is to convert the matrix into a data frame and merge it with new features from 'Feature' column. 

<code><span class="brown">#create data set for training</span>
processed_data &lt;- as.data.table(as.matrix(new_docterm_corpus))</code>
<code><span class="brown">#combing the data</span>
data_one &lt;- cbind(data.table(listing_id = tdata$listing_id, interest_level = tdata$interest_level),processed_data)</code>
<code><span class="brown">#merging the features</span>
data_one &lt;- fdata[data_one, on="listing_id"]</code>
<code><span class="brown">#split the data set into train and test</span>
train_one &lt;- data_one[interest_level != "None"]
test_one &lt;- data_one[interest_level == "None"]
test_one[,interest_level := NULL]</code>
Our data set is ready for training. 
We'll use xgboost algorithm for model training. 
If you are new to xgboost, I suggest you to refer this <a href="http://blog.hackerearth.com/beginners-tutorial-on-xgboost-parameter-tuning-r">beginners tutorial on xgboost</a>. 
At core, xgboost learns from data using boosting algorithms where progressive models are build by capitalizing on errors made by previous models.
Xgboost follows a certain way of dealing with data:
It accepts dependent variable in integer format.
The data should be in matrix format
To check model performance, we can specify a validation set such that we can check validation errors during model training. 
It is enable with both k-fold and hold-out validation strategy. 
For faster training, we'll use hold-out validation strategy.
The training data must not have dependent variable. 
It should be removed. 
Neither, it should have the identifier variable (listing_id)
Keeping its nature in mind, let's prepare the data for xgboost and train our model.
<code>library(caTools)
library(xgboost)</code>
<code><span class="brown">#stratified splitting the data</span>
sp &lt;- sample.split(Y = train_one$interest_level,SplitRatio = 0.6)</code>
<code><span class="brown">#create data for xgboost</span>
xg_val &lt;- train_one[sp]
listing_id &lt;- train_one$listing_id
target &lt;- train_one$interest_level</code>
<code>xg_val_target &lt;- target[sp]
d_train &lt;- xgb.DMatrix(data = as.matrix(train_one[,-c("listing_id","interest_level"),with=F]),label = target)
d_val &lt;- xgb.DMatrix(data = as.matrix(xg_val[,-c("listing_id","interest_level"),with=F]), label = xg_val_target)
d_test &lt;- xgb.DMatrix(data = as.matrix(test_one[,-c("listing_id"),with=F]))
param &lt;- list(booster="gbtree",</code> <code>objective="multi:softprob",</code> <code>eval_metric="mlogloss",</code> <code><span class="brown">#nthread=13,</code> <code>num_class=3,</code> <code>eta = .02,</code> <code>gamma = 1,</code> <code>max_depth = 4,</code> <code>min_child_weight = 1,</code> <code>subsample = .7,</code> <code>colsample_bytree = .5</code> <code>)</code></span>
<code>set.seed(2017)
watch &lt;- list(val=d_val, train=d_train)</code>
<code>xgb2 &lt;- xgb.train(data = d_train,</code> <code>params = param,</code> <code>watchlist=watch,</code> <code>nrounds = 500,</code> <code>print_every_n = 10</code> <code>)</code>
This model returns validation error = 0.6817. 
The score is calculated with multilogloss metric. 
Think of validation error as a proxy performance of model on future data. 
Now, let's create predictions on test data and check our score on <a href="https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/leaderboard">kaggle leaderboard</a>. 

<code>xg_pred &lt;- as.data.table(t(matrix(predict(xgb2, d_test), nrow=3, ncol=nrow(d_test))))
colnames(xg_pred) &lt;- c("high","low","medium")
xg_pred &lt;- cbind(data.table(listing_id = test$listing_id),xg_pred)
fwrite(xg_pred, "xgb_textmining.csv")</code>
It gives us 0.7043 multilogloss score on leaderboard. 
Not bad! We've used only 2 variables (features, description) from the data. 
So, what can be done for improvement ? Obviously, we can pick up non-text variables from the original data and include them in model building. 
Besides that, let's try few more text mining techniques. 

What about TF-IDF matrix ? Here's an exercise for you. 
We created <code>processed_data</code> as a data table from <code>new_docterm_corpus</code>. 
It contained simple 1 and 0 to detect the presence of a new word in the description. 
Let's create a weighted matrix using tf-idf technique. 
It's quite easy to do. 

<code><span class="brown">#TF IDF Data set</span>
data_mining_tf &lt;- as.data.table(as.matrix(weightTfIdf(new_docterm_corpus)))</code>
Here's your <strong>&emsp;exercise one</strong>: Follow the steps above and calculate validation and leaderboard score on this data. 

Does your score improve ? What else can be done ? Think! What about n - gram technique ? Until now, our matrix has one gram features i.e. 
one word per column such as apart, new, building etc. 
Now, we'll create a 2-gram document matrix and check model performance on it. 
To create a n-gram matrix, we'll use NGramTokenizer function from Rweka package.
<code>install.packages("RWeka")
library(RWeka)</code>
<code><span class="brown">#bigram function</span>
Bigram_Tokenizer &lt;- function(x){</code> <code>NGramTokenizer(x, Weka_control(min=2, max=2))</code> <code>}
<span class="brown">#create a matrix</span>
bi_docterm_matrix &lt;- DocumentTermMatrix(text_corpus, control = list(tokenize = Bigram_Tokenizer))</code>
Here's your <strong>&emsp;exercise two</strong>: Create a data table from this <code>bi_docterm_matrix</code> and check your score. 
Your next steps would are similar to what we've done above. 
For your reference, following are the steps you should take:
Remove sparse terms
Explore the new features using bar chart and wordcloud
Convert the matrix into data table
Divide the resultant data table into train and test
Train and Test the models
Once this is done, check the leaderboard score. 
Do let me know in comments if it got improved. 
The complete script of this tutorial can be <a href="https://gist.github.com/saraswatmks/4ed7979f28d184888527b2f8ceb5d1ad">found here</a>. 
 
<h2>Summary</h2>
This tutorial is meant for beginners to get started with building text mining models. 
Considering the massive volume of content being generated by companies, social media these days, there is going to be a surge in demand for people who are well versed with text mining &amp; natural language processing. 

This tutorial illustrates all the necessary steps which one must take while dealing with text data. 
For better understanding, I would suggest you to get your hands on variety of text data sets and use the steps above for analysis. 
Regardless of any programming language you use, these techniques &amp; steps are common in all. 
Did you find this tutorial helpful ? Feel free to drop your suggestions, doubts in the comments below.

<h2><br>Text Mining and Sentiment Analysis: Analysis with R</h2>
In the third article of this series, Sanil Mhatre demonstrates how to perform a sentiment analysis using R including generating a word cloud, word associations, sentiment scores, and emotion classification. 
<b>The series so far:</b>


Introduction
Power BI Visualizations
Analysis with R
Oracle Text
Data Visualization in Tableau
Sentiment Analysis with Python

This is the third article of the “Text Mining and Sentiment Analysis” Series. 

The first article introduced Azure Cognitive Services and demonstrated the setup and use of Text Analytics APIs for extracting key Phrases & Sentiment Scores from text data. 
The second article demonstrated Power BI visualizations for analyzing Key Phrases & Sentiment Scores and interpreting them to gain insights. 
This article explores R for text mining and sentiment analysis. 

I will demonstrate several common text analytics techniques and visualizations in R.
Note: This article assumes basic familiarity with R and RStudio. 
Please jump to the References section for more information on installing R and RStudio. 

The Demo data raw text file and R script are available for download from my GitHub repository; please find the link in the References section.
R is a language and environment for statistical computing and graphics. 
It provides a wide variety of statistical and graphical techniques and is highly extensible. 

R is available as free software. 
It’s easy to learn
and use and can produce well designed publication-quality plots. 

For the demos in this article, I am using R version 3.5.3 (2019-03-11), RStudio Version 1.1.456
The input file for this article has only one column, the “Raw text” of survey responses and is a text file.
A sample of the first few rows are shown in Notepad++ (showing all characters) in Figure 1.

<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-computer-description-automatica.png">
<b>Figure 1. Sample of the input text file</b>
The demo R script and demo input text file are available on my GitHub repo (please find the link in the References section).
R has a rich set of packages for Natural Language Processing (NLP) and generating plots. 

The foundational steps involve loading the text file into an R Corpus, then cleaning and stemming the data before performing analysis. 
I will demonstrate these steps and analysis like Word Frequency, Word Cloud, Word Association, Sentiment Scores and Emotion Classification using various plots and charts.
<strong>Installing and loading R packages</strong>

The following packages are used in the examples in this article:

<b>tm</b> for text mining operations like removing numbers, special characters, punctuations and stop words (Stop words in any language are the most commonly occurring words that have very little value for NLP and should be filtered out. 

Examples of stop words in English are “the”, “is”, “are”.)
<b>snowballc</b> for stemming, which is the process of reducing words to their base or root form. 
For example, a stemming algorithm would reduce the words “fishing”, “fished” and “fisher” to the stem “fish”.

<b>wordcloud</b> for generating the word cloud plot.
<b>RColorBrewer</b> for color palettes used in various plots
<b>syuzhet</b> for sentiment scores and emotion classification

<b>ggplot2</b> for plotting graphs

Open RStudio and create a new R Script. 

Use the following code to install and load these packages.

<span class="brown"># Install</span>
install.packages("tm") <span class="brown"># for text mininginstall.packages("SnowballC")  <span class="brown"># for text stemminginstall.packages("wordcloud")  <span class="brown"># word-cloud generator install.packages("RColorBrewer")  <span class="brown"># color palettesinstall.packages("syuzhet")  <span class="brown"># for sentiment analysisinstall.packages("ggplot2")  <span class="brown"># for plotting graphs <span class="brown"># Load</span>
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")

<strong>Reading file data into R</strong>

The R base function <code>read.table()</code> is generally used to read a file in table format and imports data as a data frame. 
Several variants of this function are available, for importing different file formats;


<b>read.csv() is</b> used for reading comma-separated value (csv) files, where a comma “,” is used a field separator
<b>read.delim()</b> is used for reading tab-separated values (.txt) files


The input file has multiple lines of text and no columns/fields (data is not tabular), so you will use the <code>readLines</code> function. 
This function takes a file (or URL) as input and returns a vector containing as many elements as the number of lines in the file. 
The <code>readLines</code> function simply extracts the text from its input source and returns each line as a character string. 

The <code>n=</code> argument is useful to read a limited number (subset) of lines from the input source (Its default value is -1, which reads all lines from the input source). 
When using the filename in this function’s argument, R assumes the file is in your current working directory (you can use the <code>getwd()</code> function in R console to find your current working directory). 
You can also choose the input file interactively, using the <code>file.choose()</code> function within the argument. 

The next step is to load that Vector as a Corpus. 
In R, a Corpus is a collection of text document(s) to apply text mining or NLP routines on. 
Details of using the <code>readLines</code> function are sourced from: <a href="https://www.stat.berkeley.edu/~spector/s133/Read.html">https://www.stat.berkeley.edu/~spector/s133/Read.html</a> .

In your R script, add the following code to load the data into a corpus.

<span class="brown"># Read the text file from local machine , choose file interactively</span>
text &lt;- readLines(file.choose())
<span class="brown"># Load the data as a corpus</span>
TextDoc &lt;- Corpus(VectorSource(text))

Upon running this, you will be prompted to select the input file. 

Navigate to your file and click <em>Open </em>as shown in Figure 2.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-computer-description-automatica-1.png">
<b>Figure 2. 
Select input file</b>
<strong>Cleaning up Text Data</strong>

Cleaning the text data starts with making transformations like removing special characters from the text. 
This is done using the <code>tm_map()</code> function to replace special characters like <code>/</code>, <code>@</code> and <code>|</code> with a space. 
The next step is to remove the unnecessary whitespace and convert the text to lower case.

Then remove the <em>stopwords</em>. 
They are the most commonly occurring words in a language and have very little value in terms of gaining useful information. 
They should be removed before performing further analysis. 

Examples of stopwords in English are “the, is, at, on<em>”</em>. 
There is no single universal list of stop words used by all NLP tools. 
<code>stopwords</code> in the <code>tm_map()</code> function supports several languages like English, French, German, Italian, and Spanish. 

Please note the language names are case sensitive. 
I will also demonstrate how to add your own list of stopwords, which is useful in this Team Health example for removing non-default stop words like “team”, “company”, “health”. 
Next, remove numbers and punctuation.

The last step is text stemming. 
It is the process of reducing the word to its root form. 
The stemming process simplifies the word to its common origin. 

For example, the stemming process reduces the words “fishing”, “fished” and “fisher” to its stem “fish”. 
Please note stemming uses the <em>SnowballC</em> package. 
(You may want to skip the text stemming step if your users indicate a preference to see the original “unstemmed” words in the word cloud plot)

In your R script, add the following code to transform and run to clean-up the text data.

<span class="brown">#Replacing "/", "@" and "|" with space</span>
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(TextDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")
<span class="brown"># Convert the text to lower case</span>
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
<span class="brown"># Remove numbers</span>
TextDoc <- tm_map(TextDoc, removeNumbers)
<span class="brown"># Remove english common stopwords</span>
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
<span class="brown"># Remove your own stop word</span>
<span class="brown"># specify your custom stopwords as a character vector</span>
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
<span class="brown"># Remove punctuations</span>
TextDoc <- tm_map(TextDoc, removePunctuation)
<span class="brown"># Eliminate extra white spaces</span>
TextDoc <- tm_map(TextDoc, stripWhitespace)
<span class="brown"># Text stemming - which reduces words to their root form</span>
TextDoc <- tm_map(TextDoc, stemDocument)

<strong>Building the term document matrix</strong>

After cleaning the text data, the next step is to count the occurrence of each word, to identify popular or trending topics. 
Using the function <code>TermDocumentMatrix()</code> from the text mining package, you can build a Document Matrix – a table containing the frequency of words.
In your R script, add the following code and run it to see the top 5 most frequently found words in your text.


<span class="brown"># Build a term-document matrix</span>
TextDoc_dtm <- TermDocumentMatrix(TextDoc)
dtm_m <- as.matrix(TextDoc_dtm)
<span class="brown"># Sort by descearing value of frequency</span>
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)
<span class="brown"># Display the top 5 most frequent words</span>
head(dtm_d, 5)

The following table of word frequency is the expected output of the <code>head</code> command on RStudio Console.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/word-image-1.png">

Plotting the top 5 most frequent words using a bar chart is a good basic way to visualize this word frequent data. 
In your R script, add the following code and run it to generate a bar chart, which will display in the <em>Plots</em> sections of RStudio.


<span class="brown"># Plot the most frequent words</span>
barplot(dtm_d[1:5,]$freq, las = 2, names.arg = dtm_d[1:5,]$word,
        col ="lightgreen", main ="Top 5 most frequent words",
        ylab = "Word frequencies")

The plot can be seen in Figure 3.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati.png">

<b>Figure 3. 
Bar chart of the top 5 most frequent words</b>
One could interpret the following from this bar chart:


The most frequently occurring word is “good”. 
Also notice that negative words like “not” don’t feature in the bar chart, which indicates there are no negative prefixes to change the context or meaning of the word “good” ( In short, this indicates most responses don’t mention negative phrases like “not good”).

“work”, “health” and “feel” are the next three most frequently occurring words, which indicate that most people feel good about their work and their team’s health.
Finally, the root “improv” for words like “improve”, “improvement”, “improving”, etc. 
is also on the chart, and you need further analysis to infer if its context is positive or negative


<strong>Generate the Word Cloud</strong>
A word cloud is one of the most popular ways to visualize and analyze qualitative data. 

It’s an image composed of keywords found within a body of text, where the size of each word indicates its frequency in that body of text. 
Use the word frequency data frame (table) created previously to generate the word cloud. 
In your R script, add the following code and run it to generate the word cloud and display it in the <em>Plots</em> section of RStudio.

<span class="brown">#generate word cloud</span>
set.seed(1234)
wordcloud(words = dtm_d$word, freq = dtm_d$freq, min.freq = 5,
          max.words=100, random.order=FALSE, rot.per=0.40, 
          colors=brewer.pal(8, "Dark2"))

Below is a brief description of the arguments used in the word cloud function;


<b>words</b> – words to be plotted
<b>freq</b> – frequencies of words
<b>min.freq</b> – words whose frequency is at or above this threshold value is plotted (in this case, I have set it to 5)

<b>max.words</b> – the maximum number of words to display on the plot (in the code above, I have set it 100)
<b>random.order</b> – I have set it to FALSE, so the words are plotted in order of decreasing frequency
<b>rot.per</b> – the percentage of words that are displayed as vertical text (with 90-degree rotation). 

I have set it 0.40 (40 %), please feel free to adjust this setting to suit your preferences
<b>colors</b> – changes word colors going from lowest to highest frequencies


You can see the resulting word cloud in Figure 4.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-1.png">

<b>Figure 4. 
Word cloud plot</b>
The word cloud shows additional words that occur frequently and could be of interest for further analysis. 

Words like “need”, “support”, “issu” (root for “issue(s)”, etc. 
could provide more context around the most frequently occurring words and help to gain a better understanding of the main themes.
<strong>Word Association</strong>

Correlation is a statistical technique that can demonstrate whether, and how strongly, pairs of variables are related. 
This technique can be used effectively to analyze which words occur most often in association with the most frequently occurring words in the survey responses, which helps to see the context around these words
In your R script, add the following code and run it.


<span class="brown"># Find associations </span>
findAssocs(TextDoc_dtm, terms = c("good","work","health"), corlimit = 0.25)			


You should see the results as shown in Figure 5.
<img   data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-2.png" data-lazy-load="" alt="A screenshot of a cell phone

Description automatically generated">
<b>Figure 5. 
Word association analysis for the top three most frequent terms</b>

This script shows which words are most frequently associated with the top three terms (<code>corlimit = 0.25</code> is the lower limit/threshold I have set. 
You can set it lower to see more words, or higher to see less). 
The output indicates that “integr” (which is the root for word “integrity”) and “synergi” (which is the root for words “synergy”, “synergies”, etc.) and occur 28% of the time with the word “good”. 

You can interpret this as the context around the most frequently occurring word (“good”) is positive. 
Similarly, the root of the word “together” is highly correlated with the word “work”. 
This indicates that most responses are saying that teams “work together” and can be interpreted in a positive context.

You can modify the above script to find terms associated with words that occur at least 50 times or more, instead of having to hard code the terms in your script.

<span class="brown"># Find associations for words that occur at least 50 times</span>
findAssocs(TextDoc_dtm, terms = findFreqTerms(TextDoc_dtm, lowfreq = 50), corlimit = 0.25)

<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-3.png">
<b>Figure 6: Word association output for terms occurring at least 50 times </b>
<strong>Sentiment Scores</strong>

Sentiments can be classified as positive, neutral or negative. 
They can also be represented on a numeric scale, to better express the degree of positive or negative strength of the sentiment contained in a body of text.
This example uses the Syuzhet package for generating sentiment scores, which has four sentiment dictionaries and offers a method for accessing the sentiment extraction tool developed in the NLP group at Stanford. 

The <code>get_sentiment</code> function accepts two arguments: a character vector (of sentences or words) and a method. 
The selected method determines which of the four available sentiment extraction methods will be used. 
The four methods are <code>syuzhet</code> (this is the default), <code>bing</code>, <code>afinn</code> and <code>nrc</code>. 

Each method uses a different scale and hence returns slightly different results. 
Please note the outcome of <code>nrc</code> method is more than just a numeric score, requires additional interpretations and is out of scope for this article. 
The descriptions of the <code>get_sentiment</code> function has been sourced from : <a href="https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?">https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?</a>

Add the following code to the R script and run it.

<span class="brown"># regular sentiment score using get_sentiment() function and method of your choice</span>
<span class="brown"># please note that different methods may have different scales</span>
syuzhet_vector <- get_sentiment(text, method="syuzhet")
<span class="brown"># see the first row of the vector</span>
head(syuzhet_vector)
<span class="brown"># see summary statistics of the vector</span>
summary(syuzhet_vector)

Your results should look similar to Figure 7.

<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-4.png">
<b>Figure 7. Syuzhet vector</b>

An inspection of the Syuzhet vector shows the first element has the value of <em>2.60</em>. 
It means the sum of the sentiment scores of all meaningful words in the first response(line) in the text file, adds up to 2.60. 

The scale for sentiment scores using the <code>syuzhet</code> method is decimal and ranges from -1(indicating most negative) to +1(indicating most positive). 
Note that the summary statistics of the <code>suyzhet</code> vector show a median value of 1.6, which is above zero and can be interpreted as the overall average sentiment across all the responses is positive.
Next, run the same analysis for the remaining two methods and inspect their respective vectors. 

Add the following code to the R script and run it.

<span class="brown"># bing</span>
bing_vector <- get_sentiment(text, method="bing")
head(bing_vector)
summary(bing_vector)
<span class="brown">#affin</span>
afinn_vector <- get_sentiment(text, method="afinn")
head(afinn_vector)
summary(afinn_vector)

Your results should resemble Figure 8.

<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-5.png">
<b>Figure 8. bing and afinn vectors</b>
Please note the scale of sentiment scores generated by:


<b>bing</b> – binary scale with -1 indicating negative and +1 indicating positive sentiment
<b>afinn</b> – integer scale ranging from -5 to +5


The summary statistics of <code>bing</code> and <code>afinn</code> vectors also show that the <code>Median</code> value of Sentiment scores is above 0 and can be interpreted as the overall average sentiment across the all the responses is positive.
Because these different methods use different scales, it’s better to convert their output to a common scale before comparing them. 
This basic scale conversion can be done easily using R’s built-in <code>sign</code> function, which converts all positive number to 1, all negative numbers to -1 and all zeros remain 0.

Add the following code to your R script and run it.

<span class="brown">#compare the first row of each vector using sign function</span>
rbind(
  sign(head(syuzhet_vector)),
  sign(head(bing_vector)),
  sign(head(afinn_vector))
)

Figure 9 shows the results.
<img   data-src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-6.png" data-lazy-load="" alt="A screenshot of a cell phone
Description automatically generated">
<b> Figure 9. 

Normalize scale and compare three vectors</b>
Note the first element of each row (vector) is <em>1</em>, indicating that all three methods have calculated a positive sentiment score, for the first response (line) in the text.
<strong>Emotion Classification</strong>

Emotion classification is built on the NRC Word-Emotion Association Lexicon (aka EmoLex). 
The definition of “NRC Emotion Lexicon”, sourced from <a href="http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm">http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm</a> is “The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). 
The annotations were manually done by crowdsourcing.”

To understand this, explore the <code>get_nrc_sentiments</code> function, which returns a data frame with each row representing a sentence from the original file. 
The data frame has ten columns (one column for each of the eight emotions, one column for positive sentiment valence and one for negative sentiment valence). 
The data in the columns (anger, anticipation, disgust, fear, joy, sadness, surprise, trust, negative, positive) can be accessed individually or in sets. 

The definition of <code>get_nrc_sentiments</code> has been sourced from: <a href="https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?">https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html?</a>
Add the following line to your R script and run it, to see the data frame generated from the previous execution of the <code>get_nrc_sentiment</code> function.


<span class="brown"># run nrc sentiment analysis to return data frame with each row classified as one of the following</span>
<span class="brown"># emotions, rather than a score: </span>
<span class="brown"># anger, anticipation, disgust, fear, joy, sadness, surprise, trust </span>
<span class="brown"># It also counts the number of positive and negative emotions found in each row</span>
d<-get_nrc_sentiment(text)
<span class="brown"># head(d,10) - to see top 10 lines of the get_nrc_sentiment dataframe</span>
head (d,10)

The results should look like Figure 10.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-7.png">

<b>Figure 10. Data frame returned by get_nrc_sentiment function</b>
The output shows that the first line of text has;


Zero occurrences of words associated with emotions of anger, disgust, fear, sadness and surprise
One occurrence each of words associated with emotions of anticipation and joy

Two occurrences of words associated with emotions of trust
Total of one occurrence of words associated with negative emotions
Total of two occurrences of words associated with positive emotions


The next step is to create two plots charts to help visually analyze the emotions in this survey text. 
First, perform some data transformation and clean-up steps before plotting charts. 

The first plot shows the total number of instances of words in the text, associated with each of the eight emotions. 
Add the following code to your R script and run it.


<span class="brown">#transpose</span>
td<-data.frame(t(d))
<span class="brown">#The function rowSums computes column sums across rows for each level of a grouping variable.</span>
td_new <- data.frame(rowSums(td[2:253]))
<span class="brown">#Transformation and cleaning</span>
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
<span class="brown">#Plot One - count of words associated with each sentiment</span>
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")

You can see the bar plot in Figure 11.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-8.png">

<b>Figure 11. Bar Plot showing the count of words in the text, associated with each emotion</b>
This bar chart demonstrates that words associated with the positive emotion of “trust” occurred about five hundred times in the text, whereas words associated with the negative emotion of “disgust” occurred less than 25 times. 

A deeper understanding of the overall emotions occurring in the survey response can be gained by comparing these number as a percentage of the total number of meaningful words. 
Add the following code to your R script and run it.

<span class="brown">#Plot two - count of words associated with each sentiment, expressed as a percentage</span>
barplot(
  sort(colSums(prop.table(d[, 1:8]))), 
  horiz = TRUE, 
  cex.names = 0.7, 
  las = 1, 
  main = "Emotions in Text", xlab="Percentage"
)

The Emotions bar plot can be seen in figure 12.
<img src="https://www.red-gate.com/simple-talk/wp-content/uploads/2020/05/a-screenshot-of-a-cell-phone-description-automati-9.png">

<b>Figure 12. Bar Plot showing the count of words associated with each sentiment expressed as a percentage </b>
This bar plot allows for a quick and easy comparison of the proportion of words associated with each emotion in the text. 

The emotion “trust” has the longest bar and shows that words associated with this positive emotion constitute just over 35% of all the meaningful words in this text. 
On the other hand, the emotion of “disgust” has the shortest bar and shows that words associated with this negative emotion constitute less than 2% of all the meaningful words in this text. 
Overall, words associated with the positive emotions of “trust” and “joy” account for almost 60% of the meaningful words in the text, which can be interpreted as a good sign of team health.

<strong>Conclusion</strong>
This article demonstrated reading text data into R, data cleaning and transformations. 
It demonstrated how to create a word frequency table and plot a word cloud, to identify prominent themes occurring in the text. 

Word association analysis using correlation, helped gain context around the prominent themes. 
It explored four methods to generate sentiment scores, which proved useful in assigning a numeric value to strength (of positivity or negativity) of sentiments in the text and allowed interpreting that the average sentiment through the text is trending positive. 
Lastly, it demonstrated how to implement an emotion classification with NRC sentiment and created two plots to analyze and interpret emotions found in the text.


<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... 
more custom settings?
});
</script>
</pre></body></html>