<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
img{
	margin: 2%;
}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #002010}
pre { color: gray; background-color: #001010; font-size: 80%;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
.topic{
    color: lime;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 15%;
	margin-right: 15%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head><body>

<center><h3>Understanding Support Vector Machine algorithm from examples</h3></center>
<div id="toc"><ul></ul></div>
<br>



<h2>Introduction</h2>
<p>
Most of the beginners start by learning regression. It is simple to learn and use, but does not solve our purpose! Because, you can do so much more than just Regression!</p>
<p>Regression is a sword capable of slicing and dicing data efficiently, but incapable of dealing with highly complex data. On the contrary, &#8216;Support Vector Machines&#8217; is like a sharp knife &#8211; it works on smaller datasets, but on them, it can be much more stronger and powerful in building models.</p>
<p>By now, I hope you&#8217;ve now mastered 
<a href="https://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/" target="_blank">Random Forest</a>, 
<a href="https://www.analyticsvidhya.com/blog/2015/09/naive-bayes-explained/" target="_blank">Naive Bayes Algorithm</a> and 
<a href="https://www.analyticsvidhya.com/blog/2015/09/questions-ensemble-modeling/" target="_blank">Ensemble Modeling</a>. If not, I&#8217;d suggest you to take out few minutes and read about them as well. In this article, I shall guide you through the basics to advanced knowledge of a crucial machine learning algorithm, support vector machines.</p>
<p>
<h3>Table of Contents</h3>
<ol>
<li>What is Support Vector Machine?</li>
<li>How does it work?</li>
<li>How to implement SVM in Python and R?</li>
<li>How to tune Parameters of SVM?</li>
<li>Pros and Cons associated with SVM</li>
</ol>

<h2>What is Support Vector Machine?</h2>
<p>&#8220;Support Vector Machine&#8221; (SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However,  it is mostly used in classification problems. In this algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiate the two classes very well (look at the below snapshot).</p>
<p>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_1.png"></p>
<p>Support Vectors are simply the co-ordinates of individual observation. Support Vector Machine is a frontier 疆界 which best segregates 隔离 the two classes (hyper-plane/ line).</p>
<p>You can look at 
<a href="https://www.analyticsvidhya.com/blog/2014/10/support-vector-machine-simplified/" target="_blank">definition of support vectors</a> and a few examples of its working here.</p>

<h2>How does it work?</h2>
<p>Above, we got accustomed to the process of segregating the two classes with a hyper-plane. Now the burning question is &#8220;How can we identify the right hyper-plane?&#8221;. Don&#8217;t worry, it&#8217;s not as hard as you think!</p>
<p>Let&#8217;s understand:</p>
<ul>
<li><strong>Identify the right hyper-plane (Scenario-1): </strong>Here, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle.<br />
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_21.png"><br>
You need to remember a thumb rule to identify the right hyper-plane: &#8220;Select the hyper-plane which segregates the two classes better&#8221;. In this scenario, hyper-plane &#8220;B&#8221; has excellently performed this job.</li>
<li><strong>Identify the right hyper-plane (Scenario-2): </strong>Here, we have three hyper-planes (A, B and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane?<br>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_3.png"><br>
Here, maximizing the distances between nearest data point (either class) and hyper-plane will help us to decide the right hyper-plane. This distance is called as <strong>Margin</strong>. Let&#8217;s look at the below snapshot:<br>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_4.png"><br />
Above, you can see that the margin for hyper-plane C is high as compared to both A and B. Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.</p>
</li>
<li><strong>Identify the right hyper-plane (Scenario-3):</strong>Hint:<strong> </strong>Use the rules as discussed in previous section to identify the right hyper-plane</li>
</ul>
<p style="padding-left: 30px;">
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_5.png"><br>
Some of you may have selected the hyper-plane <strong>B </strong>as it has higher margin compared to <strong>A. </strong>But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is <strong>A.</strong></p>
<ul>
<li><strong>Can we classify two classes (Scenario-4)?: </strong>Below, I am unable to segregate the two classes using a straight line, as one of star lies in the territory of other(circle) class as an outlier. <br />

<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_61.png"><br>As I have already mentioned, one star at other end is like an outlier for star class. SVM has a feature to ignore outliers and find the hyper-plane that has maximum margin. Hence, we can say, SVM is robust to outliers.<br />
<strong>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_71.png"></strong></li>
<li><strong>Find the hyper-plane to segregate to classes (Scenario-5): </strong>In the scenario below, we can&#8217;t have linear hyper-plane between the two classes, so how does SVM classify these two classes? Till now, we have only looked at the linear hyper-plane.
<br>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_8.png"><br>
<b class="topic">SVM can solve this problem. Easily!</b> It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let&#8217;s plot the data points on axis x and z:<br />

<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_9.png"><br />
In above plot, points to consider are:</p>
<ul>
<li>All values for z would be positive always because z is the squared sum of both x and y</li>
<li>In the original plot, red circles appear close to the origin of x and y axes, leading to lower value of z and star relatively away from the origin result to higher value of z.</li>
</ul>
<p>In SVM, it is easy to have a linear hyper-plane between these two classes. But, another burning question which arises is, should we need to add this feature manually to have a hyper-plane. No, SVM has a technique called the <strong>
<a href="https://en.wikipedia.org/wiki/Kernel_method" target="_blank" rel="nofollow noopener">kernel</a> trick</strong>. These are functions which takes low dimensional input space and transform it to a higher dimensional space i.e. it converts not separable problem to separable problem, these functions are called kernels. It is mostly useful in non-linear separation problem. Simply put, it does some extremely complex data transformations, then find out the process to separate the data based on the labels or outputs you&#8217;ve defined.</p>
<p>When we look at the hyper-plane in original input space it looks like a circle:<br />

<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_10.png"></li>
</ul>
<p>Now, let&#8217;s  look at the methods to apply SVM algorithm in a data science challenge.</p>

<h2>How to implement SVM in Python and R?</h2>
<p>In Python, <b class="topic">scikit-learn</b> is a widely used library for implementing machine learning algorithms, SVM is also available in scikit-learn library and follow the same structure (Import library, object creation, fitting model and prediction). Let&#8217;s look at the below code:</p>
<pre><span class="kn">#Import Library
</span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> svm
<span class="c">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset
</span><span class="c"># Create SVM classification object 
</span>model = svm.svc(kernel='linear', c=1, gamma=1) 
<span class="c"># there is various option associated with it, like changing kernel, gamma and C value. Will discuss more # about it in next section.</span><span class="c">Train the model using the training sets and check score
</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)
</span><span class="n">model</span><span class="o">.score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)
</span><span class="c">#Predict Output
predicted= model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span></pre>
<p>The <b class="topic">e1071 package in R</b> is used to create Support Vector Machines with ease. It has helper functions as well as code for the Naive Bayes Classifier. The creation of a support vector machine in R and Python follow similar approaches, let&#8217;s take a look now at the following code:</p>
<pre class="goldword">#Import Library
require(e1071) #Contains the SVM 
Train &lt;- read.csv(file.choose())
Test &lt;- read.csv(file.choose())
# there are various options associated with SVM training; like changing kernel, gamma and C value.

# create model
model &lt;- svm(Target~Predictor1+Predictor2+Predictor3,data=Train,kernel='linear',gamma=0.2,cost=100)

#Predict Output
preds &lt;- predict(model,Test)
table(preds)</pre>

<h2>How to tune Parameters of SVM?</h2>
<p>Tuning parameters value for machine learning algorithms effectively improves the model performance. Let&#8217;s look at the list of parameters available with SVM.</p>
<pre><tt class="descclassname">sklearn.svm.</tt><tt class="descname">SVC</tt><big>(</big><em>C=1.0</em>, <em>kernel='rbf'</em>, <em>degree=3</em>, <em>gamma=0.0</em>, <em>coef0=0.0</em>, <em>shrinking=True</em>, <em>probability=False</em>,<em>tol=0.001</em>, <em>cache_size=200</em>, <em>class_weight=None</em>, <em>verbose=False</em>, <em>max_iter=-1</em>, <em>random_state=None</em><big>)</big></pre>
<p>I am going to discuss about some important parameters having higher impact on model performance, &#8220;kernel&#8221;, &#8220;gamma&#8221; and &#8220;C&#8221;.</p>
<p><strong>kernel</strong>: We have already discussed about it. Here, we have various options available with kernel like, &#8220;linear&#8221;, &#8220;rbf&#8221;,&#8221;poly&#8221; and others (default value is &#8220;rbf&#8221;).  Here &#8220;rbf&#8221; and &#8220;poly&#8221; are useful for non-linear hyper-plane. Let&#8217;s look at the example, where we&#8217;ve used linear kernel on two feature of iris data set to classify their class.</p>
<p><strong>Example: </strong>Have linear kernel</p>
<pre>import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets</pre>
<pre># import some data to play with
iris = datasets.load_iris()
X = iris.data[:, :2] # we only take the first two features. We could
 # avoid this ugly slicing by using a two-dim dataset
y = iris.target</pre>
<pre># we create an instance of SVM and fit out data. We do not scale our
# data since we want to plot the support vectors
C = 1.0 # SVM regularization parameter
svc = svm.SVC(kernel='linear', C=1,gamma=0).fit(X, y)</pre>
<pre># create a mesh to plot in
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
h = (x_max / x_min)/100
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
 np.arange(y_min, y_max, h))</pre>
<pre>plt.subplot(1, 1, 1)
Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.8)</pre>
<pre>plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('SVC with linear kernel')
plt.show()
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_111.png"></pre>
<p><strong>Example: </strong>Have rbf kernel</p>
<p>Change the kernel type to rbf in below line and look at the impact.</p>
<pre>svc = svm.SVC(kernel='rbf', C=1,gamma=0).fit(X, y)</pre>
<h3>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_12.png"></h3>
<p>I would suggest you to go for linear kernel if you have large number of features (&gt;1000) because it is more likely that the data is linearly separable in high dimensional space. Also, you can RBF but do not forget to cross validate for its parameters as to avoid over-fitting.</p>
<p><strong>gamma</strong>: Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. Higher the value of gamma, will try to exact fit the as per training data set i.e. generalization error and cause over-fitting problem.</p>
<p><strong>Example: </strong>Let&#8217;s difference if we have gamma different gamma values like 0, 10 or 100.</p>
<pre>svc = svm.SVC(kernel='rbf', C=1,gamma=0).fit(X, y)


<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_15.png"></pre>
<p><strong>C: </strong>Penalty parameter C of the error term. It also controls the trade off between smooth decision boundary and classifying the training points correctly.</p>
<p>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_18.png"></p>
<p>We should always look at the cross validation score to have effective combination of these parameters and avoid over-fitting.</p>
<p>In R, SVMs can be tuned in a similar fashion as they are in Python. Mentioned below are the respective parameters for e1071 package:</p>
<ul>
<li>The kernel parameter can be tuned to take &#8220;Linear&#8221;,&#8221;Poly&#8221;,&#8221;rbf&#8221; etc.</li>
<li>The gamma value can be tuned by setting the &#8220;Gamma&#8221; parameter.</li>
<li>The C value in Python is tuned by the &#8220;Cost&#8221; parameter in R.</li>
</ul>

<h2>Pros and Cons associated with SVM</h2>
<ul>
<li><strong>Pros:</strong>
<ul>
<li>It works really well with clear margin of separation</li>
<li>It is effective in high dimensional spaces.</li>
<li>It is effective in cases where number of dimensions is greater than the number of samples.</li>
<li>It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.</li>
</ul>
</li>
<li><strong>Cons:</strong>
<ul>
<li>It doesn&#8217;t perform well, when we have large data set because the required training time is higher</li>
<li>It also doesn&#8217;t perform very well, when the data set has more noise i.e. target classes are overlapping</li>
<li>SVM doesn&#8217;t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. It is related SVC method of Python scikit-learn library.</li>
</ul>
</li>
</ul>
<h2>Practice Problem</h2>
<p>Find right additional feature to have a hyper-plane for segregating the classes in below snapshot:</p>
<p>
<img src="https://www.analyticsvidhya.com/wp-content/uploads/2015/10/SVM_19.png"></p>
<p>Answer the variable name in the comments section below. I&#8217;ll shall then reveal the answer.</p>

<h2>End Notes</h2>
<p>In this article, we looked at the machine learning algorithm, Support Vector Machine in detail.  I discussed its concept of working, process of implementation in python, the tricks to make the model efficient by tuning its parameters, Pros and Cons, and finally a problem to solve. I would suggest you to use SVM and analyse the power of this model by tuning the parameters. I also want to hear your experience with SVM, how have you tuned parameters to avoid over-fitting and reduce the training time?</p>

<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<li><a href="' + href + '" target="_self">' + text + '</a></li>');
    }

    $('h2, b.topic').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
