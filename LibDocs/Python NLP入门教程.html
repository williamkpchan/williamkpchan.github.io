<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #001010; font-size: 18px;}
pre { color: gray; background-color: #001010; font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
img{
	margin-top:1%;
	margin-bottom:2%;
}
.topic{
    color: lime;
}
.goldsha {
    color: white;
    border: 1px solid gold;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px gold inset;
}
.redsha {
    color: gold;
    border: 1px solid red;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px red inset;
}
.whitesha {
    color: red;
    border: 1px solid white;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -3px -2px 3px white inset;
}
.orangesha {
    color: yellow;
    border: 1px solid orange;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px orange inset;
}
.yellowsha {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
	display: inline-block;
}
.greensha {
    color: lightblue;
    border: 1px solid green;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px green inset;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.yellowbord {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
}
.bluebord {
    color: white;
    border: 1px solid lightblue;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px silver inset;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 15%;
	margin-right: 15%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.brownword{
    color: brown;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .brownword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head><body>

<center><h3>Python NLP入门教程</h3></center>
<div id="toc"><ul></ul></div>
<br>
<br>
<br>

<h2>Python NLP入门教程</h2>
本文简要介绍Python自然语言处理(NLP)，使用Python的NLTK库。<br>
NLTK是Python的自然语言处理工具包，在NLP领域中，最常使用的一个Python库。<br>

<br>
<b class="brownword">什么是NLP？</b>
<br>
简单来说，自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。<br>

<br>
这里讨论一些自然语言处理(NLP)的实际应用例子，如<b class="goldword">语音识别、语音翻译、理解完整的句子、理解匹配词的同义词，以及生成语法正确完整句子和段落</b>。<br>

<br>
这并不是NLP能做的所有事情。<br>

<br>
<b class="brownword">NLP实现</b>
<br>
搜索引擎: 比如谷歌，Yahoo等。<br>
谷歌搜索引擎知道你是一个技术人员，所以它显示与技术相关的结果；
<br>
社交网站推送:比如Facebook News Feed。<br>
如果News Feed算法知道你的兴趣是自然语言处理，就会显示相关的广告和帖子。<br>

<br>
语音引擎:比如Apple的Siri。<br>

<br>
垃圾邮件过滤:如谷歌垃圾邮件过滤器。<br>
和普通垃圾邮件过滤不同，它通过了解邮件内容里面的的深层意义，来判断是不是垃圾邮件。<br>

<br>
<b class="brownword">NLP库</b>
<br>
下面是一些开源的自然语言处理库(NLP)：
<br>
<ul class=" list-paddingleft-2" style="list-style-type: disc;">
<li>
Natural language toolkit (NLTK);
<br>
</li>
<li>
Apache OpenNLP;
<br>
</li>
<li>
Stanford NLP suite;
<br>
</li>
<li>
Gate NLP library
<br>
</li>
</ul>
<br>
其中自然语言工具包(NLTK)是最受欢迎的自然语言处理库(NLP)，它是用Python编写的，而且背后有非常强大的社区支持。<br>

<br>
NLTK也很容易上手，实际上，它是最简单的自然语言处理(NLP)库。<br>

<br>
在这个NLP教程中，我们将使用Python NLTK库。<br>

<br>
<b class="brownword">安装 NLTK</b>
<br>
如果您使用的是Windows/Linux/Mac，您可以使用pip安装NLTK:
<br>
<blockquote>
pip install nltk
<br>
</blockquote>
<br>
打开python终端导入NLTK检查NLTK是否正确安装：
<br>
<blockquote>
import nltk
<br>
</blockquote>
<br>
如果一切顺利，这意味着您已经成功地安装了NLTK库。<br>
首次安装了NLTK，需要通过运行以下代码来安装NLTK扩展包:
<br>
<blockquote>
import nltk<br>
nltk.download()
</blockquote>
<br>
这将弹出NLTK 下载窗口来选择需要安装哪些包:
<br>
<img src="http://mmbiz.qpic.cn/mmbiz_png/fhujzoQe7TrDIJVFhpbVtXxkeDibjSGvzGklKBTWackuYQibNSJ1FEJ9Q2l7WgMl5FWh7AibNneFBdDNJyiapicMa7A/0">
<br>
您可以安装所有的包，因为它们的大小都很小，所以没有什么问题。<br>

<br>
<b class="brownword">使用Python Tokenize文本</b>
<br>
首先，我们将抓取一个web页面内容，然后分析文本了解页面的内容。<br>

<br>
我们将使用urllib模块来抓取web页面:
<br>
<blockquote>
import urllib.request
<br>
response  =  urllib.request.urlopen('http://php.net/')
<br>
html = response.read()
<br>
print(html)
<br>
</blockquote>
<br>
从打印结果中可以看到，结果包含许多需要清理的HTML标签。<br>

<br>
然后BeautifulSoup模块来清洗这样的文字:
<br>
<blockquote>
from bs4 import BeautifulSoup
<br>
import urllib.request
<br>
response = urllib.request.urlopen('http://php.net/')<br>
html = response.read()<br>
soup = BeautifulSoup(html,"html5lib")<br>
# 这需要安装html5lib模块
<br>
text = soup.get_text(strip=True)<br>
print(text)<br>
</blockquote>
<br>
现在我们从抓取的网页中得到了一个干净的文本。<br>

<br>
下一步，将文本转换为tokens,像这样:
<br>
<blockquote>
from bs4 import BeautifulSoup
<br>
import urllib.request
<br>
response = urllib.request.urlopen('http://php.net/')<br>
html = response.read()<br>
soup = BeautifulSoup(html,"html5lib")<br>
text = soup.get_text(strip=True)<br>
tokens = text.split()<br>
print(tokens)<br>
</blockquote>
<br>
<b class="brownword">统计词频</b>
<br>
text已经处理完毕了，现在使用Python NLTK统计token的频率分布。<br>

<br>
可以通过调用NLTK中的FreqDist()方法实现:
<br>
<blockquote>
from bs4 import BeautifulSoup
<br>
import urllib.request
<br>
import nltk
<br>
response = urllib.request.urlopen('http://php.net/')<br>
html = response.read()<br>
soup = BeautifulSoup(html,"html5lib")<br>
text = soup.get_text(strip=True)<br>
tokens = text.split()<br>
freq = nltk.FreqDist(tokens)<br>
for key,val in freq.items():
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
print(str(key) + ':' + str(val))<br>
</blockquote>
<br>
如果搜索输出结果，可以发现最常见的token是PHP。<br>

<br>
您可以调用plot函数做出频率分布图:
<br>
<blockquote>
freq.plot(20, cumulative=False)<br>
# 需要安装matplotlib库
<br>
</blockquote>
<br>
<img src="http://mmbiz.qpic.cn/mmbiz_png/fhujzoQe7TrDIJVFhpbVtXxkeDibjSGvzb3nMu4Kt1trrN10IfgLp1rJT7CRpLheKuricSnWhWys6V6MsnHe9mIQ/0">
<br>
这上面这些单词。<br>
比如of,a,an等等，这些词都属于停用词。<br>

<br>
一般来说，停用词应该删除，防止它们影响分析结果。<br>

<br>
<b class="brownword">处理停用词</b>
<br>
NLTK自带了许多种语言的停用词列表，如果你获取英文停用词:
<br>
<blockquote>
from nltk.corpus import stopwords
<br>
stopwords.words('english')<br>
</blockquote>
<br>
现在，修改下代码,在绘图之前清除一些无效的token:
<br>
<blockquote>
clean_tokens = list()<br>
sr = stopwords.words('english')<br>
for token in tokens:
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
if token not in sr :<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
clean_tokens.append(token)<br>
</blockquote>
<br>
最终的代码应该是这样的:
<br>
<blockquote>
from bs4 import BeautifulSoup
<br>
import urllib.request
<br>
import nltk
<br>
from nltk.corpus import stopwords
<br>
response = urllib.request.urlopen('http://php.net/')<br>
html = response.read()<br>
soup = BeautifulSoup(html,"html5lib")<br>
text = soup.get_text(strip=True)<br>
tokens = text.split()<br>
clean_tokens = list()<br>
sr = stopwords.words('english')<br>
for token in tokens :<br>
&nbsp;&nbsp;&nbsp;&nbsp;
if not token in sr :<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
clean_tokens.append(token)<br>
freq = nltk.FreqDist(clean_tokens)<br>
for key,val in freq.items():
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
print(str(key) + ':' + str(val))<br>
</blockquote>
<br>
现在再做一次词频统计图，效果会比之前好些，因为剔除了停用词:
<br>
<blockquote>
freq.plot(20,cumulative=False)
<br>
</blockquote>
<br>
<img src="http://mmbiz.qpic.cn/mmbiz_png/fhujzoQe7TrDIJVFhpbVtXxkeDibjSGvzvSnpSAsh3tNURGpib9jhOzL9KI5mTpSiciaM4Y7Jv3vRLClz5pCbrBE6A/0">
<br>
<b class="brownword">使用NLTK Tokenize文本</b>
<br>
在之前我们用split方法将文本分割成tokens，现在我们使用NLTK来Tokenize文本。<br>

<br>
文本没有Tokenize之前是无法处理的，所以对文本进行Tokenize非常重要的。<br>
token化过程意味着将大的部件分割为小部件。<br>

<br>
你可以将段落tokenize成句子，将句子tokenize成单个词，NLTK分别提供了句子tokenizer和单词tokenizer。<br>

<br>
假如有这样这段文本:
<br>
<blockquote>
Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude
<br>
</blockquote>
<br>
使用句子tokenizer将文本tokenize成句子:
<br>
<blockquote>
from nltk.tokenize import sent_tokenize
<br>
mytext = "Hello Adam, how are you? I hope everything is going well. Today is a good day, see you dude."
<br>
print(sent_tokenize(mytext))<br>
</blockquote>
<br>
输出如下:
<br>
<blockquote>
['Hello Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']
<br>
</blockquote>
<br>
这是你可能会想，这也太简单了，不需要使用NLTK的tokenizer都可以，直接使用正则表达式来拆分句子就行，因为每个句子都有标点和空格。<br>

<br>
那么再来看下面的文本:
<br>
<blockquote>
Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude.
<br>
</blockquote>
<br>
这样如果使用标点符号拆分,Hello Mr将会被认为是一个句子，如果使用NLTK:
<br>
<blockquote>
from nltk.tokenize import sent_tokenize
<br>
mytext = "Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude."
<br>
print(sent_tokenize(mytext))<br>
</blockquote>
<br>
输出如下:
<br>
<blockquote>
['Hello Mr. Adam, how are you?', 'I hope everything is going well.', 'Today is a good day, see you dude.']
<br>
</blockquote>
<br>
这才是正确的拆分。<br>

<br>
接下来试试单词tokenizer:
<br>
<blockquote>
from nltk.tokenize import word_tokenize
<br>
mytext = "Hello Mr. Adam, how are you? I hope everything is going well. Today is a good day, see you dude."
<br>
print(word_tokenize(mytext))<br>
</blockquote>
<br>
输出如下:
<br>
<blockquote>
['Hello', 'Mr.', 'Adam', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'Today', 'is', 'a', 'good', 'day', ',', 'see', 'you', 'dude', '.']
<br>
</blockquote>
<br>
Mr.这个词也没有被分开。<br>
NLTK使用的是punkt模块的PunktSentenceTokenizer，它是NLTK.tokenize的一部分。<br>
而且这个tokenizer经过训练，可以适用于多种语言。<br>

<br>
<b class="brownword">非英文Tokenize</b>
<br>
Tokenize时可以指定语言:
<br>
<blockquote>
from nltk.tokenize import sent_tokenize
<br>
mytext = "Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour."
<br>
print(sent_tokenize(mytext,"french"))<br>
</blockquote>
<br>
输出结果如下:
<br>
<blockquote>
['Bonjour M. Adam, comment allez-vous?', "J'espère que tout va bien.", "Aujourd'hui est un bon jour."]
<br>
</blockquote>
<br>
<b class="brownword">同义词处理</b>
<br>
使用nltk.download()安装界面，其中一个包是WordNet。<br>

<br>
WordNet是一个为自然语言处理而建立的数据库。<br>
它包括一些同义词组和一些简短的定义。<br>

<br>
您可以这样获取某个给定单词的定义和示例:
<br>
<blockquote>
from nltk.corpus import wordnet
<br>
syn = wordnet.synsets("pain")<br>
print(syn[0].definition())<br>
print(syn[0].examples())<br>
</blockquote>
<br>
输出结果是:
<br>
<blockquote>
a symptom of some physical hurt or disorder
<br>['the patient developed severe pain and distension']
<br>
</blockquote>
<br>
WordNet包含了很多定义：
<br>
<blockquote>
from nltk.corpus import wordnet
<br>
syn = wordnet.synsets("NLP")<br>
print(syn[0].definition())<br>
syn = wordnet.synsets("Python")<br>
print(syn[0].definition())<br>
</blockquote>
<br>
结果如下:
<br>
<blockquote>
the branch of information science that deals with natural language information
<br>
large Old World boas
<br>
</blockquote>
<br>
可以像这样使用WordNet来获取同义词:
<br>
<blockquote>
from nltk.corpus import wordnet
<br>
synonyms = []
<br>
for syn in wordnet.synsets('Computer'):
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
for lemma in syn.lemmas():
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
synonyms.append(lemma.name())<br>
print(synonyms)<br>
</blockquote>
<br>
输出:
<br>
<blockquote>
['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system', 'calculator', 'reckoner', 'figurer', 'estimator', 'computer']
<br>
</blockquote>
<br>
<b class="brownword">反义词处理</b>
<br>
也可以用同样的方法得到反义词：
<br>
<blockquote>
from nltk.corpus import wordnet
<br>
antonyms = []
<br>
for syn in wordnet.synsets("small"):
<br>
&nbsp;&nbsp;&nbsp;&nbsp;
for l
 in syn.lemmas():
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
if
 
l.antonyms():
<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
antonyms.append(l.antonyms
()[
0].name())<br>
print(antonyms)<br>
</blockquote>
<br>
输出:
<br>
<blockquote>
['large', 'big', 'big']
<br>
</blockquote>
<br>
<b class="brownword">词干提取</b>
<br>
语言形态学和信息检索里，词干提取是去除词缀得到词根的过程，例如working的词干为work。<br>

<br>
搜索引擎在索引页面时就会使用这种技术，所以很多人为相同的单词写出不同的版本。<br>

<br>
有很多种算法可以避免这种情况，最常见的是波特词干算法。<br>
NLTK有一个名为PorterStemmer的类，就是这个算法的实现:
<br>
<blockquote>
from nltk.stem import PorterStemmer
<br>
stemmer = PorterStemmer()<br>
print(stemmer.stem('working'))<br>
print(stemmer.stem('worked'))<br>
</blockquote>
<br>
输出结果是:
<br>
<blockquote>
work
<br>
work
<br>
</blockquote>
<br>
还有其他的一些词干提取算法，比如 Lancaster词干算法。<br>

<br>
<b class="brownword">非英文词干提取</b>
<br>
除了英文之外，SnowballStemmer还支持13种语言。<br>

<br>
支持的语言:
<br>
<blockquote>
from nltk.stem import SnowballStemmer
<br>
print(SnowballStemmer.languages)<br>
</blockquote>
<br>
<blockquote>
'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish'
<br>
</blockquote>
<br>
你可以使用SnowballStemmer类的stem函数来提取像这样的非英文单词：
<br>
<blockquote>
from nltk.stem import SnowballStemmer
<br>
french_stemmer = SnowballStemmer('french')<br>
print(french_stemmer.stem("French word"))<br>
</blockquote>
<br>
<b class="brownword">单词变体还原</b>
<br>
单词变体还原类似于词干，但不同的是，变体还原的结果是一个真实的单词。<br>
不同于词干，当你试图提取某些词时，它会产生类似的词:
<br>
<blockquote>
from nltk.stem import PorterStemmer
<br>
stemmer = PorterStemmer()<br>
print(stemmer.stem('increases'))<br>
</blockquote>
<br>
结果:
<br>
<blockquote>
increas
<br>
</blockquote>
<br>
现在，如果用NLTK的WordNet来对同一个单词进行变体还原，才是正确的结果:
<br>
<blockquote>
from nltk.stem import WordNetLemmatizer
<br>
lemmatizer = WordNetLemmatizer()<br>
print(lemmatizer.lemmatize('increases'))<br>
</blockquote>
<br>
结果:
<br>
<blockquote>
increase
<br>
</blockquote>
<br>
结果可能会是一个同义词或同一个意思的不同单词。<br>

<br>
有时候将一个单词做变体还原时，总是得到相同的词。<br>

<br>
这是因为语言的默认部分是名词。<br>
要得到动词，可以这样指定：
<br>
<blockquote>
from nltk.stem import WordNetLemmatizer
<br>
lemmatizer = WordNetLemmatizer()<br>
print(lemmatizer.lemmatize('playing', pos="v"))<br>
</blockquote>
<br>
结果:
<br>
<blockquote>
play
<br>
</blockquote>
<br>
实际上，这也是一种很好的文本压缩方式，最终得到文本只有原先的50%到60%。<br>

<br>
结果还可以是动词(v)、名词(n)、形容词(a)或副词(r)：
<br>
<blockquote>
from nltk.stem import WordNetLemmatizer
<br>
lemmatizer = WordNetLemmatizer()<br>
print(lemmatizer.lemmatize('playing', pos="v"))<br>
print(lemmatizer.lemmatize('playing', pos="n"))<br>
print(lemmatizer.lemmatize('playing', pos="a"))<br>
print(lemmatizer.lemmatize('playing', pos="r"))<br>
</blockquote>
<br>
输出:
<br>
<blockquote>
play
<br>
playing
<br>
playing
<br>
playing
<br>
</blockquote>
<br>
<b class="brownword">词干和变体的区别</b>
<br>
通过下面例子来观察:
<br>
<blockquote>
from nltk.stem import WordNetLemmatizer
<br>
from nltk.stem import PorterStemmer
<br>
stemmer = PorterStemmer()<br>
lemmatizer = WordNetLemmatizer()<br>
print(stemmer.stem('stones'))<br>
print(stemmer.stem('speaking'))<br>
print(stemmer.stem('bedroom'))<br>
print(stemmer.stem('jokes'))<br>
print(stemmer.stem('lisa'))<br>
print(stemmer.stem('purple'))<br>
print('----------------------')<br>
print(lemmatizer.lemmatize('stones'))<br>
print(lemmatizer.lemmatize('speaking'))<br>
print(lemmatizer.lemmatize('bedroom'))<br>
print(lemmatizer.lemmatize('jokes'))<br>
print(lemmatizer.lemmatize('lisa'))<br>
print(lemmatizer.lemmatize('purple'))<br>
</blockquote>
<br>
输出:
<br>
<blockquote>
stone
<br>
speak
<br>
bedroom
<br>
joke
<br>
lisa
<br>
purpl
<br>
---------------------
<br>
stone
<br>
speaking
<br>
bedroom
<br>
joke
<br>
lisa
<br>
purple
<br>
</blockquote>
<br>
词干提取不会考虑语境，这也是为什么词干提取比变体还原快且准确度低的原因。<br>

<br>
个人认为，变体还原比词干提取更好。<br>
单词变体还原返回一个真实的单词，即使它不是同一个单词，也是同义词，但至少它是一个真实存在的单词。<br>

<br>
如果你只关心速度，不在意准确度，这时你可以选用词干提取。<br>

<br>
在此NLP教程中讨论的所有步骤都只是文本预处理。<br>
在以后的文章中，将会使用Python NLTK来实现文本分析。<br>

<br>
<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('b.brownword').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
