<base target="_blank"><html><head><title>statologyContents 13</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 13"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 13</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">Pandas: How to Filter Rows Based on String Length</span></h2>
You can use the following methods to filter for rows that contain a string with a specific length in a pandas DataFrame:
<b>Method 1: Filter Rows Based on String Length in One Column</b>
<b>#filter rows where col1 has a string length of 5
df.loc[df['col1'].str.len() == 5]
</b>
<b>Method 2: Filter Rows Based on String Length of Multiple Columns</b>
<b>#filter rows where col1 has string length of 5 and col2 has string length of 7
df.loc[(df['col1'].str.len() == 5) & (df['col2'].str.len() == 7)]
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'conf': ['East', 'East', 'North', 'West', 'North', 'South'],   'pos': ['Guard', 'Guard', 'Forward', 'Center', 'Center', 'Forward'],   'points': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
    conf      pos  points
0   East    Guard       5
1   East    Guard       7
2  North  Forward       7
3   West   Center       9
4  North   Center      12
5  South  Forward       9</b>
<h3>Example 1: Filter Rows Based on String Length in One Column</h3>
The following code shows how to filter for rows in the DataFrame that have a string length of <b>5</b> in the <b>conf</b> column:
<b>#filter rows where conf has a string length of 5
df.loc[df['conf'].str.len() == 5]
confpospoints
2NorthForward      7
4NorthCenter    12
5SouthForward     9
</b>
Only the rows where the <b>conf</b> column has a string length of <b>5</b> are returned.
We can see that two different strings met this criteria in the <b>conf</b> column:
“North”
“South”
Both strings have a length of <b>5</b>.
<h3>Example 2: Filter Rows Based on String Length of Multiple Columns</h3>
The following code shows how to filter for rows in the DataFrame that have a string length of <b>5</b> in the <b>conf</b> column and a string length of <b>7</b> in the <b>pos</b> column:
<b>#filter rows where conf has string length of 5 and pos has string length of 7
df.loc[(df['conf'].str.len() == 5) & (df['pos'].str.len() == 7)]
        confpospoints
2NorthForward     7
5SouthForward     9
</b>
Only the rows where the <b>conf</b> column has a string length of <b>5</b> and the <b>pos</b> column has a strength length of  <b>7</b> are returned.
<b>Note</b>: You can find the complete documentation for the <b>str.len()</b> function in pandas  here .
<h2><span class="orange">Pandas: How to Filter Rows that Contain a Specific String</span></h2>
You can use the following syntax to filter for rows that contain a certain string in a pandas DataFrame:
<b>df[df["col"].str.contains("this string")]
</b>
This tutorial explains several examples of how to use this syntax in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C'],   'conference': ['East', 'East', 'East', 'West', 'West', 'East'],   'points': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
        teamconference   points
0AEast         11
1AEast     8
2AEast     10
3BWest         6
4BWest         6
5CEast         5
</b>
<h3>Example 1: Filter Rows that Contain a Specific String</h3>
The following code shows how to filter for rows in the DataFrame that contain ‘A’ in the team column:
<b>df[df["team"].str.contains("A")]
teamconference points
0AEast   11
1AEast   8
2AEast   10
</b>
Only the rows where the team column contains ‘A’ are kept.
<h3>Example 2: Filter Rows that Contain a String in a List</h3>
The following code shows how to filter for rows in the DataFrame that contain ‘A’ or ‘B’ in the team column:
<b>df[df["team"].str.contains("A|B")]
teamconference points
0AEast   11
1AEast   8
2AEast   10
3BWest   6
4BWest   6
</b>
Only the rows where the team column contains ‘A’ or ‘B’ are kept.
<h3>Example 3: Filter Rows that Contain a Partial String</h3>
In the previous examples, we filtered based on rows that exactly matched one or more strings. 
However, if we’d like to filter for rows that contain a partial string then we can use the following syntax:
<b>#identify partial string to look for
keep= ["Wes"]
#filter for rows that contain the partial string "Wes" in the conference column
df[df.conference.str.contains('|'.join(keep))]
teamconference points
3BWest   6
4BWest   6</b>
Only the rows where the conference column contains “Wes” are kept.
<h2><span class="orange">How to Find Closest Value in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to find the row in a pandas DataFrame that contains the value closest to some specified value in a particular column:
<b>#find row with closest value to 101 in points column
df_closest = df.iloc[(df['points']-101).abs().argsort()[:1]]
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Find Closest Value in Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that contains the number of points scored by various basketball teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Nets', 'Hawks', 'Kings', 'Spurs', 'Cavs'],   'points': [99, 100, 96, 104, 89, 93]})
#view DataFrame
print(df)
    team  points
0   Mavs      99
1   Nets     100
2  Hawks      96
3  Kings     104
4  Spurs      89
5   Cavs      93
</b>
Now suppose we would like to select the row in the DataFrame that contains a value in the <b>points</b> column that is closest to <b>101</b>.
We can use the following syntax to do so:
<b>#find row with closest value to 101 in points column
df_closest = df.iloc[(df['points']-101).abs().argsort()[:1]]
#view results
print(df_closest)
   team  points
1  Nets     100
</b>
From the output we can see that the Nets have a value in the points <b>column</b> closest to <b>101</b>.
Note that we could also use <b>tolist()</b> to only display the closest value itself instead of the entire row in the pandas DataFrame:
<b>#display value closest to 101 in the points column
df_closest['points'].tolist()
[100]</b>
Also note that we can change the value after the <b>argsort()</b> function to find several closest values.
For example, we can use the following syntax to find the rows in the DataFrame with the 2 closest values to <b>101</b> in the <b>points</b> column:
<b>#find rows with two closest values to 101 in points column
df_closest2 = df.iloc[(df['points']-101).abs().argsort()[:2]]
#view results
print(df_closest2)
   team  points
1  Nets     100
0  Mavs      99
</b>
From the output we can see that the Nets have the closest value to <b>101</b> in the <b>points</b> column while the Mavs have the next closest value to <b>101</b> in the <b>points</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how perform other common tasks in pandas:
 Pandas: How to Select Rows Based on Column Values 
 Pandas: How to Combine Rows with Same Column Values 
 Pandas: How to Drop All Rows Except Specific Ones 
<h2><span class="orange">How to Find Duplicates in Pandas DataFrame (With Examples)</span></h2>
You can use the  duplicated()  function to find duplicate values in a pandas DataFrame.
This function uses the following basic syntax:
<b>#find duplicate rows across all columns
duplicateRows = df[df.duplicated()]
#find duplicate rows across specific columns
duplicateRows = df[df.duplicated(['col1', 'col2'])]
</b>
The following examples show how to use this function in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [10, 10, 12, 12, 15, 17, 20, 20],   'assists': [5, 5, 7, 9, 12, 9, 6, 6]})
#view DataFrame
print(df)
  team  points  assists
0    A      10        5
1    A      10        5
2    A      12        7
3    A      12        9
4    B      15       12
5    B      17        9
6    B      20        6
7    B      20        6
</b>
<h3>Example 1: Find Duplicate Rows Across All Columns</h3>
The following code shows how to find duplicate rows across all of the columns of the DataFrame:
<b>#identify duplicate rows
duplicateRows = df[df.duplicated()]
#view duplicate rows
duplicateRows
        teampointsassists
1A105
7B206
</b>
There are two rows that are exact duplicates of other rows in the DataFrame.
Note that we can also use the argument <b>keep=’last’</b> to display the first duplicate rows instead of the last:
<b>#identify duplicate rows
duplicateRows = df[df.duplicated(keep='last')]
#view duplicate rows
print(duplicateRows)
teampointsassists
0A105
6B206
</b>
<h3>Example 2: Find Duplicate Rows Across Specific Columns</h3>
The following code shows how to find duplicate rows across just the ‘team’ and ‘points’ columns of the DataFrame:
<b>#identify duplicate rows across 'team' and 'points' columns
duplicateRows = df[df.duplicated(['team', 'points'])]
#view duplicate rows
print(duplicateRows)
        teampointsassists
1A105
3A129
7B206
</b>
There are three rows where the values for the ‘team’ and ‘points’ columns are exact duplicates of previous rows.
<h3>Example 3: Find Duplicate Rows in One Column</h3>
The following code shows how to find duplicate rows in just the ‘team’ column of the DataFrame:
<b>#identify duplicate rows in 'team' column
duplicateRows = df[df.duplicated(['team'])]
#view duplicate rows
print(duplicateRows)
teampointsassists
1A105
2A127
3A129
5B179
6B206
7B206
</b>
There are six total rows where the values in the ‘team’ column are exact duplicates of previous rows.
<h2><span class="orange">Pandas: How to Find Earliest Date in a Column</span></h2>
You can use the following methods to find the earliest date in a column of a pandas DataFrame:
<b>Method 1: Find Earliest Date in Column</b>
<b>df['date_column'].min()
</b>
<b>Method 2: Find Row with Earliest Date in Column</b>
<b>df.iloc[df['date_column'].argmin()]</b>
The following examples shows how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.to_datetime(['2022-04-01', '2022-02-12',                           '2022-06-13', '2022-02-04',                           '2022-07-01', '2022-02-19',                           '2022-12-03', '2022-04-04']),   'sales': [12, 15, 24, 24, 14, 19, 12, 38]})
#view DataFrame
print(df)
        date  sales
0 2022-04-01     12
1 2022-02-12     15
2 2022-06-13     24
3 2022-02-04     24
4 2022-07-01     14
5 2022-02-19     19
6 2022-12-03     12
7 2022-04-04     38</b>
<h2>Example 1: Find Earliest Date in Column</h2>
We can use the following code to find the earliest date in the <b>date</b> column of the DataFrame:
<b>#find earliest date in 'date' column
df['date'].min()
Timestamp('2022-02-04 00:00:00')
</b>
From the output we can see that the earliest date in the <b>date</b> column is 2/4/2022.
<b>Note</b>: If you want to find the most recent date, simply change <b>min()</b> to <b>max()</b> in the code.
<h2>Example 2: Find Row with Earliest Date in Column</h2>
We can use the following code to find the row with the earliest date in the <b>date</b> column of the DataFrame:
<b>#find row with earliest date in 'date' column
df.iloc[df['date'].argmin()]
date     2022-02-04 00:00:00
sales                     24
Name: 3, dtype: object</b>
The output displays the entire row that contains the earliest date in the <b>date</b> column.
For example, we can see the following values in this row:
<b>date</b>: 02-04-2022
<b>sales</b>: 24
If you only want to know the index position of the row with the earliest date, you can replace <b>.iloc</b> with <b>.index</b> as follows:
<b>#find index position of row with earliest date in 'date' column
df.index[df['date'].argmin()]
3
</b>
This tells us that the row with index position <b>3</b> contains the earliest date in the <b>date</b> column.
<b>Note</b>: If you want to find the row with the most recent date, simply change <b>argmin()</b> to <b>argmax()</b> in the code.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Select Rows Between Two Dates in Pandas 
 How to Create Date Column from Year, Month and Day in Pandas 
<h2><span class="orange">Pandas: How to Find First Row that Meets Criteria</span></h2>
You can use the following syntax to find the first row in a pandas DataFrame that meets specific criteria:
<b>#get first row where value in 'team' column is equal to 'B'
df[df.team == 'B'].iloc[0]
#get index of first row where value in 'team' column is equal to 'B'
df[df.team == 'B'].index[0]</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'C'],   'points': [18, 13, 19, 14, 24, 21, 20, 28],   'assists': [5, 7, 17, 9, 12, 9, 5, 12]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    A      13        7
2    A      19       17
3    B      14        9
4    B      24       12
5    C      21        9
6    C      20        5
7    C      28       12
</b>
<h2>Example 1: Find First Row that Meets One Criteria</h2>
We can use the following syntax to find the first row where the value in the <b>team</b> column is equal to ‘B’:
<b>#find first row where team is equal to 'B'
df[df.team == 'B'].iloc[0]
team        B
points     14
assists     9
Name: 3, dtype: object
#find index of first row where team is equal to 'B'
df[df.team == 'B'].index[0]
3</b>
We can see that the first row where the value in the <b>team</b> column is equal to ‘B’ is in index position 3.
<h2>Example 2: Find First Row that Meets Multiple Criteria</h2>
We can use the following syntax to find the first row where the value in the <b>points </b>column is greater than 15 and the value in the <b>assists</b> column is greater than 10:
<b>#find first row where points > 15 and assists > 10
df[(df.points > 15) & (df.assists > 10)].iloc[0]
team        A
points     19
assists    17
Name: 2, dtype: object
#find index of first row where points > 15 and assists > 10
df[(df.points > 15) & (df.assists > 10)].index[0]
2</b>
We can see that the first row where the value in the <b>points </b>column is greater than 15 and the value in the <b>assists</b> column is greater than 10 is in index position 2.
<h2>Example 3: Find First Row that Meets One of Several Criteria</h2>
We can use the following syntax to find the first row where the value in the <b>points </b>column is greater than 15 or the value in the <b>assists</b> column is greater than 10:
<b>#find first row where points > 15 or assists > 10
df[(df.points > 15) | (df.assists > 10)].iloc[0]
team        A
points     18
assists     5
Name: 0, dtype: object
#find index of first row where points > 15 or assists > 10
df[(df.points > 15) | (df.assists > 10)].index[0]
0</b>
We can see that the first row where the value in the <b>points </b>column is greater than 15 or the value in the <b>assists</b> column is greater than 10 is in index position 0.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Rows without NaN Values in Pandas 
 How to Select Rows Based on Column Values in Pandas 
 How to Select Unique Rows in Pandas 
<h2><span class="orange">Pandas: Select Rows Where Value Appears in Any Column</span></h2>
Often you may want to select the rows of a pandas DataFrame in which a certain value appears in any of the columns.
Fortunately this is easy to do using the  <b>.any</b>  pandas function. This tutorial explains several examples of how to use this function in practice.
<h3>Example 1: Find Value in Any Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
</b>
The following syntax shows how to select all rows of the DataFrame that contain the value <b>25</b> in any of the columns:
<b>df[df.isin([25]).any(axis=1)]
        pointsassistsrebounds
025511
</b>
The following syntax shows how to select all rows of the DataFrame that contain the values <b>25, 9, or 6</b> in any of the columns:
<b>df[df.isin([25, 9, 6]).any(axis=1)]
        pointsassistsrebounds
025511
31496
419126</b>
<h3>Example 2: Find Character in Any Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'position': ['G', 'G', 'F', 'F', 'C']})
#view DataFrame
print(df)
   points  assists position
0      25        5        G
1      12        7        G
2      15        7        F
3      14        9        F
4      19       12        C
</b>
The following syntax shows how to select all rows of the DataFrame that contain the character <b>G</b> in any of the columns:
<b>df[df.isin(['G']).any(axis=1)]
pointsassistsposition
0255G
1127G
</b>
The following syntax shows how to select all rows of the DataFrame that contain the values <b>G or C</b> in any of the columns:
<b>df[df.isin(['G', 'C']).any(axis=1)] 
pointsassistsposition
0255G
1127G
41912C</b>
<h2><span class="orange">Pandas: How to Get First Row of Each Group</span></h2>
You can use the following basic syntax to get the first row of each group in a pandas DataFrame:
<b>df.groupby('column_name').nth(0)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Get First Row of Each Group in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'points': [18, 22, 19, 14, 14, 11, 20, 29],   'assists': [5, 19, 14, 8, 9, 12, 13, 8]})
#view DataFrame
df
teampointsassists
0A185
1A2219
2B1914
3B148
4B149
5C1112
6C2013
7C298
</b>
We can use the following code to get the first row for each team:
<b>#get first row for each team
df.groupby('team').nth(0)
pointsassists
team
A185
B1914
C1112</b>
We can also specify <b>as_index=False</b> to keep the original index values:
<b>#get first row for each team, keep original index values
df.groupby('team', as_index=False).nth(0)
        teampointsassists
0A185
2B1914
5C1112</b>
Also note that you can pass a list of values to the <b>nth()</b> function if you’d like to get the first n rows for each group.
For example, the following code shows how to get the first two rows for each group:
<b>#get first two rows for each team, keep original index values
df.groupby('team', as_index=False).nth((0, 1))
        teampointsassists
0A185
1A2219
2B1914
3B148
5C1112
6C2013</b>
<b>Note</b>: You can find the complete documentation for the <b>nth()</b> function  here .
<h2><span class="orange">How to Calculate a Five Number Summary in Pandas</span></h2>
A <b>five number summary </b>is a way to summarize a dataset using the following five values:
The minimum
The first quartile
The median
The third quartile
The maximum
The five number summary is useful because it provides a concise summary of the distribution of the data in the following ways:
It tells us where the  middle value  is located, using the median.
It tells us how  spread out  the data is, using the first and third quartiles.
It tells us the range of the data, using the minimum and the maximum.
The easiest way to calculate a five number summary for variables in a pandas DataFrame is to use the <b>describe()</b> function as follows:
<b>df.describe().loc[['min', '25%', '50%', '75%', 'max']]
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Calculate Five Number Summary in Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use the following syntax to calculate the five number summary for each numeric variable in the DataFrame:
<b>#calculate five number summary for each numeric variable
df.describe().loc[['min', '25%', '50%', '75%', 'max']]
      points assists rebounds
min11.0 4.0 5.00
25%14.0 6.5 6.00
50%18.5 8.0 8.50
75%20.5 9.010.25
max28.012.012.00
</b>
Here’s how to interpret the output for the <b>points</b> variable:
The minimum value is <b>11</b>.
The value at the 25th percentile is <b>14</b>.
The value at the 50th percentile is <b>18.5</b>.
The value at the 75th percentile is <b>20.5</b>.
The maximum value is <b>28</b>.
We can interpret the values for the <b>assists</b> and <b>rebounds</b> variables in a similar manner.
If you’d only like to calculate the five number summary for one specific variable in the DataFrame, you can use the following syntax:
<b>#calculate five number summary for the points variable
df['points'].describe().loc[['min', '25%', '50%', '75%', 'max']]
min    11.0
25%    14.0
50%    18.5
75%    20.5
max    28.0
Name: points, dtype: float64
</b>
The output now displays the five number summary only for the <b>points</b> variable.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Get Frequency Counts of Values in Column 
 Pandas: How to Perform Exploratory Data Analysis 
 Pandas: How to Calculate the Mean by Group 
<h2><span class="orange">How to Flatten MultiIndex in Pandas (With Examples)</span></h2>
You can use the following basic syntax to flatten a MultiIndex in pandas:
<b>#flatten all levels of MultiIndex
df.reset_index(inplace=True)
#flatten specific levels of MultiIndex
df.reset_index(inplace=True, level = ['level_name'])
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Flatten All Levels of MultiIndex in Pandas</h3>
Suppose we have the following MultiIndex pandas DataFrame:
<b>import pandas as pd
#create DataFrame
index_names = pd.MultiIndex.from_tuples([('Level1','Lev1', 'L1'),                       ('Level2','Lev2', 'L2'),                       ('Level3','Lev3', 'L3'),                       ('Level4','Lev4', 'L4')],                       names=['Full','Partial', 'ID'])
data = {'Store': ['A','B','C','D'],
        'Sales': [12, 44, 29, 35]}
df = pd.DataFrame(data, columns = ['Store','Sales'], index=index_names)
#view DataFrame
df
StoreSales
FullPartialID
Level1Lev1L1A17
Level2Lev2L2B22
Level3Lev3L3C29
Level4Lev4L4D35
</b>
We can use the following syntax to flatten every level of the MultiIndex into columns in the DataFrame:
<b>#flatten every level of MultiIndex 
df.reset_index(inplace=True)
#view updated DataFrame
df
        FullPartialIDStoreSales
0Level1Lev1L1A12
1Level2Lev2L2B44
2Level3Lev3L3C29
3Level4Lev4L4D35
</b>
Notice that each level of the MultiIndex is now a column in the DataFrame.
<h3>Example 2: Flatten Specific Levels of MultiIndex in Pandas</h3>
Suppose we have the same pandas DataFrame as the previous example:
<b>#view DataFrame
df
StoreSales
FullPartialID
Level1Lev1L1A12
Level2Lev2L2B44
Level3Lev3L3C29
Level4Lev4L4D35
</b>
The following code shows how to flatten just one specific level of the MultiIndex:
<b>#flatten 'ID' level only
df.reset_index(inplace=True, level = ['ID'])
#view updated DataFrame
df
IDStoreSales
FullPartial
Level1Lev1L1A12
Level2Lev2L2B44
Level3Lev3L3C29
Level4Lev4L4D35
</b>
And the following code shows how to flatten several specific levels of the MultiIndex:
<b>#flatten 'ID' level only
df.reset_index(inplace=True, level = ['Partial', 'ID'])
#view updated DataFrame
df
Partial IDStoreSales
Full
Level1Lev1 L1A12
Level2Lev2 L2B44
Level3Lev3 L3C29
Level4Lev4 L4D35</b>
<h2><span class="orange">Pandas: How to Get Frequency Counts of Values in Column</span></h2>
You can use the following methods to get frequency counts of values in a column of a pandas DataFrame:
<b>Method 1: Get Frequency Count of Values in Table Format</b>
<b>df['my_column'].value_counts()
</b>
<b>Method 2: Get Frequency Count of Values in Dictionary Format</b>
<b>df['my_column'].value_counts().to_dict()</b>
The following examples shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'C'],   'points': [12, 20, 25, 8, 12, 19, 27, 35]})
#view DataFrame
print(df)
  team  points
0    A      12
1    A      20
2    A      25
3    A       8
4    B      12
5    B      19
6    B      27
7    C      35
</b>
<h2>Example 1: Get Frequency Count of Values in Table Format</h2>
We can use the <b>value_counts()</b> function to get a frequency count of each unique value in the <b>team</b> column of the DataFrame and display the results in a table format:
<b>#get frequency count of values in 'team' column
df['team'].value_counts()
A    4
B    3
C    1
Name: team, dtype: int64
</b>
From the results we can see:
The value ‘A’ occurs <b>4</b> times in the team column.
The value ‘B’ occurs <b>3</b> times in the team column.
The value ‘C’ occurs <b>1</b> time in the team column.
Notice that the results are displayed in a table format.
<h2>Example 2: Get Frequency Count of Values in Dictionary Format</h2>
We can use the <b>value_counts()</b> function and the <b>to_dict()</b> function to get a frequency count of each unique value in the <b>team</b> column of the DataFrame and display the results in a dictionary format:
<b>#get frequency count of values in 'team' column and display in dictionary
df['team'].value_counts().to_dict()
{'A': 4, 'B': 3, 'C': 1}
</b>
The frequency counts of each unique value in the <b>team</b> column are shown in a dictionary format.
For example, we can see:
The value ‘A’ occurs <b>4</b> times in the team column.
The value ‘B’ occurs <b>3</b> times in the team column.
The value ‘C’ occurs <b>1</b> time in the team column.
This matches the frequency counts in the previous method.
The results are simply shown in a different format.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Use GroupBy and Value Counts 
 Pandas: How to Use GroupBy with Bin Counts 
 Pandas: How to Count Values in Column with Condition 
<h2><span class="orange">Pandas: Create Frequency Table Based on Multiple Columns</span></h2>
You can use the following basic syntax to create a frequency table in pandas based on multiple columns:
<b>df.value_counts(['column1', 'column2'])
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Frequency Table in Pandas Based on Multiple Columns</h2>
Suppose we have the following pandas DataFrame that contains information on team name, position, and points scored by various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position' : ['G', 'G', 'G', 'F', 'G', 'G', 'F', 'F'],   'points': [24, 33, 20, 15, 16, 16, 29, 25]})
#view DataFrame
print(df)
  team position  points
0    A        G      24
1    A        G      33
2    A        G      20
3    A        F      15
4    B        G      16
5    B        G      16
6    B        F      29
7    B        F      25</b>
We can use the <b>value_counts()</b> function to create a frequency table that shows the occurrence of each combination of values in the <b>team</b> and <b>position</b> columns:
<b>#count frequency of values in team and position columns
df.value_counts(['team', 'position'])
team  position
A     G           3
B     F           2
      G           2
A     F           1
dtype: int64
</b>
From the results we can see:
There are <b>3</b> occurrences of team A and position G
There are <b>2</b> occurrences of team B and position F
There are <b>2</b> occurrences of team B and position G
There is <b>1</b> occurrence of team A and position F
Note that we can use <b>reset_index()</b> to return a DataFrame as a result instead:
<b>#count frequency of values in team and position columns and return DataFrame
df.value_counts(['team', 'position']).reset_index()
        teamposition  0
0AG  3
1BF  2
2BG  2
3AF  1</b>
We can use the <b>rename()</b> function to rename the column that contains the counts:
<b>#get frequency of values in team and position column and rename count column
df.value_counts(['team', 'position']).reset_index().rename(columns={0:'count'})
        teamposition  count
0AG  3
1BF  2
2BG  2
3AF  1
</b>
The end result is a DataFrame that contains the frequency of each unique combination of values in the <b>team</b> and <b>position</b> columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Use GroupBy and Value Counts 
 Pandas: How to Use GroupBy with Bin Counts 
 Pandas: How to Count Values in Column with Condition 
<h2><span class="orange">How to Create Pandas DataFrame with Random Data</span></h2>
You can use the following basic syntax to create a pandas DataFrame that is filled with random integers:
<b>df = pd.DataFrame(np.random.randint(0,100,size=(10, 3)), columns=list('ABC'))
</b>
This particular example creates a DataFrame with <b>10</b> rows and <b>3</b> columns where each value in the DataFrame is a random integer between <b>0</b> and <b>100</b>.
The following examples shows how to use this syntax in practice.
<h3>Example 1: Create Pandas DataFrame with Random Data</h3>
The following code shows how to create a pandas DataFrame with 10 rows and 3 columns where each value in the DataFrame is a random integer between 0 and 100:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame(np.random.randint(0,100,size=(10, 3)), columns=list('ABC')) 
#view DataFrame
print(df)
    A   B   C
0  72  70  27
1  87  85   7
2   4  42  84
3  85  87  63
4  79  72  30
5  96  99  79
6  26  47  90
7  35  69  56
8  42  47   0
9  97   4  59
</b>
Note that each time you run this code, the random integers in the DataFrame will be different.
If you’d like to create a reproducible example where the random integers are the same each time, you can use the following piece of code immediately before you create the DataFrame:
<b>np.random.seed(0)
</b>
Now each time you run the code, the random integers in the DataFrame will be the same.
<h3>Example 2: Add Column of Random Data to Existing DataFrame</h3>
Suppose we have the following existing pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
We can use the following code to add a new column called “rand” that contains random integers between 0 and 100:
<b>import numpy as np
#add 'rand' column that contains 8 random integers between 0 and 100
df['rand'] = np.random.randint(0,100,size=(8, 1))
#view updated DataFrame
print(df)
  team  points  assists  rebounds  rand
0    A      18        5        11    47
1    B      22        7         8    64
2    C      19        7        10    82
3    D      14        9         6    99
4    E      14       12         6    88
5    F      11        9         5    49
6    G      20        9         9    29
7    H      28        4        12    19</b>
Notice that the new column “rand” has been added to the existing DataFrame.
<h2><span class="orange">Pandas: How to Get Column Name by Index</span></h2>
You can use the following methods to get a column name by index position in pandas:
<b>Method 1: Get One Column Name by Index Position</b>
<b>#get column name in index position 2
colname = df.columns[2]
</b>
<b>Method 2: Get Multiple Column Names by Index Positions</b>
<b>#get column names in index positions 2 and 4
colname = df.columns[[2, 4]] 
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [4, 3, 3, 2, 5, 4, 3, 8],   'blocks': [1, 0, 0, 3, 2, 2, 1, 5]})
#view DataFrame
print(df)
  team  points  assists  rebounds  steals  blocks
0    A      18        5        11       4       1
1    B      22        7         8       3       0
2    C      19        7        10       3       0
3    D      14        9         6       2       3
4    E      14       12         6       5       2
5    F      11        9         5       4       2
6    G      20        9         9       3       1
7    H      28        4        12       8       5
</b>
<h2>Example 1: Get One Column Name by Index Position</h2>
The following code shows how to get the column name for the column in index position 2 of the DataFrame:
<b>#get column name for column in index position 2
colname = df.columns[2]
#display column name
print(colname)
assists
</b>
The column name for the column in index position 2 is <b>assists</b>.
<b>Note</b>: Column index values start at 0 in Python. Thus, the column in index position 2 would be the third column in the DataFrame, which has the name <b>assists</b>.
<h2>Example 2: Get Multiple Column Names by Index Positions</h2>
The following code shows how to get the column names for the columns in index positions 2 and 4 of the DataFrame:
<b>#get column names in index positions 2 and 4
colname = df.columns[[2, 4]] 
#display column names
print(colname)
Index(['assists', 'steals'], dtype='object')
</b>
From the output we can see:
The column name for the column in index position 2 is <b>assists</b>.
The column name for the column in index position 4 is <b>steals</b>.
<b>Note</b>: In this example we chose to get the column names for two columns by index position, but you can use similar syntax to retrieve the column names for as many columns as you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Get Cell Value from Pandas DataFrame 
 How to Rename Index in Pandas DataFrame 
 How to Sort Columns by Name in Pandas 
<h2><span class="orange">How to Get Column Names in Pandas (3 Methods)</span></h2>
You can use the following methods to get the column names in a pandas DataFrame:
<b>Method 1: Get All Column Names</b>
<b>list(df)</b>
<b>Method 2: Get Column Names in Alphabetical Order</b>
<b>sorted(df)
</b>
<b>Method 3: Get Column Names with Specific Data Type</b>
<b>list(df.select_dtypes(include=['int64', 'bool']))</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9],   'playoffs': [True, False, False, True, True, True]})
#view DataFrame
print(df)
  team  points  assists  playoffs
0    A      18        5      True
1    B      22        7     False
2    C      19        7     False
3    D      14        9      True
4    E      14       12      True
5    F      11        9      True
</b>
<h2>Example 1: Get All Column Names</h2>
The easiest way to get all of the column names in a pandas DataFrame is to use<b> list()</b> as follows:
<b>#get all column names
list(df)
['team', 'points', 'assists', 'playoffs']
</b>
The result is a list that contains all four column names from the pandas DataFrame.
<h2>Example 2: Get Column Names in Alphabetical Order</h2>
To get the column names in a pandas DataFrame in alphabetical order, you can use the <b>sorted()</b> function as follows:
<b>#get column names in alphabetical order
sorted(df)
['assists', 'playoffs', 'points', 'team']
</b>
The result is a list that contains all four column names from the pandas DataFrame listed in alphabetical order.
You can also use the argument <b>reverse=True</b> to get the column names in reverse alphabetical order:
<b>#get column names in reverse alphabetical order
sorted(df, reverse=True)
['team', 'points', 'playoffs', 'assists']</b>
<h2>Example 3: Get Column Names with Specific Data Type</h2>
You can use the following syntax to view the data type of each column in the DataFrame:
<b>#view data type of each column
df.dtypes
team        object
points       int64
assists      int64
playoffs      bool
dtype: object
</b>
You can then use the <b>select_dtypes()</b> function to only get the column names with a specific data type.
For example, we can use the following syntax to only get the column names that have a data type of <b>int64</b> or <b>bool</b>:
<b>#get all columns that have data type of int64 or bool
list(df.select_dtypes(include=['int64', 'bool']))
['points', 'assists', 'playoffs']
</b>
The result is a list of column names that have a data type of <b>int64</b> or <b>bool</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Rename Columns 
 Pandas: How to Set Column as Index 
 Pandas: Get Index of Rows Whose Column Matches Value 
<h2><span class="orange">Pandas: How to Get Day of Year from Date</span></h2>
You can use the following basic syntax to get the day of year from a date column in a pandas DataFrame:
<b>df['day_of_year'] = df['date'].dt.dayofyear
</b>
This particular example creates a new column called <b>day_of_year</b> that contains the day of the year of the value in the <b>date</b> column.
Note that the values for <b>day_of_year</b> will range from 1 (January 1st) to 365 (December 31st).
The following example shows how to use this syntax in practice.
<h2>Example: Get Day of Year from Date in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about the total sales made at some store on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2022', freq='M', periods=10),   'sales': [6, 8, 10, 5, 4, 8, 8, 3, 5, 14]})
#view DataFrame
print(df)
         date  sales
0  2022-01-31      6
1  2022-02-28      8
2  2022-03-31     10
3  2022-04-30      5
4  2022-05-31      4
5  2022-06-30      8
6  2022-07-31      8
7  2022-08-31      3
8  2022-09-30      5
9  2022-10-31     14
</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following code to create a new column called <b>day_of_year </b>that contains the day of the year from the <b>date</b> column:
<b>#create new column that contains day of year in 'date' column
df['day_of_year'] = df['date'].dt.dayofyear
#view updated DataFrame
print(df)
        date  sales  day_of_year
0 2022-01-31      6           31
1 2022-02-28      8           59
2 2022-03-31     10           90
3 2022-04-30      5          120
4 2022-05-31      4          151
5 2022-06-30      8          181
6 2022-07-31      8          212
7 2022-08-31      3          243
8 2022-09-30      5          273
9 2022-10-31     14          304</b>
The new column called <b>day_of_year </b>contains the day of year from the <b>date</b> column.
It’s worth noting that if you’re working with a leap year, this function will automatically extend the range of possible values from 365 to 366.
Also note that if the column you’re working with is a string column, you must first use <b>pd.to_datetime()</b> to convert the strings to recognizable dates:
<b>#convert string column to datetime and calculate day of year
df['day_of_year'] = pd.to_datetime(df['date']).dt.dayofyear
</b>
<b>Note</b>: You can find the complete documentation for the pandas <b>dayofyear</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Select Rows Between Two Dates in Pandas 
 How to Create Date Column from Year, Month and Day in Pandas 
<h2><span class="orange">How to Use Pandas Get Dummies – pd.get_dummies</span></h2>
Often in statistics, the datasets we’re working with include  categorical variables .
These are variables that take on names or labels. Examples include:
Marital status (“married”, “single”, “divorced”)
Smoking status (“smoker”, “non-smoker”)
Eye color (“blue”, “green”, “hazel”)
Level of education (e.g. “high school”,  “Bachelor’s degree”, “Master’s degree”)
When fitting machine learning algorithms (like  linear regression ,  logistic regression ,  random forests , etc.), we often convert categorical variables to <b>dummy variables</b>, which are numeric variables that are used to represent categorical data.
For example, suppose we have a dataset that contains the categorical variable <b>Gender</b>. To use this variable as a predictor in a regression model, we would first need to convert it to a dummy variable.
To create this dummy variable, we can choose one of the values (“Male”) to represent 0 and the other value (“Female”) to represent 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummy2.png">
<h3>How to Create Dummy Variables in Pandas</h3>
To create dummy variables for a variable in a pandas DataFrame, we can use the  pandas.get_dummies()  function, which uses the following basic syntax:
<b>pandas.get_dummies(data, prefix=None, columns=None, drop_first=False)</b>
where:
<b>data</b>: The name of the pandas DataFrame
<b>prefix</b>: A string to append to the front of the new dummy variable column
<b>columns</b>: The name of the column(s) to convert to a dummy variable
<b>drop_first</b>: Whether or not to drop the first dummy variable column
The following examples show how to use this function in practice.
<h3>Example 1: Create a Single Dummy Variable</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'income': [45, 48, 54, 57, 65, 69, 78],   'age': [23, 25, 24, 29, 38, 36, 40],   'gender': ['M', 'F', 'M', 'F', 'F', 'F', 'M']})
#view DataFrame
df
        incomeagegender
04523M
14825F
25424M
35729F
46538F
56936F
67840M</b>
We can use the <b>pd.get_dummies()</b> function to turn gender into a dummy variable:
<b>#convert gender to dummy variable
pd.get_dummies(df, columns=['gender'], drop_first=True)
incomeagegender_M
045231
148250
254241
357290
465380
569360
678401</b>
The gender column is now a dummy variable where:
A value of <b>0</b> represents “Female”
A value of <b>1</b> represents “Male”
<h3>Example 2: Create Multiple Dummy Variables</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'income': [45, 48, 54, 57, 65, 69, 78],   'age': [23, 25, 24, 29, 38, 36, 40],   'gender': ['M', 'F', 'M', 'F', 'F', 'F', 'M'],   'college': ['Y', 'N', 'N', 'N', 'Y', 'Y', 'Y']})
#view DataFrame
df
incomeagegendercollege
04523MY
14825FN
25424MN
35729FN
46538FY
56936FY
67840MY</b>
We can use the <b>pd.get_dummies()</b> function to convert gender and college both into dummy variables:
<b>#convert gender to dummy variable
pd.get_dummies(df, columns=['gender', 'college'], drop_first=True)
        incomeagegender_M  college_Y
045231  1
148250  0
254241  0
357290  0
465380  1
569360  1
678401  1</b>
The gender column is now a dummy variable where:
A value of <b>0</b> represents “Female”
A value of <b>1</b> represents “Male”
And the college column is now a dummy variable where:
A value of <b>0</b> represents “No” college
A value of <b>1</b> represents “Yes” college
<h2><span class="orange">Pandas: How to Get Column Index from Column Name</span></h2>
You can use the following methods to get the column index value from a column name in pandas:
<b>Method 1: Get Column Index for One Column Name</b>
<b>df.columns.get_loc('this_column')
</b>
<b>Method 2: Get Column Index for Multiple Column Names</b>
<b>cols = ['this_column', 'that_column']
[df.columns.get_loc(c) for c in cols if c in df]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'sales': [18, 10, 14, 13, 19, 24, 25, 29],   'returns': [1, 2, 2, 3, 2, 3, 5, 4],   'recalls': [0, 0, 2, 1, 1, 2, 0, 1]})
#view DataFrame
print(df)
  store  sales  returns  recalls
0     A     18        1        0
1     A     10        2        0
2     A     14        2        2
3     A     13        3        1
4     B     19        2        1
5     B     24        3        2
6     B     25        5        0
7     B     29        4        1</b>
<h2>Example 1: Get Column Index for One Column Name</h2>
The following code shows how to get the column index value for the column with the name ‘returns’:
<b>#get column index for column with the name 'returns'
df.columns.get_loc('returns')
2</b>
The column with the name ‘returns’ has a column index value of <b>2</b>.
<b>Note</b>: Column index values start at 0 in Python. Thus, since ‘returns’ is the third column in the DataFrame, it has an index value of 2.
<h2>Example 2: Get Column Index for Multiple Column Names</h2>
The following code shows how to get the column index value for several columns in the DataFrame:
<b>#define list of columns to get index for
cols = ['store', 'returns', 'recalls']
#get column index for each column in list
[df.columns.get_loc(c) for c in cols if c in df]
[0, 2, 3]
</b>
From the output we can see:
The column with the name ‘store’ has a column index value of <b>0</b>.
The column with the name ‘returns’ has a column index value of <b>2</b>.
The column with the name ‘recalls’ has a column index value of <b>3</b>.
<b>Note</b>: You can find the complete documentation for the pandas <b>get_loc()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Get Cell Value from Pandas DataFrame 
 How to Rename Index in Pandas DataFrame 
 How to Sort Columns by Name in Pandas 
<h2><span class="orange">Pandas: Get Index of Rows Whose Column Matches Value</span></h2>
You can use the following syntax to get the index of rows in a pandas DataFrame whose column matches specific values:
<b>df.index[df['column_name']==value].tolist()
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'D'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsrebounds
0A511
1A78
2A710
3B96
4B126
5C95
6C99
7D412
</b>
<h3>Example 1: Get Index of Rows Whose Column Matches Value</h3>
The following code shows how to get the index of the rows where one column is equal to a certain value:
<b>#get index of rows where 'points' column is equal to 7
df.index[df['points']==7].tolist()
[1, 2]
</b>
This tells us that the rows with index values <b>1</b> and <b>2</b> have the value ‘7’ in the points column.
Note that we can also use the less than and greater than operators to find the index of the rows where one column is less than or greater than a certain value:
<b>#get index of rows where 'points' column is greater than 7
df.index[df['points']>7].tolist()
[3, 4, 5, 6]
</b>
This tells us that the rows with index values <b>3</b>, <b>4</b>, <b>5</b>, and <b>6</b> have a value greater than ‘7’ in the points column.
<h3>Example 2: Get Index of Rows Whose Column Matches String</h3>
The following code shows how to get the index of the rows where one column is equal to a certain string:
<b>#get index of rows where 'team' column is equal to 'B'
df.index[df['team']=='B'].tolist()
[3, 4]
</b>
This tells us that the rows with index values <b>3</b> and <b>4</b> have the value ‘B’ in the team column.
<h3>Example 3: Get Index of Rows with Multiple Conditions</h3>
The following code shows how to get the index of the rows where the values in multiple columns match certain conditions:
<b>#get index of rows where 'points' is equal to 7 <em>or</em> 12
df.index[(df['points']==7) | (df['points']==12)].tolist()
[1, 2, 4]
#get index of rows where 'points' is equal to 9 <i>and </i>'team' is equal to 'B'
df.index[(df['points']==9) & (df['team']=='B')].tolist()
[3]</b>
<h2><span class="orange">How to Get Last Row in Pandas DataFrame (With Example)</span></h2>
You can use the following methods to get the last row in a pandas DataFrame:
<b>Method 1: Get Last Row (as a Pandas Series)</b>
<b>last_row = df.iloc[-1]
</b>
<b>Method 2: Get Last Row (as a Pandas DataFrame)</b>
<b>last_row = df.iloc[-1:]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'assists': [3, 4, 4, 5, 6, 7, 8, 12, 15, 11],   'rebounds': [1, 3, 3, 5, 2, 2, 1, 1, 0, 14],   'points': [20, 22, 24, 25, 20, 28, 15, 29, 11, 12]})
#view DataFrame
print(df)
   assists  rebounds  points
0        3         1      20
1        4         3      22
2        4         3      24
3        5         5      25
4        6         2      20
5        7         2      28
6        8         1      15
7       12         1      29
8       15         0      11
9       11        14      12</b>
<h2>Example 1: Get Last Row (as a Pandas Series)</h2>
The following code shows how to get the last row of the DataFrame as a pandas Series:
<b>#get last row in Data Frame as Series
last_row = df.iloc[-1]
#view last row
print(last_row)
assists     11
rebounds    14
points      12
Name: 9, dtype: int64</b>
We can use the <b>type()</b> function to confirm that the result is indeed a pandas Series:
<b>#view type
type(last_row)
pandas.core.series.Series
</b>
The result is indeed a pandas Series.
<h2>Example 2: Get Last Row (as a Pandas DataFrame)</h2>
The following code shows how to get the last row of the DataFrame as a pandas DataFrame:
<b>#get last row in Data Frame as DataFrame
last_row = df.iloc[-1:]
#view last row
print(last_row)
   assists  rebounds  points
9       11        14      12</b>
We can use the <b>type()</b> function to confirm that the result is indeed a pandas DataFrame:
<b>#view type
type(last_row)
pandas.core.frame.DataFrame
</b>
The result is indeed a pandas DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Rows without NaN Values in Pandas 
 How to Drop All Rows Except Specific Ones in Pandas 
 How to Sum Specific Columns in Pandas 
<h2><span class="orange">Pandas: How to Get Quarter from Date</span></h2>
You can use the following methods to get the quarter from a date in a pandas DataFrame:
<b>Method 1: Get Quarter from Date (Year & Quarter Format)</b>
<b>df['quarter'] = pd.PeriodIndex(df.date, freq='Q')
</b>
If the date is in the first quarter of 2022, this will return the quarter in a format like <b>2022Q1</b>.
<b>Method 2: Get Quarter from Date (Quarter Number Format)</b>
<b>df['quarter'] = df['date'].dt.quarter</b>
If the date is in the first quarter of 2022, this will simply return the value <b>1</b>.
The following examples shows how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2022', freq='M', periods=14),   'sales': [6, 8, 10, 5, 4, 8, 8, 3, 5, 14, 8, 3, 10, 12]})
#view DataFrame
print(df)
         date  sales
0  2022-01-31      6
1  2022-02-28      8
2  2022-03-31     10
3  2022-04-30      5
4  2022-05-31      4
5  2022-06-30      8
6  2022-07-31      8
7  2022-08-31      3
8  2022-09-30      5
9  2022-10-31     14
10 2022-11-30      8
11 2022-12-31      3
12 2023-01-31     10
13 2023-02-28     12</b>
<h2>Example 1: Get Quarter from Date (Year & Quarter Format)</h2>
We can use the following code to create a new column called <b>quarter</b> that extracts the quarter from the <b>date</b> column in a year and quarter format:
<b>#create new column that displays year and quarter from date column
df['quarter'] = pd.PeriodIndex(df.date, freq='Q')
#view updated DataFrame
print(df)
         date  sales quarter
0  2022-01-31      6  2022Q1
1  2022-02-28      8  2022Q1
2  2022-03-31     10  2022Q1
3  2022-04-30      5  2022Q2
4  2022-05-31      4  2022Q2
5  2022-06-30      8  2022Q2
6  2022-07-31      8  2022Q3
7  2022-08-31      3  2022Q3
8  2022-09-30      5  2022Q3
9  2022-10-31     14  2022Q4
10 2022-11-30      8  2022Q4
11 2022-12-31      3  2022Q4
12 2023-01-31     10  2023Q1
13 2023-02-28     12  2023Q1</b>
The new column called <b>quarter</b> contains the quarter from the <b>date</b> column in a year and quarter format.
<h2>Example 2: Get Quarter from Date (Quarter Number Format)</h2>
We can use the following code to create a new column called <b>quarter</b> that extracts the quarter from the <b>date</b> column in a quarter number format
<b>#create new column that displays quarter from date column
<b>df['quarter'] = df['date'].dt.quarter
</b>
#view updated DataFrame
print(df)
         date  sales  quarter
0  2022-01-31      6        1
1  2022-02-28      8        1
2  2022-03-31     10        1
3  2022-04-30      5        2
4  2022-05-31      4        2
5  2022-06-30      8        2
6  2022-07-31      8        3
7  2022-08-31      3        3
8  2022-09-30      5        3
9  2022-10-31     14        4
10 2022-11-30      8        4
11 2022-12-31      3        4
12 2023-01-31     10        1
13 2023-02-28     12        1</b>
The new column called <b>quarter</b> contains the quarter number from the <b>date</b> column in a quarter number format.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Select Rows Between Two Dates in Pandas 
 How to Create Date Column from Year, Month and Day in Pandas 
<h2><span class="orange">How to Get Row Numbers in a Pandas DataFrame</span></h2>
Often you may want to get the row numbers in a pandas DataFrame that contain a certain value. 
Fortunately this is easy to do using the <b>.index</b> function.
This tutorial shows several examples of how to use this function in practice.
<h2>Example 1: Get Row Numbers that Match a Certain Value</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'team': ['Mavs', 'Mavs', 'Spurs', 'Celtics', 'Warriors']})
#view DataFrame 
print(df)
        pointsassiststeam
0255Mavs
1127Mavs
2157Spurs
3149Celtics
41912Warriors
</b>
We can use the following syntax to get the row numbers where ‘team’ is equal to Mavs:
<b>#get row numbers where 'team' is equal to Mavs
df[df['team'] == 'Mavs'].index
Int64Index([0, 1], dtype='int64')
</b>
We can see that the team name is equal to ‘Mavs’ at rows indices <b>0 </b>and <b>1</b>.
We can also get the row numbers where the team name is in a certain list of team names:
<b>#get row numbers where 'team' is equal to Mavs or Spurs
filter_list = ['Mavs', 'Spurs']
#return only rows where team is in the list of team names
df[df.team.isin(filter_list)].index
Int64Index([0, 1, 2], dtype='int64')
</b>
We can see that the team name is equal to ‘Mavs’ or ‘Spurs’ at rows indices <b>0</b>, <b>1</b>, and <b>2</b>.
<h2>Example 2: Get a Single Row Number</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'team': ['Mavs', 'Mavs', 'Spurs', 'Celtics', 'Warriors']})</b>
If you know that only one row matches a certain value, you can retrieve that single row number using the following syntax:
<b>#get the row number where team is equal to Celtics
df[df['team'] == 'Celtics'].index[0]
3</b>
We can see that team is equal to ‘Celtics’ at row index number <b>3</b>.
<h2>Example 3: Get Sum of Row Numbers</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'team': ['Mavs', 'Mavs', 'Spurs', 'Celtics', 'Warriors']})</b>
If you want to know the total number of rows where a column is equal to a certain value, you can use the following syntax:
<b>#find total number of rows where team is equal to Mavs
len(df[df['team'] == 'Celtics'].index)
2</b>
We can see that team is equal to ‘Mavs’ in a total of <b>2 </b>rows.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Find Unique Values in Multiple Columns in Pandas 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Count Missing Values in a Pandas DataFrame 
<h2><span class="orange">Pandas: Get Rows Which Are Not in Another DataFrame</span></h2>
You can use the following basic syntax to get the rows in one pandas DataFrame which are not in another DataFrame:
<b>#merge two DataFrames and create indicator column
df_all = df1.merge(df2.drop_duplicates(), on=['col1','col2'],   how='left', indicator=True)
#create DataFrame with rows that exist in first DataFrame only
df1_only = df_all[df_all['_merge'] == 'left_only']</b>
The following example shows how to use this syntax in practice.
<h2>Example: Get Rows in Pandas DataFrame Which Are Not in Another DataFrame</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'],     'points' : [12, 15, 22, 29, 24]}) 
print(df1)
  team  points
0    A      12
1    B      15
2    C      22
3    D      29
4    E      24
#create second DataFrame
df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],    'points' : [12, 29, 15, 19, 10]})
print(df2)
  team  points
0    A      12
1    D      29
2    F      15
3    G      19
4    H      10
</b>
We can use the following syntax to merge the two DataFrames and create an indicator column to indicate which rows belong in each DataFrame:
<b>#merge two DataFrames and create indicator column
df_all = df1.merge(df2.drop_duplicates(), on=['team','points'],   how='left', indicator=True)
#view result
print(df_all)</b>
We can then use the following syntax to only get the rows in the first DataFrame that are not in the second DataFrame:
<b>#create DataFrame with rows that exist in first DataFrame only
df1_only = df_all[df_all['_merge'] == 'left_only']
#view DataFrame
print(df1_only)
  team  points     _merge
1    B      15  left_only
2    C      22  left_only
4    E      24  left_only
</b>
Lastly, we can drop the <b>_merge</b> column if we’d like:
<b>#drop '_merge' column
df1_only = df1_only.drop('_merge', axis=1)
#view DataFrame
print(df1_only)
  team  points
1    B      15
2    C      22
4    E      24</b>
The result is a DataFrame in which all of the rows exist in the first DataFrame but not in the second DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Add Column from One DataFrame to Another in Pandas 
 How to Change the Order of Columns in Pandas 
 How to Sort Columns by Name in Pandas 
<h2><span class="orange">Pandas: How to Get Value from Series (3 Examples)</span></h2>
The following examples show how to get a value from a pandas Series in three different scenarios.
<h3>Method 1: Get Value from Pandas Series Using Index</h3>
The following code shows how to get the value in the third position of a pandas Series using the index value:
<b>import pandas as pd
#define Series
my_series = pd.Series(['A', 'B', 'C', 'D', 'E'])
#get third value in Series
print(my_series[2])
C</b>
By specifying the index value <b>2</b>, we’re able to extract the value in the third position of the pandas Series.
<h3>Method 2: Get Value from Pandas Series Using String</h3>
The following code shows how to get the value that corresponds to a specific string in a pandas Series:
<b>import pandas as pd
#define Series
my_series = pd.Series({'First':'A', 'Second':'B', 'Third':'C'})
#get value that corresponds to 'Second'
print(my_series['Second'])
B</b>
Using this syntax, we’re able to get the value that corresponds to ‘Second’ in the pandas Series.
<h3>
<b>Method 3: Get Value from Pandas Series in DataFrame</b>
</h3>
The following code shows how to get the value in a pandas Series that is a column in a pandas DataFrame
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Spurs', 'Rockets', 'Heat', 'Nets'],   'points': [100, 114, 121, 108, 101]})
#view DataFrame
print(df)
      team  points
0     Mavs     100
1    Spurs     114
2  Rockets     121
3     Heat     108
4     Nets     101
#get 'Spurs' value from team column
df.loc[df.team=='Spurs','team'].values[0]
'Spurs'</b>
By using the <b>loc</b> and <b>values</b> functions, we’re able to get the value ‘Spurs’ from the DataFrame.
<b>Related:</b>  Pandas loc vs. iloc: What’s the Difference? 
<h2><span class="orange">How to Group by 5-Minute Intervals in Pandas</span></h2>
You can use the following basic syntax to group rows by 5-minute intervals in a pandas DataFrame:
<b>df.resample('5min').sum()
</b>
This particular formula assumes that the index of your DataFrame contains datetime values and it calculates the sum of every column in the DataFrame, grouped by 5-minute intervals.
The following example shows how to use this syntax in practice.
<b>Related:</b>  An Introduction to resample() in pandas 
<h2>Example: How to Group by 5-Minute Intervals in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates and times:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2020', freq='min', periods=12),   'sales': [6, 8, 9, 11, 13, 8, 8, 15, 22, 9, 8, 4],   'returns': [0, 3, 2, 2, 1, 3, 2, 4, 1, 5, 3, 2]})
#set 'date' column as index
df = df.set_index('date')
#view DataFrame
print(df)
     sales  returns
date                               
2020-01-01 00:00:00      6        0
2020-01-01 00:01:00      8        3
2020-01-01 00:02:00      9        2
2020-01-01 00:03:00     11        2
2020-01-01 00:04:00     13        1
2020-01-01 00:05:00      8        3
2020-01-01 00:06:00      8        2
2020-01-01 00:07:00     15        4
2020-01-01 00:08:00     22        1
2020-01-01 00:09:00      9        5
2020-01-01 00:10:00      8        3
2020-01-01 00:11:00      4        2
</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by 5-minute intervals:
<b>#calculate sum of sales and returns grouped by 5-minute intervals
df.resample('5min').sum()
     sales returns
date
2020-01-01 00:00:0047 8
2020-01-01 00:05:006215
2020-01-01 00:10:0012  5</b>
Here’s how to interpret the output:
Total sales during minutes 0-4 was <b>47</b> and total returns was <b>8</b>.
Total sales during minutes 5-9 was <b>62 </b>and total returns was <b>15</b>.
Total sales during minutes 10-14 was <b>1 2</b>and total returns was <b>5</b>.
We can use similar syntax to calculate the max of the sales values and returns values, grouped by 5-minute intervals :
<b>#calculate max of sales and max of returns grouped by 5-minute intervals
df.resample('5min').max()
             sales  returns
date
2020-01-01 00:00:0013  3
2020-01-01 00:05:0022  5
2020-01-01 00:10:008  3</b>
We can use similar syntax to calculate any value we’d like grouped by 5-minute intervals.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Group by Day in Pandas 
 How to Group by Week in Pandas 
 How to Group by Month in Pandas 
 How to Group by Quarter in Pandas 
<h2><span class="orange">How to Group by Day in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to group rows by day in a pandas DataFrame:
<b>df.groupby(df.your_date_column.dt.day)['values_column'].sum()
</b>
This particular formula groups the rows by date in <b>your_date_column</b> and calculates the sum of values for the <b>values_column</b> in the DataFrame.
Note that the <b>dt.day()</b> function extracts the day from a date column in pandas.
The following example shows how to use this syntax in practice.
<h2>Example: How to Group by Day in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2020', freq='8h', periods=10),   'sales': [6, 8, 9, 11, 13, 8, 8, 15, 22, 9],   'returns': [0, 3, 2, 2, 1, 3, 2, 4, 1, 5]})
#view DataFrame
print(df)
 date  sales  returns
0 2020-01-01 00:00:00      6        0
1 2020-01-01 08:00:00      8        3
2 2020-01-01 16:00:00      9        2
3 2020-01-02 00:00:00     11        2
4 2020-01-02 08:00:00     13        1
5 2020-01-02 16:00:00      8        3
6 2020-01-03 00:00:00      8        2
7 2020-01-03 08:00:00     15        4
8 2020-01-03 16:00:00     22        1
9 2020-01-04 00:00:00      9        5</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by day:
<b>#calculate sum of sales grouped by day
df.groupby(df.date.dt.day)['sales'].sum()
date
1    23
2    32
3    45
4     9
Name: sales, dtype: int64</b>
Here’s how to interpret the output:
The total sales made on January 1st was <b>23</b>.
The total sales made on January 2nd was <b>32</b>.
The total sales made on January 3rd was <b>45</b>.
The total sales made on January 4th was <b>9</b>.
We can use similar syntax to calculate the max of the sales values grouped by month:
<b>#calculate max of sales grouped by day
df.groupby(df.date.dt.day)['sales'].max()
date
1     9
2    13
3    22
4     9
Name: sales, dtype: int64</b>
We can use similar syntax to calculate any value we’d like grouped by the day value of a date column.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Group by Week in Pandas 
 How to Group by Month in Pandas 
 How to Group by Quarter in Pandas 
<h2><span class="orange">How to Group Data by Hour in Pandas (With Example)</span></h2>
You can use the following syntax to group data by hour and perform some aggregation in pandas:
<b>df.groupby([df['time'].dt.hour]).sales.sum()
</b>
This particular example groups the values by hour in a column called <b>time</b> and then calculates the sum of values in the <b>sales</b> column for each hour.
The following example shows how to use this syntax in practice.
<h2>Example: Group Data by Hour in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the number of sales made at various times throughout the day for some store:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'time': ['2022-01-01 01:14:00', '2022-01-01 01:24:15',            '2022-01-01 02:52:19', '2022-01-01 02:54:00',            '2022-01-01 04:05:10', '2022-01-01 05:35:09'],   'sales': [18, 20, 15, 14, 10, 9]})
#convert date column to datetime
df['time'] = pd.to_datetime(df['time'])
#view DataFrame
print(df)
 time  sales
0 2022-01-01 01:14:00     18
1 2022-01-01 01:24:15     20
2 2022-01-01 02:52:19     15
3 2022-01-01 02:54:00     14
4 2022-01-01 04:05:10     10
5 2022-01-01 05:35:09      9
</b>
We can use the following syntax to group the <b>time</b> column by hours and calculate the sum of <b>sales</b> for each hour:
<b>#group by hours in time column and calculate sum of sales
df.groupby([df['time'].dt.hour]).sales.sum()
time
1    38
2    29
4    10
5     9
Name: sales, dtype: int64</b>
From the output we can see:
A total of <b>38</b> sales were made during the first hour.
A total of <b>29</b> sales were made during the second hour.
A total of <b>10</b>sales were made during the fourth hour.
A total of <b>9</b> sales were made during the fifth hour.
Note that we can also perform some other aggregation.
For example, we could calculate the <b>mean</b> number of sales per hour:
<b>#group by hours in time column and calculate mean of sales
df.groupby([df['time'].dt.hour]).sales.mean()
time
1    19.0
2    14.5
4    10.0
5     9.0
Name: sales, dtype: float64
</b>
We can also group by hours and minutes if we’d like.
For example, the following code shows how to calculate the sum of sales, grouped by hours and minutes:
<b>#group by hours and minutes in time column and calculate mean of sales
df.groupby([df['time'].dt.hour, df['time'].dt.minute]).sales.mean()
time  time
1     14      18
      24      20
2     52      15
      54      14
4     5       10
5     35       9
Name: sales, dtype: int64
</b>
From the output we can see:
The mean number of sales during 1:14 was <b>18</b>.
The mean number of sales during 1:23 was <b>20</b>.
The mean number of sales during 2:52 was <b>15</b>.
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create a Date Range in Pandas 
 How to Extract Month from Date in Pandas 
 How to Convert Timestamp to Datetime in Pandas 
<h2><span class="orange">How to Group by Month in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to group rows by month in a pandas DataFrame:
<b>df.groupby(df.your_date_column.dt.month)['values_column'].sum()
</b>
This particular formula groups the rows by date in <b>your_date_column</b> and calculates the sum of values for the <b>values_column</b> in the DataFrame.
Note that the <b>dt.month()</b> function extracts the month from a date column in pandas.
The following example shows how to use this syntax in practice.
<h2>Example: How to Group by Month in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2020', freq='W', periods=10),   'sales': [6, 8, 9, 11, 13, 8, 8, 15, 22, 9],   'returns': [0, 3, 2, 2, 1, 3, 2, 4, 1, 5]})
#view DataFrame
print(df)
        date  sales  returns
0 2020-01-05      6        0
1 2020-01-12      8        3
2 2020-01-19      9        2
3 2020-01-26     11        2
4 2020-02-02     13        1
5 2020-02-09      8        3
6 2020-02-16      8        2
7 2020-02-23     15        4
8 2020-03-01     22        1
9 2020-03-08      9        5</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by month:
<b>#calculate sum of sales grouped by month
df.groupby(df.date.dt.month)['sales'].sum()
date
1    34
2    44
3    31
Name: sales, dtype: int64</b>
Here’s how to interpret the output:
The total sales made during month 1 (January) was <b>34</b>.
The total sales made during month 2 (February) was <b>44</b>.
The total sales made during month 3 (March) was <b>31</b>.
We can use similar syntax to calculate the max of the sales values grouped by month:
<b>#calculate max of sales grouped by month
df.groupby(df.date.dt.month)['sales'].max()
date
1    11
2    15
3    22
Name: sales, dtype: int64</b>
We can use similar syntax to calculate any value we’d like grouped by the month value of a date column.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">How to Group by Quarter in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to group rows by quarter in a pandas DataFrame:
<b>#convert date column to datetime
df['date'] = pd.to_datetime(df['date'])
#calculate sum of values, grouped by quarter
df.groupby(df['date'].dt.to_period('Q'))['values'].sum()
</b>
This particular formula groups the rows by quarter in the <b>date</b> column and calculates the sum for the <b>values</b> column in the DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: How to Group by Quarter in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2022', freq='M', periods=12),   'sales': [6, 8, 10, 5, 4, 8, 8, 3, 5, 14, 8, 3]})
#view DataFrame
print(df)
         date  sales
0  2022-01-31      6
1  2022-02-28      8
2  2022-03-31     10
3  2022-04-30      5
4  2022-05-31      4
5  2022-06-30      8
6  2022-07-31      8
7  2022-08-31      3
8  2022-09-30      5
9  2022-10-31     14
10 2022-11-30      8
11 2022-12-31      3</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by quarter:
<b>#convert date column to datetime and subtract one week
df['date'] = pd.to_datetime(df['date'])
#calculate sum of sales, grouped by quarter
df.groupby(df['date'].dt.to_period('Q'))['sales'].sum()
date
2022Q1    24
2022Q2    17
2022Q3    16
2022Q4    25
Freq: Q-DEC, Name: sales, dtype: int64
</b>
Here’s how to interpret the output:
There were <b>24 </b>total sales made during the first quarter.
There were <b>17 </b>total sales made during the second quarter.
There were <b>16 </b>total sales made during the third quarter.
There were <b>25 </b>total sales made during the fourth quarter.
We can use similar syntax to calculate some other metric, grouped by quarter.
For example, we could instead calculate the max value of sales, grouped by quarter:
<b>#convert date column to datetime
df['date'] = pd.to_datetime(df['date'])
#calculate max of sales, grouped by quarter
df.groupby(df['date'].dt.to_period('Q'))['sales'].max()
date
2022Q1    10
2022Q2     8
2022Q3     8
2022Q4    14
Freq: Q-DEC, Name: sales, dtype: int64
</b>
Here’s how to interpret the output:
The max sales on an individual month during the first quarter was <b>10</b>.
The max sales on an individual month during the second quarter was <b>8</b>.
The max sales on an individual month during the third quarter was <b>8</b>.
The max sales on an individual month during the fourth quarter was <b>14</b>.
<b>Note</b>: You can find the complete documentation for the <b>groupby</b> operation in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Group by Month in Pandas DataFrame 
 How to Group by Week in Pandas DataFrame 
 Pandas: How to Use Groupby and Count with Condition 
<h2><span class="orange">How to Group by Week in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to group rows by week in a pandas DataFrame:
<b>#convert date column to datetime and subtract one week
df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')
#calculate sum of values, grouped by week
df.groupby([pd.Grouper(key='date', freq='W')])['values'].sum()
</b>
This particular formula groups the rows by week in the <b>date</b> column and calculates the sum of values for the <b>values</b> column in the DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: How to Group by Week in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/5/2022', freq='D', periods=15),   'sales': [6, 8, 9, 5, 4, 8, 8, 3, 5, 9, 8, 3, 4, 7, 7]})
#view DataFrame
print(df)
         date  sales
0  2022-01-05      6
1  2022-01-06      8
2  2022-01-07      9
3  2022-01-08      5
4  2022-01-09      4
5  2022-01-10      8
6  2022-01-11      8
7  2022-01-12      3
8  2022-01-13      5
9  2022-01-14      9
10 2022-01-15      8
11 2022-01-16      3
12 2022-01-17      4
13 2022-01-18      7
14 2022-01-19      7
</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by week:
<b>#convert date column to datetime and subtract one week
df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')
#calculate sum of values, grouped by week
df.groupby([pd.Grouper(key='date', freq='W')])['sales'].sum()
date
2022-01-02    32
2022-01-09    44
2022-01-16    18
Freq: W-SUN, Name: sales, dtype: int64
</b>
Here’s how to interpret the output:
There were <b>32 </b>total sales made during the week starting the day after 1/2/2022.
There were <b>44 </b>total sales made during the week starting the day after 1/9/2022.
There were <b>18 </b>total sales made during the week starting the day after 1/16/2022.
It’s worth noting that by default, pandas assumes that the week starts the day after Sunday (<b>W-SUN</b>). 
However, according to  the documentation  you can change this value for <b>Freq</b>.
For example, you can specify <b>Freq=W-MON</b> if you’d like each week to start the day after Monday (i.e. Tuesday) instead.
We can use similar syntax to calculate the max of the sales values grouped by week:
<b>#convert date column to datetime and subtract one week
df['date'] = pd.to_datetime(df['date']) - pd.to_timedelta(7, unit='d')
#calculate max of values, grouped by week
df.groupby([pd.Grouper(key='date', freq='W')])['sales'].max()
date
2022-01-02    9
2022-01-09    9
2022-01-16    7
Freq: W-SUN, Name: sales, dtype: int64
</b>
Here’s how to interpret the output:
The max sales on an individual day during the week starting the day after 1/2/2022 was <b>9</b>.
The max sales on an individual day during the week starting the day after 1/9/2022 was <b>9</b>.
The max sales on an individual day during the week starting the day after 1/16/2022 was <b>7</b>.
<b>Note</b>: You can find the complete documentation for the <b>groupby</b> operation in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Group by Month in Pandas DataFrame 
 How to Group by Day in Pandas DataFrame 
 How to Use Groupby and Count with Condition in Pandas 
<h2><span class="orange">Pandas: How to Use Group By with Where Condition</span></h2>
The easiest way to use<b> group by</b> with a <b>where</b> condition in pandas is to use the<b> query()</b> function:
<b>df.query("team == 'A'").groupby(["position"])["points"].mean().reset_index()
</b>
This particular example example calculates the mean value of <b>points</b>, grouped by <b>position</b>, where <b>team</b> is equal to ‘A’ in some pandas DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: How to Use Group By with Where Condition in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'F', 'G', 'G', 'F'],   'points': [22, 14, 15, 10, 8, 29, 33, 18]})
#view DataFrame
print(df)
  team position  points
0    A        G      22
1    A        G      14
2    A        F      15
3    A        F      10
4    A        F       8
5    B        G      29
6    B        G      33
7    B        F      18
</b>
We can use the following code to calculate the mean value of <b>points</b>, grouped by <b>position</b>, where <b>team</b> is equal to ‘A’:
<b>#calculate mean value of points, grouped by position, where team == 'A'
df.query("team == 'A'").groupby(["position"])["points"].mean().reset_index()
        position  points
0F  11.0
1G  18.0
</b>
From the output we can see:
The mean points value for players in position ‘F’ is on team A is <b>11</b>.
The mean points value for players in position ‘G’ on team A is <b>18</b>.
Note that we can also use the <b>&</b> operator in the <b>query()</b> function to query for rows where multiple conditions are met.
For example, the following code shows how to calculate the mean value of <b>points</b>, grouped by <b>position</b>, where <b>team</b> is equal to ‘A’ <em>and</em> position is equal to ‘G’:
<b>#calculate mean value of points by position where team is 'A' and position is 'G'
df.query("team=='A' & position=='G'").groupby(["position"])["points"].mean().reset_index()
position  points
0G  18.0
</b>
From the output we can see that the mean points value for players in position ‘G’ on team A is <b>18</b>.
Since we specified two conditions in the <b>query()</b> function, only the rows that met both conditions were used.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform a GroupBy Sum in Pandas 
 How to Use Groupby and Plot in Pandas 
 How to Count Unique Values Using GroupBy in Pandas 
<h2><span class="orange">How to Group by Year in Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to group rows by year in a pandas DataFrame:
<b>df.groupby(df.your_date_column.dt.year)['values_column'].sum()
</b>
This particular formula groups the rows by date in <b>your_date_column</b> and calculates the sum of values for the <b>values_column</b> in the DataFrame.
Note that the <b>dt.year()</b> function extracts the year from a date column in pandas.
The following example shows how to use this syntax in practice.
<h2>Example: How to Group by Year in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2020', freq='3m', periods=10),   'sales': [6, 8, 9, 11, 13, 8, 8, 15, 22, 9],   'returns': [0, 3, 2, 2, 1, 3, 2, 4, 1, 5]})
#view DataFrame
print(df)
        date  sales  returns
0 2020-01-31      6        0
1 2020-04-30      8        3
2 2020-07-31      9        2
3 2020-10-31     11        2
4 2021-01-31     13        1
5 2021-04-30      8        3
6 2021-07-31      8        2
7 2021-10-31     15        4
8 2022-01-31     22        1
9 2022-04-30      9        5</b>
<b>Related:</b>  How to Create a Date Range in Pandas 
We can use the following syntax to calculate the sum of sales grouped by year:
<b>#calculate sum of sales grouped by year
df.groupby(df.date.dt.year)['sales'].sum()
date
2020    34
2021    44
2022    31
Name: sales, dtype: int64</b>
Here’s how to interpret the output:
The total sales made during 2020 was <b>34</b>.
The total sales made during 2021 was <b>44</b>.
The total sales made during 2022 was <b>31</b>.
We can use similar syntax to calculate the max of the sales values grouped by year:
<b>#calculate max of sales grouped by year
df.groupby(df.date.dt.year)['sales'].max()
date
2020    11
2021    15
2022    22
Name: sales, dtype: int64</b>
We can use similar syntax to calculate any value we’d like grouped by the year value of a date column.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Calculate Cumulative Sum by Group 
 Pandas: How to Count Unique Values by Group 
 Pandas: How to Calculate Correlation By Group 
<h2><span class="orange">Pandas: How to Group and Aggregate by Multiple Columns</span></h2>
Often you may want to group and aggregate by multiple columns of a pandas DataFrame.
Fortunately this is easy to do using the pandas  .groupby()  and  .agg()  functions.
This tutorial explains several examples of how to use these functions in practice.
<h3>Example 1: Group by Two Columns and Find Average</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'B', 'B', 'B', 'M', 'M', 'M'],   'position': ['G', 'G', 'F', 'G', 'F', 'F', 'C', 'C'],   'assists': [5, 7, 7, 8, 5, 7, 6, 9],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10]})
#view DataFrame
print(df)
  team position  assists  rebounds
0    A        G        5        11
1    B        G        7         8
2    B        F        7        10
3    B        G        8         6
4    B        F        5         6
5    M        F        7         9
6    M        C        6         6
7    M        C        9        10
</b>
The following code shows how to group by columns ‘team’ and ‘position’ and find the mean assists:
<b>df.groupby(['team', 'position']).agg({'assists': ['mean']}).reset_index()
        teamposition  assists          mean
0AG  5.0
1BF  6.0
2BG  7.5
3MC  7.5
4MF  7.0
</b>
The output tells us:
The mean assists for players in position G on team A is <b>5.0</b>.
The mean assists for players in position F on team B is <b>6.0</b>.
The mean assists for players in position G on team B is <b>7.5</b>.
And so on.
We can also use the following code to rename the columns in the resulting DataFrame:
<b>#group by team and position and find mean assists
new = df.groupby(['team', 'position']).agg({'assists': ['mean']}).reset_index()
#rename columns 
new.columns = ['team', 'pos', 'mean_assists']
#view DataFrame
print(new)
teamposmean_assists
0AG5.0
1BF6.0
2BG7.5
3MC7.5
4MF7.0</b>
<h3>Example 2: Group by Two Columns and Find Multiple Stats</h3>
Assume we use the same pandas DataFrame as the previous example:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'B', 'B', 'B', 'M', 'M', 'M'],   'position': ['G', 'G', 'F', 'G', 'F', 'F', 'C', 'C'],   'assists': [5, 7, 7, 8, 5, 7, 6, 9],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10]})</b>
The following code shows how to find the median and max number of rebounds, grouped on columns ‘team’ and ‘position’:
<b>df.groupby(['team', 'position']).agg({'rebounds': ['median', 'max']}).reset_index()
        teampositionrebounds         medianmax
0AG 1111
1BF 810
2BG 78
3MC 810
4MF 99
</b>
The output tells us:
The median rebounds assists for players in position G on team A is <b>11</b>.
The max rebounds for players in position G on team A is <b>11</b>.
The median rebounds for players in position F on team B is <b>8</b>.
The max rebounds for players in position F on team B is <b>10</b>.
And so on.
<h2><span class="orange">How to Apply Function to Pandas Groupby</span></h2>
You can use the following basic syntax to use the <b>groupby()</b> and <b>apply()</b> functions together in a pandas DataFrame:
<b>df.groupby('var1').apply(lambda x: some function)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points_for': [18, 22, 19, 14, 11, 20, 28],   'points_against': [14, 21, 19, 14, 12, 20, 21]})
#view DataFrame
print(df)
  team  points_for  points_against
0    A          18              14
1    A          22              21
2    A          19              19
3    B          14              14
4    B          11              12
5    B          20              20
6    B          28              21</b>
<h3>Example 1: Use groupby() and apply() to Find Relative Frequencies</h3>
The following code shows how to use the <b>groupby(</b>) and <b>apply()</b> functions to find the relative frequencies of each team name in the pandas DataFrame:
<b>#find relative frequency of each team name in DataFrame
df.groupby('team').apply(lambda x: x['team'].count() / df.shape[0])
team
A    0.428571
B    0.571429
dtype: float64
</b>
From the output we can see that team A occurs in 42.85% of all rows and team B occurs in 57.14% of all rows.
<h3>Example 2: Use groupby() and apply() to Find Max Values</h3>
The following code shows how to use the <b>groupby(</b>) and <b>apply()</b> functions to find the max “points_for” values for each team:
<b>#find max "points_for" values for each team
df.groupby('team').apply(lambda x: x['points_for'].max())
team
A    22
B    28
dtype: int64</b>
From the output we can see that the max points scored by team A is 22 and the max points scored by team B is 28.
<h3>Example 3: Use groupby() and apply() to Perform Custom Calculation</h3>
The following code shows how to use the <b>groupby(</b>) and <b>apply()</b> functions to find the mean difference between “points_for” and “points_against” for each team:
<b>#find max "points_for" values for each team
df.groupby('team').apply(lambda x: (x['points_for'] - x['points_against']).mean())
team
A    1.666667
B    1.500000
dtype: float64
</b>
From the output we can see that the mean difference between “points for” and “points against” is <b>1.67</b> for team A and <b>1.50</b> for team B.
<h2><span class="orange">Pandas: How to Create Bar Plot from GroupBy</span></h2>
You can use the following syntax to create a bar plot from a GroupBy function in pandas:
<b>#calculate sum of values by group
df_groups = df.groupby(['group_var'])['values_var'].sum()
#create bar plot by group
df_groups.plot(kind='bar')</b>
The following example shows how to use this syntax in practice. 
<h2>Example: Create Bar Plot from GroupBy in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A',            'B', 'B', 'B', 'B', 'B',            'C', 'C', 'C', 'C', 'C'],   'points': [12, 29, 34, 14, 10, 11, 7, 36,              34, 22, 41, 40, 45, 36, 38]})
#view first five rows of DataFrame
df.head()
teampoints
0A12
1A29
2A34
3A14
4A10</b>
We can use the following syntax to calculate the sum of points scored by each team and create a bar plot to visualize the sum for each team:
<b>import matplotlib.pyplot as plt
#calculate sum of points for each team
df.groupby('team')['points'].sum()
#create bar plot by group
df_groups.plot(kind='bar')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/groupby_bar1.jpg"387">
The x-axis shows the name of each team and the y-axis shows the sum of the points scored by each team.
We can also use the following code to make the plot look a bit better:
<b>import matplotlib.pyplot as plt
#calculate sum of points for each team
df_groups = df.groupby(['team'])['points'].sum()
#create bar plot with custom aesthetics
df_groups.plot(kind='bar', title='Total Points by Team',
               ylabel='Total Points', xlabel='Team', figsize=(10, 6))
#rotate x-axis ticks vertically
plt.xticks(rotation=0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/groupby_bar2.jpg">
<b>Note</b>: You can find the complete documentation for the <b>GroupBy</b> function  here .
<h2><span class="orange">Pandas: How to Use GroupBy with Bin Counts</span></h2>
You can use the following syntax to calculate the bin counts of one variable grouped by another variable in pandas:
<b>#define bins
groups = df.groupby(['group_var', pd.cut(df.value_var, bins)])
#display bin count by group variable
groups.size().unstack()
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Use GroupBy with Bin Counts in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',            'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'points': [4, 7, 7, 11, 12, 15, 19, 19,             5, 5, 11, 12, 14, 14, 15, 15]})
#view DataFrame
print(df)
   team  points
0     A       4
1     A       7
2     A       7
3     A      11
4     A      12
5     A      15
6     A      19
7     A      19
8     B       5
9     B       5
10    B      11
11    B      12
12    B      14
13    B      14
14    B      15
15    B      15
</b>
We can use the following syntax to calculate the frequency of points for each team, grouped into specific bins:
<b>#define groups
groups = df.groupby(['team', pd.cut(df.points, [0, 10, 15, 20])])
#display bin count grouped by team
groups.size().unstack()
points(0, 10](10, 15] (15, 20]
  team
     A      3       32
     B      2       60
</b>
Here’s how to interpret the output:
A total of <b>3</b> players on team A scored between 0 and 10 points.
A total of <b>3</b> players on team A scored between 10 and 15 points.
A total of <b>2</b> players on team A scored between 15 and 20 points.
And so on.
Note that we can specify whichever bins we’d like within the <b>pd.cut()</b> function.
For example, we could define just two bins:
<b>#define groups
groups = df.groupby(['team', pd.cut(df.points, [0, 10, 20])])
#display bin count grouped by team
groups.size().unstack()
points(0, 10](10, 20]
  team
     A     3      5
     B     2      6</b>
Here’s how to interpret the output:
A total of <b>3</b> players on team A scored between 0 and 10 points.
A total of <b>5</b> players on team A scored between 10 and 20 points.
A total of <b>2</b> players on team B scored between 0 and 10 points.
A total of <b>6</b> players on team B scored between 10 and 20 points.
<b>Note 1</b>: You can find the complete documentation for the <b>GroupBy</b> function  here .
<b>Note 2</b>: You can find the complete documentation for the <b>cut</b> function  here .
<h2><span class="orange">Pandas: How to Concatenate Strings from Using GroupBy</span></h2>
You can use the following basic syntax to concatenate strings from using GroupBy in pandas:
<b>df.groupby(['group_var'], as_index=False).agg({'string_var': ' '.join})
</b>
This particular formula groups rows by the <b>group_var</b> column and then concatenates the strings in the <b>string_var</b> column.
The following example shows how to use this syntax in practice.
<h3>Example: How to Concatenate Strings from Using GroupBy</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'quarter': [1, 1, 2, 2, 1, 1, 2, 2],   'employee': ['Andy', 'Bob', 'Chad', 'Diane',                'Elana', 'Frank', 'George', 'Hank']})
#view DataFrame
print(df)</b>
We can use the following syntax to group the rows of the DataFrame by <b>store</b> and <b>quarter</b> and then concatenate the strings in the <b>employee</b> column:
<b>#group by store and quarter, then concatenate employee strings
df.groupby(['store', 'quarter'], as_index=False).agg({'employee': ' '.join})
storequarteremployee
0A1Andy Bob
1A2Chad Diane
2B1Elana Frank
3B2George Hank</b>
The result is a DataFrame grouped by <b>store</b> and <b>quarter</b> with the strings in the <b>employee</b> column concatenated together with a space.
We could also concatenate the strings using a different separator such as the <b>&</b> symbol:
<b>#group by store and quarter, then concatenate employee strings
df.groupby(['store', 'quarter'], as_index=False).agg({'employee': ' & '.join})
storequarter employee
0A1 Andy & Bob
1A2 Chad & Diane
2B1 Elana & Frank
3B2 George & Hank</b>
Notice that the strings in the employee column are now separated by the <b>&</b> symbol.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">How to Calculate Correlation By Group in Pandas</span></h2>
You can use the following basic syntax to calculate the correlation between two variables by group in pandas:
<b>df.groupby('group_var')[['values1','values2']].corr().unstack().iloc[:,1]
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Correlation By Group in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [2, 7, 9, 3, 12, 10, 14, 21]})
#view DataFrame
print(df)</b>
We can use the following code to calculate the correlation between <b>points</b> and <b>assists</b>, grouped by <b>team</b>:
<b>#calculate correlation between points and assists, grouped by team
df.groupby('team')[['points','assists']].corr().unstack().iloc[:,1]
team
A    0.603053
B    0.981798
Name: (points, assists), dtype: float64</b>
From the output we can see:
The correlation coefficient between points and assists for team A is <b>.603053</b>.
The correlation coefficient between points and assists for team B is <b>.981798</b>.
Since both correlation coefficients are positive, this tells us that the relationship between points and assists for both teams is positive.
That is, players who tend to score more points also tend to record more assists.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
Note that we could shorten the syntax by not using the <b>unstack</b> and <b>iloc</b> functions, but the results are uglier:
<b>df.groupby('team')[['points','assists']].corr()
points  assists
team
Apoints1.000000  0.603053
        assists0.603053  1.000000
Bpoints1.000000  0.981798
        assists0.981798  1.000000
</b>
This syntax produces a correlation matrix for both teams, which provides us with excessive information.
<h2><span class="orange">How to Count Unique Values Using Pandas GroupBy</span></h2>
You can use the following basic syntax to count the number of unique values by group in a pandas DataFrame:
<b>df.groupby('group_column')['count_column'].nunique()
</b>
The following examples show how to use this syntax with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'G', 'F', 'F', 'G', 'G', 'F', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 9, 9, 4, 7, 7],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 13, 15]})
#view DataFrame
df
teamposition pointsrebounds
0AG 511
1AG 78
2AG 710
3AF 96
4AF 126
5BG 95
6BG 99
7BF 412
8BF 713
9BF 715
</b>
<h3>Example 1: Group By One Column & Count Unique Values</h3>
The following code shows how to count the number of unique values in the ‘points’ column for each team:
<b>#count number of unique values in 'points' column grouped by 'team' column
df.groupby('team')['points'].nunique()
team
A    4
B    3
Name: points, dtype: int64
</b>
From the output we can see:
There are <b>4</b> unique ‘points’ values for team A.
There are <b>3</b> unique ‘points’ values for team B.
Note that we can also use the <b>unique()</b> function to display each unique ‘points’ value by team:
<b>#display unique values in 'points' column grouped by 'team'
df.groupby('team')['points'].unique()
team
A    [5, 7, 9, 12]
B        [9, 4, 7]
Name: points, dtype: object</b>
<h3>Example 2: Group By Multiple Columns & Count Unique Values</h3>
The following code shows how to count the number of unique values in the ‘points’ column, grouped by team <em>and</em> position:
<b>#count number of unique values in 'points' column grouped by 'team' and 'position'
df.groupby(['team', 'position'])['points'].nunique()
team  position
A     F           2
      G           2
B     F           2
      G           1
Name: points, dtype: int64</b>
From the output we can see:
There are <b>2</b> unique ‘points’ values for players in position ‘F’ on team A.
There are <b>2</b> unique ‘points’ values for players in position ‘G’ on team A.
There are <b>2</b> unique ‘points’ values for players in position ‘F’ on team B.
There is<b>1</b> unique ‘points’ value for players in position ‘G’ on team B.
Once again, we can use the <b>unique()</b> function to display each unique ‘points’ value by team and position:
<b>#display unique values in 'points' column grouped by 'team' and 'position'
df.groupby(['team', 'position'])['points'].unique()
team  position
A     F           [9, 12]
      G            [5, 7]
B     F            [4, 7]
      G               [9]
Name: points, dtype: object</b>
<h2><span class="orange">Pandas: How to Use Groupby and Count with Condition</span></h2>
You can use the following basic syntax to perform a groupby and count with condition in a pandas DataFrame:
<b>df.groupby('var1')['var2'].apply(lambda x: (x=='val').sum()).reset_index(name='count')
</b>
This particular syntax groups the rows of the DataFrame based on <b>var1</b> and then counts the number of rows where <b>var2</b> is equal to ‘val.’
The following example shows how to use this syntax in practice.
<h3>Example: Groupby and Count with Condition in Pandas</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'pos': ['Gu', 'Fo', 'Fo', 'Fo', 'Gu', 'Gu', 'Fo', 'Fo'],   'points': [18, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team pos  points
0    A  Gu      18
1    A  Fo      22
2    A  Fo      19
3    A  Fo      14
4    B  Gu      14
5    B  Gu      11
6    B  Fo      20
7    B  Fo      28</b>
The following code shows how to group the DataFrame by the <b>team</b> variable and count the number of rows where the <b>pos</b> variable is equal to ‘Gu’:
<b>#groupby team and count number of 'pos' equal to 'Gu'
df_count = df.groupby('team')['pos'].apply(lambda x: (x=='Gu').sum()).reset_index(name='count')
#view results
print(df_count)
  team  count
0    A      1
1    B      2
</b>
From the output we can see:
Team A has <b>1</b> row where the pos column is equal to ‘Gu’
Team B has <b>2</b> rows where the pos column is equal to ‘Gu’
We can use similar syntax to perform a groupby and count with some numerical condition.
For example, the following code shows how to group by the <b>team</b> variable and count the number of rows where the <b>points </b>variable is greater than 15:
<b>#groupby team and count number of 'points' greater than 15
df_count = df.groupby('team')['points'].apply(lambda x: (x>15).sum()).reset_index(name='count')
#view results
print(df_count)
  team  count
0    A      3
1    B      2</b>
From the output we can see:
Team A has <b>3</b> rows where the points column is greater than 15
Team B has <b>2</b> rows where the points column is greater than 15 
You can use similar syntax to perform a groupby and count with any specific condition you’d like.
<h2><span class="orange">How to Count Observations by Group in Pandas</span></h2>
Often you may be interested in counting the number of  observations  by group in a pandas DataFrame. 
Fortunately this is easy to do using the <b>groupby()</b> and <b>size()</b> functions with the following syntax:
<b>df.groupby('column_name').size()
</b>
This tutorial explains several examples of how to use this function in practice using the following data frame:
<b>import numpy as np
import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C'],   'division':['E', 'W', 'E', 'E', 'W', 'W', 'E'],   'rebounds': [11, 8, 7, 6, 6, 5, 12]})
#display DataFrame
print(df)
  team division  rebounds
0    A        E        11
1    A        W         8
2    B        E         7
3    B        E         6
4    B        W         6
5    C        W         5
6    C        E        12</b>
<h3>Example 1: Count by One Variable</h3>
The following code shows how to count the total number of observations by team:
<b>#count total observations by variable 'team'
df.groupby('team').size()
team
A    2
B    3
C    2
dtype: int64
</b>
From the output we can see that:
Team A has 2 observations
Team B has 3 observations
Team C has 2 observations
Note that the previous code produces a Series. In most cases we want to work with a DataFrame, so we can use the <b>reset_index()</b> function to produce a DataFrame instead:
<b>df.groupby('team').size().reset_index(name='obs')
        teamobs
0A2
1B3
2C2
</b>
<h3>Example 2: Count and Sort by One Variable</h3>
We can also use the <b>sort_values()</b> function to sort the group counts.
We can specify <b>ascending=False</b> to sort group counts from largest to smallest or <b>ascending=True</b> to sort from smallest to largest:
<b>df.groupby('team').size().reset_index(name='obs').sort_values(['obs'], ascending=True)
        teamobs
0A2
2C2
1B3</b>
<h3>Example 3: Count by Multiple Variables</h3>
We can also count the number of observations grouped by multiple variables in a pandas DataFrame:
<b>#count observations grouped by team and division
df.groupby(['team', 'division']).size().reset_index(name='obs')
        teamdivision  obs
0AE  1
1AW  1
2BE  2
3BW  1
4CE  1
5CW  1
</b>
From the output we can see that:
1 observation belongs to Team A and division E
1 observation belongs to Team A and division W
2 observations belongs to Team B and division E
1 observation belongs to Team B and division W
1 observation belongs to Team C and division E
1 observation belongs to Team C and division W
<h2><span class="orange">Pandas: How to Use describe() by Group</span></h2>
You can use the <b>describe()</b> function to generate descriptive statistics for variables in a pandas DataFrame.
You can use the following basic syntax to use the <b>describe()</b> function with the <b>groupby()</b> function in pandas:
<b>df.groupby('group_var')['values_var'].describe()
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Use describe() by Group in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about basketball players on two different teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [8, 12, 14, 14, 15, 22, 27, 24],   'assists':[2, 2, 3, 5, 7, 6, 8, 12]})
#view DataFrame
print(df)
  team  points  assists
0    A       8        2
1    A      12        2
2    A      14        3
3    A      14        5
4    B      15        7
5    B      22        6
6    B      27        8
7    B      24       12</b>
We can use the <b>describe()</b> function along with the <b>groupby()</b> function to summarize the values in the <b>points</b> column for each <b>team</b>:
<b>#summarize points by team
df.groupby('team')['points'].describe()
countmeanstd        min25%50%75%max
team
A4.012.02.8284278.011.0013.014.0014.0
B4.022.05.09902015.020.2523.024.7527.0</b>
From the output, we can see the following values for the <b>points</b> variable for each team:
<b>count</b> (number of observations)
<b>mean</b> (mean points value)
<b>std</b> (standard deviation of points values)
<b>min</b> (minimum points value)
<b>25</b>% (25th percentile of points)
<b>50</b>% (50th percentile (i.e. median) of points)
<b>75</b>% (75th percentile of points)
<b>max</b> (maximum points value)
If you’d like the results to be displayed in a DataFrame format, you can use the <b>reset_index()</b> argument:
<b>#summarize points by team
df.groupby('team')['points'].describe().reset_index()
        teamcountmeanstd        min25%50%75%max
0A4.012.02.8284278.011.0013.014.0014.0
1B4.022.05.09902015.020.2523.024.7527.0
</b>
The variable <b>team</b> is now a column in the DataFrame and the index values are 0 and 1.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Calculate Cumulative Sum by Group 
 Pandas: How to Count Unique Values by Group 
 Pandas: How to Calculate Correlation By Group 
<h2><span class="orange">Pandas: How to Use groupby with diff</span></h2>
You can use the following basic syntax to use the <b>groupby()</b> function with the<b> diff()</b> function in pandas:
<b>df = df.sort_values(by=['group_var1', 'group_var2'])
df['diff'] = df.groupby(['group_var1'])['values_var'].diff().fillna(0)
</b>
This particular example sorts the rows of the DataFrame by two specific variables, then groups by <b>group_var1</b> and calculates the difference between rows in the <b>values_var</b> column.
Note that <b>fillna(0)</b> tells pandas to insert a zero whenever the value of the group variable changes between consecutive rows in the DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: How to Use groupby with diff in Pandas</h2>
Suppose we have the following pandas DataFrame that contains the total sales made by two different stores on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'date': pd.to_datetime(['2022-01-01', '2022-01-02',                           '2022-01-03', '2022-01-04',                           '2022-01-01', '2022-01-02',                           '2022-01-03', '2022-01-04']),   'sales': [12, 15, 24, 24, 14, 19, 12, 38]})
#view DataFrame
print(df)
  store       date  sales
0     A 2022-01-01     12
1     A 2022-01-02     15
2     A 2022-01-03     24
3     A 2022-01-04     24
4     B 2022-01-01     14
5     B 2022-01-02     19
6     B 2022-01-03     12
7     B 2022-01-04     38</b>
Now suppose that we would like to create a new column called <b>sales_diff</b> that contains the difference in sales values between consecutive dates, grouped by store.
We can use the following syntax to do so:
<b>#sort DataFrame by store and date
df = df.sort_values(by=['store', 'date'])
#create new column that contains difference between sales grouped by store
df['sales_diff'] = df.groupby(['store'])['sales'].diff().fillna(0)
#view update DataFrame
print(df)
  store       date  sales  sales_diff
0     A 2022-01-01     12         0.0
1     A 2022-01-02     15         3.0
2     A 2022-01-03     24         9.0
3     A 2022-01-04     24         0.0
4     B 2022-01-01     14         0.0
5     B 2022-01-02     19         5.0
6     B 2022-01-03     12        -7.0
7     B 2022-01-04     38        26.0
</b>
The new <b>sales_diff</b> column contains the difference in sales values between consecutive dates, grouped by store.
For example, we can see:
The difference in sales at store A between 1/1/2022 and 1/2/2022 is <b>3</b>.
The difference in sales at store A between 1/2/2022 and 1/3/2022 is <b>9</b>.
The difference in sales at store A between 1/3/2022 and 1/4/2022 is <b>0</b>.
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform a GroupBy Sum in Pandas 
 How to Use Groupby and Plot in Pandas 
 How to Count Unique Values Using GroupBy in Pandas 
<h2><span class="orange">Pandas: How to Get Group After Using groupby()</span></h2>
You can use the following methods to get a specific group after using the <b>groupby()</b> function on a pandas DataFrame:
<b>Method 1: Get Group After Using groupby()</b>
<b>grouped_df.get_group('A')
</b>
<b>Method 2: Get Specific Columns of Group After Using groupby()</b>
<b>grouped_df[['column1', 'column3']].get_group('A')</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'sales': [12, 15, 24, 24, 14, 19, 12, 38],   'refunds': [4, 8, 7, 7, 10, 5, 4, 11]})
#view DataFrame
print(df)
  store  sales  refunds
0     A     12        4
1     A     15        8
2     A     24        7
3     A     24        7
4     B     14       10
5     B     19        5
6     B     12        4
7     B     38       11</b>
<h2>Example 1: Get Group After Using groupby() </h2>
The following code shows how to use the <b>groupby(</b>) function to group the rows by store name, then use the <b>get_group()</b> function to retrieve all rows that belong to the group with the group name ‘A’:
<b>#group rows of DataFrame based on value in 'store' column
grouped_stores = df.groupby(['store'])
#get all rows that belong to group name 'A'
grouped_stores.get_group('A')
    store    sales  refunds
0A12  4
1A15  8
2A24  7
3A24  7
</b>
Notice that <b>get_group()</b> returns all rows that belong to the group with the group name ‘A’.
<h2>Example 2: Get Specific Columns of Group After Using groupby() </h2>
The following code shows how to use the <b>groupby(</b>) function to group the rows by store name, then use the <b>get_group()</b> function to retrieve all rows that belong to the group with the group name ‘A’ only for the ‘sales’ and ‘refunds’ columns:
<b>#group rows of DataFrame based on value in 'store' column
grouped_stores = df.groupby(['store'])
#get all rows that belong to group name 'A' for sales and refunds columns
grouped_stores[['store', 'refunds']].get_group('A')
    store  refunds
0A 4
1A 8
2A 7
3A 7
</b>
Notice that <b>get_group()</b> returns all rows that belong to the group with the group name ‘A’ for the ‘sales’ and ‘refunds’ columns only.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform a GroupBy Sum in Pandas 
 How to Use Groupby and Plot in Pandas 
 How to Count Unique Values Using GroupBy in Pandas 
<h2><span class="orange">Pandas: A Simple Formula for “Group By Having”</span></h2>
You can use the following basic syntax to perform the equivalent of a SQL “GROUP BY HAVING” statement in pandas:
<b>df.groupby('some_column').filter(lambda x: some condition)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C'],   'position': ['G', 'F', 'F', 'G', 'F', 'F', 'G', 'G'],   'points': [30, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team position  points
0    A        G      30
1    A        F      22
2    A        F      19
3    B        G      14
4    B        F      14
5    B        F      11
6    C        G      20
7    C        G      28
</b>
<h2>Example 1: Pandas Group By Having with Count</h2>
The following code shows how to group the rows by the value in the <b>team</b> column, then filter for only the teams that have a count greater than 2:
<b>#group by team and filter for teams with count > 2
df.groupby('team').filter(lambda x: len(x) > 2)
        teamposition points
0AG 30
1AF 22
2AF 19
3BG 14
4BF 14
5BF 11</b>
Notice that only the rows with a team value of ‘A’ or ‘B’ are returned since these are the two teams that have a count greater than 2.
<h2>Example 2: Pandas Group By Having with Mean</h2>
The following code shows how to group the rows by the value in the <b>team</b> column, then filter for only the teams that have a mean <b>points</b> value greater than 20:
<b>#group by team and filter for teams with mean points > 20
df.groupby('team').filter(lambda x: x['points'].mean() > 20)
        teamposition points
0AG 30
1AF 22
2AF 19
6CG 20
7CG 28</b>
Notice that only the rows with a team value of ‘A’ or ‘C’ are returned since these are the two teams that have a mean points value greater than 20.
<h2>Example 3: Pandas Group By Having with Sum</h2>
The following code shows how to group the rows by the value in the <b>team</b> column, then filter for only the teams that have a sum of <b>points</b> equal to exactly 48:
<b>#group by team and filter for teams with sum of points equal to 48
df.groupby('team').filter(lambda x: x['points'].sum() == 48)
        teamposition points
6CG 20
7CG 28</b>
Notice that only the rows with a team value of ‘C’ are returned since this is the one team that has a sum of points equal to 48.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: Get Index of Rows Whose Column Matches Value 
 Pandas: How to Select Columns Containing a Specific String 
 Pandas: How to Check if Column Contains String 
<h2><span class="orange">Pandas: How to Group By Index and Perform Calculation</span></h2>
You can use the following methods to group by one or more index columns in pandas and perform some calculation:
<b>Method 1: Group By One Index Column</b>
<b>df.groupby('index1')['numeric_column'].max()
</b>
<b>Method 2: Group By Multiple Index Columns</b>
<b>df.groupby(['index1', 'index2'])['numeric_column'].sum()</b>
<b>Method 3: Group By Index Column and Regular Column</b>
<b>df.groupby(['index1', 'numeric_column1'])['numeric_column2'].nunique()</b>
The following examples show how to use each method with the following pandas DataFrame that has a MultiIndex:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'G', 'F', 'F', 'G', 'G', 'F', 'F', 'F'],   'points': [7, 7, 7, 19, 16, 9, 10, 10, 8, 8],   'rebounds': [8, 8, 8, 10, 11, 12, 13, 13, 15, 11]})
#set 'team' column to be index column
df.set_index(['team', 'position'], inplace=True)
#view DataFrame
df
 points rebounds
teamposition
AG 7 8
        G 7 8
        G 7 8
        F 19 10
        F 16 11
BG 9 12
        G 10 13
        F 10 13
        F 8 15
        F 8 11
</b>
<h3>Method 1: Group By One Index Column</h3>
The following code shows how to find the max value of the ‘points’ column, grouped by the ‘position’ index column:
<b>#find max value of 'points' grouped by 'position index column
df.groupby('position')['points'].max()
position
F    19
G    10
Name: points, dtype: int64
</b>
<h3>Method 2: Group By Multiple Index Columns</h3>
The following code shows how to find the sum of the ‘points’ column, grouped by the ‘team’ and ‘position’ index columns:
<b>#find max value of 'points' grouped by 'position index column
df.groupby(['team', 'position'])['points'].sum()
team  position
A     F           35
      G           21
B     F           26
      G           19
Name: points, dtype: int64</b>
<h3>Method 3: Group By Index Column & Regular Column</h3>
The following code shows how to find the number of unique values in the ‘rebounds’ column, grouped by the index column ‘team’ and the ordinary column ‘points’:
<b>#find max value of 'points' grouped by 'position index column
df.groupby(['team', 'points'])['rebounds'].nunique()
team  points
A     7         1
      16        1
      19        1
B     8         2
      9         1
      10        1
Name: rebounds, dtype: int64</b>
<h2><span class="orange">Pandas: How to Group Rows into List Using GroupBy</span></h2>
You can use the following methods to group DataFrame rows into a list using GroupBy in pandas:
<b>Method 1: Group Rows into List for One Column</b>
<b>df.groupby('group_var')['values_var'].agg(list).reset_index(name='values_var')
</b>
<b>Method 2: Group Rows into List for Multiple Columns</b>
<b>df.groupby('team').agg(list)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'C'],   'points': [10, 10, 12, 15, 19, 23, 20, 20, 26],   'assists': [6, 8, 9, 11, 13, 8, 8, 15, 10]})
#view DataFrame
print(df)
  team  points  assists
0    A      10        6
1    A      10        8
2    A      12        9
3    A      15       11
4    B      19       13
5    B      23        8
6    C      20        8
7    C      20       15
8    C      26       10
</b>
<h3>Example 1: Group Rows into List for One Column</h3>
We can use the following syntax to group rows by the <b>team</b> column and product one list for the values in the <b>points</b> column:
<b>#group points values into list by team
df.groupby('team')['points'].agg(list).reset_index(name='points')
     teampoints
0A[10, 10, 12, 15]
1B[19, 23]
2C[20, 20, 26]</b>
We can see that a list of points values is produced for each unique team in the DataFrame.
<h3>Example 2: Group Rows into List for Multiple Columns</h3>
We can use the following syntax to group rows by the <b>team</b> column and product a list of values for both the <b>points</b> and <b>assists</b> columns:
<b>#group points and assists values into lists by team
df.groupby('team').agg(list)
points         assists
team
A[10, 10, 12, 15] [6, 8, 9, 11]
B[19, 23] [13, 8]
C[20, 20, 26] [8, 15, 10]</b>
We can see that a list of points values and a list of assists values are produced for each unique team in the DataFrame.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">Pandas: How to Calculate Mode in a GroupBy Object</span></h2>
You can use the following syntax to calculate the  mode  in a GroupBy object in pandas:
<b>df.groupby(['group_var'])['value_var'].agg(pd.Series.mode)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Mode in a GroupBy Object</h3>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'C'],   'points': [10, 10, 12, 15, 19, 23, 20, 20, 26]})
#view DataFrame
print(df)
  team  points
0    A      10
1    A      10
2    A      12
3    A      15
4    B      19
5    B      23
6    C      20
7    C      20
8    C      26</b>
We can use the following syntax to calculate the mode points value for each team:
<b>#calculate mode points value for each team
df.groupby(['team'])['points'].agg(pd.Series.mode)
team
A          10
B    [19, 23]
C          20
Name: points, dtype: object
</b>
Here’s how to interpret the output:
The mode points value for team A is <b>10</b>.
The mode points values for team B are <b>19</b> and <b>23</b>.
The mode points value for team C is <b>20</b>.
If one group happens to have multiple modes then you can use the following syntax to display each mode on a different row:
<b>#calculate mode points value for each team
df.groupby(['team'])['points'].apply(pd.Series.mode)
team   
A     0    10
B     0    19
      1    23
C     0    20
Name: points, dtype: int64</b>
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">Pandas: How to Use GroupBy on a MultiIndex</span></h2>
You can use the following basic syntax to use GroupBy on a pandas DataFrame with a multiindex:
<b>#calculate sum by level 0 and 1 of multiindex
df.groupby(level=[0,1]).sum()
#calculate count by level 0 and 1 of multiindex
df.groupby(level=[0,1]).count()
#calculate max value by level 0 and 1 of multiindex
df.groupby(level=[0,1]).max()
...</b>
Each of these examples calculate some metric grouped by two levels of a multiindex pandas DataFrame.
The following example shows how to use this syntax in practice.
<h3>Example: Use GroupBy on MultiIndex in pandas</h3>
Suppose we have the following pandas DataFrame with a multiindex:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'points': [6, 8, 9, 11, 13, 8, 8, 15]})
#define multiindex
df.set_index(['team', 'position'], inplace=True)
#view DataFrame
print(df)
               points
team position        
A    G              6
     G              8
     F              9
     F             11
B    G             13
     G              8
     F              8
     F             15
</b>
We can use the following syntax to calculate the sum of the points values grouped by both levels of the multiindex:
<b>#calculate sum of points grouped by both levels of the multiindex:
df.groupby(level=[0,1]).sum()
 points
teamposition
AF 20
        G        14
BF 23
        G        21
</b>
We can use similar syntax to calculate the max of the points values grouped by both levels of the multiindex:
<b>#calculate max of points grouped by both levels of the multiindex:
df.groupby(level=[0,1]).max()
 points
teamposition
AF 11
        G         8
BF 15
        G        13
</b>
We can use similar syntax to calculate any value we’d like grouped by several levels of a multiindex.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">Pandas: How to Use Groupby with Multiple Aggregations</span></h2>
You can use the following basic syntax to use a groupby with multiple aggregations in pandas:
<b>df.groupby('team').agg(
    mean_points=('points', np.mean),
    sum_points=('points', np.sum),
    std_points=('points', np.std))
</b>
This particular formula groups the rows of the DataFrame by the variable called <b>team </b>and then calculates several summary statistics for the variable called <b>points</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Using Groupby with Multiple Aggregations in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Mavs', 'Mavs', 'Heat', 'Heat', 'Heat'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
   team  points  assists
0  Mavs      18        5
1  Mavs      22        7
2  Mavs      19        7
3  Heat      14        9
4  Heat      14       12
5  Heat      11        9
</b>
We can use the following syntax to group the rows of the DataFrame by <b>team</b> and then calculate the mean, sum, and standard deviation of <b>points</b> for each team:
<b>import numpy as np
#group by team and calculate mean, sum, and standard deviation of points
df.groupby('team').agg(
    mean_points=('points', np.mean),
    sum_points=('points', np.sum),
    std_points=('points', np.std))
      mean_pointssum_pointsstd_points
team
Heat13.000000        39  1.732051
Mavs19.666667        59  2.081666</b>
The output displays the mean, sum, and standard deviation of the <b>points</b> variable for each <b>team</b>.
You can use similar syntax to perform a groupby and calculate as many aggregations as you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Count Unique Values Using Pandas GroupBy 
 How to Apply Function to Pandas Groupby 
 How to Create Bar Plot from Pandas GroupBy 
<h2><span class="orange">Pandas: How to Use GroupBy with nlargest()</span></h2>
You can use the following syntax to display the n largest values by group in a pandas DataFrame:
<b>#display two largest values by group
df.groupby('group_var')['values_var'].nlargest(2)
</b>
And you can use the following syntax to perform some operation (like taking the sum) on the n largest values by group in a pandas DataFrame:
<b>#find sum of two largest values by group
df.groupby('group_var')['values_var'].apply(lambda grp: grp.nlargest(2).sum())
</b>
The following examples shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'points': [12, 29, 34, 14, 10, 11, 7, 36, 34, 22]})
#view DataFrame
print(df)
  team  points
0    A      12
1    A      29
2    A      34
3    A      14
4    A      10
5    B      11
6    B       7
7    B      36
8    B      34
9    B      22
</b>
<h3>Example 1: Display N Largest Values by Group</h3>
We can use the following syntax to display the two largest <b>points</b> values grouped by <b>team</b>:
<b>#display two largest points values grouped by team
df.groupby('team')['points'].nlargest(2)
team   
A     2    34
      1    29
B     7    36
      8    34
Name: points, dtype: int64
</b>
The output shows the two largest <b>points</b> values for each <b>team</b>, along with their index positions in the original DataFrame.
<h3>Example 2: Perform Operation on N Largest Values by Group</h3>
We can use the following syntax to calculate the sum of the two largest <b>points</b> values grouped by <b>team</b>:
<b>#calculate sum of two largest points values for each team
df.groupby('team')['points'].apply(lambda grp: grp.nlargest(2).sum())
team
A    63
B    70
Name: points, dtype: int64
</b>
Here’s how to interpret the output:
The sum of the two largest points values for team A is <b>63</b>.
The sum of the two largest points values for team B is <b>70</b>.
We can use similar syntax to calculate the mean of the two largest <b>points</b> values grouped by <b>team</b>:
<b>#calculate  mean of two largest points values for each team
df.groupby('team')['points'].apply(lambda grp: grp.nlargest(2).mean())
team
A    31.5
B    35.0
Name: points, dtype: float64
</b>
Here’s how to interpret the output:
The mean of the two largest points values for team A is <b>31.5</b>.
The mean of the two largest points values for team B is <b>35.0</b>.
<b>Note</b>: You can find the complete documentation for the <b>GroupBy</b> function  here .
<h2><span class="orange">Pandas: How to Calculate Percentage of Total Within Group</span></h2>
You can use the following syntax to calculate the percentage of a total within groups in pandas:
<b>df['values_var'] / df.groupby('group_var')['values_var'].transform('sum')</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Percentage of Total Within Group</h3>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'points': [12, 29, 34, 14, 10, 11, 7, 36, 34, 22]})
#view DataFrame
print(df)
  team  points
0    A      12
1    A      29
2    A      34
3    A      14
4    A      10
5    B      11
6    B       7
7    B      36
8    B      34
9    B      22
</b>
We can use the following syntax to create a new column in the DataFrame that shows the percentage of total points scored, grouped by team:
<b>#calculate percentage of total points scored grouped by team
df['team_percent'] = df['points'] / df.groupby('team')['points'].transform('sum')
#view updated DataFrame
print(df)
  team  points  team_percent
0    A      12      0.121212
1    A      29      0.292929
2    A      34      0.343434
3    A      14      0.141414
4    A      10      0.101010
5    B      11      0.100000
6    B       7      0.063636
7    B      36      0.327273
8    B      34      0.309091
9    B      22      0.200000</b>
The <b>team_percent</b> column shows the percentage of total points scored by that player within their team.
For example, players on team A scored a total of <b>99</b> points.
Thus, the player in the first row of the DataFrame who scored <b>12</b> points scored a total of 12/99 = <b>12.12%</b> of the total points for team A.
Similarly, the player in the second row of the DataFrame who scored <b>29</b> points scored a total of 29/99 = <b>29.29%</b> of the total points for team A.
And so on.
<b>Note</b>: You can find the complete documentation for the <b>GroupBy</b> function  here .
<h2><span class="orange">Pandas: How to Use Groupby and Plot (With Examples)</span></h2>
You can use the following methods to perform a groupby and plot with a pandas DataFrame:
<b>Method 1: Group By & Plot Multiple Lines in One Plot</b>
<b>#define index column
df.set_index('day', inplace=True)
#group data by product and display sales as line chart
df.groupby('product')['sales'].plot(legend=True)
</b>
<b>Method 2: Group By & Plot Lines in Individual Subplots</b>
<b>pd.pivot_table(df.reset_index(),
               index='day', columns='product', values='sales'
              ).plot(subplots=True)</b>
The following example shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],   'product': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'sales': [4, 7, 8, 12, 15, 8, 11, 14, 19, 20]})
#view DataFrame
df
dayproductsales
01A4
12A7
23A8
34A12
45A15
51B8
62B11
73B14
84B19
95B20
</b>
<h3>Method 1: Group By & Plot Multiple Lines in One Plot</h3>
The following code shows how to group the DataFrame by the ‘product’ variable and plot the ‘sales’ of each product in one chart:
<b>#define index column
df.set_index('day', inplace=True)
#group data by product and display sales as line chart
df.groupby('product')['sales'].plot(legend=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/groupbyPlot1.png">
The x-axis displays the day, the y-axis displays the sales, and each individual line displays the sales of the individual products.
<h3>Method 2: Group By & Plot Lines in Individual Subplots</h3>
The following code shows how to group the DataFrame by the ‘product’ variable and plot the ‘sales’ of each product in individual subplots:
<b>pd.pivot_table(df.reset_index(),
               index='day', columns='product', values='sales'
              ).plot(subplots=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/groupbyPlot2.png">
The first plot shows the sales of product A and the second plot shows the sales of product B.
Note that we can also use the <b>layout</b> argument to specify the layout of the subplots.
For example, we could specify the subplots to be in a grid with one row and two columns:
<b>pd.pivot_table(df.reset_index(),
               index='day', columns='product', values='sales'
              ).plot(subplots=True, layout=(1,2))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/groupbyPlot3.png">
<h2><span class="orange">Pandas: How to Calculate Rank in a GroupBy Object</span></h2>
You can use the following syntax to calculate the rank of values in a GroupBy object in pandas:
<b>df['rank'] = df.groupby(['group_var'])['value_var'].rank()
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Rank in a GroupBy Object</h3>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'C', 'C', 'C'],   'points': [10, 10, 12, 15, 19, 23, 20, 20, 26]})
#view DataFrame
print(df)
  team  points
0    A      10
1    A      10
2    A      12
3    A      15
4    B      19
5    B      23
6    C      20
7    C      20
8    C      26</b>
We can use the following syntax to calculate the rank of the points values for each team:
<b>#add ranking column to data frame
df['points_rank'] = df.groupby(['team'])['points'].rank()
#view updated DataFrame
print(df)
  team  points  points_rank
0    A      10          1.5
1    A      10          1.5
2    A      12          3.0
3    A      15          4.0
4    B      19          1.0
5    B      23          2.0
6    C      20          1.5
7    C      20          1.5
8    C      26          3.0</b>
By default, the <b>rank()</b> function assigns ranking values in ascending order and uses the average rank when ties are present.
However, we can use the <b>method</b> and <b>ascending</b> arguments to rank the values in a different manner:
<b>#add ranking column to data frame
df['points_rank'] = df.groupby(['team'])['points'].rank('dense', ascending=False)
#view updated DataFrame
print(df)
  team  points  points_rank
0    A      10          3.0
1    A      10          3.0
2    A      12          2.0
3    A      15          1.0
4    B      19          2.0
5    B      23          1.0
6    C      20          2.0
7    C      20          2.0
8    C      26          1.0
</b>
This method assigns a value of 1 to the largest value in each group.
You can find a complete list of ranking methods you can use with the <b>rank()</b> function  here .
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">Pandas: How to Rename Columns in Groupby Function</span></h2>
You can use the following basic syntax to rename columns in a <b>groupby()</b> function in pandas:
<b>df.groupby('group_col').agg(sum_col1=('col1', 'sum'),            mean_col2=('col2', 'mean'),            max_col3=('col3', 'max'))
</b>
This particular example calculates three aggregated columns and names them <b>sum_col1</b>, <b>mean_col2</b>, and <b>max_col3</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Rename Columns in Groupby Function in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [30, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 6, 6, 5, 8, 7, 7, 9],   'rebounds': [4, 13, 15, 10, 7, 7, 5, 11]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      30        5         4
1    A      22        6        13
2    A      19        6        15
3    A      14        5        10
4    B      14        8         7
5    B      11        7         7
6    B      20        7         5
7    B      28        9        11
</b>
We can use the following syntax to group the rows by the <b>team</b> column, then calculate three aggregated columns while providing specific names to the aggregated columns:
<b>#calculate several aggregated columns by group and rename aggregated columns
df.groupby('team').agg(sum_points=('points', 'sum'),       mean_assists=('assists', 'mean'),       max_rebounds=('rebounds', 'max'))
sum_pointsmean_assistsmax_rebounds
team
A        85        5.50          15
B        73        7.75          11
</b>
Notice that the three aggregated columns have the custom names that we provided in the <b>agg()</b> function.
Also note that we could use NumPy functions to calculate the sum, mean, and max values within the <b>agg()</b> function if we’d like.
<b>import numpy as np
#calculate several aggregated columns by group and rename aggregated columns
df.groupby('team').agg(sum_points=('points', np.sum),       mean_assists=('assists', np.mean),       max_rebounds=('rebounds', np.max))
sum_pointsmean_assistsmax_rebounds
team
A        85        5.50          15
B        73        7.75          11</b>
These results match the ones from the previous example.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to List All Column Names in Pandas 
 How to Sort Columns by Name in Pandas 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">Pandas: How to Use GroupBy & Sort Within Groups</span></h2>
You can use the following syntax to group rows in a pandas DataFrame and then sort the values within groups:
<b>df.sort_values(['var1','var2'],ascending=False).groupby('var1').head()
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Use GroupBy & Sort Within Groups in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the sales made at two different store locations:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['B', 'B', 'A', 'A', 'B', 'B', 'A', 'A'],   'sales': [12, 25, 8, 14, 10, 20, 30, 30]})
#view DataFrame
print(df)
  store  sales
0     B     12
1     B     25
2     A      8
3     A     14
4     B     10
5     B     20
6     A     30
7     A     30
</b>
We can use the following syntax to group the rows by the <b>store</b> column and sort in descending order based on the <b>sales</b> column:
<b>#group by store and sort by sales values in descending order
df.sort_values(['store','sales'],ascending=False).groupby('store').head()
storesales
1B25
5B20
0B12
4B10
6A30
7A30
3A14
2A8</b>
Note that we could also drop the <b>ascending=False</b> argument to sort the sales values in ascending order:
<b>#group by store and sort by sales values in ascending order
df.sort_values(['store','sales']).groupby('store').head()
storesales
2A8
3A14
6A30
7A30
4B10
0B12
5B20
1B25</b>
Note that the <b>head()</b> function only displays the first 5 values by group.
To display the top n values by group, simply use <b>head(n)</b> instead.
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">How to Calculate Standard Deviation by Group in Pandas</span></h2>
You can use the following methods to calculate the standard deviation by group in pandas:
<b>Method 1: Calculate Standard Deviation of One Column Grouped by One Column</b>
<b>df.groupby(['group_col'])['value_col'].std()
</b>
<b>Method 2: Calculate Standard Deviation of Multiple Columns Grouped by One Column</b>
<b>df.groupby(['group_col'])['value_col1', 'value_col2'].std()</b>
<b>Method 3: Calculate Standard Deviation of One Column Grouped by Multiple Columns</b>
<b>df.groupby(['group_col1', 'group_col2'])['value_col'].std()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'F', 'F', 'G', 'F', 'F', 'G', 'G'],   'points': [30, 22, 19, 14, 14, 11, 20, 28],   'assists': [4, 3, 7, 7, 12, 15, 8, 4]})
#view DataFrame
print(df)
  team position  points  assists
0    A        G      30        4
1    A        F      22        3
2    A        F      19        7
3    A        G      14        7
4    B        F      14       12
5    B        F      11       15
6    B        G      20        8
7    B        G      28        4
</b>
<h2>Example 1: Calculate Standard Deviation of One Column Grouped by One Column</h2>
The following code shows how to calculate the standard deviation of the <b>points</b> column, grouped by the <b>team</b> column:
<b>#calculate standard deviation of points grouped by team
df.groupby('team')['points'].std()
team
A    6.70199
B    7.50000
Name: points, dtype: float64
</b>
From the output we can see:
The standard deviation of points for team A is <b>6.70199</b>.
The standard deviation of points for team B is <b>7.5</b>.
<h2>
<b>Example 2: Calculate Standard Deviation of Multiple Columns Grouped by One Column</b>
</h2>
The following code shows how to calculate the standard deviation of the <b>points</b> column and the standard deviation of the <b>assists</b> column, grouped by the <b>team</b> column:
<b>#calculate standard deviation of points and assists grouped by team
df.groupby('team')[['points', 'assists']].std()
pointsassists
team
A6.701992.061553
B7.500004.787136
</b>
The output displays the standard deviation of the <b>points</b> column and the <b>assists</b> column for each team.
<h2>Example 3: Calculate Standard Deviation of One Column Grouped by Multiple Columns</h2>
The following code shows how to calculate the standard deviation of the <b>points</b> column, grouped by the <b>team</b> and <b>position</b> columns:
<b>#calculate standard deviation of points, grouped by team and position
df.groupby(['team', 'position'])['points'].std()
team  position
A     F            2.121320
      G           11.313708
B     F            2.121320
      G            5.656854
Name: points, dtype: float64
</b>
From the output we can see:
The standard deviation of points for players on team A and position F is <b>2.12</b>.
The standard deviation of points for players on team A and position G is <b>11.31</b>.
The standard deviation of points for players on team B and position F is <b>2.12</b>.
The standard deviation of points for players on team B and position G is <b>5.65</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Calculate Mean by Group in Pandas 
 How to Calculate Max Value by Group in Pandas 
 How to Calculate Sum by Group in Pandas 
 How to Calculate Quantiles by Group in Pandas 
<h2><span class="orange">How to Perform a GroupBy Sum in Pandas (With Examples)</span></h2>
You can use the following basic syntax to find the sum of values by group in pandas:
<b>df.groupby(['group1','group2'])['sum_col'].sum().reset_index()
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'G', 'F', 'F', 'C'],   'points': [25, 17, 14, 9, 12, 9, 6, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition pointsrebounds
0AG 2511
1AG 178
2AF 1410
3AC 96
4BG 126
5BF 95
6BF 69
7BC 412</b>
<h3>Example 1: Group by One Column, Sum One Column</h3>
The following code shows how to group by one column and sum the values in one column:
<b>#group by team and sum the points
df.groupby(['team'])['points'].sum().reset_index()
teampoints
0A65
1B31
</b>
From the output we can see that:
The players on team A scored a sum of <b>65</b> points.
The players on team B scored a sum of <b>31</b> points.
<h3>Example 2: Group by Multiple Columns, Sum Multiple Columns</h3>
The following code shows how to group by multiple columns and sum multiple columns:
<b>#group by team and position, sum points and rebounds
df.groupby(['team', 'position'])['points', 'rebounds'].sum().reset_index()
        teamposition pointsrebounds
0AC 96
1AF 1410
2AG 4219
3BC 412
4BF 1514
5BG 126
</b>
From the output we can see that:
The players on team A in the ‘C’ position scored a sum of <b>9 </b>points and <b>6</b> rebounds.
The players on team A in the ‘F’ position scored a sum of <b>14 </b>points and <b>10</b> rebounds.
The players on team A in the ‘G’ position scored a sum of <b>42 </b>points and <b>19 </b>rebounds.
And so on.
Note that the <b>reset_index()</b> function prevents the grouping columns from becoming part of the index. 
For example, here’s what the output looks like if we don’t use it:
<b>#group by team and position, sum points and rebounds
df.groupby(['team', 'position'])['points', 'rebounds'].sum()
 pointsrebounds
teamposition
AC 96
F14 10
G42 19
BC 412
F15 14
G12 6</b>
Depending on how you’d like the results to appear, you may or may not choose to use the <b>reset_index()</b> function.
<h2><span class="orange">How to Convert Pandas GroupBy Output to DataFrame</span></h2>
This tutorial explains how to convert the output of a pandas GroupBy into a pandas DataFrame.
<h3>Example: Convert Pandas GroupBy Output to DataFrame</h3>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'G', 'F', 'F', 'F'],   'points': [5, 7, 7, 10, 12, 22, 15, 10]})
#view DataFrame
print(df)
  team position  points
0    A        G       5
1    A        G       7
2    A        F       7
3    A        C      10
4    B        G      12
5    B        F      22
6    B        F      15
7    B        F      10
</b>
We can use the following syntax to count the number of players, grouped by <b>team</b> and <b>position</b>:
<b>#count number of players, grouped by team and position
group = df.groupby(['team', 'position']).size()
#view output
print(group)
team  position
A     C           1
      F           1
      G           2
B     F           3
      G           1
dtype: int64
</b>
From the output, we can see the total count of players, grouped by <b>team</b> and <b>position</b>.
However, suppose we want our output to display the team name in each row like this:
<b>  team position  count
0    A        C      1
1    A        F      1
2    A        G      2
3    B        F      3
4    B        G      1
</b>
To achieve this output, we can simply use <b>reset_index()</b> when performing the GroupBy:
<b>#count number of players, grouped by team and position
df_out = df.groupby(['team', 'position']).size().reset_index(name='count')
#view output
print(df_out)
  team position  count
0    A        C      1
1    A        F      1
2    A        G      2
3    B        F      3
4    B        G      1
</b>
The output now appears in the format that we wanted.
Note that the <b>name</b> argument within <b>reset_index()</b> specifies the name for the new column produced by GroupBy.
We can also confirm that the result is indeed a pandas DataFrame:
<b>#display object type of df_out
type(df_out)
pandas.core.frame.DataFrame
</b>
<b>Note</b>: You can find the complete documentation for the GroupBy operation in pandas  here .
<h2><span class="orange">How to Use groupby() and transform() Functions in Pandas</span></h2>
You can use the following methods to use the <b>groupby()</b> and <b>transform()</b> functions together in a pandas DataFrame:
<b>Method 1: Use groupby() and transform() with built-in function</b>
<b>df['new'] = df.groupby('group_var')['value_var'].transform('mean')
</b>
<b>Method 2: Use groupby() and transform() with custom function</b>
<b>df['new'] = df.groupby('group_var')['value_var'].transform(lambda x: some function)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [30, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team  points
0    A      30
1    A      22
2    A      19
3    A      14
4    B      14
5    B      11
6    B      20
7    B      28
</b>
<h3>Example 1: Use groupby() and transform() with built-in function</h3>
The following code shows how to use the <b>groupby(</b>) and <b>transform()</b> functions to add a new column to the DataFrame called mean_points:
<b>#create new column called mean_points
df['mean_points'] = df.groupby('team')['points'].transform('mean')
#view updated DataFrame
print(df)
  team  points  mean_points
0    A      30        21.25
1    A      22        21.25
2    A      19        21.25
3    A      14        21.25
4    B      14        18.25
5    B      11        18.25
6    B      20        18.25
7    B      28        18.25</b>
The mean points value for players on team A was <b>21.25</b> and the mean points value for players on team B was <b>18.25</b>, so these values were assigned accordingly to each player in a new column.
Note that we could also use another built-in function such as <b>sum()</b> to create a new column that shows the sum of points scored for each team:
<b>#create new column called sum_points
df['sum_points'] = df.groupby('team')['points'].transform('sum')
#view updated DataFrame
print(df)
  team  points  sum_points
0    A      30          85
1    A      22          85
2    A      19          85
3    A      14          85
4    B      14          73
5    B      11          73
6    B      20          73
7    B      28          73</b>
The sum of points for players on team A was <b>85 </b>and the sum of points for players on team B was <b>73</b>, so these values were assigned accordingly to each player in a new column.
<h3>Example 2: Use groupby() and transform() with custom function</h3>
The following code shows how to use the <b>groupby(</b>) and <b>transform()</b> functions to create a custom function that calculates the percentage of total points scored by each player on their respective teams:
<b>#create new column called percent_of_points
df['percent_of_points'] = df.groupby('team')['points'].transform(lambda x: x/x.sum())
#view updated DataFrame
print(df)
  team  points  percent_of_points
0    A      30           0.352941
1    A      22           0.258824
2    A      19           0.223529
3    A      14           0.164706
4    B      14           0.191781
5    B      11           0.150685
6    B      20           0.273973
7    B      28           0.383562
</b>
Here’s how to interpret the output:
The first player on team A scored 30 out of 85 total points among team A players. Thus, his percentage of total points scored was 30/85 = <b>0.352941</b>.
The second player on team A scored 22 out of 85 total points among team A players. Thus, his percentage of total points scored was 22/85 = <b>0.258824</b>.
And so on.
Note that we can use the <b>lambda</b> argument within the <b>transform()</b> function to perform any custom calculation that we’d like.
<h2><span class="orange">Pandas: How to Use GroupBy and Value Counts</span></h2>
You can use the following basic syntax to count the frequency of unique values by group in a pandas DataFrame:
<b>df.groupby(['column1', 'column2']).size().unstack(fill_value=0)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Use GroupBy and Value Counts in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'position':['G', 'G', 'F', 'F', 'C', 'G', 'F', 'F', 'F', 'F'],   'points': [8, 8, 10, 10, 11, 8, 9, 10, 10, 10]})
#view DataFrame
print(df)
  team position  points
0    A        G       8
1    A        G       8
2    A        F      10
3    A        F      10
4    A        C      11
5    B        G       8
6    B        F       9
7    B        F      10
8    B        F      10
9    B        F      10
</b>
We can use the following syntax to count the frequency of the <b>points</b> values, grouped by the <b>team</b> and <b>position</b> columns:
<b>#count frequency of points values, grouped by team and position
df.groupby(['team', 'position', 'points']).size().unstack(fill_value=0)
points891011
teamposition
AC0001
        F0020
        G2000
BF0130
        G1000</b>
Here’s how to interpret the output:
The value 8 occurred in the points column <b>0</b> times for players on team A and position C.
The value 9 occurred in the points column <b>0</b> times for players on team A and position C.
The value 10 occurred in the points column <b>0</b> times for players on team A and position C.
The value 11 occurred in the points column <b>1</b> time for players on team A and position C.
And so on.
We could also use the following syntax to count the frequency of the <b>positions</b>, grouped by <b>team</b>:
<b>#count frequency of positions, grouped by team
df.groupby(['team', 'position']).size().unstack(fill_value=0)
positionCFG
team
A        122
B        041
</b>
Here’s how to interpret the output:
The value ‘C’ occurred <b>1</b> time on team A.
The value ‘F’ occurred <b>2</b> times on team A.
The value ‘G’ occurred <b>2</b> times on team A.
The value ‘C’ occurred <b>0</b> times on team B.
The value ‘F’ occurred <b>4</b> times on team B.
The value ‘G’ occurred <b>1</b> time on team B.
And so on.
<h2><span class="orange">How to Use Pandas head() Function (With Examples)</span></h2>
You can use the <b>head()</b> function to view the first <em>n</em> rows of a pandas DataFrame.
This function uses the following basic syntax:
<b>df.head()
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412
</b>
<h3>Example 1: View First 5 Rows of DataFrame</h3>
By default, the <b>head()</b> function displays the first five rows of a DataFrame:
<b>#view first five rows of DataFrame
df.head()
pointsassistsrebounds
025511
11278
215710
31496
419126</b>
<h3>Example 2: View First <em>n</em> Rows of DataFrame</h3>
We can use the <b>n</b> argument to view the first <em>n</em> rows of a pandas DataFrame:
<b>#view first three rows of DataFrame
df.head(n=3)
        pointsassistsrebounds
025511
11278
215710
</b>
<h3>Example 3: View First <em>n</em> Rows of Specific Column</h3>
The following code shows how to view the first five rows of a specific column in a DataFrame:
<b>#view first five rows of values in 'points' column
df['points'].head()
0    25
1    12
2    15
3    14
4    19
Name: points, dtype: int64
</b>
<h3>Example 4: View First <em>n</em> Rows of Several Columns</h3>
The following code shows how to view the first five rows of several specific columns in a DataFrame:
<b>#view first five rows of values in 'points' and 'assists' columns
df[['points', 'assists']].head()
pointsassists
0255
1127
2157
3149
41912
</b>
<h2><span class="orange">How to Plot Histograms by Group in Pandas</span></h2>
You can use the following methods to plot histograms by group in a pandas DataFrame:
<b>Method 1: Plot Histograms by Group Using Multiple Plots</b>
<b>df['values_var'].hist(by=df['group_var'])
</b>
<b>Method 2: Plot Histograms by Group Using One Plot</b>
<b>plt.hist(group1, alpha=0.5, label='group1')</b>
<b>plt.hist(group2, alpha=0.5, label='group2')</b>
<b>plt.hist(group3, alpha=0.5, label='group3')</b>
The following examples show how to use each method in practice with the following pandas DataFrame that shows the points scored by basketball players on three different teams:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(1)
#create DataFrame
df = pd.DataFrame({'team': np.repeat(['A', 'B', 'C'], 100),   'points': np.random.normal(loc=20, scale=2, size=300)})
#view head of DataFrame
print(df.head())
  team     points
0    A  23.248691
1    A  18.776487
2    A  18.943656
3    A  17.854063
4    A  21.730815    </b>
<h2>Example 1: Plot Histograms by Group Using Multiple Plots</h2>
The following code shows how to create three histograms that display the distribution of points scored by players on each of the three teams:
<b>#create histograms of points by team
df['points'].hist(by=df['team'])
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histgroup1.jpg"525">
We can also use the <b>edgecolor</b> argument to add edge lines to each histogram and the <b>figsize</b> argument to increase the size of each histogram to make them easier to view:
<b>#create histograms of points by team
df['points'].hist(by=df['team'], edgecolor='black', figsize = (8,6)) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histgroup2.jpg"603">
<h2>Example 2: Plot Histograms by Group Using One Plot</h2>
The following code shows how to create three histograms and place them all on the same plot:
<b>import matplotlib.pyplot as plt
#define points values by group
A = df.loc[df['team'] == 'A', 'points']
B = df.loc[df['team'] == 'B', 'points']
C = df.loc[df['team'] == 'C', 'points']
#add three histograms to one plot
plt.hist(A, alpha=0.5, label='A')
plt.hist(B, alpha=0.5, label='B')
plt.hist(C, alpha=0.5, label='C')
#add plot title and axis labels
plt.title('Points Distribution by Team')
plt.xlabel('Points')
plt.ylabel('Frequency')
#add legend
plt.legend(title='Team')
#display plot
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histgroup3.jpg"504">
The end result is one plot that displays three overlaid histograms.
<b>Note</b>: The <b>alpha</b> argument specifies the transparency of each histogram. This value can range from 0 to 1. By setting this value equal to 0.5, we’re able to better view each overlaid histogram.
<h2>Additional Resources</h2>
The following tutorials explain how to create other common plots in Python:
 How to Plot Multiple Lines in Matplotlib 
 How to Create Boxplot from Pandas DataFrame 
 How to Plot Multiple Pandas Columns on Bar Chart 
<h2><span class="orange">Pandas: How to Create a Histogram with Log Scale</span></h2>
You can use the <b>logx</b> and <b>logy</b> arguments to create histograms with log scales on the x-axis and y-axis, respectively, in pandas:
<b>#create histogram with log scale on x-axis
df['my_column'].plot(kind='hist', logx=True)
#create histogram with log scale on y-axis
df['my_column'].plot(kind='hist', logy=True)
</b>
The following example shows how to use these arguments to create histograms with log scales in pandas.
<b>Related:</b>  When Should You Use a Log Scale in Charts? 
<h2>Example: Create Histogram with Log Scale in Pandas</h2>
Suppose we have the following pandas DataFrame with 5,000 rows:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(1)
#create DataFrame
df = pd.DataFrame({'values': np.random.lognormal(size=5000)})
#view first five rows of DataFrame
print(df.head())
     values
0  5.075096
1  0.542397
2  0.589682
3  0.341992
4  2.375974</b>
We can use the following syntax to create a histogram with a linear scale on both the x-axis and y-axis:
<b>#create histogram
df['values'].plot(kind='hist')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/logscale1.jpg"544">
The x-axis and y-axis both currently have a linear scale.
We can use the <b>logx=True</b> argument to convert the x-axis to a log scale:
<b>#create histogram with log scale on x-axis
df['values'].plot(kind='hist', logx=True)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/logscale2.jpg">
The values on the x-axis now follow a log scale.
And we can use the <b>logy=True</b> argument to convert the y-axis to a log scale:
<b>#create histogram with log scale on y-axis
df['values'].plot(kind='hist', logy=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/logscale3.jpg">
The values on the y-axis now follow a log scale.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Create a Histogram from Pandas DataFrame 
 How to Create a Histogram from a Pandas Series 
 How to Plot Histograms by Group in Pandas 
<h2><span class="orange">How to Modify the X-Axis Range in Pandas Histogram</span></h2>
You can use the <b>range</b> argument to modify the x-axis range in a pandas histogram:
<b>plt.hist(df['var1'], range=[10, 30])
</b>
In this particular example, we set the x-axis to range from 10 to 30.
The following example shows how to use the <b>range</b> argument in practice.
<h2>Example: Modifying the X-Axis Range in Pandas Histogram</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(1)
#create DataFrame
df = pd.DataFrame({'team': np.repeat(['A', 'B', 'C'], 100),   'points': np.random.normal(loc=20, scale=2, size=300)})
#view head of DataFrame
print(df.head())
  team     points
0    A  23.248691
1    A  18.776487
2    A  18.943656
3    A  17.854063
4    A  21.730815
</b>
If we create a histogram for the <b>points</b> variable, pandas will automatically choose the range for the x-axis based on the minimum and maximum values of the <b>points</b> variable:
<b>import matplotlib.pyplot as plt
#create histogram for points variable
plt.hist(df['points'], edgecolor='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histx1.jpg"509">
The x-axis ranges from 14 to 25.
We can use the  describe()  function to view the minimum and maximum values for the <b>points</b> variable:
<b>#summarize distribution of points variable
df['points'].describe()
count    300.000000
mean      20.148800
std        1.890841
min       14.413830
25%       18.818254
50%       20.176352
75%       21.372843
max       25.056651
Name: points, dtype: float64
</b>
We can see that the minimum value is 14.41 and the maximum value is 25.06, which explains why the x-axis in the plot currently ranges from 14 to 25.
However, we can use the <b>range</b> argument to force the x-axis to range from 10 to 30 instead:
<b>import matplotlib.pyplot as plt
#create histogram for points variable with custom x-axis range
plt.hist(df['points'], edgecolor='black', range=[10, 30])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/range1.jpg"527">
Notice that the x-axis now ranges from 10 to 30.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Create a Histogram from Pandas DataFrame 
 How to Create a Histogram from a Pandas Series 
 How to Plot Histograms by Group in Pandas 
<h2><span class="orange">How to Create a Histogram from Pandas DataFrame</span></h2>
You can use the following basic syntax to create a histogram from a pandas DataFrame:
<b>df.hist(column='col_name')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Plot a Single Histogram</h3>
The following code shows how to create a single histogram for a particular column in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29, 29, 31, 31, 33],   'assists': [5, 7, 7, 9, 12, 9, 9, 4, 7, 7, 8, 9],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 10, 7, 7, 9]})
#view first five rows of DataFrame
df.head()
pointsassistsrebounds
025511
11278
215710
31496
419126
#create histogram for 'points' column
df.hist(column='points')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/histpandas1.png">
We can also customize the histogram with specific colors, styles, labels, and number of bins:
<b>#create custom histogram for 'points' column
df.hist(column='points', bins=5, grid=False, rwidth=.9, color='purple')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/histpandas2.png">
The x-axis displays the points scored per player and the y-axis shows the frequency for the number of players who scored that many points.
<h3>Example 2: Plot Multiple Histograms</h3>
The following code shows how to plot multiple histograms from a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team':['A', 'A', 'A', 'A', 'A', 'A',           'B', 'B', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29, 29, 31, 31, 33]})
#view first five rows
df.head()
        teampoints
0A25
1A12
2A15
3A14
4A19
#create histogram for each team
df.hist(column='points', by='team', bins=3, grid=False, rwidth=.9,
        color='purple', sharex=True)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/histpandas3-1.png">
Note that the <b>sharex</b> argument specifies that the two histograms should share the same x-axis.
This makes it easier to compare the distribution of values between the two histograms.
<h2><span class="orange">Pandas: Formula for “If Value in Column Then”</span></h2>
You can use the following syntax in pandas to assign values to one column based on the values in another column:
<b>df['new'] = df['col'].map(lambda x: 'new1' if 'A' in x else 'new2' if 'B' in x else '')
</b>
This particular syntax will create a new column called “new” that takes on the following values:
<b>new1</b> if the value in <b>col</b> is equal to A.
<b>new2</b> if the value in <b>col</b> is equal to B.
An empty string if the value in <b>col</b> is equal to any other value.
The following example shows how to use this syntax in practice.
<h2>Example: Using a Formula for “If Value in Column Then” in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'C', 'C'],   'points': [14, 22, 25, 34, 30, 12, 10, 18]})
#view DataFrame
print(df)
  team  points
0    A      14
1    A      22
2    A      25
3    A      34
4    B      30
5    B      12
6    C      10
7    C      18
</b>
Now suppose that we would like to create a new column called <b>city</b> whose values depend on the corresponding value in the <b>team</b> column.
We can use the following syntax to do so:
<b>#create new column called city whose values depend on values in team column
df['city'] = df['team'].map(lambda x: 'Atlanta' if 'A' in x else 'Boston' if 'B' in x else '')
#view updated DataFrame                            
print(df)
  team  points     city
0    A      14  Atlanta
1    A      22  Atlanta
2    A      25  Atlanta
3    A      34  Atlanta
4    B      30   Boston
5    B      12   Boston
6    C      10         
7    C      18       
</b>
This particular syntax created a new column called <b>city</b> that takes on the following values:
<b>Atlanta </b>if the value in <b>team </b>is equal to A.
<b>Boston</b> if the value in <b>team </b>is equal to B.
An empty string if the value in <b>team </b>is equal to any other value.
Note that in this example we used an empty string after the last <b>else</b> statement to simply leave values blank that didn’t meet any condition.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: Get Index of Rows Whose Column Matches Value 
 Pandas: How to Select Columns Containing a Specific String 
 Pandas: How to Check if Column Contains String 
<h2><span class="orange">How to Impute Missing Values in Pandas (Including Example)</span></h2>
You can use the following basic syntax to impute missing values in a pandas DataFrame:
<b>df['column_name'] = df['column_name'].interpolate()
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Interpolate Missing Values in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the total sales made by a store during 15 consecutive days:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],   'sales': [3, 6, 8, 10, 14, 17, 20, np.nan, np.nan, np.nan,             np.nan, 35, 39, 44, 49]})
#view DataFrame
print(df)
    day  sales
0     1    3.0
1     2    6.0
2     3    8.0
3     4   10.0
4     5   14.0
5     6   17.0
6     7   20.0
7     8    NaN
8     9    NaN
9    10    NaN
10   11    NaN
11   12   35.0
12   13   39.0
13   14   44.0
14   15   49.0</b>
Notice that we’re missing sales numbers for four days in the data frame.
If we create a simple line chart to visualize the sales over time, here’s what it would look like:
<b>#create line chart to visualize sales
df['sales'].plot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/imputePandas1.png">
To fill in the missing values, we can use the <b>interpolate()</b> function as follows:
<b>#interpolate missing values in 'sales' column
df['sales'] = df['sales'].interpolate()
#view DataFrame
print(df)
    day  sales
0     1    3.0
1     2    6.0
2     3    8.0
3     4   10.0
4     5   14.0
5     6   17.0
6     7   20.0
7     8   23.0
8     9   26.0
9    10   29.0
10   11   32.0
11   12   35.0
12   13   39.0
13   14   44.0
14   15   49.0</b>
Notice that each of the missing values has been replaced.
If we create another line chart to visualize the updated data frame, here’s what it would look like:
<b>#create line chart to visualize sales
df['sales'].plot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/imputePandas2.png">
Notice that the values chosen by the <b>interpolate()</b> function seem to fit the trend in the data quite well.
<b>Note</b>: You can find the complete documentation for the <b>interpolate()</b> function  here .
<h2><span class="orange">How to Convert Pandas Index to a List (With Examples)</span></h2>
You can use one of the following two methods to convert the index of a pandas DataFrame to a list:
<b>Method 1: Use list()</b>
<b>index_list = list(df.index.values)
</b>
<b>Method 2: Use tolist()</b>
<b>index_list = df.index.values.tolist()</b>
Both methods will return the exact same result, but the second method will tend to be faster on extremely large DataFrames.
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A18511
1B2278
2C19710
3D1496
4E14126
5F1195
6G2099
7H28412
</b>
<h3>Method 1: Use list()</h3>
The following code shows how to use <b>list()</b> to quickly convert the index of the pandas DataFrame to a list:
<b>#convert index to list
index_list = list(df.index.values)
#view list
index_list
[0, 1, 2, 3, 4, 5, 6, 7]
</b>
Notice that the list contains the original values in the index of the DataFrame.
We can also use <b>type()</b> to verify that the result is a list:
<b>#check object type
type(index_list)
list</b>
<h3>Method 2: Use tolist()</h3>
The following code shows how to use <b>list()</b> to quickly convert the index of the pandas DataFrame to a list:
<b>#convert index to list
index_list = df.index.values.tolist()
#view list
index_list
[0, 1, 2, 3, 4, 5, 6, 7]
</b>
Once again, the list contains the original values in the index of the DataFrame.
We can also use <b>type()</b> to verify that the result is a list:
<b>#check object type
type(index_list)
list</b>
<h2><span class="orange">Pandas: How to Insert Row at Specific Index Position</span></h2>
You can use the following basic syntax to insert a row into a a specific index position in a pandas DataFrame:
<b>#insert row in between index position 2 and 3
df.loc[2.5] = value1, value2, value3, value4
#sort index
df = df.sort_index().reset_index(drop=True)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Insert Row at Specific Index Position in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use the following syntax to insert a row in between index position 2 and 3:
<b>#insert row in between index position 2 and 3
df.loc[2.5] = 'Z', 10, 5, 7
#sort index
df = df.sort_index().reset_index(drop=True)
#view updated DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    Z      10        5         7
4    D      14        9         6
5    E      14       12         6
6    F      11        9         5
7    G      20        9         9
8    H      28        4        12
</b>
Notice that a row has been inserted in between the previous index position 2 and 3 with the following information:
team: Z
points: 10
assists: 5
rebounds: 7
By using the <b>sort_index()</b> and <b>reset_index()</b> functions, we were then able to reassign values to the index ranging from 0 to 8.
Note that the new row must contain the same number of values as the number of existing columns.
For example, if we attempted to insert a new row with only three values, we would receive an error:
<b>#attempt to insert row with only three values
df.loc[2.5] = 10, 5, 7
ValueError: cannot set a row with mismatched columns
</b>
We receive a  ValueError  because the number of values in the new row does not match the number of existing columns in the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Insert a Column Into a Pandas DataFrame 
 How to Add Rows to a Pandas DataFrame 
 How to Drop Rows in Pandas DataFrame Based on Condition 
<h2><span class="orange">How to Insert a Row Into a Pandas DataFrame</span></h2>
You can use the following basic syntax to insert a row into a a specific location in a pandas DataFrame:
<b>import pandas as pd
import numpy as np
#insert row with values [1, 7, 6] into existing DataFrame at index=4
pd.DataFrame(np.insert(df.values, 4, values=[1, 7, 6], axis=0))
</b>
The following example shows how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'C'],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
        teamassistsrebounds
0A511
1A78
2B710
3B96
4C126
</b>
<h3>Example 1: Insert Values Into First Row of Pandas DataFrame</h3>
We can use the following syntax to insert a row of values into the first row of a pandas DataFrame:
<b>#insert values into first row of DataFrame
df2 = pd.DataFrame(np.insert(df.values, 0, values=['A', 3, 4], axis=0))
#define column names of DataFrame
df2.columns = df.columns
#view updated DataFrame
df2
teamassistsrebounds
0A34
1A511
2A78
3B710
4B96
5C126</b>
<h3>Example 2: Insert Values Into Specific Row of Pandas DataFrame</h3>
We can use the following syntax to insert a row of values into a specific row of a pandas DataFrame:
<b>#insert values into third row (index position=2) of DataFrame
df2 = pd.DataFrame(np.insert(df.values, 2, values=['A', 3, 4], axis=0))
#define column names of DataFrame
df2.columns = df.columns
#view updated DataFrame
df2
        teamassistsrebounds
0A511
1A78
2A34
3B710
4B96
5C126</b>
<h3>Example 3: Insert Values Into Last Row of Pandas DataFrame</h3>
We can use the following syntax to insert a row of values into the last row of a pandas DataFrame:
<b>#insert values into last row of DataFrame
df2 = pd.DataFrame(np.insert(df.values, len(df.index), values=['A', 3, 4], axis=0))
#define column names of DataFrame
df2.columns = df.columns
#view updated DataFrame
df2
teamassistsrebounds
0A511
1A78
2B710
3B96
4C126
5A34</b>
<b>Note</b>: You can find the complete documentation for the NumPy <b>insert()</b> function  here .
<h2><span class="orange">How to Find the Intersection Between Series in Pandas</span></h2>
You can use the following basic syntax to find the intersection between two Series in pandas:
<b>set(series1) & set(series2)
</b>
Recall that the  intersection  of two sets is simply the set of values that are in <em>both</em> sets.
The following examples show how to calculate the intersection between pandas Series in practice.
<h3>Example 1: Calculate Intersection Between Two Pandas Series</h3>
The following code shows how to calculate the intersection between two pandas Series:
<b>import pandas as pd
#create two Series
series1 = pd.Series([4, 5, 5, 7, 10, 11, 13])
series2 = pd.Series([4, 5, 6, 8, 10, 12, 15])
#find intersection between the two series
set(series1) & set(series2)
{4, 5, 10}
</b>
The result is a set that contains the values <b>4</b>, <b>5</b>, and <b>10</b>.
These are the only three values that are in both the first and second Series.
Also note that this syntax works with pandas Series that contain strings:
<b>import pandas as pd
#create two Series
series1 = pd.Series(['A', 'B', 'C', 'D', 'E'])
series2 = pd.Series(['A', 'B', 'B', 'B', 'F'])
#find intersection between the two series
set(series1) & set(series2)
{'A', 'B'}</b>
The only strings that are in both the first and second Series are <b>A</b> and <b>B</b>.
<h3>Example 2: Calculate Intersection Between Three Pandas Series</h3>
The following code shows how to calculate the intersection between three pandas Series:
<b>import pandas as pd
#create three Series
series1 = pd.Series([4, 5, 5, 7, 10, 11, 13])
series2 = pd.Series([4, 5, 6, 8, 10, 12, 15])
series3 = pd.Series([3, 5, 6, 8, 10, 18, 21])
#find intersection between the three series
set(series1) & set(series2) & set(series3)
{5, 10}
</b>
The result is a set that contains the values <b>5 </b>and <b>10</b>.
These are the only values that are in all three Series.
<h2><span class="orange">How to Use “Is Not Null” in Pandas (With Examples)</span></h2>
You can use the pandas <b>notnull()</b> function to test whether or not elements in a pandas DataFrame are null.
If an element is equal to NaN or None, then the function will return <b>False</b>.
Otherwise, the function will return <b>True</b>.
Here are several common ways to use this function in practice:
<b>Method 1: Filter for Rows with No Null Values in Any Column</b>
<b>df[df.notnull().all(1)]
</b>
<b>Method 2: Filter for Rows with No Null Values in Specific Column</b>
<b>df[df[['this_column']].notnull().all(1)]</b>
<b>Method 3: Count Number of Non-Null Values in Each Column</b>
<b>df.notnull().sum()</b>
<b>Method 4: Count Number of Non-Null Values in Entire DataFrame</b>
<b>df.notnull().sum().sum()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, np.nan],   'assists': [5, np.nan, 7, 9, 12, 9, 9, np.nan],   'rebounds': [11, 8, 10, 6, 6, 5, np.nan, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A    18.0      5.0      11.0
1    B    22.0      NaN       8.0
2    C    19.0      7.0      10.0
3    D    14.0      9.0       6.0
4    E    14.0     12.0       6.0
5    F    11.0      9.0       5.0
6    G    20.0      9.0       NaN
7    H     NaN      NaN      12.0
</b>
<h2>Example 1: Filter for Rows with No Null Values in Any Column</h2>
The following code shows how to filter the DataFrame to only show rows with no null values in any column:
<b>#filter for rows with no null values in any column
df[df.notnull().all(1)]
        teampointsassistsrebounds
0A18.05.011.0
2C19.07.010.0
3D14.09.06.0
4E14.012.06.0
5F11.09.05.0</b>
Notice that each of the rows in this filtered DataFrame have no null values in any column.
<h2>Example 2: Filter for Rows with No Null Values in Specific Column</h2>
The following code shows how to filter the DataFrame to only show rows with no null values in the <b>assists</b> column:
<b>#filter for rows with no null values in the 'assists' column
df[df[['assists']].notnull().all(1)]
teampointsassistsrebounds
0A18.05.011.0
2C19.07.010.0
3D14.09.06.0
4E14.012.06.0
5F11.09.05.0
6G20.09.0NaN</b>
Notice that each of the rows in this filtered DataFrame have no null values in the <b>assists</b> column.
<h2>Example 3: Count Number of Non-Null Values in Each Column</h2>
The following code shows how to count the number of non-null values in each column of the DataFrame:
<b>#count number of non-null values in each column
df.notnull().sum()
team        8
points      7
assists     6
rebounds    7
dtype: int64
</b>
From the output we can see:
The <b>team</b> column has 8 non-null values.
The <b>points </b>column has 7 non-null values.
The <b>assists </b>column has 6 non-null values.
The <b>rebounds </b>column has 7 non-null values.
<h2>Example 4: Count Number of Non-Null Values in Entire DataFrame</h2>
The following code shows how to count the number of non-null values in the entire DataFrame:
<b>#count number of non-null values in entire DataFrame
df.notnull().sum().sum()
28
</b>
From the output we can see there are <b>28</b> non-null values in the entire DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common filtering operations in pandas:
 How to Filter a Pandas DataFrame by Column Values 
 How to Filter for “Not Contains” in Pandas 
 How to Filter a Pandas DataFrame on Multiple Conditions 
<h2><span class="orange">How to Iterate Over Columns in Pandas DataFrame</span></h2>
You can use the following basic syntax to iterate over columns in a pandas DataFrame:
<b>for name, values in df.iteritems():
  print(values)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
</b>
<h3>Example 1: Iterate Over All Columns in DataFrame</h3>
The following code shows how to iterate over every column in a pandas DataFrame:
<b>for name, values in df.iteritems():
  print(values)
0    25
1    12
2    15
3    14
4    19
Name: points, dtype: int64
0     5
1     7
2     7
3     9
4    12
Name: assists, dtype: int64
0    11
1     8
2    10
3     6
4     6
Name: rebounds, dtype: int64</b>
We can also use the following syntax to iterate over every column and print just the column names:
<b>for name, values in df.iteritems():
  print(name)
points
assists
rebounds
</b>
<h3>Example 2: Iterate Over Specific Columns</h3>
The following syntax shows how to iterate over specific columns in a pandas DataFrame:
<b>for name, values in df[['points', 'rebounds']].iteritems():
  print(values)
0    25
1    12
2    15
3    14
4    19
Name: points, dtype: int64
0    11
1     8
2    10
3     6
4     6
Name: rebounds, dtype: int64
</b>
We can also use the following syntax to iterate over a range of specific columns:
<b>for name, values in df.iloc[:, 0:2].iteritems():
  print(values)
0    25
1    12
2    15
3    14
4    19
Name: points, dtype: int64
0     5
1     7
2     7
3     9
4    12
Name: assists, dtype: int64
</b>
You can find the complete documentation for the <b>iteritems()</b> function  here .
<h2><span class="orange">How to Iterate Over Rows in Pandas DataFrame</span></h2>
You can use the following basic syntax to iterate over rows in a pandas DataFrame:
<b>for index, row in df.iterrows():
  print(row)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
</b>
<h3>Example 1: Iterate Over All Rows in DataFrame</h3>
The following code shows how to iterate over every row in a pandas DataFrame:
<b>for index, row in df.iterrows():
  print(row)
points      25
assists      5
rebounds    11
Name: 0, dtype: int64
points      12
assists      7
rebounds     8
Name: 1, dtype: int64
points      15
assists      7
rebounds    10
Name: 2, dtype: int64
points      14
assists      9
rebounds     6
Name: 3, dtype: int64
points      19
assists     12
rebounds     6
Name: 4, dtype: int64</b>
We can also use the following syntax to iterate over every row and print just the index of each row:
<b>for index, row in df.iterrows():
  print(index)
0
1
2
3
4
</b>
<h3>Example 2: Iterate Over Specific Rows</h3>
The following syntax shows how to iterate over specific rows in a pandas DataFrame:
<b>#iterate over first three rows only
for index, row in df.iloc[0:3, :].iterrows():
  print(row)
points      25
assists      5
rebounds    11
Name: 0, dtype: int64
points      12
assists      7
rebounds     8
Name: 1, dtype: int64
points      15
assists      7
rebounds    10
Name: 2, dtype: int64
</b>
You can find the complete documentation for the <b>iterrows()</b> function  here .
<h2><span class="orange">Pandas Join vs. Merge: What’s the Difference?</span></h2>
Both the <b>join()</b> and the <b>merge()</b> functions can be used to combine two pandas DataFrames.
Here’s the main difference between the two functions:
The <b>join()</b> function combines two DataFrames by index.
The <b>merge()</b> function combines two DataFrames by whatever column you specify.
These functions use the following basic syntax:
<b>#use join() to combine two DataFrames by index
df1.join(df2)
#use merge() to combine two DataFrames by specific column name
df1.merge(df2, on='column_name')
</b>
In cases where you know that you want to join two DataFrames by index, the <b>join()</b> function can be used to save some typing.
The following examples show how to use each function in practice.
<h3>Example 1: How to Use the join() Function</h3>
The following code shows how to use the <b>join()</b> function to combine two DataFrames:
<b>import pandas as pd
#create two DataFrames
df1 = pd.DataFrame({'name': ['A', 'B', 'C'], 'points': [8, 12, 19]}).set_index('name')
df2 = pd.DataFrame({'name': ['A', 'B', 'C'], 'steals': [4, 5, 2]}).set_index('name')
#view two DataFrames
print(df1); print(df2)
      points                 steals
name                   name
A          8           A          4
B         12           B          5
C         19           C          2
#use join() function to join together two DataFrames
df1.join(df2)
pointssteals
name
A84
B125
C192</b>
By default, the <b>join()</b> function joined together the two DataFrames using the index column.
<h3>Example 2: How to Use the merge() Function</h3>
The following code shows how to use the <b>merge()</b> function to combine two DataFrames:
<b>import pandas as pd
#create two DataFrames
df1 = pd.DataFrame({'name': ['A', 'B', 'C'], 'points': [8, 12, 19]}).set_index('name')
df2 = pd.DataFrame({'name': ['A', 'B', 'C'], 'steals': [4, 5, 2]}).set_index('name')
#view two DataFrames
print(df1); print(df2)
      points                 steals
name                   name
A          8           A          4
B         12           B          5
C         19           C          2
#use join() function to join together two DataFrames
df1.merge(df2, on='name')
pointssteals
name
A84
B125
C192</b>
Notice that the <b>merge()</b> function returned the exact same result, but we had to explicitly tell pandas to join the DataFrames using the ‘name’ column.
<h2><span class="orange">How to Keep Certain Columns in Pandas (With Examples)</span></h2>
You can use the following methods to only keep certain columns in a pandas DataFrame:
<b>Method 1: Specify Columns to Keep</b>
<b>#only keep columns 'col1' and 'col2'
df[['col1', 'col2']]
</b>
<b>Method 2: Specify Columns to Drop</b>
<b>#drop columns 'col3' and 'col4'
df[df.columns[~df.columns.isin(['col3', 'col4'])]]
</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [11, 7, 8, 10, 13, 13],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
teampointsassistsrebounds
0A11511
1A778
2A8710
3B1096
4B13126
5B1395
</b>
<h3>Method 1: Specify Columns to Keep</h3>
The following code shows how to define a new DataFrame that only keeps the “team” and “points” columns:
<b>#create new DataFrame and only keep 'team' and 'points' columns
df2 = df[['team', 'points']]
#view new DataFrame
df2
        teampoints
0A11
1A7
2A8
3B10
4B13
5B13</b>
Notice that the resulting DataFrame only keeps the two columns that we specified.
<h3>Method 2: Specify Columns to Drop</h3>
The following code shows how to define a new DataFrame that drops the “assists” and “rebounds” columns from the original DataFrame:
<b>#create new DataFrame and that drops 'assists' and 'rebounds'
df2 = df[df.columns[~df.columns.isin(['assists', 'rebounds'])]]
#view new DataFrame
df2
        teampoints
0A11
1A7
2A8
3B10
4B13
5B13</b>
Notice that the resulting DataFrame drops the “assists” and “rebounds” columns from the original DataFrame and keeps the remaining columns.
<h2><span class="orange">How to Fix in Pandas: KeyError: “[‘Label’] not found in axis”</span></h2>
One error you may encounter when using pandas is:
<b>KeyError: "['Label'] not found in axis"
</b>
This error usually occurs when you attempt to drop a column from a pandas DataFrames and forget to specify <b>axis=1</b>.
By default, the axis argument is set to <b>0</b> which refers to rows. You must specify <b>axis=1</b> to tell pandas to look at the columns.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'points': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
        teamassistspoints
0A511
1A78
2A710
3A96
4B126
5B95
6B99
7B412
</b>
Now suppose we attempt to drop the “points” column from the DataFrame:
<b>#attempt to drop "points" column
df_new = df.drop('points')
KeyError: "['points'] not found in axis"
</b>
By default, the <b>drop()</b> function uses <b>axis=0</b>, which refers to the rows of the DataFrame.
Since there is no row name called “points” we receive an error.
<h3>How to Fix the Error</h3>
To tell pandas to look at the columns instead, we must specify <b>axis=1</b> as follows:
<b>#drop "points" column
df_new = df.drop('points', axis=1)
#view updated DataFrame
print(df)
teamassists
0A5
1A7
2A7
3A9
4B12
5B9
6B9
7B4
</b>
Notice that the “points” column has been dropped from the DataFrame and we don’t receive any error.
This is because we used <b>axis=1</b>, so pandas knew to look at the column names for “points” when deciding which values to drop from the DataFrame.
<h2><span class="orange">How to Fix KeyError in Pandas (With Example)</span></h2>
One error you may encounter when using pandas is:
<b>KeyError: 'column_name'
</b>
This error occurs when you attempt to access some column in a pandas DataFrame that does not exist.
Typically this error occurs when you simply misspell a column names or include an accidental space before or after the column name.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we create the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412</b>
Then suppose we attempt to print the values in a column called ‘point’:
<b>#attempt to print values in 'point' column
print(df['point'])
KeyError                                  Traceback (most recent call last)
/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3360             try:
-> 3361                 return self._engine.get_loc(casted_key)
   3362             except KeyError as err:
/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
/srv/conda/envs/notebook/lib/python3.7/site-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()
pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()
KeyError: 'point'
</b>
Since there is no ‘point’ column in our DataFrame, we receive a <b>KeyError</b>.
<h3>How to Fix the Error</h3>
The way to fix this error is to simply make sure we spell the column name correctly. 
If we’re unsure of all of the column names in the DataFrame, we can use the following syntax to print each column name:
<b>#display all column names of DataFrame
print(df.columns.tolist())
['points', 'assists', 'rebounds']
</b>
We can see that there is a column called ‘points’, so we can fix our error by spelling the column name correctly:
<b>#print values in 'points' column
print(df['points'])
0    25
1    12
2    15
3    14
4    19
5    23
6    25
7    29
Name: points, dtype: int64
</b>
We avoid an error because we spelled the column name correctly.
<h2><span class="orange">How to Calculate Lag by Group in Pandas</span></h2>
You can use the following methods to calculate lagged values by group in a pandas DataFrame:
<b>Method 1: Calculate Lag by One Group</b>
<b>df['lagged_values'] = df.groupby(['group'])['values'].shift(1)</b>
<b>Method 2: Calculate Lag by Multiple Groups</b>
<b>df['lagged_values'] = df.groupby(['group1', 'group2'])['values'].shift(1)</b>
Note that the value in the <b>shift()</b> function indicates the number of values to calculate the lag for.
The following examples show how to use each method in practice.
<h2>Example 1: Calculate Lag by One Group</h2>
Suppose we have the following pandas DataFrame that shows the sales made by two stores on consecutive days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'sales': [18, 10, 14, 13, 19, 24, 25, 29]})
#view DataFrame
print(df)
  store  sales
0     A     18
1     A     10
2     A     14
3     A     13
4     B     19
5     B     24
6     B     25
7     B     29</b>
We can use the following syntax to create a lag column that displays the sales for the previous day for each store:
<b>#add column that displays lag of sales column by store
df['lagged_sales'] = df.groupby(['store'])['sales'].shift(1)
#view updated DataFrame
print(df)
  store  sales  lagged_sales
0     A     18           NaN
1     A     10          18.0
2     A     14          10.0
3     A     13          14.0
4     B     19           NaN
5     B     24          19.0
6     B     25          24.0
7     B     29          25.0
</b>
Here’s how to interpret the output:
The first value in the lag column is <b>NaN</b> since there is no prior value in the sales column for store A.
The second value in the lag column is <b>18</b> since this is the prior value in the sales column for store A.
And so on.
<h2>Example 2: Calculate Lag by Multiple Groups</h2>
Suppose we have the following pandas DataFrame that shows the sales made by employees at two stores on consecutive days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'employee':['O', 'O', 'R', 'R', 'O', 'O', 'R', 'R'],   'sales': [18, 10, 14, 13, 19, 24, 25, 29]})
#view DataFrame
print(df)
  store employee  sales
0     A        O     18
1     A        O     10
2     A        R     14
3     A        R     13
4     B        O     19
5     B        O     24
6     B        R     25
7     B        R     29
</b>
We can use the following syntax to create a lag column that displays the sales for the previous day for each employee at each store:
<b>#add column that displays lag of sales column by store and employee
df['lagged_sales'] = df.groupby(['store', 'employee'])['sales'].shift(1)
#view updated DataFrame
print(df)
  store employee  sales  lagged_sales
0     A        O     18           NaN
1     A        O     10          18.0
2     A        R     14           NaN
3     A        R     13          14.0
4     B        O     19           NaN
5     B        O     24          19.0
6     B        R     25           NaN
7     B        R     29          25.0
</b>
The new <b>lagged_sales</b> column displays the sales for the previous day for each employee at each store.
<b>Note</b>: In this example we grouped by two columns, but you can group by as many columns as you’d like by including as many variable names as you’d like in the <b>groupby()</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop Columns in Pandas 
 How to Exclude Columns in Pandas 
 How to Apply a Function to Selected Columns in Pandas 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Create a Lag Column in Pandas (With Examples)</span></h2>
You can use the <b>shift()</b> function in pandas to create a column that displays the lagged values of another column.
This function uses the following basic syntax:
<b>df['lagged_col1'] = df['col1'].shift(1)</b>
Note that the value in the <b>shift()</b> function indicates the number of values to calculate the lag for.
The following example shows how to use this syntax in practice.
<h2>Example: Create a Lag Column in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some store on 10 consecutive days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],   'sales': [18, 10, 14, 13, 19, 24, 25, 29, 15, 18]})
#view DataFrame
print(df)
   day  sales
0    1     18
1    2     10
2    3     14
3    4     13
4    5     19
5    6     24
6    7     25
7    8     29
8    9     15
9   10     18
</b>
We can use the <b>shift()</b> function to create a lag column that displays the sales for the previous day for each row:
<b>#add column that represents lag of sales column
df['sales_previous_day'] = df['sales'].shift(1)
#view updated DataFrame
print(df)
   day  sales  sales_previous_day
0    1     18                 NaN
1    2     10                18.0
2    3     14                10.0
3    4     13                14.0
4    5     19                13.0
5    6     24                19.0
6    7     25                24.0
7    8     29                25.0
8    9     15                29.0
9   10     18                15.0
</b>
Here’s how to interpret the output:
The first value in the lag column is <b>NaN</b> since there is no prior value in the <b>sales</b> column.
The second value in the lag column is <b>18</b> since this is the prior value in the <b>sales</b> column.
The third value in the lag column is <b>10</b> since this is the prior value in the <b>sales</b> column.
And so on.
Note that we can also add multiple lag columns to the DataFrame if we’d like:
<b>#add two lag columns
df['sales_previous_day'] = df['sales'].shift(1)
df['sales_previous_day2'] = df['sales'].shift(2) 
#view updated DataFrame
print(df)
   day  sales  sales_previous_day  sales_previous_day2
0    1     18                 NaN                  NaN
1    2     10                18.0                  NaN
2    3     14                10.0                 18.0
3    4     13                14.0                 10.0
4    5     19                13.0                 14.0
5    6     24                19.0                 13.0
6    7     25                24.0                 19.0
7    8     29                25.0                 24.0
8    9     15                29.0                 25.0
9   10     18                15.0                 29.0
</b>
You can use the same general approach to add as many lag columns as you’d like.
<b>Note</b>: To create a lead column, simply use negative values in the <b>shift()</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop Columns in Pandas 
 How to Exclude Columns in Pandas 
 How to Apply a Function to Selected Columns in Pandas 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to List All Column Names in Pandas (4 Methods)</span></h2>
You can use one of the following four methods to list all column names of a pandas DataFrame:
<b>Method 1: Use Brackets</b>
<b>[column for column in df]
</b>
<b>Method 2: Use tolist()</b>
<b>df.columns.values.tolist()</b>
<b>Method 3: Use list()</b>
<b>list(df)</b>
<b>Method 4: Use list() with column values</b>
<b>list(df.columns.values)</b>
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5],   'blocks': [6, 6, 3, 2, 7, 9]})
#view DataFrame
df
pointsassistsrebounds blocks
025511 6
11278 6
215710 3
31496 2
419126 7
52395 9
</b>
<h3>Method 1: Use Brackets</h3>
The following code shows how to list all column names of a pandas DataFrame using brackets:
<b>[column for column in df]
['points', 'assists', 'rebounds', 'blocks']
</b>
<h3>Method 2: Use tolist()</h3>
The following code shows how to list all column names using the <b>.tolist()</b> function:
<b>df.columns.values.tolist()
['points', 'assists', 'rebounds', 'blocks'] 
</b>
<h3>Method 3: Use list()</h3>
The following code shows how to list all column names using the <b>list()</b> function:
<b>list(df)
['points', 'assists', 'rebounds', 'blocks'] 
</b>
<h3>Method 4: Use list() with column values</h3>
The following code shows how to list all column names using the <b>list()</b> function with column values:
<b>list(df.columns.values)
['points', 'assists', 'rebounds', 'blocks'] </b>
Notice that all four methods return the same results.
Note that for extremely large DataFrames, the <b>df.columns.values.tolist()</b> method tends to perform the fastest.
<h2><span class="orange">Pandas: How to Use loc to Select Multiple Columns</span></h2>
You can use the <b>loc</b> function in pandas to select multiple columns in a DataFrame by label.
Here are the most common ways to do so:
<b>Method 1: Select Multiple Columns by Name</b>
<b>df.loc[:, ['col2', 'col4']]
</b>
<b>Method 2: Select All Columns in Range</b>
<b>df.loc[:, 'col2':'col4']</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'assists': [11, 8, 10, 6, 6, 5, 9, 12],   'rebounds': [6, 7, 7, 6, 10, 12, 10, 9]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A       5       11         6
1    A       7        8         7
2    A       7       10         7
3    A       9        6         6
4    B      12        6        10
5    B       9        5        12
6    B       9        9        10
7    B       4       12         9
</b>
<h2>Example 1: Select Multiple Columns by Name</h2>
The following code shows how to use the <b>loc</b> function to select the ‘points’ and ‘rebounds’ columns from the DataFrame:
<b>#select points and rebounds columns
df.loc[:, ['points', 'rebounds']]
        pointsrebounds
056
177
277
396
41210
5912
6910
749
</b>
Notice that each row from the ‘points’ and ‘rebounds’ columns are returned.
Also note that the order you specify the columns in the <b>loc</b> function is the order they’ll be returned in.
For example, we could return the ‘rebounds’ column first and then the ‘points’ column:
<b>#select rebounds and points columns
df.loc[:, ['rebounds', 'points']]
rebounds points
06 5
17 7
27 7
36 9
410 12
512 9
610 9
79 4
</b>
<h2>Example 2: Select All Columns in Range</h2>
The following code shows how to use the <b>loc</b> function to select all columns between the ‘points’ and ‘rebounds’ columns in the DataFrame:
<b>#select all columns between points and rebounds columns
df.loc[:, 'points':'rebounds']
pointsassistsrebounds
05116
1787
27107
3966
412610
59512
69910
74129
</b>
Notice that all columns between the ‘points’ and ‘rebounds’ columns in the DataFrame are returned.
<b>Note</b>: To select columns by index position, use the  iloc  function instead.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Select Rows by Multiple Conditions Using Pandas loc 
 How to Select Rows Based on Column Values in Pandas 
 How to Select Rows by Index in Pandas 
<h2><span class="orange">How to Select Rows by Multiple Conditions Using Pandas loc</span></h2>
You can use the following methods to select rows of a pandas DataFrame based on multiple conditions:
<b>Method 1: Select Rows that Meet Multiple Conditions</b>
<b>df.loc[((df['col1'] == 'A') & (df['col2'] == 'G'))]
</b>
<b>Method 2: Select Rows that Meet One of Multiple Conditions</b>
<b>df.loc[((df['col1'] > 10) | (df['col2'] &lt; 8))] </b>
The following examples show how to use each of these methods in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition assists rebounds
0AG 5 11
1AG 7 8
2AF 7 10
3AF 9 6
4BG 12 6
5BG 9 5
6BF 9 9
7BF 4 12
</b>
<h3>Method 1: Select Rows that Meet Multiple Conditions</h3>
The following code shows how to only select rows in the DataFrame where the team is equal to ‘A’ and the position is equal to ‘G’:
<b>#select rows where team is equal to 'A' and position is equal to 'G'
df.loc[((df['team'] == 'A') & (df['position'] == 'G'))]
teamposition assists rebounds
0AG 5 11
1AG 7 8
</b>
There were only two rows in the DataFrame that met both of these conditions.
<h3>Method 2: Select Rows that Meet One of Multiple Conditions</h3>
The following code shows how to only select rows in the DataFrame where the assists is greater than 10 <em>or</em> where the rebounds is less than 8:
<b>#select rows where assists is greater than 10 or rebounds is less than 8
df.loc[((df['assists'] > 10) | (df['rebounds'] &lt; 8))]
teamposition assists rebounds
3AF 9 6
4BG 12 6
5BG 9 5
</b>
There were only three rows in the DataFrame that met both of these conditions.
<b>Note:</b> In these two examples we filtered rows based on two conditions but using the <b>&</b> and <b>|</b> operators, we can filter on as many conditions as we’d like.
<h2><span class="orange">Pandas loc vs. iloc: What’s the Difference?</span></h2>
When it comes to selecting rows and columns of a pandas DataFrame, <b>loc</b> and <b>iloc</b> are two commonly used functions.
Here is the subtle difference between the two functions:
<b>loc</b> selects rows and columns with specific <b>labels</b>
<b>iloc</b> selects rows and columns at specific <b>integer positions</b>
The following examples show how to use each function in practice.
<h3>Example 1: How to Use loc in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'assists': [11, 8, 10, 6, 6, 5, 9, 12]},   index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])
#view DataFrame
df
teampointsassists
AA511
BA78
CA710
DA96
EB126
FB95
GB99
HB412</b>
We can use <b>loc</b> to select specific rows of the DataFrame based on their index labels:
<b>#select rows with index labels 'E' and 'F'
df.loc[['E', 'F']]
teampointsassists
EB126
FB95</b>
We can use <b>loc</b> to select specific rows and specific columns of the DataFrame based on their labels:
<b>#select 'E' and 'F' rows and 'team' and 'assists' columns
df.loc[['E', 'F'], ['team', 'assists']]
teamassists
EB12
FB9
</b>
We can use <b>loc</b> with the <b>:</b> argument to select ranges of rows and columns based on their labels:
<b>#select 'E' and 'F' rows and 'team' and 'assists' columns
df.loc['E': , :'assists']
        teampointsassists
EB126
FB95
GB99
HB412</b>
<h3>Example 2: How to Use iloc in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'assists': [11, 8, 10, 6, 6, 5, 9, 12]},   index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])
#view DataFrame
df
teampointsassists
AA511
BA78
CA710
DA96
EB126
FB95
GB99
HB412</b>
We can use <b>iloc</b> to select specific rows of the DataFrame based on their integer position:
<b>#select rows in index positions 4 through 6 (not including 6)
df.iloc[4:6]
teampointsassists
EB126
FB95</b>
We can use <b>iloc</b> to select specific rows and specific columns of the DataFrame based on their index positions:
<b>#select rows in range 4 through 6 and columns in range 0 through 2
df.iloc[4:6, 0:2]
teamassists
EB12
FB9
</b>
We can use <b>loc</b> with the <b>:</b> argument to select ranges of rows and columns based on their labels:
<b>#select rows from 4 through end of rows and columns up to third column
df.iloc[4: , :3]
        teampointsassists
EB126
FB95
GB99
HB412</b>
<h2><span class="orange">Pandas: How to Reshape DataFrame from Long to Wide</span></h2>
You can use the following basic syntax to convert a pandas DataFrame from a long format to a wide format:
<b>df = pd.pivot(df, index='col1', columns='col2', values='col3')
</b>
In this scenario, <b>col1</b> will become the index, <b>col2</b> will become the columns, and <b>col3</b> will be used as the values inside the DataFrame.
The following example shows how to use this syntax in practice.
<h3>Example: Reshape Pandas DataFrame from Long to Wide</h3>
Suppose we have the following pandas DataFrame in a long format:
<b>import pandas as pd
#create DataFrame in long format
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'player': [1, 2, 3, 4, 1, 2, 3, 4],   'points': [11, 8, 10, 6, 12, 5, 9, 4]})
#view DataFrame
df
teamplayerpoints
0A111
1A28
2A310
3A46
4B112
5B25
6B39
7B44
</b>
We can use the following syntax to reshape this DataFrame from a long format to a wide format:
<b>#reshape DataFrame from long format to wide format
df = pd.pivot(df, index='team', columns='player', values='points')
#view updated DataFrame
df
player1234
team
A118106
B12594</b>
The DataFrame is now in a wide format.
We used ‘team’ as the index column, ‘player’ as the columns, and ‘points’ as the values inside of the DataFrame.
Note that we could instead use ‘player’ as the index column and ‘team’ as the columns if we’d like:
<b>#reshape DataFrame from long format to wide format
df = pd.pivot(df, index='player', columns='team', values='points')
#view updated DataFrame
df
teamAB
player
11112
285
3109
464
</b>
This DataFrame is also in a wide format.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot()</b> function  here .
<h2><span class="orange">Pandas: How to Find Max Value Across Multiple Columns</span></h2>
You can use the following methods to find the max value across multiple columns in a pandas DataFrame:
<b>Method 1: Find Max Value Across Multiple Columns</b>
<b>df[['col1', 'col2', 'col3']].max(axis=1)
</b>
<b>Method 2: Add New Column Containing Max Value Across Multiple Columns</b>
<b>df['new_col'] = df[['col1', 'col2', 'col3']].max(axis=1)</b>
The following examples show how to use each of these methods in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],   'points': [28, 17, 19, 14, 23, 26, 5],   'rebounds': [5, 6, 4, 7, 14, 12, 9],   'assists': [10, 13, 7, 8, 4, 5, 8]})
#view DataFrame
print(df)
  player  points  rebounds  assists
0      A      28         5       10
1      B      17         6       13
2      C      19         4        7
3      D      14         7        8
4      E      23        14        4
5      F      26        12        5
6      G       5         9        8</b>
<h2>Example 1: Find Max Value Across Multiple Columns</h2>
The following code shows how to find the max value in each row across the points and rebounds columns:
<b>#find max value across points and rebounds columns
df[['points', 'rebounds']].max(axis=1)
0    28
1    17
2    19
3    14
4    23
5    26
6     9
dtype: int64
</b>
Here’s how to interpret the output:
The max value across the points and rebounds columns for the first row was <b>28</b>.
The max value across the points and rebounds columns for the second row was <b>17</b>.
The max value across the points and rebounds columns for the third row was <b>19</b>.
And so on.
<h2>Example 2: Add New Column Containing Max Value Across Multiple Columns</h2>
The following code shows how to add a new column to the DataFrame that contains the max value in each row across the points and rebounds columns:
<b>#add new column that contains max value across points and rebounds columns
df['max_points_rebs'] = df[['points', 'rebounds']].max(axis=1)
#view updated DataFrame
print(df)
  player  points  rebounds  assists  max_points_rebs
0      A      28         5       10               28
1      B      17         6       13               17
2      C      19         4        7               19
3      D      14         7        8               14
4      E      23        14        4               23
5      F      26        12        5               26
6      G       5         9        8                9</b>
The new column titled <b>max_points_rebs</b> now contains the max value across the points and rebounds columns for each row in the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Move Column to Front of DataFrame 
 Pandas: How to Check if Column Contains String 
 Pandas: How to Add Empty Column to DataFrame 
<h2><span class="orange">How to Calculate the Mean by Group in Pandas (With Examples)</span></h2>
You can use the following methods to calculate the mean value by group in pandas:
<b>Method 1: Calculate Mean of One Column Grouped by One Column</b>
<b>df.groupby(['group_col'])['value_col'].mean()
</b>
<b>Method 2: Calculate Mean of Multiple Columns Grouped by One Column</b>
<b>df.groupby(['group_col'])['value_col1', 'value_col2'].mean()</b>
<b>Method 3: Calculate Mean of One Column Grouped by Multiple Columns</b>
<b>df.groupby(['group_col1', 'group_col2'])['value_col'].mean()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'F', 'F', 'G', 'F', 'F', 'G', 'G'],   'points': [30, 22, 19, 14, 14, 11, 20, 28],   'assists': [4, 3, 7, 7, 12, 15, 8, 4]})
#view DataFrame
print(df)
  team position  points  assists
0    A        G      30        4
1    A        F      22        3
2    A        F      19        7
3    A        G      14        7
4    B        F      14       12
5    B        F      11       15
6    B        G      20        8
7    B        G      28        4
</b>
<h2>Example 1: Calculate Mean of One Column Grouped by One Column</h2>
The following code shows how to calculate the mean value of the <b>points</b> column, grouped by the <b>team</b> column:
<b>#calculate mean of points grouped by team
df.groupby('team')['points'].mean()
team
A    21.25
B    18.25
Name: points, dtype: float64
</b>
From the output we can see:
The mean points value for team A is <b>21.25</b>.
The mean points value for team B is <b>18.25</b>.
<h2>Example 2: Calculate Mean of Multiple Columns Grouped by One Column</h2>
The following code shows how to calculate the mean value of the <b>points</b> column and the mean value of the <b>assists</b> column, grouped by the <b>team</b> column:
<b>#calculate mean of points and mean of assists grouped by team
df.groupby('team')[['points', 'assists']].mean()
       pointsassists
team
A21.25   5.25
B18.25   9.75
</b>
The output displays the mean <b>points</b> value and mean <b>assists</b> value for each team.
<h2>Example 3: Calculate Mean of One Column Grouped by Multiple Columns</h2>
The following code shows how to calculate the mean value of the <b>points</b> column, grouped by the <b>team</b> and <b>position</b> columns:
<b>#calculate mean of points, grouped by team and position
df.groupby(['team', 'position'])['points'].mean()
team  position
A     F           20.5
      G           22.0
B     F           12.5
      G           24.0
Name: points, dtype: float64
</b>
From the output we can see:
The mean points value for players on team A and position F is <b>20.5</b>.
The mean points value for players on team A and position G is <b>22</b>.
The mean points value for players on team B and position F is <b>12.5</b>.
The mean points value for players on team B and position G is <b>24</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in pandas:
 How to Find the Max Value by Group in Pandas 
 How to Find Sum by Group in Pandas 
 How to Calculate Quantiles by Group in Pandas 
<h2><span class="orange">How to Find the Median Value by Group in Pandas</span></h2>
You can use the following basic syntax to calculate the median value by group in pandas:
<b>df.groupby(['group_variable'])['value_variable'].median().reset_index()
</b>
You can also use the following syntax to calculate the median value, grouped by several columns:
<b>df.groupby(['group1', 'group2'])['value_variable'].median().reset_index()</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Find Median Value by One Group</h3>
Suppose we have the following pandas DataFrames:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition pointsrebounds
0AG 511
1AG 78
2AF 710
3AF 96
4BG 126
5BG 95
6BF 99
7BF 412</b>
We can use the following code to find the median value of the ‘points’ column, grouped by team:
<b>#calculate median points by team
df.groupby(['team'])['points'].median().reset_index()
teampoints
0A7.0
1B9.0</b>
From the output we can see:
The median points scored by players on team A is <b>7</b>.
The median points scored by players on team B is <b>9</b>.
Note that we can also find the median value of two variables at once:
<b>#calculate median points and median rebounds by team
df.groupby(['team'])[['points', 'rebounds']].median()
teampointsrebounds
0A7.09.0
1B9.07.5
</b>
<h3>Example 2: Find Median Value by Multiple Groups</h3>
The following code shows how to find the median value of the ‘points’ column, grouped by team and position:
<b>#calculate median points by team
df.groupby(['team', 'position'])['points'].median().reset_index()
teamposition points
0AF 8.0
1AG 6.0
2BF 6.5
3BG 10.5</b>
From the output we can see:
The median points scored by players in the ‘F’ position on team A is <b>8</b>.
The median points scored by players in the ‘G’ position on team A is <b>6</b>.
The median points scored by players in the ‘F’ position on team B is <b>6.5</b>.
The median points scored by players in the ‘G’ position on team B is <b>10.5</b>.
<h2><span class="orange">How to Calculate the Median in Pandas (With Examples)</span></h2>
You can use the <b>median()</b> function to find the median of one or more columns in a pandas DataFrame:
<b>#find median value in specific column
df['column1'].median()
#find median value in several columns
df[['column1', 'column2']].median()
#find median value in every numeric column
df.median()
</b>
The following examples show how to use this function in practice with the following pandas DataFrame:
<b>#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [25, pd.NA, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
playerpointsassistsrebounds
0A25511
1BNA78
2C15710
3D1496
4E19126
5F2395
6G2599
7H29412</b>
<h3>Example 1: Find Median of a Single Column</h3>
The following code shows how to find the median value of a single column in a pandas DataFrame:
<b>#find median value of <em>points</em> column
df['points'].median()
23.0</b>
The median value in the points column is <b>23</b>. 
Note that by default, the <b>median()</b> function ignores any missing values when calculating the median.
<h3>Example 2: Find Median of Multiple Columns</h3>
The following code shows how to find the median value of multiple columns in a pandas DataFrame:
<b>#find median value of <em>points</em> and <em>rebounds</em> columns
df[['points', 'rebounds']].median()
points      23.0
rebounds     8.5
dtype: float64</b>
<h3>Example 3: Find Median of All Numeric Columns</h3>
The following code shows how to find the median value of all numeric columns in a pandas DataFrame:
<b>#find median value of all numeric columns
df.median()
points      23.0
assists      8.0
rebounds     8.5
dtype: float64</b>
<h2><span class="orange">Pandas: How to Merge Columns Sharing Same Name</span></h2>
You can use the following basic syntax to merge together columns in a pandas DataFrame that share the same column name:
<b>#define function to merge columns with same names together
def same_merge(x): return ','.join(x[x.notnull()].astype(str))
#define new DataFrame that merges columns with same names together
df_new = df.groupby(level=0, axis=1).apply(lambda x: x.apply(same_merge, axis=1))</b>
The following example shows how to use this syntax in practice.
<h2>Example: Merge Together Columns Sharing Same Name in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'A': [5, 6, 8, np.nan, 4, np.nan, np.nan],   'A1': [np.nan, 12, np.nan, 10, np.nan, 6, 4],   'B': [2, 7, np.nan, np.nan, 2, 4, np.nan],   'B1': [5, np.nan, 6, 15, 1, np.nan, 4]})
#rename columns so there are duplicate column names
df.columns = ['A', 'A', 'B', 'B']
#view DataFrame
print(df)
     A     A    B     B
0  5.0   NaN  2.0   5.0
1  6.0  12.0  7.0   NaN
2  8.0   NaN  NaN   6.0
3  NaN  10.0  NaN  15.0
4  4.0   NaN  2.0   1.0
5  NaN   6.0  4.0   NaN
6  NaN   4.0  NaN   4.0</b>
Notice that two columns have a name of ‘A’ and two columns have a name of ‘B.’
We can use the following code to merge the columns that have the same column names and concatenate their values together with a comma:
<b>#define function to merge columns with same names together
def same_merge(x): return ','.join(x[x.notnull()].astype(str))
#define new DataFrame that merges columns with same names together
df_new = df.groupby(level=0, axis=1).apply(lambda x: x.apply(same_merge, axis=1))
#view new DataFrame
print(df_new)
          A        B
0       5.0  2.0,5.0
1  6.0,12.0      7.0
2       8.0      6.0
3      10.0     15.0
4       4.0  2.0,1.0
5       6.0      4.0
6       4.0      4.0</b>
The new DataFrame has merged together the columns with the same names and concatenated their values together with a comma.
If you would like to use a different separator, simply change the comma separator to something else in the <b>same_merge()</b> function.
For example, the following code shows how to use a semi-colon separator instead:
<b>#define function to merge columns with same names together
def same_merge(x): return ';'.join(x[x.notnull()].astype(str))
#define new DataFrame that merges columns with same names together
df_new = df.groupby(level=0, axis=1).apply(lambda x: x.apply(same_merge, axis=1))
#view new DataFrame
print(df_new)
          A        B
0       5.0  2.0;5.0
1  6.0;12.0      7.0
2       8.0      6.0
3      10.0     15.0
4       4.0  2.0;1.0
5       6.0      4.0
6       4.0      4.0
</b>
The new DataFrame has merged together the columns with the same names and concatenated their values together with a semi-colon.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Duplicate Columns in Pandas 
 How to List All Column Names in Pandas 
 How to Sort Columns by Name in Pandas 
<h2><span class="orange">How to Merge Pandas DataFrames on Multiple Columns</span></h2>
Often you may want to merge two pandas DataFrames on multiple columns. Fortunately this is easy to do using the pandas  merge()  function, which uses the following syntax:
<b>pd.merge(df1, df2, left_on=['col1','col2'], right_on = ['col1','col2'])</b>
This tutorial explains how to use this function in practice.
<h3>Example 1: Merge on Multiple Columns with Different Names</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create and view first DataFrame
df1 = pd.DataFrame({'a1': [0, 0, 1, 1, 2],   'b': [0, 0, 1, 1, 1],   'c': [11, 8, 10, 6, 6]})
print(df1)
   a1  b   c
0   0  0  11
1   0  0   8
2   1  1  10
3   1  1   6
4   2  1   6
#create and view second DataFrame 
df2 = pd.DataFrame({'a2': [0, 1, 1, 1, 3],   'b': [0, 0, 0, 1, 1],   'd': [22, 24, 25, 33, 37]})
print(df2)
   a2  b   d
0   0  0  22
1   1  0  24
2   1  0  25
3   1  1  33
4   3  1  37
</b>
The following code shows how to perform a left join using multiple columns from both DataFrames:
<b>pd.merge(df1, df2, how='left', left_on=['a1', 'b'], right_on = ['a2','b'])
        a1bca2d
000110.022.0
10080.022.0
211101.033.0
31161.033.0
4216NaNNaN
</b>
<h3>Example 2: Merge on Multiple Columns with Same Names</h3>
Suppose we have the following two pandas DataFrames with the same column names:
<b>import pandas as pd
#create DataFrames
df1 = pd.DataFrame({'a': [0, 0, 1, 1, 2],   'b': [0, 0, 1, 1, 1],   'c': [11, 8, 10, 6, 6]})
df2 = pd.DataFrame({'a': [0, 1, 1, 1, 3],   'b': [0, 0, 0, 1, 1],   'd': [22, 24, 25, 33, 37]})
</b>
In this case we can simplify use <b>on = [‘a’, ‘b’] </b>since the column names are the same in both DataFrames:
<b>pd.merge(df1, df2, how='left', on=['a', 'b'])
abcd
0001122.0
100822.0
2111033.0
311633.0
4216NaN
</b>
<h2><span class="orange">How to Merge Multiple DataFrames in Pandas (With Example)</span></h2>
You can use the following syntax to merge multiple DataFrames at once in pandas:
<b>import pandas as pd
from functools import reduce
#define list of DataFrames
dfs = [df1, df2, df3]
#merge all DataFrames into one
final_df = reduce(lambda  left,right: pd.merge(left,right,on=['column_name'],                            how='outer'), dfs)</b>
The following example shows how to use this syntax in practice:
<h2>Example: Merge Multiple DataFrames in Pandas</h2>
Suppose we have the following three pandas DataFrames that contain information about basketball players on various teams:
<b>import pandas as pd
#create DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D'],    'points': [18, 22, 19, 14]})
df2 = pd.DataFrame({'team': ['A', 'B', 'C'],    'assists': [4, 9, 14]})
df3 = pd.DataFrame({'team': ['C', 'D', 'E', 'F'],    'rebounds': [10, 17, 11, 10]})
#view DataFrames
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
print(df2)
  team  assists
0    A        4
1    B        9
2    C       14
print(df3)
  team  rebounds
0    C        10
1    D        17
2    E        11
3    F        10
</b>
We can use the following syntax to merge all three DataFrames into one:
<b>from functools import reduce
#define list of DataFrames
dfs = [df1, df2, df3]
#merge all DataFrames into one
final_df = reduce(lambda  left,right: pd.merge(left,right,on=['team'],                            how='outer'), dfs)
#view merged DataFrame
print(final_df)
  team  points  assists  rebounds
0    A    18.0      4.0       NaN
1    B    22.0      9.0       NaN
2    C    19.0     14.0      10.0
3    D    14.0      NaN      17.0
4    E     NaN      NaN      11.0
5    F     NaN      NaN      10.0</b>
The final result is one DataFrame that contains information from all three DataFrames.
Notice that <b>NaN</b> values are used to fill in empty cells in the final DataFrame.
To use a value other than <b>NaN</b> to fill in empty cells, we can use the <b>fillna()</b> function: 
<b>from functools import reduce
#define list of DataFrames
dfs = [df1, df2, df3]
#merge all DataFrames into one
final_df = reduce(lambda  left,right: pd.merge(left,right,on=['team'],                            how='outer'), dfs).fillna('none')
#view merged DataFrame
print(final_df)
  team points assists rebounds
0    A   18.0     4.0     none
1    B   22.0     9.0     none
2    C   19.0    14.0     10.0
3    D   14.0    none     17.0
4    E   none    none     11.0
5    F   none    none     10.0</b>
Each of the empty cells are now filled with ‘<b>none</b>‘ instead of <b>NaN</b>.
<b>Note</b>: You can find the complete documentation for the <b>merge</b> function in pandas  here .
<h2><span class="orange">Pandas: How to Merge Two DataFrames with Different Column Names</span></h2>
You can use the following basic syntax to merge two pandas DataFrames with different column names:
<b>pd.merge(df1, df2, left_on='left_column_name', right_on='right_column_name')
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Merge Two Pandas DataFrames with Different Column Names</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [4, 4, 6, 8, 9, 5]})
#view DataFrame
print(df1)
  team  points
0    A       4
1    B       4
2    C       6
3    D       8
4    E       9
5    F       5
#create second  DataFrame
df2 = pd.DataFrame({'team_name': ['A', 'B', 'C', 'D', 'E', 'F'],    'rebounds': [12, 7, 8, 8, 5, 11]})
#view DataFrame
print(df2)
  team_name  rebounds
0         A        12
1         B         7
2         C         8
3         D         8
4         E         5
5         F        11
</b>
We can use the following syntax to perform an inner join, using the <b>team</b> column in the first DataFrame and the <b>team_name</b> column in the second DataFrame:
<b>#merge DataFrames
df3 = pd.merge(df1, df2, left_on='team', right_on='team_name')
#view result
print(df3)
  team  points team_name  rebounds
0    A       4         A        12
1    B       4         B         7
2    C       6         C         8
3    D       8         D         8
4    E       9         E         5
5    F       5         F        11
</b>
Notice that we’re able to successfully perform an inner join even though the two column names that we used for the join were different in each DataFrame.
Note that we can also use the following code to drop the <b>team_name</b> column from the final merged DataFrame since the values in this column match those in the <b>team</b> column:
<b>#drop team_name column
df3.drop('team_name', axis=1, inplace=True)
#view updated DataFrame
print(df3)
  team  points  rebounds
0    A       4        12
1    B       4         7
2    C       6         8
3    D       8         8
4    E       9         5
5    F       5        11
</b>
Notice that the <b>team_name </b>column has been dropped from the DataFrame.
<b>Related:</b>  How to Drop Columns in Pandas (4 Examples) 
<h2><span class="orange">How to Merge Two Pandas DataFrames on Index</span></h2>
Often you may want to merge two pandas DataFrames by their indexes. There are three ways to do so in pandas:
<b>1.</b> Use  join : By default, this performs a left join.
<b>df1.join(df2)</b>
<b>2. </b>Use  merge . By default, this performs an inner join.
<b>pd.merge(df1, df2, left_index=True, right_index=True)</b>
<b>3. </b>Use  concat . By default, this performs an outer join.
<b>pd.concat([df1, df2], axis=1)</b>
The following examples illustrate how to use each of these functions on the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75],   'points': [25, 20, 14, 16, 27, 20, 12, 15]},   index=list('abcdefgh'))
print(df1)
   rating  points
a      90      25
b      85      20
c      82      14
d      88      16
e      94      27
f      90      20
g      76      12
h      75      15
#create second DataFrame 
df2 = pd.DataFrame({'assists': [5, 7, 7, 8, 5, 7],   'rebounds': [11, 8, 10, 6, 6, 9]},   index=list('acdgmn'))           
print(df2)
   assists  rebounds
a        5        11
c        7         8
d        7        10
g        8         6
m        5         6
n        7         9
</b>
<h3>Example 1: Merge DataFrames Using Join</h3>
The following code shows how to use <b>join() </b>to merge the two DataFrames:
<b>df1.join(df2)
        ratingpointsassistsrebounds
a90255.011.0
b8520NaNNaN
c82147.08.0
d88167.010.0
e9427NaNNaN
f9020NaNNaN
g76128.06.0
h7515NaNNaN
</b>
The <b>join() </b>function performs a left join by default, so each of the indexes in the first DataFrame are kept.
<h3>Example 2: Merge DataFrames Using Merge</h3>
The following code shows how to use <b>merge() </b>to merge the two DataFrames:
<b>pd.merge(df1, df2, left_index=True, right_index=True)
ratingpointsassistsrebounds
a9025511
c821478
d8816710
g761286
</b>
The <b>merge() </b>function performs an inner join by default, so only the indexes that appear in both DataFrames are kept.
<h3>Example 3: Merge DataFrames Using Concat</h3>
The following code shows how to use <b>concat() </b>to merge the two DataFrames:
<b>pd.concat([df1, df2], axis=1)
        ratingpointsassistsrebounds
a90.025.05.011.0
b85.020.0NaNNaN
c82.014.07.08.0
d88.016.07.010.0
e94.027.0NaNNaN
f90.020.0NaNNaN
g76.012.08.06.0
h75.015.0NaNNaN
mNaNNaN5.06.0
nNaNNaN7.09.0
</b>
The <b>concat() </b>function performs an outer join by default, so each index value from each DataFrame is kept.
<h2><span class="orange">How to Merge Two or More Series in Pandas (With Examples)</span></h2>
You can use the following syntax to quickly merge two or more series together into a single pandas DataFrame:
<b>df = pd.concat([series1, series2, ...], axis=1)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Merge Two Series in Pandas</h3>
The following code shows how to merge together two pandas Series into a single pandas DataFrame:
<b>import pandas as pd
#define series
series1 = pd.Series(['Mavs', 'Rockets', 'Spurs'], name='Team')
series2 = pd.Series([109, 103, 98], name='Points')
#merge series into DataFrame
df = pd.concat([series1, series2], axis=1)
#view DataFrame
df
        TeamPoints
0Mavs109
1Rockets103
2Spurs98
</b>
Note that if one series is longer than the other, pandas will automatically provide NaN values for missing values in the resulting DataFrame:
<b>import pandas as pd
#define series
series1 = pd.Series(['Mavs', 'Rockets', 'Spurs'], name='Team')
series2 = pd.Series([109, 103], name='Points')
#merge series into DataFrame
df = pd.concat([series1, series2], axis=1)
#view DataFrame
df
        TeamPoints
0Mavs109
1Rockets103
2SpursNaN</b>
<h3>Example 2: Merge Multiple Series in Pandas</h3>
The following code shows how to merge multiple series into a single pandas DataFrame:
<b>import pandas as pd
#define series
series1 = pd.Series(['Mavs', 'Rockets', 'Spurs'], name='Team')
series2 = pd.Series([109, 103, 98], name='Points')
series3 = pd.Series([22, 18, 15], name='Assists')
series4 = pd.Series([30, 35, 28], name='Rebounds')
#merge series into DataFrame
df = pd.concat([series1, series2, series3, series4], axis=1)
#view DataFrame
df
TeamPointsAssistsRebounds
0Mavs1092230
1Rockets1031835
2Spurs981528</b>
<h2><span class="orange">Pandas: How to Find Minimum Value Across Multiple Columns</span></h2>
You can use the following methods to find the minimum value across multiple columns in a pandas DataFrame:
<b>Method 1: Find Minimum Value Across Multiple Columns</b>
<b>df[['col1', 'col2', 'col3']].min(axis=1)
</b>
<b>Method 2: Add New Column Containing Minimum Value Across Multiple Columns</b>
<b>df['new_col'] = df[['col1', 'col2', 'col3']].min(axis=1)</b>
The following examples show how to use each of these methods in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],   'points': [28, 17, 19, 14, 23, 26, 5],   'rebounds': [5, 6, 4, 7, 14, 12, 9],   'assists': [10, 13, 7, 8, 4, 5, 8]})
#view DataFrame
print(df)
  player  points  rebounds  assists
0      A      28         5       10
1      B      17         6       13
2      C      19         4        7
3      D      14         7        8
4      E      23        14        4
5      F      26        12        5
6      G       5         9        8</b>
<h2>Example 1: Find Minimum Value Across Multiple Columns</h2>
The following code shows how to find the minimum value in each row across the points and rebounds columns:
<b>#find minimum value across points and rebounds columns
df[['points', 'rebounds']].min(axis=1)
0     5
1     6
2     4
3     7
4    14
5    12
6     5
dtype: int64
</b>
Here’s how to interpret the output:
The minimum value across the points and rebounds columns for the first row was <b>5</b>.
The minimum value across the points and rebounds columns for the second row was <b>6</b>.
The minimum value across the points and rebounds columns for the third row was <b>4</b>.
And so on.
<h2>Example 2: Add New Column Containing Minimum Value Across Multiple Columns</h2>
The following code shows how to add a new column to the DataFrame that contains the minimum value in each row across the points and rebounds columns:
<b>#add new column that contains min value across points and rebounds columns
df['min_points_rebs'] = df[['points', 'rebounds']].min(axis=1)
#view updated DataFrame
print(df)
  player  points  rebounds  assists  min_points_rebs
0      A      28         5       10                5
1      B      17         6       13                6
2      C      19         4        7                4
3      D      14         7        8                7
4      E      23        14        4               14
5      F      26        12        5               12
6      G       5         9        8                5</b>
The new column titled <b>min_points_rebs</b> now contains the minimum value across the points and rebounds columns for each row in the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Move Column to Front of DataFrame 
 Pandas: How to Check if Column Contains String 
 Pandas: How to Add Empty Column to DataFrame 
<h2><span class="orange">How to Find the Minimum Value by Group in Pandas</span></h2>
You can use the following methods to find the minimum value by group in a pandas DataFrame:
<b>Method 1: Groupby minimum of one column</b>
<b>df.groupby('group_column')['values_column'].min()
</b>
<b>Method 2: Groupby minimum of multiple columns</b>
<b>df.groupby('group_column')['values_column1', 'values_column2'].min()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C'],   'points':[24, 23, 27, 11, 14, 8, 13],   'rebounds': [11, 8, 7, 6, 6, 5, 12]})
#display DataFrame
print(df)
  team  points  rebounds
0    A      24        11
1    A      23         8
2    B      27         7
3    B      11         6
4    B      14         6
5    C       8         5
6    C      13        12</b>
<h2>Example 1: Groupby Minimum of One Column</h2>
The following code shows how to find the minimum value of the <b>points</b> column, grouped by the <b>team</b> column:
<b>#find minimum value of points, grouped by team
df.groupby('team')['points'].min() 
team
A    23
B    11
C     8
Name: points, dtype: int64
</b>
From the output we can see:
The minimum value of points for team A is <b>23</b>.
The minimum value of points for team B is <b>11</b>.
The minimum value of points for team C is <b>8</b>.
<h2>
<b>Example 2: Groupby Minimum of Multiple Columns</b>
</h2>
The following code shows how to find the minimum value of the <b>points</b> and <b>rebounds</b> columns, grouped by the <b>team</b> column:
<b>#find minimum value of points and rebounds, grouped by team
df.groupby('team')[['points', 'rebounds']].min() 
    points  rebounds
team
A23   8
B11   6
C8   5
</b>
From the output we can see:
Team A:
Minimum points: <b>23</b>
Minimum rebounds: <b>8</b>
Team B:
Minimum points: <b>11</b>
Minimum rebounds: <b>6</b>
Team C:
Minimum points: <b>8</b>
Minimum rebounds: <b>5</b>
<b>Note</b>: It’s important that you use double brackets when specifying the value columns, otherwise you may receive an error.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Calculate the Sum of Columns in Pandas 
 How to Calculate the Mean of Columns in Pandas 
 How to Find the Max Value of Columns in Pandas 
<h2><span class="orange">Pandas: How to Move Column to Front of DataFrame</span></h2>
You can use the following methods to move columns to the front of a pandas DataFrame:
<b>Method 1: Move One Column to Front</b>
<b>df = df[['my_col'] + [x for x in df.columns if x != 'my_col']]
</b>
<b>Method 2: Move Multiple Columns to Front</b>
<b>cols_to_move = ['my_col1', 'my_col2']
df = df[cols_to_move + [x for x in df.columns if x not in cols_to_move]]</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
<h2>Example 1: Move One Column to Front</h2>
The following code shows how to move the ‘assists’ column to the front of the DataFrame:
<b>#move 'assists' column to front
df = df[['assists'] + [x for x in df.columns if x != 'assists']]
#view updated DataFrame
print(df)
   assists team  points  rebounds
0        5    A      18        11
1        7    B      22         8
2        7    C      19        10
3        9    D      14         6
4       12    E      14         6
5        9    F      11         5
6        9    G      20         9
7        4    H      28        12</b>
The ‘assists’ column has been moved to the front of the DataFrame and every other column has remained in the same order.
<h2>Example 2: Move Multiple Columns to Front</h2>
The following code shows how to move both the ‘points’ and ‘rebounds’ columns to the front of the DataFrame:
<b>#define columns to move to front
cols_to_move = ['points', 'rebounds']
#move columns to front
df = df[cols_to_move + [x for x in df.columns if x not in cols_to_move]]
#view updated DataFrame
print(df)
   points  rebounds team  assists
0      18        11    A        5
1      22         8    B        7
2      19        10    C        7
3      14         6    D        9
4      14         6    E       12
5      11         5    F        9
6      20         9    G        9
7      28        12    H        4
</b>
The ‘points’ and ‘rebounds’ columns have both been moved to the front of the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Insert a Column Into a Pandas DataFrame 
 How to Drop the Index Column in Pandas 
 How to Combine Two Columns in Pandas 
<h2><span class="orange">Pandas: How to Calculate a Moving Average by Group</span></h2>
You can use the following basic syntax to calculate a moving average by group in pandas:
<b>#calculate 3-period moving average of 'values' by 'group'
df.groupby('group')['values'].transform(lambda x: x.rolling(3, 1).mean())
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Moving Average by Group in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the total sales made by two stores during five sales periods:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'period': [1, 2, 3, 4, 5, 1, 2, 3, 4, 5],   'sales': [7, 7, 9, 13, 14, 13, 13, 19, 20, 26]})
#view DataFrame
df
storeperiodsales
0A17
1A27
2A39
3A413
4A514
5B113
6B213
7B319
8B420
9B526
</b>
We can use the following code to calculate a 3-day moving average of sales for each store:
<b>#calculate 3-day moving average of sales by store
df['ma'] = df.groupby('store')['sales'].transform(lambda x: x.rolling(3, 1).mean())
#view updated DataFrame
df
        storeperiodsalesma
0A177.000000
1A277.000000
2A397.666667
3A4139.666667
4A51412.000000
5B11313.000000
6B21313.000000
7B31915.000000
8B42017.333333
9B52621.666667
</b>
<b>Note</b>: x.rolling(3, 1) means to calculate a <b>3</b>-period moving average and require <b>1</b> as the minimum number of periods.
The ‘ma’ column shows the  3-day moving average of sales for each store.
To calculate a different moving average, simply change the value in the <b>rolling()</b> function.
For example, we could calculate the 2-day moving average of sales for each store instead:
<b>#calculate 2-day moving average of sales by store
df['ma'] = df.groupby('store')['sales'].transform(lambda x: x.rolling(2, 1).mean())
#view updated DataFrame
df
        storeperiodsalesma
0A177.0
1A277.0
2A398.0
3A41311.0
4A51413.5
5B11313.0
6B21313.0
7B31916.0
8B42019.5
9B52623.0
</b>
<h2><span class="orange">How to Multiply Two Columns in Pandas (With Examples)</span></h2>
You can use the following methods to multiply two columns in a pandas DataFrame:
<b>Method 1: Multiply Two Columns</b>
<b>df['new_column'] = df.column1 * df.column2</b>
<b>Method 2: Multiply Two Columns Based on Condition</b>
<b>new_column = df.column1 * df.column2
#update values based on condition
df['new_column'] = new_column.where(df.column2 == 'value1', other=0)
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Multiply Two Columns</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'price': [22, 20, 25, 30, 4, 8, 12, 10],   'amount': [3, 1, 3, 3, 2, 4, 3, 5]})
#view DataFrame
print(df)
   price  amount
0     22       3
1     20       1
2     25       3
3     30       3
4      4       2
5      8       4
6     12       3
7     10       5</b>
We can use the following syntax to multiply the <b>price</b> and <b>amount</b> columns and create a new column called <b>revenue</b>:
<b>#multiply price and amount columns
df['revenue'] = df.price * df.amount
#view updated DataFrame
print(df)
   price  amount  revenue
0     22       3       66
1     20       1       20
2     25       3       75
3     30       3       90
4      4       2        8
5      8       4       32
6     12       3       36
7     10       5       50</b>
Notice that the values in the new <b>revenue </b>column are the product of the values in the <b>price</b> and <b>amount</b> columns.
<h2>Example 2: Multiply Two Columns Based on Condition</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'price': [22, 20, 25, 30, 4, 8, 12, 10],   'amount': [3, 1, 3, 3, 2, 4, 3, 5],   'type': ['Sale', 'Refund', 'Sale', 'Sale',            'Sale', 'Refund', 'Refund', 'Sale']})
#view DataFrame
print(df)
   price  amount    type
0     22       3    Sale
1     20       1  Refund
2     25       3    Sale
3     30       3    Sale
4      4       2    Sale
5      8       4  Refund
6     12       3  Refund
7     10       5    Sale</b>
We can multiply together the <b>price</b> and <b>amount</b> columns and then use the <b>where()</b> function to modify the results based on the value in the <b>type</b> column:
<b>#multiply price and amount columns
revenue = df.price * df.amount
#update values based on type
df['revenue'] = revenue.where(df.type == 'Sale', other=0)
#view updated DataFrame
print(df)
   price  amount    type  revenue
0     22       3    Sale       66
1     20       1  Refund        0
2     25       3    Sale       75
3     30       3    Sale       90
4      4       2    Sale        8
5      8       4  Refund        0
6     12       3  Refund        0
7     10       5    Sale       50</b>
Notice that the <b>revenue </b>column takes on the following values:
The product of price and amount if type is equal to “Sale”
0 otherwise
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Columns by Index in a Pandas DataFrame 
 How to Rename Index in Pandas DataFrame 
 How to Drop Columns by Index in Pandas 
<h2><span class="orange">Pandas: How to Use a mutate() Function Equivalent to R</span></h2>
In the R programming language, we can use the <b>mutate()</b> function from the <b>dplyr</b> package to quickly add new columns to a data frame that are calculated from existing columns.
For example, the following code shows how to calculate the mean value of a specific column in R and add that value as a new column in a data frame:
<b>library(dplyr)
#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'), points=c(30, 22, 19, 14, 14, 11, 20, 28))
#add new column that shows mean points by team
df &lt;- df %>%
      group_by(team) %>%
      mutate(mean_points = mean(points))
#view updated data frame
df
  team  points mean_points           
1 A         30        21.2
2 A         22        21.2
3 A         19        21.2
4 A         14        21.2
5 B         14        18.2
6 B         11        18.2
7 B         20        18.2
8 B         28        18.2
</b>
The equivalent of the <b>mutate()</b> function in pandas is the <b>transform()</b> function.
The following example shows how to use this function in practice.
<h2>Example: Using transform() in pandas to Replicate mutate() in R</h2>
Suppose we have the following pandas DataFrame that shows the points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [30, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team  points
0    A      30
1    A      22
2    A      19
3    A      14
4    B      14
5    B      11
6    B      20
7    B      28
</b>
We can use the <b>transform()</b> function to add a new column called <b>mean_points</b> that shows the mean points scored by each team:
<b>#add new column to DataFrame that shows mean points by team
df['mean_points'] = df.groupby('team')['points'].transform('mean')
#view updated DataFrame
print(df)
  team  points  mean_points
0    A      30        21.25
1    A      22        21.25
2    A      19        21.25
3    A      14        21.25
4    B      14        18.25
5    B      11        18.25
6    B      20        18.25
7    B      28        18.25
</b>
The mean points value for players on team A was <b>21.25</b> and the mean points value for players on team B was <b>18.25</b>, so these values were assigned accordingly to each player in a new column.
Notice that this matches the results we got from using the<b> mutate()</b> function in the introductory example.
It’s worth noting that you can also use <b>lambda</b> to perform some custom calculation within the <b>transform()</b> function.
For example, the following code shows how to use <b>lambda</b> to calculate the percentage of total points scored by each player on their respective teams:
<b>#create new column called percent_of_points
df['percent_of_points'] = df.groupby('team')['points'].transform(lambda x: x/x.sum())
#view updated DataFrame
print(df)
  team  points  percent_of_points
0    A      30           0.352941
1    A      22           0.258824
2    A      19           0.223529
3    A      14           0.164706
4    B      14           0.191781
5    B      11           0.150685
6    B      20           0.273973
7    B      28           0.383562
</b>
Here’s how to interpret the output:
The first player on team A scored 30 out of 85 total points among team A players. Thus, his percentage of total points scored was 30/85 = <b>0.352941</b>.
The second player on team A scored 22 out of 85 total points among team A players. Thus, his percentage of total points scored was 22/85 = <b>0.258824</b>.
And so on.
Note that we can use the <b>lambda</b> argument within the <b>transform()</b> function to perform any custom calculation that we’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform a GroupBy Sum in Pandas 
 How to Use Groupby and Plot in Pandas 
 How to Count Unique Values Using GroupBy in Pandas 
<h2><span class="orange">How to Print Pandas DataFrame with No Index</span></h2>
You can use the following methods to print a pandas DataFrame without the index:
<b>Method 1: Use to_string() Function</b>
<b>print(df.to_string(index=False))
</b>
<b>Method 2: Create Blank Index Before Printing</b>
<b>df.index=[''] * len(df)
print(df)</b>
Both methods will print the DataFrame without the index.
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
Notice that the DataFrame contains an index with values ranging from 0 to 7.
<h2>Example 1: Use to_string() Function</h2>
The following code shows how to use the <b>to_string()</b> function to print the DataFrame without the index:
<b>#print DataFrame without index
print(df.to_string(index=False))
team  points  assists  rebounds
   A      18        5        11
   B      22        7         8
   C      19        7        10
   D      14        9         6
   E      14       12         6
   F      11        9         5
   G      20        9         9
   H      28        4        12
</b>
Notice that all four columns are printed without the index column.
<h2>Example 2: Create Blank Index Before Printing</h2>
The following code shows how to first create an index column with all blank values and then print the DataFrame:
<b>#define index to have all blank values
df.index=[''] * len(df)
#print DataFrame
print(df)
 team  points  assists  rebounds
    A      18        5        11
    B      22        7         8
    C      19        7        10
    D      14        9         6
    E      14       12         6
    F      11        9         5
    G      20        9         9
    H      28        4        12
</b>
Notice that all four columns are printed without the index column.
Also notice that this example matches the DataFrame that we printed in the previous example.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Show All Rows of a Pandas DataFrame 
 How to Flatten MultiIndex in Pandas DataFrame 
 How to Transpose a Pandas DataFrame without Index 
<h2><span class="orange">Pandas: How to Filter for “Not Contains”</span></h2>
You can use the following methods to perform a “Not Contains” filter in a pandas DataFrame:
<b>Method 1: Filter for Rows that Do Not Contain Specific String</b>
<b>filtered_df = df[df['my_column'].str.contains('some_string') == False]
</b>
<b>Method 2: Filter for Rows that Do Not Contain One of Several Specific Strings</b>
<b>filtered_df = df[df['my_column'].str.contains('string1|string2|string3') == False]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Nets', 'Rockets', 'Mavs', 'Spurs', 'Kings', 'Nuggets'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
print(df)
      team  points  assists  rebounds
0     Nets      18        5        11
1  Rockets      22        7         8
2     Mavs      19        7        10
3    Spurs      14        9         6
4    Kings      14       12         6
5  Nuggets      11        9         5
</b>
<h2>Example 1: Filter for Rows that Do Not Contain Specific String</h2>
The following code shows how to filter the pandas DataFrame for rows where the <b>team</b> column does not contain “ets” in the name:
<b>#filter for rows that do not contain 'ets' in the 'team' column
filtered_df = df[df['team'].str.contains('ets') == False]
#view filtered DataFrame
print(filtered_df)
    team  points  assists  rebounds
2   Mavs      19        7        10
3  Spurs      14        9         6
4  Kings      14       12         6</b>
Notice that the resulting DataFrame does not contain any rows where the value in the <b>team</b> column contains “ets” in the name.
In particular, the following teams were filtered out of the DataFrame:
N<b>ets</b>
Rock<b>ets</b>
Nugg<b>ets</b>
Notice that each of these team names contained “ets” in the name.
<h2>Example 2: Filter for Rows that Do Not Contain One of Several Specific Strings</h2>
The following code shows how to filter the pandas DataFrame for rows where the <b>team</b> column does not contain “ets” in the name:
<b>#filter for rows that do not contain 'ets' or 'urs' in the 'team' column
filtered_df = df[df['team'].str.contains('ets|urs') == False]
#view filtered DataFrame
print(filtered_df)
    team  points  assists  rebounds
2   Mavs      19        7        10
4  Kings      14       12         6</b>
Notice that the resulting DataFrame does not contain any rows where the value in the <b>team</b> column contains “ets” or “urs” in the name.
<b>Note</b>: The <b>|</b> operator stands for “OR” in pandas.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common filtering operations in pandas:
 How to Filter a Pandas DataFrame by Column Values 
 How to Filter Pandas DataFrame Rows by Date 
 How to Filter a Pandas DataFrame on Multiple Conditions 
<h2><span class="orange">How to Use “NOT IN” Filter in Pandas (With Examples)</span></h2>
You can use the following syntax to perform a “NOT IN” filter in a pandas DataFrame:
<b>df[~df['col_name'].isin(values_list)]
</b>
Note that the values in <b>values_list</b> can be either numeric values or character values.
The following examples show how to use this syntax in practice.
<h2>Example 1: Perform “NOT IN” Filter with One Column</h2>
The following code shows how to filter a pandas DataFrame for rows where a team name is not in a list of names:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#define list of teams we don't want
values_list = ['A', 'B']
#filter for rows where team name is not in list
df[~df['team'].isin(values_list)]
        teampointsassistsrebounds
6C2599
7C29412
</b>
And the following code shows how to filter a pandas DataFrame for rows where the ‘points’ column does not contain certain values:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#define list of values we don't want
values_list = [12, 15, 25]
#filter for rows where team name is not in list
df[~df['team'].isin(values_list)]
teampointsassistsrebounds
3B1496
4B19126
5B2395
7C29412
</b>
<h2>Example 2: Perform “NOT IN” Filter with Multiple Columns</h2>
The following code shows how to filter a pandas DataFrame for rows where certain team names are not in one of several columns:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'star_team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'backup_team': ['B', 'B', 'C', 'C', 'D', 'D', 'D', 'E'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#define list of teams we don't want
values_list = ['C', 'E']
#filter for rows where team name is not in one of several columns
df[~df[['star_team', 'backup_team']].isin(values_list).any(axis=1)] 
        star_team backup_team  pointsassistsrebounds
0A  B       25511
1A  B       1278
4B  D       19126
5B  D       2395</b>
Notice that we filtered out every row where teams ‘C’ or ‘E’ appeared in either the ‘star_team’ column or the ‘backup_team’ column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common filtering operations in pandas:
 How to Use “Is Not Null” in Pandas 
 How to Filter a Pandas DataFrame by Column Values 
 How to Filter Pandas DataFrame Rows by Date 
 How to Filter a Pandas DataFrame on Multiple Conditions 
<h2><span class="orange">How to Count Number of Rows in Pandas DataFrame</span></h2>
There are three methods you can use to quickly count the number of rows in a pandas DataFrame:
<b>#count number of rows in index column of data frame
len(df.index)
#find length of data frame
len(df)
#find number of rows in data frame
df.shape[0]</b>
Each method will return the exact same answer.
For small datasets, the difference in speed between these three methods is negligible.
For extremely large datasets, it’s recommended to use <b>len(df.index)</b> since this has been shown to be the fastest method.
The following example shows how to use each of these methods in practice.
<h3>Example: Count Number of Rows in Pandas DataFrame</h3>
The following code shows how to use the three methods mentioned earlier to count the number of rows in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'y': [8, 12, 15, 14, 19, 23, 25, 29, 31, 30, 31, 31],   'x1': [5, 7, 7, 9, 12, 9, 9, 4, 5, 4, 7, 7],   'x2': [11, 8, 10, 6, 6, 5, 9, 12, 8, 8, 9, 9],   'x3': [2, 2, 3, 2, 5, 5, 7, 9, 11, 7, 7, 8]})
#view DataFrame
df
yx1x2x3
085112
112782
2157103
314962
4191265
523955
625997
7294129
8315811
930487
1031797
1131798
#count number of rows in index column of data frame
len(df.index)
12
#find length of data frame
len(df)
12
#find number of rows in data frame
df.shape[0]
12</b>
Notice that each method returns the exact same result. The DataFrame has <b>12</b> rows.
<h2><span class="orange">How to Fix in Pandas: Out of bounds nanosecond timestamp</span></h2>
One error you may encounter when using pandas is:
<b>OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2300-01-10 00:00:00
</b>
This error occurs when you attempt to create a timestamp that is outside of the following range:
<b>import pandas as pd
#display minimum timestamp allowed
print(pd.Timestamp.min)
1677-09-21 00:12:43.145224193
#display maximum timestamp allowed 
print(pd.Timestamp.max)
2262-04-11 23:47:16.854775807
</b>
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to create a date range in pandas that contains the following three dates:
1/1/2020
1/1/2150
1/1/2300
We can use the  date_range()  function to attempt to create this date range:
<b>import pandas as pd
#attempt to create date range
some_dates = pd.date_range(start='1/1/2000', end='1/1/2300', periods=3)
OutOfBoundsDatetime: Out of bounds nanosecond timestamp: 2300-01-10 00:00:00
</b>
We receive the <b>OutOfBoundsDatetime</b> error because the timestamp 1/1/2300 is greater than the max nanosecond timestamp allowed by pandas.
Even if you don’t want to store the timestamp using nanoseconds as the unit, pandas will automatically do so.
<h3>How to Fix the Error</h3>
The easiest way to get around this error is to use the <b>errors = ‘coerce’</b> argument, which coerces any timestamps outside of the minimum or maximum range to NaT values.
For example, we can use the following code to create a date range and automatically coerce any timestamps outside of the allowable range to NaT values:
<b>import pandas as pd
#create date range
some_dates = ['1/1/2000', '1/1/2150', '1/1/2300']
#convert date range to datetime and automatically coerce errors
some_dates = pd.to_datetime(some_dates, errors = 'coerce')
#show datetimes
print(some_dates)
DatetimeIndex(['2000-01-01', '2150-01-01', 'NaT'], dtype='datetime64[ns]', freq=None)
</b>
The result is a date range with three datetime values and the last datetime is NaT since it exceeded the max value allowed by pandas.
Notice that we don’t receive any error this time when creating the date range.
<h2><span class="orange">How to Perform an Outer Join in Pandas (With Example)</span></h2>
An <b>outer join</b> is a type of join that returns all rows from two pandas DataFrames.
You can use the following basic syntax to perform an outer join in pandas:
<b>import pandas as pd
df1.merge(df2, on='some_column', how='outer')
</b>
The following example shows how to use this syntax in practice.
<h2>Example: How to Perform an Outer Join in Pandas</h2>
Suppose we have the following two pandas DataFrames that contain information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [18, 22, 19, 14, 14, 11, 20, 28]})
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'J', 'K'],    'assists': [4, 9, 14, 13, 10, 8]})
#view DataFrames
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
4    E      14
5    F      11
6    G      20
7    H      28
print(df2)
  team  assists
0    A        4
1    B        9
2    C       14
3    D       13
4    J       10
5    K        8</b>
We can use the following code to perform an outer join, matching the rows between the DataFrames based on the values in the <b>team</b> column and keeping all rows from both DataFrames:
<b>#perform outer join
df1.merge(df2, on='team', how='outer')
        teampointsassists
0A18.04.0
1B22.09.0
2C19.014.0
3D14.013.0
4E14.0NaN
5F11.0NaN
6G20.0NaN
7H28.0NaN
8JNaN10.0
9KNaN8.0
</b>
The result is a DataFrame that contains all rows from each DataFrame.
Notice that <b>NaN</b> values have been filled in for each row where the value in the <b>team</b> column did not exist in both DataFrames.
<b>Note</b>: You can find the complete documentation for the <b>merge</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform an Anti-Join in Pandas 
 How to Perform an Inner Join in Pandas 
 How to Perform a Cross Join in Pandas 
<h2><span class="orange">How to Calculate Percent Change in Pandas</span></h2>
You can use the <b>pct_change()</b> function to calculate the percent change between values in pandas:
<b>#calculate percent change between values in pandas Series
s.pct_change()
#calculate percent change between rows in pandas DataFrame
df['column_name'].pct_change()
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Percent Change in pandas Series</h3>
The following code shows how to calculate percent change between values in a pandas Series:
<b>import pandas as pd
#create pandas Series
s = pd.Series([6, 14, 12, 18, 19])
#calculate percent change between consecutive values
s.pct_change() 
0         NaN
1    1.333333
2   -0.142857
3    0.500000
4    0.055556
dtype: float64
</b>
Here’s how these values were calculated:
Index 1: (14 – 6) / 6 = <b>1.333333</b>
Index 2: (12 – 14) / 14 = <b>-.142857</b>
Index 3: (18 – 12) / 12 = <b>0.5</b>
Index 4: (19 – 18) / 18 = <b>.055556</b>
Note that you can also use the <b>periods</b> argument to calculate the percent change between values at different intervals:
<b>import pandas as pd
#create pandas Series
s = pd.Series([6, 14, 12, 18, 19])
#calculate percent change between values 2 positions apart
s.pct_change(periods=2) 
0         NaN
1         NaN
2    1.000000
3    0.285714
4    0.583333
dtype: float64</b>
Here’s how these values were calculated:
Index 2: (12 – 6) / 6 = <b>1.000000</b>
Index 3: (18 – 14) / 14 = <b>0.285714</b>
Index 4: (19 – 12) / 12 = <b>.583333</b>
<h3>Example 2: Percent Change in pandas DataFrame</h3>
The following code shows how to calculate the percent change between consecutive rows in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'period': [1, 2, 3, 4, 5],   'sales': [6, 7, 7, 9, 12]}) 
#view DataFrame
df
        periodsales
016
127
237
349
4512
#calculate percent change between consecutive values in 'sales' column
df['sales_pct_change'] = df['sales'].pct_change()
#view updated DataFrame
df
periodsalessales_pct_change
016NaN
1270.166667
2370.000000
3490.285714
45120.333333
</b>
Here is how these values were calculated:
Index 1: (7 – 6) / 6 = <b>.166667</b>
Index 2: (7 – 7) / 7 = <b>0.000000</b>
Index 3: (9 – 7) / 7 = <b>.285714</b>
Index 4: (12 – 9) / 9 = <b>.333333</b>
You can find the complete documentation for the <b>pct_change()</b> function  here .
<h2><span class="orange">How to Calculate Percentile Rank in Pandas (With Examples)</span></h2>
The <b>percentile rank</b> of a value tells us the percentage of values in a dataset that rank equal to or below a given value.
You can use the following methods to calculate percentile rank in pandas:
<b>Method 1: Calculate Percentile Rank for Column</b>
<b>df['percent_rank'] = df['some_column'].rank(pct=True)
</b>
<b>Method 2: Calculate Percentile Rank by Group</b>
<b>df['percent_rank'] = df.groupby('group_var')['value_var'].transform('rank', pct=True)
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'A', 'A',            'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'points': [2, 5, 5, 7, 9, 13, 15, 17, 22, 24, 30, 31, 38, 39]})
#view DataFrame
print(df)
   team  points
0     A       2
1     A       5
2     A       5
3     A       7
4     A       9
5     A      13
6     A      15
7     B      17
8     B      22
9     B      24
10    B      30
11    B      31
12    B      38
13    B      39
</b>
<h2>Example 1: Calculate Percentile Rank for Column</h2>
The following code shows how to calculate the percentile rank of each value in the points column:
<b>#add new column that shows percentile rank of points
df['percent_rank'] = df['points'].rank(pct=True)
#view updated DataFrame
print(df)
   team  points  percent_rank
0     A       2      0.071429
1     A       5      0.178571
2     A       5      0.178571
3     A       7      0.285714
4     A       9      0.357143
5     A      13      0.428571
6     A      15      0.500000
7     B      17      0.571429
8     B      22      0.642857
9     B      24      0.714286
10    B      30      0.785714
11    B      31      0.857143
12    B      38      0.928571
13    B      39      1.000000
</b>
Here’s how to interpret the values in the <b>percent_rank</b> column:
<b>7.14%</b> of the points values are equal to or less than 2.
<b>17.86%</b> of the points values are equal to or less than 5.
<b>28.57%</b> of the points values are equal to or less than 7.
And so on.
<h2>Example 2: Calculate Percentile Rank by Group</h2>
The following code shows how to calculate the percentile rank of each value in the points column, grouped by team:
<b>#add new column that shows percentile rank of points, grouped by team
df['percent_rank'] = df.groupby('team')['points'].transform('rank', pct=True)
#view updated DataFrame
print(df)
   team  points  percent_rank
0     A       2      0.142857
1     A       5      0.357143
2     A       5      0.357143
3     A       7      0.571429
4     A       9      0.714286
5     A      13      0.857143
6     A      15      1.000000
7     B      17      0.142857
8     B      22      0.285714
9     B      24      0.428571
10    B      30      0.571429
11    B      31      0.714286
12    B      38      0.857143
13    B      39      1.000000
</b>
Here’s how to interpret the values in the <b>percent_rank</b> column:
<b>14.3%</b> of the points values for team A are equal to or less than 2.
<b>35.7%</b> of the points values for team A are equal to or less than 5.
<b>57.1%</b> of the points values for team A are equal to or less than 7.
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Calculate Percent Change in Pandas 
 How to Calculate Cumulative Percentage in Pandas 
 How to Calculate Percentage of Total Within Group in Pandas 
<h2><span class="orange">How to Create Pie Chart from Pandas DataFrame</span></h2>
You can use the following basic syntax to create a pie chart from a pandas DataFrame:
<b>df.groupby(['group_column']).sum().plot(kind='pie', y='value_column')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Create Basic Pie Chart</h3>
Suppose we have the following two pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 25, 14, 19, 53, 25, 29]})
#view DataFrame
print(df)
  team  points
0    A      25
1    A      12
2    B      25
3    B      14
4    B      19
5    B      53
6    C      25
7    C      29</b>
We can use the following syntax to create a pie chart that displays the portion of total points scored by each team:
<b>df.groupby(['team']).sum().plot(kind='pie', y='points')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/pie1.png">
<h3>Example 2: Create Custom Pie Chart</h3>
We can use the following arguments to customize the appearance of the pie chart:
<b>autopct</b>: Display percentages in pie chart
<b>colors</b>: Specify colors to use in pie chart
<b>title</b>: Add title to pie chart
The following code shows how to use these arguments in practice:
<b>df.groupby(['team']).sum().plot(kind='pie', y='points', autopct='%1.0f%%',                colors = ['red', 'pink', 'steelblue'],                title='Points Scored by Team'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/pie2.png">
Note that the colors will be assigned to the categories as they appear in the DataFrame.
For example, team ‘A’ appears first in the DataFrame, which is why it received the color ‘red’ in the pie chart.
<h2><span class="orange">Pandas: How to Modify Column Names in Pivot Table</span></h2>
Often you may want to modify or format the column names in a pandas pivot table in a specific way.
Fortunately this is easy to do using built-in functions in pandas.
The following example shows how to do so.
<h3>Example: Modify Column Names in Pandas Pivot Table</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'G', 'C', 'F', 'F'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
print(df)
teamposition points
0AG 4
1AG 4
2AF 6
3AC 8
4BG 9
5BC 5
6BF 5
7BF 12</b>
We can use the following code to create a pivot table in pandas that shows the mean value of <b>points</b> for each <b>team</b> and <b>position</b> in the DataFrame:
<b>#create pivot table
piv = pd.pivot_table(df, values='points', index='team', columns='position')
#view pivot table
print(piv)
position    C    F    G
team                   
A         8.0  6.0  4.0
B         5.0  8.5  9.0</b>
Now suppose we would like to get rid of the word <b>position</b> in the pivot table and remove the extra <b>team</b> row from the pivot table.
We can use the following syntax to do so:
<b>#format column names
piv.columns = ['_'.join(str(s).strip() for s in col if s) for col in piv.columns]
#reset index
piv.reset_index(inplace=True)
#view updated pivot table
print(piv)
  team    C    F    G
0    A  8.0  6.0  4.0
1    B  5.0  8.5  9.0
</b>
Notice that we were able to get rid of the word <b>position</b> in the pivot table and remove the extra <b>team</b> row from the pivot table.
Also note that this general solution will work for a pivot table with a MultiIndex as well.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2><span class="orange">Pandas: How to Create Pivot Table with Count of Values</span></h2>
You can use one of the following methods to create a pivot table in pandas that displays the counts of values in certain columns:
<b>Method 1: Pivot Table With Counts</b>
<b>pd.pivot_table(df, values='col1', index='col2', columns='col3',
               aggfunc='count')
</b>
<b>Method 2: Pivot Table With Unique Counts</b>
<b>pd.pivot_table(df, values='col1', index='col2', columns='col3',
               aggfunc=pd.Series.nunique)</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'G', 'F', 'F', 'F'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
df
teamposition points
0AG 4
1AG 4
2AF 6
3AC 8
4BG 9
5BF 5
6BF 5
7BF 12</b>
<h3>Method 1: Create Pandas Pivot Table With Counts</h3>
The following code shows how to create a pivot table in pandas that shows the total count of ‘points’ values for each ‘team’ and ‘position’ in the DataFrame:
<b>#create pivot table
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position',          aggfunc='count')
#view pivot table
df_pivot
positionC  F  G
team
   A      1.01.02.0
   B      NaN3.01.0</b>
From the output we can see:
There is <b>1</b> value in the ‘points’ column for team A at position C.
There is <b>1</b> value in the ‘points’ column for team A at position F.
There are <b>2</b> values in the ‘points’ column for team A at position G.
And so on.
<h3>Method 2: Create Pandas Pivot Table With Unique Counts</h3>
The following code shows how to create a pivot table in pandas that shows the total unique number of ‘points’ values for each ‘team’ and ‘position’ in the DataFrame:
<b>#create pivot table
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position',          aggfunc=pd.Series.nunique)
#view pivot table
df_pivot
positionC  F  G
team
   A      1.01.01.0
   B      NaN2.01.0</b>
From the output we can see:
There is <b>1</b> unique value in the ‘points’ column for team A at position C.
There is <b>1</b> unique value in the ‘points’ column for team A at position F.
There is <b>1</b> unique value in the ‘points’ column for team A at position G.
And so on.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2><span class="orange">Pandas: How to Add Filter to Pivot Table</span></h2>
You can use the following basic syntax to add a filtering condition to a pandas pivot table:
<b>df[df.col1 == 'A'].pivot_table(index='col1', values=['col2', 'col3'], aggfunc='sum')
</b>
This particular example creates a pivot table that displays the sum of values in <b>col2</b> and <b>col3</b>, grouped by <b>col1</b>.
The filter before the <b>pivot_table()</b> function specifies that we only want to include rows where the value in <b>col1</b> of the original DataFrame has a value of ‘A’.
The following example shows how to use this syntax in practice.
<h2>Example: How to Add Filter to Pandas Pivot Table</h2>
 Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B',            'B', 'B', 'C', 'C', 'C', 'C'],   'points': [4, 4, 2, 8, 9, 5, 5, 7, 8, 8, 4, 3],   'assists': [2, 2, 5, 5, 4, 7, 5, 3, 9, 8, 4, 4]})
#view DataFrame
print(df)
   team  points  assists
0     A       4        2
1     A       4        2
2     A       2        5
3     A       8        5
4     B       9        4
5     B       5        7
6     B       5        5
7     B       7        3
8     C       8        9
9     C       8        8
10    C       4        4
11    C       3        4
</b>
We can use the following code to create a pivot table in pandas that shows the sum of the values in the <b>points</b> and <b>assists</b> columns grouped by <b>team</b> only for the rows where the original DataFrame has a value in the <b>team</b> column equal to ‘A’:
<b>#create pivot table for rows where team is equal to 'A'
df[df.team == 'A'].pivot_table(index='team', values=['points', 'assists'],                 aggfunc='sum')
        assistspoints
team
A1418</b>
Notice that the pivot table only summarizes the values in the <b>points</b> and <b>assists</b> columns for the rows where the <b>team</b> is equal to ‘A’.
You can also use the operators <b>&</b> and <b>|</b> to apply a filter that uses “AND” or “OR” logic.
For example, we can use the following syntax to create a pivot table that filters for rows where the value in the <b>team</b> column of the original DataFrame is equal to ‘A’ or ‘B’:
<b>#create pivot table for rows where team is equal to 'A' or 'B'
df[(df.team == 'A') | (df.team == 'B')].pivot_table(index='team',                                    values=['points', 'assists'],                                    aggfunc='sum')
assistspoints
team
A1418
B1926</b>
Notice that the pivot table only summarizes the values in the <b>points</b> and <b>assists</b> columns for the rows where the <b>team</b> is equal to ‘A’ or ‘B’.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Sort Pivot Table by Values in Column 
 Pandas: How to Create Pivot Table with Sum of Values 
 Pandas: How to Add Subtotals to Pivot Table 
 Pandas: How to Modify Column Names in Pivot Table 
<h2><span class="orange">Pandas: Create Pivot Table with Multiple aggfunc</span></h2>
You can use the following syntax to create a pivot table in pandas and provide multiple values to the <b>aggfunc</b> argument:
<b>df.pivot_table(index='col1', values='col2', aggfunc=('sum', 'mean'))
</b>
This particular example creates a pivot table that displays the sum and the mean of values in <b>col2</b>, grouped by <b>col1</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Create Pandas Pivot Table with Multiple aggfunc</h2>
 Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B',            'B', 'B', 'C', 'C', 'C', 'C'],   'points': [4, 4, 2, 8, 9, 5, 5, 7, 8, 8, 4, 3],   'assists': [2, 2, 5, 5, 4, 7, 5, 3, 9, 8, 4, 4]})
#view DataFrame
print(df)
   team  points  assists
0     A       4        2
1     A       4        2
2     A       2        5
3     A       8        5
4     B       9        4
5     B       5        7
6     B       5        5
7     B       7        3
8     C       8        9
9     C       8        8
10    C       4        4
11    C       3        4
</b>
We can use the following code to create a pivot table that summarizes both the sum and the mean number of <b>points</b> scored by each <b>team</b>:
<b>#create pivot table to summarize sum and mean of points by team
df.pivot_table(index='team', values='points', aggfunc=('sum', 'mean'))
meansum
team
A4.5018
B6.5026
C5.7523
</b>
The resulting pivot table summarizes the mean and the sum of the points scored by each team.
For example, we can see:
Players on team <b>A</b> had a mean points value of <b>4.50</b> and a sum points value of <b>18</b>.
Players on team <b>B</b> had a mean points value of <b>6.50</b> and a sum points value of <b>26</b>.
Players on team <b>C</b> had a mean points value of <b>5.75</b> and a sum points value of <b>23</b>.
Note that we aggregated using the sum and the mean in this example, but we could also aggregate by other metrics such as:
count
min
max
median
std (standard deviation)
The following example shows how to aggregate the values in the <b>points</b> column by these metrics for each team:
<b>#create pivot table to summarize several metrics for points by team
df.pivot_table(index='team', values='points',
               aggfunc=('count', 'min', 'max', 'median', 'std'))
        countmaxmedianminstd
team
A484.022.516611
B496.051.914854
C486.032.629956
</b>
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Sort Pivot Table by Values in Column 
 Pandas: How to Create Pivot Table with Sum of Values 
 Pandas: How to Add Subtotals to Pivot Table 
 Pandas: How to Modify Column Names in Pivot Table 
<h2><span class="orange">Pandas: How to Remove MultiIndex in Pivot Table</span></h2>
To remove a multiIndex from a pandas pivot table, you can use the <b>values</b> argument along with the <b>reset_index()</b> function:
<b>pd.pivot_table(df, index='col1', columns='col2', values='col3').reset_index()
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Remove MultiIndex in Pandas Pivot Table</h2>
 Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'F', 'F', 'F'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
print(df)
  team position  points
0    A        G       4
1    A        G       4
2    A        F       6
3    A        F       8
4    B        G       9
5    B        F       5
6    B        F       5
7    B        F      12</b>
Now suppose we create the following pivot table to summarize the mean value of <b>points</b> by <b>team</b> and <b>position</b>:
<b>#create pivot table to summarize mean points by team and position
pd.pivot_table(df, index='team', columns='position')
        points
positionF    G
team
A7.000000   4.0
B7.333333   9.0
</b>
The resulting pivot table summarizes the mean value of <b>points</b> by <b>team</b> and <b>position</b>, but contains a multiIndex.
To remove the multiIndex, we can use the <b>values</b> argument within the<b> pivot_table()</b> function and add <b>reset_index()</b> to the end:
<b>#create pivot table to summarize mean points by team and position
pd.pivot_table(df, index='team', columns='position', values='points').reset_index()
position  teamF  G
0  A7.000000  4.0
1  B7.333333  9.0
</b>
The resulting pivot table summarizes the mean value of <b>points</b> by <b>team</b> and <b>position</b> and no longer has a multiIndex.
Note that the <b>pivot_table()</b> function calculates the mean value by default.
To calculate a different metric, such as the sum, use the <b>aggfunc</b> argument as follows:
<b>#create pivot table to summarize sum of points by team and position
pd.pivot_table(df, index='team', columns='position', values='points',
               aggfunc='sum').reset_index()
position  teamF   G
0  A14  8
1  B22  9</b>
The resulting pivot table summarizes the sum of values of <b>points</b> by <b>team</b> and <b>position</b> and also has no multiIndex.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Add Filter to Pivot Table 
 Pandas: How to Sort Pivot Table by Values in Column 
 Pandas: How to Add Subtotals to Pivot Table 
<h2><span class="orange">Pandas: How to Replace NaN Values in Pivot Table with Zeros</span></h2>
You can use the <b>fill_value</b> argument in pandas to replace NaN values in a pivot table with zeros instead.
You can use the following basic syntax to do so:
<b>pd.pivot_table(df, values='col1', index='col2', columns='col3', fill_value=0)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Replace NaN Values in Pivot Table with Zeros</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'F', 'F', 'F', 'F'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
print(df)
teamposition points
0AG 4
1AG 4
2AF 6
3AC 8
4BF 9
5BF 5
6BF 5
7BF 12</b>
We can use the following code to create a pivot table in pandas that shows the mean value of <b>points</b> for each <b>team</b> and <b>position</b> in the DataFrame:
<b>#create pivot table
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position')
#view pivot table
print(df_pivot)
position    C     F    G
team                    
A         8.0  6.00  4.0
B         NaN  7.75  NaN</b>
Notice that there are two NaN values in the pivot table because there are no players who have a position of <b>C</b> or <b>G</b> on team B in the original DataFrame, so both of these positions have NaN values in the pivot table.
To fill in these NaN values with zeros in the pivot table, we can use the <b>fill_value</b> argument:
<b>#create pivot table with zeros instead of NaN values
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position',          fill_value=0)
#view pivot table
print(df_pivot)
position  C     F  G
team                
A         8  6.00  4
B         0  7.75  0</b>
Notice that each of the NaN values from the previous pivot table have been filled with zeros.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2><span class="orange">Pandas: How to Sort Pivot Table by Values in Column</span></h2>
You can use the following basic syntax to sort a pandas pivot table based on the values in a column:
<b>my_pivot_table.sort_values(by=['some_column'], ascending=False)
</b>
This particular example sorts the values in a pivot table called <b>my_pivot_table</b> based on the values in <b>some_column</b> in descending order.
The following example shows how to use this syntax in practice.
<h2>Example: Sort Pandas Pivot Table by Values in Column</h2>
 Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B',            'B', 'B', 'C', 'C', 'C', 'C'],   'points': [4, 4, 2, 8, 9, 5, 5, 7, 8, 8, 4, 3],   'assists': [2, 2, 5, 5, 4, 7, 5, 3, 9, 8, 4, 4]})
#view DataFrame
print(df)
   team  points  assists
0     A       4        2
1     A       4        2
2     A       2        5
3     A       8        5
4     B       9        4
5     B       5        7
6     B       5        5
7     B       7        3
8     C       8        9
9     C       8        8
10    C       4        4
11    C       3        4
</b>
We can use the following code to create a pivot table in pandas that shows the sum of the values in the <b>points</b> and <b>assists</b> columns for each team:
<b>#create pivot table
df_pivot = df.pivot_table(index=['team'], values=['points', 'assists'], aggfunc='sum')
#view pivot table
print(df_pivot)
      assists  points
team                 
A          14      18
B          19      26
C          25      23
</b>
By default, pandas sorts the rows of the pivot table in alphabetical order based on the value in the index column, which happens to be the <b>team</b> column.
However, we can use the <b>sort_values()</b> function to instead sort the rows of the pivot table based on the values in the <b>points</b> column:
<b>#sort pivot table by value in 'points' column in descending order
sorted_df_pivot = df_pivot.sort_values(by=['points'], ascending=False)
#view sorted pivot table
print(sorted_df_pivot)
      assists  points
team                 
B          19      26
C          25      23
A          14      18
</b>
Notice that the rows of the pivot table are now sorted based on the values in the <b>points</b> column.
Note that if you leave off the <b>ascending=False</b> argument, the rows will be sorted by the values in the <b>points</b> column in ascending order instead:
<b>#sort pivot table by value in 'points' column in ascending order
sorted_df_pivot = df_pivot.sort_values(by=['points'])
#view sorted pivot table
print(sorted_df_pivot)
      assists  points
team                 
A          14      18
C          25      23
B          19      26
</b>
Notice that the rows in the pivot table are now sorted by the values in the <b>points</b> column in ascending (smallest to largest) order.
<b>Note #1:</b> You can also sort by multiple columns in the pivot table by passing multiple values to the <b>by</b> argument within the <b>sort_values()</b> function.
<b>Note #2</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Reshape DataFrame from Long to Wide 
 Pandas: How to Reshape DataFrame from Wide to Long 
 Pandas: How to Group and Aggregate by Multiple Columns 
<h2><span class="orange">Pandas: How to Add Subtotals to Pivot Table</span></h2>
Often you may want to add subtotals to a pandas pivot table.
Fortunately this is easy to do using built-in functions in pandas.
The following example shows how to do so.
<h2>Example: Add Subtotals to Pandas Pivot Table</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'F', 'F', 'F'],   'all_star': ['Y', 'N', 'Y', 'Y', 'N', 'N', 'N', 'Y'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
print(df)
  team position all_star  points
0    A        G        Y       4
1    A        G        N       4
2    A        F        Y       6
3    A        F        Y       8
4    B        G        N       9
5    B        F        N       5
6    B        F        N       5
7    B        F        Y      12</b>
We can use the following code to create a pivot table in pandas that shows the sum of <b>points</b> for each combination of <b>team</b>, <b>all_star</b>, and <b>position</b> in the DataFrame:
<b>#create pivot table
my_table = pd.pivot_table(df, values='points',              index=['team', 'all_star'],              columns='position',              aggfunc='sum')
#view pivot table
print(my_table)
position          F    G
team all_star           
A    N          NaN  4.0
     Y         14.0  4.0
B    N         10.0  9.0
     Y         12.0  NaN</b>
Now suppose we would like to add a <b>subtotals</b> row that shows the subtotal of points for each team and position.
We can use the following syntax to do so:
<b>#add subtotals row to pivot table
pd.concat([
    y.append(y.sum().rename((x, 'Total')))
    for x, y in my_table.groupby(level=0)
]).append(my_table.sum().rename(('Grand', 'Total')))
positionFG
teamall_star
A       NNaN4.0
               Y7.04.0
           Total7.08.0
B       N5.09.0
               Y12.0NaN
           Total17.09.0
Grand   Total24.017.0
</b>
We now have two subtotal rows that show the subtotal of points for each team and position, along with a grand total row that shows the grand total of each column.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Create Pivot Table with Count of Values 
 Pandas: How to Replace NaN Values in Pivot Table with Zeros 
 Pandas: How to Convert Pivot Table to DataFrame 
<h2><span class="orange">Pandas: How to Create Pivot Table with Sum of Values</span></h2>
You can use the following basic syntax to create a pivot table in pandas that displays the sum of values in certain columns:
<b>pd.pivot_table(df, values='col1', index='col2', columns='col3', aggfunc='sum')
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Pandas Pivot Table With Sum of Values</h2>
 Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'F', 'F', 'F'],   'points': [4, 4, 6, 8, 9, 5, 5, 12]})
#view DataFrame
print(df)
  team position  points
0    A        G       4
1    A        G       4
2    A        F       6
3    A        F       8
4    B        G       9
5    B        F       5
6    B        F       5
7    B        F      12</b>
The following code shows how to create a pivot table in pandas that shows the sum of ‘points’ values for each ‘team’ and ‘position’ in the DataFrame:
<b>#create pivot table
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position',          aggfunc='sum')
#view pivot table
print(df_pivot)
position   F  G
team           
A         14  8
B         22  9</b>
From the output we can see:
Players on team A in position F scored a total of <b>14</b> points.
Players on team A in position G scored a total of <b>8 </b>points.
Players on team B in position F scored a total of <b>22 </b>points.
Players on team B in position G scored a total of <b>9 </b>points.
Note that we can also use the <b>margins</b> argument to display the margin sums in the pivot table:
<b>#create pivot table with margins
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position',          aggfunc='sum', margins=True, margins_name='Sum')
#view pivot table
print(df_pivot)
position   F   G  Sum
team                 
A         14   8   22
B         22   9   31
Sum       36  17   53</b>
The pivot table now displays the row sums and column sums.
<b>Note</b>: You can find the complete documentation for the pandas <b>pivot_table()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Reshape DataFrame from Long to Wide 
 Pandas: How to Reshape DataFrame from Wide to Long 
 Pandas: How to Group and Aggregate by Multiple Columns 
<h2><span class="orange">How to Convert Pandas Pivot Table to DataFrame</span></h2>
You can use the following syntax to convert a pandas pivot table to a pandas DataFrame:
<b>df = pivot_name.reset_index()</b>
The following example shows how to use this syntax in practice.
<h3>Example: Convert Pivot Table to DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'points': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition points
0AG 11
1AG 8
2AF 10
3AF 6
4BG 6
5BG 5
6BF 9
7BF 12
</b>
We can use the following code to create a pivot table that displays the mean points scored by team and position:
<b>#create pivot table
df_pivot = pd.pivot_table(df, values='points', index='team', columns='position')
#view pivot table
df_pivot
positionF  G
team
A      8.09.5
B     10.55.5</b>
We can then use the <b>reset_index()</b> function to convert this pivot table to a pandas DataFrame:
<b>#convert pivot table to DataFrame
df2 = df_pivot.reset_index()
#view DataFrame
df2
teamFG
0A8.09.5
1B10.55.5
</b>
The result is a pandas DataFrame with two rows and three columns.
We can also use the following syntax to  rename the columns  of the DataFrame:
<b>#convert pivot table to DataFrame
df2.columns = ['team', 'Forward_Pts', 'Guard_Pts']
#view updated DataFrame
df2
        teamForward_Pts  Guard_Pts
0A8.0     9.5
1B10.5     5.5</b>
<h2><span class="orange">How to Add Axis Labels to Plots in Pandas (With Examples)</span></h2>
You can use the following basic syntax to add axis labels to a plot in pandas:
<b>df.plot(xlabel='X-Axis Label', ylabel='Y-Axis Label')
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Add Axis Labels to Plot in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the total sales made at three stores during consecutive days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store1_sales': [4, 7, 9, 12, 10, 14, 16, 19, 22, 25],   'store2_sales': [3, 3, 4, 6, 7, 6, 8, 10, 14, 19],   'store3_sales': [2, 2, 4, 2, 5, 5, 6, 8, 8, 11]})
#view DataFrame
print(df)
   store1_sales  store2_sales  store3_sales
0             4             3             2
1             7             3             2
2             9             4             4
3            12             6             2
4            10             7             5
5            14             6             5
6            16             8             6
7            19            10             8
8            22            14             8
9            25            19            11</b>
If we create a plot to visualize the sales by store, the pandas <b>plot()</b> function will not add axis labels to the plot by default:
<b>#plot sales by store
df.plot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/axis1.jpg"508">
To add axis labels, we must use the <b>xlabel</b> and <b>ylabel</b> arguments in the <b>plot()</b> function:
<b>#plot sales by store, add axis labels
df.plot(xlabel='Day', ylabel='Sales')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/axis2.jpg">
Notice that the x-axis and y-axis now have the labels that we specified within the <b>plot()</b> function.
Note that you don’t have to use both the <b>xlabel</b> and <b>ylabel</b> arguments.
For example, you may choose to only add a label to the y-axis:
<b>#plot sales by store, add label to y-axis only
df.plot(ylabel='Sales')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/axis3.jpg"533">
Notice that a label has been placed on the y-axis but no label has been placed on the x-axis since we didn’t use the <b>xlabel</b> argument.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create Pie Chart from Pandas DataFrame 
 How to Make a Scatterplot From Pandas DataFrame 
 How to Create a Histogram from Pandas DataFrame 
<h2><span class="orange">How to Plot Distribution of Column Values in Pandas</span></h2>
You can use the following methods to plot a distribution of column values in a pandas DataFrame:
<b>Method 1: Plot Distribution of Values in One Column</b>
<b>df['my_column'].plot(kind='kde')
</b>
<b>Method 2: Plot Distribution of Values in One Column, Grouped by Another Column</b>
<b>df.groupby('group_column')['values_column'].plot(kind='kde')</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',            'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'points': [3, 3, 4, 5, 4, 7, 7, 7, 10, 11,               8, 7, 8, 9, 12, 12, 12, 14, 15, 17]})
#view DataFrame
print(df)
   team  points
0     A       3
1     A       3
2     A       4
3     A       5
4     A       4
5     A       7
6     A       7
7     A       7
8     A      10
9     A      11
10    B       8
11    B       7
12    B       8
13    B       9
14    B      12
15    B      12
16    B      12
17    B      14
18    B      15
19    B      17</b>
<h2>Example 1: Plot Distribution of Values in One Column</h2>
The following code shows how to plot the distribution of values in the <b>points</b> column:
<b>#plot distribution of values in points column
df['points'].plot(kind='kde')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/kde1.jpg"514">
Note that<b> kind=’kde’</b> tells pandas to use <b>kernel density estimation</b>, which produces a smooth curve that summarizes the distribution of values for a variable.
If you’d like to create a histogram instead, you can specify <b>kind=’hist’</b> as follows:
<b>#plot distribution of values in points column using histogram
df['points'].plot(kind='hist', edgecolor='black')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/kde2.jpg"504">
This method uses bars to represent frequencies of values in the <b>points</b> column as opposed to a smooth line that summarizes the shape of the distribution.
<h2>Example 2: Plot Distribution of Values in One Column, Grouped by Another Column</h2>
The following code shows how to plot the distribution of values in the <b>points</b> column, grouped by the <b>team</b> column:
<b>import matplotlib.pyplot as plt
#plot distribution of points by team 
df.groupby('team')['points'].plot(kind='kde')
#add legend
plt.legend(['A', 'B'], title='Team')
#add x-axis label
plt.xlabel('Points')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/kde3.jpg"526">
The blue line shows the distribution of points for players on team A while the orange line shows the distribution of points for players on team B.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Add Titles to Plots in Pandas 
 How to Adjust the Figure Size of a Pandas Plot 
 How to Plot Multiple Pandas DataFrames in Subplots 
 How to Create and Customize Plot Legends in Pandas 
<h2><span class="orange">How to Use Index in Pandas Plot (With Examples)</span></h2>
You can use one of the following methods to use the values in the index of a pandas DataFrame as the x-axis values in a plot:
<b>Method 1: Use plot()</b>
<b>df.plot(y='my_column')
</b>
If you don’t specify a variable to use for the x-axis then pandas will use the index values by default.
<b>Method 2: Use plot() with use_index=True</b>
<b>df.plot(y='my_column', use_index=True)</b>
The <b>use_index=True</b> argument explicitly tells pandas to use the index values for the x-axis.
Both of these methods will produce the same result.
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DatFrame
df = pd.DataFrame({'sales': [8, 8, 9, 12, 13, 14, 22, 26, 25, 22]},   index=pd.date_range('1/1/2020', periods=10, freq='m'))
#view DataFrame
print(df)
            sales
2020-01-31      8
2020-02-29      8
2020-03-31      9
2020-04-30     12
2020-05-31     13
2020-06-30     14
2020-07-31     22
2020-08-31     26
2020-09-30     25
2020-10-31     22</b>
<h2>Example 1: Use plot()</h2>
The following code shows how to use the<b> plot(</b>) function in pandas to create a line chart that uses the index values in the DataFrame as the x-axis and the values in the <b>sales</b> column as the y-axis values:
<b>#create line chart and use index values as x-axis values
df.plot(y='sales')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/index11.jpg">
Notice that the plot automatically uses the dates in the index of the DataFrame as the values on the x-axis of the line chart.
Since we didn’t specify a variable to use on the x-axis, pandas used the index values by default.
<h2>Example 2: Use plot() with use_index=True</h2>
The following code shows how to use the<b> plot(</b>) function with the argument <b>use_index=True</b> to create a line chart that uses the index values in the DataFrame as the x-axis and the values in the <b>sales</b> column as the y-axis values:
<b>#create line chart and use index values as x-axis values
df.plot(y='sales', use_index=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/index11.jpg">
Once again the plot uses the dates in the index of the DataFrame as the values on the x-axis of the line chart.
Notice that this chart matches the previous chart.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Add Titles to Plots 
 Pandas: How to Create Plot Legends 
 Pandas: How to Create Bar Plot from GroupBy 
<h2><span class="orange">Pandas: How to Create and Customize Plot Legends</span></h2>
You can use the following basic syntax to add a legend to a plot in pandas:
<b>plt.legend(['A', 'B', 'C', 'D'], loc='center left', title='Legend Title')
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Create and Customize Plot Legend in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A':7, 'B':12, 'C':15, 'D':17}, index=['Values'])
</b>
We can use the following syntax to create a bar chart to visualize the values in the DataFrame and add a legend with custom labels:
<b>import matplotlib.pyplot as plt
#create bar chart
df.plot(kind='bar')
#add legend to bar chart
plt.legend(['A Label', 'B Label', 'C Label', 'D Label'])
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/leg1.jpg"523">
We can also use the <b>loc</b> argument and the <b>title</b> argument to modify the location and the title of the legend:
<b>import matplotlib.pyplot as plt
#create bar chart
df.plot(kind='bar')
#add custom legend to bar chart
plt.legend(['A Label', 'B Label', 'C Label', 'D Label'],
            loc='upper left', title='Labels')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/leg2.jpg"541">
Lastly, we can use the <b>size</b> argument to modify the font size in the legend:
<b>import matplotlib.pyplot as plt
#create bar chart
df.plot(kind='bar')
#add custom legend to bar chart
plt.legend(['A Label', 'B Label', 'C Label', 'D Label'], prop={'size': 20})</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/leg3.jpg"475">
Notice that the font size in the legend is much larger now.
<h2><span class="orange">Pandas: How to Plot Multiple Columns on Bar Chart</span></h2>
You can use the following syntax to plot multiple columns of a pandas DataFrame on a single bar chart:
<b>df[['x', 'var1', 'var2', 'var3']].plot(x='x', kind='bar')
</b>
The <b>x</b> column will be used as the x-axis variable and <b>var1</b>, <b>var2</b>, and <b>var3</b> will be used as the y-axis variables.
The following examples show how to use this function in practice.
<h3>Example 1: Plot Columns on a Bar Chart</h3>
The following code shows how to plot three columns on a bar chart, specifying that the column named <b>period</b> should be used as the x-axis variable:
<b>import pandas as pd
import matplotlib.pyplot as plt
#create fake data
df = pd.DataFrame({'period': [1, 2, 3, 4, 5, 6, 7, 8],   'A': [9, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, 7, 9, 12, 9, 9, 14],   'C': [5, 4, 7, 13, 15, 15, 18, 31]})
#plot columns on bar chart
df[['period', 'A', 'B', 'C']].plot(x='period', kind='bar')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleColumns1.png">
We could also choose to plot only certain columns, such as <b>A</b> and <b>B</b>:
<b>df[['period', 'A', 'B']].plot(x='period', kind='bar')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleColumns2.png">
<h3>Example 2: Plot Columns on a Stacked Bar Chart</h3>
To create a stacked bar chart, we simply need to specify <b>stacked=True</b> in the plot function:
<b>import pandas as pd
import matplotlib.pyplot as plt
#create fake data
df = pd.DataFrame({'period': [1, 2, 3, 4, 5, 6, 7, 8],   'A': [9, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, 7, 9, 12, 9, 9, 14],   'C': [5, 4, 7, 13, 15, 15, 18, 31]})
#create stacked bar chart
df[['period', 'A', 'B', 'C']].plot(x='period', kind='bar', stacked=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleColumns3.png">
To change the colors of the bars, simply use the <b>color</b> argument as follows:
<b>df[['period', 'A', 'B', 'C']].plot(x='period', kind='bar', stacked=True,                   color=['red', 'pink', 'gold'])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleColumns4.png">
<h2><span class="orange">How to Plot Multiple Series from a Pandas DataFrame</span></h2>
You can use the following syntax to plot multiple series from a single pandas DataFrame:
<b>plt.plot(df['series1'])
plt.plot(df['series2'])
plt.plot(df['series3'])
</b>
The following step-by-step example shows how to use this syntax in practice.
<h3>Step 1: Create the Data</h3>
First, let’s create a pandas DataFrame that contains the total sales made by three companies during an 8-week period:
<b>import pandas as pd
#create data
df = pd.DataFrame({'A': [9, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, 7, 9, 12, 9, 9, 14],   'C': [5, 4, 7, 13, 15, 15, 18, 31]})
#view data
print(df)
    A   B   C
0   9   5   5
1  12   7   4
2  15   7   7
3  14   9  13
4  19  12  15
5  23   9  15
6  25   9  18
7  29  14  31</b>
<h3>Step 2: Plot Multiple Series</h3>
Next, let’s plot the sales of each company on the same chart:
<b>import matplotlib.pyplot as plt
#plot each series
plt.plot(df['A'])
plt.plot(df['B'])
plt.plot(df['C'])
#display plot
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleSeries1.png">
<h3>Step 3: Add a Legend and Labels</h3>
Next, let’s add a legend and some axes labels to make the plot easier to read:
<b>#plot individual lines with custom colors and labels
plt.plot(df['A'], label='A', color='green')
plt.plot(df['B'], label='B', color='steelblue')
plt.plot(df['C'], label='C', color='purple')
#add legend
plt.legend(title='Group')
#add axes labels and a title
plt.ylabel('Sales', fontsize=14)
plt.xlabel('Time', fontsize=14)
plt.title('Sales by Group', fontsize=16)
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/pandasMultipleSeries2.png">
You can find more pandas tutorials on  this page .
<h2><span class="orange">How to Add Titles to Plots in Pandas (With Examples)</span></h2>
You can use the <b>title</b> argument to add a title to a plot in pandas:
<b>Method 1: Create One Title</b>
<b>df.plot(kind='hist', title='My Title')
</b>
<b>Method 2: Create Multiple Titles for Individual Subplots</b>
<b>df.plot(kind='hist', subplots=True, title=['Title1', 'Title2'])
</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [10, 10, 12, 12, 15, 17, 20, 20],   'assists': [5, 5, 7, 9, 12, 9, 6, 6]})
#view DataFrame
print(df)
  team  points  assists
0    A      10        5
1    A      10        5
2    A      12        7
3    A      12        9
4    B      15       12
5    B      17        9
6    B      20        6
7    B      20        6
</b>
<h3>Example 1: Create One Title</h3>
The following code shows how to add one title to a pandas histogram:
<b>#create histogram with title
df.plot(kind='hist', title='My Title')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/title11.jpg"559">
<h3>Example 2: Create Multiple Titles for Individual Subplots</h3>
The following code shows how to create individual titles for subplots in pandas:
<b>df.plot(kind='hist', subplots=True, title=['Title1', 'Title2'])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/title12.jpg"561">
Notice that each individual subplot has its own title.
Note that you can also pass a list of title names to the <b>title</b> argument:
<b>#define list of subplot titles
title_list = ['Title1', 'Title2']
#pass list of subplot titles to <em>title</em> argument
df.plot(kind='hist', subplots=True, title=title_list)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/title12.jpg"561">
This plot matches the one we created in the previous example.
<h2><span class="orange">How to Plot Two Columns from Pandas DataFrame</span></h2>
There are two common ways to plot the values from two columns in a pandas DataFrame:
<b>Method 1: Plot Two Columns as Points on Scatter Plot</b>
<b>import matplotlib.pyplot as plt
plt.scatter(df['column1'], df['column2'])</b>
<b>Method 2: Plot Two Columns as Lines on Line Chart</b>
<b>df.plot(x='column1', y=['column2', 'column3'])
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Plot Two Columns on Scatter Plot</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
We can use the following code to create a scatter plot that displays the <b>points</b> column on the x-axis and the <b>assists</b> column on the y-axis:
<b>import matplotlib.pyplot as plt
#create scatter plot
plt.scatter(df['points'], df['assists'])
#add axis labels
plt.xlabel('Points')
plt.ylabel('Assists')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/two1.jpg">
The x-axis contains the values from the <b>points</b> column and the y-axis contains the values from the <b>assists</b> column.
<h2>Example 2: Plot Two Columns on Line Chart</h2>
Suppose we have the following pandas DataFrame that contains information about points scored and points allowed by a basketball team in six different games:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'game': [1, 2, 3, 4, 5, 6],   'points_for': [99, 94, 92, 90, 87, 85],   'points_against': [89, 76, 78, 78, 85, 87]})
#view DataFrame
print(df)
   game  points_for  points_against
0     1          99              89
1     2          94              76
2     3          92              78
3     4          90              78
4     5          87              85
5     6          85              87
</b>
We can use the following code to create a line chart that displays the values for <b>point_for</b> on one line and <b>points_against</b> on another line while using the value for <b>game</b> on the x-axis:
<b>#plot points_for and points_against columns on same y-axis
df.plot(x='game', y=['points_for', 'points_against'])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/two2.jpg"508">
The blue line represents the value for the <b>points_for</b> column in each game and the orange line represents the values for the <b>points_against</b> column in each game.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Use Groupby and Plot in Pandas 
 How to Plot Distribution of Column Values in Pandas 
 How to Adjust the Figure Size of a Pandas Plot 
<h2><span class="orange">Pandas: How to Plot Value Counts (With Example)</span></h2>
You can use the <b>value_counts()</b> function in pandas to count the occurrences of values in a given column of a DataFrame.
You can use one of the following methods to plot the values produced by the <b>value_counts()</b> function:
<b>Method 1: Plot Value Counts in Descending Order</b>
<b>df.my_column.value_counts().plot(kind='bar')
</b>
<b>Method 2: Plot Value Counts in Ascending Order</b>
<b>df.my_column.value_counts().sort_values().plot(kind='bar') </b>
<b>Method 3: Plot Value Counts in Order They Appear in DataFrame</b>
<b>df.my_column.value_counts()[df.my_column.unique()].plot(kind='bar')</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'B', 'C'],   'points': [15, 12, 18, 20, 22, 28, 35, 40]})
#view DataFrame
print(df)
  team  points
0    A      15
1    A      12
2    B      18
3    B      20
4    B      22
5    B      28
6    B      35
7    C      40
#calculate occurrences of each value in 'team' column
df.team.value_counts()
B    5
A    2
C    1
Name: team, dtype: int64
</b>
<h2>Example 1: Plot Value Counts in Descending Order</h2>
The following code shows how to plot the value counts in a bar chart in descending order:
<b>#plot value counts of team in descending order
df.team.value_counts().plot(kind='bar')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/plotc1.jpg"498">
The x-axis displays the team name and the y-axis displays the frequency of each team.
Notice that the bars are sorted in descending order by default.
<b>Note</b>: If you’d like to create a horizontal bar chart instead, simply replace <b>bar</b> with <b>barh</b> in the <b>kind</b> argument.
<h2>Example 2: Plot Value Counts in Ascending Order</h2>
The following code shows how to plot the value counts in a bar chart in ascending order:
<b>#plot value counts of team in descending order
df.team.value_counts().sort_values().plot(kind='bar')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/plotc2.jpg"519">
Notice that the bars are now sorted in ascending order.
<h2>Example 3: Plot Value Counts in Order They Appear in DataFrame</h2>
The following code shows how to plot the value counts in a bar chart based on the order they appear in the DataFrame:
<b>#plot value counts of team in order they appear in DataFrame
df.team.value_counts()[df.team.unique()].plot(kind='bar')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/plotc3.jpg"495">
The bars are now sorted based on the order in which they appear in the DataFrame.
For example, the value ‘A’ occurs first in the <b>team</b> column, then ‘B’ occurs, then ‘C’ occurs.
Thus, this is the order the bars are placed in the bar chart.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Add Titles to Plots 
 Pandas: How to Create Plot Legends 
 Pandas: How to Create Bar Plot from GroupBy 
<h2><span class="orange">Pandas: How to Pop Rows from DataFrame</span></h2>
You can use the <b>pop()</b> function to quickly remove a column from a pandas DataFrame.
In order to use the <b>pop()</b> function to remove rows, you must first transpose the DataFrame and then use the <b>pop()</b> function to remove the columns (i.e. the rows of the original DataFrame):
<b>#pop the row in index position 3
df.T.pop(3)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Pop Rows from pandas DataFrame</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
</b>
Now suppose we would like to remove the row in index position 3 of the DataFrame.
We can transpose the DataFrame and then use the <b>pop()</b> function to remove the row in index position 3:
<b>#define transposed DataFrame
df_transpose = df.T
#remove row in index position 3 of original DataFrame
df_transpose.pop(3)
team        D
points     14
assists     9
Name: 3, dtype: object</b>
We can then transpose the DataFrame once again to get back the original DataFrame with one row removed:
<b>#transpose back to original DataFrame
df = df_transpose.T
#view updated DataFrame
print(df)
  team points assists
0    A     18       5
1    B     22       7
2    C     19       7
4    E     14      12
5    F     11       9
</b>
Notice that the row in index position 3 has been removed from the DataFrame. 
All other rows in the DataFrame remain untouched.
<b>Note</b>: You can find the complete documentation for the <b>pop()</b> function in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Insert a Row Into a Pandas DataFrame 
 How to Drop First Row in Pandas DataFrame 
 How to Drop Rows in Pandas DataFrame Based on Condition 
<h2><span class="orange">How to Print One Column of a Pandas DataFrame</span></h2>
You can use the following methods to print one column of a pandas DataFrame:
<b>Method 1: Print Column Without Header</b>
<b>print(df['my_column'].to_string(index=False))
</b>
<b>Method 2: Print Column With Header</b>
<b>print(df[['my_column']].to_string(index=False)) </b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29, 32],   'assists': [5, 7, 7, 9, 12, 9, 9, 4, 5],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 8]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
8      32        5         8
</b>
<h2>Example 1: Print Column Without Header</h2>
The following code shows how to print the values in the <b>points</b> column without the column header:
<b>#print the values in the points column without header
print(df['points'].to_string(index=False))
25
12
15
14
19
23
25
29
</b>
By using the <b>to_string()</b> function, we are able to print only the values in the <b>points</b> column without the column header or the row index values.
<h2>Example 2: Print Column With Header</h2>
The following code shows how to print the values in the <b>points</b> column with the column header:
<b>#print the values in the points column with column header
print(df[['points']].to_string(index=False))
 points
     25
     12
     15
     14
     19
     23
     25
     29
     32
</b>
Notice that the values in the <b>points</b> column along with the column header are printed.
<b>Note</b>: The only difference between this example and the previous one is that we used double brackets around the column name, which allowed us to print the column header along with the values.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Show All Rows of a Pandas DataFrame 
 How to Check dtype for All Columns in Pandas DataFrame 
<h2><span class="orange">How to Print Specific Row of Pandas DataFrame</span></h2>
You can use the following methods to print a specific row of a pandas DataFrame:
<b>Method 1: Print Row Based on Index Position</b>
<b>print(df.iloc[[3]])
</b>
<b>Method 2: Print Row Based on Index Label</b>
<b>print(df.loc[['this_label']])</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [18, 22, 19, 14, 10, 11, 20, 28],   'assists': [4, 5, 5, 4, 9, 12, 11, 8],   'rebounds': [3, 9, 12, 4, 4, 9, 8, 2]},    index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])
#view DataFrame
print(df)
   points  assists  rebounds
A      18        4         3
B      22        5         9
C      19        5        12
D      14        4         4
E      10        9         4
F      11       12         9
G      20       11         8
H      28        8         2</b>
<b>Related:</b>  Pandas loc vs. iloc: What’s the Difference? 
<h2>Example 1: Print Row Based on Index Position</h2>
The following code shows how to print the row located at index position 3 in the DataFrame:
<b>#print row located at index position 3
print(df.iloc[[3]])
   points  assists  rebounds
D      14        4         4</b>
Notice that only the row located at index position 3 is printed.
To print multiple specific rows by index position, simply include multiple values in the <b>iloc</b> function:
<b>#print rows located at index positions 3 and 5
print(df.iloc[[3, 5]])
   points  assists  rebounds
D      14        4         4
F      11       12         9
</b>
Notice that only the rows located at index positions 3 and 5 are printed.
<h2>Example 2: Print Row Based on Index Label</h2>
The following code shows how to print the row with an index label of ‘C’ in the DataFrame:
<b>#print row with index label 'C'
print(df.loc[['C']])
   points  assists  rebounds
C      19        5        12</b>
Notice that only the row with an index label of ‘C’ is printed.
To print multiple specific rows by index labels, simply include multiple labels in the <b>loc</b> function:
<b>#print rows with index labels 'C' and 'F'
print(df.loc[['C', 'F']])
   points  assists  rebounds
C      19        5        12
F      11       12         9
</b>
Notice that only the rows with index labels ‘C’ and ‘F’ are printed.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Print One Column of a Pandas DataFrame 
 How to Show All Rows of a Pandas DataFrame 
<h2><span class="orange">How to Calculate Quantiles by Group in Pandas</span></h2>
You can use the following basic syntax to calculate quantiles by group in Pandas:
<b>df.groupby('grouping_variable').quantile(.5)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculate Quantile by Group</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame 
df = pd.DataFrame({'team': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],   'score': [3, 4, 4, 5, 5, 8, 1, 2, 2, 3, 3, 5]})
#view first five rows
df.head()
teamscore
013
114
214
315
415 
</b>
The following code shows how to calculate the 90th percentile of values in the ‘points’ column, grouped by the ‘team’ column:
<b>df.groupby('team').quantile(.90)
score
team
16.5
24.0
</b>
Here’s how to interpret the output:
The 90th percentile of ‘points’ for team 1 is <b>6.5</b>.
The 90th percentile of ‘points’ for team 2 is <b>4.0</b>.
<h3>Example 2: Calculate Several Quantiles by Group</h3>
The following code shows how to calculate several quantiles at once by group:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': [1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],   'score': [3, 4, 4, 5, 5, 8, 1, 2, 2, 3, 3, 5]})
#create functions to calculate 1st and 3rd quartiles
def q1(x):
    return x.quantile(0.25)
def q3(x):
    return x.quantile(0.75)
#calculate 1st and 3rd quartiles by group
vals = {'score': [q1, q3]}
df.groupby('team').agg(vals)
score
        q1q3
team
14.05.0
22.03.0</b>
Here’s how to interpret the output:
The first and third quartile of scores for team 1 is <b>4.0</b> and <b>5.0</b>, respectively.
The first and third quartile of scores for team 2 is <b>2.0</b> and <b>3.0</b>, respectively.
<h2><span class="orange">Pandas: How to Query Column Name with Space</span></h2>
You can use the following syntax to perform a pandas query using a column name with a space:
<b>df.query('`this column` == 20')</b>
Note that you must use backticks (<b> `</b> ) in the query instead of quotation marks.
The following example shows how to use  this syntax in practice.
<h2>Example: Query Column in Pandas DataFrame with Space</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E', 'F', 'G'],     'points scored' : [12, 20, 40, 20, 24, 10, 31]}) 
#view DataFrame
print(df)
  team  points scored
0    A             12
1    B             20
2    C             40
3    D             20
4    E             24
5    F             10
6    G             31</b>
Now suppose that we would like to query for the rows where the <b>points scored</b> column is equal to 20.
If we use the <b>query()</b> function with quotation marks, we’ll receive an error:
<b>#attempt to get rows where points scored column is equal to 20
df.query('"points scored" == 20')
TypeError: argument of type 'int' is not iterable
</b>
Instead, we must use the <b>query()</b> function with backticks:
<b>#get rows where points scored column is equal to 20
df.query('`points scored` == 20')
teampoints scored
1B20
3D20
</b>
The query returns the two rows in the DataFrame where the <b>points scored</b> column is equal to 20.
Notice that we don’t receive any error either because we used backticks instead of quotation marks within the <b>query()</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Filter Rows Based on String Length 
 Pandas: How to Drop Rows Based on Condition 
 Pandas: How to Use “NOT IN” Filter 
<h2><span class="orange">Pandas: How to Use LIKE inside query()</span></h2>
You can use the following methods to use <b>LIKE</b> (similar to SQL) inside a pandas <b>query()</b> function to find rows that contain a particular pattern:
<b>Method 1: Find Rows that Contain One Pattern</b>
<b>df.query('my_column.str.contains("pattern1")')
</b>
<b>Method 2: Find Rows that Contain One of Several Patterns</b>
<b>df.query('my_column.str.contains("pattern1|pattern2")')</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Cavs', 'Heat', 'Mavs', 'Mavs', 'Nets',            'Heat', 'Cavs', 'Jazz', 'Jazz', 'Hawks'],   'points': [3, 3, 4, 5, 4, 7, 8, 7, 12, 14],   'rebounds': [15, 14, 14, 10, 8, 14, 13, 9, 5, 4]})
#view DataFrame
print(df)
    team  points  rebounds
0   Cavs       3        15
1   Heat       3        14
2   Mavs       4        14
3   Mavs       5        10
4   Nets       4         8
5   Heat       7        14
6   Cavs       8        13
7   Jazz       7         9
8   Jazz      12         5
9  Hawks      14         4
</b>
<h2>Example 1: Find Rows that Contain One Pattern</h2>
The following code shows how to use the <b>query()</b> function to find all rows in the DataFrame that contain “avs” in the <b>team</b> column:
<b>df.query('team.str.contains("avs")')
        teampointsrebounds
0Cavs315
2Mavs414
3Mavs510
6Cavs813
</b>
Each row that is returned contains “avs” somewhere in the <b>team</b> column.
Also note that this syntax is case-sensitive.
Thus, if we used “AVS” instead then we would not receive any results because no row contains uppercase “AVS” in the <b>team</b> column.
<h2>Example 2: Find Rows that Contain One of Several Patterns</h2>
The following code shows how to use the <b>query()</b> function to find all rows in the DataFrame that contain “avs” or “eat” in the <b>team</b> column:
<b>df.query('team.str.contains("avs|eat")')
        teampointsrebounds
0Cavs315
1Heat314
2Mavs414
3Mavs510
5Heat714
6Cavs813
</b>
Each row that is returned contains either “avs” or “eat” somewhere in the <b>team</b> column.
<b>Note</b>: The <b>|</b> operator stands for “or” in pandas. Feel free to use as many as these operators as you’d like to search for even more string patterns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Filter Rows Based on String Length 
 Pandas: How to Drop Rows Based on Condition 
 Pandas: How to Use “NOT IN” Filter 
<h2><span class="orange">The Ultimate Guide: How to Read CSV Files with Pandas</span></h2>
CSV (comma-separated value) files are one of the most common ways to store data.
Fortunately the pandas function  read_csv()  allows you to easily read in CSV files into Python in almost any format you’d like.
This tutorial explains several ways to read CSV files into Python using the following CSV file named <b>‘data.csv’</b>:
<b>playerID,team,points
1,Lakers,26
2,Mavs,19
3,Bucks,24
4,Spurs,22</b>
<h2>Example 1: Read CSV File into pandas DataFrame</h2>
The following code shows how to read the CSV file into a pandas DataFrame:
<b>#import CSV file as DataFrame
df = pd.read_csv('data.csv')</b>
<b>#view DataFrame
df
        playerID  team  points
01  Lakers  26
12  Mavs  19
23  Bucks  24
34  Spurs  22</b>
<h2>Example 2: Read Specific Columns from CSV File</h2>
The following code shows how to read only the columns titled ‘playerID’ and ‘points’ in the CSV file into a pandas DataFrame:
<b>#import only specific columns from CSV file
df = pd.read_csv('data.csv', usecols=['playerID', 'points'])</b>
<b>#view DataFrame
df
playerID  points
01  26
12  19
23  24
34  22</b>
Alternatively you can specify column indices to read into a pandas DataFrame:
<b>#import only specific columns from CSV file
df = pd.read_csv('data.csv', usecols=[0, 1])</b>
<b>#view DataFrame
df
        playerID  team
01  Lakers
12  Mavs
23  Bucks
34  Spurs</b>
<h2>Example 3: Specify Header Row when Importing CSV File</h2>
In some cases, the header row might not be the first row in a CSV file.
For example, consider the following CSV file in which the header row actually appears in the second row:
<b>random,data,values
playerID,team,points
1,Lakers,26
2,Mavs,19
3,Bucks,24
4,Spurs,22</b>
To read this CSV file into a pandas DataFrame, we can specify <b>header=1 </b>as follows:
<b>#import from CSV file and specify that header starts on second row
df = pd.read_csv('data.csv', header=1)</b>
<b>#view DataFrame
df
        playerID teampoints
01 Lakers26
12 Mavs19
23 Bucks24
34 Spurs22</b>
<h2>Example 4: Skip Rows when Importing CSV File</h2>
You can also easily skip rows when importing a CSV file by using the <b>skiprows </b>argument.
For example, the following code shows how to skip the second row when importing the CSV file:
<b>#import from CSV file and skip second row
df = pd.read_csv('data.csv', skiprows=[1])</b>
<b>#view DataFrame
df
        playerID teampoints
02 Mavs19
13 Bucks24
24 Spurs22</b>
And the following code shows how to skip the second <em>and</em> third row when importing the CSV file:
<b>#import from CSV file and skip second and third rows
df = pd.read_csv('data.csv', skiprows=[1, 2])</b>
<b>#view DataFrame
df
        playerID teampoints
13 Bucks24
24 Spurs22</b>
<h2>Example 5: Read CSV Files with Custom Delimiter</h2>
Occasionally you may have a CSV file with a delimiter that is different from a comma.
For example, suppose our CSV file has an underscore as a delimiter:
<b>playerID_team_points
1_Lakers_26
2_Mavs_19
3_Bucks_24
4_Spurs_22
</b>
To read this CSV file into pandas, we can use the <b>sep </b>argument to specify the delimiter to use when reading the file:
<b>#import from CSV file and specify delimiter to use
df = pd.read_csv('data.csv', sep='_')</b>
<b>#view DataFrame
df
playerID teampoints
01 Lakers26
12 Mavs19
23 Bucks24
34 Spurs22</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Read a Text File with Pandas 
 How to Read Excel Files with Pandas 
 How to Read TSV Files with Pandas 
 How to Read HTML Tables with Pandas 
<h2><span class="orange">The Ultimate Guide: How to Read Excel Files with Pandas</span></h2>
Excel files are one of the most common ways to store data. Fortunately the pandas function  read_excel()  allows you to easily read in Excel files.
This tutorial explains several ways to read Excel files into Python using pandas.
<h3>Example 1: Read Excel File into a pandas DataFrame</h3>
Suppose we have the following Excel file:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pdImportExcel1.png">
The following code shows how to use the <b>read_excel() </b>function to import this Excel file into a pandas DataFrame:
<b>import pandas as pd
#import Excel file
df = pd.read_excel('data.xlsx')
#view DataFrame
df
        playerID teampoints
01 Lakers26
12 Mavs19
23 Bucks24
34 Spurs22
</b>
<h3>Example 2: Read Excel File with Index Column</h3>
Sometimes you may also have an Excel file in which one of the columns is an index column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pdImportExcel2.png">
In this case you can use <b>index_col </b>to tell pandas which column to use as the index column when importing:
<b>import pandas as pd
#import Excel file, specifying the index column
df = pd.read_excel('data.xlsx', index_col='index')
#view DataFrame
df
playerIDteampoints
index
11        Lakers26
22        Mavs19
33        Bucks24
44        Spurs22</b>
<h3>Example 3: Read Excel File Using Sheet Name</h3>
You can also read specific sheet names from an Excel file into a pandas DataFrame. For example, consider the following Excel file:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pdImportExcel3.png">
To read a specific sheet in as a pandas DataFrame, you can use the <b>sheet_name() </b>argument:
<b>import pandas as pd
#import only second sheet
df = pd.read_excel('data.xlsx', sheet_name='second sheet')
#view DataFrame
df
playerIDteampoints
01Lakers26
12Mavs19
23Bucks24
34Spurs22
</b>
<h3>Common Error: Install xlrd</h3>
When you attempt to use the <b>read_excel() </b>function, you may encounter the following error:
<b>ImportError: Install xlrd >= 1.0.0 for Excel support
</b>
In this case, you need to first install xlrd:
<b>pip install xlrd</b>
Once this is installed, you may proceed to use the <b>read_excel() </b>function.
<h2><span class="orange">How to Read HTML Tables with Pandas (Including Example)</span></h2>
You can use the pandas  read_html()  function to read HTML tables into a pandas DataFrame.
This function uses the following basic syntax:
<b>df = pd.read_html('https://en.wikipedia.org/wiki/National_Basketball_Association')
</b>
The following example shows how to use this function to read in a table of NBA team names from  this Wikipedia page .
<h3>Example: Read HTML Table with Pandas</h3>
Before using the <b>read_html()</b> function, you’ll likely have to install lxml:
<b>pip install lxml</b>
<b>Note</b>: If you’re using a Jupyter notebook, you need to restart the kernel after performing this installation.
Next, we can use the <b>read_html()</b> function to read every HTML table on  this Wikipedia page :
<b>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from unicodedata import normalize
#read all HTML tables from specific URL
tabs = pd.read_html('https://en.wikipedia.org/wiki/National_Basketball_Association')
#display total number of tables read
len(tabs)
44</b>
We can see that a total of 44 HTML tables were found on this page.
I know that the table I’m interested in has the word “Division” in it, so I can use the <b>match</b> argument to only retrieve HTML tables that contain this word:
<b>#read HTML tables from specific URL with the word "Division" in them
tabs = pd.read_html('https://en.wikipedia.org/wiki/National_Basketball_Association',    match='Division')
#display total number of tables read
len(tabs)
1</b>
I can then  list the names  of the columns of the table:
<b>#define table
df = tabs[0]
#list all column names of table
list(df)
[('Division', 'Eastern Conference'),
 ('Team', 'Eastern Conference'),
 ('Location', 'Eastern Conference'),
 ('Arena', 'Eastern Conference'),
 ('Capacity', 'Eastern Conference'),
 ('Coordinates', 'Eastern Conference'),
 ('Founded', 'Eastern Conference'),
 ('Joined', 'Eastern Conference'),
 ('Unnamed: 8_level_0', 'Eastern Conference')]</b>
I’m only interested in the first two columns, so I can  filter  the DataFrame to only contain these columns:
<b>#filter DataFrame to only contain first two columns
df_final = df.iloc[:, 0:2]
#rename columns
df_final.columns = ['Division', 'Team']
#view first few rows of final DataFrame
print(df_final.head())
   Division                Team
0  Atlantic      Boston Celtics
1  Atlantic       Brooklyn Nets
2  Atlantic     New York Knicks
3  Atlantic  Philadelphia 76ers
4  Atlantic     Toronto Raptors
</b>
The final table contains only the ‘Division’ and ‘Team’ columns.
<h2><span class="orange">How to Read a Text File with Pandas (Including Examples)</span></h2>
To read a text file with pandas in Python, you can use the following basic syntax:
<b>df = pd.read_csv("data.txt", sep=" ")
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Read a Text File with a Header</h3>
Suppose we have the following text file called <b>data.txt</b> with a header:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/txtPandas1.png">
To read this file into a pandas DataFrame, we can use the following syntax:
<b>import pandas as pd
#read text file into pandas DataFrame
df = pd.read_csv("data.txt", sep=" ")
#display DataFrame
print(df)
   column1  column2
0        1        4
1        3        4
2        2        5
3        7        9
4        9        1
5        6        3
6        4        4
7        5        2
8        4        8
9        6        8
</b>
We can print the class of the DataFrame and find the number of rows and columns using the following syntax:
<b>#display class of DataFrame
print(type(df))
&lt;class 'pandas.core.frame.DataFrame'>
#display number of rows and columns in DataFrame
df.shape
(10, 2)
</b>
We can see that <b>df</b> is a pandas DataFrame with 10 rows and 2 columns.
<h3>Read a Text File with No Header</h3>
Suppose we have the following text file called <b>data.txt</b> with no headers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/txtPandas2.png">
To read this file into a pandas DataFrame, we can use the following syntax:
<b>#read text file into pandas DataFrame
df = pd.read_csv("data.txt", sep=" ", header=None)
#display DataFrame
print(df)
   0  1
0  1  4
1  3  4
2  2  5
3  7  9
4  9  1
5  6  3
6  4  4
7  5  2
8  4  8
9  6  8
</b>
Since the text file had no headers, pandas simply named the columns <b>0</b> and <b>1</b>.
<h3>Read a Text File with No Header & Specify Column Names</h3>
If we’d like, we can assign column names while importing the text file by using the <b>names</b> argument:
<b>#read text file into pandas DataFrame and specify column names
df = pd.read_csv("data.txt", sep=" ", header=None, names=["A", "B"])
#display DataFrame
print(df)
   A  B
0  1  4
1  3  4
2  2  5
3  7  9
4  9  1
5  6  3
6  4  4
7  5  2
8  4  8
9  6  8
</b>
<h2><span class="orange">How to Read a TSV File with Pandas (Including Examples)</span></h2>
To read a TSV file with pandas in Python, you can use the following basic syntax:
<b>df = pd.read_csv("data.txt", sep="\t")
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Read a TSV File with a Header</h3>
Suppose we have the following TSV file called <b>data.txt </b>with a header:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tab1.png">
To read this file into a pandas DataFrame, we can use the following syntax:
<b>import pandas as pd
#read TSV file into pandas DataFrame
df = pd.read_csv("data.txt", sep="\t")
#view DataFrame
print(df)
column1column2
014
134
225
379
491
563
657
788
831
949
</b>
We can print the class of the DataFrame and find the number of rows and columns using the following syntax:
<b>#display class of DataFrame
print(type(df))
&lt;class 'pandas.core.frame.DataFrame'>
#display number of rows and columns in DataFrame
df.shape
(10, 2)
</b>
We can see that <b>df</b> is a pandas DataFrame with 10 rows and 2 columns.
<h3>Read a TSV File with No Header</h3>
Suppose we have the following TSV file called <b>data.txt</b> with no headers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tab2.png">
To read this file into a pandas DataFrame, we can use the following syntax:
<b>#read TSV file into pandas DataFrame
df = pd.read_csv("data.txt", sep="\t", header=None)
#view DataFrame
print(df)
        01
014
134
225
379
491
563
657
788
831
949
</b>
Since the text file had no headers, pandas simply named the columns <b>0</b> and <b>1</b>.
<h3>Read TSV File with No Header & Specify Column Names</h3>
If we’d like, we can assign column names while importing the text file by using the <b>names</b> argument:
<b>#read TSV file into pandas DataFrame and specify column names
df = pd.read_csv("data.txt", sep="\t", header=None, names=["A", "B"])
#display DataFrame
print(df)
AB
014
134
225
379
491
563
657
788
831
949
</b>
<h2><span class="orange">Pandas: How to Remove Specific Characters from Strings</span></h2>
You can use the following methods to remove specific characters from strings in a column in a pandas DataFrame:
<b>Method 1: Remove Specific Characters from Strings</b>
<b>df['my_column'] = df['my_column'].str.replace('this_string', '')
</b>
<b>Method 2: Remove All Letters from Strings</b>
<b>df['my_column'] = df['my_column'].str.replace('\D', '', regex=True)</b>
<b>Method 3: Remove All Numbers from Strings</b>
<b>df['my_column'] = df['my_column'].str.replace('\d+', '', regex=True)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['Mavs2', 'Nets44', 'Kings33', 'Cavs90', 'Heat576'],   'points' : [12, 15, 22, 29, 24]})
#view DataFrame
print(df)
      team  points
0    Mavs2      12
1   Nets44      15
2  Kings33      22
3   Cavs90      29
4  Heat576      24
</b>
<h2>Example 1: Remove Specific Characters from Strings</h2>
We can use the following syntax to remove ‘avs’ from each string in the <b>team</b> column:
<b>#remove 'avs' from strings in team column
df['team'] = df['team'].str.replace('avs', '')
#view updated DataFrame
print(df)
      team  points
0       M2      12
1   Nets44      15
2  Kings33      22
3      C90      29
4  Heat576      24
</b>
Notice that ‘avs’ was removed from the rows that contained ‘Mavs’ and ‘Cavs’ in the <b>team</b> column.
<h2>Example 2: Remove All Letters from Strings</h2>
We can use the following syntax to remove all letters from each string in the <b>team</b> column:
<b>#remove letters from strings in team column
df['team'] = df['team'].str.replace('\D', '', regex=True)
#view updated DataFrame
print(df)
  team  points
0    2      12
1   44      15
2   33      22
3   90      29
4  576      24
</b>
Notice that all letters have been removed from each string in the <b>team</b> column.
Only the numerical values remain.
<h2>Example 3: Remove All Numbers from Strings</h2>
We can use the following syntax to remove all numbers from each string in the <b>team</b> column:
<b>#remove numbers from strings in team column
df['team'] = df['team'].str.replace('\d+', '', regex=True)
#view updated DataFrame
print(df)
    team  points
0   Mavs      12
1   Nets      15
2  Kings      22
3   Cavs      29
4   Heat      24
</b>
Notice that all numbers have been removed from each string in the <b>team</b> column.
Only the letters remain.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Replace NaN Values with Zeros in Pandas 
 How to Replace Empty Strings with NaN in Pandas 
 How to Replace Values in Column Based on Condition in Pandas 
<h2><span class="orange">Pandas: How to Remove Duplicates but Keep Row with Max Value</span></h2>
You can use the following methods to remove duplicates in a pandas DataFrame but keep the row that contains the max value in a particular column:
<b>Method 1: Remove Duplicates in One Column and Keep Row with Max</b>
<b>df.sort_values('var2', ascending=False).drop_duplicates('var1').sort_index()
</b>
<b>Method 2: Remove Duplicates in Multiple Columns and Keep Row with Max</b>
<b>df.sort_values('var3', ascending=False).drop_duplicates(['var1', 'var2']).sort_index()</b>
The following examples show how to use each method in practice.
<h2>Example 1: Remove Duplicates in One Column and Keep Row with Max</h2>
Suppose we have the following pandas DataFrame that contains information about points scored by basketball players on various teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'points': [20, 24, 28, 30, 14, 19, 29, 40, 22]})
#view DataFrame
print(df)
  team  points
0    A      20
1    A      24
2    A      28
3    B      30
4    B      14
5    B      19
6    C      29
7    C      40
8    C      22
</b>
We can use the following syntax to drop rows with duplicate <b>team</b> names but keep the rows with the max values for <b>points</b>:
<b>#drop duplicate teams but keeps row with max points
df_new = df.sort_values('points', ascending=False).drop_duplicates('team').sort_index()
#view DataFrame
print(df_new)
  team  points
2    A      28
3    B      30
7    C      40</b>
Each row with a duplicate <b>team</b> name has been dropped, but the rows with the max value for <b>points</b> have been kept for each <b>team</b>.
<h2>Example 2: Remove Duplicates in Multiple Columns and Keep Row with Max</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'position': ['G', 'G', 'F', 'G', 'F', 'F', 'G', 'G', 'F'],   'points': [20, 24, 28, 30, 14, 19, 29, 40, 22]})
#view DataFrame
print(df)
  team position  points
0    A        G      20
1    A        G      24
2    A        F      28
3    B        G      30
4    B        F      14
5    B        F      19
6    C        G      29
7    C        G      40
8    C        F      22
</b>
We can use the following syntax to drop rows with duplicate <b>team</b> <em>and</em> <b>position</b> names but keep the rows with the max values for <b>points</b>:
<b>#drop rows with duplicate team and positions but keeps row with max points
df_new = df.sort_values('points', ascending=False).drop_duplicates(['team', 'position']).sort_index()
#view DataFrame
print(df_new)
  team position  points
1    A        G      24
2    A        F      28
3    B        G      30
5    B        F      19
7    C        G      40
8    C        F      22</b>
Each row with a duplicate <b>team</b> and <b>position</b> name has been dropped, but the rows with the max value for <b>points</b> have been kept for each combination of <b>team</b> and <b>position.</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Duplicate Rows in Pandas 
 How to Drop Duplicate Columns in Pandas 
 How to Count Duplicates in Pandas 
<h2><span class="orange">Pandas: How to Remove Special Characters from Column</span></h2>
You can use the following basic syntax to remove special characters from a column in a pandas DataFrame:
<b>df['my_column'] = df['my_column'].str.replace('\W', '', regex=True)
</b>
This particular example will remove all characters in <b>my_column</b> that are not letters or numbers.
The following example shows how to use this syntax in practice.
<h2>Example: Remove Special Characters from Column in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['Mavs$', 'Nets', 'Kings!!', 'Spurs%', '&Heat&'],   'points' : [12, 15, 22, 29, 24]})
#view DataFrame
print(df)
      team  points
0    Mavs$      12
1     Nets      15
2  Kings!!      22
3   Spurs%      29
4   &Heat&      24</b>
Suppose we would like to remove all special characters from values in the <b>team</b> column.
We can use the following syntax to do so:
<b>#remove special characters from team column
df['team'] = df['team'].str.replace('\W', '', regex=True)
#view updated DataFrame
print(df)
    team  points
0   Mavs      12
1   Nets      15
2  Kings      22
3  Spurs      29
4   Heat      24</b>
Notice that all special characters have been removed from values in the <b>team</b> column.
<b>Note</b>: The regex <b>\W</b> is used to find all non-word characters, i.e. characters which are not alphabetical or numerical.
In this example, we replaced each non-word character with an empty value which is equivalent to removing the non-word characters.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Replace NaN Values with Zeros in Pandas 
 How to Replace Empty Strings with NaN in Pandas 
 How to Replace Values in Column Based on Condition in Pandas 
<h2><span class="orange">Pandas: How to Rename Columns with Dictionary</span></h2>
You can use the following basic syntax to rename columns with a dictionary in pandas:
<b>#define dictionary
some_dict = {'old_col1': 'new_col1',
             'old_col2': 'new_col2',
             'old_col3': 'new_col3'}
 
#rename columns in DataFrame using dictionary
df.rename(columns=some_dict, inplace=True)
</b>
<b>Note</b>: We must specify <b>inplace=True</b> to modify the column names of the original DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: Rename Columns in Pandas with Dictionary</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'rebounds': [10, 14, 14, 13, 13, 12, 10, 7],    'points': [30, 22, 19, 14, 14, 11, 20, 28],    'assists': [5, 6, 6, 5, 8, 7, 7, 9]})
#view DataFrame
print(df)
   rebounds  points  assists
0        10      30        5
1        14      22        6
2        14      19        6
3        13      14        5
4        13      14        8
5        12      11        7
6        10      20        7
7         7      28        9</b>
We can use the following syntax to rename each of the columns in the DataFrame using a dictionary:
<b>#define dictionary with new column names
some_dict = {'rebounds': 'rebs',
             'points': 'pts',
             'assists': 'ast'}
 
#rename columns in DataFrame using dictionary
df.rename(columns=some_dict, inplace=True)
#view updated DataFrame
print(df)
   rebs  pts  ast
0    10   30    5
1    14   22    6
2    14   19    6
3    13   14    5
4    13   14    8
5    12   11    7
6    10   20    7
7     7   28    9</b>
Notice that each of the columns have been renamed based on the values we specified in the dictionary.
It’s worth noting that you don’t have to rename every single column using a dictionary.
For example, we could create a dictionary to only rename the <b>points</b> and <b>assists</b> columns in the DataFrame:
<b>#define dictionary with new column names for points and assists only
some_dict = {'points': 'pts',
             'assists': 'ast'}
 
#rename columns in DataFrame using dictionary
df.rename(columns=some_dict, inplace=True)
#view updated DataFrame
print(df)
   rebounds  pts  ast
0        10   30    5
1        14   22    6
2        14   19    6
3        13   14    5
4        13   14    8
5        12   11    7
6        10   20    7
7         7   28    9
</b>
Only the <b>points</b> and <b>assists</b> columns were renamed.
Since the <b>rebounds</b> column was not included in the dictionary, it was not renamed in the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to List All Column Names in Pandas 
 How to Sort Columns by Name in Pandas 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">How to Rename Columns in Pandas (With Examples)</span></h2>
You can use one of the following three methods to rename columns in a pandas DataFrame:
<b>Method 1: Rename Specific Columns</b>
<b>df.rename(columns = {'old_col1':'new_col1', 'old_col2':'new_col2'}, inplace = True)
</b>
<b>Method 2: Rename All Columns</b>
<b>df.columns = ['new_col1', 'new_col2', 'new_col3', 'new_col4']</b>
<b>Method 3: Replace Specific Characters in Columns</b>
<b>df.columns = df.columns.str.replace('old_char', 'new_char')</b>
The following examples show how to use each of these methods in practice.
<b>Related:</b>  How to Get Column Names in Pandas (3 Methods) 
<h2>Method 1: Rename Specific Columns</h2>
The following code shows how to rename specific columns in a pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'team':['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#list column names
list(df)
['team', 'points', 'assists', 'rebounds']
#rename specific column names
df.rename(columns = {'team':'team_name', 'points':'points_scored'}, inplace = True)
#view updated list of column names
list(df)
['team_name', 'points_scored', 'assists', 'rebounds']
</b>
Notice that the ‘team’ and ‘points’ columns were renamed while all other column names remained the same.
<h2>Method 2: Rename All Columns</h2>
The following code shows how to rename all columns in a pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'team':['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#list column names
list(df)
['team', 'points', 'assists', 'rebounds']
#rename all column names
df.columns = ['_team', '_points', '_assists', '_rebounds']
#view updated list of column names
list(df)
['_team', '_points', '_assists', '_rebounds']
</b>
Note that it’s faster to use this method when you want to rename most or all of the column names in the DataFrame.
<h2>Method 3: Replace Specific Characters in Columns</h2>
The following code shows how to replace a specific character in each column name:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'$team':['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   '$points': [25, 12, 15, 14, 19, 23, 25, 29],   '$assists': [5, 7, 7, 9, 12, 9, 9, 4],   '$rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#list column names
list(df)
['team', 'points', 'assists', 'rebounds']
#rename $ with blank in every column name
df.columns = df.columns.str.replace('$', '')
#view updated list of column names
list(df)
['team', 'points', 'assists', 'rebounds']
</b>
Notice that this method allowed us to quickly remove the ‘$’ from each column name.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to List All Column Names in Pandas 
 How to Sort Columns by Name in Pandas 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">Pandas: How to Replace Zero with NaN</span></h2>
You can use the following basic syntax to replace zeros with NaN values in a pandas DataFrame:
<b>df.replace(0, np.nan, inplace=True)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Replace Zero with NaN in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 0, 15, 14, 19, 23, 25, 29],   'assists': [5, 0, 7, 0, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 0, 9, 0]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1       0        0         8
2      15        7        10
3      14        0         6
4      19       12         6
5      23        9         0
6      25        9         9
7      29        4         0
</b>
We can use the following syntax to replace each zero in the DataFrame with a NaN value:
<b>import numpy as np
#replace all zeros with NaN values
df.replace(0, np.nan, inplace=True)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    25.0      5.0      11.0
1     NaN      NaN       8.0
2    15.0      7.0      10.0
3    14.0      NaN       6.0
4    19.0     12.0       6.0
5    23.0      9.0       NaN
6    25.0      9.0       9.0
7    29.0      4.0       NaN
</b>
Notice that each zero in every column of the DataFrame has been replaced with NaN.
<b>Note</b>: We must use the argument <b>inplace=True</b> or else the changes won’t be made to the original DataFrame.
<b>Related:</b>  How to Replace NaN Values with Zero in Pandas 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Replace Specific Values in Pandas 
 How to Filter a Pandas DataFrame by Column Values 
 How to Fill NA Values for Multiple Columns in Pandas 
<h2><span class="orange">Pandas: How to Replace Empty Strings with NaN</span></h2>
You can use the following syntax to replace empty strings with NaN values in pandas:
<b>df = df.replace(r'^\s*$', np.nan, regex=True)</b>
The following example shows how to use this syntax in practice.
<b>Related:</b>  How to Replace NaN Values with String in Pandas 
<h3>Example: Replace Empty Strings with NaN</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', ' ', 'D', 'E', ' ', 'G', 'H'],   'position': [' ', 'G', 'G', 'F', 'F', ' ', 'C', 'C'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition pointsrebounds
0A 511
1BG 78
2G 710
3DF 96
4EF 126
5 95
6GC 99
7HC 412</b>
Notice that there are several empty strings in both the <b>team</b> and <b>position</b> columns.
We can use the following syntax to replace these empty strings with NaN values:
<b>import numpy as np
#replace empty values with NaN
df = df.replace(r'^\s*$', np.nan, regex=True)
#view updated DataFrame
df
teamposition pointsrebounds
0ANaN 511
1BG 78
2NaNG 710
3DF 96
4EF 126
5NaNNaN 95
6GC 99
7HC 4127</b>
Notice that each of the empty strings have been replaced with NaN.
<b>Note</b>: You can find the complete documentation for the <b>replace</b> function in pandas  here .
<h2><span class="orange">Pandas: How to Replace inf with Zero</span></h2>
You can use the following syntax to replace inf and -inf values with zero in a pandas DataFrame:
<b>df.replace([np.inf, -np.inf], 0, inplace=True)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Replace inf with Zero in Pandas</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, np.inf, 19, np.inf, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, np.inf],   'rebounds': [np.inf, 8, 10, 6, 6, -np.inf, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A18.05.0inf
1Binf7.08.0
2C19.07.010.0
3Dinf9.06.0
4E14.012.06.0
5F11.09.0-inf
6G20.09.09.0
7H28.0inf12.0
</b>
Notice that there are several inf and -inf values throughout the DataFrame.
We can use the following syntax to replace these inf and -inf values with zero:
<b>#replace inf and -inf with zero
df.replace([np.inf, -np.inf], 0, inplace=True)
#view updated DataFrame
df
teampointsassists rebounds
0A18.05.0 0.0
1B0.07.0 8.0
2C19.07.0 10.0
3D0.09.0 6.0
4E14.012.0 6.0
5F11.09.0 0.0
6G20.09.0 9.0
7H28.00.0 12.0</b>
Notice that each of the inf and -inf values have been replaced with zero.
<b>Note</b>: You can find the complete documentation for the <b>replace</b> function in pandas  here .
<h2><span class="orange">Pandas: How to Replace inf with Max Value</span></h2>
You can use the following methods to replace <b>inf</b> and <b>-inf</b> values with the max value in a pandas DataFrame:
<b>Method 1: Replace inf with Max Value in One Column</b>
<b>#find max value of column
max_value = np.nanmax(df['my_column'][df['my_column'] != np.inf])
#replace inf and -inf in column with max value of column 
df['my_column'].replace([np.inf, -np.inf], max_value, inplace=True)
</b>
<b>Method 2: Replace inf with Max Value in All Columns</b>
<b>#find max value of entire data frame
max_value = np.nanmax(df[df != np.inf])
#replace inf and -inf in all columns with max value
df.replace([np.inf, -np.inf], max_value, inplace=True)</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'points': [18, np.inf, 19, np.inf, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, np.inf],   'rebounds': [np.inf, 8, 10, 6, 6, -np.inf, 9, 12]})
#view DataFrame
print(df)
   points  assists  rebounds
0    18.0      5.0       inf
1     inf      7.0       8.0
2    19.0      7.0      10.0
3     inf      9.0       6.0
4    14.0     12.0       6.0
5    11.0      9.0      -inf
6    20.0      9.0       9.0
7    28.0      inf      12.0
</b>
<h2>Example 1: Replace inf with Max Value in One Column</h2>
The following code shows how to replace the <b>inf</b> and <b>-inf</b> values in the rebounds column with the max value of the rebounds column:
<b>#find max value of rebounds
max_value = np.nanmax(df['rebounds'][df['rebounds'] != np.inf])
#replace inf and -inf in rebounds with max value of rebounds
df['rebounds'].replace([np.inf, -np.inf], max_value, inplace=True)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    18.0      5.0      12.0
1     inf      7.0       8.0
2    19.0      7.0      10.0
3     inf      9.0       6.0
4    14.0     12.0       6.0
5    11.0      9.0      12.0
6    20.0      9.0       9.0
7    28.0      inf      12.0</b>
Notice that each <b>inf</b> and <b>-inf</b> value in the rebounds column has been replaced with the max value in that column of <b>12</b>.
<h2>Example 2: Replace inf with Max Value in All Columns</h2>
The following code shows how to replace the <b>inf</b> and <b>-inf</b> values in every column with the max value of the entire data frame:
<b>#find max value of entire data frame
max_value = np.nanmax(df[df != np.inf])
#replace all inf and -inf with max value
df.replace([np.inf, -np.inf], max_value, inplace=True)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    18.0      5.0      28.0
1    28.0      7.0       8.0
2    19.0      7.0      10.0
3    28.0      9.0       6.0
4    14.0     12.0       6.0
5    11.0      9.0      28.0
6    20.0      9.0       9.0
7    28.0     28.0      12.0
</b>
Notice that each <b>inf</b> and <b>-inf</b> value in every column has been replaced with the max value in the entire data frame of <b>28</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Impute Missing Values in Pandas 
 How to Count Missing Values in Pandas 
 How to Fill NaN Values with Mean in Pandas 
<h2><span class="orange">Pandas: How to Replace Multiple Values in One Column</span></h2>
You can use the following basic syntax to replace multiple values in one column of a pandas DataFrame:
<b>df = df.replace({'my_column' : {'old1' : 'new1', 'old2' : 'new2', 'old3' : 'new3'}})
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Replace Multiple Values in One Column in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'position': ['G', 'G', 'F', 'F', 'F', 'C', 'C'],   'points': [28, 17, 19, 14, 23, 26, 5],   'rebounds': [5, 6, 4, 7, 14, 12, 9],   'assists': [10, 13, 7, 8, 4, 5, 8]})
#view DataFrame
print(df)
  position  points  rebounds  assists
0        G      28         5       10
1        G      17         6       13
2        F      19         4        7
3        F      14         7        8
4        F      23        14        4
5        C      26        12        5
6        C       5         9        8
</b>
Suppose we would like to make the following replacements in the <b>position</b> column:
Replace ‘G’ with ‘Guard’
Replace ‘F’ with ‘Forward’
Replace C with ‘Center’
We can use the following syntax to do so:
<b>#replace multiple values in position column
df = df.replace({'position' : {'G' : 'Guard', 'F' : 'Forward', 'C' : 'Center'}})
#view updated DataFrame
print(df)
  position  points  rebounds  assists
0    Guard      28         5       10
1    Guard      17         6       13
2  Forward      19         4        7
3  Forward      14         7        8
4  Forward      23        14        4
5   Center      26        12        5
6   Center       5         9        8
</b>
Notice that multiple values have been replaced in the <b>position</b> column.
We can use similar syntax to replace multiple values in a numeric column.
For example, the following code shows how to make the following replacements in the <b>assists</b> column:
Replace 10 with 20
Replace 13 with 15
Replace 8 with 10
We can use the following syntax to do so:
<b>#replace multiple values in assists column
df = df.replace({'assists' : {10:20, 13:15, 8:10}})
#view updated DataFrame
print(df)
  position  points  rebounds  assists
0        G      28         5       20
1        G      17         6       15
2        F      19         4        7
3        F      14         7       10
4        F      23        14        4
5        C      26        12        5
6        C       5         9       10
</b>
Notice that multiple values have been replaced in the <b>assists</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Replace NaN Values with Zeros in Pandas 
 How to Replace Empty Strings with NaN in Pandas 
 How to Replace Values in Column Based on Condition in Pandas 
<h2><span class="orange">Pandas: How to Replace NaN with None</span></h2>
You can use the following basic syntax to replace <b>NaN</b> values with <b>None</b> in a pandas DataFrame:
<b>df = df.replace(np.nan, None)
</b>
This function is particularly useful when you need to export a pandas DataFrame to a database that uses <b>None</b> to represent missing values instead of <b>NaN</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Replace NaN with None in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'A': [5, 6, 8, np.nan, 4, 15, 13],   'B': [np.nan, 12, np.nan, 10, 23, 6, 4],   'C': [2, 7, 6, 3, 2, 4, np.nan],   'D': [5, np.nan, 6, 15, 1, np.nan, 4]})
#view DataFrame
print(df)
      A     B    C     D
0   5.0   NaN  2.0   5.0
1   6.0  12.0  7.0   NaN
2   8.0   NaN  6.0   6.0
3   NaN  10.0  3.0  15.0
4   4.0  23.0  2.0   1.0
5  15.0   6.0  4.0   NaN
6  13.0   4.0  NaN   4.0</b>
Notice that there are several <b>NaN</b> values throughout the DataFrame.
To replace each <b>NaN</b> value with <b>None</b>, we can use the following syntax:
<b>#replace all NaN values with None
df = df.replace(np.nan, None)
#view updated DataFrame
print(df)
      A     B     C     D
0   5.0  None   2.0   5.0
1   6.0  12.0   7.0  None
2   8.0  None   6.0   6.0
3  None  10.0   3.0  15.0
4   4.0  23.0   2.0   1.0
5  15.0   6.0   4.0  None
6  13.0   4.0  None   4.0
</b>
Notice that each <b>NaN</b> in every column of the DataFrame has been replaced with <b>None</b>.
Note that if you’d like to only replace <b>NaN</b> values with <b>None</b> in one particular column, you can use the following syntax:
<b>#replace NaN values with None in column 'B' only
df['B'] = df['B'].replace(np.nan, None)
#view updated DataFrame
print(df)
      A     B    C     D
0   5.0  None  2.0   5.0
1   6.0  12.0  7.0   NaN
2   8.0  None  6.0   6.0
3   NaN  10.0  3.0  15.0
4   4.0  23.0  2.0   1.0
5  15.0   6.0  4.0   NaN
6  13.0   4.0  NaN   4.0
</b>
Notice that the <b>NaN</b> values have been replaced with <b>None</b> in column ‘B’ only.
<b>Related:</b>  How to Replace NaN Values with Zero in Pandas 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Replace Specific Values in Pandas 
 How to Filter a Pandas DataFrame by Column Values 
 How to Fill NA Values for Multiple Columns in Pandas 
<h2><span class="orange">Pandas: How to Replace NaN Values with String</span></h2>
You can use the following methods to replace NaN values with strings in a pandas DataFrame:
<b>Method 1: Replace NaN Values with String in Entire DataFrame</b>
<b>df.fillna('', inplace=True)
</b>
<b>Method 2: Replace NaN Values with String in Specific Columns</b>
<b>df[['col1', 'col2']] = df[['col1','col2']].fillna('')</b>
<b>Method 3: Replace NaN Values with String in One Column</b>
<b>df.col1 = df.col1.fillna('')</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame with some NaN values
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [np.nan, 11, 7, 7, 8, 6, 14, 15],   'assists': [5, np.nan, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, np.nan, 6, 5, 9, np.nan]})
#view DataFrame
df
teampointsassistsrebounds
0ANaN5.011.0
1A11.0NaN8.0
2A7.07.010.0
3A7.09.0NaN
4B8.012.06.0
5B6.09.05.0
6B14.09.09.0
7B15.04.0NaN
</b>
<h3>Method 1: Replace NaN Values with String in Entire DataFrame</h3>
The following code shows how to replace every NaN value in an entire DataFrame with an empty string:
<b>#replace NaN values in all columns with empty string
df.fillna('', inplace=True)
#view updated DataFrame
df
teampointsassistsrebounds
0A5.011.0
1A11.08.0
2A7.07.010.0
3A7.09.0
4B8.012.06.0
5B6.09.05.0
6B14.09.09.0
7B15.04.0
</b>
Notice that every NaN value in each column has been replaced with an empty string.
<h3>Method 2: Replace NaN Values with String in Specific Columns</h3>
The following code shows how to replace NaN values in specific columns with a specific string:
<b>#replace NaN values in 'points' and 'rebounds' columns with 'none'
df[['points', 'rebounds']] = df[['points', 'rebounds']].fillna('none')
#view updated DataFrame
df
        teampointsassistsrebounds
0Anone5.011.0
1A11.0NaN8.0
2A7.07.010.0
3A7.09.0none
4B8.012.06.0
5B6.09.05.0
6B14.09.09.0
7B15.04.0none
</b>
Notice that the NaN values in the ‘points’ and ‘rebounds’ columns were replaced with the string ‘none’, but the NaN values in the ‘assists’ column remained unchanged.
<h3>Method 3: Replace NaN Values with String in One Column</h3>
The following code shows how to replace NaN values in one column with a specific string:
<b>#replace NaN values in 'points' column with 'zero'
df.points = df.points.fillna('zero')
#view updated DataFrame
df
teampointsassistsrebounds
0Azero5.011.0
1A11.0NaN8.0
2A7.07.010.0
3A7.09.0NaN
4B8.012.06.0
5B6.09.05.0
6B14.09.09.0
7B15.04.0NaN
</b>
Notice that the NaN value in the ‘points’ column was replaced replaced with the string ‘zero’, but the NaN values in the ‘assists’ and ‘rebounds’ columns remained unchanged.
<h2><span class="orange">How to Replace NaN Values with Zero in Pandas</span></h2>
You can use the following methods to replace NaN values with zeros in a pandas DataFrame:
<b>Method 1: Replace NaN Values with Zero in One Column</b>
<b>df['col1'] = df['col1'].fillna(0)
</b>
<b>Method 2: Replace NaN Values with Zero in Several Columns</b>
<b>df[['col1', 'col2']] = df[['col1', 'col2']].fillna(0)</b>
<b>Method 3: Replace NaN Values with Zero in All Columns</b>
<b>df = df.fillna(0)</b>
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'points': [25, np.nan, 15, 14, 19, 23, 25, 29],   'assists': [5, np.nan, 7, np.nan, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, np.nan, 9, np.nan]})
#view DataFrame
print(df)
   points  assists  rebounds
0    25.0      5.0      11.0
1     NaN      NaN       8.0
2    15.0      7.0      10.0
3    14.0      NaN       6.0
4    19.0     12.0       6.0
5    23.0      9.0       NaN
6    25.0      9.0       9.0
7    29.0      4.0       NaN
</b>
<h3>Method 1: Replace NaN Values with Zero in One Column</h3>
The following code shows how to replace NaN values with zero in just the ‘assists’ column:
<b>#replace NaN values with zero in 'assists' column
df['assists'] = df['assists'].fillna(0)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    25.0      5.0      11.0
1     NaN      0.0       8.0
2    15.0      7.0      10.0
3    14.0      0.0       6.0
4    19.0     12.0       6.0
5    23.0      9.0       NaN
6    25.0      9.0       9.0
7    29.0      4.0       NaN</b>
Notice that the NaN values in the ‘assists’ column have been replaced with zeros, but the NaN values in every other column still remain.
<h3>Method 2: Replace NaN Values with Zero in Several Columns</h3>
The following code shows how to replace NaN values with zero in the ‘points’ and ‘assists’ columns:
<b>#replace NaN values with zero in 'points' and 'assists' column
df[['points', 'assists']] = df[['points', 'assists']].fillna(0)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    25.0      5.0      11.0
1     0.0      0.0       8.0
2    15.0      7.0      10.0
3    14.0      0.0       6.0
4    19.0     12.0       6.0
5    23.0      9.0       NaN
6    25.0      9.0       9.0
7    29.0      4.0       NaN</b>
<h3>Method 3: Replace NaN Values with Zero in All Columns</h3>
The following code shows how to replace NaN values with zero in every column of the DataFrame:
<b>#replace NaN values with zero in all columns
df = df.fillna(0)
#view updated DataFrame
print(df)
   points  assists  rebounds
0    25.0      5.0      11.0
1     0.0      0.0       8.0
2    15.0      7.0      10.0
3    14.0      0.0       6.0
4    19.0     12.0       6.0
5    23.0      9.0       0.0
6    25.0      9.0       9.0
7    29.0      4.0       0.0</b>
<h2><span class="orange">Pandas: How to Replace Values in Column Based on Condition</span></h2>
You can use the following basic syntax to replace values in a column of a pandas DataFrame based on a condition:
<b>#replace values in 'column1' that are greater than 10 with 20
df.loc[df['column1'] > 10, 'column1'] = 20
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Replace Values in Column Based on One Condition</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 13, 9, 14],   'assists': [3, 8, 2, 6, 6, 5, 9, 5]})
#view DataFrame
df
teamposition pointsassists
0AG 53
1AG 78
2AF 72
3AF 96
4BG 126
5BG 135
6BF 99
7BF 145       
</b>
We can use the following code to replace every value in the ‘points’ column that is greater than 10 with a value of 20:
<b>#replace any values in 'points' column greater than 10 with 20
df.loc[df['points'] > 10, 'points'] = 20
#view updated DataFrame
df
teamposition pointsassists
0AG 53
1AG 78
2AF 72
3AF 96
4BG 206
5BG 205
6BF 99
7BF 205
</b>
Notice that each of the three values in the ‘points’ column that were greater than 10 got replaced with the value 20.
<h3>Example 2: Replace Values in Column Based on Multiple Conditions</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 13, 9, 14],   'assists': [3, 8, 2, 6, 6, 5, 9, 5]})
#view DataFrame
df
teamposition pointsassists
0AG 53
1AG 78
2AF 72
3AF 96
4BG 126
5BG 135
6BF 99
7BF 145       
</b>
We can use the following code to replace every value in the ‘position’ column where points is less than 10 <em>or</em> where assists is less than 5 with the string ‘Bad’:
<b>#replace string in 'position' column with 'bad' if points &lt; 10 or assists &lt; 5
df.loc[(df['points'] &lt; 10) | (df['assists'] &lt; 5), 'position'] = 'Bad'
#view updated DataFrame
df
        teamposition pointsassists
0ABad 53
1ABad 78
2ABad 72
3ABad 96
4BG 206
5BG 205
6BBad 99
7BF 205
</b>
Similarly, we can use the following code to replace every value in the ‘position’ column where points is less than 10 <i>and </i>where assists is less than 5 with the string ‘Bad’:
<b>#replace string in 'position' column with 'bad' if points &lt; 10 and assists &lt; 5
df.loc[(df['points'] &lt; 10) & (df['assists'] &lt; 5), 'position'] = 'Bad'
#view updated DataFrame
df
        teamposition pointsassists
0ABad 53
1AG 78
2ABad 72
3AF 96
4BG 126
5BG 135
6BF 99
7BF 145</b>
Notice that the two rows where points was less than 10 <em>and</em> assists was less than 5 had their ‘position’ value replaced with the string ‘Bad’.
<h2><span class="orange">How to Replace Values in a Pandas DataFrame (With Examples)</span></h2>
Often you may want to replace the values in one or more columns of a pandas DataFrame.
Fortunately this is easy to do using the  .replace()  function.
This tutorial provides several examples of how to use this function in practice on the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C'],   'division':['E', 'W', 'E', 'E', 'W', 'W', 'E'],   'rebounds': [11, 8, 7, 6, 6, 5, 12]})
#view DataFrame
print(df)
  team division  rebounds
0    A        E        11
1    A        W         8
2    B        E         7
3    B        E         6
4    B        W         6
5    C        W         5
6    C        E        12</b>
<h2>Example 1: Replace a Single Value in an Entire DataFrame</h2>
The following code shows how to replace a single value in an entire pandas DataFrame:
<b>#replace 'E' with 'East'
df = df.replace(['E'],'East')
#view DataFrame
print(df)
  team division  rebounds
0    A     East        11
1    A        W         8
2    B     East         7
3    B     East         6
4    B        W         6
5    C        W         5
6    C     East        12</b>
<h2>Example 2: Replace Multiple Values in an Entire DataFrame</h2>
The following code shows how to replace multiple values in an entire pandas DataFrame:
<b>#replace 'E' with 'East' and 'W' with 'West'
df = df.replace(['E', 'W'],['East', 'West'])
#view DataFrame
print(df)
        teamdivision  rebounds
0AEast  11
1AWest  8
2BEast  7
3BEast  6
4BWest  6
5CWest  5
6CEast  12
</b>
<h2>Example 3: Replace a Single Value in a Single Column</h2>
The following code shows how to replace a single value in a single column:
<b>#replace 6 with 0 in <em>rebounds</em> column
df['rebounds'] = df['rebounds'].replace(6, 0)
#view DataFrame
print(df)
        teamdivision  rebounds
0AE  11
1AW  8
2BE  7
3BE  0
4BW  0
5CW  5
6CE  12
</b>
<h2>Example 4: Replace Multiple Values in a Single Column</h2>
The following code shows how to replace multiple values in a single column:
<b>#replace 6, 11, and 8 with 0, 1 and 2 in <em>rebounds</em> column
df['rebounds'] = df['rebounds'].replace([6, 11, 8], [0, 1, 2])
#view DataFrame
print(df)
teamdivisionrebounds
0AE1
1AW2
2BE7
3BE0
4BW0
5CW5
6CE12
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Replace NaN Values with Zeros in Pandas 
 How to Replace Empty Strings with NaN in Pandas 
 How to Replace Values in Column Based on Condition in Pandas 
<h2><span class="orange">How to Replicate Rows in a Pandas DataFrame</span></h2>
You can use the following basic syntax to replicate each row in a pandas DataFrame a certain number of times:
<b>#replicate each row 3 times
df_new = pd.DataFrame(np.repeat(df.values, 3, axis=0))
</b>
The number in the second argument of the NumPy <b>repeat()</b> function specifies the number of times to replicate each row.
The following example shows how to use this syntax in practice.
<h2>Example: Replicate Rows in a Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 20, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 5],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      20        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        5         5
</b>
We can use the following syntax to replicate each row in the DataFrame three times:
<b>import numpy as np
#define new DataFrame as original DataFrame with each row repeated 3 times
df_new = pd.DataFrame(np.repeat(df.values, 3, axis=0))
#assign column names of original DataFrame to new DataFrame
df_new.columns = df.columns
#view new DataFrame
print(df_new)
   team points assists rebounds
0     A     18       5       11
1     A     18       5       11
2     A     18       5       11
3     B     20       7        8
4     B     20       7        8
5     B     20       7        8
6     C     19       7       10
7     C     19       7       10
8     C     19       7       10
9     D     14       9        6
10    D     14       9        6
11    D     14       9        6
12    E     14      12        6
13    E     14      12        6
14    E     14      12        6
15    F     11       5        5
16    F     11       5        5
17    F     11       5        5</b>
The new DataFrame contains each of the rows from the original DataFrame, replicated three times each.
Notice that the index values have also been reset.
The values in the index now range from 0 to 17.
<b>Note</b>: You can find the complete documentation for the NumPy <b>repeat()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Find the Difference Between Two Columns 
 Pandas: How to Find the Difference Between Two Rows 
 Pandas: How to Sort Columns by Name 
<h2><span class="orange">Pandas: How to Reset Index After Using dropna()</span></h2>
You can use the following basic syntax to reset an index of a pandas DataFrame after using the <b>dropna()</b> function to remove rows with missing values:
<b>df = df.dropna().reset_index(drop=True)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Reset Index in Pandas After Using dropna()</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
import numpy as np
#create dataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, np.nan, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, np.nan, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, np.nan, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A    18.0      5.0      11.0
1    B     NaN      7.0       8.0
2    C    19.0      7.0      10.0
3    D    14.0      9.0       6.0
4    E    14.0     12.0       6.0
5    F    11.0      NaN       5.0
6    G    20.0      9.0       NaN
7    H    28.0      4.0      12.0
</b>
Now suppose we use the <b>dropna()</b> function to drop all rows from the DataFrame that have a missing value in any column:
<b>#drop rows with nan values in any column
df = df.dropna()
#view updated DataFrame
print(df)
  team  points  assists  rebounds
0    A    18.0      5.0      11.0
2    C    19.0      7.0      10.0
3    D    14.0      9.0       6.0
4    E    14.0     12.0       6.0
7    H    28.0      4.0      12.0</b>
Notice that the index still contains the original index values for each row.
To reset the index after using the <b>dropna()</b> function, we can use the following syntax:
<b>#drop rows with nan values in any column
df = df.dropna().reset_index(drop=True)
#view updated DataFrame
print(df)
  team  points  assists  rebounds
0    A    18.0      5.0      11.0
1    C    19.0      7.0      10.0
2    D    14.0      9.0       6.0
3    E    14.0     12.0       6.0
4    H    28.0      4.0      12.0
</b>
Notice that each of the rows with missing values have been removed and the index values have been reset.
The index values now range from 0 to 4.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Filter by Index Value in Pandas 
 How to Use First Column as Index in Pandas 
<h2><span class="orange">How to Reset an Index in Pandas DataFrame (With Examples)</span></h2>
You can use the following syntax to reset an index in a pandas DataFrame:
<b>df.reset_index(drop=True, inplace=True)
</b>
Note the following arguments:
<b>drop</b>: Specifying <b>True</b> prevents pandas from saving the original index as a column in the DataFrame.
<b>inplace</b>: Specifying <b>True</b> allows pandas to replace the index in the original DataFrame instead of creating a copy of the DataFrame.
The following examples show how to use this sytnax in practice.
<h3>Example 1: Reset Index & Drop Old Index</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]},   index=[0, 4, 3, 5, 2, 1, 7, 6])
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
4      12        7         8
3      15        7        10
5      14        9         6
2      19       12         6
1      23        9         5
7      25        9         9
6      29        4        12
</b>
The following code shows how to reset the index of the DataFrame and drop the old index completely:
<b>#reset index
df.reset_index(drop=True, inplace=True)
#view updated DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12</b>
Notice that the index has been reset and the values in the index now range from 0 to 7.
<h3>Example 2: Reset Index & Retain Old Index as Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]},   index=['A', 'C', 'D', 'B', 'E', 'G', 'F', 'H'])
#view DataFrame
print(df)
   points  assists  rebounds
A      25        5        11
C      12        7         8
D      15        7        10
B      14        9         6
E      19       12         6
G      23        9         5
F      25        9         9
H      29        4        12
</b>
The following code shows how to reset the index of the DataFrame and retain the old index as a column in the DataFrame:
<b>#reset index and retain old index as a column
df.reset_index(inplace=True)
#view updated DataFrame
print(df)
  index  points  assists  rebounds
0     A      25        5        11
1     C      12        7         8
2     D      15        7        10
3     B      14        9         6
4     E      19       12         6
5     G      23        9         5
6     F      25        9         9
7     H      29        4        12</b>
Notice that the index has been reset and the values in the index now range from 0 to 7.
Also notice that the old index (with letters) is retained as a new column in the DataFrame called ‘index.’
<h2><span class="orange">Pandas: Return Row with Max Value in Particular Column</span></h2>
You can use the following methods to return the row of a pandas DataFrame that contains the max value in a particular column:
<b>Method 1: Return Row with Max Value</b>
<b>df[df['my_column'] == df['my_column'].max()]
</b>
<b>Method 2: Return Index of Row with Max Value</b>
<b>df['my_column'].idxmax()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 28, 20],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      28        9         9
7    H      20        4        12</b>
<h2>Example 1: Return Row with Max Value</h2>
The following code shows how to return the row in the DataFrame with the max value in the <b>points</b> column:
<b>#return row with max value in points column
df[df['points'] == df['points'].max()]
        teampointsassistsrebounds
6G2899
</b>
The max value in the points column was <b>28</b>, so the row that contained this value was returned.
<h2>Example 2: Return Index of Row with Max Value</h2>
The following code shows how to return only the index of the row with the max value in the <b>points</b> column:
<b>#return row that contains max value in points column
df['points'].idxmax()
6</b>
The row in index position <b>6</b> contained the max value in the <b>points</b> column, so a value of <b>6</b> was returned.
<b>Related:</b>  How to Use idxmax() Function in Pandas (With Examples) 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Find the Max Value by Group in Pandas 
 How to Find the Max Value of Columns in Pandas 
<h2><span class="orange">How to Reverse a Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to reverse the rows in a pandas DataFrame:
<b>df_reversed = df[::-1]
</b>
If you’d like to reverse the rows in the DataFrame <em>and</em> reset the index values, you can use the following syntax:
<b>df_reversed = df[::-1].reset_index(drop=True)</b>
The following example shows how to use this syntax in practice.
<h2>Example: How to Reverse a Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
We can use the following syntax to reverse the rows in the DataFrame:
<b>#create new DataFrame with rows reversed
df_reversed = df[::-1]
#view new DataFrame
print(df_reversed)
  team  points  assists
7    H      28        4
6    G      20        9
5    F      11        9
4    E      14       12
3    D      14        9
2    C      19        7
1    B      22        7
0    A      18        5
</b>
Notice that the order of the rows in the DataFrame have been reversed.
However, each row still contains its original index value.
If you’d like to reverse the rows of the DataFrame <em>and</em> reset the index values, you can use the following syntax:
<b>#create reversed DataFrame and reset index values
df_reversed = df[::-1].reset_index(drop=True)
#view new DataFrame
print(df_reversed)
  team  points  assists
0    H      28        4
1    G      20        9
2    F      11        9
3    E      14       12
4    D      14        9
5    C      19        7
6    B      22        7
7    A      18        5
</b>
Notice that the order of rows has been reversed and the index values have been reset.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Rows with NaN Values in Pandas 
 How to Find First Row that Meets Criteria in Pandas 
 How to Get Last Row in Pandas 
<h2><span class="orange">How to Calculate a Rolling Maximum in Pandas (With Examples)</span></h2>
You can use the following methods to calculate a rolling maximum value in a pandas DataFrame:
<b>Method 1: Calculate Rolling Maximum</b>
<b>df['rolling_max'] = df.values_column.cummax()</b>
<b>Method 2: Calculate Rolling Maximum by Group</b>
<b>df['rolling_max'] = df.groupby('group_column').values_column.cummax()
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Calculate Rolling Maximum</h2>
Suppose we have the following pandas DataFrame that shows the sales made each day at some store:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],   'sales': [4, 6, 5, 8, 14, 13, 13, 12, 9, 8, 19, 14]})
#view DataFrame
print(df)
    day  sales
0     1      4
1     2      6
2     3      5
3     4      8
4     5     14
5     6     13
6     7     13
7     8     12
8     9      9
9    10      8
10   11     19
11   12     14
</b>
We can use the following syntax to create a new column that displays the rolling maximum value of sales:
<b>#add column that displays rolling maximum of sales
df['rolling_max'] = df.sales.cummax()
#view updated DataFrame
print(df)
    day  sales  rolling_max
0     1      4            4
1     2      6            6
2     3      5            6
3     4      8            8
4     5     14           14
5     6     13           14
6     7     13           14
7     8     12           14
8     9      9           14
9    10      8           14
10   11     19           19
11   12     14           19</b>
The new column titled <b>rolling_max</b> displays the rolling maximum value of sales.
<h2>
<b>Example 2: Calculate Rolling Maximum by Group</b>
</h2>
Suppose we have the following pandas DataFrame that shows the sales made each day at two different stores:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'A', 'A',             'B', 'B', 'B', 'B', 'B', 'B'],   'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],   'sales': [4, 6, 5, 8, 14, 13, 13, 12, 9, 8, 19, 14]})
#view DataFrame
print(df)
   store  day  sales
0      A    1      4
1      A    2      6
2      A    3      5
3      A    4      8
4      A    5     14
5      A    6     13
6      B    7     13
7      B    8     12
8      B    9      9
9      B   10      8
10     B   11     19
11     B   12     14
</b>
We can use the following syntax to create a new column that displays the rolling maximum value of sales grouped by store:
<b>#add column that displays rolling maximum of sales grouped by store
df['rolling_max'] = df.groupby('store').sales.cummax()
#view updated DataFrame
print(df)
   store  day  sales  rolling_max
0      A    1      4            4
1      A    2      6            6
2      A    3      5            6
3      A    4      8            8
4      A    5     14           14
5      A    6     13           14
6      B    7     13           13
7      B    8     12           13
8      B    9      9           13
9      B   10      8           13
10     B   11     19           19
11     B   12     14           19</b>
The new column titled <b>rolling_max</b> displays the rolling maximum value of sales, grouped by store.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">How to Calculate Rolling Median in Pandas (With Examples)</span></h2>
A <b>rolling median </b>is the median of a certain number of previous periods in a time series.
To calculate the rolling median for a column in a pandas DataFrame, we can use the following syntax:
<b>#calculate rolling median of previous 3 periods
df['column_name'].rolling(3).median()
</b>
The following example shows how to use this function in practice.
<h3>Example : Calculate Rolling Median of Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'month': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],   'leads': [13, 15, 16, 15, 17, 20, 22, 24, 25, 26, 23, 24],   'sales': [22, 24, 23, 27, 26, 26, 27, 30, 33, 32, 27, 25]})
#view DataFrame
df
monthleadssales
011322
121524
231623
341527
451726
562026
672227
782430
892533
9102632
10112327
11122425
</b>
We can use the following syntax to create a new column that contains the rolling median of ‘sales’ for the previous 3 periods:
<b>#calculate 3-month rolling median
df['sales_rolling3'] = df['sales'].rolling(3).median()
#view updated data frame
df
monthleadssalessales_rolling3
011322NaN
121524NaN
23162323.0
34152724.0
45172626.0
56202626.0
67222726.0
78243027.0
89253330.0
910263232.0
1011232732.0
1112242527.0</b>
We can manually verify that the rolling median sales displayed for month 3 is the median of the previous 3 months:
Median of 22, 24, 23 = <b>23.0</b>
Similarly, we can verify the rolling median sales of month 4:
Median of 24, 23, 27 = <b>24.0</b>
We can use similar syntax to calculate the rolling 6-month median:
<b>#calculate 6-month rolling median
df['sales_rolling6'] = df['sales'].rolling(6).median() 
#view updated data frame
df
monthleadssalessales_rolling3sales_rolling6
011322NaNNaN
121524NaNNaN
23162323.0NaN
34152724.0NaN
45172626.0NaN
56202626.025.0
67222726.026.0
78243027.026.5
89253330.027.0
910263232.028.5
1011232732.028.5
1112242527.028.5
</b>
<h2><span class="orange">How to Round a Single Column in Pandas DataFrame</span></h2>
You can use the following basic syntax to round the values in a single column of a pandas DataFrame:
<b>df.my_column = df.my_column.round()
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Round a Single Column in Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that contains information about various athletes:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'athlete': ['A', 'B', 'C', 'D', 'E', 'F'],   'time': [12.443, 15.8, 16.009, 5.06, 11.075, 12.9546],   'points': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
  athlete     time  points
0       A  12.4430       5
1       B  15.8000       7
2       C  16.0090       7
3       D   5.0600       9
4       E  11.0750      12
5       F  12.9546       9</b>
We can use the following code to round each value in the <b>time</b> column to the nearest integer:
<b>#round values in 'time' column of DataFrame
df.time = df.time.round()
#view updated DataFrame
print(df)
  athlete  time  points
0       A  12.0       5
1       B  16.0       7
2       C  16.0       7
3       D   5.0       9
4       E  11.0      12
5       F  13.0       9
</b>
Each value in the <b>time</b> column has been rounded to the nearest integer.
For example:
<b>12.443</b> has been rounded to <b>12</b>.
<b>15.8</b> has been rounded to <b>16</b>.
<b>16.009</b> has been rounded to <b>16</b>.
And so on.
To round the values in a column to a specific number of decimal places, simply specify that value in the <b>round()</b> function.
For example, we can use the following code to round each value in the <b>time</b> column to two decimal places:
<b>#round values in 'time' column to two decimal places
df.time = df.time.round(2)
#view updated DataFrame
print(df)
  athlete   time  points
0       A  12.44       5
1       B  15.80       7
2       C  16.01       7
3       D   5.06       9
4       E  11.08      12
5       F  12.95       9</b>
Each value in the <b>time</b> column has been rounded to two decimal places.
For example:
<b>12.443</b> has been rounded to <b>12.44</b>.
<b>15.8</b> has been rounded to <b>15.80</b>.
<b>16.009</b> has been rounded to <b>1601</b>.
And so on.
Also note that the values in the other numeric column, <b>points</b>, have remained unchanged.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Show All Rows of a Pandas DataFrame 
 How to Check dtype for All Columns in Pandas DataFrame 
<h2><span class="orange">How to Convert Pandas DataFrame Row to List (With Example)</span></h2>
You can use the following basic syntax to convert a row in a pandas DataFrame to a list:
<b>row_list = df.loc[2, :].values.flatten().tolist()</b>
This particular syntax converts the values in row index position <b>2</b> of the DataFrame into a list.
The following example shows how to use this syntax in practice.
<h2>Example: Convert Pandas DataFrame Row to List</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use the following syntax to convert the values in row index position <b>2</b> to a list:
<b>#convert row at index 2 to list
row_list = df.loc[2, :].values.flatten().tolist()
#view results
print(row_list)
['C', 19, 7, 10]</b>
We can see that the values in row index position 2 have been converted to a list with four values.
 We can confirm that the result is indeed a list by using the <b>type()</b> function:
<b>#view type
print(type(row_list))
&lt;class 'list'></b>
If you only want the values from specific columns to be included in the list, you can specify the columns by name.
For example, we can use the following syntax to convert the values in row index position 2 to a list for the <b>team</b> and <b>points</b> columns only:
<b>#convert values in row index position 2 to list (for team and points columns)
row_list = df.loc[2, ['team', 'points']].values.flatten().tolist()
#view results
print(row_list)
['C', 19]</b>
Notice that only the values in the <b>team</b> and <b>points</b> columns have been included in the list.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Convert Specific Columns in Pandas to NumPy Array 
 How to Convert List to a Column in Pandas 
 How to Add a Total Row to Pandas DataFrame 
<h2><span class="orange">How to Access Sample Datasets in Pandas</span></h2>
Often you may want to access sample datasets in pandas to play around with and practice different functions.
Fortunately you can build sample pandas datasets by using the built-in <b>testing</b> feature.
The following examples show how to use this feature.
<h3>Example 1: Create Pandas Dataset with All Numeric Columns</h3>
The following code shows how to create a pandas dataset with all numeric columns:
<b>import pandas as pd
#create sample dataset
df1 = pd.util.testing.makeDataFrame()
#view dimensions of dataset
print(df1.shape)
(30, 4)
#view first five rows of dataset
print(df1.head())
   A         B         C         D
s8tpz0W5mF -0.751223  0.956338 -0.441847  0.695612
CXQ9YhLhk8 -0.210881 -0.231347 -0.227672 -0.616171
KAbcor6sQK  0.727880  0.128638 -0.989993  1.094069
IH3bptMpdb -1.599723  1.570162 -0.221688  2.194936
gaR9ZxBTrH  0.025171 -0.446555  0.169873 -1.583553
</b>
By default, the <b>makeDataFrame()</b> function creates a pandas DataFrame with 30 rows and 4 columns in which all of the columns are numeric.
<h3>Example 2: Create Pandas Dataset with Mixed Columns</h3>
The following code shows how to create a pandas dataset with all numeric columns:
<b>import pandas as pd
#create sample dataset
df2 = pd.util.testing.makeMixedDataFrame()
#view dimensions of dataset
print(df2.shape)
(5, 4)
#view first five rows of dataset
print(df2.head())
     A    B     C          D
0  0.0  0.0  foo1 2009-01-01
1  1.0  1.0  foo2 2009-01-02
2  2.0  0.0  foo3 2009-01-05
3  3.0  1.0  foo4 2009-01-06
4  4.0  0.0  foo5 2009-01-07
</b>
By default, the <b>makeMixedDataFrame()</b> function creates a pandas DataFrame with 5 rows and 4 columns in which the columns are a variety of data types.
We can use the following code to display the  data type of each column :
<b>#display data type of each column
df2.dtypes
A           float64
B           float64
C            object
D    datetime64[ns]
dtype: object
</b>
From the output we can see:
Column A is numeric
Column B is numeric
Column C is a string
Column D is a date
<h3>Example 3: Create Pandas Dataset with Missing Values</h3>
The following code shows how to create a pandas dataset with some missing values in various columns:
<b>import pandas as pd
#create sample dataset
df3 = pd.util.testing.makeMissingDataFrame()
#view dimensions of dataset
print(df3.shape)
(30, 4)
#view first five rows of dataset
print(df3.head())
   A         B         C         D
YgAQaNaGfG  0.444376 -2.264920  1.117377 -0.087507
JoT4KxJeHd  1.913939  1.287006 -0.331315 -0.392949
tyrA2P6wz3       NaN  2.988521  0.399583  0.095831
1qvPc9DU1t  0.028716  1.311452 -0.237756 -0.150362
3aAXYtXjIO -1.069339  0.332067  0.204074       NaN
</b>
By default, the <b>makeMissingDataFrame()</b> function creates a pandas DataFrame with 30 rows and 4 columns in which there are some missing values (NaN) in various columns.
This function is particularly useful because it allows you to work with a dataset that has some missing values, which is common in real-world datasets.
<h2><span class="orange">How to Randomly Sample Rows in Pandas (With Examples)</span></h2>
You can use the following basic syntax to randomly sample rows from a pandas DataFrame:
<b>#randomly select one row
df.sample()
#randomly select <em>n</em> rows
df.sample(n=5)
#randomly select <em>n</em> rows with repeats allowed
df.sample(n=5, replace=True) 
#randomly select a fraction of the total rows
df.sample(frac=0.3)
#randomly select <em>n</em> rows by group
df.groupby('team', group_keys=False).apply(lambda x: x.sample(2))
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame 
df
        teampointsassists rebounds
0A255 11
1A127 8
2A157 10
3A149 6
4B1912 6
5B239 5
6B259 9
7B294 12
</b>
<h3>Example 1: Randomly Select One Row</h3>
The following code shows how to randomly select one row from the DataFrame:
<b>#randomly select one row
df.sample()
        teampointsassistsrebounds
5B2395
</b>
<h3>Example 2: Randomly Select <em>n</em> Rows</h3>
The following code shows how to randomly select <em>n</em> rows from the DataFrame:
<b>#randomly select <em>n</em> rows
df.sample(n=5)
        teampointsassistsrebounds
5B2395
2A15710
4B19126
6B2599
1A1278</b>
<h3>Example 3: Randomly Select <em>n</em> Rows with Repeats Allowed</h3>
The following code shows how to randomly select <em>n</em> rows from the DataFrame, with repeat rows allowed:
<b>#randomly select 5 rows with repeats allowed
df.sample(n=5, replace=True) 
teampointsassistsrebounds
6B2599
7B29412
5B2395
1A1278
5B2395
</b>
<h3>Example 4: Randomly Select A Fraction of the Total Rows</h3>
The following code shows how to randomly select a fraction of the total rows from the DataFrame
<b>#randomly select 25% of rows
df.sample(frac=0.25) 
teampointsassistsrebounds
2A15710
1A1278</b>
<h3>Example 5: Randomly Select <em>n</em> Rows by Group</h3>
The following code shows how to randomly select <em>n</em> rows by group from the DataFrame
<b>#randomly select 2 rows from each team
df.groupby('team', group_keys=False).apply(lambda x: x.sample(2))
        teampointsassistsrebounds
0A25511
2A15710
7B29412
4B19126</b>
Notice that 2 rows from team ‘A’ and 2 rows from team ‘B’ were randomly sampled.
<b>Note</b>: You can find the complete documentation for the pandas <b>sample()</b> function  here .
<h2><span class="orange">How to Save Pandas DataFrame for Later Use (With Example)</span></h2>
Often you may want to save a pandas DataFrame for later use without the hassle of importing the data again from a CSV file.
The easiest way to do this is by using  to_pickle()  to save the DataFrame as a pickle file:
<b>df.to_pickle("my_data.pkl")</b>
This will save the DataFrame in your current working environment.
You can then use  read_pickle()  to quickly read the DataFrame from the pickle file:
<b>df = pd.read_pickle("my_data.pkl")
</b>
The following example shows how to use these functions in practice.
<h2>Example: Save and Load Pandas DataFrame</h2>
Suppose we create the following pandas DataFrame that contains information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use <b>df.info()</b> to view the data type of each variable in the DataFrame:
<b>#view DataFrame info
print(df.info())
&lt;class 'pandas.core.frame.DataFrame'>
RangeIndex: 8 entries, 0 to 7
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   team      8 non-null      object
 1   points    8 non-null      int64 
 2   assists   8 non-null      int64 
 3   rebounds  8 non-null      int64 
dtypes: int64(3), object(1)
memory usage: 292.0+ bytes
None
</b>
We can use the <b>to_pickle()</b> function to save this DataFrame to a pickle file with a <b>.pkl</b> extension:
<b>#save DataFrame to pickle file
df.to_pickle("my_data.pkl")
</b>
Our DataFrame is now saved as a pickle file in our current working environment.
We can then use the <b>read_pickle()</b> function to quickly read the DataFrame:
<b>#read DataFrame from pickle file
df= pd.read_pickle("my_data.pkl")
#view DataFrame
print(df)
teampointsassistsrebounds
0A18511
1B2278
2C19710
3D1496
4E14126
5F1195
6G2099
7H28412
</b>
We can use <b>df.info()</b> again to confirm that the data type of each column is the same as before:
<b>#view DataFrame info
print(df.info())
&lt;class 'pandas.core.frame.DataFrame'>
RangeIndex: 8 entries, 0 to 7
Data columns (total 4 columns):
 #   Column    Non-Null Count  Dtype 
---  ------    --------------  ----- 
 0   team      8 non-null      object
 1   points    8 non-null      int64 
 2   assists   8 non-null      int64 
 3   rebounds  8 non-null      int64 
dtypes: int64(3), object(1)
memory usage: 292.0+ bytes
None</b>
The benefit of using pickle files is that the data type of each column is retained when we save and load the DataFrame.
This provides an advantage over saving and loading CSV files because we don’t have to perform any transformations on the DataFrame since the pickle file preserves the original state of the DataFrame.
<h2><span class="orange">How to Make a Scatterplot From a Pandas DataFrame</span></h2>
There are two ways to create a scatterplot using data from a pandas DataFrame:
<b>1. Use pandas.DataFrame.plot.scatter</b>
One way to create a scatterplot is to use the built-in pandas  plot.scatter()  function:
<b>import pandas as pd
df.plot.scatter(x = 'x_column_name', y = 'y_columnn_name')</b>
<b>2. Use matplotlib.pyplot.scatter</b>
Another way to create a scatterplot is to use the Matplotlib  pyplot.scatter()  function:
<b>import matplotlib.pyplot as plt
plt.scatter(df.x, df.y)
</b>
This tutorial provides an example of how to use each of these methods.
<h3>Example 1: Use Pandas</h3>
The following code shows how to use the  plot.scatter()  function to create a simple scatterplot:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [1, 3, 3, 4, 5, 7, 9, 12, 13, 15],   'y': [5, 7, 9, 7, 6, 12, 14, 18, 15, 22]})
#create scatterplot
df.plot.scatter(x='x', y='y')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/scatter1.png">
Note that you can use the <b>s</b> and <b>c</b> arguments to modify the size and color of the points, respectively:
<b>df.plot.scatter(x='x', y='y', s=60, c='green')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/scatter2.png">
<h3>Example 2: Use Matplotlib</h3>
The following code shows how to use the  pyplot.scatter()  function to create a scatterplot:
<b>import pandas as pd
import matplotlib.pyplot as plt
#create DataFrame
df = pd.DataFrame({'x': [1, 3, 3, 4, 5, 7, 9, 12, 13, 15],   'y': [5, 7, 9, 7, 6, 12, 14, 18, 15, 22]})
#create scatterplot
plt.scatter(df.x, df.y)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/scatter3.png">
Note that you can use the <b>s</b> and <b>c</b> arguments to modify the size and color of the points, respectively:
<b>plt.scatter(df.x, df.y, s=60, c='purple')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/scatter4.png">
<em>You can find more Python tutorials  here .</em>
<h2><span class="orange">How to Select Columns by Index in a Pandas DataFrame</span></h2>
Often you may want to select the columns of a pandas DataFrame based on their index value.
If you’d like to select columns based on integer indexing, you can use the <b>.iloc</b> function.
If you’d like to select columns based on label indexing, you can use the <b>.loc</b> function.
This tutorial provides an example of how to use each of these functions in practice.
<h3>Example 1: Select Columns Based on Integer Indexing</h3>
The following code shows how to create a pandas DataFrame and use <b>.iloc</b> to select the column with an index integer value of <b>3</b>:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [11, 7, 8, 10, 13, 13],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
teampointsassistsrebounds
0A11511
1A778
2A8710
3B1096
4B13126
5B1395
#select column with index position 3
df.iloc[:, 3]
0    11
1     8
2    10
3     6
4     6
5     5
Name: rebounds, dtype: int64</b>
We can use similar syntax to select multiple columns:
<b>#select columns with index positions 1 and 3
df.iloc[:, [1, 3]]
        pointsrebounds
01111
178
2810
3106
4136
5135
</b>
Or we could select all columns in a range:
<b>#select columns with index positions in range 0 through 3
df.iloc[:, 0:3]
        teampointsassists
0A115
1A77
2A87
3B109
4B1312
5B139</b>
<h3>Example 2: Select Columns Based on Label Indexing</h3>
The following code shows how to create a pandas DataFrame and use <b>.loc</b> to select the column with an index label of <b>‘rebounds’</b>:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [11, 7, 8, 10, 13, 13],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
teampointsassistsrebounds
0A11511
1A778
2A8710
3B1096
4B13126
5B1395
#select column with index label 'rebounds'
df.loc[:, 'rebounds']
0    11
1     8
2    10
3     6
4     6
5     5
Name: rebounds, dtype: int64</b>
We can use similar syntax to select multiple columns with different index labels:
<b>#select the columns with index labels 'points' and 'rebounds'
df.loc[:, ['points', 'rebounds']]
pointsrebounds
01111
178
2810
3106
4136
5135</b>
Or we could select all columns in a range:
<b>#select columns with index labels between 'team' and 'assists'
df.loc[:, 'team':'assists']
teampointsassists
0A115
1A77
2A87
3B109
4B1312
5B139</b>
<b>Related:</b>  Pandas loc vs. iloc: What’s the Difference? 
<h2><span class="orange">Pandas: How to Select Columns Based on Partial Match</span></h2>
You can use the following methods to select columns in a pandas DataFrame based on partial matching:
<b>Method 1: Select Columns Based on One Partial Match</b>
<b>#select columns that contain 'team'
df.loc[:, df.columns.str.contains('team')]
</b>
<b>Method 2: Select Columns Based on Multiple Partial Matches</b>
<b>#select columns that contain 'team' or 'rebounds'
df.loc[:, df.columns.str.contains('team|rebounds')] 
</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team_name': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'team_points': [5, 7, 7, 9, 12, 9, 9, 4],   'assists': [11, 8, 10, 6, 6, 5, 9, 12],   'rebounds': [6, 7, 7, 6, 10, 12, 10, 9]})
#view DataFrame
print(df)
  team_name  team_points  assists  rebounds
0         A            5       11         6
1         A            7        8         7
2         A            7       10         7
3         A            9        6         6
4         B           12        6        10
5         B            9        5        12
6         B            9        9        10
7         B            4       12         9
</b>
<h2>Example 1: Select Columns Based on One Partial Match</h2>
The following code shows how to select all columns in the pandas DataFrame that contain ‘team’ in the column name:
<b>#select columns that contain 'team'
df_team_cols = df.loc[:, df.columns.str.contains('team')]
#view results
print(df_team_cols)
  team_name  team_points
0         A            5
1         A            7
2         A            7
3         A            9
4         B           12
5         B            9
6         B            9
7         B            4
</b>
Notice that both columns that contain ‘team’ in the name are returned.
<h2>Example 2: Select Columns Based on Multiple Partial Matches</h2>
The following code shows how to select all columns in the pandas DataFrame that contain ‘team’ or ‘rebounds’ in the column name:
<b>#select columns that contain 'team' or 'rebounds'
df_team_rebs = df.loc[:, df.columns.str.contains('team|rebounds')]
#view results
print(df_team_rebs)
  team_name  team_points  rebounds
0         A            5         6
1         A            7         7
2         A            7         7
3         A            9         6
4         B           12        10
5         B            9        12
6         B            9        10
7         B            4         9
</b>
All columns that contain either ‘team’ or ‘rebounds’ in the name are returned.
<b>Note</b>: The <b>|</b> operator represents “OR” in pandas.
Feel free to use as many of these operators as you’d like to search for as many partial string matches as you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Select Columns by Name in Pandas 
 How to Select Columns by Index in Pandas 
 How to Select Columns by Data Type in Pandas 
<h2><span class="orange">Pandas: How to Select Columns Based on Condition</span></h2>
You can use the following methods to select columns in a pandas DataFrame by condition:
<b>Method 1: Select Columns Where At Least One Row Meets Condition</b>
<b>#select columns where at least one row has a value greater than 2
df.loc[:, (df > 2).any()]
</b>
<b>Method 2: Select Columns Where All Rows Meet Condition</b>
<b>#select columns where all rows have a value greater than 2
df.loc[:, (df > 2).all()] 
</b>
<b>Method 3: Select Columns Where At Least One Row Meets Multiple Conditions</b>
<b>#select columns where at least one row has a value between 10 and 15
df.loc[:, ((df>=10) & (df&lt;=15)).any()]</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'apples': [7, 3, 3, 4, 3],   'oranges': [2, 0, 2, 0, 1],   'bananas': [5, 0, 4, 0, 12]},    index=['Farm1', 'Farm2', 'Farm3', 'Farm4', 'Farm5'])
#view DataFrame
print(df)
       apples  oranges  bananas
Farm1       7        2        5
Farm2       3        0        0
Farm3       3        2        4
Farm4       4        0        0
Farm5       3        1       12
</b>
<h2>Example 1: Select Columns Where At Least One Row Meets Condition</h2>
We can use the following code to select the columns in the DataFrame where at least one row in the column has a value greater than 2:
<b>#select columns where at least one row has a value greater than 2
df.loc[:, (df > 2).any()]
applesbananas
Farm175
Farm230
Farm334
Farm400
Farm5312</b>
Notice that the <b>apples</b> and <b>bananas</b> columns are returned because both of these columns have at least one row with a value greater than 2.
<h2>Example 2: Select Columns Where All Rows Meet Condition</h2>
We can use the following code to select the columns in the DataFrame where every row in the column has a value greater than 2:
<b>#select columns where every row has a value greater than 2
df.loc[:, (df > 2).all()]
apples
Farm17
Farm23
Farm33
Farm44
Farm53</b>
Notice that only the <b>apples</b> column is returned because it is the only column where every row in the column has a value greater than 2.
<h2>Example 3: Select Columns Where At Least One Row Meets Multiple Conditions</h2>
We can use the following code to select the columns in the DataFrame where at least one row in the column has a value between 10 and 15:
<b>#select columns where every row has a value greater than 2
df.loc[:, ((df>=10) & (df&lt;=15)).any()]
bananas
Farm15
Farm20
Farm34
Farm40
Farm512</b>
Notice that only the <b>bananas </b>column is returned because it is the only column where at least one row in the column has a value between 10 and 15.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Select Columns by Name in Pandas 
 How to Select Columns by Index in Pandas 
 How to Select Columns Containing a Specific String in Pandas 
<h2><span class="orange">How to Select Columns by Name in Pandas (3 Examples)</span></h2>
You can use the following methods to select columns by name in a pandas DataFrame:
<b>Method 1: Select One Column by Name</b>
<b>df.loc[:, 'column1']
</b>
<b>Method 2: Select Multiple Columns by Name</b>
<b>df.loc[:, ['column1', 'column3', 'column4']] </b>
<b>Method 3: Select Columns in Range by Name</b>
<b>df.loc[:, 'column2':'column4'] </b>
The following examples show how to use each of these methods in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'mavs': [10, 12, 14, 15, 19, 22, 27],   'cavs': [18, 22, 19, 14, 14, 11, 20],   'hornets': [5, 7, 7, 9, 12, 9, 14],   'spurs': [10, 12, 14, 13, 13, 19, 22],   'nets': [10, 14, 25, 22, 25, 17, 12]})
#view DataFrame
print(df)
   mavs  cavs  hornets  spurs  nets
0    10    18        5     10    10
1    12    22        7     12    14
2    14    19        7     14    25
3    15    14        9     13    22
4    19    14       12     13    25
5    22    11        9     19    17
6    27    20       14     22    12
</b>
<h2>Example 1: Select One Column by Name</h2>
The following code shows how to select the ‘spurs’ column in the DataFrame:
<b>#select column with name 'spurs'
df.loc[:, 'spurs']
0    10
1    12
2    14
3    13
4    13
5    19
6    22
Name: spurs, dtype: int64</b>
Only the values from the ‘spurs’ column are returned.
<h2>Example 2: Select Multiple Columns by Name</h2>
The following code shows how to select the cavs, spurs, and nets columns in the DataFrame:
<b>#select columns with names cavs, spurs, and nets
df.loc[:, ['cavs', 'spurs', 'nets']]
        cavsspursnets
0181010
1221214
2191425
3141322
4141325
5111917
6202212</b>
Only the values from the cavs, spurs, and nets columns are returned.
<h2>Example 3: Select Columns in Range by Name</h2>
The following code shows how to select all columns between the names ‘hornets’ and ‘nets’ in the DataFrame:
<b>#select all columns between hornets and nets
df.loc[:, 'hornets':'nets']
        hornetsspursnets
051010
171214
271425
391322
4121325
591917
6142212</b>
All of the columns between the names ‘hornets’ and ‘nets’ are returned.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Move Column to Front of DataFrame 
 Pandas: How to Check if Column Contains String 
 Pandas: How to Add Empty Column to DataFrame (3 Examples) 
<h2><span class="orange">Pandas: How to Select Columns by Data Type</span></h2>
You can use the following methods to select columns in a pandas DataFrame that are equal to a specific data type:
<b>Method 1: Select Columns Equal to Specific Data Type</b>
<b>#select all columns that have an int or float data type
df.select_dtypes(include=['int', 'float'])
</b>
<b>Method 2: Select Columns Not Equal to Specific Data Type</b>
<b>#select all columns that don't have a bool or object data type
df.select_dtypes(exclude=['bool', 'object'])
</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9],   'minutes': [10.1, 12.0, 9.0, 8.0, 8.4, 7.5],   'all_star': [True, False, False, True, True, True]})
#view DataFrame
print(df)
  team  points  assists  minutes  all_star
0    A      18        5     10.1      True
1    B      22        7     12.0     False
2    C      19        7      9.0     False
3    D      14        9      8.0      True
4    E      14       12      8.4      True
5    F      11        9      7.5      True</b>
<h2>Example 1: Select Columns Equal to Specific Data Type</h2>
We can use the following code to select all columns in the DataFrame that have a data type equal to either <b>int</b> or <b>float</b>:
<b>#select all columns that have an int or float data type
df.select_dtypes(include=['int', 'float'])
pointsassistsminutes
018510.1
122712.0
21979.0
31498.0
414128.4
51197.5
</b>
Notice that only the columns with a data type equal to <b>int</b> or <b>float</b> are selected.
<h2>Example 2: Select Columns Not Equal to Specific Data Type</h2>
We can use the following code to select all columns in the DataFrame that do not have a data type equal to either <b>bool </b>or <b>object</b>:
<b>#select all columns that don't have a bool or object data type
df.select_dtypes(exclude=['bool', 'object'])
pointsassistsminutes
018510.1
122712.0
21979.0
31498.0
414128.4
51197.5
</b>
Notice that only the columns that don’t have a data type equal to <b>bool </b>or <b>object</b> are selected.
Also note that you can use the following syntax to display the data type of each column in the DataFrame:
<b>#display data type of all columns
df.dtypes
team         object
points        int64
assists       int64
minutes     float64
all_star       bool
dtype: object
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Check dtype for All Columns in DataFrame 
 Pandas: Get Index of Rows Whose Column Matches Value 
 Pandas: How to Set Column as Index 
<h2><span class="orange">Pandas: How to Select Columns Containing a Specific String</span></h2>
You can use the following methods to select columns that contain a particular string in a pandas DataFrame:
<b>Method 1: Select Columns that Contain One Specific String</b>
<b>df.filter(regex='string1')
</b>
<b>Method 2: Select Columns that Contain One of Several Strings</b>
<b>df.filter(regex='string1|string2|string3') </b>
The following examples show how to use each of these methods in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'mavs': [10, 12, 14, 15, 19, 22, 27],   'cavs': [18, 22, 19, 14, 14, 11, 20],   'hornets': [5, 7, 7, 9, 12, 9, 14],   'spurs': [10, 12, 14, 13, 13, 19, 22],   'nets': [10, 14, 25, 22, 25, 17, 12]})
#view DataFrame
print(df)
   mavs  cavs  hornets  spurs  nets
0    10    18        5     10    10
1    12    22        7     12    14
2    14    19        7     14    25
3    15    14        9     13    22
4    19    14       12     13    25
5    22    11        9     19    17
6    27    20       14     22    12
</b>
<h2>Example 1: Select Columns that Contain One Specific String</h2>
The following code shows how to use the <b>filter()</b> function to select only the columns that contain the string “avs” somewhere in their name:
<b>#select columns that contain 'avs' in the name
df2 = df.filter(regex='avs')
#view DataFrame
print(df2)
   mavs  cavs
0    10    18
1    12    22
2    14    19
3    15    14
4    19    14
5    22    11
6    27    20
</b>
Only the columns that contain “avs” in the name are returned.
In this case, “mavs” and “cavs” are the only columns that are returned.
<h2>Example 2: Select Columns that Contain One of Several Strings</h2>
The following code shows how to use the <b>filter()</b> function to select only the columns that contain “avs” or “ets” somewhere in their name:
<b>#select columns that contain 'avs' in the name
df2 = df.filter(regex='avs|ets')
#view DataFrame
print(df2)
   mavs  cavs  hornets  nets
0    10    18        5    10
1    12    22        7    14
2    14    19        7    25
3    15    14        9    22
4    19    14       12    25
5    22    11        9    17
6    27    20       14    12</b>
Only the columns that contain “avs” or “ets” in the name are returned.
Note that the vertical bar ( <b>|</b> ) is the “<b>OR</b>” operator in pandas.
Feel free to chain together as many of these “OR” operators as you’d like to select columns that contain one of several different strings.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Move Column to Front of DataFrame 
 Pandas: How to Check if Column Contains String 
 Pandas: How to Add Empty Column to DataFrame (3 Examples) 
<h2><span class="orange">How to Select Multiple Columns in Pandas (With Examples)</span></h2>
There are three basic methods you can use to select multiple columns of a pandas DataFrame:
<b>Method 1: Select Columns by Index</b>
<b>df_new = df.iloc[:, [0,1,3]]</b>
<b>Method 2: Select Columns in Index Range</b>
<b>df_new = df.iloc[:, 0:3]</b>
<b>Method 3: Select Columns by Name</b>
<b>df_new = df[['col1', 'col2']]
</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'blocks': [4, 7, 7, 6, 5, 8, 9, 10]})
#view DataFrame
df
pointsassistsrebounds blocks
025511 4
11278 7
215710 7
31496 6
419126 5
52395 8
62599 9
729412 10
</b>
<h3>Method 1: Select Columns by Index</h3>
The following code shows how to select columns in index positions 0, 1, and 3:
<b>#select columns in index positions 0, 1, and 3
df_new = df.iloc[:, [0,1,3]]
#view new DataFrame
df_new
        pointsassistsblocks
02554
11277
21577
31496
419125
52398
62599
729410
</b>
Notice that the columns in index positions 0, 1, and 3 are selected.
<b>Note</b>: The first column in a pandas DataFrame is located in position 0.
<h3>Method 2: Select Columns in Index Range</h3>
The following code shows how to select columns in the index range 0 to 3:
<b>#select columns in index range 0 to 3
df_new = df.iloc[:, 0:3]
#view new DataFrame
df_new
        pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412
</b>
Note that the column located in the last value in the range (3) will not be included in the output.
<h3>Method 3: Select Columns by Name</h3>
The following code shows how to select columns by name:
<b>#select columns called 'points' and 'blocks'
df_new = df[['points', 'blocks']]
#view new DataFrame
df_new
        pointsblocks
0254
1127
2157
3146
4195
5238
6259
72910
</b>
<h2><span class="orange">How to Select Only Numeric Columns in Pandas</span></h2>
You can use the following basic syntax to select only numeric columns in a pandas DataFrame:
<b>import pandas as pd
import numpy as np
df.select_dtypes(include=np.number)
</b>
The following example shows how to use this function in practice.
<h2>Example: Select Only Numeric Columns in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use the following syntax to select only the numeric columns in the DataFrame:
<b>import numpy as np
#select only the numeric columns in the DataFrame
df.select_dtypes(include=np.number)
        pointsassistsrebounds
018511
12278
219710
31496
414126
51195
62099
728412</b>
Notice that only the three numeric columns have been selected – <b>points</b>, <b>assists</b>, and <b>rebounds</b>.
We can verify that these columns are numeric by using the <b>dtypes()</b> function to display the data type of each variable in the DataFrame:
<b>#display data type of each variable in DataFrame
df.dtypes
team        object
points       int64
assists      int64
rebounds     int64
dtype: object
</b>
From the output we can see that <b>team</b> is an object (i.e. string) while <b>points</b>, <b>assists</b>, and <b>rebounds</b> are all numeric.
Note that we can also use the following code to get a list of the numeric columns in the DataFrame:
<b>#display list of numeric variables in DataFrame
df.select_dtypes(include=np.number).columns.tolist()
['points', 'assists', 'rebounds']
</b>
This allows us to quickly see the names of the numeric variables in the DataFrame without seeing their actual values.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Columns by Name in Pandas 
 How to Select Columns by Index in Pandas 
 How to Select Columns Containing a Specific String in Pandas 
<h2><span class="orange">Pandas: How to Select Rows Based on Column Values</span></h2>
You can use one of the following methods to select rows in a pandas DataFrame based on column values:
<b>Method 1: Select Rows where Column is Equal to Specific Value</b>
<b>df.loc[df['col1'] == value]
</b>
<b>Method 2: Select Rows where Column Value is in List of Values</b>
<b>df.loc[df['col1'].isin([value1, value2, value3, ...])]</b>
<b>Method 3: Select Rows Based on Multiple Column Conditions</b>
<b>df.loc[(df['col1'] == value) & (df['col2'] &lt; value)]
</b>
The following example shows how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'points': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'blocks': [4, 7, 7, 6, 5, 8, 9, 10]})
#view DataFrame
df
teampointsrebounds blocks
0A511 4
1A78 7
2B710 7
3B96 6
4B126 5
5C95 8
6C99 9
7C412 10
</b>
<h3>Method 1: Select Rows where Column is Equal to Specific Value</h3>
The following code shows how to select every row in the DataFrame where the ‘points’ column is equal to 7:
<b>#select rows where 'points' column is equal to 7
df.loc[df['points'] == 7]
teampointsrebounds blocks
1A78 7
2B710 7
</b>
<h3>Method 2: Select Rows where Column Value is in List of Values</h3>
The following code shows how to select every row in the DataFrame where the ‘points’ column is equal to 7, 9, or 12:
<b>#select rows where 'points' column is equal to 7
df.loc[df['points'].isin([7, 9, 12])]
        teampointsrebounds blocks
1A78 7
2B710 7
3B96 6
4B126 5
5C95 8
6C99 9</b>
<h3>Method 3: Select Rows Based on Multiple Column Conditions</h3>
The following code shows how to select every row in the DataFrame where the ‘team’ column is equal to ‘B’ and where the ‘points’ column is greater than 8:
<b>#select rows where 'team' is equal to 'B' and points is greater than 8
df.loc[(df['team'] == 'B') & (df['points'] > 8)]
teampointsrebounds blocks
3B96 6
4B126 5</b>
Notice that only the two rows where the team is equal to ‘B’ and the ‘points’ is greater than 8 are returned.
<h2><span class="orange">Pandas: How to Select Rows Between Two Dates</span></h2>
You can use the following syntax to select rows between two specific dates in a pandas DataFrame:
<b>df[df.date.between('2022-01-02', '2022-01-06')]
</b>
This particular example selects all rows in the DataFrame between 2022-01-02 and 2022-01-06.
The following example shows how to use this syntax in practice.
<h2>Example: Select Rows Between Two Dates in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2022', periods=8),   'sales': [18, 20, 15, 14, 10, 9, 8, 12],   'returns': [5, 7, 7, 9, 12, 3, 2, 4]})
#view DataFrame
print(df)
        date  sales  returns
0 2022-01-01     18        5
1 2022-01-02     20        7
2 2022-01-03     15        7
3 2022-01-04     14        9
4 2022-01-05     10       12
5 2022-01-06      9        3
6 2022-01-07      8        2
7 2022-01-08     12        4
</b>
We can use the following syntax to select only the rows that fall between the date 2022-01-02 and 2022-01-06:
<b>#select all rows where date is between 2022-01-02 and 2022-01-06
df[df.date.between('2022-01-02', '2022-01-06')]
              date      sales   returns
12022-01-02207
22022-01-03157
32022-01-04149
42022-01-051012
52022-01-0693</b>
Notice that only the rows between the dates 2022-01-02 and 2022-01-06 are selected.
If you’d like, you can also define the start and end dates outside of the <b>between()</b> function:
<b>#define start and end dates
start_date = '2022-01-02'
end_date = '2022-01-06'
#select all rows where date is between start and end
df[df.date.between(start_date, end_date)]
              datesalesreturns
12022-01-02207
22022-01-03157
32022-01-04149
42022-01-051012
52022-01-0693
</b>
This produces the same result.
Note that if your date column is not in a recognizable datetime format, you may first need to use the following code to convert it to a datetime format:
<b>df['date'] = pd.to_datetime(df['date']) 
</b>
Once you’ve done this, you can proceed to use the <b>between()</b> function to select rows between specific dates.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create a Date Range in Pandas 
 How to Extract Month from Date in Pandas 
 How to Convert Timestamp to Datetime in Pandas 
<h2><span class="orange">Pandas: Select Rows from DataFrame Using Boolean Series</span></h2>
You can use the following basic syntax to select rows in a pandas DataFrame based on values in a boolean series:
<b>#define boolean series
bools = pd.Series([True, False, True, True, False, False, False, True])
#select rows in DataFrame based on values in boolean series
df[bools.values]
</b>
This allows you to select each of the rows in the pandas DataFrame where the corresponding value in the boolean series is <b>True</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Select Rows from Pandas DataFrame Using Boolean Series</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can use the following syntax to select all rows in the DataFrame where the corresponding value in a boolean series is <b>True</b>:
<b>#define boolean series
bools = pd.Series([True, False, True, True, False, False, False, True])
#select rows in DataFrame based on values in boolean series
df[bools.values]
     team   points  assists  rebounds
0A18  5   11
2C19  7   10
3D14  9    6
7H28  4        12
</b>
Notice that the only rows returned are the ones where the corresponding value in the boolean series is <b>True</b>.
Also note that you can use the following syntax to only select the rows in the “points” column of the DataFrame where the corresponding value in the boolean series is <b>True</b>.
<b>#define boolean series
bools = pd.Series([True, False, True, True, False, False, False, True])
#select rows in points column based on values in boolean series
df['points'][bools.values]
0    18
2    19
3    14
7    28
Name: points, dtype: int64
</b>
Notice that the only rows returned from the “points” column are the ones where the corresponding value in the boolean series is <b>True</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Filter Rows Based on String Length in Pandas 
 How to Select Rows without NaN Values in Pandas 
 How to Select Rows Based on Column Values in Pandas 
<h2><span class="orange">How to Select Rows by Index in a Pandas DataFrame</span></h2>
Often you may want to select the rows of a pandas DataFrame based on their index value.
If you’d like to select rows based on integer indexing, you can use the <b>.iloc</b> function.
If you’d like to select rows based on label indexing, you can use the <b>.loc</b> function.
This tutorial provides an example of how to use each of these functions in practice.
<h3>Example 1: Select Rows Based on Integer Indexing</h3>
The following code shows how to create a pandas DataFrame and use <b>.iloc</b> to select the row with an index integer value of <b>4</b>:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(0)
#create DataFrame
df = pd.DataFrame(np.random.rand(6,2), index=range(0,18,3), columns=['A', 'B'])
#view DataFrame
df
       A       B
00.5488140.715189
30.6027630.544883
60.4236550.645894
90.4375870.891773
120.9636630.383442
150.7917250.528895
#select the 5th row of the DataFrame
df.iloc[[4]]
       A       B
120.9636630.383442</b>
We can use similar syntax to select multiple rows:
<b>#select the 3rd, 4th, and 5th rows of the DataFrame
df.iloc[[2, 3, 4]]
       A       B
60.4236550.645894
90.4375870.891773
120.9636630.383442
</b>
Or we could select all rows in a range:
<b>#select the 3rd, 4th, and 5th rows of the DataFrame
df.iloc[2:5]
       A       B
60.4236550.645894
90.4375870.891773
120.9636630.383442</b>
<h3>Example 2: Select Rows Based on Label Indexing</h3>
The following code shows how to create a pandas DataFrame and use <b>.loc</b> to select the row with an index label of <b>3</b>:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(0)
#create DataFrame
df = pd.DataFrame(np.random.rand(6,2), index=range(0,18,3), columns=['A', 'B'])
#view DataFrame
df
       A       B
00.5488140.715189
30.6027630.544883
60.4236550.645894
90.4375870.891773
120.9636630.383442
150.7917250.528895
#select the row with index label '3'
df.loc[[3]]
               A       B
30.6027630.544883</b>
We can use similar syntax to select multiple rows with different index labels:
<b>#select the rows with index labels '3', '6', and '9'
df.loc[[3, 6, 9]]
       A       B
30.6027630.544883
60.4236550.645894
90.4375870.891773</b>
<h3>The Difference Between .iloc and .loc</h3>
The examples above illustrate the subtle difference between <b>.iloc</b> an <b>.loc</b>:
<b>.iloc</b> selects rows based on an <b>integer index</b>. So, if you want to select the 5th row in a DataFrame, you would use df.iloc[[4]] since the first row is at index 0, the second row is at index 1, and so on.
<b>.loc</b> selects rows based on a <b>labeled index</b>. So, if you want to select the row with an index label of 5, you would directly use df.loc[[5]].
<h2><span class="orange">Pandas: How to Select Rows of DataFrame by Timestamp</span></h2>
You can use the following basic syntax to select rows between two timestamps in a pandas DataFrame:
<b>df[(df['tstamp'] > '2022-10-25 04:30:00') & (df['tstamp'] &lt; '2022-10-27 11:00:00')]
</b>
This syntax assumes that <b>tstamp </b>already has a dtype of datetime.
If it doesn’t, you can use the following syntax to convert it to a datetime column:
<b>df['tstamp'] = pd.to_datetime(df['tstamp'])
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Select Rows of Pandas DataFrame by Timestamp</h2>
Suppose we have the following pandas DataFrame that contains information about sales at some retail store:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'tstamp': ['2022-10-25 04:00:00', '2022-10-25 11:55:12',                 '2022-10-26 02:00:00', '2022-10-27 10:30:00',                 '2022-10-27 14:25:00', '2022-10-28 01:15:27'],   'sales': [18, 22, 19, 14, 14, 11]})
#view DataFrame
print(df)
                tstamp  sales
0  2022-10-25 04:00:00     18
1  2022-10-25 11:55:12     22
2  2022-10-26 02:00:00     19
3  2022-10-27 10:30:00     14
4  2022-10-27 14:25:00     14
5  2022-10-28 01:15:27     11
</b>
Suppose we would like to select only the rows between the following two timestamps:
2022-10-25 04:30:00
2022-10-27 11:00:00
We can use the following syntax to do so:
<b>#convert timestamp column to datetime dtype
df['tstamp'] = pd.to_datetime(df['tstamp'])
#select rows between two timestamps
df[(df['tstamp'] > '2022-10-25 04:30:00') & (df['tstamp'] &lt; '2022-10-27 11:00:00')]
tstamp             sales
12022-10-25 11:55:1222
22022-10-26 02:00:0019
32022-10-27 10:30:0014</b>
Notice that only the rows between the two timestamps that we specified are selected.
Also note that you can select rows by timestamp using only a date value.
For example, we could use the following code to select all rows where the timestamp is greater than 2022-10-27:
<b>#convert timestamp column to datetime dtype
df['tstamp'] = pd.to_datetime(df['tstamp'])
#select rows with timestamp after 2022-10-27
df[df['tstamp'] > '2022-10-27']
tstamp             sales
32022-10-27 10:30:0014
42022-10-27 14:25:0014
52022-10-28 01:15:2711</b>
Notice that only the rows where the value in the <b>tsamp</b> column are after 2022-10-27 are selected.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Convert Datetime to Date in Pandas 
 How to Convert Columns to DateTime in Pandas 
 How to Sort a Pandas DataFrame by Date 
<h2><span class="orange">Pandas: Select Rows where Two Columns Are Equal</span></h2>
You can use the following methods to select rows in a pandas DataFrame where two columns are (or are not) equal:
<b>Method 1: Select Rows where Two Columns Are Equal</b>
<b>df.query('column1 == column2')
</b>
<b>Method 2: Select Rows where Two Columns Are Not Equal</b>
<b>df.query('column1 != column2') </b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'painting': ['A', 'B', 'C', 'D', 'E', 'F'],   'rater1': ['Good', 'Good', 'Bad', 'Bad', 'Good', 'Good'],   'rater2': ['Good', 'Bad', 'Bad', 'Good', 'Good', 'Good']})
#view DataFrame
print(df)
  painting rater1 rater2
0        A   Good   Good
1        B   Good    Bad
2        C    Bad    Bad
3        D    Bad   Good
4        E   Good   Good
5        F   Good   Good</b>
<h2>Example 1: Select Rows where Two Columns Are Equal</h2>
We can use the following syntax to select only the rows in the DataFrame where the values in the <b>rater1</b> and <b>rater2</b> column are equal:
<b>#select rows where rater1 is equal to rater2
df.query('rater1 == rater2')
 painting  rater1  rater2
0A    Good    Good
2C     Bad     Bad
4E    Good    Good
5F    Good    Good</b>
Notice that only the rows where <b>rater1</b> and <b>rater2</b> are equal are selected.
We could also use the<b> len()</b> function if we simply want to count how many rows have equal values in the <b>rater1</b> and <b>rater2</b> columns:
<b>#count the number of rows where rater1 is equal to rater2
len(df.query('rater1 == rater2'))
4</b>
This tells us that there are <b>4</b> rows where the values in the <b>rater1</b> and <b>rater2</b> column are equal.
<h2>Example 2: Select Rows where Two Columns Are Not Equal</h2>
We can use the following syntax to select only the rows in the DataFrame where the values in the <b>rater1</b> and <b>rater2</b> column are not equal:
<b>#select rows where rater1 is not equal to rater2
df.query('rater1 != rater2')
 painting  rater1  rater2
1B    Good     Bad
3D     Bad    Good</b>
Notice that only the rows where <b>rater1</b> and <b>rater2</b> are not equal are selected.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Select Rows with NaN Values in Pandas (With Examples)</span></h2>
You can use the following methods to select rows with NaN values in pandas:
<b>Method 1: Select Rows with NaN Values in Any Column</b>
<b>df.loc[df.isnull().any(axis=1)]
</b>
<b>Method 2: Select Rows with NaN Values in Specific Column</b>
<b>df.loc[df['this_column'].isnull()]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, np.NaN, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, np.NaN, 9, 9, np.NaN],   'rebounds': [11, 8, 10, 6, 6, 5, 9, np.NaN]})
#view DataFrame
print(df)</b>
<h2>Example 1: Select Rows with NaN Values in Any Column</h2>
We can use the following syntax to select rows with NaN values in any column of the DataFrame:
<b>#create new DataFrame that only contains rows with NaNs in any column
df_nan_rows = df.loc[df.isnull().any(axis=1)]
#view results
print(df_nan_rows)
  team  points  assists  rebounds
1    B     NaN      7.0       8.0
4    E    14.0      NaN       6.0
7    H    28.0      NaN       NaN   
</b>
Notice that each row in the resulting DataFrame contains a NaN value in at least one column.
<h2>Example 2: Select Rows with NaN Values in Specific Column</h2>
We can use the following syntax to select rows with NaN values in the <b>assists</b> column of the DataFrame:
<b>#create new DataFrame that only contains rows with NaNs in assists column
df_assists_nans = df.loc[df['assists'].isnull()]
#view results
print(df_assists_nans)
  team  points  assists  rebounds
4    E    14.0      NaN       6.0
7    H    28.0      NaN       NaN   
</b>
Notice that each row in the resulting DataFrame contains a NaN value in the <b>assists</b> column.
There is one row with a NaN value in the <b>points</b> column, but this row is not selected since it doesn’t have a NaN value in the <b>assists</b> column as well.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Drop Rows with NaN Values 
 Pandas: How to Replace NaN Values with String 
 Pandas: How to Fill NaN Values with Mean 
<h2><span class="orange">How to Select Rows without NaN Values in Pandas</span></h2>
You can use the following methods to select rows without NaN values in pandas:
<b>Method 1: Select Rows without NaN Values in All Columns</b>
<b>df[~df.isnull().any(axis=1)]
</b>
<b>Method 2: Select Rows without NaN Values in Specific Column</b>
<b>df[~df['this_column'].isna()]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],   'points': [np.nan, 12, 15, 25, np.nan, 22, 30],   'assists': [4, np.nan, 5, 9, 12, 14, 10]})
#view DataFrame
print(df)
  team  points  assists
0    A     NaN      4.0
1    B    12.0      NaN
2    C    15.0      5.0
3    D    25.0      9.0
4    E     NaN     12.0
5    F    22.0     14.0
6    G    30.0     10.0</b>
<h2>Example 1: Select Rows without NaN Values in All Columns</h2>
We can use the following syntax to select rows without NaN values in every column of the DataFrame:
<b>#create new DataFrame that only contains rows without NaNs
no_nans = df[~df.isnull().any(axis=1)]
#view results
print(no_nans)
  team  points  assists
2    C    15.0      5.0
3    D    25.0      9.0
5    F    22.0     14.0
6    G    30.0     10.0   
</b>
Notice that each row in the resulting DataFrame contains no NaN values in any column.
<h2>Example 2: Select Rows without NaN Values in Specific Column</h2>
We can use the following syntax to select rows without NaN values in the <b>points</b> column of the DataFrame:
<b>#create new DataFrame that only contains rows without NaNs in points column
no_points_nans = df[~df['points'].isna()]
#view results
print(no_points_nans)
  team  points  assists
1    B    12.0      NaN
2    C    15.0      5.0
3    D    25.0      9.0
5    F    22.0     14.0
6    G    30.0     10.0</b>
Notice that each row in the resulting DataFrame contains no NaN values in the <b>points</b> column.
There is one row with a NaN value in the <b>assists</b> column, but the row is kept in the DataFrame since the value in the <b>points</b> column of that row is not NaN.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Drop Rows with NaN Values 
 Pandas: How to Replace NaN Values with String 
 Pandas: How to Fill NaN Values with Mean 
<h2><span class="orange">Pandas: How to Filter Series by Value</span></h2>
You can use the following methods to filter the values in a pandas Series:
<b>Method 1: Filter Values Based on One Condition</b>
<b>#filter for values equal to 7
my_series.loc[lambda x : x == 7]
</b>
<b>Method 2: Filter Values Using “OR” Condition</b>
<b>#filter for values less than 10 <em>or</em> greater than 20
my_series.loc[lambda x : (x &lt; 10) | (x > 20)]
</b>
<b>Method 3: Filter Values Using “AND” Condition</b>
<b>#filter for values greater than 10 <i>and </i>less than 20
my_series.loc[lambda x : (x > 10) & (x &lt; 20)] 
</b>
<b>Method 4: Filter Values Contained in List</b>
<b>#filter for values that are equal to 4, 7, or 23
my_series[my_series.isin([4, 7, 23])]
</b>
This tutorial explains how to use each method in practice with the following pandas Series:
<b>import pandas as pd
#create pandas Series
data = pd.Series([4, 7, 7, 12, 19, 23, 25, 30])
#view pandas Series
print(data)
0     4
1     7
2     7
3    12
4    19
5    23
6    25
7    30
dtype: int64</b>
<h2>Example 1: Filter Values Based on One Condition</h2>
The following code shows how to filter the pandas Series for values equal to 7:
<b>#filter for values equal to 7
data.loc[lambda x : x == 7]
1    7
2    7
dtype: int64</b>
We can also filter for values <b>not equal</b> to 7:
<b>#filter for values not equal to 7
data.loc[lambda x : x != 7]
0     4
3    12
4    19
5    23
6    25
7    30
dtype: int644</b>
<h2>Example 2: Filter Values Using “OR” Condition</h2>
The following code shows how to filter the pandas Series for values less than 10 <b>or</b> greater than 20:
<b>#filter for values less than 10 <em>or</em> greater than 20
data.loc[lambda x : (x &lt; 10) | (x > 20)]
0     4
1     7
2     7
5    23
6    25
7    30
dtype: int64</b>
<h2>Example 3: Filter Values Using “AND” Condition</h2>
The following code shows how to filter the pandas Series for values greater than 10 <b>and</b> less than 20:
<b>#filter for values greater than 10 <i>and </i>less than 20
data.loc[lambda x : (x > 10) & (x &lt; 20)]
3    12
4    19
dtype: int64</b>
<h2>Example 4: Filter Values Contained in List</h2>
The following code shows how to filter the pandas Series for values that are contained in a list:
<b>#filter for values that are equal to 4, 7, or 23
data[data.isin([4, 7, 23])]
0     4
1     7
2     7
5    23
dtype: int64
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common filtering operations in Python:
 How to Filter Pandas DataFrame Rows that Contain a Specific String 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">How to Create a Histogram from a Pandas Series</span></h2>
You can use the following basic syntax to create a histogram from a pandas Series:
<b>my_series.plot(kind='hist')
</b>
The following examples show how to use this syntax in practice.
<b>Note</b>: If you are using an online Python notebook and don’t see any histogram appear after using this syntax, you may need to specify  %matplotlib inline  first.
<h2>Example 1: Create Frequency Histogram</h2>
The following code shows how to create a frequency histogram from a pandas Series:
<b>import pandas as pd
#create Series
data = pd.Series([2, 2, 2, 3, 3, 4, 5, 7, 8, 9, 12, 12, 14, 15, 16, 16, 18,    19, 22, 22, 22, 25, 26, 27, 30, 33, 33, 33, 34, 35])
#create histogram from Series
data.plot(kind='hist')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histseries1.jpg"504">
The x-axis displays the values for the pandas Series while the y-axis displays the frequency of each value.
<h2>Example 2: Create Density Histogram</h2>
To create a density histogram from a pandas Series, we can specify <b>density=True</b> within the <b>plot()</b> function:
<b>import pandas as pd
#create Series
data = pd.Series([2, 2, 2, 3, 3, 4, 5, 7, 8, 9, 12, 12, 14, 15, 16, 16, 18,    19, 22, 22, 22, 25, 26, 27, 30, 33, 33, 33, 34, 35])
#create histogram from Series
data.plot(kind='hist', density=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histseries2.jpg"503">
The x-axis displays the values for the pandas Series while the y-axis displays the density.
<h2>Example 3: Create Custom Histogram</h2>
Lastly, we can use the following syntax to customize the color of the bars in the histogram, the number of bins used, the axis labels, and the plot title:
<b>import pandas as pd
#create Series
data = pd.Series([2, 2, 2, 3, 3, 4, 5, 7, 8, 9, 12, 12, 14, 15, 16, 16, 18,    19, 22, 22, 22, 25, 26, 27, 30, 33, 33, 33, 34, 35])
#create histogram with custom color, edgecolor, and number of bins
my_hist = data.plot(kind='hist', color='red', edgecolor='black', bins=20)
#add x-axis label
my_hist.set_xlabel('Values')
#add title
my_hist.set_title('Distribution of Values')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/histseries3.jpg"518">
<h2>Additional Resources</h2>
The following tutorials explain how to create other common plots in Python:
 How to Plot Multiple Lines in Matplotlib 
 How to Create Boxplot from Pandas DataFrame 
 How to Plot Multiple Pandas Columns on Bar Chart 
<h2><span class="orange">How to Convert Pandas Series to NumPy Array (With Examples)</span></h2>
You can use the following syntax to convert a pandas Series to a NumPy array:
<b>seriesName.to_numpy()
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert Series to NumPy Array</h3>
The following code shows how to convert a pandas Series to a NumPy array:
<b>import pandas as pd
import numpy as np
#define series
x = pd.Series([1, 2, 5, 6, 9, 12, 15])
#convert series to NumPy array
new_array = x.to_numpy() 
#view NumPy array
new_array
array([ 1,  2,  5,  6,  9, 12, 15])
#confirm data type
type(new_array)
numpy.ndarray
</b>
Using the <b>type()</b> function, we confirm that the pandas Series has indeed been converted to a NumPy array.
<h3>Example 2: Convert DataFrame Column to NumPy Array</h3>
The following code shows how to convert a column in a pandas DataFrame to a NumPy array:
<b>import pandas as pd
import numpy as np
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#convert 'points' column to NumPy array
new_array = df['points'].to_numpy() 
#view NumPy array
new_array
array([25, 12, 15, 14, 19, 23, 25, 29])
#confirm data type
type(new_array)
numpy.ndarray</b>
We can use <b>dtype()</b> to check the data type of the new NumPy array as well:
<b>#check data type
new_array.dtype
dtype('int64')
</b>
We can see that the new NumPy array is an integer.
<h2><span class="orange">Pandas: How to Set Column as Index</span></h2>
You can use the following syntax to set a column in a pandas DataFrame as the index:
<b>#set one column as index
df.set_index('col1')
#set multiple columns as multi index
df.set_index(['col1', 'col2'])
</b>
The following examples show how to use this syntax in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [5, 7, 7, 9, 12, 9],   'assists': [11, 8, 10, 6, 6, 5],   'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'conference': [1, 2, 3, 4, 5, 6]})
#view DataFrame
df
pointsassiststeamconference
0511A1
178B2
2710C3
396D4
4126E5
595F6
</b>
<h3>Example 1: Set One Column as Index</h3>
The following code shows how to set one column of the pandas DataFrame as the index:
<b>df.set_index('team')
pointsassistsconference
team
A5111
B782
C7103
D964
E1265
F956</b>
<h3>Example 2: Set Multiple Columns as Index</h3>
The following code shows how to set multiple columns of the pandas DataFrame as a Multi-Index::
<b>df.set_index(['team', 'conference'])
   points  assists
teamconference
A1   5   11
B2   7   8
C3   7   10
D4   9   6
E5   12   6
F6   9   5</b>
<h2><span class="orange">How to Set First Row as Header in Pandas</span></h2>
You can use the following basic syntax to set the first row of a pandas DataFrame as the header:
<b>df.columns = df.iloc[0]
df = df[1:]
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Set First Row as Header in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'Bad Name 1': ['team', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'Bad Name 2': ['points', 18, 22, 19, 14, 14, 11, 20, 28],   'Bad Name 3': ['assists', 5, 7, 7, 9, 12, 9, 9, 4],   'Bad Name 4': ['rebounds', 11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  Bad Name 1 Bad Name 2 Bad Name 3 Bad Name 4
0       team     points    assists   rebounds
1          A         18          5         11
2          B         22          7          8
3          C         19          7         10
4          D         14          9          6
5          E         14         12          6
6          F         11          9          5
7          G         20          9          9
8          H         28          4         12
</b>
Suppose the first row contains the values that we actually want to use in the header.
To set the first row as the header, we can use the following syntax:
<b>#set column names equal to values in row index position 0
df.columns = df.iloc[0]
#remove first row from DataFrame
df = df[1:]
#view updated DataFrame
print(df)
0 team points assists rebounds
1    A     18       5       11
2    B     22       7        8
3    C     19       7       10
4    D     14       9        6
5    E     14      12        6
6    F     11       9        5
7    G     20       9        9
8    H     28       4       12
</b>
Notice that the values in the first row are now used as the header.
If you’d like to reset the index of the DataFrame, use the following code:
<b>#reset index values
df.reset_index(drop=True, inplace=True)
#view updated DataFrame
print(df)
0 team points assists rebounds
0    A     18       5       11
1    B     22       7        8
2    C     19       7       10
3    D     14       9        6
4    E     14      12        6
5    F     11       9        5
6    G     20       9        9
7    H     28       4       12
</b>
The index is now reset so that the first row has an index value of <b>0</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Columns by Name in Pandas 
 How to Select Columns by Index in Pandas 
 How to Select Columns Containing a Specific String in Pandas 

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
