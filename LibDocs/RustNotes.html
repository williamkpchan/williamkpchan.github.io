<base target="_blank"><html><head><title>Rust Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script src="D:/Dropbox/Public/commonfunctions.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script>
  var showTopicNumber = true;
  var bookid = "Rust Notes"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2, h3 {color: gold; display:block;}
strong {color: orange;}
pre{width:100%;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Rust Notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="https://www.scrapingbee.com/blog/web-scraping-rust/" class="whitebut ">web-scraping-rust</a>
<a href="https://blog.logrocket.com/web-scraping-rust/" class="whitebut ">web-scraping-rust</a>
</div>

<pre>
<h2>Web Scraping with Rust</h2>
Youâ€™ll use two Rust libraries, <code>reqwest</code> and <code>scraper</code>, to scrape the top one hundred movies list from IMDb.

<h3>Implementing a Web Scraper in Rust</h3>
Your target for scraping will be <a href="https://www.imdb.com/">IMDb</a>, a database of movies, TV series, and other media.
In the end, youâ€™ll have a Rust program that can scrape the <a href="https://www.imdb.com/search/title/?groups=top_100&amp;sort=user_rating,desc&amp;count=100">top one hundred movies by user rating</a> at any given moment.

<h3>Creating the Project and Adding Dependencies</h3>
To start off, you need to create a basic Rust project and add all the dependencies youâ€™ll be using. 
This is best done with Cargo.
To generate a new project for a Rust binary, run:
<code>cargo new web_scraper</code>

Next, add the required libraries to the dependencies. 
For this project, youâ€™ll use <code>reqwest</code> and <code>scraper</code>.

Open the <code>web_scraper</code> folder in your favorite code editor and open the <code>cargo.toml</code> file. 
At the end of the file, add the libraries:
<code>[dependencies]
reqwest = {version = "0.11", features = ["blocking"]}
scraper = "0.12.0"
</code>Now you can move to <code>src/main.rs</code> and start creating your web scraper.

<h3>Getting the Website HTML</h3>
Scraping a page usually involves getting the HTML code of the page and then parsing it to find the information you need. 
Therefore, youâ€™ll need to make the code of the IMDb page available in your Rust program. 

To do that, you first need to understand how browsers work, because theyâ€™re your usual way of interacting with web pages.
To display a web page in the browser, the browser (client) sends an HTTP request to the server, which responds with the source code of the web page. 
The browser then renders this code.

HTTP has various different types of requests, such as GET (for getting the contents of a resource) and POST (for sending information to the server). 
To get the code of an IMDb web page in your Rust program, youâ€™ll need to mimic the behavior of browsers by sending an HTTP GET request to IMDb.
In Rust, you can use <a href="https://docs.rs/reqwest/latest/reqwest/"><code>reqwest</code></a> for that. 

This commonly used Rust library provides the features of an HTTP client. 
It can do a lot of the things that a regular browser can do, such as open pages, log in, and store cookies.
To request the code of a page, you can use the <code>reqwest::blocking::get</code> method:

<code>fn main() {
let response = reqwest::blocking::get(
"https://www.imdb.com/search/title/?groups=top_100&amp;sort=user_rating,desc&amp;count=100",
)
.unwrap()
.text()
.unwrap();
}
</code><code>response</code> will now contain the full HTML code of the page you requested.

<h3>Extracting Information from HTML</h3>
The hardest part of a web scraping project is usually getting the specific information you need out of the HTML document. 
For this purpose, a commonly used tool in Rust is the <a href="https://docs.rs/scraper/0.12.0/scraper/"><code>scraper</code></a> library. 

It works by parsing the HTML document into a tree-like structure. 
You can use <a href="https://www.w3schools.com/cssref/css_selectors.asp">CSS selectors</a> to query the elements youâ€™re interested in.
The first step is to parse your entire HTML document using the library:

<code>    let document = scraper::Html::parse_document(&amp;response);
</code>Next, find and select the parts you need. 
To do that, you need to check the websiteâ€™s code and find a collection of CSS selectors that uniquely identifies those items.

The simplest way to do this is via your regular browser. 
Find the element you need, then check the code of that element by inspecting it:
<img src="https://i.imgur.com/qaM1KG2.png" >

In the case of IMDb, the element you need is the name of the movie. 
When you check the element, youâ€™ll see that itâ€™s wrapped in an <code>&lt;a&gt;</code> tag:
<code>&lt;a href="/title/tt0111161/?ref_=adv_li_tt"&gt;The Shawshank Redemption&lt;/a&gt;

</code>Unfortunately, this tag is not unique. 
Since there are a lot of <code>&lt;a&gt;</code> tags on the page, it wouldnâ€™t be a smart idea to scrape them all, as most of them wonâ€™t be the items you need. 
Instead, find the tag unique to movie titles and then navigate to the <code>&lt;a&gt;</code> tag inside that tag.

In this case, you can pick the <code>lister-item-header</code> class:
<code>&lt;h3 class="lister-item-header"&gt;
&lt;span class="lister-item-index unbold text-primary"&gt;1.&lt;/span&gt;

&lt;a href="/title/tt0111161/?ref_=adv_li_tt"&gt;The Shawshank Redemption&lt;/a&gt;
&lt;span class="lister-item-year text-muted unbold"&gt;(1994)&lt;/span&gt;
&lt;/h3&gt;

</code>Now you need to create a query using the <code>scraper::Selector::parse</code> method.
Youâ€™ll give it a <code>h3.lister-item-header&gt;a</code> selector. 
In other words, it finds <code>&lt;a&gt;</code> tags that have as a parent an <code>&lt;h3&gt;</code> tag that is of a <code>lister-item-header</code> class.

Use the following query:
<code>    let title_selector = scraper::Selector::parse("h3.lister-item-header&gt;a").unwrap();
</code>Now you can apply this query to your parsed document with the <code>select</code> method. 

To get the actual titles of movies instead of the HTML elements, youâ€™ll map each HTML element to the HTML thatâ€™s inside it:
<code>    let titles = document.select(&amp;title_selector).map(|x| x.inner_html());
</code><code>titles</code> is now an iterator holding the names of all the top one hundred titles.

All you need to do now is to print out these names. 
To do that, first <code>zip</code> your title list with the numbers 1 to 100. 
Then call the <code>for_each</code> method on the resulting iterator, which will print each item of the iterator on a separate line:

<code>    titles
.zip(1..101)
.for_each(|(item, number)| println!("{}. 

{}", number, item));
</code>Your web scraper is now done.
Hereâ€™s the complete code of the scraper:

<code>fn main() {
let response = reqwest::blocking::get(
"https://www.imdb.com/search/title/?groups=top_100&amp;sort=user_rating,desc&amp;count=100",
)
.unwrap()
.text()
.unwrap();
let document = scraper::Html::parse_document(&amp;response);
let title_selector = scraper::Selector::parse("h3.lister-item-header&gt;a").unwrap();

let titles = document.select(&amp;title_selector).map(|x| x.inner_html());
titles
.zip(1..101)
.for_each(|(item, number)| println!("{}. 
{}", number, item));
}

</code>If you save the file and run it with <code>cargo run</code>, you should get the list of top one hundred movies at any given moment:
<code>1. 
The Shawshank Redemption

2. The Godfather
3. The Dark Knight
4. The Lord of the Rings: The Return of the King
5. Schindler's List
6. The Godfather: Part II
7. 12 Angry Men
8. Pulp Fiction
9. Inception
10. The Lord of the Rings: The Two Towers

...
</code><h3>Conclusion</h3>
In this tutorial, you learned how to use Rust to create a simple web scraper. 

Rust isnâ€™t a popular language for scripting, but as you saw, it gets the job done quite easily.
This is just the starting point in Rust web scraping. 
There are many ways you could upgrade this scraper, depending on your needs.

Here are some options you can try out as an exercise:
<ul>
<li><strong>Parse data into a custom struct:</strong> You can create a typed Rust struct that holds movie data. 

This will make it easier to print the data and work with it further inside your program.</li>
<li><strong>Save data in a file:</strong> Instead of printing out movie data, you can instead save it in a file.</li>
<li><strong>Create a <a href="https://docs.rs/reqwest/latest/reqwest/blocking/struct.Client.html"><code>Client</code></a> that logs into an IMDb account:</strong> You might want IMDb to display movies according to your preferences before you parse them. 

For example, IMDb shows film titles in the language of the country you live in. 
If this is an issue, you will need to configure your IMDb preferences and then create a web scraper that can log in and scrape with preferences.</li>
</ul>

However, sometimes working with CSS selectors isnâ€™t enough. 
You might need a more advanced solution that simulates actions taken by a real browser. 
In that case, you can use <a href="https://docs.rs/thirtyfour/latest/thirtyfour/"><code>thirtyfour</code></a>, Rustâ€™s UI testing library, for more powerful web scraping action.

<h2>Web Scraping With Rust</h2>
The main libraries, or crates, I'll be utilizing are the following:
<a href="https://github.com/seanmonstar/reqwest">reqwest</a>
An easy and powerful Rust HTTP Client
<a href="https://github.com/programble/scraper">scraper</a>
HTML parsing and querying with CSS selectors
<a href="https://github.com/utkarshkukreti/select.rs">select.rs</a>
A Rust library to extract useful data from HTML documents, suitable for web scraping

I'll present a couple different scripts to get a feel for each crate.

<h3>Grabbing All Links</h3>
The first script will perform a fairly basic task: grabbing all links from the page. For this, we'll utilize <code>reqwest</code> and <code>select.rs</code>. As you can see the syntax is fairly concise and straightforward.

extern crate reqwest;
extern crate select;

use select::document::Document;
use select::predicate::Name;

fn main() {
    hacker_news("https://news.ycombinator.com");
}

fn hacker_news(url: &amp;str) {
    let mut resp = reqwest::get(url).unwrap();
    assert!(resp.status().is_success());

    Document::from_read(resp)
        .unwrap()
        .find(Name("a"))
        .filter_map(|n| n.attr("href"))
        .for_each(|x| println!("{}", x));
}

The main things to note are <code>unwrap()</code> and the <code>|x|</code> notation. The first is Rust's way of telling the compiler we don't care about error handling right now. <code>unwrap()</code> will give us the value out of an <code>Option&lt;T&gt;</code> for <code>Some(v)</code>, however if the value is <code>None</code> the function will panic - not ideal for production settings. This is a common pattern when developing. The second notation is Rust's lambda syntax. Other than that, it's fairly straightforward. We send a get request to the Hacker News home page, then read in the HTML response to Document. Next we find all links and print them. If you run this you'll see the following:

<a target="_blank" href="https://camo.githubusercontent.com/24078c241b65d7b5d925baaa46d6d387c3c04db0b30c3fb94156e9674626bbcb/68747470733a2f2f692e696d6775722e636f6d2f645a49616b36542e706e67"><img src="https://camo.githubusercontent.com/24078c241b65d7b5d925baaa46d6d387c3c04db0b30c3fb94156e9674626bbcb/68747470733a2f2f692e696d6775722e636f6d2f645a49616b36542e706e67" alt="all-links" data-canonical-src="https://i.imgur.com/dZIak6T.png" style="max-width: 100%;"></a>

<h3>Using CSS Selectors</h3>
For the second example we'll use the <code>scraper</code> crate. The main advantage of <code>scraper</code> is using CSS selectors. A great tool for this is the Chrome extension <a href="http://selectorgadget.com" rel="nofollow">Selector Gadget</a>. This extension makes grabbing elements trivial. All you'll need to do is navigate to your page of interest, click the icon and select.

<a target="_blank" href="https://camo.githubusercontent.com/eb0b49d2464d1ff8f0abc69f3db440e201f6fe21673c7fa5b3244451317fe2eb/68747470733a2f2f692e696d6775722e636f6d2f4e65354b5051452e706e67"><img src="https://camo.githubusercontent.com/eb0b49d2464d1ff8f0abc69f3db440e201f6fe21673c7fa5b3244451317fe2eb/68747470733a2f2f692e696d6775722e636f6d2f4e65354b5051452e706e67" alt="css-select" data-canonical-src="https://i.imgur.com/Ne5KPQE.png" style="max-width: 100%;"></a>

Now that we know the post headline translates to <code>.storylink</code> we can retrieve it with ease.

extern crate reqwest;
extern crate scraper;

// importation syntax 
use scraper::{Html, Selector};

fn main() {
    hn_headlines("https://news.ycombinator.com");
}

fn hn_headlines(url: &amp;str) {

   let mut resp = reqwest::get(url).unwrap(); 
   assert!(resp.status().is_success());

   let body = resp.text().unwrap();
   // parses string of HTML as a document
   let fragment = Html::parse_document(&amp;body);
   // parses based on a CSS selector
   let stories = Selector::parse(".storylink").unwrap();

   // iterate over elements matching our selector
   for story in fragment.select(&amp;stories) {
        // grab the headline text and place into a vector
        let story_txt = story.text().collect::&lt;Vec&lt;_&gt;&gt;();
        println!("{:?}", story_txt);
    }
}

Perhaps the most foreign part of this syntax is the <code>::</code> annotations. The symbol merely designates a path. So, <code>Html::parse_document</code> allows us to know that <code>parse_document()</code> is a method on the <code>Html</code> struct, which is from the crate <code>scraper</code>. Other than that, we read our get request's response into a document, specified our selector, and then looped over every instance collecting the headline in a vector and printed to stdout. The example output is below.

<a target="_blank" href="https://camo.githubusercontent.com/495d41587279abfd68266bc6ea24200cd7f68e5294f67c2d6795eb67be51cf81/68747470733a2f2f692e696d6775722e636f6d2f3958636b3867562e706e67"><img src="https://camo.githubusercontent.com/495d41587279abfd68266bc6ea24200cd7f68e5294f67c2d6795eb67be51cf81/68747470733a2f2f692e696d6775722e636f6d2f3958636b3867562e706e67" alt="scraper-headline" data-canonical-src="https://i.imgur.com/9Xck8gV.png" style="max-width: 100%;"></a>

<h3>More Than One Attribute</h3>
At this point, all we've really done is grab a single element from a page, rather boring. In order to get something that can aid in the construction of the final project we'll need multiple attributes. We'll switch back to using the <code>select.rs</code> crate for this task. This is due to an increased level of control over specifying exactly what we want.

The first thing to do in this situation is inspect the element of the page. Specifically, we want to know what our post section is called.

<a target="_blank" href="https://camo.githubusercontent.com/29af823af1855a6339bc89bc42991c767927fe408606331f24abc8a0d6ebf6e7/68747470733a2f2f692e696d6775722e636f6d2f716f634c6845322e6a7067"><img src="https://camo.githubusercontent.com/29af823af1855a6339bc89bc42991c767927fe408606331f24abc8a0d6ebf6e7/68747470733a2f2f692e696d6775722e636f6d2f716f634c6845322e6a7067" alt="inspect" data-canonical-src="https://i.imgur.com/qocLhE2.jpg" style="max-width: 100%;"></a>

From the image it's pretty clear it's a class called <code>"athing"</code>. We need the top level attribute in order to iterate through every occurrence and select our desired fields.

extern crate reqwest;
extern crate select;

use select::document::Document;
use select::predicate::{Class, Name, Predicate};

fn main() {
    hacker_news("https://news.ycombinator.com");
}

fn hacker_news(url: &amp;str) {

    let resp = reqwest::get(url).unwrap();
    assert!(resp.status().is_success());

    let document = Document::from_read(resp).unwrap();

    // finding all instances of our class of interest
    for node in document.find(Class("athing")) {
        // grabbing the story rank
        let rank = node.find(Class("rank")).next().unwrap();
        // finding class, then selecting article title
        let story = node.find(Class("title").descendant(Name("a")))
            .next()
            .unwrap()
            .text();
        // printing out | rank | story headline
        println!("\n | {} | {}\n", rank.text(), story);
        // same as above
        let url = node.find(Class("title").descendant(Name("a"))).next().unwrap();
        // however, we don't grab text
        // instead find the "href" attribute, which gives us the url
        println!("{:?}\n", url.attr("href").unwrap());
    }
}

We've now got a working scraper that will gives us the rank, headline and url. However, UI is important, so let's have a go at adding some visual flair.

<h3>Adding Some Panache</h3>
This next part will build off of the <a href="https://github.com/phsym/prettytable-rs">PrettyTable</a> crate. PrettyTable is a rust library to print aligned and formatted tables, as seen below.

<code>+---------+------+---------+
| ABC     | DEFG | HIJKLMN |
+---------+------+---------+
| foobar  | bar  | foo     |
+---------+------+---------+
| foobar2 | bar2 | foo2    |
+---------+------+---------+
</code>

One of the benefits of PrettyTable is it's ability add custom formatting. Thus, for our example we will add an orange background for a consistent look.

// specifying we'll be using a macro from
// the prettytable crate (ex: row!())
#[macro_use]
extern crate prettytable;
extern crate reqwest;
extern crate select;

use select::document::Document;
use select::predicate::{Class, Name, Predicate};
use prettytable::Table;

fn main() {
    hacker_news("https://news.ycombinator.com");
}

fn hacker_news(url: &amp;str) {

    let resp = reqwest::get(url).unwrap();
    assert!(resp.status().is_success());

    let document = Document::from_read(resp).unwrap();

    let mut table = Table::new();

    // same as before
    for node in document.find(Class("athing")) {
        let rank = node.find(Class("rank")).next().unwrap();
        let story = node.find(Class("title").descendant(Name("a")))
            .next()
            .unwrap()
            .text();
        let url = node.find(Class("title").descendant(Name("a")))
            .next()
            .unwrap();
        let url_txt = url.attr("href").unwrap();
        // shorten strings to make table aesthetically appealing
        // otherwise table will look mangled by long URLs
        let url_trim = url_txt.trim_left_matches('/');
        let rank_story = format!(" | {} | {}", rank.text(), story);
        // [FdBybl-&gt;] specifies row formatting
        // F (foreground) d (black text)
        // B (background) y (yellow text) l (left-align)
        table.add_row(row![FdBybl-&gt;rank_story]);
        table.add_row(row![Fy-&gt;url_trim]);
    }
    // print table to stdout
    table.printstd();
}

The end result of running this script is as follows:

<a target="_blank" href="https://camo.githubusercontent.com/8b0e7793955d47b7d387e73263231908427d9ab4cdd324977ee8fa610dd0b053/68747470733a2f2f692e696d6775722e636f6d2f654e6c4e3232762e706e67"><img src="https://camo.githubusercontent.com/8b0e7793955d47b7d387e73263231908427d9ab4cdd324977ee8fa610dd0b053/68747470733a2f2f692e696d6775722e636f6d2f654e6c4e3232762e706e67" alt="final" data-canonical-src="https://i.imgur.com/eNlN22v.png" style="max-width: 100%;"></a>

Hopefully, this brief intro serves as a good jumping off point for exploring Rust as an everyday tool. Despite Rust being a statically typed, compiled, and non-gc language it remains a joy to work with, especially <a href="https://doc.rust-lang.org/cargo/" rel="nofollow">Cargo</a> - Rust's package manager. If you are considering learning a low level language for speed concerns, and are coming from a high-level language such as Python or Javasciprt, Rust is a fabolous choice.

<em>Here are a few resources to get up and running</em>:
<a href="https://doc.rust-lang.org/book/second-edition/" rel="nofollow">The Book</a>
<a href="https://www.amazon.com/Programming-Rust-Fast-Systems-Development/dp/1491927283/ref=sr_1_1?ie=UTF8&amp;qid=1515194775&amp;sr=8-1&amp;keywords=programming+rust" rel="nofollow">Programming Rust</a>
<a href="https://rustbyexample.com" rel="nofollow">Rust by Example</a>
<a href="https://rust-lang-nursery.github.io/rust-cookbook/" rel="nofollow">Rust Cookbook</a>
<a href="https://users.rust-lang.org" rel="nofollow">Rust Forum</a>
<a href="https://www.reddit.com/r/rust/" rel="nofollow">r/rust</a>

<h2>one-line functions for reading and writing</h2>
Read a file to a String
use std::fs;

fn main() {
    let data = fs::read_to_string("/etc/hosts").expect("Unable to read file");
    println!("{}", data);
}

Read a file as a Vec&lt;u8>
use std::fs;

fn main() {
    let data = fs::read("/etc/hosts").expect("Unable to read file");
    println!("{}", data.len());
}

Write a file
use std::fs;

fn main() {
    let data = "Some data!";
    fs::write("/tmp/foo", data).expect("Unable to write file");
}


<h2>Read lines of strings from a file</h2>
Writes a three-line message to a file, then reads it back a line at a time with the Lines iterator created by BufRead::lines.
File implements Read which provides BufReader trait.
File::create opens a File for writing,
File::open for reading.

use std::fs::File;
use std::io::{Write, BufReader, BufRead, Error};

fn main() -> Result<(), Error> {
    let path = "lines.txt";

    let mut output = File::create(path)?;
    write!(output, "Rust\nğŸ’–\nFun")?;

    let input = File::open(path)?;
    let buffered = BufReader::new(input);

    for line in buffered.lines() {
        println!("{}", line?);
    }
    Ok(())
}

<h2>ç»ˆç«¯æ ·å¼</h2>
æœ‰æ ·å¼çš„ï¼ˆå‰æ™¯è‰²ï¼ŒèƒŒæ™¯è‰²ï¼Œç²—ä½“ï¼Œæ–œä½“ï¼Œå°±æ˜¯
ç»ˆç«¯è‡ªå¸¦çš„é‚£äº›ï¼‰ï¼Œå¦ä¸€æ–¹é¢æœ‰éœ€è¦ç»å¸¸æ”¹åŠ¨å’Œå˜åŒ–ï¼Œå†™æ­»åœ¨ä»£ç é‡Œæ€»æ„Ÿè§‰ä¸å¤ªæ–¹ä¾¿ï¼Œäºæ˜¯åšäº†ä¸ªæ ‡è®°è¯­è¨€æ¥æŠŠè¿™äº›å¸¦æ ·å¼çš„æ–‡æœ¬å†™æˆçº¯æ–‡æœ¬ï¼Œåœ¨éœ€è¦çš„æ—¶å€™ç¼–è¯‘æˆç›®æ ‡æ¡†æ¶æ‰€éœ€è¦çš„ç»“æ„ã€‚
<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/A9lbmbycqtoHfEoNyns6gD6ia7ajGYbFibkl0nZNbicRwXINaDPhKKjCha83hzaa84iaNiawAhQbzrn2HiaWOU2uJKvA/640">

https://github.com/7sDream/tui-markup
https://docs.rs/tui-markup/0.2.0/tui_markup/
https://github.com/7sDream/tui-markup
https://docs.rs/tui-markup/0.2.0/tui_markup/

<h2>Rust Wasm å›¾ç‰‡è½¬ ASCII</h2>
https://mp.weixin.qq.com/s?__biz=MzI1MjAzNDI1MA==&mid=2648216805&idx=2&sn=20a59d153cbda25d82ead8e9fe0b2d56

<h2>rust æ”¯ä»˜å®æ”¯ä»˜ SDK</h2>
https://mp.weixin.qq.com/s?__biz=MzI1MjAzNDI1MA==&mid=2648216324&idx=2&sn=1faeb20cc8a6b89de1cbda260a29206d





<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});

//d3.selectAll("h2").style("color", function() {
//   return "hsl(" + Math.random() * 360 + ",80%,50%)";
//});

</script>
</body>
</html>
