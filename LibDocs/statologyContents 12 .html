<base target="_blank"><html><head><title>statologyContents 12</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 12"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 12</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Perform a One-Way ANOVA in SPSS</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This type of test is called a one-way ANOVA because we are analyzing how <b>one </b>predictor variable impacts a response variable.
If we were instead interested in how <b>two</b> predictor variables impact a response variable, we could conduct a  two-way ANOVA .
This tutorial explains how to conduct a one-way ANOVA in SPSS.
<h3>Example: One-Way ANOVA in SPSS</h3>
Suppose a researcher recruits 30 students to participate in a study. The students are randomly assigned to use one of three studying techniques for the next month to prepare for an exam. At the end of the month, all of the students take the same test. 
The test scores for the students are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss1.png">
Use the following steps to perform a one-way ANOVA to determine if the average scores are the same across all three groups.
<b>Step 1: Visualize the data.</b>
First, we’ll create  boxplots  to visualize the distribution of test scores for each of the three studying techniques. Click the <b>Graphs </b>tab, then click <b>Chart Builder</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss2.png">
Select <b>Boxplot </b>in the <b>Choose from: </b>window. Then drag the first chart titled <b>Simple boxplot </b>into the main editing window. Drag the variable <b>technique </b>onto the x-axis and <b>score </b>onto the y-axis.
Then click <b>Element Properties</b>, then <b>Y-axis1</b>. Change the <b>minimum </b>value to 60. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss3.png">
The following boxplots will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss4.png">
We can see that the distribution of test scores tend to be higher for students who used technique 2 compared to students who used techniques 1 and 3. To determine if these differences in scores are statistically significant, we’ll perform a one-way ANOVA.
<b>Step 2: Perform a one-way ANOVA.</b>
Click the <b>Analyze </b>tab, then <b>Compare Means</b>, then <b>One-Way ANOVA</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss5.png">
In the new window that pops up, place the variable <b>score </b>into the box labelled Dependent list and the variable <b>technique </b>into the box labelled Factor.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss6.png">
Then click <b>Post Hoc </b>and check the box next to <b>Tukey</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss7.png">
Then click <b>Options </b>and check the box next to <b>Descriptive</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss8.png">
Lastly, click <b>OK</b>.
<b>Step 3: Interpret the output.</b>
Once you click <b>OK</b>, the results of the one-way ANOVA will appear. Here is how to interpret the output:
<b>Descriptives Table</b>
This table displays descriptive statistics for each of the three groups in our dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss9.png">
The most relevant numbers include:
<b>N: </b>The number of students in each group.
<b>Mean: </b>The mean test score for each group.
<b>Std. Deviation: </b>The standard deviation of test scores for each group.
<b>ANOVA Table</b>
This table displays the results of the one-way ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss10.png">
The most relevant numbers include:
<b>F: </b>The overall F-statistic.
<b>Sig: </b>The p-value that corresponds to the F-statistic (4.545) with df numerator (2) and df denominator (27). In this case, the p-value turns out to be <b>.020</b>.
Recall that a one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null hypothesis):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>= … = μ<sub>k </sub>(all the population means are equal)
<b>H<sub>A </sub>(alternative hypothesis):</b> at least one population mean is different<sub> </sub>from the rest
Since the p-value from the ANOVA table is less than .05, we have sufficient evidence to reject the null hypothesis and conclude that at least one of the group means is different from the rest.
To find out exactly which group means differ from one another, we can refer to the last table in the ANOVA output.
<b>Multiple Comparisons Table</b>
This table displays the Tukey post-hoc multiple comparisons between each of the three groups. We are mostly interested in the <b>Sig. </b>column, which displays the p-values for the differences in means between each group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss11.png">
From the table we can see the p-values for the following comparisons:
Technique 1 vs. 2: | p-value = <b>0.024</b>
Technique 1 vs. 3 | p-value = <b>0.883</b>
Technique 2 vs. 3 | p-value = <b>0.067</b>
The only group comparison that has a p-value less than .05 is between technique 1 and technique 2. 
This tells us that there is a statistically significant difference in average test scores between students who used technique 1 compared to students who used technique 2.
However, there is no statistically significant difference between technique 1 and 3, or between technique 2 and 3.
<b>Step 4: Report the results.</b>
Lastly, we can report the results of the one-way ANOVA. Here is an example of how to do so:
A one-way ANOVA was performed to determine if three different studying techniques lead to different test scores.
 
A total of 10 students used each of the three studying techniques for one month before all taking the same test.
 
A one-way ANOVA revealed that there was a statistically significant difference in test scores between at least two groups (F(2, 27) = 4.545, p = 0.020).
 
Tukey’s test for multiple comparisons found that mean test scores were significantly different between students who used technique 1 and technique 2 (p = .024, 95% C.I. = [-14.48, -.92]).
 
There was no statistically significant difference between scores for techniques 1 and 3 (p=.883) or between scores for techniques 2 and 3 (p = .067).
<h2><span class="orange">How to Perform a One-Way ANOVA in Stata</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This type of test is called a <em>one-way</em> ANOVA because we are analyzing how <em>one </em>predictor variable impacts a response variable. If we were instead interested in how two predictor variables impact a response variable, we could conduct a  <em>two-way </em>ANOVA .
This tutorial explains how to conduct a one-way ANOVA in Stata.
<h2>Example: One-Way ANOVA in Stata</h2>
In this example we will use the built-in Stata dataset called <em>systolic </em>to perform a one-way ANOVA. This dataset contains the following three variables for 58 different individuals:
Drug used
Patient’s disease
Change in systolic blood pressure
We will use the following steps to perform a one-way ANOVA to find out if the type of drug used leads to a significant impact in the change in systolic blood pressure.
<b>Step 1: Load the data.</b>
First, load the data by typing <b>webuse systolic </b>in the command box and clicking Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata1.png">
<b>Step 2: View the raw data.</b>
Before we perform a one-way ANOVA, let’s first view the raw data. Along the top menu bar, go to <b>Data > Data Editor > Data Editor (Browse)</b>. This will show us the actual data for all 58 patients:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata2.png">
<b>Step 3: Visualize the data.</b>
Next, let’s visualize the data. We’ll create  boxplots  to view the distribution of systolic blood pressure values for each category of drug.
Along the top menu bar, go to <b>Graphics > Box plot</b>. Under variables, choose Systolic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata3.png">
Then, in the Categories subheading under Grouping variable, choose drug:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata4.png">
Click <em>OK</em>. A chart with four boxplots will automatically be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata.png">
We can immediately see that the distribution of changes in systolic blood pressure vary between the drug categories, but a one-way ANOVA will tell us if these differences are statistically significant.
<b>Step 4: Perform a one-way ANOVA.</b>
Along the top menu bar, go to <b>Statistics > Linear models and related > ANOVA/MANOVA > One-Way ANOVA</b>.
Under response variable, choose systolic. Under factor variable, choose drug. Then click the box next to <em>Produce summary table</em> so that we can see some basic descriptive statistics for each group. Then click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata5.png">
The following output will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata6.png">
The F-statistic is <b>9.09 </b>and the corresponding p-value is <b>0.0001</b>. Since the p-value is less than alpha = 0.05, we can reject the null hypothesis that the mean change in systolic blood pressure for each group is equal.
In other words, there is a statistically significant difference in the mean change in systolic blood pressure between at least two of the drug groups.
<b>Step 5: Perform multiple comparison tests.</b>
Next, we can perform multiple comparison tests to actually find out which group means are different from each other.
Along the top menu bar, go to <b>Statistics > Summaries, tables, and tests > Summary and descriptive statistics > Pairwise comparisons of</b> <b>means</b>.
For Variable, choose the response variable <em>systolic</em>. For Over, choose the explanatory variable <em>drug</em>. For Multiple comparisons adjustment, choose <em>Tukey’s method</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata7.png">
Then, under the <em>Reporting</em> subheading click the button next to <em>Effects tables </em>and check the box next to <em>Show effects table with confidence intervals and p-values</em>. Then click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata9.png">
The following results will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata10.png">
Each row represents a comparison between two specific drug groups. For example, the first row compares the mean systolic blood pressure change between drug group 2 and drug group 1. The p-value for this comparison is <b>0.999</b>, which is extremely high and not smaller than 0.05. This means there is no statistically significant difference between drug groups 1 and 2.
However, we can see that the p-values for the following comparisons are all less than 0.05:
drug 3 vs. 1 | p-value = <b>0.001</b>
drug 4 vs. 1 | p-value = <b>0.010</b>
drug 3 vs. 2 | p-value = <b>0.001</b>
drug 4 vs. 2 | p-value = <b>0.015</b>
This means that the difference in mean systolic blood pressure change is statistically significant between each of these groups.
<b>Step 6: Report the results.</b>
Lastly, we will report the results of our One-Way ANOVA analysis. Here is an example of how to do so:
A one-way ANOVA was performed to determine if four different types of drugs had different impacts on systolic blood pressure.
The following table summarizes the number of participants in each group along with the mean change in systolic blood pressure and the standard deviation in systolic blood pressure for each group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayStata11.png">
A one-way ANOVA revealed that there was a statistically significant difference between at least two groups (F(3, 54) = 9.09, p = 0.001).
Tukey’s test for multiple comparisons found that the change in systolic blood pressure was statistically significantly higher for drug 3 compared to drug 1 (17.32 +/- 4.15, p = 0.001), for drug 3 compared to drug 2 (16.78 +/- 4.15, p = 0.001), for drug 4 compared to drug 1 (12.57 +/- 3.85, p = 0.010), and for drug 4 compared to drug 2 (12.03 +/- 3.85, p = 0.015).
There was no statistically significant difference between drug groups 1 and 2 (.533 +/- 3.91, p = 0.999) or between drug groups 3 and 4 (4.75 +/- 4.09, p = 0.654).
<h2><span class="orange">One-Way ANOVA vs. Repeated Measures ANOVA: The Difference</span></h2>
Two types of ANOVA models that students often get confused between are the one-way ANOVA and the repeated measures one-way ANOVA.
Here’s the simple difference:
A <b>one-way ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
A <b>repeated measures one-way ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more groups <em>in which the same subjects show up in each group</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rep1.png">
For example, suppose a professor wants to determine if three different studying techniques lead to different mean exam scores. To test this, he recruits 15 students and randomly assigns 5 students to use each studying technique for one week before the exam.
He could use a <b>one-way ANOVA</b> to test for differences between the group means since each student only appears in one group each.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rep2.png">
However, suppose the professor recruits just 5 students and has each student use each studying technique during three different weeks to prepare for tests of equal difficulty.
In this scenario, he could use a <b>repeated measures one-way ANOVA</b> to test for differences between the group means since each student appears in each group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rep3.png">
<h3>When to Use a Repeated Measures ANOVA</h3>
A repeated measures ANOVA is used in two specific situations:
<b>1. Measuring the mean scores of subjects during three or more time points.</b> For example, you might want to measure the resting heart rate of subjects one month before they start a training program, once during the middle of the program, and one month after the program to see if there is a significant difference in mean resting heart rate across these three time points.
  
Since the heart rate of each subject is measured <em>repeatedly</em>, we can use a repeated measures ANOVA to determine if there is a significant difference in mean heart rate across these three time periods.
<b>2. Measuring the mean scores of subjects under three different conditions.</b> For example, you might have subjects watch three different movies and rate each one based on how much they enjoyed it. 
  
Again, the same subjects show up in each group, so we need to use a repeated measures ANOVA to test for the difference in means across these three conditions. 
<h3>Pros & Cons of the Repeated Measures ANOVA</h3>
A repeated measures one-way ANOVA offers the following <b>pros</b> over the ordinary one-way ANOVA:
<b>1.</b> It’s faster and more cost-effective to recruit a small number of individuals to participate in a repeated measures one-way ANOVA since researchers can simply obtain data from the same individuals multiple times.
<b>2.</b> Researchers can attribute a portion of the variance in the data to the individuals themselves, which makes it easier to detect true differences that exist between the different treatments.
However, a repeated measures one-way ANOVA comes with the following <b>cons:</b>
<b>1.</b> If one individual drops out of the experiment, researchers lose out on more data compared to an ordinary one-way ANOVA.
<b>2.</b> There is the potential for individuals to suffer from  order effects  – which refers to differences in participant behavior as a result of the order in which treatments are presented to them. For example, individuals may become tired or fatigued by time they experience the last treatment.
<h2><span class="orange">One-Way ANOVA: Definition, Formula, and Example</span></h2>
A <b>one-way ANOVA</b> (“analysis of variance”) compares the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means.
This tutorial explains the following:
The motivation for performing a one-way ANOVA.
The assumptions that should be met to perform a one-way ANOVA.
The process to perform a one-way ANOVA.
An example of how to perform a one-way ANOVA.
<h3>One-Way ANOVA: Motivation</h3>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a college entrance exam. Since there are millions of high school students around the country, it would be too time-consuming and costly to go around to each student and let them use one of the exam prep programs.
Instead, we might select three  random samples  of 100 students from the population and allow each sample to use one of the three test prep programs to prepare for the exam. Then, we could record the scores for each student once they take the exam.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay1.png">
However, it’s virtually guaranteed that the mean exam score between the three samples will be at least a little different. <b>The question is whether or not this difference is statistically significant</b>. Fortunately, a one-way ANOVA allows us to answer this question.
<h3>One-Way ANOVA: Assumptions</h3>
For the results of a one-way ANOVA to be valid, the following assumptions should be met:
<b>1. Normality </b>– Each sample was drawn from a normally distributed population.
<b>2. Equal Variances </b>– The variances of the populations that the samples come from are equal. You can use  Bartlett’s Test  to verify this assumption.
<b>3. Independence </b>– The observations in each group are independent of each other and the observations within groups were obtained by a random sample.
Read  this article  for in-depth details on how to check these assumptions.
<h3>One-Way ANOVA: The Process</h3>
A one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null hypothesis):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>= … = μ<sub>k </sub>(all the population means are equal)
<b>H<sub>1 </sub>(alternative hypothesis):</b> at least one population mean is different<sub> </sub>from the rest
You will typically use some statistical software (such as R, Excel, Stata, SPSS, etc.) to perform a one-way ANOVA since it’s cumbersome to perform by hand.
No matter which software you use, you will receive the following table as output:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F</b></th>
<th style="text-align: center;"><b>p</b></th>
</tr>
<tr>
<td><b>Treatment</b></td>
<td style="text-align: center;">SSR</td>
<td style="text-align: center;">df<sub>r</sub></td>
<td style="text-align: center;">MSR</td>
<td style="text-align: center;">MSR/MSE</td>
<td>F<sub>df<sub>r</sub>, df<sub>e</sub></sub></td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">SSE</td>
<td style="text-align: center;">df<sub>e</sub></td>
<td style="text-align: center;">MSE</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">SST</td>
<td style="text-align: center;">df<sub>t</sub></td>
<td> </td>
<td> </td>
<td> </td>
</tr>
</tbody></table>
where:
<b>SSR:</b> regression sum of squares
<b>SSE:</b> error sum of squares
<b>SST:</b> total sum of squares (SST = SSR + SSE)
<b>df<sub>r</sub>:</b> regression degrees of freedom (df<sub>r </sub>= k-1)
<b>df<sub>e</sub>:</b> error degrees of freedom (df<sub>e </sub>= n-k)
<b>df<sub>t</sub>:</b> total degrees of freedom (df<sub>t </sub>= n-1)
<b>k: </b>total number of groups
<b>n: </b>total observations
<b>MSR: </b>regression mean square (MSR = SSR/df<sub>r</sub>)
<b>MSE: </b>error mean square (MSE = SSE/df<sub>e</sub>)
<b>F: </b>The F test statistic (F = MSR/MSE)
<b>p: </b>The p-value that corresponds to F<sub>dfr, dfe</sub>
If the p-value is less than your chosen significance level (e.g. 0.05), then you can reject the null hypothesis and conclude that at least one of the population means is different from the others.
<b>Note: </b>If you reject the null hypothesis, this indicates that at least one of the population means is different from the others, but the ANOVA table doesn’t specify <em>which </em>population means are different. To determine this, you need to perform  post hoc tests , also known as “multiple comparisons” tests.
<h3>One-Way ANOVA: Example</h3>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a certain exam. To test this, we recruit 30 students to participate in a study and split them into three groups.
The students in each group are  randomly assigned  to use one of the three exam prep programs for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same exam. 
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png">
To perform a one-way ANOVA on this data, we will use the Statology  One-Way ANOVA Calculator  with the following input:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay3.png">
From the output table we see that the F test statistic is <b>2.358 </b>and the corresponding p-value is <b>0.11385</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay4.png">
Since this p-value is not less than 0.05, we fail to reject the null hypothesis.
This means we don’t have sufficient evidence to say that there is a statistically significant difference between the mean exam scores of the three groups.
<h2><span class="orange">One-Way Repeated Measures ANOVA Calculator</span></h2>
The one-way repeated measures ANOVA calculator compares the means of three or more samples in which each subject shows up in each sample.
Simply enter the values for up to five samples into the cells below, then press the “Calculate” button.
<table><tbody>
<tr>
<th style="background-color: #bee3ff"><b>Group 1</b></th>
            <th style="background-color: #bee3ff"><b>Group 2</b></th>
            <th style="background-color: #bee3ff"><b>Group 3</b></th>
            <th style="background-color: #bee3ff"><b>Group 4</b></th>
            <th style="background-color: #bee3ff"><b>Group 5</b></th>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
</tbody></table>
<input type="button" id="button" onclick="calc()" value="Calculate">
<table><tbody>
<tr>
<th style="background-color: #bee3ff"><b>Source</b></th>
            <th style="background-color: #bee3ff"><b>SS</b></th>
            <th style="background-color: #bee3ff"><b>df</b></th>
            <th style="background-color: #bee3ff"><b>MS</b></th>
            <th style="background-color: #bee3ff"><b>F</b></th>
            <th style="background-color: #bee3ff"><b>P</b></th>
        </tr>
<tr>
<td>Between</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td>Subject</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td>Error</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
</tbody></table>
<script>
    var div_table = document.getElementById('words_table');
//create function that performs calculations
function calc() {
//define addition function
function add(a, b) {
    return a + b;
}
//get raw data for each treament group
        var values_a_input = document.getElementsByClassName('table_span_a');var values_a_array = [];
for (var i = 0; i < values_a_input.length; i++) {
      values_a_array[i] = values_a_input[i].innerText;
         }
    values_a_array = values_a_array.filter(n => n);
var group_a = values_a_array.map(Number);
        var values_b_input = document.getElementsByClassName('table_span_b');var values_b_array = [];
for (var i = 0; i < values_b_input.length; i++) {
      values_b_array[i] = values_b_input[i].innerText;
         }
values_b_array = values_b_array.filter(n => n);
var group_b = values_b_array.map(Number);
        var values_c_input = document.getElementsByClassName('table_span_c');var values_c_array = [];
for (var i = 0; i < values_c_input.length; i++) {
      values_c_array[i] = values_c_input[i].innerText;
         }
values_c_array = values_c_array.filter(n => n);
var group_c = values_c_array.map(Number);
        var values_d_input = document.getElementsByClassName('table_span_d');var values_d_array = [];
for (var i = 0; i < values_d_input.length; i++) {
      values_d_array[i] = values_d_input[i].innerText;
         }
values_d_array = values_d_array.filter(n => n);
var group_d = values_d_array.map(Number);
        var values_e_input = document.getElementsByClassName('table_span_e');var values_e_array = [];
for (var i = 0; i < values_e_input.length; i++) {
      values_e_array[i] = values_e_input[i].innerText;
         }
values_e_array = values_e_array.filter(n => n);
var group_e = values_e_array.map(Number);
//find raw data for each subject
    var person1_input = document.getElementsByClassName('person1');var person1_array = [];
for (var i = 0; i < person1_input.length; i++) {person1_array[i] = person1_input[i].innerText;}
    person1_array = person1_array.filter(n => n);var person1 = person1_array.map(Number);var sum_person1 = person1.reduce(add, 0);
    
    var person2_input = document.getElementsByClassName('person2');var person2_array = [];
for (var i = 0; i < person2_input.length; i++) {person2_array[i] = person2_input[i].innerText;}
    person2_array = person2_array.filter(n => n);var person2 = person2_array.map(Number);var sum_person2 = person2.reduce(add, 0);
    
    var person3_input = document.getElementsByClassName('person3');var person3_array = [];
for (var i = 0; i < person3_input.length; i++) {person3_array[i] = person3_input[i].innerText;}
    person3_array = person3_array.filter(n => n);var person3 = person3_array.map(Number);var sum_person3 = person3.reduce(add, 0);
    
    var person4_input = document.getElementsByClassName('person4');var person4_array = [];
for (var i = 0; i < person4_input.length; i++) {person4_array[i] = person4_input[i].innerText;}
    person4_array = person4_array.filter(n => n);var person4 = person4_array.map(Number);var sum_person4 = person4.reduce(add, 0);
var person5_input = document.getElementsByClassName('person5');var person5_array = [];
for (var i = 0; i < person5_input.length; i++) {person5_array[i] = person5_input[i].innerText;}
    person5_array = person5_array.filter(n => n);var person5 = person5_array.map(Number);var sum_person5 = person5.reduce(add, 0);
var person6_input = document.getElementsByClassName('person6');var person6_array = [];
for (var i = 0; i < person6_input.length; i++) {person6_array[i] = person6_input[i].innerText;}
    person6_array = person6_array.filter(n => n);var person6 = person6_array.map(Number);var sum_person6 = person6.reduce(add, 0);
var person7_input = document.getElementsByClassName('person7');var person7_array = [];
for (var i = 0; i < person7_input.length; i++) {person7_array[i] = person7_input[i].innerText;}
    person7_array = person7_array.filter(n => n);var person7 = person7_array.map(Number);var sum_person7 = person7.reduce(add, 0);
var person8_input = document.getElementsByClassName('person8');var person8_array = [];
for (var i = 0; i < person8_input.length; i++) {person8_array[i] = person8_input[i].innerText;}
    person8_array = person8_array.filter(n => n);var person8 = person8_array.map(Number);var sum_person8 = person8.reduce(add, 0);
var person9_input = document.getElementsByClassName('person9');var person9_array = [];
for (var i = 0; i < person9_input.length; i++) {person9_array[i] = person9_input[i].innerText;}
    person9_array = person9_array.filter(n => n);var person9 = person9_array.map(Number);var sum_person9 = person9.reduce(add, 0);
var person10_input = document.getElementsByClassName('person10');var person10_array = [];
for (var i = 0; i < person10_input.length; i++) {person10_array[i] = person10_input[i].innerText;}
    person10_array = person10_array.filter(n => n);var person10 = person10_array.map(Number);var sum_person10 = person10.reduce(add, 0);
var person11_input = document.getElementsByClassName('person11');var person11_array = [];
for (var i = 0; i < person11_input.length; i++) {person11_array[i] = person11_input[i].innerText;}
    person11_array = person11_array.filter(n => n);var person11 = person11_array.map(Number);var sum_person11 = person11.reduce(add, 0);
var person12_input = document.getElementsByClassName('person12');var person12_array = [];
for (var i = 0; i < person12_input.length; i++) {person12_array[i] = person12_input[i].innerText;}
    person12_array = person12_array.filter(n => n);var person12 = person12_array.map(Number);var sum_person12 = person12.reduce(add, 0);
var person13_input = document.getElementsByClassName('person13');var person13_array = [];
for (var i = 0; i < person13_input.length; i++) {person13_array[i] = person13_input[i].innerText;}
    person13_array = person13_array.filter(n => n);var person13 = person13_array.map(Number);var sum_person13 = person13.reduce(add, 0);
var person14_input = document.getElementsByClassName('person14');var person14_array = [];
for (var i = 0; i < person14_input.length; i++) {person14_array[i] = person14_input[i].innerText;}
    person14_array = person14_array.filter(n => n);var person14 = person14_array.map(Number);var sum_person14 = person14.reduce(add, 0);
var person15_input = document.getElementsByClassName('person15');var person15_array = [];
for (var i = 0; i < person15_input.length; i++) {person15_array[i] = person15_input[i].innerText;}
    person15_array = person15_array.filter(n => n);var person15 = person15_array.map(Number);var sum_person15 = person15.reduce(add, 0);
var person16_input = document.getElementsByClassName('person16');var person16_array = [];
for (var i = 0; i < person16_input.length; i++) {person16_array[i] = person16_input[i].innerText;}
    person16_array = person16_array.filter(n => n);var person16 = person16_array.map(Number);var sum_person16 = person16.reduce(add, 0);
var person17_input = document.getElementsByClassName('person17');var person17_array = [];
for (var i = 0; i < person17_input.length; i++) {person17_array[i] = person17_input[i].innerText;}
    person17_array = person17_array.filter(n => n);var person17 = person17_array.map(Number);var sum_person17 = person17.reduce(add, 0);
var person18_input = document.getElementsByClassName('person18');var person18_array = [];
for (var i = 0; i < person18_input.length; i++) {person18_array[i] = person18_input[i].innerText;}
    person18_array = person18_array.filter(n => n);var person18 = person18_array.map(Number);var sum_person18 = person18.reduce(add, 0);
var person19_input = document.getElementsByClassName('person19');var person19_array = [];
for (var i = 0; i < person19_input.length; i++) {person19_array[i] = person19_input[i].innerText;}
    person19_array = person19_array.filter(n => n);var person19 = person19_array.map(Number);var sum_person19 = person19.reduce(add, 0);
var person20_input = document.getElementsByClassName('person20');var person20_array = [];
for (var i = 0; i < person20_input.length; i++) {person20_array[i] = person20_input[i].innerText;}
    person20_array = person20_array.filter(n => n);var person20 = person20_array.map(Number);var sum_person20 = person20.reduce(add, 0);
var person21_input = document.getElementsByClassName('person21');var person21_array = [];
for (var i = 0; i < person21_input.length; i++) {person21_array[i] = person21_input[i].innerText;}
    person21_array = person21_array.filter(n => n);var person21 = person21_array.map(Number);var sum_person21 = person21.reduce(add, 0);
var person22_input = document.getElementsByClassName('person22');var person22_array = [];
for (var i = 0; i < person22_input.length; i++) {person22_array[i] = person22_input[i].innerText;}
    person22_array = person22_array.filter(n => n);var person22 = person22_array.map(Number);var sum_person22 = person22.reduce(add, 0);
var person23_input = document.getElementsByClassName('person23');var person23_array = [];
for (var i = 0; i < person23_input.length; i++) {person23_array[i] = person23_input[i].innerText;}
    person23_array = person23_array.filter(n => n);var person23 = person23_array.map(Number);var sum_person23 = person23.reduce(add, 0);
var person24_input = document.getElementsByClassName('person24');var person24_array = [];
for (var i = 0; i < person24_input.length; i++) {person24_array[i] = person24_input[i].innerText;}
    person24_array = person24_array.filter(n => n);var person24 = person24_array.map(Number);var sum_person24 = person24.reduce(add, 0);
var person25_input = document.getElementsByClassName('person25');var person25_array = [];
for (var i = 0; i < person25_input.length; i++) {person25_array[i] = person25_input[i].innerText;}
    person25_array = person25_array.filter(n => n);var person25 = person25_array.map(Number);var sum_person25 = person25.reduce(add, 0);
var person26_input = document.getElementsByClassName('person26');var person26_array = [];
for (var i = 0; i < person26_input.length; i++) {person26_array[i] = person26_input[i].innerText;}
    person26_array = person26_array.filter(n => n);var person26 = person26_array.map(Number);var sum_person26 = person26.reduce(add, 0);
var person27_input = document.getElementsByClassName('person3');var person27_array = [];
for (var i = 0; i < person27_input.length; i++) {person3_array[i] = person27_input[i].innerText;}
    person27_array = person27_array.filter(n => n);var person27 = person27_array.map(Number);var sum_person27 = person27.reduce(add, 0);
var person28_input = document.getElementsByClassName('person28');var person28_array = [];
for (var i = 0; i < person28_input.length; i++) {person28_array[i] = person28_input[i].innerText;}
    person28_array = person28_array.filter(n => n);var person28 = person28_array.map(Number);var sum_person28 = person28.reduce(add, 0);
var person29_input = document.getElementsByClassName('person29');var person29_array = [];
for (var i = 0; i < person29_input.length; i++) {person29_array[i] = person29_input[i].innerText;}
    person29_array = person29_array.filter(n => n);var person29 = person29_array.map(Number);var sum_person29 = person29.reduce(add, 0);
var person30_input = document.getElementsByClassName('person30');var person30_array = [];
for (var i = 0; i < person30_input.length; i++) {person30_array[i] = person30_input[i].innerText;}
    person30_array = person30_array.filter(n => n);var person30 = person30_array.map(Number);var sum_person30 = person30.reduce(add, 0);
//find mean of each group
if (group_a.length > 0) { var mean_group_a = math.mean(group_a); };
if (group_b.length > 0) { var mean_group_b = math.mean(group_b); };
if (group_c.length > 0) { var mean_group_c = math.mean(group_c); };
if (group_d.length > 0) { var mean_group_d = math.mean(group_d); };
if (group_e.length > 0) { var mean_group_e = math.mean(group_e); };
//find total mean, total count, total variance of all groups
var all_groups = group_a.concat(group_b, group_c, group_d, group_e);
var mean_all_groups = math.mean(all_groups);
var var_all_groups = jStat.variance(all_groups,true);
var count_all_groups = all_groups.length;
//find sum of each group
if (group_a.length > 0) { var sum_group_a = group_a.reduce(add, 0); } else { var sum_group_a = 0;};
if (group_b.length > 0) { var sum_group_b = group_b.reduce(add, 0); } else { var sum_group_b = 0;};
if (group_c.length > 0) { var sum_group_c = group_c.reduce(add, 0); } else { var sum_group_c = 0;};
if (group_d.length > 0) { var sum_group_d = group_d.reduce(add, 0); } else { var sum_group_d = 0;};
if (group_e.length > 0) { var sum_group_e = group_e.reduce(add, 0); } else { var sum_group_e = 0;};
//find total sum
var sum_all_groups = [sum_group_a, sum_group_b, sum_group_c, sum_group_d, sum_group_e].reduce(add, 0);
//find squared sum of each group
if (group_a.length > 0) { var sumSquare_group_a = (group_a.map(function(x) { return Math.pow(x, 2); })).reduce(add,0); } else { var sumSquare_group_a = 0;};
if (group_b.length > 0) { var sumSquare_group_b = (group_b.map(function(x) { return Math.pow(x, 2); })).reduce(add,0); } else { var sumSquare_group_b = 0;};
if (group_c.length > 0) { var sumSquare_group_c = (group_c.map(function(x) { return Math.pow(x, 2); })).reduce(add,0); } else { var sumSquare_group_c = 0;};
if (group_d.length > 0) { var sumSquare_group_d = (group_d.map(function(x) { return Math.pow(x, 2); })).reduce(add,0); } else { var sumSquare_group_d = 0;};
if (group_e.length > 0) { var sumSquare_group_e = (group_e.map(function(x) { return Math.pow(x, 2); })).reduce(add,0); } else { var sumSquare_group_e = 0;};
//find total sum squares
var sumSquare_all_groups = [sumSquare_group_a, sumSquare_group_b, sumSquare_group_c, sumSquare_group_d, sumSquare_group_e].reduce(add, 0);
//find difference between group means and overall mean
if (group_a.length > 0) { var mean_group_a_diff = mean_group_a - mean_all_groups; } else { var mean_group_a_diff = 0;};
if (group_b.length > 0) { var mean_group_b_diff = mean_group_b - mean_all_groups; } else { var mean_group_b_diff = 0;};
if (group_c.length > 0) { var mean_group_c_diff = mean_group_c - mean_all_groups; } else { var mean_group_c_diff = 0;};
if (group_d.length > 0) { var mean_group_d_diff = mean_group_d - mean_all_groups; } else { var mean_group_d_diff = 0;};
if (group_e.length > 0) { var mean_group_e_diff = mean_group_e - mean_all_groups; } else { var mean_group_e_diff = 0;};
//find length of each group
if (group_a.length > 0) { var count_group_a = group_a.length; } else { var count_group_a = 0;};
if (group_b.length > 0) { var count_group_b = group_b.length; } else { var count_group_b = 0;};
if (group_c.length > 0) { var count_group_c = group_c.length; } else { var count_group_c = 0;};
if (group_d.length > 0) { var count_group_d = group_d.length; } else { var count_group_d = 0;};
if (group_e.length > 0) { var count_group_e = group_e.length; } else { var count_group_e = 0;};
//find total number of treatments
if (group_a.length > 0) { var flag_group_a = 1; } else { var flag_group_a = 0;};
if (group_b.length > 0) { var flag_group_b = 1; } else { var flag_group_b = 0;};
if (group_c.length > 0) { var flag_group_c = 1; } else { var flag_group_c = 0;};
if (group_d.length > 0) { var flag_group_d = 1; } else { var flag_group_d = 0;};
if (group_e.length > 0) { var flag_group_e = 1; } else { var flag_group_e = 0;};
var total_treatments = [flag_group_a, flag_group_b, flag_group_c, flag_group_d, flag_group_e].reduce(add, 0);
//find total number of subjects
if (person1.length > 0) { var flag_person1 = 1; } else { var flag_person1 = 0;};
if (person2.length > 0) { var flag_person2 = 1; } else { var flag_person2 = 0;};
if (person3.length > 0) { var flag_person3 = 1; } else { var flag_person3 = 0;};
if (person4.length > 0) { var flag_person4 = 1; } else { var flag_person4 = 0;};
if (person5.length > 0) { var flag_person5 = 1; } else { var flag_person5 = 0;};
if (person6.length > 0) { var flag_person6 = 1; } else { var flag_person6 = 0;};
if (person7.length > 0) { var flag_person7 = 1; } else { var flag_person7 = 0;};
if (person8.length > 0) { var flag_person8 = 1; } else { var flag_person8 = 0;};
if (person9.length > 0) { var flag_person9 = 1; } else { var flag_person9 = 0;};
if (person10.length > 0) { var flag_person10 = 1; } else { var flag_person10 = 0;};
if (person11.length > 0) { var flag_person11 = 1; } else { var flag_person11 = 0;};
if (person12.length > 0) { var flag_person12 = 1; } else { var flag_person12 = 0;};
if (person13.length > 0) { var flag_person13 = 1; } else { var flag_person13 = 0;};
if (person14.length > 0) { var flag_person14 = 1; } else { var flag_person14 = 0;};
if (person15.length > 0) { var flag_person15 = 1; } else { var flag_person15 = 0;};
if (person16.length > 0) { var flag_person16 = 1; } else { var flag_person16 = 0;};
if (person17.length > 0) { var flag_person17 = 1; } else { var flag_person17 = 0;};
if (person18.length > 0) { var flag_person18 = 1; } else { var flag_person18 = 0;};
if (person19.length > 0) { var flag_person19 = 1; } else { var flag_person19 = 0;};
if (person20.length > 0) { var flag_person20 = 1; } else { var flag_person20 = 0;};
if (person21.length > 0) { var flag_person21 = 1; } else { var flag_person21 = 0;};
if (person22.length > 0) { var flag_person22 = 1; } else { var flag_person22 = 0;};
if (person23.length > 0) { var flag_person23 = 1; } else { var flag_person23 = 0;};
if (person24.length > 0) { var flag_person24 = 1; } else { var flag_person24 = 0;};
if (person25.length > 0) { var flag_person25 = 1; } else { var flag_person25 = 0;};
if (person26.length > 0) { var flag_person26 = 1; } else { var flag_person26 = 0;};
if (person27.length > 0) { var flag_person27 = 1; } else { var flag_person27 = 0;};
if (person28.length > 0) { var flag_person28 = 1; } else { var flag_person28 = 0;};
if (person29.length > 0) { var flag_person29 = 1; } else { var flag_person29 = 0;};
if (person30.length > 0) { var flag_person30 = 1; } else { var flag_person30 = 0;};
var total_subjects = [flag_person1, flag_person2, flag_person3, flag_person4, flag_person5, flag_person6, flag_person7, flag_person8, flag_person9, flag_person10, flag_person11, flag_person12, flag_person13, flag_person14, flag_person15, flag_person16, flag_person17, flag_person18, flag_person19, flag_person20, flag_person21, flag_person22, flag_person23, flag_person24, flag_person25, flag_person26, flag_person27, flag_person28, flag_person29, flag_person30].reduce(add, 0);
//find sum of squares
var ss_total = var_all_groups * (count_all_groups - 1);
var ss_subject = ((Math.pow(sum_person1,2)+Math.pow(sum_person2,2)+Math.pow(sum_person3,2)+Math.pow(sum_person4,2)+Math.pow(sum_person5,2)+Math.pow(sum_person6,2)+Math.pow(sum_person7,2)+Math.pow(sum_person8,2)+Math.pow(sum_person9,2)+Math.pow(sum_person10,2)+Math.pow(sum_person11,2)+Math.pow(sum_person12,2)+Math.pow(sum_person13,2)+Math.pow(sum_person14,2)+Math.pow(sum_person15,2)+Math.pow(sum_person16,2)+Math.pow(sum_person17,2)+Math.pow(sum_person18,2)+Math.pow(sum_person19,2)+Math.pow(sum_person20,2)+Math.pow(sum_person21,2)+Math.pow(sum_person22,2)+Math.pow(sum_person23,2)+Math.pow(sum_person24,2)+Math.pow(sum_person25,2)+Math.pow(sum_person26,2)+Math.pow(sum_person27,2)+Math.pow(sum_person28,2)+Math.pow(sum_person29,2)+Math.pow(sum_person30,2)) / total_treatments) - ((Math.pow(sum_all_groups, 2)) / count_all_groups)
var ss_between = ((Math.pow(sum_group_a,2)+Math.pow(sum_group_b,2)+Math.pow(sum_group_c,2)+Math.pow(sum_group_d,2)+Math.pow(sum_group_e,2)) / total_subjects)- ((Math.pow(sum_all_groups, 2)) / count_all_groups)
var ss_residual = ss_total - ss_subject - ss_between;
//find degrees of freedom
df_total = count_all_groups - 1;
df_between = total_treatments - 1;
df_subject = total_subjects - 1;
df_residual = df_total - df_between - df_subject;
//find mean sum of squares
var ms_between = ss_between / df_between;
var ms_subject = ss_subject / df_subject;
var ms_residual = ss_residual / df_residual;
//find f value and p value
        var f = ms_between / ms_residual;
        var p = 1 - jStat.centralF.cdf(f, df_between, df_residual);
        
//--------------OUTPUT RESULTS-----------//
document.getElementById('ss_between').innerHTML = ss_between.toFixed(1);
document.getElementById('ss_subject').innerHTML = ss_subject.toFixed(1);
document.getElementById('ss_residual').innerHTML = ss_residual.toFixed(1);
document.getElementById('df_between').innerHTML = df_between.toFixed(0);
document.getElementById('df_subject').innerHTML = df_subject.toFixed(0);
document.getElementById('df_residual').innerHTML = df_residual.toFixed(0);
document.getElementById('ms_between').innerHTML = ms_between.toFixed(1);
document.getElementById('ms_subject').innerHTML = ms_subject.toFixed(1);
document.getElementById('ms_residual').innerHTML = ms_residual.toFixed(1);
document.getElementById('f').innerHTML = f.toFixed(3);
document.getElementById('p').innerHTML = p.toFixed(5);
}
</script>
<h2><span class="orange">One-Way vs. Two-Way ANOVA: When to Use Each</span></h2>
An <b>ANOVA</b>, short for “Analysis of Variance”, is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
The two most common types of ANOVAs are the one-way ANOVA and the two-way ANOVA. 
<b>One-way ANOVA:</b> Used to determine how one factor affects a response variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway1.png">
<b>Two-way ANOVA:</b> Used to determine how two factors affect a response variable, and to determine whether or not there is an interaction between the two factors on the response variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway2.png">
The following examples provide an example of how to perform each type of ANOVA.
<h3>Example: One-Way ANOVA</h3>
Suppose a professor wants to know if three different studying techniques lead to different exam scores. To test this, he recruits 30 students to participate in a study and randomly assigns each one to use one of the three techniques to prepare for an exam. At the end of one month, all of the students take the same test.
The test scores for each student are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway3.png">
The professor performs a one-way ANOVA and gets the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway4.png">
The F test statistic is <b>2.3575</b> and the corresponding p-value is <b>0.1138</b>. Since this p-value is not less than .05, we do not have sufficient evidence to say that the three studying techniques lead to different mean exam scores.
<h3>Example: Two-Way ANOVA</h3>
Suppose a botanist wants to know whether or not plant growth is influenced by sunlight exposure and watering frequency. She plants 40 seeds and lets them grow for two months under different conditions for sunlight exposure and watering frequency. After two months, she records the height of each plant. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway5.png">
The professor performs a two-way ANOVA and gets the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/oneway_twoway6.png">
Here’s how to interpret the results:
The p-value for the interaction between watering frequency and sunlight exposure was <b>0.310898</b>. This is not statistically significant at alpha level 0.05.
The p-value for watering frequency was <b>0.975975</b>. This is not statistically significant at alpha level 0.05.
The p-value for sunlight exposure was <b>0.000003</b>. This is statistically significant at alpha level 0.05.
These results indicate that sunlight exposure is the only factor that has a statistically significant effect on plant height.
And because there is no interaction effect, the effect of sunlight exposure is consistent across each level of watering frequency. That is, whether a plant is watered daily or weekly has no impact on how sunlight exposure affects a plant.
<h3>Practice: Which ANOVA Should You Use?</h3>
Use the following practice problems to gain a better understanding of when you should use a one-way or two-way ANOVA.
<b>Problem #1: Farming</b>
A farmer wants to know if three different fertilizers lead to different crop yields. To test this, he sprinkles each type of fertilizer on 10 different fields and measures the total yield at the end of the growing season.
<em>Which type of ANOVA should he use to determine if the different fertilizers lead to different crop yields?</em>
<b>Answer:</b> He should use a one-way ANOVA because there is only one factor he is studying: Fertilizer. A one-way ANOVA can tell him whether or not there is a statistically significant difference in crop yields between the three different types of fertilizer.
<b>Problem #2: Biology</b>
A biologist want to know how different levels of soil (low, medium, high) and watering frequency (weekly, monthly) impact the growth of a certain plant.
<em>Which type of ANOVA should she use to determine if the different combinations of sunlight exposure and watering frequency lead to different levels of plant growth?</em>
<b>Answer:</b> She should use a two-way ANOVA because there are two factors she is studying: Sunlight exposure and watering frequency. A two-way ANOVA can tell her whether or not different levels of each factor affect plant growth differently and whether or not there is an interaction effect between sunlight and watering frequency on plant growth.
<b>Problem #3: Medication</b>
A medical researcher want to know if four different medications lead to different mean blood pressure reductions in patients. He randomly assigns 20 patients to use each medication for one month, then measure the blood pressure reduction in each patient.
<em>Which type of ANOVA should he use to determine if the four different medications have different effects on blood pressure reduction?</em>
<b>Answer:</b> He should use a one-way ANOVA because there is only one factor he is studying: Medication type. A one-way ANOVA can tell him whether or not there is a statistically significant difference in mean blood pressure reduction between the four types of medications.
 An Introduction to the One-Way ANOVA 
 How to Perform a One-Way ANOVA in Excel 
 How to Perform a One-Way ANOVA in R 
And use these tutorials to gain a better understanding of the two-way ANOVA:
 An Introduction to the Two-Way ANOVA 
 How to Perform a Two-Way ANOVA in Excel 
 How to Perform a Two-Way ANOVA in R 
<h2><span class="orange">How to Fix: Only size-1 arrays can be converted to Python scalars</span></h2>
One error you may encounter when using Python is:
<b>TypeError: only size-1 arrays can be converted to Python scalars
</b>
This error occurs most often when you attempt to use <b>np.int()</b> to convert a NumPy array of float values to an array of integer values.
However, this function only accepts a single value instead of an array of values.
Instead, you should use <b>x.astype(int)</b> to convert a NumPy array of float values to an array of integer values because this function is able to accept an array.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we create the following NumPy array of float values:
<b>import numpy as np
#create NumPy array of float values
x = np.array([3, 4.5, 6, 7.7, 9.2, 10, 12, 14.1, 15])
</b>
Now suppose we attempt to convert this array of float values to an array of integer values:
<b>#attempt to convert array to integer values
np.int(x)
TypeError: only size-1 arrays can be converted to Python scalars 
</b>
We receive a <b>TypeError</b> because the <b>np.int()</b> function only accepts single values, not an array of values.
<h3>How to Fix the Error</h3>
In order to convert a NumPy array of float values to integer values, we can instead use the following code:
<b>#convert array of float values to integer values
x.astype(int)
array([ 3,  4,  6,  7,  9, 10, 12, 14, 15])
</b>
Notice that the array of values has been converted to integers and we don’t receive any error because the <b>astype()</b> function is able to handle an array of values.
<b>Note</b>: You can find the complete documentation for the <b>astype()</b> function  here .
<h2><span class="orange">What is an Open Ended Distribution?</span></h2>
In statistics, an <b>open ended distribution</b> is a frequency distribution in which one or more classes (or “bins”) are open-ended.
For example, the following frequency distribution represents an open ended distribution in which the smallest class is open ended:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/open1.png">
And the following frequency distribution shows an open ended distribution in which the largest class is open ended:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/open2-1.png">
Conversely, a closed ended distribution is one in which each class in the frequency distribution has an upper and lower boundary, such as the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/open3-1.png">
<h3>What Causes Open Ended Distributions?</h3>
Open ended distributions are often the result of researchers choosing to collect data in such a way that one of the classes ends up being open ended.
For example, suppose a researcher surveys residents in a certain city and asks them about their annual household income.
The researcher may choose to make the largest possible response “> $100,000” because he knows that high-income residents may not be comfortable sharing how much they make if it’s significantly more than $100,000.
Conversely, the researcher may choose to make the smallest possible response open ended because he knows that residents who earn very little will also not be comfortable sharing how little they make.
In a nutshell, researchers often include open ended classes in their surveys because they want to maximize the number of individuals who feel comfortable responding to the survey questions.
<h3>The Problem with Open Ended Distributions</h3>
The problem with open ended distributions is that true data gets  censored . In other words, we might know the number of individuals who earn over $100k in a certain city, but we don’t actually know their exact annual incomes.
It’s possible that some individuals may earn $150k, $250k, $500k, or even more but we have no idea since each of these individuals is only able to indicate that they make “>$100,000” on the survey.
Because data is censored in open ended distributions, we’re also unable to calculate the exact mean and standard deviation of the values in the dataset since we don’t have access to all of the raw data values.
<h3>How to Analyze an Open Ended Distribution</h3>
Since we can’t calculate the exact mean of an open ended distribution, we often use the <b>median</b> as a measure of the “center” of the dataset.
Recall that the median represents the middle value of the dataset.
When working with open ended distributions, we can use the following formula to find the best estimate of the median:
<b>Best Estimate of Median:</b> L + ( (n/2 – F) / f ) * w
where:
<b>L:</b> The lower limit of the median group
<b>n:</b> The total number of observations
<b>F:</b> The cumulative frequency up to the median group
<b>f:</b> The frequency of the median group
<b>w:</b> The width of the median group
For example, suppose we have the following open ended distribution from earlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/open2-1.png">
There are a total of 72 values in the dataset. Thus, we know the <b>median value</b> will be located between the value of the 36th and 37th largest value in the dataset. Each of these values is located within the class “$60,000 – $79,999” so we know that the median income lies within this range.
Our best estimate of the median would be:
Median: 60,000 + ( (72/2 – 25) / 19 ) * 19,999 = <b>$71,578</b>
This value represents our best estimate of the median annual income for individuals in this dataset.
<h2><span class="orange">How to Fix: ValueError: operands could not be broadcast together with shapes</span></h2>
One error you may encounter when using Python is:
<b>ValueError: operands could not be broadcast together with shapes (2,2) (2,3) 
</b>
This error occurs when you attempt to perform matrix multiplication using a multiplication sign (<b>*</b>) in Python instead of the <b>numpy.dot()</b> function.
The following examples shows how to fix this error in each scenario.
<h3>How to Reproduce the Error</h3>
Suppose we have a 2×2 matrix C, which has 2 rows and 2 columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/np1.png">
Suppose we also have a 2×3 matrix D, which has 2 rows and 3 columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/np2.png">
Here is how to multiply matrix C by matrix D:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/np3.png">
This results in the following matrix:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/np4.png">
Suppose we attempt to perform this matrix multiplication in Python using a multiplication sign (*) as follows:
<b>import numpy as np
#define matrices
C = np.array([7, 5, 6, 3]).reshape(2, 2)
D = np.array([2, 1, 4, 5, 1, 2]).reshape(2, 3)
#print matrices
print(C)
[[7 5]
 [6 3]]
print(D)
[[2 1 4]
 [5 1 2]]
#attempt to multiply two matrices together
C*D
ValueError: operands could not be broadcast together with shapes (2,2) (2,3)  
</b>
We receive a <b>ValueError</b>. We can refer to the  NumPy documentation  to understand why we received this error:
When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when
 
they are equal, or
one of them is 1
If these conditions are not met, a <b>ValueError: operands could not be broadcast together</b> exception is thrown, indicating that the arrays have incompatible shapes.
Since our two matrices do not have the same value for their trailing dimensions (matrix C has a trailing dimension of 2 and matrix D has a trailing dimension of 3), we receive an error.
<h3>How to Fix the Error</h3>
The easiest way to fix this error is to simply using the <b>numpy.dot()</b> function to perform the matrix multiplication:
<b>import numpy as np
#define matrices
C = np.array([7, 5, 6, 3]).reshape(2, 2)
D = np.array([2, 1, 4, 5, 1, 2]).reshape(2, 3)
#perform matrix multiplication
C.dot(D)
array([[39, 12, 38],
       [27,  9, 30]])
</b>
Notice that we avoid a <b>ValueError</b> and we’re able to successfully multiply the two matrices.
Also note that the results match the results that we calculated by hand earlier.
<h2><span class="orange">How to Use optim Function in R (2 Examples)</span></h2>
You can use the <b>optim</b> function in R for general-purpose optimizations.
This function uses the following basic syntax:
<b>optim(par, fn, data, ...)
</b>
where:
<b>par</b>: Initial values for the parameters to be optimized over
<b>fn</b>: A function to be minimized or maximized
<b>data</b>: The name of the object in R that contains the data
The following examples show how to use this function in the following scenarios:
<b>1.</b> Find coefficients for a linear regression model.
<b>2.</b> Find coefficients for a quadratic regression model.
Let’s jump in!
<h3>Example 1: Find Coefficients for Linear Regression Model</h3>
The following code shows how to use the <b>optim()</b> function to find the coefficients for a linear regression model by minimizing the  residual sum of squares :
<b>#create data frame
df &lt;- data.frame(x=c(1, 3, 3, 5, 6, 7, 9, 12), y=c(4, 5, 8, 6, 9, 10, 13, 17))
#define function to minimize residual sum of squares
min_residuals &lt;- function(data, par) {   with(data, sum((par[1] + par[2] * x - y)^2))
}
#find coefficients of linear regression model
optim(par=c(0, 1), fn=min_residuals, data=df)
$par
[1] 2.318592 1.162012
$value
[1] 11.15084
$counts
function gradient 
      79       NA 
$convergence
[1] 0
$message
NULL
</b>
Using the values returned under <b>$par</b>, we can write the following fitted linear regression model:
y = 2.318 + 1.162x
We can verify this is correct by using the built-in <b>lm()</b> function in R to calculate the regression coefficients:
<b>#find coefficients of linear regression model using lm() function
lm(y ~ x, data=df)
Call:
lm(formula = y ~ x, data = df)
Coefficients:
(Intercept)            x  
      2.318        1.162
</b>
These coefficient values match the ones we calculated using the <b>optim()</b> function.
<h3>Example 2: Find Coefficients for Quadratic Regression Model</h3>
The following code shows how to use the <b>optim()</b> function to find the coefficients for a  quadratic regression model  by minimizing the residual sum of squares:
<b>#create data frame
df &lt;- data.frame(x=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60), y=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))
#define function to minimize residual sum of squares
min_residuals &lt;- function(data, par) {   with(data, sum((par[1] + par[2]*x + par[3]*x^2 - y)^2))
}
#find coefficients of quadratic regression model
optim(par=c(0, 0, 0), fn=min_residuals, data=df)
$par
[1] -18.261320   6.744531  -0.101201
$value
[1] 309.3412
$counts
function gradient 
     218       NA 
$convergence
[1] 0
$message
NULL</b>
Using the values returned under <b>$par</b>, we can write the following fitted quadratic regression model:
y = -18.261 + 6.744x – 0.101x<sup>2</sup>
We can verify this is correct by using the built-in <b>lm()</b> function in R:
<b>#create data frame
df &lt;- data.frame(x=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60), y=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))
#create a new variable for x^2
df$x2 &lt;- df$x^2
#fit quadratic regression model
quadraticModel &lt;- lm(y ~ x + x2, data=df)
#display coefficients of quadratic regression model
summary(quadraticModel)$coef
               Estimate  Std. Error    t value     Pr(>|t|)
(Intercept) -18.2536400 6.185069026  -2.951243 1.839072e-02
x             6.7443581 0.485515334  13.891133 6.978849e-07
x2           -0.1011996 0.007460089 -13.565470 8.378822e-07</b>
These coefficient values match the ones we calculated using the <b>optim()</b> function.
<h2><span class="orange">How to Use “OR” Operator in Pandas (With Examples)</span></h2>
You can use the <b>|</b> symbol as an “OR” operator in pandas.
For example, you can use the following basic syntax to filter for rows in a pandas DataFrame that satisfy condition 1 <b>or</b> condition 2:
<b>df[(condition1) | (condition2)]
</b>
The following examples show how to use this “OR” operator in different scenarios.
<h3>Example 1: Use “OR” Operator to Filter Rows Based on Numeric Values in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
        teampointsassistsrebounds
0A25511
1A1278
2B15710
3B1496
4B19126
5B2395
6C2599
7C29412</b>
We can use the following syntax to filter for rows in the DataFrame where the value in the points column is greater than 20 <b>or</b> the value in the assists column is equal to 9:
<b>#filter rows where points > 20 or assists = 9
df[(df.points > 20) | (df.assists == 9)]
        teampointsassistsrebounds
0A25511
3B1496
5B2395
6C2599
7C29412</b>
The only rows returned are the ones where the points value is greater than 20 <b>or</b> the assists value is equal to 9.
<h3>Example 2: Use “OR” Operator to Filter Rows Based on String Values in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'position': ['G', 'G', 'F', 'F', 'C', 'F', 'C', 'C'],   'conference': ['W', 'W', 'W', 'W', 'E', 'E', 'E', 'E'],   'points': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team position conference  points
0    A        G          W      11
1    B        G          W       8
2    C        F          W      10
3    D        F          W       6
4    E        C          E       6
5    F        F          E       5
6    G        C          E       9
7    H        C          E      12</b>
We can use the following syntax to filter for rows in the DataFrame where the value in the position column is equal to G <b>or</b> the value in the position column is equal to F <b>or</b> the value in the team column is equal to H:
<b>#filter rows based on string values
df[(df.team == 'H') | (df.position == 'G') | (df.position == 'F')]
     team position conference points
0A G    W  11
1B G    W   8
2C F    W  10
3D F    W   6
5F F    E   5
7H C    E  12</b>
The only rows returned are the ones that meet at least one of the three conditions that we specified.
<h2><span class="orange">How to Use “OR” Operator in R (With Examples)</span></h2>
You can use the <b>|</b> symbol as an “OR” operator in R.
For example, you can use the following basic syntax to filter for rows in a data frame in R that satisfy condition 1 <b>or</b> condition 2:
<b>df[(condition1) | (condition2), ]
</b>
The following examples show how to use this “OR” operator in different scenarios.
<h2>Example 1: Use “OR” Operator to Filter Rows Based on Numeric Values in R</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'), points=c(25, 12, 15, 14, 19, 23, 25, 29), assists=c(5, 7, 7, 9, 12, 9, 9, 4), rebounds=c(11, 8, 10, 6, 6, 5, 9, 12))
#view data frame
df
  team points assists rebounds
1    A     25       5       11
2    A     12       7        8
3    B     15       7       10
4    B     14       9        6
5    B     19      12        6
6    B     23       9        5
7    C     25       9        9
8    C     29       4       12</b>
We can use the following syntax to filter for rows in the data frame where the value in the points column is greater than 20 <b>or</b> the value in the assists column is equal to 9:
<b>#filter rows where points > 20 or assists = 9
df[(df$points > 20) | (df$assists == 9), ]
  team points assists rebounds
1    A     25       5       11
4    B     14       9        6
6    B     23       9        5
7    C     25       9        9
8    C     29       4       12</b>
The only rows returned are the ones where the points value is greater than 20 <b>or</b> the assists value is equal to 9.
<h2>Example 2: Use “OR” Operator to Filter Rows Based on String Values in R</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'), position=c('G', 'G', 'F', 'F', 'C', 'F', 'C', 'C'), conference=c('W', 'W', 'W', 'W', 'E', 'E', 'E', 'E'), points=c(11, 8, 10, 6, 6, 5, 9, 12))
#view data frame
df
  team position conference points
1    A        G          W     11
2    B        G          W      8
3    C        F          W     10
4    D        F          W      6
5    E        C          E      6
6    F        F          E      5
7    G        C          E      9
8    H        C          E     12
</b>
We can use the following syntax to filter for rows in the data frame where the value in the position column is equal to G <b>or</b> the value in the position column is equal to F <b>or</b> the value in the team column is equal to H:
<b>#filter rows based on string values
df[(df$team == 'H') | (df$position == 'G') | (df$position == 'F'), ]
  team position conference points
1    A        G          W     11
2    B        G          W      8
3    C        F          W     10
4    D        F          W      6
6    F        F          E      5
8    H        C          E     12</b>
The only rows returned are the ones that meet at least one of the three conditions that we specified.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common operators in R:
 How to Use Dollar Sign ($) Operator in R 
 How to Use the Tilde Operator (~) in R 
 How to Use “NOT IN” Operator in R 
<h2><span class="orange">How to Order the Bars in a ggplot2 Bar Chart</span></h2>
By default,  ggplot2  orders the bars in a bar chart using the following orders:
Factor variables are ordered by factor levels.
Character variables are order in alphabetical order.
However, often you may be interested in ordering the bars in some other specific order.
This tutorial explains how to do so using the following data frame:
<b>#create data frame
df &lt;- data.frame(team = c('B', 'B', 'B', 'A', 'A', 'C'), points = c(12, 28, 19, 22, 32, 45), rebounds = c(5, 7, 7, 12, 11, 4))
#view structure of data frame
str(df)
'data.frame':6 obs. of  3 variables:
 $ team    : Factor w/ 3 levels "A","B","C": 2 2 2 1 1 3
 $ points  : num  12 28 19 22 32 45
 $ rebounds: num  5 7 7 12 11 4</b>
<h2>Example 1: Order the Bars Based on a Specific Factor Order</h2>
If we attempt to create a bar chart to display the frequency by team, the bars will automatically appear in alphabetical order:
<b>library(ggplot2)
ggplot(df, aes(x=team)) +
  geom_bar()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/orderBarR1.png">
The following code shows how to order the bars by a specific order:
<b>#specify factor level order
df$team = factor(df$team, levels = c('C', 'A', 'B'))
#create bar chart again 
ggplot(df, aes(x=team)) +
  geom_bar()</b>
<h2>Example 2: Order the Bars Based on Numerical Value</h2>
We can also order the bars based on numerical values. For example, the following code shows how to order the bars from largest to smallest frequency using the <b>reorder()</b> function:
<b>library(ggplot2)
ggplot(df, aes(x=reorder(team, team, function(x)-length(x)))) +
  geom_bar()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/orderBarR2.png">
We can also order the bars from smallest to largest frequency by taking out the minus sign in the <b>function() </b>call within <b>reorder()</b> function:
<b>library(ggplot2)
ggplot(df, aes(x=reorder(team, team, function(x) length(x)))) +
  geom_bar()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/orderBarR3.png">
<h2>Additional Resources</h2>
 Documentation  for the <b>geom_bar()</b> function.
 Documentation  for the <b>reorder()</b> function.
 A complete list  of R tutorials on Statology.
<h2><span class="orange">What Are Order Effects? (Explanation & Examples)</span></h2>
Often in experimental studies, researchers will have participants provide responses to several different treatments.
In these types of studies, <b>order effects </b>refer to differences in participant responses as a result of the order in which treatments are presented to them.
For example, suppose researchers measure the percentage of free throws made by basketball players in which each player is instructed to shoot 10 free throws using a certain technique – <b>A, B, or C</b> – when shooting.
Here are the various orders in which players could shoot the free throws:
ABC
ACB
BCA
BAC
CAB
CBA
Regardless of whether or not one technique is better, it’s likely that players will become slightly tired or fatigued by time they use the last technique which means they’ll likely perform worst using that technique.
This is an example of an <b>order effect</b>. The order in which players try each technique has an effect on the percentage of free throws they make.
<h3>Types of Order Effects</h3>
There are several different types of order effects that can occur in experiments, including:
<b>Practice Effects: </b>Participants may become better at a certain task as they become more familiar with the testing environment. For example, participants may become faster in the latter trials of experiments that measure response time simply because they’ve had practice in previous trials.
<b>Fatigue Effects: </b>Participants may perform worse near the end of an experiment simply because they’ve become fatigued from performing some task over and over again.
<b>Boredom Effects:</b> Participants may perform worse near the end of an experiment simply because they get bored if a task is overly repetitive or long. 
<b> Carryover Effects : </b>Participants may respond to treatments differently depending on the treatment they were exposed to previously. For example, in experiments in which participants have to estimate the weight of objects they are likely to be influenced by how heavy the previous object was that they esimated.
In any experiment involving repeated measurements of the same individuals, one or more of these order effects may appear which could skew the results.
<h3>How to Prevent Order Effects</h3>
Depending on the type of order effect expected to occur, researchers can take the following steps to prevent them:
<b>Practice Effects: </b>To prevent practice effects, researchers could give each participant some time to warm up with the task to prevent them from getting better at the task <em>during </em>the experiment.
<b>Fatigue Effects: </b>To prevent fatigue effects, researchers could make a task shorter and/or less intense to perform.
<b>Boredom Effects:</b> To prevent boredom effects, researchers could make a task shorter or add more variation to prevent boredom. 
<b>Carryover Effects: </b>To prevent carryover effects, researchers could add more time in between tasks so participants aren’t influenced by their previous trial.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/order1.png">
<h3>On Using Counterbalancing</h3>
In any experiment involving repeated measurements of the same individuals, researchers can implement <b>counterbalancing</b> – a technique in which every possible order of treatments is used the same number of times. 
For example, in the previous basketball experiment we could have 5 players shoot free throws using the order of ABC, another 5 players shoot using the order of ACB, another 5 players shoot using the order of BCA, and so on.
By using each order the same number of times, we can “counterbalance” any order effects. The downside of this method is that it can be too time-consuming or costly to actually implement every order an equal number of times. 
For example, if we have three different treatment conditions then the total number of unique orders would be 3! = <b>6</b>. If we have four treatment conditions, this number jumps up to 4! = <b>24</b>. If we have five treatment conditions, it becomes 5! = <b>120</b>. This number can quickly become unreasonable to implement in a study.
<h2><span class="orange">Orthogonal Vector Calculator</span></h2>
Given vector a = [a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>] and vector b = [b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>], we can say that the two vectors are <b>orthogonal</b> if their dot product is equal to zero. The dot product of vector a and vector b, denoted as <b>a · b</b>, is given by:
<b>a · b</b> = a<sub>1</sub> * b<sub>1</sub> + a<sub>2</sub> * b<sub>2</sub> + a<sub>3</sub> * b<sub>3</sub>
To find out if two vectors are orthogonal, simply enter their coordinates in the boxes below and then click the “Check orthogonality” button.
<label for="equation"><b>Vector a</b></label>
<label for="ax"><b>x</b></label>
<input type="number" id="ax" min="0" value="2"><label for="ay"><b>y</b></label>
<input type="number" id="ay" min="0" value="5"><label for="az"><b>z</b></label>
<input type="number" id="az" min="0" value="8">
<label for="equation"><b>Vector b</b></label>
<label for="bx"><b>x</b></label>
<input type="number" id="bx" value="1"><label for="by"><b>y</b></label>
<input type="number" id="by" value="2"><label for="bz"><b>z</b></label>
<input type="number" id="bz" value="4">
<input type="button" id="button_calc" onclick="calc()" value="Check orthogonality">
Dot product (a · b): <b>44.00</b>
44.00
<script>
function calc() {
//get input values
var ax = document.getElementById('ax').value*1;
var ay = document.getElementById('ay').value*1;
var az = document.getElementById('az').value*1;
var bx = document.getElementById('bx').value*1;
var by = document.getElementById('by').value*1;
var bz = document.getElementById('bz').value*1;
var arr1 = [ax, ay, az];
var arr2 = [bx, by, bz];
var answer = 0;
for(var i=0; i< arr1.length; i++) {
    answer += arr1[i]*arr2[i];
}
var yes_no = "are not"
if (answer == 0) {
   yes_no = "are"
}
//output
document.getElementById('answer').innerHTML = answer.toFixed(2);
document.getElementById('explanation').innerHTML = "Vectors a and b <b>" + yes_no + "</b> orthogonal since their dot product equals " + answer.toFixed(2);
}
</script>
<h2><span class="orange">Outcome vs. Event: What’s the Difference?</span></h2>
Two terms that students often confuse in statistics are <b>outcome</b> and <b>event</b>.
Here’s the subtle difference between the two terms:
<b>Outcome:</b> The result of a random experiment.
For example, there are six potential outcomes when rolling a die: 1, 2, 3, 4, 5, or 6.
<b>Event:</b> A set of outcomes that has a probability assigned to it.
For example, one possible “event” could be rolling an even number. The probability that this event occurs is 1/2.
The following examples show more scenarios that illustrate the difference between outcomes and events.
<h3>Example 1: Deck of Cards</h3>
Suppose we randomly draw a card from a standard deck of 52 cards.
The four possible <b>outcomes</b> for the suit of the card include:
Heart
Spade
Diamond
Club
One of these four outcomes must occur.
However, there are many different <b>events</b> that we may be interested in assigning a probability to. For example:
<b>Event 1: Draw a Heart</b>
The probability that this event occurs is 13/52 or 1/4.
<b>Event 2: Draw a Heart or a Spade</b>
The probability that this event occurs is 26/52 or 1/2.
<b>Event 3: Draw a card that is <em>not</em> a Heart</b>
The probability that this event occurs is 39/52 or 3/4.
There are many more events that we could come up with and assign a probability to, but these are just three simple ones.
<h3>Example 2: Pulling Marbles from a Bag</h3>
Suppose a bag has 3 red marbles, 5 green marbles, and 2 blue marbles.
If we close our eyes and randomly select one marble from the bag, the three possible <b>outcomes</b> for the color of the marble include:
Red
Green
Blue
One of these four outcomes must occur.
However, there are many different <b>events</b> that we may be interested in assigning a probability to. For example:
<b>Event 1: Draw a Blue Marble</b>
The probability that this event occurs is 2/10 or 1/5.
<b>Event 2: Draw a Blue or Green Marble</b>
The probability that this event occurs is 7/10.
<b>Event 3: Draw a Marble that is <em>not</em> Blue</b>
The probability that this event occurs is 8/10 or 4/5.
These are three events that we can easily calculate probabilities for.
<h2><span class="orange">How to Do an Outer Join in R (With Examples)</span></h2>
There are two common ways to perform an outer join in R:
<b>Method 1: Use Base R</b>
<b>merge(df1, df2, by='column_to_join_on', all=TRUE)
</b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
full_join(df1, df2, by='column_to_join_on')</b>
Each method will return all rows from both tables.
Both methods will produce the same result, but the <b>dplyr</b> method will tend to work faster on extremely large datasets.
The following examples show how to use each of these functions in practice with the following data frames:
<b>#define first data frame
df1 = data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'), points=c(18, 22, 19, 14, 14, 11, 20, 28))
df1
  team points
1    A     18
2    B     22
3    C     19
4    D     14
5    E     14
6    F     11
7    G     20
8    H     28
#define second data frame
df2 = data.frame(team=c('A', 'B', 'C', 'D', 'L', 'M'), assists=c(4, 9, 14, 13, 10, 8))
df2
  team assists
1    A       4
2    B       9
3    C      14
4    D      13
5    L      10
6    M       8</b>
<h3>Example 1: Outer Join Using Base R</h3>
We can use the <b>merge()</b> function in base R to perform an outer join, using the ‘team’ column as the column to join on:
<b>#perform outer join using base R
df3 &lt;- merge(df1, df2, by='team', all=TRUE)
#view result
df3
   team points assists
1     A     18       4
2     B     22       9
3     C     19      14
4     D     14      13
5     E     14      NA
6     F     11      NA
7     G     20      NA
8     H     28      NA
9     L     NA      10
10    M     NA       8
</b>
Notice that all of the rows from both data frames are returned.
<h3>Example 2: Outer Join Using dplyr</h3>
We can use the <b>full_join()</b> function from the  dplyr  package to perform an outer join, using the ‘team’ column as the column to join on:
<b>library(dplyr)
#perform outer join using dplyr 
df3 &lt;- full_join(df1, df2, by='team')
#view result
df3
   team points assists
1     A     18       4
2     B     22       9
3     C     19      14
4     D     14      13
5     E     14      NA
6     F     11      NA
7     G     20      NA
8     H     28      NA
9     L     NA      10
10    M     NA       8</b>
Notice that this matches the result we obtained from using the <b>merge()</b> function in base R.
<h2><span class="orange">Outlier Boundary Calculator</span></h2>
The interquartile range, often abbreviated IQR, is the difference between the 25th percentile (Q1) and the 75th percentile (Q3) in a dataset.
We often declare an observation to be an outlier in a dataset if it has a value 1.5 times greater than the IQR or 1.5 times less than the IQR.
This calculator uses this formula to automatically calculate the upper and lower outlier boundaries for a given dataset.
Simply enter the list of the comma-separated values for the dataset, then click the “Calculate” button:
<b>Dataset values:</b>
<textarea id="x" rows="5" cols="40">1, 3, 3, 4, 8, 11, 13, 14, 15, 17, 22, 24, 26, 46</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Q1: 5.0000</b>
<b>Q3: 20.7500</b>
<b>Interquartile Range: 15.7500</b>
<b>Lower Outlier Boundary: -18.6250</b>
<b>Upper Outlier Boundary: 44.3750</b>
<script>
function calc() {
var x = document.getElementById('x').value.split(',').map(Number);
var Q1 = jStat.percentile(x, 0.25);
var Q3 = jStat.percentile(x, 0.75);
var IQR = Q3-Q1;
var lower = Q1 - 1.5*IQR;
var upper = Q3-(-1.5*IQR);
document.getElementById('Q1').innerHTML = Q1.toFixed(4);
document.getElementById('Q3').innerHTML = Q3.toFixed(4);
document.getElementById('IQR').innerHTML = IQR.toFixed(4);
document.getElementById('lower').innerHTML = lower.toFixed(4);
document.getElementById('upper').innerHTML = upper.toFixed(4);
  
} //end calc function
</script>
<h2><span class="orange">Outlier Calculator</span></h2>
td, tr, th {
    border: 1px solid black;
}
table {
    border-collapse: collapse;
}
td, th {
    min-width: 50px;
    height: 21px;
}
    .label_radio {
text-align: center;
}
#text_area_input {
padding-left: 35%;
float: left;
}
svg:not(:root) {
  overflow: visible;
}
</style>
An <b>outlier</b> is defined as any observation in a dataset that is 1.5 IQRs greater than the third quartile or 1.5 IQRs less than the first quartile, where IQR stands for “interquartile range” and is the difference between the first and third quartile.
To identify outliers for a given dataset, enter your comma separated data in the box below, then click the “Identify Outliers” button:
<textarea id="input_data" name="x" rows="5" cols="40"></textarea>
<input type="button" id="button" onclick="calc()" value="Identify Outliers">
<b>Outliers:</b> 
Minimum: 
First quartile: 
Median: 
Third quartile: 
Maximum: 
<script>
//function to get weird object keys from object that holds boxplot
function resolve(path, obj=self, separator='.') {
    var properties = Array.isArray(path) ? path : path.split(separator)
    return properties.reduce((prev, curr) => prev && prev[curr], obj)
}
function calc() {
var input_data = document.getElementById('input_data').value.match(/\d+/g).map(Number);
var data = [];
for (var i=0; i < input_data.length; i++) {
    data.push({
        Q1: input_data[i]
    });
}
var box = (values) => {
  var sorted = values.slice();
  sorted.sort((a, b) => a - b);
  var max = sorted[sorted.length - 1];
  var min = sorted[0];
  //var upper = d3.quantile(sorted, 0.75);
  //var mid = d3.quantile(sorted, 0.5);
  //var lower = d3.quantile(sorted, 0.25);
// find the median of the sample
    var mid = math.median(sorted);
    // split the data by the median
    var _firstHalf = sorted.filter(function(f){ return f < mid})
    var _secondHalf = sorted.filter(function(f){ return f > mid})
    // find the medians for each split, calculate IQR
    var lower = math.median(_firstHalf);
    var upper = math.median(_secondHalf);
  var lowerLimit = lower - ((upper - lower) * 1.5);
  var lowerWhisker = sorted.find(function(d) { return d > lowerLimit; });
  var upperLimit = upper - (-1 * ((upper - lower) * 1.5));
  var upperWhisker = sorted.reverse().find(function(d) { return d < upperLimit; });
  return {
    upper: upper, mid: mid, lower: lower, lowerWhisker: lowerWhisker,
    upperWhisker: upperWhisker, max: max, min:min,
    outliers: values.filter(function(d) { return d > upperWhisker || d < lowerWhisker; })
  }
}
var flatten = function(arrays) {
  return arrays.reduce(function(a, b) {
    return a.concat(b);
  }, [])
}
  var quarters = Object.keys(data[0]);
  var series = quarters.map(function(q) {
    return {
      quarter: q,
      data: box(data.map(function(d) { return Number(d[q]); }))
    };
  })
  var yFormat = d3.format(',.0f');
  var yExtent = fc.extentLinear()
    .accessors([function(d) { return d.max; }, function(d) { return d.min; }])
    .pad([0, 0.1])
    .include([0])
  var boxplot = fc.seriesSvgBoxPlot()
      .crossValue(function(d) { return d.quarter; })
      .medianValue(function(d) { return d.data.mid; })
      .barWidth(50)
      .upperQuartileValue(function(d) { return d.data.upper; })
      .lowerQuartileValue(function(d) { return d.data.lower; })
      .highValue(function(d) { return d.data.upperWhisker; })
      .lowValue(function(d) { return d.data.lowerWhisker; });
  var point = fc.seriesSvgPoint()
      .crossValue(function(d) { return d[0]; })
      .mainValue(function(d) { return d[1]; });
  var label = fc.seriesSvgPoint()
      .crossValue(function(d) { return d[0]; })
      .mainValue(function(d) { return d[1]; })
      .decorate(function(selection) {
        selection.enter()
            .select('path')
            .attr('display', 'none')
        selection.enter()
            .append('text')
            .style('text-anchor', 'middle')
            .attr('transform', function(d, i) { return  'translate(' + (i % 2 === 0 ? -50 : 50) + ', 5)'; })
            .text(function(d) { return yFormat(d[1]); })
            .attr('stroke', 'transparent')
            .attr('fill', 'black');
      });
  var multi = fc.seriesSvgMulti()
      .series([boxplot, point, label])
      .mapping((data, index, series) => {
        switch(series[index]) {
          case point:
            return flatten(data.map(function(s) {
              return s.data.outliers.map(function(o) { return [s.quarter, o]; })
            }))
          case boxplot:
            return data;
        }
      });
  var chart = fc.chartSvgCartesian(
        d3.scalePoint(),
        d3.scaleLinear()
      )
      .xDomain(quarters)
      .xPadding(0.1)
      .yDomain(yExtent(series.map(function(d) { return d.data; })))
      .yTickFormat(yFormat)
      .plotArea(multi);
      
      //output results
      var min = resolve('0->data->min', series, '->');
      var quartile1 = resolve('0->data->lower', series, '->');
      var mid = resolve('0->data->mid', series, '->');
      var quartile3 = resolve('0->data->upper', series, '->');
      var max = resolve('0->data->max', series, '->');
      var outliers = resolve('0->data->outliers', series, '->');console.log(outliers);
      
      if (outliers.length == 0) {
      document.getElementById('outliers').innerHTML = "<b>none</b>";
      } else {
      document.getElementById('outliers').innerHTML = "<b>" + outliers + "</b>";
      }
      document.getElementById('min').innerHTML = min;
      document.getElementById('quartile1').innerHTML = quartile1;
      document.getElementById('mid').innerHTML = mid;
      document.getElementById('quartile3').innerHTML = quartile3;
      document.getElementById('max').innerHTML = max;
  
} //end calc function
</script>
<h2><span class="orange">How to Easily Find Outliers in Excel</span></h2>
An <b>outlier </b>is an observation that lies abnormally far away from other values in a dataset. Outliers can be problematic because they can effect the results of an analysis.
We will use the following dataset in Excel to illustrate two methods for finding outliers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel1.png">
<b>Related: </b> How to Calculate Average Excluding Outliers in Excel 
<h3>Method 1: Use the interquartile range</h3>
The  interquartile range  (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) in a dataset. It measures the spread of the middle 50% of values.
We can define an observation to be an outlier if it is 1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).
The following image shows how to calculate the interquartile range in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel2.png">
Next, we can use the formula mentioned above to assign a “1” to any value that is an outlier in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel3.png">
We see that only one value – <b>164 </b>– turns out to be an outlier in this dataset.
<h3>Method 2: Use z-scores</h3>
A  z-score  tells you how many standard deviations a given value is from the mean. We use the following formula to calculate a z-score:
<b>z</b> = (X – μ) / σ
where:
X is a single raw data value
μ is the population mean
σ is the population standard deviation
We can define an observation to be an outlier if it has a z-score less than -3 or greater than 3.
The following image shows how to calculate the mean and standard deviation for a dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel4.png">
We can then use the mean and standard deviation to find the z-score for each individual value in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel5.png">
We can then assign a “1” to any value that has a z-score less than -3 or greater than 3:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/outlierExcel6.png">
Using this method, we see that there are no outliers in the dataset.
<b>Note: </b>Sometimes a z-score of 2.5 is used instead of 3. In this case, the individual value of <b>164 </b>would be considered an outlier since it has a z-score greater than 2.5. When using the z-score method, use your best judgement for which z-score value you consider to be an outlier.
<h3>How to Handle Outliers</h3>
If an outlier is present in your data, you have a few options:
<b>1. Make sure the outlier is not the result of a data entry error.</b>
Sometimes an individual simply enters the wrong data value when recording data. If an outlier is present, first verify that the value was entered correctly and that it wasn’t an error.
<b>2. Remove the outlier.</b>
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<b>3. Assign a new value to the outlier</b>.
If the outlier is the result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
<h2><span class="orange">How to Easily Find Outliers in Google Sheets</span></h2>
An <b>outlier </b>is an observation that lies abnormally far away from other values in a dataset.
We often define an observation to be an outlier if it is 1.5 times the interquartile range greater than the third quartile or 1.5 times the interquartile range less than the first quartile.
<b>Note:</b> The interquartile range is the difference between the third quartile (75th percentile) and the first quartile (25th percentile) in a dataset. It measures the spread of the middle 50% of values.
The following step-by-step example shows how to use this formula to find outliers in a dataset in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for the following dataset into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/outlierSheets1.png">
<h3>Step 2: Calculate the Interquartile Range</h3>
Next, let’s calculate the first quartile, third quartile, and interquartile range of the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/outlierSheets2.png">
<h3>Step 3: Identify Outliers</h3>
Next, we can use the following formula to assign a “1” to any value that is an outlier in the dataset:
<b>=IF(A2&lt;$B$18-$B$20*1.5, 1, IF(A2>$B$19+$B$20*1.5, 1, 0))
</b>
This formula checks to see if an observation is 1.5 times the interquartile range greater than the third quartile or 1.5 times the interquartile range less than the first quartile.
If either is true, the observation is assigned a “1” to designate it as an outlier.
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/outlierSheets3.png">
We see that only one value in our dataset turns out to be an outlier: <b>164</b>.
<h3>How to Deal With Outliers</h3>
If an outlier is present in your data, you have a few options:
<b>1. Make sure the outlier is not the result of a data entry error.</b>
Sometimes data simple gets recorded incorrectly. If an outlier is present, first check that the value was entered correctly and that it wasn’t an error.
<b>2. Assign a new value to the outlier</b>.
If the outlier is the result of a data entry error, you may decide to assign a new value to it such as the mean or the median of the dataset.
<b>3. Remove the outlier.</b>
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report that you removed an outlier.
<h2><span class="orange">5 Examples of Outliers in Real Life</span></h2>
An <b>outlier</b> is a data point that lies abnormally far away from other values in a dataset.
We often define a data point to be an outlier if it is 1.5 times the interquartile range greater than the third quartile or 1.5 times the interquartile range less than the first quartile of a dataset.
<b>Note</b>: The interquartile range is the difference between the third quartile (75th percentile) and the first quartile (25th percentile) in a dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/iqrOutlier1.png">
The following scenarios share examples of outliers in real life situations.
<h3>Example 1: Outliers in Income</h3>
One real-world scenario where outliers often appear is income distribution.
For example, the 25th percentile (Q1) of annual income in a certain country may be $15,000 per year and the 75th percentile (Q3) may be $120,000 per year.
The interquartile range (IQR) would be calculated as $120,000 – $15,000 = $105,000.
This means that anyone with an income outside of the following boundaries would be considered an outlier:
<b>Lower Boundary</b>: Q1 – 1.5*IQR = $15,000 – 1.5*$105,000 = -$142,500
<b>Upper Boundary</b>: Q3 + 1.5*IQR = $120,000 + 1.5*$105,000 = $277,500
Someone like Elon Musk who has a net worth in the billions of dollars would be considered an outlier in terms of annual income.
<em><b>Note</b>: The value for outliers beyond the lower boundary will not always make sense, e.g. it’s not possible to earn a negative annual income.</em>
<h3>Example 2: Outliers in Breath-Holding</h3>
Another real-world scenario where outliers often appear is breath-holding.
For example, the 25th percentile (Q1) for how long individuals can hold their breath is around 15 seconds while the 75th percentile (Q3) is around 75 seconds.
The interquartile range (IQR) would be calculated as 75 – 15 = 60.
This means that anyone who is able to hold their breath outside of the following boundaries would be considered an outlier:
<b>Lower Boundary</b>: Q1 – 1.5*IQR = 15 – 1.5*60 = -75 seconds
<b>Upper Boundary</b>: Q3 + 1.5*IQR = 75 + 1.5*60 = 165 seconds
Any  freedivers  who can hold their breath for 10 minutes or longer would be considered outliers because they can hold their breath much longer than 165 seconds.
<h3>Example 3: Outliers in Animal Height</h3>
Another real-world scenario where outliers often appear is height of animals.
For example, the 25th percentile (Q1) of horse height is around 5 feet and the 75th percentile (Q3) is around 5.5 feet.
The interquartile range (IQR) would be calculated as 5.5 – 5 = 0.5 feet.
This means that any horse with a height outside of the following boundaries would be considered an outlier:
<b>Lower Boundary</b>: Q1 – 1.5*IQR = 5 – 1.5*0.5 = 4.25 feet
<b>Upper Boundary</b>: Q3 + 1.5*IQR = 5 + 1.5*0.5 = 5.75 feet
According to the  Guinness World Records , the record for tallest horse ever is just above 7 feet. Since this is above the upper boundary of 5.75 feet, this horse would clearly be considered an outlier.
<h3>Example 4: Outliers in Movie Ticket Sales</h3>
Another real-world scenario where outliers often appear is movie ticket sales.
For example, the 25th percentile (Q1) of gross ticket sales for movies is around $2 million and the 75th percentile (Q3) is around $15 million.
The interquartile range (IQR) would be calculated as $15 million – $2 million= $13 million.
This means that any movie with gross sales outside of the following boundaries would be considered an outlier:
<b>Lower Boundary</b>: Q1 – 1.5*IQR = $2 million – 1.5*$13 million = -$17.5 million
<b>Upper Boundary</b>: Q3 + 1.5*IQR = $15 million + 1.5*$13 million = $34.5 million
Most Star Wars movies have grossed far more than $34.5 million, which makes them outliers in terms of ticket sales.
<h3>Example 5: Outliers in Points Scored per Game</h3>
Yet another real-world field where outliers often appear is professional sports.
For example, the 25th percentile (Q1) of points scored by NBA players is around 5 points per game and the 75th percentile (Q3) is around 15 points per game.
The interquartile range (IQR) would be calculated as 15 – 5 = 10 points.
This means that any player who averages outside of the following boundaries would be considered an outlier:
<b>Lower Boundary</b>: Q1 – 1.5*IQR = 5 – 1.5*10 = -10 points
<b>Upper Boundary</b>: Q3 + 1.5*IQR = 15 + 1.5*10 = 30 points
During  many NBA seasons , the highest scoring player typically averages just over 30 points per game which makes them an outlier.
<h2><span class="orange">How to Identify Outliers in SPSS</span></h2>
An <b>outlier </b>is an observation that lies abnormally far away from other values in a dataset. Outliers can be problematic because they can effect the results of an analysis.
This tutorial explains how to identify and handle outliers in SPSS.
<h3>How to Identify Outliers in SPSS</h3>
Suppose we have the following dataset that shows the annual income (in thousands) for 15 individuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS1.png">
One way to determine if outliers are present is to create a box plot for the dataset. To do so, click the <b>Analyze</b> tab, then <b>Descriptive Statistics</b>, then <b>Explore</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS2.png">
In the new window that pops up, drag the variable <b>income </b>into the box labelled Dependent List. Then click <b>Statistics </b>and make sure the box next to <b>Percentiles </b>is checked. Then click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS5.png">
Once you click <b>OK</b>, a box plot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS4.png">
If there are no circles or asterisks on either end of the box plot, this is an indication that no outliers are present.
SPSS considers any data value to be an outlier if it lies outside of the following ranges:
3rd quartile + 1.5*interquartile range
1st quartile – 1.5*interquartile range
We can calculate the interquartile range by taking the difference between the 75th and 25th percentile in the row labeled <b>Tukey’s Hinges </b>in the output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS6.png">
For this dataset, the interquartile range is 82 – 36 = <b>46</b>. Thus, any values outside of the following ranges would be considered outliers:
82 + 1.5*46 = <b>151</b>
36 – 1.5*46 = <b>-33</b>
Obviously income can’t be negative, so the lower bound in this example isn’t useful. However, any income over 151 would be considered an outlier.
For example, suppose the largest value in our dataset was instead 152. Here is the box plot for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS7.png">
The circle is an indication that an outlier is present in the data. The number 15 indicates which observation in the dataset is the outlier.
SPSS also considers any data value to be an <b>extreme outlier</b> if it lies outside of the following ranges:
3rd quartile + 3*interquartile range
1st quartile – 3*interquartile range
Thus, any values outside of the following ranges would be considered extreme outliers in this example:
82 + 3*46 = <b>220</b>
36 – 3*46 = <b>-102</b>
For example, suppose the largest value in our dataset was 221. Here is the box plot for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS8.png">
The asterisk (*) is an indication that an extreme outlier is present in the data. The number 15 indicates which observation in the dataset is the extreme outlier.
<h3>How to Handle Outliers</h3>
If an outlier is present in your data, you have a few options:
<b>1. Make sure the outlier is not the result of a data entry error.</b>
Sometimes an individual simply enters the wrong data value when recording data. If an outlier is present, first verify that the value was entered correctly and that it wasn’t an error.
<b>2. Remove the outlier.</b>
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<b>3. Assign a new value to the outlier</b>.
If the outlier turns out to be a result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
<h2><span class="orange">What is Overfitting in Machine Learning? (Explanation & Examples)</span></h2>
In the field of machine learning, we often build models so that we can make accurate predictions about some phenomenon.
For example, suppose we want to build a  regression model  that uses the predictor variable <em>hours spent studying</em> to predict the response variable <em>ACT score</em> for students in high school.
To build this model, we’ll collect data about hours spent studying and the corresponding ACT Score for hundreds of students in a certain school district.
Then we’ll use this data to <em>train</em> a model that can make predictions about the score a given student will receive based on their total hours studied.
To assess how useful the model is, we can measure how well the model predictions match the observed data. One of the most commonly used metrics for doing so is the mean squared error (MSE), which is calculated as:
MSE = (1/n)*Σ(y<sub>i</sub> – f(x<sub>i</sub>))<sup>2</sup>
where:
<b>n:</b> Total number of observations
<b>y<sub>i</sub>:</b> The response value of the i<sup>th</sup> observation
<b>f(x<sub>i</sub>):</b> The predicted response value of the i<sup>th</sup> observation
The closer the model predictions are to the observations, the smaller the MSE will be.
However, one of the biggest mistakes made in machine learning is optimizing models to reduce <b>training MSE</b> – i.e. how closely the model predictions match up with the data that we used to train the model. 
When a model focuses too much on reducing training MSE, it often works too hard to find patterns in the training data that are just caused by random chance. Then when the model is applied to unseen data, it performs poorly.
This phenomenon is known as <b>overfitting</b>. It occurs when we “fit” a model too closely to the training data and we thus end up building a model that isn’t useful for making predictions about new data.
<h3>Example of Overfitting</h3>
To understand overfitting, let’s return to the example of creating a regression model that uses <em>hours spent studying</em> to predict <em>ACT score</em>.
Suppose we gather data for 100 students in a certain school district and create a quick scatterplot to visualize the relationship between the two variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/overfitting1.png">
The relationship between the two variables appears to be quadratic, so suppose we fit the following quadratic regression model:
Score = 60.1 + 5.4*(Hours) – 0.2*(Hours)<sup>2</sup>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/overfitting2.png">
This model has a training mean squared error (MSE) of <b>3.45</b>. That is, the mean squared difference between the predictions made by the model and the actual ACT scores is 3.45.
However, we could reduce this training MSE by fitting a higher-order polynomial model. For example, suppose we fit the following model:
Score = 64.3 – 7.1*(Hours) + 8.1*(Hours)<sup>2</sup> – 2.1*(Hours)<sup>3 </sup>+ 0.2*(Hours)<sup>4 </sup>– 0.1*(Hours)<sup>5</sup> + 0.2(Hours)<sup>6</sup>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/overfitting3.png">
Notice how the regression line hugs the actual data much more closely than the previous regression line.
This model has a training mean squared error (MSE) of just <b>0.89</b>. That is, the mean squared difference between the predictions made by the model and the actual ACT scores is 0.89.
This training MSE is much smaller than the one produced by the previous model.
However, we don’t really care about the <b>training MSE</b> – i.e. how closely the model predictions match up with the data that we used to train the model. Instead, we care mostly about the <b>test MSE</b> – the MSE when our model is applied to unseen data.
If we applied the higher-order polynomial regression model above to an unseen dataset, it would likely perform worse than the simpler quadratic regression model. That is, it would produce a higher test MSE which is exactly what we don’t want.
<h3>How to Detect & Avoid Overfitting</h3>
The easiest way to detect overfitting is to perform cross-validation. The most commonly used method is known as  k-fold cross validation  and it works as follows:
<b>Step 1: Randomly divide a dataset into <em>k</em> groups, or “folds”, of roughly equal size.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold1.png">
<b>Step 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds. Calculate the test MSE on the observations in the fold that was held out.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold2.png">
<b>Step 3: Repeat this process <em>k</em> times, using a different set each time as the holdout set.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold3.png">
<b>Step 4: Calculate the overall test MSE to be the average of the <em>k</em> test MSE’s.</b>
<b>Test MSE = (1/k)*ΣMSE<sub>i</sub></b>
where:
<b>k: </b>Number of folds
<b>MSE<sub>i</sub></b>: Test MSE on the i<sup>th</sup> iteration
This test MSE gives us a good idea of how a given model will perform on unseen data.
In practice we can fit several different models and perform k-fold cross-validation on each model to find out its test MSE. Then we can choose the model with the lowest test MSE as the best model to use for making predictions in the future.
This ensures that we select a model that is likely to perform best on future data, as opposed to a model that simply minimizes the training MSE and “fits” historical data well.
<h2><span class="orange">How to Overlay Normal Curve on Histogram in R (2 Examples)</span></h2>
Often you may want to overlay a normal curve on a histogram in R.
The following examples show how to do so in base R and in  ggplot2 .
<h3>Example 1: Overlay Normal Curve on Histogram in Base R</h3>
We can use the following code to create a histogram in base R and overlay a normal curve on the histogram:
<b>#make this example reproducible
set.seed(0)
#define data
data &lt;- rnorm(1000)
#create histogram
hist_data &lt;- hist(data)
#define x and y values to use for normal curve
x_values &lt;- seq(min(data), max(data), length = 100)
y_values &lt;- dnorm(x_values, mean = mean(data), sd = sd(data)) 
y_values &lt;- y_values * diff(hist_data$mids[1:2]) * length(data) 
#overlay normal curve on histogram
lines(x_values, y_values, lwd = 2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/overlayhist1.jpg">
The black curve in the plot represents the normal curve.
Feel free to use the <b>col</b>, <b>lwd</b>, and <b>lty</b> arguments to modify the color, line width, and type of the line, respectively:
<b>#overlay normal curve with custom aesthetics
lines(x_values, y_values, col='red', lwd=5, lty='dashed')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/overlayhist2.jpg"445">
<h3>Example 2: Overlay Normal Curve on Histogram in ggplot2</h3>
We can use the following code to create a histogram in ggplot2 and overlay a normal curve on the histogram:
<b>library(ggplot2) 
#make this example reproducible
set.seed(0)
#define data
data &lt;- data.frame(x=rnorm(1000))
#create histogram and overlay normal curve
ggplot(data, aes(x)) +
  geom_histogram(aes(y = ..density..), fill='lightgray', col='black') +
  stat_function(fun = dnorm, args = list(mean=mean(data$x), sd=sd(data$x)))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/overlayhist3.jpg">
The black curve in the plot represents the normal curve.
Feel free to use the <b>col</b>, <b>lwd</b>, and <b>lty </b>arguments to modify the color, line width, and type of the line, respectively:
<b>#overlay normal curve with custom aesthetics
ggplot(data, aes(x)) +
  geom_histogram(aes(y = ..density..), fill='lightgray', col='black') +
  stat_function(fun = dnorm, args = list(mean=mean(data$x), sd=sd(data$x)),
                col='red', lwd=2, lty='dashed'))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/overlay3.jpg"474">
<b>Note</b>: You can find the complete documentation for <b>stat_function</b>  here .
<h2><span class="orange">How to Calculate the P-Value of a Chi-Square Statistic in R</span></h2>
Whenever you conduct a Chi-Square test, you will end up with a Chi-Square test statistic. You can then find the p-value that corresponds to this test statistic to determine whether or not the test results are statistically significant. 
To find the p-value that corresponds to a Chi-Square test statistic in R, you can use the  pchisq() function , which uses the following syntax:
<b>pchisq(q, df, lower.tail = TRUE)</b>
where:
<b>q: </b>The Chi-Square test statistic
<b>df: </b>The degrees of freedom
<b>lower.tail: </b>If TRUE, the probability to the left of <b>q </b>in the Chi-Square distribution is returned. If FALSE, the probability to the right of <b>q </b>in the Chi-Square distribution is returned. Default is TRUE.
The following examples show how to use this function in practice.
<h3>Example 1: Chi-Square Goodness of Fit Test</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
After performing a  Chi-Square Goodness of Fit test , the researcher finds the following:
Chi-Square Test Statistic (X<sup>2</sup>): <b>4.36</b>
Degrees of freedom: (df): <b>4</b>
To find the p-value associated with this Chi-Square test statistic and degrees of freedom, we can use the following code in R:
<b>#find p-value for the Chi-Square test statistic
pchisq(q=4.36, df=4, lower.tail=FALSE)
[1] 0.3594721</b>
The p-value turns out to be <b>0.359</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h3>Example 2: Chi-Square Test of Independence</h3>
Researchers want to know whether or not gender is associated with political party preference. They take a simple random sample of 500 voters and survey them on their political party preference. After performing a  Chi-Square Test of Independence , they find the following:
Chi-Square Test Statistic (X<sup>2</sup>): <b>0.8642</b>
Degrees of freedom: (df): <b>2</b>
To find the p-value associated with this Chi-Square test statistic and degrees of freedom, we can use the following code in R:
<b>#find p-value for the Chi-Square test statistic
pchisq(q=0.8642, df=2, lower.tail=FALSE)
[1] 0.6491445</b>
The p-value turns out to be <b>0.649</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
<b>Related: </b> How to Perform a Chi-Square Test of Independence in R 
<em>Find the complete documentation for the pchisq() function  here .</em>
<h2><span class="orange">How to Find the P-value for a Correlation Coefficient in Excel</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
To determine if a correlation coefficient is statistically significant, you can calculate the corresponding t-score and p-value.
The formula to calculate the t-score of a correlation coefficient (r) is:
<b>t</b> = r√(n-2) / √(1-r<sup>2</sup>)
The p-value is calculated as the corresponding two-sided p-value for the t-distribution with n-2 degrees of freedom.
<h3>P-Value for a Correlation Coefficient in Excel</h3>
The following formulas show how to calculate the p-value for a given correlation coefficient and sample size in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/pvaluecorrexcel1.png">
For a correlation coefficient of r = 0.56 and sample size n = 14, we find that:
<b>t-score: </b>2.341478
<b>p-value: </b>0.037285
Recall that for a correlation test we have the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> The correlation between the two variables is zero.
<b>The alternative hypothesis: (Ha):</b> The correlation between the two variables is <em>not </em>zero, e.g. there is a statistically significant correlation.
If we use a significance level of α = .05, then we would reject the null hypothesis in this case since the p-value (0.037285) is less than .05. We would conclude that the correlation coefficient is statistically significant.
<h2><span class="orange">How to Calculate the P-Value of an F-Statistic in Excel</span></h2>
An F-test produces an F-statistic. To find the p-value associated with an F-statistic in Excel, you can use the following command:
<b>=F.DIST.RT(x, degree_freedom1, degree_freedom2)</b>
where:
<b>x:</b> the value of the F-statistic
<b>degree_freedom1:</b> numerator degrees of freedom
<b>degree_freedom2:</b> denominator degrees of freedom
For example, here is how to find the p-value associated with an F-statistic of 5.4, with numerator degrees of freedom = 2 and denominator degrees of freedom = 9:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/fstat_p1.png">
The p-value is <b>0.02878</b>.
One of the most common uses of an F-test is for  testing the overall significance of a regression model . In the following example, we show how to calculate the p-value of the F-statistic for a regression model.
<h3>Example: Calculating p-value from F-statistic</h3>
Suppose we have a dataset that shows the total number of hours studied, total prep exams taken, and final exam score received for 12 different students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/fstat_p2.png">
If we fit a linear regression model to this data using <b>study_hours </b>and <b>prep_exams<i> </i></b>as the explanatory variables and <b>score </b>as the response variable, we will get the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/fstat_p3-1.png">
The F-statistic for the overall regression model is <b>5.0905</b>. This F-statistic has 2 degrees of freedom for the numerator and 9 degrees of freedom for the denominator.
Excel automatically calculates that the p-value for this F-statistic is<b> 0.0332</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/fstat_p4.png">
In order to calculate this p-value ourselves, we could use the following code:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/fstat_p5.png">
Notice that we get the same p-value as the linear regression output.
<h2><span class="orange">How to Find a P-Value from a t-Score in Excel</span></h2>
Often in statistics, a  hypothesis test  will result in a t-score test statistic. Once we find this t-score, we typically find the p-value associated with it. If this p-value is less than a certain alpha level (e.g. 0.10, 0.05, 0.01), then we reject the null hypothesis of the test and conclude that our findings are significant.
This tutorial explains how to find the p-value from a t-score in Excel using the function <b>T.DIST</b>, which takes the following arguments:
<b>T.DIST</b>(x, deg_freedom)
where:
<b>x: </b>The t-score we’re interested in.
<b>deg_freedom: </b>The degrees of freedom.
Let’s check out a couple examples.
<h3>Example 1: P-value from t-score (two-tailed)</h3>
A botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. In  a random sample  of 12 plants, she finds that the sample mean height is 14.33 inches and the sample standard deviation is 1.37 inches.
Conduct a two-tailed hypothesis test using an alpha level of .05 to determine if the mean height is equal to 15 inches.
<b>Step 1: State the hypotheses. </b>
The null hypothesis (H<sub>0</sub>): μ = 15
The alternative hypothesis: (Ha): μ ≠ 15
<b>Step 2: Find the t-score and degrees of freedom.</b>
t-score  =  (x-μ) / (s/√n)  = (14.33-15) / (1.37/√12)  =<b> -1.694</b>.
degrees of freedom = n-1 = 12-1 = <b>11</b>.
<b>Step 3: Find the p-value of the t-score using Excel.</b>
To find the p-value for the t-score, we will use the following formula in Excel:
<b>=T.DIST.2T(ABS(-1.694), 11)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/tscorepvalueexcel1.png">
This tells us that the two-tailed p-value is <b>0.1184</b>.
<b>Step 4: Reject or fail to reject the null hypothesis.</b>
Since the p-value of <b>0.1184 </b>is not less than our chosen alpha level of <b>.05</b>, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the mean height of plants is different from 15 inches.
<h3>Example 2: P-value from t-score (one-tailed)</h3>
A company wants to know whether or not a new type of battery has a a longer mean life than the current standard battery, which has a mean life of 18 hours. In  a random sample  of 25 of the new batteries, they find that the mean life is 19 hours with a standard deviation of 4 hours.
Conduct a one-tailed hypothesis test using an alpha level of .05 to determine if the mean life of the new battery is longer than the mean life of the current standard battery.
<b>Step 1: State the hypotheses. </b>
The null hypothesis (H<sub>0</sub>): μ ≤ 18
The alternative hypothesis: (Ha): μ > 18
<b>Step 2: Find the t-score and degrees of freedom.</b>
t-score  =  (x-μ) / (s/√n)  = (19-18) / (4/√25)  =<b> 1.25</b>.
degrees of freedom = n-1 = 25-1 = <b>24</b>.
<b>Step 3: Find the p-value of the t-score using Excel.</b>
To find the p-value for the t-score, we will use the following formula in Excel:
<b>=T.DIST.RT(1.25, 24)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/tscorepvalueexcel2.png">
This tells us that the one-sided p-value is <b>0.1117</b>.
<b>Step 4: Reject or fail to reject the null hypothesis.</b>
Since the p-value of <b>0.1117 </b>is greater than our chosen alpha level of <b>.05</b>, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the mean life of the new battery is longer than the mean life of the current standard battery.
<em><b>For more statistics tutorials in Excel, be sure to check out our complete list of  Excel Guides .</b></em>
<h2><span class="orange">How to Find a P-Value from a t-Score in Python</span></h2>
Often in statistics we’re interested in determining the  p-value  associated with a certain t-score that results from a  hypothesis test . If this p-value is below some significance level, we can reject the null hypothesis of our hypothesis test.
To find the p-value associated with a t-score in Python, we can use the  scipy.stats.t.sf() function , which uses the following syntax:
<b>scipy.stats.t.sf(abs(x), df)</b>
where:
<b>x: </b>The t-score
<b>df: </b>The degrees of freedom
The following examples illustrate how to find the p-value associated with a t-score for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>-0.77</b> and df = <b>15 </b>in a left-tailed hypothesis test.
<b>import scipy.stats
#find p-value
scipy.stats.t.sf(abs(-.77), df=15)
0.2266283049085413
</b>
The p-value is <b>0.2266</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<h3>Right-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>1.87</b> and df = <b>24 </b>in a right-tailed hypothesis test.
<b>import scipy.stats
#find p-value
scipy.stats.t.sf(abs(1.87), df=24)
0.036865328383323424
</b>
The p-value is <b>0.0368</b>. If we use a significance level of α = 0.05, we would reject the null hypothesis of our hypothesis test because this p-value is less than 0.05.
<h3>Two-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>1.24 </b>and df = <b>22 </b>in a two-tailed hypothesis test.
<b>import scipy.stats
#find p-value for two-tailed test
scipy.stats.t.sf(abs(1.24), df=22)*2
0.22803901531680093
</b>
<em>To find this two-tailed p-value we simply multiplied the one-tailed p-value by two.</em>
The p-value is <b>0.2280</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<em><b>Related: </b>You can also use this online  T Score to P Value Calculator  to find p-values.</em>
<h2><span class="orange">Three Ways to Find a P-Value from a t Statistic</span></h2>
A  hypothesis test  is a formal statistical test we use to reject or fail to reject a statistical hypothesis. 
Whether we conduct a hypothesis test for a mean, a proportion, a difference in means, or a difference in proportions, we often end up with a t statistic for our test.
Once we have a t statistic, we can then find a corresponding p-value that we can use to reject or fail to reject the null hypothesis of our test.
This tutorial explains three different ways to find a p-value from a t statistic.
<h2>Three Ways to Find a P-Value from a t Statistic</h2>
In each of the following examples, we’ll find the p-value for a right-tailed test with a t statistic of <b>1.441 </b>and <b>13 </b>degrees of freedom.
<h3>Technique 1: t Score to P Value Calculator</h3>
The first way to find a p-value from a t statistic is to use an online calculator like the  T Score to P Value Calculator . We can simply enter the value for t and the degrees of freedom, then select “one-tailed”, then click the “Calculate” button:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/t_to_p3.jpg">
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/t_to_p4.jpg">
The corresponding p-value is <b>0.08662</b>.
<h3>Technique 2: t Distribution Table</h3>
Another way to find the p-value for a given t statistic is to use the  t distribution table . 
Using the table, look up the row that has degrees of freedom (DF) = 13, then find the values that 1.441 lies between. It turns out to be 1.35 and 1.771. Next, look up at the top of the table for “one-tail” and you’ll notice that these values correspond with 0.1 and 0.05. This tells us that the corresponding p-value is somewhere between 0.05 and 0.1. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/t_dist.png">
Notice the drawback of using the t distribution table: it does not tell us the exact p-value; it only gives us a range of values.
<h3>Technique 3: TI-83 or TI-84 Calculator</h3>
Another way to find the p-value for a given t statistic is to use a graphing calculator like a TI-83 or TI-84.
On your calculator, click <b>2ND VARS</b> (to get to <b>DISTR</b>), scroll down, and click the <b>tcdf</b> function. The syntax to use this function to find the p-value for a right-tailed test is as follows:
tcdf(smaller value, larger value, degrees of freedom)
Since we are conducting a right-tailed test, we can use 1.441 as the smaller value, 9999 as the larger value, and 13 as the degrees of freedom:
tcdf(1.441, 9999, 13)
This returns a value of <b>0.08662</b>, which matches the p-value that we got from the online calculator.
<h2><span class="orange">How to Calculate a P-Value from a Z-Score by Hand</span></h2>
In most cases, when you find a z-score in statistics you can simply use a  Z Score to P-Value Calculator  to find the corresponding p-value.
However, sometimes you may be forced to calculate a p-value from a z-score by hand. In this case, you need to use the values found in a  z table .
The following examples show how to calculate a p-value from a z-score by hand using a z-table.
<h3>Example 1: Find P-Value for a Left-Tailed Test</h3>
Suppose we conduct a left-tailed hypothesis test and get a z-score of <b>-1.22</b>. What is the p-value that corresponds to this z-score?
To find the p-value, we can simply locate the value <b>-1.22</b> in the  z table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/zhand1.png">
The p-value that corresponds to a z-score of -1.22 is <b>0.1112</b>.
<h3>Example 2: Find P-Value for a Right-Tailed Test</h3>
Suppose we conduct a right-tailed hypothesis test and get a z-score of <b>1.43</b>. What is the p-value that corresponds to this z-score?
To find the p-value, we can first locate the value <b>1.43</b> in the  z table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/zhand2.png">
Since we’re conducting a right-tailed test, we can then subtract this value from 1.
So our final p-value is: 1 – 0.9236 = <b>0.0764</b>.
<h3>Example 3: Find P-Value for a Two-Tailed Test</h3>
Suppose we conduct a two-tailed hypothesis test and get a z-score of <b>-0.84</b>. What is the p-value that corresponds to this z-score?
To find the p-value, we can first locate the value <b>-0.84</b> in the  z table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/zhand3.png">
Since we’re conducting a two-tailed test, we can then multiply this value by 2.
So our final p-value is: 0.2005 * 2 = <b>0.401</b>.
<h2><span class="orange">How to Find a P-Value from a Z-Score in Python</span></h2>
Often in statistics we’re interested in determining the  p-value  associated with a certain z-score that results from a  hypothesis test . If this p-value is below some significance level, we can reject the null hypothesis of our hypothesis test.
To find the p-value associated with a z-score in Python, we can use the  scipy.stats.norm.sf() function , which uses the following syntax:
<b>scipy.stats.norm.sf(abs(x))</b>
where:
<b>x: </b>The z-score
The following examples illustrate how to find the p-value associated with a z-score for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>-0.77</b> in a left-tailed hypothesis test.
<b>import scipy.stats
#find p-value
scipy.stats.norm.sf(abs(-0.77))
0.22064994634264962
</b>
The p-value is <b>0.2206</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<h3>Right-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>1.87</b> in a right-tailed hypothesis test.
<b>import scipy.stats
#find p-value
scipy.stats.norm.sf(abs(1.87))
0.030741908929465954
</b>
The p-value is <b>0.0307</b>. If we use a significance level of α = 0.05, we would reject the null hypothesis of our hypothesis test because this p-value is less than 0.05.
<h3>Two-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>1.24</b> in a two-tailed hypothesis test.
<b>import scipy.stats
#find p-value for two-tailed test
scipy.stats.norm.sf(abs(1.24))*2
0.21497539414917388
</b>
<em>To find this two-tailed p-value we simply multiplied the one-tailed p-value by two.</em>
The p-value is <b>0.2149</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<em><b>Related: </b>You can also use this online  Z Score to P Value Calculator  to find p-values.</em>
<h2><span class="orange">How to Find P-Values in Google Sheets (Step-by-Step)</span></h2>
The easiest way to calculate p-values in Google Sheets is to use the <b>T.TEST()</b> function, which finds the p-value associated with a t-test and uses the following syntax:
<b>T.TEST(range 1, range2, tails, type)</b>
where:
<b>range1:</b> The first sample of data
<b>range2:</b> The second sample of data
<b>tails:</b> The number of tails to use for the test
<b>1:</b> One-tailed (or “one-sided”) t-test
<b>2: </b>Two-tailed (or “two-sided) t-test
<b>type:</b> The type of t-test
<b>1:</b> Paired t-test
<b>2:</b> Two sample t-test with equal variance
<b>3:</b> Two sample t-test with unequal variance
This function returns the p-value that corresponds with the t-test.
The following step-by-step example shows how to use this function in practice.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset that contains the height of two different plant species:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pvalueSheets1.png">
<h3>Step 2: Calculate the P-Value of the t-Test</h3>
Next, suppose we want to perform a t-test to determine if the mean height between the two plant species is equal.
The following screenshots show which formulas to use to calculate the p-values of the tests.
<b>Paired Samples t-Test</b>
We can use the following formula to calculate the p-value for a  paired samples t-test :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pvalueSheets2.png">
The p-value turns out to be <b>0.1586</b>. Since this is not less than α = .05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height between the two species is different.
<b>Two Sample t-Test with Equal Variance</b>
We can use the following formula to calculate the p-value for a two sample t-test with equal variance:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pvalueSheets3.png">
The p-value turns out to be <b>0.5300</b>. Since this is not less than α = .05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height between the two species is different.
<b>Two Sample t-Test with Unequal Variance</b>
We can use the following formula to calculate the p-value for a two sample t-test with unequal variance:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pvalueSheets4.png">
The p-value turns out to be <b>0.5302</b>. Since this is not less than α = .05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height between the two species is different.
<h2><span class="orange">How to Interpret a P-Value Less Than 0.001 (With Examples)</span></h2>
A  hypothesis test  is used to test whether or not some assumption about a  population parameter  is true.
Whenever we perform a hypothesis test, we always define a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the  p-value  of the hypothesis test is less than some significance level (e.g. α = .001), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that the alternative hypothesis is true.
If the p-value is not less than .001, then we fail to reject the null hypothesis and conclude that we do not have sufficient evidence to say that the alternative hypothesis is true.
The following examples explain how to interpret a p-value less than .001 and how to interpret a p-value greater than .001 in practice.
<h2>Example: Interpret a P-Value Less Than 0.001</h2>
Suppose a factory claims that they produce batteries with an average weight of 2 ounces.
An auditor comes in and tests the null hypothesis that the mean weight of a battery is 2 ounces vs. the alternative hypothesis that the mean weight is not 2 ounces, using a 0.001 level of significance.
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 2 ounces
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ ≠ 2 ounces
The auditor conducts a hypothesis test for the mean and ends up with a p-value of <b>0.0006</b>.
Since the p-value of <b>0.0006 </b>is less than the significance level of <b>0.01</b>, the auditor rejects the null hypothesis.
He concludes that there is sufficient evidence to say that the true average weight of a battery produced at this factory is not 2 ounces.
<h2>Example: Interpret a P-Value Greater Than 0.001</h2>
Suppose that some crop grows an average of 40 inches during a growing season.
 However, an agricultural scientist believes that a certain fertilizer will cause this crop to grow more than 40 inches, on average.
To test this, she applies the fertilizer to a  random sample  of crops in a certain field during the growing season.
She then performs a hypothesis test using the following hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 40 inches (fertilizer will have no effect on the mean growth)
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ > 40 inches (fertilizer will cause mean growth to increase)
Upon conducting a hypothesis test for the mean, the scientist gets a p-value of <b>0.3488</b>.
Since the p-value of <b>0.3488</b> is greater than the significance level of <b>0.001</b>, the scientist fails to reject the null hypothesis.
She concludes that there is not sufficient evidence to say that the fertilizer leads to an increase in mean crop growth.
<h2>Additional Resources</h2>
The following tutorials provide additional information about p-values and hypothesis tests:
 An Explanation of P-Values and Statistical Significance 
 The Difference Between T-Values and P-Values in Statistics 
 P-Value vs. Alpha: What’s the Difference? 
<h2><span class="orange">How to Interpret a P-Value Less Than 0.01 (With Examples)</span></h2>
A  hypothesis test  is used to test whether or not some assumption about a  population parameter  is true.
Whenever we perform a hypothesis test, we always define a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the  p-value  of the hypothesis test is less than some significance level (e.g. α = .01), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that the alternative hypothesis is true.
If the p-value is not less than .01, then we fail to reject the null hypothesis and conclude that we do not have sufficient evidence to say that the alternative hypothesis is true.
The following examples explain how to interpret a p-value less than .01 and how to interpret a p-value greater than .01 in practice.
<h2>Example: Interpret a P-Value Less Than 0.01</h2>
Suppose a factory claims that they produce batteries with an average weight of 2 ounces.
An auditor comes in and tests the null hypothesis that the mean weight of a battery is 2 ounces vs. the alternative hypothesis that the mean weight is not 2 ounces, using a 0.01 level of significance.
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 2 ounces
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ ≠ 2 ounces
The auditor conducts a hypothesis test for the mean and ends up with a p-value of <b>0.0046</b>.
Since the p-value of <b>0.0046 </b>is less than the significance level of <b>0.01</b>, the auditor rejects the null hypothesis.
He concludes that there is sufficient evidence to say that the true average weight of a battery produced at this factory is not 2 ounces.
<h2>Example: Interpret a P-Value Greater Than 0.01</h2>
Suppose that some crop grows an average of 20 inches during a three-month growing season. However, an agricultural scientist believes that a certain fertilizer will cause this crop to grow more than 20 inches, on average.
To test this, she applies the fertilizer to every crop in a certain field during the three-month growing season.
She then performs a hypothesis test using the following hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 20 inches (fertilizer will have no effect on the mean growth)
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ > 20 inches (fertilizer will cause mean growth to increase)
Upon conducting a hypothesis test for the mean, the scientist gets a p-value of <b>0.3488</b>.
Since the p-value of <b>0.3488</b> is greater than the significance level of <b>0.01</b>, the scientist fails to reject the null hypothesis.
She concludes that there is not sufficient evidence to say that the fertilizer leads to an increase in mean crop growth.
<h2>Additional Resources</h2>
The following tutorials provide additional information about p-values and hypothesis tests:
 An Explanation of P-Values and Statistical Significance 
 The Difference Between T-Values and P-Values in Statistics 
 P-Value vs. Alpha: What’s the Difference? 
<h2><span class="orange">How to Interpret a P-Value Less Than 0.05 (With Examples)</span></h2>
A  hypothesis test  is used to test whether or not some hypothesis about a  population parameter  is true.
Whenever we perform a hypothesis test, we always define a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the p-value of the hypothesis test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that the alternative hypothesis is true.
If the p-value is not less than .05, then we fail to reject the null hypothesis and conclude that we do not have sufficient evidence to say that the alternative hypothesis is true.
The following examples explain how to interpret a p-value less than .05 and how to interpret a p-value greater than .05 in practice.
<h3>Example: Interpret a P-Value Less Than 0.05</h3>
Suppose a factory claims that they produce tires that each weigh 200 pounds.
An auditor comes in and tests the null hypothesis that the mean weight of a tire is 200 pounds against the alternative hypothesis that the mean weight of a tire is not 200 pounds, using a 0.05 level of significance.
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 200
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ ≠ 200
Upon conducting a hypothesis test for a mean, the auditor gets a p-value of <b>0.0154</b>.
Since the p-value of <b>0.0154</b> is less than the significance level of <b>0.05</b>, the auditor rejects the null hypothesis and concludes that there is sufficient evidence to say that the true average weight of a tire is not 200 pounds.
<h3>Example: Interpret a P-Value Greater Than 0.05</h3>
Suppose a biologist believes that a certain fertilizer will cause plants to grow more during a three-month period than they normally do, which is currently 20 inches. To test this, she applies the fertilizer to each of the plants in her laboratory for three months.
She then performs a hypothesis test using the following hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> μ = 20 inches (the fertilizer will have no effect on the mean plant growth)
<b>The alternative hypothesis: (H<sub>A</sub>):</b> μ > 20 inches (the fertilizer will cause mean plant growth to increase)
Upon conducting a hypothesis test for a mean, the biologist gets a p-value of <b>0.2338</b>.
Since the p-value of <b>0.2338</b> is greater than the significance level of <b>0.05</b>, the biologist fails to reject the null hypothesis and concludes that there is not sufficient evidence to say that the fertilizer leads to increased plant growth.
<h2><span class="orange">How to Find the P-Value of a Chi-Square Statistic in Excel</span></h2>
Whenever you conduct a Chi-Square test, you will end up with a Chi-Square test statistic. You can then find the p-value that corresponds to this test statistic to determine whether or not the test results are statistically significant. 
To find the p-value that corresponds to a Chi-Square test statistic in Excel, you can use the CHISQ.DIST.RT() function, which uses the following syntax:
<b>=CHISQ.DIST.RT(x, deg_freedom)</b>
where:
<b>x: </b>The Chi-Square test statistic
<b>deg_freedom: </b>The degrees of freedom
The following examples show how to use this function in practice.
<h3>Example 1: Chi-Square Goodness of Fit Test</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
After performing a  Chi-Square Goodness of Fit test , the researcher finds the following:
Chi-Square Test Statistic (X<sup>2</sup>): <b>4.36</b>
Degrees of freedom: (df): <b>4</b>
To find the p-value associated with this Chi-Square test statistic and degrees of freedom, we can use the following formula in Excel:
<b>=CHISQ.DIST.RT(4.36, 4)</b>
Here’s what that looks like in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/chi_p_excel1.png">
The p-value turns out to be <b>0.359472</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h3>Example 2: Chi-Square Test of Independence</h3>
Researchers want to know whether or not gender is associated with political party preference. They take a simple random sample of 500 voters and survey them on their political party preference. After performing a  Chi-Square Test of Independence , they find the following:
Chi-Square Test Statistic (X<sup>2</sup>): <b>0.8642</b>
Degrees of freedom: (df): <b>2</b>
To find the p-value associated with this Chi-Square test statistic and degrees of freedom, we can use the following code in Excel:
<b>=CHISQ.DIST.RT(<b>0.8642</b>, 2)</b>
Here’s what that looks like in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/chi_p_excel2.png">
The p-value turns out to be <b>0.649144</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
<i>You can find more Excel tutorials  here .</i>
<h2><span class="orange">How to Calculate the P-Value of a T-Score in R</span></h2>
Often in statistics we’re interested in determining the p-value associated with a certain t-score that results from a  hypothesis test . If this p-value is below some significance level, we can reject the null hypothesis of our hypothesis test.
To find the p-value associated with a t-score in R, we can use the pt() function, which uses the following syntax:
<b>pt(q, df, lower.tail = TRUE)</b>
where:
<b>q: </b>The t-score
<b>df: </b>The degrees of freedom
<b>lower.tail: </b>If TRUE, the probability to the left of <b>q </b>in the t distribution is returned. If FALSE, the probability to the right is returned. Default is TRUE.
The following examples illustrate how to find the p-value associated with a t-score for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>-0.77</b> and df = <b>15 </b>in a left-tailed hypothesis test.
<b>#find p-value
pt(q=-.77, df=15, lower.tail=TRUE)
[1] 0.2266283
</b>
The p-value is <b>0.2266</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<h3>Right-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>1.87</b> and df = <b>24 </b>in a right-tailed hypothesis test.
<b>#find p-value
pt(q=1.87, df=24, lower.tail=FALSE)
[1] 0.03686533
</b>
The p-value is <b>0.0368</b>. If we use a significance level of α = 0.05, we would reject the null hypothesis of our hypothesis test because this p-value is less than 0.05.
<h3>Two-tailed test</h3>
Suppose we want to find the p-value associated with a t-score of <b>1.24 </b>and df = <b>22 </b>in a two-tailed hypothesis test.
<b>#find two-tailed p-value
2*pt(q=1.24, df=22, lower.tail=FALSE)
[1] 0.228039
</b>
<em style="color: #000000;">To find this two-tailed p-value we simply multiplied the one-tailed p-value by two.</em>
The p-value is <b>0.2280</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<em><b>Related: </b>You can also use this online  T Score to P Value Calculator  to find p-values.</em>
<h2><span class="orange">How to Calculate the P-Value of a Z-Score in R</span></h2>
Often in statistics we’re interested in determining the p-value associated with a certain z-score that results from a  hypothesis test . If this p-value is below some significance level, we can reject the null hypothesis of our hypothesis test.
To find the p-value associated with a z-score in R, we can use the pnorm() function, which uses the following syntax:
<b>pnorm(q, mean = 0, sd = 1, lower.tail = TRUE)</b>
where:
<b>q: </b>The z-score
<b>mean: </b>The mean of the normal distribution. Default is 0.
<b>sd: </b>The standard deviation of the normal distribution. Default is 1.
<b>lower.tail: </b>If TRUE, the probability to the left of <b>q </b>in the normal distribution is returned. If FALSE, the probability to the right is returned. Default is TRUE.
The following examples illustrate how to find the p-value associated with a z-score for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>-0.77</b> in a left-tailed hypothesis test.
<b>#find p-value
pnorm(q=-0.77, lower.tail=TRUE)
[1] 0.2206499
</b>
The p-value is <b>0.2206</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<h3>Right-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>1.87</b> in a right-tailed hypothesis test.
<b>#find p-value
pnorm(q=1.87, lower.tail=FALSE)
[1] 0.03074191
</b>
The p-value is <b>0.0307</b>. If we use a significance level of α = 0.05, we would reject the null hypothesis of our hypothesis test because this p-value is less than 0.05.
<h3>Two-tailed test</h3>
Suppose we want to find the p-value associated with a z-score of <b>1.24</b> in a two-tailed hypothesis test.
<b>#find p-value for two-tailed test
2*pnorm(q=1.24, lower.tail=FALSE)
[1] 0.2149754
</b>
<em>To find this two-tailed p-value we simply multiplied the one-tailed p-value by two.</em>
The p-value is <b>0.2149</b>. If we use a significance level of α = 0.05, we would fail to reject the null hypothesis of our hypothesis test because this p-value is not less than 0.05.
<em><b>Related: </b>You can also use this online  Z Score to P Value Calculator  to find p-values.</em>
<h2><span class="orange">P-Value vs. Alpha: What’s the Difference?</span></h2>
Two terms that students often get confused in statistics are <b>p-value</b> and <b>alpha</b>. 
Both terms are used in  hypothesis tests , which are formal statistical tests we use to reject or fail to reject some hypothesis.
For example, suppose we hypothesize that a new pill reduces blood pressure in patients more than the current standard pill.
To test this, we can conduct a hypothesis test where we define the following null and alternative hypotheses:
<b>Null hypothesis:</b> There is no difference between the new pill and the standard pill.
<b>Alternative hypothesis:</b> There <em>is</em> a difference between the new pill and the standard pill.
If we assume the null hypothesis is true, the <b>p-value</b> of the test tells us the probability of obtaining an effect at least as large as the one we actually observed in the sample data. 
For example, suppose we find that the p-value of the hypothesis test is 0.02.
Here’s how to interpret this p-value: If there truly was no difference between the new pill and the standard pill, then 2% of the times that we perform this hypothesis test we would obtain the effect observed in the sample data, or larger, simply due to random sample error.
This tells us that obtaining the sample data that we actually did would be pretty rare if indeed there was no difference between the new pill and the standard pill.
Thus, we would be inclined to reject the statement in the null hypothesis and conclude that there <em>is</em> a difference between the new pill and the standard pill.
But what threshold should we use to determine if our p-value is low enough to reject the null hypothesis?
This is where alpha comes in!
<h3>The Alpha Level</h3>
The <b>alpha level</b> of a hypothesis test is the threshold we use to determine whether or not our p-value is low enough to reject the null hypothesis. It is often set at 0.05 but it is sometimes set as low as 0.01 or as high as 0.10.
For example, if we set the alpha level of a hypothesis test at 0.05 and we get a p-value of 0.02, then we would reject the null hypothesis since the p-value is less than the alpha level. Thus, we would conclude that we have sufficient evidence to say the alternative hypothesis is true.
It’s important to note that the alpha level also defines the probability of incorrectly rejecting a true null hypothesis.
For example, suppose that we want to test whether or not there is a difference in mean blood pressure reduction between a new pill and the current pill. And suppose there actually is <em>no</em> difference between the two pills.
If we set the alpha level of a hypothesis test at 0.05 then this means that if we repeated the process of performing the hypothesis test many times, we would expect to incorrectly reject the null hypothesis in about 5% of the tests.
<h3>How to Choose the Alpha Level</h3>
As mentioned earlier, the most common choice for the alpha level of a hypothesis test is 0.05. However, in some situations where there are serious consequences for making incorrect conclusions, we may set the alpha level to be even lower, perhaps at 0.01.
For example, in the medical field it’s common for researchers to set the alpha level at 0.01 because they want to be highly confident that the results of a hypothesis test are reliable.
Conversely, in fields like marketing it may be more common to set the alpha level at a higher level like 0.10 because the consequences for being wrong aren’t life or death.
It’s worth noting that increasing the alpha level of a test will increase the chances of finding a significance test result, but it also increases the chances that we incorrectly reject a true null hypothesis.
<h3>Summary:</h3>
Here’s what we learned in this article:
<b>1. </b>A <b>p-value</b> tells us the probability of obtaining an effect at least as large as the one we actually observed in the sample data. 
<b>2.</b> An <b>alpha level</b> is the probability of incorrectly rejecting a true null hypothesis.
<b>3.</b> If the p-value of a hypothesis test is less than the alpha level, then we can reject the null hypothesis.
<b>4.</b> Increasing the alpha level of a test increases the chances that we can find a significant test result, but it also increases the chances that we incorrectly reject a true null hypothesis.
<h2><span class="orange">An Explanation of P-Values and Statistical Significance</span></h2>
In statistics, <b>p-values</b> are commonly used in hypothesis testing for t-tests, chi-square tests, regression analysis, ANOVAs, and a variety of other statistical methods.
Despite being so common, people often interpret p-values incorrectly, which can lead to errors when interpreting the findings from an analysis or a study. 
This post explains how to understand and interpret p-values in a clear, practical way.
<h2>Hypothesis Testing</h2>
To understand p-values, we first need to understand the concept of  hypothesis testing .
A <b>hypothesis test</b> is a formal statistical test we use to reject or fail to reject some hypothesis. For example, we may hypothesize that a new drug, method, or procedure provides some benefit over a current drug, method, or procedure. 
To test this, we can conduct a hypothesis test where we use a null and alternative hypothesis:
<b>Null hypothesis</b> – There is no effect or difference between the new method and the old method.
<b>Alternative hypothesis</b> – There does exist some effect or difference between the new method and the old method.
A p-value indicates how believable the null hypothesis is, given the sample data. Specifically, assuming the null hypothesis is true, the p-value tells us the probability of obtaining an effect at least as large as the one we actually observed in the sample data. 
If the p-value of a hypothesis test is sufficiently low, we can reject the null hypothesis. Specifically, when we conduct a hypothesis test, we must choose a significance level at the outset. Common choices for significance levels are 0.01, 0.05, and 0.10.
If the p-values is <em>less than </em>our significance level, then we can reject the null hypothesis.
Otherwise, if the p-value is <em>equal to or greater than </em>our significance level, then we fail to reject the null hypothesis. 
<h2>How to Interpret a P-Value</h2>
The textbook definition of a p-value is:
A <b>p-value</b> is the probability of observing a sample statistic that is at least as extreme as your sample statistic, given that the null hypothesis is true.
For example, suppose a factory claims that they produce tires that have a mean weight of 200 pounds. An auditor hypothesizes that the true mean weight of tires produced at this factory is different from 200 pounds so he runs a hypothesis test and finds that the p-value of the test is 0.04. Here is how to interpret this p-value:
If the factory does indeed produce tires that have a mean weight of 200 pounds, then 4% of all audits will obtain the effect observed in the sample, or larger, because of random sample error. This tells us that obtaining the sample data that the auditor did would be pretty rare if indeed the factory produced tires that have a mean weight of 200 pounds. 
Depending on the significance level used in this hypothesis test, the auditor would likely reject the null hypothesis that the true mean weight of tires produced at this factory is indeed 200 pounds. The sample data that he obtained from the audit is not very consistent with the null hypothesis.
<h2>How <em>Not </em>to Interpret a P-Value</h2>
The biggest misconception about p-values is that they are equivalent to the probability of making a mistake by rejecting a true null hypothesis (known as a Type I error).
There are two primary reasons that p-values can’t be the error rate:
<b>1. </b>P-values are calculated based on the assumption that the null hypothesis is true and that the difference between the sample data and the null hypothesis is simple caused by random chance. Thus, p-values can’t tell you the probability that the null is true or false since it is 100% true based on the perspective of the calculations.
<b>2. </b>Although a low p-value indicates that your sample data are unlikely assuming the null is true, a p-value still can’t tell you which of the following cases is more likely:
The null is false
The null is true but you obtained an odd sample
In regards to the previous example, here is a correct and incorrect way to interpret the p-value:
<b>Correct Interpretation:</b> Assuming the factory does produce tires with a mean weight of 200 pounds, you would obtain the observed difference that you <em>did </em>obtain in your sample or a more extreme difference in 4% of audits due to random sampling error.
<b>Incorrect Interpretation:</b> If you reject the null hypothesis, there is a 4% chance that you are making a mistake.
<h2>Examples of Interpreting P-Values</h2>
The following examples illustrate correct ways to interpret p-values in the context of hypothesis testing.
<h3>Example 1</h3>
A phone company claims that 90% of its customers are satisfied with their service. To test this claim, an independent researcher gathered a  simple random sample  of 200 customers and asked them if they are satisfied with their service, to which 85% responded yes. The p-value associated with this sample data turned out to be 0.018.
<b>Correct interpretation of p-value: </b>Assuming that 90% of the customers actually are satisfied with their service, the researcher would obtain the observed difference that he <em>did </em>obtain in his sample or a more extreme difference in 1.8% of audits due to random sampling error.
<h3>Example 2</h3>
A company invents a new battery for phones. The company claims that this new battery will work for at least 10 minutes longer than the old battery. To test this claim, a researcher takes a simple random sample of 80 new batteries and 80 old batteries. The new batteries run for an average of 120 minutes with a standard deviation of 12 minutes and the old batteries run for an average of 115 minutes with a standard deviation of 15 minutes. The p-value that results from the test for a difference in population means is 0.011.
<b>Correct interpretation of p-value: </b>Assuming that the new battery works for the same amount of time or less than the old battery, the researcher would obtain the observed difference or a more extreme difference in 1.1% of studies due to random sampling error.
<h2><span class="orange">What is Paired Data? (Explanation & Examples)</span></h2>
When two datasets are of equal length and each observation in one dataset can be “paired” with an observation in another dataset, we call this <b>paired data</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pairedData1.png">
In order for two datasets to be paired, it’s important that each  observation  in one dataset can only be paired with one observation in the other dataset. 
<h3>Examples of Paired Data</h3>
Here are a few examples of paired data:
<b>Example 1: Duplicate Measurements.</b>
Suppose researchers want to know if a scale is consistent in weighing boxes at all hours of the day in a certain warehouse. To test this, researchers use the scale to weigh 30 different boxes in the morning and then again in the evening.
The end result is two datasets in which the morning and evening weights of each box can be “paired” with themselves.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pairedData2.png">
<b>Example 2: Pre-Post Measurements.</b>
A doctor wants to know if a new drug is capable of reducing blood pressure in patients. To test this, he measures the blood pressure of 20 different patients before and after using the drug for one week.
The end result is two datasets, in which the before and after blood pressure of each individual can be “paired” with themselves.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pairedData3.png">
<h3>How to Analyze Paired Data</h3>
There are two common ways to analyze paired data:
<b>1. Perform a paired t-test.</b>
One way to analyze paired data is to perform a  paired samples t-test , which compares the means of two  samples  when each observation in one sample can be paired with an observation in the other sample. 
This test tells us whether the mean value is equal between the two datasets.
<b>2. Calculate the correlation between the two datasets.</b>
Another way to analyze paired data is to calculate the  correlation  between the two datasets.
This gives us an idea of the direction and the strength of the relationship between the values in the two datasets.
<h3>Paired Data vs. Unpaired Data</h3>
Unlike paired data, <b>unpaired data</b> occurs when the observations of one dataset cannot be uniquely paired with an observation in another dataset.
For example, suppose researchers want to know whether or not a certain training program increases the average vertical jump of basketball players.
One way to test this using <b>paired data</b> would be to measure the max vertical jump of the same 20 players before and after using the training program:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pairedData4.png">
To test this using <b>unpaired data</b>, the researchers could measure the max vertical jump of 20 players who did not use the training program and then measure the max vertical jump of 20 different players who did use the training program:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pairedData5.png">
When we’re working with paired data, we use a  paired samples t-test  to determine if the difference between the sample means is different.
And when we’re working with unpaired data, we use an  independent samples t-test  to determine if the difference between the sample means is different.
<h2><span class="orange">Paired Samples t-test Calculator</span></h2>
A <b>paired samples t-test</b> is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
To perform a paired samples t-test, simply fill in the information below and then click the “Calculate” button.
<b>Sample 1</b>
<textarea id="rawData1" rows="5" cols="40">301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304</textarea>
<b>Sample 2</b>
<textarea id="rawData2" rows="5" cols="40">302, 309, 324, 313, 312, 310, 305, 298, 299, 300, 289, 294</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>t = </b> -1.608761
<b>df = </b> 22
<b>p-value (one-tailed) = </b> 0.060963
<b>p-value (two-tailed) = </b> 0.121926
<script>
function calc() {
//get raw data
var raw1 = document.getElementById('rawData1').value.split(',').map(Number);
var raw2 = document.getElementById('rawData2').value.split(',').map(Number);
//calculate paired differences
var diff = [];
for (var i = 0; i < raw1.length; i++) {
            diff.push(raw1[i]-raw2[i]);
        } 
console.log(diff);
//calculate test statistic t
var xdiff = math.mean(diff)
var s = math.std(diff)
var n = raw1.length;
var df = n-1;
var t = xdiff /(s/Math.sqrt(n));
//calculate p-value
if (t<0) {
var p1 = jStat.studentt.cdf(t, df);
var p2 = p1*2;
} else {
var p1 = 1-jStat.studentt.cdf(t, df);
var p2 = p1*2;
}
document.getElementById('t').innerHTML = t.toFixed(6);
document.getElementById('df').innerHTML = df;
document.getElementById('p1').innerHTML = p1.toFixed(6);
document.getElementById('p2').innerHTML = p2.toFixed(6);
}
</script>
<h2><span class="orange">How to Conduct a Paired Samples t-Test in Excel</span></h2>
A  <b>paired samples t-test</b>  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This tutorial explains how to conduct a paired samples t-test in Excel.
<h2>How to Conduct a Paired Samples t-Test in Excel</h2>
Suppose we want to know whether a certain study program significantly impacts student performance on a particular exam. To test this, we have 20 students in a class take a pre-test. Then, we have each of the students participate in the study program for two weeks. Then, the students retake a test of similar difficulty.
To compare the difference between the mean scores on the first and second test, we use a paired samples t-test because for each student their first test score can be paired with their second test score.
The following image shows the pre-test score and post-test score for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/pairedTestExcel.png">
Perform the following steps to conduct a paired samples t-test to determine if there is a significant difference in the mean test scores between the pre-test and post-test.
<b>Step 1: Open the Data Analysis ToolPak.</b>
On the Data tab along the top ribbon, click “Data Analysis.”
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this as an option to click on, you need to first  download the Analysis ToolPak , which is completely free.
<b>Step 2: Select the appropriate test to use.</b>
Select the option that says <em>t-Test: Paired Two Sample for Means</em> and then click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/pairedTestExcel1.png">
<b>Step 3: Enter the necessary info.</b>
Enter the range of values for Variable 1 (the pre-test scores), Variable 2 (the post-test scores), the hypothesized mean difference (in this case we put “0” because we want to know if the true mean difference between pre-test scores and post-test scores is 0), and the output range where we would like to see the results of the test displayed. Then, click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/pairedTestExcel2.png">
<b>Step 4: Interpret the results.</b>
Once you click OK in the previous step, the results of the t-test will be displayed. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/pairedTestExcel3.png">
Here is how to interpret the results:
<b>Mean: </b>This is the mean for each sample. The mean pre-test score is <b>85.4 </b>and the mean post-test score is <b>87.2</b>.
<b>Variance: </b>This is the variance for each sample. The variance of the pre-test scores is <b>51.51 </b>and the variance for the post-test scores is <b>36.06</b>.
<b>Observations: </b>This is the number of observations in each sample. Both samples have <b>20 </b>observations.
<b>Pearson Correlation: </b>The correlation between the pre-test scores and post-test scores. It turns out to be <b>0.918</b>.
<b>Hypothesized mean difference: </b>The number that we “hypothesize” is the difference between the two means. In this case, we chose <b>0</b> because we want to test whether or not there is any difference at all between pre-test and post-test scores.
<b>df: </b>The degrees of freedom for the t-test. This is calculated as n-1 where n is the number of pairs. In this case, df = 20 – 1 = <b>19</b>.
<b>t Stat: </b>The test statistic <em>t</em>, which turns out to be <b>-2.78</b>.
<b>P(T&lt;=t) two-tail: </b>The p-value for a two-tailed t-test. In this case, p = <b>0.011907</b>. This is smaller than alpha = 0.05, so we reject the null hypothesis. We have sufficient evidence to say that there is a statistically significant difference between the mean pre-test and post-test score.
<b>t Critical two-tail: </b>This is the critical value of the test, found by identifying the value in the  t Distribution table  that corresponds with a two-tailed test with alpha = 0.05 and df = 19. This turns out to be <b>2.093024</b>. Since the absolute value of our test statistic <em>t </em>is greater than this value, we reject the null hypothesis. We have sufficient evidence to say that there is a statistically significant difference between the mean pre-test and post-test score.
Note that the p-value and the critical value approach will both lead to the same conclusion.
<h2><span class="orange">How to Conduct a Paired Samples T-Test in Python</span></h2>
A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This tutorial explains how to conduct a paired samples t-test in Python.
<h2>Example: Paired Samples T-Test in Python</h2>
Suppose we want to know whether a certain study program significantly impacts student performance on a particular exam. To test this, we have 15 students in a class take a pre-test. Then, we have each of the students participate in the study program for two weeks. Then, the students retake a test of similar difficulty.
To compare the difference between the mean scores on the first and second test, we use a paired samples t-test because for each student their first test score can be paired with their second test score.
Perform the following steps to conduct a paired samples t-test in Python.
<b>Step 1: Create the data.</b>
First, we’ll create two arrays to hold the pre and post-test scores:
<b>pre = [88, 82, 84, 93, 75, 78, 84, 87, 95, 91, 83, 89, 77, 68, 91]</b>
<b>post = [91, 84, 88, 90, 79, 80, 88, 90, 90, 96, 88, 89, 81, 74, 92]</b>
<b>Step 2: Conduct a Paired Samples T-Test.</b>
Next, we’ll use the  ttest_rel() function  from the scipy.stats library to conduct a paired samples t-test, which uses the following syntax:
<b>ttest_rel(a, b)</b>
where:
<b>a: </b>an array of sample observations from group 1
<b>b: </b>an array of sample observations from group 2
Here’s how to use this function in our specific example:
<b>import scipy.stats as stats
#perform the paired samples t-test
stats.ttest_rel(pre, post)
(statistic=-2.9732, pvalue=0.0101)
</b>
The test statistic is <b>-2.9732 </b>and the corresponding two-sided p-value is <b>0.0101</b>.
<b>Step 3: Interpret the results.</b>
In this example, the paired samples t-test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>The mean pre-test and post-test scores are equal
<b>H<sub>A</sub>:</b>The mean pre-test and post-test scores are <em>not</em> equal
Since the p-value (<b>0.0101</b>) is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say that the true mean test score is different for students before and after participating in the study program.
<h2><span class="orange">How to Perform a Paired Samples t-test in R</span></h2>
A  paired samples t-test  is a statistical test that compares the means of two samples when each observation in one sample can be paired with an  observation  in the other sample.
For example, suppose we want to know whether a certain study program significantly impacts student performance on a particular exam. To test this, we have 20 students in a class take a pre-test. Then, we have each of the students participate in the study program each day for two weeks. Then, the students retake a test of similar difficulty.
To compare the difference between the mean scores on the first and second test, we use a paired t-test because for each student their first test score can be paired with their second test score.
<h2>How to Conduct a Paired t-test</h2>
To conduct a paired t-test, we can use the following approach: 
<b>Step 1: State the null and alternative hypotheses.</b>
<b>H<sub>0</sub>: μ<sub>d</sub> = 0 </b>
<b>H<sub>a</sub>: μ<sub>d</sub> ≠ 0</b> (two-tailed)
<b>H<sub>a</sub>: μ<sub>d</sub> > 0</b> (one-tailed)
<b>H<sub>a</sub>: μ<sub>d</sub> &lt; 0</b> (one-tailed)
<em>where <b> μ<sub>d</sub> </b>is the mean difference.</em>
<b>Step 2: Find the test statistic and corresponding p-value.</b>
Let <em>a</em> = the student’s score on the first test and <em>b</em> = the student’s score on the second test. To test the null hypothesis that the true mean difference between the test scores is zero:
Calculate the difference between each pair of scores (d<sub>i</sub> = b<sub>i</sub> – a<sub>i</sub>)
Calculate the mean difference (d)
Calculate the standard deviation of the differences s<sub>d</sub>
Calculate the t-statistic, which is T = d / (s<sub>d</sub> / √n)
Find the corresponding p-value for the t-statistic with <em>n-1 </em>degrees of freedom.
<b>Step 3: Reject or fail to reject the null hypothesis, based on the significance level.</b>
If the p-value is less than our chosen significance level, we reject the null hypothesis and conclude that there is a statistically significant difference between the means of the two groups. Otherwise, we fail to reject the null hypothesis.
<h2>How to Conduct a Paired t-test in R</h2>
To conduct a paired t-test in R, we can use the built-in <b>t.test()</b> function with the following syntax:
<b>t.test</b>(x, y, paired = TRUE, alternative = “two.sided”)
<b>x,y:</b> the two numeric vectors we wish to compare
<b>paired:</b> a logical value specifying that we want to compute a paired t-test
<b>alternative:</b> the alternative hypothesis. This can be set to “two.sided” (default), “greater” or “less”.
The following example illustrates how to conduct a paired t-test to find out if there is a significant difference in the mean scores between a pre-test and a post-test for 20 students.
<h3>Create the Data</h3>
First, we’ll create the dataset:
<b>#create the dataset
data &lt;- data.frame(score = c(85 ,85, 78, 78, 92, 94, 91, 85, 72, 97,             84, 95, 99, 80, 90, 88, 95, 90, 96, 89,             84, 88, 88, 90, 92, 93, 91, 85, 80, 93,             97, 100, 93, 91, 90, 87, 94, 83, 92, 95),   group = c(rep('pre', 20), rep('post', 20)))
#view the dataset
data
#   score group
#1     85   pre
#2     85   pre
#3     78   pre
#4     78   pre
#5     92   pre
#6     94   pre
#7     91   pre
#8     85   pre
#9     72   pre
#10    97   pre
#11    84   pre
#12    95   pre
#13    99   pre
#14    80   pre
#15    90   pre
#16    88   pre
#17    95   pre
#18    90   pre
#19    96   pre
#20    89   pre
#21    84  post
#22    88  post
#23    88  post
#24    90  post
#25    92  post
#26    93  post
#27    91  post
#28    85  post
#29    80  post
#30    93  post
#31    97  post
#32   100  post
#33    93  post
#34    91  post
#35    90  post
#36    87  post
#37    94  post
#38    83  post
#39    92  post
#40    95  post
</b>
<h3>Visualize the Differences</h3>
Next, we’ll look at summary statistics of the two groups using the <b>group_by() </b>and <b>summarise()</b> functions from the  <b>dplyr</b>  library:
<b>#load <em>dplyr </em>library
library(dplyr)
#find sample size, mean, and standard deviation for each group
data %>%
  group_by(group) %>%
  summarise(
    count = n(),
    mean = mean(score),
    sd = sd(score)
  )
# A tibble: 2 x 4
#  group count  mean    sd
#     
#1 post     20  90.3  4.88
#2 pre      20  88.2  7.24</b>
We can also create  boxplots  using the <b>boxplot()</b> function in R to view the distribution of scores for the pre and post groups:
<b>boxplot(score~group,
  data=data,
  main="Test Scores by Group",
  xlab="Group",
  ylab="Score",
  col="steelblue",
  border="black"
)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/pair_t_test_r3.jpg">
From both the summary statistics and the boxplots, we can see that the mean score in the <em>post </em>group is slightly higher than the mean score in the <em>pre </em>group. We can also see that the scores for the <em>post </em>group have less variability than the scores in the <em>pre </em>group.
To find out if the difference between the means for these two groups is statistically significant, we can proceed to conduct a paired t-test.
<h3>Conduct a Paired t-test</h3>
Before we conduct the paired t-test, we should check that the distribution of differences is normally (or approximately normally) distributed. To do so, we can create a new vector defined as the difference between the pre and post scores, and perform a shapiro-wilk test for normality on this vector of values:
<b>#define new vector for difference between post and pre scores
differences &lt;- with(data, score[group == "post"] - score[group == "pre"])
#perform shapiro-wilk test for normality on this vector of values
shapiro.test(differences)
#Shapiro-Wilk normality test
#
#data:  differences
#W = 0.92307, p-value = 0.1135
#</b>
The p-value of the test is 0.1135, which is greater than alpha = 0.05. Thus, we fail to reject the null hypothesis that our data is normally distributed. This means we can now proceed to conduct the paired t-test.
We can use the following code to conduct a paired t-test:
<b>t.test(score ~ group, data = data, paired = TRUE)
#Paired t-test
#
#data:  score by group
#t = 1.588, df = 19, p-value = 0.1288
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# -0.6837307  4.9837307
#sample estimates:
#mean of the differences 
#                   2.15 
</b>
From the output, we can see that:
The test statistic <b>t </b>is <b>1.588</b>.
The p-value for this test statistic with 19 degrees of freedom (df) is <b>0.1288</b>.
The 95% confidence interval for the mean difference is <b>(-0.6837, 4.9837)</b>.
The mean difference between the scores for the pre and post group is <b>2.15</b>.
Thus, since our p-value is less than our significance level of 0.05 we will fail to reject the null hypothesis that the two groups have statistically significant means.
In other words, we do not have sufficient evidence to say that the mean scores between the pre and post groups are statistically significantly different. This means the study program had no significant effect on test scores.
In addition, our 95% confidence interval says that we are “95% confident” that the true mean difference between the two groups is between <b>-0.6837</b> and <b>4.9837</b>.
Since the value <em>zero</em> is contained in this confidence interval, this means that <em>zero </em>could in fact be the true difference between the mean scores, which is why we failed to reject the null hypothesis in this case.
<h2><span class="orange">How to Perform a Paired Samples t-test in SPSS</span></h2>
A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This tutorial explains how to conduct a paired samples t-test in SPSS.
<h3>Example: Paired samples t-test in SPSS</h3>
Researchers want to know if a new fuel treatment leads to a change in the average mpg of a certain car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with and without the fuel treatment.
Since each car receives the treatment, we can conduct a paired t-test in which each car is paired with itself to determine if there is a difference in average mpg with and without the fuel treatment, using the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>μ<b><sub>1</sub></b> = μ<b><sub>2 </sub></b>(average mpg between the two populations is equal)
<b>H<sub>1</sub>: </b>μ<b><sub>1</sub></b> ≠ μ<b><sub>2 </sub></b>(average mpg between the two populations is not equal)
The following screenshot shows the mpg for each car with (mpg1) and without (mpg2) fuel treatment:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pairedSPSS1.png">
Perform the following steps to conduct a paired t-test:
<b>Step 1: Choose the Paired-Samples T Test option.</b>
Click the <b>Analyze </b>tab, then <b>Compare Means</b>, then <b>Paired-Samples T Test</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pairedSPSS2.png">
<b>Step 2: Fill in the necessary values to perform the test.</b>
Drag <b>mpg1 </b>into the box under Variable1 and drag <b>mpg2 </b>into the box under Variable2. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pairedSPSS3.png">
<b>Step 3: Interpret the results.</b>
Once you click <b>OK</b>, the results of the paired sample t-test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pairedSPSS4.png">
The first table displays the following summary statistics for both groups:
<b>N: </b>The sample size of each group
<b>Mean: </b>The mean mpg of cars in each group
<b>Std. Deviation: </b>The standard deviation of the mpg of cars in each group
<b>Std. Error Mean: </b>The standard error of the mean mpg, calculated as s/√n
The last table shows the results of the paired samples t-test:
<b>t: </b>The test statistic, found to be -2.244
<b>df: </b>The degrees of freedom, calculated as #pairs-1 = 12-1 = 11
<b>Sig. (2-tailed): </b>The two-sided p-value that corresponds to a t value of -2.244 with df=11
Since the p-value of the test (.046) is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say that the true mean mpg is different between cars that receive treatment and cars that don’t.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our paired samples t-test. Here is an example of how to do so:
A paired t-test was conducted on 12 cars to determine if a new fuel treatment lead to a difference in mean miles per gallon.
 
Results showed that the mean mpg was statistically significantly different between the two groups (t = -2.244 w/ df=11, p = .046) at a significance level of 0.05.
 
A 95% confidence interval for the true difference in population means resulted in the interval of (-3.466, -.034).
<h2><span class="orange">How to Perform a Paired Samples t-test in Stata</span></h2>
A  <b>paired samples t-test</b>  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This tutorial explains how to conduct a paired samples t-test in Stata.
<h2>Example: Paired samples t-test in Stata</h2>
Researchers want to know if a new fuel treatment leads to a change in the average mpg of a certain car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with and without the fuel treatment.
Since each car receives the treatment, we can conduct a paired t-test in which each car is paired with itself to determine if there is a difference in average mpg with and without the fuel treatment.
Perform the following steps to conduct a paired t-test in Stata.
<b>Step 1: Load the data.</b>
First, load the data by typing <b>use http://www.stata-press.com/data/r13/fuel </b>in the command box and clicking Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pairedTestStata1.png">
<b>Step 2: View the raw data.</b>
Before we perform a paired t-test, let’s first view the raw data. Along the top menu bar, go to <b>Data > Data Editor > Data Editor (Browse)</b>. The first column, <em>mpg1</em>, shows the mpg for the first car without fuel treatment while the second column, <em>mpg2</em>, shows the mpg for the first car with the fuel treatment.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pairedTestStata2.png">
<b>Step 3: Perform a paired t-test.</b>
Along the top menu bar, go to <b>Statistics > Summaries, tables, and tests > Classical tests of hypotheses > t test (mean-comparison test)</b>.
Choose <em>Paired.</em> For First variable, choose <em>mpg1</em>. For Second variable, choose <em>mpg2</em>. For Confidence level, choose any level you’d like. A value of 95 corresponds to a significance level of 0.05. We will leave this at 95. Lastly, click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pairedTestStata3.png">
The results of the paired t-test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pairedTestStata4.png">
We are given the following information for each group:
Obs: </b>The number of observations. There are 12 observations in each group.
<b>Mean: </b>The mean mpg. In group 0, the mean is 21. In group 1, the mean is 22.75.
<b>Std. Err: </b>The standard error, calculated as σ / √n
<b>Std. Dev: </b>The standard deviation of mpg.
<b>95% Conf. Interval: </b>The 95% confidence interval for the true population mean of mpg.
<b>t: </b>The test statistic of the paired t-test.
<b>degrees of freedom: </b>The degrees of freedom to be used for the test, calculated as #pairs-1 = 12-1 = 11.
The p-values for three different two sample t-tests are displayed at the bottom of the results. Since we are interested in understanding if the average mpg is simply different between the two groups, we will look at the results of the middle test (in which the alternative hypothesis is Ha: diff !=0) which has a p-value of <b>0.0463</b>.
Since this value is smaller than our significance level of 0.05, we reject the null hypothesis. We have sufficient evidence to say that the true mean mpg is different between the two groups.
<b>Step 5: Report the results.</b>
Lastly, we will report the results of our paired t-test. Here is an example of how to do so:
A paired t-test was conducted on 12 cars to determine if a new fuel treatment lead to a difference in mean miles per gallon.
 
Results showed that the mean mpg was statistically significantly<em> </em>different between the two groups (t = -2.2444 w/ df=11, p = .0463) at a significance level of 0.05.
 
A 95% confidence interval for the true difference in population means resulted in the interval of (-3.466, -.034).
 
Based on these results, the new fuel treatment leads to a statistically significantly higher mpg for cars.
<h2><span class="orange">How to Perform a Paired Samples t-test on a TI-84 Calculator</span></h2>
A  <b>paired samples t-test</b>  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This tutorial explains how to conduct a paired t-test on a TI-84 calculator.
<h3>Example: Paired samples t-test on a TI-84 Calculator</h3>
Researchers want to know if a new fuel treatment leads to a change in the average mpg of a certain car. To test this, they conduct an experiment in which they measure the mpg of 11 cars with and without the fuel treatment.
Since each car receives the treatment, we can conduct a paired t-test in which each car is paired with itself to determine if there is a difference in average mpg with and without the fuel treatment.
Perform the following steps to conduct a paired t-test on a TI-84 calculator.
<b>Step 1: Input the data.</b>
First, we will input the data values for both samples. Press  Stat  and then press  EDIT . Enter the following values for the control group (no fuel treatment) in column L1 and the values for the treatment group variable (received fuel treatment) in column L2, followed by the difference between these two values in column L3.
<b>Note: </b>At the top of the third column, highlight L3. Then press  2nd  and  1  to create L1, followed by a minus sign, then press 2nd  and  2  to create L2. Then press Enter. Each of the values in column L3 will automatically populate using the formula L1-L2.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/pairedTestTI1.png">
<b>Step 2: Perform the paired t-test.</b>
To perform the paired t-test, we will simply perform a t-test on column L3, which contains the values for the paired differences.
Press Stat. Scroll over to <b>TESTS</b>. Scroll down to <b>2:T-Test</b> and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/pairedTestTI2.png">
The calculator will ask for the following information:
<b>Inpt: </b>Choose whether you are working with raw data (Data) or summary statistics (Stats). In this case, we will highlight Data and press ENTER.
<b>μ<sub>0</sub>:</b> The mean difference to be used in the null hypothesis. We will type 0 and press  ENTER.
<b>List: </b>The list that contains the differences between the two samples. We will type L3 and press  ENTER. <em>Note: To get L3 to appear, press 2nd and then press 3.</em>
<b>Freq: </b>The frequency. Leave this set to 1.
<b>μ</b>:The alternative hypothesis to be used. Since we are performing a two-tailed test, we will highlight<b> ≠μ<sub>0 </sub></b>and press ENTER. This indicates that our alternative hypothesis is μ≠0. The other two options would be used for left-tailed tests (&lt;μ<sub>0</sub>) and right-tailed tests (>μ<sub>0</sub>) .
Lastly, highlight Calculate and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/pairedTestTI3.png">
<b>Step 3: Interpret the results.</b>
Our calculator will automatically produce the results of the one-sample t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/pairedTestTI4.png">
Here is how to interpret the results:
<b>μ≠0</b>: This is the alternative hypothesis for the test.
<b>t=-1.8751</b>: This is the t test-statistic. 
<b>p=0.0903</b>: This is the p-value that corresponds to the test-statistic.
<b>x=-1.5455</b>. This is the mean difference of group 1 – group 2.
<b>s<sub>x</sub>=2.7336</b>. This is the standard deviation of the differences.
<b>n=11</b>: This is the total number of paired samples.
Because the  p-value  of the test (0.0903) is not less than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that there is any difference between the average mpg of the two groups. That is, we do not have sufficient evidence to say that the fuel treatment affects mpg.
<h2><span class="orange">Paired Samples t-test: Definition, Formula, and Example</span></h2>
A <b>paired samples t-test</b> is used to compare the means of two samples when each  observation  in one sample can be paired with an observation in the other sample.
This tutorial explains the following:
The motivation for performing a paired samples t-test.
The formula to perform a paired samples t-test.
The assumptions that should be met to perform a paired samples t-test.
An example of how to perform a paired samples t-test.
<h2>Paired Samples t-test: Motivation</h2>
A paired samples t-test is commonly used in two scenarios:
<b>1. A measurement is taken on a subject before and after some treatment</b> – e.g. the max vertical jump of college basketball players is measured before and after participating in a training program.
<b>2. A measurement is taken under two different conditions </b>– e.g. the response time of a patient is measured on two different drugs.
In both cases we are interested in comparing the mean measurement between two groups in which each observation in one sample can be paired with an observation in the other sample.
<h2>
<b>Paired Samples t-test: Formula</b>
</h2>
A paired samples t-test always uses the following null hypothesis:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
The alternative hypothesis can be either two-tailed, left-tailed, or right-tailed:
<b>H<sub>1</sub> (two-tailed): </b>μ<b><sub>1</sub></b> ≠ μ<sub>2</sub> (the two population means are not equal)
<b>H<sub>1</sub> (left-tailed): </b>μ<sub>1</sub> &lt; μ<sub>2</sub> (population 1 mean is less than population 2 mean)
<b>H<sub>1</sub> (right-tailed): </b>μ<b><sub>1</sub></b>> μ<sub>2</sub> (population 1 mean is greater than population 2 mean)
We use the following formula to calculate the test statistic t:
<b>t = x<sub>diff</sub> / (s<sub>diff</sub>/√n)</b>
where:
<b>x<sub>diff</sub>: </b>sample mean of the differences
<b>s: </b>sample standard deviation of the differences
<b>n: </b>sample size (i.e. number of pairs)
If the p-value that corresponds to the test statistic t with (n-1) degrees of freedom is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h2>Paired Samples t-test: Assumptions</h2>
For the results of a paired samples t-test to be valid, the following assumptions should be met:
The participants should be selected randomly from the population.
The differences between the pairs should be approximately normally distributed.
There should be no extreme outliers in the differences.
<h2>
<b>Paired Samples t-test: Example</b>
</h2>
Suppose we want to know whether or not a certain training program is able to increase the max vertical jump (in inches) of college basketball players.
To test this, we may recruit a  simple random sample  of 20 college basketball players and measure each of their max vertical jumps. Then, we may have each player use the training program for one month and then measure their max vertical jump again at the end of the month.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/pairedT1.png">
To determine whether or not the training program actually had an effect on max vertical jump, we will perform a paired samples t-test at significance level α = 0.05 using the following steps:
<b>Step 1: Calculate the summary data for the differences.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/pairedT2.png">
<b>x<sub>diff</sub>: </b>sample mean of the differences = <b>-0.95</b>
<b>s: </b>sample standard deviation of the differences = <b>1.317</b>
<b>n: </b>sample size (i.e. number of pairs) = <b>20</b>
<b>Step 2: Define the hypotheses.</b>
We will perform the paired samples t-test with the following hypotheses:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>1</sub>: </b>μ<sub>1</sub> ≠ μ<sub>2</sub> (the two population means are not equal)
<b>Step 3: Calculate the test statistic <em>t</em>.</b>
<b>t = x<sub>diff</sub> / (s<sub>diff</sub>/√n) </b> = -0.95 / (1.317/<b>√</b>20) = <b>-3.226</b>
<b>Step 4: Calculate the p-value of the test statistic <em>t</em>.</b>
According to the  T Score to P Value Calculator , the p-value associated with t = -3.226 and degrees of freedom = n-1 = 20-1 = 19 is <b>0.00445</b>.
<b>Step 5: Draw a conclusion.</b>
Since this p-value is less than our significance level α = 0.05, we reject the null hypothesis. We have sufficient evidence to say that the mean max vertical jump of players is different before and after participating in the training program.
<em><b>Note: </b>You can also perform this entire paired samples t-test by simply using the  Paired Samples t-test Calculator .</em>
<h2>Additional Resources</h2>
The following tutorials explain how to perform a paired samples t-test using different statistical programs:
 How to Perform a Paired Samples t-Test in Excel 
 How to Perform a Paired Samples t-test in SPSS 
 How to Perform a Paired Samples t-test in Stata 
 How to Perform a Paired Samples t-test on a TI-84 Calculator 
 How to Perform a Paired Samples t-test in R 
 How to Perform a Paired Samples t-Test in Python 
 How to Perform a Paired Samples t-Test by Hand 
<h2><span class="orange">The Three Assumptions Made in a Paired t-Test</span></h2>
A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
This type of test makes the following assumptions about the data:
<b>1. Independence: </b>Each observation should be independent of every other observation.
<b>2. Normality: </b>The differences between the pairs should be approximately normally distributed.
<b>3. No Extreme Outliers: </b>There should be no extreme outliers in the differences.
If one or more of these assumptions are violated, then the results of the paired samples t-test may be unreliable or misleading.
In this tutorial we provide an explanation of each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2>Assumption 1: Independence</h2>
A paired samples t-test makes the assumption that each  observation  is independent of every other observation.
<h3>How to Check this Assumption</h3>
The easiest way to check this assumption is to verify that each observation was collected using a  random sampling method .
If a random sampling method was used (such as simple random sampling) then we can assume that each observation is independent of every other observation.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated, the results of the paired samples t-test are completely invalid.
In this scenario, it’s best to collect new observations using a random sampling method to ensure that each observation is independent.
<h2>Assumption 2: Normality</h2>
A paired samples t-test assumes that the differences between the pairs should be approximately normally distributed.
This is a crucial assumption because if the differences between the pairs are not normally distributed then it isn’t valid to use the p-value from the test to draw conclusions.
<h3>How to Check this Assumption</h3>
The easiest way to check this assumption is to simply make a histogram of the paired differences and visually check whether or not the histogram exhibits a bell shape.
For example, if the histogram looks like this then we would say that the normality assumption is met:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape2.png">
However, if the histogram looks something like this then we would say the normality assumption is not met:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape6.png">
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated then we can perform a  Wilcoxon Signed Rank Test , which is considered the non-parametric equivalent to the paired samples t-test and does not make the assumption that the paired differences are normally distributed.
<h2>Assumption 3: No Extreme Outliers</h2>
A paired samples t-test makes the assumption that there are no extreme outliers in the data.
<h3>How to Check this Assumption</h3>
The easiest way to check this assumption is to create a boxplot of the paired differences and visually check if there are any outliers.
For example, suppose the boxplot of paired differences looks like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/pairedAssumption1.jpg"431">
Most of the paired differences are around zero, but there is one paired difference equal to about 19, which is a clear outlier.
<b>Note</b>: A circle is typically used in a boxplot to indicate an outlier value.
However, suppose the boxplot of paired differences looked like this instead:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/pairedAssumption2.jpg"411">
There are no clear outliers in this boxplot so we would assume that there are no extreme outliers in the data.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated, then the results of the paired samples t-test could be unusually effected by the outlier.
In this scenario, you can remove the outlier if you suspect that it represents a faulty data point or is the result of a data entry error.
Alternatively, you can keep the outlier and simply make a note of it when reporting the results of the paired samples t-test.
<h2><span class="orange">How to Perform a Paired t-Test by Hand</span></h2>
A  paired samples t-test  is used to compare the means of two samples when each  observation  in one sample can be paired with an observation in the other sample.
The following step-by-step example shows how to perform a paired samples t-test to determine if the population means are equal between the following two groups:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairHand1.png">
<h3>Step 1: Calculate the Test Statistic</h3>
The test statistic of a paired t-test is calculated as:
<b>t = x<sub>diff</sub> / (s<sub>diff</sub>/√n)</b>
where:
<b>x<sub>diff</sub>: </b>sample mean of the differences
<b>s: </b>sample standard deviation of the differences
<b>n: </b>sample size (i.e. number of pairs)
We will calculate the mean of the differences between the two groups and the standard deviation of the differences between the two groups:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairHand2.png">
Thus, our test statistic can be calculated as:
t = x<sub>diff</sub> / (s<sub>diff</sub>/√n)
t = 1.75 / (1.422/√12)
t = <b>4.26</b>
<h3>Step 2: Calculate the Critical Value</h3>
Next, we need to find the critical value to compare our test statistic to.
For this example, we’ll use a two-tailed test with α = .05 and df = n-1 degrees of freedom.
According to the  t-Distribution table , the critical value that corresponds to these values is <b>2.201</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairHand3.png">
<h3>Step 3: Reject or Fail to Reject the Null Hypothesis</h3>
Our paired samples t-test uses the following null and alternative hypothesis:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>A</sub>: </b>μ<b><sub>1</sub></b> ≠ μ<sub>2</sub> (the two population means are not equal)
Since the absolute value of our test statistic (<b>4.26</b>) is greater than the critical value found in the t-table (<b>2.201</b>), we reject the null hypothesis.
This means we have sufficient evidence to say that the mean between the two groups is not equal.
<b>Bonus:</b> Feel free to use the  Paired Samples t-test Calculator  to confirm your results.
<h2><span class="orange">How to Perform a Paired Samples t-Test in SAS</span></h2>
A  paired samples t-test  is used to compare the means of two samples when each  observation  in one sample can be paired with an observation in the other sample.
This tutorial explains how to perform a paired samples t-test in SAS.
<h3>Example: Paired Samples t-Test in SAS</h3>
Suppose a professor wants to determine if a certain study program affects test scores. To test this, he  randomly selects  15 students to take a pre-test. Then, he has each student use the study program for one month and then a post-test of similar difficulty.
The test scores for each of the 15 students are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/paired1.jpg"304">
To compare the difference between the mean scores on the pre-test and post-test, the professor can use a paired samples t-test because for each student their pre-test score can be paired with their post-test score.
Use the following steps to perform this paired samples t-test in SAS:
<b>Step 1: Create the Data</b>
First, let’s use the following code to create the dataset in SAS:
<b>/*create dataset*/
data test_scores;
    input pre post;
    datalines;
88 91
82 84
84 88
93 90
75 79
78 80
84 88
87 90
95 90
91 96
83 88
89 89
77 81
68 74
91 92
;
run;
/*view dataset*/
proc print data=test_scores;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/paired2.jpg"126">
<b>Step 2: Perform the Paired Samples t-test</b>
Next, we can use <b>proc ttest</b> to perform the paired samples t-test:
<b>/*perform paired samples t-test*/
proc ttest data=test_scores alpha=.05;
    paired pre*post;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/paired3.jpg">
From the output we can see the following:
Mean difference between pre-test and post-test score: <b>-2.3333</b>
95% Confidence Interval for Mean difference: <b>[-4.0165, -.6502]</b>
We can also see the t test statistic and corresponding two-sided p-value:
t test statistic: <b>-2.97</b>
p-value: <b>.0101</b>
In this example, the paired samples t-test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>: </b>The mean pre-test and post-test scores are equal
<b>H<sub>A</sub>: </b>The mean pre-test and post-test scores are <em>not</em> equal
Since the p-value (<b>0.0101</b>) is less than 0.05, we reject the null hypothesis.
This means we have sufficient evidence to say that the true mean test score is different for students before and after participating in the study program.
<h2><span class="orange">Paired vs. Unpaired t-test: What’s the Difference?</span></h2>
In statistics, there are two types of <b>two sample t-tests</b>:
<b>Paired t-test:</b> Used to compare the means of two samples when each individual in one sample also appears in the other sample.
<b>Unpaired t-test:</b> Used to compare the means of two samples when each individual in one sample is independent of every individual in the other sample.
<em><b>Note:</b> An unpaired t-test is more commonly called an independent samples t-test.</em>
For example, suppose a professor wants to determine whether or not two different studying techniques lead to different mean exam scores.
To perform a <b>paired t-test</b>, he could recruit 10 students and have them use one studying technique for one month and take an exam, then have them use the second studying technique for one month and take another exam of equal difficulty.
Here’s what the data would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/paired_unpaired1.png">
Since each student appears in each group, the professor would perform a paired t-test to determine if the mean scores are different between the two groups.
To perform an <b>unpaired t-test</b>, he could recruit 20 total students and randomly split them into two groups of 10. He could assign one group to use one studying technique for one month and assign the other group to use the second studying technique for one month and have all students take the same exam.
Here’s what the data would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/paired_unpaired2.png">
Since the students in one group are completely independent of the students in the other group, the professor would perform an unpaired t-test to determine if the mean scores are different between the two groups.
<h3>Assumptions</h3>
Paired and unpaired t-tests both make the following assumptions:
The data in both samples was obtained using a  random sampling method .
The data in both samples should be roughly normally distributed.
There should be no extreme outliers in either sample.
These assumptions should be checked before performing either t-test to ensure that the results of the test are reliable.
<h3>Pros & Cons</h3>
The paired t-test offers the following <b>pros</b>:
<b>A smaller sample size is required.</b> Notice that the paired t-test in the previous example only required 10 total students while the unpaired t-test required 20 total students.
<b>Each sample contains individuals with the same characteristics.</b> The two groups are guaranteed to have individuals with equal ability, intellect, etc. because the same individuals appear in each group.
However, a paired t-test comes with the following potential <b>cons</b>:
<b>The potential for sample size reduction.</b> If an individual drops out of the study, the sample size of <em>each</em> group is reduced by one since that individual appears in each group.
<b>The potential for order effects.</b>  Order effects  refer to differences in outcomes between the two groups due to the order that treatments were presented to individuals. For example, an individual may score higher on the second exam simply due to the fact that they improved their exam-taking abilities rather than due to the studying technique.
Keep these pros and cons in mind when deciding to use a paired vs. unpaired t-test.
 An Introduction to the Paired Samples t-test 
 How to Perform a Paired Samples t-Test in Excel 
And use the following tutorials to gain a better understanding of unpaired t-tests (AKA independent samples t-tests):
 An Introduction to the Two Sample t-test 
 How to Perform a Two Sample t-Test in Excel 
<h2><span class="orange">How to Create a Pairs Plot in Python</span></h2>
A <b>pairs plot</b> is a matrix of  scatterplots  that lets you understand the pairwise relationship between different variables in a dataset.
The easiest way to create a pairs plot in Python is to use the  seaborn.pairplot(df)  function.
The following examples show how to use this function in practice.
<h3>Example 1: Pairs Plot for All Variables</h3>
The following code shows how to create a pairs plot for every numeric variable in the seaborn dataset called <b>iris</b>:
<b>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#define dataset
iris = sns.load_dataset("iris")
#create pairs plot for all numeric variables
sns.pairplot(iris)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairsPython1-1.png">
The way to interpret the matrix is as follows:
The distribution of each variable is shown as a histogram along the diagonal boxes.
All other boxes display a scatterplot of the relationship between each pairwise combination of variables. For example, the box in the bottom left corner of the matrix displays a scatterplot of values for <b>petal_width </b>vs. <b>sepal_length</b>.
This single plot gives us an idea of the relationship between each pair of variables in our dataset.
<h3>Example 2: Pairs Plot for Specific Variables</h3>
We can also specify only certain variables to include in the pairs plot:
<b>sns.pairplot(iris[['sepal_length', 'sepal_width']])
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairsPython2.png">
<h3>Example 3: Pairs Plot with Color by Category</h3>
We can also create a pairs plot that colors each point in each plot based on some categorical variable using the <b>hue</b> argument:
<b>sns.pairplot(iris, hue='species')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pairsPython3.png">
By using the <b>hue</b> argument, we can gain an even better understanding of the data.
<h2><span class="orange">How to Create and Interpret Pairs Plots in R</span></h2>
A <b>pairs plot </b>is a matrix of scatterplots that lets you understand the pairwise relationship between different variables in a dataset.
Fortunately it’s easy to create a pairs plot in R by using the  pairs()  function. This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Pairs Plot of All Variables</h3>
The following code illustrates how to create a basic pairs plot for all variables in a data frame in R:
<b>#make this example reproducible 
set.seed(0)
#create data frame 
var1 &lt;- rnorm(1000)
var2 &lt;- var1 + rnorm(1000, 0, 2)
var3 &lt;- var2 - rnorm(1000, 0, 5)
 
df &lt;- data.frame(var1, var2, var3)
#create pairs plot 
pairs(df)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pairsR1-1.png">
The way to interpret the matrix is as follows:
The variable names are shown along the diagonals boxes.
All other boxes display a scatterplot of the relationship between each pairwise combination of variables. For example, the box in the top right corner of the matrix displays a scatterplot of values for <b>var1 </b>and <b>var3</b>. The box in the middle left displays a scatterplot of values for <b>var1 </b>and <b>var2</b>, and so on.
This single plot gives us an idea of the relationship between each pair of variables in our dataset. For example, <b>var1</b> and <b>var2</b> seem to be positively correlated while <b>var1</b> and <b>var3</b> seem to have little to no correlation.
<h3>Example 2: Pairs Plot of Specific Variables</h3>
The following code illustrates how to create a basic pairs plot for just the first two variables in a dataset:
<b>#create pairs plot for var1 and var2 only
pairs(df[, 1:2])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pairsR.png">
<h3>Example 3: Modify the Aesthetics of a Pairs Plot</h3>
The following code illustrates how to modify the aesthetics of a pairs plot, including the title, the color, and the labels:
<b>pairs(df,
      col = 'blue', #modify color
      labels = c('First', 'Second', 'Third'), #modify labels
      main = 'Custom Title') #modify title</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pairsR2.png">
<h3>Example 4: Obtaining Correlations with ggpairs</h3>
You can also obtain the  Pearson correlation coefficient  between variables by using the <b>ggpairs()</b> function from the GGally library. The following code illustrates how to use this function:
<b>#install necessary libraries
install.packages('ggplot2')
install.packages('GGally')
#load libraries
library(ggplot2)
library(GGally)
#create pairs plot
ggpairs(df)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/pairsR3.png">
The way to interpret this matrix is as follows:
The variable names are displayed on the outer edges of the matrix.
The boxes along the diagonals display the density plot for each variable.
The boxes in the lower left corner display the scatterplot between each variable.
The boxes in the upper right corner display the Pearson correlation coefficient between each variable. For example, the correlation between var1 and var2 is <b>0.425</b>.
The benefit of using <b>ggpairs() </b>over the base R function <b>pairs()</b> is that you can obtain more information about the variables. Specifically, you can see the correlation coefficient between each pairwise combination of variables as well as a density plot for each individual variable.
<em>You can find the complete documentation for the ggpairs() function  here .</em>
<h2><span class="orange">How to Perform Post-Hoc Pairwise Comparisons in R</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
A one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: All group means are equal.
<b>H<sub>A</sub></b>: Not all group means are equal.
If the overall  p-value  of the ANOVA is less than a certain significance level (e.g. α = .05) then we reject the null hypothesis and conclude that not all of the group means are equal.
In order to find out which group means are different, we can then perform <b>post-hoc pairwise comparisons</b>.
The following example shows how to perform the following post-hoc pairwise comparisons in R:
The Tukey Method
The Scheffe Method
The Bonferroni Method
The Holm Method
<h3>Example: One-Way ANOVA in R</h3>
Suppose a teacher wants to know whether or not three different studying techniques lead to different exam scores among students. To test this, she  randomly assigns  10 students to use each studying technique and records their exam scores.
We can use the following code in R to perform a one-way ANOVA to test for differences in mean exam scores between the three groups:
<b>#create data frame
df &lt;- data.frame(technique = rep(c("tech1", "tech2", "tech3"), each=10), score = c(76, 77, 77, 81, 82, 82, 83, 84, 85, 89,           81, 82, 83, 83, 83, 84, 87, 90, 92, 93,           77, 78, 79, 88, 89, 90, 91, 95, 95, 98))
#perform one-way ANOVA
model &lt;- aov(score ~ technique, data = df)
#view output of ANOVA
summary(model)
            Df Sum Sq Mean Sq F value Pr(>F)  
technique    2  211.5  105.73   3.415 0.0476 *
Residuals   27  836.0   30.96                 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
The overall p-value of the ANOVA (.0476) is less than α = .05 so we’ll reject the null hypothesis that the mean exam score is the same for each studying technique.
We can proceed to perform post-hoc pairwise comparisons to determine which groups have different means.
<h3>The Tukey Method</h3>
The Tukey post-hoc method is best to use when the sample size of each group is equal.
We can use the built-in <b>TukeyHSD()</b> function to perform the Tukey post-hoc method in R:
<b>#perform the Tukey post-hoc method
TukeyHSD(model, conf.level=.95)
  Tukey multiple comparisons of means
    95% family-wise confidence level
Fit: aov(formula = score ~ technique, data = df)
$technique
            diff        lwr       upr     p adj
tech2-tech1  4.2 -1.9700112 10.370011 0.2281369
tech3-tech1  6.4  0.2299888 12.570011 0.0409017
tech3-tech2  2.2 -3.9700112  8.370011 0.6547756</b>
From the output we can see that the only p-value (“<b>p adj</b>“) less than .05 is for the difference between technique and technique 3.
Thus, we would conclude that there is only a statistically significant difference in mean exam scores between students who used technique 1 and technique 3.
<h3>The Scheffe Method</h3>
The Scheffe method is the most conservative post-hoc pairwise comparison method and produces the widest confidence intervals when comparing group means.
We can use the<b> ScheffeTest()</b> function from the  DescTools  package to perform the Scheffe post-hoc method in R:
<b>library(DescTools)
#perform the Scheffe post-hoc method
ScheffeTest(model)
  Posthoc multiple comparisons of means: Scheffe Test 
    95% family-wise confidence level
$technique
            diff      lwr.ci    upr.ci   pval    
tech2-tech1  4.2 -2.24527202 10.645272 0.2582    
tech3-tech1  6.4 -0.04527202 12.845272 0.0519 .  
tech3-tech2  2.2 -4.24527202  8.645272 0.6803    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 156</b>
From the output we can see that there are no p-values less than .05, so we would conclude that there is no statistically significant difference in mean exam scores among any groups.
<h3>The Bonferroni Method</h3>
The Bonferroni method is best to use when you have a set of planned pairwise comparisons you’d like to make.
We can use the following syntax in R to perform the Bonferroni post-hoc method: 
<b>#perform the Bonferroni post-hoc method
pairwise.t.test(df$score, df$technique, p.adj='bonferroni')
Pairwise comparisons using t tests with pooled SD 
data:  df$score and df$technique 
      tech1 tech2
tech2 0.309 -    
tech3 0.048 1.000
P value adjustment method: bonferroni</b>
From the output we can see that the only p-value less than .05 is for the difference between technique and technique 3.
Thus, we would conclude that there is only a statistically significant difference in mean exam scores between students who used technique 1 and technique 3.
<h3>The Holm Method</h3>
The Holm method is also used when you have a set of planned pairwise comparisons you’d like to make beforehand and it tends to have even higher power than the Bonferroni method, so it’s often preferred.
We can use the following syntax in R to perform the Holm post-hoc method: 
<b>#perform the Holm post-hoc method
pairwise.t.test(df$score, df$technique, p.adj='holm')
Pairwise comparisons using t tests with pooled SD 
data:  df$score and df$technique 
      tech1 tech2
tech2 0.206 -    
tech3 0.048 0.384
P value adjustment method: holm </b>
From the output we can see that the only p-value less than .05 is for the difference between technique and technique 3.
Thus, again we would conclude that there is only a statistically significant difference in mean exam scores between students who used technique 1 and technique 3.
<h2><span class="orange">How to Create a 3D Pandas DataFrame (With Example)</span></h2>
You can use the  xarray  module to quickly create a 3D pandas DataFrame.
This tutorial explains how to create the following 3D pandas DataFrame using functions from the xarray module:
<b>              product_A  product_B  product_C
year quarter                                 
2021 Q1        1.624345   0.319039         50
     Q2       -0.611756   0.319039         50
     Q3       -0.528172   0.319039         50
     Q4       -1.072969   0.319039         50
2022 Q1        0.865408  -0.249370         50
     Q2       -2.301539  -0.249370         50
     Q3        1.744812  -0.249370         50
     Q4       -0.761207  -0.249370         50
</b>
<h2>Example: Create 3D Pandas DataFrame</h2>
The following code shows how to create a 3D dataset using functions from <b>xarray</b> and <b>NumPy</b>:
<b>import numpy as np
import xarray as xr
#make this example reproducible
np.random.seed(1)
#create 3D dataset
xarray_3d = xr.Dataset(
    {"product_A": (("year", "quarter"), np.random.randn(2, 4))},
    coords={
        "year": [2021, 2022],
        "quarter": ["Q1", "Q2", "Q3", "Q4"],
        "product_B": ("year", np.random.randn(2)),
        "product_C": 50,
    },
)
#view 3D dataset
print(xarray_3d)
Dimensions:    (year: 2, quarter: 4)
Coordinates:
  * year       (year) int32 2021 2022
  * quarter    (quarter) &lt;U2 'Q1' 'Q2' 'Q3' 'Q4'
    product_B  (year) float64 0.319 -0.2494
    product_C  int32 50
Data variables:
    product_A  (year, quarter) float64 1.624 -0.6118 -0.5282 ... 1.745 -0.7612</b>
<b>Note</b>: The NumPy  randn()  function returns sample values from the  standard normal distribution .
We can then use the <b>to_dataframe()</b> function to convert this dataset to a pandas DataFrame:
<b>#convert xarray to DataFrame
df_3d = xarray_3d.to_dataframe()
#view 3D DataFrame
print(df_3d)
              product_A  product_B  product_C
year quarter                                 
2021 Q1        1.624345   0.319039         50
     Q2       -0.611756   0.319039         50
     Q3       -0.528172   0.319039         50
     Q4       -1.072969   0.319039         50
2022 Q1        0.865408  -0.249370         50
     Q2       -2.301539  -0.249370         50
     Q3        1.744812  -0.249370         50
     Q4       -0.761207  -0.249370         50</b>
The result is a 3D pandas DataFrame that contains information on the number of sales made of three different products during two different years and four different quarters per year.
We can use the <b>type()</b> function to confirm that this object is indeed a pandas DataFrame:
<b>#display type of df_3d
type(df_3d)
pandas.core.frame.DataFrame
</b>
The object is indeed a pandas DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in pandas:
 Pandas: How to Find Unique Values in a Column 
 Pandas: How to Find the Difference Between Two Rows 
 Pandas: How to Count Missing Values in DataFrame 
<h2><span class="orange">Pandas: How to Add Column from One DataFrame to Another</span></h2>
You can use one of the following two methods to add a column from one pandas DataFrame to another DataFrame:
<b>Method 1: Add Column from One DataFrame to Last Column Position in Another</b>
<b>#add some_col from df2 to last column position in df1
df1['some_col']= df2['some_col']
</b>
<b>Method 2: Add Column from One DataFrame to Specific Position in Another</b>
<b>#insert some_col from df2 into third column position in df1
df1.insert(2, 'some_col', df2['some_col'])</b>
The following examples show how to use each method in practice with the following pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B'],    'position': ['G', 'G', 'F', 'C', 'G', 'C'],    'points': [4, 4, 6, 8, 9, 5]})
#view DataFrame
print(df1)
  team position  points
0    A        G       4
1    A        G       4
2    A        F       6
3    A        C       8
4    B        G       9
5    B        C       5
#create second DataFrame
df2 = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B'],    'rebounds': [12, 7, 8, 8, 5, 11]})
#view DataFrame
print(df2)
  team  rebounds
0    A        12
1    A         7
2    A         8
3    A         8
4    B         5
5    B        11</b>
<h3>Example 1: Add Column from One DataFrame to Last Column Position in Another</h3>
The following code shows how to add the <b>rebounds</b> column from the second DataFrame to the last column position of the first DataFrame:
<b>#add rebounds column from df2 to df1
df1['rebounds']= df2['rebounds']
#view updated DataFrame
print(df1)
  team position  points  rebounds
0    A        G       4        12
1    A        G       4         7
2    A        F       6         8
3    A        C       8         8
4    B        G       9         5
5    B        C       5        11</b>
Notice that the <b>rebounds</b> column from the second DataFrame has been added to the last column position of the first DataFrame.
<h3>Example 2: Add Column from One DataFrame to Specific Column Position in Another</h3>
The following code shows how to add the <b>rebounds</b> column from the second DataFrame to the third column position of the first DataFrame:
<b>#insert rebounds column from df2 into third column position of df1
df1.insert(2, 'rebounds', df2['rebounds'])
#view updated DataFrame
print(df1)
  team position  rebounds  points
0    A        G        12       4
1    A        G         7       4
2    A        F         8       6
3    A        C         8       8
4    B        G         5       9
5    B        C        11       5
</b>
Notice that the <b>rebounds</b> column from the second DataFrame has been added to the third column position of the first DataFrame.
<h2><span class="orange">Pandas: How to Add Column with Constant Value</span></h2>
You can use the following methods to add a column with a constant value to a pandas DataFrame:
<b>Method 1: Add One Column with Constant Value</b>
<b>df['new'] = 5
</b>
<b>Method 2: Add Multiple Columns with Same Constant Value</b>
<b>df[['new1', 'new2', 'new3']] = 5</b>
<b>Method 3: Add Multiple Columns with Different Constant Values</b>
<b>#define dictionary of new values
new_constants = {'new1': 5, 'new2': 10, 'new3': 15}
#add multiple columns with different constant values
df = df.assign(**new_constants)</b>
The following examples show how to use each method with the following pandas DataFrames:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
<h2>Example 1: Add One Column with Constant Value</h2>
The following code shows how to add one column with a value of 5 for each row:
<b>#add column with constant value
df['new'] = 5
#view updated DataFrame
print(df)
  team  points  assists  new
0    A      18        5    5
1    B      22        7    5
2    C      19        7    5
3    D      14        9    5
4    E      14       12    5
5    F      11        9    5
6    G      20        9    5
7    H      28        4    5
</b>
The new column called <b>new </b>is filled with the constant value of 5 for each row.
<h2>Example 2: Add Multiple Columns with Same Constant Value</h2>
The following code shows how to add multiple columns that all have the same constant value of 5:
<b>#add three new columns each with a constant value of 5
df[['new1', 'new2', 'new3']] = 5
#view updated DataFrame
print(df)
  team  points  assists  new1  new2  new3
0    A      18        5     5     5     5
1    B      22        7     5     5     5
2    C      19        7     5     5     5
3    D      14        9     5     5     5
4    E      14       12     5     5     5
5    F      11        9     5     5     5
6    G      20        9     5     5     5
7    H      28        4     5     5     5</b>
Notice that each new column contains the value 5 in each row.
<h2>Example 3: Add Multiple Columns with Different Constant Values</h2>
The following code shows how to add multiple columns with different constant values:
<b>#define dictionary of new values
new_constants = {'new1': 5, 'new2': 10, 'new3': 15}
#add multiple columns with different constant values
df = df.assign(**new_constants)
#view updated DataFrame
print(df)
  team  points  assists  new1  new2  new3
0    A      18        5     5    10    15
1    B      22        7     5    10    15
2    C      19        7     5    10    15
3    D      14        9     5    10    15
4    E      14       12     5    10    15
5    F      11        9     5    10    15
6    G      20        9     5    10    15
7    H      28        4     5    10    15
</b>
Notice that each of the three new columns have a different constant value.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Add Empty Column to Pandas DataFrame 
 How to Change Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Add a Count Column to a Pandas DataFrame</span></h2>
You can use the following basic syntax to add a ‘count’ column to a pandas DataFrame:
<b>df['var1_count'] = df.groupby('var1')['var1'].transform('count')
</b>
This particular syntax adds a column called <b>var1_count</b> to the DataFrame that contains the count of values in the column called <b>var1</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Add Count Column in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'pos': ['Gu', 'Fo', 'Fo', 'Fo', 'Gu', 'Gu', 'Fo', 'Fo'],   'points': [18, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team pos  points
0    A  Gu      18
1    A  Fo      22
2    A  Fo      19
3    B  Fo      14
4    B  Gu      14
5    B  Gu      11
6    B  Fo      20
7    B  Fo      28</b>
We can use the following code to add a column called <b>team_count</b> that contains the count of each team:
<b>#add column that shows total count of each team
df['team_count'] = df.groupby('team')['team'].transform('count')
#view updated DataFrame
print(df)
  team pos  points  team_count
0    A  Gu      18           3
1    A  Fo      22           3
2    A  Fo      19           3
3    B  Fo      14           5
4    B  Gu      14           5
5    B  Gu      11           5
6    B  Fo      20           5
7    B  Fo      28           5</b>
There are <b>3</b> rows with a team value of A and <b>5</b> rows with a team value of B.
Thus:
For each row where the team is equal to A, the value in the <b>team_count</b> column is <b>3</b>.
For each row where the team is equal to B, the value in the <b>team_count</b> column is <b>5</b>.
You can also add a ‘count’ column that groups by multiple variables.
For example, the following code shows how to add a ‘count’ column that groups by the <b>team</b> and <b>pos</b> variables:
<b>#add column that shows total count of each team and position
df['team_pos_count'] = df.groupby(['team', 'pos')['team'].transform('count')
#view updated DataFrame
print(df)
  team pos  points  team_pos_count
0    A  Gu      18               1
1    A  Fo      22               2
2    A  Fo      19               2
3    B  Fo      14               3
4    B  Gu      14               2
5    B  Gu      11               2
6    B  Fo      20               3
7    B  Fo      28               3
</b>
From the output we can see:
There is <b>1</b> row that contains A in the <b>team</b> column and Gu in the <b>pos</b> column.
There are <b>2</b> rows that contain A in the <b>team</b> column and Fo in the <b>pos</b> column.
There are <b>3</b> rows that contain B in the <b>team</b> column and Fo in the <b>pos</b> column.
There are <b>2</b> rows that contain B in the <b>team</b> column and Gu in the <b>pos</b> column.
<h2><span class="orange">How to Add Empty Column to Pandas DataFrame (3 Examples)</span></h2>
You can use the following methods to add empty columns to a pandas DataFrame:
<b>Method 1: Add One Empty Column with Blanks</b>
<b>df['empty_column'] = ""
</b>
<b>Method 2: Add One Empty Column with NaN Values</b>
<b>df['empty_column'] = np.nan</b>
<b>Method 3: Add Multiple Empty Columns with NaN Values</b>
<b>df[['empty1', 'empty2', 'empty3']] = np.nan</b>
The following examples show how to use each method with the following pandas DataFrames:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
<h2>Example 1: Add One Empty Column with Blanks</h2>
The following code shows how to add one empty column with all blank values:
<b>#add empty column
df['blanks'] = ""
#view updated DataFrame
print(df)
  team  points  assists blanks
0    A      18        5       
1    B      22        7       
2    C      19        7       
3    D      14        9       
4    E      14       12       
5    F      11        9       
6    G      20        9       
7    H      28        4   </b>
The new column called <b>blanks</b> is filled with blank values.
<h2>Example 2: Add One Empty Column with NaN Values</h2>
The following code shows how to add one empty column with all NaN values:
<b>import numpy as np
#add empty column with NaN values
df['empty'] = np.nan
#view updated DataFrame
print(df)
  team  points  assists  empty
0    A      18        5    NaN
1    B      22        7    NaN
2    C      19        7    NaN
3    D      14        9    NaN
4    E      14       12    NaN
5    F      11        9    NaN
6    G      20        9    NaN
7    H      28        4    NaN</b>
The new column called <b>empty </b>is filled with NaN values.
<h2>Example 3: Add Multiple Empty Columns with NaN Values</h2>
The following code shows how to add multiple empty columns with all NaN values:
<b>import numpy as np
#add three empty columns with NaN values
df[['empty1', 'empty2', 'empty3']] = np.nan
#view updated DataFrame
print(df)
  team  points  assists  empty1  empty2  empty3
0    A      18        5     NaN     NaN     NaN
1    B      22        7     NaN     NaN     NaN
2    C      19        7     NaN     NaN     NaN
3    D      14        9     NaN     NaN     NaN
4    E      14       12     NaN     NaN     NaN
5    F      11        9     NaN     NaN     NaN
6    G      20        9     NaN     NaN     NaN
7    H      28        4     NaN     NaN     NaN</b>
Notice that all three of the new columns are filled with NaN values.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Add Leading Zeros to Strings in Pandas</span></h2>
You can use the following syntax to add leading zeros to strings in a pandas DataFrame:
<b>df['ID'] = df['ID'].apply('{:0>7}'.format)
</b>
This particular formula adds as many leading zeros as necessary to the strings in the column titled ‘ID’ until each string has a length of <b>7</b>.
Feel free to replace the <b>7</b> with another value to add a different number of leading zeros.
The following example shows how to use this syntax in practice.
<h3>Example: Add Leading Zeros to Strings in Pandas</h3>
Suppose we have the following pandas DataFrame that contains information about sales and refunds for various stores:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'ID': ['A25', 'B300', 'C6', 'D447289', 'E416', 'F19'],   'sales': [18, 12, 27, 30, 45, 23],   'refunds': [1, 3, 3, 2, 5, 0]})
#view DataFrame
print(df)
        ID  sales  refunds
0      A25     18        1
1     B300     12        3
2       C6     27        3
3  D447289     30        2
4     E416     45        5
5      F19     23        0
</b>
Notice that the length of the strings in the ‘ID’ column are not all equal.
However, we can see that the longest string is <b>7</b> characters.
We can use the following syntax to add leading zeros to the strings in the ‘ID’ column so that each string has a length of <b>7</b>:
<b>#add leading zeros to 'ID' column
df['ID'] = df['ID'].apply('{:0>7}'.format)
#view updated DataFrame
print(df)
        ID  sales  refunds
0  0000A25     18        1
1  000B300     12        3
2  00000C6     27        3
3  D447289     30        2
4  000E416     45        5
5  0000F19     23        0
</b>
Notice that leading zeros have been added to the strings in the ‘ID’ column so that each string now has the same length.
<b>Note</b>: You can find the complete documentation for the <b>apply </b>function in pandas  here .
<h2><span class="orange">How to Add Multiple Columns to Pandas DataFrame</span></h2>
You can use the following methods to add multiple columns to a pandas DataFrame:
<b>Method 1: Add Multiple Columns that Each Contain One Value</b>
<b>df[['new1', 'new2', 'new3']] = pd.DataFrame([[4, 'hey', np.nan]], index=df.index)
</b>
<b>Method 2: Add Multiple Columns that Each Contain Multiple Values</b>
<b>df['new1'] = [1, 5, 5, 4, 3, 6]
df['new2'] = ['hi', 'hey', 'hey', 'hey', 'hello', 'yo']
df['new3'] = [12, 4, 4, 3, 6, 7]</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9]})
#view DataFrame
df
        teampointsassists
0A185
1B227
2C197
3D149
4E1412
5F119
</b>
<h3>Method 1: Add Multiple Columns that Each Contain One Value</h3>
The following code shows how to add three new columns to the pandas DataFrame in which each new column only contains one value:
<b>#add three new columns to DataFrame
df[['new1', 'new2', 'new3']] = pd.DataFrame([[4, 'hey', np.nan]], index=df.index)
#view updated DataFrame
df
        teampointsassistsnew1new2new3
0A1854heyNaN
1B2274heyNaN
2C1974heyNaN
3D1494heyNaN
4E14124heyNaN
5F1194heyNaN
</b>
Notice that three new columns – <b>new1</b>, <b>new2</b>, and <b>new3</b> – have been added to the DataFrame.
Also notice that each new column contains only one specific value.
<h3>Method 2: Add Multiple Columns that Each Contain Multiple Values</h3>
The following code shows how to add three new columns to the pandas DataFrame in which each new column contains multiple values:
<b>#add three new columns to DataFrame
df['new1'] = [1, 5, 5, 4, 3, 6]
df['new2'] = ['hi', 'hey', 'hey', 'hey', 'hello', 'yo']
df['new3'] = [12, 4, 4, 3, 6, 7]
#view updated DataFrame
df
teampointsassistsnew1new2new3
0A1851hi12
1B2275hey4
2C1975hey4
3D1494hey3
4E14123hello6
5F1196yo7
</b>
Notice that three new columns – <b>new1</b>, <b>new2</b>, and <b>new3</b> – have been added to the DataFrame.
Also notice that each new column contains multiple values.
<h2><span class="orange">Pandas: How to Add Prefix to Column Names</span></h2>
You can use the following methods to add a prefix to column names in a pandas DataFrame:
<b>Method 1: Add Prefix to All Column Names</b>
<b>df = df.add_prefix('my_prefix_')
</b>
<b>Method 2: Add Prefix to Specific Column Names</b>
<b>#specify columns to add prefix to
cols = ['col1', 'col3']
#add prefix to specific columns
df = df.rename(columns={c: 'my_prefix_'+c for c in df.columns if c in cols})</b>
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5],   'blocks': [6, 6, 3, 2, 7, 9]})
#view DataFrame
print(df)
   points  assists  rebounds  blocks
0      25        5        11       6
1      12        7         8       6
2      15        7        10       3
3      14        9         6       2
4      19       12         6       7
5      23        9         5       9
</b>
<h2>Method 1: Add Prefix to All Column Names</h2>
The following code shows how to add the prefix ‘_total’ to all column names:
<b>#add 'total_' as prefix to each column name
df = df.add_prefix('total_')
#view updated DataFrame
print(df)
   total_points  total_assists  total_rebounds  total_blocks
0            25              5              11             6
1            12              7               8             6
2            15              7              10             3
3            14              9               6             2
4            19             12               6             7
5            23              9               5             9</b>
Notice that the prefix ‘_total’ has been added to all column names.
<h2>Method 2: Add Prefix to Specific Column Names</h2>
The following code shows how to add the prefix ‘total_’ to only the <b>points</b> and <b>assists</b> columns:
<b>#specify columns to add prefix to
cols = ['points', 'assists']
#add _'total' as prefix to specific columns
df = df.rename(columns={c: 'total_'+c for c in df.columns if c in cols})
#view updated DataFrame
print(df)
   total_points  total_assists  rebounds  blocks
0            25              5        11       6
1            12              7         8       6
2            15              7        10       3
3            14              9         6       2
4            19             12         6       7
5            23              9         5       9</b>
Notice that the prefix ‘total_’ has only been added to the <b>points</b> and <b>assists</b> columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop Columns in Pandas 
 How to Exclude Columns in Pandas 
 How to Change the Order of Columns in Pandas 
 How to Apply Function to Selected Columns in Pandas 
<h2><span class="orange">Pandas: How to Add New Column with Row Numbers</span></h2>
There are two ways to add a new column that contains row numbers in a pandas DataFrame:
<b>Method 1: Use assign()</b>
<b>df = df.assign(row_number=range(len(df)))
</b>
<b>Method 2: Use reset_index()</b>
<b>df['row_number'] = df.reset_index().index</b>
Both methods produce the same result.
The following examples show how to use each method  in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [5, 17, 7, 19, 12, 13, 9, 24],   'assists': [4, 7, 7, 6, 8, 7, 10, 11]})
#view DataFrame
print(df)
  team  points  assists
0    A       5        4
1    B      17        7
2    C       7        7
3    D      19        6
4    E      12        8
5    F      13        7
6    G       9       10
7    H      24       11</b>
<h2>Example 1: Use assign() to Add Row Number Column</h2>
The following code shows how to use the <b>assign()</b> function to add a new column called <b>row_number</b> that displays the row number of each row in the DataFrame:
<b>#add column that contains row numbers
df = df.assign(row_number=range(len(df)))
#view updated DataFrame
print(df)
  team  points  assists  row_number
0    A       5        4           0
1    B      17        7           1
2    C       7        7           2
3    D      19        6           3
4    E      12        8           4
5    F      13        7           5
6    G       9       10           6
7    H      24       11           7</b>
Notice that the values in the <b>row_number</b> column range from 0 to 7.
<h2>Example 2: Use reset_index() to Add Row Number Column</h2>
The following code shows how to use the <b>reset_index()</b> function to add a new column called <b>row_number</b> that displays the row number of each row in the DataFrame:
<b>#add column that contains row numbers
df['row_number'] = df.reset_index().index
#view updated DataFrame
print(df)
  team  points  assists  row_number
0    A       5        4           0
1    B      17        7           1
2    C       7        7           2
3    D      19        6           3
4    E      12        8           4
5    F      13        7           5
6    G       9       10           6
7    H      24       11           7</b>
Notice that the values in the <b>row_number</b> column range from 0 to 7.
This matches the results from the previous example.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Find the Difference Between Two Columns 
 Pandas: How to Find the Difference Between Two Rows 
 Pandas: How to Subtract Two Columns 
<h2><span class="orange">How to Add Rows to a Pandas DataFrame (With Examples)</span></h2>
You can use the <b>df.loc()</b> function to add a row to the end of a pandas DataFrame:
<b>#add row to end of DataFrame
df.loc[len(df.index)] = [value1, value2, value3, ...]
</b>
And you can use the <b>df.append()</b> function to append several rows of an existing DataFrame to the end of another DataFrame:
<b>#append rows of <em>df2</em> to end of existing DataFrame
df = df.append(df2, ignore_index = True)
</b>
The following examples show how to use these functions in practice.
<h3>Example 1: Add One Row to Pandas DataFrame</h3>
The following code shows how to add one row to the end of a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [10, 12, 12, 14, 13, 18],   'rebounds': [7, 7, 8, 13, 7, 4],   'assists': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
#add new row to end of DataFrame
df.loc[len(df.index)] = [20, 7, 5]
#view updated DataFrame
df
        pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
6207 5
</b>
<h3>Example 2: Add Several Rows to Pandas DataFrame</h3>
The following code shows how to add several rows of an existing DataFrame to the end of another DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [10, 12, 12, 14, 13, 18],   'rebounds': [7, 7, 8, 13, 7, 4],   'assists': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
#define second DataFrame
df2 = pd.DataFrame({'points': [21, 25, 26],    'rebounds': [7, 7, 13],    'assists': [11, 3, 3]})
#add new row to end of DataFrame
df = df.append(df2, ignore_index = True)
#view updated DataFrame
df
        pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
6217 11
7257 3
82613 3
</b>
Note that the two DataFrames should have the same column names in order to successfully append the rows of one DataFrame to the end of another.
<h2><span class="orange">Pandas: How to Add Row to Empty DataFrame</span></h2>
You can use the following basic syntax to add a row to an empty pandas DataFrame:
<b>#define row to add
some_row = pd.DataFrame([{'column1':'value1', 'column2':'value2'}])
#add row to empty DataFrame
df = pd.concat([df, some_row])
</b>
The following examples show how to use this syntax in practice.
<h2>Example 1: Add One Row to Empty DataFrame</h2>
The following code shows how to add one row to an empty pandas DataFrame:
<b>import pandas as pd
#create empty DataFrame
df = pd.DataFrame()
#define row to add
row_to_append = pd.DataFrame([{'team':'Mavericks', 'points':'31'}])
#add row to empty DataFrame
df = pd.concat([df, row_to_append])
#view updated DataFrame
print(df)
        team points
0  Mavericks     31
</b>
Notice that we created an empty DataFrame by using <b>pd.DataFrame()</b>, then added one row to the DataFrame by using the <b>concat()</b> function.
<h2>Example 2: Add Multiple Rows to Empty DataFrame</h2>
The following code shows how to add multiple rows to an empty pandas DataFrame:
<b>import pandas as pd
#create empty DataFrame
df = pd.DataFrame()
#define rows to add
rows_to_append = pd.DataFrame([{'team':'Mavericks', 'points':'31'},               {'team':'Hawks', 'points':'20'},               {'team':'Hornets', 'points':'25'},               {'team':'Jazz', 'points':'43'}])
#add row to empty DataFrame
df = pd.concat([df, rows_to_append])
#view updated DataFrame
print(df)
        team points
0  Mavericks     31
1      Hawks     20
2    Hornets     25
3       Jazz     43
</b>
Once again we created an empty DataFrame by using <b>pd.DataFrame()</b>, then added multiple rows to the DataFrame by using the <b>concat()</b> function.
<b>Note</b>: You can find the complete documentation for the pandas <b>concat()</b> function  here . 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">Pandas: How to Add String to Each Value in Column</span></h2>
You can use the following methods to add a string to each value in a column of a pandas DataFrame:
<b>Method 1: Add String to Each Value in Column</b>
<b>df['my_column'] = 'some_string' + df['my_column'].astype(str)
</b>
<b>Method 2: Add String to Each Value in Column Based on Condition</b>
<b>#define condition
mask = (df['my_column'] == 'A')
#add string to values in column equal to 'A'
df.loc[mask, 'my_column'] = 'some_string' + df['my_column'].astype(str)
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    A      22        7         8
2    A      19        7        10
3    A      14        9         6
4    B      14       12         6
5    B      11        9         5
6    B      20        9         9
7    B      28        4        12
</b>
<h2>Example 1: Add String to Each Value in Column</h2>
The following code shows how to add the string ‘team_’ to each value in the <b>team</b> column:
<b>#add string 'team_' to each value in team column
df['team'] = 'team_' + df['team'].astype(str)
#view updated DataFrame
print(df)
     team  points  assists  rebounds
0  team_A      18        5        11
1  team_B      22        7         8
2  team_C      19        7        10
3  team_D      14        9         6
4  team_E      14       12         6
5  team_F      11        9         5
6  team_G      20        9         9
7  team_H      28        4        12</b>
Notice that the prefix ‘team_’ has been added to each value in the <b>team</b> column.
You can also use the following syntax to instead add ‘_team’ as a suffix to each value in the <b>team</b> column:
<b>#add suffix 'team_' to each value in team column
df['team'] = df['team'].astype(str) + '_team'
#view updated DataFrame
print(df)
     team  points  assists  rebounds
0  A_team      18        5        11
1  A_team      22        7         8
2  A_team      19        7        10
3  A_team      14        9         6
4  B_team      14       12         6
5  B_team      11        9         5
6  B_team      20        9         9
7  B_team      28        4        12
</b>
<h2>Example 2: Add String to Each Value in Column Based on Condition</h2>
The following code shows how to add the prefix ‘team_’ to each value in the <b>team</b> column where the value is equal to ‘A’:
<b>#define condition
mask = (df['team'] == 'A')
#add string 'team_' to values that meet the condition
df.loc[mask, 'team'] = 'team_' + df['team'].astype(str)
#view updated DataFrame
print(df)
     team  points  assists  rebounds
0  team_A      18        5        11
1  team_A      22        7         8
2  team_A      19        7        10
3  team_A      14        9         6
4       B      14       12         6
5       B      11        9         5
6       B      20        9         9
7       B      28        4        12</b>
Notice that the prefix ‘team_’ has only been added to the values in the <b>team</b> column whose value was equal to ‘A’.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Select Columns Containing a Specific String 
 Pandas: How to Select Rows that Do Not Start with String 
 Pandas: How to Check if Column Contains String 
<h2><span class="orange">Pandas: How to Add/Subtract Time to Datetime</span></h2>
You can use the following basic syntax to add or subtract time to a datetime in pandas:
<b>#add time to datetime
df['new_datetime'] = df['my_datetime'] + pd.Timedelta(hours=5, minutes=10, seconds=3)
#subtract time from datetime
df['new_datetime'] = df['my_datetime'] - pd.Timedelta(hours=5, minutes=10, seconds=3) 
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Add/Subtract Time to Datetime in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some store during 10 different datetimes:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'time': pd.date_range('2022-01-01', periods=10),   'sales': [14, 22, 25, 29, 31, 10, 12, 8, 22, 25]})
#view DataFrame
print(df)
        time  sales
0 2022-01-01     14
1 2022-01-02     22
2 2022-01-03     25
3 2022-01-04     29
4 2022-01-05     31
5 2022-01-06     10
6 2022-01-07     12
7 2022-01-08      8
8 2022-01-09     22
9 2022-01-10     25</b>
We can use the pandas <b>Timedelta</b> function to add 5 hours, 10 minutes, and 3 seconds to each datetime value in the “time” column:
<b>#create new column that contains time + 5 hours, 10 minutes, 3 seconds
df['time_plus_some'] = df['time'] + pd.Timedelta(hours=5, minutes=10, seconds=3)
#view updated DataFrame
print(df)
        time  sales      time_plus_some
0 2022-01-01     14 2022-01-01 05:10:03
1 2022-01-02     22 2022-01-02 05:10:03
2 2022-01-03     25 2022-01-03 05:10:03
3 2022-01-04     29 2022-01-04 05:10:03
4 2022-01-05     31 2022-01-05 05:10:03
5 2022-01-06     10 2022-01-06 05:10:03
6 2022-01-07     12 2022-01-07 05:10:03
7 2022-01-08      8 2022-01-08 05:10:03
8 2022-01-09     22 2022-01-09 05:10:03
9 2022-01-10     25 2022-01-10 05:10:03</b>
And we can just as easily create a new column that subtracts 5 hours, 10 minutes, and 3 seconds from each datetime value in the “time” column:
<b>#create new column that contains time - 5 hours, 10 minutes, 3 seconds
df['time_minus_some'] = df['time'] - pd.Timedelta(hours=5, minutes=10, seconds=3)
#view updated DataFrame
print(df)
        time  sales     time_minus_some
0 2022-01-01     14 2021-12-31 18:49:57
1 2022-01-02     22 2022-01-01 18:49:57
2 2022-01-03     25 2022-01-02 18:49:57
3 2022-01-04     29 2022-01-03 18:49:57
4 2022-01-05     31 2022-01-04 18:49:57
5 2022-01-06     10 2022-01-05 18:49:57
6 2022-01-07     12 2022-01-06 18:49:57
7 2022-01-08      8 2022-01-07 18:49:57
8 2022-01-09     22 2022-01-08 18:49:57
9 2022-01-10     25 2022-01-09 18:49:57
</b>
<b>Note #1</b>: In these examples we used a specific number of hours, minutes, and seconds, but you can also use only one of these units if you’d like. For example, you can specify <b>pd.Timedelta(hours=5)</b> to simply add five hours to a datetime value.
<b>Note #2</b>: You can find the complete documentation for the pandas <b>Timedelta</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Convert Timedelta to Int in Pandas 
 How to Convert DateTime to String in Pandas 
 How to Convert Timestamp to Datetime in Pandas 
 How to Create Date Column from Year, Month and Day in Pandas 
<h2><span class="orange">Pandas: How to Add Suffix to Column Names</span></h2>
You can use the following methods to add a suffix to column names in a pandas DataFrame:
<b>Method 1: Add Suffix to All Column Names</b>
<b>df = df.add_suffix('_my_suffix')
</b>
<b>Method 2: Add Suffix to Specific Column Names</b>
<b>#specify columns to add suffix to
cols = ['col1', 'col3']
#add suffix to specific columns
df = df.rename(columns={c: c+'_my_suffix' for c in df.columns if c in cols})</b>
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5],   'blocks': [6, 6, 3, 2, 7, 9]})
#view DataFrame
print(df)
   points  assists  rebounds  blocks
0      25        5        11       6
1      12        7         8       6
2      15        7        10       3
3      14        9         6       2
4      19       12         6       7
5      23        9         5       9
</b>
<h2>Method 1: Add Suffix to All Column Names</h2>
The following code shows how to add the suffix ‘_total’ to all column names:
<b>#add '_total' as suffix to each column name
df = df.add_suffix('_total')
#view updated DataFrame
print(df)
   points_total  assists_total  rebounds_total  blocks_total
0            25              5              11             6
1            12              7               8             6
2            15              7              10             3
3            14              9               6             2
4            19             12               6             7
5            23              9               5             9</b>
Notice that the suffix ‘_total’ has been added to all column names.
<b>Note</b>: To add a prefix to column names, simply use <b>add_prefix</b> instead.
<h2>Method 2: Add Suffix to Specific Column Names</h2>
The following code shows how to add the suffix ‘_total’ to only the <b>points</b> and <b>assists</b> columns:
<b>#specify columns to add suffix to
cols = ['points', 'assists']
#add _'total' as suffix to specific columns
df = df.rename(columns={c: c+'_total' for c in df.columns if c in cols})
#view updated DataFrame
print(df)
   points_total  assists_total  rebounds  blocks
0            25              5        11       6
1            12              7         8       6
2            15              7        10       3
3            14              9         6       2
4            19             12         6       7
5            23              9         5       9</b>
Notice that the suffix ‘_total’ has only been added to the <b>points</b> and <b>assists</b> columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop Columns in Pandas 
 How to Exclude Columns in Pandas 
 How to Change the Order of Columns in Pandas 
 How to Apply Function to Selected Columns in Pandas 
<h2><span class="orange">How to Add a Total Row to Pandas DataFrame</span></h2>
You can use the following basic syntax to add a ‘total’ row to the bottom of a pandas DataFrame:
<b>df.loc['total']= df.sum()
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Add a Total Row to Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5],   'blocks': [6, 6, 3, 2, 7, 9]})
#view DataFrame
print(df)
  team  assists  rebounds  blocks
0    A        5        11       6
1    B        7         8       6
2    C        7        10       3
3    D        9         6       2
4    E       12         6       7
5    F        9         5       9
</b>
We can use the following syntax to add a ‘total’ row at the bottom of the DataFrame that shows the sum of values in each column:
<b>#add total row
df.loc['total']= df.sum()
#view updated DataFrame
print(df)
         team  assists  rebounds  blocks
0           A        5        11       6
1           B        7         8       6
2           C        7        10       3
3           D        9         6       2
4           E       12         6       7
5           F        9         5       9
total  ABCDEF       49        46      33</b>
A new row has been added to the bottom of the DataFrame that shows the sum of values in each column.
Note that for character columns, the ‘total’ is simply the concatenation of every character in the column.
If you’d like, you can set the ‘total’ value in the <b>team</b> column to simply be blank:
<b>#set last value in team column to be blank
df.loc[df.index[-1], 'team'] = ''
#view updated DataFrame
print(df)
      team  assists  rebounds  blocks
0        A        5        11       6
1        B        7         8       6
2        C        7        10       3
3        D        9         6       2
4        E       12         6       7
5        F        9         5       9
total            49        46      33
</b>
The last value in the <b>team</b> column is now blank, as opposed to being a concatenation of every character in the column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Rows without NaN Values in Pandas 
 How to Drop All Rows Except Specific Ones in Pandas 
 How to Sum Specific Columns in Pandas 
<h2><span class="orange">How to Add Two Pandas DataFrames (With Example)</span></h2>
You can use the following basic syntax to add the values in two pandas DataFrames:
<b>df3 = df1.add(df2, fill_value=0)
</b>
This will produce a new DataFrame that contains the sum of the corresponding elements in each individual DataFrame.
If an element exists in one DataFrame and not the other, the existing element will be used in the resulting DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: How to Add Two Pandas DataFrames</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'points': [18, 22, 19, 14, 11],    'assists': [5, 11, 7, 9, 12]})
#view first DataFrame
print(df1)
   points  assists
0      18        5
1      22       11
2      19        7
3      14        9
4      11       12
#create second DataFrame
df2 = pd.DataFrame({'points': [10, 5, 4, 3, 9, 14],    'assists': [9, 7, 4, 2, 3, 3]})
#view second DataFrame
print(df2)
   points  assists
0      10        9
1       5        7
2       4        4
3       3        2
4       9        3
5      14        3</b>
We can use the following syntax to create a new DataFrame that takes the sum of corresponding elements in each individual DataFrame:
<b>#create new DataFrame by adding two DataFrames
df3 = df1.add(df2, fill_value=0)
#view new DataFrame
print(df3)
   points  assists
0    28.0     14.0
1    27.0     18.0
2    23.0     11.0
3    17.0     11.0
4    20.0     15.0
5    14.0      3.0
</b>
Notice that the resulting DataFrame contains the sum of corresponding elements in each individual DataFrame.
Note that the row with an index value of 5 only existed in the second DataFrame, so the values in this row are simply the values from the second DataFrame.
Also notice that since we performed addition, each of the values in the new DataFrame are represented as float values with one decimal place.
To convert each of these values back to an integer, we can use the <b>astype()</b> function:
<b>#convert all columns in new DataFrame to integer
df3 = df3.astype('int64')
#view updated DataFrame
print(df3)
   points  assists
0      28       14
1      27       18
2      23       11
3      17       11
4      20       15
5      14        3
</b>
Each of the values in the new DataFrame are now integers.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: Add Column from One DataFrame to Another 
 Pandas: Get Rows Which Are Not in Another DataFrame 
 Pandas: How to Check if Multiple Columns are Equal 
<h2><span class="orange">Pandas: How to Annotate Bars in Bar Plot</span></h2>
You can use the following methods to annotate bars in a pandas bar plot:
<b>Method 1: Annotate Bars in Simple Bar Plot</b>
<b>ax = df.plot.bar()
ax.bar_label(ax.containers[0])
</b>
<b>Method 2: Annotate Bars in Grouped Bar Plot</b>
<b>ax = df.plot.bar()
for container in ax.containers:
    ax.bar_label(container)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Annotate Bars in Simple Bar Plot</h3>
The following code shows how to annotate bars in a simple bar plot:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'product': ['A', 'B', 'C', 'D', 'E'],   'sales': [4, 7, 8, 15, 12]})
#view DataFrame
print(df)
  product  sales
0       A      4
1       B      7
2       C      8
3       D     15
4       E     12
#create bar plot to visualize sales by product
ax = df.plot.bar(x='product', y='sales', legend=False)
#annotate bars
ax.bar_label(ax.containers[0])
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/annotatebar1.jpg">
Notice that the actual value for the sales is shown at the top of each bar.
<h3>Example 2: Annotate Bars in Grouped Bar Plot</h3>
The following code shows how to annotate bars in a grouped bar plot:
<b>#create DataFrame
df = pd.DataFrame({'productA': [14, 10],   'productB': [17, 19]},    index=['store 1', 'store 2'])
#view DataFrame
print(df)
         productA  productB
store 1        14        17
store 2        10        19
#create grouped bar plot
ax = df.plot.bar()
#annotate bars in bar plot
for container in ax.containers:
    ax.bar_label(container)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/annotatebar2.jpg">
Notice that annotations have been added to each individual bar in the plot.
<h2><span class="orange">How to Perform an Anti-Join in Pandas</span></h2>
An <b>anti-join</b> allows you to return all rows in one dataset that do not have matching values in another dataset.
You can use the following syntax to perform an anti-join between two pandas DataFrames:
<b>outer = df1.merge(df2, how='outer', indicator=True)
anti_join = outer[(outer._merge=='left_only')].drop('_merge', axis=1)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Perform an Anti-Join in Pandas</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [18, 22, 19, 14, 30]})
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
4    E      30
#create second DataFrame
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'F', 'G'],    'points': [18, 22, 19, 22, 29]})
print(df2)
  team  points
0    A      18
1    B      22
2    C      19
3    F      22
4    G      29
</b>
We can use the following code to return all rows in the first DataFrame that do not have a matching team in the second DataFrame:
<b>#perform outer join
outer = df1.merge(df2, how='outer', indicator=True)
#perform anti-join
anti_join = outer[(outer._merge=='left_only')].drop('_merge', axis=1)
#view results
print(anti_join)
  team  points
3    D      14
4    E      30</b>
We can see that there are exactly two teams from the first DataFrame that do not have a matching team name in the second DataFrame.
The anti-join worked as expected.
The end result is one DataFrame that only contains the rows where the team name belongs to the first DataFrame but not the second DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Do an Inner Join in Pandas 
 How to Do a Left Join in Pandas 
 How to Do a Cross Join in Pandas 
<h2><span class="orange">How to Append Multiple Pandas DataFrames (With Example)</span></h2>
You can use the following basic syntax to append multiple pandas DataFrames at once:
<b>import pandas as pd
#append multiple DataFrames
df_big = pd.concat([df1,df2, df3], ignore_index=True) 
</b>
This particular syntax will append <b>df1</b>, <b>df2</b>, and <b>df3</b> into a single pandas DataFrame called <b>df_big</b>.
The following example shows how to use this syntax in practice.
<h3>Example 1: Append Multiple Pandas DataFrames at Once</h3>
The following code shows how to append multiple pandas DataFrames at once:
<b>import pandas as pd
#create three DataFrames
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],    'points':[12, 5, 13, 17, 27]})
df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],    'points':[24, 26, 27, 27, 12]})
df3 = pd.DataFrame({'player': ['K', 'L', 'M', 'N', 'O'],    'points':[9, 5, 5, 13, 17]})
#append all DataFrames into one DataFrame
df_big = pd.concat([df1,df2, df3], ignore_index=True)
#view resulting DataFrame
print(df_big)
        playerpoints
0A12
1B5
2C13
3D17
4E27
5F24
6G26
7H27
8I27
9J12
10K9
11L5
12M5
13N13
14O17</b>
The result is one big DataFrame that contains all of the rows from each of the three individual DataFrames.
The argument <b>ignore_index=True</b> tells pandas to ignore the original index numbers in each DataFrame and to create a new index that starts at 0 for the new DataFrame.
For example, consider what happens when we don’t use <b>ignore_index=True</b> when stacking the following two DataFrames:
<b>import pandas as pd
#create two DataFrames with indices
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],    'points':[12, 5, 13, 17, 27]},    index=[0, 1, 2, 3, 4])
df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],    'points':[24, 26, 27, 27, 12]},    index=[2, 4, 5, 6, 9])
#stack the two DataFrames together
df_big = pd.concat([df1,df2])
#view resulting DataFrame
print(df_big)
        playerpoints
0A12
1B5
2C13
3D17
4E27
2F24
4G26
5H27
6I27
9J12
</b>
The resulting DataFrame kept its original index values from the two DataFrames.
In general, you should use <b>ignore_index=True </b>when appending multiple DataFrames unless you have a specific reason for keeping the original index values.
<h2><span class="orange">How to Append Two Pandas DataFrames (With Examples)</span></h2>
You can use the following basic syntax to append two pandas DataFrames into one DataFrame:
<b>big_df = pd.concat([df1, df2], ignore_index=True)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Append Two Pandas DataFrames</h3>
The following code shows how to append two pandas DataFrames together into one DataFrame:
<b>import pandas as pd
#create two DataFrames
df1 = pd.DataFrame({'x': [25, 14, 16, 27, 20, 12, 15, 14, 19],    'y': [5, 7, 7, 5, 7, 6, 9, 9, 5],    'z': [8, 8, 10, 6, 6, 9, 6, 9, 7]})
df2 = pd.DataFrame({'x': [58, 60, 65],    'y': [14, 22, 23],    'z': [9, 12, 19]})
#append two DataFrames together
combined = pd.concat([df1, df2], ignore_index=True)
#view final DataFrame
combined
xyz
02558
11478
216710
32756
42076
51269
61596
71499
81957
958149
10602212
11652319</b>
<h3>Example 2: Append More Than Two Pandas DataFrames</h3>
Note that you can use the <b>pd.concat()</b> function to append more than two pandas DataFrames together:
<b>import pandas as pd
#create three DataFrames
df1 = pd.DataFrame({'x': [25, 14, 16],    'y': [5, 7, 7]})
df2 = pd.DataFrame({'x': [58, 60, 65],    'y': [14, 22, 23]})
df3 = pd.DataFrame({'x': [58, 61, 77],    'y': [10, 12, 19]})
#append all three DataFrames together
combined = pd.concat([df1, df2, df3], ignore_index=True)
#view final DataFrame
combined
xy
0255
1147
2167
35814
46022
56523
65810
76112
87719
</b>
Note that if we didn’t use the <b>ignore_index</b> argument, the index of the resulting DataFrame would retain the original index values for each individual DataFrame:
<b>#append all three DataFrames together
combined = pd.concat([df1, df2, df3])
#view final DataFrame
combined
xy
0255
1147
2167
05814
16022
26523
05810
16112
27719</b>
You can find the complete online documentation for the <b>pandas.concat()</b> function  here .
<h2><span class="orange">Pandas: How to Apply Function to Every Row in DataFrame</span></h2>
You can use the following basic syntax to apply a function to every row in a pandas DataFrame:
<b>df['new_col'] = df.apply(lambda x: some function, axis=1)
</b>
This syntax applies a function to each row in a pandas DataFrame and returns the results in a new column.
The following example shows how to use this syntax in practice.
<h2>Example: Apply Function to Every Row in DataFrame</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A': [5, 4, 7, 9, 12, 9, 9, 4],   'B': [10, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
    A   B
0   5  10
1   4   8
2   7  10
3   9   6
4  12   6
5   9   5
6   9   9
7   4  12
</b>
Now suppose we would like to apply a function that multiplies the values in column A and column B and then divides by 2.
We can use the following syntax to apply this function to each row in the DataFrame:
<b>#create new column by applying function to each row in DataFrame
df['z'] = df.apply(lambda x: x['A'] * x['B'] / 2, axis=1)
#view updated DataFrame
print(df)
    A   B     z
0   5  10  25.0
1   4   8  16.0
2   7  10  35.0
3   9   6  27.0
4  12   6  36.0
5   9   5  22.5
6   9   9  40.5
7   4  12  24.0</b>
Column z displays the results of the function.
For example:
First row: A * B / 2 = 5 * 10 / 2 = <b>25</b>
Second row: A * B / 2 = 4 * 8 / 2 = <b>16</b>
Third row: A * B / 2 = 7 * 10 / 2 = <b>35</b>
And so on.
You can use similar syntax with <b>lambda</b> to apply any function you’d like to every row in a pandas DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Apply Function to Pandas Groupby 
 How to Perform a GroupBy Sum in Pandas 
 How to Use Groupby and Plot in Pandas 
<h2><span class="orange">How to Use Pandas apply() inplace</span></h2>
The pandas  apply()  function can be used to apply a function across rows or columns of a pandas DataFrame.
This function is different from other functions like <b>drop()</b> and <b>replace()</b> that provide an inplace argument:
<b>df.drop(['column1'], inplace=True)
df.rename({'old_column' : 'new_column'}, inplace=True)
</b>
The <b>apply()</b> function has no inplace argument, so we must use the following syntax to transform a DataFrame inplace:
<b>df = df.apply(lambda x: x*2)</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
        pointsassists rebounds
0255 11
1127 8
2157 10
3149 6
41912 6
5239 5
6259 9
7294 12
</b>
<h3>Example 1: Use apply() inplace for One Column</h3>
The following code shows how to use <b>apply()</b> to transform one data frame column inplace:
<b>#multiply all values in 'points' column by 2 inplace
df.loc[:, 'points'] = df.points.apply(lambda x: x*2)
#view updated DataFrame
df
pointsassistsrebounds
050511
12478
230710
32896
438126
54695
65099
758412
</b>
<h3>Example 2: Use apply() inplace for Multiple Columns</h3>
The following code shows how to use <b>apply()</b> to transform multiple data frame columns inplace:
<b>multiply all values in 'points' and 'rebounds' column by 2 inplace
df[['points', 'rebounds']] = df[['points', 'rebounds']].apply(lambda x: x*2)
#view updated DataFrame
df
pointsassistsrebounds
050522
124716
230720
328912
4381212
546910
650918
758424
</b>
<h3>Example 3: Use apply() inplace for All Columns</h3>
The following code shows how to use <b>apply()</b> to transform all data frame columns inplace:
<b>#multiply values in all columns by 2
df = df.apply(lambda x: x*2)
#view updated DataFrame
df
pointsassistsrebounds
0501022
1241416
2301420
3281812
4382412
5461810
6501818
758824
</b>
<h2><span class="orange">Pandas: How to Use Apply & Lambda Together</span></h2>
You can use the following basic syntax to apply a lambda function to a pandas DataFrame:
<b>df['col'] = df['col'].apply(lambda x: 'value1' if x &lt; 20 else 'value2')
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
<h3>Example 1: Use Apply and Lambda to Create New Column</h3>
The following code shows how to use <b>apply</b> and <b>lambda</b> to create a new column whose values are dependent on the values of an existing column:
<b>#create new column called 'status'
df['status'] = df['points'].apply(lambda x: 'Bad' if x &lt; 20 else 'Good')
#view updated DataFrame
print(df)
  team  points  assists status
0    A      18        5    Bad
1    B      22        7   Good
2    C      19        7    Bad
3    D      14        9    Bad
4    E      14       12    Bad
5    F      11        9    Bad
6    G      20        9   Good
7    H      28        4   Good</b>
In this example, we created a new column called <b>status</b> that took on the following values:
‘<b>Bad</b>‘ if the value in the points column was less than 20.
‘<b>Good</b>‘ if the value in the points column was greater than or equal to 20.
<h3>Example 2: Use Apply and Lambda to Modify Existing Column</h3>
The following code shows how to use <b>apply</b> and <b>lambda</b> to modifying an existing column in the DataFrame:
<b>#modify existing 'points' column
df['points'] = df['points'].apply(lambda x: x/2 if x &lt; 20 else x*2)
#view updated DataFrame
print(df)
  team  points  assists
0    A     9.0        5
1    B    44.0        7
2    C     9.5        7
3    D     7.0        9
4    E     7.0       12
5    F     5.5        9
6    G    40.0        9
7    H    56.0        4</b>
In this example, we modified the values in the existing <b>points</b> column by using the following rule in the lambda function:
If the value is less than 20, divide the value by 2.
If the value is greater than or equal to 20, multiply the value by 2.
Using this lambda function, we were able to modify the values in the existing <b>points</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in pandas:
 How to Apply Function to Pandas Groupby 
 How to Fill NaN with Values from Another Column in Pandas 
<h2><span class="orange">How to Use the assign() Method in Pandas (With Examples)</span></h2>
The <b>assign()</b> method can be used to add new columns to a pandas DataFrame.
This method uses the following basic syntax:
<b>df.assign(new_column = values)
</b>
It’s important to note that this method will only output the new DataFrame to the console, but it won’t actually modify the original DataFrame.
To modify the original DataFrame, you would need to store the results of the <b>assign()</b> method in a new variable.
The following examples show how to use the <b>assign()</b> method in different ways with the following pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12</b>
<h2>Example 1: Assign New Variable to DataFrame</h2>
The following code shows how to use the <b>assign()</b> method to add a new variable to the DataFrame called <b>points2</b> whose values are equal to the values in the <b>points</b> column multiplied by two:
<b>#add new variable called points2
df.assign(points2 = df.points * 2)
    points  assistsrebounds   points2
025  5      1150
112  7       824
215  7      1030
314  9       628
419 12       638
523  9       546
625  9       950
729  4      1258
</b>
Note that this <b>assign()</b> method doesn’t change the original DataFrame.
If we print the original DataFrame, we’ll see that it remains unchanged:
<b>#print original DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
</b>
To save the results of the <b>assign()</b> method, we can store the results in a new DataFrame:
<b>#add new variable called points2 and save results in new DataFrame
df.assign(points2 = df.points * 2)
#view new DataFrame
print(df_new)
   points  assists  rebounds  points2
0      25        5        11       50
1      12        7         8       24
2      15        7        10       30
3      14        9         6       28
4      19       12         6       38
5      23        9         5       46
6      25        9         9       50
7      29        4        12       58
</b>
 The new DataFrame called <b>df_new</b> now contains the <b>points2</b> column that we created.
<h2>Example 2: Assign Multiple New Variables to DataFrame</h2>
The following code shows how to use the <b>assign()</b> method to add three new variables to the DataFrame:
<b>#add three new variables to DataFrame and store results in new DataFrame
df_new = df.assign(points2 = df.points * 2,   assists_rebs = df.assists + df.rebounds,   conference = 'Western')
#view new DataFrame
print(df_new)
   points  assists  rebounds  points2  assists_rebs conference
0      25        5        11       50            16    Western
1      12        7         8       24            15    Western
2      15        7        10       30            17    Western
3      14        9         6       28            15    Western
4      19       12         6       38            18    Western
5      23        9         5       46            14    Western
6      25        9         9       50            18    Western
7      29        4        12       58            16    Western
</b>
Notice that three new columns have been added to the DataFrame.
<b>Note</b>: You can find the complete documentation for the pandas <b>assign()</b> method  here .
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in pandas:
 How to Use describe() Function in Pandas 
 How to Use idxmax() Function in Pandas 
 How to Apply a Function to Selected Columns in Pandas 
<h2><span class="orange">Pandas at vs. loc: What’s the Difference?</span></h2>
When it comes to selecting rows and columns of a pandas DataFrame, <b>.loc </b>and <b>.at</b> are two commonly used functions.
Here is the subtle difference between the two functions:
<b>.loc</b> can take multiple rows and columns as input arguments
<b>.at</b> can only take one row and one column as input arguments
The following examples show how to use each function in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
<h2>Example 1: How to Use loc in Pandas</h2>
The following code shows how to use <b>.loc</b> to access the value in the DataFrame located at index position 0 of the points column:
<b>#select value located at index position 0 of the points column
df.loc[0, 'points']
18</b>
This returns a value of <b>18</b>.
And the following code shows how to use <b>.loc</b> to access rows between index values 0 and 4 along with the columns points and assists:
<b>#select rows between index values 0 and 4 and columns 'points' and 'assists'
df.loc[0:4, ['points', 'assists']]
        pointsassists
0185
1227
2197
3149
41412</b>
Whether we’d like to access one single value or a group of rows and columns, the <b>.loc</b> function can do both.
<h2>Example 2: How to Use at in Pandas</h2>
The following code shows how to use <b>.at </b>to access the value in the DataFrame located at index position 0 of the points column:
<b>#select value located at index position 0 of the points column
df.at[0, 'points']
18</b>
This returns a value of <b>18</b>.
However, suppose we try to use <b>at </b>to access rows between index values 0 and 4 along with the columns points and assists:
<b>#try to select rows between index values 0 and 4 and columns 'points' and 'assists'
df.at[0:4, ['points', 'assists']] 
TypeError: unhashable type: 'list'
</b>
We receive an error because the <b>at</b> function is unable to take multiple rows or multiple columns as input arguments.
<h2>Conclusion</h2>
When you’d like to access just one value in a pandas DataFrame, both the <b>loc</b> and <b>at</b> functions will work fine.
However, when you’d like to access a group of rows and columns, only the <b>loc</b> function is able to do so.
<b>Related:</b>  Pandas loc vs. iloc: What’s the Difference? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Select Rows by Multiple Conditions Using Pandas loc 
 How to Select Rows Based on Column Values in Pandas 
 How to Select Rows by Index in Pandas 
<h2><span class="orange">How to Calculate the Average of Selected Columns in Pandas</span></h2>
You can use the following methods to calculate the average row values for selected columns in a pandas DataFrame:
<b>Method 1: Calculate Average Row Value for All Columns</b>
<b>df.mean(axis=1)
</b>
<b>Method 2: Calculate Average Row Value for Specific Columns</b>
<b>df[['col1', 'col3']].mean(axis=1)</b>
The following examples shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [14, 19, 9, 21, 25, 29, 20, 11],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
pointsassistsrebounds
014511
11978
29710
32196
425126
52995
62099
711412</b>
<h3>Method 1: Calculate Average Row Value for All Columns</h3>
The following code shows how to create a new column in the DataFrame that displays the average row value for all columns:
<b>#define new column that shows the average row value for all columns
df['average_all'] = df.mean(axis=1)
#view updated DataFrame
df
pointsassistsrebounds  average_all
014511  10.000000
11978  11.333333
29710  8.666667
32196  12.000000
425126  14.333333
52995  14.333333
62099  12.666667
711412  9.000000
</b>
Here’s how to interpret the output:
The average value of the first row is calculated as: (14+5+11) / 3 = <b>10</b>.
The average value of the second row is calculated as: (19+7+8) / 3 = <b>11.33</b>.
And so on.
<h3>Method 2: Calculate Average Row Value for Specific Columns</h3>
The following code shows how to calculate the average row value for just the “points” and “rebounds” columns:
<b>#define new column that shows average of row values for points and rebounds columns
df['avg_points_rebounds'] = df[['points', 'rebounds']].mean(axis=1)
#view updated DataFrame
df
        pointsassistsrebounds  avg_points_rebounds
014511  12.5
11978  13.5
29710  9.5
32196  13.5
425126  15.5
52995  17.0
62099  14.5
711412  11.5
</b>
Here’s how to interpret the output:
The average value of “points” and “rebounds” in the first row is calculated as: (14+11) / 2 = <b>12.5</b>.
The average value of “points” and “rebounds” in the second row is calculated as: (19+8) / 2 = <b>13.5</b>.
And so on.
<h2><span class="orange">Pandas: Get Business Days Between Start & End Date</span></h2>
You can use the following methods to get business days in pandas:
<b>Method 1: Get Business Days (excludes all weekends)</b>
<b>business_days = pd.bdate_range('2022-01-01', '2022-12-31')
</b>
<b>Method 2: Get Business Days (excludes all weekends <em>and</em> Federal holidays)</b>
<b>from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import CustomBusinessDay
#define US business days
us_bus = CustomBusinessDay(calendar=USFederalHolidayCalendar())
#get all business days between certain start and end dates
us_business_days = pd.bdate_range('2022-01-01', '2022-12-31', freq=us_bus)
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Get Business Days (excludes all weekends)</h2>
One way to get a list of business days between two dates in pandas is to use the <b>bdate_range()</b> function.
It’s worth noting that this function simply counts the number of days between a start and end date, excluding weekends.
For example, we can use the following syntax to count the number of business days between 1/1/2022 and 12/31/2022:
<b>import pandas as pd
#get all business days between certain start and end dates
business_days = pd.bdate_range('2022-01-01', '2022-12-31')
#view first ten business days
print(business_days[0:10])
DatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',
               '2022-01-07', '2022-01-10', '2022-01-11', '2022-01-12',
               '2022-01-13', '2022-01-14'],
              dtype='datetime64[ns]', freq='B')
#view total number of business days
len(business_days)
260</b>
The object called <b>business_days</b> contains every business day between the specified start and end dates.
And by using the <b>len()</b> function, we see that the total number of business days between the specified start and end dates is <b>260</b>.
<h2>Example 2: Get Business Days (excludes all weekends <em>and</em> Federal holidays)</h2>
To get a list of business days between two dates in pandas that excludes both weekends and Federal holidays, we must use functions from the pandas <b>tseries</b> module.
For example, we can use the following syntax to count the number of business days (all days excluding weekends and Federal holidays) between 1/1/2022 and 12/31/2022:
<b>from pandas.tseries.holiday import USFederalHolidayCalendar
from pandas.tseries.offsets import CustomBusinessDay
#define US business days
us_bus = CustomBusinessDay(calendar=USFederalHolidayCalendar())
#get all business days between certain start and end dates
us_business_days = pd.bdate_range('2022-01-01', '2022-12-31', freq=us_bus)
#view first ten business days
print(us_business_days[0:10])
DatetimeIndex(['2022-01-03', '2022-01-04', '2022-01-05', '2022-01-06',
               '2022-01-07', '2022-01-10', '2022-01-11', '2022-01-12',
               '2022-01-13', '2022-01-14'],
              dtype='datetime64[ns]', freq='C')
#view total number of business days
len(us_business_days)
250</b>
The object called <b>us_business_days</b> contains every business day (all days excluding weekends and Federal holidays) between the specified start and end dates.
And by using the <b>len()</b> function, we see that the total number of business days between the specified start and end dates is <b>250</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Convert Datetime to Date in Pandas 
 How to Extract Month from Date in Pandas 
<h2><span class="orange">How to Fix: Can only compare identically-labeled series objects</span></h2>
One error you may encounter when using pandas is:
<b>ValueError: Can only compare identically-labeled DataFrame objects
</b>
This error occurs when you attempt to compare two pandas DataFrames and either the index labels or the column labels do not perfectly match.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'points': [25, 12, 15, 14],   'assists': [5, 7, 13, 12]})
df2 = pd.DataFrame({'points': [25, 12, 15, 14],    'assists': [5, 7, 13, 12]},     index=[3, 2, 1, 0])
#view DataFrames
print(df1)
   points  assists
0      25        5
1      12        7
2      15       13
3      14       12
print(df2)
   points  assists
3      25        5
2      12        7
1      15       13
0      14       12</b>
Notice that the column labels match, but the index labels do not.
If we attempt to compare the two DataFrames, we’ll receive an error:
<b>#attempt to compare the DataFrames
df1 = df2
ValueError: Can only compare identically-labeled DataFrame objects
</b>
<h3>How to Fix the Error</h3>
There are a few methods we can use to address this error.
<b>Method 1: Compare DataFrames (including index labels)</b>
We can use the following syntax to compare the two DataFrames to see if they perfectly match (including the index labels):
<b>df1.equals(df2)
False
</b>
This tells us that the two DataFrames do not perfectly match (including the index labels).
<b>Method 2: Compare DataFrames (ignore index labels)</b>
We can use the following syntax to compare the two DataFrames to see if they perfectly match, while completely ignoring the index labels:
<b>df1.reset_index(drop=True).equals(df2.reset_index(drop=True))
True</b>
This tells us that the two DataFrames perfectly match (not accounting for the index labels).
<b>Method 3: Compare DataFrames Row by Row</b>
We can use the following syntax to compare the two DataFrames row by row to see which row values match:
<b>df1.reset_index(drop=True) == df2.reset_index(drop=True)
      pointsassists
0True   True
1True   True
2True   True
3True   True</b>
This allows us to see which values match in each row.
<h2><span class="orange">Pandas: How to Change Column Names to Lowercase</span></h2>
You can use the following syntax to change the column names in a pandas DataFrame to lowercase:
<b>df.columns = df.columns.str.lower()
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Change Column Names to Lowercase in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'Team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'Points': [18, 22, 19, 14, 14, 11, 20, 28],   'ASSISTS': [5, 7, 7, 9, 12, 9, 9, 4],   'Rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
</b>
Notice that none of the column names are currently lowercase.
We can use the following syntax to change all column names to lowercase:
<b>#convert all column names to lowercase
df.columns = df.columns.str.lower()
#view updated DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
Notice that all four column names have now been converted to lowercase.
Note that we can also use the following syntax to list out all of the column names without displaying any of the column values:
<b>#display all column names
list(df)
['team', 'points', 'assists', 'rebounds']
</b>
We can see that each of the column names has been converted to lowercase.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions with columns of a pandas DataFrame:
 How to Drop Columns in Pandas 
 How to Apply a Function to Selected Columns in Pandas 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Change the Order of Columns in Pandas DataFrame</span></h2>
You can use the following syntax to quickly change the order of columns in a pandas DataFrame:
<b>df[['column2', 'column3', 'column1']]
</b>
The following examples show how to use this syntax with the following pandas DataFrame:
<b>import pandas as pd
#create new DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#display DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412
</b>
<h3>Example 1: Change the Order of Columns by Name</h3>
The following code shows how to change the order of the columns in the DataFrame based on name:
<b>#change order of columns by name
df[['rebounds', 'assists', 'points']]
rebounds assists points
011 5 25
18 7 12
210 7 15
36 9 14
46 12 19
55 9 23
69 9 25
712 4 29</b>
<h3>Example 2: Change the Order by Adding New First Column</h3>
The following code shows how to change the order of the columns in the DataFrame by inserting a new column in the first position:
<b>#define new column to add
steals = [2, 3, 3, 4, 3, 2, 1, 2]
#insert new column in first position
df.insert(0, 'steals', steals)
#display dataFrame
df
        stealspointsassistsrebounds
0225511
131278
2315710
341496
4319126
522395
612599
7229412</b>
<h3>Example 3: Change the Order by Adding New Last Column</h3>
The following code shows how to change the order of the columns in the DataFrame by inserting a new column in the last position of the DataFrame:
<b>#define new column to add
steals = [2, 3, 3, 4, 3, 2, 1, 2]
#insert new column in last position
df.insert(len(df.columns), 'steals', steals)
#display dataFrame
df
pointsassistsrebounds steals
025511 2
11278 3
215710 3
31496 4
419126 3
52395 2
62599 1
729412 2
</b>
<h2><span class="orange">How to Change Column Type in Pandas (With Examples)</span></h2>
Columns in a pandas DataFrame can take on one of the following types:
<b>object</b> (strings)
<b>int64</b> (integers)
<b>float64</b> (numeric values with decimals)
<b>bool</b> (True or False values)
<b>datetime64</b> (dates and times)
The easiest way to convert a column from one data type to another is to use the <b>astype()</b> function.
You can use the following methods with the <b>astype()</b> function to convert columns from one data type to another:
<b>Method 1: Convert One Column to Another Data Type</b>
<b>df['col1'] = df['col1'].astype('int64')
</b>
<b>Method 2: Convert Multiple Columns to Another Data Type</b>
<b>df[['col1', 'col2']] = df[['col1', 'col2']].astype('int64')</b>
<b>Method 3: Convert All Columns to Another Data Type</b>
<b>df = df.astype('int64')</b>
The following examples show how to use  each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'ID': ['1', '2', '3', '4', '5', '6'],   'tenure': [12.443, 15.8, 16.009, 5.06, 11.075, 12.9546],   'sales': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
  ID   tenure  sales
0  1  12.4430      5
1  2  15.8000      7
2  3  16.0090      7
3  4   5.0600      9
4  5  11.0750     12
5  6  12.9546      9
#view data type of each column
print(df.dtypes)
ID         object
tenure    float64
sales       int64
dtype: object
</b>
<h2>Example 1: Convert One Column to Another Data Type</h2>
The following code shows how to use the <b>astype()</b> function to convert the <b>tenure</b> column from a float to an integer:
<b>#convert tenure column to int64
df['tenure'] = df['tenure'].astype('int64')
#view updated data type for each column
print(df.dtypes)
ID        object
tenure     int64
sales      int64
dtype: object
</b>
Notice that the <b>tenure</b> column has been converted to int64 while all other columns have retained their original data type.
<h2>Example 2: Convert Multiple Columns to Another Data Type</h2>
The following code shows how to use the <b>astype()</b> function to convert both the <b>ID</b> and <b>tenure </b>column to integer:
<b>#convert ID and tenure columns to int64
df[['ID', 'tenure']] = df[['ID', 'tenure']].astype('int64')
#view updated data type for each column
print(df.dtypes)
ID         int64
tenure     int64
sales      int64
dtype: object
</b>
Notice that both the <b>ID</b> and <b>tenure</b> columns have been converted to int64.
<h2>Example 3: Convert All Columns to Another Data Type</h2>
The following code shows how to use the <b>astype()</b> function to convert all columns in the DataFrame to an integer data type:
<b>#convert all columns to int64
df = df.astype('int64')
#view updated data type for each column
print(df.dtypes)
ID        int64
tenure    int64
sales     int64
dtype: object
</b>
Notice that all columns have been converted to int64.
<b>Note</b>: You can find the complete documentation for the pandas <b>astype()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common conversions in pandas:
 How to Convert Pandas DataFrame Columns to Strings 
 How to Convert Timestamp to Datetime in Pandas 
 How to Convert Datetime to Date in Pandas 
 How to Convert Strings to Float in Pandas 
<h2><span class="orange">How to Change One or More Index Values in Pandas</span></h2>
You can use the following syntax to change a single index value in a pandas DataFrame:
<b>df.rename(index={'Old_Value':'New_Value'}, inplace=True)
</b>
And you can use the following syntax to change several index values at once:
<b>df.rename(index={'Old1':'New1', 'Old2':'New2'}, inplace=True)</b>
The following examples shows how to use this syntax in practice.
<h3>Example 1: Change One Index Value in Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#make 'team' column the index column
df.set_index('team', inplace=True)
#view DataFrame
df
pointsassistsrebounds
team
A25511
B1278
C15710
D1496
E19126
F2395
G2599
H29412
</b>
We can use the following code to replace the ‘A’ value in the index column to be ‘P’ instead:
<b>#replace 'A' with 'P' in index
df.rename(index={'A':'P'}, inplace=True)
#view updated DataFrame
df
        pointsassistsrebounds
team
P25511
B1278
C15710
D1496
E19126
F2395
G2599
H29412</b>
Notice that the ‘A’ value in the original index has been replaced while all other values remained the same.
<h3>Example 2: Change Multiple Index Values in Pandas DataFrame</h3>
Suppose we have the same pandas DataFrame as before:
<b>#view DataFrame
df
pointsassistsrebounds
team
A25511
B1278
C15710
D1496
E19126
F2395
G2599
H29412
</b>
We can use the following code to replace the ‘A’ and ‘B’ values in the index column:
<b>#replace 'A' with 'P' and replace 'B' with 'Q' in index
df.rename(index={'A':'P', 'B':'Q'}, inplace=True)
#view updated DataFrame
df
pointsassistsrebounds
team
P25511
Q1278
C15710
D1496
E19126
F2395
G2599
H29412
</b>
Notice that the ‘A’ and ‘B’ values in the original index have been replaced while all other values remained the same.
You can use this exact same syntax to replace as many values as you’d like in the index.
<h2><span class="orange">Pandas: How to Check dtype for All Columns in DataFrame</span></h2>
You can use the following methods to check the data type ( dtype ) for columns in a pandas DataFrame:
<b>Method 1: Check dtype of One Column</b>
<b>df.column_name.dtype
</b>
<b>Method 2: Check dtype of All Columns</b>
<b>df.dtypes
</b>
<b>Method 3: Check which Columns have Specific dtype</b>
<b>df.dtypes[df.dtypes == 'int64']</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9],   'all_star': [True, False, False, True, True, True]})
#view DataFrame
print(df)
  team  points  assists  all_star
0    A      18        5      True
1    B      22        7     False
2    C      19        7     False
3    D      14        9      True
4    E      14       12      True
5    F      11        9      True</b>
<h3>Example 1: Check dtype of One Column</h3>
We can use the following syntax to check the data type of just the <b>points</b> column in the DataFrame:
<b>#check dtype of points column
df.points.dtype
dtype('int64')
</b>
From the output we can see that the <b>points</b> column has a data type of integer.
<h3>Example 2: Check dtype of All Columns</h3>
We can use the following syntax to check the data type of all columns in the DataFrame:
<b>#check dtype of all columns
df.dtypes
team        object
points       int64
assists      int64
all_star      bool
dtype: object
</b>
From the output we can see:
<b>team</b> column: object (this is the same as a string)
<b>points</b> column: integer
<b>assists</b> column: integer
<b>all_star</b> column: boolean
By using this one line of code, we can see the data type of each column in the DataFrame.
<h3>Example 3: Check which Columns have Specific dtype</h3>
We can use the following syntax to check which columns in the DataFrame have a data type of int64:
<b>#show all columns that have a class of int64
df.dtypes[df.dtypes == 'int64']
points     int64
assists    int64
dtype: object</b>
From the output we can see that the <b>points</b> and <b>assists</b> columns both have a data type of int64.
We can use similar syntax to check which columns have other data types.
For example, we can use the following syntax to check which columns in the DataFrame have a data type of object:
<b>#show all columns that have a class of object (i.e. string)
df.dtypes[df.dtypes == 'O']
team    object
dtype: object</b>
We can see that only the <b>team</b> column has a data type of ‘O’, which stands for object.
<h2><span class="orange">How to Check if Cell is Empty in Pandas DataFrame</span></h2>
You can use the following basic syntax to check if a specific cell is empty in a pandas DataFrame:
<b>#check if value in first row of column 'A' is empty
print(pd.isnull(df.loc[0, 'A']))
#print value in first row of column 'A'
print(df.loc[0, 'A'])
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Check if Cell is Empty in Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, np.nan, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, np.nan, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, np.nan]})
#view DataFrame
df
teampointsassistsrebounds
0A18.05.011.0
1BNaN7.08.0
2C19.07.010.0
3D14.09.06.0
4E14.0NaN6.0
5F11.09.05.0
6G20.09.09.0
7H28.04.0NaN
</b>
We can use the following code to check if the value in row index number <b>one</b> and column <b>points</b> is null:
<b>#check if value in index row 1 of column 'points' is empty
print(pd.isnull(df.loc[1, 'points']))
True</b>
A value of <b>True</b> indicates that the value in row index number one of the “points” column is indeed empty.
We can also use the following code to print the actual value in row index number one of the “points” column:
<b>#print value in index row 1 of column 'points'
print(df.loc[1, 'points'])
nan</b>
The output tells us that the value in row index number one of the “points” column is <b>nan</b>, which is equivalent to an empty cell.
<h2><span class="orange">Pandas: How to Check if Column Contains String</span></h2>
You can use the following methods to check if a column of a pandas DataFrame contains a string:
<b>Method 1: Check if Exact String Exists in Column</b>
<b>(df['col'].eq('exact_string')).any()</b>
<b>Method 2: Check if Partial String Exists in Column</b>
<b>df['col'].str.contains('partial_string').any()
</b>
<b>Method 3: Count Occurrences of Partial String in Column</b>
<b>df['col'].str.contains('partial_string').sum()</b>
This tutorial explains how to use each method in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C'],   'conference': ['East', 'East', 'South', 'West', 'West', 'East'],   'points': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
        teamconference   points
0AEast         11
1AEast     8
2ASouth        10
3BWest         6
4BWest         6
5CEast         5
</b>
<h3>Example 1: Check if Exact String Exists in Column</h3>
The following code shows how to check if the exact string ‘Eas’ exists in the <b>conference</b> column of the DataFrame:
<b>#check if exact string 'Eas' exists in conference column
(df['conference'].eq('Eas')).any()
False</b>
The output returns <b>False</b>, which tells us that the exact string ‘Eas’ does not exist in the <b>conference</b> column of the DataFrame.
<h3>Example 2: Check if Partial String Exists in Column</h3>
The following code shows how to check if the partial string ‘Eas’ exists in the conference</b> column of the DataFrame:
<b>#check if partial string 'Eas' exists in conference column
df['conference'].str.contains('Eas').any()
True</b>
The output returns <b>True</b>, which tells us that the partial string ‘Eas’ does exist in the <b>conference</b> column of the DataFrame.
<h3>Example 3: Count Occurrences of Partial String in Column</h3>
The following code shows how to count the number of times the partial string ‘Eas’ occurs in the <b>conference</b> column of the DataFrame:
<b>#count occurrences of partial string 'Eas' in conference column
df['conference'].str.contains('East').sum()
3</b>
The output returns <b>3</b>, which tells us that the partial string ‘Eas’ occurs 3 times in the <b>conference</b> column of the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">How to Check if Column Exists in Pandas (With Examples)</span></h2>
You can use the following methods to check if a column exists in a pandas DataFrame:
<b>Method 1: Check if One Column Exists</b>
<b>'column1' in df.columns
</b>
This will return <b>True</b> if ‘column1’ exists in the DataFrame, otherwise it will return <b>False</b>.
<b>Method 2: Check if Multiple Columns Exist</b>
<b>{'column1', 'column2'}.issubset(df.columns)</b>
This will return <b>True</b> if ‘column1’ <em>and</em> ‘column2’ exists in the DataFrame, otherwise it will return <b>False</b>.
The following examples shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
<h3>Example 1: Check if One Column Exists</h3>
We can use the following code to see if the column ‘team’ exists in the DataFrame:
<b>#check if 'team' column exists in DataFrame
'team' in df.columns
True
</b>
The column ‘team’ does exist in the DataFrame, so pandas returns a value of <b>True</b>.
We can also use an <b>if</b> statement to perform some operation if the column ‘team’ exists:
<b>#if 'team' exists, create new column called 'team_name'
if 'team' in df.columns:
    df['team_name'] = df['team']
    
#view updated DataFrame
print(df)
  team  points  assists  rebounds team_name
0    A      18        5        11         A
1    B      22        7         8         B
2    C      19        7        10         C
3    D      14        9         6         D
4    E      14       12         6         E
5    F      11        9         5         F
6    G      20        9         9         G
7    H      28        4        12         H
</b>
<h3>Example 2: Check if Multiple Columns Exist</h3>
We can use the following code to see if the columns ‘team’ <em>and</em> ‘player’ exist in the DataFrame:
<b>#check if 'team' and 'player' columns both exist in DataFrame
{'team', 'player'}.issubset(df.columns)
False</b>
The column ‘team’ exists in the DataFrame but ‘player’ does not, so pandas returns a value of <b>False</b>.
We could also use the following code to see if both ‘points’ and ‘assists’ exist in the DataFrame:
<b>#check if 'points' and 'assists' columns both exist in DataFrame
{'points', 'assists'}.issubset(df.columns)
True</b>
Both columns exist, so pandas returns a value of <b>True</b>.
We can then use an <b>if</b> statement to perform some operation if ‘points’ and ‘assists’ both exist:
<b>#if both exist, create new column called 'total' that finds sum of points and assists
if {'points', 'assists'}.issubset(df.columns):
    df['total'] = df['points'] + df['assists']
    
#view updated DataFrame
print(df)
     team   points assists rebounds  total
0A18 5 1123
1B22 7  829
2C19 7 1026
3D14 9  623
4E1412  626
5F11 9  520
6G20 9  929
7H28 4 1232
</b>
Since ‘points’ and ‘assists’ both exist in the DataFrame, pandas went ahead and created a new column called ‘total’ that shows the sum of the ‘points’ and ‘assists’ columns.
<h2><span class="orange">Pandas: How to Check if Multiple Columns are Equal</span></h2>
You can use the following methods to check if multiple columns are equal in pandas:
<b>Method 1: Check if All Columns Are Equal</b>
<b>df['matching'] = df.eq(df.iloc[:, 0], axis=0).all(1)
</b>
<b>Method 2: Check if Specific Columns Are Equal</b>
<b>df['matching'] = df.apply(lambda x: x.col1 == x.col3 == x.col4, axis=1)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A': [4, 0, 3, 3, 6, 8, 7],   'B': [4, 2, 3, 5, 6, 4, 7],   'C': [4, 0, 3, 3, 5, 10, 7],   'D': [4, 0, 3, 3, 3, 8, 7]})
#view DataFrame
print(df)
   A  B   C  D
0  4  4   4  4
1  0  2   0  0
2  3  3   3  3
3  3  5   3  3
4  6  6   5  3
5  8  4  10  8
6  7  7   7  7
</b>
<h2>Example 1: Check if All Columns Are Equal</h2>
We can use the following syntax to check if the value in every column in the DataFrame is equal for each row:
<b>#create new column that checks if all columns match in each row
df['matching'] = df.eq(df.iloc[:, 0], axis=0).all(1)
#view updated DataFrame
print(df)
   A  B   C  D  matching
0  4  4   4  4      True
1  0  2   0  0     False
2  3  3   3  3      True
3  3  5   3  3     False
4  6  6   5  3     False
5  8  4  10  8     False
6  7  7   7  7      True
</b>
If the value in each column is equal, then the matching column returns <b>True</b>.
Otherwise, it returns <b>False</b>.
Note that you can convert <b>True</b> and <b>False</b> values to <b>1</b> and <b>0</b> by using <b>astype(int)</b> as follows:
<b>#create new column that checks if all columns match in each row
df['matching'] = df.eq(df.iloc[:, 0], axis=0).all(1).astype(int)
#view updated DataFrame
print(df)
   A  B   C  D  matching
0  4  4   4  4         1
1  0  2   0  0         0
2  3  3   3  3         1
3  3  5   3  3         0
4  6  6   5  3         0
5  8  4  10  8         0
6  7  7   7  7         1
</b>
<h2>Example 2: Check if Specific Columns Are Equal</h2>
We can use the following syntax to check if the value in columns A, C, and D in the DataFrame are equal for each row:
<b>#create new column that checks if values in columns A, C, and D are equal
df['matching'] = df.apply(lambda x: x.A == x.C == x.D, axis=1)
#view updated DataFrame
print(df)
   A  B   C  D  matching
0  4  4   4  4      True
1  0  2   0  0      True
2  3  3   3  3      True
3  3  5   3  3      True
4  6  6   5  3     False
5  8  4  10  8     False
6  7  7   7  7      True
</b>
If the value in columns A, C, and D are equal, then the matching column returns <b>True</b>.
Otherwise, it returns <b>False</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">Pandas: Check if Row in One DataFrame Exists in Another</span></h2>
You can use the following syntax to add a new column to a pandas DataFrame that shows if each row exists in another DataFrame:
<b>#merge two DataFrames on specific columns
all_df = pd.merge(df1, df2, on=['column1', 'column2'], how='left', indicator='exists')
#drop unwanted columns
all_df = all_df.drop('column3', axis=1)
#add column that shows if each row in one DataFrame exists in another
all_df['exists'] = np.where(all_df.exists == 'both', True, False)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Check if Row in One Pandas DataFrame Exist in Another</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'],     'points' : [12, 15, 22, 29, 24]}) 
print(df1)
  team  points
0    A      12
1    B      15
2    C      22
3    D      29
4    E      24
#create second DataFrame
df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],    'points' : [12, 29, 15, 19, 10],    'assists' : [4, 7, 7, 10, 12]})
print(df2)
  team  points  assists
0    A      12        4
1    D      29        7
2    F      15        7
3    G      19       10
4    H      10       12
</b>
We can use the following syntax to add a column called <b>exists</b> to the first DataFrame that shows if each value in the <b>team</b> and <b>points</b> column of each row exists in the second DataFrame:
<b>import numpy as np
#merge two dataFrames and add indicator column
all_df = pd.merge(df1, df2, on=['team', 'points'], how='left', indicator='exists')
#drop assists columns
all_df = all_df.drop('assists', axis=1)
#add column to show if each row in first DataFrame exists in second
all_df['exists'] = np.where(all_df.exists == 'both', True, False)
#view updated DataFrame
print (all_df)
  team  points  exists
0    A      12    True
1    B      15   False
2    C      22   False
3    D      29    True
4    E      24   False
</b>
The new <b>exists</b> column shows if each value in the <b>team</b> and <b>points</b> column of each row exists in the second DataFrame.
From the output we can see:
A Team value of <b>A</b> and points value of <b>12</b> does exist in the second DataFrame.
A Team value of <b>B</b> and points value of <b>15</b> does not exist in the second DataFrame.
A Team value of <b>C</b> and points value of <b>22</b> does not exist in the second DataFrame.
A Team value of <b>D</b> and points value of <b>29</b> does exist in the second DataFrame.
A Team value of <b>E</b> and points value of <b>24</b> does not exist in the second DataFrame.
Also note that you can specify values other than True and False in the <b>exists</b> column by changing the values in the NumPy <b>where()</b> function.
For example, you could instead use ‘exists’ and ‘not exists’ as follows:
<b>#add column to show if each row in first DataFrame exists in second
all_df['exists'] = np.where(all_df.exists == 'both', 'exists', 'not exists')
#view updated DataFrame
print (all_df)
  team  points      exists
0    A      12      exists
1    B      15  not exists
2    C      22  not exists
3    D      29      exists
4    E      24  not exists
</b>
Notice that the values in the <b>exists</b> column have been changed.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: Add Column from One DataFrame to Another 
 Pandas: Get Rows Which Are Not in Another DataFrame 
 Pandas: How to Check if Multiple Columns are Equal 
<h2><span class="orange">Pandas: Check if String Contains Multiple Substrings</span></h2>
You can use the following methods to check if a string in a pandas DataFrame contains multiple substrings:
<b>Method 1: Check if String Contains One of Several Substrings</b>
<b>df['string_column'].str.contains('|'.join(['string1', 'string2']))
</b>
<b>Method 2: Check if String Contains Several Substrings</b>
<b>df['string_column'].str.contains(r'^(?=.*string1)(?=.*string2)')</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['Good East Team', 'Good West Team', 'Great East Team',             'Great West Team', 'Bad East Team', 'Bad West Team'],   'points' : [93, 99, 105, 110, 85, 88]})
#view DataFrame
print(df)
              team  points
0   Good East Team      93
1   Good West Team      99
2  Great East Team     105
3  Great West Team     110
4    Bad East Team      85
5    Bad West Team      88
</b>
<h2>
<b>Example 1: Check if String Contains One of Several Substrings</b>
</h2>
We can use the following syntax to check if each string in the <b>team</b> column contains either the substring “Good” <b>or</b> “East”:
<b>#create new column that checks if each team name contains 'Good' or 'East'
df['good_or_east'] = df['team'].str.contains('|'.join(['Good', 'East']))
#view updated DataFrame
print(df)
              team  points  good_or_east
0   Good East Team      93          True
1   Good West Team      99          True
2  Great East Team     105          True
3  Great West Team     110         False
4    Bad East Team      85          True
5    Bad West Team      88         False</b>
The new <b>good_or_east</b> column returns the following values:
<b>True</b> if team contains “Good” or “East”
<b>False</b> if team contains neither “Good” nor “East”
<b>Note</b>: The <b>|</b> operator stands for “or” in pandas.
<h2>
<b>Example 2: Check if String Contains Several Substrings</b>
</h2>
We can use the following syntax to check if each string in the <b>team</b> column contains the substring “Good” <b>and </b>“East”:
<b>#create new column that checks if each team name contains 'Good' and 'East'
df['good_and_east'] = df['team'].str.contains(r'^(?=.*Good)(?=.*East)')
#view updated DataFrame
print(df)
              team  points  good_and_east
0   Good East Team      93           True
1   Good West Team      99          False
2  Great East Team     105          False
3  Great West Team     110          False
4    Bad East Team      85          False
5    Bad West Team      88          False</b>
The new <b>good_and_east</b> column returns the following values:
<b>True</b> if team contains “Good” and “East”
<b>False</b> if team doesn’t contain “Good” and “East”
Notice that only one <b>True</b> value is returned since there is only one team name that contains the substring “Good” <b>and</b> the substring “East.”
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: Add Column from One DataFrame to Another 
 Pandas: Get Rows Which Are Not in Another DataFrame 
 Pandas: How to Check if Multiple Columns are Equal 
<h2><span class="orange">Pandas: How to Check if Two DataFrames Are Equal</span></h2>
You can use the following basic syntax to check if two pandas DataFrames are equal:
<b>df1.equals(df2)
</b>
This will return a value of <b>True</b> or <b>False</b>.
If two DataFrames are not equal, then you can use the following syntax to find the rows in the second DataFrame that do not exist in the first DataFrame:
<b>#perform outer join on two DataFrames
all_df = df1.merge(df2, indicator=True, how='outer')
#find which rows only exist in second DataFrame
only_df2 = all_df[all_df['_merge'] == 'right_only']
only_df2 = only_df2.drop('_merge', axis=1)</b>
The following example shows how to use this syntax in practice.
<h2>Example: Check if Two pandas DataFrames Are Equal</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team' : ['A', 'B', 'C', 'D', 'E'],     'points' : [12, 15, 22, 29, 24]}) 
print(df1)
  team  points
0    A      12
1    B      15
2    C      22
3    D      29
4    E      24
#create second DataFrame
df2 = pd.DataFrame({'team' : ['A', 'D', 'F', 'G', 'H'],    'points' : [12, 29, 15, 19, 10]})
print(df2)
  team  points
0    A      12
1    D      29
2    F      15
3    G      19
4    H      10
</b>
We can use the following syntax to check if the two DataFrames are equal:
<b>#check if two DataFrames are equal</b>
<b>df1.equals(df2)
False</b>
The output returns <b>False</b>, which means the two DataFrames are not equal.
We can then use the following syntax to find which rows exist in the second DataFrame but not in the first:
<b>#perform outer join on two DataFrames
all_df = df1.merge(df2, indicator=True, how='outer')
#find which rows only exist in second DataFrame
only_df2 = all_df[all_df['_merge'] == 'right_only']
only_df2 = only_df2.drop('_merge', axis=1)
#view results
print(only_df2)
  team  points
5    F      15
6    G      19
7    H      10</b>
From the output we can see that there are three rows in the second DataFrame that do not exist in the first DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: Add Column from One DataFrame to Another 
 Pandas: Get Rows Which Are Not in Another DataFrame 
 Pandas: How to Check if Multiple Columns are Equal 
<h2><span class="orange">Pandas: How to Check if Value Exists in Column</span></h2>
You can use the following methods to check if a particular value exists in a column of a pandas DataFrame:
<b>Method 1: Check if One Value Exists in Column</b>
<b>22 in df['my_column'].values</b>
<b>Method 2: Check if One of Several Values Exist in Column</b>
<b>df['my_column'].isin([44, 45, 22]).any()
</b>
The following examples show how to use each method in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
<h2>Example 1: Check if One Value Exists in Column</h2>
The following code shows how to check if the value <b>22</b> exists in the <b>points</b> column:
<b>#check if 22 exists in the 'points' column
22 in df['points'].values
True</b>
The output returns <b>True</b>, which tells us that the value <b>22</b> does exist in the <b>points</b> column.
We can use the same syntax with string columns as well.
For example, the following code shows how to check if the string ‘J’ exists in the <b>team</b> column:
<b>#check if 'J' exists in the 'team' column
'J' in df['team'].values
False</b>
The output returns <b>False</b>, which tells us that the string ‘J’ does not exist in the <b>team </b>column.
<h2>Example 2: Check if One of Several Values Exist in Column</h2>
The following code shows how to check if any of the values in the list [44, 45, 22] exist in the <b>points</b> column:
<b>#check if 44, 45 or 22 exist in the 'points' column
df['points'].isin([44, 45, 22]).any()
True</b>
The output returns <b>True</b>, which tells us that at least one of the values in the list [44, 45, 22] exists in the <b>points</b> column of the DataFrame.
We can use the same syntax with string columns as well.
For example, the following code shows how to check if any string in the list [‘J’, ‘K’, ‘L’] exists in the <b>team</b> column:
<b>#check if J, K, or L exists in the 'team' column
df['team'].isin(['J', 'K', 'L']).any() 
False</b>
The output returns <b>False</b>, which tells us that none of the strings in the list exist in the <b>team </b>column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">How to Slice Pandas DataFrame into Chunks</span></h2>
You can use the following basic syntax to slice a pandas DataFrame into smaller chunks:
<b>#specify number of rows in each chunk
n=3
#split DataFrame into chunks
list_df = [df[i:i+n] for i in range(0,len(df),n)]
</b>
You can then access each chunk by using the following syntax:
<b>#access first chunk
<b>list_df[0]</b>
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Split Pandas DataFrame into Chunks</h2>
Suppose we have the following pandas DataFrame with nine rows that contain information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],   'points': [18, 22, 19, 14, 14, 11, 20, 28, 23],   'assists': [5, 7, 7, 9, 12, 9, 9, 4, 11],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 10]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
8    I      23       11        10</b>
We can use the following syntax to split the DataFrame into chunks where each chunk has 3 rows:
<b>#specify number of rows in each chunk
n=3
#split DataFrame into chunks
list_df = [df[i:i+n] for i in range(0,len(df),n)]</b>
We can then use the following syntax to access each chunk:
<b>#view first chunk
print(list_df[0])
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
#view second chunk
print(list_df[1])
  team  points  assists  rebounds
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
#view third chunk
print(list_df[2])
  team  points  assists  rebounds
6    G      20        9         9
7    H      28        4        12
8    I      23       11        10
</b>
Notice that each chunk contains three rows, just as we specified.
Note that in this example we used a DataFrame with only nine rows as a simple example.
In practice, you’ll likely be working with a DataFrame with hundreds of thousands or even millions of rows.
You can use the same syntax that we used in this example to split your DataFrame into chunks of specific sizes.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Split DataFrame By Column Value 
 Pandas: How to Split String Column into Multiple Columns 
 Pandas: How to Split a Column of Lists into Multiple Columns 
<h2><span class="orange">Pandas: How to Coalesce Values from Multiple Columns into One</span></h2>
You can use the following methods to coalesce the values from multiple columns of a pandas DataFrame into one column:
<b>Method 1: Coalesce Values by Default Column Order</b>
<b>df['coalesce'] = df.bfill(axis=1).iloc[:, 0]
</b>
<b>Method 2: Coalesce Values Using Specific Column Order</b>
<b>df['coalesce'] = df[['col3', 'col1', 'col2']].bfill(axis=1).iloc[:, 0]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'points': [np.nan, np.nan, 19, np.nan, 14],   'assists': [np.nan, 7, 7, 9, np.nan],   'rebounds': [3, 4, np.nan, np.nan, 6]})
#view DataFrame
print(df)
   points  assists  rebounds
0     NaN      NaN       3.0
1     NaN      7.0       4.0
2    19.0      7.0       NaN
3     NaN      9.0       NaN
4    14.0      NaN       6.0
</b>
<h3>Method 1: Coalesce Values by Default Column Order</h3>
The following code shows how to coalesce the values in the points, assists, and rebounds columns into one column, using the first non-null value across the three columns as the coalesced value:
<b>#create new column that contains first non-null value from three existing columns 
df['coalesce'] = df.bfill(axis=1).iloc[:, 0]
#view updated DataFrame
print(df)
   points  assists  rebounds  coalesce
0     NaN      NaN       3.0       3.0
1     NaN      7.0       4.0       7.0
2    19.0      7.0       NaN      19.0
3     NaN      9.0       NaN       9.0
4    14.0      NaN       6.0      14.0
</b>
Here’s how the value in the <b>coalesce</b> column was chosen:
First row: The first non-null value was <b>3.0</b>.
Second row: The first non-null value was <b>7.0</b>.
Third row: The first non-null value was <b>19.0</b>.
Fourth row: The first non-null value was <b>9.0</b>.
Fifth row: The first non-null value was <b>14.0</b>.
<h3>Method 2: Coalesce Values Using Specific Column Order</h3>
The following code shows how to coalesce the values in the three columns by analyzing the columns in the following order: assists, rebounds, points.
<b>#coalesce values in specific column order
df['coalesce'] = df[['assists', 'rebounds', 'points']].bfill(axis=1).iloc[:, 0]
#view updated DataFrame
print(df)
   points  assists  rebounds  coalesce
0     NaN      NaN       3.0       3.0
1     NaN      7.0       4.0       7.0
2    19.0      7.0       NaN       7.0
3     NaN      9.0       NaN       9.0
4    14.0      NaN       6.0       6.0
</b>
Here’s the logic that was used to decide which value to place in the <b>coalesce</b> column:
If the value in the <b>assists</b> column is non-null then use that value.
Otherwise, if the value in the <b>rebounds</b> column is non-null then use that value.
Otherwise, if the value in the <b>points</b> column is non-null then use that value.
<b>Note</b>: You can find the complete documentation for the <b>bfill()</b> function  here .
<h2><span class="orange">Pandas: How to Quickly Convert Column to List</span></h2>
You can use one of the following methods to convert a column in a pandas DataFrame to a list:
<b>Method 1: Use tolist()</b>
<b>df['my_column'].tolist()</b>
<b>Method 2: Use list()</b>
<b>list(df['my_column'])</b>
Both methods will return the exact same result.
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [99, 90, 93, 86, 88, 82],   'assists': [33, 28, 31, 39, 34, 30]})
#view DataFrame
print(df)
  team  points  assists
0    A      99       33
1    A      90       28
2    A      93       31
3    B      86       39
4    B      88       34
5    B      82       30</b>
<h3>Method 1: Convert Column to List Using tolist()</h3>
The following code shows how to use the <b>tolist()</b> function to convert the ‘points’ column in the DataFrame to a list:
<b>#convert column to list
my_list = df['points'].tolist()
#view list
print(my_list)
[99, 90, 93, 86, 88, 82]</b>
We can confirm that the result is a list by using the <b>type()</b> function:
<b>#check data type
type(my_list)
list
</b>
<h3>Method 2: Convert Column to List Using list()</h3>
The following code shows how to use the <b>list()</b> function to convert the ‘points’ column in the DataFrame to a list:
<b>#convert column to list
my_list = list(df['points'])
#view list
print(my_list)
[99, 90, 93, 86, 88, 82]</b>
We can confirm that the result is a list by using the <b>type()</b> function:
<b>#check data type
type(my_list)
list</b>
Notice that both methods return the exact same results.
Note that for extremely large DataFrames, the <b>tolist()</b> method tends to perform the fastest.
<h2><span class="orange">Pandas: How to Convert Specific Columns to NumPy Array</span></h2>
You can use the following methods to convert specific columns in a pandas DataFrame to a NumPy array:
<b>Method 1: Convert One Column to NumPy Array</b>
<b>column_to_numpy = df['col1'].to_numpy()
</b>
<b>Method 2: Convert Multiple Columns to NumPy Array</b>
<b>columns_to_numpy = df[['col1', 'col3', 'col4']].to_numpy()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
<h2>Example 1: Convert One Column to NumPy Array</h2>
The following code shows how to convert the <b>points</b> column in the DataFrame to a NumPy array:
<b>#convert points column to NumPy array
column_to_numpy = df['points'].to_numpy()
#view result
print(column_to_numpy)
[18 22 19 14 14 11 20 28]
</b>
We can confirm that the result is indeed a NumPy array by using the <b>type()</b> function:
<b>#view data type
print(type(column_to_numpy))
&lt;class 'numpy.ndarray'>
</b>
<h2>Example 2: Convert Multiple Columns to NumPy Array</h2>
The following code shows how to convert the <b>team </b>and <b>assists</b> columns in the DataFrame to a multidimensional NumPy array:
<b>#convert team and assists columns to NumPy array
columns_to_numpy = df[['team', 'assists']].to_numpy()
#view result
print(columns_to_numpy)
[['A' 5]
 ['B' 7]
 ['C' 7]
 ['D' 9]
 ['E' 12]
 ['F' 9]
 ['G' 9]
 ['H' 4]]
</b>
We can confirm that the result is indeed a NumPy array by using the <b>type()</b> function:
<b>#view data type
print(type(columns_to_numpy))
&lt;class 'numpy.ndarray'>
</b>
We can also use the <b>shape</b> function to view the shape of the resulting NumPy array:
<b>#view shape of array
print(columns_to_numpy.shape)
(8, 2)
</b>
We can see that the resulting NumPy array has 8 rows and 2 columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">Pandas: How to Set Column Widths</span></h2>
By default, Jupyter notebooks only display a maximum width of <b>50</b> for columns in a pandas DataFrame.
However, you can force the notebook to show the entire width of each column in the DataFrame by using the following syntax:
<b>pd.set_option('display.max_colwidth', None)
</b>
This will set the max column width value for the entire Jupyter notebook session.
If you only want to temporarily display an entire column width, you can use the following syntax:
<b>from pandas import option_context
with option_context('display.max_colwidth', None):
    print(df)
</b>
Lastly, you can reset the default column width settings in a Jupyter notebook by using the following syntax:
<b>pd.reset_option('display.max_colwidth')
</b>
The following example shows how to use these functions in practice.
<h2>Example: Set Column Widths in Pandas</h2>
Suppose we create a pandas DataFrame with some extremely long strings in one column:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'string_column': ['A really really long string that contains lots of words', 'More words', 'Words', 'Cool words', 'Hey', 'Hi', 'Sup', 'Yo'],   'value_column': [12, 15, 24, 24, 14, 19, 12, 38]})
#view DataFrame
print(df)
                       string_column  value_column
0  A really really long string that contains lots...            12
1                                         More words            15
2                                              Words            24
3                                         Cool words            24
4                                                Hey            14
5                                                 Hi            19
6                                                Sup            12
7                                                 Yo            38
</b>
By default, pandas cuts off the <b>string_column</b> to only have a width of 50.
To display the entire width of the column, we can use the following syntax:
<b>#specify no max value for the column width
pd.set_option('display.max_colwidth', None)
#view DataFrame
print(df)
                             string_column  value_column
0  A really really long string that contains lots of words            12
1                                               More words            15
2                                                    Words            24
3                                               Cool words            24
4                                                      Hey            14
5                                                       Hi            19
6                                                      Sup            12
7                                                       Yo            38
</b>
Notice that all of the text in the <b>string_column</b> is now shown.
Note that using this method will set the max column width for the entire Jupyter session.
To only temporarily display the max column width, we can use the following syntax:
<b>from pandas import option_context
with option_context('display.max_colwidth', None):
    print(df)
                             string_column  value_column
0  A really really long string that contains lots of words            12
1                                               More words            15
2                                                    Words            24
3                                               Cool words            24
4                                                      Hey            14
5                                                       Hi            19
6                                                      Sup            12
7                                                       Yo            38
</b>
To reset the default settings and only display a max width of 50 for each column, we can use the following syntax:
<b>pd.reset_option('display.max_colwidth')</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Show All Columns of Pandas DataFrame 
 How to Show All Rows of a Pandas DataFrame 
 Pandas: How to Get Cell Value from DataFrame 
<h2><span class="orange">Pandas: How to Combine Rows with Same Column Values</span></h2>
You can use the following basic syntax to combine rows with the same column values in a pandas DataFrame:
<b>#define how to aggregate various fields
agg_functions = {'field1': 'first', 'field2': 'sum', 'field': 'sum'}
#create new DataFrame by combining rows with same id values
df_new = df.groupby(df['id']).aggregate(agg_functions)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Combine Rows with Same Column Values in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about sales and returns made by various employees at a company:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'id': [101, 101, 102, 103, 103, 103],   'employee': ['Dan', 'Dan', 'Rick', 'Ken', 'Ken', 'Ken'],   'sales': [4, 1, 3, 2, 5, 3],   'returns': [1, 2, 2, 1, 3, 2]})
#view DataFrame
print(df)
    id employee  sales  returns
0  101      Dan      4        1
1  101      Dan      1        2
2  102     Rick      3        2
3  103      Ken      2        1
4  103      Ken      5        3
5  103      Ken      3        2</b>
We can use the following syntax to combine rows that have the same value in the <b>id</b> column and then aggregate the remaining columns:
<b>#define how to aggregate various fields
agg_functions = {'employee': 'first', 'sales': 'sum', 'returns': 'sum'}
#create new DataFrame by combining rows with same id values
df_new = df.groupby(df['id']).aggregate(agg_functions)
#view new DataFrame
print(df_new)
    employee  sales  returns
id                          
101      Dan      5        3
102     Rick      3        2
103      Ken     10        6
</b>
The new DataFrame combined all of the rows in the previous DataFrame that had the same value in the <b>id</b> column and then calculated the sum of the values in the <b>sales</b> and <b>returns</b> columns.
<b>Note</b>: Refer to the  pandas documentation  for a complete list of aggregations available to use with the <b>GroupBy()</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Find the Difference Between Two Columns 
 Pandas: How to Find the Difference Between Two Rows 
 Pandas: How to Sort Columns by Name 
<h2><span class="orange">How to Combine Two Columns in Pandas (With Examples)</span></h2>
You can use the following syntax to combine two text columns into one in a pandas DataFrame:
<b>df['new_column'] = df['column1'] + df['column2']
</b>
If one of the columns isn’t already a string, you can convert it using the <b>astype(str)</b> command:
<b>df['new_column'] = df['column1'].astype(str) + df['column2']</b>
And you can use the following syntax to combine multiple text columns into one:
<b>df['new_column'] = df[['col1', 'col2', 'col3', ...]].agg(' '.join, axis=1) </b>
The following examples show how to combine text columns in practice.
<h3>Example 1: Combine Two Columns</h3>
The following code shows how to combine two text columns into one in a pandas DataFrame:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#combine first and last name column into new column, with space in between 
df['full_name'] = df['first'] + ' ' + df['last']
#view resulting dataFrame
df
teamfirstlast pointsfull_name
0MavsDirkNowitzki 26Dirk Nowitzki
1LakersKobeBryant 31Kobe Bryant
2SpursTimDuncan 22Tim Duncan
3CavsLebronJames 29Lebron James
</b>
We joined the first and last name column with a space in between, but we could also use a different separator such as a dash:
<b>#combine first and last name column into new column, with dash in between 
df['full_name'] = df['first'] + '-' + df['last']
#view resulting dataFrame
df
teamfirstlast pointsfull_name
0MavsDirkNowitzki 26Dirk-Nowitzki
1LakersKobeBryant 31Kobe-Bryant
2SpursTimDuncan 22Tim-Duncan
3CavsLebronJames 29Lebron-James</b>
<h3>Example 2: Convert to Text & Combine Two Columns</h3>
The following code shows how to convert one column to text, then join it to another column:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#convert points to text, then join to last name column 
df['name_points'] = df['last'] + df['points'].astype(str)
#view resulting dataFrame
df
        teamfirstlast pointsname_points
0MavsDirkNowitzki 26Nowitzki26
1LakersKobeBryant 31Bryant31
2SpursTimDuncan 22Duncan22
3CavsLebronJames 29James29</b>
<h3>Example 3: Combine More Than Two Columns</h3>
The following code shows how to join multiple columns into one column:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#join team, first name, and last name into one column
df['team_and_name'] = df[['team', 'first', 'last']].agg(' '.join, axis=1)
#view resulting dataFrame
df
teamfirstlast pointsteam_name
0MavsDirkNowitzki 26Mavs Dirk Nowitzki
1LakersKobeBryant 31Lakers Kobe Bryant
2SpursTimDuncan 22Spurs Tim Duncan
3CavsLebronJames 29Cavs Lebron James</b>
<h2><span class="orange">Pandas: How to Compare Columns in Two Different DataFrames</span></h2>
You can use the following methods to compare columns in two different pandas DataFrames:
<b>Method 1: Count Matching Values Between Columns</b>
<b>df1['my_column'].isin(df2['my_column']).value_counts()
</b>
<b>Method 2: Display Matching Values Between Columns</b>
<b>pd.merge(df1, df2, on=['my_column'], how='inner')</b>
The following examples show how to use each method with the following pandas DataFrames:
<b>import numpy as np
import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['Mavs', 'Rockets', 'Spurs', 'Heat', 'Nets'],    'points': [22, 30, 15, 17, 14]})
#view DataFrame
print(df1)
      team  points
0     Mavs      22
1  Rockets      30
2    Spurs      15
3     Heat      17
4     Nets      14
#create second DataFrame
df2 = pd.DataFrame({'team': ['Mavs', 'Thunder', 'Spurs', 'Nets', 'Cavs'],    'points': [25, 40, 31, 32, 22]})
#view DataFrame
print(df2)
      team  points
0     Mavs      25
1  Thunder      40
2    Spurs      31
3     Nets      32
4     Cavs      22
</b>
<h2>Example 1: Count Matching Values Between Columns</h2>
The following code shows how to count the number of matching values between the <b>team</b> columns in each DataFrame:
<b>#count matching values in team columns
df1['team'].isin(df2['team']).value_counts()
True     3
False    2
Name: team, dtype: int64
</b>
We can see that the two DataFrames have <b>3</b> team names in common and <b>2</b> team names that are different.
<h2>Example 2: Display Matching Values Between Columns</h2>
The following code shows how to display the actual matching values between the <b>team</b> columns in each DataFrame:
<b>#display matching values between team columns
pd.merge(df1, df2, on=['team'], how='inner')
teampoints_x  points_y
0Mavs22  25
1Spurs15  31
2Nets14  32</b>
From the output we can see that the two DataFrames have the following values in common in the <b>team</b> columns:
Mavs
Spurs
Nets
<b>Related:</b>  How to Do an Inner Join in Pandas (With Example) 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">Pandas: How to Apply Conditional Formatting to Cells</span></h2>
You can use the <b>df.style.applymap()</b> function to apply conditional formatting to cells in a pandas DataFrame.
The following example shows how to use this function in practice.
<h2>Example: Apply Conditional Formatting to Cells in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [4, 5, 5, 4, 9, 12, 11, 8],   'rebounds': [3, 9, 12, 4, 4, 9, 8, 2]})
#view DataFrame
print(df)
   points  assists  rebounds
0      18        4         3
1      22        5         9
2      19        5        12
3      14        4         4
4      14        9         4
5      11       12         9
6      20       11         8
7      28        8         2
</b>
We can use the following code to apply a light green background to each cell in the DataFrame that has a value less than 10:
<b>#define function for conditional formatting
def cond_formatting(x):
    if x &lt; 10:
        return 'background-color: lightgreen'
    else:
        return None
    
#display DataFrame with conditional formatting applied    
df.style.applymap(cond_formatting)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/condi1.jpg">
Notice that each cell in the DataFrame that has a value less than 10 now has a light green background.
<b>Note</b>: If the conditional formatting is not working in a Jupyter notebook, be sure to run the command <b>%pip install Jinja2</b> first.
We can also use the <b>color</b> and <b>font-weight</b> arguments to apply more complex conditional formatting.
The following example shows how to do so:
<b>#define function for conditional formatting
def cond_formatting(x):
    if x &lt; 10:
        return 'background-color: lightgreen; color: red; font-weight: bold'
    elif x &lt; 15:
        return 'background-color: yellow'
    else:
        return None
    
#display DataFrame with conditional formatting applied    
df.style.applymap(cond_formatting)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/condi2.jpg">
Here is how the conditional formatting function worked in this example:
For values less than <b>10</b>, use a light green background with bold red font
For values ≥ <b>10</b> but less than <b>15</b>, use a yellow background
For values greater than <b>15</b>, use no conditional formatting 
Feel free to use as many <b>if</b>, <b>elif</b>, and <b>else</b> functions as you’d like to apply as many conditional formatting rules to the cells in the DataFrame as you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add Table Title to Pandas DataFrame 
 How to Show All Rows of a Pandas DataFrame 
 How to Show All Columns of a Pandas DataFrame 
<h2><span class="orange">How to Convert Boolean Values to Integer Values in Pandas</span></h2>
You can use the following basic syntax to convert a column of boolean values to a column of integer values in pandas:
<b>df.column1 = df.column1.replace({True: 1, False: 0})
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Convert Boolean to Integer in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],   'points': [18, 22, 19, 14, 14, 11, 20],   'playoffs': [True, False, False, False, True, False, True]})
#view DataFrame
df</b>
We can use  dtypes  to quickly check the data type of each column:
<b>#check data type of each column
df.dtypes
team        object
points       int64
playoffs      bool
dtype: object</b>
We can see that the ‘playoffs’ column is of type <b>boolean</b>.
We can use the following code to quickly convert the True/False values in the ‘playoffs’ column into 1/0 integer values:
<b>#convert 'playoffs' column to integer
df.playoffs = df.playoffs.replace({True: 1, False: 0})
#view updated DataFrame
df
teampointsplayoffs
0A181
1B220
2C190
3D140
4E141
5F110
6G201
</b>
Each <b>True</b> value was converted to <b>1</b> and each <b>False</b> value was converted to <b>0</b>.
We can use dtypes again to verify that the ‘playoffs’ column is now an integer:
<b>#check data type of each column</b>
<b>df.dtypes
team        object
points       int64
playoffs     int64
dtype: object
</b>
We can see that the ‘playoffs’ column is now of type <b>int64</b>.
<h2><span class="orange">How to Convert Pandas DataFrame Columns to int</span></h2>
You can use the following syntax to convert a column in a pandas DataFrame to an integer type:
<b>df['col1'] = df['col1'].astype(int)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert One Column to Integer</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': ['25', '20', '14', '16', '27'],   'assists': ['5', '7', '7', '8', '11']})
#view data types for each column
df.dtypes
player     object
points     object
assists    object
dtype: object
</b>
We can see that none of the columns currently have an integer data type.
The following code shows how to convert the ‘points’ column in the DataFrame to an integer type:
<b>#convert 'points' column to integer
df['points'] = df['points'].astype(int)
#view data types of each column
df.dtypes
player     object
points      int64
assists    object
dtype: object
</b>
We can see that the ‘points’ column is now an integer, while all other columns remained unchanged.
<h3>Example 2: Convert Multiple Columns to Integer</h3>
The following code shows how to convert multiple columns in a DataFrame to an integer:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': ['25', '20', '14', '16', '27'],   'assists': ['5', '7', '7', '8', '11']})
#convert 'points' and 'assists' columns to integer
df[['points', 'assists']] = df[['points', 'assists']].astype(int)
#view data types for each column
df.dtypes
player     object
points      int64
assists     int64
dtype: object</b>
We can see that the ‘points’ and ‘assists’ columns have been converted to integer while the ‘player’ column remains unchanged.
<h2><span class="orange">Pandas: How to Convert Date to YYYYMMDD Format</span></h2>
You can use the following syntax to convert a date column in a pandas DataFrame to a YYYYMMDD format:
<b>#convert date column to datetime
df['date_column'] = pd.to_datetime(df['date_column'])
#convert date to YYYYMMDD format
df['date_column'] = df['date_column'].dt.strftime('%Y%m%d').astype(int)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Convert Date to YYYYMMDD Format in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/1/2022', freq='MS', periods=8),   'sales': [18, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
        date  sales
0 2022-01-01     18
1 2022-02-01     22
2 2022-03-01     19
3 2022-04-01     14
4 2022-05-01     14
5 2022-06-01     11
6 2022-07-01     20
7 2022-08-01     28</b>
Now suppose that we would like to format the values in the <b>date</b> column as YYYYMMDD.
We can use the following syntax to do so:
<b>#convert date column to datetime
df['date'] = pd.to_datetime(df['date'])
#convert date to YYYYMMDD format
df['date'] = df['date'].dt.strftime('%Y%m%d').astype(int)
#view updated DataFrame
print(df)
       date  sales
0  20220101     18
1  20220201     22
2  20220301     19
3  20220401     14
4  20220501     14
5  20220601     11
6  20220701     20
7  20220801     28</b>
Notice that the values in the <b>date</b> column are now formatted in a YYYYMMDD format.
Note that in this example, the <b>date</b> column already had a class of datetime.
However, we can use the <b>to_datetime()</b> function anyway to ensure that a given column has a class of datetime before applying a YYYYMMDD format.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Select Rows Between Two Dates in Pandas 
 How to Calculate a Difference Between Two Dates in Pandas 
<h2><span class="orange">How to Convert DateTime to String in Pandas (With Examples)</span></h2>
You can use the following basic syntax to convert a column from DateTime to string in pandas:
<b>df['column_name'].dt.strftime('%Y-%m-%d')
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Convert DateTime to String in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the sales made by some store on four different days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': pd.to_datetime(pd.Series(['20210101', '20210105',                                    '20210106', '20210109'])),   'sales': [1440, 1845, 2484, 2290]})
#view DataFrame
df
       day     sales
02021-01-011440
12021-01-051845
22021-01-062484
32021-01-092290
</b>
We can use the <b>dtypes</b> function to view the data type of each column in the DataFrame:
<b>#view data type of each column
df.dtypes
day      datetime64[ns]
sales             int64
dtype: object
</b>
We can see that the “day” column has a <b>DateTime</b> class.
To convert “day” into a string, we can use the following syntax:
<b>#convert 'day' column to string
df['day'] = df['day'].dt.strftime('%Y-%m-%d')
#view updated DataFrame
df
       day     sales
02021-01-011440
12021-01-051845
22021-01-062484
32021-01-092290
</b>
We can use the <b>dtypes</b> function again to verify that the “day” column is now a string:
<b>#view data type of each column
df.dtypes
day      object
sales     int64
dtype: object
</b>
<b>Note</b>: You can find the complete documentation for the <b>dt.strftime()</b> function  here .
<h2><span class="orange">How to Convert Dictionary to Pandas DataFrame (2 Examples)</span></h2>
You can use one of the following methods to convert a dictionary in Python to a pandas DataFrame:
<b>Method 1: Use dict.items()</b>
<b>df = pd.DataFrame(list(some_dict.items()), columns = ['col1', 'col2'])
</b>
<b>Method 2: Use from_dict()</b>
<b>df = pd.DataFrame.from_dict(some_dict, orient='index').reset_index()
df.columns = ['col1', 'col2']</b>
Both methods produce the same result.
The following examples show how to use each method in practice.
<h3>Example 1: Convert Dictionary to DataFrame Using dict.items()</h3>
Suppose we have the following dictionary in Python:
<b>#create dictionary
some_dict = {'Lebron':26,'Luka':30,'Steph':22,'Nicola':29, 'Giannis':31}
</b>
We can use the following code to convert this dictionary into a pandas DataFrame:
<b>import pandas as pd
#convert dictionary to pandas DataFrame
df = pd.DataFrame(list(some_dict.items()), columns = ['Player', 'Points'])
#view DataFrame
df
        PlayerPoints
0Lebron26
1Luka30
2Steph22
3Nicola29
4Giannis31
</b>
We can also use the <b>type()</b> function to confirm that the result is a pandas DataFrame:
<b>#display type of df
type(df)
pandas.core.frame.DataFrame
</b>
<h3>Example 2: Convert Dictionary to DataFrame Using from_dict()</h3>
Suppose we have the following dictionary in Python:
<b>#create dictionary
some_dict = {'Lebron':26,'Luka':30,'Steph':22,'Nicola':29, 'Giannis':31}
</b>
We can use the following code to convert this dictionary into a pandas DataFrame:
<b>import pandas as pd
#convert dictionary to pandas DataFrame
df = pd.DataFrame.from_dict(some_dict, orient='index').reset_index()
#define column names of DataFrame
df.columns = ['Player', 'Points']
#view DataFrame
df
        PlayerPoints
0Lebron26
1Luka30
2Steph22
3Nicola29
4Giannis31
</b>
We can also use the <b>type()</b> function to confirm that the result is a pandas DataFrame:
<b>#display type of df
type(df)
pandas.core.frame.DataFrame</b>
Notice that this method produces the exact same result as the previous method.
<h2><span class="orange">How to Convert Floats to Integers in Pandas</span></h2>
You can use the following syntax to convert a column in a pandas DataFrame from a float to an integer:
<b>df['float_column'] = df['float_column'].astype(int)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert One Column from Float to Integer</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': [25.2, 27.0, 14.5, 17.6, 20.7],   'assists': [5.1, 7.7, 10.3, 8.6, 9.5]})
#view data types for each column
df.dtypes
player      object
points     float64
assists    float64
dtype: object</b>
We can see that the <b>points</b> and <b>assists</b> columns both have a data type of float.
The following code shows how to convert the <b>points</b> column from a float to an integer:
<b>#convert 'points' column to integer
df['points'] = df['points'].astype(int)
#view data types of each column
df.dtypes
player      object
points       int32
assists    float64
dtype: object
</b>
We can see that the <b>points</b> column is now an integer, while all other columns remained unchanged.
<h3>Example 2: Convert Multiple Columns to Integer</h3>
The following code shows how to convert multiple columns in a DataFrame from a float to an integer:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': [25.2, 27.0, 14.5, 17.6, 20.7],   'assists': [5.1, 7.7, 10.3, 8.6, 9.5]})
#convert 'points' and 'assists' columns to integer
df[['points', 'assists']] = df[['points', 'assists']].astype(int)
#view data types for each column
df.dtypes
player     object
points      int32
assists     int32
dtype: object
</b>
We can see that the <b>points</b> and <b>assists</b> columns have both been converted from floats to integers.
<h2><span class="orange">How to Convert Index to Column in Pandas (With Examples)</span></h2>
You can use the following basic syntax to convert an index of a pandas DataFrame to a column:
<b>#convert index to column
df.reset_index(inplace=True)
</b>
If you have a MultiIndex pandas DataFrame, you can use the following syntax to convert a specific level of the index to a column:
<b>#convert specific level of MultiIndex to column
df.reset_index(inplace=True, level = ['Level1'])</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert Index to Column</h3>
The following code shows how to convert an index of a pandas DataFrame to a column:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
#convert index to column
df.reset_index(inplace=True)
#view updated DataFrame
df
indexpointsassistsrebounds
0025511
111278
2215710
331496
4419126
</b>
<h3>Example 2: Convert MultiIndex to Columns</h3>
Suppose we have the following MultiIndex pandas DataFrame:
<b>import pandas as pd
#create DataFrame
index_names = pd.MultiIndex.from_tuples([('Level1','Lev1', 'L1'),                       ('Level2','Lev2', 'L2'),                       ('Level3','Lev3', 'L3'),                       ('Level4','Lev4', 'L4')],                       names=['Full','Partial', 'ID'])
data = {'Store': ['A','B','C','D'],
        'Sales': [17, 22, 29, 35]}
df = pd.DataFrame(data, columns = ['Store','Sales'], index=index_names)
#view DataFrame
df
    Store    Sales
FullPartialID
Level1Lev1L1A17
Level2Lev2L2B22
Level3Lev3L3C29
Level4Lev4L4D35</b>
The following code shows how to convert every level of the MultiIndex to columns in a pandas DataFrame:
<b>#convert all levels of index to columns
df.reset_index(inplace=True)
#view updated DataFrame
df
        FullPartialIDStoreSales
0Level1Lev1L1A17
1Level2Lev2L2B22
2Level3Lev3L3C29
3Level4Lev4L4D35</b>
We could also use the following code to only convert a specific level of the MultiIndex to a column:
<b>#convert just 'ID' index to column in DataFrame
df.reset_index(inplace=True, level = ['ID'])
#view updated DataFrame
df
IDStoreSales
FullPartial
Level1Lev1L1A17
Level2Lev2L2B22
Level3Lev3L3C29
Level4Lev4L4D35</b>
Notice that just the ‘ID’ level was converted to a column in the DataFrame.
<h2><span class="orange">How to Convert List to a Column in Pandas</span></h2>
You can use the following basic syntax to convert a list to a column in a pandas DataFrame:
<b>df['new_column'] = pd.Series(some_list)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Convert List to a Column in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12
</b>
The following code shows how to convert a list called <b>steals</b> to a column in the DataFrame:
<b>#create list
steals = [4, 4, 3, 2, 3, 5, 0, 1]
#convert list to DataFrame column
df['steals'] = pd.Series(steals)
#view updated DataFame
print(df)
  team  points  assists  rebounds  steals
0    A      18        5        11       4
1    B      22        7         8       4
2    C      19        7        10       3
3    D      14        9         6       2
4    E      14       12         6       3
5    F      11        9         5       5
6    G      20        9         9       0
7    H      28        4        12       1
</b>
Notice that <b>steals</b> has been added as a new column to the pandas DataFrame.
Note that if the list has fewer elements than the number of rows in the existing DataFrame, then <b>NaN</b> values will be filled in the column:
<b>#create list
steals = [4, 4, 3, 2, 3]
#convert list to DataFrame column
df['steals'] = pd.Series(steals)
#view updated DataFame
print(df)
  team  points  assists  rebounds  steals
0    A      18        5        11     4.0
1    B      22        7         8     4.0
2    C      19        7        10     3.0
3    D      14        9         6     2.0
4    E      14       12         6     3.0
5    F      11        9         5     NaN
6    G      20        9         9     NaN
7    H      28        4        12     NaN
</b>
Notice that the last three values in the new steals column are simply <b>NaN</b> values generated by pandas. 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Get Cell Value from Pandas DataFrame 
 How to Rename Index in Pandas DataFrame 
 How to Sort Columns by Name in Pandas 
<h2><span class="orange">How to Convert Object to Float in Pandas (With Examples)</span></h2>
You can use one of the following methods to convert a column in a pandas DataFrame from object to float:
<b>Method 1: Use astype()</b>
<b>df['column_name'] = df['column_name'].astype(float)</b>
<b>Method 2: Use to_numeric()</b>
<b><b>df['column_name'] = pd.to_numeric(df['column_name'])</b></b>
Both methods produce the same result.
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': ['18', '22.2', '19.1', '14', '14', '11.5', '20', '28'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team points  assists
0    A     18        5
1    B   22.2        7
2    C   19.1        7
3    D     14        9
4    E     14       12
5    F   11.5        9
6    G     20        9
7    H     28        4
#check data type of each column
print(df.dtypes)
team       object
points     object
assists     int64
dtype: object
</b>
<h3>Method 1: Use astype() to Convert Object to Float</h3>
The following code shows how to use the <b>astype()</b> function to convert the points column in the DataFrame from an object to a float:
<b>#convert points column from object to float
df['points'] = df['points'].astype(float)
#view updated DataFrame
print(df)
  team  points  assists
0    A    18.0        5
1    B    22.2        7
2    C    19.1        7
3    D    14.0        9
4    E    14.0       12
5    F    11.5        9
6    G    20.0        9
7    H    28.0        4
#view updated data types
print(df.dtypes)
team        object
points     float64
assists      int64
dtype: object</b>
Notice that the points column now has a data type of <b>float64</b>.
<h3>Method 2: Use to_numeric() to Convert Object to Float</h3>
The following code shows how to use the <b>to_numeric()</b> function to convert the points column in the DataFrame from an object to a float:
<b>#convert points column from object to float
df['points'] = pd.to_numeric(df['points'], errors='coerce')
#view updated DataFrame
print(df)
  team  points  assists
0    A    18.0        5
1    B    22.2        7
2    C    19.1        7
3    D    14.0        9
4    E    14.0       12
5    F    11.5        9
6    G    20.0        9
7    H    28.0        4
#view updated data types
print(df.dtypes)
team        object
points     float64
assists      int64
dtype: object</b>
Notice that the points column now has a data type of <b>float64</b>.
Also note that this method produces the exact same result as the previous method.
<h2><span class="orange">Pandas: How to Convert object to int</span></h2>
You can use the following syntax to convert a column in a pandas DataFrame from an object to an integer:
<b>df['object_column'] = df['int_column'].astype(str).astype(int)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': ['25', '27', '14', '17', '20'],   'assists': ['5', '7', '10', '8', '9']})
#view data types for each column
df.dtypes
player     object
points     object
assists    object
dtype: object</b>
<h2>Example 1: Convert One Column from Object to Integer</h2>
The following code shows how to convert the <b>points</b> column from an object to an integer:
<b>#convert 'points' column to integer
df['points'] = df['points'].astype(str).astype(int)
#view data types of each column
df.dtypes
player     object
points      int32
assists    object
dtype: object
</b>
We can see that the <b>points</b> column is now an integer, while all other columns remained unchanged.
<h2>Example 2: Convert Multiple Columns to Integer</h2>
The following code shows how to convert multiple columns in a DataFrame from an object to an integer:
<b>#convert 'points' and 'assists' columns to integer
df[['points', 'assists']] = df[['points', 'assists']].astype(str).astype(int)
#view data types for each column
df.dtypes
player     object
points      int32
assists     int32
dtype: object
</b>
We can see that the <b>points</b> and <b>assists</b> columns have both been converted from objects to integers.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common conversions in pandas:
 How to Convert Pandas DataFrame Columns to Strings 
 How to Convert Timestamp to Datetime in Pandas 
 How to Convert Datetime to Date in Pandas 
<h2><span class="orange">How to Convert Timedelta to Int in Pandas (With Examples)</span></h2>
You can use the following methods to convert a timedelta column to an integer column in a pandas DataFrame:
<b>Method 1: Convert Timedelta to Integer (Days)</b>
<b>df['days'] = df['timedelta_column'].dt.days
</b>
<b>Method 2: Convert Timedelta to Integer (Hours)</b>
<b>df['hours'] = df['timedelta_column'] / pd.Timedelta(hours=1)</b>
<b>Method 3: Convert Timedelta to Integer (Minutes)</b>
<b>df['minutes'] = df['timedelta_column'] / pd.Timedelta(minutes=1)</b>
The following example shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'promotion': ['A', 'B', 'C', 'D'],   'start': ['2021-10-04 13:29:00', '2021-10-07 12:30:00',             '2021-10-15 04:20:00', '2021-10-18 15:45:03'],   'end':   ['2021-10-08 11:29:06', '2021-10-15 10:30:07',             '2021-10-29 05:50:15', '2021-10-22 15:40:03']})
#convert start date and end date columns to datetime
df['start'] = pd.to_datetime(df['start'])
df['end'] = pd.to_datetime(df['end'])
#create new column that contains timedelta between start and end
df['duration'] = df['end'] - df['start']
#view DataFrame
print(df)
  promotion               start                 end         duration
0         A 2021-10-04 13:29:00 2021-10-08 11:29:06  3 days 22:00:06
1         B 2021-10-07 12:30:00 2021-10-15 10:30:07  7 days 22:00:07
2         C 2021-10-15 04:20:00 2021-10-29 05:50:15 14 days 01:30:15
3         D 2021-10-18 15:45:03 2021-10-22 15:40:03  3 days 23:55:00
</b>
<h2>Example 1: Convert Timedelta to Integer (Days)</h2>
The following code shows how to create a new column called <b>days</b> that converts the timedelta in the <b>duration</b> column into an integer value that represents the number of days in the timedelta column.
<b>#create new column that converts timedelta into integer number of days
df['days'] = df['duration'].dt.days
#view updated DataFrame
print(df)
  promotion               start                 end         duration  days
0         A 2021-10-04 13:29:00 2021-10-08 11:29:06  3 days 22:00:06     3
1         B 2021-10-07 12:30:00 2021-10-15 10:30:07  7 days 22:00:07     7
2         C 2021-10-15 04:20:00 2021-10-29 05:50:15 14 days 01:30:15    14
3         D 2021-10-18 15:45:03 2021-10-22 15:40:03  3 days 23:55:00     3
</b>
We can use <b>dtype </b>to check the data type of this new column:
<b>#check data type
df.days.dtype
dtype('int64')
</b>
The new column is an integer.
<h2>Example 2: Convert Timedelta to Integer (Hours)</h2>
The following code shows how to create a new column called <b>hours </b>that converts the timedelta in the <b>duration</b> column into a numeric value that represents the total number of hours in the timedelta column.
<b>#create new column that converts timedelta into total number of hours
df['hours'] = df['duration'] / pd.Timedelta(hours=1)
#view updated DataFrame
print(df)
  promotion               start                 end         duration      hours
0         A 2021-10-04 13:29:00 2021-10-08 11:29:06  3 days 22:00:06   94.001667  
1         B 2021-10-07 12:30:00 2021-10-15 10:30:07  7 days 22:00:07  190.001944
2         C 2021-10-15 04:20:00 2021-10-29 05:50:15 14 days 01:30:15  337.504167
3         D 2021-10-18 15:45:03 2021-10-22 15:40:03  3 days 23:55:00   95.916667
</b>
We can use <b>dtype </b>to check the data type of this new column:
<b>#check data type
df.hours.dtype
dtype('float64')
</b>
The new column is a float.
<h2>Example 3: Convert Timedelta to Integer (Minutes)</h2>
The following code shows how to create a new column called <b>minutes </b>that converts the timedelta in the <b>duration</b> column into a numeric value that represents the total number of minutes in the timedelta column.
<b>#create new column that converts timedelta into total number of minutes
df['minutes'] = df['duration'] / pd.Timedelta(minutes=1)
#view updated DataFrame
print(df)
  promotion               start                 end         duration        minutes
0         A 2021-10-04 13:29:00 2021-10-08 11:29:06  3 days 22:00:06    5640.100000  
1         B 2021-10-07 12:30:00 2021-10-15 10:30:07  7 days 22:00:07   11400.116667
2         C 2021-10-15 04:20:00 2021-10-29 05:50:15 14 days 01:30:15   20250.250000
3         D 2021-10-18 15:45:03 2021-10-22 15:40:03  3 days 23:55:00    5755.000000
</b>
We can use <b>dtype </b>to check the data type of this new column:
<b>#check data type
df.minutes.dtype
dtype('float64')
</b>
The new column is a float.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Convert Columns to DateTime in Pandas 
 How to Convert Datetime to Date in Pandas 
 How to Extract Month from Date in Pandas 
<h2><span class="orange">How (And Why) to Make Copy of Pandas DataFrame</span></h2>
Whenever you create a subset of a pandas DataFrame and then modify the subset, the original DataFrame will also be modified.
For this reason, it’s always a good idea to use <b>.copy()</b> when subsetting so that any modifications you make to the subset won’t also be made to the original DataFrame.
The following examples demonstrate how (and why) to make a copy of a pandas DataFrame when subsetting.
<h2>Example 1: Subsetting a DataFrame Without Copying</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
Now suppose we create a subset that contains only the first four rows of the original DataFrame:
<b>#define subsetted DataFrame
df_subset = df[0:4]
#view subsetted DataFrame
print(df_subset)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6</b>
If we modify one of the values in the subset, the value in the original DataFrame will also be modified:
<b>#change first value in team column
df_subset.team[0] = 'X'
#view subsetted DataFrame
print(df_subset)
  team  points  assists
0    X      18        5
1    B      22        7
2    C      19        7
3    D      14        9
#view original DataFrame
print(df)
  team  points  assists
0    X      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
Notice that the first value in the team column has been changed from ‘A’ to ‘X’ in both the subsetted DataFrame <b>and</b> the original DataFrame.
This is because we didn’t make a copy of the original DataFrame.
<h2>Example 2: Subsetting a DataFrame With Copying</h2>
Once again suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
Once again suppose we create a subset that contains only the first four rows of the original DataFrame, but this time we use <b>.copy()</b> to make a copy of the original DataFrame:
<b>#define subsetted DataFrame
df_subset = df[0:4].copy()</b>
Now suppose we change the first value in the team column of the subsetted DataFrame:
<b>#change first value in team column
df_subset.team[0] = 'X'
#view subsetted DataFrame
print(df_subset)
  team  points  assists
0    X      18        5
1    B      22        7
2    C      19        7
3    D      14        9
#view original DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
Notice that the first value in the team column has been changed from ‘A’ to ‘X’ only in the subsetted DataFrame.
The original DataFrame remains untouched since we used<b> .copy()</b> to make a copy of it when creating the subset.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">How to Calculate Correlation Between Two Columns in Pandas</span></h2>
You can use the following syntax to calculate the correlation between two columns in a pandas DataFrame:
<b>df['column1'].corr(df['column2'])
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculate Correlation Between Two Columns</h3>
The following code shows how to calculate the correlation between columns in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view first five rows of DataFrame
df.head()
        pointsassists  rebounds
0255 11
1127 8
2157 10
3149 6
41912 6
#calculate correlation between points and assists
df['points'].corr(df['assists'])
-0.359384
</b>
The correlation coefficient is <b>-0.359</b>. Since this correlation is negative, it tells us that points and assists are negatively correlated.
In other words, as values in the points column increase, the values in the assists column tend to decrease.
<h3>Example 2: Calculate Significance of Correlation</h3>
To determine whether or not a correlation coefficient is statistically significant, you can use the <b>pearsonr(x, y)</b> function from the  SciPy  library.
The following code shows how to use this function in practice:
<b>import pandas as pd
from scipy.stats import pearsonr
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#calculate p-value of correlation coefficient between points and assists
pearsonr(df['points'], df['assists'])
(-0.359384, 0.38192)
</b>
The first value in the output displays the correlation coefficient (-0.359384) and the second value displays the p-value (0.38192) associated with this correlation coefficient.
Since the  p-value  is not less than α = 0.05, we would conclude that the correlation between points and assists is not statistically significant.
<h2><span class="orange">How to Use corrwith() in Pandas (With Examples)</span></h2>
You can use the <b>corrwith()</b> function in pandas to calculate the pairwise correlation between numerical columns <em>with the same name</em> in two different pandas DataFrames.
This function uses the following basic syntax:
<b>df1.corrwith(df2)
</b>
<b>Note</b>: This function is different than the <b>corr()</b> function, which is used to calculate the correlation between two numerical columns within the same DataFrame.
The following example shows how to use the <b>corrwith()</b> function in practice.
<h2>Example: How to Use corrwith() in Pandas</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [18, 22, 29, 25, 14, 11],    'assists': [4, 5, 5, 4, 8, 12],    'rebounds': [10, 6, 4, 6, 3, 5]})
print(df1)
  team  points  assists  rebounds
0    A      18        4        10
1    B      22        5         6
2    C      29        5         4
3    D      25        4         6
4    E      14        8         3
5    F      11       12         5
#create second DataFrame 
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [22, 25, 27, 35, 25, 20],    'assists': [15, 13, 8, 8, 5, 8],    'rebs': [4, 11, 12, 8, 7, 10]})
print(df2)
  team  points  assists  rebs
0    A      22       15     4
1    B      25       13    11
2    C      27        8    12
3    D      35        8     8
4    E      25        5     7
5    F      20        8    10</b>
We can use the <b>corrwith()</b> function to calculate the correlation between the numeric columns with the same names in the two DataFrames:
<b>#calculate correlation between numeric columns with same names in each DataFrame
df1.corrwith(df2)
points      0.677051
assists    -0.478184
rebounds         NaN
rebs             NaN
dtype: float64</b>
From the output we can see:
The correlation between the values in the <b>points</b> columns in the two DataFrames is <b>0.677</b>.
The correlation between the values in the <b>assists</b> columns in the two DataFrames is <b>-0.478</b>.
Since the column names <b>rebounds</b> and <b>rebs</b> didn’t exist in both DataFrames, a value of <b>NaN</b> is returned for each of these columns.
<b>Note # 1</b>: By default, the <b>corrwith()</b> function calculates the Pearson correlation coefficient between columns, but you can also specify method=’kendall’ or method=’spearman’ to instead calculate a different type of correlation coefficient.
<b>Note #2</b>: You can find the complete documentation for the <b>corrwith()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Calculate Correlation By Group in Pandas 
 How to Calculate Rolling Correlation in Pandas 
 How to Calculate Correlation Between Two Columns in Pandas 
<h2><span class="orange">How to Fix in Pandas: could not convert string to float</span></h2>
One common error you may encounter when using pandas is:
<b>ValueError: could not convert string to float: '$400.42'
</b>
This error usually occurs when you attempt to convert a string to a float in pandas, yet the string contains one or more of the following:
Spaces
Commas
Special characters
When this occurs, you must first remove these characters from the string before converting it to a float.
The following example shows how to resolve this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'B', 'C', 'D'],   'revenue': ['$400.42', '$100.18', '$243.75', '$194.22']})
#view DataFrame
print(df)
  store  revenue
0     A  $400.42
1     B  $100.18
2     C  $243.75
3     D  $194.22
#view data type of each column
print(df.dtypes)
store      object
revenue    object
dtype: object
</b>
Now suppose we attempt to convert the <b>revenue</b> column from a string to a float:
<b>#attempt to convert 'revenue' from string to float
df['revenue'] = df['revenue'].astype(float)
ValueError: could not convert string to float: '$400.42' 
</b>
We receive an error since the <b>revenue</b> column contains a dollar sign in the strings.
<h2>How to Fix the Error</h2>
The way to resolve this error is to use the <b>replace()</b> function to replace the dollar signs in the <b>revenue</b> column with nothing before performing the conversion:
<b>#convert revenue column to float
df['revenue'] = df['revenue'].apply(lambda x: float(x.split()[0].replace('$', '')))
#view updated DataFrame
print(df)
  store  revenue
0     A   400.42
1     B   100.18
2     C   243.75
3     D   194.22
#view data type of each column
print(df.dtypes)
store       object
revenue    float64
dtype: object</b>
Notice that we’re able to convert the <b>revenue</b> column from a string to a float and we don’t receive any error since we removed the dollar signs before performing the conversion.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in Python:
 How to Fix in Python: ‘numpy.ndarray’ object is not callable 
 How to Fix: TypeError: ‘numpy.float64’ object is not callable 
 How to Fix: Typeerror: expected string or bytes-like object 
<h2><span class="orange">How to Count Duplicates in Pandas (With Examples)</span></h2>
You can use the following methods to count duplicates in a pandas DataFrame:
<b>Method 1: Count Duplicate Values in One Column</b>
<b>len(df['my_column'])-len(df['my_column'].drop_duplicates())
</b>
<b>Method 2: Count Duplicate Rows</b>
<b>len(df)-len(df.drop_duplicates())</b>
<b>Method 3: Count Duplicates for Each Unique Row</b>
<b>df.groupby(df.columns.tolist(), as_index=False).size()</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'G', 'F', 'G', 'G', 'F', 'F'],   'points': [5, 5, 8, 10, 5, 7, 10, 10]})
#view DataFrame
print(df)
  team position  points
0    A        G       5
1    A        G       5
2    A        G       8
3    A        F      10
4    B        G       5
5    B        G       7
6    B        F      10
7    B        F      10</b>
<h2>Example 1: Count Duplicate Values in One Column</h2>
The following code shows how to count the number of duplicate values in the <b>points</b> column:
<b>#count duplicate values in points column
len(df['points'])-len(df['points'].drop_duplicates())
4</b>
We can see that there are <b>4</b> duplicate values in the <b>points</b> column.
<h2>Example 2: Count Duplicate Rows</h2>
The following code shows how to count the number of duplicate rows in the DataFrame:
<b>#count number of duplicate rows
len(df)-len(df.drop_duplicates())
2</b>
We can see that there are <b>2</b> duplicate rows in the DataFrame.
We can use the following syntax to view these 2 duplicate rows:
<b>#display duplicated rows
df[df.duplicated()]
        teamposition points
1AG 5
7BF 10
</b>
<h2>Example 3: Count Duplicates for Each Unique Row</h2>
The following code shows how to count the number of duplicates for each unique row in the DataFrame:
<b>#display number of duplicates for each unique row
df.groupby(df.columns.tolist(), as_index=False).size()
        teamposition pointssize
0AF 101
1AG 52
2AG 81
3BF 102
4BG 51
5BG 71
</b>
The <b>size</b> column displays the number of duplicates for each unique row.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Duplicate Rows in Pandas 
 How to Drop Duplicate Columns in Pandas 
 How to Select Columns by Index in Pandas 
<h2><span class="orange">How to Count Missing Values in a Pandas DataFrame</span></h2>
Often you may be interested in counting the number of missing values in a pandas DataFrame.
This tutorial shows several examples of how to count missing values using the following DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame with some missing values
df = pd.DataFrame({'a': [4, np.nan, np.nan, 7, 8, 12],   'b': [np.nan, 6, 8, 14, 29, np.nan],   'c': [11, 8, 10, 6, 6, np.nan]})
#view DataFrame
print(df)
      a     b     c
0   4.0   NaN  11.0
1   NaN   6.0   8.0
2   NaN   8.0  10.0
3   7.0  14.0   6.0
4   8.0  29.0   6.0
5  12.0   NaN   NaN
</b>
<h3>Count the Total Missing Values in Entire DataFrame</h3>
The following code shows how to calculate the total number of missing values in the entire DataFrame:
<b>df.isnull().sum().sum()
5</b>
This tells us that there are <b>5 </b>total missing values.
<h3>Count the Total Missing Values per Column</h3>
The following code shows how to calculate the total number of missing values in each column of the DataFrame:
<b>df.isnull().sum()
a    2
b    2
c    1
</b>
This tells us:
Column ‘a’ has <b>2 </b>missing values.
Column ‘b’ has <b>2 </b>missing values.
Column ‘c’ has <b>1 </b>missing value.
You can also display the number of missing values as a percentage of the entire column:
<b>df.isnull().sum()/len(df)*100
a    33.333333
b    33.333333
c    16.666667
</b>
This tells us:
<b>33.33% </b>of values in Column ‘a’ are missing.
<b>33.33% </b>of values in Column ‘b’ are missing.
<b>16.67% </b>of values in Column ‘c’ are missing.
<h3>Count the Total Missing Values per Row</h3>
The following code shows how to calculate the total number of missing values in each row of the DataFrame:
<b>df.isnull().sum(axis=1)
0    1
1    1
2    1
3    0
4    0
5    2</b>
This tells us:
Row 1 has <b>1 </b>missing value.
Row 2 has <b>1 </b>missing value.
Row 3 has <b>1 </b>missing value.
Row 4 has <b>0 </b>missing values.
Row 5 has <b>0 </b>missing values.
Row 6 has <b>2 </b>missing values.
<h2><span class="orange">Pandas: How to Count Occurrences of Specific Value in Column</span></h2>
You can use the following syntax to count the occurrences of a specific value in a column of a pandas DataFrame:
<b>df['column_name'].value_counts()[value]
</b>
Note that <b>value</b> can be either a number or a character.
The following examples show how to use this syntax in practice.
<h3>Example 1: Count Occurrences of String in Column</h3>
The following code shows how to count the number of occurrences of a specific string in a column of a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#count occurrences of the value 'B' in the 'team' column
df['team'].value_counts()['B']
4</b>
From the output we can see that the string ‘B’ occurs <b>4</b> times in the ‘team’ column.
Note that we can also use the following syntax to find how frequently each unique value occurs in the ‘team’ column:
<b>#count occurrences of every unique value in the 'team' column
df['team'].value_counts()
B    4
A    2
C    2
Name: team, dtype: int64</b>
<h3>Example 2: Count Occurrences of Numeric Value in Column</h3>
The following code shows how to count the number of occurrences of a numeric value in a column of a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#count occurrences of the value 9 in the 'assists' column
df['assists'].value_counts()[9]
3</b>
From the output we can see that the value 9 occurs <b>3</b> times in the ‘assists’ column.
We can also use the following syntax to find how frequently each unique value occurs in the ‘assists’ column:
<b>#count occurrences of every unique value in the 'assists' column
df['assists'].value_counts()
9     3
7     2
5     1
12    1
4     1
Name: assists, dtype: int64</b>
From the output we can see:
The value 9 occurs 3 times.
The value 7 occurs 2 times.
The value 5 occurs 1 time.
And so on.
<h2><span class="orange">Pandas: Count Occurrences of True and False in a Column</span></h2>
You can use the following basic syntax to count the occurrences of True and False values in a column of a pandas DataFrame:
<b>df['my_boolean_column'].value_counts()
</b>
This will count the occurrences of both True and False values.
If you only want to count one of the specific values, you can use the following syntax:
<b>#count occurrences of True
df['my_boolean_column'].values.sum()
#count occurrences of False
(~df['my_boolean_column']).values.sum()</b>
The following example shows how to use this syntax in practice.
<h2>Example: Count Occurrences of True and False in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C', 'C'],   'points': [18, 22, 19, 14, 14, 28, 20],   'all_star': [True, False, False, True, False, True, True]})
#view DataFrame
print(df)
  team  points  all_star
0    A      18      True
1    A      22     False
2    A      19     False
3    B      14      True
4    B      14     False
5    C      28      True
6    C      20      True
</b>
We can use the <b>value_counts()</b> function to count the occurrences of both True and False values in the <b>all_star</b> column:
<b>#count occurrences of True and False in all_star column
df['all_star'].value_counts()
True     4
False    3
Name: all_star, dtype: int64
</b>
From the output we can see:
The value True occurs <b>4</b> times in the <b>all_star</b> column.
The value False occurs <b>3</b> times in the <b>all_star</b> column.
You can also use the following syntax to only count the occurrences of True:
<b>#count occurrences of True in all_star column
df['all_star'].values.sum()
4</b>
And you can use the following syntax to only count the occurrences of False:
<b>#count occurrences of False in all_star column
(~df['all_star']).values.sum()
3</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Use GroupBy and Value Counts 
 Pandas: How to Use GroupBy with Bin Counts 
 Pandas: How to Count Values in Column with Condition 
<h2><span class="orange">Pandas: How to Count Unique Combinations of Two Columns</span></h2>
You can use the following syntax to count the number of unique combinations across two columns in a pandas DataFrame:
<b>df[['col1', 'col2']].value_counts().reset_index(name='count')
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Count Unique Combinations of Two Columns in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the <b>team</b> and <b>position</b> of various basketball players:
<b>import pandas as pd
#create dataFrame
df = pd.DataFrame({'team': ['Mavs', 'Mavs', 'Mavs', 'Mavs',            'Heat', 'Heat', 'Heat', 'Heat'],   'position': ['Guard', 'Guard', 'Guard', 'Forward',                'Guard', 'Forward', 'Forward', 'Guard']})
#view DataFrame
df
        teamposition
0MavsGuard
1MavsGuard
2MavsGuard
3MavsForward
4HeatGuard
5HeatForward
6HeatForward
7HeatGuard</b>
We can use the following syntax to count the number of unique combinations of <b>team</b> and <b>position</b>:
<b>df[['team', 'position']].value_counts().reset_index(name='count')
        teamposition  count
0MavsGuard  3
1HeatForward  2
2HeatGuard  2
3MavsForward  1
</b>
From the output we can see:
There are <b>3</b> occurrences of the Mavs-Guard combination.
There are <b>2</b> occurrences of the Heat-Forward combination.
There are <b>2</b> occurrences of the Heat-Guard combination.
There is <b>1</b> occurrence of the Mavs-Forward combination.
Note that you can also sort the results in order of count ascending or descending.
For example, we can use the following code to sort the results in order of count <b>ascending</b>:
<b>df[['team', 'position']].value_counts(ascending=True).reset_index(name='count')
        teamposition  count
0MavsForward  1
1HeatForward  2
2HeatGuard  2
3MavsGuard  3</b>
The results are now sorted by count from smallest to largest.
<b>Note</b>: You can find the complete documentation for the pandas <b>value_counts()</b> function  here .
<h2><span class="orange">How to Count Unique Values in Pandas (With Examples)</span></h2>
You can use the  nunique()  function to count the number of unique values in a pandas DataFrame.
This function uses the following basic syntax:
<b>#count unique values in each column
df.nunique()
#count unique values in each row
df.nunique(axis=1)
</b>
The following examples show how to use this function in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [8, 8, 13, 13, 22, 22, 25, 29],   'assists': [5, 8, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 11, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A8511
1A888
2A13711
3A1396
4B22126
5B2295
6B2599
7B29412
</b>
<h3>Example 1: Count Unique Values in Each Column</h3>
The following code shows how to count the number of unique values in each column of a DataFrame:
<b>#count unique values in each column
df.nunique()
team        2
points      5
assists     5
rebounds    6
dtype: int64
</b>
From the output we can see:
The ‘team’ column has <b>2</b> unique values
The ‘points’ column has <b>5</b> unique values
The ‘assists’ column has <b>5</b> unique values
The ‘rebounds’ column has <b>6</b> unique values
<h3>Example 2: Count Unique Values in Each Row</h3>
The following code shows how to count the number of unique values in each row of a DataFrame:
<b>#count unique values in each row
df.nunique(axis=1)
0    4
1    2
2    4
3    4
4    4
5    4
6    3
7    4
dtype: int64
</b>
From the output we can see:
The first row has <b>4</b> unique values
The second row has <b>2</b> unique values
The third row has <b>4</b> unique values
And so on.
<h3>Example 3: Count Unique Values by Group</h3>
The following code shows how to count the number of unique values by group in a DataFrame:
<b>#count unique 'points' values, grouped by team
df.groupby('team')['points'].nunique()
team
A    2
B    3
Name: points, dtype: int64</b>
From the output we can see:
Team ‘A’ has <b>2</b> unique ‘points’ values
Team ‘B’ has <b>3</b> unique ‘points’ values
<h2><span class="orange">Pandas: How to Count Values in Column with Condition</span></h2>
You can use the following methods to count the number of values in a pandas DataFrame column with a specific condition:
<b>Method 1: Count Values in One Column with Condition</b>
<b>len(df[df['col1']=='value1'])
</b>
<b>Method 2: Count Values in Multiple Columns with Conditions</b>
<b>len(df[(df['col1']=='value1') & (df['col2']=='value2')])</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'pos': ['Gu', 'Fo', 'Fo', 'Fo', 'Gu', 'Gu', 'Fo', 'Fo'],   'points': [18, 22, 19, 14, 14, 11, 20, 28]})
#view DataFrame
print(df)
  team pos  points
0    A  Gu      18
1    A  Fo      22
2    A  Fo      19
3    A  Fo      14
4    B  Gu      14
5    B  Gu      11
6    B  Fo      20
7    B  Fo      28</b>
<h3>Example 1: Count Values in One Column with Condition</h3>
The following code shows how to count the number of values in the <b>team</b> column where the value is equal to ‘A’:
<b>#count number of values in team column where value is equal to 'A'
len(df[df['team']=='A'])
4</b>
We can see that there are <b>4</b> values in the team column where the value is equal to ‘A.’
<h3>Example 2: Count Values in Multiple Columns with Conditions</h3>
The following code shows how to count the number of rows in the DataFrame where the <b>team</b> column is equal to ‘B’ and the <b>pos</b> column is equal to ‘Gu’:
<b>#count rows where team is 'B' and pos is 'Gu'
len(df[(df['team']=='B') & (df['pos']=='Gu')])
2</b>
We can see that there are <b>2</b> rows in the DataFrame that meet both of these conditions.
We can use similar syntax to count the number of rows that meet any number of conditions we’d like.
For example, the following code shows how to count the number of rows that meet three conditions:
<b>team</b> is equal to ‘B’
<b>pos</b> is equal to ‘Gu’
<b>points</b> is greater than 12
<b>#count rows where team is 'B' and pos is 'Gu' and points > 15
len(df[(df['team']=='B') & (df['pos']=='Gu') & (df['points']>12)])
1</b>
We can see that only <b>1</b> row in the DataFrame meets all three of these conditions.
<h2><span class="orange">Pandas: How to Create Boolean Column Based on Condition</span></h2>
You can use the following basic syntax to create a boolean column based on a condition in a pandas DataFrame:
<b>df['boolean_column'] = np.where(df['some_column'] > 15, True, False)
</b>
This particular syntax creates a new boolean column with two possible values:
<b>True</b> if the value in <b>some_column</b> is greater than 15.
<b>False</b> if the value in <b>some_column</b> is less than or equal to 15.
The following example shows how to use this syntax in practice.
<h2>Example: Create Boolean Column Based on Condition in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [5, 17, 7, 19, 12, 13, 9, 24]})
#view DataFrame
print(df)
  team  points
0    A       5
1    A      17
2    A       7
3    A      19
4    B      12
5    B      13
6    B       9
7    B      24   
</b>
We can use the following code to create a new column called <b>good_player</b> that returns <b>True</b> if the value in the points column is greater than 15 or <b>False</b> otherwise:
<b>import numpy as np
#create new boolean column based on value in points column
df['good_player'] = np.where(df['points'] > 15, True, False)
#view updated DataFrame
print(df)
  team  points  good_player
0    A       5        False
1    A      17         True
2    A       7        False
3    A      19         True
4    B      12        False
5    B      13        False
6    B       9        False
7    B      24         True</b>
Notice that the new column called <b>good_player</b> only contains two values: <b>True</b> or <b>False</b>.
We can use the<b> dtypes()</b> function to verify that the new <b>good_player</b> column is indeed a boolean column:
<b>#display data type of good_player column
df['good_player'].dtype
dtype('bool')
</b>
The new <b>good_player</b> column is indeed a boolean column.
Also note that you could return numeric values such as <b>1</b> and <b>0</b> instead of <b>True</b> and <b>False</b> if you’d like:
<b>import numpy as np
#create new boolean column based on value in points column
df['good_player'] = np.where(df['points'] > 15, 1, 0)
#view updated DataFrame
print(df)
  team  points  good_player
0    A       5            0
1    A      17            1
2    A       7            0
3    A      19            1
4    B      12            0
5    B      13            0
6    B       9            0
7    B      24            1
</b>
The <b>good_player</b> column now contains a <b>1</b> if the corresponding value in the <b>points</b> column is greater than 15.
Otherwise, it contains a value of <b>0</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Select Rows by Multiple Conditions in Pandas 
 How to Create a New Column Based on a Condition in Pandas 
 How to Filter a Pandas DataFrame on Multiple Conditions 
<h2><span class="orange">How to Create Categorical Variables in Pandas (With Examples)</span></h2>
You can use one of the following methods to create a  categorical variable  in pandas:
<b>Method 1: Create Categorical Variable from Scratch</b>
<b>df['cat_variable'] = ['A', 'B', 'C', 'D']
</b>
<b> Method 2: Create Categorical Variable from Existing Numerical Variable</b>
<b>df['cat_variable'] = pd.cut(df['numeric_variable'],            bins=[0, 15, 25, float('Inf')],            labels=['Bad', 'OK', 'Good'])</b>
The following examples show how to use each method in practice.
<h2>Example 1: Create Categorical Variable from Scratch</h2>
The following code shows how to create a pandas DataFrame with one categorical variable called <b>team</b> and one numerical variable called <b>points</b>:
<b>import pandas as pd
#create DataFrame with one categorical variable and one numeric variable
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [12, 15, 19, 22, 24, 25, 26, 30]})
#view DataFrame
print(df)
  team  points
0    A      12
1    B      15
2    C      19
3    D      22
4    E      24
5    F      25
6    G      26
7    H      30
#view data type of each column in DataFrame
print(df.dtypes)
team      object
points     int64
dtype: object</b>
By using <b>df.dtypes</b>, we can  see the  data type of each variable  in the DataFrame.
We can see:
The variable team is an <b>object</b>.
The variable points is an <b>integer</b>.
In Python, an <b>object</b> is equivalent to a character or “categorical” variable. Thus, the team variable is a categorical variable.
<h2>Example 2: Create Categorical Variable from Existing Numerical Variable</h2>
The following code shows how to create a categorical variable called <b>status</b> from the existing numerical variable called <b>points</b> in the DataFrame:
<b>import pandas as pd
#create DataFrame with one categorical variable and one numeric variable
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [12, 15, 19, 22, 24, 25, 26, 30]})
#create categorical variable 'status' based on existing numerical 'points' variable
df['status'] = pd.cut(df['points'],      bins=[0, 15, 25, float('Inf')],      labels=['Bad', 'OK', 'Good'])
#view updated DataFrame
print(df)
  team  points status
0    A      12    Bad
1    B      15    Bad
2    C      19     OK
3    D      22     OK
4    E      24     OK
5    F      25     OK
6    G      26   Good
7    H      30   Good</b>
Using the <b>cut()</b> function, we created a new categorical variable called <b>status</b> that takes the following values:
‘<b>Bad</b>‘ if the value in the points column is less than or equal to 15.
Else, ‘<b>OK</b>‘ if the value in the points column is less than or equal to 25.
Else, ‘<b>Good</b>‘.
Note that when using the <b>cut()</b> function, the number of <b>labels</b> must be one less than the number of <b>bins</b>.
In our example, we used four values for <b>bins</b> to define the bin edges and three values for <b>labels</b> to specify the labels to use for the categorical variable.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Create Dummy Variables in Pandas 
 How to Convert Categorical Variable to Numeric in Pandas 
 How to Convert Boolean Values to Integer Values in Pandas 
<h2><span class="orange">Pandas: How to Create Column If It Doesn’t Exist</span></h2>
You can use the following basic syntax to create a column in a pandas DataFrame if it doesn’t already exist:
<b>df['my_column'] = df.get('my_column', df['col1'] * df['col2'])</b> 
This particular syntax creates a new column called <b>my_column</b> if it doesn’t already exist in the DataFrame and it is defined as the product of the existing columns <b>col1</b> and <b>col2</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Create Column in Pandas If It Doesn’t Exist</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],   'sales': [4, 6, 5, 8, 14, 13, 13, 12, 9, 8, 19, 14],   'price': [1, 2, 2, 1, 2, 4, 4, 3, 3, 2, 2, 3]})
#view DataFrame
print(df)
    day  sales  price
0     1      4      1
1     2      6      2
2     3      5      2
3     4      8      1
4     5     14      2
5     6     13      4
6     7     13      4
7     8     12      3
8     9      9      3
9    10      8      2
10   11     19      2
11   12     14      3
</b>
Now suppose we attempt to add a column called <b>price</b> if it doesn’t already exist and define it as a column in which each value is equal to 100:
<b>#attempt to add column called 'price'
df['price'] = df.get('price', 100)    
#view updated DataFrame
print(df)
    day  sales  price
0     1      4      1
1     2      6      2
2     3      5      2
3     4      8      1
4     5     14      2
5     6     13      4
6     7     13      4
7     8     12      3
8     9      9      3
9    10      8      2
10   11     19      2
11   12     14      3
</b>
Since a column called <b>price</b> already exists, pandas simply doesn’t add it to the DataFrame.
However, suppose we attempt to add a new column called <b>revenue</b> if it doesn’t already exist and define it as a column in which the values are the product of the sales and price columns:
<b>#attempt to add column called 'revenue'
df['revenue'] = df.get('revenue', df['sales'] * df['price'])
#view updated DataFrame
print(df)
    day  sales  price  revenue
0     1      4      1        4
1     2      6      2       12
2     3      5      2       10
3     4      8      1        8
4     5     14      2       28
5     6     13      4       52
6     7     13      4       52
7     8     12      3       36
8     9      9      3       27
9    10      8      2       16
10   11     19      2       38
11   12     14      3       42
</b>
This <b>revenue</b> column is added to the DataFrame because it did not already exist.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Filter a Pandas DataFrame on Multiple Conditions 
 How to Use “NOT IN” Filter in Pandas DataFrame 
<h2><span class="orange">Pandas: How to Create New DataFrame from Existing DataFrame</span></h2>
There are three common ways to create a new pandas DataFrame from an existing DataFrame:
<b>Method 1: Create New DataFrame Using Multiple Columns from Old DataFrame</b>
<b>new_df = old_df[['col1','col2']].copy()
</b>
<b>Method 2: Create New DataFrame Using One Column from Old DataFrame</b>
<b>new_df = old_df[['col1']].copy()</b>
<b>Method 3: Create New DataFrame Using All But One Column from Old DataFrame</b>
<b>new_df = old_df.drop('col1', axis=1)</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
old_df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],       'points': [18, 22, 19, 14, 14, 11, 20, 28],       'assists': [5, 7, 7, 9, 12, 9, 9, 4],       'rebounds': [11, 8, 10, 6, 6, 7, 9, 12]})
#view DataFrame
print(old_df)
</b>
<h3>Example 1: Create New DataFrame Using Multiple Columns from Old DataFrame</h3>
The following code shows how to create a new DataFrame using multiple columns from the old DataFrame:
<b>#create new DataFrame from existing DataFrame
new_df = old_df[['points','rebounds']].copy()
#view new DataFrame
print(new_df)
   points  rebounds
0      18        11
1      22         8
2      19        10
3      14         6
4      14         6
5      11         7
6      20         9
7      28        12
#check data type of new DataFrame
type(new_df)
pandas.core.frame.DataFrame
</b>
Notice that this new DataFrame only contains the <b>points</b> and <b>rebounds</b> columns from the old DataFrame.
<b>Note</b>: It’s important to use the <b>copy()</b> function when creating the new DataFrame so that we avoid any  SettingWithCopyWarning  if we happen to modify the new DataFrame in any way.
<h3>Example 2: Create New DataFrame Using One Column from Old DataFrame</h3>
The following code shows how to create a new DataFrame using one column from the old DataFrame:
<b>#create new DataFrame from existing DataFrame
new_df = old_df[['points']].copy()
#view new DataFrame
print(new_df)
   points
0      18
1      22
2      19
3      14
4      14
5      11
6      20
7      28
#check data type of new DataFrame
type(new_df)
pandas.core.frame.DataFrame
</b>
Notice that this new DataFrame only contains the <b>points</b> and column from the old DataFrame.
<h3>Example 3: Create New DataFrame Using All But One Column from Old DataFrame</h3>
The following code shows how to create a new DataFrame using all but one column from the old DataFrame:
<b>#create new DataFrame from existing DataFrame
new_df = old_df.drop('points', axis=1)
#view new DataFrame
print(new_df)
  team  assists  rebounds
0    A        5        11
1    A        7         8
2    A        7        10
3    A        9         6
4    B       12         6
5    B        9         7
6    B        9         9
7    B        4        12
#check data type of new DataFrame
type(new_df)
pandas.core.frame.DataFrame
</b>
Notice that this new DataFrame contains all of the columns from the original DataFrame <em>except</em> the <b>points</b> column.
<h2><span class="orange">How to Create Pandas DataFrame from Series (With Examples)</span></h2>
Often you may want to create a pandas DataFrame from one or more pandas Series.
The following examples show how to create a pandas DataFrame using existing series as either the rows or columns of the DataFrame.
<h3>Example 1: Create Pandas DataFrame Using Series as Columns</h3>
Suppose we have the following three pandas Series:
<b>import pandas as pd
#define three Series
name = pd.Series(['A', 'B', 'C', 'D', 'E'])
points = pd.Series([34, 20, 21, 57, 68])
assists = pd.Series([8, 12, 14, 9, 11])
</b>
We can use the following code to convert each series into a DataFrame and then concatenate them all into one DataFrame:
<b>#convert each Series to a DataFrame
name_df = name.to_frame(name='name')
points_df = points.to_frame(name='points')
assists_df = assists.to_frame(name='assists')
#concatenate three Series into one DataFrame
df = pd.concat([name_df, points_df, assists_df], axis=1)
#view final DataFrame
print(df)
  name  points  assists
0    A      34        8
1    B      20       12
2    C      21       14
3    D      57        9
4    E      68       11</b>
Notice that the three series are each represented as columns in the final DataFrame.
<h3>Example 2: Create Pandas DataFrame Using Series as Rows</h3>
Suppose we have the following three pandas Series:
<b>import pandas as pd
#define three Series
row1 = pd.Series(['A', 34, 8])
row2 = pd.Series(['B', 20, 12])
row3 = pd.Series(['C', 21, 14])
</b>
We can use the following code to combine each of the Series into a pandas DataFrame, using each Series as a row in the DataFrame:
<b>#create DataFrame using Series as rows
df = pd.DataFrame([row1, row2, row3])
#create column names for DataFrame
df.columns = ['col1', 'col2', 'col3']
#view resulting DataFrame
print(df)
col1col2col3
0A348
1B2012
2C2114
</b>
Notice that the three series are each represented as rows in the final DataFrame.
<h2><span class="orange">How to Create Pandas DataFrame from a String</span></h2>
You can use the following basic syntax to create a pandas DataFrame from a string:
<b>import pandas as pd
import io   
df = pd.read_csv(io.StringIO(string_data), sep=",")
</b>
This particular syntax creates a pandas DataFrame using the values contained in the string called <b>string_data</b>.
The following examples show how to use this syntax in practice.
<h2>Example 1: Create DataFrame from String with Comma Separators</h2>
The following code shows how to create a pandas DataFrame from a string in which the values in the string are separated by commas:
<b>import pandas as pd
import io
#define string
string_data="""points, assists, rebounds
5, 15, 22
7, 12, 9
4, 3, 18
2, 5, 10
3, 11, 5
"""
#create pandas DataFrame from string
df = pd.read_csv(io.StringIO(string_data), sep=",")
#view DataFrame
print(df)
   points   assists   rebounds
0       5        15         22
1       7        12          9
2       4         3         18
3       2         5         10
4       3        11          5
</b>
The result is a pandas DataFrame with five rows and three columns.
<h2>Example 2: Create DataFrame from String with Semicolon Separators</h2>
The following code shows how to create a pandas DataFrame from a string in which the values in the string are separated by semicolons:
<b>import pandas as pd
import io
#define string
string_data="""points;assists;rebounds
5;15;22
7;12;9
4;3;18
2;5;10
3;11;5
"""
#create pandas DataFrame from string
df = pd.read_csv(io.StringIO(string_data), sep=";")
#view DataFrame
print(df)
   points   assists   rebounds
0       5        15         22
1       7        12          9
2       4         3         18
3       2         5         10
4       3        11          5
</b>
The result is a pandas DataFrame with five rows and three columns.
If you have a string with a different separator, simply use the <b>sep </b>argument within the <b>read_csv()</b> function to specify the separator.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Convert Pandas DataFrame Columns to Strings 
 How to Convert Timestamp to Datetime in Pandas 
 How to Convert Datetime to Date in Pandas 
<h2><span class="orange">Pandas: How to Create Empty DataFrame with Column Names</span></h2>
You can use the following basic syntax to create an empty pandas DataFrame with specific column names:
<b>df = pd.DataFrame(columns=['Col1', 'Col2', 'Col3'])
</b>
The following examples shows how to use this syntax in practice.
<h3>Example 1: Create DataFrame with Column Names & No Rows</h3>
The following code shows how to create a pandas DataFrame with specific column names and no rows:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame(columns=['A', 'B', 'C', 'D', 'E'])
#view DataFrame
df
A   B   C   D   E
</b>
We can use <b>shape</b> to get the size of the DataFrame:
<b>#display shape of DataFrame
df.shape
(0, 5)</b>
This tells us that the DataFrame has <b>0</b> rows and <b>5</b> columns.
We can also use <b>list()</b> to get a list of the column names:
<b>#display list of column names
list(df)
['A', 'B', 'C', 'D', 'E']
</b>
<h3>Example 2: Create DataFrame with Column Names & Specific Number of Rows</h3>
The following code shows how to create a pandas DataFrame with specific column names and a specific number of rows:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame(columns=['A', 'B', 'C', 'D', 'E'],  index=range(1, 10))
#view DataFrame
df
        ABCDE
1NaNNaNNaNNaNNaN
2NaNNaNNaNNaNNaN
3NaNNaNNaNNaNNaN
4NaNNaNNaNNaNNaN
5NaNNaNNaNNaNNaN
6NaNNaNNaNNaNNaN
7NaNNaNNaNNaNNaN
8NaNNaNNaNNaNNaN
9NaNNaNNaNNaNNaN</b>
Notice that every value in the DataFrame is filled with a NaN value.
Once again, we can use <b>shape</b> to get the size of the DataFrame:
<b>#display shape of DataFrame
df.shape
(9, 5)</b>
This tells us that the DataFrame has <b>9</b> rows and <b>5</b> columns.
<h2><span class="orange">Pandas: Create Date Column from Year, Month and Day</span></h2>
You can use the following basic syntax to create a date column from year, month, and day columns in a pandas DataFrame:
<b>df['date'] = pd.to_datetime(dict(year=df.year, month=df.month, day=df.day))
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Date Column from Year, Month and Day in Pandas</h2>
Suppose we have the following pandas DataFrame that shows the sales made by some company on various dates:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'year': [2021, 2022, 2022, 2022, 2022, 2022, 2022, 2022],   'month': [7, 1, 1, 2, 5, 10, 11, 12],   'day': [4, 15, 25, 27, 27, 24, 10, 18],   'sales': [140, 200, 250, 180, 130, 87, 90, 95]})
#view DataFrame
print(df)
   year  month  day  sales
0  2021      7    4    140
1  2022      1   15    200
2  2022      1   25    250
3  2022      2   27    180
4  2022      5   27    130
5  2022     10   24     87
6  2022     11   10     90
7  2022     12   18     95
</b>
We can use the following syntax to create a new column called <b>date</b> that combines the values from the <b>year</b>, <b>month</b>, and <b>day</b> columns in the DataFrame to create a date for each row:
<b>#create date column from year, month, and day columns
df['date'] = pd.to_datetime(dict(year=df.year, month=df.month, day=df.day))
#view updated DataFrame
print(df)
   year  month  day  sales       date
0  2021      7    4    140 2021-07-04
1  2022      1   15    200 2022-01-15
2  2022      1   25    250 2022-01-25
3  2022      2   27    180 2022-02-27
4  2022      5   27    130 2022-05-27
5  2022     10   24     87 2022-10-24
6  2022     11   10     90 2022-11-10
7  2022     12   18     95 2022-12-18
</b>
Notice that the <b>date</b> column contains date values based on the values from the <b>year</b>, <b>month</b>, and <b>day</b> columns in each row.
If we use <b>df.info()</b> to get information about each column in the DataFrame, we can see that the new <b>date</b> column has a data type of <b>datetime64</b>:
<b>#display information about each column in DataFrame
df.info()
&lt;class 'pandas.core.frame.DataFrame'>
RangeIndex: 8 entries, 0 to 7
Data columns (total 5 columns):
 #   Column  Non-Null Count  Dtype         
---  ------  --------------  -----         
 0   year    8 non-null      int64         
 1   month   8 non-null      int64         
 2   day     8 non-null      int64         
 3   sales   8 non-null      int64         
 4   date    8 non-null      datetime64[ns]
dtypes: datetime64[ns](1), int64(4)
memory usage: 388.0 bytes</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add and Subtract Days from a Date in Pandas 
 How to Select Rows Between Two Dates in Pandas 
 How to Calculate a Difference Between Two Dates in Pandas 
<h2><span class="orange">How to Create a Duplicate Column in Pandas DataFrame</span></h2>
You can use the following basic syntax to create a duplicate column in a pandas DataFrame:
<b>df['my_column_duplicate'] = df.loc[:, 'my_column']
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Duplicate Column in Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29, 32],   'assists': [5, 7, 7, 9, 12, 9, 9, 4, 5],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 8]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
8      32        5         8
</b>
We can use the following code to create a duplicate of the <b>points</b> column and name it <b>points_duplicate</b>:
<b>#create duplicate points column
df['points_duplicate'] = df.loc[:, 'points']
#view updated DataFrame
print(df)
   points  assists  rebounds  points_duplicate
0      25        5        11                25
1      12        7         8                12
2      15        7        10                15
3      14        9         6                14
4      19       12         6                19
5      23        9         5                23
6      25        9         9                25
7      29        4        12                29
8      32        5         8                32</b>
Notice that the <b>points_duplicate</b> column contains the exact same values as the <b>points</b> column.
Note that the duplicate column must have a different column name than the original column, otherwise a duplicate column will not be created.
For example, if we attempt to use the following code to create a duplicate column, it won’t work:
<b>#attempt to create duplicate points column
df['points'] = df.loc[:, 'points']
#view updated DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
8      32        5         8
</b>
No duplicate column was created.
The duplicate column must have a different column name than the original column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Show All Rows of a Pandas DataFrame 
 How to Check dtype for All Columns in Pandas DataFrame 
<h2><span class="orange">Pandas: Create New Column Using Multiple If Else Conditions</span></h2>
You can use the following syntax to create a new column in a pandas DataFrame using multiple if else conditions:
<b>#define conditions
conditions = [
    (df['column1'] == 'A') & (df['column2'] &lt; 20),
    (df['column1'] == 'A') & (df['column2'] >= 20),
    (df['column1'] == 'B') & (df['column2'] &lt; 20),
    (df['column1'] == 'B') & (df['column2'] >= 20)
]
#define results
results = ['result1', 'result2', 'result3', 'result4']
#create new column based on conditions in column1 and column2
df['new_column'] = np.select(conditions, results)
</b>
This particular example creates a column called <b>new_column</b> whose values are based on the values in <b>column1</b> and <b>column2</b> in the DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: Create New Column Using Multiple If Else Conditions in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [15, 18, 22, 24, 12, 17, 20, 28]})
#view DataFrame
print(df)
  team  points
0    A      15
1    A      18
2    A      22
3    A      24
4    B      12
5    B      17
6    B      20
7    B      28
</b>
Now suppose we would like to create a new column called <b>class</b> that classifies each player into one of the following four groups:
<b>Bad_A</b> if team is A and points &lt; 20
<b>Good_A</b> if team is A and points ≥ 20
<b>Bad_B</b> if team is B and points &lt; 20
<b>Good_B</b> if team is B and points ≥ 20
We can use the following syntax to do so:
<b>import numpy as np
#define conditions
conditions = [
    (df['team'] == 'A') & (df['points'] &lt; 20),
    (df['team'] == 'A') & (df['points'] >= 20),
    (df['team'] == 'B') & (df['points'] &lt; 20),
    (df['team'] == 'B') & (df['points'] >= 20)
]
#define results
results = ['Bad_A', 'Good_A', 'Bad_B', 'Good_B']
#create new column based on conditions in column1 and column2
df['class'] = np.select(conditions, results)
#view updated DataFrame
print(df)
  team  points   class
0    A      15   Bad_A
1    A      18   Bad_A
2    A      22  Good_A
3    A      24  Good_A
4    B      12   Bad_B
5    B      17   Bad_B
6    B      20  Good_B
7    B      28  Good_B</b>
The new column called <b>class</b> displays the classification of each player based on the values in the <b>team</b> and <b>points</b> columns.
<b>Note</b>: You can find the complete documentation for the NumPy <b>select()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Create Boolean Column Based on Condition 
 Pandas: How to Count Values in Column with Condition 
 Pandas: How to Use Groupby and Count with Condition 
<h2><span class="orange">How to Create a Tuple from Two Columns in Pandas</span></h2>
You can use the following basic syntax to create a tuple from two columns in a pandas DataFrame:
<b>df['new_column'] = list(zip(df.column1, df.column2))
</b>
This particular formula creates a new column called <b>new_column</b>, which is a tuple formed by <b>column1</b> and <b>column2</b> in the DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: Create Tuple from Two Columns in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
We can use the following syntax to create a new column called <b>points_assists</b>, which is a tuple formed by the values in the <b>points</b> and <b>assists</b> columns:
<b>#create new column that is a tuple of points and assists columns
df['points_assists'] = list(zip(df.points, df.assists))
#view updated DataFrame
print(df)
  team  points  assists points_assists
0    A      18        5        (18, 5)
1    B      22        7        (22, 7)
2    C      19        7        (19, 7)
3    D      14        9        (14, 9)
4    E      14       12       (14, 12)
5    F      11        9        (11, 9)
6    G      20        9        (20, 9)
7    H      28        4        (28, 4)
</b>
The new column called <b>points_assists</b> is a tuple formed by the <b>points</b> and <b>assists</b> columns.
Note that you can also include more than two columns in a tuple if you’d like.
For example, the following code shows how to create a tuple that uses values from all three original columns in the DataFrame:
<b>#create new column that is a tuple of team, points and assists columns
df['all_columns'] = list(zip(df.team, df.points, df.assists))
#view updated DataFrame
print(df)
  team  points  assists  all_columns
0    A      18        5   (A, 18, 5)
1    B      22        7   (B, 22, 7)
2    C      19        7   (C, 19, 7)
3    D      14        9   (D, 14, 9)
4    E      14       12  (E, 14, 12)
5    F      11        9   (F, 11, 9)
6    G      20        9   (G, 20, 9)
7    H      28        4   (H, 28, 4)
</b>
You can use this same basic syntax to create a tuple column with as many columns as you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Duplicate Rows in Pandas 
 How to Drop Duplicate Columns in Pandas 
 How to Count Duplicates in Pandas 
<h2><span class="orange">How to Perform a Cross Join in Pandas (With Example)</span></h2>
You can use the following basic syntax to perform a cross join in pandas:
<b>#create common key
df1['key'] = 0
df2['key'] = 0
#outer merge on common key (e.g. a cross join)
df1.merge(df2, on='key', how='outer')
</b>
The following example shows how to use this function in practice.
<h2>Example: Perform Cross Join in Pandas</h2>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D'],    'points': [18, 22, 19, 14]})
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
#create second  DataFrame
df2 = pd.DataFrame({'team': ['A', 'B', 'F'],    'assists': [4, 9, 8]})
print(df2)
  team  assists
0    A        4
1    B        9
2    F        8
</b>
The following code shows how to perform a cross join on the two DataFrames:
<b>#create common key
df1['key'] = 0
df2['key'] = 0
#perform cross join
df3 = df1.merge(df2, on='key', how='outer')
#drop key columm
del df3['key']
#view results
print(df3)
   team_x  points team_y  assists
0       A      18      A        4
1       A      18      B        9
2       A      18      F        8
3       B      22      A        4
4       B      22      B        9
5       B      22      F        8
6       C      19      A        4
7       C      19      B        9
8       C      19      F        8
9       D      14      A        4
10      D      14      B        9
11      D      14      F        8</b>
The result is one DataFrame that contains every possible combination of rows from each DataFrame.
For example, the first row of the first DataFrame contains team <b>A</b> and <b>18</b> points. This row is matched with every single row in the second DataFrame.
Next, the second row of the first DataFrame contains team <b>B</b> and <b>22</b> points. This row is also matched with every single row in the second DataFrame.
The end result is a DataFrame with 12 rows.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Do a Left Join in Pandas 
 How to Do a Left Join in Pandas 
 Pandas Join vs. Merge: What’s the Difference? 
<h2><span class="orange">How to Calculate a Reversed Cumulative Sum in Pandas</span></h2>
The  cumsum()  function can be used to calculate the cumulative sum of values in a column of a pandas DataFrame.
You can use the following syntax to calculate a <b>reversed cumulative sum</b> of values in a column:
<b>df['cumsum_reverse'] = df.loc[::-1, 'my_column'].cumsum()[::-1]</b>
This particular syntax adds a new column called <b>cumsum_reverse</b> to a pandas DataFrame that shows the reversed cumulative sum of values in the column titled <b>my_column</b>.
The following example shows how to use this syntax in practice.
<h3>Example: Calculate a Reversed Cumulative Sum in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the total sales made by some store during 10 consecutive days:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],   'sales': [3, 6, 0, 2, 4, 1, 0, 1, 4, 7]})
#view DataFrame
df
      day   sales
013
126
230
342
454
561
670
781
894
9107
</b>
We can use the following syntax to calculate a <b>reversed cumulative sum</b> of the sales column:
<b>#add new column that shows reverse cumulative sum of sales
df['cumsum_reverse_sales'] = df.loc[::-1, 'sales'].cumsum()[::-1]
#view updated DataFrame
df
daysalescumsum_reverse_sales
01328
12625
23019
34219
45417
56113
67012
78112
89411
91077</b>
The new column titled <b>cumsum_reverse_sales</b> shows the cumulative sales <em>starting from the last row</em>.
Here’s how we would interpret the values in the <b>cumsum_reverse_sales</b> column:
The cumulative sum of sales for day 10 is <b>7</b>.
The cumulative sum of sales for day 10 and day 9 is <b>11</b>.
The cumulative sum of sales for day 10, day 9, and day 8 is <b>12</b>.
The cumulative sum of sales for day 10, day 9, day 8, and day 7 is <b>12</b>.
And so on.
<h2><span class="orange">How to Calculate Cumulative Count in Pandas</span></h2>
You can use the following methods to calculate a cumulative count in a pandas DataFrame:
<b>Method 1: Cumulative Count by Group</b>
<b>df['cum_count'] = df.groupby('col1').cumcount()
</b>
<b>Method 2: Cumulative Count by Multiple Groups</b>
<b>df['cum_count'] = df.groupby(['col1', 'col2']).cumcount() </b>
The following examples shows how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'G', 'F', 'G','G', 'F', 'F'],   'points': [14, 22, 25, 34, 30, 12, 10, 18]})
#view DataFrame
print(df)
  team position  points
0    A        G      14
1    A        G      22
2    A        G      25
3    A        F      34
4    B        G      30
5    B        G      12
6    B        F      10
7    B        F      18
</b>
<h2>Example 1: Cumulative Count by Group in Pandas</h2>
We can use the following syntax to create a new column called <b>team_cum_count</b> that displays the cumulative count for each <b>team</b> in the DataFrame:
<b>#calculate cumulative count by team
df['team_cum_count'] = df.groupby('team').cumcount()
#view updated DataFrame
print(df)
  team position  points  team_cum_count
0    A        G      14               0
1    A        G      22               1
2    A        G      25               2
3    A        F      34               3
4    B        G      30               0
5    B        G      12               1
6    B        F      10               2
7    B        F      18               3</b>
The new column called <b>team_cum_count</b> contains the cumulative count of each <b>team</b>, starting with a value of zero.
If you’d like the count to start at one instead, simply add one to the end of the line:
<b>#calculate cumulative count (starting at 1) by team
df['team_cum_count'] = df.groupby('team').cumcount() + 1
#view updated DataFrame
print(df)
  team position  points  team_cum_count
0    A        G      14               1
1    A        G      22               2
2    A        G      25               3
3    A        F      34               4
4    B        G      30               1
5    B        G      12               2
6    B        F      10               3
7    B        F      18               4
</b>
The new column called <b>team_cum_count</b> contains the cumulative count of each <b>team</b>, starting with a value of one.
<h2>Example 2: Calculate Cumulative Count by Group in Pandas</h2>
We can use the following syntax to create a new column called <b>team_pos_cum_count</b> that displays the cumulative count for each <b>team </b>and <b>position</b> in the DataFrame:
<b>#calculate cumulative count by team
df['team_pos_cum_count'] = df.groupby(['team', 'position']).cumcount() 
#view updated DataFrame
print(df)
  team position  points  team_pos_cum_count
0    A        G      14                   0
1    A        G      22                   1
2    A        G      25                   2
3    A        F      34                   0
4    B        G      30                   0
5    B        G      12                   1
6    B        F      10                   0
7    B        F      18                   1
</b>
The new column called <b>team_pos_cum_count</b> contains the cumulative count of each <b>team</b> and <b>position</b> starting with a value of zero.
<b>Note</b>: You can find the complete documentation for the <b>cumcount </b>function in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Sum Specific Columns in Pandas 
 How to Sum Columns Based on a Condition in Pandas 
 How to Calculate a Reversed Cumulative Sum in Pandas 
<h2><span class="orange">How to Calculate Cumulative Percentage in Pandas</span></h2>
You can use the following basic syntax to calculate the cumulative percentage of values in a column of a pandas DataFrame:
<b>#calculate cumulative sum of column
df['cum_sum'] = df['col1'].cumsum()
#calculate cumulative percentage of column (rounded to 2 decimal places)
df['cum_percent'] = round(100*df.cum_sum/df['col1'].sum(),2)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Cumulative Percentage in Pandas</h3>
Suppose we have the following pandas DataFrame that shows the number of units a company sells during consecutive years:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'year': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],   'units_sold': [60, 75, 77, 87, 104, 134, 120, 125, 140, 150]})
#view DataFrame
print(df)
   year  units_sold
0     1          60
1     2          75
2     3          77
3     4          87
4     5         104
5     6         134
6     7         120
7     8         125
8     9         140
9    10         150
</b>
Next, we can use the following code to add a column that shows the cumulative number of units sold and cumulative percentage of units sold:
<b>#calculate cumulative sum of units sold
df['cum_sum'] = df['units_sold'].cumsum()
#calculate cumulative percentage of units sold
df['cum_percent'] = round(100*df.cum_sum/df['units_sold'].sum(),2)
#view updated DataFrame
print(df)
   year  units_sold  cum_sum  cum_percent
0     1          60       60         5.60
1     2          75      135        12.59
2     3          77      212        19.78
3     4          87      299        27.89
4     5         104      403        37.59
5     6         134      537        50.09
6     7         120      657        61.29
7     8         125      782        72.95
8     9         140      922        86.01
9    10         150     1072       100.00
</b>
We interpret the cumulative percentages as follows:
<b>5.60%</b> of all sales were made in year 1.
<b>12.59</b> of all sales were made in years 1 and 2 combined.
<b>19.78%</b> of all sales were made in years 1, 2, and 3 combined.
And so on.
Note that you can simply change the value in the <b>round()</b> function to change the number of decimal points shown as well.
For example, we could round the cumulative percentage to zero decimal places instead:
<b>#calculate cumulative sum of units sold
df['cum_sum'] = df['units_sold'].cumsum()
#calculate cumulative percentage of units sold
df['cum_percent'] = round(100*df.cum_sum/df['units_sold'].sum(),0)
#view updated DataFrame
print(df)
   year  units_sold  cum_sum  cum_percent
0     1          60       60          6.0
1     2          75      135         13.0
2     3          77      212         20.0
3     4          87      299         28.0
4     5         104      403         38.0
5     6         134      537         50.0
6     7         120      657         61.0
7     8         125      782         73.0
8     9         140      922         86.0
9    10         150     1072        100.0</b>
The cumulative percentages are now rounded to zero decimal places.
<h2><span class="orange">Pandas: How to Calculate Cumulative Sum by Group</span></h2>
You can use the following syntax to calculate a cumulative sum by group in pandas:
<b>df['cumsum_col'] = df.groupby(['col1'])['col2'].cumsum()
</b>
This particular formula calculates the cumulative sum of <b>col2</b>, grouped by <b>col1</b>, and displays the results in a new column titled <b>cumsum_col</b>.
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Cumulative Sum by Group in Pandas</h3>
Suppose we have the following pandas DataFrame that contains information about sales for various stores:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'sales': [4, 7, 10, 5, 8, 9, 12, 15, 10, 8]})
#view DataFrame
print(df)
  store  sales
0     A      4
1     A      7
2     A     10
3     A      5
4     A      8
5     B      9
6     B     12
7     B     15
8     B     10
9     B      8</b>
We can use the following syntax to calculate the cumulative sum of sales for each store:
<b>#add column that shows cumulative sum of sales by store
df['cumsum_sales'] = df.groupby(['store'])['sales'].cumsum()
#view updated DataFrame
print(df)
  store  sales  cumsum_sales
0     A      4             4
1     A      7            11
2     A     10            21
3     A      5            26
4     A      8            34
5     B      9             9
6     B     12            21
7     B     15            36
8     B     10            46
9     B      8            54
</b>
The <b>cumsum_sales</b> column shows the cumulative sales, grouped by each store.
<b>Note</b>: You can find the complete documentation for the <b>cumsum </b>function in pandas  here .
<h2><span class="orange">How to Fix: pandas data cast to numpy dtype of object. Check input data with np.asarray(data).</span></h2>
One error you may encounter when using Python is:
<b>ValueError: Pandas data cast to numpy dtype of object. Check input data with
np.asarray(data).
</b>
This error occurs when you attempt to fit a regression model in Python and fail to convert categorical variables to  dummy variables  first before fitting the model.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'points': [14, 19, 8, 12, 17, 19, 22, 25]})
#view DataFrame
df
teamassistsrebounds points
0A511 14
1A78 19
2A710 8
3A96 12
4B126 17
5B95 19
6B99 22
7B412 25</b>
Now suppose we attempt to fit a  multiple linear regression model  using team, assists, and rebounds as predictor variables and points as the  response variable :
<b>import statsmodels.api as sm
#define response variable
y = df['points']
#define predictor variables
x = df[['team', 'assists', 'rebounds']]
#add constant to predictor variables
x = sm.add_constant(x)
#attempt to fit regression model
model = sm.OLS(y, x).fit()
ValueError: Pandas data cast to numpy dtype of object. Check input data with
np.asarray(data).
</b>
We receive an error because the variable “team” is categorical and we did not convert it to a dummy variable before fitting the regression model.
<h3>How to Fix the Error</h3>
The easiest way to fix this error is to convert the “team” variable to a dummy variable using the  pandas.get_dummies()  function.
<b>Note</b>: Check out  this tutorial  for a quick refresher on dummy variables in regression models.
The following code shows how to convert “team” to a dummy variable:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'points': [14, 19, 8, 12, 17, 19, 22, 25]})
#convert "team" to dummy variable
df = pd.get_dummies(df, columns=['team'], drop_first=True)
#view updated DataFrame
df
        assistsrebounds pointsteam_B
0511 140
178 190
2710 80
396 120
4126 171
595 191
699 221
7412 251</b>
The values in the “team” column have been converted from “A” and “B” to 0 and 1.
We can now fit the multiple linear regression model using the new “team_B” variable:
<b>import statsmodels.api as sm
#define response variable
y = df['points']
#define predictor variables
x = df[['team_B', 'assists', 'rebounds']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit regression model
model = sm.OLS(y, x).fit()
#view summary of model fit
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                 points   R-squared:                       0.701
Model:                            OLS   Adj. R-squared:                  0.476
Method:                 Least Squares   F-statistic:                     3.119
Date:                Thu, 11 Nov 2021   Prob (F-statistic):              0.150
Time:                        14:49:53   Log-Likelihood:                -19.637
No. Observations:                   8   AIC:                             47.27
Df Residuals:                       4   BIC:                             47.59
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         27.1891     17.058      1.594      0.186     -20.171      74.549
team_B         9.1288      3.032      3.010      0.040       0.709      17.548
assists       -1.3445      1.148     -1.171      0.307      -4.532       1.843
rebounds      -0.5174      1.099     -0.471      0.662      -3.569       2.534
==============================================================================
Omnibus:                        0.691   Durbin-Watson:                   3.075
Prob(Omnibus):                  0.708   Jarque-Bera (JB):                0.145
Skew:                           0.294   Prob(JB):                        0.930
Kurtosis:                       2.698   Cond. No.                         140.
==============================================================================
</b>
Notice that we’re able to fit the regression model without any errors this time.
<b>Note</b>: You can find the complete documentation for the <b>ols()</b> function from the statsmodels library  here .
<h2><span class="orange">How to Get Cell Value from Pandas DataFrame</span></h2>
You can use the following syntax to get a cell value from a pandas DataFrame:
<b>#iloc method
df.iloc[0]['column_name']
#at method
df.at[0,'column_name']
#values method
df['column_name'].values[0]
</b>
Note that all three methods will return the same value.
The following examples show how to use each of these methods with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
        pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412
</b>
<h3>Method 1: Get Cell Value Using iloc Function</h3>
The following code shows how to use the <b>.iloc</b> function to get various cell values in the pandas DataFrame:
<b>#get value in first row in 'points' column
df.iloc[0]['points']
25
#get value in second row in 'assists' column
df.iloc[1]['assists']
7</b>
<h3>
<b>Method 2: Get Cell Value Using at Function</b>
</h3>
The following code shows how to use the <b>.at </b>function to get various cell values in the pandas DataFrame:
<b>#get value in first row in 'points' column
df.at[0, 'points']
25
#get value in second row in 'assists' column
df.at[1, 'assists']
7</b>
<h3>
<b>Method 3: Get Cell Value Using values Function</b>
</h3>
The following code shows how to use the <b>.values </b>function to get various cell values in the pandas DataFrame:
<b>#get value in first row in 'points' column
df['points'].values[0] 
25
#get value in second row in 'assists' column
df['assists'].values[1] 
7</b>
Notice that all three methods return the same values.
<h2><span class="orange">How to Get First Column of Pandas DataFrame (With Examples)</span></h2>
You can use the following syntax to get the first column of a pandas DataFrame:
<b>df.iloc[:, 0]
</b>
The result is a pandas Series. If you’d like the result to be a pandas DataFrame instead, you can use the following syntax:
<b>df.iloc[:, :1]</b>
The following examples show how to use this syntax in practice.
<h3>
<b>Example 1: Get First Column of Pandas DataFrame (Return a Series)</b>
</h3>
The following code shows how to get the first column of a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
#get first column
first_col = df.iloc[:, 0]
#view first column
print(first_col)
0    25
1    12
2    15
3    14
4    19
5    23
6    25
7    29
Name: points, dtype: int64
</b>
The result is a pandas Series:
<b>#check type of <em>first_col</em>
print(type(first_col))
&lt;class 'pandas.core.series.Series'>
</b>
<h3>Example 2: Get First Column of Pandas DataFrame (Return a DataFrame)</h3>
The following code shows how to get the first column of a pandas DataFrame and return a DataFrame as a result:
<b>#get first column (and return a DataFrame)
first_col = df.iloc[:, :1]
#view first column
print(first_col)
   points
0      25
1      12
2      15
3      14
4      19
5      23
6      25
7      29
#check type of <em>first_col
</em>print(type(first_col))
&lt;class 'pandas.core.frame.DataFrame'>
</b>
<h2><span class="orange">How to Get First Row of Pandas DataFrame (With Examples)</span></h2>
You can use the following methods to get the first row of a pandas DataFrame:
<b>Method 1: Get First Row of DataFrame</b>
<b>df.iloc[0]
</b>
<b>Method 2: Get First Row of DataFrame for Specific Columns</b>
<b>df[['column1', 'column2']].iloc[0]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12</b>
<h2>Example 1: Get First Row of Pandas DataFrame</h2>
The following code shows how to get the first row of a pandas DataFrame:
<b>#get first row of DataFrame
df.iloc[0]
points      25
assists      5
rebounds    11
Name: 0, dtype: int64</b>
Notice that the values in the first row for each column of the DataFrame are returned.
<h2>Example 2: Get First Row of Pandas DataFrame for Specific Columns</h2>
The following code shows how to get the values in the first row of the pandas DataFrame for only the <b>points</b> and <b>rebounds</b> columns:
<b>#get first row of values for points and rebounds columns
df[['points', 'rebounds']].iloc[0]
points      25
rebounds    11
Name: 0, dtype: int64
</b>
Notice that the values in the first row for the <b>points</b> and <b>rebounds</b> columns are returned.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Get First Column of Pandas DataFrame 
 How to Add Rows to a Pandas DataFrame 
 How to Count Number of Rows in Pandas DataFrame 
<h2><span class="orange">How to Add Header Row to Pandas DataFrame (With Examples)</span></h2>
You can use one of the following three methods to add a header row to a pandas DataFrame:
<b>#add header row when creating DataFrame
df = pd.DataFrame(data=[data_values],  columns=['col1', 'col2', 'col3'])
#add header row after creating DataFrame
df = pd.DataFrame(data=[data_values])
df.columns = ['A', 'B', 'C']
#add header row when importing CSV
df = pd.read_csv('data.csv', names=['A', 'B', 'C'])
</b>
The following examples show how to use each of these methods in practice.
<h3>Example 1: Add Header Row When Creating DataFrame</h3>
The following code shows how to add a header row when creating a pandas DataFrame:
<b>import pandas as pd
import numpy as np
#add header row when creating DataFrame 
df = pd.DataFrame(data=np.random.randint(0, 100, (10, 3)),  columns =['A', 'B', 'C'])
#view DataFrame
df
ABC
0814782
1927188
2617996
3562268
4646641
5984983
6709411
71611
8558739
9155867</b>
<h3>Example 2: Add Header Row After Creating DataFrame</h3>
The following code shows how to add a header row <em>after</em> creating a pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame(data=np.random.randint(0, 100, (10, 3))) 
#add header row to DataFrame
df.columns = ['A', 'B', 'C']
#view DataFrame
df
ABC
0814782
1927188
2617996
3562268
4646641
5984983
6709411
71611
8558739
9155867
</b>
<h3>Example 3: Add Header Row When Importing DataFrame</h3>
The following code shows how to add a header row using the <b>names</b> argument when importing a pandas DataFrame from a CSV file:
<b>import pandas as pd
import numpy as np
#import CSV file and specify header row names
df = pd.read_csv('data.csv', names=['A', 'B', 'C'])
#view DataFrame
df
ABC
0814782
1927188
2617996
3562268
4646641
5984983
6709411
71611
8558739
9155867</b>
<b>Related:</b>  How to Read CSV Files with Pandas 
<h2><span class="orange">How to Fix: TypeError: ‘DataFrame’ object is not callable</span></h2>
One common error you may encounter when using pandas is:
<b>TypeError: 'DataFrame' object is not callable
</b>
This error usually occurs when you attempt to perform some calculation on a variable in a pandas DataFrame by using round <b>()</b> brackets instead of square <b>[ ]</b> brackets.
The following example shows how to use this syntax in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
Now suppose we attempt to calculate the mean value in the “points” column:
<b>#attempt to calculate mean value in points column
df('points').mean()
TypeError: 'DataFrame' object is not callable
</b>
Since we used round <b>()</b> brackets, pandas thinks that we’re attempting to call the DataFrame as a function.
Since the DataFrame is not a function, we receive an error.
<h3>How to Fix the Error</h3>
The way to resolve this error is to simply use square <b>[ ]</b> brackets when accessing the points column instead round <b>()</b> brackets:
<b>#calculate mean value in points column
df['points'].mean()
18.25
</b>
We’re able to calculate the mean of the points column (18.25) without receiving any error since we used squared brackets.
Also note that we could use the following dot notation to calculate the mean of the points column as well:
<b>#calculate mean value in points column
df.points.mean()
18.25</b>
Notice that we don’t receive any error this time either.
<h2><span class="orange">How to Add Table Title to Pandas DataFrame</span></h2>
You can use the <b>set_title()</b> function from matplotlib to add a title to a table created from a pandas DataFrame:
<b>ax.set_title('Some Title')
</b>
The following example shows how to use this function in practice.
<h2>Example: Add Table Title to Pandas DataFrame</h2>
Suppose we have the following pandas DataFrame that shows the points and assists for various basketball teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
We can use the following code to create a table in matplotlib that displays the values from the DataFrame and use <b>set_title()</b> to specify a title for the table:
<b>import matplotlib.pyplot as plt
#initialize figure
fig = plt.figure(figsize = (8, .2))
ax = fig.add_subplot(111)
#create table
ax.table(cellText = df.values, rowLabels = df.index, 
         colLabels = df.columns, cellLoc='center')
#add title to table
ax.set_title('Points and Assists by Team')
#turn axes off
ax.axis('off')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/title1.jpg">
<b>Note</b>: You can find the complete documentation for the <b>table()</b> function in matplotlib  here .
Notice that the title ‘Points and Assists by Team’ has been added above the table.
Also note that you can use the <b>fontdict</b> and <b>loc</b> arguments to modify the title font and title location:
<b>import matplotlib.pyplot as plt
#initialize figure
fig = plt.figure(figsize = (8, .2))
ax = fig.add_subplot(111)
#create table
ax.table(cellText = df.values, rowLabels = df.index, 
         colLabels = df.columns, cellLoc='center')
#add title to table
ax.set_title('Points and Assists by Team', 
              fontdict={'fontsize': 20,        'fontweight': 'bold',        'color': 'steelblue'},
              loc='left')
#turn axes off
ax.axis('off')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/title2.jpg"667">
Notice that the title font is now larger, bold, left-aligned, and blue.
Refer to the  matplotlib documentation  for a complete list of ways you can modify the appearance of the title.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Add Titles to Plots in Pandas 
 How to Make a Scatterplot From Pandas DataFrame 
 How to Create a Histogram from Pandas DataFrame 
<h2><span class="orange">Pandas: Quickly Convert DataFrame to Dictionary</span></h2>
You can use the following syntax to convert a pandas DataFrame to a dictionary:
<b>df.to_dict()
</b>
Note that <b>to_dict()</b> accepts the following potential arguments:
<b>dict:</b> (default) Keys are column names. Values are dictionaries of index:data pairs.
<b>list:</b> Keys are column names. Values are lists of column data.
<b>series:</b> Keys are column names. Values are Series of column data.
<b>split:</b> Keys are ‘columns’, ‘data’, and ‘index’.
<b>records:</b> Keys are column names. Values are data in cells.
<b>index:</b> Keys are index labels. Values are data in cells.
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'C'],   'points': [5, 7, 9, 12, 9],   'rebounds': [11, 8, 6, 6, 5]})
#view DataFrame
df
teampointsrebounds
0A511
1A78
2B96
3B126
4C95
</b>
<h3>Example 1: Convert DataFrame to Dictionary (‘dict’)</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the default ‘<b>dict</b>‘ method:
<b>df.to_dict()
{'team': {0: 'A', 1: 'A', 2: 'B', 3: 'B', 4: 'C'},
 'points': {0: 5, 1: 7, 2: 9, 3: 12, 4: 9},
 'rebounds': {0: 11, 1: 8, 2: 6, 3: 6, 4: 5}}</b>
<h3>
<b>Example 2: Convert DataFrame to Dictionary (‘list’)</b>
</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the ‘<b>list</b>‘ method:
<b>df.to_dict('list')
{'team': ['A', 'A', 'B', 'B', 'C'],
 'points': [5, 7, 9, 12, 9],
 'rebounds': [11, 8, 6, 6, 5]}</b>
<h3>
<b>Example 3: Convert DataFrame to Dictionary (‘series’)</b>
</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the ‘<b>series</b>‘ method:
<b>df.to_dict('series')
{'team': 0    A
 1    A
 2    B
 3    B
 4    C
 Name: team, dtype: object,
 'points': 0     5
 1     7
 2     9
 3    12
 4     9
 Name: points, dtype: int64,
 'rebounds': 0    11
 1     8
 2     6
 3     6
 4     5
 Name: rebounds, dtype: int64}
</b>
<h3>
<b>Example 4: Convert DataFrame to Dictionary (‘split’)</b>
</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the ‘<b>split</b>‘ method:
<b>df.to_dict('split')
{'index': [0, 1, 2, 3, 4],
 'columns': ['team', 'points', 'rebounds'],
 'data': [['A', 5, 11], ['A', 7, 8], ['B', 9, 6], ['B', 12, 6], ['C', 9, 5]]}</b>
<h3>
<b>Example 5: Convert DataFrame to Dictionary (‘records’)</b>
</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the ‘<b>records</b>‘ method:
<b>df.to_dict('records')
[{'team': 'A', 'points': 5, 'rebounds': 11},
 {'team': 'A', 'points': 7, 'rebounds': 8},
 {'team': 'B', 'points': 9, 'rebounds': 6},
 {'team': 'B', 'points': 12, 'rebounds': 6},
 {'team': 'C', 'points': 9, 'rebounds': 5}]</b>
<h3>
<b>Example 6: Convert DataFrame to Dictionary (‘index’)</b>
</h3>
The following code shows how to convert a pandas DataFrame to a dictionary using the ‘<b>index</b>‘ method:
<b>df.to_dict('index')
{0: {'team': 'A', 'points': 5, 'rebounds': 11},
 1: {'team': 'A', 'points': 7, 'rebounds': 8},
 2: {'team': 'B', 'points': 9, 'rebounds': 6},
 3: {'team': 'B', 'points': 12, 'rebounds': 6},
 4: {'team': 'C', 'points': 9, 'rebounds': 5}}</b>
<h2><span class="orange">Convert Pandas DataFrame to NumPy Array (With Examples)</span></h2>
You can use the following syntax to convert a pandas DataFrame to a NumPy array:
<b>df.to_numpy()
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert DataFrame with Same Data Types</h3>
The following code shows how to convert a pandas DataFrame to a NumPy array when each of the columns in the DataFrame is the same data type:
<b>import pandas as pd
#create data frame
df1 = pd.DataFrame({'rebounds': [7, 7, 8, 13, 7, 4],    'points': [5, 7, 7, 9, 12, 9],    'assists': [11, 8, 10, 6, 6, 5]})
#view data frame
print(df1)
   rebounds  points  assists
0         7       5       11
1         7       7        8
2         8       7       10
3        13       9        6
4         7      12        6
5         4       9        5
#convert DataFrame to NumPy array
new = df1.to_numpy()
#view NumPy array
print(new)
[[ 7  5 11]
 [ 7  7  8]
 [ 8  7 10]
 [13  9  6]
 [ 7 12  6]
 [ 4  9  5]]
#confirm that <em>new</em> is a NumPy array
print(type(new))
&lt;class 'numpy.ndarray'> 
#view data type
print(new.dtype)
int64
</b>
The Numpy array has a data type of <b>int64</b> since each column in the original pandas DataFrame was an integer.
<h3>Example 2: Convert DataFrame with Mixed Data Types</h3>
The following code shows how to convert a pandas DataFrame to a NumPy array when the columns in the DataFrame are not all the same data type:
<b>import pandas as pd
#create data frame
df2 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [5, 7, 7, 9, 12, 9],    'assists': [11, 8, 10, 6, 6, 5]})
#view data frame
print(df2)
  player  points  assists
0      A       5       11
1      B       7        8
2      C       7       10
3      D       9        6
4      E      12        6
5      F       9        5
#convert DataFrame to NumPy array
new = df2.to_numpy()
#view NumPy array
print(new)
[['A' 5 11]
 ['B' 7 8]
 ['C' 7 10]
 ['D' 9 6]
 ['E' 12 6]
 ['F' 9 5]]
#confirm that <em>new</em> is a NumPy array
print(type(new))
&lt;class 'numpy.ndarray'> 
#view data type
print(new.dtype)
object
</b>
The Numpy array has a data type of <b>object</b> since not every column in the original pandas DataFrame was the same data type.
<h3>Example 3: Convert DataFrame & Set NA Values</h3>
The following code shows how to convert a pandas DataFrame to a NumPy array and specify the values to be set for any NA values in the original DataFrame:
<b>import pandas as pd
#create data frame
df3 = pd.DataFrame({'player': ['A', 'B', pd.NA, 'D', 'E', 'F'],    'points': [5, 7, pd.NA, 9, pd.NA, 9],    'assists': [11, 8, 10, 6, 6, 5]})
#view data frame
print(df3)
  player points  assists
0      A      5       11
1      B      7        8
2   &lt;NA>   &lt;NA>       10
3      D      9        6
4      E   &lt;NA>        6
5      F      9        5
#convert DataFrame to NumPy array
new = df3.to_numpy(na_value='none')
#view NumPy array
print(new)
[['A' 5 11]
 ['B' 7 8]
 ['none' 'none' 10]
 ['D' 9 6]
 ['E' 'none' 6]
 ['F' 9 5]]
#confirm that <em>new</em> is a NumPy array
print(type(new))
&lt;class 'numpy.ndarray'> 
#view data type
print(new.dtype)
object</b>
<h2><span class="orange">Pandas: How to Calculate a Difference Between Two Dates</span></h2>
You can use the following syntax to calculate a difference between two dates in a pandas DataFrame:
<b>df['diff_days'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'D')
</b>
This particular example calculates the difference between the dates in the <b>end_date</b> and <b>start_date</b> columns in terms of days.
Note that we can replace the ‘D’ in the <b>timedelta64()</b> function with the following values to calculate the date difference in different units:
<b>W</b>: Weeks
<b>M</b>: Months
<b>Y</b>: Years
The following examples show how to calculate a date difference in a pandas DataFrame in practice.
<h3>Example 1: Calculate Difference Between Two Dates with Datetime Columns</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'start_date': pd.date_range(start='1/5/2020', periods=6, freq='W'),   'end_date': pd.date_range(start='6/1/2020', periods=6, freq='M')})
#view DataFrame
print(df)
  start_date   end_date
0 2020-01-05 2020-06-30
1 2020-01-12 2020-07-31
2 2020-01-19 2020-08-31
3 2020-01-26 2020-09-30
4 2020-02-02 2020-10-31
5 2020-02-09 2020-11-30
#view dtype of each column in DataFrame
df.dtypes
start_date    datetime64[ns]
end_date      datetime64[ns]
dtype: object
</b>
Since both columns in the DataFrame already have a dtype of <b>datetime64</b>, we can use the following syntax to calculate the difference between the start and end dates:
<b>import numpy as np
#create new columns that contains date differences
df['diff_days'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'D')
df['diff_weeks'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'W')
df['diff_months'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'M')
df['diff_years'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'Y')
#view updated DataFrame
print(df)
  start_date   end_date  diff_days  diff_weeks  diff_months  diff_years
0 2020-01-05 2020-06-30      177.0   25.285714     5.815314    0.484610
1 2020-01-12 2020-07-31      201.0   28.714286     6.603832    0.550319
2 2020-01-19 2020-08-31      225.0   32.142857     7.392349    0.616029
3 2020-01-26 2020-09-30      248.0   35.428571     8.148011    0.679001
4 2020-02-02 2020-10-31      272.0   38.857143     8.936528    0.744711
5 2020-02-09 2020-11-30      295.0   42.142857     9.692191    0.807683
</b>
The new columns contain the date differences between the start and end dates in terms of days, weeks, months, and years.
<h3>Example 2: Calculate Difference Between Two Dates with String Columns</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'start_date': ['2020-01-05', '2020-01-12', '2020-01-19'],   'end_date': ['2020-06-30', '2020-07-31', '2020-08-31']})
#view dtype of each column
print(df.dtypes)
start_date    object
end_date      object
dtype: object
</b>
Since neither column in the DataFrame has a dtype of <b>datetime64</b>, we will receive an error if we attempt to calculate the difference between the dates:
<b>import numpy as np
#attempt to calculate date difference
df['diff_days'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'D')
TypeError: unsupported operand type(s) for -: 'str' and 'str'
</b>
We must first use <b>pd.to_datetime</b> to convert each column to a datetime format before calculating the difference between the dates:
<b>import numpy as np
#convert columns to datetime
df[['start_date','end_date']] = df[['start_date','end_date']].apply(pd.to_datetime)
#calculate difference between dates
df['diff_days'] = (df['end_date'] - df['start_date']) / np.timedelta64(1, 'D')
#view updated DataFrame
print(df)
  start_date   end_date  diff_days
0 2020-01-05 2020-06-30      177.0
1 2020-01-12 2020-07-31      201.0
2 2020-01-19 2020-08-31      225.0
</b>
Since we first converted each column to a datetime format, we were able to successfully calculate the difference between the dates without any errors.
<h2><span class="orange">How to Create a Date Range in Pandas (3 Examples)</span></h2>
You can use the <b>pandas.date_range()</b> function to create a date range in pandas.
This function uses the following basic syntax:
<b>pandas.date_range(start, end, periods, freq, …)</b>
where:
<b>start</b>: The start date
<b>end</b>: The end date
<b>periods:</b> The number of periods to generate
<b>freq</b>: The frequency to use (refer to  this list  for frequency aliases)
The following examples show how to use this function in practice.
<h3>Example 1: Create Date Range with Individual Days</h3>
The following code shows how to create a date range composed of individual days with a specific start and end date:
<b>import pandas as pd
#create 10-day date range 
pd.date_range(start='1/1/2020', end='1/10/2020')
DatetimeIndex(['2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04',
               '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08',
               '2020-01-09', '2020-01-10'],
              dtype='datetime64[ns]', freq='D')</b>
The result is a list of 10 days that range from the specified start date to the specified end date.
<h3>Example 2: Create Date Range with Specific Number of Periods</h3>
The following code shows how to create a date range that has a specific number of equally-spaced periods between a certain start and end date:
<b>import pandas as pd
#create 10-day date range with 3 equally-spaced periods
pd.date_range(start='1/1/2020', end='1/10/2020', periods=3)
DatetimeIndex(['2020-01-01 00:00:00', '2020-01-05 12:00:00',
               '2020-01-10 00:00:00'],
              dtype='datetime64[ns]', freq=None)
</b>
The result is a list of 3 equally-spaced days that range from the specified start date to the specified end date.
<h3>Example 3: Create Date Range with Specific Frequency</h3>
The following code shows how to create a date range that starts on a specific date and has a frequency of six month start dates:
<b>import pandas as pd
#create date range with six month start dates
pd.date_range(start='1/1/2020', freq='MS', periods=6)
DatetimeIndex(['2020-01-01', '2020-02-01', '2020-03-01', '2020-04-01',
               '2020-05-01', '2020-06-01'],
              dtype='datetime64[ns]', freq='MS')</b>
The result is a list of six dates that are each one month apart. Note that ‘<b>MS</b>‘ stands for ‘Month Start.’ You can find a complete list of date aliases  here .
The following code shows how to create a date range that starts on a specific date and has a yearly frequency:
<b>import pandas as pd
#create date range with six consecutive years
pd.date_range(start='1/1/2020', freq='YS', periods=6)
DatetimeIndex(['2020-01-01', '2021-01-01', '2022-01-01', '2023-01-01',
               '2024-01-01', '2025-01-01'],
              dtype='datetime64[ns]', freq='AS-JAN')</b>
The result is a list of six dates that are each one year apart.
<b>Note</b>: You can find the complete online documentation for the <b>pd.date_range()</b> function  here .
<h2><span class="orange">How to Find Day of the Week in Pandas</span></h2>
You can use the following methods to find the day of the week in pandas:
<b>Method 1: Find Day of Week as Integer</b>
<b>df['date_column'].dt.weekday
</b>
<b>Method 2: Find Day of Week as String Name</b>
<b>df['date_column'].dt.day_name()
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/5/2022', freq='D', periods=10),   'sales': [6, 8, 9, 5, 4, 8, 8, 3, 5, 9]})
#view DataFrame
print(df)
        date  sales
0 2022-01-05      6
1 2022-01-06      8
2 2022-01-07      9
3 2022-01-08      5
4 2022-01-09      4
5 2022-01-10      8
6 2022-01-11      8
7 2022-01-12      3
8 2022-01-13      5
9 2022-01-14      9
</b>
<h2>Example 1: Find Day of Week as Integer</h2>
The following code shows how to add a new column to the DataFrame that shows the day of the week in the <b>date</b> column as an integer:
<b>#add new column that displays day of week as integer
df['day_of_week'] = df['date'].dt.weekday
#view updated DataFrame
print(df)
        date  sales  day_of_week
0 2022-01-05      6            2
1 2022-01-06      8            3
2 2022-01-07      9            4
3 2022-01-08      5            5
4 2022-01-09      4            6
5 2022-01-10      8            0
6 2022-01-11      8            1
7 2022-01-12      3            2
8 2022-01-13      5            3
9 2022-01-14      9            4</b>
The new <b>day_of_week</b> column displays the day of the week where:
<b>0</b>: Monday
<b>1</b>: Tuesday
<b>2</b>: Wednesday
<b>3</b>: Thursday
<b>4</b>: Friday
<b>5</b>: Saturday
<b>6</b>: Sunday
<h2>Example 2: Find Day of Week as String Name</h2>
The following code shows how to add a new column to the DataFrame that shows the day of the week in the <b>date</b> column as an integer:
<b>#add new column that displays day of week as string name
df['day_of_week'] = df['date'].dt.day_name()
#view updated DataFrame
print(df)
        date  sales day_of_week
0 2022-01-05      6   Wednesday
1 2022-01-06      8    Thursday
2 2022-01-07      9      Friday
3 2022-01-08      5    Saturday
4 2022-01-09      4      Sunday
5 2022-01-10      8      Monday
6 2022-01-11      8     Tuesday
7 2022-01-12      3   Wednesday
8 2022-01-13      5    Thursday
9 2022-01-14      9      Friday</b>
The new <b>day_of_week</b> column displays the day of the week as a string name.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Group by Week in Pandas DataFrame 
 How to Group by Month in Pandas DataFrame 
<h2><span class="orange">How to Use describe() Function in Pandas (With Examples)</span></h2>
You can use the <b>describe()</b> function to generate descriptive statistics for a pandas DataFrame.
This function uses the following basic syntax:
<b>df.describe()
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A25511
1A1278
2B15710
3B1496
4B19126
5C2395
6C2599
7C29412
</b>
<h3>Example 1: Describe All Numeric Columns</h3>
By default, the <b>describe()</b> function only generates descriptive statistics for numeric columns in a pandas DataFrame:
<b>#generate descriptive statistics for all numeric columns
df.describe()
points        assists   rebounds
count8.0000008.00000   8.000000
mean20.2500007.75000   8.375000
std6.1586182.54951   2.559994
min12.0000004.00000   5.000000
25%14.7500006.50000   6.000000
50%21.0000008.00000   8.500000
75%25.0000009.00000   10.250000
max29.00000012.00000   12.000000
</b>
Descriptive statistics are shown for the three numeric columns in the DataFrame.
<b>Note:</b> If there are missing values in any columns, pandas will automatically exclude these values when calculating the descriptive statistics.
<h3>Example 2: Describe All Columns</h3>
To calculate descriptive statistics for every column in the DataFrame, we can use the <b>include=’all’</b> argument:
<b>#generate descriptive statistics for all columns
df.describe(include='all')
teampoints    assistsrebounds
count88.000000    8.000008.000000
unique3NaN    NaN        NaN
topBNaN    NaN        NaN
freq3NaN    NaN        NaN
meanNaN20.250000   7.750008.375000
stdNaN6.158618    2.549512.559994
minNaN12.000000   4.000005.000000
25%NaN14.750000   6.500006.000000
50%NaN21.000000   8.000008.500000
75%NaN25.000000   9.0000010.250000
maxNaN29.000000   12.0000012.000000</b>
<h3>Example 3: Describe Specific Columns</h3>
The following code shows how to calculate descriptive statistics for one specific column in the pandas DataFrame:
<b>#calculate descriptive statistics for 'points' column only
df['points'].describe()
count     8.000000
mean     20.250000
std       6.158618
min      12.000000
25%      14.750000
50%      21.000000
75%      25.000000
max      29.000000
Name: points, dtype: float64
</b>
The following code shows how to calculate descriptive statistics for several specific columns:
<b>#calculate descriptive statistics for 'points' and 'assists' columns only
df[['points', 'assists']].describe()
pointsassists
count8.0000008.00000
mean20.2500007.75000
std6.1586182.54951
min12.0000004.00000
25%14.7500006.50000
50%21.0000008.00000
75%25.0000009.00000
max29.00000012.00000
</b>
You can find the complete documentation for the <b>describe()</b> function  here .
<h2><span class="orange">Pandas: How to Find the Difference Between Two Rows</span></h2>
You can use the <b>DataFrame.diff()</b> function to find the difference between two rows in a pandas DataFrame.
This function uses the following syntax:
<b>DataFrame.diff(periods=1, axis=0)</b>
where:
<b>periods:</b> The number of previous rows for calculating the difference.
<b>axis:</b> Find difference over rows (0) or columns (1).
The following examples show how to use this function in practice.
<h3>Example 1: Find Difference Between Each Previous Row</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'period': [1, 2, 3, 4, 5, 6, 7, 8],   'sales': [12, 14, 15, 15, 18, 20, 19, 24],   'returns': [2, 2, 3, 3, 5, 4, 4, 6]})
#view DataFrame
df
periodsalesreturns
01122
12142
23153
34153
45185
56204
67194
78246</b>
The following code shows how to find the difference between every current row in a DataFrame and the previous row:
<b>#add new column to represent sales differences between each row
df['sales_diff'] = df['sales'].diff()
#view DataFrame
df
        periodsalesreturns sales_diff
01122 NaN
12142 2.0
23153 1.0
34153 0.0
45185 3.0
56204 2.0
67194-1.0
78246 5.0
</b>
Note that we can also find the difference between several rows prior. For example, the following code shows how to find the difference between each current row and the row that occurred three rows earlier:
<b>#add new column to represent sales differences between current row and 3 rows earlier
df['sales_diff'] = df['sales'].diff(periods=3)
#view DataFrame
df
        periodsalesreturns sales_diff
01122 NaN
12142 NaN
23153 NaN
34153 3.0
45185 4.0
56204 5.0
67194 4.0
78246 6.0</b>
<h3>Example 2: Find Difference Based on Condition</h3>
We can also filter the DataFrame to show rows where the difference between the current row and the previous row is less than or greater than some value.
For example, the following code returns only the rows where the value in the current row is <b>less than</b> the value in the previous row:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'period': [1, 2, 3, 4, 5, 6, 7, 8],   'sales': [12, 14, 15, 13, 18, 20, 19, 24],   'returns': [2, 2, 3, 3, 5, 4, 4, 6]})
#find difference between each current row and the previous row
df['sales_diff'] = df['sales'].diff()
#filter for rows where difference is less than zero
df = df[df['sales_diff']&lt;0]
#view DataFrame
df
        periodsalesreturns sales_diff
34133 -2.0
67194 -1.0</b>
<h2><span class="orange">Pandas: How to Find the Difference Between Two Columns</span></h2>
To find the difference between any two columns in a pandas DataFrame, you can use the following syntax:
<b>df['difference'] = df['column1'] - df['column2']
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Find Difference Between Two Columns</h3>
Suppose we have the following pandas DataFrame that shows the total sales for two regions (A and B) during eight consecutive sales periods:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'period': [1, 2, 3, 4, 5, 6, 7, 8],   'A_sales': [12, 14, 15, 13, 18, 20, 19, 24],   'B_sales': [14, 19, 20, 22, 24, 20, 17, 23]})
#view DataFrame
df
periodA_salesB_sales
011214
121419
231520
341322
451824
562020
671917
782423</b>
The following code shows how calculate the difference between the sales in region B and region A for each sales period:
<b>#add new column to represent difference between B sales and A sales
df['diff'] = df['B_sales'] - df['A_sales']
#view DataFrame
df
        periodA_salesB_sales diff
011214 2
121419 5
231520 5
341322 9
451824 6
562020 0
671917-2
782423-1
</b>
We could also calculate the absolute difference in sales by using the <b>pandas.Series.abs()</b> function:
<b>#add new column to represent absolute difference between B sales and A sales
df['diff'] = pd.Series.abs(df['B_sales'] - df['A_sales'])
#view DataFrame
df
periodA_salesB_salesdiff
0112142
1214195
2315205
3413229
4518246
5620200
6719172
7824231
</b>
<h3>Example 2: Find Difference Between Columns Based on Condition</h3>
We can also filter the DataFrame to only show rows where the difference between the columns is less than or greater than some value.
For example, the following code returns only the rows where the the sales in region A is <b>greater</b> than the sales in region B:
<b>#add new column to represent difference between B sales and A sales
df['diff'] = df['B_sales'] - df['A_sales']
#display rows where sales in region A is greater than sales in region B
df[df['diff']&lt;0]
        periodA_salesB_salesdiff
671917-2
782423-1
</b>
<h2><span class="orange">Pandas: How to Select Rows that Do Not Start with String</span></h2>
You can use the following basic syntax to select rows that do not start with a specific string in a pandas DataFrame:
<b>df[~df.my_column.str.startswith(('this', 'that'))]
</b>
This particular formula selects all rows in the DataFrame where the column called <b>my_column</b> does not start with the string <b>this</b> or the string <b>that</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Select Rows that Do Not Start with String in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about sales for various stores:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'store': ['Upper East', 'Upper West', 'Lower East', 'West', 'CTR'],   'sales': [150, 224, 250, 198, 177]})
#view DataFrame
print(df)
        store  sales
0  Upper East    150
1  Upper West    224
2  Lower East    250
3        West    198
4         CTR    177     
</b>
We can use the following syntax to select all rows in the DataFrame that do not start with the strings ‘Upper’ or ‘Lower’ in the <b>store</b> column:
<b>#select all rows where store does not start with 'Upper' or 'Lower'
df[~df.store.str.startswith(('Upper', 'Lower'))]
storesales
3West198
4CTR177</b>
Notice that the only rows returned are the ones where the <b>store</b> column does not start with ‘Upper’ or ‘Lower.’
If you’d like, you can also define the tuple of strings outside of the <b>startswith()</b> function:
<b>#define tuple of strings
some_strings = ('Upper', 'Lower')
#select all rows where store does not start with strings in tuple
df[~df.store.str.startswith(some_strings)]
storesales
3West198
4CTR177</b>
This produces the same result as the previous method.
<b>Note</b>: You can find the complete documentation for the <b>startswith </b>function in pandas  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Filter Rows Based on String Length 
 Pandas: How to Check if Column Contains String 
 Pandas: How to Concatenate Strings from Using GroupBy 
<h2><span class="orange">Pandas: How to Drop All Columns Except Specific Ones</span></h2>
You can use the following methods to drop all columns except specific ones from a pandas DataFrame:
<b>Method 1: Use Double Brackets</b>
<b>df = df[['col2', 'col6']]
</b>
<b>Method 2: Use .loc</b>
<b>df = df.loc[:, ['col2', 'col6']]</b>
Both methods drop all columns in the DataFrame except the columns called <b>col2</b> and <b>col6</b>.
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame with six columns
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [4, 3, 3, 2, 5, 4, 3, 8],   'blocks': [1, 0, 0, 3, 2, 2, 1, 5]})
#view DataFrame
print(df)
  team  points  assists  rebounds  steals  blocks
0    A      18        5        11       4       1
1    B      22        7         8       3       0
2    C      19        7        10       3       0
3    D      14        9         6       2       3
4    E      14       12         6       5       2
5    F      11        9         5       4       2
6    G      20        9         9       3       1
7    H      28        4        12       8       5
</b>
<h2>Example 1: Drop All Columns Except Specific Ones Using Double Brackets</h2>
We can use the following syntax to drop all columns in the DataFrame except the ones called <b>points</b> and <b>blocks</b>:
<b>#drop all columns except points and blocks
df = df[['points', 'blocks']]
#view updated DataFrame
print(df)
   points  blocks
0      18       1
1      22       0
2      19       0
3      14       3
4      14       2
5      11       2
6      20       1
7      28       5</b>
Notice that only the <b>points</b> and <b>blocks</b> columns remain.
All other columns have been dropped.
<h2>Example 2: Drop All Columns Except Specific Ones Using .loc</h2>
We can also use the .loc function to drop all columns in the DataFrame except the ones called <b>points</b> and <b>blocks</b>:
<b>#drop all columns except points and blocks
df = df.loc[:, ['points', 'blocks']]
#view updated DataFrame
print(df)
   points  blocks
0      18       1
1      22       0
2      19       0
3      14       3
4      14       2
5      11       2
6      20       1
7      28       5</b>
Notice that only the <b>points</b> and <b>blocks</b> columns remain.
This matches the results from the previous example.
<b>Related:</b>  Pandas loc vs. iloc: What’s the Difference? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop First Row in Pandas DataFrame 
 How to Drop First Column in Pandas DataFrame 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">Pandas: How to Drop All Rows Except Specific Ones</span></h2>
You can use the following methods to drop all rows except specific ones from a pandas DataFrame:
<b>Method 1: Drop All Rows Except Those with Specific Value in Column</b>
<b>#drop all rows except where team column is equal to 'Mavs'
df = df.query("team == 'Mavs'")
</b>
<b>Method 2: Drop All Rows Except Those with One of Several Specific Values in Column</b>
<b>#drop all rows except where team is equal to 'Mavs' or 'Heat'</b>
<b>df = df.query("team == 'Mavs' | team == 'Heat'")</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Mavs', 'Heat', 'Heat', 'Cavs', 'Cavs'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
   team  points  assists
0  Mavs      18        5
1  Mavs      22        7
2  Heat      19        7
3  Heat      14        9
4  Cavs      14       12
5  Cavs      11        9</b>
<h2>Example 1: Drop All Rows Except Those with Specific Value in Column</h2>
We can use the following syntax to drop all rows except those with a value of ‘Mavs’ in the <b>team</b> column:
<b>#drop all rows except where team column is equal to 'Mavs'
df = df.query("team == 'Mavs'")
#view updated DataFrame
print(df)
   team  points  assists
0  Mavs      18        5
1  Mavs      22        7</b>
Notice that every row has been dropped except the rows that have a value of ‘Mavs’ in the <b>team</b> column.
<h2>Example 2: Drop All Rows Except Those with One of Several Specific Values in Column</h2>
We can use the following syntax to drop all rows except those with a value of ‘Mavs’ or ‘Heat’ in the <b>team</b> column:
<b>#drop all rows except where team column is equal to 'Mavs'
df = df.query("team == 'Mavs' | team == 'Heat'")
#view updated DataFrame
print(df)
   team  points  assists
0  Mavs      18        5
1  Mavs      22        7
2  Heat      19        7
3  Heat      14        9</b>
Notice that every row has been dropped except the rows that have a value of ‘Mavs’ or ‘Heat’ in the <b>team</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop First Row in Pandas DataFrame 
 How to Drop First Column in Pandas DataFrame 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">How to Drop Columns by Index in Pandas</span></h2>
You can use the following syntax to drop one column from a pandas DataFrame by index number:
<b>#drop first column from DataFrame
df.drop(df.columns[0], axis=1, inplace=True)
</b>
And you can use the following syntax to drop multiple columns from a pandas DataFrame by index numbers:
<b>#drop first, second, and fourth column from DataFrame
cols = [0, 1, 3]
df.drop(df.columns[cols], axis=1, inplace=True)</b>
If your DataFrame has duplicate column names, you can use the following syntax to drop a column by index number:
<b>#define list of columns
cols = [x for x in range(df.shape[1])]
#drop second column
cols.remove(1)
#view resulting DataFrame
df.iloc[:, cols]</b>
The following examples show how to drop columns by index in practice.
<h3>Example 1: Drop One Column by Index</h3>
The following code shows how to drop the first column in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#drop first column from DataFrame
df.drop(df.columns[0], axis=1, inplace=True)
#view resulting dataFrame
df
        firstlast points
0DirkNowitzki 26
1KobeBryant 31
2TimDuncan 22
3LebronJames 29
</b>
<h3>Example 2: Drop Multiple Columns by Index</h3>
The following code shows how to drop multiple columns in a pandas DataFrame by index:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#drop first, second and fourth columns from DataFrame
cols = [0, 1, 3] 
df.drop(df.columns[cols], axis=1, inplace=True)
#view resulting dataFrame
df
        last
0Nowitzki
1Bryant
2Duncan
3James
</b>
<h3>Example 3: Drop One Column by Index with Duplicates</h3>
The following code shows how to drop a column by index number in a pandas DataFrame when duplicate column names exist:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]},   columns=['team', 'last', 'last', 'points'])
#define list of columns range
cols = [x for x in range(df.shape[1])]
#remove second column in DataFrame
cols.remove(1)
#view resulting DataFrame
df.iloc[:, cols]
teamlast points
0MavsNowitzki 26
1LakersBryant 31
2SpursDuncan 22
3CavsJames 29
</b>
<h2><span class="orange">Pandas: How to Drop Column if it Exists</span></h2>
You can use the following basic syntax to drop one or more columns in a pandas DataFrame if they exist:
<b>df = df.drop(['column1', 'column2'], axis=1, errors='ignore')
</b>
<b>Note:</b> If you don’t use the argument <b>errors=’ignore’</b> then you’ll receive an error if you attempt to drop a column that doesn’t exist.
The following example shows how to use this syntax in practice.
<h2>Example: Drop Column if it Exists in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9],   'minutes': [10.1, 12.0, 9.0, 8.0, 8.4, 7.5],   'all_star': [True, False, False, True, True, True]})
#view DataFrame
print(df)
  team  points  assists  minutes  all_star
0    A      18        5     10.1      True
1    B      22        7     12.0     False
2    C      19        7      9.0     False
3    D      14        9      8.0      True
4    E      14       12      8.4      True
5    F      11        9      7.5      True
</b>
Now suppose we attempt to drop the columns with the names <b>minutes_played</b> and <b>points</b>:
<b>#drop minutes_played and points columns
df = df.drop(['minutes_played', 'points'], axis=1)
KeyError: "['minutes_played', 'points'] not found in axis"
</b>
We receive an error because the column <b>minutes_played</b> does not exist as a column name in the DataFrame.
Instead, we need to use the <b>drop()</b> function with the <b>errors=’ignore’</b> argument:
<b>#drop minutes_played and points columns
df = df.drop(['minutes_played', 'points'], axis=1, errors='ignore')
#view updated DataFrame
print(df)
  team  assists  minutes  all_star
0    A        5     10.1      True
1    B        7     12.0     False
2    C        7      9.0     False
3    D        9      8.0      True
4    E       12      8.4      True
5    F        9      7.5      True
</b>
Notice that the <b>points</b> column has been dropped from the DataFrame.
Also notice that we don’t receive any error even though we attempted to drop a column called <b>minutes_played</b>, which does not exist.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Drop Unnamed Columns 
 Pandas: How to Drop All Columns Except Specific Ones 
 Pandas: How to Drop All Rows Except Specific Ones 
<h2><span class="orange">Pandas: How to Drop Columns Not in List</span></h2>
You can use the following basic syntax to drop columns from a pandas DataFrame that are not in a specific list:
<b>#define columns to keep
keep_cols = ['col1', 'col2', 'col3']
#create new dataframe by dropping columns not in list
new_df = df[df.columns.intersection(keep_cols)]
</b>
This particular example will drop any columns from the DataFrame that are not equal to <b>col1</b>, <b>col2</b>, or <b>col3</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Drop Columns Not in List in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [4, 4, 10, 12, 8, 5, 5, 2]})
#view DataFrame
print(df)
  team  points  assists  rebounds  steals
0    A      18        5        11       4
1    B      22        7         8       4
2    C      19        7        10      10
3    D      14        9         6      12
4    E      14       12         6       8
5    F      11        9         5       5
6    G      20        9         9       5
7    H      28        4        12       2</b>
Now suppose that we would like to create a new DataFrame that drops all columns that are not in the following list of columns: <b>team</b>, <b>points</b>, and <b>steals</b>.
We can use the following syntax to do so:
<b>#define columns to keep
keep_cols = ['team', 'points', 'steals']
#create new dataframe by dropping columns not in list
new_df = df[df.columns.intersection(keep_cols)]
#view new dataframe
print(new_df)
  team  points  steals
0    A      18       4
1    B      22       4
2    C      19      10
3    D      14      12
4    E      14       8
5    F      11       5
6    G      20       5
7    H      28       2
</b>
Notice that each of the columns from the original DataFrame that are not in the <b>keep_cols</b> list have been dropped from the new DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop First Row in Pandas 
 How to Drop First Column in Pandas 
 How to Drop Duplicate Columns in Pandas 
 How to Drop All Columns Except Specific Ones in Pandas 
<h2><span class="orange">How to Drop Duplicate Columns in Pandas (With Examples)</span></h2>
You can use the following basic syntax to drop duplicate columns in pandas:
<b>df.T.drop_duplicates().T
</b>
The following examples show how to use this syntax in practice.
<h3>Example: Drop Duplicate Columns in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame with duplicate columns
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [25, 12, 15, 14, 19, 23, 25, 29],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
df.columns = ['team', 'points', 'points', 'rebounds']
#view DataFrame
df
teampointspointsrebounds
0A252511
1A12128
2A151510
3A14146
4B19196
5B23235
6B25259
7B292912
</b>
We can use the following code to remove the duplicate ‘points’ column:
<b>#remove duplicate columns
df.T.drop_duplicates().T
        teampointsrebounds
0A2511
1A128
2A1510
3A146
4B196
5B235
6B259
7B2912
</b>
Notice that the ‘points’ column has been removed while all other columns remained in the DataFrame.
It’s also worth noting that this code will remove duplicate columns even if the columns have different names, yet contain identical values.
For example, suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame with duplicate columns
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'points2': [25, 12, 15, 14, 19, 23, 25, 29],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointspoints2rebounds
0A252511
1A12128
2A151510
3A14146
4B19196
5B23235
6B25259
7B292912
</b>
Notice that the ‘points’ and ‘points2’ columns contain identical values.
We can use the following code to remove the duplicate ‘points2’ column:
<b>#remove duplicate columns
df.T.drop_duplicates().T
        teampointsrebounds
0A2511
1A128
2A1510
3A146
4B196
5B235
6B259
7B2912</b>
<h2><span class="orange">Pandas: How to Drop Duplicates Across Multiple Columns</span></h2>
You can use the following methods to drop duplicate rows across multiple columns in a pandas DataFrame:
<b>Method 1: Drop Duplicates Across All Columns</b>
<b>df.drop_duplicates()
</b>
<b>Method 2: Drop Duplicates Across Specific Columns</b>
<b>df.drop_duplicates(['column1', 'column3'])</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'region': ['East', 'East', 'East', 'West', 'West', 'West'],   'store': [1, 1, 2, 1, 2, 2],   'sales': [5, 5, 7, 9, 12, 8]})
#view DataFrame
print(df)
  region  store  sales
0   East      1      5
1   East      1      5
2   East      2      7
3   West      1      9
4   West      2     12
5   West      2      8</b>
<h3>Example 1: Drop Duplicates Across All Columns</h3>
The following code shows how to drop rows that have duplicate values across all columns:
<b>#drop rows that have duplicate values across all columns
df.drop_duplicates()
regionstoresales
0East15
2East27
3West19
4West212
5West28
</b>
The row in index position 1 had the same values across all columns as the row in index position 0, so it was dropped from the DataFrame.
By default, pandas keeps the first duplicate row. However, you can use the <b>keep</b> argument to specify to keep the last duplicate row instead:
<b>#drop rows that have duplicate values across all columns (keep last duplicate)
df.drop_duplicates(keep='last')
regionstoresales
1East15
2East27
3West19
4West212
5West28</b>
<h3>Example 2: Drop Duplicates Across Specific Columns</h3>
You can use the following code to drop rows that have duplicate values across only the <b>region</b> and <b>store</b> columns:
<b>#drop rows that have duplicate values across region and store columns
df.drop_duplicates(['region', 'store'])
regionstoresales
0East15
2East27
3West19
4West212
</b>
A total of two rows were dropped from the DataFrame because they had duplicate values in the <b>region</b> and <b>store</b> columns.
<b>Note</b>: You can find the complete documentation for the <b>drop_duplicates()</b> function  here .
<h2><span class="orange">How to Drop Duplicate Rows in a Pandas DataFrame</span></h2>
The easiest way to drop duplicate rows in a pandas DataFrame is by using the  drop_duplicates()  function, which uses the following syntax:
<b>df.drop_duplicates(subset=None, keep=’first’, inplace=False)</b>
where:
<b>subset:</b> Which columns to consider for identifying duplicates. Default is all columns.
<b>keep:</b> Indicates which duplicates (if any) to keep. 
<b>first:</b> Delete all duplicate rows except first.
<b>last:</b> Delete all duplicate rows except last.
<b>False</b>: Delete all duplicates.
<b>inplace:</b> Indicates whether to drop duplicates in place or return a copy of the DataFrame.
This tutorial provides several examples of how to use this function in practice on the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['a', 'b', 'b', 'c', 'c', 'd'],   'points': [3, 7, 7, 8, 8, 9],   'assists': [8, 6, 7, 9, 9, 3]})
#display DataFrame
print(df)
  team  points  assists
0    a       3        8
1    b       7        6
2    b       7        7
3    c       8        9
4    c       8        9
5    d       9        3</b>
<h3>Example 1: Remove Duplicates Across All Columns</h3>
The following code shows how to remove rows that have duplicate values across <em>all </em>columns:
<b>df.drop_duplicates()
        teampointsassists
0a38
1b76
2b77
3c89
5d93
</b>
By default, the drop_duplicates() function deletes all duplicates except the first.
However, we could use the <b>keep=False</b> argument to delete all duplicates entirely:
<b>df.drop_duplicates(keep=False)
teampointsassists
0a38
1b76
2b77
5d93
</b>
<h3>
<b>Example 2: Remove Duplicates Across Specific Columns</b>
</h3>
The following code shows how to remove rows that have duplicate values across just the columns titled <em>team</em> and <em>points</em>:
<b>df.drop_duplicates(subset=['team', 'points'])
        teampointsassists
0a38
1b76
3c89
5d93
</b>
<h2><span class="orange">How to Drop First Column in Pandas DataFrame (3 Methods)</span></h2>
You can use one of the following three methods to drop the first column in a pandas DataFrame:
<b>Method 1: Use drop</b>
<b>df.drop(columns=df.columns[0], axis=1, inplace=True)
</b>
<b>Method 2: Use iloc</b>
<b>df = df.iloc[: , 1:]</b>
<b>Method 3: Use del</b>
<b>del df[df.columns[0]]</b>
Each method produces the same result.
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition assists rebounds
0AG 5 11
1AG 7 8
2AF 7 10
3AF 9 6
4BG 12 6
5BG 9 5
6BF 9 9
7BF 4 12
</b>
<h3>Method 1: Use drop</h3>
The following code shows how to use the <b>drop()</b> function to drop the first column of the pandas DataFrame:
<b>#drop first column of DataFrame
df.drop(columns=df.columns[0], axis=1, inplace=True)
#view updated DataFrame
df
position assists rebounds
0G 5 11
1G 7 8
2F 7 10
3F 9 6
4G 12 6
5G 9 5
6F 9 9
7F 4 12
</b>
Notice that the first column called ‘team’ has been removed from the DataFrame.
Also note that we must use <b>inplace=True</b> for the column to be removed in the original DataFrame.
<h3>Method 2: Use iloc</h3>
The following code shows how to use the <b>iloc</b> function to drop the first column of the pandas DataFrame:
<b>#drop first column of DataFrame
df = df.iloc[: , 1:]
#view updated DataFrame
df
position assists rebounds
0G 5 11
1G 7 8
2F 7 10
3F 9 6
4G 12 6
5G 9 5
6F 9 9
7F 4 12
</b>
Notice that the first column called ‘team’ has been removed from the DataFrame.
<h3>Method 3: Use del</h3>
The following code shows how to use the <b>del</b> function to drop the first column of the pandas DataFrame:
<b>#drop first column of DataFrame
del df[df.columns[0]]
#view updated DataFrame
df
position assists rebounds
0G 5 11
1G 7 8
2F 7 10
3F 9 6
4G 12 6
5G 9 5
6F 9 9
7F 4 12
</b>
Notice that the first column called ‘team’ has been removed from the DataFrame.
<h2><span class="orange">How to Drop First Row in Pandas DataFrame (2 Methods)</span></h2>
You can use one of the following methods to drop the first row in a pandas DataFrame:
<b>Method 1: Use drop</b>
<b>df.drop(index=df.index[0], axis=0, inplace=True)
</b>
<b>Method 2: Use iloc</b>
<b>df = df.iloc[1: , :]</b>
Each method produces the same result.
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposition assists rebounds
0AG 5 11
1AG 7 8
2AF 7 10
3AF 9 6
4BG 12 6
5BG 9 5
6BF 9 9
7BF 4 12
</b>
<h3>Method 1: Use drop</h3>
The following code shows how to use the <b>drop()</b> function to drop the first row of the pandas DataFrame:
<b>#drop first row of DataFrame
df.drop(index=df.index[0], axis=0, inplace=True) 
#view updated DataFrame
df
teamposition assists rebounds
1AG 7 8
2AF 7 10
3AF 9 6
4BG 12 6
5BG 9 5
6BF 9 9
7BF 4 12
</b>
Notice that the first row has been removed from the DataFrame.
Also note that we must use <b>inplace=True</b> for the row to be removed in the original DataFrame.
<h3>Method 2: Use iloc</h3>
The following code shows how to use the <b>iloc</b> function to drop the first row of the pandas DataFrame:
<b>#drop first row of DataFrame
df = df.iloc[1: , :]
#view updated DataFrame
df
teamposition assists rebounds
1AG 7 8
2AF 7 10
3AF 9 6
4BG 12 6
5BG 9 5
6BF 9 9
7BF 4 12
</b>
Notice that the first row has been removed from the DataFrame.
<h2><span class="orange">How to Drop Rows by Index in Pandas (With Examples)</span></h2>
You can use the following syntax to drop one row from a pandas DataFrame by index number:
<b>#drop first row from DataFrame
df = df.drop(index=0)
</b>
And you can use the following syntax to drop multiple rows from a pandas DataFrame by index numbers:
<b>#drop first, second, and fourth row from DataFrame
df = df.drop(index=[0, 1, 3])</b>
If your DataFrame has strings as index values, you can simply pass the names as strings to drop:
<b>df = df.drop(index=['first', 'second', 'third'])
</b>
The following examples show how to drop rows by index in practice.
<h3>Example 1: Drop One Row by Index</h3>
The following code shows how to drop the second row in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#view DataFrame
df
        teamfirstlast points
0MavsDirkNowitzki 26
1LakersKobeBryant 31
2SpursTimDuncan 22
3CavsLebronJames 29
#drop second row from DataFrame
df = df.drop(index=1) 
#view resulting dataFrame
df
        teamfirstlast points
0MavsDirkNowitzki 26
2SpursTimDuncan 22
3CavsLebronJames 29
</b>
<h3>Example 2: Drop Multiple Rows by Index</h3>
The following code shows how to drop multiple rows in a pandas DataFrame by index:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'first': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]})
#view DataFrame
df
        teamfirstlast points
0MavsDirkNowitzki 26
1LakersKobeBryant 31
2SpursTimDuncan 22
3CavsLebronJames 29
#drop first, second, and fourth row from DataFrame
df = df.drop(index=[0, 1, 3]) 
#view resulting dataFrame
df
teamfirstlastpoints
2SpursTimDuncan22</b>
<h3>Example 3: Drop Rows When Index is a String</h3>
The following code shows how to drop rows from a pandas DataFrame by index when the index is a string instead of a number:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'last': ['Nowitzki', 'Bryant', 'Duncan', 'James'],   'points': [26, 31, 22, 29]},   index=['A', 'B', 'C', 'D'])
#view DataFrame
df
        teamfirstlast points
AMavsDirkNowitzki 26
BLakersKobeBryant 31
CSpursTimDuncan 22
DCavsLebronJames 29
#remove rows with index values 'A' and 'C'
df = df.drop(index=['A', 'C'])
#view resulting DataFrame
df
teamfirstlastpoints
BLakersKobeBryant31
DCavsLebronJames29</b>
<h2><span class="orange">Pandas: Drop Rows Based on Multiple Conditions</span></h2>
You can use the following methods to drop rows based on multiple conditions in a pandas DataFrame:
<b>Method 1: Drop Rows that Meet One of Several Conditions</b>
<b>df = df.loc[~((df['col1'] == 'A') | (df['col2'] > 6))]
</b>
This particular example will drop any rows where the value in col1 is equal to A <b>or</b> the value in col2 is greater than 6.
<b>Method 2: Drop Rows that Meet Several Conditions</b>
<b>df = df.loc[~((df['col1'] == 'A') & (df['col2'] > 6))] 
</b>
This particular example will drop any rows where the value in col1 is equal to A <b>and</b> the value in col2 is greater than 6.
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'pos': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'assists': [5, 7, 7, 9, 12, 9, 3, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposassistsrebounds
0AG511
1AG78
2AF710
3AF96
4BG126
5BG95
6BF39
7BF412</b>
<h2>Example 1: Drop Rows that Meet One of Several Conditions</h2>
The following code shows how to drop rows in the DataFrame where the value in the <b>team</b> column is equal to A <b>or</b> the value in the <b>assists</b> column is greater than 6:
<b>#drop rows where value in team column == 'A' or value in assists column > 6
df = df.loc[~((df['team'] == 'A') | (df['assists'] > 6))]
#view updated DataFrame
print(df)
  team pos  assists  rebounds
6    B   F        3         9
7    B   F        4        12</b>
Notice that any rows where the team column was equal to A or the assists column was greater than 6 have been dropped.
For this particular DataFrame, six of the rows were dropped.
<b>Note</b>: The <b>|</b> symbol represents “OR” logic in pandas.
<h2>Example 2: Drop Rows that Meet Several Conditions</h2>
The following code shows how to drop rows in the DataFrame where the value in the <b>team</b> column is equal to A <b>and </b>the value in the <b>assists</b> column is greater than 6:
<b>#drop rows where value in team column == 'A' and value in assists column > 6
df = df.loc[~((df['team'] == 'A') & (df['assists'] > 6))]
#view updated DataFrame
print(df)
  team pos  assists  rebounds
0    A   G        5        11
4    B   G       12         6
5    B   G        9         5
6    B   F        3         9
7    B   F        4        12
</b>
Notice that any rows where the team column was equal to A and the assists column was greater than 6 have been dropped.
For this particular DataFrame, three of the rows were dropped.
<b>Note</b>: Th <b>&</b> symbol represents “AND” logic in pandas.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Drop Rows that Contain a Specific Value in Pandas 
 How to Drop Rows that Contain a Specific String in Pandas 
 How to Drop Rows by Index in Pandas 
<h2><span class="orange">Pandas: How to Drop Rows that Contain a Specific String</span></h2>
You can use the following syntax to drop rows that contain a certain string in a pandas DataFrame:
<b>df[df["col"].str.contains("this string")==False]
</b>
This tutorial explains several examples of how to use this syntax in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C'],   'conference': ['East', 'East', 'East', 'West', 'West', 'East'],   'points': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
        teamconference   points
0AEast         11
1AEast     8
2AEast     10
3BWest         6
4BWest         6
5CEast         5
</b>
<h3>Example 1: Drop Rows that Contain a Specific String</h3>
The following code shows how to drop all rows in the DataFrame that contain ‘A’ in the team column:
<b>df[df["team"].str.contains("A")==False]
        teamconference  points
3BWest    6
4BWest    6
5CEast    5
</b>
<h3>Example 2: Drop Rows that Contain a String in a List</h3>
The following code shows how to drop all rows in the DataFrame that contain ‘A’ or ‘B’ in the team column:
<b>df[df["team"].str.contains("A|B")==False]
teamconference   points
5CEast     5
</b>
<h3>Example 3: Drop Rows that Contain a Partial String</h3>
In the previous examples, we dropped rows based on rows that exactly matched one or more strings. 
However, if we’d like to drop rows that contain a partial string then we can use the following syntax:
<b>#identify partial string to look for
discard = ["Wes"]
#drop rows that contain the partial string "Wes" in the conference column
df[~df.conference.str.contains('|'.join(discard))]
teamconferencepoints
0AEast11
1AEast8
2AEast10
5CEast5</b>
You can find more pandas tutorials on  this page .
<h2><span class="orange">How to Drop Rows in Pandas DataFrame Based on Condition</span></h2>
We can use the following syntax to drop rows in a pandas DataFrame based on condition:
<b>Method 1: Drop Rows Based on One Condition</b>
<b>df = df[df.col1 > 8]
</b>
<b>Method 2: Drop Rows Based on Multiple Conditions</b>
<b>df = df[(df.col1 > 8) & (df.col2 != 'A')]
</b>
<b>Note</b>: We can also use the <b>drop()</b> function to drop rows from a DataFrame, but this function has been shown to be much slower than just assigning the DataFrame to a filtered version of itself.
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'pos': ['G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teamposassistsrebounds
0AG511
1AG78
2AF710
3AF96
4BG126
5BG95
6BF99
7BF412</b>
<h3>Method 1: Drop Rows Based on One Condition</h3>
The following code shows how to drop rows in the DataFrame based on one condition:
<b>#drop rows where value in 'assists' column is less than or equal to 8
df = df[df.assists > 8] 
#view updated DataFrame
df
teamposassistsrebounds
3AF96
4BG126
5BG95
6BF99
</b>
Any row that had a value less than or equal to 8 in the ‘assists’ column was dropped from the DataFrame.
<h3>Method 2: Drop Rows Based on Multiple Conditions</h3>
The following code shows how to drop rows in the DataFrame based on multiple conditions:
<b>#only keep rows where 'assists' is greater than 8 and rebounds is greater than 5
df = df[(df.assists > 8) & (df.rebounds > 5)]
#view updated DataFrame
df
teamposassistsrebounds
3AF96
4BG126
5BG95
6BF99
</b>
The only rows that we kept in the DataFrame were the ones where the assists value was greater than 8 <em>and</em> the rebounds value was greater than 5.
Note that we can also use the <b>|</b> operator to apply an “or” filter:
<b>#only keep rows where 'assists' is greater than 8 or rebounds is greater than 10
df = df[(df.assists > 8) | (df.rebounds > 10)]
#view updated DataFrame
df
teamposassistsrebounds
0AG511
3AF96
4BG126
5BG95
6BF99
7BF412
</b>
The only rows that we kept in the DataFrame were the ones where the assists value was greater than 8 <i>or </i>the rebounds value was greater than 10.
Any rows that didn’t meet one of these conditions was dropped.
<h2><span class="orange">Pandas: How to Drop Rows that Contain a Specific Value</span></h2>
You can use the following syntax to drop rows in a pandas DataFrame that contain a specific value in a certain column:
<b>#drop rows that contain specific 'value' in 'column_name'
df = df[df.column_name != value]
</b>
You can use the following syntax to drop rows in a pandas DataFrame that contain any value in a certain list:
<b>#define values
values = [value1, value2, value3, ...]
#drop rows that contain any value in the list
df = df[df.column_name.isin(values) == False]</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Drop Rows that Contain a Specific Value</h3>
The following code shows how to drop any rows that contain a specific value in one column:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'name': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'rebounds': [11, 7, 14, 7],   'points': [26, 31, 22, 29]})
#view DataFrame
df
        teamnamerebounds points
0MavsDirk11 26
1LakersKobe7 31
2SpursTim14 22
3CavsLebron7 29
#drop any rows that have 7 in the rebounds column
df = df[df.rebounds != 7]
#view resulting DataFrame
df
        teamnamerebounds points
0MavsDirk11 26
2SpursTim14 22</b>
<h3>Example 2: Drop Rows that Contain Values in a List</h3>
The following code shows how to drop any rows in the DataFrame that contain any value in a list:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'name': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'rebounds': [11, 7, 14, 7],   'points': [26, 31, 22, 29]})
#view DataFrame
df
        teamnamerebounds points
0MavsDirk11 26
1LakersKobe7 31
2SpursTim14 22
3CavsLebron7 29
#define list of values
values = [7, 11]
#drop any rows that have 7 or 11 in the rebounds column
df = df[df.rebounds.isin(values) == False]
#view resulting DataFrame
df
        teamnamerebounds points
2SpursTim14 22</b>
<h3>Example 3: Drop Rows that Contain Specific Values in Multiple Columns</h3>
The following code shows how to drop any rows in the DataFrame that contain a specific value in one of several columns:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Lakers', 'Spurs', 'Cavs'],   'name': ['Dirk', 'Kobe', 'Tim', 'Lebron'],   'rebounds': [11, 7, 14, 7],   'points': [26, 31, 22, 29]})
#view DataFrame
df
        teamnamerebounds points
0MavsDirk11 26
1LakersKobe7 31
2SpursTim14 22
3CavsLebron7 29
#drop any rows that have 11 in the rebounds column or 31 in the points column
df = df[(df.rebounds != 11) & (df.points != 31)]
#view resulting DataFrame
df
teamnamereboundspoints
2SpursTim1422
3CavsLebron729
</b>
<h2><span class="orange">How to Exclude Columns in Pandas (With Examples)</span></h2>
You can use the following syntax to exclude columns in a pandas DataFrame:
<b>#exclude column1
df.loc[:, df.columns!='column1']
#exclude column1, column2, ...
df.loc[:, ~df.columns.isin(['column1', 'column2', ...])]
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Exclude One Column</h3>
The following code shows how to select all columns except one in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame 
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'blocks': [2, 3, 3, 5, 3, 2, 1, 2]})
#view DataFrame
df
pointsassistsrebounds blocks
025511 2
11278 3
215710 3
31496 5
419126 3
52395 2
62599 1
729412 2
#select all columns except 'rebounds'
df.loc[:, df.columns!='rebounds']
        pointsassistsblocks
02552
11273
21573
31495
419123
52392
62591
72942
</b>
<h3>Example 2: Exclude Multiple Columns</h3>
The following code shows how to select all columns except specific ones in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame 
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'blocks': [2, 3, 3, 5, 3, 2, 1, 2]})
#view DataFrame
df
pointsassistsrebounds blocks
025511 2
11278 3
215710 3
31496 5
419126 3
52395 2
62599 1
729412 2
#select all columns except 'rebounds' and 'assists'
df.loc[:, ~df.columns.isin(['rebounds', 'assists'])]
pointsblocks
0252
1123
2153
3145
4193
5232
6251
7292
</b>
Using this syntax, you can exclude any number of columns that you’d like by name.
<h2><span class="orange">How to Use the Pandas explode() Function (With Examples)</span></h2>
You can use the pandas  explode()  function to transform each element in a list to a row in a DataFrame.
This function uses the following basic syntax:
<b>df.explode('variable_to_explode')
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Use explode() Function with Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': [['A', 'B', 'C'], ['D', 'E', 'F'], ['G', 'H', 'I']],   'position':['Guard', 'Forward', 'Center'],   'points': [7, 14, 19]})
#view DataFrame
df
team        position  points
0[A, B, C]Guard  7
1[D, E, F]Forward  14
2[G, H, I]Center  19
</b>
Notice that the <b>team</b> column contains lists of team names.
We can use the <b>explode()</b> function to explode each element in each list into a row:
<b>#explode team column
df.explode('team')
        teamposition  points
0AGuard  7
0BGuard  7
0CGuard  7
1DForward  14
1EForward  14
1FForward  14
2GCenter  19
2HCenter  19
2ICenter  19</b>
Notice that the <b>team</b> column no longer contains lists. We “exploded” each element of each list into a row.
Also notice that some rows now have the same index value.
We can use the <b>reset_index()</b> function to reset the index when exploding the team column:
<b>#explode team column and reset index of resulting dataFrame
df.explode('team').reset_index(drop=True)
teamposition  points
0AGuard  7
1BGuard  7
2CGuard  7
3DForward  14
4EForward  14
5FForward  14
6GCenter  19
7HCenter  19
8ICenter  19
</b>
Notice that each row now has a unique index value.
<h2><span class="orange">How to Extract Month from Date in Pandas (With Examples)</span></h2>
You can use the following basic syntax to extract the month from a date in pandas:
<b>df['month'] = pd.DatetimeIndex(df['date_column']).month
</b>
The following example shows how to use this function in practice.
<h3>Example: Extract Month from Date in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'sales_date': ['2020-01-18', '2020-02-20', '2020-03-21'],   'total_sales': [675, 500, 575]})
#view DataFrame
print(df)
   sales_date  total_sales
0  2020-01-18          675
1  2020-02-20          500
2  2020-03-21          575
</b>
We can use the following syntax to create a new column that contains the <b>month</b> of the ‘sales_date’ column:
<b>#extract month as new column
df['month'] = pd.DatetimeIndex(df['sales_date']).month
#view updated DataFrame
print(df)
sales_datetotal_salesmonth
02020-01-18675        1
12020-02-20500        2
22020-03-21575        3</b>
We can also use the following syntax to create a new column that contains the <b>year</b> of the ‘sales_date’ column:
<b>#extract year as new column
df['year'] = pd.DatetimeIndex(df['sales_date']).year
#view updated DataFrame
print(df)
        sales_datetotal_salesmonthyear
02020-01-18675        12020
12020-02-20500        22020
22020-03-21575        32020
</b>
Note that if there are any NaN values in the DataFrame, this function will automatically produce NaN values for the corresponding values in the new month and year columns.
<b>Related:</b>  How to Sort a Pandas DataFrame by Date 
<h2><span class="orange">Pandas: How to Use factorize() to Encode Strings as Numbers</span></h2>
The pandas  factorize()  function can be used to encode strings as numeric values.
You can use the following methods to apply the <b>factorize()</b> function to columns in a pandas DataFrame:
<b>Method 1: Factorize One Column</b>
<b>df['col1'] = pd.factorize(df['col'])[0]
</b>
<b>Method 2: Factorize Specific Columns</b>
<b>df[['col1', 'col3']] = df[['col1', 'col3']].apply(lambda x: pd.factorize(x)[0])
</b>
<b>Method 3: Factorize All Columns</b>
<b>df = df.apply(lambda x: pd.factorize(x)[0])</b>
The following example shows how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'conf': ['West', 'West', 'East', 'East'],   'team': ['A', 'B', 'C', 'D'],   'position': ['Guard', 'Forward', 'Guard', 'Center'] })
#view DataFrame
df
   conf team position
0  West    A    Guard
1  West    B  Forward
2  East    C    Guard
3  East    D   Center
</b>
<h3>Example 1: Factorize One Column</h3>
The following code shows how to factorize one column in the DataFrame:
<b>#factorize the conf column only
df['conf'] = pd.factorize(df['conf'])[0]
#view updated DataFrame
df
confteamposition
00AGuard
10BForward
21CGuard
31DCenter</b>
Notice that only the ‘conf’ column has been factorized.
Every value that used to be ‘West’ is now 0 and every value that used to be ‘East’ is now 1.
<h3>Example 2: Factorize Specific Columns</h3>
The following code shows how to factorize specific columns in the DataFrame:
<b>#factorize conf and team columns only
df[['conf', 'team']] = df[['conf', 'team']].apply(lambda x: pd.factorize(x)[0])
#view updated DataFrame
df
        confteamposition
000Guard
101Forward
212Guard
313Center
</b>
Notice that the ‘conf’ and ‘team’ columns have both been factorized.
<h3>Example 3: Factorize All Columns</h3>
The following code shows how to factorize all columns in the DataFrame:
<b>#factorize all columns
df = df.apply(lambda x: pd.factorize(x)[0])
#view updated DataFrame
df
     confteamposition
0000
1011
2120
3132
</b>
Notice that all of the columns have been factorized.
<h2><span class="orange">How to Adjust the Figure Size of a Pandas Plot</span></h2>
You can use the <b>figsize</b> parameter to quickly adjust the figure size of a plot in pandas:
<b>df.plot.scatter(x='x', y='y', figsize=(8,4))
</b>
The first value in the <b>figsize</b> parameter specifies the <b>width</b> and the second value specifies the <b>height</b> of the plot.
The following examples show how to use this parameter in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DatFrame
df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],   'y': [5, 7, 7, 9, 10, 14, 13, 15, 19, 16]})
#view head of DataFrame
df.head()
        xy
015
127
237
349
4510
</b>
<h2>Example 1: Create Plot with Default Size</h2>
The following code shows how to create a scatter plot in pandas using the default plot size:
<b>#create scatter plot with default size
df.plot.scatter(x='x', y='y')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/size1.jpg"456">
<h2>Example 2: Create Horizontal Plot</h2>
The following code shows how to create a scatter plot in pandas in which the width is twice as long as the height:
<b>#create scatter plot with longer width than height
df.plot.scatter(x='x', y='y', figsize=(8,4))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/size2.jpg"561">
Notice that the plot is much wider than it is tall.
<h2>Example 3: Create Vertical Plot</h2>
The following code shows how to create a scatter plot in pandas in which the height is twice as long as the width:
<b>#create scatter plot with longer height than width 
df.plot.scatter(x='x', y='y', figsize=(4,8))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/size3.jpg"289">
Notice that the plot is much taller than it is wide.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Add Titles to Plots 
 Pandas: How to Create Plot Legends 
 Pandas: How to Create Bar Plot from GroupBy 
<h2><span class="orange">Pandas: How to Use fillna() with Specific Columns</span></h2>
You can use the following methods with <b>fillna()</b> to replace NaN values in specific columns of a pandas DataFrame:
<b>Method 1: Use fillna() with One Specific Column</b>
<b>df['col1'] = df['col1'].fillna(0)
</b>
<b>Method 2: Use fillna() with Several Specific Columns</b>
<b>df[['col1', 'col2']] = df[['col1', 'col2']].fillna(0) </b>
This tutorial explains how to use this function with the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'rating': [np.nan, 85, np.nan, 88, 94, 90, 76, 75, 87, 86],   'points': [25, np.nan, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, np.nan, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
        ratingpointsassistsrebounds
0NaN25.05.011
185.0NaN7.08
2NaN14.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 1: Use fillna() with One Specific Column</h3>
The following code shows how to use<b> fillna()</b> to replace the NaN values with zeros in just the “rating” column:
<b>#replace NaNs with zeros in 'rating' column
df['rating'] = df['rating'].fillna(0)
#view DataFrame 
df
ratingpointsassistsrebounds
00.025.05.011
185.0NaN7.08
20.014.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
Notice that the NaN values have been replaced only in the “rating” column and every other column remained untouched.
<h3>
<b>Example 2: Use fillna() with Several Specific Columns</b>
</h3>
The following code shows how to use <b>fillna()</b> to replace the NaN values with zeros in both the “rating” and “points” columns:
<b>#replace NaNs with zeros in 'rating' and 'points' columns
df[['rating', 'points']] = df[['rating', 'points']].fillna(0)
#view DataFrame
df
ratingpointsassistsrebounds
00.025.05.011
185.00.07.08
20.014.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
Notice that the NaN values have been replaced in the “rating” and “points” columns but the other columns remain untouched.
<b>Note</b>: You can find the complete documentation for the pandas <b>fillna()</b> function  here .
<h2><span class="orange">Pandas: How to Fill NaN Values with Values from Another Column</span></h2>
You can use the following syntax to replace NaN values in a column of a pandas DataFrame with the values from another column:
<b>df['col1'] = df['col1'].fillna(df['col2'])
</b>
This particular syntax will replace any NaN values in <b>col1</b> with the corresponding values in <b>col2</b>.
The following example shows how to use this syntax in practice.
<h3>Example: Replace Missing Values with Another Column</h3>
Suppose we have the following pandas DataFrame with some missing values:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'team1': ['Mavs', np.nan, 'Nets', 'Hawks', np.nan, 'Jazz'],   'team2': ['Spurs', 'Lakers', 'Kings', 'Celtics', 'Heat', 'Magic']})
#view DataFrame
df
        team1team2
0MavsSpurs
1NaNLakers
2NetsKings
3HawksCeltics
4NaNHeat
5JazzMagic
</b>
Notice that there are two NaN values in the <b>team1</b> column.
We can use the <b>fillna()</b> function to fill the NaN values in the <b>team1 </b>column with the corresponding value in the <b>team2 </b>column:
<b>#fill NaNs in team1 column with corresponding values in team2 column
df['team1'] = df['team1'].fillna(df['team2'])
#view updated DataFrame 
df
        team1team2
0MavsSpurs
1LakersLakers
2NetsKings
3HawksCeltics
4HeatHeat
5JazzMagic
</b>
Notice that both of the NaN values in the <b>team1</b> column were replaced with the corresponding values in the <b>team2</b> column.
<b>Note</b>: You can find the complete online documentation for the <b>fillna()</b> function  here .
<h2><span class="orange">Pandas: How to Fill NaN Values with Mean (3 Examples)</span></h2>
You can use the <b>fillna()</b> function to replace NaN values in a pandas DataFrame.
Here are three common ways to use this function:
<b>Method 1: Fill NaN Values in One Column with Mean</b>
<b>df['col1'] = df['col1'].fillna(df['col1'].mean())</b>
<b>Method 2: Fill NaN Values in Multiple Columns with Mean</b>
<b>df[['col1', 'col2']] = df[['col1', 'col2']].fillna(df[['col1', 'col2']].mean())</b>
<b>Method 3: Fill NaN Values in All Columns with Mean</b>
<b>df = df.fillna(df.mean())</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'rating': [np.nan, 85, np.nan, 88, 94, 90, 76, 75, 87, 86],   'points': [25, np.nan, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, np.nan, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
        ratingpointsassistsrebounds
0NaN25.05.011
185.0NaN7.08
2NaN14.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 1: Fill NaN Values in One Column with Mean</h3>
The following code shows how to fill the NaN values in the <b>rating</b> column with the mean value of the <b>rating</b> column:
<b>#fill NaNs with column mean in 'rating' column
df['rating'] = df['rating'].fillna(df['rating'].mean())
#view updated DataFrame 
df
ratingpointsassistsrebounds
085.12525.05.011
185.000NaN7.08
285.12514.07.010
388.00016.0NaN6
494.00027.05.06
590.00020.07.09
676.00012.06.06
775.00015.09.010
887.00014.09.010
986.00019.05.07
</b>
The mean value in the <b>rating</b> column was <b>85.125</b> so each of the NaN values in the <b>rating</b> column were filled with this value.
<h3>
<b>Example 2: Fill NaN Values in Multiple Columns with Mean</b>
</h3>
The following code shows how to fill the NaN values in both the <b>rating</b> and <b>points</b> columns with their respective column means:
<b>#fill NaNs with column means in 'rating' and 'points' columns
df[['rating', 'points']] = df[['rating', 'points']].fillna(df[['rating', 'points']].mean())
#view updated DataFrame
df
ratingpointsassistsrebounds
085.12525.05.011
185.00018.07.08
285.12514.07.010
388.00016.0NaN6
494.00027.05.06
590.00020.07.09
676.00012.06.06
775.00015.09.010
887.00014.09.010
986.00019.05.07
</b>
The NaN values in both the <b>ratings</b> and <b>points</b> columns were filled with their respective column means.
<h3>Example 3: Fill NaN Values in All Columns with Mean</h3>
The following code shows how to fill the NaN values in each column with the column means:
<b>#fill NaNs with column means in each column 
df = df.fillna(df.mean())
#view updated DataFrame
df
        ratingpointsassists  rebounds
085.12525.05.000000  11
185.00018.07.000000  8
285.12514.07.000000  10
388.00016.06.666667  6
494.00027.05.000000  6
590.00020.07.000000  9
676.00012.06.000000  6
775.00015.09.000000  10
887.00014.09.000000  10
986.00019.05.000000  7
</b>
Notice that the NaN values in each column were filled with their column mean.
You can find the complete online documentation for the <b>fillna()</b> function  here .
<h2><span class="orange">Pandas: How to Fill NaN Values with Median (3 Examples)</span></h2>
You can use the <b>fillna()</b> function to replace NaN values in a pandas DataFrame.
Here are three common ways to use this function:
<b>Method 1: Fill NaN Values in One Column with Median</b>
<b>df['col1'] = df['col1'].fillna(df['col1'].median())</b>
<b>Method 2: Fill NaN Values in Multiple Columns with Median</b>
<b>df[['col1', 'col2']] = df[['col1', 'col2']].fillna(df[['col1', 'col2']].median())</b>
<b>Method 3: Fill NaN Values in All Columns with Median</b>
<b>df = df.fillna(df.median())</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'rating': [np.nan, 85, np.nan, 88, 94, 90, 76, 75, 87, 86],   'points': [25, np.nan, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, np.nan, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
        ratingpointsassistsrebounds
0NaN25.05.011
185.0NaN7.08
2NaN14.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 1: Fill NaN Values in One Column with Median</h3>
The following code shows how to fill the NaN values in the <b>rating</b> column with the median value of the <b>rating</b> column:
<b>#fill NaNs with column median in 'rating' column
df['rating'] = df['rating'].fillna(df['rating'].median())
#view updated DataFrame 
df
        ratingpointsassistsrebounds
086.525.05.011
185.0NaN7.08
286.514.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
The median value in the <b>rating</b> column was <b>86.5 </b>so each of the NaN values in the <b>rating</b> column were filled with this value.
<h3>
<b>Example 2: Fill NaN Values in Multiple Columns with Median</b>
</h3>
The following code shows how to fill the NaN values in both the <b>rating</b> and <b>points</b> columns with their respective column medians:
<b>#fill NaNs with column medians in 'rating' and 'points' columns
df[['rating', 'points']] = df[['rating', 'points']].fillna(df[['rating', 'points']].median())
#view updated DataFrame
df
ratingpointsassistsrebounds
086.525.05.011
185.016.07.08
286.514.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
The NaN values in both the <b>ratings</b> and <b>points</b> columns were filled with their respective column medians.
<h3>Example 3: Fill NaN Values in All Columns with Median</h3>
The following code shows how to fill the NaN values in each column with their column median:
<b>#fill NaNs with column medians in each column 
df = df.fillna(df.median())
#view updated DataFrame
df
ratingpointsassistsrebounds
086.525.05.011
185.016.07.08
286.514.07.010
388.016.07.06
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
Notice that the NaN values in each column were filled with their column median.
You can find the complete online documentation for the <b>fillna()</b> function  here .
<h2><span class="orange">Pandas: How to Fill NaN Values with Mode</span></h2>
You can use the following syntax to replace NaN values in a column of a pandas DataFrame with the mode value of the column:
<b>df['col1'] = df['col1'].fillna(df['col1'].mode()[0])</b>
The following example shows how to use this syntax in practice.
<h3>Example: Replace Missing Values with Mode in Pandas</h3>
Suppose we have the following pandas DataFrame with some missing values:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'rating': [np.nan, 85, np.nan, 88, 94, 90, 75, 75, 87, 86],   'points': [25, np.nan, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, np.nan, 5, 7, 6, 9, 9, 7],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
        ratingpointsassistsrebounds
0NaN25.05.011
185.0NaN7.08
2NaN14.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
675.012.06.06
775.015.09.010
887.014.09.010
986.019.07.07
</b>
We can use the <b>fillna()</b> function to fill the NaN values in the <b>rating</b> column with the mode value of the <b>rating</b> column:
<b>#fill NaNs with column mode in 'rating' column
df['rating'] = df['rating'].fillna(df['rating'].mode()[0])
#view updated DataFrame 
df
ratingpointsassistsrebounds
075.025.05.011
185.0NaN7.08
275.014.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
675.012.06.06
775.015.09.010
887.014.09.010
986.019.07.07
</b>
The mode value in the <b>rating</b> column was <b>75 </b>so each of the NaN values in the <b>rating</b> column were filled with this value.
<b>Note</b>: You can find the complete online documentation for the <b>fillna()</b> function  here .
<h2><span class="orange">How to Use Pandas fillna() to Replace NaN Values</span></h2>
You can use the <b>fillna()</b> function to replace NaN values in a pandas DataFrame.
This function uses the following basic syntax:
<b>#replace NaN values in one column
df['col1'] = df['col1'].fillna(0)
#replace NaN values in multiple columns
df[['col1', 'col2']] = df[['col1', 'col2']].fillna(0) 
#replace NaN values in all columns
df = df.fillna(0)</b>
This tutorial explains how to use this function with the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame with some NaN values
df = pd.DataFrame({'rating': [np.nan, 85, np.nan, 88, 94, 90, 76, 75, 87, 86],   'points': [25, np.nan, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, np.nan, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
        ratingpointsassistsrebounds
0NaN25.05.011
185.0NaN7.08
2NaN14.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 1: Replace NaN Values in One Column</h3>
The following code shows how to replace the NaN values with zeros in the “rating” column:
<b>#replace NaNs with zeros in 'rating' column
df['rating'] = df['rating'].fillna(0)
#view DataFrame 
df
ratingpointsassistsrebounds
00.025.05.011
185.0NaN7.08
20.014.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 2: Replace NaN Values in Multiple Columns</h3>
The following code shows how to replace the NaN values with zeros in both the “rating” and “points” columns:
<b>#replace NaNs with zeros in 'rating' and 'points' columns
df[['rating', 'points']] = df[['rating', 'points']].fillna(0)
#view DataFrame
df
ratingpointsassistsrebounds
00.025.05.011
185.00.07.08
20.014.07.010
388.016.0NaN6
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
<h3>Example 3: Replace NaN Values in All Columns</h3>
The following code shows how to replace the NaN values in every column with zeros:
<b>#replace NaNs with zeros in all columns 
df = df.fillna(0)
#view DataFrame
df
        ratingpointsassistsrebounds
00.025.05.011
185.00.07.08
20.014.07.010
388.016.00.06
494.027.05.06
590.020.07.09
676.012.06.06
775.015.09.010
887.014.09.010
986.019.05.07
</b>
You can find the complete online documentation for the <b>fillna()</b> function  here .
<h2><span class="orange">Pandas: How to Filter by Index Value</span></h2>
You can use the following basic syntax to filter the rows of a pandas DataFrame based on index values:
<b>df_filtered = df[df.index.isin(some_list)]
</b>
This will filter the pandas DataFrame to only include the rows whose index values are contained in <b>some_list</b>.
The following examples show how to use this syntax in practice.
<h2>Example 1: Filter by Numeric Index Values</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
   points  assists  rebounds
0      18        5        11
1      22        7         8
2      19        7        10
3      14        9         6
4      14       12         6
5      11        9         5
6      20        9         9
7      28        4        12</b>
Notice that the index values are numeric.
Suppose we would like to filter for rows where the index value is equal to 1, 5, 6, or 7.
We can use the following syntax to do so:
<b>#define list of index values
some_list = [1, 5, 6, 7]
#filter for rows in list
df_filtered = df[df.index.isin(some_list)]
#view filtered DataFrame
print(df_filtered)
   points  assists  rebounds
1      22        7         8
5      11        9         5
6      20        9         9
7      28        4        12</b>
Notice that the only rows returned are those whose index value is equal to 1, 5, 6, or 7.
<h2>Example 2: Filter by Non-Numeric Index Values</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]},   index=['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'])
#view DataFrame
print(df)
   points  assists  rebounds
A      18        5        11
B      22        7         8
C      19        7        10
D      14        9         6
E      14       12         6
F      11        9         5
G      20        9         9
H      28        4        12</b>
Notice that the index values are character values.
Suppose we would like to filter for rows where the index value is equal to A, C, F, or G.
We can use the following syntax to do so:
<b>#define list of index values
some_list = ['A', 'C', 'F', 'G']
#filter for rows in list
df_filtered = df[df.index.isin(some_list)]
#view filtered DataFrame
print(df_filtered)
   points  assists  rebounds
A      18        5        11
C      19        7        10
F      11        9         5
G      20        9         9</b>
Notice that the only rows returned are those whose index value is equal to A, C, F, or G.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Insert a Row Into a Pandas DataFrame 
 How to Drop First Row in Pandas DataFrame 
 How to Drop Rows in Pandas DataFrame Based on Condition 
<h2><span class="orange">How to Filter a Pandas DataFrame on Multiple Conditions</span></h2>
Often you may want to filter a pandas DataFrame on more than one condition. Fortunately this is easy to do using boolean operations.
This tutorial provides several examples of how to filter the following pandas DataFrame on multiple conditions:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'C'],   'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame 
df
        teampointsassistsrebounds
0A25511
1A1278
2B15710
3B1496
4C19126
</b>
<h3>Example 1: Filter on Multiple Conditions Using ‘And’</h3>
The following code illustrates how to filter the DataFrame using the <em>and</em> (<b>&</b>) operator:
<b>#return only rows where points is greater than 13 and assists is greater than 7
df[(df.points > 13) & (df.assists > 7)]
        teampointsassistsrebounds
3B1496
4C19126
#return only rows where team is 'A' and points is greater than or equal to 15
df[(df.team == 'A') & (df.points >= 15)]
        teampointsassistsrebounds
0A25511
</b>
<h3>Example 2: Filter on Multiple Conditions Using ‘Or’</h3>
The following code illustrates how to filter the DataFrame using the <i>or </i>(<b>|</b>) operator:
<b>#return only rows where points is greater than 13 or assists is greater than 7
df[(df.points > 13) | (df.assists > 7)]
        teampointsassistsrebounds
0A25511
2B15710
3B1496
4C19126
#return only rows where team is 'A' or points is greater than or equal to 15
df[(df.team == 'A') | (df.points >= 15)]
        teampointsassistsrebounds
0A25511
1A1278
2B15710
4C19126</b>
<h3>Example 3: Filter on Multiple Conditions Using a List</h3>
The following code illustrates how to filter the DataFrame where the row values are in some list.
<b>#define a list of values
filter_list = [12, 14, 15]
#return only rows where points is in the list of values
df[df.points.isin(filter_list)]
teampointsassistsrebounds
1A1278
2B15710
3B1496
#define another list of values
filter_list2 = ['A', 'C']
#return only rows where team is in the list of values
df[df.team.isin(filter_list2)]
        teampointsassistsrebounds
0A25511
1A1278
4C19126</b>
<em>You can find more pandas tutorials  here .</em>

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
