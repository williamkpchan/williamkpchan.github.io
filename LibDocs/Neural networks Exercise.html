<head>
<meta http-equiv="Content-Type" content="text/html;  charset=utf-8">

<style type="text/css">

body {
 margin-top: 5%; 
 margin-bottom: 5%; 
 margin-right: 10%; 
 margin-left: 10%; 
 background-color: #000000; 
 color: #109080; 
 font-size: 24px; 
}
a { text-decoration: none; 
	color: #28B8B8; }
a:visited {	color: #389898; }
A:hover {	color: yellow; }
A:focus {	color: red; }
code { color: #28B8B8;  background-color: #002030}
pre { color: gray;  background-color: #001010;  font-size: 16px; }
div {display: inline-block;  width: 48%;  padding: 2px;  border-radius: 4px;  
border: 1px solid gray;  margin: 3px;  vertical-align:middle; }
</STYLE>
</head>
<body>

<h1 class="entry-title" itemprop="headline">Neural networks Exercises (Part-1)</h1>

<p>Neural network have become a corner stone of machine learning in the last decade. <br>
Created in the late 1940s with the intention to create computer programs who mimics the way neurons process information, those kinds of algorithm have long been believe to be only an academic curiosity, deprived of practical use since they require a lot of processing power and other machine learning algorithm outperform them. <br>
However since the mid 2000s, the creation of new neural network types and techniques, couple with the increase availability of fast computers made the neural network a powerful tool that every data analysts or programmer must know.</p>
<p>In this series of articles, we&#8217;ll see how to fit a neural network with R, we&#8217;ll learn the core concepts we need to know to well apply those algorithms and how to evaluate if our model is appropriate to use in production. <br>
This set of exercises is an introduction to neural networks where we&#8217;ll use them to create two simple regression and clustering model. Doing so, we&#8217;ll use a lot of basic concepts we&#8217;ll explore further in future sets. If you want more informations about neural network, your can <a href="https://www.tutorialspoint.com/artificial_intelligence/artificial_intelligence_neural_networks.htm" class="external" rel="nofollow"> see this page.</a></p>
<p>Answers to the exercises are available <a href="http://r-exercises.com/2017/06/08/neural-networks-solutions-part-1/">here</a>.</p>


<p><strong>Exercise 1</strong><br/>
We&#8217;ll start by creating the data set on which we want to do a simple regression. Set the seed to 42, generate 200 random points between -10 and 10 and store them in a vector named X. Then, create a vector named Y containing the value of <code>sin(x)</code>. Neural network are a lot more flexible than most regression algorithms and can fit complex function with ease. The biggest challenge is to find the appropriate network function appropriate to the situation.</p>
<pre>
####################
#                  #
#    Exercise 1    #
#                  #
####################
set.seed(42)
x&llt;-runif(200, -10, 10)
y&lt;-sin(x)
</pre>

<p><strong>Exercise 2</strong><br/>
A network function is made of three components: the network of neurons, the weight of each connection between neuron and the activation function of each neuron. For this example, we&#8217;ll use a feed-forward neural network and the logistic activation which are the defaults for the package <code>nnet</code>. We take one number as input of our neural network and we want one number as the output so the size of the input and output layer are both of one. For the hidden layer, we&#8217;ll start with three neurons. It&#8217;s good practice to randomize the initial weights, so create a vector of 10 random values, picked in the interval <code>[-1,1]</code>.</p>
<pre>
####################
#                  #
#    Exercise 2    #
#                  #
####################
weight&lt;-runif(10, -1, 1)
</pre>

<p><strong>Exercise 3</strong><br/>
Neural networks have a strong tendency of overfitting your data, meaning they become really good at describing the relationship between the values in your data set, but are not effective with data that wasn&#8217;t used to train your model. As a consequence, we need to cross-validate our model. Set the seed to 42, then create a training set containing 75% of the values in your initial data set and a test set containing the rest of your data.</p>
<pre>
####################
#                  #
#    Exercise 3    #
#                  #
####################
set.seed(42)
index&lt;-sample(1:length(x),round(0.75*length(x)),replace=FALSE)
reg.train&lt;-data.frame(X=x[index],Y=y[index])
reg.test&lt;-data.frame(X=x[-index],Y=y[-index])
</pre>

<p><strong>Exercise 4</strong><br/>
Load the <code>nnet</code> package and use the function of the same name to create your model. Pass your weights via the <code>Wts</code> argument and set the <code>maxit</code> argument to 50. We want to fit a function which can have for output multiple possible values. To do so, set the <code>linout</code> argument to <code>true</code>. Finally, take the time to look at the structure of your model.</p>

<strong>Learn more</strong> about neural networks in the online course <a href="http://www.r-exercises.com/product/machine-learning-a-z-hands-on-python-r-in-data-science/">Machine Learning A-Z™: Hands-On Python &#038; R In Data Science</a>. In this course you will learn how to:</p>
<ul>
<li>Work with Deep Learning networks and related packagse in R</li>
<li>Create Natural Language Processing models</li>
<li>And much more</li>
</ul>
<p>
<pre>
####################
#                  #
#    Exercise 4    #
#                  #
####################
library(nnet)
set.seed(42)
reg.model.1&lt;-nnet(reg.train$X,reg.train$Y,size=3,maxit=50,Wts=weight,linout=TRUE)
## # weights:  10
## initial  value 103.169943 
## iter  10 value 70.636986
## iter  20 value 69.759785
## iter  30 value 63.215384
## iter  40 value 45.634297
## iter  50 value 39.876476
## final  value 39.876476 
## stopped after 50 iterations
str(reg.model.1)
## List of 15
##  $ n            : num [1:3] 1 3 1
##  $ nunits       : int 6
##  $ nconn        : num [1:7] 0 0 0 2 4 6 10
##  $ conn         : num [1:10] 0 1 0 1 0 1 0 2 3 4
##  $ nsunits      : num 5
##  $ decay        : num 0
##  $ entropy      : logi FALSE
##  $ softmax      : logi FALSE
##  $ censored     : logi FALSE
##  $ value        : num 39.9
##  $ wts          : num [1:10] -7.503 2.202 3.004 -0.806 -4.69 ...
##  $ convergence  : int 1
##  $ fitted.values: num [1:150, 1] -0.196 0.568 -0.353 -0.205 -0.161 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##  $ residuals    : num [1:150, 1] 0.692 0.3079 -0.0398 0.4262 -0.7024 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##  $ call         : language nnet.default(x = reg.train$X, y = reg.train$Y, size = 3, Wts = weight,      linout = TRUE, maxit = 50)
##  - attr(*, &quot;class&quot;)= chr &quot;nnet&quot;
</pre>

<p><strong>Exercise 5</strong><br/>
Predict the output for the test set and compute the RMSE of your predictions. Plot the function sin(x) and then plot your predictions.</p>

<pre>
####################
#                  #
#    Exercise 5    #
#                  #
####################
predict.model.1&lt;-predict(reg.model.1,data.frame(X=reg.test$X))
str(predict.model.1)
##  num [1:50, 1] -0.201 0.184 -0.873 -0.981 0.598 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : NULL
rmse.reg&lt;-sqrt(sum((reg.test$Y-predict.model.1)^2))
rmse.reg
## [1] 3.41651
plot(sin, -10, 10)
points(reg.test$X,predict.model.1)
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part1-1.png" title="plot of chunk NN_part1" alt="plot of chunk NN_part1" class="plot"/>


<p><strong>Exercise 6</strong><br/>
The number of neurons in the hidden layer, as well as the number of hidden layer used, has a great influence on the effectiveness of your model. Repeat the exercises three to five, but this time use a hidden layer with seven neurons and initiate randomly 22 weights.</p>

<pre>
####################
#                  #
#    Exercise 6    #
#                  #
####################
set.seed(42)
reg.model.2&lt;-nnet(reg.train$X,reg.train$Y,size=7,maxit=50,Wts=runif(22, -1, 1),linout=TRUE)
## # weights:  22
## initial  value 353.642846 
## iter  10 value 55.906010
## iter  20 value 42.700328
## iter  30 value 22.757713
## iter  40 value 16.910492
## iter  50 value 12.770497
## final  value 12.770497 
## stopped after 50 iterations

str(reg.model.2)

## List of 15
##  $ n            : num [1:3] 1 7 1
##  $ nunits       : int 10
##  $ nconn        : num [1:11] 0 0 0 2 4 6 8 10 12 14 ...
##  $ conn         : num [1:22] 0 1 0 1 0 1 0 1 0 1 ...
##  $ nsunits      : num 9
##  $ decay        : num 0
##  $ entropy      : logi FALSE
##  $ softmax      : logi FALSE
##  $ censored     : logi FALSE
##  $ value        : num 12.8
##  $ wts          : num [1:22] 0.894 0.992 1.997 1.506 8.113 ...
##  $ convergence  : int 1
##  $ fitted.values: num [1:150, 1] 0.585 0.6145 -0.464 0.0943 -0.8862 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##  $ residuals    : num [1:150, 1] -0.0891 0.2612 0.071 0.1269 0.0232 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..$ : NULL
##   .. ..$ : NULL
##  $ call         : language nnet.default(x = reg.train$X, y = reg.train$Y, size = 7, Wts = runif(22,      -1, 1), linout = TRUE, maxit = 50)
##  - attr(*, &quot;class&quot;)= chr &quot;nnet&quot;

predict.model.2&lt;-predict(reg.model.2,data.frame(X=reg.test$X))
str(predict.model.2)

##  num [1:50, 1] 1.13 0.153 -0.931 -0.962 0.647 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : NULL

rmse.reg&lt;-sqrt(sum((reg.test$Y-predict.model.2)^2))
rmse.reg

## [1] 2.188407

plot(sin, -10, 10)
points(reg.test$X,predict.model.2)

</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part1-2.png" title="plot of chunk NN_part1" alt="plot of chunk NN_part1" class="plot"/>

<p><strong>Exercise 7</strong><br/>
Now let us use neural networks to solve a clustering problems, so let&#8217;s load the <code>iris</code> data set! It is good practice to normalize your input data to uniformize the behavior of your model over different range of value and have a faster training. Normalize each factor so that they have a mean of zero and a standard deviation of 1, then create your train and test set.</p>

<pre>
####################
#                  #
#    Exercise 7    #
#                  #
####################
data&lt;-iris

scale.data&lt;-data.frame(lapply(data[,1:4], function(x) scale(x)))
scale.data$Species&lt;-data$Species

index&lt;-sample(1:nrow(scale.data),round(0.75*nrow(scale.data)),replace=FALSE)
clust.train&lt;-scale.data[index,]
clust.test&lt;-scale.data[-index,]
</pre>

<p><strong>Exercise 8</strong><br/>
Use the <code>nnet()</code> and use a hidden layer of ten neurons to create your model. We want to fit a function which have a finite amount of value as output. To do so, set the code&gt;linout argument to <code>true</code>. Look at the structure of your model. With clustering problem, the output is usually a factor that is coded as multiple dummy variables, instead of a single numeric value. As a consequence, the output layer have as one less neuron than the number of levels of the output factor.</p>
<pre>
####################
#                  #
#    Exercise 8    #
#                  #
####################
set.seed(42)
clust.model&lt;-nnet(Species~.,size=10,Wts=runif(83, -1, 1),data=clust.train)

## # weights:  83
## initial  value 187.294915 
## iter  10 value 10.386561
## iter  20 value 5.337510
## iter  30 value 2.311922
## iter  40 value 1.426508
## iter  50 value 1.387440
## iter  60 value 1.386324
## final  value 1.386294 
## converged

</pre>

<p><strong>Exercise 9</strong><br/>
Make prediction with the values of the test set.</p>


<pre>
####################
#                  #
#    Exercise 9    #
#                  #
####################
predict.model.clust&lt;-predict(clust.model,clust.test[,1:4],type=&quot;class&quot;)
</pre>

<p><strong>Exercise 10</strong><br/>
Create the confusion table of your prediction and compute the accuracy of the model.</p>
<pre>
####################
#                  #
#    Exercise 10   #
#                  #
####################
Table&lt;-table(clust.test$Species ,predict.model.clust)
Table

##             predict.model.clust
##              setosa versicolor virginica
##   setosa         16          0         0
##   versicolor      0          9         0
##   virginica       0          0        13

accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy

## [1] 1
</pre>

<h1>Neural networks Exercises (Part-2)</h1>

<p>Neural network have become a corner stone of machine learning in the last decade. Created in the late 1940s with the intention to create computer programs who mimics the way neurons process information, those kinds of algorithm have long been believe to be only an academic curiosity, deprived of practical use since they require a lot of processing power and other machine learning algorithm outperform them. However since the mid 2000s, the creation of new neural network types and techniques, couple with the increase availability of fast computers made the neural network a powerful tool that every data analysts or programmer must know.</p>
<p>In this series of articles, we&#8217;ll see how to fit a neural network with R, we&#8217;ll learn the core concepts we need to know to well apply those algorithms and how to evaluate if our model is appropriate to use in production. Today, we&#8217;ll practice how to use the <code>nnet</code> and <code>neuralnet</code> packages to create a feedforward neural networks, which we introduce in the <a href="http://www.r-exercises.com/2017/06/08/neural-networks-exercises-part-1/"> last set of exercises</a>. In this type of neural network, all the neuron from the input layer are linked to the neuron from the hidden layer and all of those neuron are linked to the output layer, like seen on this <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network#/media/File:Feed_forward_neural_net.gif" class="external" rel="nofollow">image</a>. Since there&#8217;s no cycle in this network, the information flow in one direction from the input layer to the hidden layers to the output layer. For more information about those types of neural network you can <a href="http://www.iro.umontreal.ca/~bengioy/ift6266/H12/html/mlp_en.html" class="external" rel="nofollow">read this page</a>.</p>
<p>Answers to the exercises are available <a href="http://www.r-exercises.com/2017/06/15/neural-networks-solutions-part-2/">here</a>.</p>


<p><strong>Exercise 1</strong><br/>
We&#8217;ll start by practicing what we&#8217;ve seen in the <a href="http://www.r-exercises.com/2017/06/08/neural-networks-exercises-part-1/"> last set of exercises</a>. Load the <code>MASS</code> package and the biopsy dataset, then prepare your data to be feed to a neural network.</p>

<pre>
####################
#                  #
#    Exercise 1    #
#                  #
####################
library(MASS)
data&lt;-biopsy

scale.0.1&lt;-function(x){
  (x-min(x,na.rm=TRUE))/(max(x,na.rm=TRUE)-min(x,na.rm=TRUE))
}

norm.data&lt;-data.frame(lapply(data[,2:10], function(x) scale.0.1(x)))
norm.data$class&lt;-data$class
str(norm.data)
</pre>
<pre>
## 'data.frame':	699 obs. of  10 variables:
##  $ V1   : num  0.444 0.444 0.222 0.556 0.333 ...
##  $ V2   : num  0 0.333 0 0.778 0 ...
##  $ V3   : num  0 0.333 0 0.778 0 ...
##  $ V4   : num  0 0.444 0 0 0.222 ...
##  $ V5   : num  0.111 0.667 0.111 0.222 0.111 ...
##  $ V6   : num  0 1 0.111 0.333 0 ...
##  $ V7   : num  0.222 0.222 0.222 0.222 0.222 ...
##  $ V8   : num  0 0.111 0 0.667 0 ...
##  $ V9   : num  0 0 0 0 0 ...
##  $ class: Factor w/ 2 levels "benign","malignant": 1 1 1 1 1 2 1 1 1 1 ...
</pre>

<pre>
sum(is.na(norm.data))
</pre>

<pre>
## [1] 16
</pre>

<pre>
norm.data&lt;-norm.data[complete.cases(norm.data),]

set.seed(42)
index&lt;-sample(1:nrow(norm.data),round(0.75*nrow(norm.data)),replace=FALSE)
train&lt;-norm.data[index,]
test&lt;-norm.data[-index,]

</pre>


<p><strong>Exercise 2</strong><br/>
We&#8217;ll use the <code>nnet()</code> function from the package of the same name to do a logistic regression on the biopsy data set using a feedforward neural network. If you remember the<br/>
<a href="http://www.r-exercises.com/2017/06/08/neural-networks-exercises-part-1/"> last set of exercises</a> you know that we have to choose the number of neuron in the hidden layer of our feedforward neural network. There&#8217;s no rule or equation which can tell us the optimal number of neurons to use, so the best way to find the better model is to do a bunch of cross-validation of our model with different number of neurons in the hidden layer and choose the one who would fit best the data. A good range to test with this process is between one neuron and the number of input variables.</p>
<p>Write a function that take a train data set, a test data set and a range of integer corresponding to the number of neurons to be used as parameter. Then this function should, for each possible number of neuron in the hidden layer, train a neural network made with <code>nnet()</code>, make prediction on the test set and return the accuracy of the prediction.</p>



<pre>
####################
#                  #
#    Exercise 2    #
#                  #
####################
library(nnet)
cross.val.nnet&lt;-function(train,test,low_range,high_range){

  acc&lt;-NULL
  for (h in low_range:high_range)
  {
    temp.nn&lt;-nnet(class~.,size=h,data=train)
    pred&lt;-predict(temp.nn,test,type="class")

    Table&lt;-table(test$class,pred)
    accuracy&lt;-sum(diag(Table))/sum(Table)
    acc&lt;-c(acc,accuracy)
  }

  return(acc)
}

</pre>


<p><strong>Exercise 3</strong><br/>
Use your function on your data set and plot the result. Which should be the number of neurons to use in the hidden layer of your feedforward neural network.</p>


<pre>
####################
#                  #
#    Exercise 3    #
#                  #
####################
set.seed(42)
cross.val.nnet(train,test,1,9)
</pre>

<pre>
## # weights:  12
## initial  value 364.417657 
## iter  10 value 44.380633
## iter  20 value 33.313356
## iter  30 value 32.921823
## iter  40 value 32.751413
## iter  50 value 32.458291
## iter  60 value 32.429321
## iter  70 value 32.418764
## iter  80 value 32.331775
## iter  90 value 32.331194
## iter 100 value 32.308230
## final  value 32.308230 
## stopped after 100 iterations
## # weights:  23
## initial  value 336.035945 
## iter  10 value 50.031115
## iter  20 value 44.900883
## iter  30 value 42.117609
## iter  40 value 42.032382
## iter  50 value 42.031243
## iter  60 value 42.016893
## final  value 42.016821 
## converged
## # weights:  34
## initial  value 336.180770 
## iter  10 value 30.685458
## iter  20 value 25.271087
## iter  30 value 22.064008
## iter  40 value 19.725271
## iter  50 value 19.632863
## iter  60 value 19.619118
## iter  70 value 19.617317
## final  value 19.617252 
## converged
## # weights:  45
## initial  value 343.727637 
## iter  10 value 31.305943
## iter  20 value 19.634686
## iter  30 value 9.829079
## iter  40 value 7.646967
## iter  50 value 6.991482
## iter  60 value 6.770721
## iter  70 value 6.744635
## iter  80 value 6.734181
## iter  90 value 6.731891
## iter 100 value 6.731581
## final  value 6.731581 
## stopped after 100 iterations
## # weights:  56
## initial  value 333.483628 
## iter  10 value 38.324417
## iter  20 value 29.270707
## iter  30 value 25.440351
## iter  40 value 24.094437
## iter  50 value 23.641925
## iter  60 value 23.542413
## iter  70 value 23.516243
## iter  80 value 23.483435
## iter  90 value 23.472333
## iter 100 value 23.470332
## final  value 23.470332 
## stopped after 100 iterations
## # weights:  67
## initial  value 315.985097 
## iter  10 value 38.536149
## iter  20 value 35.606775
## iter  30 value 31.851684
## iter  40 value 31.174335
## iter  50 value 28.020057
## iter  60 value 27.167285
## iter  70 value 26.660018
## iter  80 value 25.339098
## iter  90 value 23.868263
## iter 100 value 23.198007
## final  value 23.198007 
## stopped after 100 iterations
## # weights:  78
## initial  value 320.459388 
## iter  10 value 35.029957
## iter  20 value 27.567841
## iter  30 value 22.786864
## iter  40 value 18.465745
## iter  50 value 15.734893
## iter  60 value 15.484935
## iter  70 value 15.456661
## iter  80 value 15.456458
## iter  90 value 15.456260
## iter 100 value 15.456211
## final  value 15.456211 
## stopped after 100 iterations
## # weights:  89
## initial  value 399.807517 
## iter  10 value 31.543391
## iter  20 value 21.925367
## iter  30 value 11.961561
## iter  40 value 10.369314
## iter  50 value 8.827267
## iter  60 value 8.344943
## iter  70 value 7.987507
## iter  80 value 7.679489
## iter  90 value 7.343846
## iter 100 value 6.233574
## final  value 6.233574 
## stopped after 100 iterations
## # weights:  100
## initial  value 311.715981 
## iter  10 value 33.204124
## iter  20 value 16.947828
## iter  30 value 6.369743
## iter  40 value 1.847597
## iter  50 value 0.030143
## iter  60 value 0.002284
## final  value 0.000064 
## converged
</pre>

<pre>
## [1] 0.9649122807 0.9707602339 0.9649122807 0.9473684211 0.9590643275
## [6] 0.9707602339 0.9649122807 0.9649122807 0.9415204678
</pre>

<pre>
#So we use 2 or 6 neurons in the hidden layer

</pre>


<p><strong>Exercise 4</strong><br/>
The <code>nnet()</code> function is easy to use, but doesn&#8217;t give us a lot of option to customize our neural network. As a consequence, it&#8217;s a good package to use if you have to do a quick model to test a hypothesis, but for more complex model the <code>neuralnet</code> package is a lot more powerful. Documentation for this package can be found <a href="ftp://oss.ustc.edu.cn/CRAN/web/packages/neuralnet/neuralnet.pdf" class="external" rel="nofollow">here</a>.</p>
<p>Use the <code>neuralnet()</code> function with the default parameter and the number of neuron in the hidden layer set to the answer of the last exercise. Note that this function can only handle numeric value and cannot deal with factors. Then use the <code>compute()</code> function to make prediction on the values of the test set and compute the accuracy of your model.</p>


<pre>
####################
#                  #
#    Exercise 4    #
#                  #
####################
library(neuralnet)
train$class&lt;-as.numeric(train$class)
test$class&lt;-as.numeric(test$class)

n &lt;- names(train)
f &lt;- as.formula(paste("class ~", paste(n[!n %in% "class"], collapse = " + ")))

set.seed(42)
model.neuralnet.1&lt;-neuralnet(f,data=train,hidden=6)

pred.model.1 &lt;- compute(model.neuralnet.1,test[,1:9])
str(pred.model.1)
</pre>

<pre>
## List of 2
##  $ neurons   :List of 2
##   ..$ : num [1:171, 1:10] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..- attr(*, "dimnames")=List of 2
##   .. .. ..$ : chr [1:171] "4" "6" "9" "11" ...
##   .. .. ..$ : chr [1:10] "1" "V1" "V2" "V3" ...
##   ..$ : num [1:171, 1:7] 1 1 1 1 1 1 1 1 1 1 ...
##   .. ..- attr(*, "dimnames")=List of 2
##   .. .. ..$ : chr [1:171] "4" "6" "9" "11" ...
##   .. .. ..$ : NULL
##  $ net.result: num [1:171, 1] 1.915 2.012 0.985 0.981 1.038 ...
##   ..- attr(*, "dimnames")=List of 2
##   .. ..$ : chr [1:171] "4" "6" "9" "11" ...
##   .. ..$ : NULL
</pre>

<pre>
pred.model.1$net.result &lt;-round(pred.model.1$net.result)
Table&lt;-table(test$class,pred.model.1$net.result)
Table
</pre>

<pre>
##    
##       1   2
##   1 106   5
##   2   1  59
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9649122807
</pre>

<pre>
</pre>


<p><strong>Exercise 5</strong><br/>
The <code>nnet()</code> function use by default the <a href="https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm" class="external" rel="nofollow">BFGS algorithm</a> to adjust the value of the weights until the output values of our model are close to the values of our data set. The <code>neuralnet</code> package give us the option to use more efficient algorithm to compute those value which result in faster processing time and overall better estimation. For example, by default this function use the resilient backpropagation with weight backtracking.</p>
<p>Use the <code>neuralnet()</code> function with the parameter <code>algorithm</code> set to &#8216;rprop-&#8216;, which stand for resilient backpropagation without weight backtracking.<br/>
Then test your model and print the accuracy.</p>


<pre>
####################
#                  #
#    Exercise 5    #
#                  #
####################
#'rprop-'
set.seed(42)
model.neuralnet.2&lt;-neuralnet(f, data=train, hidden=6, algorithm ='rprop-')

pred.model.2 &lt;- compute(model.neuralnet.2,test[,1:9])
pred.model.2$net.result &lt;-round(pred.model.2$net.result)
pred.model.2$net.result[which(pred.model.5$net.result&lt;=1)]&lt;-1
pred.model.2$net.result[which(pred.model.5$net.result&gt;=2)]&lt;-2

Table&lt;-table(test$class,pred.model.2$net.result)
Table
</pre>

<pre>
##    
##       1   2
##   1 106   5
##   2   2  58
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9590643275
</pre>

<pre>
</pre>


<p><strong>Exercise 6</strong><br/>
Two other algorithm can be used with the <code>neuralnet()</code> function: <code>'sag'</code> and <code>'slr'</code>. Those two strings tell the function to use the globally convergent algorithm (grprop) and to modify the learning rate associated with the smallest absolute gradient (sag) or the smallest learning rate (slr). When using those algorithm, it can be useful to pass a vector or list containing the lowest and highest limit for the learning rate to the learningrate.limit parameter.</p>
<p>Again, use the <code>neuralnet()</code> function twice, once with parameter <code>algorithm</code> set to <code>'sag'</code> and then to <code>'slr'</code>. In both cases set the <code>learningrate.limit</code> parameter to c(0.1,1) and change the <code>stepmax</code> parameter to <code>1e+06</code>.</p>


<pre>
####################
#                  #
#    Exercise 6    #
#                  #
####################
#'sag'
set.seed(42)
model.neuralnet.3&lt;-neuralnet(f, data=train, hidden=6, algorithm ='sag',learningrate.limit=c(0.01,1),stepmax = 1e+06)
</pre>

<pre>
## Warning: algorithm did not converge in 1 of 1 repetition(s) within the
## stepmax
</pre>

<pre>
pred.model.3 &lt;- compute(model.neuralnet.3,test[,1:9])
</pre>

<pre>
## Warning in is.na(weights): is.na() applied to non-(list or vector) of type
## 'NULL'
</pre>

<pre>
## Error in nrow[w] * ncol[w]: non-numeric argument to binary operator
</pre>

<pre>
pred.model.3$net.result &lt;-round(pred.model.3$net.result)

Table&lt;-table(test$class,pred.model.3$net.result)
Table
</pre>

<pre>
##    
##       1   2   3
##   1 106   4   1
##   2   4  56   0
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9473684211
</pre>

<pre>
#'slr'
set.seed(42)
model.neuralnet.4&lt;-neuralnet(f, data=train, hidden=6, algorithm ='slr',learningrate.limit=c(0.01,1), stepmax = 1e+06)
</pre>

<pre>
## Warning: algorithm did not converge in 1 of 1 repetition(s) within the
## stepmax
</pre>

<pre>
pred.model.4 &lt;- compute(model.neuralnet.4,test[,1:9])
</pre>

<pre>
## Warning in is.na(weights): is.na() applied to non-(list or vector) of type
## 'NULL'
</pre>

<pre>
## Error in nrow[w] * ncol[w]: non-numeric argument to binary operator
</pre>

<pre>
pred.model.4$net.result &lt;-round(pred.model.4$net.result)

Table&lt;-table(test$class,pred.model.4$net.result)
Table
</pre>

<pre>
##    
##       1   2   3
##   1 105   6   0
##   2   6  48   6
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.8947368421
</pre>

<pre>
</pre>


<p><strong>Exercise 7</strong><br/>
The learning rate determine how much the backpropagation can affect the weight at each iteration. A high learning rate mean that during the training of the neural network, each iteration can strongly change the value of the weight or, to put in other way, the algorithm learn a lot of each observation in your data set. This mean that outlier could easily affect your weight and make your algorithm diverge from the path of the ideal weights for your problem. A small learning rate mean that the algorithm learn less from each observation in your data set, so your neural network is less affected by outlier, but this mean that you will need more observations to make a good model.</p>
<p>Use the <code>neuralnet()</code> function with parameter <code>algorithm</code> set to &#8216;rprop+&#8217; twice: once with the <code>learningrate</code> parameter set to 0.01 and another time with the <code>learningrate</code> parameter set to 1. Notice the difference in running time in both cases.</p>


<pre>
####################
#                  #
#    Exercise 7    #
#                  #
####################
#'learningrate=0.001'
set.seed(42)
model.neuralnet.5&lt;-neuralnet(f, data=train, hidden=6, algorithm ='rprop-', learningrate=0.001)

pred.model.5 &lt;- compute(model.neuralnet.5,test[,1:9])
pred.model.5$net.result &lt;-round(pred.model.5$net.result)

Table&lt;-table(test$class,pred.model.5$net.result)
Table
</pre>

<pre>
##    
##       0   1   2
##   1   2 104   5
##   2   0   2  58
</pre>

<pre>
pred.model.5$net.result[which(pred.model.5$net.result&lt;=1)]&lt;-1
pred.model.5$net.result[which(pred.model.5$net.result&gt;=2)]&lt;-2

Table&lt;-table(test$class,pred.model.5$net.result)
Table
</pre>

<pre>
##    
##       1   2
##   1 106   5
##   2   2  58
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9590643275
</pre>

<pre>
#'learningrate=1'
set.seed(42)
model.neuralnet.6&lt;-neuralnet(f, data=train, hidden=6, algorithm ='rprop-', learningrate=1)

pred.model.6 &lt;- compute(model.neuralnet.6,test[,1:9])
pred.model.6$net.result &lt;-round(pred.model.6$net.result)

Table&lt;-table(test$class,pred.model.6$net.result)
Table
</pre>

<pre>
##    
##       0   1   2
##   1   2 104   5
##   2   0   2  58
</pre>

<pre>
pred.model.6$net.result[which(pred.model.6$net.result==0)]&lt;-1

Table&lt;-table(test$class,pred.model.6$net.result)
Table
</pre>

<pre>
##    
##       1   2
##   1 106   5
##   2   2  58
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9590643275
</pre>

<pre>
</pre>

<p><strong>Exercise 8</strong><br/>
The <code>neuralnet</code> package give us the ability of make a visual representation of the neural network you made. Use the <code>plot()</code> function to visualize one of the neural networks of the last exercise.</p>


<pre>
####################
#                  #
#    Exercise 8    #
#                  #
####################
plot(model.neuralnet.5)</pre>
<img class="plot" title="plot of neural network #7" src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part2-1.png" alt="plot of neural network #7"/>

</pre>

<p><strong>Exercise 9</strong><br/>
Until now, we&#8217;ve used feedfordward neural network with one hidden layer of neurons, but we could use more. In fact, the state of the art neural network use often 100 of hidden layer for modeling complex behavior. For basic regression problems or even basic digits recognition problems, one layer is enough, but if you want to use more, you can do so with the <code>neuralnet()</code> function by passing a vector of integer to the hidden parameter representing the number of neurons in each layer.</p>

<p>Create a feedforward neural network with three hidden layers of nine neurons and use it on your data.</p>


<pre>
####################
#                  #
#    Exercise 9    #
#                  #
####################
 set.seed(42)
 model.neuralnet.7<-neuralnet(f, data=train, hidden=c(9,9,9))
 pred.model.7 <- compute(model.neuralnet.7,test[,1:9])
 pred.model.7$net.result <-round(pred.model.7$net.result)
 Table<-table(test$class,pred.model.7$net.result)
 Table


<pre>
##    
##       1   2
##   1 107   4
##   2   3  57
</pre>

<pre>
accuracy&lt;-sum(diag(Table))/sum(Table)
accuracy
</pre>

<pre>
## [1] 0.9590643275
</pre>

<pre>
</pre>


<p><strong>Exercise 10</strong><br/>
Plot the feedforward neural network from the last exercise.</p>
<pre>
####################
#                  #
#    Exercise 10   #
#                  #
####################
plot(model.neuralnet.7)</pre>
<img class="plot" title="plot of neural network #9" src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part2-2.png" alt="plot of neural network #9"/>


<h1>Neural networks Exercises (Part-3)</h1>

<br/>
Neural network have become a corner stone of machine learning in the last decade. Created in the late 1940s with the intention to create computer programs who mimics the way neurons process information, those kinds of algorithm have long been believe to be only an academic curiosity, deprived of practical use since they require a lot of processing power and other machine learning algorithm outperform them. However since the mid 2000s, the creation of new neural network types and techniques, couple with the increase availability of fast computers made the neural network a powerful tool that every data analysts or programmer must know.</p>
<p>In this series of articles, we&#8217;ll see how to fit a neural network with R, we&#8217;ll learn the core concepts we need to know to well apply those algorithms and how to evaluate if our model is appropriate to use in production. In the <a href="//www.r-exercises.com/2017/06/15/neural-networks-exercises-part-2/">last exercises sets</a>, we have seen how to implement a feed-forward neural network in R. That kind of neural network is quite useful to match a single input value to a specific output value, either a dependent variable in regression problems or a class in clustering problems. However sometime, a sequence of input can give a lot more of information to the network than a single value. For example, if you want to train a neural network to predict which letter will come next in a word based on which letters have been typed, making prediction based on the last letter entered can give good results, but if all the previous letter are used for making the predictions the results should be better since the arrangement of previous letter can give important information about the rest of the word.</p>
<p>In today&#8217;s exercise set, we will see a type of neural network that is design to make use of the information made available by using sequence of inputs. Those &#8221;recurrent neural networks&#8221; do so by using a hidden state at time t-1 that influence the calculation of the weight at time t. For more information about this type of neural network, you can read this <a href="https://deeplearning4j.org/lstm.html" class="external" rel="nofollow">article</a> which is a good introduction on the subject.</p>


<p>Answers to the exercises are available <a href="http://www.r-exercises.com/2017/06/21/neural-networks-solutions-part-3/">here</a>.</p>


<p><strong>Exercise 1</strong><br/>
We will start by using a recurrent neural network to predict the values of a time series. Load the <code>tsEuStockMarkets</code> dataset from the <code>dataset</code> package and save the first 1400 observations from the &#8220;DAX&#8221; time series as your working dataset.</p>


<pre>

####################
#                  #
#    Exercise 1    #
#                  #
####################
library(datasets)
str(EuStockMarkets)
</pre>

<pre>
##  Time-Series [1:1860, 1:4] from 1991 to 1999: 1629 1614 1607 1621 1618 ...
##  - attr(*, &quot;dimnames&quot;)=List of 2
##   ..$ : NULL
##   ..$ : chr [1:4] &quot;DAX&quot; &quot;SMI&quot; &quot;CAC&quot; &quot;FTSE&quot;
</pre>
<pre>
data&lt;-EuStockMarkets[,1]
df.data&lt;-as.data.frame(t(matrix(data, 1)))

</pre>

<p><strong>Exercise 2</strong><br/>
Process the dataset so he can be used in a neural network.</p>


<pre>

####################
#                  #
#    Exercise 2    #
#                  #
####################
scale.0.1&lt;-function(x){
  return ((x-min(x,na.rm=TRUE))/(max(x,na.rm=TRUE)-min(x,na.rm=TRUE)))
}

scale.data&lt;-scale.0.1(df.data[1:1400,])
plot(scale.data)
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-1.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
sum(is.na(scale.data))
</pre>

<pre>
## [1] 0
</pre>
<pre>
</pre>

<p><strong>Exercise 3</strong><br/>
Create two matrix containing 10 sequences of 140 observations from the previous dataset. The first one must be made of the original observations and will be the input of our neural network. The second one will be the output and since we want to predict the value of the stock market at time t+1 based on the value at time t, this matrix will be the same as the first one were all the elements are shifted from one position. Make sure that each sequence are coded as a row of each matrix.</p>


<pre>

####################
#                  #
#    Exercise 3    #
#                  #
####################
#10 samples of 140 observations
X&lt;-matrix(scale.data[1:1400],nrow=140)
Y&lt;-matrix(c(scale.data[2:1400],0),nrow=140)

X &lt;- t(X)
Y &lt;- t(Y)

</pre>

<p><strong>Exercise 4</strong><br/>
Set the seed to 42 and choose randomly eight sequences to train your model and two sequences that will be used for validation later. Once it&#8217;s done, load the <code>rnn</code> package and use the <code>trainr()</code> function to train a recurrent neural network on the training dataset. For now, use a learning rate of 0.01, one hidden layer of one neuron and 500 epoch.</p>


<pre>

####################
#                  #
#    Exercise 4    #
#                  #
####################
library(rnn)
set.seed(42)
index&lt;-sample(1:10,8,replace=FALSE)
model.stock.1hl&lt;-trainr(Y[index,],X[index,],learningrate = 0.01, hidden_dim = 1,numepochs = 500)

</pre>

<p><strong>Exercise 5</strong><br/>
Use the function <code>predictr</code> to make prediction on all the 10 sequences of your original data matrix, then plot the real values and the predicted value on the same graph. Also draw the plot of the prediction on the test set and the real value of your dataset.</p>


<pre>

####################
#                  #
#    Exercise 5    #
#                  #
####################
pred.1hl &lt;- predictr(model.stock.1hl, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.1hl)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-2.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index,])))
points(as.vector(t(pred.1hl[-index,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-3.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>


<p><strong>Exercise 6</strong><br/>
The last model seems to underestimate the stock values that are higher than 0.5. Repeat the step of exercise 3 and 4 but this time use 10 hidden layers. Once it&#8217;s done calculate the RMSE of your predictions. This will be the baseline model for the rest of this exercise set.</p>


<pre>

####################
#                  #
#    Exercise 6    #
#                  #
####################
model.stock.10hl&lt;-trainr(Y[index,],X[index,],learningrate = 0.01, hidden_dim = 10,numepochs = 500)

pred.10hl &lt;- predictr(model.stock.10hl, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.10hl)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-4.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index,])))
points(as.vector(t(pred.10hl[-index,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-5.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
rmse.10.hl&lt;-sqrt((sum(Y[-index,]-pred.10hl[-index,])^2)/(nrow(Y[-index,])*ncol(Y[-index,])))
rmse.10.hl
</pre>

<pre>
## [1] 0.3016284
</pre>


<p><strong>Exercise 7</strong><br/>
One interesting method often used to accelerate the training of a neural network is the “Nesterov momentum”. This procedure is based on the fact that while trying to find the weights that minimize the cost function of your neural network, optimization algorithm like gradient descend &#8220;zigzag&#8221; around a straight path to the minimum value. By adding a momentum matrix, which keeps track of the general direction of the gradient, to the gradient we can minimize the deviation from this optimal path and speeding the convergence of the algorithm. You can see this <a href="https://www.youtube.com/watch?v=8yg2mRJx-z4" class="external" rel="nofollow">video</a> for more information about this concept.</p>
<p>Repeat the last exercise, but this time use 250 epochs and a momentum of 0.7.</p>


<pre>

####################
#                  #
#    Exercise 7    #
#                  #
####################
model.stock.momentum&lt;-trainr(Y[index,],X[index,],learningrate = 0.01, hidden_dim = 20,momentum=0.7, numepochs = 250)

pred.momentum &lt;- predictr(model.stock.momentum, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.momentum)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-6.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index,])))
points(as.vector(t(pred.momentum[-index,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-7.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
rmse.momentum&lt;-sqrt((sum(Y[-index,]-pred.momentum[-index,])^2)/(nrow(Y[-index,])*ncol(Y[-index,])))
rmse.momentum
</pre>

<pre>
## [1] 0.365263
</pre>

<p><strong>Exercise 8</strong><br/>
As special type of recurrent neural network trained by backpropagation through time is called the Long Short-Term Memory (LSTM) network. This type of recurrent neural network is quite useful in a deep learning context, since this method is robust again the vanishing gradient problem. We will see both of those concepts more in detail in a future exercise set, but for now you can read about it <a href="https://en.wikipedia.org/wiki/Long_short-term_memory" class="external" rel="nofollow">here</a>.</p>
<p>The <code>trainr()</code> function give us the ability to train a LSTM network by setting the <code>network_type</code> parameter to &#8220;lstm&#8221;. Use this algorithm with 500 epochs and 20 neuron in the hidden layer to predict the value of your time series.</p>


<pre>

####################
#                  #
#    Exercise 8    #
#                  #
####################
model.stock.LSTM&lt;-trainr(Y[index,],X[index,],learningrate = 0.01, hidden_dim = 20,network_type = &quot;lstm&quot;, numepochs = 500)

pred.LSTM &lt;- predictr(model.stock.LSTM, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.LSTM)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-8.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index,])))
points(as.vector(t(pred.LSTM[-index,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-9.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
rmse.LSTM&lt;-sqrt((sum(Y[-index,]-pred.LSTM[-index,])^2)/(nrow(Y[-index,])*ncol(Y[-index,])))
rmse.LSTM
</pre>

<pre>
## [1] 0.8217892
</pre>

<p><strong>Exercise 9</strong><br/>
When working with a recurrent neural network it is important to choose an input sequence length that give the algorithm the maximum information possible without adding useless noise to the input. Until now we used 10 sequences of 140 observations. Train a recurrent neural network on 28 sequences of 50 observations, make prediction and compute the RMSE to see if this encoding had an effect on your predictions.</p>


<pre>

####################
#                  #
#    Exercise 9    #
#                  #
####################
#28 samples of 50 observations
X&lt;-matrix(scale.data[1:1400],nrow=50)
Y&lt;-matrix(c(scale.data[2:1400],0),nrow=50)

X &lt;- t(X)
Y &lt;- t(Y)

set.seed(42)
index.50&lt;-sample(1:28,21,replace=FALSE)

model.stock.50&lt;-trainr(Y[index.50,],X[index.50,],learningrate = 0.01, hidden_dim = 10,numepochs = 500)

pred.50 &lt;- predictr(model.stock.50, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.50)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-10.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index.50,])))
points(as.vector(t(pred.50[-index.50,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-11.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
rmse.50&lt;-sqrt((sum(Y[-index.50,]-pred.50[-index.50,])^2)/(nrow(Y[-index.50,])*ncol(Y[-index.50,])))
rmse.50
</pre>

<pre>
## [1] 0.02927242
</pre>


<p><strong>Exercise 10</strong><br/>
Try to use all of the 1860 observation in the &#8220;DAX&#8221; time series to train and test a recurrent neural network. Then post the setting you used for your model and why you choose them in the comments.</p>

<pre>

####################
#                  #
#    Exercise 10   #
#                  #
####################
dax.data&lt;-scale.0.1(df.data)

#93 samples of 20 observations
X&lt;-matrix(dax.data[1:1860,],nrow=20)
Y&lt;-matrix(c(dax.data[2:1860,],0),nrow=20)

X &lt;- t(X)
Y &lt;- t(Y)
set.seed(42)
index.dax&lt;-sample(1:93,70,replace=FALSE)

model.dax&lt;-trainr(Y[index.dax,],X[index.dax,],learningrate = 0.01, hidden_dim = c(10,10),momentum=0.1,numepochs = 500)

pred.dax &lt;- predictr(model.dax, X)

plot(as.vector(t(Y)))
points(as.vector(t(pred.dax)),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-12.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
plot(as.vector(t(Y[-index.dax,])))
points(as.vector(t(pred.dax[-index.dax,])),col='red')
</pre>
<img src="http://r-exercises.com/wp-content/uploads/2017/06/NN_part3-13.png" title="plot of chunk NN_part3" alt="plot of chunk NN_part3" class="plot"/>
<pre>
rmse.dax&lt;-sqrt((sum(Y[-index.dax,]-pred.dax[-index.dax,])^2)/(nrow(Y[-index.dax,])*ncol(Y[-index.dax,])))
rmse.dax
</pre>

<pre>
## [1] 0.2719791
</pre>

</body></html>
