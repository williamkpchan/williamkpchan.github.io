<base target="_blank"><html><head><title>Scrape a Website Using Node.js and Puppeteer</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script type='text/javascript' src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script src="https://d3js.org/d3.v4.min.js"></script>

<script>
  var showTopicNumber = true;
  var bookid = "Scrape a Website Using Node.js and Puppeteer"
  var markerName = "h2, h3"

</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
pre{width:100%;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Scrape a Website Using Node.js and Puppeteer</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>

<pre>
<br>
<br>
<h3>Introduction</h3>
In this tutorial, you will build a web scraping application using <a href="https://www.npmjs.com/">Node.js</a> and <a href="https://pptr.dev/">Puppeteer</a>. 
First, you will code your app to open <a href="https://www.chromium.org/getting-involved/download-chromium">Chromium</a> and load a special website designed as a web-scraping sandbox: <a href="http://books.toscrape.com">books.toscrape.com</a>. 
In the next two steps, you will scrape all the books on a single page of books.toscrape and then all the books across multiple pages. 
In the remaining steps, you will filter your scraping by book category and then save your data as a JSON file.
This tutorial scrapes a special website, <a href="http://books.toscrape.com">books.toscrape.com</a>, which was specifically designed to test scraper applications. 

<h2>Prerequisites</h2>
Node.js installed on your development machine. 

<h2>Step 1 — Setting Up the Web Scraper</h2>
With Node.js installed, you can begin setting up your web scraper. 
First, you will create a project root directory and then install the required dependencies. 
This tutorial requires just one dependency, and you will install it using Node.js&rsquo;s default package manager <a href="https://www.digitalocean.com/community/tutorial_series/how-to-code-in-node-js">npm</a>. 
npm comes preinstalled with Node.js, so you don’t need to install it.
Create a folder for this project and then move inside:
<code>mkdir book-scraper
cd book-scraper</code>
You will run all subsequent commands from this directory.
We need to install one package using npm, or the node package manager. 
First initialize npm in order to create a <code>packages.json</code> file, which will manage your project&rsquo;s dependencies and metadata.
Initialize npm for your project:
<code>npm init</code>
npm will present a sequence of prompts. 
You can press <code>ENTER</code> to every prompt, or you can add personalized descriptions. 
Make sure to press <code>ENTER</code> and leave the default values in place when prompted for <code>entry point:</code> and <code>test command:</code>. 
Alternately, you can pass the <code>y</code> flag to <code>npm</code>—<code>npm init -y</code>—and it will submit all the default values for you.
Your output will look something like this:
<code>Output
{
  "name": "sammy_scraper",
  "version": "1.0.0",
  "description": "a web scraper",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"
  },
  "keywords": [],
  "author": "sammy the shark",
  "license": "ISC"
}
Is this OK? (yes) yes</code>
Type <code>yes</code> and press <code>ENTER</code>. 
npm will save this output as your <code>package.json</code> file.
Now use npm to install Puppeteer:
<code>npm install --save puppeteer</code>
This command installs both Puppeteer and a version of Chromium that the Puppeteer team knows will work with their API.

With npm, Puppeteer, and any additional dependencies installed, your <code>package.json</code> file requires one last configuration before you start coding. 
In this tutorial, you will launch your app from the command line with <code>npm run start</code>. 
You must add some information about this <code>start</code> script to <code>package.json</code>. 
Specifically, you must add one line under the <code>scripts</code> directive regarding your <code>start</code> command.
Open the file in your preferred text editor:
<code>nano package.json</code>
Find the <code>scripts:</code> section and add the following configurations. 
Remember to place a comma at the end of the <code>test</code> script line, or your file will not parse correctly. 

<code>Output
{
  . . .
  "scripts": {
    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1",
    "start": "node index.js"
  },
  . . .
  "dependencies": {
    "puppeteer": "^5.2.1"
  }
}</code>
You will also notice that <code>puppeteer</code> now appears under <code>dependencies</code> near the end of the file. 
Your <code>package.json</code> file will not require any more revisions. 
Save your changes and close your editor. 

You are now ready to start coding your scraper. 
In the next step, you will set up a browser instance and test your scraper&rsquo;s basic functionality.
<h2>Step 2 — Setting Up the Browser Instance</h2>
When you open a traditional browser, you can do things like click buttons, navigate with your mouse, type, open the dev tools, and more. 
A headless browser like Chromium allows you to do these same things, but programmatically and without a user interface. 
In this step, you will set up your scraper&rsquo;s browser instance. 
When you launch your application, it will automatically open Chromium and navigate to books.toscrape.com. 
These initial actions will form the basis of your program.
Your web scraper will require four <code>.js</code> files: <code>browser.js, index,js, pageController.js</code>, and <code>pageScraper.js</code>. 
In this step, you will create all four files and then continually update them as your program grows in sophistication. 
Start with <code>browser.js</code>; this file will contain the script that starts your browser.
From your project&rsquo;s root directory, create and open <code>browser.js</code> in a text editor:
<code>nano browser.js</code>
First, you will <code>require</code> Puppeteer and then create an <code>async</code> function called <code>startBrowser()</code>. 
This function will start the browser and return an instance of it. 
Add the following code:
./book-scraper/browser.js
<code>const puppeteer = require('puppeteer');
async function startBrowser(){
    let browser;
    try {
        console.log("Opening the browser......");
        browser = await puppeteer.launch({
            headless: false,
            args: ["--disable-setuid-sandbox"],
            'ignoreHTTPSErrors': true
        });
    } catch (err) {
        console.log("Could not create a browser instance =&gt; : ", err);
    }
    return browser;
}
module.exports = {
    startBrowser
};</code>
<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#puppeteerlaunchoptions">Puppeteer has a <code>.launch()</code> method</a> that launches an instance of a browser. 
This method returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>, so you have to <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Async_await">make sure the Promise resolves by using a <code>.then</code> or <code>await</code> block</a>.
You are using <code>await</code> to make sure the Promise resolves, wrapping this instance around <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch">a <code>try-catch</code> code block</a>, and then returning an instance of the browser.
Notice that the <code>.launch()</code> method takes a JSON parameter with several values:
<strong>headless</strong> - <code>false</code> means the browser will run with an Interface so you can watch your script execute, while <code>true</code> means the browser will run in headless mode. 
Note well, however, that if you want to deploy your scraper to the cloud, set <code>headless</code> back to <code>true</code>. 
Most virtual machines are headless and do not include a user interface, and hence can only run the browser in headless mode. 
Puppeteer also includes a <code>headful</code> mode, but that should be used solely for testing purposes.
<strong>ignoreHTTPSErrors</strong> - <code>true</code> allows you to visit websites that aren&rsquo;t hosted over a secure HTTPS protocol and ignore any HTTPS-related errors.
Save and close the file.
Now create your second <code>.js</code> file, <code>index.js</code>:
<code>nano index.js</code>
Here you will <code>require</code> <code>browser.js</code> and <code>pageController.js</code>. 
You will then call the <code>startBrowser()</code> function and pass the created browser instance to our page controller, which will direct its actions. 
Add the following code:
./book-scraper/index.js
<code>const browserObject = require('./browser');
const scraperController = require('./pageController');
//Start the browser and create a browser instance
let browserInstance = browserObject.startBrowser();
// Pass the browser instance to the scraper controller
scraperController(browserInstance)</code>
Save and close the file.
Create your third <code>.js</code> file, <code>pageController.js</code>:
<code>nano pageController.js</code>
<code>pageController.js</code> controls your scraping process. 
It uses the browser instance to control the <code>pageScraper.js</code> file, which is where all the scraping scripts execute. 
Eventually, you will use it to specify what book category you want to scrape. 
For now, however, you just want to make sure that you can open Chromium and navigate to a web page:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        await pageScraper.scraper(browser); 
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
This code exports a function that takes in the browser instance and passes it to a function called <code>scrapeAll()</code>. 
This function, in turn, passes this instance to <code>pageScraper.scraper()</code> as an argument which uses it to scrape pages.
Save and close the file.
Finally, create your last <code>.js</code> file, <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Here you will create an object literal with a <code>url</code> property and a <code>scraper()</code> method. 
The <code>url</code> is the web URL of the web page you want to scrape, while the <code>scraper()</code> method contains the code that will perform your actual scraping, although at this stage it merely navigates to a URL. 
Add the following code:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        await page.goto(this.url);
    }
}
module.exports = scraperObject;</code>
<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#browsernewpage">Puppeteer has a <code>newPage()</code> method</a> that creates a new page instance in the browser, and these page instances can do quite a few things. 
In our <code>scraper()</code> method, you created a page instance and then used the <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagegotourl-options"><code>page.goto()</code> method</a> to navigate to <a href="http://books.toscrape.com">the books.toscrape.com homepage</a>.
Save and close the file.
Your program&rsquo;s file-structure is now complete. 
The first level of your project&rsquo;s directory tree will look like this:
<code>Output.
├── browser.js
├── index.js
├── node_modules
├── package-lock.json
├── package.json
├── pageController.js
└── pageScraper.js</code>
Now run the command <code>npm run start</code> and watch your scraper application execute:
<code>npm run start</code>
It will automatically open a Chromium browser instance, open a new page in the browser, and navigate to books.toscrape.com.
In this step, you created a Puppeteer application that opened Chromium and loaded the homepage for a dummy online bookstore—books.toscrape.com. 
In the next step, you will scrape the data for every book on that homepage.
<h2>Step 3 — Scraping Data from a Single Page</h2>
Before adding more functionality to your scraper application, open your preferred web browser and manually navigate to the <a href="http://books.toscrape.com/">books to scrape homepage</a>. 
Browse the site and get a sense of how data is structured.
<img src="https://assets.digitalocean.com/articles/67187/web_scraper.png" alt="Books to scrape websites image">
You will find a category section on the left and books displayed on the right. 
When you click on a book, the browser navigates to a new URL that displays relevant information regarding that particular book.
In this step, you will replicate this behavior, but with code; you will automate the business of navigating the website and consuming its data.
First, if you inspect the source code for the homepage using the Dev Tools inside your browser, you will notice that the page lists each book&rsquo;s data under a <code>section</code> tag. 
Inside the <code>section</code> tag every book is under a <code>list</code> (<code>li</code>) tag, and it is here that you find the link to the book&rsquo;s dedicated page, the price, and the in-stock availability.
<img src="https://assets.digitalocean.com/articles/67187/bookstoscrape_devtools.png" alt="books.toscrape source code viewed in dev tools">
You&rsquo;ll be scraping these book URLs, filtering for books that are in-stock, navigating to each individual book page, and scraping that book&rsquo;s data.
Reopen your <code>pageScraper.js</code> file:
<code>nano pageScraper.js</code>
Add the following highlighted content. 
You will nest another <code>await</code> block inside <code>await page.goto(this.url);</code>:
./book-scraper/pageScraper.js
<code>
const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Wait for the required DOM to be rendered
        await page.waitForSelector('.page_inner');
        // Get the link to all the required books
        let urls = await page.$$eval('section ol &gt; li', links =&gt; {
            // Make sure the book to be scraped is in stock
            links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
            // Extract the links from the data
            links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
            return links;
        });
        console.log(urls);
    }
}
module.exports = scraperObject;</code>
In this code block, you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagewaitforselectorselector-options">the <code>page.waitForSelector()</code> method</a>. 
This waited for the div that contains all the book-related information to be rendered in the DOM, and then you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pageevalselector-pagefunction-args">the <code>page.$$eval()</code> method</a>. 
This method gets the URL element with the selector <code>section ol li</code> (be sure that you always return only a string or a number from the <code>page.$eval()</code> and <code>page.$$eval()</code> methods).
Every book has two statuses; a book is either <code>In Stock</code> or <code>Out of stock</code>. 
You only want to scrape books that are <code>In Stock</code>. 
Because <code>page.$$eval()</code> returns an array of all matching elements, you have filtered this array to ensure that you are only working with in-stock books. 
You did this by searching for and evaluating the class <code>.instock.availability</code>. 
You then mapped out the <code>href</code> property of the book links and returned it from the method.
Save and close the file.
Re-run your application:
<code>npm run start</code>
The browser will open, navigate to the web page, and then close once the task completes. 
Now check your console; it will contain all the scraped URLs:
<code>Output
&gt; book-scraper@1.0.0 start /Users/sammy/book-scraper
&gt; node index.js
Opening the browser......
Navigating to http://books.toscrape.com...
[
  'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',
  'http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',
  'http://books.toscrape.com/catalogue/soumission_998/index.html',
  'http://books.toscrape.com/catalogue/sharp-objects_997/index.html',
  'http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',
  'http://books.toscrape.com/catalogue/the-requiem-red_995/index.html',
  'http://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',
  'http://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',
  'http://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',
  'http://books.toscrape.com/catalogue/the-black-maria_991/index.html',
  'http://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',
  'http://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',
  'http://books.toscrape.com/catalogue/set-me-free_988/index.html',
  'http://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',
  'http://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',
  'http://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',
  'http://books.toscrape.com/catalogue/olio_984/index.html',
  'http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',
  'http://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',
  'http://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html'
]</code>
This is a great start, but you want to scrape all the relevant data for a particular book and not only its URL. 
You will now use these URLs to open each page and scrape the book&rsquo;s title, author, price, availability, UPC, description, and image URL.
Reopen <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Add the following code, which will loop through each scraped link, open a new page instance, and then retrieve the relevant data:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Wait for the required DOM to be rendered
        await page.waitForSelector('.page_inner');
        // Get the link to all the required books
        let urls = await page.$$eval('section ol &gt; li', links =&gt; {
            // Make sure the book to be scraped is in stock
            links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
            // Extract the links from the data
            links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
            return links;
        });
        // Loop through each of those links, open a new page instance and get the relevant data from them
        let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
            let dataObj = {};
            let newPage = await browser.newPage();
            await newPage.goto(link);
            dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
            dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
            dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                // Strip new line and tab spaces
                text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                // Get the number of stock available
                let regexp = /^.*\((.*)\).*$/i;
                let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                return stockAvailable;
            });
            dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
            dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
            dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
            resolve(dataObj);
            await newPage.close();
        });
        for(link in urls){
            let currentPageData = await pagePromise(urls[link]);
            // scrapedData.push(currentPageData);
            console.log(currentPageData);
        }
    }
}
module.exports = scraperObject; </code>
You have an array of all URLs. 
You want to loop through this array, open up the URL in a new page, scrape data on that page, close that page, and open a new page for the next URL in the array. 
Notice that you wrapped this code in a Promise. 
This is because you want to be able to wait for each action in your loop to complete. 
Therefore each Promise opens a new URL and won&rsquo;t resolve until the program has scraped all the data on the URL, and then that page instance has closed.
<strong>Warning:</strong> note well that you waited for the Promise using a <code>for-in</code> loop. 
Any other loop will be sufficient but avoid iterating over your URL arrays using an array-iteration method like <code>forEach</code>, or any other method that uses a callback function. 
This is because the callback function will have to go through the callback queue and event loop first, hence, multiple page instances will open all at once. 
This will place a much larger strain on your memory.

Take a closer look at your <code>pagePromise</code> function. 
Your scraper first created a new page for each URL, and then you used the <code>page.$eval()</code> function to target selectors for relevant details that you wanted to scrape on the new page. 
Some of the texts contain whitespaces, tabs, newlines, and other non-alphanumeric characters, which you stripped off using a regular expression. 
You then appended the value for every piece of data scraped in this page to an Object and resolved that object.
Save and close the file.
Run the script again:
<code>npm run start</code>
The browser opens the homepage and then opens each book page and logs the scraped data from each of those pages. 
This output will print to your console:
<code>Output
Opening the browser......
Navigating to http://books.toscrape.com...
{
  bookTitle: 'A Light in the Attic',
  bookPrice: '£51.77',
  noAvailable: '22',
  imageUrl: 'http://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg',
  bookDescription: "It's hard to imagine a world without A Light in the Attic. 
[...]',
  upc: 'a897fe39b1053632'
}
{
  bookTitle: 'Tipping the Velvet',
  bookPrice: '£53.74',
  noAvailable: '20',
  imageUrl: 'http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg',
  bookDescription: `"Erotic and absorbing...Written with starling power."--"The New York Times Book Review " Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler [...]`,
  upc: '90fa61229261140a'
}
{
  bookTitle: 'Soumission',
  bookPrice: '£50.10',
  noAvailable: '20',
  imageUrl: 'http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg',
  bookDescription: 'Dans une France assez proche de la nôtre, [...]',
  upc: '6957f44c3847a760'
}
...</code>
In this step, you scraped relevant data for every book on the homepage of books.toscrape.com, but you could add much more functionality. 
Each page of books, for instance, is paginated; how do you get books from these other pages? Also, on the left side of the website you found book categories; what if you don&rsquo;t want all the books, but you just want books from a particular genre? You will now add these features.
<h2>Step 4 — Scraping Data From Multiple Pages</h2>
Pages on books.toscrape.com that are paginated have a <code>next</code> button beneath their content, while pages that are not paginated do not.
You will use the presence of this button to determine if the page is paginated or not. 
Since the data on each page is of the same structure and has the same markup, you won&rsquo;t be writing a scraper for every possible page. 
Rather, you will use the practice of <a href="https://www.digitalocean.com/community/tutorials/js-understanding-recursion">recursion</a>.
First, you need to change the structure of your code a bit to accommodate recursively navigating to several pages.
Reopen <code>pagescraper.js</code>:
<code>nano pagescraper.js</code>
You will add a new function called <code>scrapeCurrentPage()</code> to your <code>scraper()</code> method. 
This function will contain all the code that scrapes data from a particular page and then click the next button if it exists. 
Add the following highlighted code:
./book-scraper/pageScraper.js scraper()
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        let scrapedData = [];
        // Wait for the required DOM to be rendered
        async function scrapeCurrentPage(){
            await page.waitForSelector('.page_inner');
            // Get the link to all the required books
            let urls = await page.$$eval('section ol &gt; li', links =&gt; {
                // Make sure the book to be scraped is in stock
                links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
                // Extract the links from the data
                links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
                return links;
            });
            // Loop through each of those links, open a new page instance and get the relevant data from them
            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
                let dataObj = {};
                let newPage = await browser.newPage();
                await newPage.goto(link);
                dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
                dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
                dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                    // Strip new line and tab spaces
                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                    // Get the number of stock available
                    let regexp = /^.*\((.*)\).*$/i;
                    let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                    return stockAvailable;
                });
                dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
                dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
                dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
                resolve(dataObj);
                await newPage.close();
            });
            for(link in urls){
                let currentPageData = await pagePromise(urls[link]);
                scrapedData.push(currentPageData);
                // console.log(currentPageData);
            }
            // When all the data on this page is done, click the next button and start the scraping of the next page
            // You are going to check if this button exist first, so you know if there really is a next page.
            let nextButtonExist = false;
            try{
                const nextButton = await page.$eval('.next &gt; a', a =&gt; a.textContent);
                nextButtonExist = true;
            }
            catch(err){
                nextButtonExist = false;
            }
            if(nextButtonExist){
                await page.click('.next &gt; a');   
                return scrapeCurrentPage(); // Call this function recursively
            }
            await page.close();
            return scrapedData;
        }
        let data = await scrapeCurrentPage();
        console.log(data);
        return data;
    }
}
module.exports = scraperObject;</code>
You set the <code>nextButtonExist</code> variable to false initially, and then check if the button exists. 
If the <code>next</code> button exists, you set <code>nextButtonExists</code> to <code>true</code> and proceed to click the <code>next</code> button, and then call this function recursively.
If <code>nextButtonExists</code> is false, it returns the <code>scrapedData</code> array as usual.
Save and close the file.
Run your script again:
<code>npm run start</code>
This might take a while to complete; your application, after all, is now scraping the data from over 800 books. 
Feel free to either close the browser or press <code>CTRL + C</code> to cancel the process.
You have now maximized your scraper&rsquo;s capabilities, but you&rsquo;ve created a new problem in the process. 
Now the issue is not too little data but too much data. 
In the next step, you will fine-tune your application to filter your scraping by book category.
<h2>Step 5 — Scraping Data by Category</h2>
To scrape data by category, you will need to modify both your <code>pageScraper.js</code> file and your <code>pageController.js</code> file.
Open <code>pageController.js</code> in a text editor:
<code>nano pageController.js</code>
Call the scraper so that it only scrapes travel books. 
Add the following code:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        await browser.close();
        console.log(scrapedData)
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
You are now passing two parameters into your <code>pageScraper.scraper()</code> method, with the second parameter being the category of books you want to scrape, which in this example is <code>Travel</code>. 
But your <code>pageScraper.js</code> file does not recognize this parameter yet. 
You will need to adjust this file, too.
Save and close the file.
Open <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Add the following code, which will add your category parameter, navigate to that category page, and then begin scraping through the paginated results:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser, category){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Select the category of book to be displayed
        let selectedCategory = await page.$$eval('.side_categories &gt; ul &gt; li &gt; ul &gt; li &gt; a', (links, _category) =&gt; {
            // Search for the element that has the matching text
            links = links.map(a =&gt; a.textContent.replace(/(\r\n\t|\n|\r|\t|^\s|\s$|\B\s|\s\B)/gm, "") === _category ? a : null);
            let link = links.filter(tx =&gt; tx !== null)[0];
            return link.href;
        }, category);
        // Navigate to the selected category
        await page.goto(selectedCategory);
        let scrapedData = [];
        // Wait for the required DOM to be rendered
        async function scrapeCurrentPage(){
            await page.waitForSelector('.page_inner');
            // Get the link to all the required books
            let urls = await page.$$eval('section ol &gt; li', links =&gt; {
                // Make sure the book to be scraped is in stock
                links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
                // Extract the links from the data
                links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
                return links;
            });
            // Loop through each of those links, open a new page instance and get the relevant data from them
            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
                let dataObj = {};
                let newPage = await browser.newPage();
                await newPage.goto(link);
                dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
                dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
                dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                    // Strip new line and tab spaces
                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                    // Get the number of stock available
                    let regexp = /^.*\((.*)\).*$/i;
                    let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                    return stockAvailable;
                });
                dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
                dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
                dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
                resolve(dataObj);
                await newPage.close();
            });
            for(link in urls){
                let currentPageData = await pagePromise(urls[link]);
                scrapedData.push(currentPageData);
                // console.log(currentPageData);
            }
            // When all the data on this page is done, click the next button and start the scraping of the next page
            // You are going to check if this button exist first, so you know if there really is a next page.
            let nextButtonExist = false;
            try{
                const nextButton = await page.$eval('.next &gt; a', a =&gt; a.textContent);
                nextButtonExist = true;
            }
            catch(err){
                nextButtonExist = false;
            }
            if(nextButtonExist){
                await page.click('.next &gt; a');   
                return scrapeCurrentPage(); // Call this function recursively
            }
            await page.close();
            return scrapedData;
        }
        let data = await scrapeCurrentPage();
        console.log(data);
        return data;
    }
}
module.exports = scraperObject;</code>
This code block uses the category that you passed in to get the URL where the books of that category reside.
The <code>page.$$eval()</code> can take in arguments by passing the argument as a third parameter to the <code>$$eval()</code> method, and defining it as the third parameter in the callback as such:
example page.$$eval() function
<code>page.$$eval('selector', function(elem, args){
    // .......
}, args)</code>
This was what you did in your code; you passed the category of books you wanted to scrape, mapped through all the categories to check which one matches, and then returned the URL of this category.
This URL is then used to navigate to the page that displays the category of books you want to scrape using the <code>page.goto(selectedCategory)</code> method.
Save and close the file.
Run your application again. 
You will notice that it navigates to the <code>Travel</code> category, recursively opens books in that category page by page, and logs the results:
<code>npm run start</code>
In this step, you scraped data across multiple pages and then scraped data across multiple pages from one particular category. 
In the final step, you will modify your script to scrape data across multiple categories and then save this scraped data to a stringified JSON file.
<h2>Step 6 — Scraping Data from Multiple Categories and Saving the Data as JSON</h2>
In this final step, you will make your script scrape data off of as many categories as you want and then change the manner of your output. 
Rather than logging the results, you will save them in a structured file called <code>data.json</code>.
You can quickly add more categories to scrape; doing so requires only one additional line per genre.
Open <code>pageController.js</code>:
<code>nano pageController.js</code>
Adjust your code to include additional categories. 
The example below adds <code>HistoricalFiction</code> and <code>Mystery</code> to our existing <code>Travel</code> category:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        scrapedData['HistoricalFiction'] = await pageScraper.scraper(browser, 'Historical Fiction');
        scrapedData['Mystery'] = await pageScraper.scraper(browser, 'Mystery');
        await browser.close();
        console.log(scrapedData)
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
Save and close the file.
Run the script again and watch it scrape data for all three categories:
<code>npm run start</code>
With the scraper fully-functional, your final step involves saving your data in a more useful format. 
You will now store it in a JSON file using <a href="https://nodejs.org/api/fs.html">the <code>fs</code> module in Node.js</a>.
First, reopen <code>pageController.js</code>:
<code>nano pageController.js</code>
Add the following highlighted code:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
const fs = require('fs');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        scrapedData['HistoricalFiction'] = await pageScraper.scraper(browser, 'Historical Fiction');
        scrapedData['Mystery'] = await pageScraper.scraper(browser, 'Mystery');
        await browser.close();
        fs.writeFile("data.json", JSON.stringify(scrapedData), 'utf8', function(err) {
            if(err) {
                return console.log(err);
            }
            console.log("The data has been scraped and saved successfully! View it at './data.json'");
        });
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
First, you are requiring Node,js&rsquo;s <code>fs</code> module in <code>pageController.js</code>. 
This ensures that you can save your data as a JSON file. 
Then you are adding code so that when the scraping completes and the browser closes, the program will create a new file called <code>data.json</code>. 
Note that the contents of <code>data.json</code> are <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify">stringified JSON</a>. 
Therefore, when reading the content of <code>data.json</code>, always <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON">parse it as JSON</a> before reusing the data.
Save and close the file.
You have now built a web-scraping application that scrapes books across multiple categories and then stores your scraped data in a JSON file. 
As your application grows in complexity, you might want to store this scraped data in a database or serve it over an API. 
How this data is consumed is really up to you.
<h2>Conclusion</h2>
In this tutorial, you built a web crawler that scraped data across multiple pages recursively and then saved it in a JSON file. 
In short, you learned a new way to automate data-gathering from websites.
Puppeteer has quite a lot of features that were not within the scope of this tutorial. 
To learn more, check out <a href="https://www.digitalocean.com/community/tutorials/tooling-puppeteer">Using Puppeteer for Easy Control Over Headless Chrome</a>. 
You can also visit <a href="http://pptr.dev/">Puppeteer&rsquo;s official documentation</a>.
<script type='text/javascript' src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>

var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
