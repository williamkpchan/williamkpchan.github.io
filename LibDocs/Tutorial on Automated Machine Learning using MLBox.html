
<base target="_blank">


<head>


<meta http-equiv="Content-Type" content="text/html; charset=utf-8">



<style type="text/css">



body {
 margin: 10%;
 font-size: 20px;
 background-color: #000000;
 color: #109030;
}
a { text-decoration: none;
	color: #28B8B8;}
a:visited {	color: #389898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: green; background-color: #002000}
pre { color: gray; background-color: #001010}
}
</style>


</head>



<body>

<h1 itemprop="name" class="entry-title">
Tutorial on Automated Machine Learning using MLBox</h1>

<h2>
Introduction</h2>

<p>
Recently, one of my friends and I were solving a practice problem. <strong>
After 8 hours of hard work &amp; coding, </strong>
my friend Shubham got a score of 1153 (position 219). Here is his position on leaderboard:</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03232136/score.png" width=1000>
</p>

<p>
On the other hand<strong>
, I was able to achieve this by writing only 8 lines of code:</strong>
</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03231600/Screenshot-from-2017-07-03-23-15-37.png" width=1000>
</p>

<p>
How did I get there?</p>

<p>
What if I tell you there exists a library called <strong>
<a href="https://github.com/AxeldeRomblay/MLBox" target="_blank" rel="noopener noreferrer">
MLBox</a>
</strong>
 , which does most of the heavy lifting in machine learning for you in minimal lines of code? From missing value imputation to feature engineering using state-of-the-art <em>
Entity Embeddings </em>
for categorical features, MLBox has it all.</p>

<p>
In these 8 lines of code using MLBox, I have also performed hyperparameter optimisation and tested around 50 models with blazing speed &#8211; isn&#8217;t that awesome? You will be able to use this library by end of this article.</p>

<p>
&nbsp;</p>

<h2>
Table of Contents</h2>

<ol>

<li>
What is MLBox?</li>

<li>
MLBox in comparison to other Machine Learning libraries.</li>

<li>
Installing MLBox</li>

<li>
Layout/Pipeline of the MLBox</li>

<li>
Building a Machine Learning Regressor using MLBox</li>

<li>
Basic Understanding of Drift</li>

<li>
Basic Understanding of Entity Embedding</li>

<li>
Pros and Cons of MLBox</li>

<li>
End Notes</li>

</ol>

<p>
&nbsp;</p>

<h2>
1. What is MLBox?</h2>

<p>
According to the developer of MLBox,</p>

<p>
<strong>
&#8220;<em>
MLBox is a powerful Automated Machine Learning Python library.</em>
</strong>
<em>
 It provides the following features:</em>
</p>

<ul>

<li>
<em>
Fast reading and distributed data preprocessing/cleaning/formatting</em>
</li>

<li>
<em>
Highly robust feature selection and leak detection</em>
</li>

<li>
<em>
Accurate hyper-parameter optimisation in high-dimensional space</em>
</li>

<li>
<em>
State-of-the-art predictive models for classification and regression (Deep Learning, Stacking, LightGBM,&#8230;)</em>
</li>

<li>
<em>
Prediction with models interpretation</em>
&#8220;</li>

</ul>

<p>
&nbsp;</p>

<h2>
2. MLBox in comparison to the other Machine Learning Libraries</h2>

<p>
MLBox focuses on the below three points in particular in comparison to the other libraries:</p>

<ol>

<li>
Drift Identification &#8211; A method to make the distribution of train data similar to the test data.</li>

<li>
Entity Embedding &#8211; A categorical features encoding technique inspired from word2vec.</li>

<li>
 Hyperparameter Optimization</li>

</ol>

<p>
We will be studying about these below in some detail to have an idea about what they do.</p>

<p>
&nbsp;</p>

<h2>
3. Installing MLBox</h2>

<p>
MLBox is currently available for Linux only. MLBox was primarily developed using Python 2 and last night it was extended to Python 3. We will be installing the latest 3.0 dev version of MLBox. Follow the below steps to install MLBox into your Linux System.</p>

<ol>

<li>
Create a new conda environment with Python 3.x and anaconda using the command below.<br />

<code>
conda create -n Python3 python=3 anaconda    #Here Python3 is the name of the #environment that we just created.</code>
</li>

<li>
Activate the Python3 environment using the command below.<br />

<code>
source activate Python3</code>
</li>

<li>
Download the MLBox tar file using-<br />

<code>
curl  -OL https://github.com/AxeldeRomblay/mlbox/tarball/3.0-dev</code>
</li>

<li>
Extract the downloaded tar file using<br />

<code>
sudo tar -xzvf 3.0-dev</code>
</li>

<li>
Go to the following directory<br />

<code>
cd AxeldeRomblay-MLBox-2befaee</code>
</li>

<li>
Install the MLBox package using the below commands<br />

<code>
cd python-package<br />

cd dist</code>
<br />

<code>
pip install *.whl</code>
</li>

<li>
Install additional libraries using<br />

<code>
pip install lightgbm</code>
<br />

<code>
pip install xgboost</code>
<br />

<code>
pip install hyperopt</code>
<br />

<code>
pip install tensorflow</code>
<br />

<code>
pip install keras</code>
<br />

<code>
pip install matplotlib</code>
<br />

<code>
pip install ipyparallel</code>
<br />

<code>
pip install pandas</code>
</li>

<li>
Check whether the MLBox has been properly installed by using the following commands<br />

<code>
python</code>
<br />

<code>
import</code>
 <code>
mlbox<br />

</code>
If the mlbox library is loaded without any error, you have successfully installed the mlbox library. Next, we will go ahead awnd install some additional libraries that MLBox uses under the hood.</li>

</ol>

<p>
<strong>
Note</strong>
 &#8211; <em>
This library is currently under very active development and therefore there may be the cases that something that works now may break the next day. For example, this library worked pretty well till 2 days ago for Python 2.7 and didn&#8217;t work so good for Python 3.6. But at the time of writing, I am experiencing some issues with the 2.7 version and the 3 version is working fine for now. Also please feel free to open issues on the github repository and asking for help in the comments below.</em>
</p>

<p>
&nbsp;</p>

<h2>
4. Layout / Pipeline of MLBox</h2>

<p>
The entire pipeline of MLBox looks like below-</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03230616/Screenshot-from-2017-07-03-23-05-23.png" width=1000>
</p>

<p>
&nbsp;</p>

<p>
The entire pipeline of MLBox has been divided into 3 sections/sub-packages.</p>

<ol>

<li>
Pre-Processing</li>

<li>
Optimisation</li>

<li>
Prediction</li>

</ol>

<p>
We will be studying about these 3 sub-packages in detail below.</p>

<p>
&nbsp;</p>

<h3>
Pre-Processing</h3>

<p>
All the functionalities inside this sub-package can be used via the command-<br />

<code>
from mlbox.preprocessing import *</code>
</p>

<p>
This sub-package provides functionalities related to two major functions.</p>

<ol>

<li>

<h4>
Reading and cleaning a file</h4>

<p>
This package supports reading a wide variety of file formats like csv, Excel, hdf5, JSON etc. but in this article, we will be primarily seeing the most common <strong>
&#8220;.csv&#8221; </strong>
file format. Follow the below steps to read a csv file.</p>

<p>
<strong>
Step1:</strong>
 Create an object of the Reader class with the separator as a parameter. &#8220;,&#8221; is the separator in the case of a csv file.<br />

<code>
s=","</code>
<br />

<code>
r=Reader(s)   #initialising the object of Reader Class</code>
</p>

<p>
<strong>
Step2:</strong>
 Make a list of the train and test file paths and also identify the target variable name.<br />

<code>
path=["path of the train csv file","path of the test csv file "]</code>
<br />

<code>
target_name="name of the target variable in the train file"<br />

</code>
<br />

<strong>
Step3:</strong>
 Performing the cleaning operation and creating a cleaned train and test file.<code>
<br />

data=r.train_test_split(path,target_name)<br />

</code>
The cleaning steps performed in the above step are-<br />

-deleting unnamed columns<br />

-removing duplicates<br />

-extracting month, year and day of the week from a Date column</p>

<p>
<code>
</code>
</li>

<li>

<h4>
Removing the Drifting Variables</h4>

<p>
The drifting Variables are explained in the later section. To remove the drifting variables, follow the below steps.</p>

<p>
<strong>
Step1:</strong>
 Create an object of class Drift_thresholder<br />

<code>
<code>
dft=Drift_thresholder()</code>
</code>
</p>

<p>
<strong>
Step2:</strong>
 Use the fit_transform method of the created object to remove the drift variables.<br />

<code>
data=dft.fit_transform(data)</code>
</li>

</ol>

<p>
&nbsp;</p>

<h3>
Optimisation</h3>

<p>
All the functionalities inside this sub-package can be used via the command-<br />

<code>
from mlbox.optimisation import *</code>
</p>

<p>
This is the section where this library scores the maximum points. This hyper-parameter optimisation method in this library uses the <strong>
hyperopt </strong>
library which is very fast and you can almost optimise anything in this library from choosing the right missing value imputation method to the depth of an XGBOOST model. This library creates a high-dimensional space of the parameters to be optimised and chooses the best combination of the parameters that lowers the validation score.</p>

<p>
Below is the table of the four broad optimisations that are done in the MLBox library with terms to the right of hyphen that can be optimised for different values.</p>

<p>
<strong>
Missing Values Encoder(ne)</strong>
 &#8211; <strong>
numerical_strategy </strong>
(when the column to be imputed is a continuous column eg- mean, median etc), <strong>
categorical_strategy</strong>
(when the column to be imputed is a categorical column e.g.- NaN values etc)</p>

<p>
<strong>
Categorical Values Encoder(ce)</strong>
&#8211; <strong>
strategy </strong>
(method of encoding categorical variables e.g.- label_encoding, dummification, random_projection, entity_embedding)</p>

<p>
<strong>
Feature Selector(fs)</strong>
&#8211; <strong>
strategy </strong>
(different methods for feature selection e.g. l1, variance, rf_feature_importance), <strong>
threshold </strong>
(the percentage of features to be discarded)</p>

<p>
<strong>
Estimator(est)</strong>
&#8211;<strong>
strategy </strong>
(different algorithms that can be used as estimators eg- LightGBM, xgboost etc.), <strong>
**params</strong>
(parameters specific to the algorithm being used eg- max_depth, n_estimators etc.)</p>

<p>
Let us take an example and create a hyperparameter space to be optimised. Let us state all the parameters that I want to optimise:</p>

<p>
Algorithm to be used- <strong>
LightGBM</strong>
<br />

LightGBM max_depth-<strong>
[3,5,7,9]</strong>
<br />

LightGBM n_estimators-<strong>
[250,500,700,1000]</strong>
<br />

Feature selection-<strong>
[variance, l1, random forest feature importance]</strong>
<br />

Missing values imputation &#8211; <strong>
numerical(mean,median),categorical(NAN values)</strong>
<br />

categorical values encoder- <strong>
label encoding, entity embedding and random projection</strong>
</p>

<p>
Let us now create our hyper-parameter space. Before that, remember, hyper-parameter is a dictionary of key and value pairs where value is also a dictionary given by the syntax<br />

{&#8220;search&#8221;:strategy,&#8221;space&#8221;:list}, where strategy can be either &#8220;choice&#8221; or &#8220;uniform&#8221; and list is the list of values.</p>

<p>
<code>
<strong>
space</strong>
={'<strong>
ne_numerical_strategy'</strong>
:{"search":"choice","space":['mean','median']},</code>
<br />

<code>
<strong>
'ne_categorical_strategy'</strong>
:{"search":"choice","space":[np.NaN]},</code>
<br />

<code>
<strong>
'ce_strategy'</strong>
:{"search":"choice","space":['label_encoding','entity_embedding','random_projection']},</code>
<br />

<code>
<strong>
'fs_strategy'</strong>
:{"search":"choice","space":['l1','variance','rf_feature_importance']},</code>
<br />

<code>
<strong>
'fs_threshold'</strong>
:{"search":"uniform","space":[0.01, 0.3]},</code>
<br />

<code>
<strong>
'est_max_depth'</strong>
:{"search":"choice","space":[3,5,7,9]},</code>
<br />

<code>
<strong>
'est_n_estimators'</strong>
:{"search":"choice","space":[250,500,700,1000]}}</code>
</p>

<p>
Now we will see the steps to choose the best combination from the above space using the following steps:</p>

<p>
<strong>
Step1</strong>
: Create an object of class Optimiser which has the parameters as <strong>
&#8216;scoring&#8217;</strong>
 and <strong>
&#8216;n_folds&#8217;. </strong>
Scoring is the metric against which we want to optimise our hyper-parameter space and n_folds is the number of folds of cross-validation<br />

Scoring values for Classification- <em>
<code>
"accuracy"</code>
, <code>
"roc_auc"</code>
, <code>
"f1"</code>
, <code>
"log_loss"</code>
, <code>
"precision"</code>
,</em>
 <em>
<code>
"recall"<br />

</code>
</em>
<code>
Scoring values for Regression- </code>
<em>
<code>
"mean_absolute_error", "mean_squarred_error", "median_absolute_error", "r2"<br />

</code>
</em>
<code>
opt=Optimiser(scoring="accuracy",n_folds=5)</code>
</p>

<p>
<strong>
Step2</strong>
: Use the optimise function of the object created above which takes the hyper-parameter space, dictionary created by the train_test_split and number of iterations as the parameters. This function returns the best hyper-paramters from the hyper-parameter space.<br />

<code>
best=opt.optimise(space,data,40)</code>
</p>

<p>
&nbsp;</p>

<h3>
Prediction</h3>

<p>
All the functions in this sub-package can be installed using the command below.<br />

<code>
from mlbox.prediction import *</code>
</p>

<p>
This sub-package predicts on the test dataset using the best hyper-parameters calculated using the <strong>
optimisation</strong>
 sub-package. To predict on the test dataset, go through the following steps.</p>

<p>
<strong>
Step1</strong>
: Create an object of class Predictor<br />

pred=Predictor()</p>

<p>
<strong>
Step2</strong>
: Use the fit_predict method of the object created above which takes a set of hyperparameters and dictionary created through train_test_split as the parameter.<br />

pred.fit_predict(best,data)</p>

<p>
The above method saves the <em>
feature importance</em>
, <em>
drift variables coefficients</em>
 and <em>
the final predictions</em>
 into a separate folder named &#8216;save&#8217;.</p>

<p>
&nbsp;</p>

<h2>
5. Building a Machine Learning Regressor using MLBox</h2>

<p>
We are now going to build a Machine Learning Classifier in just 7 lines of code with hyperparameter optimisation. We are going to solve the <a href="https://datahack.analyticsvidhya.com/contest/practice-problem-big-mart-sales-iii/">
Big Marts sales problem</a>
. Download the train and test file and keep them in a single folder. Using the MLBox library, we are going to submit our first prediction without even having to look at the data. You can find the code below to make the prediction for the problem.</p>

<p>
<strong>
<em>
<code>
# coding: utf-8</code>
</em>
</strong>
</p>

<p>
<strong>
<em>
<code>
# importing the required libraries</code>
</em>
</strong>
<br />

<code>
from mlbox.preprocessing import *</code>
<br />

<code>
from mlbox.optimisation import *</code>
<br />

<code>
from mlbox.prediction import *</code>
</p>

<p>
<code>
</code>
<strong>
<em>
<code>
# reading and cleaning the train and test files</code>
</em>
</strong>
<br />

<code>
df=Reader(sep=",").train_test_split(['/home/nss/Downloads/mlbox_blog/train.csv',</code>
</p>

<p>
<code>
 '/home/nss/Downloads/mlbox_blog/test.csv'],'Item_Outlet_Sales')</code>
</p>

<p>
<code>
</code>
<strong>
<em>
<code>
# removing the drift variables</code>
</em>
</strong>
<br />

<code>
df=Drift_thresholder().fit_transform(df)</code>
</p>

<p>
<strong>
<em>
<code>
# setting the hyperparameter space</code>
</em>
</strong>
<br />

<code>
space={'ne__numerical_strategy':{"search":"choice","space":['mean','median']},</code>
<br />

<code>
'ne__categorical_strategy':{"search":"choice","space":[np.NaN]},</code>
<br />

<code>
'ce__strategy':{"search":"choice","space":['label_encoding','entity_embedding','random_projection']},</code>
<br />

<code>
'fs__strategy':{"search":"choice","space":['l1','variance','rf_feature_importance']},</code>
<br />

<code>
'fs__threshold':{"search":"uniform","space":[0.01, 0.3]},</code>
<br />

<code>
'est__max_depth':{"search":"choice","space":[3,5,7,9]},</code>
<br />

<code>
'est__n_estimators':{"search":"choice","space":[250,500,700,1000]}}</code>
</p>

<p>
<strong>
<em>
<code>
# calculating the best hyper-parameter</code>
</em>
</strong>
<br />

<code>
best=Optimiser(scoring="mean_squared_error",n_folds=5).optimise(space,df,40)</code>
</p>

<p>
<strong>
<em>
<code>
# predicting on the test dataset</code>
</em>
</strong>
<br />

<code>
Predictor().fit_predict(best,df)</code>
</p>

<p>
The above code ranked 108(top 1%) on the Public Leaderboard without having to even open the train and test file. I think this is pretty awesome.</p>

<p>
Below is the image of feature importance as calculated by LightGBM.</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/04105909/LightGBM_feature_importance.png" width=1000>
</p>

<p>
&nbsp;</p>

<h2>
6. Basic Understanding of Drift</h2>

<p>
Drift is not a common topic but a very important one and it deserves an article of its own. But I will try to explain the functionality of Drift_Thresholder in brief.</p>

<p>
In general, we assume that train and test dataset are created through the same generative algorithm or process but this assumption is quite strong and we do not see this behaviour in the real world. In the real world, the data generator or the process may change. For example, in a sales prediction model, the customer behaviour changes over time and hence the data generated will be different than the data that was used to create the model. This is called drift.</p>

<p>
Another point to note is that in a dataset, both the independent features and the dependent feature may drift. When the independent features changes, it is called the covariate shift and when the relationship between the independent and dependent features change, it is called the concept shift. MLBox deals with the covariate shift.</p>

<p>
&nbsp;</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03224647/Screenshot-from-2017-07-03-22-46-17.png" width=1000>
</p>

<p>
The general algorithm for detection of drift is as follows-</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03224943/Screenshot-from-2017-07-03-22-49-17.png" width=1000>
</p>

<h2>
7. Basic Understanding of Entity Embedding</h2>

<p>
Entity Embeddings owe their existence to the word2vec embeddings in the sense that they function the same way as word vectors do. For example, we know that in word vector representation, we can do things like below.</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03225441/analogy-small.jpg">
</p>

<p>
&nbsp;</p>

<p>
In the similar sense, categorical variables could be encoded to create new informative features. Their effect was evident to the world in Kaggle&#8217;s Rossmann Sales Problem where a team used Entity Embeddings along with Neural Network and came third without performing any significant feature engineering. The entire code and the research paper on Entity Embeddings that resulted from the competition could be found <a href="https://www.kaggle.com/c/rossmann-store-sales/discussion/17974">
here</a>
. The Entity Embeddings were able to capture the relationship between the German states as shown below.</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/07/03230100/68747470733a2f2f706c6f742e6c792f253745656e74726f6e2f302f2e706e67.png" width=1000>
</p>

<p>
I don&#8217;t want to bog you down with the explanation of Entity Embeddings here. It deserves its own article. In MLBox, you can use Entity Embedding as a black box for encoding categorical variables.</p>

<h2>
8. Pros and Cons of MLBox</h2>

<p>
This library has its own sets of pros and cons.</p>

<p>
The pros are &#8211;</p>

<ol>

<li>
Automatic task identification i.e Classification or Regression</li>

<li>
Basic Pre-processing while reading the data</li>

<li>
Removal of Drifting variables</li>

<li>
Extremely fast and accurate hyperparameter optimisation.</li>

<li>
A wide variety of Feature Selection Methods.</li>

<li>
Minimal lines of code.</li>

<li>
Feature Engineering via Entity Embeddings</li>

</ol>

<p>
The cons are-</p>

<ol>

<li>
It is still under active development and things may break or make at any point in time.</li>

<li>
No support for Unsupervised Learning</li>

<li>
Basic Feature Engineering. You still have to create your own features.</li>

<li>
Purely mathematical based feature selection method. This method may remove variables which make sense from the business perspective.</li>

<li>
Not truly an Automated Machine Learning Library.</li>

</ol>

<p>
So, I suggest you weigh the pros and cons before making this your mainstream library for Machine Learning.</p>

<h2>
9. End Notes</h2>

<p>
I was really excited to try this library as soon as I read about its release on Github. I spent the next couple of days studying the library and simplifying it for you to use it on the go. I must say that I am really impressed with the library and am going to explore even more. With just 8 lines of code, I was able to break into top 1% and without having to spend time explicitly on handling data and hyperparameter optimisation, I could dedicate more time to feature engineering and check them on the fly. Please feel free to comment for any help or ideas below.</p>



</body>

</html>

