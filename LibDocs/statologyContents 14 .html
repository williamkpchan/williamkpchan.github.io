<base target="_blank"><html><head><title>statologyContents 14</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 14"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 14</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Set Value for a Specific Cell in Pandas DataFrame</span></h2>
You can use the following basic syntax to set the value for a specific cell in a pandas DataFrame:
<b>#set value at row index 0 and column 'col_name' to be 99
df.at[0, 'col_name'] = 99
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412</b>
<h3>Example 1: Set Value of One Cell in Pandas </h3>
The following code shows how to set the value in the 3rd index position of the ‘points’ column to 99:
<b>#set value in 3rd index position and 'points' column to be 99
df.at[3, 'points'] = 99
#view updated DataFrame
df
        pointsassistsrebounds
025511
11278
215710
39996
419126
52395
62599
729412
</b>
Notice that the value in the 3rd index position of the ‘points’ column was changed and all other values in the DataFrame remained the same.
<h3>Example 2: Set Value of Multiple Cells in Pandas</h3>
The following code shows how to set the value of multiple cells in a range simultaneously:
<b>#set values in index positions 0 to 3 in 'points' column to be 99 
df.at[0:3, 'points'] = 99
#view updated DataFrame
df
pointsassistsrebounds
099511
19978
299710
39996
419126
52395
62599
729412
</b>
<h3>Example 3: Set Values Conditionally in Pandas</h3>
The following code shows how to set the values in the ‘rebounds’ column to be 99 <em>only if</em> the value in the points column is greater than 20:
<b>#set values in 'rebounds' column to be 99 if value in points column is greater than 20
df.loc[df['points']>20, ['rebounds']] = 99
#view updated DataFrame
df
pointsassistsrebounds
025599
11278
215710
31496
419126
523999
625999
729499
</b>
Notice that each value in the rebounds column was changed to 99 if the value in the points column was greater than 20.
All other values remained the same.
<h2><span class="orange">How to Fix in Pandas: SettingWithCopyWarning</span></h2>
One warning you may encounter when using pandas is:
<b>SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
</b>
This warning appears when pandas encounters something called chain assignment – the combination of chaining and assignment all in one step.
It’s important to note that this is merely a <em>warning</em>, not an error. Your code will still run, but the results may not always match what you thought they would be.
The easiest way to suppress this warning is to use the following bit of code:
<b>pd.options.mode.chained_assignment = None</b>
The following example shows how to address this warning in practice.
<h3>How to Reproduce the Warning</h3>
Suppose we create the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A': [25, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, 7, 9, 12, 9, 9, 4],   'C': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
ABC
025511
11278
215710
31496
419126
52395
62599
729412</b>
Now suppose we create a new DataFrame that only contains column ‘A’ from the original DataFrame and we divide each value in column ‘A’ by 2:
<b>#define new DataFrame
df2 = df[['A']]
#divide all values in column 'A' by 2
df2['A'] = df['A'] / 2
/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:2:
SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead
</b>
We receive the <b>SettingWithCopyWarning</b> message because we set new values for column ‘A’ on a “slice” from the original DataFrame.
However, if we look at the new DataFrame we created then we’ll see that each value was actually successfully divided by 2:
<b>#view new DataFrame
df2
     A
0 12.5
1 6.0
2 7.5
3 7.0
4 9.5
5 11.5
6 12.5
7 14.5
</b>
Although we received a warning message, pandas still did what we thought it would do.
<h3>How to Avoid the Warning</h3>
To avoid the warning, it’s recommended to use the<b> .loc[row indexer, col indexer]</b> syntax as follows:
<b>#define new DataFrame
df2 = df.loc[:, ['A']]
#divide each value in column 'A' by 2
df2['A'] = df['A'] / 2
#view result
df2
     A
0 12.5
1 6.0
2 7.5
3 7.0
4 9.5
5 11.5
6 12.5
7 14.5
</b>
The new DataFrame contains all of the values from column ‘A’ in the original DataFrame, divided by two, and no warning message appears.
If we’d like to prevent the warning message from ever showing, we can use the following bit of code:
<b>#prevent SettingWithCopyWarning message from appearing
pd.options.mode.chained_assignment = None
</b>
For an in-depth explanation for why chained assignment should be avoided, refer to the  online pandas documentation .
<h2><span class="orange">How to Shift a Column in Pandas (With Examples)</span></h2>
You can use the <b>shift()</b> function to shift the values of a column up or down in a pandas DataFrame:
<b>#shift values down by 1
df['column1'] = df['column1'].shift(1)
#shift values up by 1
df['column1'] = df['column1'].shift(-1)
</b>
The following examples show how to use this function in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'product': ['A', 'B', 'C', 'D', 'E', 'F'],   'sales': [4, 7, 8, 12, 15, 19]})
#view DataFrame
df
        productsales
0A4
1B7
2C8
3D12
4E15
5F19</b>
<h3>Example 1: Shift One Column Up or Down</h3>
The following code shows how to shift all of the values of the ‘product’ column down by 1:
<b>#shift all 'product' values down by 1
df['product'] = df['product'].shift(1)
#view updated DataFrame
df
productsales
0NaN4
1A7
2B8
3C12
4D15
5E19
</b>
Notice that each value in the ‘product’ column has been shifted down by 1 and the first value in the column has been replaced with NaN.
Also notice that the last value in the product column (‘F’) has been removed from the DataFrame entirely.
To keep the ‘F’ value in the DataFrame, we need to first add an empty row to the bottom of the DataFrame and then perform the shift:
<b>import numpy as np
#add empty row to bottom of DataFrame
df.loc[len(df.index)] = [np.nan, np.nan]
#shift all 'product' values down by 1
df['product'] = df['product'].shift(1)
#view updated DataFrame
df
productsales
0NaN4.0
1A7.0
2B8.0
3C12.0
4D15.0
5E19.0
6FNaN
</b>
Notice that the ‘F’ value is retained as the last value in the ‘product’ column.
<h3>Example 2: Shift Multiple Columns Up or Down</h3>
The following code shows how to shift all of the values of the ‘product’ and ‘sales’ columns up by 2:
<b>#shift all 'product' and 'sales' values up by 2
df[['product', 'sales']] = df[['product', 'sales']].shift(-2)
#view updated DataFrame
df
        productsales
0C8.0
1D12.0
2E15.0
3F19.0
4NaNNaN
5NaNNaN
</b>
Notice that each value in the ‘product’ and ‘sales’ column has been shifted up by 2 and the bottom two values in each column have been replaced with NaN.
<b>Note</b>: You can find the complete documentation for the <b>shift()</b> function  here .
<h2><span class="orange">How to Show All Columns of a Pandas DataFrame</span></h2>
By default, Jupyter notebooks only displays 20 columns of a pandas DataFrame.
You can easily force the notebook to show all columns by using the following syntax:
<b>pd.set_option('max_columns', None)
</b>
You can also use the following syntax to display all of the column names in the DataFrame:
<b>print(df.columns.tolist())
</b>
Lastly, you can reset the default settings in a Jupyter notebook to only show 20 columns by using the following syntax:
<b>pd.reset_option('max_columns')</b>
The following example shows how to use these functions in practice.
<h3>Example: Show All Columns in Pandas DataFrame</h3>
Suppose we create a pandas DataFrame with 5 rows and 30 columns.
If we attempt to display the DataFrame in a Jupyter notebook, only 20 total columns will be shown:
<b>import pandas as pd
import numpy as np
#create dataFrame with 5 rows and 30 columns
df = pd.DataFrame(index=np.arange(5), columns=np.arange(30))
#view dataFrame
df</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/showCol1.png">
To display all of the columns, we can use the following syntax:
<b>#specify that all columns should be shown
pd.set_option('max_columns', None)
#view DataFrame
df</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/showCol2.png">
Notice that all 30 columns are now shown in the notebook.
We can also use the following syntax to simply display all column names in the DataFrame:
<b>print(df.columns.tolist())
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,
21, 22, 23, 24, 25, 26, 27, 28, 29]
</b>
To reset the default settings and only display a max of 20 columns, we can use the following syntax:
<b>pd.reset_option('max_columns')</b>
<h3>How to Show All Rows  in Pandas DataFrame</h3>
If you’d like to show every row in a pandas DataFrame, you can use the following syntax:
<b>pd.set_option('max_rows', None) 
</b>
You can also specify a max number of rows to display in a pandas DataFrame. For example, you could specify that only a max of 10 rows should be shown:
<b>pd.set_option('max_rows', 10) </b>
<h2><span class="orange">How to Show All Rows of a Pandas DataFrame</span></h2>
You can force a Jupyter notebook to show all rows in a pandas DataFrame by using the following syntax:
<b>pd.set_option('display.max_rows', None)
</b>
This tells the notebook to set no maximum on the number of rows that are shown.
The following example shows how to use this syntax in practice.
<h2>Example: Show All Rows in Pandas DataFrame</h2>
Suppose we create a pandas DataFrame with 500 rows and 3 columns.
If we attempt to display the DataFrame in a Jupyter notebook, only the first five rows and last five rows will be shown:
<b>import pandas as pd
import numpy as np
#create dataFrame with 500 rows and 3 columns
df = pd.DataFrame(index=np.arange(500), columns=np.arange(3))
#view dataFrame
df</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/showall1.jpg"513">
To display all of the rows, we can use the following syntax:
<b>#specify that all rows should be shown
pd.set_option('display.max_rows', None)
#view DataFrame
df</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/showall2.jpg"345">
The results are too long to display in a single screenshot, but the Jupyter notebook does indeed display all 500 rows.
To reset the default display settings, we can use the following syntax:
<b>pd.reset_option('display.max_rows')</b>
If we attempt to display the DataFrame in the Jupyter notebook, only the first five rows and last five rows will be shown once again.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Show All Columns of a Pandas DataFrame 
 How to Save Pandas DataFrame for Later Use 
<h2><span class="orange">Pandas: How to Skip Rows when Reading CSV File</span></h2>
You can use the following methods to skip rows when reading a CSV file into a pandas DataFrame:
<b>Method 1: Skip One Specific Row</b>
<b>#import DataFrame and skip 2nd row
df = pd.read_csv('my_data.csv', skiprows=[2])
</b>
<b>Method 2: Skip Several Specific Rows</b>
<b>#import DataFrame and skip 2nd and 4th row
df = pd.read_csv('my_data.csv', skiprows=[2, 4])</b>
<b>Method 3: Skip First N Rows</b>
<b>#import DataFrame and skip first 2 rows
df = pd.read_csv('my_data.csv', skiprows=2)</b>
The following examples show how to use each method in practice with the following CSV file called <b>basketball_data.csv</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/skip1-1.jpg"435">
<h2>Example 1: Skip One Specific Row</h2>
We can use the following code to import the CSV file and skip the second row:
<b>import pandas as pd
#import DataFrame and skip 2nd row
df = pd.read_csv('basketball_data.csv', skiprows=[2])
#view DataFrame
df
        teampointsrebounds
0A2210
1C296
2D302
</b>
Notice that the second row (with team ‘B’) was skipped when importing the CSV file into the pandas DataFrame.
<b>Note</b>: The first row in the CSV file is considered to be row 0.
<h2>Example 2: Skip Several Specific Rows</h2>
We can use the following code to import the CSV file and skip the second and fourth rows:
<b>import pandas as pd
#import DataFrame and skip 2nd and 4th rows
df = pd.read_csv('basketball_data.csv', skiprows=[2, 4])
#view DataFrame
df
        teampointsrebounds
0A2210
1C296
</b>
Notice that the second and fourth rows (with team ‘B’ and ‘D’) were skipped when importing the CSV file into the pandas DataFrame.
<h2>Example 3: Skip First N Rows</h2>
We can use the following code to import the CSV file and skip the first two rows:
<b>import pandas as pd
#import DataFrame and skip first 2 rows
df = pd.read_csv('basketball_data.csv', skiprows=2)
#view DataFrame
df
        B149
0C296
1D302
</b>
Notice that the first two rows in the CSV file were skipped and the next available row (with team ‘B’) became the header row for the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Read Excel Files with Pandas 
 How to Export Pandas DataFrame to Excel 
 How to Export NumPy Array to CSV File 
<h2><span class="orange">How to Slice Columns in Pandas DataFrame (With Examples)</span></h2>
You can use the following methods to slice the columns in a pandas DataFrame:
<b>Method 1: Slice by Specific Column Names</b>
<b>df_new = df.loc[:, ['col1', 'col4']]
</b>
<b>Method 2: Slice by Column Names in Range</b>
<b>df_new = df.loc[:, 'col1':'col4']</b>
<b>Method 3: Slice by Specific Column Index Positions</b>
<b>df_new = df.iloc[:, [0, 3]]</b>
<b>Method 4: Slice by Column Index Position Range</b>
<b>df_new = df.iloc[:, 0:3]</b>
Note the subtle difference between <b>loc</b> and <b>iloc</b> in each of these methods:
<b>loc</b> selects rows and columns with specific <b>labels</b>
<b>iloc</b> selects rows and columns at specific <b>integer positions</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame with six columns
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [4, 3, 3, 2, 5, 4, 3, 8],   'blocks': [1, 0, 0, 3, 2, 2, 1, 5]})
#view DataFrame
print(df)
  team  points  assists  rebounds  steals  blocks
0    A      18        5        11       4       1
1    B      22        7         8       3       0
2    C      19        7        10       3       0
3    D      14        9         6       2       3
4    E      14       12         6       5       2
5    F      11        9         5       4       2
6    G      20        9         9       3       1
7    H      28        4        12       8       5
</b>
<h2>Example 1: Slice by Specific Column Names</h2>
We can use the following syntax to create a new DataFrame that only contains the columns <b>team</b> and <b>rebounds</b>:
<b>#slice columns team and rebounds
df_new = df.loc[:, ['team', 'rebounds']]
#view new DataFrame
print(df_new)
  team  rebounds
0    A        11
1    B         8
2    C        10
3    D         6
4    E         6
5    F         5
6    G         9
7    H        12
</b>
<h2>Example 2: Slice by Column Names in Range</h2>
We can use the following syntax to create a new DataFrame that only contains the columns in the range between <b>team</b> and <b>rebounds</b>:
<b>#slice columns between team and rebounds
df_new = df.loc[:, 'team':'rebounds']
#view new DataFrame
print(df_new)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
<h2>Example 3: Slice by Specific Column Index Positions</h2>
We can use the following syntax to create a new DataFrame that only contains the columns in the index positions <b>0</b> and <b>3</b>:
<b>#slice columns in index positions 0 and 3
df_new = df.iloc[:, [0, 3]]
#view new DataFrame
print(df_new)
  team  rebounds
0    A        11
1    B         8
2    C        10
3    D         6
4    E         6
5    F         5
6    G         9
7    H        12</b>
<h2>Example 4: Slice by Column Index Position Range</h2>
We can use the following syntax to create a new DataFrame that only contains the columns in the index position range between <b>0</b> and <b>3</b>:
<b>#slice columns in index position range between 0 and 3
df_new = df.iloc[:, 0:3]
#view new DataFrame
print(df_new)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4</b>
<b>Note</b>: When using an index position range, the last index position in the range will not be included. For example, the <b>rebounds</b> column in index position 3 is not included in the new DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop First Row in Pandas DataFrame 
 How to Drop First Column in Pandas DataFrame 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">Pandas: How to Sort DataFrame Alphabetically</span></h2>
You can use the following methods to sort the rows of a pandas DataFrame alphabetically:
<b>Method 1: Sort by One Column Alphabetically</b>
<b>#sort A to Z
df.sort_values('column1')
#sort Z to A
df.sort_values('column1', ascending=False)
</b>
<b>Method 2: Sort by Multiple Columns Alphabetically</b>
<b>#sort by column1 from Z to A, then by column2 from A to Z
df.sort_values(['column1', 'column2'], ascending=(False, True))</b>
The following example shows how to use each method in practice.
<h3>Example 1: Sort by One Column Alphabetically</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Spurs', 'Lakers', 'Nuggets', 'Hawks'],   'points': [120, 108, 99, 104, 115]})
#view DataFrame
print(df)
      team  points
0     Mavs     120
1    Spurs     108
2   Lakers      99
3  Nuggets     104
4    Hawks     115</b>
We can use the following syntax to sort the rows of the DataFrame by team name from <b>A to Z</b>:
<b>#sort by team name A to Z
df_sorted = df.sort_values('team')
#view sorted DataFrame
print(df_sorted)
      team  points
4    Hawks     115
2   Lakers      99
0     Mavs     120
3  Nuggets     104
1    Spurs     108</b>
Notice that the rows are now sorted by team name from A to Z.
We could also sort from <b>Z to A</b>:
<b>#sort by team name Z to A
df_sorted = df.sort_values('team', ascending=False)
#view sorted DataFrame
print(df_sorted)
      team  points
1    Spurs     108
3  Nuggets     104
0     Mavs     120
2   Lakers      99
4    Hawks     115</b>
And we could also use the <b>reset_index()</b> function to reset the index values in the sorted DataFrame:
<b>#sort by team name A to Z and reset index
df_sorted = df.sort_values('team').reset_index(drop=True)
#view sorted DataFrame
print(df_sorted)
      team  points
0    Hawks     115
1   Lakers      99
2     Mavs     120
3  Nuggets     104
4    Spurs     108
</b>
<h3>Example 2: Sort by Multiple Columns Alphabetically</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'conference': ['West', 'West', 'West', 'East', 'East'],   'team': ['Mavs', 'Spurs', 'Lakers', 'Heat', 'Hawks'],   'points': [120, 108, 99, 104, 115]})
#view DataFrame
print(df)
  conference    team  points
0       West    Mavs     120
1       West   Spurs     108
2       West  Lakers      99
3       East    Heat     104
4       East   Hawks     115
</b>
We can use the following syntax to sort the rows of the DataFrame by conference name from A to Z, then by team name from Z to A:
<b>#sort by conference name A to Z, then by team name Z to A
df_sorted = df.sort_values(['conference', 'team'], ascending=(True, False))
#view sorted DataFrame
print(df_sorted)
  conference    team  points
3       East    Heat     104
4       East   Hawks     115
1       West   Spurs     108
0       West    Mavs     120
2       West  Lakers      99
</b>
The rows are sorted by conference name from A to Z, then by team name from Z to A.
<b>Note</b>: You can find the complete documentation for the pandas <b>sort_values()</b> function  here .
<h2><span class="orange">How to Sort a Pandas DataFrame by Date (With Examples)</span></h2>
Often you may want to sort a pandas DataFrame by a column that contains dates. Fortunately this is easy to do using the  <b>sort_values()</b>  function.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Sort by Date Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'sales': [4, 11, 13, 9],   'customers': [2, 6, 9, 7],   'date': ['2020-01-25', '2020-01-18', '2020-01-22', '2020-01-21']})
#view DataFrame
print(df)
   sales  customers        date
0      4          2  2020-01-25
1     11          6  2020-01-18
2     13          9  2020-01-22
3      9          7  2020-01-21
</b>
First, we need to use the <b>to_datetime() </b>function to convert the ‘date’ column to a datetime object:
<b>df['date'] = pd.to_datetime(df['date'])
</b>
Next, we can sort the DataFrame based on the ‘date’ column using the <b>sort_values()</b> function:
<b>df.sort_values(by='date')
        salescustomersdate
1116  2020-01-18
397  2020-01-21
2139  2020-01-22
042  2020-01-25
</b>
By default, this function sorts dates in ascending order. However, you can specify <b>ascending=False</b> to instead sort in descending order:
<b>df.sort_values(by='date', ascending=False)
salescustomersdate
042  2020-01-25
2139  2020-01-22
397  2020-01-21
1116  2020-01-18</b>
<h3>Example 2: Sort by Multiple Date Columns</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'person': ['A', 'B', 'C', 'D'],   'order_date': ['2020-01-15', '2020-01-15', '2020-01-20', '2020-01-20'],   'receive_date': ['2020-01-25', '2020-01-18', '2020-01-22', '2020-01-21']})
#view DataFrame
print(df)
  person  order_date receive_date
0      A  2020-01-15   2020-01-25
1      B  2020-01-15   2020-01-18
2      C  2020-01-20   2020-01-22
3      D  2020-01-20   2020-01-21</b>
We can use the <b>sort_values </b>function to sort the DataFrame by multiple columns by simply providing multiple column names to the function:
<b>#convert both date columns to datetime objects
df[['order_date','receive_date']] = df[['order_date','receive_date']].apply(pd.to_datetime)
#sort DateFrame by order_date, then by receive_date
df.sort_values(by=['order_date', 'receive_date'])
        personorder_datereceive_date
1B2020-01-152020-01-18
0A2020-01-152020-01-25
3D2020-01-202020-01-21
2C2020-01-202020-01-22</b>
The DataFrame is now sorted in ascending order by order_date, then in ascending order by receive_date.
<h2><span class="orange">Pandas: Sort DataFrame by Both Index and Column</span></h2>
You can use the following syntax to sort a pandas DataFrame by both index and column:
<b>df = df.sort_values(by = ['column_name', 'index'], ascending = [False, True])
</b>
The following examples show how to use this syntax in practice.
<h3>Examples: Sort DataFrame by Both Index and Column</h3>
The following code shows how to sort a pandas DataFrame by the column named <b>points</b> and then by the <b>index</b> column:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'id': [1, 2, 3, 4, 5, 6, 7, 8],   'points': [25, 15, 15, 14, 20, 20, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]}).set_index('id')
#view first few rows
df.head()
pointsassistsrebounds
id
125511
21578
315710
41496
520126
#sort by points and then by index
df.sort_values(by = ['points', 'id'], ascending = [False, True])
pointsassistsrebounds
id
829412
125511
72599
520126
62095
21578
315710
41496
</b>
The resulting DataFrame is sorted by points in descending order and then by the index in ascending order (if there happen to be two players who score the same number of points).
Note that if we don’t use the <b>ascending</b> argument, then each column will use ascending as the default sorting method:
<b>#sort by points and then by index
df.sort_values(by = ['points', 'id'])
        pointsassistsrebounds
id
41496
21578
315710
520126
62095
125511
72599
829412
</b>
If the index column is currently not named, you can rename it and then sort accordingly:
<b>#sort by points and then by index
df.rename_axis('index').sort_values(by = ['points', 'id'])
        pointsassistsrebounds
id
41496
21578
315710
520126
62095
125511
72599
829412</b>
<h2><span class="orange">How to Sort by Multiple Columns in Pandas (With Examples)</span></h2>
You can use the following basic syntax to sort a pandas DataFrame by multiple columns:
<b>df = df.sort_values(['column1', 'column2'], ascending=(False, True))
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Sort by Multiple Columns in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [14, 20, 9, 20, 25, 29, 20, 25],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
        pointsassistsrebounds
014511
12078
29710
32096
425126
52995
62099
725412
</b>
We can use the following syntax to sort the rows of the DataFrame by <b>points</b> ascending, then by <b>assists</b> descending:
<b>#sort by points ascending, then assists ascending
df = df.sort_values(['points', 'assists'])
#view updated DataFrame
df
pointsassistsrebounds
29710
014511
12078
32096
62099
725412
425126
52995
</b>
Notice that the rows are sorted by <b>points</b> ascending (smallest to largest), then by <b>assists</b> ascending.
We can also use the <b>ascending</b> argument to specify whether to sort each column in an ascending or descending manner:
<b>#sort by points descending, then assists ascending
df = df.sort_values(['points', 'assists'], ascending = (False, True)))
#view updated DataFrame
df
        pointsassistsrebounds
52995
725412
425126
12078
32096
62099
014511
29710
</b>
Notice that the rows are sorted by <b>points</b> descending (largest to smallest), then by <b>assists</b> ascending.
In these examples we sorted the DataFrame by two columns, but we can use this exact syntax to sort by any number of columns that we’d like.
<b>Note</b>: You can find the complete documentation for the pandas <b>sort_values()</b> function  here .
<h2><span class="orange">Pandas: How to Sort Columns by Name</span></h2>
You can use the following syntax to quickly sort a pandas DataFrame by column names:
<b>df = df[['column1', 'column4', 'column3', 'column2']]
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Sort Pandas DataFrame by Column Names</h3>
The following code shows how to sort a pandas DataFrame by column names:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [2, 3, 3, 2, 5, 3, 2, 1]})
#list column names
list(df)
['points', 'assists', 'rebounds', 'steals']
#sort columns by names
df = df[['steals', 'assists', 'rebounds', 'points']]
df
stealsassistsrebounds  points
02511  25
1378  12
23710  15
3296  14
45126  19
5395  23
6299  25
71412  29
</b>
<h3>Example 2: Sort Pandas DataFrame by List</h3>
The following code shows how to sort a pandas DataFrame by a list of names:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [2, 3, 3, 2, 5, 3, 2, 1]})
#define list of column names
name_order = ['steals', 'assists', 'rebounds', 'points']
#sort columns by list
df = df[name_order]
df
stealsassistsrebounds  points
02511  25
1378  12
23710  15
3296  14
45126  19
5395  23
6299  25
71412  29
</b>
<h3>Example 3: Sort Pandas DataFrame Alphabetically</h3>
The following code shows how to sort a pandas DataFrame alphabetically:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12],   'steals': [2, 3, 3, 2, 5, 3, 2, 1]})
#sort columns alphabetically
df = df[sorted(df.columns)]
df
assistspointsrebounds  steals
052511  2
17128  3
271510  3
39146  2
412196  5
59235  3
69259  2
742912  1
</b>
<h2><span class="orange">Pandas: How to Split a Column of Lists into Multiple Columns</span></h2>
You can use the following basic syntax to split a column of lists into multiple columns in a pandas DataFrame:
<b>#split column of lists into two new columns
split = pd.DataFrame(df['my_column'].to_list(), columns = ['new1', 'new2'])
#join split columns back to original DataFrame
df = pd.concat([df, split], axis=1) </b>
The following example shows how to use this syntax in practice.
<h2>Example: Split Column of Lists into Multiple Columns in Pandas</h2>
Suppose we have the following pandas DataFrame in which the column called <b>points</b> contains lists of values:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', 'Heat', 'Kings', 'Suns'],   'points': [[99, 105], [94, 113], [99, 97], [87, 95]]})
#view DataFrame
print(df)
    team     points
0   Mavs  [99, 105]
1   Heat  [94, 113]
2  Kings   [99, 97]
3   Suns   [87, 95]</b>
We can use the following syntax to create a new DataFrame in which the <b>points</b> column is split into two new columns called <b>game1</b> and <b>game2</b>:
<b>#split column of lists into two new columns
split = pd.DataFrame(df['my_column'].to_list(), columns = ['new1', 'new2'])
#view DataFrame
print(split)
   game1  game2
0     99    105
1     94    113
2     99     97
3     87     95
</b>
If we’d like, we can then join this split DataFrame back with the original DataFrame by using the <b>concat()</b> function:
<b>#join split columns back to original DataFrame
df = pd.concat([df, split], axis=1) 
#view updated DataFrame
print(df)
    team     points  game1  game2
0   Mavs  [99, 105]     99    105
1   Heat  [94, 113]     94    113
2  Kings   [99, 97]     99     97
3   Suns   [87, 95]     87     95
</b>
Lastly, we can drop the original <b>points</b> column from the DataFrame if we’d like:
<b>#drop original points column
df = df.drop('points', axis=1)
#view updated DataFrame
print(df)
    team  game1  game2
0   Mavs     99    105
1   Heat     94    113
2  Kings     99     97
3   Suns     87     95
</b>
The end result is a DataFrame in which the original <b>points</b> column of lists is now split into two new columns called <b>game1</b> and <b>game2</b>.
<b>Note</b>: If your column of lists has an uneven number of values in each list, pandas will simply fill in missing values with <b>NaN</b> values when splitting the lists into columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Print Pandas DataFrame with No Index 
 How to Show All Rows of a Pandas DataFrame 
 How to Check dtype for All Columns in Pandas DataFrame 
<h2><span class="orange">How to Split String Column in Pandas into Multiple Columns</span></h2>
You can use the following basic syntax to split a string column in a pandas DataFrame into multiple columns:
<b>#split column A into two columns: column A and column B
df[['A', 'B']] = df['A'].str.split(',', 1, expand=True)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Split Column by Comma</h3>
The following code shows how to split a column in a pandas DataFrame, based on a comma, into two separate columns:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs, West', 'Spurs, West', 'Nets, East'],   'points': [112, 104, 127]})
#view DataFrame
df
teampoints
0Mavs, West112
1Spurs, West104
2Nets, East127
#split team column into two columns
df[['team', 'conference']] = df['team'].str.split(',', 1, expand=True)
#view updated DataFrame
df
teampointsconference
0Mavs112West
1Spurs104West
2Nets127East</b>
Note that you can also reorder the columns after performing the split if you’d like:
<b>#reorder columns
df = df[['team', 'conference', 'points']]
#view DataFrame
df
teamconference points
0MavsWest   112
1SpursWest   104
2NetsEast   127
</b>
<h3>Example 2: Split Column by Other Delimiters</h3>
We can use the same syntax to split a column by other delimiters.
For example, we can split a column by a <b>space</b>:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs West', 'Spurs West', 'Nets East'],   'points': [112, 104, 127]})
#split team column into two columns
df[['team', 'conference']] = df['team'].str.split(' ', 1, expand=True)
#view updated DataFrame
df
teamconference points
0MavsWest   112
1SpursWest   104
2NetsEast   127
</b>
We can also split a column by a <b>slash</b>:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs/West', 'Spurs/West', 'Nets/East'],   'points': [112, 104, 127]})
#split team column into two columns
df[['team', 'conference']] = df['team'].str.split('/', 1, expand=True)
#view updated DataFrame
df
teamconference points
0MavsWest   112
1SpursWest   104
2NetsEast   127</b>
Using this syntax, we can split a column by any delimiter we’d like.
<h2><span class="orange">Pandas: How to Split DataFrame By Column Value</span></h2>
You can use the following basic syntax to split a pandas DataFrame by column value:
<b>#define value to split on
x = 20
#define df1 as DataFrame where 'column_name' is >= 20
df1 = df[df['column_name'] >= x]
#define df2 as DataFrame where 'column_name' is &lt; 20
df2 = df[df['column_name'] &lt; x]
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Split Pandas DataFrame by Column Value</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [22, 24, 19, 18, 14, 29, 31, 16],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
        teampointsrebounds
0A2211
1B248
2C1910
3D186
4E146
5F295
6G319
7H1612</b>
We can use the following code to split the DataFrame into two DataFrames where the first contains the rows where ‘points’ is greater than or equal to 20 and the second contains the rows where ‘points’ is less than 20:
<b>#define value to split on
x = 20
#define df1 as DataFrame where 'points' is >= 20
df1 = df[df['points'] >= x]
print(df1)
  team  points  rebounds
0    A      22        11
1    B      24         8
5    F      29         5
6    G      31         9
#define df2 as DataFrame where 'points' is &lt; 20
df2 = df[df['points'] &lt; x]
print(df2)
  team  points  rebounds
2    C      19        10
3    D      18         6
4    E      14         6
7    H      16        12
</b>
Note that we can also use the <b>reset_index()</b> function to reset the index values for each resulting DataFrame:
<b>#define value to split on
x = 20
#define df1 as DataFrame where 'points' is >= 20
df1 = df[df['points'] >= x].reset_index(drop=True)
print(df1)
  team  points  rebounds
0    A      22        11
1    B      24         8
2    F      29         5
3    G      31         9
#define df2 as DataFrame where 'points' is &lt; 20
df2 = df[df['points'] &lt; x].reset_index(drop=True)
print(df2)
  team  points  rebounds
0    C      19        10
1    D      18         6
2    E      14         6
3    H      16        12</b>
Notice that the index for each resulting DataFrame now starts at 0.
<h2><span class="orange">How to Split a Pandas DataFrame into Multiple DataFrames</span></h2>
You can use the following basic syntax to split a pandas DataFrame into multiple DataFrames based on row number:
<b>#split DataFrame into two DataFrames at row 6
df1 = df.iloc[:6]
df2 = df.iloc[6:]
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Split Pandas DataFrame into Two DataFrames</h3>
The following code shows how to split one pandas DataFrame into two DataFrames:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [1, 1, 1, 3, 3, 4, 5, 5, 5, 6, 7, 9],   'y': [5, 7, 7, 9, 12, 9, 9, 4, 3, 3, 1, 10]})
#view DataFrame
df
xy
015
117
217
339
4312
549
659
754
853
963
1071
11910
#split original DataFrame into two DataFrames
df1 = df.iloc[:6]
df2 = df.iloc[6:]
#view resulting DataFrames
print(df1)
   x   y
0  1   5
1  1   7
2  1   7
3  3   9
4  3  12
5  4   9
print(df2)
    x   y
6   5   9
7   5   4
8   5   3
9   6   3
10  7   1
11  9  10
</b>
Notice that <b>df1</b> contains the first six rows of the original DataFrame and <b>df2</b> contains the last six rows of the original DataFrame.
<h3>Example 2: Split Pandas DataFrame into Multiple DataFrames</h3>
The following code shows how to split a pandas 
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [1, 1, 1, 3, 3, 4, 5, 5, 5, 6, 7, 9],   'y': [5, 7, 7, 9, 12, 9, 9, 4, 3, 3, 1, 10]})
#split into three DataFrames
df1 = df.iloc[:3]
df2 = df.iloc[3:6]
df3 = df.iloc[6:]
#view resulting DataFrames
print(df1)
   x  y
0  1  5
1  1  7
2  1  7
print(df2)
   x   y
3  3   9
4  3  12
5  4   9
print(df3)
    x   y
6   5   9
7   5   4
8   5   3
9   6   3
10  7   1
11  9  10
</b>
In this example we chose to split one DataFrame into three DataFrames, but using this syntax we can split a pandas DataFrame into any number of DataFrames that we’d like.
<h2><span class="orange">How to Create a Stacked Bar Chart in Pandas</span></h2>
You can use the following basic syntax to create a stacked bar chart in pandas:
<b>df.groupby(['var1', 'var2']).size().unstack().plot(kind='bar', stacked=True)
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Stacked Bar Chart in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'F', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team position  points
0    A        G       5
1    A        G       7
2    A        F       7
3    A        F       9
4    B        G      12
5    B        F       9
6    B        F       9
7    B        F       4</b>
We can use the following code to create a stacked bar chart that displays the total count of <b>position</b>, grouped by <b>team</b>:
<b>df.groupby(['team', 'position']).size().unstack().plot(kind='bar', stacked=True)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/stacked1.jpg"566">
The x-axis shows the <b>team</b> name and the y-axis shows the total count of <b>position</b> for each team.
From the chart we can see that team A has <b>2</b> guards (G) and <b>2</b> forwards (F) while team <b>B</b> has 1 guard and <b>3</b> forwards.
We can also use the <b>color</b> and <b>title</b> arguments to modify the color of the bars and add a title to the chart:
<b>df.groupby(['team', 'position']).size().unstack().plot(kind='bar', stacked=True,
            color=['steelblue','pink'], title='Position Count by Team')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/stacked2.jpg">
A title has been added to the top of the plot and the colors of the bars have been changed to steelblue and pink, just as we specified.
<h2>Additional Resources</h2>
The following tutorials explain how to create other common charts in Python:
 How to Create Heatmaps in Python 
 How to Create a Bell Curve in Python 
 How to Create an Ogive Graph in Python 
<h2><span class="orange">How to Calculate Standard Deviation in Pandas (With Examples)</span></h2>
You can use the  DataFrame.std()  function to calculate the standard deviation of values in a pandas DataFrame.
You can use the following methods to calculate the standard deviation in practice:
<b>Method 1: Calculate Standard Deviation of One Column</b>
<b>df<span>[<span>'column_name'<span>]<span>.std<span>(<span>) </b>
<b>Method 2: Calculate Standard Deviation of Multiple Columns</b>
<b>df<span>[[<span>'column_name1', 'column_name2']<span>]<span>.std<span>(<span>) </b>
<b>Method 3: Calculate Standard Deviation of All Numeric Columns</b>
<b>df<span>.std<span>(<span>) </b>
Note that the <b>std()</b> function will automatically ignore any NaN values in the DataFrame when calculating the standard deviation.
The following examples shows how to use each method with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
teampointsassistsrebounds
0A25511
1A1278
2B15710
3B1496
4B19126
5B2395
6C2599
7C29412
</b>
<h3>Method 1: Calculate Standard Deviation of One Column</h3>
The following code shows how to calculate the standard deviation of one column in the DataFrame:
<b>#calculate standard deviation of 'points' column
df<span>[<span>'points'<span>]<span>.std<span>(<span>) 
6.158617655657106
</b>
The standard deviation turns out to be <b>6.1586</b>.
<h3>Method 2: Calculate Standard Deviation of Multiple Columns</h3>
The following code shows how to calculate the standard deviation of multiple columns in the DataFrame:
<b>#calculate standard deviation of 'points' and 'rebounds' columns
df[['points', 'rebounds']].std()
points      6.158618
rebounds    2.559994
dtype: float64</b>
The standard deviation of the ‘points’ column is <b>6.1586</b> and the standard deviation of the ‘rebounds’ column is <b>2.5599</b>.
<h3>Method 3: Calculate Standard Deviation of All Numeric Columns</h3>
The following code shows how to calculate the standard deviation of every numeric column in the DataFrame:
<b>#calculate standard deviation of all numeric columns
df.std()
points      6.158618
assists     2.549510
rebounds    2.559994
dtype: float64
</b>
Notice that pandas did not calculate the standard deviation of the ‘team’ column since it was not a numeric column.
<h2><span class="orange">Pandas: How to Strip Whitespace from Columns</span></h2>
You can use the following methods to strip whitespace from columns in a pandas DataFrame:
<b>Method 1: Strip Whitespace from One Column</b>
<b>df['my_column'] = df['my_column'].str.strip()
</b>
<b>Method 2: Strip Whitespace from All String Columns</b>
<b>df = df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavs', ' Heat', ' Nets ', 'Cavs', 'Hawks', 'Jazz '],   'position': ['Point Guard', ' Small Forward', 'Center  ',                'Power Forward', ' Point Guard ', 'Center'],   'points': [11, 8, 10, 6, 22, 29]})
#view DataFrame
print(df)
     team        position  points
0    Mavs     Point Guard      11
1    Heat   Small Forward       8
2   Nets         Center        10
3    Cavs   Power Forward       6
4   Hawks    Point Guard       22
5   Jazz           Center      29</b>
<h2>Example 1: Strip Whitespace from One Column</h2>
The following code shows how to strip whitespace from every string in the <b>position</b> column:
<b>#strip whitespace from position column
df['position'] = df['position'].str.strip()
#view updated DataFrame
print(df)
     team       position  points
0    Mavs    Point Guard      11
1    Heat  Small Forward       8
2   Nets          Center      10
3    Cavs  Power Forward       6
4   Hawks    Point Guard      22
5   Jazz          Center      29
</b>
Notice that all whitespace has been stripped from each string that had whitespace in the <b>position</b> column.
<h2>Example 2: Strip Whitespace from All String Columns</h2>
The following code shows how to strip whitespace from each string in all string columns of the DataFrame:
<b>#strip whitespace from all string columns
df = df.apply(lambda x: x.str.strip() if x.dtype == 'object' else x)
#view updated DataFrame
print(df)
    team       position  points
0   Mavs    Point Guard      11
1   Heat  Small Forward       8
2   Nets         Center      10
3   Cavs  Power Forward       6
4  Hawks    Point Guard      22
5   Jazz         Center      29
</b>
Notice that all whitespace has been stripped from both the <b>team</b> and <b>position</b> columns, which are the two string columns in the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Select Columns Containing a Specific String 
 Pandas: How to Filter Rows Based on String Length 
 How to Create Pandas DataFrame from a String 
<h2><span class="orange">Pandas: How to Plot Multiple DataFrames in Subplots</span></h2>
You can use the following basic syntax to plot multiple pandas DataFrames in subplots:
<b>import matplotlib.pyplot as plt
#define subplot layout
fig, axes = plt.subplots(nrows=2, ncols=2)
#add DataFrames to subplots
df1.plot(ax=axes[0,0])
df2.plot(ax=axes[0,1])
df3.plot(ax=axes[1,0])
df4.plot(ax=axes[1,1])
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Plot Multiple Pandas DataFrames in Subplots</h2>
Suppose we have four pandas DataFrames that contain information on sales and returns at four different retail stores:
<b>import pandas as pd
#create four DataFrames
df1 = pd.DataFrame({'sales': [2, 5, 5, 7, 9, 13, 15, 17, 22, 24],    'returns': [1, 2, 3, 4, 5, 6, 7, 8, 7, 5]})
df2 = pd.DataFrame({'sales': [2, 5, 11, 18, 15, 15, 14, 9, 6, 7],    'returns': [1, 2, 0, 2, 2, 4, 5, 4, 2, 1]})
df3 = pd.DataFrame({'sales': [6, 8, 8, 7, 8, 9, 10, 7, 8, 12],    'returns': [1,0, 1, 1, 1, 2, 3, 2, 1, 3]})
df4 = pd.DataFrame({'sales': [10, 7, 7, 6, 7, 6, 4, 3, 3, 2],    'returns': [4, 4, 3, 3, 2, 3, 2, 1, 1, 0]})</b>
We can use the following syntax to plot each of these DataFrames in a subplot that has a layout of 2 rows and 2 columns:
<b>import matplotlib.pyplot as plt
#define subplot layout
fig, axes = plt.subplots(nrows=2, ncols=2)
#add DataFrames to subplots
df1.plot(ax=axes[0,0])
df2.plot(ax=axes[0,1])
df3.plot(ax=axes[1,0])
df4.plot(ax=axes[1,1])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/sub1.jpg">
Each of the four DataFrames is displayed in a subplot.
Note that we used the <b>axes</b> argument to specify where each DataFrame should be placed.
For example, the DataFrame called <b>df1</b> was placed in the position with a row index value of <b>0</b> and a column index value of <b>0</b> (e.g. the subplot in the upper left corner).
Also note that you can change the layout of the subplots by using the <b>nrows</b> and <b>ncols</b> arguments.
For example, the following code shows how to arrange the subplots in four rows and one column:
<b>import matplotlib.pyplot as plt
#define subplot layout
fig, axes = plt.subplots(nrows=4, ncols=1)
#add DataFrames to subplots
df1.plot(ax=axes[0])
df2.plot(ax=axes[1])
df3.plot(ax=axes[2])
df4.plot(ax=axes[3])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/sub2.jpg"555">
The subplots are now arranged in a layout with four rows and one column.
Note that if you’d like the subplots to have the same y-axis and x-axis scales, you can use the <b>sharey</b> and <b>sharex</b> arguments.
For example, the following code shows how to use the <b>sharey</b> argument to force all of the subplots to have the same y-axis scale:
<b>import matplotlib.pyplot as plt
#define subplot layout, force subplots to have same y-axis scale
fig, axes = plt.subplots(nrows=4, ncols=1, sharey=True)
#add DataFrames to subplots
df1.plot(ax=axes[0])
df2.plot(ax=axes[1])
df3.plot(ax=axes[2])
df4.plot(ax=axes[3])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/sub3.jpg"558">
Notice that the y-axis for each subplot now ranges from 0 to 20.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create Pie Chart from Pandas DataFrame 
 How to Make a Scatterplot From Pandas DataFrame 
 How to Create a Histogram from Pandas DataFrame 
<h2><span class="orange">Pandas: How to Get Substring of Entire Column</span></h2>
You can use the following basic syntax to get the substring of an entire column in a pandas DataFrame:
<b>df['some_substring'] = df['string_column'].str[1:4]
</b>
This particular example creates a new column called <b>some_substring</b> that contains the characters from positions 1 through 4 in the <b>string_column</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Get Substring of Entire Column in Pandas</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['Mavericks', 'Warriors', 'Rockets', 'Hornets', 'Lakers'],   'points': [120, 132, 108, 118, 106]})
#view DataFrame
print(df)
        team  points
0  Mavericks     120
1   Warriors     132
2    Rockets     108
3    Hornets     118
4     Lakers     106</b>
We can use the following syntax to create a new column that contains the characters in the <b>team</b> column between positions 1 and 4:
<b>#create column that extracts characters in positions 1 through 4 in team column
df['team_substring'] = df['team'].str[1:4]
#view updated DataFrame
print(df)
        team  points team_substring
0  Mavericks     120            ave
1   Warriors     132            arr
2    Rockets     108            ock
3    Hornets     118            orn
4     Lakers     106            ake
</b>
The new column called <b>team_substring</b> contains the characters in the team column between positions 1 and 4.
Note that if you attempt to use this syntax to extract a substring from a numeric column, you’ll receive an error:
<b>#attempt to extract characters in positions 0 through 2 in points column
df['points_substring'] = df['points'].str[:2]
AttributeError: Can only use .str accessor with string values!
</b>
Instead, you must convert the numeric column to a string by using <b>astype(str)</b> first:
<b>#extract characters in positions 0 through 2 in points column
df['points_substring'] = df['points'].astype(str).str[:2]
#view updated DataFrame
print(df)
        team  points points_substring
0  Mavericks     120               12
1   Warriors     132               13
2    Rockets     108               10
3    Hornets     118               11
4     Lakers     106               10</b>
This time we’re able to successfully extract characters in positions 0 through 2 of the <b>points</b> column because we first converted it to a string.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: Check if String Contains Multiple Substrings 
 Pandas: How to Add String to Each Value in Column 
 Pandas: How to Select Columns Containing a Specific String 
<h2><span class="orange">How to Subtract Two Columns in Pandas DataFrame</span></h2>
You can use the following syntax to subtract one column from another in a pandas DataFrame:
<b>#subtract column 'B' from column 'A'
df['A-B'] = df.A- df.B
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Subtract Two Columns in Pandas</h3>
The following code shows how to subtract one column from another in a pandas DataFrame and assign the result to a new column:
<b>import pandas as pd
#create DataFrame 
df = pd.DataFrame({'A': [25, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, 8, 9, 12, 9, 12, 4],   'C': [11, 8, 10, 6, 6, 5, 9, 12]})
#subtract column B from column A
df['A-B'] = df.A - df.B
#view DataFrame
df
        ABCA-B
02551120
112785
2158107
314965
4191267
5239514
62512913
72941225
</b>
The new column called ‘<b>A-B</b>‘ displays the results of subtracting the values in column B from the values in column A.
<h3>Example 2: Subtract Two Columns with Missing Values</h3>
If we subtract one column from another in a pandas DataFrame and there happen to be missing values in one of the columns, the result of the subtraction will always be a missing value:
<b>import pandas as pd
import numpy as np
#create DataFrame with some missing values
df = pd.DataFrame({'A': [25, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, np.nan, 9, 12, np.nan, 12, 4],   'C': [np.nan, 8, 10, 6, 6, 5, 9, 12]}) 
#subtract column B from column A
df['A-B'] = df.A - df.B
#view DataFrame
df
ABCA-B
0255.0NaN20.0
1127.08.05.0
215NaN10.0NaN
3149.06.05.0
41912.06.07.0
523NaN5.0NaN
62512.09.013.0
7294.012.025.0
</b>
If you’d like, you can replace all of the missing values in the dataFrame with zeros using the <b>df.fillna(0)</b> function before subtracting one column from another:
<b>import pandas as pd
import numpy as np
#create DataFrame with some missing values
df = pd.DataFrame({'A': [25, 12, 15, 14, 19, 23, 25, 29],   'B': [5, 7, np.nan, 9, 12, np.nan, 12, 4],   'C': [np.nan, 8, 10, 6, 6, 5, 9, 12]}) 
#replace all missing values with zeros
df = df.fillna(0)
#subtract column B from column A
df['A-B'] = df.A - df.B
#view DataFrame
df
ABCA-B
0255.00.020.0
1127.08.05.0
2150.010.015.0
3149.06.05.0
41912.06.07.0
5230.05.023.0
62512.09.013.0
7294.012.025.0</b>
<h2><span class="orange">Pandas: How to Subtract Two DataFrames</span></h2>
You can use the following syntax to subtract one pandas DataFrame from another:
<b>df1.subtract(df2)
</b>
If you have a character column in each DataFrame, you may first need to move it to the index column of each DataFrame:
<b>df1.set_index('char_column').subtract(df2.set_index('char_column'))</b>
The following examples show how to use each syntax in practice.
<h2>Example 1: Subtract Two Pandas DataFrames (Numerical Columns Only)</h2>
Suppose we have the following two pandas DataFrames that only have numerical columns:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'points': [5, 17, 7, 19, 12, 13, 9, 24],    'assists': [4, 7, 7, 6, 8, 7, 10, 11]})
print(df1)
   points  assists
0       5        4
1      17        7
2       7        7
3      19        6
4      12        8
5      13        7
6       9       10
7      24       11
#create second DataFrame
df2 = pd.DataFrame({'points': [4, 22, 10, 3, 7, 8, 12, 10],    'assists': [3, 5, 5, 4, 7, 14, 9, 5]})
print(df2)
   points  assists
0       4        3
1      22        5
2      10        5
3       3        4
4       7        7
5       8       14
6      12        9
7      10        5
</b>
The following code shows how to subtract the corresponding values between the two DataFrames:
<b>#subtract corresponding values between the two DataFrames
df1.subtract(df2)
pointsassists
011
1-52
2-32
3162
451
55-7
6-31
7146
</b>
<h2>Example 2: Subtract Two Pandas DataFrames (Mix of Character & Numerical Columns)</h2>
Suppose we have the following two pandas DataFrames that each have a character column called <b>team</b>:
<b>import pandas as pd
#create first DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [5, 17, 7, 19, 12, 13, 9, 24],    'assists': [4, 7, 7, 6, 8, 7, 10, 11]})
print(df1)
  team  points  assists
0    A       5        4
1    B      17        7
2    C       7        7
3    D      19        6
4    E      12        8
5    F      13        7
6    G       9       10
7    H      24       11
#create second DataFrame
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [4, 22, 10, 3, 7, 8, 12, 10],    'assists': [3, 5, 5, 4, 7, 14, 9, 3]})
print(df2)
  team  points  assists
0    A       4        3
1    B      22        5
2    C      10        5
3    D       3        4
4    E       7        7
5    F       8       14
6    G      12        9
7    H      10        3</b>
The following code shows how to move the <b>team</b> column to the index column of each DataFrame and then subtract the corresponding values between the two DataFrames:
<b>#move 'team' column to index of each DataFrame and subtract corresponding values
df1.set_index('team').subtract(df2.set_index('team'))
pointsassists
team
A11
B-52
C-32
D162
E51
F5-7
G-31
H148
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Find the Difference Between Two Columns 
 Pandas: How to Find the Difference Between Two Rows 
 Pandas: How to Subtract Two Columns 
<h2><span class="orange">Pandas: How to Sum Columns Based on a Condition</span></h2>
You can use the following syntax to sum the values of a column in a pandas DataFrame based on a condition:
<b>df.loc[df['col1'] == some_value, 'col2'].sum()
</b>
This tutorial provides several examples of how to use this syntax in practice using the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C'],   'conference': ['East', 'East', 'East', 'West', 'West', 'East'],   'points': [11, 8, 10, 6, 6, 5],   'rebounds': [7, 7, 6, 9, 12, 8]})
#view DataFrame
df
        teamconference  points  rebounds
0AEast    11    7
1AEast    8    7
2AEast    10    6
3BWest    6    9
4BWest    6    12
5CEast    5    8</b>
<h3>Example 1: Sum One Column Based on One Condition</h3>
The following code shows how to find the sum of the points for the rows where team is equal to ‘A’:
<b>df.loc[df['team'] == 'A', 'points'].sum()
29</b>
<h3>Example 2: Sum One Column Based on Multiple Conditions </h3>
The following code shows how to find the sum of the points for the rows where team is equal to ‘A’ <em>and</em> where conference is equal to ‘East’:
<b>df.loc[(df['team'] == 'A') & (df['conference'] == 'East'), 'points'].sum()
29</b>
<h3>Example 3: Sum One Column Based on One of Several Conditions</h3>
The following code shows how to find the sum of the points for the rows where team is equal to ‘A’ <i>or </i>‘B’:
<b>df.loc[df['team'].isin(['A', 'B']), 'points'].sum()
41</b>
You can find more pandas tutorials on  this page .
<h2><span class="orange">How to Sum Specific Columns in Pandas (With Examples)</span></h2>
You can use the following methods to find the sum of a specific set of columns in a pandas DataFrame:
<b>Method 1: Find Sum of All Columns</b>
<b>#find sum of all columns
df['sum'] = df.sum(axis=1)
</b>
<b>Method 2: Find Sum of Specific Columns</b>
<b>#specify the columns to sum
cols = ['col1', 'col4', 'col5']
#find sum of columns specified 
df['sum'] = df[cols].sum(axis=1)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
   points  assists  rebounds
0      18        5        11
1      22        7         8
2      19        7        10
3      14        9         6
4      14       12         6
5      11        9         5
6      20        9         9
7      28        4        12
</b>
<h3>Example 1: Find Sum of All Columns</h3>
The following code shows how to sum the values of the rows across all columns in the DataFrame:
<b>#define new column that contains sum of all columns
df['sum_stats'] = df.sum(axis=1)
#view updated DataFrame
df
pointsassistsrebounds sum_stats
018511 34
12278 37
219710 36
31496 29
414126 32
51195 25
62099 38
728412 44
</b>
The <b>sum_stats</b> column contains the sum of the row values across all columns.
For example, here’s how the values were calculated:
Sum of row 0: 18 + 5 + 11 = <b>34</b>
Sum of row 1: 22 + 7 + 8 = <b>37</b>
Sum of row 2: 19 + 7 + 10 = <b>36</b>
And so on.
<h3>Example 2: Find Sum of Specific Columns</h3>
The following code shows how to sum the values of the rows across all columns in the DataFrame:
<b>#specify the columns to sum
cols = ['points', 'assists']
#define new column that contains sum of specific columns
df['sum_stats'] = df[cols].sum(axis=1)
#view updated DataFrame
df
pointsassistsrebounds sum_stats
018511 23
12278 29
219710 26
31496 23
414126 26
51195 20
62099 29
728412 32
</b>
The <b>sum_stats</b> column contains the sum of the row values across the ‘points’ and ‘assists’ columns.
For example, here’s how the values were calculated:
Sum of row 0: 18 + 5 + 11 = <b>23</b>
Sum of row 1: 22 + 7 = <b>29</b>
Sum of row 2: 19 + 7 = <b>26</b>
And so on.
<h2><span class="orange">How to Sum Specific Rows in Pandas (With Examples)</span></h2>
You can use the following methods to find the sum of specific rows in a pandas DataFrame:
<b>Method 1: Sum Specific Rows by Index</b>
<b>#sum rows in index positions 0, 1, and 4
df.iloc[[0, 1, 4]].sum()
</b>
<b>Method 2: Sum Specific Rows by Label</b>
<b>#sum rows with index labels 'A', 'B', and 'E'
df.loc[['A', 'B', 'E']].sum() 
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [28, 17, 19, 14, 23, 26, 5],   'rebounds': [5, 6, 4, 7, 14, 12, 9],   'assists': [10, 13, 7, 8, 4, 5, 8]})
#set index
df = df.set_index([pd.Index(['A', 'B', 'C', 'D', 'E', 'F', 'G'])])
#view DataFrame
print(df)
   points  rebounds  assists
A      28         5       10
B      17         6       13
C      19         4        7
D      14         7        8
E      23        14        4
F      26        12        5
G       5         9        8</b>
<h2>Example 1: Sum Specific Rows by Index</h2>
The following code shows how to sum the values in the rows with index values 0, 1, and 4 for each column in the DataFrame:
<b>#sum rows in index positions 0, 1, and 4
df.iloc[[0, 1, 4]].sum()
points      68
rebounds    25
assists     27
dtype: int64</b>
From the output we can see:
The sum of rows with index values 0, 1, and 4 for the <b>points</b> column is <b>68</b>.
The sum of rows with index values 0, 1, and 4 for the <b>rebounds</b> column is <b>25</b>.
The sum of rows with index values 0, 1, and 4 for the <b>assists</b> column is <b>27</b>.
Also note that you can sum a specific range of rows by using the following syntax:
<b>#sum rows in index positions between 0 and 4
df.iloc[0:4].sum()
points      78
rebounds    22
assists     38
dtype: int64
</b>
From the output we can see the sum of the rows with index values between 0 and 4 (not including 4) for each of the columns in the DataFrame.
<h2>Example 2: Sum Specific Rows by Label</h2>
The following code shows how to sum the values in the rows with index labels ‘A’, ‘B’, and ‘E’ for each column in the DataFrame:
<b>#sum rows with index labels 'A', 'B', and 'E'
df.loc[['A', 'B', 'E']].sum()
points      68
rebounds    25
assists     27
dtype: int64
</b>
From the output we can see:
The sum of rows with index values ‘A’, ‘B’, and ‘E’ for the <b>points</b> column is <b>68</b>.
The sum of rows with index values ‘A’, ‘B’, and ‘E’ for the <b>rebounds</b> column is <b>25</b>.
The sum of rows with index values ‘A’, ‘B’, and ‘E’ for the <b>assists</b> column is <b>27</b>.
<b>Related:</b>  The Difference Between loc vs. iloc in Pandas 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform a SUMIF Function in Pandas 
 How to Perform a GroupBy Sum in Pandas 
 How to Sum Columns Based on a Condition in Pandas 
<h2><span class="orange">How to Perform a SUMIF Function in Pandas</span></h2>
You can use the following syntax to find the sum of rows in a pandas DataFrame that meet some criteria:
<b>#find sum of each column, grouped by one column
<b>df.groupby('group_column').sum() 
</b>
#find sum of one specific column, grouped by one column
<b>df.groupby('group_column')['sum_column'].sum() </b>
</b>
The following examples show how to use this syntax with the following data frame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['a', 'a', 'b', 'b', 'b', 'c', 'c'],   'points': [5, 8, 14, 18, 5, 7, 7],   'assists': [8, 8, 9, 3, 8, 7, 4],   'rebounds': [1, 2, 2, 1, 0, 4, 1]})
#view DataFrame
df
teampointsassistsrebounds
0a581
1a882
2b1492
3b1831
4b580
5c774
6c741
</b>
<h3>Example 1: Perform a SUMIF Function on One Column</h3>
The following code shows how to find the sum of points for each team:
<b>df.groupby('team')['points'].sum()
team
a    13
b    37
c    14</b>
This tells us:
Team ‘a’ scored a total of <b>13</b> points
Team ‘b’ scored a total of <b>37</b> points
Team ‘c’ scored a total of <b>14</b> points
<h3>Example 2: Perform a SUMIF Function on Multiple Columns</h3>
The following code shows how to find the sum of points and rebounds for each team:
<b>df.groupby('team')[['points', 'rebounds']].sum()
pointsrebounds
team
a133
b373
c145
</b>
<h3>Example 3: Perform a SUMIF Function on All Columns</h3>
The following code shows how to find the sum of all columns in the data frame for each team:
<b><b>df.groupby('team').sum()
pointsassistsrebounds
team
a13163
b37203
c14115
</b></b>
<h2><span class="orange">How to Perform t-Tests in Pandas (3 Examples)</span></h2>
The following examples show how to perform three different t-tests using a pandas DataFrame:
Independent Two Sample t-Test
Welch’s Two Sample t-Test
Paired Samples t-Test
<h3>Example 1: Independent Two Sample t-Test in Pandas</h3>
An  independent two sample t-test  is used to determine if two population means are equal.
For example, suppose a professor wants to know if two different studying methods lead to different mean exam scores.
To test this, he recruits 10 students to use method A and 10 students to use method B.
The following code shows how to enter the scores of each student in a pandas DataFrame and then use the  ttest_ind()  function from the <b>SciPy</b> library to perform an independent two sample t-test:
<b>import pandas as pd
from scipy.stats import ttest_ind
#create pandas DataFrame
df = pd.DataFrame({'method': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',              'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'score': [71, 72, 72, 75, 78, 81, 82, 83, 89, 91, 80, 81, 81,             84, 88, 88, 89, 90, 90, 91]})
#view first five rows of DataFrame
df.head()
  method  score
0      A     71
1      A     72
2      A     72
3      A     75
4      A     78
#define samples
group1 = df[df['method']=='A']
group2 = df[df['method']=='B']
#perform independent two sample t-test
ttest_ind(group1['score'], group2['score'])
Ttest_indResult(statistic=-2.6034304605397938, pvalue=0.017969284594810425)
</b>
From the output we can see:
t test statistic: –<b>2.6034</b>
p-value: <b>0.0179</b>
Since the p-value is less than .05, we reject the null hypothesis of the t-test and conclude that there is sufficient evidence to say that the two methods lead to different mean exam scores.
<h3>Example 2: Welch’s t-Test in Pandas</h3>
 Welch’s t-test  is similar to the independent two sample t-test, except it does not assume that the two populations that the samples came from have  equal variance .
To perform Welch’s t-test on the exact same dataset as the previous example, we simply need to specify <b>equal_var=False</b> within the <b>ttest_ind</b>() function as follows:
<b>import pandas as pd
from scipy.stats import ttest_ind
#create pandas DataFrame
df = pd.DataFrame({'method': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',              'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'score': [71, 72, 72, 75, 78, 81, 82, 83, 89, 91, 80, 81, 81,             84, 88, 88, 89, 90, 90, 91]})
#define samples
group1 = df[df['method']=='A']
group2 = df[df['method']=='B']
#perform Welch's t-test
ttest_ind(group1['score'], group2['score'], equal_var=False)
Ttest_indResult(statistic=-2.603430460539794, pvalue=0.02014688617423973)
</b>
From the output we can see:
t test statistic: –<b>2.6034</b>
p-value: <b>0.0201</b>
Since the p-value is less than .05, we reject the null hypothesis of Welch’s t-test and conclude that there is sufficient evidence to say that the two methods lead to different mean exam scores.
<h3>Example 3: Paired Samples t-Test in Pandas</h3>
A  paired samples t-test  is used to determine if two population means are equal in which each observation in one sample can be paired with an observation in the other sample.
For example, suppose a professor wants to know if two different studying methods lead to different mean exam scores.
To test this, he recruits 10 students to use method A and then take a test. Then, he lets the same 10 students used method B to prepare for and take another test of similar difficulty.
Since all of the students appear in both samples, we can perform a paired samples t-test in this scenario.
The following code shows how to enter the scores of each student in a pandas DataFrame and then use the  ttest_rel()  function from the <b>SciPy</b> library to perform a paired samples t-test:
<b>import pandas as pd
from scipy.stats import ttest_rel
#create pandas DataFrame
df = pd.DataFrame({'method': ['A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A',              'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B', 'B'],   'score': [71, 72, 72, 75, 78, 81, 82, 83, 89, 91, 80, 81, 81,             84, 88, 88, 89, 90, 90, 91]})
#view first five rows of DataFrame
df.head()
  method  score
0      A     71
1      A     72
2      A     72
3      A     75
4      A     78
#define samples
group1 = df[df['method']=='A']
group2 = df[df['method']=='B']
#perform independent two sample t-test
ttest_rel(group1['score'], group2['score'])
Ttest_relResult(statistic=-6.162045351967805, pvalue=0.0001662872100210469)
</b>
From the output we can see:
t test statistic: –<b>6.1620</b>
p-value: <b>0.0001</b>
Since the p-value is less than .05, we reject the null hypothesis of the paired samples t-test and conclude that there is sufficient evidence to say that the two methods lead to different mean exam scores.
<h2><span class="orange">Pandas: How to Calculate a Difference Between Two Times</span></h2>
You can use the following syntax to calculate a difference between two times in a pandas DataFrame:
<b>#calculate time difference in hours
df['hours_diff'] = (df.end_time - df.start_time) / pd.Timedelta(hours=1)
#calculate time difference in minutes
df['min_diff'] = (df.end_time - df.start_time) / pd.Timedelta(minutes=1)
#calculate time difference in seconds
df['sec_diff'] = (df.end_time - df.start_time) / pd.Timedelta(seconds=1)
</b>
This particular example calculates the difference between the times in the <b>end_time</b> and <b>start_time</b> columns of some pandas DataFrame.
The following example shows how to use this syntax in practice.
<h2>Example: Calculate Difference Between Two Times in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df=pd.DataFrame({'start_time':pd.date_range(start='5/25/2020',periods=6,freq='15min'), 'end_time':pd.date_range(start='5/26/2020',periods=6,freq='30min')})
#view DataFrame
print(df)
           start_time            end_time
0 2020-05-25 00:00:00 2020-05-26 00:00:00
1 2020-05-25 00:15:00 2020-05-26 00:30:00
2 2020-05-25 00:30:00 2020-05-26 01:00:00
3 2020-05-25 00:45:00 2020-05-26 01:30:00
4 2020-05-25 01:00:00 2020-05-26 02:00:00
5 2020-05-25 01:15:00 2020-05-26 02:30:00
</b>
We can use the following syntax to calculate the time difference between the <b>start_time</b> and <b>end_time</b> columns in terms of hours, minutes, and seconds:
<b>#calculate time difference in hours
df['hours_diff'] = (df.end_time - df.start_time) / pd.Timedelta(hours=1)
#calculate time difference in minutes
df['min_diff'] = (df.end_time - df.start_time) / pd.Timedelta(minutes=1)
#calculate time difference in seconds
df['sec_diff'] = (df.end_time - df.start_time) / pd.Timedelta(seconds=1)
#view updated DataFrame
print(df)
           start_time            end_time  hours_diff  min_diff  sec_diff
0 2020-05-25 00:00:00 2020-05-26 00:00:00       24.00    1440.0   86400.0
1 2020-05-25 00:15:00 2020-05-26 00:30:00       24.25    1455.0   87300.0
2 2020-05-25 00:30:00 2020-05-26 01:00:00       24.50    1470.0   88200.0
3 2020-05-25 00:45:00 2020-05-26 01:30:00       24.75    1485.0   89100.0
4 2020-05-25 01:00:00 2020-05-26 02:00:00       25.00    1500.0   90000.0
5 2020-05-25 01:15:00 2020-05-26 02:30:00       25.25    1515.0   90900.0
</b>
The new columns contain the time differences between the <b>start_time</b> and <b>end_time</b> columns in various units.
For example, consider the first row:
The difference between the start time and end time is <b>24 hours</b>.
The difference between the start time and end time is <b>1,440 minutes</b>.
The difference between the start time and end time is <b>86,400 seconds</b>.
Note that in this example, the <b>start_time</b> and <b>end_time</b> columns are already formatted as datetimes.
If your time columns are instead currently formatted as strings, you can use <b>pd.to_datetime</b> to first convert each column to a datetime format before calculating the difference between the times:
<b>#convert columns to datetime format
df[['start_time', 'end_time']] = df[['start_time', 'end_time]].apply(pd.to_datetime)
</b>
You can then proceed to calculate the time differences between the columns since they are both now in a datetime format that pandas can recognize.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create a Date Range in Pandas 
 How to Extract Month from Date in Pandas 
 How to Convert Timestamp to Datetime in Pandas 
<h2><span class="orange">Pandas: How to Calculate Timedelta in Months</span></h2>
You can use the following function to calculate a timedelta in months between two columns of a pandas DataFrame:
<b>def month_diff(x, y):
    end = x.dt.to_period('M').view(dtype='int64')
    start = y.dt.to_period('M').view(dtype='int64')
    return end-start
</b>
The following example shows how to use this function in practice.
<h3>Example: Calculate Timedelta in Months in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'event': ['A', 'B', 'C'],   'start_date': ['20210101', '20210201', '20210401'],   'end_date': ['20210608', '20210209', '20210801'] })
#convert start date and end date columns to datetime
df['start_date'] = pd.to_datetime(df['start_date'])
df['end_date'] = pd.to_datetime(df['end_date'])
#view DataFrame
print(df)
  event start_date   end_date
0     A 2021-01-01 2021-06-08
1     B 2021-02-01 2021-02-09
2     C 2021-04-01 2021-08-01
</b>
Now suppose we’d like to calculate the timedelta (in months) between the <b>start_date</b> and <b>end_date</b> columns.
To do so, we’ll first define the following function:
<b>#define function to calculate timedelta in months between two columns
def month_diff(x, y):
    end = x.dt.to_period('M').view(dtype='int64')
    start = y.dt.to_period('M').view(dtype='int64')
    return end-start</b>
Next, we’ll use this function to calculate the timedelta in months between the <b>start_date</b> and <b>end_date</b> columns:
<b>#calculate month difference between start date and end date columns
df['month_difference'] = month_diff(df.end_date, df.start_date)
#view updated DataFrame
df
    eventstart_date  end_datemonth_difference
0A2021-01-012021-06-085
1B2021-02-012021-02-090
2C2021-04-012021-08-014</b>
The <b>month_difference</b> column displays the timedelta (in months) between the <b>start_date</b> and <b>end_date</b> columns.
<h2><span class="orange">How to Convert Timestamp to Datetime in Pandas</span></h2>
You can use the following basic syntax to convert a timestamp to a datetime in a pandas DataFrame:
<b>timestamp.to_pydatetime()
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Convert a Single Timestamp to a Datetime</h3>
The following code shows how to convert a single timestamp to a datetime:
<b>#define timestamp
stamp = pd.Timestamp('2021-01-01 00:00:00')
#convert timestamp to datetime
stamp.to_pydatetime()
datetime.datetime(2021, 1, 1, 0, 0)</b>
<h3>Example 2: Convert an Array of Timestamps to Datetimes</h3>
The following code shows how to convert an array of timestamps to a datetime:
<b>#define array of timestamps
stamps = pd.date_range(start='2020-01-01 12:00:00', periods=6, freq='H')
#view array of timestamps
stamps
DatetimeIndex(['2020-01-01 12:00:00', '2020-01-01 13:00:00',
               '2020-01-01 14:00:00', '2020-01-01 15:00:00',
               '2020-01-01 16:00:00', '2020-01-01 17:00:00'],
              dtype='datetime64[ns]', freq='H')
#convert timestamps to datetimes
stamps.to_pydatetime()
array([datetime.datetime(2020, 1, 1, 12, 0),
       datetime.datetime(2020, 1, 1, 13, 0),
       datetime.datetime(2020, 1, 1, 14, 0),
       datetime.datetime(2020, 1, 1, 15, 0),
       datetime.datetime(2020, 1, 1, 16, 0),
       datetime.datetime(2020, 1, 1, 17, 0)], dtype=object)</b>
<h3>Example 3: Convert a Pandas Column of Timestamps to Datetimes</h3>
The following code shows how to convert a pandas column of timestamps to datetimes:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'stamps': pd.date_range(start='2020-01-01 12:00:00',             periods=6,             freq='H'),   'sales': [11, 14, 25, 31, 34, 35]})
#convert column of timestamps to datetimes
df.stamps = df.stamps.apply(lambda x: x.date())
#view DataFrame
df
stamps        sales
02020-01-0111
12020-01-0114
22020-01-0125
32020-01-0131
42020-01-0134
52020-01-0135
</b>
<h2><span class="orange">Pandas: How to Append Data to Existing CSV File</span></h2>
You can use the following syntax in pandas to append data to an existing CSV file:
<b>df.to_csv('existing.csv', mode='a', index=False, header=False)
</b>
Here’s how to interpret the arguments in the <b>to_csv()</b> function:
<b>‘existing.csv’:</b> The name of the existing CSV file.
<b>mode=’a’:</b> Use the ‘append’ mode as opposed to ‘w’ – the default ‘write’ mode.
<b>index=False:</b> Do not include an index column when appending the new data.
<b>header=False:</b> Do not include a header when appending the new data.
The following step-by-step example shows how to use this function in practice.
<h3>Step 1: View Existing CSV File</h3>
Suppose we have the following existing CSV file:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/append1.png">
<h3>Step 2: Create New Data to Append</h3>
Let’s create a new pandas DataFrame to append to the existing CSV file:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['D', 'D', 'E', 'E'],   'points': [6, 4, 4, 7],   'rebounds': [15, 18, 9, 12]})
#view DataFrame
df
        teampointsrebounds
0D615
1D418
2E49
3E712
</b>
<h3>Step 3: Append New Data to Existing CSV</h3>
The following code shows how to append this new data to the existing CSV file:
<b>df.to_csv('existing.csv', mode='a', index=False, header=False)
</b>
<h3>Step 4: View Updated CSV</h3>
When we open the existing CSV file, we can see that the new data has been appended:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/append2.png">
<h3>Notes on Appending Data</h3>
When appending data to an existing CSV file, be sure to check whether the existing CSV has an index column or not.
If the existing CSV file does not have an index file, you need to specify <b>index=False</b> in the <b>to_csv()</b> function when appending the new data to prevent pandas from adding an index column.
<h2><span class="orange">How to Export Pandas DataFrame to CSV (With Example)</span></h2>
You can use the following syntax to export a pandas DataFrame to a CSV file:
<b>df.to_csv(r'C:\Users\Bob\Desktop\my_data.csv', index=False)
</b>
Note that <b>index=False</b> tells Python to drop the index column when exporting the DataFrame. Feel free to drop this argument if you’d like to keep the index column.
The following step-by-step example shows how to use this function in practice.
<h3>Step 1: Create the Pandas DataFrame</h3>
First, let’s create a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
</b>
<h3>Step 2: Export the DataFrame to CSV File</h3>
Next, let’s export the DataFrame to a CSV file:
<b><b>#export DataFrame to CSV file</b>
df.to_csv(r'C:\Users\Bob\Desktop\my_data.csv', index=False)
</b>
<h3>Step 3: View the CSV File</h3>
Lastly, we can navigate to the location where we exported the CSV file and view it:
<b>points,assists,rebounds
25,5,11
12,7,8
15,7,10
14,9,6
19,12,6
23,9,5
</b>
Notice that the index column is not in the file since we specified <b>index=False</b>.
Also notice that the headers are in the file since the default argument in the to_csv() function is <b>headers=True</b>.
Just for fun, here’s what the CSV file would look like if we had left out the <b>index=False</b> argument:
<b>,points,assists,rebounds
0,25,5,11
1,12,7,8
2,15,7,10
3,14,9,6
4,19,12,6
5,23,9,5</b>
Reference the  pandas documentation  for an in-depth guide to the <b>to_csv()</b> function.
<h2><span class="orange">How to Convert a Pandas DataFrame to JSON</span></h2>
Often you might be interested in converting a pandas DataFrame to a JSON format.
Fortunately this is easy to do using the  to_json()  function, which allows you to convert a DataFrame to a JSON string with one of the following formats:
<b>‘split’ :</b> dict like {‘index’ -> [index], ‘columns’ -> [columns], ‘data’ -> [values]}
<b>‘records’ :</b> list like [{column -> value}, … , {column -> value}]
<b>‘index’ :</b> dict like {index -> {column -> value}}
<b>‘columns’ :</b> dict like {column -> {index -> value}}
<b>‘values’ :</b> just the values array
<b>‘table’ :</b> dict like {‘schema’: {schema}, ‘data’: {data}}
This tutorial shows how to convert a DataFrame to each of the six formats using the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 19],   'assists': [5, 7, 7, 12]})  
#view DataFrame
df
        pointsassists
0255
1127
2157
31912
</b>
<h3>Method 1: ‘Split’</h3>
<b>df.to_json(orient='split')
{
   "columns": [
      "points",
      "assists"
   ],
   "index": [
      0,
      1,
      2,
      3
   ],
   "data": [
      [
         25,
         5
      ],
      [
         12,
         7
      ],
      [
         15,
         7
      ],
      [
         19,
         12
      ]
   ]
}
</b>
<h3>Method 2: ‘Records’</h3>
<b>df.to_json(orient='records')
[
   {
      "points": 25,
      "assists": 5
   },
   {
      "points": 12,
      "assists": 7
   },
   {
      "points": 15,
      "assists": 7
   },
   {
      "points": 19,
      "assists": 12
   }
] </b>
<h3>Method 3: ‘Index’</h3>
<b>df.to_json(orient='index') 
{
   "0": {
      "points": 25,
      "assists": 5
   },
   "1": {
      "points": 12,
      "assists": 7
   },
   "2": {
      "points": 15,
      "assists": 7
   },
   "3": {
      "points": 19,
      "assists": 12
   }
}</b>
<h3>Method 4: ‘Columns’</h3>
<b>df.to_json(orient='columns') 
{
   "points": {
      "0": 25,
      "1": 12,
      "2": 15,
      "3": 19
   },
   "assists": {
      "0": 5,
      "1": 7,
      "2": 7,
      "3": 12
   }
}
</b>
<h3>Method 5: ‘Values’</h3>
<b>df.to_json(orient='values') 
[
   [
      25,
      5
   ],
   [
      12,
      7
   ],
   [
      15,
      7
   ],
   [
      19,
      12
   ]
]</b>
<h3>Method 6: ‘Table’</h3>
<b>df.to_json(orient='table') 
{
   "schema": {
      "fields": [
         {
            "name": "index",
            "type": "integer"
         },
         {
            "name": "points",
            "type": "integer"
         },
         {
            "name": "assists",
            "type": "integer"
         }
      ],
      "primaryKey": [
         "index"
      ],
      "pandas_version": "0.20.0"
   },
   "data": [
      {
         "index": 0,
         "points": 25,
         "assists": 5
      },
      {
         "index": 1,
         "points": 12,
         "assists": 7
      },
      {
         "index": 2,
         "points": 15,
         "assists": 7
      },
      {
         "index": 3,
         "points": 19,
         "assists": 12
      }
   ]
}</b>
<h3>How to Export a JSON File</h3>
You can use the following syntax to export a JSON file to a specific file path on your computer:
<b>#create JSON file 
json_file = df.to_json(orient='records') 
#export JSON file
with open('my_data.json', 'w') as f:
    f.write(json_file)
</b>
<em>You can find the complete documentation for the pandas to_json() function  here .</em>
<h2><span class="orange">How to Convert Pandas DataFrame Columns to Strings</span></h2>
Often you may wish to convert one or more columns in a pandas DataFrame to strings. Fortunately this is easy to do using the built-in pandas  astype(str)  function.
This tutorial shows several examples of how to use this function.
<h3>Example 1: Convert a Single DataFrame Column to String</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],   'points': [25, 20, 14, 16, 27],   'assists': [5, 7, 7, 8, 11]})
#view DataFrame 
df
        playerpointsassists
0A255
1B207
2C147
3D168
4E2711
</b>
We can identify the data type of each column by using <b>dtypes:</b>
<b>df.dtypes
player     object
points      int64
assists     int64
dtype: object
</b>
We can see that the column “player” is a string while the other two columns “points” and “assists” are integers.
We can convert the column “points” to a string by simply using <b>astype(str) </b>as follows:
<b>df['points'] = df['points'].astype(str)
</b>
We can verify that this column is now a string by once again using <b>dtypes:</b>
<b>df.dtypes
player     object
points     object
assists     int64
dtype: object</b>
<h3>Example 2: Convert Multiple DataFrame Columns to Strings</h3>
We can convert both columns “points” and “assists” to strings by using the following syntax:
<b>df[['points', 'assists']] = df[['points', 'assists']].astype(str)</b>
And once again we can verify that they’re strings by using <b>dtypes:</b>
<b>df.dtypes
player     object
points     object
assists    object
dtype: object</b>
<h3>Example 3: Convert an Entire DataFrame to Strings</h3>
Lastly, we can convert every column in a DataFrame to strings by using the following syntax:
<b>#convert every column to strings
df = df.astype(str)
#check data type of each column
df.dtypes
player     object
points     object
assists    object
dtype: object
</b>
<em>You can find the complete documentation for the astype() function  here .</em>
<h2><span class="orange">Pandas: How to Get Top N Rows by Group</span></h2>
You can use the following basic syntax to get the top N rows by group in a pandas DataFrame:
<b>df.groupby('group_column').head(2).reset_index(drop=True)
</b>
This particular syntax will return the top <b>2</b> rows by group.
Simply change the value inside the <b>head()</b> function to return a different number of top rows.
The following examples show how to use this syntax with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'G', 'F', 'F', 'G', 'G', 'F', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 9, 9, 4, 7, 7]})
#view DataFrame
print(df)
  team position  points
0    A        G       5
1    A        G       7
2    A        G       7
3    A        F       9
4    A        F      12
5    B        G       9
6    B        G       9
7    B        F       4
8    B        F       7
9    B        F       7
</b>
<h2>Example 1: Get Top N Rows Grouped by One Column</h2>
The following code shows how to return the top 2 rows, grouped by the <b>team</b> variable:
<b>#get top 2 rows grouped by team
df.groupby('team').head(2).reset_index(drop=True)
        teamposition  points
0AG  5
1AG  7
2BG  9
3BG  9
</b>
The output displays the top 2 rows, grouped by the <b>team</b> variable.
<h2>Example 2: Get Top N Rows Grouped by Multiple Columns</h2>
The following code shows how to return the top 2 rows, grouped by the <b>team</b> and <b>position</b> variables:
<b>#get top 2 rows grouped by team and position
df.groupby(['team', 'position']).head(2).reset_index(drop=True)
teamposition  points
0AG  5
1AG  7
2AF  9
3AF  12
4BG  9
5BG  9
6BF  4
7BF  7</b>
The output displays the top 2 rows, grouped by the <b>team</b> and <b>position</b> variables.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Find Unique Values in a Column 
 Pandas: How to Find Unique Values in Multiple Columns 
 Pandas: How to Count Occurrences of Specific Value in Column 
<h2><span class="orange">How to Create a Train and Test Set from a Pandas DataFrame</span></h2>
When fitting  machine learning models  to datasets, we often split the dataset into two sets:
<b>1. Training Set:</b> Used to train the model (70-80% of original dataset)
<b>2. Testing Set:</b> Used to get an unbiased estimate of the model performance (20-30% of original dataset)
In Python, there are two common ways to split a pandas DataFrame into a training set and testing set:
<b>Method 1: Use train_test_split() from sklearn</b>
<b>from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.2, random_state=0)</b>
<b>Method 2: Use sample() from pandas</b>
<b>train = df.sample(frac=0.8,random_state=0)
test = df.drop(train.index)</b>
The following examples show how to use each method with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(1)
#create DataFrame with 1,000 rows and 3 columns
df = pd.DataFrame({'x1': np.random.randint(30, size=1000),   'x2': np.random.randint(12, size=1000),   'y': np.random.randint(2, size=1000)})
#view first few rows of DataFrame
df.head()
        x1x2y
0511
11180
21241
3870
4900
</b>
<h3>Example 1: Use train_test_split() from sklearn</h3>
The following code shows how to use the <b>train_test_split()</b> function from <b>sklearn</b> to split the pandas DataFrame into training and test sets:
<b>from sklearn.model_selection import train_test_split
#split original DataFrame into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=0)
#view first few rows of each set
print(train.head())
     x1  x2  y
687  16   2  0
500  18   2  1
332   4  10  1
979   2   8  1
817  11   1  0
print(test.head())
     x1  x2  y
993  22   1  1
859  27   6  0
298  27   8  1
553  20   6  0
672   9   2  1
#print size of each set
print(train.shape, test.shape)
(800, 3) (200, 3)
</b>
From the output we can see that two sets have been created:
Training set: 800 rows and 3 columns
Testing set: 200 rows and 3 columns
Note that <b>test_size</b> controls the percentage of observations from the original DataFrame that will belong to the testing set and the <b>random_state</b> value makes the split reproducible.
<h3>Example 2: Use sample() from pandas</h3>
The following code shows how to use the <b>sample()</b> function from <b>pandas </b>to split the pandas DataFrame into training and test sets:
<b>#split original DataFrame into training and testing sets
train = df.sample(frac=0.8,random_state=0)
test = df.drop(train.index)
#view first few rows of each set
print(train.head())
     x1  x2  y
993  22   1  1
859  27   6  0
298  27   8  1
553  20   6  0
672   9   2  1
print(test.head())
    x1  x2  y
9   16   5  0
11  12  10  0
19   5   9  0
23  28   1  1
28  18   0  1
#print size of each set
print(train.shape, test.shape)
(800, 3) (200, 3)
</b>
From the output we can see that two sets have been created:
Training set: 800 rows and 3 columns
Testing set: 200 rows and 3 columns
Note that <b>frac </b>controls the percentage of observations from the original DataFrame that will belong to the training set and the <b>random_state</b> value makes the split reproducible.
<h2><span class="orange">How to Transpose a Pandas DataFrame without Index</span></h2>
You can use the following syntax to transpose a pandas DataFrame and leave out the index:
<b>df.set_index('first_col').T
</b>
This simply sets the first column of the DataFrame as the index and then performs the transpose.
The following example shows how to use this syntax in practice.
<h2>Example: Transpose Pandas DataFrame without Index</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F'],   'points': [18, 22, 19, 14, 14, 11],   'assists': [5, 7, 7, 9, 12, 9]})
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9</b>
If we transpose the DataFrame, the index values will be shown along the top:
<b>#transpose DataFrame
df.T
012345
teamABCDEF
points182219141411
assists5779129
</b>
To transpose the DataFrame without the index, we can first use the <b>set_index()</b> function:
<b>#transpose DataFrame without index
df.set_index('team').T
teamABCDEF
points182219141411
assists5779129</b>
Notice that the index values are no longer shown along the top of the transposed DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop Columns in Pandas 
 How to Drop Rows in Pandas DataFrame Based on Condition 
 How to Drop Rows that Contain a Specific Value in Pandas 
<h2><span class="orange">How to Fix in Pandas: TypeError: no numeric data to plot</span></h2>
One error you may encounter when using pandas is:
<b>TypeError: no numeric data to plot</b>
This error occurs when you attempt to plot values from a pandas DataFrame, but there are no numeric values to plot.
This error typically occurs when you think a certain column in the DataFrame is numeric but it turns out to be a different data type.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B'],   'points': ['5', '7', '7', '9', '12'],   'rebounds': ['11', '8', '10', '6', '6'],   'blocks': ['4', '7', '7', '6', '5']})
#view DataFrame
df
teampointsrebounds blocks
0A511 4
1A78 7
2B710 7
3B96 6
4B126 5
</b>
Now suppose we attempt to create a line plot for the three variables that we believe are numeric: points, rebounds, and blocks:
<b>#attempt to create line plot for points, rebounds, and blocks
df[['points', 'rebounds', 'blocks']].plot()
ValueError: no numeric data to plot
</b>
We receive an error because none of these columns are actually numeric.
<h3>How to Fix the Error</h3>
We can use the <b>dtypes</b> function to see what data type each column is in our DataFrame:
<b>#display data type of each column in DataFrame
df.dtypes
team        object
points      object
rebounds    object
blocks      object
dtype: object
</b>
We can see that none of the columns in the DataFrame are numeric.
We can use the <b>.astype()</b> function to convert specific columns to numeric:
<b>#convert points, rebounds, and blocks columns to numeric
df['points']=df['points'].astype(float)
df['rebounds']=df['rebounds'].astype(float)
df['blocks']=df['blocks'].astype(float)</b>
We can then use the <b>plot()</b> function again:
<b>#create line plot for points, rebounds, and blocks
df[['points', 'rebounds', 'blocks']].plot()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/float1.png">
We’re able to successfully create a line plot for points, rebounds, and blocks because each variable is now numeric.
We can verify this by using the <b>dtypes</b> function once again:
<b>#display data type of each column in DataFrame
df.dtypes
team         object
points      float64
rebounds    float64
blocks      float64
dtype: object
</b>
<h2><span class="orange">Pandas: How to Get Unique Values from Index Column</span></h2>
You can use the following methods to get the unique values from the index column of a pandas DataFrame:
<b>Method 1: Get Unique Values from Index Column</b>
<b>df.index.unique()
</b>
<b>Method 2: Get Unique Values from Specific Column in MultiIndex</b>
<b>df.index.unique('some_column')</b>
The following examples show how to use this syntax in practice.
<h2>Example 1: Get Unique Values from Index Column</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]},   index = [0, 1, 1, 1, 2, 2, 3, 4])
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
1    C      19        7
1    D      14        9
2    E      14       12
2    F      11        9
3    G      20        9
4    H      28        4</b>
We can use the following syntax to get the unique values from the index column of the DataFrame:
<b>#get unique values from index column 
df.index.unique()
Int64Index([0, 1, 2, 3, 4], dtype='int64')</b>
The output displays each of the unique values from the index column.
We can also use the <b>len()</b> function to count the number of unique values in the index column:
<b>#count number of unique values in index column 
len(df.index.unique())
5</b>
We can see that there are <b>5</b> unique values in the index column of the DataFrame.
<h2>Example 2: Get Unique Values from Specific Column in MultiIndex</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#define index values
index_names = pd.MultiIndex.from_tuples([('West', 'A'),                         ('West', 'A'),                         ('West', 'B'),                         ('East', 'C'),                         ('East', 'C'),                         ('East', 'D')],                       names=['Division', 'Team'])
#define data values
data = {'Sales': [12, 44, 29, 35, 44, 19]}
#create DataFrame
df = pd.DataFrame(data, index=index_names)
#view DataFrame
print(df)
               Sales
Division Team       
West     A        12
         A        44
         B        29
East     C        35
         C        44
         D        19
</b>
Notice that this DataFrame has a multiIndex.
We can use the following syntax to get the unique values from just the <b>Team</b> column from the multiIndex:
<b>#get unique values from Team column in multiIndex
df.index.unique('Team')
Index(['A', 'B', 'C', 'D'], dtype='object', name='Team')
</b>
The output displays the four unique values from the <b>Team</b> column of the multiIndex: A, B, C, and D.
We can use similar syntax to extract the unique values from the <b>Division</b> column of the multiIndex:
<b>#get unique values from Division column in multiIndex
df.index.unique('Division')
Index(['West', 'East'], dtype='object', name='Division')
</b>
The output displays the two unique values from the <b>Division </b>column of the multiIndex: West and East.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in pandas:
 How to Convert Index to Column in Pandas 
 How to Rename Index in Pandas 
 How to Set Column as Index in Pandas 
<h2><span class="orange">How to Find Unique Values in Multiple Columns in Pandas</span></h2>
Often you may be interested in finding all of the unique values across multiple columns in a pandas DataFrame. Fortunately this is easy to do using the pandas  unique()  function combined with the  ravel()  function:
<b>unique()</b>: Returns unique values in order of appearance.
<b>ravel(): </b>Returns a flattened data series.
For example, suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'col1': ['a', 'b', 'c', 'd', 'e'],   'col2': ['a', 'c', 'e', 'f', 'g'],   'col3': [11, 8, 10, 6, 6]})
#view DataFrame 
print(df)
  col1 col2  col3
0    a    a    11
1    b    c     8
2    c    e    10
3    d    f     6
4    e    g     6
</b>
<h3>Return Array of Unique Values</h3>
The following code shows how to find the unique values in <b>col1 </b>and <b>col2</b>:
<b>pd.unique(df[['col1', 'col2']].values.ravel())
array(['a', 'b', 'c', 'e', 'd', 'f', 'g'], dtype=object)
</b>
From the output we can see that there are <b>7 </b>unique values across these two columns: <b>a, b, c, d, e, f, g</b>.
<h3>Return DataFrame of Unique Values</h3>
If you’d like to return these values as a DataFrame instead of an array, you can use the following code:
<b>uniques = pd.unique(df[['col1', 'col2']].values.ravel())
pd.DataFrame(uniques)
0
0a
1b
2c
3e
4d
5f
6g
</b>
<h3>Return Number of Unique Values</h3>
If you simply want to know the number of unique values across multiple columns, you can use the following code:
<b>uniques = pd.unique(df[['col1', 'col2']].values.ravel())
len(uniques)
7</b>
This tell us that there are <b>7 </b>unique values across these two columns.
<h2><span class="orange">How to Select Unique Rows in a Pandas DataFrame</span></h2>
You can use the following syntax to select unique rows in a pandas DataFrame:
<b>df = df.drop_duplicates()
</b>
And you can use the following syntax to select unique rows across specific columns in a pandas DataFrame:
<b>df = df.drop_duplicates(subset=['col1', 'col2', ...])</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'a': [4, 4, 3, 8],   'b': [2, 2, 6, 8],   'c': [2, 2, 9, 9]})
#view DataFrame
df
abc
0422
1422
2369
3889
</b>
<h3>Example 1: Select Unique Rows Across All Columns</h3>
The following code shows how to select unique rows across all columns of the pandas DataFrame:
<b>#drop duplicates from DataFrame
df = df.drop_duplicates()
#view DataFrame
df
abc
0422
2369
3889
</b>
The first and second row were duplicates, so pandas dropped the second row.
By default, the <b>drop_duplicates()</b> function will keep the first duplicate. However, you can specify to keep the last duplicate instead:
<b>#drop duplicates from DataFrame, keep last duplicate
df = df.drop_duplicates(keep='last')
#view DataFrame
df
abc
1422
2369
3889</b>
<h3>Example 2: Select Unique Rows Across Specific Columns</h3>
The following code shows how to select unique rows across just column ‘c’ in the DataFrame:
<b>#drop duplicates from column 'c' in DataFrame
df = df.drop_duplicates(subset=['c'])
#view DataFrame
df
abc
0422
2369</b>
Two rows were dropped from the DataFrame.
<h2><span class="orange">Pandas: How to Find Unique Values in a Column</span></h2>
The easiest way to obtain a list of unique values in a pandas DataFrame column is to use the  unique()  function.
This tutorial provides several examples of how to use this function with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C'],   'conference': ['East', 'East', 'East', 'West', 'West', 'East'],   'points': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
        teamconference  points
0AEast    11
1AEast    8
2AEast    10
3BWest    6
4BWest    6
5CEast    5</b>
<h3>Find Unique Values in One Column</h3>
The following code shows how to find the unique values in a single column of the DataFrame:
<b>df.team.unique()
array(['A', 'B', 'C'], dtype=object)</b>
We can see that the unique values in the team column include “A”, “B”, and “C.”
<h3>Find Unique Values in All Columns</h3>
The following code shows how to find the unique values in all columns of the DataFrame:
<b>for col in df:
  print(df[col].unique())
['A' 'B' 'C']
['East' 'West']
[11  8 10  6  5]
</b>
<h3>Find and Sort Unique Values in a Column</h3>
The following code shows how to find and sort by unique values in a single column of the DataFrame:
<b>#find unique points values
points = df.points.unique()
#sort values smallest to largest
points.sort()
#display sorted values
points
array([ 5,  6,  8, 10, 11])
</b>
<h3>Find and Count Unique Values in a Column</h3>
The following code shows how to find and count the occurrence of unique values in a single column of the DataFrame:
<b>df.team.value_counts()
A    3
B    2
C    1
Name: team, dtype: int64
</b>
<h2><span class="orange">How to Unpivot a Pandas DataFrame (With Example)</span></h2>
In pandas, you can use the  melt()  function to unpivot a DataFrame – converting it from a wide format to a  long format .
This function uses the following basic syntax:
<b>df_unpivot = pd.melt(df, id_vars='col1', value_vars=['col2', 'col3', ...])
</b>
where:
<b>id_vars</b>: The columns to use as identifiers
<b>value_vars</b>: The columns to unpivot
The following example shows how to use this syntax in practice.
<h3>Example: Unpivot a Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],   'points': [18, 22, 19, 14, 14],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6</b>
We can use the following syntax to “unpivot” the DataFrame:
<b>#unpivot DataFrame from wide format to long format
df_unpivot = pd.melt(df, id_vars='team', value_vars=['points', 'assists', 'rebounds'])
#view updated DataFrame
print(df_unpivot)
   team  variable  value
0     A    points     18
1     B    points     22
2     C    points     19
3     D    points     14
4     E    points     14
5     A   assists      5
6     B   assists      7
7     C   assists      7
8     D   assists      9
9     E   assists     12
10    A  rebounds     11
11    B  rebounds      8
12    C  rebounds     10
13    D  rebounds      6
14    E  rebounds      6
</b>
We used the <b>team</b> column as the identifier column and we chose to unpivot the <b>points</b>, <b>assists</b>, and <b>rebounds</b> columns.
The result is a DataFrame in a long format.
Note that we can also use the <b>var_name</b> and <b>value_name</b> arguments to specify the names of the columns in the unpivoted DataFrame:
<b>#unpivot DataFrame from wide format to long format 
df_unpivot = pd.melt(df, id_vars='team', value_vars=['points', 'assists', 'rebounds'],
             var_name='metric', value_name='amount')
#view updated DataFrame
print(df_unpivot)
   team    metric  amount
0     A    points      18
1     B    points      22
2     C    points      19
3     D    points      14
4     E    points      14
5     A   assists       5
6     B   assists       7
7     C   assists       7
8     D   assists       9
9     E   assists      12
10    A  rebounds      11
11    B  rebounds       8
12    C  rebounds      10
13    D  rebounds       6
14    E  rebounds       6
</b>
Notice that the new columns are now labeled <b>metric</b> and <b>amount</b>.
<h2><span class="orange">Pandas: Update Column Values Based on Another DataFrame</span></h2>
Often you may want to update the values in one column of a pandas DataFrame using values from another DataFrame.
Fortunately this is easy to do using the <b>merge()</b> function in pandas.
The following example shows how to do so.
<h2>Example: Update Column Values in Pandas DataFrame Based on Another DataFrame</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [18, 22, 19, 14, 14, 11, 20, 28],    'assists': [0, 0, 0, 1, 0, 0, 0, 1]})
#view DataFrame
print(df1)
  team  points  assists
0    A      18        0
1    B      22        0
2    C      19        0
3    D      14        1
4    E      14        0
5    F      11        0
6    G      20        0
7    H      28        1</b>
Now suppose the values in the <b>assists</b> column are not updated in this DataFrame.
However, suppose we have the following second DataFrame that does have updated values for the <b>assists</b> column:
<b>#create second DataFrame
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [18, 22, 19, 14, 14, 11, 20, 28],    'assists': [8, 7, 7, 4, 9, 12, 3, 5]})
#view second DataFrame
print(df2)
  team  points  assists
0    A      18        8
1    B      22        7
2    C      19        7
3    D      14        4
4    E      14        9
5    F      11       12
6    G      20        3
7    H      28        5
</b>
We can use the following syntax to update the values in the <b>assists</b> column of the first DataFrame using the values in the <b>assists</b> column of the second DataFrame:
<b>#merge two DataFrames
df1 = df1.merge(df2, on='team', how='left')
#drop original DataFrame columns
df1.drop(['points_x', 'assists_x'], inplace=True, axis=1)
#rename columns
df1.rename(columns={'points_y':'points','assists_y':'assists'}, inplace=True)
#view updated DataFrame
print(df1)
  team  points  assists
0    A      18        8
1    B      22        7
2    C      19        7
3    D      14        4
4    E      14        9
5    F      11       12
6    G      20        3
7    H      28        5
</b>
Notice that the values in the <b>assists</b> column of the first DataFrame have been updated using the values from the <b>assists</b> column in the second DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Drop First Row in Pandas DataFrame 
 How to Drop First Column in Pandas DataFrame 
 How to Drop Duplicate Columns in Pandas 
<h2><span class="orange">Pandas: How to Use First Column as Index</span></h2>
You can use the following methods to use the first column as the index column in a pandas DataFrame:
<b>Method 1: Use First Column as Index When Importing DataFrame</b>
<b>df = pd.read_csv('my_data.csv', index_col=0)
</b>
<b>Method 2: Use First Column as Index with Existing DataFrame</b>
<b>df = df.set_index(['column1'])
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Use First Column as Index When Importing DataFrame</h2>
Suppose we have the following CSV file called <b>my_data.csv</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/set1.jpg"501">
If we import the CSV file without specifying an index column, pandas will simply create an index column with numerical values starting at 0:
<b>#import CSV file without specifying index column
df = pd.read_csv('my_data.csv')
#view DataFrame
print(df)
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
However, we can use the <b>index_col</b> argument to specify that the first column in the CSV file should be used as the index column:
<b>#import CSV file and specify index column
df = pd.read_csv('my_data.csv', index_col=0)
#view DataFrame
print(df)
      points  assists
team                 
A         18        5
B         22        7
C         19        7
D         14        9
E         14       12
F         11        9
G         20        9
H         28        4
</b>
Notice that the <b>team</b> column is now used as the index column.
<h2>Example 2: Use First Column as Index with Existing DataFrame</h2>
Suppose we have the following existing pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
df
  team  points  assists
0    A      18        5
1    B      22        7
2    C      19        7
3    D      14        9
4    E      14       12
5    F      11        9
6    G      20        9
7    H      28        4
</b>
We can use the <b>set_index()</b> function to set the <b>team</b> column as the index column:
<b>#set 'team' column as index column
df = df.set_index(['team'])
#view updated DataFrame
print(df)
      points  assists
team                 
A         18        5
B         22        7
C         19        7
D         14        9
E         14       12
F         11        9
G         20        9
H         28        4
</b>
Notice that the <b>team</b> column is now used as the index column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Select Columns by Index in a Pandas DataFrame 
 How to Rename Index in Pandas DataFrame 
 How to Drop Columns by Index in Pandas 
<h2><span class="orange">Pandas: How to Represent value_counts as Percentage</span></h2>
You can use the <b>value_counts()</b> function in pandas to count the occurrences of values in a given column of a DataFrame.
To represent the values as percentages, you can use one of the following methods:
<b>Method 1: Represent Value Counts as Percentages (Formatted as Decimals)</b>
<b>df.my_col.value_counts(normalize=True)
</b>
<b>Method 2: Represent Value Counts as Percentages (Formatted with Percent Symbols)</b>
<b>df.my_col.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'</b>
<b>Method 3: Represent Value Counts as Percentages (Along with Counts)</b>
<b>counts = df.my_col.value_counts()</b>
<b>percs = df.my_col.value_counts(normalize=True)</b>
<b>pd.concat([counts,percs], axis=1, keys=['count', 'percentage'])</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'B', 'C'],   'points': [15, 12, 18, 20, 22, 28, 35, 40]})
#view DataFrame
print(df)
  team  points
0    A      15
1    A      12
2    B      18
3    B      20
4    B      22
5    B      28
6    B      35
7    C      40
</b>
<h2>Example 1: Represent Value Counts as Percentages (Formatted as Decimals)</h2>
The following code shows how to count the occurrence of each value in the <b>team</b> column and represent the occurrences as a percentage of the total, formatted as a decimal:
<b>#count occurrence of each value in 'team' column as percentage of total
df.team.value_counts(normalize=True)
B    0.625
A    0.250
C    0.125
Name: team, dtype: float64
</b>
From the output we can see:
The value <b>B</b> represents 62.5% of the occurrences in the team column.
The value <b>A</b> represents 25% of the occurrences in the team column.
The value <b>C</b> represents 12.5% of the occurrences in the team column.
Notice that the percentages are formatted as decimals.
<h2>Example 2: Represent Value Counts as Percentages (Formatted with Percent Symbols)</h2>
The following code shows how to count the occurrence of each value in the <b>team</b> column and represent the occurrences as a percentage of the total, formatted with percent symbols:
<b>#count occurrence of each value in 'team' column as percentage of total
df.team.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'
B    62.5%
A    25.0%
C    12.5%
Name: team, dtype: object
</b>
Notice that the percentages are formatted as strings with percent symbols.
<h2>Example 3: Represent Value Counts as Percentages (Along with Counts)</h2>
The following code shows how to count the occurrence of each value in the <b>team</b> column and represent the occurrences as both counts and percentages:
<b>#count occurrence of each value in 'team' column
counts = df.team.value_counts()
#count occurrence of each value in 'team' column as percentage of total 
percs = df.team.value_counts(normalize=True)
#concatenate results into one DataFrame
pd.concat([counts,percs], axis=1, keys=['count', 'percentage'])
        countpercentage
B50.625
A20.250
C10.125</b>
Notice that the <b>count</b> column displays the count of each unique value in the team column while the <b>percentage</b> column displays each unique value as a percentage of the total occurrences.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Plot Value Counts 
 Pandas: How to Use GroupBy and Value Counts 
 Pandas: How to Plot Histograms by Group 
<h2><span class="orange">Pandas: How to Sort Results of value_counts()</span></h2>
You can use the <b>value_counts()</b> function in pandas to count the occurrences of values in a given column of a DataFrame.
You can use one of the following methods to sort the results of the <b>value_counts()</b> function:
<b>Method 1: Sort Counts in Descending Order (Default)</b>
<b>df.my_column.value_counts() </b>
<b>Method 2: Sort Counts in Ascending Order</b>
<b>df.my_column.value_counts().sort_values()</b>
<b>Method 3: Sort Counts in Order They Appear in DataFrame</b>
<b>df.my_column.value_counts()[df.my_column.unique()]</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'B', 'C'],   'points': [15, 12, 18, 20, 22, 28, 35, 40]})
#view DataFrame
print(df)
  team  points
0    A      15
1    A      12
2    B      18
3    B      20
4    B      22
5    B      28
6    B      35
7    C      40
</b>
<h2>Example 1: Sort Counts in Descending Order</h2>
The following code shows how to count the occurrences of each unique value in the team column and sort the counts in descending order:
<b>#count occurrences of each value in team column and sort in descending order
df.team.value_counts()
B    5
A    2
C    1
Name: team, dtype: int64
</b>
Notice that the counts are sorted in descending order by default.
<h2>Example 2: Sort Counts in Ascending Order</h2>
The following code shows how to count the occurrences of each unique value in the team column and sort the counts in ascending order:
<b>#count occurrences of each value in team column and sort in ascending order
df.team.value_counts().sort_values()
C    1
A    2
B    5
Name: team, dtype: int64
</b>
Notice that the counts are now sorted in ascending order, i.e. smallest to largest.
<h2>Example 3: Sort Counts in Order they Appear in DataFrame</h2>
The following code shows how to count the occurrences of each unique value in the team column and sort the counts in order in which the unique values appear in the DataFrame:
<b>#count occurrences of each value in team column and sort in order they appear
df.team.value_counts()[df.team.unique()]
A    2
B    5
C    1
Name: team, dtype: int64
</b>
Notice that the counts are now sorted based on the order in which the unique values appear in the DataFrame.
For example, the value ‘A’ occurs first in the team column, then ‘B’ occurs, then ‘C’ occurs.
Thus, this is the order in which the counts appear in the output.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Plot Value Counts 
 Pandas: How to Use GroupBy and Value Counts 
 Pandas: How to Represent value_counts as Percentage 
<h2><span class="orange">How to Use Pandas value_counts() Function (With Examples)</span></h2>
You can use the <b>value_counts()</b> function to count the frequency of unique values in a pandas Series.
This function uses the following basic syntax:
<b>my_series.value_counts()
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Count Frequency of Unique Values</h3>
The following code shows how to count the occurrences of unique values in a pandas Series:
<b>import pandas as pd
#create pandas Series
my_series = pd.Series([3, 3, 3, 3, 4, 4, 7, 7, 8, 9])
#count occurrences of unique values in Series
my_series.value_counts()
3    4
4    2
7    2
8    1
9    1
dtype: int64</b>
This tells us:
The value 3 occurs <b>4</b> times.
The value 4 occurs <b>2</b> times.
The value 7 occurs <b>2</b> times.
And so on.
<h3>Example 2: Count Frequency of Unique Values (Including NaNs)</h3>
By default, the <b>value_counts()</b> function does not show the frequency of NaN values.
However, you can use the <b>dropna</b> argument to display the frequency of NaN values:
<b>import pandas as pd
import numpy as np
#create pandas Series with some NaN values
my_series = pd.Series([3, 3, 3, 3, 4, 4, 7, 7, 8, 9, np.nan, np.nan])
#count occurrences of unique values in Series, including NaNs
my_series.value_counts(dropna=False)
3.0    4
4.0    2
7.0    2
NaN    2
8.0    1
9.0    1
dtype: int64</b>
<h3>Example 3: Count Relative Frequency of Unique Values</h3>
The following code shows how to use the <b>normalize</b> argument to count the relative frequency of unique values in a pandas Series:
<b>import pandas as pd
#create pandas Series
my_series = pd.Series([3, 3, 3, 3, 4, 4, 7, 7, 8, 9])
#count occurrences of unique values in Series
my_series.value_counts(normalize=True)
3    0.4
4    0.2
7    0.2
8    0.1
9    0.1
dtype: float64</b>
This tells us:
The value 3 represents <b>40%</b> of all values in the Series.
The value 4 represents <b>20%</b> of all values in the Series.
The value 7 represents <b>20%</b> of all values in the Series.
And so on.
<h3>Example 4: Count Frequency in Bins</h3>
The following code shows how to use the <b>bins</b> argument to count the frequency of values in a pandas Series that fall into equal-sized bins:
<b>import pandas as pd
#create pandas Series
my_series = pd.Series([3, 3, 3, 3, 4, 4, 7, 7, 8, 9])
#count occurrences of unique values in Series
my_series.value_counts(bins=3)
(3.0, 5.0]       6
(5.0, 7.0]       2
(7.0, 9.0]       2
dtype: int64
</b>
This tells us:
There are <b>6</b> values that fall in the range 3 to 5.
There are <b>2</b> values that fall in the range 5 to 7.
There are <b>2</b> values that fall in the range 7 to 9.
<h3>Example 5: Count Frequency of Values in Pandas DataFrame</h3>
We can also use the <b>value_counts()</b> function to calculate the frequency of unique values in a specific column of a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [9, 9, 9, 10, 10, 13, 15, 22],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#count occurrences of unique values in 'points' column
df['points'].value_counts()
9     3
10    2
13    1
15    1
22    1
Name: points, dtype: int64</b>
<h2><span class="orange">How to Perform a VLOOKUP in Pandas</span></h2>
You can use the following basic syntax to perform a VLOOKUP (similar to Excel) in pandas:
<b>pd.merge(df1,
         df2,
         on ='column_name',
         how ='left')
</b>
The following step-by-step example shows how to use this syntax in practice. 
<h3>Step 1: Create Two DataFrames</h3>
First, let’s import pandas and create two pandas DataFrames:
<b>import pandas as pd
#define first DataFrame
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F'],    'team': ['Mavs', 'Mavs', 'Mavs', 'Mavs', 'Nets', 'Nets']})
#define second DataFrame
df2 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [22, 29, 34, 20, 15, 19]})
#view df1
print(df1)
  player  team
0      A  Mavs
1      B  Mavs
2      C  Mavs
3      D  Mavs
4      E  Nets
5      F  Nets
#view df2
print(df2)
  player  points
0      A      22
1      B      29
2      C      34
3      D      20
4      E      15
5      F      19
</b>
<h3>Step 2: Perform VLOOKUP Function</h3>
The <b>VLOOKUP</b> function in Excel allows you to look up a value in a table by matching on a column.
The following code shows how to look up a player’s team by using <b>pd.merge()</b> to match player names between the two tables and return the player’s team:
<b>#perform VLOOKUP
joined_df = pd.merge(df1,     df2,     on ='player',     how ='left')
#view results
joined_df
playerteampoints
0AMavs22
1BMavs29
2CMavs34
3DMavs20
4ENets15
5FNets19</b>
Notice that the resulting pandas DataFrame contains information for the player, their team, and their points scored.
You can find the complete online documentation for the pandas <b>merge()</b> function  here .
<h2><span class="orange">How to Calculate a Weighted Average in Pandas</span></h2>
You can use the following function to calculate a weighted average in Pandas:
<b>def w_avg(df, values, weights):
    d = df[values]
    w = df[weights]
    return (d * w).sum() / w.sum()
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Weighted Average in Pandas</h3>
The following code shows how to use the weighted average function to calculate a weighted average for a given dataset, using “price” as the values and “amount” as the weights:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'sales_rep': ['A', 'A', 'A', 'B', 'B', 'B'],   'price': [8, 5, 6, 7, 12, 14],   'amount': [1, 3, 2, 2, 5, 4]})
#view DataFrame
df
sales_rep  price  amount
0A   8  1
1A   5  3
2A   6  2
3B   7  2
4B   12  5
5B   14  4
#find weighted average of price
w_avg(df, 'price', 'amount')
9.705882352941176
</b>
The weighted average of “price” turns out to be <b>9.706</b>.
<h3>Example 2: Groupby and Weighted Average in Pandas</h3>
The following code shows how to use the weighted average function to calculate the weighted average of price, <em>grouped by</em> sales rep:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'sales_rep': ['A', 'A', 'A', 'B', 'B', 'B'],   'price': [8, 5, 6, 7, 12, 14],   'amount': [1, 3, 2, 2, 5, 4]})
#find weighted average of price, grouped by sales rep
df.groupby('sales_rep').apply(w_avg, 'price', 'amount')
sales_rep
A     5.833333
B    11.818182
dtype: float64
</b>
We can see the following:
The weighted average of “price” for sales rep A is <b>5.833</b>.
The weighted average of “price for sales rep B is <b>11.818</b>.
<h2><span class="orange">How to Use where() Function in Pandas (With Examples)</span></h2>
The <b>where()</b> function can be used to replace certain values in a pandas DataFrame.
This function uses the following basic syntax:
<b>df.where(cond, other=nan)
</b>
For every value in a pandas DataFrame where <b>cond</b> is True, the original value is retained.
For every value where <b>cond</b> is False, the original value is replaced by the value specified by the <b>other</b> argument.
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
52395
62599
729412</b>
<h3>Example 1: Replace Values in Entire DataFrame</h3>
The following code shows how to use the <b>where()</b> function to replace all values that don’t meet a certain condition in an entire pandas DataFrame with a NaN value.
<b>#keep values that are greater than 7, but replace all others with NaN
df.where(df>7)
pointsassistsrebounds
025NaN11.0
112NaN8.0
215NaN10.0
3149.0NaN
41912.0NaN
5239.0NaN
6259.09.0
729NaN12.0
</b>
We can also use the <b>other</b> argument to replace values with something other than NaN.
<b>#keep values that are greater than 7, but replace all others with 'low'
df.where(df>7, other='low')
pointsassistsrebounds
025low11
112low8
215low10
3149low
41912low
5239low
62599
729low12
</b>
<h3>Example 2: Replace Values in Specific Column of DataFrame</h3>
The following code shows how to use the <b>where()</b> function to replace all values that don’t meet a certain condition in a specific column of a DataFrame.
<b>#keep values greater than 15 in 'points' column, but replace others with 'low'
df['points'] = df['points'].where(df['points']>15, other='low')
#view DataFrame
df
pointsassistsrebounds
025511
1low78
2low710
3low96
419126
52395
62599
729412</b>
You can find the complete online documentation for the pandas <b>where()</b> function  here .
<h2><span class="orange">Pandas: How to Reshape DataFrame from Wide to Long</span></h2>
You can use the following basic syntax to convert a pandas DataFrame from a wide format to a long format:
<b>df = pd.melt(df, id_vars='col1', value_vars=['col2', 'col3', ...])
</b>
In this scenario, <b>col1</b> is the column we use as an identifier and <b>col2</b>, <b>col3</b>, etc. are the columns we unpivot.
The following example shows how to use this syntax in practice.
<h3>Example: Reshape Pandas DataFrame from Wide to Long</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D'],   'points': [88, 91, 99, 94],   'assists': [12, 17, 24, 28],   'rebounds': [22, 28, 30, 31]})
#view DataFrame
df
teampointsassistsrebounds
0A881222
1B911728
2C992430
3D942831</b>
We can use the following syntax to reshape this DataFrame from a wide format to a long format:
<b>#reshape DataFrame from wide format to long format
df = pd.melt(df, id_vars='team', value_vars=['points', 'assists', 'rebounds'])
#view updated DataFrame
df
teamvariablevalue
0Apoints        88
1Bpoints        91
2Cpoints        99
3Dpoints        94
4Aassists        12
5Bassists        17
6Cassists        24
7Dassists        28
8Arebounds22
9Brebounds28
10Crebounds30
11Drebounds31</b>
The DataFrame is now in a long format.
We used the ‘team’ column as the identifier column and we unpivoted the ‘points’, ‘assists’, and ‘rebounds’ columns.
Note that we can also use the <b>var_name</b> and <b>value_name</b> arguments to specify the names of the columns in the new long DataFrame:
<b>#reshape DataFrame from wide format to long format
df = pd.melt(df, id_vars='team', value_vars=['points', 'assists', 'rebounds'],
             var_name='metric', value_name='amount')
#view updated DataFrame
df
teammetric amount
0Apoints 88
1Bpoints 91
2Cpoints 99
3Dpoints 94
4Aassists 12
5Bassists 17
6Cassists 24
7Dassists 28
8Arebounds 22
9Brebounds 28
10Crebounds 30
11Drebounds 31
</b>
<b>Note</b>: You can find the complete documentation for the pandas <b>melt()</b> function  here .
<h2><span class="orange">How to Use the par() Function in R</span></h2>
You can use the <b>par()</b> function in R to create multiple plots at once.
This function uses the following basic syntax:
<b>#define plot area as four rows and two columns
par(mfrow = c(4, 2))    
#create plots
plot(1:5)
plot(1:20)
...</b>
The following examples show how to use this function in practice.
<h3>Example 1: Display Multiple Plots with par()</h3>
The following code shows how to use the <b>par()</b> function to define a plotting area with 3 rows and 1 column:
<b>#define plot area as three rows and one column
par(mfrow = c(3, 1))    
#create plots
plot(1:5, pch=19, col='red')
plot(1:10, pch=19, col='blue')
plot(1:20, pch=19, col='green')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/par1-1.png">
<h3>Example 2: Specify Margins of Plots with mar()</h3>
The following code shows how to use the <b>mar()</b> argument to specify the margins around each plot in the following order: bottom, left, top, right.
<b>Note:</b> The default is mar = c(5.1, 4.1, 4.1, 2.1)
<b>#define plot area with tiny bottom margin and huge right margin
par(mfrow = c(3, 1), mar = c(0.5, 4, 4, 20))    
#create plots
plot(1:5, pch=19, col='red')
plot(1:10, pch=19, col='blue')
plot(1:20, pch=19, col='green')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/par2.png">
Notice how the plots look less wide because we made the margin on the right so large.
<h3>Example 3: Specify Text Size of Plots with cex()</h3>
The following code shows how to use the <b>cex.lab()</b> and <b>cex.axis()</b> arguments to specify the size of the axis labels and the tick labels, respectively.
<b>Note:</b> The default is cex.lab = 1 and cex.axis = 1
<b>#define plot area with large axis labels
par(mfrow = c(3, 1), mar = c(5, 10, 4, 1), cex.axis = 3, cex.lab = 3)    
#create plots
plot(1:5, pch=19, col='red')
plot(1:10, pch=19, col='blue')
plot(1:20, pch=19, col='green')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/par3-1.png">
Once you’re finished using the par() function, you can use the <b>dev.off()</b> function to reset the par options.
<b>#reset par() options
dev.off()</b>
It’s a good to use <b>dev.off()</b> each time you’re done using the par() function. 
<h2><span class="orange">What is Parallel Forms Reliability? (Definition & Example)</span></h2>
In statistics, <b>parallel forms reliability</b> measures the correlation between two equivalent forms of a test. 
The process for calculating parallel forms reliability is as follows:
<b>Step 1: Split a test in half.</b>
For example, randomly split a 100-question test into Test A that contains 50 questions and Test B that also has 50 questions.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/parallelforms1.png">
<b>Step 2: Administer the first half to all students, then administer the second half to all students.</b>
For example, administer Test A to all 20 students in a certain class and record their scores. Then, perhaps a month later, administer Test B to the same 20 students and record their scores on that test as well.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/parallelforms2.png">
<b>Step 3:</b> <b>Calculate the correlation between test scores for the two tests.</b>
Calculate the correlation between the scores of the two tests. A test is said to have <b>parallel forms reliability</b> if the correlation between scores is high.
<h3>When to Use Parallel Forms Reliability</h3>
Parallel forms reliability is often used in academic settings when a professor doesn’t want students to be able to have access to test questions in advance.
For example, if the professor gives out test A to all students at the beginning of the semester and then gives out the same test A at the end of the semester, the students may simply memorize the questions and answers from the first test.
However, by giving out a different test B at the end of the semester (that is hopefully equal in difficulty), the professor is able to assess the knowledge of the students while guaranteeing that the students have not seen the questions before.
<h3>Potential Drawbacks of Parallel Forms Reliability</h3>
There are two potential drawbacks of parallel forms reliability:
<b>1. It requires a lot of questions.</b>
Parallel forms reliability works best for tests that have a large number of questions (e.g. 100 questions) because the number we calculate for the correlation will be more reliable.
<b>2. There is no guarantee that the two halves are actually parallel.</b>
When we randomly split a test into two halves, there is no guarantee that the two halves will actually be parallel or “equal” in difficulty. This means that the scores could differ between the two tests simply because one half turns out to be more difficult than the other.
<h3>Parallel Forms Reliability vs. Split-Half Reliability</h3>
Parallel forms reliability is similar to  split-half reliability , but there’s a slight difference:
<b>Split-half reliability:</b>
This involves splitting a test into two halves and administering each half to the same group of students. The order that the students take the test in isn’t important.
The point of this method is to measure  internal consistency . Ideally we would like the correlation between the halves to be high because this indicates that all parts of the test are contributing equally to what is being measured.
<b>Parallel forms reliability:</b>
This involves splitting a test into two halves – call them “A” and “B” – and administering each half to the same group of students.
However, it’s important that all students take test “A” first and then take test “B” so that knowing the answers to test “A” doesn’t provide any benefit to students who later take test “B.”
<h2><span class="orange">What is a Parameter of Interest in Statistics?</span></h2>
In statistics, a <b>parameter</b> is a number that describes some characteristic of a population.
Examples of parameters include:
Population mean (e.g. mean height of all U.S. citizens)
Population proportion (e.g. proportion of U.S. citizens that support a law)
Population variance (e.g. variance of annual income among U.S. households)
Since it’s often too time-consuming and costly to collect data on every individual element in a  population , researchers will collect a  random sample  from the population and use a sample statistic to estimate the population parameter.
For example, instead of collecting data on the annual income of every household in a certain state, researchers may instead collect data for 2,000 households and use the mean income of the households in the sample to estimate the mean income for households in the entire state.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample1.png">
A <b>parameter of interest</b> is the specific parameter that we’re interested in estimating during a data collection process.
The following examples illustrate different parameters of interest in real-world scenarios.
<h3>Example 1: Estimating a Population Mean</h3>
Suppose a biologist is interested in finding the mean weight of a certain population of 800 turtles.
Since it would be too time-consuming to go around and weigh every individual turtle, she may instead collect a random sample of 30 turtles and use the sample mean weight to estimate the population mean weight.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample2.png">
In this example, the <b>parameter of interest</b> is the population mean.
In order to estimate the value of this parameter, the biologist will use the sample mean.
For example, if the mean weight of turtles in the sample is 190.4 pounds, then the best estimate for the mean weight among turtles in the population will also be 190.4 pounds.
<h3>Example 2: Estimating a Population Proportion</h3>
Suppose a politician is interested in finding out the proportion of residents in a certain city with a population of 5,0000 that support a certain law.
Since it would be too costly to go around and survey every individual resident, he may instead collect a random sample of 1,000 residents and use the sample proportion to estimate the population proportion.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample3.png">
In this example, the <b>parameter of interest</b> is the population proportion.
In order to estimate the value of this parameter, the politician will use the sample proportion.
For example, if 25% of residents in the sample support the law, then the best estimate for the proportion of residents in the population that support the law will also be 25%.
<h2><span class="orange">The Four Assumptions of Parametric Tests</span></h2>
In statistics, <b>parametric tests</b> are tests that make assumptions about the underlying distribution of data.
Common parametric tests include:
 One sample t-test 
 Two sample t-test 
 One-way ANOVA 
In order for the results of parametric tests to be valid, the following four assumptions should be met:
<b>1. Normality</b> – Data in each group should be normally distributed.
<b>2. Equal Variance</b> – Data in each group should have approximately equal variance.
<b>3. Independence</b> – Data in each group should be randomly and independently sampled from the population.
<b>4. No Outliers</b> – There should be no extreme outliers.
This tutorial provides a brief explanation of each assumption along with how to check if each assumption is met.
<h3>Assumption 1: Normality</h3>
Parametric tests assume that each group is roughly normally distributed.
If the sample sizes of each group are small (n &lt; 30), then we can use a Shapiro-Wilk test to determine if each sample size is normally distributed.
If the p-value of the test is less than a certain significance level, then the data is likely not normally distributed.
However, if the sample sizes are large then it’s better to use a  Q-Q plot  to visually check if the data is normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/qqplot.jpg"459">
If the data points roughly fall along a straight diagonal line in a Q-Q plot, then the dataset likely follows a normal distribution.
<h3>Assumption 2: Equal Variance</h3>
Parametric tests assume that the variance of each group is roughly equal.
We can visually check if this assumption is met by creating side-by-side boxplots for each group to see if the boxplots of each group are roughly the same size.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/compareBox1.png">
Another way to check if this assumption is met is to use the following rule of thumb: If the ratio of the largest variance to the smallest variance is less than 4, then we can assume the variances are approximately equal and use the two sample t-test.
For example, suppose group 1 has a variance of 24.5 and group 2 has a variance of 15.2. The ratio of the larger sample variance to the smaller sample variance would be calculated as:
<b>Ratio:</b> 24.5 / 15.2 = 1.61
Since this ratio is less than 4, we could assume that the variances between the groups are approximately equal.
<h3>Assumption 3: Independence</h3>
Parametric tests assume that the observations in each group are independent of observations in every other group.
The easiest way to check this assumption is to verify that the data was collected using a  probability sampling method  – a method in which every member in a population has an equal probability of being selected to be in the sample.
Examples of probability sampling methods include:
Simple random sampling
Stratified random sampling
Cluster random sampling
Systematic random sampling
If one of these methods was used to collect the data, we can assume that this assumption is met.
<h3>Assumption 4: No Outliers</h3>
Parametric tests assume that there are no extreme outliers in any group that could adversely affect the results of the test.
One way to visually check for outliers is to create boxplots for each group to see if there are any clear outliers that are much larger than the rest of the other observations in the group.
Another way to detect outliers is to perform Grubbs’ Test, which is a formal statistical test that can be used to identify outliers.
The following tutorials explain how to perform Grubbs’ Test in various statistical softwares:
 How to Conduct Grubbs’ Test in Excel 
 How to Perform Grubbs’ Test in R 
 How to Perform Grubbs’ Test in Python 
<h2><span class="orange">How to Create a Pareto Chart in Google Sheets (Step-by-Step)</span></h2>
A <b>Pareto chart</b> is a type of chart that uses bars to display the individual frequencies of categories and a line to display the cumulative frequencies.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto1.png">
This tutorial provides a step-by-step example of how to create a Pareto chart in Google Sheets.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset that shows the number of sales by product for some company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto2.png">
<h3>Step 2: Calculate the Cumulative Frequencies</h3>
Next, type the following formula into cell C2 to calculate the cumulative frequency:
<b>=SUM($B$2:B2)/SUM($B$2:$B$7)
</b>
Copy this formula down to each cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto3.png">
<h3>Step 3: Insert Combo Chart</h3>
Next, highlight all three columns of data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto4.png">
Click the <b>Insert</b> tab along the top ribbon, then click <b>Chart</b> in the dropdown options. This will automatically insert the following combo chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto5.png">
<h3>Step 4: Add a Right Y Axis</h3>
Next, right click on any of the bars in the chart. In the dropdown menu that appears, click <b>Series</b> and then click <b>Cumulative</b>.
In the menu that appears on the right, choose <b>Right Axis</b> under the <b>Axis</b> dropdown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto6.png">
This will automatically add another y-axis on the right side of the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pareto7.png">
The Pareto chart is now complete. The blue bars display the individual sales of each product and the red line displays the cumulative sales of the products.
<h2><span class="orange">How to Create a Pareto Chart in R (Step-by-Step)</span></h2>
A <b>Pareto chart</b> is a type of chart that displays the frequencies of different categories along with the cumulative frequencies of categories.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/paretoR2.png">
This tutorial provides a step-by-step example of how to create a Pareto chart in R.
<h3>Step 1: Create the Data</h3>
Suppose we conduct a survey in which we ask 350 different people to identify their favorite cereal brand between brands A, B, C, D, and E.
The following dataset shows the total votes for each brand:
<b>#create data
df &lt;- data.frame(favorite=c('A', 'B', 'C', 'D', 'E', 'F'), count=c(140, 97, 58, 32, 17, 6))
#view data
df
  favorite count
1        A   140
2        B    97
3        C    58
4        D    32
5        E    17
6        F     6</b>
<h3>Step 2: Create the Pareto Chart</h3>
To create a Pareto chart to visualize the results of this survey, we can use the <b>pareto.chart()</b> function from the <b>qcc</b> package:
<b>library(qcc)
#create Pareto chart
pareto.chart(df$count)
Pareto chart analysis for df$count
     Frequency  Cum.Freq. Percentage Cum.Percent.
  A 140.000000 140.000000  40.000000    40.000000
  B  97.000000 237.000000  27.714286    67.714286
  C  58.000000 295.000000  16.571429    84.285714
  D  32.000000 327.000000   9.142857    93.428571
  E  17.000000 344.000000   4.857143    98.285714
  F   6.000000 350.000000   1.714286   100.000000
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/paretoR1.png">
The table in the output shows us the frequency and cumulative frequency of each brand. For example:
Frequency of brand A: <b>140</b> | Cumulative frequency: <b>140</b>
Frequency of brand B: <b>97</b> | Cumulative frequency of A, B: <b>237</b>
Frequency of brand C: <b>58</b> | Cumulative frequency of A, B, C: <b>295</b>
And so on.
<h3>Step 3: Modify the Pareto Chart (Optional)</h3>
The following code shows how to modify the title of the chart along with the color palette used:
<b>pareto.chart(df$count,
             main='Pareto Chart for Favorite Cereal Brands',
             col=heat.colors(length(df$count)))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/paretoR2.png">
<em>You can find a complete list of color palettes available in  this R Color Cheat Sheet .</em>
<h2><span class="orange">How to Create a Pareto Chart in Python (Step-by-Step)</span></h2>
A <b>Pareto chart</b> is a type of chart that displays the ordered frequencies of categories along with the cumulative frequencies of categories.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto_chart1.png">
This tutorial provides a step-by-step example of how to create a Pareto chart in Python.
<h3>Step 1: Create the Data</h3>
Suppose we conduct a survey in which we ask 350 different people to identify their favorite cereal brand between brands A, B, C, D, and E.
We can create the following pandas DataFrame to hold the results of the survey:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'count': [97, 140, 58, 6, 17, 32]})
df.index = ['B', 'A', 'C', 'F', 'E', 'D']
#sort DataFrame by count descending
df = df.sort_values(by='count', ascending=False)
#add column to display cumulative percentage
df['cumperc'] = df['count'].cumsum()/df['count'].sum()*100
#view DataFrame
df
countcumperc
A14040.000000
B9767.714286
C5884.285714
D3293.428571
E1798.285714
F6100.000000</b>
<h3>Step 2: Create the Pareto Chart</h3>
We can use the following code to create the Pareto chart:
<b>import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
#define aesthetics for plot
color1 = 'steelblue'
color2 = 'red'
line_size = 4
#create basic bar plot
fig, ax = plt.subplots()
ax.bar(df.index, df['count'], color=color1)
#add cumulative percentage line to plot
ax2 = ax.twinx()
ax2.plot(df.index, df['cumperc'], color=color2, marker="D", ms=line_size)
ax2.yaxis.set_major_formatter(PercentFormatter())
#specify axis colors
ax.tick_params(axis='y', colors=color1)
ax2.tick_params(axis='y', colors=color2)
#display Pareto chart
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto_chart1.png">
The x-axis displays the different brands ordered from highest to lowest frequency.
The left-hand y-axis shows the frequency of each brand and the right-hand y-axis shows the cumulative frequency of the brands.
For example, we can see:
Brand A accounts for about 40% of total survey responses.
Brands A and B account for about 70% of total survey responses.
Brands A, B, and C account for about 85% of total survey responses.
And so on.
<h3>Step 3: Customize the Pareto Chart (Optional)</h3>
You can change the colors of the bars and the size of the cumulative percentage line to make the Pareto chart look however you’d like.
For example, we could change the bars to be pink and change the line to be purple and slightly thicker:
<b><span>import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
#define aesthetics for plot
color1 = 'pink'
color2 = '<span>purple'
line_size = 6
#create basic bar plot
fig, ax = plt.subplots()
ax.bar(df.index, df['count'], color=color1)
#add cumulative percentage line to plot
ax2 = ax.twinx()
ax2.plot(df.index, df['cumperc'], color=color2, marker="D", ms=line_size)
ax2.yaxis.set_major_formatter(PercentFormatter())
#specify axis colors
ax.tick_params(axis='y', colors=color1)
ax2.tick_params(axis='y', colors=color2)
#display Pareto chart
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto_chart2.png">
<h2><span class="orange">Pareto Chart vs. Histogram: What’s the Difference?</span></h2>
Two charts that look somewhat similar are <b>pareto charts</b> and <b>histograms</b>. However, these two charts are not the same and they’re each used in unique situations.
A <b>pareto chart</b> is a type of chart that displays quantitative <em>or</em>  qualitative data  on the x-axis and uses bars ordered from highest to lowest frequency on the y-axis to visualize which values occur most often in a dataset.
A <b>histogram</b> is a type of chart that displays ranges of quantitative data on the x-axis and uses bars to represent the frequency of values in each range.
The following examples illustrate how to create and interpret each type of chart.
<h3>Example 1: Creating and Interpreting a Pareto Chart</h3>
Suppose we have the following dataset that shows the total sales of various products for a certain company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto11.png">
We can create the following pareto chart to visualize which products contribute most to the total sales:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto13.png">
From this chart we can quickly see that product B has the highest total sales, followed by product A, followed by product F, etc.
The line in the chart also displays the percentage of cumulative sales by product. Note that the percentage of total sales is shown on the right axis.
For example:
Product B accounts for roughly 25% of total sales
Products B and A account for roughly 50% of total sales
Products B, A, and F account for roughly 65% of total sales.
And so on.
 
<h3>Example 2: Creating and Interpreting a Histogram</h3>
Suppose we have the following dataset that shows the total number of points scored by various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto15.png">
We can create the following histogram to visualize the distribution of points scored by the players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pareto14.png">
From the histogram we can quickly gain an understanding of the distribution of points scored.
For example, we can see that most players score between 16 and 25 points, with very few scoring less than 5 and very few scoring 36 or more.
We can also see that the distribution is roughly “bell shaped” – indicating the the distribution of points scored is roughly  normally distributed .
<h3>Summary: Differences Between Pareto Charts & Histograms</h3>
Here’s a quick summary of the differences between pareto charts and histograms:
<b>Difference #1:</b> A pareto chart can use quantitative or qualitative data on the x-axis. Conversely, a histogram can only use quantitative data on the x-axis.
<b>Difference #2: </b>A pareto chart orders each bar from highest to lowest frequency. Conversely, a histogram places each bar in order from smallest to largest numerical values on the x-axis.
<b>Difference #3: </b>A pareto chart uses a line to represent cumulative frequencies of each bar. Conversely, a histogram does not use any such line.
Both pareto charts and histograms use bars to represent frequencies, but that’s about all they have in common.
<h2><span class="orange">What is a Parsimonious Model?</span></h2>
A <b>parsimonious model</b> is a model that achieves a desired level of goodness of fit using as few  explanatory variables  as possible.
The reasoning for this type of model stems from the idea of  Occam’s Razor  (sometimes called the “Principle of Parsimony”) which says that the simplest explanation is most likely the right one.
Applied to statistics, a model that has few parameters but achieves a satisfactory level of goodness of fit should be preferred over a model that has a ton of parameters and achieves only a slightly higher level of goodness of fit.
There are two reasons for this:
<b>1. Parsimonious models are easier to interpret and understand.</b> Models with fewer parameters are easier to understand and explain.
<b>2. Parsimonious models tend to have more predictive ability. </b>Models with fewer parameters tend to perform better when applied to new data.
Consider the following two examples to illustrate these ideas.
<h3>Example 1: Parsimonious Models = Easy Interpretation</h3>
Suppose we want to build a model using a set of explanatory variables related to real estate to predict house prices. Consider the following two models along with their adjusted R-squared:
<b>Model 1:</b>
<b>Equation: </b>House price = 8,830 + 81*(sq. ft.)
<b>Adjusted R<sup>2</sup>: </b>0.7734
<b>Model 2:</b>
<b>Equation: </b>House price = 8,921 + 77*(sq. ft.) + 7*(sq. ft.)<sup>2</sup> – 9*(age) + 600*(rooms) + 38*(baths)
<b>Adjusted R<sup>2</sup>: </b>0.7823
The first model only has one explanatory variable and an adjusted R<sup>2</sup> of .7734 while the second model has five explanatory variables with only a slightly higher adjusted R<sup>2</sup>.
Based on the principle of parsimony, we would prefer to use the first model because each model has roughly the same ability to explain the variation in house prices but the first model is <em>much </em>easier to understand and explain.
For example, in the first model we know that a one unit increase in square footage of a house is associated with an average house price increase of $81. That’s simple to understand and explain.
However, in the second example the coefficient estimates are much harder to interpret. For example, one additional room in the house is associated with an average house price increase of $600, assuming that square footage, age of the house, and number of baths is held constant. That’s much harder to understand and explain.
<h3>Example 2: Parsimonious Models = Better Predictions</h3>
Parsimonious models also tend to make more accurate predictions on new datasets because they’re less likely to <em> overfit  </em>the original dataset.
In general, models with more parameters will produce tighter fits and higher R<sup>2</sup> values compared to models with fewer parameters. Unfortunately, including too many parameters in a model can cause the model to fit the noise (or “randomness”) of the data, rather than the true underlying relationship between the explanatory and the response variables.
This means that a highly complex model with many parameters is likely to perform poorly on a new dataset that it hasn’t seen before compared to a simpler model with fewer parameters.
<h3>How to Choose a Parsimonious Model</h3>
There could be an entire course dedicated to the topic of <b>model selection</b>, but essentially choosing a parsimonious model comes down to choosing a model that performs best according to some metric.
Commonly used metrics that evaluate models on their performance on a training dataset <em>and </em>their number of parameters include:
<b>1. Akaike Information Criterion (AIC)</b>
The AIC of a model can be calculated as:
<b>AIC = -2/n * LL + 2 * k/n</b>
where:
<b>n: </b>Number of observations in the training dataset.
<b>LL: </b>Log-likelihood of the model on the training dataset.
<b>k: </b>Number of parameters in the model.
Using this method, you can calculate the AIC of each model and then select the model with the lowest AIC value as the best model.
This approach tends to favor more complex models compared to the next method, BIC.
<b>2. Bayesian Information Criterion (BIC)</b>
The BIC of a model can be calculated as:
<b>BIC = -2 * LL + log(n) * k</b>
where:
<b>n: </b>Number of observations in the training dataset.
<b>log: </b>The natural logarithm (with base e)
<b>LL: </b>Log-likelihood of the model on the training dataset.
<b>k: </b>Number of parameters in the model.
Using this method, you can calculate the BIC of each model and then select the model with the lowest BIC value as the best model.
This approach tends to favor models with fewer parameters compared to the AIC method.
<b>3. Minimum Description Length (MDL)</b>
The MDL is a way of evaluating models that comes from the field of information theory. It can be calculated as:
<b>MDL = L(h) + L(D | h)</b>
where:
<b>h: </b>The model.
<b>D: </b>Predictions made by the model.
<b>L(h): </b>Number of bits required to represent the model.
<b>L(D | h): </b>Number of bits required to represent the predictions from the model on the training data.
Using this method, you can calculate the MDL of each model and then select the model with the lowest MDL value as the best model.
Depending on the type of problem you’re working on, one of these methods – AIC, BIC, or MDL – may be preferred over the others as a way of selecting a parsimonious model.
<h2><span class="orange">How to Calculate Partial Correlation in Excel</span></h2>
In statistics, we often use the  Pearson correlation coefficient  to measure the linear relationship between two variables. However, sometimes we’re interested in understanding the relationship between two variables <b>while controlling for a third variable</b>.
For example, suppose we want to measure the association between the number of hours a student studies and the final exam score they receive, while controlling for the student’s current grade in the class. In this case, we could use a <b>partial correlation </b>to measure the relationship between hours studied and final exam score.
This tutorial explains how to calculate partial correlation in Excel.
<h3>Example: Partial Correlation in Excel</h3>
Suppose we have a dataset that shows the following information for 10 students:
Current grade in a class
Hours spent studying for the final exam
Final exam score
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrExcel1.png">
Use the following steps to find the partial correlation between hours studied and exam score while controlling for current grade.
<b>Step 1: Calculate each pairwise correlation.</b>
First, we’ll calculate the correlation between each pairwise combination of the variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrExcel2.png">
<b>Step 2: Calculate the partial correlation between hours and exam score.</b>
The formula to calculate the partial correlation between variable A and variable B while controlling for variable C is as follows:
Partial correlation = (r<sub>A,B</sub> – r<sub>A,C</sub>*r<sub>B,C</sub>) / √((1-r<sup>2</sup><sub>A,B</sub>)(1-r<sup>2</sup><sub>B,C</sub>))
The following screenshot shows how to use this formula to calculate the partial correlation between hours and exam score, controlling for current grade:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrExcel3.png">
The partial correlation is <b>0.190626</b>. To determine if this correlation is statistically significant, we can find the corresponding p-value.
<b>Step 3: Calculate the p-value of the partial correlation.</b>
The test statistic <em>t </em>can be calculated as: 
t = r√(n-3) / √(1-r<sup>2</sup>)
The following screenshot shows how to use this formula to calculate the test statistic and the corresponding p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrExcel4.png">
The test statistic <em>t </em>is <b>0.51377</b>. The total degrees of freedom is n-3 = 10-3 = <b>7</b>. The corresponding p-value is <b>0.623228</b>. Since this value is not less than 0.05, this means the partial correlation between hours and exam score is not statistically significant.
<h2><span class="orange">How to Calculate Partial Correlation in Python</span></h2>
In statistics, we often use the  Pearson correlation coefficient  to measure the linear relationship between two variables. However, sometimes we’re interested in understanding the relationship between two variables <b>while controlling for a third variable</b>.
For example, suppose we want to measure the association between the number of hours a student studies and the final exam score they receive, while controlling for the student’s current grade in the class. In this case, we could use a <b>partial correlation </b>to measure the relationship between hours studied and final exam score.
This tutorial explains how to calculate partial correlation in Python.
<h3>Example: Partial Correlation in Python</h3>
Suppose we have the following Pandas DataFrame that displays the current grade, total hours studied, and final exam score for 10 students:
<b>import numpy as np
import panda as pd
data = {'currentGrade':  [82, 88, 75, 74, 93, 97, 83, 90, 90, 80],
        'hours': [4, 3, 6, 5, 4, 5, 8, 7, 4, 6],
        'examScore': [88, 85, 76, 70, 92, 94, 89, 85, 90, 93],
        }
df = pd.DataFrame(data, columns = ['currentGrade','hours', 'examScore'])
df
   currentGrade  hours  examScore
0            82      4         88
1            88      3         85
2            75      6         76
3            74      5         70
4            93      4         92
5            97      5         94
6            83      8         89
7            90      7         85
8            90      4         90
9            80      6         93
</b>
To calculate the partial correlation between <b>hours </b>and <b>examScore</b> while controlling for <b>currentGrade</b>, we can use the <b>partial_corr()</b> function from the  pingouin package , which uses the following syntax:
<b>partial_corr(data, x, y, covar)</b>
where:
<b>data:</b> name of the dataframe
<b>x, y:</b> names of columns in the dataframe
<b>covar:</b> the name of the covariate column in the dataframe (e.g. the variable you’re controlling for)
Here is how to use this function in this particular example:
<b>#install and import pingouin package 
pip install pingouin
import pingouin as pg
#find partial correlation between hours and exam score while controlling for grade
pg.partial_corr(data=df, x='hours', y='examScore', covar='currentGrade')
         n    r       CI95%   r2adj_r2p-val BF10power
pearson100.191[-0.5, 0.73]0.036-0.2380.5980.4380.082
</b>
We can see that the partial correlation between hours studied and final exam score is <b>.191</b>, which is a small positive correlation. As hours studied increases, exam score tends to increase as well, assuming current grade is held constant.
To calculate the partial correlation between multiple variables at once, we can use the <b>.pcorr() </b>function:
<b>#calculate all pairwise partial correlations, rounded to three decimal places
df.pcorr().round(3)
     currentGradehoursexamScore
currentGrade    1.000      -0.311    0.736
hours           -0.3111.000    0.191
examScore    0.7360.191    1.000
</b>
The way to interpret the output is as follows:
The partial correlation between current grade and hours studied is <b>-0.311</b>.
The partial correlation between current grade and exam score <b>0.736</b>.
The partial correlation between hours studied and exam score <b>0.191</b>.
<h2><span class="orange">How to Calculate Partial Correlation in R</span></h2>
In statistics, we often use the  Pearson correlation coefficient  to measure the linear relationship between two variables.
However, sometimes we’re interested in understanding the relationship between two variables <b>while controlling for a third variable</b>.
For example, suppose we want to measure the association between the number of hours a student studies and the final exam score they receive, while controlling for the student’s current grade in the class.
In this case, we could use a <b>partial correlation </b>to measure the relationship between hours studied and final exam score.
This tutorial explains how to calculate partial correlation in R.
<h2>Example: Partial Correlation in R</h2>
Suppose we have the following data frame that displays the current grade, total hours studied, and final exam score for 10 students:
<b>#create data frame
df &lt;- data.frame(currentGrade = c(82, 88, 75, 74, 93, 97, 83, 90, 90, 80), hours = c(4, 3, 6, 5, 4, 5, 8, 7, 4, 6), examScore = c(88, 85, 76, 70, 92, 94, 89, 85, 90, 93))
#view data frame
df
   currentGrade hours examScore
1            82     4        88
2            88     3        85
3            75     6        76
4            74     5        70
5            93     4        92
6            97     5        94
7            83     8        89
8            90     7        85
9            90     4        90
10           80     6        93
</b>
To calculate the partial correlation between each pairwise combination of variables in the dataframe, we can use the <b>pcor()</b> function from the  ppcor library :
<b>library(ppcor)
#calculate partial correlations
pcor(df)
$estimate
             currentGrade      hours examScore
currentGrade    1.0000000 -0.3112341 0.7355673
hours          -0.3112341  1.0000000 0.1906258
examScore       0.7355673  0.1906258 1.0000000
$p.value
             currentGrade     hours  examScore
currentGrade   0.00000000 0.4149353 0.02389896
hours          0.41493532 0.0000000 0.62322848
examScore      0.02389896 0.6232285 0.00000000
$statistic
             currentGrade      hours examScore
currentGrade    0.0000000 -0.8664833 2.8727185
hours          -0.8664833  0.0000000 0.5137696
examScore       2.8727185  0.5137696 0.0000000
$n
[1] 10
$gp
[1] 1
$method
[1] "pearson"
</b>
Here is how to interpret the output:
<b>Partial correlation between hours studied and final exam score:</b>
The partial correlation between hours studied and final exam score is <b>.191</b>, which is a small positive correlation. As hours studied increases, exam score tends to increase as well, assuming current grade is held constant.
The p-value for this partial correlation is <b>.623</b>, which is not statistically significant at α = 0.05.
<b>Partial correlation between current grade and final exam score:</b>
The partial correlation between current grade and final exam score is <b>.736</b>, which is a strong positive correlation. As current grade increases, exam score tends to increase as well, assuming hours studied is held constant.
The p-value for this partial correlation is <b>.024</b>, which is statistically significant at α = 0.05.
<b>Partial correlation between current grade and hours studied:</b>
The partial correlation between current grade and hours studied and final exam score is <b>-.311</b>, which is a mild negative correlation. As current grade increases, final exam score tends to decreases, assuming final exam score is held constant.
The  p-value  for this partial correlation is <b>0.415</b>, which is not statistically significant at α = 0.05.
The output also tells us that the method used to calculate the partial correlation was “pearson.”
Within the<b> pcor()</b> function, we could also specify “kendall” or “pearson” as alternative methods to calculate the correlations.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Calculate Spearman Rank Correlation in R 
 How to Calculate Cross Correlation in R 
 How to Calculate Rolling Correlation in R 
 How to Calculate Point-Biserial Correlation in R 
<h2><span class="orange">How to Calculate Partial Correlation in SPSS</span></h2>
In statistics, we often use the  Pearson correlation coefficient  to measure the linear relationship between two variables.
However, sometimes we’re interested in understanding the relationship between two variables <b>while controlling for a third variable</b>.
For example, suppose we want to measure the association between the number of hours a student studies and the final exam score they receive, while controlling for the student’s current grade in the class.
In this case, we could use a <b>partial correlation </b>to measure the relationship between hours studied and final exam score.
This tutorial explains how to calculate partial correlation in SPSS.
<h3>Example: Partial Correlation in SPSS</h3>
Suppose we have a dataset that shows the following information for 10 students:
Current grade in a class
Hours spent studying for the final exam
Final exam score
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrSPSS1.png">
Perform the following steps to calculate the partial correlation between hours and exam, while controlling for grade:
Click the <b>Analyze </b>tab.
Click <b>Correlate</b>.
Click <b>Partial</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrSPSS2.png">
In the window that pops up, drag <b>hours </b>and <b>exam </b>into the box that says <b>Variables </b>and drag <b>grade </b>into the box that says <b>Controlling for</b>. Then click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrSPSS3.png">
Once you click OK, the following screen will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/partialcorrSPSS4.png">
We can see that the partial correlation between hours studied and final exam score is <b>.191</b>, which is a small positive correlation. As hours studied increases, exam score tends to increase as well, assuming current grade is held constant.
The corresponding two-tailed p-value is <b>.623</b>. Since this value is not less than 0.05, this means the partial correlation between hours and exam score is not statistically significant.
<h2><span class="orange">What is Partial Eta Squared? (Definition & Example)</span></h2>
<b>Partial eta squared</b> is a way to measure the  effect size  of different variables in ANOVA models.
It measures the proportion of variance explained by a given variable of the total variance remaining after accounting for variance explained by other variables in the model.
<h3>How to Calculate Partial Eta Squared</h3>
The formula to calculate Partial eta squared is as follows:
<b>Partial eta squared = SS<sub>effect</sub> / (SS<sub>effect </sub>+ SS<sub>error</sub>)</b>
where:
<b>SS<sub>effect</sub>:</b> The sum of squares of an effect for one variable.
<b>SS<sub>error</sub>:</b> The sum of squares error in the ANOVA model.
The value for Partial eta squared ranges from 0 to 1, where values closer to 1 indicate a higher proportion of variance that can be explained by a given variable in the model after accounting for variance explained by other variables in the model.
The following rules of thumb are used to interpret values for Partial eta squared:
<b>.01:</b> Small effect size
<b>.06:</b> Medium effect size
<b>.14 or higher:</b> Large effect size
<h3>Example: Calculating Partial Eta Squared</h3>
Suppose we want to determine if exercise intensity and gender impact weight loss.
To test this, we recruit 30 men and 30 women to participate in an experiment in which we randomly assign 10 of each to follow a program of either no exercise, light exercise, or intense exercise for one month.
The following table shows the results of a  two-way ANOVA  using exercise and gender as factors and weight loss as the  response variable :
<b>            Df Sum Sq Mean Sq F value p value    
gender       1   15.8   15.80   9.916 0.00263
exercise     2  505.6  252.78 158.610 0.00000
Residuals   56   89.2    1.59  
</b>
We can calculate the partial eta squared for gender and exercise as follows:
Partial eta squared for gender: 15.8 / (15.8+89.2) = <b>.15044</b>
Partial eta squared for exercise: 505.6 / (505.6+89.2) = <b>.85</b>
We would conclude that the effect size for exercise is very large while the effect size for gender is quite small.
These results match the p-values shown in the output of the ANOVA table. The p-value for exercise ( 0.00000) is much smaller than the p-value for gender (.00263), which indicates that exercise is much more significant at predicting weight loss.
<h3>Eta Squared vs. Partial Eta Squared</h3>
 Eta squared  measures the proportion of variance that a given variable accounts for out of the total variance in an ANOVA model. It is calculated as:
<b>Eta squared = SS<sub>effect</sub> / SS<sub>total</sub></b>
where:
<b>SS<sub>effect</sub>:</b> The sum of squares of an effect for one variable.
<b>SS<sub>total</sub>:</b> The total sum of squares in the ANOVA model.
When there is only one predictor variable in the model (i.e. a one-way ANOVA), then the value for eta squared and partial eta squared will be equal.
By default, programs like SPSS report partial eta squared values in the output of ANOVA tables. Thus, it’s important to know the subtle difference between eta squared and partial eta squared.
<h2><span class="orange">How to Perform a Partial F-Test in Excel</span></h2>
A  partial F-test  is used to determine whether or not there is a statistically significant difference between a  regression model  and some nested version of the same model.
A <em>nested</em> model is simply one that contains a subset of the predictor variables in the overall regression model.
For example, suppose we have the following regression model with four predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub> + β<sub>4</sub>x<sub>4</sub> + ε
One example of a nested model would be the following model with only two of the original predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + ε
To determine if these two models are significantly different, we can perform a partial F-test, which calculates the following F test statistic:
F = ((RSS<sub>reduced</sub> – RSS<sub>full</sub>)/p)  /  (RSS<sub>full</sub>/n-k)
where:
<b>RSS<sub>reduced</sub></b>: The residual sum of squares of the reduced (i.e. “nested”) model.
<b>RSS<sub>full</sub></b>: The residual sum of squares of the full model.
<b>p:</b> The number of predictors removed from the full model.
<b>n:</b> The total observations in the dataset.
<b>k: </b>The number of coefficients (including the intercept) in the full model.
This test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>:</b> All coefficients removed from the full model are zero.
<b>H<sub>A</sub>:</b> At least one of the coefficients removed from the full model is non-zero.
If the  p-value  corresponding to the F test-statistic is below a certain significance level (e.g. 0.05), then we can reject the null hypothesis and conclude that at least one of the coefficients removed from the full model is significant.
The following example shows how to perform a partial F-test in Excel.
<h3>Example: Partial F-Test in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/partialFExcel1.png">
Suppose we would like to determine if there is a difference between the following two regression models:
<b>Full Model: </b>y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub> + β<sub>4</sub>x<sub>4</sub>
<b>Reduced Model: </b>y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub>
We can proceed to perform  multiple linear regression in Excel  for each model to get the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/partialFExcel2.png">
We can then use the following formula to calculate the F test-statistic for the partial F-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/partialFExcel3.png">
The test statistic turns out to be <b>2.064</b>.
We can then use the following formula to calculate the corresponding p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/partialFExcel4.png">
The p-value turns out to be <b>0.1974</b>.
Since this p-value is not less than .05, we will fail to reject the null hypothesis. This means we don’t have sufficient evidence to say that either of the predictor variables <em>x3</em> or <em>x4</em> are statistically significant.
In other words, adding <em>x3</em> and <em>x4</em> to the regression model do not significantly improve the fit of the model.
<h2><span class="orange">What is a Partial F-Test?</span></h2>
A <b>partial F-test </b>is used to determine whether or not there is a statistically significant difference between a  regression model  and some nested version of the same model.
A <em>nested</em> model is simply one that contains a subset of the predictor variables in the overall regression model.
For example, suppose we have the following regression model with four predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub> + β<sub>4</sub>x<sub>4</sub> + ε
One example of a nested model would be the following model with only two of the original predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + ε
To determine if these two models are significantly different, we can perform a partial F-test.
<h3>Partial F-Test: The Basics</h3>
A partial F-test calculates the following F test-statistic:
F = ((RSS<sub>reduced</sub> – RSS<sub>full</sub>)/p)  /  (RSS<sub>full</sub>/n-k)
where:
<b>RSS<sub>reduced</sub></b>: The residual sum of squares of the reduced (i.e. “nested”) model.
<b>RSS<sub>full</sub></b>: The residual sum of squares of the full model.
<b>p:</b> The number of predictors removed from the full model.
<b>n:</b> The total observations in the dataset.
<b>k: </b>The number of coefficients (including the intercept) in the full model.
Note that the residual sum of squares will always be smaller for the full model since adding predictors will always lead to some reduction in error.
Thus, a partial F-test essentially tests whether the group of predictors that you removed from the full model are actually useful and need to be included in the full model.
This test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> All coefficients removed from the full model are zero.
<b>H<sub>A</sub>:</b> At least one of the coefficients removed from the full model is non-zero.
If the p-value corresponding to the F test-statistic is below a certain significance level (e.g. 0.05), then we can reject the null hypothesis and conclude that at least one of the coefficients removed from the full model is significant.
<h3>Partial F-Test: An Example</h3>
In practice, we use the following steps to perform a partial F-test:
<b>1. </b>Fit the full regression model and calculate RSS<sub>full</sub>.
<b>2. </b>Fit the nested regression model and calculate RSS<sub>reduced</sub>.
<b>3. </b>Perform an ANOVA to compare the full and reduced model, which will produce the F test-statistic needed to compare the models.
For example, the following code shows how to fit the following two regression models in R using data from the built-in <b>mtcars</b> dataset:
<b>Full model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb + β<sub>3</sub>hp + β<sub>4</sub>cyl
<b>Reduced model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb
<b>#fit full model
model_full &lt;- lm(mpg ~ disp + carb + hp + cyl, data = mtcars)
#fit reduced model
model_reduced &lt;- lm(mpg ~ disp + carb, data = mtcars)
#perform ANOVA to test for differences in models
anova(model_reduced, model_full)
Analysis of Variance Table
Model 1: mpg ~ disp + carb
Model 2: mpg ~ disp + carb + hp + cyl
  Res.Df    RSS Df Sum of Sq      F Pr(>F)
1     29 254.82                           
2     27 238.71  2    16.113 0.9113  0.414</b>
From the output we can see that the F test-statistic from the ANOVA is <b>0.9113</b> and the corresponding p-value is <b>0.414</b>.
Since this p-value is not less than .05, we will fail to reject the null hypothesis. This means we don’t have sufficient evidence to say that either of the predictor variables <em>hp</em> or <em>cyl</em> are statistically significant.
In other words, adding <em>hp</em> and <em>cyl</em> to the regression model do not significantly improve the fit of the model.
<h2><span class="orange">Partial Least Squares in Python (Step-by-Step)</span></h2>
One of the most common problems that you’ll encounter in machine learning is  multicollinearity . This occurs when two or more predictor variables in a dataset are highly correlated.
When this occurs, a model may be able to fit a training dataset well but it may perform poorly on a new dataset it has never seen because it  overfits  the training set.
One way to get around this problem is to use a method known as  partial least squares , which works as follows:
Standardize both the predictor and response variables.
Calculate <em>M</em> linear combinations (called “PLS components”) of the original <em style="color: #000000;">p</em> predictor variables that explain a significant amount of variation in both the response variable and the predictor variables.
Use the method of least squares to fit a linear regression model using the PLS components as predictors.
Use  k-fold cross-validation  to find the optimal number of PLS components to keep in the model.
This tutorial provides a step-by-step example of how to perform partial least squares in Python.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import the necessary packages to perform partial least squares in Python:
<b>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import scale 
from sklearn import model_selection
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import train_test_split
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import mean_squared_error
</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use a dataset called <b>mtcars</b>, which contains information about 33 different cars. We’ll use <b>hp</b> as the response variable and the following variables as the predictors:
mpg
disp
drat
wt
qsec
The following code shows how to load and view this dataset:
<b>#define URL where data is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data_full = pd.read_csv(url)
#select subset of data
data = data_full[["mpg", "disp", "drat", "wt", "qsec", "hp"]]
#view first six rows of data
data[0:6]
        mpgdispdratwtqsechp
021.0160.03.902.62016.46110
121.0160.03.902.87517.02110
222.8108.03.852.32018.6193
321.4258.03.083.21519.44110
418.7360.03.153.44017.02175
518.1225.02.763.46020.22105</b>
<h3>Step 3: Fit the Partial Least Squares Model</h3>
The following code shows how to fit the PLS model to this data.
Note that <b>cv = RepeatedKFold()</b> tells Python to use  k-fold cross-validation  to evaluate the performance of the model. For this example we choose k = 10 folds, repeated 3 times.
<b>#define predictor and response variables
X = data[["mpg", "disp", "drat", "wt", "qsec"]]
y = data[["hp"]]
#define cross-validation method
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
mse = []
n = len(X)
# Calculate MSE with only the intercept
score = -1*model_selection.cross_val_score(PLSRegression(n_components=1),
           np.ones((n,1)), y, cv=cv, scoring='neg_mean_squared_error').mean()    
mse.append(score)
# Calculate MSE using cross-validation, adding one component at a time
for i in np.arange(1, 6):
    pls = PLSRegression(n_components=i)
    score = -1*model_selection.cross_val_score(pls, scale(X), y, cv=cv,
               scoring='neg_mean_squared_error').mean()
    mse.append(score)
#plot test MSE vs. number of components
plt.plot(mse)
plt.xlabel('Number of PLS Components')
plt.ylabel('MSE')
plt.title('hp')
</b>
<h3><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/plsPython1.png"></h3>
The plot displays the number of PLS components along the x-axis and the test MSE (mean squared error) along the y-axis.
From the plot we can see that the test MSE decreases by adding in two PLS components, yet it begins to increase as we add more than two PLS components.
Thus, the optimal model includes just the first two PLS components.
<h3>Step 4: Use the Final Model to Make Predictions</h3>
We can use the final PLS model with two PLS components to make predictions on new observations.
The following code shows how to split the original dataset into a training and testing set and use the PLS model with two PLS components to make predictions on the testing set.
<b>#split the dataset into training (70%) and testing (30%) sets
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) 
#calculate RMSE
pls = PLSRegression(n_components=2)
pls.fit(scale(X_train), y_train)
np.sqrt(mean_squared_error(y_test, pls.predict(scale(X_test))))
29.9094
</b>
We can see that the test RMSE turns out to be <b>29.9094</b>. This is the average deviation between the predicted value for <em>hp</em> and the observed value for <em>hp</em> for the observations in the testing set.
The complete Python code use in this example can be found  here .
<h2><span class="orange">Partial Least Squares in R (Step-by-Step)</span></h2>
One of the most common problems that you’ll encounter in machine learning is  multicollinearity . This occurs when two or more predictor variables in a dataset are highly correlated.
When this occurs, a model may be able to fit a training dataset well but it may perform poorly on a new dataset it has never seen because it  overfits  the training set.
One way to get around this problem is to use a method known as  partial least squares , which works as follows:
Standardize both the predictor and response variables.
Calculate <em>M</em> linear combinations (called “PLS components”) of the original <em style="color: #000000;">p</em> predictor variables that explain a significant amount of variation in both the response variable and the predictor variables.
Use the method of least squares to fit a linear regression model using the PLS components as predictors.
Use  k-fold cross-validation  to find the optimal number of PLS components to keep in the model.
This tutorial provides a step-by-step example of how to perform partial least squares in R.
<h3>Step 1: Load Necessary Packages</h3>
The easiest way to perform partial least squares in R is by using functions from the  pls  package.
<b>#install pls package (if not already installed)
install.packages("pls")
load pls package
library(pls)
</b>
<h3>Step 2: Fit Partial Least Squares Model</h3>
For this example, we’ll use the built-in R dataset called <b>mtcars</b> which contains data about various types of cars:
<b>#view first six rows of mtcars dataset
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
For this example we’ll fit a partial least squares (PLS) model using <em>hp</em> as the  response variable  and the following variables as the predictor variables:
mpg
disp
drat
wt
qsec
The following code shows how to fit the PLS model to this data. Note the following arguments:
<b>scale=TRUE</b>: This tells R that each of the variables in the dataset should be scaled to have a mean of 0 and a standard deviation of 1. This ensures that no predictor variable is overly influential in the model if it happens to be measured in different units.
<b>validation=”CV”</b>: This tells R to use  k-fold cross-validation  to evaluate the performance of the model. Note that this uses k=10 folds by default. Also note that you can specify “LOOCV” instead to perform  leave-one-out cross-validation .
<b>#make this example reproducible
set.seed(1)
#fit PCR model
model &lt;- plsr(hp~mpg+disp+drat+wt+qsec, data=mtcars, scale=TRUE, validation="CV")</b>
<h3>Step 3: Choose the Number of PLS Components</h3>
Once we’ve fit the model, we need to determine the number of PLS components worth keeping.
The way to do so is by looking at the test root mean squared error (test RMSE) calculated by the k-fold cross-validation:
<b>#view summary of model fitting
summary(model)
Data: X dimension: 32 5 
Y dimension: 32 1
Fit method: kernelpls
Number of components considered: 5
VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps
CV           69.66    40.57    35.48    36.22    36.74    36.67
adjCV        69.66    40.41    35.12    35.80    36.27    36.20
TRAINING: % variance explained
    1 comps  2 comps  3 comps  4 comps  5 comps
X     68.66    89.27    95.82    97.94   100.00
hp    71.84    81.74    82.00    82.02    82.03
</b>
There are two tables of interest in the output:
<b>1. VALIDATION: RMSEP</b>
This table tells us the test RMSE calculated by the k-fold cross validation. We can see the following:
If we only use the intercept term in the model, the test RMSE is <b>69.66</b>.
If we add in the first PLS component, the test RMSE drops to <b>40.57.</b>
If we add in the second PLS component, the test RMSE drops to <b>35.48.</b>
We can see that adding additional PLS components actually leads to an increase in test RMSE. Thus, it appears that it would be optimal to only use two PLS components in the final model.
<b>2. TRAINING: % variance explained</b>
This table tells us the percentage of the variance in the response variable explained by the PLS components. We can see the following:
By using just the first PLS component, we can explain <b>68.66%</b> of the variation in the response variable.
By adding in the second PLS component, we can explain <b>89.27%</b> of the variation in the response variable.
Note that we’ll always be able to explain more variance by using more PLS components, but we can see that adding in more than two PLS components doesn’t actually increase the percentage of explained variance by much.
We can also visualize the test RMSE (along with the test MSE and R-squared) based on the number of PLS components by using the <b>validationplot()</b> function. 
<b>#visualize cross-validation plots
validationplot(model)
validationplot(model, val.type="MSEP")
validationplot(model, val.type="R2")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/plsR1.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/plsR2.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/plsR3.png">
In each plot we can see that the model fit improves by adding in two PLS components, yet it tends to get worse when we add more PLS components.
Thus, the optimal model includes just the first two PLS components.
<h3>Step 4: Use the Final Model to Make Predictions</h3>
We can use the final model with two PLS components to make predictions on new observations.
The following code shows how to split the original dataset into a training and testing set and use the final model with two PLS components to make predictions on the testing set.
<b>#define training and testing sets
train &lt;- mtcars[1:25, c("hp", "mpg", "disp", "drat", "wt", "qsec")]
y_test &lt;- mtcars[26:nrow(mtcars), c("hp")]
test &lt;- mtcars[26:nrow(mtcars), c("mpg", "disp", "drat", "wt", "qsec")]
    
#use model to make predictions on a test set
model &lt;- plsr(hp~mpg+disp+drat+wt+qsec, data=train, scale=TRUE, validation="CV")
pcr_pred &lt;- predict(model, test, ncomp=2)
#calculate RMSE
sqrt(mean((pcr_pred - y_test)^2))
[1] 54.89609
</b>
We can see that the test RMSE turns out to be <b>54.89609</b>. This is the average deviation between the predicted value for <em>hp</em> and the observed value for <em>hp</em> for the observations in the testing set.
Note that an  equivalent principal components regression model  with two principal components produced a test RMSE of <b>56.86549</b>. Thus, the PLS model slightly outperformed the PCR model for this dataset.
The complete R code use in this example can be found  here .
<h2><span class="orange">An Introduction to Partial Least Squares</span></h2>
One of the most common problems that you’ll encounter in machine learning is  multicollinearity . This occurs when two or more predictor variables in a dataset are highly correlated.
When this occurs, a model may be able to fit a training dataset well but it may perform poorly on a new dataset it has never seen because it  overfits  the training set.
One way to get around the problem of multicollinearity is to use  principal components regression , which calculates <em>M</em> linear combinations (known as “principal components”) of the original <em>p</em> predictor variables  and then uses the method of least squares to fit a linear regression model using the principal components as predictors.
The drawback of principal components regression (PCR) is that it does not consider the  response variable  when calculating the principal components.
Instead, it only considers the magnitude of the variance among the predictor variables captured by the principal components. Because of this, it’s possible that in some cases the principal components with the largest variances aren’t actually able to predict the response variable well.
A technique that is related to PCR is known as <b>partial least squares</b>. Similar to PCR, partial least squares calculates <em>M</em> linear combinations (known as “PLS components”) of the original <em>p</em> predictor variables and uses the method of least squares to fit a linear regression model using the PLS components as predictors.
But unlike PCR, partial least squares attempts to find linear combinations that explain the variation in <em>both</em> the response variable and the predictor variables.
<h3>Steps to Perform Partial Least Squares</h3>
In practice, the following steps are used to perform partial least squares.
<b>1. </b>Standardize the data such that all of the predictor variables and the response variable have a mean of 0 and a standard deviation of 1. This ensures that each variable is measured on the same scale.
<b>2.</b> Calculate Z<sub>1</sub>, … , Z<sub>M</sub> to be the <em>M</em> linear combinations of the original <em>p</em> predictors.
Z<sub>m</sub> = ΣΦ<sub>jm</sub>X<sub>j</sub> for some constants Φ<sub>1m</sub>, Φ<sub>2m</sub>, Φ<sub>pm</sub>, m = 1, …, M.
To calculate Z<sub>1</sub>, set Φ<sub>j1 </sub>equal to the coefficient from the simple linear regression of Y onto X<sub>j</sub>is the linear combination of the predictors that captures the most variance possible.
To calculate Z<sub>2</sub>, regression each variable on Z<sub>1</sub> and take the residuals. Then calculate Z<sub>2</sub> using this orthogonalized data in exactly the same manner that Z<sub>1</sub> was calculated.
Repeat this process <em>M</em> times to obtain the <em>M</em> PLS components.
<b>3.</b> Use the method of least squares to fit a linear regression model using the PLS components Z<sub>1</sub>, … , Z<sub>M</sub> as predictors.
<b>4.</b> Lastly, use  k-fold cross-validation  to find the optimal number of PLS components to keep in the model. The “optimal” number of PLS components to keep is typically the number that produces the lowest test mean-squared error (MSE).
<h3>Conclusion</h3>
In cases where multicollinearity is present in a dataset, partial least squares tends to perform better than ordinary least squares regression. However, it’s a good idea to fit several different models so that we can identify the one that generalizes best to unseen data.
In practice, we fit many different types of models (PLS,  PCR ,  Ridge ,  Lasso ,  Multiple Linear Regression , etc.) to a dataset and use k-fold cross-validation to identify the model that produces the lowest test MSE on new data.
<h2><span class="orange">Partial Regression Coefficient: Definition & Example</span></h2>
A <b>partial regression coefficient</b> is the name given to the regression coefficients in a  multiple linear regression model .
This is in contrast to a plain old “regression coefficient”, which is the name given to the regression coefficient in a  simple linear regression model .
The way to interpret a partial regression coefficient is: The average change in the  response variable  associated with a one unit increase in a given predictor variable, assuming all other predictor variables are held constant.
The following example explains how to identify and interpret partial regression coefficients in a multiple linear regression model.
<h3>Example: Interpreting Partial Regression Coefficients</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain college entrance exam.
To explore this relationship, we can fit a multiple linear regression model using <b>hours studied</b> and <b>prep exams taken </b>as predictor variables and <b>exam score </b>as a response variable.
The following regression table shows the output of the model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
Here is how to interpret the partial regression coefficients:
<b>Hours:</b> For each additional hour spent studying, exam score increases by an average of <b>5.56</b> points, assuming the number of prep exams is held constant.
Here’s another way to think about this: If student A and student B both take the same amount of prep exams but student A studies for one hour more, then student A is expected to earn a score that is 5.56 points higher than student B.
<b>Prep Exams:</b> For each additional prep exam taken, exam score decreases by an average of <b>0.60</b> points, assuming the number of hours studied is held constant.
Another way to think about this: If student A and student B both study for the same number of hours but student A takes one additional prep exam, then student A is expected to earn a score that is 0.60 points lower than student B.
Using the coefficients from the regression output, we can write the estimated multiple linear regression equation:
Exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams) 
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study and the number of prep exams they take.
For example, a student who studies for three hours and takes one prep exam is expected to receive a score of <b>83.75</b>:
Exam score = 67.67 + 5.56*(3) – 0.60*(1) = 83.75
<h2><span class="orange">How to Create Partial Residual Plots in R</span></h2>
 Multiple linear regression  is a statistical method we can use to understand the relationship between multiple predictor variables and a  response variable .
However, one of the key  assumptions  of multiple linear regression is that there exists a linear relationship between each predictor variable and the response variable.
If this assumption is violated, then the results of the regression model can be unreliable.
One way to check this assumption is to create a <b>partial residual plot</b>, which displays the  residuals  of one predictor variable against the response variable.
The following example shows how to create partial residual plots for a regression model in R.
<h3>Example: How to Create Partial Residual Plots in R</h3>
Suppose we fit a regression model with three predictor variables in R:
<b>#make this example reproducible
set.seed(0)
#define response variable
y &lt;- c(1:1000)
#define three predictor variables
x1 &lt;- c(1:1000)*runif(n=1000)
x2 &lt;- (c(1:1000)*rnorm(n=1000))^2
x3 &lt;- (c(1:1000)*rnorm(n=1000))^3
#fit multiple linear regression model
model &lt;- lm(y~x1+x2+x3))</b>
We can use the <b>crPlots()</b> function from the <b>car</b> package in R to create partial residual plots for each predictor variable in the model:
<b>library(car)
#create partial residual plots
crPlots(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/partial1.jpg">
The blue line shows the expected residuals if the relationship between the predictor and response variable was linear. The pink line shows the actual residuals.
If the two lines are significantly different, then this is evidence of a nonlinear relationship.
From the plots above we can see that the residuals for both x2 and x3 appear to be nonlinear.
This violates the assumption of linearity for multiple linear regression. One way to fix this issue is to use a square root or cubic transformation on the predictor variables:
<b>library(car)
#fit new model with transformed predictor variables
model_transformed &lt;- lm(y~x1+sqrt(x2)+log10(x3^(1/3)))
#create partial residual plots for new model
crPlots(model_transformed)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/partial2.jpg"448">
From the partial residual plots we can see that x2 now has a more linear relationship with the response variable.
The predictor variable x3 is still somewhat nonlinear so we may decide to try another transformation or possibly drop the variable from the model altogether.
<h2><span class="orange">Pearson’s Coefficient of Skewness in Excel (Step-by-Step)</span></h2>
Developed by biostatistician  Karl Pearson , <b>Pearson’s coefficient of skewness</b> is a way to measure the  skewness  in a sample dataset.
There are actually two methods that can be used to calculate Pearson’s coefficient of skewness:
<b>Method 1: Using the Mode</b>
Skewness = (Mean – Mode) / Sample standard deviation
<b>Method 2: Using the Median</b>
Skewness = 3(Mean – Median) / Sample standard deviation
In general, the second method is preferred because the mode is not always a good indication of where the “central” value of a dataset lies and there can be more than one mode in a given dataset.
The following step-by-step example shows how to calculate both versions of the Pearson’s coefficient of skewness for a given dataset in Excel.
<h3>Step 1: Create the Dataset</h3>
First, let’s create the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pearsonSkew1.png">
<h3>Step 2: Calculate the Pearson Coefficient of Skewness (Using the Mode)</h3>
Next, we can use the following formula to calculate the Pearson Coefficient of Skewness using the mode:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pearsonSkew2.png">
The skewness turns out to be <b>1.295</b>.
<h3>Step 3: Calculate the Pearson Coefficient of Skewness (Using the Median)</h3>
We can also use the following formula to calculate the Pearson Coefficient of Skewness using the median:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pearsonSkew3.png">
The skewness turns out to be <b>0.569</b>.
<h3>How to Interpret Skewness</h3>
We interpret the Pearson coefficient of skewness in the following ways:
A <b>value of 0</b> indicates no skewness. If we created a histogram to visualize the distribution of values in a dataset, it would be perfectly symmetrical.
A <b>positive value</b> indicates positive skew or “right” skew. A histogram would reveal a “tail” on the right side of the distribution.
A <b>negative value</b> indicates a negative skew or “left” skew. A histogram would reveal a “tail” on the left side of the distribution.
In our previous example, the skewness was positive which indicates that the distribution of data values was positively skewed or “right” skewed.
<h2><span class="orange">The Five Assumptions for Pearson Correlation</span></h2>
The <b>Pearson correlation coefficient</b> (also known as the “product-moment correlation coefficient”) measures the linear association between two variables.
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
However, before we calculate the Pearson correlation coefficient between two variables we should make sure that five assumptions are met:
<b>1. Level of Measurement:</b> The two variables should be measured at the <b>interval</b> or <b>ratio</b> level.
<b>2. Linear Relationship:</b> There should exist a linear relationship between the two variables.
<b>3. Normality: </b>Both variables should be roughly normally distributed.
<b>4. Related Pairs: </b>Each observation in the dataset should have a pair of values.
<b>5. No Outliers: </b>There should be no extreme outliers in the dataset.
In this article, we provide an explanation for each assumption along with how to determine if the assumption is met.
<h2>Assumption 1: Level of Measurement</h2>
To calculate a Pearson correlation coefficient between two variables, both of the variables should be measured at the <b>interval</b> or <b>ratio</b> level.
The following graphic provides a quick explanation of the four levels that variables can be measured at:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/levels_measurement.jpg"659">
Some examples of variables that can be measured on an <b>interval</b> scale include:
<b>Temperature:</b> Measured in Fahrenheit or Celsius
<b>Credit Scores:</b> Measured from 300 to 850
<b>SAT Scores:</b> Measured from 400 to 1,600
Some examples of variables that can be measured on a <b>ratio</b> scale include:
<b>Height:</b> Measured in centimeters, inches, feet, etc.
<b>Weight: </b>Measured in kilograms, pounds, etc.
<b>Length: </b>Measured in centimeters, inches, feet, etc.
If the variables are measured at an <b>ordinal</b> level, then you should instead calculate the  Spearman correlation coefficient  between them.
<b>Related:</b>  Levels of Measurement: Nominal, Ordinal, Interval and Ratio 
<h2>Assumption 2: Linear Relationship</h2>
To calculate a Pearson Correlation coefficient between two variables, there should exist a linear relationship between the two variables.
The easiest way to check this assumption is to simply create a scatter plot of the two variables. If the points in the plot fall roughly along a straight line, then a linear relationship exists:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/assum1.png">
However, if the points are randomly scattered about the plot or if they exhibit some other type of relationship (like quadratic) then a linear relationship does not exist between the variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/assum2.png">
In this case, a Pearson Correlation coefficient won’t do a good job of capturing the relationship between the variables.
<h2>Assumption 3: Normality</h2>
A Pearson Correlation coefficient also assumes that both variables are roughly  normally distributed .
You can check this assumption visually by creating a histogram or a Q-Q plot for each variable.
<b>1. Histogram</b>
If a histogram for a dataset is roughly bell-shaped, then it’s likely that the data is normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityAssume1.png">
<b>2. Q-Q Plot</b>
A Q-Q plot, short for “quantile-quantile” plot, is a type of plot that displays theoretical quantiles along the x-axis (i.e. where your data would lie if it did follow a normal distribution) and sample quantiles along the y-axis (i.e. where your data actually lies).
If the data values fall along a roughly straight line at a 45-degree angle, then the data is assumed to be normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityAssume2.png">
You can also perform a formal statistical test to determine if a variable is normally distributed.
If the  p-value  of the test is less than a certain significance level (like α = 0.05) then you have sufficient evidence to say that the data is <em>not</em> normally distributed.
There are three statistical tests that are commonly used to test for normality:
<b>1. The Jarque-Bera Test</b>
 How to Perform a Jarque-Bera Test in Excel 
 How to Perform a Jarque-Bera Test in R 
 How to Perform a Jarque-Bera Test in Python 
<b>2. The Shapiro-Wilk Test</b>
 How to Perform a Shapiro-Wilk Test in R 
 How to Perform a Shapiro-Wilk Test in Python 
<b>3. The Kolmogorov-Smirnov Test</b>
 How to Perform a Kolmogorov-Smirnov Test in R 
 How to Perform a Kolmogorov-Smirnov Test in Python 
<h2>Assumption 4: Related Pairs</h2>
A Pearson Correlation coefficient also assumes that each  observation  in the dataset should have a pair of values.
This assumption is easy to check. For example, if you’re calculating the correlation between weight and height then simply verify that each observation in the dataset has one measurement for weight and one measurement for height.
<h2>Assumption 5: No Outliers</h2>
A Pearson Correlation coefficient also assumes that there are no extreme outliers in the dataset since outliers heavily affect the calculation of the correlation coefficient.
To illustrate this, consider the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/assum3.png">
The Pearson Correlation coefficient between X and Y is <b>0.949</b>.
However, suppose we have one outlier in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/assum4.png">
The Pearson Correlation coefficient between X and Y is now <b>0.711</b>.
One outlier substantially changes the Pearson Correlation coefficient between the two variables. In this case, it could make sense to remove the outlier from the dataset.
<b>Related:</b>  The Complete Guide: When to Remove Outliers in Data 
<h2><span class="orange">Pearson Correlation Coefficient</span></h2>
The <b>Pearson correlation coefficient</b> (also known as the “product-moment correlation coefficient”) is a measure of the linear association between two variables <em>X </em>and <em>Y. </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
<h2>The Formula to Find the Pearson Correlation Coefficient</h2>
The formula to find the Pearson correlation coefficient, denoted as <em>r</em>, for a sample of data is ( via Wikipedia ):
<img class="lazy" data-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b9c2079a3ffc1aacd36201ea0a3fb2460dc226f"323">
You will likely never have to compute this formula by hand since you can use software to do this for you, but it’s helpful to have an understanding of what exactly this formula is doing by walking through an example.
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correlationExample.jpg"135">
If we plotted these (X, Y) pairs on a scatterplot, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl1-1.jpg"> 
Just from looking at this scatterplot we can tell that there is a positive association between variables X and Y: when X increases, Y tends to increase as well. But to quantify exactly how positively associated these two variables are, we need to find the Pearson correlation coefficient.
Let’s focus on just the numerator of the formula:
<img class="lazy" data-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b9c2079a3ffc1aacd36201ea0a3fb2460dc226f">
For each (X, Y) pair in our dataset, we need to find the difference between the x value and the mean x value, the difference between the y value and the mean y value, then multiply these two numbers together.
For example, our first (X, Y) pair is (2, 2). The mean x value in this dataset is 5 and the mean y value in this dataset is 7. So, the difference between the x value in this pair and the mean x value is 2 – 5 = -3. The difference between the y value in this pair and the mean y value is 2 – 7 = -5. Then, when we multiply these two numbers together we get -3 * -5 = 15.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl3.jpg"> 
Here’s a visual look at what we just did: 
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl3-1.jpg"> 
Next, we just need to do this for every single pair:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl5.jpg">  <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl6.jpg"> 
The last step to get the numerator of the formula is to simply add up all of these values:
15 + 3 +3 + 15 = <b>36</b>
Next, the denominator of the formula tells us to find the sum of all the squared differences for both x and y, then multiply these two numbers together, then take the square root:
<img class="lazy" data-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b9c2079a3ffc1aacd36201ea0a3fb2460dc226f">
So, first we’ll find the sum of the squared differences for both x and y:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl7.jpg"529"> 
Then we’ll multiply these two numbers together: 20 * 68 = 1,360.
Lastly, we’ll take the square root: √1,360 = <b>36.88</b>
So, we found the numerator of the formula to be 36 and the denominator to be 36.88. This means that our Pearson correlation coefficient is r = 36 / 36.88 = <b>0.976</b>
This number is close to 1, which indicates that there is a strong positive linear relationship between our variables <em>X </em>and <em>Y</em>. This confirms the relationship that we saw in the scatterplot.
<h2>Visualizing Correlations</h2>
Recall that a Pearson correlation coefficient tells us the <b>type</b> of linear relationship (positive, negative, none) between two variables as well as the <b>strength</b> of that relationship (weak, moderate, strong).
When we make a scatterplot of two variables, we can <em>see </em>the actual relationship between two variables. Here are the many different types of linear relationships we might see:
<b>Strong, positive relationship:</b> As the variable on the x-axis increases, the variable on the y-axis increases as well. The dots are packed together tightly, which indicates a strong relationship. <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/pos_strong.jpg"323"> 
Pearson correlation coefficient: <b>0.94</b>
<b>Weak, positive relationship:</b> As the variable on the x-axis increases, the variable on the y-axis increases as well. The dots are fairly spread out, which indicates a weak relationship.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/pos_weak.jpg"323"> 
Pearson correlation coefficient: <b>0.44</b>
<b>No relationship: </b>There is no clear relationship (positive or negative) between the variables. 
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/none.jpg"324"> 
Pearson correlation coefficient: <b>0.03</b>
<b>Strong, negative relationship: </b>As the variable on the x-axis increases, the variable on the y-axis decreases. The dots are packed tightly together, which indicates a strong relationship.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/neg_strong.jpg"321"> 
Pearson correlation coefficient: <b>-0.87</b>
<b>Weak, negative relationship:</b> As the variable on the x-axis increases, the variable on the y-axis decreases. The dots are fairly spread out, which indicates a weak relationship.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/neg_weak.jpg"324"> 
Pearson correlation coefficient: –<b>0.46</b>
<h2>Testing for Significance of a Pearson Correlation Coefficient</h2>
When we find the Pearson correlation coefficient for a set of data, we’re often working with a <em>sample </em>of data that comes from a larger <em>population</em>. This means that it’s possible to find a non-zero correlation for two variables even if they’re actually not correlated in the overall population.
For example, suppose we make a scatterplot for variables <em>X </em>and <em>Y </em>for every data point in the entire population and it looks like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl12.jpg"> 
Clearly these two variables are not correlated. However, it’s possible that when we take a sample of 10 points from the population, we choose the following points:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl13.jpg"> 
We may find that the Pearson correlation coefficient for this sample of points is 0.93, which indicates a strong positive correlation despite the population correlation being zero.
In order to test for whether or not a correlation between two variables is statistically significant, we can find the following test statistic:
Test statistic T = r * √(n-2) / (1-r<sup>2</sup>)
where <em>n </em>is the number of pairs in our sample, <em>r</em> is the Pearson correlation coefficient, and test statistic T follows a t distribution with n-2 degrees of freedom.
Let’s walk through an example of how to test for the significance of a Pearson correlation coefficient.
<h3>Example</h3>
The following dataset shows the height and weight of 12 individuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correlationExample2.jpg"193">
The scatterplot below shows the value of these two variables:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl15.jpg"> 
The Pearson correlation coefficient for these two variables is r = 0.836.
The test statistic T = .836 * √(12-2) / (1-.836<sup>2</sup>) = 4.804.
According to our  t distribution calculator , a t score of 4.804 with 10 degrees of freedom has a p-value of .0007. Since .0007 &lt; .05, we can conclude that the correlation between weight and height in this example is statistically significant at alpha = .05.
<h2>Cautions</h2>
While a Pearson correlation coefficient can be useful in telling us whether or not two variables have a linear association, we must keep three things in mind when interpreting a Pearson correlation coefficient:
<b>1. Correlation does not imply causation.</b> Just because two variables are correlated does not mean that one is necessarily <em>causing </em>the other to occur more or less often. A classic example of this is the positive correlation between ice cream sales and shark attacks. When ice cream sales increase during certain times of the year, shark attacks also tend to increase.
Does this mean ice cream consumption is <em>causing </em>shark attacks? Of course not! It just means that during the summer, both ice cream consumption and shark attacks tend to increase since ice cream is more popular during the summer and more people go in the ocean during the summer.
<b>2. Correlations are sensitive to outliers. </b>One extreme outlier can dramatically change a Pearson correlation coefficient. Consider the example below:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl9.jpg"> 
Variables <em>X </em>and <em>Y </em>have a Pearson correlation coefficient of <b>0.00</b>. But imagine that we have one outlier in the dataset:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl10.jpg"> 
Now the Pearson correlation coefficient for these two variables is <b>0.878</b>. This one outlier changes everything. This is why, when you calculate the correlation for two variables, it’s a good idea to visualize the variables using a scatterplot to check for outliers. 
<b>3. A Pearson correlation coefficient does not capture nonlinear relationships between two variables. </b>Imagine that we have two variables with the following relationship:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/correl11.jpg"> 
The Pearson correlation coefficient for these two variables is 0.00 because they have no linear relationship. However, these two variables do have a nonlinear relationship: The y values are simply the x values squared.
When using the Pearson correlation coefficient, keep in mind that you’re merely testing to see if two variables are <em>linearly</em> related. Even if a Pearson correlation coefficient tells us that two variables are uncorrelated, they could still have some type of nonlinear relationship. This is another reason that it’s helpful to create a scatterplot when analyzing the relationship between two variables – it may help you detect a nonlinear relationship.
<h2><span class="orange">Pearson Correlation Critical Values Table</span></h2>
The table below shows the Pearson correlation critical values for different significance levels and degrees of freedom. Note that degrees of freedom = n-2 where n = # pairs of data.
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/ppmc.png"></figure>
<h2><span class="orange">What Are Pearson Residuals? (Definition & Example)</span></h2>
<b>Pearson residuals</b> are used in a  Chi-Square Test of Independence  to analyze the difference between observed cell counts and expected cell counts in a contingency table.
The formula to calculate a <b>Pearson residual</b> is:
r<sub>ij</sub> = (O<sub>ij</sub> – E<sub>ij</sub>) / √E<sub>ij</sub>
where:
<b>r<sub>ij</sub></b>: The Pearson residual for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
<b>O<sub>ij</sub></b>: The observed value for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
<b>E<sub>ij</sub></b>: The expected value for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
A similar metric is the <b>Standardized (adjusted) Pearson residual</b>, which is calculated as:
r<sub>ij</sub> = (O<sub>ij</sub> – E<sub>ij</sub>) / √E<sub>ij</sub>(1-n<sub>i+</sub>)(1-n<sub>+j</sub>)
where:
<b>r<sub>ij</sub></b>: The Pearson residual for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
<b>O<sub>ij</sub></b>: The observed value for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
<b>E<sub>ij</sub></b>: The expected value for the cell in the i<sup>th</sup> column and j<sup>th</sup> row
<b>p<sub>i+</sub></b>: The row total divided by the grand total
<b>p<sub>+j</sub></b>: The column total divided by the grand total
Standardized Pearson residuals are normally distributed with a mean of 0 and standard deviation of 1. Any standardized Pearson residual with an absolute value above certain thresholds (e.g. 2 or 3) indicates a lack of fit.
The following example shows how to calculate Pearson residuals in practice.
<h3>Example: Calculating Pearson Residuals</h3>
Suppose researchers want to use a Chi-Square Test of Independence to determine whether or not gender is associated with political party preference.
They decide to take a simple random sample of 500 voters and survey them on their political party preference.
The following contingency table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
Before we calculate the Pearson residuals, we must first calculate the expected counts for each cell in the contingency table. We can use the following formula to do so:
Expected value = (row sum * column sum) / table sum.
For example, the expected value for Male Republicans is: (230*250) / 500 = <b>115</b>.
We can repeat this formula to obtain the expected value for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
Next, we can calculate the <b>Pearson residual</b> for each cell in the table.
For example, the Pearson residual for the cell that contains Male Republicans would be calculated as:
r<sub>ij</sub> = (O<sub>ij</sub> – E<sub>ij</sub>) / √E<sub>ij</sub>
r<sub>ij</sub> = (120 – 115) / √115
r<sub>ij</sub> = 0.466
We can repeat this formula to obtain the Pearson residual for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">0.446</td>
<td style="text-align: center;">-0.259</td>
<td style="text-align: center;">-0.383</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">-0.446</td>
<td style="text-align: center;">0.259</td>
<td style="text-align: center;">0.383</td>
</tr>
</tbody></table>
Next, we can calculate the <b>Standardized Pearson residual</b> for each cell in the table.
For example, the Standardized Pearson residual for the cell that contains Male Republicans would be calculated as:
r<sub>ij</sub> = (O<sub>ij</sub> – E<sub>ij</sub>) / √E<sub>ij</sub>(1-p<sub>i+</sub>)(1-p<sub>+j</sub>)
r<sub>ij</sub> = (120 – 115) / √115(1-250/500)(1-230/500)
r<sub>ij</sub> = 0.897
We can repeat this formula to obtain the Standardized Pearson residual for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">0.897</td>
<td style="text-align: center;">-0.463</td>
<td style="text-align: center;">-0.595</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">-0.897</td>
<td style="text-align: center;">0.463</td>
<td style="text-align: center;">0.595</td>
</tr>
</tbody></table>
We can see that none of the Pearson Standardized Residuals have an absolute value greater than 3, which indicates that none of the cells contribute to a significant lack of fit.
If we use  this online calculator  to perform a Chi-Square Test of Independence, we’ll find that the p-value of the test is <b>0.649198</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/ence1.png">
Since this p-value is not less than .05, we do not have sufficient evidence to say that there is an association between gender and political party preference.
<h2><span class="orange">How to Calculate Percent Change in Google Sheets (With Examples)</span></h2>
The <b>percent change</b> in values between one period and another period is calculated as:
<b>Percent change = (Value<sub>2</sub> – Value<sub>1</sub>) / Value<sub>1</sub> * 100</b>
For example, suppose a company makes 50 sales one month, then makes 56 sales the next month. We can use the following formula to calculate the percent change in sales from one month to the next:
Percent change = (Value<sub>2</sub> – Value<sub>1</sub>) / Value<sub>1</sub> * 100
Percent change = (56 – 50) / 50 * 100
Percent change = 12%
This tells us that sales grew by 12% from the first month to the second month.
To calculate percent change in Google Sheets, we can use the following formula:
<b>=to_percent((A2-A1)/A1)
</b>
Note that this formula automatically displays the result in a percentage format thanks to the <b>to_percent()</b> function.
The following example shows how to use this formula in practice.
<h3>Example: Calculating Percent Change in Google Sheets</h3>
Suppose we have the following dataset that shows the sales of a certain company during 10 consecutive sales periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/percentChange1.png">
We can use the following formula to calculate the percent change in sales between the first and second period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/percentChange2.png">
We can then drag this formula down to every remaining cell in column C to automatically calculate the percent change in sales between each consecutive period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/percentChange3.png">
From the output we can see:
Sales increased by <b>11%</b> from period 1 to period 2.
Sales increased by <b>4%</b> from period 2 to period 3.
Sales decreased by <b>1%</b> from period 3 to period 4.
And so on.
<h2><span class="orange">How to Create a Percent Frequency Distribution in Excel</span></h2>
A <b>percent frequency distribution</b> can be used to understand what percentage of a distribution is composed of certain values.
The following step-by-step example shows how to create a percent frequency distribution in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that contains information about 20 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq1.png">
<h3>Step 2: Calculate Frequencies</h3>
Next, we’ll use the <b>UNIQUE()</b> function to produce an array of unique team values in column A:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq2.png">
Next, we’ll use the<b> COUNTIF()</b> function to count the number of times each team appears:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq3.png">
<h3>Step 3: Convert Frequencies to Percentages</h3>
Next, we’ll convert each frequency to a percentage by dividing each individual frequency by the sum of the frequencies:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq4.png">
Next, we’ll highlight each of the values in column F and click the <b>percentage</b> (%) icon in the <b>Number</b> group along the top ribbon:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq5.png">
Each value will automatically be displayed as a percentage:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/freq6.png">
From the output we can see that:
<b>35%</b> of all players belong to team A
<b>25%</b> of all players belong to team B
<b>15%</b> of all players belong to team C
<b>25%</b> of all players belong to team D
Note that the percentages add up to 100%.
<h2><span class="orange">Format Numbers as Percentages in R (With Examples)</span></h2>
The easiest way to format numbers as percentages in R is to use the <b>percent()</b> function from the  scales  package. This function uses the following syntax:
<b>percent(x, accuracy = 1)</b>
where:
<b>x:</b> The object to format as a percentage.
<b>accuracy:</b> A number to round to. For example, use .01 to round to two decimal places.
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Format Percentages in a Vector</h3>
The following code shows how to format numbers as percentages in a vector:
<b>library(scales)
#create data
data &lt;- c(.3, .7, .14, .18, .22, .78)
#format numbers as percentages
percent(data, accuracy = 1)
[1] "30%" "70%" "14%" "18%" "22%" "78%"
#format numbers as percentages with one decimal place
percent(data, accuracy = 0.1)
[1] "30.0%" "70.0%" "14.0%" "18.0%" "22.0%" "78.0%"
#format numbers as percentages with two decimal places
percent(data, accuracy = 0.01)
[1] "30.00%" "70.00%" "14.00%" "18.00%" "22.00%" "78.00%"
</b>
<h3>Example 2: Format Percentages in a Data Frame Column</h3>
The following code shows how to format numbers as percentages in a column of a data frame:
<b>library(scales)
#create data frame
df = data.frame(region = c('A', 'B', 'C', 'D'),
                growth = c(.3, .7, .14, .18))
#view data frame
df
  region growth
1      A   0.30
2      B   0.70
3      C   0.14
4      D   0.18
#format numbers as percentages in growth column
df$growth &lt;- percent(df$growth, accuracy=1)
#view updated data frame
df
  region growth
1      A    30%
2      B    70%
3      C    14%
4      D    18%</b>
<h3>Example 3: Format Percentages in Multiple Data Frame Columns</h3>
The following code shows how to format numbers as percentages in multiple columns of a data frame:
<b>library(scales)
#create data frame
df = data.frame(region = c('A', 'B', 'C', 'D'),
                growth = c(.3, .7, .14, .18),
                trend = c(.04, .09, .22, .25))
#view data frame
df
  region growth trend
1      A   0.30  0.04
2      B   0.70  0.09
3      C   0.14  0.22
4      D   0.18  0.25
#format numbers as percentages in growth and trend columns
df[2:3] &lt;- sapply(df[2:3], function(x) percent(x, accuracy=1))
#view updated data frame
df
  region growth trend
1      A    30%    4%
2      B    70%    9%
3      C    14%   22%
4      D    18%   25%
</b>
You can find more R tutorials on  this page .
<h2><span class="orange">PERCENTILE.EXC vs. PERCENTILE.INC in Excel: What’s the Difference?</span></h2>
The n<sup>th</sup><b>percentile</b> of a dataset is the value that cuts off the first <em>n</em> percent of the data values when all of the values are sorted from least to greatest.
For example, the 90th percentile of a dataset is the value that cuts of the bottom 90% of the data values from the top 10% of data values.
There are three different functions you can use to calculate percentiles in Excel:
<b>1. PERCENTILE.EXC:</b> This function returns the k<sup>th</sup> percentile of a dataset, <b>excluding</b> the values 0 and 1.
<b>2. PERCENTILE.INC:</b> This function returns the k<sup>th</sup> percentile of a dataset, <b>including</b> the values 0 and 1.
<b>3. PERCENTILE:</b> This function returns the k<sup>th</sup> percentile of a dataset as well. It will return the exact same value as the <b>PERCENTILE.INC</b> function.
The following example shows how to use the various PERCENTILE functions in Excel.
<h3>Example: PERCENTILE.EXC vs. PERCENTILE.INC in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/percExcel1.png">
The following screenshot shows how to calculate the 20<sup>th</sup> percentile for the dataset using the three different percentile formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/percExcel2.png">
Using the <b>PERCENTILE </b>or <b>PERCENTILE.INC</b> functions, we calculate the 20<sup>th</sup> percentile to be 6.
Using the <b>PERCENTILE.EXC</b> function we calculate the 20<sup>th</sup> percentile to be 5.4.
<h3>When to Use PERCENTILE.EXC vs. PERCENTILE.INC</h3>
In almost all cases, it makes more sense to use the <b>PERCENTILE.INC</b> function because this function includes the values 0 and 1 when calculating the percentiles.
It’s also worth nothing that both the R programming language and the Python programming language use formulas to calculate percentiles that match the <b>PERCENTILE.INC</b> function in Excel.
The following tutorials explain how to calculate the percentiles of a dataset in both R and Python:
 How to Calculate Percentiles in R 
 How to Calculate Percentiles in Python 
No matter which function you use to calculate percentiles, the difference between the values calculated by <b>PERCENTILE.INC</b> and <b>PERCENTILE.EXC</b> will be very similar in most cases.
In some cases, it’s even possible that the two functions will return the same values depending on the sequence of numbers in the dataset.
<h2><span class="orange">How to Find Percentiles from Z-Scores on a TI-84 Calculator</span></h2>
To find the percentile of a z-score on a TI-84 calculator, you can use the following syntax:
<b>normalcdf(-99, z-score, μ, σ)</b>
where:
<b>μ </b>= population mean
<b>σ </b>= population standard deviation
To access this function on a TI-84 calculator, simply press 2nd then press VARS then scroll down to normalcdf( and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/clt1.png">
The following examples show how to use this function in practice.
<h3>Example 1: Find Percentile of a Negative Z-Score</h3>
Suppose we would like to find the percentile that corresponds to a z-score of <b>-1.44</b>.
We can use the following syntax on a TI-84 calculator to find this percentile:
<b>normalcdf(-99, -1.44, 0, 1)</b>
<b>Note:</b> We use -99 as the “lower bound” to simulate a value of negative infinity.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pz1.png">
The percentile that corresponds to a z-score of -1.44 is <b>0.0749</b>. This means that only 7.49% of values in the normal distribution fall below a z-score of -1.44.
<h3>Example 2: Find Percentile of a Positive Z-Score</h3>
Suppose we would like to find the percentile that corresponds to a z-score of <b>0.56</b>.
We can use the following syntax on a TI-84 calculator to find this percentile:
<b>normalcdf(-99, 0.56, 0, 1)</b>
Once again we use -99 as the “lower bound” to simulate a value of negative infinity.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/pz2.png">
The percentile that corresponds to a z-score of 0.56 is <b>0.7123</b>. This means that 71.23% of values in the normal distribution fall below a z-score of 0.56.
<h3>The Relationship Between Percentiles and Z-Scores</h3>
Z-scores can take on any value between negative infinity and infinity. Percentiles, however, can only take on values between 0 and 100.
A z-score of 0 corresponds to a percentile of exactly 0.50. Thus, any z-score greater than 0 corresponds to a percentile greater than 0.50 and any z-score less than 0 corresponds to a percentile less than 0.50.
<h2><span class="orange">How to Perform a Percentile IF Function in Excel</span></h2>
You can use the following formula to perform a Percentile IF function in Excel:
<b>=PERCENTILE(IF(GROUP_RANGE=GROUP, VALUES_RANGE), k)
</b>
This formula finds the kth percentile of all values that belong to a certain group.
When you type this formula into a cell in Excel, you need to press <b>Ctrl + Shift + Enter</b> since this is an array formula.
The following example shows how to use this function in practice.
<h2>Example: Percentile IF Function in Excel</h2>
Suppose we have the following dataset that shows the exam score received by 20 students who belong to either class A or class B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/percentileif1.png">
Now suppose we’d like to find the 90th percentile of the exam scores for each class.
To do so, we can use the <b>=UNIQUE()</b> function to first create a list of the unique class names. We’ll type the following formula into cell F2:
<b>=UNIQUE(B2:B21)</b>
Once we press enter, a list of unique class names will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/percentileif.png">
Next, we can use the <b>=PERCENTILE()</b> function to find the 90th percentile of exam scores in each class.
We’ll type the following formula into cell G2 and press <b>Ctrl + Shift + Enter</b> so Excel knows this is an array formula:
<b>=PERCENTILE(IF(B2:B21=F2, C2:C21), 0.9)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/percentileif2.png">
We’ll then copy and paste this formula into the remaining cells in column G:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/percentileif3.png">
From the output we can see:
The value at the 90th percentile of exam scores in class A was <b>93.2</b>.
The value at the 90th percentile of exam scores in class B was <b>89.8</b>.
<b>Note:</b> We chose to calculate the 90th percentile, but you can calculate any percentile you’d like. For example, to calculate the 75th percentile of exam scores for each class you can replace <b>0.9</b> with <b>0.75</b> in the formula.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Calculate a Five Number Summary in Excel 
 How to Calculate the Mean and Standard Deviation in Excel 
 How to Calculate the Interquartile Range (IQR) in Excel 
<h2><span class="orange">How to Use a Percentile IF Formula in Google Sheets</span></h2>
You can use the following formula to perform a <b>Percentile IF</b> function in Excel:
<b>=PERCENTILE(IF(GROUP_RANGE=GROUP, VALUES_RANGE), k)
</b>
This formula finds the kth percentile of all values that belong to a certain group.
When you type this formula into a cell in Google Sheets, you need to press <b>Ctrl + Shift + Enter</b> since this is an array formula.
The following example shows how to use this function in practice.
<h2>Example: Percentile IF Function in Google Sheets</h2>
Suppose we have the following dataset that shows the exam score received by 20 students who belong to either class A or class B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/percentileif1.jpg"466">
Now suppose we’d like to find the 90th percentile of the exam scores only among students who were in class A.
To do so, we can type the following formula into cell F2:
<b>=PERCENTILE(IF(B2:B21=E2, C2:C21), 0.9)</b>
Once we press <b>Ctrl</b> + <b>Shift</b> + <b>Enter</b>, the 90th percentile of exam scores among students in class A will be shown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/percentileif4.jpg">
From the output we can see the value at the 90th percentile of exam scores in class A was <b>93.2</b>.
To calculate the 90th percentile of exam scores for class B, we can simply change the value in cell E2.
The formula will automatically calculated the 90th percentile of exam scores among students in class B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/percentileif5.jpg"629">
We can see that the value at the 90th percentile of exam scores in class B was <b>89.8</b>.
Note that in these examples we chose to calculate the 90th percentile, but you can calculate any percentile you’d like.
For example, to calculate the 75th percentile of exam scores for each class you can replace <b>0.9</b> with <b>0.75</b> in the formula.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Calculate a Five Number Summary in Google Sheets 
 How to Calculate Mean and Standard Deviation in Google Sheets 
 How to Calculate Deciles in Google Sheets 
<h2><span class="orange">How to Calculate Percentile Rank in Excel (With Examples)</span></h2>
You can use the <b>PERCENTRANK</b> function in Excel to calculate the rank of a value in a dataset as a percentage of the total dataset.
This function uses the following basic syntax:
<b>=PERCENTRANK(A2:A16, A2)
</b>
This particular example calculates the percentile rank of value <b>A2</b> within the range <b>A2:A16</b>.
There are also two other percentile rank functions in Excel:
<b>PERCENTRANK.INC</b>: Calculates the percentile rank of a value, including the smallest and largest values.
<b>PERCENTRANK.EXC</b>: Calculates the percentile rank of a value, excluding the smallest and largest values.
The following examples show how to use these functions in practice.
<h3>Example: Calculate Percentile Rank in Excel</h3>
Suppose we have the following dataset that shows the exam scores received by 15 students in a certain class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/percentilerank1.jpg"459">
Now suppose we would like to calculate the percentile rank of each student’s score.
We can type the following formula into cell B2:
<b>=PERCENTRANK($A$2:$A$16, A2)</b>
We can then copy and paste this formula down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/percentilerank2.jpg">
Here’s how to interpret each of the percentile rank values:
The student who scored a 2 ranked at percentile <b>0</b> (or 0%) in the class.
The students who scored a 5 ranked at percentile <b>.071</b> (or 7.1%) in the class.
The student who scored a 7 ranked at percentile <b>.214</b> (or 21.4%) in the class.
And so on.
Note that when we use the <b>PERCENTRANK</b> function, the smallest value in the dataset will always have a percentile rank of 0 and the largest value in the dataset will always have a percentile rank of 1.
The following screenshot also shows how to use the <b>PERCENTRANK.INC</b> and <b>PERCENTRANK.EXC</b> functions:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/percentilerank3.jpg"505">
There are two things to notice here:
<b>1.</b> The <b>PERCENTILE.INC</b> function returns the exact same values as the <b>PERCENTRANK</b> function.
<b>2.</b> The <b>PERCENTILE.EXC</b> function does not return a value of 0 and 1 for the smallest and largest values in the dataset, respectively.
You can find the complete documentation for the <b>PERCENTRANK</b> function in Excel  here .
<h2><span class="orange">How to Calculate Percentile Rank for Grouped Data</span></h2>
You can use the following formula to calculate percentile rank for grouped data:
<b>Percentile Rank = L + (RN/100 – M) / F * C</b>
where:
<b>L</b>: The lower bound of the interval that contains the percentile rank
<b>R</b>: The percentile rank
<b>N</b>: The total frequency
<b>M</b>: The cumulative frequency leading up to the interval that contains the percentile rank
<b>F</b>: The frequency of the interval that contains the percentile rank
<b>C</b>: The class width
The following example shows how to use this formula in practice.
<h3>Example: Calculate Percentile Rank for Grouped Data</h3>
Suppose we have the following frequency distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/percentGrouped1.jpg"478">
Now suppose we’d like to calculate the value at the 64th percentile of this distribution.
The interval that contains the 64th percentile will be the <b>21-25</b> interval since 64 is between the cumulative frequencies of 58 and 70.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/percentGrouped2.jpg"644">
Knowing this, we can find each of the values necessary to plug into our formula:
<b>L</b>: The lower bound of the interval that contains the percentile rank
The lower bound of the interval is <b>21</b>.
<b>R</b>: The percentile rank
The percentile we’re interested in is <b>64</b>.
<b>N</b>: The total frequency
The total cumulative frequency in the table is <b>92</b>.
<b>M</b>: The cumulative frequency leading up to the interval that contains the percentile rank
The cumulative frequency leading up to the 21-25 class is <b>58</b>.
<b>F</b>: The frequency of the interval that contains the percentile rank
The frequency of the 21-25 class is <b>12</b>.
<b>C</b>: The class width
The class width is calculated as 25 – 21 = <b>4</b>.
We can then plug in all of these values into the formula from earlier to find the value at the 64th percentile:
Percentile Rank = L + (RN/100 – M) / F * C
64th Percentile Rank = 21 + (64*92/100 – 58) / 12 * 4
Percentile Rank = 21.293
The value at the 64th percentile is <b>21.293</b>.
<h2><span class="orange">How to Calculate Percentile Rank in R (2 Examples)</span></h2>
The <b>percentile rank</b> of a value tells us the percentage of values in a dataset that rank equal to or below a given value.
You can use the following methods to calculate percentile rank in R:
<b>Method 1: Calculate Percentile Rank for Entire Dataset</b>
<b>library(dplyr)
df %>%
  mutate(percent_rank = rank(x)/length(x))
</b>
<b>Method 2: Calculate Percentile Rank by Group</b>
<b>library(dplyr)
df %>%
  group_by(group_var) %>%
  mutate(percent_rank = rank(x)/length(x))
</b>
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=rep(c('A', 'B'), each=7), points=c(2, 5, 5, 7, 9, 13, 15, 17, 22, 24, 30, 31, 38, 39))
#view data frame
df
   team points
1     A      2
2     A      5
3     A      5
4     A      7
5     A      9
6     A     13
7     A     15
8     B     17
9     B     22
10    B     24
11    B     30
12    B     31
13    B     38
14    B     39</b>
<h3>Example 1: Calculate Percentile Rank for Entire Dataset</h3>
The following code shows how to use functions from the  dplyr  package in R to calculate the percentile rank of each value in the points column:
<b>library(dplyr)
#calculate percentile rank of points values
df %>%
  mutate(percent_rank = rank(points)/length(points))
   team points percent_rank
1     A      2   0.07142857
2     A      5   0.17857143
3     A      5   0.17857143
4     A      7   0.28571429
5     A      9   0.35714286
6     A     13   0.42857143
7     A     15   0.50000000
8     B     17   0.57142857
9     B     22   0.64285714
10    B     24   0.71428571
11    B     30   0.78571429
12    B     31   0.85714286
13    B     38   0.92857143
14    B     39   1.00000000</b>
Here’s how to interpret the values in the <b>percent_rank</b> column:
<b>7.14%</b> of the points values are equal to or less than 2.
<b>17.86%</b> of the points values are equal to or less than 5.
<b>28.57%</b> of the points values are equal to or less than 7.
And so on.
<h3>Example 2: Calculate Percentile Rank by Group</h3>
The following code shows how to use functions from the  dplyr  package in R to calculate the percentile rank of each value in the points column, grouped by team:
<b>library(dplyr)
#calculate percentile rank of points values grouped by team
df %>%
  group_by(team) %>%
  mutate(percent_rank = rank(points)/length(points))
# A tibble: 14 x 3
# Groups:   team [2]
   team  points percent_rank
             
 1 A          2        0.143
 2 A          5        0.357
 3 A          5        0.357
 4 A          7        0.571
 5 A          9        0.714
 6 A         13        0.857
 7 A         15        1    
 8 B         17        0.143
 9 B         22        0.286
10 B         24        0.429
11 B         30        0.571
12 B         31        0.714
13 B         38        0.857
14 B         39        1   </b>
Here’s how to interpret the values in the <b>percent_rank</b> column:
<b>14.3%</b> of the points values for team A are equal to or less than 2.
<b>35.7%</b> of the points values for team A are equal to or less than 5.
<b>57.1%</b> of the points values for team A are equal to or less than 7.
And so on.
<h2><span class="orange">Percentile to Z-Score Calculator</span></h2>
This calculator finds the z-score associated with a given percentile.
Simply enter a percentile in the box below and then click the “Calculate” button.
<label for="perc"><b>Percentile (between 0 and 1)</b></label>
<input type="number" id="perc" value="0.25">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Z-Score: </b> -0.6745
<script>
function calc() {
//get input values
var perc  = document.getElementById('perc').value*1;
//find z-score
var z = jStat.normal.inv(perc, 0, 1 )
//output
document.getElementById('z').innerHTML = z.toFixed(4);
}
</script>
<h2><span class="orange">Percentile vs. Quartile vs. Quantile: What’s the Difference?</span></h2>
Three terms that students often confuse in statistics are percentiles, quartiles, and quantiles.
Here’s a simple definition of each:
<b>Percentiles:</b> Range from 0 to 100.
<b>Quartiles:</b> Range from 0 to 4.
<b>Quantiles:</b> Range from any value to any other value.
Note that percentiles and quartiles are simply <em>types</em> of quantiles.
Some types of quantiles even have specific names, including:
4-quantiles are called <em>quartiles</em>.
5-quantiles are called <em>quintiles</em>.
8-quantiles are called <em>octiles</em>.
10-quantiles are called <em>deciles</em>.
100-quantiles are called <em>percentiles</em>.
Note that percentiles and quartiles share the following relationship:
0 percentile = 0 quartile (also called the minimum)
25th percentile = 1st quartile
50th percentile = 2nd quartile (also called the median)
75th percentile = 3rd quartile
100th percentile = 4th quartile (also called the maximum)
<h3>Example: Find Percentiles & Quartiles</h3>
Suppose we have the following dataset with 20 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/perc1.png">
Using statistical software (like Excel, R, Python, etc.) we can find the following percentiles and quartiles for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/perc2.png">
Here’s how to interpret these values:
The 0 percentile and 0 quartile is <b>3</b>.
The 25th percentile and 1st quartile is <b>8.5</b>.
The 50th percentile and 2nd quartile is <b>16.5</b>.
The 75th percentile and 3rd quartile is <b>23.5</b>.
The 100th percentile and 4th quartile is <b>37</b>.
<h3>When to Use Percentiles vs. Quartiles</h3>
<b>Percentiles</b> can be used to answer questions such as:
<b>What score does a student need to earn on a particular test to be in the top 10% of scores?</b>
To answer this, we would find the 90th percentile of all scores, which is the value that separates the bottom 90% of values from the top 10%.
<b>What heights encompass the middle 40% of heights for students at a particular school?</b>
To answer this, we would find the 70th percentile of heights and 30th percentile of heights, which are the two values that determine the upper and lower bounds for the middle 40% of heights.
<b>Quartiles</b> can be used to answer questions such as:
<b>What score does a student need to earn on a test to be in the top quarter of scores?</b>
To answer this, we would find the 3rd quartile of all scores, which is the value that separates the bottom 75% of values from the top 25%.
<b>What is the interquartile range of a given dataset?</b>
The interquartile range (IQR) is the range of the middle 50% of data values. To find the IQR for a given dataset, we can calculate 3rd quartile – 1st quartile.
<h2><span class="orange">How to Calculate Percentiles in Python (With Examples)</span></h2>
The n<sup>th</sup><b>percentile</b> of a dataset is the value that cuts off the first <em>n</em> percent of the data values when all of the values are sorted from least to greatest.
For example, the 90th percentile of a dataset is the value that cuts of the bottom 90% of the data values from the top 10% of data values.
We can quickly calculate percentiles in Python by using the  numpy.percentile()  function, which uses the following syntax:
<b>numpy.percentile(a, q)</b>
where:
<b>a:</b> Array of values
<b>q:</b> Percentile or sequence of percentiles to compute, which must be between 0 and 100 inclusive.
This tutorial explains how to use this function to calculate percentiles in Python.
<h3>How to Find Percentiles of an Array</h3>
The following code illustrates how to find various percentiles for a given array in Python:
<b>import numpy as np
#make this example reproducible
np.random.seed(0)
#create array of 100 random integers distributed between 0 and 500
data = np.random.randint(0, 500, 100)
#find the 37th percentile of the array
np.percentile(data, 37)
173.26
#Find the quartiles (25th, 50th, and 75th percentiles) of the array
np.percentile(data, [25, 50, 75])
array([116.5, 243.5, 371.5])
</b>
<h3>How to Find Percentiles of a DataFrame Column</h3>
The following code shows how to find the 95th percentile value for a single pandas DataFrame column:
<b>import numpy as np 
import pandas as pd
#create DataFrame
df = pd.DataFrame({'var1': [25, 12, 15, 14, 19, 23, 25, 29, 33, 35],   'var2': [5, 7, 7, 9, 12, 9, 9, 4, 14, 15],   'var3': [11, 8, 10, 6, 6, 5, 9, 12, 13, 16]})
#find 90th percentile of var1 column
np.percentile(df.var1, 95)
34.1
</b>
<h3>How to Find Percentiles of Several DataFrame Columns</h3>
The following code shows how to find the 95th percentile value for a several columns in a pandas DataFrame:
<b>import numpy as np 
import pandas as pd
#create DataFrame
df = pd.DataFrame({'var1': [25, 12, 15, 14, 19, 23, 25, 29, 33, 35],   'var2': [5, 7, 7, 9, 12, 9, 9, 4, 14, 15],   'var3': [11, 8, 10, 6, 6, 5, 9, 12, 13, 16]})
#find 95th percentile of each column
df.quantile(.95)
var1    34.10
var2    14.55
var3    14.65
#find 95th percentile of just columns var1 and var2
df[['var1', 'var2']].quantile(.95)
var1    34.10
var2    14.55</b>
Note that we were able to use the pandas  quantile()  function in the examples above to calculate percentiles.
<b>Related:</b>  How to Calculate Percentiles in R (With Examples) 
<h2><span class="orange">How to Easily Calculate Percentiles in R (With Examples)</span></h2>
The n<sup>th</sup><b>percentile</b> of a dataset is the value that cuts off the first <em>n</em> percent of the data values when all of the values are sorted from least to greatest.
For example, the 90th percentile of a dataset is the value that cuts of the bottom 90% of the data values from the top 10% of data values.
One of the most commonly used percentiles is the 50th percentile, which represents the median value of a dataset: this is the value at which 50% of all data values fall below.
Percentiles can be used to answer questions such as:
<b>What score does a student need to earn on a particular test to be in the top 10% of scores?</b> To answer this, we would find the 90th percentile of all scores, which is the value that separates the bottom 90% of values from the top 10%.
<b>What heights encompass the middle 50% of heights for students at a particular school?</b> To answer this, we would find the 75th percentile of heights and 25th percentile of heights, which are the two values that determine the upper and lower bounds for the middle 50% of heights.
<h2>How to Calculate Percentiles in R</h2>
We can easily calculate percentiles in R using the <b>quantile()</b> function, which uses the following syntax:
<b>quantile</b>(x, probs = seq(0, 1, 0.25))
<b>x:</b> a numeric vector whose percentiles we wish to find
<b>probs:</b> a numeric vector of probabilities in [0,1] that represent the percentiles we wish to find
<h3>Finding Percentiles of a Vector</h3>
The following code illustrates how to find various percentiles for a given vector in R:
<b>#create vector of 100 random values uniformly distributed between 0 and 500
data &lt;- runif(100, 0, 500)
#Find the quartiles (25th, 50th, and 75th percentiles) of the vector
quantile(data, probs = c(.25, .5, .75))
#      25%       50%       75% 
# 97.78961 225.07593 356.47943 
#Find the deciles (10th, 20th, 30th, ..., 90th percentiles) of the vector
quantile(data, probs = seq(.1, .9, by = .1))
#      10%       20%       30%       40%       50%       60%       70%       80% 
# 45.92510  87.16659 129.49574 178.27989 225.07593 300.79690 337.84393 386.36108 
#      90% 
#423.28070
#Find the 37th, 53rd, and 87th percentiles
quantile(data, probs = c(.37, .53, .87))
#     37%      53%      87% 
#159.9561 239.8420 418.4787 
</b>
<h3>Finding Percentiles of a Data Frame Column</h3>
To illustrate how to find the percentiles of a specific data frame column, we’ll use the built-in dataset <em>iris</em>:
<b>#view first six rows of <em>iris</em> dataset
head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
</b>
The following code shows how to find the 90th percentile value for the column <em>Sepal.Length</em>:
<b>quantile(iris$Sepal.Length, probs = 0.9)
#90% 
#6.9 </b>
<h3>Finding Percentiles of Several Data Frame Columns</h3>
We can also find percentiles for several columns at once using the  <b>apply()</b>  function:
<b>#define columns we want to find percentiles for
small_iris&lt;- iris[ , c('Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width')]
#use <em>apply()</em> function to find 90th percentile for every column
apply(small_iris, 2, function(x) quantile(x, probs = .9))
#Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
#        6.90         3.61         5.80         2.20 </b>
<h3>Finding Percentiles by Group</h3>
We can also find percentiles by group in R using the <b>group_by()</b> function from the   <b>dplyr</b>  library.
The following code illustrates how to find the 90th percentile of <em>Sepal.Length</em> for each of the
three species in the iris dataset:
<b>#load <em>dplyr </em>library
library(dplyr)
#find 90th percentile of <em>Sepal.Length </em>for each of the three species
iris %>%
  group_by(Species) %>%
  summarise(percent90 = quantile(Sepal.Length, probs = .9))
# A tibble: 3 x 2
#  Species    percent90
#            
#1 setosa          5.41
#2 versicolor      6.7 
#3 virginica       7.61</b>
The following code illustrates how to find the 90th percentile for all of the variables by Species:
<b>iris %>%
  group_by(Species) %>%
  summarise(percent90_SL = quantile(Sepal.Length, probs = .9),
            percent90_SW = quantile(Sepal.Width, probs = .9),
            percent90_PL = quantile(Petal.Length, probs = .9),
            percent90_PW = quantile(Petal.Width, probs = .9))
# A tibble: 3 x 5
#  Species    percent90_SL percent90_SW percent90_PL percent90_PW
#                                      
#1 setosa             5.41         3.9          1.7          0.4 
#2 versicolor         6.7          3.11         4.8          1.51
#3 virginica          7.61         3.31         6.31         2.4 
</b>
<h2>Visualizing Percentiles</h2>
There is no built-in function to visualize the percentiles of a dataset in R, but we can create a plot to visualize the percentiles relatively easily. 
The following code illustrates how to create a plot of the percentiles for the data values of <em>Sepal.Length </em>from the <em>iris </em>dataset:
<b>n = length(iris$Sepal.Length)
plot((1:n - 1)/(n - 1), sort(iris$Sepal.Length), type="l",
  main = "Visualizing Percentiles",
  xlab = "Percentile",
  ylab = "Value")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/percentilePlot1.jpg">
<h2><span class="orange">What is Perfect Multicollinearity? (Definition & Examples)</span></h2>
In statistics,  multicollinearity  occurs when two or more predictor variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model.
If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model.
The most extreme case of multicollinearity is known as <b>perfect multicollinearity</b>. This occurs when at least two predictor variables have an exact linear relationship between them.
For example, suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/perfectMult1.png">
Notice that the values for predictor variable x<sub>2</sub> are simply the values of x<sub>1</sub> multiplied by 2.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/perfectMult2.png">
This is an example of <b>perfect multicollinearity</b>.
<h2>The Problem with Perfect Multicollinearity</h2>
When perfect multicollinearity is present in a dataset, the method of ordinary least squares is unable to produce estimates for regression coefficients.
This is because it’s not possible to estimate the marginal effect of one predictor variable (x<sub>1</sub>) on the response variable (y) while holding another predictor variable (x<sub>2</sub>) constant because x<sub>2</sub> always moves exactly when x<sub>1</sub> moves.
In short, perfect multicollinearity makes it impossible to estimate a value for every coefficient in a regression model.
<h2>How to Handle Perfect Multicollinearity</h2>
The simplest way to handle perfect multicollinearity is to drop one of the variables that has an exact linear relationship with another variable.
For example, in our previous dataset we could simply drop x<sub>2</sub> as a predictor variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/perfectMult3.png">
We would then fit a regression model using x<sub>1</sub> as a predictor variable and y as the response variable.
<h2>Examples of Perfect Multicollinearity</h2>
The following examples show the three most common scenarios of perfect multicollinearity in practice.
<h3>1. One Predictor Variable is a Multiple of Another</h3>
Suppose we want to use “height in centimeters” and “height in meters” to predict the weight of a certain species of dolphin.
Here’s what our dataset might look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/perfectMult4.png">
Notice that the value for “height in centimeters” is simply equal to “height in meters” multiplied by 100. This is a case of perfect multicollinearity.
If we attempt to fit a multiple linear regression model in R using this dataset, we won’t be able to produce a coefficient estimate for the “meters” predictor variable:
<b>#define data
df &lt;- data.frame(weight=c(400, 460, 470, 475, 490, 440, 430, 490, 500, 540), m=c(1.3, .7, .6, 1.3, 1.2, 1.5, 1.2, 1.6, 1.1, 1.4), cm=c(130, 70, 60, 130, 120, 150, 120, 160, 110, 140))
#fit multiple linear regression model
model &lt;- lm(weight~m+cm, data=df)
#view summary of model
summary(model)
Call:
lm(formula = weight ~ m + cm, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-70.501 -25.501   5.183  19.499  68.590 
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  458.676     53.403   8.589 2.61e-05 ***
m              9.096     43.473   0.209    0.839    
cm                NA         NA      NA       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 41.9 on 8 degrees of freedom
Multiple R-squared:  0.005442,Adjusted R-squared:  -0.1189 
F-statistic: 0.04378 on 1 and 8 DF,  p-value: 0.8395</b>
<h3>2. One Predictor Variable is a Transformed Version of Another</h3>
Suppose we want to use “points” and “scaled points” to predict the rating of basketball players.
Let’s assume that the variable “scaled points” is calculated as:
Scaled points = (points – μ<sub>points</sub>) / σ<sub>points</sub>
Here’s what our dataset might look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/perfectMult5.png">
Notice that each value for “scaled points” is simply a standardized version of “points.” This is a case of perfect multicollinearity.
If we attempt to fit a multiple linear regression model in R using this dataset, we won’t be able to produce a coefficient estimate for the “scaled points” predictor variable:
<b>#define data
df &lt;- data.frame(rating=c(88, 83, 90, 94, 96, 78, 79, 91, 90, 82), pts=c(17, 19, 24, 29, 33, 15, 14, 29, 25, 22))
df$scaled_pts &lt;- (df$pts - mean(df$pts)) / sd(df$pts)
#fit multiple linear regression model
model &lt;- lm(rating~pts+scaled_pts, data=df)
#view summary of model
summary(model)
Call:
lm(formula = rating ~ pts + scaled_pts, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-4.4932 -1.3941 -0.2935  1.3055  5.8412 
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  67.4218     3.5896  18.783 6.67e-08 ***
pts           0.8669     0.1527   5.678 0.000466 ***
scaled_pts        NA         NA      NA       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 2.953 on 8 degrees of freedom
Multiple R-squared:  0.8012,Adjusted R-squared:  0.7763 
F-statistic: 32.23 on 1 and 8 DF,  p-value: 0.0004663
</b>
<h3>3. The Dummy Variable Trap</h3>
Another scenario where perfect multicollinearity can occur is known as the  dummy variable trap . This is when we want to use a categorical variable in a regression model and convert it into a “dummy variable” that takes on values of 0, 1, 2, etc.
For example, suppose we would like to use predictor variables “age” and “marital status” to predict income:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummy4.png">
To use “marital status” as a predictor variable, we need to first convert it to a dummy variable.
To do so, we can let “Single” be our baseline value since it occurs most often and assign values of 0 or 1 to “Married” and “Divorce” as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummy6.png">
A mistake would be to create three new dummy variables as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummyvartrap1.png">
In this case, the variable “Single” is a perfect linear combination of the “Married” and “Divorced” variables. This is an example of perfect multicollinearity.
If we attempt to fit a multiple linear regression model in R using this dataset, we won’t be able to produce a coefficient estimate for every predictor variable:
<b>#define data
df &lt;- data.frame(income=c(45, 48, 54, 57, 65, 69, 78, 83, 98, 104, 107), age=c(23, 25, 24, 29, 38, 36, 40, 59, 56, 64, 53), single=c(1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0), married=c(0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1), divorced=c(0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0))
#fit multiple linear regression model
model &lt;- lm(income~age+single+married+divorced, data=df)
#view summary of model
summary(model)
Call:
lm(formula = income ~ age + single + married + divorced, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-9.7075 -5.0338  0.0453  3.3904 12.2454 
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)  16.7559    17.7811   0.942  0.37739   
age           1.4717     0.3544   4.152  0.00428 **
single       -2.4797     9.4313  -0.263  0.80018   
married           NA         NA      NA       NA   
divorced     -8.3974    12.7714  -0.658  0.53187   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 8.391 on 7 degrees of freedom
Multiple R-squared:  0.9008,Adjusted R-squared:  0.8584 
F-statistic:  21.2 on 3 and 7 DF,  p-value: 0.0006865
</b>
<h2><span class="orange">Permuted Block Randomization: Definition & Examples</span></h2>
<b>Permuted block randomization </b>is a technique that can be used to randomly assign individuals in an experiment to a certain treatment within a block.
For example, suppose we want to test whether or not fertilizer A or fertilizer B leads to more growth in 24 plants across six different fields. Our <em>treatments</em> are fertilizer A and fertilizer B while our <em>blocks</em> are the different fields.
We can use the following steps to set up a permuted block randomization for this experiment:
<b>Step 1: Place each plant in one of the six blocks based on their field.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/permutedBlock1.png">
<b>Step 2: Generate all of the possible treatment arrangements.</b>
The total possible treatment arrangements can be calculated as:
<b>Total arrangements = b! / (b – t)!</b>
where:
<b>b: </b>Block size
<b>t: </b>Total number of treatments
In this example, there will be 4! / (4-2)! = 12/2 = <b>6 total arrangements</b>.
Here’s what they’ll look like if we list them out:
AABB
ABBA
ABAB
BBAA
BABA
BAAB
<b>Step 3: Randomly assign one arrangement to each block.</b>
Next, we’ll randomly assign one of the treatment arrangements to each block:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/permutedBlock2.png">
Notice that each block has a different treatment arrangement. Thus, our permuted block randomization is complete and we can proceed with the experiment.
<h3>
<b>Potential Advantages & Disadvantages</b>
</h3>
There are two main <b>advantages</b> of using a permuted block randomization:
<b>1.</b> Each block has the same number of individuals in each treatment.
<b>2.</b> There are an equal number of individuals assigned to each treatment at <em>any point in the experiment</em>. This is especially valuable if the experiment were to end early because researchers would have the same amount of data for each treatment group.
There is one potential <b>disadvantage </b>to using a permutated block randomization:
<b>1. </b>If the researchers know the block size then they may be able to know which treatment group a given individual will be assigned to late in the block. For example, if the block size is 4 (like in the example above) and 2 plants have already been assigned to fertilizer A, then the researcher will know that the last plant will be assigned to fertilizer B.
In a given experiment, researchers should ideally not know which individuals are assigned to which treatment so they don’t unknowingly act in a certain way to produce desired results.
One way to remedy this problem is through <em>blinding</em>, in which a third-party assigns individuals to treatments so that the researchers aren’t aware of the treatment assignments.
<h2><span class="orange">Phi Coefficient Calculator</span></h2>
svg:not(:root) {
  overflow: visible;
}
td input {
  max-width:60px;
  max-height:30px;
}
</style>
A <b> Phi Coefficient </b> is a measure of the association between two binary variables.
To calculate the Phi Coefficient for a 2×2 table of two random variables, simply fill in the cells of the table below and then click “Calculate.”
<table><tbody>
<tr style="max-height:10px">
<th style="min-width:120px"></th>
<th><b><span>Group 1</b></th>
<th><b><span>Group 2</b></th>
</tr>
<tr>
<td>Category 1</td>
<td><input type="text" id="o11" value="4"></td>
<td><input type="text" id="o12" value="9"></td>
</tr>
<tr>
<td>Category 2</td>
<td><input type="text" id="o21" value="8"></td>
<td><input type="text" id="o22" value="4"></td>
</tr>
</tbody></table>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Phi Coefficient (Φ): <b>-0.358974</b>
<script>
function calc() {
//get input data
var o11 = document.getElementById('o11').value;
var o12 = document.getElementById('o12').value;
var o21 = document.getElementById('o21').value;
var o22 = document.getElementById('o22').value;
//find Phi Coefficient
var p = (o11*o22-o12*o21) / Math.sqrt((o11-(-1*o12))*(o21-(-1*o22))*(o11-(-1*o21))*(o12-(-1*o22)));
//output results
document.getElementById('p').innerHTML = p.toFixed(6);
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate a Phi Coefficient in R</span></h2>
A <b>Phi Coefficient</b> (sometimes called a <em>mean square contingency coefficient</em>) is a measure of the association between two binary variables.
For a given 2×2 table for two random variables <em>x </em>and <em>y</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/phiCoef1.png">
The Phi Coefficient can be calculated as:
<b>Φ = (AD-BC) / √(A+B)(C+D)(A+C)(B+D)</b>
<h3>Example: Calculating a Phi Coefficient in R</h3>
Suppose we want to know whether or not gender is associated with political party preference so we take a  simple random sample  of 25 voters and survey them on their political party preference.
The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/phiCoef2.png">
We can use the following code to enter this data into a 2×2 matrix in R:
<b>#create 2x2 table
data = matrix(c(4, 8, 9, 4), nrow = 2)
#view dataset
data
     [,1] [,2]
[1,]    4    9
[2,]    8    4</b>
We can then use the  phi()  function from the <b>psych</b> package to calculate the Phi Coefficient between the two variables:
<b>#load psych package
library(psych)
#calculate Phi Coefficient
phi(data)
[1] -0.36
</b>
The Phi Coefficient turns out to be <b>-0.36</b>.
Note that the phi function rounds to 2 digits by default, but you can specify the function to round to as many digits as you’d like:
<b>#calculate Phi Coefficient and round to 6 digits
phi(data, digits = 6)
[1] -0.358974
</b>
<h3>How to Interpret a Phi Coefficient</h3>
Similar to a Pearson Correlation Coefficient, a Phi Coefficient takes on values between -1 and 1 where:
<b>-1</b> indicates a perfectly negative relationship between the two variables.
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a perfectly positive relationship between the two variables.
In general, the further away a Phi Coefficient is from zero, the stronger the relationship between the two variables.
In other words, the further away a Phi Coefficient is from zero, the more evidence there is for some type of systematic pattern between the two variables.
<h2><span class="orange">Phi Coefficient: Definition & Examples</span></h2>
A <b>Phi Coefficient</b> (sometimes called a <em>mean square contingency coefficient</em>) is a measure of the association between two binary variables.
For a given 2×2 table for two random variables <em>x </em>and <em>y</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/phiCoef1.png">
The Phi Coefficient can be calculated as:
<b>Φ = (AD-BC) / √(A+B)(C+D)(A+C)(B+D)</b>
<h3>Example: Calculating a Phi Coefficient</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 25 voters and survey them on their political party preference. The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/phiCoef2.png">
We can calculate the Phi Coefficient between the two variables as:
Φ = (4*4-9*8) / √(4+9)(8+4)(4+8)(9+4) = (16-72) / √24336 = <b>-0.3589</b>
<em><b>Note: </b>We could have also calculated this using the  Phi Coefficient Calculator .</em>
<h3>How to Interpret a Phi Coefficient</h3>
Similar to a Pearson Correlation Coefficient, a Phi Coefficient takes on values between -1 and 1 where:
<b>-1</b> indicates a perfectly negative relationship between the two variables.
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a perfectly positive relationship between the two variables.
In general, the further away a Phi Coefficient is from zero, the stronger the relationship between the two variables.
In other words, the further away a Phi Coefficient is from zero, the more evidence there is for some type of systematic pattern between the two variables.
<h2><span class="orange">How to Create a Pie Chart in Google Sheets (With Example)</span></h2>
A <b>pie chart</b> is a type of chart that is shaped like a circle and uses slices to represent proportions of a whole.
The following step-by-step example shows how to create a pie chart in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s enter some data that shows the total sales for 6 different products:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie11.png">
<h3>Step 2: Create the Pie Chart</h3>
Next, highlight the values in the range A1:B7. Then click the <b>Insert</b> tab and then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie12.png">
The following pie chart will automatically be inserted:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie13.png">
<h3>Step 3: Customize the Pie Chart</h3>
To customize the pie chart, click anywhere on the chart. Then click the three vertical dots in the top right corner of the chart. Then click <b>Edit chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie14.png">
In the <b>Chart editor</b> panel that appears on the right side of the screen, click the <b>Customize</b> tab to see a variety of options for customizing the chart.
First, we can click <b>Pie chart</b> and change the <b>Slice label</b> to <b>Percentage</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie14-1.png">
Next, we can click <b>Pie slice</b> and change the individual colors of the slices in the chart if we’d like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie15.png">
Next, we can click <b>Chart & axis titles</b> and change the <b>Chart title</b> to whatever we’d like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie16.png">
Next, we can click <b>Legend</b> and change the <b>Position</b> to wherever we’d like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie17.png">
The final pie chart looks like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pie18.png">
<h2><span class="orange">How to Create Pie Charts in SPSS</span></h2>
A <b>pie chart</b> is a circular chart that uses “pie slices” to display the relative sizes of data.
This tutorial explains how to create and interpret pie charts in SPSS.
<h3>Example: Pie Chart in SPSS</h3>
Suppose we have the following dataset that shows the state of residence for 15 different people:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pieSPSS1.png">
We can create a pie chart to visualize the frequencies of people from different states by clicking on the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Frequencies</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pieSPSS2.png">
The following window will pop up:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pieSPSS3.png">
Drag <b>state </b>over to the box labeled <b>Variable(s)</b>, then click on <b>Charts </b>and make sure that <b>Pie charts </b>is selected:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pieSPSS4.png">
Click <b>Continue</b>, then press <b>OK</b>. The following pie chart will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pieSPSS5.png">
From the chart we can see that 5 people are from Indiana (blue), 3 are from Kentucky (red), and 7 are from Ohio (green). The table above the pie chart also shows these numbers in percentage forms:
33.3% of people are from Indiana
20.0% of people are from Kentucky
46.7% of people are from Ohio
The pie chart helps us easily see that nearly half of the people in the dataset are from Ohio, as nearly half of the entire chart is green.
<h2><span class="orange">How to Create and Modify Pie Charts in Stata</span></h2>
A <b>pie chart </b>is a circular chart that uses “pie slices” to display the relative sizes of data.
This tutorial explains how to create and modify pie charts in Stata.
<h2>How to Create Pie Charts in Stata</h2>
We’ll use a dataset called <em>census </em>to illustrate how to create and modify pie charts in Stata.
First, load the data by typing the following into the Command box:
<b>sysuse census</b>
We can get a quick look at the dataset by typing the following into the Command box:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata1.png">
<h3>Basic Pie Chart</h3>
We can create a pie chart to display the population size of each region using the following syntax:
<b>graph pie pop, over(region)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata2.png">
<h3>Pie Chart with Labels</h3>
We can add labels directly to the pie chart by using the <b>plabel(_all name) </b>command:
<b><b>graph pie pop, over(region) plabel(_all name)</b></b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata7.png">
We can also modify the labels to be slightly larger with a white font so they’re easier to read:
<b>graph pie pop, over(region) plabel(_all name, size(*1.5) color(white))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata8.png">
<h3>Pie Chart with no Legend</h3>
If we choose to use labels, we may decide that a legend is unnecessary. We can turn off the legend by using the <b>legend(off) </b>command:
<b>graph pie pop, over(region) plabel(_all name, size(*1.5) color(white)) legend(off)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata9.png">
<h2>How to Modify Pie Charts in Stata</h2>
We can use several different commands to modify the appearance of the pie charts.
<h3>Adding a Title</h3>
We can add a title to the plot using the <b>title() </b>command:
<b>graph pie pop, over(region) title(“Population by Region”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata3.png">
<h3>Adding a Subtitle</h3>
We can also add a subtitle underneath the title using the <b>subtitle() </b>command:
<b>graph pie pop, over(region) title(“Population by Region”) subtitle(“n = 4 total regions”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata4.png">
<h3>Adding a Comment</h3>
We can also add a note or comment at the bottom of the graph by using the <b>note() </b>command:
<b>graph pie pop, over(region) note(“Source: 1980 Census Data”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata5.png">
<h3>Changing Colors</h3>
We can change the colors of the slices in the pie chart by using the  following syntax:
<b>pie(slice #, color(specific_color)</b>
<b>graph pie pop, over(region) pie(1, color(pink)) pie(2, color(brown)) pie(3, color(purple)) pie(4, color(yellow))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/pieChartStata6.png">
A full list of available colors can be found in the  Stata Documentation for colors .
<h2><span class="orange">How to Perform Piecewise Regression in R (Step-by-Step)</span></h2>
<b>Piecewise regression</b> is a regression method we often use when there are clear “breakpoints” in a dataset.
The following step-by-step example shows how to perform piecewise regression in R.
<h3>Step 1: Create the Data</h3>
First, let’s create the following data frame:
<b>#view DataFrame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), y=c(2, 4, 5, 6, 8, 10, 12, 13, 15, 19, 24, 28, 31, 34, 39, 44))
#view first six rows of data frame
head(df)
  x  y
1 1  2
2 2  4
3 3  5
4 4  6
5 5  8
6 6 10
</b>
<h3>Step 2: Visualize the Data</h3>
Next, let’s create a scatterplot to visualize the data:
<b>#create scatterplot of x vs. y
plot(df$x, df$y, pch=16, col='steelblue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/seg1.png">
We can see that the relationship between x and y appears to abruptly change around <b>x = 9</b>.
<h3>Step 3: Fit the Piecewise Regression Model</h3>
We can use the <b>segmented()</b> function from the  segmented  package in R to fit a piecewise regression model to our dataset:
<b>library(segmented)
#fit simple linear regression model
fit &lt;- lm(y ~ x, data=df)
#fit piecewise regression model to original model, estimating a breakpoint at x=9
segmented.fit &lt;- segmented(fit, seg.Z = ~x, psi=9)
#view summary of segmented model
summary(segmented.fit)
Call: 
segmented.lm(obj = fit, seg.Z = ~x, psi = 9)
Estimated Break-Point(s):
         Est. St.Err
psi1.x 8.762   0.26
Meaningful coefficients of the linear terms:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  0.32143    0.48343   0.665    0.519    
x            1.59524    0.09573  16.663 1.16e-09 ***
U1.x         2.40476    0.13539  17.762       NA    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 0.6204 on 12 degrees of freedom
Multiple R-Squared: 0.9983,  Adjusted R-squared: 0.9978 
Convergence attained in 2 iter. (rel. change 0)
</b>
The <b>segmented()</b> function detects a breakpoint at x = 8.762.
The fitted piecewise regression model is:
If x ≤ 8.762:  y = .32143 + 1.59524*(x)
If x > 8.762:  y = .32143 + 1.59524*(8.762) + (1.59524+2.40476)*(x-8.762)
For example, suppose we have a value of <b>x = 5</b>. The estimated y value would be:
y = .32143 + 1.59524*(x)
y = .32143 + 1.59524*(5)
y = <b>8.297</b>
Or suppose we have a value of <b>x = 12</b>. The estimated y value would be:
y = .32143 + 1.59524*(8.762) + (1.59524+2.40476)*(12-8.762)
y = <b>27.25</b>
<h3>Step 4: Visualize the Final Piecewise Regression Model</h3>
We can use the following code to visualize the final piecewise regression model on top of our original data:
<b>#plot original data
plot(df$x, df$y, pch=16, col='steelblue')
#add segmented regression model
plot(segmented.fit, add=T)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/seg2.png">
It appears that the piecewise regression model fits the data quite well.
<h2><span class="orange">What is Pillai’s Trace? (Definition & Example)</span></h2>
A  one-way ANOVA  is used to determine whether or not different levels of an explanatory variable lead to statistically different results in some  response variable .
For example, we might be interested in understanding whether or not three levels of education (Associate’s degree, Bachelor’s degree, Master’s degree) lead to  statistically different  annual incomes. In this case, we have one explanatory variable and one response variable.
A  MANOVA  is an extension of the one-way ANOVA in which there is more than one response variable. For example, we might be interested in understanding whether education leads to different annual incomes <em>and </em>different amounts of student loan debt. In this case, we have one explanatory variable and two response variables.
One of the test statistics that is produced by a MANOVA is <b>Pillai’s trace</b>.
<h3>What is Pillai’s Trace?</h3>
<b>Pillai’s trace<em> </em></b>is a test statistic produced by a MANOVA. It is a value that ranges from 0 to 1.
The closer Pillai’s trace is to 1, the stronger the evidence that the explanatory variable has a statistically significant effect on the values of the response variables.
Pillai’s trace, often denoted V, is calculated as:
V = trace(H(H+E)<sup>-1</sup>)
where:
<b>H:</b> The hypothesis sum of squares and cross products matrix
<b>E:</b> The error sum of squares and cross products matrix
When performing a MANOVA, most statistical software will use Pillai’s trace to calculate a rough approximation to an F-statistic along with a corresponding p-value.
If this p-value is less than some significance level (i.e. α = .05) then we reject the null hypothesis of the MANOVA and conclude that the explanatory variable has a significant effect on the values of the response variables.
<h3>When to Use Pillai’s Trace</h3>
When performing a MANOVA, most statistical software will actually produce four test statistics:
Pillai’s Trace
Wilks’ Lambda
Lawley-Hotelling Trace
Roy’s Largest Root
It is recommended to use Pillai’s trace as the test statistic when the assumptions of a MANOVA are violated. As a reminder, MANOVA makes the following assumptions:
The  residuals  follow a multivariate normal probability distribution with means equal to zero.
The variance-covariance matrices of each group of residuals are equal.
The  observations  are independent.
When one or more of these assumptions are violated, Pillai’s trace tends to be the most robust test statistic.
<h3>Example of Calculating Pillai’s Trace</h3>
In  this tutorial , we perform a MANOVA in Stata using the following variables:
<b>Explanatory variable:</b> Level of education (Associate, Bachelor, or Master)
<b>Response variables:</b> Annual income, Total student loan debt
The following screenshot shows the output of the MANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/manovaStata3.png">
Note that the MANOVA produced four test statistics:
<b>Wilks’ lambda: </b>F-Statistic = 5.02, P-value = 0.0023.
<b>Pillai’s trace: </b>F-Statistic = 4.07, P-value = 0.0071.
<b>Lawley-Hotelling trace: </b>F-Statistic = 5.94, P-value = 0.0008.
<b>Roy’s largest root: </b>F-Statistic = 13.10, P-value = 0.0002.
The F-value for each test statistic varies but each corresponding p-value is less than .05, so we would reject the null hypothesis of the MANOVA and conclude that level of education has a significant effect on annual income and total student loan debt.
<h2><span class="orange">How to Use the Pipe Operator in R (With Examples)</span></h2>
You can use the pipe operator (<b>%>%</b>) in R to “pipe” together a sequence of operations.
This operator is most commonly used with the  dplyr  package in R to perform a sequence of operations on a data frame.
The basic syntax for the pipe operator is:
<b>df %>% 
  do_this_operation %>% 
  then_do_this_operation %>%
  then_do_this_operation ...</b>
The pipe operator simply feeds the results of one operation into the next operation below it.
The advantage of using the pipe operator is that it makes code extremely easy to read.
The following examples show how to use the pipe operator in different scenarios with the built-in  mtcars  dataset in R.
<b>#view first six rows of mtcars dataset
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
<h2>Example 1: Use Pipe Operator to Summarize One Variable</h2>
The following code shows how to use the pipe (<b>%>%</b>) operator to group by the <b>cyl</b> variable and then summarize the mean value of the <b>mpg</b> variable:
<b>library(dplyr)
#summarize mean mpg grouped by cyl
mtcars %>% 
  group_by(cyl) %>% 
  summarise(mean_mpg = mean(mpg))
# A tibble: 3 x 2
    cyl mean_mpg
      
1     4     26.7
2     6     19.7
3     8     15.1
</b>
From the output we can see:
The mean mpg value for the cars with a cyl value of 4 is <b>26.7</b>.
The mean mpg value for the cars with a cyl value of 6 is <b>19.7</b>.
The mean mpg value for the cars with a cyl value of 8 is <b>15.1</b>.
Notice how easy the pipe operator makes it to interpret the code as well.
Essentially, it says:
Take the <b>mtcars</b> data frame.
Group it by the <b>cyl</b> variable.
Then summarize the mean value of the <b>mpg</b> variable.
<h2>Example 2: Use Pipe Operator to Group & Summarize Multiple Variables</h2>
The following code shows how to use the pipe (<b>%>%</b>) operator to group by the <b>cyl</b> and <b>am</b> variables, and then summarize the mean of the <b>mpg</b> variable and the standard deviation of the <b>hp</b> variable:
<b>library(dplyr)
#summarize mean mpg and standard dev of hp grouped by cyl and am
mtcars %>% 
  group_by(cyl, am) %>% 
  summarise(mean_mpg = mean(mpg),
            sd_hp = sd(hp))
# A tibble: 6 x 4
# Groups:   cyl [3]
    cyl    am mean_mpg sd_hp
        
1     4     0     22.9 19.7 
2     4     1     28.1 22.7 
3     6     0     19.1 9.18
4     6     1     20.6 37.5 
5     8     0     15.0 33.4 
6     8     1     15.4 50.2 
</b>
From the output we can see:
For cars with a cyl value of 4 and am value of 0, the mean mpg value is <b>22.9</b> and the standard deviation of the hp value is <b>19.7</b>.
For cars with a cyl value of 4 and am value of 1, the mean mpg value is <b>28.1 </b>and the standard deviation of the hp value is <b>22.7</b>.
And so on.
Once again, notice how easy the pipe operator makes it to interpret the code as well.
Essentially, it says:
Take the <b>mtcars</b> data frame.
Group it by the <b>cyl</b> and the <b>am</b> variables.
Then summarize the mean value of the <b>mpg</b> variable and the standard deviation of the <b>hp</b> variable.
<h2>Example 3: Use Pipe Operator to Create New Variables</h2>
The following code shows how to use the pipe (<b>%>%</b>) operator along with the  mutate  function from the <b>dplyr</b> package to create two new variables in the mtcars data frame:
<b>library(dplyr)
#add two new variables in mtcars
new_mtcars &lt;- mtcars %>%
                mutate(mpg2 = mpg*2,       mpg_root = sqrt(mpg))
#view first six rows of new data frame
head(new_mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb mpg2 mpg_root
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 42.0 4.582576
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 42.0 4.582576
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 45.6 4.774935
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 42.8 4.626013
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 37.4 4.324350
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 36.2 4.254409</b>
From the output we can see:
The new <b>mpg2</b> column contains the values of the <b>mpg</b> column multiplied by 2.
The new <b>mpg_root</b> column contains the square root of the values in the <b>mpg</b> column.
Once again, notice how easy the pipe operator makes it to interpret the code as well.
Essentially, it says:
Take the <b>mtcars</b> data frame.
Create a new column called <b>mpg2</b> and a new column called <b>mpg_root</b>.
<b>Related:</b>  How to Use the transmute() Function in dplyr 
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use the Tilde Operator (~) in R 
 How to Use Dollar Sign ($) Operator in R 
 How to Use “NOT IN” Operator in R 
 How to Use %in% Operator in R 
<h2><span class="orange">How to Quickly Create Pivot Tables in R</span></h2>
In Excel, <b>pivot tables</b> offer an easy way to group and summarize data.
For example, if we have the following dataset in Excel then we can use a pivot table to quickly summarize the total sales by region:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pivot1.png">
This tells us:
Region A had 51 total sales
Region B had 85 total sales
Region C had 140 total sales
Or we could summarize by another metric such as the average sales by region:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/pivot2.png">
It turns out that we can quickly create similar pivot tables in R by using the <b>group_by()</b> and <b>summarize()</b> functions from the  dplyr  package.
This tutorial provides several examples of how to do so.
<h3>Example: Create Pivot Tables in R</h3>
First, let’s create the same dataset in R that we used in the previous examples from Excel:
<b>#create data frame
df &lt;- data.frame(region=c('A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C'), device=c('X', 'X', 'Y', 'X', 'Y', 'Y', 'X', 'X', 'Y', 'Y'), sales=c(12, 18, 21, 22, 34, 29, 38, 36, 34, 32))
#view data frame
df
   region device sales
1       A      X    12
2       A      X    18
3       A      Y    21
4       B      X    22
5       B      Y    34
6       B      Y    29
7       C      X    38
8       C      X    36
9       C      Y    34
10      C      Y    32
</b>
Next, let’s load the dplyr package and use the <b>group_by()</b> and <b>summarize()</b> functions to group by region and find the sum of sales by region:
<b>library(dplyr)
#find sum of sales by region
df %>%
  group_by(region) %>% 
  summarize(sum_sales = sum(sales))
# A tibble: 3 x 2
  region sum_sales
        
1 A             51
2 B             85
3 C            140</b>
We can see that these numbers match the numbers shown in the introductory Excel example.
We can also calculate the average sales by region:
<b>#find average sales by region
df %>%
  group_by(region) %>% 
  summarize(mean_sales = mean(sales))
# A tibble: 3 x 2
  region  mean_sales
        
1 A             17  
2 B             28.3
3 C             35  </b>
Once again, these numbers match the numbers shown in the Excel example from earlier.
Note that we can also group by multiple variables. For example, we could find the sum of sales grouped by region <em>and</em> device type:
<b>#find sum of sales by region and device type
df %>%
  group_by(region, device) %>% 
  summarize(sum_sales = sum(sales))
# A tibble: 6 x 3
# Groups:   region [3]
  region device sum_sales
          
1 A      X             30
2 A      Y             21
3 B      X             22
4 B      Y             63
5 C      X             74
6 C      Y             66  </b>
<h2><span class="orange">How to Use pivot_longer() in R</span></h2>
The <b>pivot_longer()</b> function from the  tidyr  package in R can be used to pivot a data frame from a wide format to a long format.
This function uses the following basic syntax:
<b>library(tidyr)
<b>df %>% pivot_longer(cols=c('var1', 'var2', ...),    names_to='col1_name',    values_to='col2_name') </b>
</b>
where:
<b>cols</b>: The names of the columns to pivot
<b>names_to</b>: The name for the new character column
<b>values_to</b>: The name for the new values column
The following example shows how to use this function in practice.
<b>Related:</b>  Long vs. Wide Data: What’s the Difference? 
<h3>Example: Use pivot_longer() in R</h3>
Suppose we have the following data frame in R that shows the number of points scored by various basketball players in different years:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D'), year1=c(12, 15, 19, 19), year2=c(22, 29, 18, 12))
#view data frame
df
  player year1 year2
1      A    12    22
2      B    15    29
3      C    19    18
4      D    19    12</b>
We can use the<b> pivot_longer()</b> function to pivot this data frame into a long format:
<b>library(tidyr)
#pivot the data frame into a long format
df %>% pivot_longer(cols=c('year1', 'year2'),    names_to='year',    values_to='points')
# A tibble: 8 x 3
  player year  points
      
1 A      year1     12
2 A      year2     22
3 B      year1     15
4 B      year2     29
5 C      year1     19
6 C      year2     18
7 D      year1     19
8 D      year2     12
</b>
Notice that the column names <b>year1</b> and <b>year2</b> are now used as values in a new column called “year” and the values from these original columns are placed into one new column called “points.”
The final result is a long data frame.
<b>Note</b>: You can find the complete documentation for the <b>pivot_longer()</b> function  here .
<h2><span class="orange">How to Use pivot_wider() in R</span></h2>
The <b>pivot_wider()</b> function from the  tidyr  package in R can be used to pivot a data frame from a long format to a wide format.
This function uses the following basic syntax:
<b>library(tidyr)
df %>% pivot_wider(names_from = var1, values_from = var2)</b>
where:
<b>names_from</b>: The column whose values will be used as column names
<b>values_from</b>: The column whose values will be used as cell values
The following example shows how to use this function in practice.
<b>Related:</b>  Long vs. Wide Data: What’s the Difference? 
<h3>Example: Use pivot_wider() in R</h3>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(player=rep(c('A', 'B'), each=4), year=rep(c(1, 1, 2, 2), times=2), stat=rep(c('points', 'assists'), times=4), amount=c(14, 6, 18, 7, 22, 9, 38, 4))
#view data frame
df
  player year    stat amount
1      A    1  points     14
2      A    1 assists      6
3      A    2  points     18
4      A    2 assists      7
5      B    1  points     22
6      B    1 assists      9
7      B    2  points     38
8      B    2 assists      4</b>
We can use the<b> pivot_wider()</b> function to pivot this data frame into a wide format:
<b>library(tidyr)
#pivot the data frame into a wide format
df %>% pivot_wider(names_from = stat, values_from = amount)
# A tibble: 4 x 4
  player  year points assists
         
1 A          1     14       6
2 A          2     18       7
3 B          1     22       9
4 B          2     38       4
</b>
Notice that the values from the <b>stat</b> column are now used as column names and the values from the <b>amount</b> column are used as cell values in these new columns.
The final result is a wide data frame.
<b>Note</b>: You can find the complete documentation for the <b>pivot_wider()</b> function  here .
<h2><span class="orange">How to Plot a Beta Distribution in R (With Examples)</span></h2>
You can use the following syntax to plot a Beta distribution in R:
<b>#define range
p = seq(0, 1, length=100)
#create plot of Beta distribution with shape parameters 2 and 10
plot(p, dbeta(p, 2, 10), type='l')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Plot One Beta Distribution</h3>
The following code shows how to plot a single Beta distribution:
<b>#define range
p = seq(0,1, length=100)
#create plot of Beta distribution with shape parameters 2 and 10
plot(p, dbeta(p, 2, 10), type='l')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotBeta1.png">
You can also customize the colors and axes labels of the plot:
<b>#define range
p = seq(0,1, length=100)
#create custom plot of Beta distribution
plot(p, dbeta(p, 2, 10), ylab='density',
     type ='l', col='purple', main='Beta Distribution')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotbeta2.png">
<h3>Example 2: Plot Multiple Beta Distributions</h3>
The following code shows how to plot multiple Beta distributions with different shape parameters:
<b>#define range
p = seq(0,1, length=100)
#plot several Beta distributions
plot(p, dbeta(p, 2, 10), ylab='density', type ='l', col='purple')
lines(p, dbeta(p, 2, 2), col='red') 
lines(p, dbeta(p, 5, 2), col='blue')
#add legend
legend(.7, 4, c('Beta(2, 10)','Beta(2, 2)','Beta(1,1)'),
       lty=c(1,1,1),col=c('purple', 'red', 'blue'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotbeta3.png">
<h2><span class="orange">How to Plot a Binomial Distribution in R</span></h2>
To plot the probability mass function for a  binomial distribution  in R, we can use the following functions:
<b>dbinom(x, size, prob)</b> to create the probability mass function
<b>plot(x, y, type = ‘h’) </b>to plot the probability mass function, specifying the plot to be a histogram (type=’h’)
To plot the probability mass function, we simply need to specify <b>size </b>(e.g. number of trials) and <b>prob </b>(e.g. probability of success on a given trial) in the <b>dbinom() </b>function.
For example, the following code illustrates how to plot a probability mass function for a binomial distribution with size = 20 and prob = 0.3:
<b>success &lt;- 0:20
plot(success, dbinom(success, size=20, prob=.3),type='h')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomR1.png">
The x-axis shows the number of successes and the y-axis shows the probability of obtaining that number of successes in 20 trials.
We can add a title, change the axes labels, and increase the line width to make the plot more aesthetically pleasing:
<b>success &lt;- 0:20
plot(success,dbinom(success,size=20,prob=.3),
     type='h',
     main='Binomial Distribution (n=20, p=0.3)',
     ylab='Probability',
     xlab ='# Successes',
     lwd=3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomR2.png">
You can use the following code to obtain the actual probabilities for each number of successes shown in the plot:
<b>#prevent R from displaying numbers in scientific notation 
options(scipen=999) 
#define range of successes
success &lt;- 0:20
#display probability of success for each number of trials
dbinom(success, size=20, prob=.3)
[1] 0.00079792266297612 0.00683933711122388 0.02784587252426865
[4] 0.07160367220526231 0.13042097437387065 0.17886305056987975
[7] 0.19163898275344257 0.16426198521723651 0.11439673970486122
[10] 0.06536956554563482 0.03081708090008504 0.01200665489613703
[13] 0.00385928193090119 0.00101783259716075 0.00021810698510587
[16] 0.00003738976887529 0.00000500755833151 0.00000050496386536
[19] 0.00000003606884753 0.00000000162716605 0.00000000003486784
</b>
<h2><span class="orange">How to Plot Categorical Data in Pandas (With Examples)</span></h2>
There are three common ways to visualize  categorical data :
Bar Charts
Boxplots by Group
Mosaic Plots
The following examples show how to create each of these plots for a pandas DataFrame in Python.
<h2>Example 1: Bar Charts</h2>
The following code shows how to create a bar chart to visualize the frequency of teams in a certain pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'C', 'C', 'D'],   'points': [18, 22, 29, 25, 14, 11, 10, 15]})
#create bar plot to visualize frequency of each team
df['team'].value_counts().plot(kind='bar', xlabel='Team', ylabel='Count', rot=0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/plotcat1.jpg">
The x-axis displays each team name and the y-axis shows the frequency of each team in the DataFrame.
<b>Note</b>: The argument <b>rot=0</b> tells pandas to rotate the x-axis labels to be parallel to the x-axis.
<h2>Example 2: Boxplots by Group</h2>
Grouped boxplots are a useful way to visualize a numeric variable, grouped by a categorical variable.
For example, the following code shows how to create boxplots that show the distribution of points scored, grouped by team:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [18, 22, 29, 25, 14, 11, 10, 15]})
#create boxplot of points, grouped by team
df.boxplot(column=['points'], by='team', grid=False, color='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/plotcat2.jpg"554">
The x-axis displays the teams and the y-axis displays the distribution of points scored by each team.
<h2>Example 3: Mosaic Plot</h2>
A mosaic plot is a type of plot that displays the frequencies of two different categorical variables in one plot.
For example, the following code shows how to create a mosaic plot that shows the frequency of the categorical variables ‘result’ and ‘team’ in one plot:
<b>import pandas as pd
from statsmodels.graphics.mosaicplot import mosaic
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'result': ['W', 'L', 'L', 'W', 'W', 'L', 'L', 'W', 'W']})
#create mosaic plot
mosaic(df, ['team', 'result']);</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/plotcat3.jpg">
The x-axis displays the teams and the y-axis displays the frequency of results for each team.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Use Groupby and Plot in Pandas 
 How to Plot Distribution of Column Values in Pandas 
 How to Adjust the Figure Size of a Pandas Plot 
<h2><span class="orange">How to Plot Categorical Data in R (With Examples)</span></h2>
In statistics,  categorical data  represents data that can take on names or labels.
Examples include:
Smoking status (“smoker”, “non-smoker”)
Eye color (“blue”, “green”, “hazel”)
Level of education (e.g. “high school”, “Bachelor’s degree”, “Master’s degree”)
Three plots that are commonly used to visualize this type of data include:
Bar Charts
Mosaic Plots
Boxplots by Group
The following examples show how to create each of these plots in R.
<h3>Example 1: Bar Charts</h3>
The following code shows how to create a bar chart to visualize the frequency of teams in a certain data frame:
<b>library(ggplot2) 
#create data frame
df &lt;- data.frame(result = c('W', 'L', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'L'), team = c('B', 'B', 'B', 'B', 'D', 'A', 'A', 'A', 'C', 'C'), points = c(12, 28, 19, 22, 32, 45, 22, 28, 13, 19), rebounds = c(5, 7, 7, 12, 11, 4, 10, 7, 8, 8))
#create bar chart of teams
ggplot(df, aes(x=team)) +
  geom_bar()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/category1.png">
The x-axis displays each team name and the y-axis shows the frequency of each team in the data frame.
We can also use the following code to order the bars in the chart from largest to smallest:
<b>#create bar chart of teams, ordered from large to small
ggplot(df, aes(x=reorder(team, team, function(x)-length(x)))) +
  geom_bar(fill='steelblue') +
  labs(x='Team')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/category2.png">
<h3>Example 2: Boxplots by Group</h3>
Grouped boxplots are a useful way to visualize a numeric variable, grouped by a categorical variable.
For example, the following code shows how to create boxplots that show the distribution of points scored, grouped by team:
<b>library(ggplot2) 
#create data frame
df &lt;- data.frame(result = c('W', 'L', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'L'), team = c('B', 'B', 'B', 'B', 'D', 'A', 'A', 'A', 'C', 'C'), points = c(12, 28, 19, 22, 32, 45, 22, 28, 13, 19), rebounds = c(5, 7, 7, 12, 11, 4, 10, 7, 8, 8))
#create boxplots of points, grouped by team
ggplot(df, aes(x=team, y=points)) +
  geom_boxplot(fill='steelblue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/category3.png">
The x-axis displays the teams and the y-axis displays the distribution of points scored by each team.
<h3>Example 3: Mosaic Plot</h3>
A mosaic plot is a type of plot that displays the frequencies of two different categorical variables in one plot.
For example, the following code shows how to create a mosaic plot that shows the frequency of the categorical variables ‘result’ and ‘team’ in one plot:
<b>#create data frame
df &lt;- data.frame(result = c('W', 'L', 'W', 'W', 'W', 'L', 'W', 'L', 'W', 'L'), team = c('B', 'B', 'B', 'B', 'D', 'A', 'A', 'A', 'C', 'C'), points = c(12, 28, 19, 22, 32, 45, 22, 28, 13, 19), rebounds = c(5, 7, 7, 12, 11, 4, 10, 7, 8, 8))
#create table of counts
counts &lt;- table(df$result, df$team)
#create mosaic plot
mosaicplot(counts, xlab='Game Result', ylab='Team',
           main='Wins by Team', col='steelblue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/category4.png">
The x-axis displays the game result and the y-axis displays the four different teams.
<h2><span class="orange">How to Plot a Chi-Square Distribution in Excel</span></h2>
This step-by-step tutorial explains how to plot the following Chi-Square distribution in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi5.jpg">
<h3>Step 1: Define the X Values</h3>
First, let’s define a range of x-values to use for our plot.
For this example, we’ll create a range from 0 to 20:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi1.jpg"440">
<h3>Step 2: Calculate the Y Values</h3>
The y values on the plot will represent the PDF values associated with the Chi-Square distribution.
We can type the following formula into cell <b>B2</b> to calculate the PDF value of the Chi-Square distribution associated with an x value of 0 and a degrees of freedom value of 3:
<b>=CHISQ.DIST(A2, $E$1, FALSE)
</b>
We can then copy and paste this formula down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi2.jpg"468">
<h3>Step 3: Plot the Chi-Square Distribution</h3>
Next, highlight the cell range <b>A2:B22</b>, then click the <b>Insert</b> tab along the top ribbon, then click the <b>Scatter</b> option within the <b>Charts</b> group and click <b>Scatter with Smooth Lines</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/scattersmoothexcel.jpg"519">
The following chart will be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi3.jpg"658">
The x-axis shows the values of a  random variable  that follows a Chi-Square distribution with 3 degrees of freedom and the y-axis shows the corresponding PDF values of the Chi-Square distribution.
Note that if you change the value for the degrees of freedom in cell <b>E1</b>, the chart will automatically update.
For example, we could change the degrees of freedom to 7:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi4.jpg"638">
Notice that the shape of the plot automatically changes to reflect a Chi-Square distribution with 7 degrees of freedom.
<h3>Step 4: Modify the Appearance of the Plot</h3>
Feel free to add a title, axis labels, and remove the gridlines to make the plot more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/plotchi5.jpg">
<h2><span class="orange">How to Easily Plot a Chi-Square Distribution in R</span></h2>
To create a density plot for a Chi-square distribution in R, we can use the following functions:
<b>dchisq()</b> to create the probability density function
<b>curve()</b> to plot the probability density function
All we need to do to create the plot is specify the <em>degrees of freedom </em>for <b>dchisq() </b>along with the <em>to </em>and <em>from </em>points for <b>curve()</b>.
For example, the following code illustrates how to create a density plot for a chi-square distribution with 10 degrees of freedom where the x-axis of the plot ranges from 0 to 40:
<b>curve(dchisq(x, df = 10), from = 0, to = 40)
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot1.jpg">
<h2>Modifying the Density Plot</h2>
We can also modify the density plot by adding a title, changing the y-axis label, increasing the line width, and modifying the line color:
<b>curve(dchisq(x, df = 10), from = 0, to = 40,
      main = 'Chi-Square Distribution (df = 10)', #add title
      ylab = 'Density', #change y-axis label
      lwd = 2, #increase line width to 2
      col = 'steelblue') #change line color to steelblue</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot2.jpg">
<h2>Filling in the Density Plot</h2>
In addition to creating the density plot, we can fill in part of the plot using the <b>polygon() </b>function based on a starting and ending value.
The following code illustrates how to fill in the portion of the density plot for the x values ranging from 10 to 40:
<b>#create density curve
curve(dchisq(x, df = 10), from = 0, to = 40,
main = 'Chi-Square Distribution (df = 10)',
ylab = 'Density',
lwd = 2)
#create vector of x values
x_vector &lt;- seq(10, 40)
#create vector of chi-square density values
p_vector &lt;- dchisq(x_vector, df = 10)
#fill in portion of the density plot from 0 to 40
polygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),
        col = adjustcolor('red', alpha=0.3), border = NA)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot3.jpg">
The following code illustrates how to fill in the portion of the density plot for the x values ranging from 0 to 10:
<b>#create density curve
curve(dchisq(x, df = 10), from = 0, to = 40,
main = 'Chi-Square Distribution (df = 10)',
ylab = 'Density',
lwd = 2)
#create vector of x values
x_vector &lt;- seq(0, 10)
#create vector of chi-square density values
p_vector &lt;- dchisq(x_vector, df = 10)
#fill in portion of the density plot from 0 to 10
polygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),
        col = adjustcolor('red', alpha=0.3), border = NA)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot4.jpg">
The following code illustrates how to fill in the portion of the density plot for the x values lying <em>outside</em> of the middle 95% of the distribution:
<b>#create density curve
curve(dchisq(x, df = 10), from = 0, to = 40,
main = 'Chi-Square Distribution (df = 10)',
ylab = 'Density',
lwd = 2)
#find upper and lower values for middle 95% of distribution
lower95 &lt;- qchisq(.025, 10)
upper95 &lt;- qchisq(.975, 10)
#create vector of x values
x_lower95 &lt;- seq(0, lower95)
#create vector of chi-square density values
p_lower95 &lt;- dchisq(x_lower95, df = 10)
#fill in portion of the density plot from 0 to lower 95% value
polygon(c(x_lower95, rev(x_lower95)), c(p_lower95, rep(0, length(p_lower95))),
        col = adjustcolor('red', alpha=0.3), border = NA)
#create vector of x values
x_upper95 &lt;- seq(upper95, 40)
#create vector of chi-square density values
p_upper95 &lt;- dchisq(x_upper95, df = 10)
#fill in portion of the density plot for upper 95% value to end of plot
polygon(c(x_upper95, rev(x_upper95)), c(p_upper95, rep(0, length(p_upper95))),
        col = adjustcolor('red', alpha=0.3), border = NA)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot5.jpg">Lastly, the following code illustrates how to fill in the portion of the density plot for the x values lying <i>inside </i>of the middle 95% of the distribution:
<b>#create density curve
curve(dchisq(x, df = 10), from = 0, to = 40,
main = 'Chi-Square Distribution (df = 10)',
ylab = 'Density',
lwd = 2)
#find upper and lower values for middle 95% of distribution
lower95 &lt;- qchisq(.025, 10)
upper95 &lt;- qchisq(.975, 10)
#create vector of x values
x_vector &lt;- seq(lower95, upper95)
#create vector of chi-square density values
p_vector &lt;- dchisq(x_vector, df = 10)
#fill in density plot
polygon(c(x_vector, rev(x_vector)), c(p_vector, rep(0, length(p_vector))),
        col = adjustcolor('red', alpha=0.3), border = NA)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/chiPlot6.jpg">
<h2><span class="orange">How to Plot a Chi-Square Distribution in Python</span></h2>
To plot a Chi-Square distribution in Python, you can use the following syntax:
<b>#x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 20, 0.001)
#plot Chi-square distribution with 4 degrees of freedom
plt.plot(x, chi2.pdf(x, df=4))
</b>
The <b>x</b> array defines the range for the x-axis and the <b>plt.plot()</b> produces the curve for the Chi-square distribution with the specified degrees of freedom.
The following examples show how to use these functions in practice.
<h3>Example 1: Plot a Single Chi-Square Distribution</h3>
The following code shows how to plot a single Chi-square distribution curve with 4 degrees of freedom
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2
#x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 20, 0.001)
#plot Chi-square distribution with 4 degrees of freedom
plt.plot(x, chi2.pdf(x, df=4))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/chisquarePython1.png">
You can also modify the color and the width of the line in the graph:
<b>plt.plot(x, chi2.pdf(x, df=4), color='red', linewidth=3)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/chisquarePython2.png">
<h3>Example 2: Plot Multiple Chi-Square Distributions</h3>
The following code shows how to plot multiple Chi-square distribution curves with different degrees of freedom:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2
#x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 20, 0.001)
#define multiple Chi-square distributions
plt.plot(x, chi2.pdf(x, df=4), label='df: 4')
plt.plot(x, chi2.pdf(x, df=8), label='df: 8') 
plt.plot(x, chi2.pdf(x, df=12), label='df: 12') 
#add legend to plot
plt.legend()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/chisquarePython3.png">
Feel free to modify the colors of the lines and add a title and axes labels to make the chart complete:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import chi2
#x-axis ranges from 0 to 20 with .001 steps
x = np.arange(0, 20, 0.001)
#define multiple Chi-square distributions
plt.plot(x, chi2.pdf(x, df=4), label='df: 4', color='gold')
plt.plot(x, chi2.pdf(x, df=8), label='df: 8', color='red')
plt.plot(x, chi2.pdf(x, df=12), label='df: 12', color='pink') 
#add legend to plot
plt.legend(title='Parameters')
#add axes labels and a title
plt.ylabel('Density')
plt.xlabel('x')
plt.title('Chi-Square Distributions', fontsize=14)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/chisquarePython4-2.png">
Refer to the  matplotlib documentation  for an in-depth explanation of the <b>plt.plot()</b> function.
<h2><span class="orange">How to Plot Confidence Intervals in Excel (With Examples)</span></h2>
A <b>confidence interval</b> represents a range of values that is likely to contain some  population parameter  with a certain level of confidence.
This tutorial explains how to plot confidence intervals on bar charts in Excel.
<h3>Example 1: Plot Confidence Intervals on Bar Graph</h3>
Suppose we have the following data in Excel that shows the mean of four different categories along with the corresponding margin of error for the 95% confidence intervals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI1.png">
To create a bar chart to visualize the category means, highlight cells in the range <b>A1:B5</b> and then click the <b>Insert</b> tab along the top ribbon. Then click <b>Insert Column or Bar Chart</b> within the Charts group.
This will produce the following bar chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI2.png">
To add confidence interval bands, click the plus sign (+) in the top right corner of the bar chart, then click <b>Error Bars</b>, then <b>More Options</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI3.png">
In the window that appears to the right, click the <b>Custom</b> button at the bottom. In the new window that appears, choose <b>=Sheet1!$C$2:$C$5</b> for both the positive error value and negative error value. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI4.png">
This will produce the following confidence interval bands in the bar chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI5.png">
Feel free to change the color of the bars as well to make the confidence interval bands easier to see:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotCI6.png">
The top of the bar represents the mean value for each category and the error bars show the range of the confidence interval for each mean.
For example:
The mean value for category A is 12 and the 95% confidence interval for this mean ranges from 10 to 14.
The mean value for category B is 15 and the 95% confidence interval for this mean ranges from 12 to 18.
The mean value for category C is 18 and the 95% confidence interval for this mean ranges from 13 to 23.
The mean value for category D is 13 and the 95% confidence interval for this mean ranges from 10 to 16.
<h2><span class="orange">How to Plot a Confidence Interval in Python</span></h2>
A <b>confidence interval </b>is a range of values that is likely to contain a population parameter with a certain level of confidence.
This tutorial explains how to plot a confidence interval for a dataset in Python using the  seaborn visualization library .
<h3>Plotting Confidence Intervals Using lineplot()</h3>
The first way to plot a confidence interval is by using the  lineplot() function , which connects all of the data points in a dataset with a line and displays a confidence band around each point:
<b>import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#create some random data
np.random.seed(0)
x = np.random.randint(1, 10, 30)
y = x+np.random.normal(0, 1, 30)
#create lineplot
ax = sns.lineplot(x, y)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/plotCIpython1.png">
By default, the lineplot() function uses a 95% confidence interval but can specify the confidence level to use with the <b>ci </b>command.
The smaller the confidence level, the more narrow the confidence interval will be around the line. For example, here’s what an 80% confidence interval looks like for the exact same dataset:
<b>#create lineplot
ax = sns.lineplot(x, y, ci=80)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/plotCIpython2.png">
<h3>Plotting Confidence Intervals Using regplot()</h3>
You can also plot confidence intervals by using the  regplot() function , which displays a scatterplot of a dataset with confidence bands around the estimated regression line:
<b>import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
#create some random data
np.random.seed(0)
x = np.random.randint(1, 10, 30)
y = x+np.random.normal(0, 1, 30)
#create regplot
ax = sns.regplot(x, y)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/plotCIpython3.png">
Similar to lineplot(), the regplot() function uses a 95% confidence interval by default but can specify the confidence level to use with the <b>ci </b>command.
Again, the smaller the confidence level the more narrow the confidence interval will be around the regression line. For example, here’s what an 80% confidence interval looks like for the exact same dataset:
<b>#create regplot
ax = sns.regplot(x, y, ci=80)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/plotCIpython4.png">
<h2><span class="orange">How to Plot a Confidence Interval in R</span></h2>
A  confidence interval  is a range of values that is likely to contain a population parameter with a certain level of confidence.
This tutorial explains how to plot a confidence interval for a dataset in R.
<h3>Example: Plotting a Confidence Interval in R</h3>
Suppose we have the following dataset in R with 100 rows and 2 columns:
<b>#make this example reproducible
set.seed(0)
#create dataset
x &lt;- rnorm(100)
y &lt;- x*2 + rnorm(100)
df &lt;- data.frame(x = x, y = y)
#view first six rows of dataset
head(df)
           x          y
1  1.2629543  3.3077678
2 -0.3262334 -1.4292433
3  1.3297993  2.0436086
4  1.2724293  2.5914389
5  0.4146414 -0.3011029
6 -1.5399500 -2.5031813
</b>
To create a plot of the relationship between x and y, we can first fit a linear regression model:
<b>model &lt;- lm(y ~ x, data = df)
</b>
Next, we can create a plot of the estimated linear regression line using the  abline()  function and the lines() function to create the actual confidence bands:
<b>#get predicted y values using regression equation
newx &lt;- seq(min(df$x), max(df$x), length.out=100)
preds &lt;- predict(model, newdata = data.frame(x=newx), interval = 'confidence')
#create plot of x vs. y, but don't display individual points (type='n') 
plot(y ~ x, data = df, type = 'n')
#add fitted regression line
abline(model)
#add dashed lines for confidence bands
lines(newx, preds[ ,3], lty = 'dashed', col = 'blue')
lines(newx, preds[ ,2], lty = 'dashed', col = 'blue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plotConfIntR1.png">
The black line displays the fitted linear regression line while the two dashed blue lines display the confidence intervals.
If you’d like, you can also fill in the area between the confidence interval lines and the estimated linear regression line using the following code:
<b>#create plot of x vs. y
plot(y ~ x, data = df, type = 'n')
#fill in area between regression line and confidence interval
polygon(c(rev(newx), newx), c(rev(preds[ ,3]), preds[ ,2]), col = 'grey', border = NA)
#add fitted regression line
abline(model)
#add dashed lines for confidence bands
lines(newx, preds[ ,3], lty = 'dashed', col = 'blue')
lines(newx, preds[ ,2], lty = 'dashed', col = 'blue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plotConfIntR2.png">
Here’s the complete code from start to finish:
<b>#make this example reproducible
set.seed(0)
#create dataset
x &lt;- rnorm(100)
y &lt;- x*2 + rnorm(100)
df &lt;- data.frame(x = x, y = y)
#fit linear regression model
model &lt;- lm(y ~ x, data = df) 
#get predicted y values using regression equation
newx &lt;- seq(min(df$x), max(df$x), length.out=100)
preds &lt;- predict(model, newdata = data.frame(x=newx), interval = 'confidence')
#create plot of x vs. y
plot(y ~ x, data = df, type = 'n')
#fill in area between regression line and confidence interval
polygon(c(rev(newx), newx), c(rev(preds[ ,3]), preds[ ,2]), col = 'grey', border = NA)
#add fitted regression line
abline(model)
#add dashed lines for confidence bands
lines(newx, preds[ ,3], lty = 'dashed', col = 'blue')
lines(newx, preds[ ,2], lty = 'dashed', col = 'blue')</b>
<h2><span class="orange">How to Plot a Decision Tree in R (With Example)</span></h2>
In  machine learning , a <b>decision tree</b> is a type of model that uses a set of predictor variables to build a decision tree that predicts the value of a response variable.
The easiest way to plot a decision tree in R is to use the <b>prp()</b> function from the <b>rpart.plot</b> package.
The following example shows how to use this function in practice.
<h2>Example: Plotting a Decision Tree in R</h2>
For this example, we’ll use the <b>Hitters</b> dataset from the <b>ISLR</b> package, which contains various information about 263 professional baseball players.
We will use this dataset to build a regression tree that uses home runs and years played to predict the salary of a given player.
The following code shows how to fit this regression tree and how to use the <b>prp()</b> function to plot the tree:
<b>library(ISLR)
library(rpart)
library(rpart.plot)
#build the initial decision tree
tree &lt;- rpart(Salary ~ Years + HmRun, data=Hitters, control=rpart.control(cp=.0001))
</b>
<b>#identify best cp value to use
best &lt;- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
#produce a pruned tree based on the best cp value
pruned_tree &lt;- prune(tree, cp=best)
#plot the pruned tree
prp(pruned_tree)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/decision1.jpg"367">
Note that we can also customize the appearance of the decision tree by using the <b>faclen</b>, <b>extra</b>, <b>roundint</b>, and <b>digits</b> arguments within the <b>prp</b>() function:
<b>#plot decision tree using custom arguments
prp(pruned_tree,
    faclen=0, #use full names for factor labels
    extra=1, #display number of observations for each terminal node
    roundint=F, #don't round to integers in output
    digits=5) #display 5 decimal places in output</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/decision2.jpg">
We can see that the tree has six terminal nodes.
Each terminal node shows the predicted salary of players in that node along with the number of observations from the original dataset that belong to that note.
For example, we can see that in the original dataset there were 90 players with less than 4.5 years of experience and their average salary was <b>$225.83k</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree4.png">
We can also use the tree to predict a given player’s salary based on their years of experience and average home runs.
For example, a player who has 7 years of experience and 4 average home runs has a predicted salary of <b>$502.81k</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree5.png">
This is one advantage of using a decision tree: We can easily visualize and interpret the results.
<h2>Additional Resources</h2>
The following tutorials provide additional information about decision trees:
 An Introduction to Classification and Regression Trees 
 Decision Tree vs. Random Forests: What’s the Difference? 
 How to Fit Classification and Regression Trees in R 
<h2><span class="orange">How to Plot an Equation in Excel</span></h2>
Often you may be interested in plotting an equation or a function in Excel. Fortunately this is easy to do with built-in Excel formulas.
This tutorial provides several examples of how to plot equations/functions in Excel.
<h3>Example 1: Plot a Linear Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 2x + 5</b>
The following image shows how to create the y-values for this linear equation in Excel, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel1.png">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Within the <b>Charts </b>group, click on the plot option called <b>Scatter</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel3.png">
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel2.png">
We can see that the plot follows a straight line since the equation that we used was linear in nature.
<h3>Example 2: Plot a Quadratic Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 3x<sup>2</sup></b>
The following image shows how to create the y-values for this equation in Excel, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel3.png">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Within the <b>Charts </b>group, click on the plot option called <b>Scatter</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel3.png">
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel4.png">
We can see that the plot follows a curved line since the equation that we used was quadratic.
<h3>Example 3: Plot a Reciprocal Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 1/x</b>
The following image shows how to create the y-values for this equation in Excel, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel5.png">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Within the <b>Charts </b>group, click on the plot option called <b>Scatter</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel3.png">
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel6.png">
We can see that the plot follows a curved line downwards since this represents the equation y = 1/x.
<h3>Example 4: Plot a Sine Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = sin(x)</b>
The following image shows how to create the y-values for this equation in Excel, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel7.png">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Within the <b>Charts </b>group, click on the plot option called <b>Scatter with Smooth Lines and Markers</b>.
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/plot_function_excel8.png">
<h3>Conclusion</h3>
You can use a similar technique to plot any function or equation in Excel. Simply choose a range of x-values to use in one column, then use an equation in a separate column to define the y-values based on the x-values.
<h2><span class="orange">How to Plot an Equation in Google Sheets</span></h2>
Often you may want to plot an equation or a function in Google Sheets.
Fortunately this is easy to do with built-in formulas.
This tutorial provides several examples of how to plot equations/functions in Google Sheets.
<h3>Example 1: Plot a Linear Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 2x + 5</b>
The following image shows how to create the y-values for this linear equation in Google Sheets, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets1.jpg"475">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets2.jpg"486">
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets3.jpg"566">
We can see that the plot follows a straight line since the equation that we used was linear.
<h3>Example 2: Plot a Quadratic Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 3x<sup>2</sup></b>
The following image shows how to create the y-values for this equation in Google Sheets, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets4-1.jpg"481">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Then click <b>Chart</b>.
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets5.jpg"552">
We can see that the plot follows a curved line since the equation that we used was quadratic.
<h3>Example 3: Plot a Reciprocal Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = 1/x</b>
The following image shows how to create the y-values for this equation in Google Sheets, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets6.jpg"461">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Then click <b>Chart</b>.
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets7.jpg"565">
We can see that the plot follows a curved line downwards since this represents the equation y = 1/x.
<h3>Example 4: Plot a Sine Equation</h3>
Suppose you’d like to plot the following equation:
<b>y = sin(x)</b>
The following image shows how to create the y-values for this equation in Google Sheets, using the range of 1 to 10 for the x-values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets8.jpg"464">
Next, highlight the values in the range <b>A2:B11</b>. Then click on the <b>Insert </b>tab. Then click <b>Chart</b>.
The following plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/plotsheets9.jpg"573">
You can use a similar technique to plot any function or equation in Google Sheets.
Simply choose a range of x-values to use in one column, then use an equation in a separate column to define the y-values based on the x-values.
<h2><span class="orange">How to Plot an Equation in R (With Examples)</span></h2>
You can use the following basic syntax to plot an equation or function in R:
<b>Method 1: Use Base R</b>
<b>curve(2*x^2+5, from=1, to=50, , xlab="x", ylab="y")
</b>
<b>Method 2: Use ggplot2</b>
<b>library(ggplot2)
#define equation
my_equation &lt;- function(x){2*x^2+5}
#plot equation
ggplot(data.frame(x=c(1, 50)), aes(x=x)) + 
  stat_function(fun=my_equation)
</b>
Both of these particular examples plot the equation <b>y = 2x<sup>2</sup> + 5</b>.
The following examples show how to use each method in practice.
<h2>Example 1: Plot Equation in Base R</h2>
Suppose you’d like to plot the following equation:
<b>y = 2x<sup>2</sup> + 5</b>
You can use the following syntax in base R to do so:
<b>curve(2*x^2+5, from=1, to=50, , xlab="x", ylab="y")</b>
This produces the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/ploteq1.jpg">
If you’d like to plot points instead, simply specify type=”p” in the <b>curve()</b> function:
<b>curve(2*x^2+5, from=1, to=50, , xlab="x", ylab="y", type="p")</b>
This produces the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/ploteq2.jpg"489">
<h2>Example 2: Plot Equation in ggplot2</h2>
Suppose you’d like to plot the following equation:
<b>y = 2x<sup>2</sup> + 5</b>
You can use the following syntax in  ggplot2  to do so:
<b>library(ggplot2)
#define equation
my_equation &lt;- function(x){2*x^2+5}
#plot equation
ggplot(data.frame(x=c(1, 50)), aes(x=x)) + 
  stat_function(fun=my_equation)</b>
This produces the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/ploteq3.jpg"481">
Notice that this plot matches the one we created in the previous example in base R.
<b>Note</b>: To plot a different equation, simply change the values defined for the <b>my_equation</b> variable.
<h2><span class="orange">How to Plot an Exponential Distribution in R</span></h2>
The  exponential distribution  is a probability distribution that is used to model the time we must wait until a certain event occurs.
If a  random variable  <em>X</em> follows an exponential distribution, then the <b>probability density function</b> of <em>X</em> can be written as:
<b><em>f</em>(x; λ) = λe<sup>-λx</sup></b>
where:
<b>λ:</b> the rate parameter
<b>e:</b> A constant roughly equal to 2.718
The <b>cumulative distribution function</b> of <em>X</em> can be written as:
<b><em>F</em>(x; λ) = 1 – e<sup>-λx</sup></b>
This tutorial explains how to plot a PDF and CDF for the exponential distribution in R.
<h3>Plotting a Probability Density Function</h3>
The following code shows how to plot a PDF of an exponential distribution with rate parameter λ = 0.5:
<b>curve(dexp(x, rate = .5), from=0, to=10, col='blue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/exp3.png">
The following code shows how to plot multiple PDF’s of an exponential distribution with various rate parameters:
<b>#plot PDF curves
curve(dexp(x, rate = .5), from=0, to=10, col='blue')
curve(dexp(x, rate = 1), from=0, to=10, col='red', add=TRUE)
curve(dexp(x, rate = 1.5), from=0, to=10, col='purple', add=TRUE)
#add legend
legend(7, .5, legend=c("rate=.5", "rate=1", "rate=1.5"),
       col=c("blue", "red", "purple"), lty=1, cex=1.2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/exp4.png">
<h3>Plotting a Cumulative Distribution Function</h3>
The following code shows how to plot a CDF of an exponential distribution with rate parameter λ = 0.5:
<b>curve(pexp(x, rate = .5), from=0, to=10, col='blue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/exp5.png">
The following code shows how to plot multiple CDF’s of an exponential distribution with various rate parameters:
<b>#plot CDF curves
curve(pexp(x, rate = .5), from=0, to=10, col='blue')
curve(pexp(x, rate = 1), from=0, to=10, col='red', add=TRUE)
curve(pexp(x, rate = 1.5), from=0, to=10, col='purple', add=TRUE)
#add legend
legend(7, .9, legend=c("rate=.5", "rate=1", "rate=1.5"),
       col=c("blue", "red", "purple"), lty=1, cex=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/exp6.png">
<h2><span class="orange">How to Plot lm() Results in R</span></h2>
You can use the following methods to plot the results of the <b>lm()</b> function in R:
<b>Method 1: Plot lm() Results in Base R</b>
<b>#create scatterplot
plot(y ~ x, data=data)
#add fitted regression line to scatterplot
abline(fit)
</b>
<b>Method 2: Plot lm() Results in ggplot2</b>
<b>library(ggplot2)
#create scatterplot with fitted regression line
ggplot(data, aes(x = x, y = y)) + 
  geom_point() +
  stat_smooth(method = "lm")</b>
The following examples shows how to use each method in practice with the built-in  mtcars dataset  in R.
<h3>Example 1: Plot lm() Results in Base R</h3>
The following code shows how to plot the results of the<b> lm()</b> function in base R:
<b>#fit regression model
fit &lt;- lm(mpg ~ wt, data=mtcars)
#create scatterplot
plot(mpg ~ wt, data=mtcars)
#add fitted regression line to scatterplot
abline(fit)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/plotlm1.jpg"451">
The points in the plot represent the raw data values and the straight diagonal line represents the fitted regression line.
<h3>Example 2: Plot lm() Results in ggplot2</h3>
The following code shows how to plot the results of the<b> lm()</b> function using the  ggplot2  data visualization package:
<b>library(ggplot2)
#fit regression model
fit &lt;- lm(mpg ~ wt, data=mtcars)
#create scatterplot with fitted regression line
ggplot(mtcars, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "lm")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/plotlm2.jpg"453">
The blue line represents the fitted regression line and the grey bands represent the 95% confidence interval limits.
To remove the confidence interval limits, simply use <b>se=FALSE</b> in the <b>stat_smooth()</b> argument:
<b>library(ggplot2) 
#fit regression model
fit &lt;- lm(mpg ~ wt, data=mtcars)
#create scatterplot with fitted regression line
ggplot(mtcars, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "lm", se=FALSE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/plotlm3.jpg">
You can also add the fitted regression equation inside the chart by using the <b>stat_regline_equation()</b> function from the <b>ggpubr</b> package:
<b>library(ggplot2)
library(ggpubr)
#fit regression model
fit &lt;- lm(mpg ~ wt, data=mtcars)
#create scatterplot with fitted regression line
ggplot(mtcars, aes(x = x, y = y)) +
  geom_point() +
  stat_smooth(method = "lm", se=FALSE) +
  stat_regline_equation(label.x.npc = "center")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/plotlm4.jpg"450">
<h2><span class="orange">How to Plot a Log-Normal Distribution in Excel</span></h2>
This step-by-step tutorial explains how to plot the following log-normal distribution in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormalExcel4.jpg"538">
<h3>Step 1: Define the X Values</h3>
First, let’s define a range of x-values to use for our plot.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormalExcel1.jpg"427">
<h3>Step 2: Calculate the Y Values</h3>
The y values on the plot will represent the PDF values associated with the log-normal distribution.
We can type the following formula into cell <b>B2</b> to calculate the PDF value of the log-normal distribution associated with an x value of 0.01, a mean value of 1, and a standard deviation of 1:
<b>=LOGNORM.DIST(A2, $E$1, $E$2, FALSE)
</b>
We can then copy and paste this formula down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormalExcel2.jpg"558">
<h3>Step 3: Plot the Log-Normal Distribution</h3>
Next, highlight the cell range <b>A2:B22</b>, then click the <b>Insert</b> tab along the top ribbon, then click the <b>Scatter</b> option within the <b>Charts</b> group and click <b>Scatter with Smooth Lines</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/scattersmoothexcel.jpg"519">
The following chart will be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormalExcel3.jpg"623">
The x-axis shows the values of a  random variable  that follows a log-normal distribution with a mean value of 1 and a standard deviation of 1 and the y-axis shows the corresponding PDF values of the log-normal distribution.
Note that if you change the values for the mean or standard deviation in cells <b>E1</b> and <b>E2</b>, respectively, the plot will automatically update.
<h3>Step 4: Modify the Appearance of the Plot</h3>
Feel free to add a title, axis labels, and remove the gridlines to make the plot more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormalExcel4.jpg"538">
<h2><span class="orange">How to Plot a Log Normal Distribution in R</span></h2>
To plot the probability density function for a log normal distribution in R, we can use the following functions:
<b>dlnorm(x, meanlog = 0, sdlog = 1)</b> to create the probability density function.
<b>curve(function, from = NULL, to = NULL)</b> to plot the probability density function.
For example, the following code illustrates how to plot a probability density function for a log normal distribution with mean = 0 and standard deviation = 1 (on a log scale) where the x-axis of the plot ranges from 0 to 10:
<b>curve(dlnorm(x, meanlog=0, sdlog=1), from=0, to=10)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logNormR1.png">
By default, meanlog = 0 and sdlog =1 which means we can produce the exact same plot without specifying these parameters in the <b>dlnorm() </b>function:
<b>curve(dlnorm(x), from=0, to=10)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logNormR1.png">
We can add a title, change the y-axis label, increase the line width, and even change the line color to make the plot more aesthetically pleasing:
<b>curve(dlnorm(x), from=0, to=10, 
    main = 'Log Normal Distribution', #add title
    ylab = 'Density', #change y-axis label
    lwd = 2, #increase line width to 2
    col = 'steelblue') #change line color to steelblue 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logNormR2.png">
We can also add more than one curve to the graph to compare log normal distributions with different standard deviations. For example, the following code creates log normal distribution plots with sdlog = .3, sdlog = .5, and sdlog = 1:
<b>curve(dlnorm(x, meanlog=0, sdlog=.3), from=0, to=10, col='blue')
curve(dlnorm(x, meanlog=0, sdlog=.5), from=0, to=10, col='red', add=TRUE)
curve(dlnorm(x, meanlog=0, sdlog=1), from=0, to=10, col='purple', add=TRUE)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logNormR3.png">
We can add a legend to the plot by using the <b>legend() </b>function, which takes on the following syntax:
<b>legend(x, y=NULL, legend, fill, col, bg, lty, cex)</b>
where:
<b>x, y:</b> the x and y coordinates used to position the legend
<b>legend:</b> the text to go in the legend
<b>fill:</b> fill color inside the legend
<b>col: </b>the list of colors to be used for the lines inside the legend
<b>bg: </b>the background color for the legend
<b>lty: </b>line style
<b>cex: </b>text size in the legend
In our example we will use the following syntax to create a legend:
<b>#create density plots
curve(dlnorm(x, meanlog=0, sdlog=.3), from=0, to=10, col='blue')
curve(dlnorm(x, meanlog=0, sdlog=.5), from=0, to=10, col='red', add=TRUE)
curve(dlnorm(x, meanlog=0, sdlog=1), from=0, to=10, col='purple', add=TRUE)
#add legend
legend(6, 1.2, legend=c("sdlog=.3", "sdlog=.5", "sdlog=1"),
       col=c("blue", "red", "purple"), lty=1, cex=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logNormR4.png">
<h2><span class="orange">How to Plot a Logistic Regression Curve in Python</span></h2>
You can use the  regplot()  function from the seaborn data visualization library to plot a logistic regression curve in Python:
<b>import seaborn as sns
sns.regplot(x=x, y=y, data=df, logistic=True, ci=None)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Plotting a Logistic Regression Curve in Python</h3>
For this example, we’ll use the <b>Default</b> dataset from the  Introduction to Statistical Learning book . We can use the following code to load and view a summary of the dataset:
<b>#import dataset from CSV file on Github
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/default.csv"
data = pd.read_csv(url)
#view first six rows of dataset
data[0:6]
        defaultstudentbalance        income
000729.52649544361.625074
101817.18040712106.134700
2001073.54916431767.138947
300529.25060535704.493935
400785.65588338463.495879
501919.5885307491.558572  
</b>
This dataset contains the following information about 10,000 individuals:
<b>default:</b> Indicates whether or not an individual defaulted.
<b>student:</b> Indicates whether or not an individual is a student.
<b>balance:</b> Average balance carried by an individual.
<b>income:</b> Income of the individual.
Suppose we would like to build a logistic regression model that uses “balance” to predict the probability that a given individual defaults.
We can use the following code to plot a logistic regression curve:
<b>#define the predictor variable and the response variable
x = data['balance']
y = data['default']
#plot logistic regression curve
sns.regplot(x=x, y=y, data=data, logistic=True, ci=None)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/regplot1.png">
The x-axis shows the values of the predictor variable “balance” and the y-axis displays the predicted probability of defaulting.
We can clearly see that higher values of balance are associated with higher probabilities that an individual defaults.
Note that you can also use <b>scatter_kws</b> and <b>line_kws</b> to modify the colors of the points and the curve in the plot:
<b>#define the predictor variable and the response variable
x = data['balance']
y = data['default']
#plot logistic regression curve with black points and red line
sns.regplot(x=x, y=y, data=data, logistic=True, ci=None),
            scatter_kws={'color': 'black'}, line_kws={'color': 'red'})</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/regplot2.png">
Feel free to choose whichever colors you’d like in the plot.
<h2><span class="orange">How to Plot a Logistic Regression Curve in R</span></h2>
Often you may be interested in plotting the curve of a fitted  logistic regression model  in R.
Fortunately this is fairly easy to do and this tutorial explains how to do so in both base R and ggplot2.
<h3>Example: Plot a Logistic Regression Curve in Base R</h3>
The following code shows how to fit a logistic regression model using variables from the built-in mtcars dataset in R and then how to plot the logistic regression curve:
<b>#fit logistic regression model
model &lt;- glm(vs ~ hp, data=mtcars, family=binomial)
#define new data frame that contains predictor variable
newdata &lt;- data.frame(hp=seq(min(mtcars$hp), max(mtcars$hp),len=500))
#use fitted model to predict values of vs
newdata$vs = predict(model, newdata, type="response")
#plot logistic regression curve
plot(vs ~ hp, data=mtcars, col="steelblue")
lines(vs ~ hp, newdata, lwd=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/logisticCurve1.png">
The x-axis displays the values of the predictor variable <b>hp</b> and the y-axis displays the predicted probability of the response variable <b>am</b>.
We can clearly see that higher values of the predictor variable <b>hp</b> are associated with lower probabilities of the response variable <b>vs</b> being equal to 1.
<h3>Example: Plot a Logistic Regression Curve in ggplot2</h3>
The following code shows how to fit the same logistic regression model and how to plot the logistic regression curve using the data visualization library  ggplot2 :
<b>library(ggplot2)
#plot logistic regression curve
ggplot(mtcars, aes(x=hp, y=vs)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/logisticCurve2.png">
Note that this is the exact same curve produced in the previous example using base R.
Feel free to modify the style of the curve as well. For example, we could turn the curve into a red dashed line:
<b>library(ggplot2)
#plot logistic regression curve
ggplot(mtcars, aes(x=hp, y=vs)) + 
  geom_point(alpha=.5) +
  stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial),
              col="red", lty=2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/logisticCurve3.png">
<h2><span class="orange">How to Plot Multiple Columns in R (With Examples)</span></h2>
Often you may want to plot multiple columns from a data frame in R. Fortunately this is easy to do using the visualization library  ggplot2 .
This tutorial shows how to use ggplot2 to plot multiple columns of a data frame on the same graph and on different graphs.
<h3>Example 1: Plot Multiple Columns on the Same Graph</h3>
The following code shows how to generate a data frame, then “melt” the data frame into a long format, then use ggplot2 to create a line plot for each column in the data frame:
<b>#load necessary libraries
library(ggplot2)
library(reshape2)
#create data frame 
df &lt;- data.frame(index=c(1, 2, 3, 4, 5, 6), var1=c(4, 4, 5, 4, 3, 2), var2=c(1, 2, 4, 4, 6, 9), var3=c(9, 9, 9, 5, 5, 3))
#melt data frame into long format
df &lt;- melt(df ,  id.vars = 'index', variable.name = 'series')
#create line plot for each column in data frame
ggplot(df, aes(index, value)) +
  geom_line(aes(colour = series))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/multipleLinesR.png">
<h3>Example 2: Plot Multiple Columns on Different Graphs</h3>
The following code shows how to generate a data frame, then “melt” the data frame into a long format, then use ggplot2 to create a line plot for each column in the data frame, splitting up each line into its own plot:
<b>#load necessary libraries
library(ggplot2)
library(reshape2)
#create data frame 
df &lt;- data.frame(index=c(1, 2, 3, 4, 5, 6), var1=c(4, 4, 5, 4, 3, 2), var2=c(1, 2, 4, 4, 6, 9), var3=c(9, 9, 9, 5, 5, 3))
#melt data frame into long format
df &lt;- melt(df ,  id.vars = 'index', variable.name = 'series')
#create line plot for each column in data frame
ggplot(df, aes(index, value)) +
  geom_line() +
  facet_grid(series ~ .)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/multipleLinesR2.png">
<h2><span class="orange">How to Plot Multiple Linear Regression Results in R</span></h2>
When we perform  simple linear regression  in R, it’s easy to visualize the fitted regression line because we’re only working with a single predictor variable and a single  response variable .
For example, the following code shows how to fit a simple linear regression model to a dataset and plot the results:
<b>#create dataset
data &lt;- data.frame(x = c(1, 1, 2, 4, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11),   y = c(13, 14, 17, 23, 24, 25, 25, 24, 28, 32, 33, 35, 40, 41))
#fit simple linear regression model
model &lt;- lm(y ~ x, data = data)
#create scatterplot of data
plot(data$x, data$y)
#add fitted regression line
abline(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/plotMultipleRegression1.png">
However, when we perform  multiple linear regression  it becomes difficult to visualize the results because there are several predictor variables and we can’t simply plot a regression line on a 2-D plot.
Instead, we can use <b>added variable plots</b> (sometimes called “partial regression plots”), which are individual plots that display the relationship between the response variable and one predictor variable, <em>while controlling for the presence of other predictor variables in the model</em>.
The following example shows how to perform multiple linear regression in R and visualize the results using added variable plots.
<h3>Example: Plotting Multiple Linear Regression Results in R</h3>
Suppose we fit the following multiple linear regression model to a dataset in R using the built-in <b>mtcars</b> dataset:
<b>#fit multiple linear regression model
model &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)
#view results of model
summary(model)
Call:
lm(formula = mpg ~ disp + hp + drat, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-5.1225 -1.8454 -0.4456  1.1342  6.4958 
Coefficients:
             Estimate Std. Error t value Pr(>|t|)   
(Intercept) 19.344293   6.370882   3.036  0.00513 **
disp        -0.019232   0.009371  -2.052  0.04960 * 
hp          -0.031229   0.013345  -2.340  0.02663 * 
drat         2.714975   1.487366   1.825  0.07863 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 3.008 on 28 degrees of freedom
Multiple R-squared:  0.775,Adjusted R-squared:  0.7509 
F-statistic: 32.15 on 3 and 28 DF,  p-value: 3.28e-09
</b>
From the results we can see that the p-values for each of the coefficients is less than 0.1. For the sake of simplicity, we’ll assume that each of the predictor variables are significant and should be included in the model.
To produce added variable plots, we can use the <b>avPlots()</b> function from the <b>car</b> package:
<b>#load car package
library(car)
#produce added variable plots
avPlots(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/addedVarPlots1.png">
Here is how to interpret each plot:
The x-axis displays a single predictor variable and the y-axis displays the response variable.
The blue line shows the association between the predictor variable and the response variable, <em>while holding the value of all other predictor variables constant</em>.
The points that are labelled in each plot represent the 2  observations  with the largest  residuals  and the 2 observations with the largest partial leverage.
Note that the angle of the line in each plot matches the sign of the coefficient from the estimated regression equation.
For example, here are the estimated coefficients for each predictor variable from the model:
<b>disp: </b>-0.019232
<b>hp: </b>-0.031229
<b>drat: </b>2.714975
Notice that the angle of the line is positive in the added variable plot for <em>drat</em> while negative for both <em>disp</em> and <em>hp</em>, which matches the signs of their estimated coefficients:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/addedVarPlots2.png">
Although we can’t plot a single fitted regression line on a 2-D plot since we have multiple predictor variables, these added variable plots allow us to observe the relationship between each individual predictor variable and the response variable while holding other predictor variables constant.
<h2><span class="orange">How to Plot Multiple Lines in Excel (With Examples)</span></h2>
You can easily plot multiple lines on the same graph in Excel by simply highlighting several rows (or columns) and creating a line plot.
The following examples show how to plot multiple lines on one graph in Excel, using different formats.
<h3>Example 1: Plot Multiple Lines with Data Arranged by Columns</h3>
Suppose we have the following dataset that displays the total sales for three different products during different years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines1.png">
We can use the following steps to plot each of the product sales as a line on the same graph:
Highlight the cells in the range <b>B1:D8</b>.
Click the <b>Insert</b> Tab along the top ribbon.
In the <b>Charts</b> group, click the first chart option in the section titled <b>Insert Line or Area Chart</b>.
The following chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines2.png">
<h3>Example 2: Plot Multiple Lines with Data Arranged by Rows</h3>
Suppose we have the following dataset that displays the total sales for three different products during different years, arranged by rows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines3.png">
We can use the following steps to plot each of the product sales as a line on the same graph:
Highlight the cells in the range <b>A1:H4</b>.
Click the <b>Insert</b> Tab along the top ribbon.
In the <b>Charts</b> group, click the first chart option in the section titled <b>Insert Line or Area Chart</b>.
The following chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines4.png">
<h3>Example 3: Plot Multiple Lines with Flipped Columns & Rows</h3>
Suppose we have the following dataset that displays the total sales for four different products during two different years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines5.png">
We can use the following steps to plot each of the product sales as a line on the same graph:
Highlight the cells in the range <b>A1:C5</b>.
Click the <b>Insert</b> Tab along the top ribbon.
In the <b>Charts</b> group, click the first chart option in the section titled <b>Insert Line or Area Chart</b>.
The following chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines6.png">
Notice that the products are displayed along the x-axis instead of the years.
To switch the rows and columns, we can click anywhere on the chart and then click the <b>Switch Row/Column</b> button under the <b>Chart Design</b> tab:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines7.png">
Once we click this button, the rows and columns will be switched and the years will be displayed along the x-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/lines8.png">
<h2><span class="orange">How to Plot Multiple Lines in Matplotlib</span></h2>
You can display  multiple lines in a single Matplotlib plot by using the following syntax:
<b>import matplotlib.pyplot as plt
plt.plot(df['column1'])
plt.plot(df['column2'])
plt.plot(df['column3'])
...
plt.show()
</b>
This tutorial provides several examples of how to plot multiple lines in one chart using the following pandas DataFrame:
<b>import numpy as np 
import pandas as pd
#make this example reproducible
np.random.seed(0)
#create dataset
period = np.arange(1, 101, 1)
leads = np.random.uniform(1, 50, 100)
prospects = np.random.uniform(40, 80, 100)
sales = 60 + 2*period + np.random.normal(loc=0, scale=.5*period, size=100)
df = pd.DataFrame({'period': period,    'leads': leads,   'prospects': prospects,   'sales': sales})
#view first 10 rows
df.head(10)
        period    leadsprospects    sales
0127.89186267.11266162.563318
1236.04427950.80031962.920068
2330.53540569.40776164.278797
3427.69927678.48754267.124360
4521.75908549.95012668.754919
5632.64881263.04629377.788596
6722.44177363.68167777.322973
7844.69687762.89007676.350205
8948.21947548.92326572.485540
91019.78863478.10996084.221815
</b>
<h3>Plot Multiple Lines in Matplotlib</h3>
The following code shows how to plot three individual lines in a single plot in matplotlib:
<b>import matplotlib.pyplot as plt 
#plot individual lines
plt.plot(df['leads'])
plt.plot(df['prospects'])
plt.plot(df['sales'])
#display plot
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/multMatplotlib1.png">
<h3>Customize Lines in Matplotlib</h3>
You can also customize the color, style, and width of each line:
<b>#plot individual lines with custom colors, styles, and widths
plt.plot(df['leads'], color='green')
plt.plot(df['prospects'], color='steelblue', linewidth=4)
plt.plot(df['sales'], color='purple', linestyle='dashed')
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/multMatplotlib2.png">
<h3>Add a Legend in Matplotlib</h3>
You can also add a legend so you can tell the lines apart:
<b>#plot individual lines with custom colors, styles, and widths
plt.plot(df['leads'], label='Leads', color='green')
plt.plot(df['prospects'], label='Prospects', color='steelblue', linewidth=4)
plt.plot(df['sales'], label='Sales', color='purple', linestyle='dashed')
#add legend
plt.legend()
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/multMatplotlib3.png">
<h3>Add Axis Labels and Titles in Matplotlib</h3>
Lastly, you can add axis labels and a title to make the plot complete:
<b>#plot individual lines with custom colors, styles, and widths
plt.plot(df['leads'], label='Leads', color='green')
plt.plot(df['prospects'], label='Prospects', color='steelblue', linewidth=4)
plt.plot(df['sales'], label='Sales', color='purple', linestyle='dashed')
#add legend
plt.legend()
#add axis labels and a title
plt.ylabel('Sales', fontsize=14)
plt.xlabel('Period', fontsize=14)
plt.title('Company Metrics', fontsize=16)
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/multMatplotlib4.png">
<em>You can find more Matplotlib tutorials  here .</em>
<h2><span class="orange">How to Plot Multiple ROC Curves in Python (With Example)</span></h2>
One way to visualize the performance of  classification models  in machine learning is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve.
Often you may want to fit several classification models to one dataset and create a ROC curve for each model to visualize which model performs best on the data.
The following step-by-step example shows how plot multiple ROC curves in Python.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import several necessary packages in Python:
<b>from sklearn import metrics
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np
import matplotlib.pyplot as plt
</b>
<h3>Step 2: Create Fake Data</h3>
Next, we’ll use the  make_classification()  function from sklearn to create a fake dataset with 1,000 rows, four predictor variables, and one binary response variable:
<b>#create fake dataset
X, y = datasets.make_classification(n_samples=1000,                    n_features=4,                    n_informative=3,                    n_redundant=1,                    random_state=0)
#split dataset into training and testing set
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3,random_state=0)</b>
<h3>Step 3: Fit Multiple Models & Plot ROC Curves</h3>
Next, we’ll fit a logistic regression model and then a gradient boosted model to the data and plot the ROC curve for each model on the same plot:
<b>#set up plotting area
plt.figure(0).clf()
#fit logistic regression model and plot ROC curve
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="Logistic Regression, AUC="+str(auc))
#fit gradient boosted model and plot ROC curve
model = GradientBoostingClassifier()
model.fit(X_train, y_train)
y_pred = model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = metrics.roc_curve(y_test, y_pred)
auc = round(metrics.roc_auc_score(y_test, y_pred), 4)
plt.plot(fpr,tpr,label="Gradient Boosting, AUC="+str(auc))
#add legend
plt.legend()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/multipleroc1.jpg">
The blue line shows the ROC curve for the logistic regression model and the orange line shows the ROC curve for the gradient boosted model.
The more that a ROC curve hugs the top left corner of the plot, the better the model does at classifying the data into categories.
To quantify this, we can calculate the AUC – area under the curve – which tells us how much of the plot is located under the curve.
The closer AUC is to 1, the better the model.
From our plot we can see the following AUC metrics for each model:
AUC of logistic regression model: <b>0.7902</b>
AUC of gradient boosted model: <b>0.9712</b>
Clearly the gradient boosted model does a better job of classifying the data into categories compared to the logistic regression model.
<h2><span class="orange">How to Plot a Normal Distribution in Python (With Examples)</span></h2>
To plot a  normal distribution  in Python, you can use the following syntax:
<b>#x-axis ranges from -3 and 3 with .001 steps
x = np.arange(-3, 3, 0.001)
#plot normal distribution with mean 0 and standard deviation 1
plt.plot(x, norm.pdf(x, 0, 1))
</b>
The <b>x</b> array defines the range for the x-axis and the <b>plt.plot()</b> produces the curve for the normal distribution with the specified mean and standard deviation.
The following examples show how to use these functions in practice.
<h3>Example 1: Plot a Single Normal Distribution</h3>
The following code shows how to plot a single normal distribution curve with a mean of 0 and a standard deviation of 1:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
#x-axis ranges from -3 and 3 with .001 steps
x = np.arange(-3, 3, 0.001)
#plot normal distribution with mean 0 and standard deviation 1
plt.plot(x, norm.pdf(x, 0, 1))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalDistPython1.png">
You can also modify the color and the width of the line in the graph:
<b>plt.plot(x, norm.pdf(x, 0, 1), color='red', linewidth=3)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalDistPython2.png">
<h3>Example 2: Plot Multiple Normal Distributions</h3>
The following code shows how to plot multiple normal distribution curves with different means and standard deviations:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
#x-axis ranges from -5 and 5 with .001 steps
x = np.arange(-5, 5, 0.001)
#define multiple normal distributions
plt.plot(x, norm.pdf(x, 0, 1), label='μ: 0, σ: 1')
plt.plot(x, norm.pdf(x, 0, 1.5), label='μ:0, σ: 1.5')
plt.plot(x, norm.pdf(x, 0, 2), label='μ:0, σ: 2')
#add legend to plot
plt.legend()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalDistPython3.png">
Feel free to modify the colors of the lines and add a title and axes labels to make the chart complete:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
#x-axis ranges from -5 and 5 with .001 steps
x = np.arange(-5, 5, 0.001)
#define multiple normal distributions
plt.plot(x, norm.pdf(x, 0, 1), label='μ: 0, σ: 1', color='gold')
plt.plot(x, norm.pdf(x, 0, 1.5), label='μ:0, σ: 1.5', color='red')
plt.plot(x, norm.pdf(x, 0, 2), label='μ:0, σ: 2', color='pink')
#add legend to plot
plt.legend(title='Parameters')
#add axes labels and a title
plt.ylabel('Density')
plt.xlabel('x')
plt.title('Normal Distributions', fontsize=14)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalDistPython4.png">
Refer to the  matplotlib documentation  for an in-depth explanation of the <b>plt.plot()</b> function.
<h2><span class="orange">How to Plot a Normal Distribution in R</span></h2>
To plot a  normal distribution  in R, we can either use base R or install a fancier package like ggplot2.
<h2>Using Base R</h2>
Here are three examples of how to create a normal distribution plot using Base R.
<h3>Example 1: Normal Distribution with mean = 0 and standard deviation = 1</h3>
To create a normal distribution plot with mean = 0 and standard deviation = 1, we can use the following code:
<b>#Create a sequence of 100 equally spaced numbers between -4 and 4
x &lt;- seq(-4, 4, length=100)
#create a vector of values that shows the height of the probability distribution
#for each value in x
y &lt;- dnorm(x)
#plot x and y as a scatterplot with connected lines (type = "l") and add
#an x-axis with custom labels
plot(x,y, type = "l", lwd = 2, axes = FALSE, xlab = "", ylab = "")
axis(1, at = -3:3, labels = c("-3s", "-2s", "-1s", "mean", "1s", "2s", "3s"))</b>
This generates the following plot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/normalPlot1.png">
<h3>Example 2: Normal Distribution with mean = 0 and standard deviation = 1 (less code)</h3>
We could also create a normal distribution plot without defining <em>x </em>and <em>y</em>, and instead simply using the “curve” function using the following code:
<b>curve(dnorm, -3.5, 3.5, lwd=2, axes = FALSE, xlab = "", ylab = "")
axis(1, at = -3:3, labels = c("-3s", "-2s", "-1s", "mean", "1s", "2s", "3s"))</b>
This generates the exact same plot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/normalPlot1.png">
<h3>Example 3: Normal Distribution with customized mean and standard deviation</h3>
To create a normal distribution plot with a user-defined mean and standard deviation, we can use the following code:
<b>#define population mean and standard deviation
population_mean &lt;- 50
population_sd &lt;- 5
#define upper and lower bound
lower_bound &lt;- population_mean - population_sd
upper_bound &lt;- population_mean + population_sd
#Create a sequence of 1000 x values based on population mean and standard deviation
x &lt;- seq(-4, 4, length = 1000) * population_sd + population_mean
#create a vector of values that shows the height of the probability distribution
#for each value in x
y &lt;- dnorm(x, population_mean, population_sd)
#plot normal distribution with customized x-axis labels
plot(x,y, type = "l", lwd = 2, axes = FALSE, xlab = "", ylab = "")
sd_axis_bounds = 5
axis_bounds &lt;- seq(-sd_axis_bounds * population_sd + population_mean,    sd_axis_bounds * population_sd + population_mean,    by = population_sd)
axis(side = 1, at = axis_bounds, pos = 0)</b>
This generates the following plot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/normalPlot2.png">
<h2>Using ggplot2</h2>
Another way to create a normal distribution plot in R is by using the ggplot2 package. Here are two examples of how to create a normal distribution plot using ggplot2.
<h3>Example 1: Normal Distribution with mean = 0 and standard deviation = 1</h3>
To create a normal distribution plot with mean = 0 and standard deviation = 1, we can use the following code:
<b>#install (if not already installed) and load ggplot2
if(!(require(ggplot2))){install.packages('ggplot2')}
#generate a normal distribution plot
ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
stat_function(fun = dnorm)</b>
This generates the following plot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/normalPlot3.png">
<h3>Example 2: Normal Distribution using the ‘mtcars’ dataset</h3>
The following code illustrates how to create a normal distribution for the <em>miles per gallon</em> column in the built-in R dataset <em>mtcars</em>:
<b>ggplot(mtcars, aes(x = mpg)) +
stat_function(
fun = dnorm,
args = with(mtcars, c(mean = mean(mpg), sd = sd(mpg)))
) +
scale_x_continuous("Miles per gallon")</b>
This generates the following plot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/normalPlot4.png">
<h2><span class="orange">How to Plot a Poisson Distribution in R</span></h2>
To plot the probability mass function for a  Poisson distribution  in R, we can use the following functions:
<b>dpois(x, lambda)</b> to create the probability mass function
<b>plot(x, y, type = ‘h’) </b>to plot the probability mass function, specifying the plot to be a histogram (type=’h’)
To plot the probability mass function, we simply need to specify <b>lambda </b>(e.g. the rate of occurrence of events) in the <b>dpois() </b>function.
For example, the following code illustrates how to plot a probability mass function for a Poisson distribution with lambda = 5:
<b>#define range of "successes"
success &lt;- 0:20
#create plot of probability mass function
plot(success, dpois(success, lambda=5), type='h')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonR1.png">
The x-axis shows the number of “successes” – e.g. the number of events that occurred – and the y-axis shows the probability of obtaining that number of successes in 20 trials.
We can add a title, change the axes labels, and increase the line width to make the plot more aesthetically pleasing:
<b>success &lt;- 0:20
plot(success, dpois(success, lambda=5),
     type='h',
     main='Poisson Distribution (lambda = 5)',
     ylab='Probability',
     xlab ='# Successes',
     lwd=3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonR2.png">
We can use the following code to obtain the actual probabilities for each number of successes shown in the plot:
<b>#prevent R from displaying numbers in scientific notation 
options(scipen=999) 
#define range of successes
success &lt;- 0:20
#display probability of success for each number of trials
dpois(success, lambda=5)
[1] 0.0067379469991 0.0336897349954 0.0842243374886 0.1403738958143
[5] 0.1754673697679 0.1754673697679 0.1462228081399 0.1044448629571
[9] 0.0652780393482 0.0362655774156 0.0181327887078 0.0082421766854
[13] 0.0034342402856 0.0013208616483 0.0004717363030 0.0001572454343
[17] 0.0000491391982 0.0000144527054 0.0000040146404 0.0000010564843
[21] 0.0000002641211
</b>
<h2><span class="orange">How to Plot a Polynomial Regression Curve in R</span></h2>
<b>Polynomial regression</b> is a regression technique we use when the relationship between a predictor variable and a  response variable  is nonlinear.
This tutorial explains how to plot a polynomial regression curve in R.
<b>Related:</b>  The 7 Most Common Types of Regression 
<h3>Example: Plot Polynomial Regression Curve in R</h3>
The following code shows how to fit a polynomial regression model to a dataset and then plot the polynomial regression curve over the raw data in a scatterplot:
<b>#define data
x &lt;- runif(50, 5, 15)
y &lt;- 0.1*x^3 - 0.5 * x^2 - x + 5 + rnorm(length(x),0,10) 
 
#plot x vs. y
plot(x, y, pch=16, cex=1.5) 
 
#fit polynomial regression model
fit &lt;- lm(y ~ x + I(x^2) + I(x^3))
 
#use model to get predicted values
pred &lt;- predict(fit)
ix &lt;- sort(x, index.return=T)$ix
#add polynomial curve to plot
lines(x[ix], pred[ix], col='red', lwd=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/polycurve1.png">
We can also add the fitted polynomial regression equation to the plot using the <b>text()</b> function:
<b>#define data
x &lt;- runif(50, 5, 15)
y &lt;- 0.1*x^3 - 0.5 * x^2 - x + 5 + rnorm(length(x),0,10) 
 
#plot x vs. y
plot(x, y, pch=16, cex=1.5) 
 
#fit polynomial regression model
fit &lt;- lm(y ~ x + I(x^2) + I(x^3))
 
#use model to get predicted values
pred &lt;- predict(fit)
ix &lt;- sort(x, index.return=T)$ix
#add polynomial curve to plot
lines(x[ix], pred[ix], col='red', lwd=2)
#get model coefficients
coeff &lt;- round(fit$coefficients , 2)
#add fitted model equation to plot
text(9, 200 , paste("Model: ", coeff[1], " + ", coeff[2],    "*x", "+", coeff[3], "*x^2", "+", coeff[4], "*x^3"), cex=1.3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/polycurve2.png">
Note that the <b>cex</b> argument controls the font size of the text. The default value is 1, so we chose to use a value of <b>1.3</b> to make the text easier to read.
<h2><span class="orange">How to Plot Predicted Values in R (With Examples)</span></h2>
Often you may want to plot the predicted values of a regression model in R in order to visualize the differences between the predicted values and the actual values.
This tutorial provides examples of how to create this type of plot in base R and ggplot2.
<h3>Example 1: Plot of Predicted vs. Actual Values in Base R</h3>
The following code shows how to fit a  multiple linear regression model  in R and then create a plot of predicted vs. actual values:
<b>#create data
df &lt;- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12), x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14), y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))
#fit multiple linear regression model
model &lt;- lm(y ~ x1 + x2, data=df)
#plot predicted vs. actual values
plot(x=predict(model), y=df$y,
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Predicted vs. Actual Values')
#add diagonal line for estimated regression line
abline(a=0, b=1)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/predictedR1.png">
The x-axis displays the predicted values from the model and the y-axis displays the actual values from the dataset. The diagonal line in the middle of the plot is the estimated regression line.
Since each of the data points lies fairly close to the estimated regression line, this tells us that the regression model does a pretty good job of fitting the data.
We can also create a data frame that shows the actual and predicted values for each data point:
<b>#create data frame of actual and predicted values
values &lt;- data.frame(actual=df$y, predicted=predict(model))
#view data frame
values
   actual predicted
1      22  22.54878
2      24  23.56707
3      24  23.96341
4      25  24.98171
5      25  25.37805
6      27  26.79268
7      29  28.60366
8      31  30.41463
9      32  33.86585
10     36  34.88415
</b>
<h3>Example 2: Plot of Predicted vs. Actual Values in ggplot2</h3>
The following code shows how to create a plot of predicted vs. actual values using the  ggplot2  data visualization package:
<b>library(ggplot2) 
#create data
df &lt;- data.frame(x1=c(3, 4, 4, 5, 5, 6, 7, 8, 11, 12), x2=c(6, 6, 7, 7, 8, 9, 11, 13, 14, 14), y=c(22, 24, 24, 25, 25, 27, 29, 31, 32, 36))
#fit multiple linear regression model
model &lt;- lm(y ~ x1 + x2, data=df)
#plot predicted vs. actual values
ggplot(df, aes(x=predict(model), y=y)) + 
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/predictedR2.png">
Once again, the x-axis displays the predicted values from the model and the y-axis displays the actual values from the dataset. 
<h2><span class="orange">How to Plot a ROC Curve in Python (Step-by-Step)</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the “true positive rate.”
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative. This is also called the “true negative rate.”
One way to visualize these two metrics is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve. This is a plot that displays the sensitivity and specificity of a logistic regression model.
The following step-by-step example shows how to create and interpret a ROC curve in Python.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import the packages necessary to perform logistic regression in Python:
<b>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import matplotlib.pyplot as plt
</b>
<h3>Step 2: Fit the Logistic Regression Model</h3>
Next, we’ll import a dataset and fit a logistic regression model to it:
<b>#import dataset from CSV file on Github
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/default.csv"
data = pd.read_csv(url)
#define the predictor variables and the response variable
X = data[['student', 'balance', 'income']]
y = data['default']
#split the dataset into training (70%) and testing (30%) sets
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) 
#instantiate the model
log_regression = LogisticRegression()
#fit the model using the training data
log_regression.fit(X_train,y_train)</b>
<h3>Step 3: Plot the ROC Curve</h3>
Next, we’ll calculate the true positive rate and the false positive rate and create a ROC curve using the Matplotlib data visualization package:
<b>#define metrics
y_pred_proba = log_regression.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
#create ROC curve
plt.plot(fpr,tpr)
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/rocPython1.png">
The more that the curve hugs the top left corner of the plot, the better the model does at classifying the data into categories.
As we can see from the plot above, this logistic regression model does a pretty poor job of classifying the data into categories.
To quantify this, we can calculate the AUC – area under the curve – which tells us how much of the plot is located under the curve.
The closer AUC is to 1, the better the model. A model with an AUC equal to 0.5 is no better than a model that makes random classifications.
<h3>Step 4: Calculate the AUC</h3>
We can use the following code to calculate the AUC of the model and display it in the lower right corner of the ROC plot:
<b>#define metrics
y_pred_proba = log_regression.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/rocPython2.png">
The AUC for this logistic regression model turns out to be <b>0.5602</b>. Since this is close to 0.5, this confirms that the model does a poor job of classifying data.
<b>Related:</b>  How to Plot Multiple ROC Curves in Python 
<h2><span class="orange">How to Plot the Rows of a Matrix in R (With Examples)</span></h2>
Occasionally you may want to plot the rows of a matrix in R as individual lines. Fortunately this is easy to do using the following syntax:
<b>matplot(t(matrix_name), type = "l")
</b>
This tutorial provides an example of how to use this syntax in practice.
<h3>Example: Plot the Rows of a Matrix in R</h3>
First, let’s create a fake matrix to work with that contains three rows:
<b>#make this example reproducible
set.seed(1)
#create matrix
data &lt;- matrix(sample.int(50, 21), nrow=3)
#view matrix
data
     [,1] [,2] [,3] [,4] [,5] [,6] [,7]
[1,]    4   34   14   21    7   40   12
[2,]   39   23   18   41    9   25   36
[3,]    1   43   33   10   15   47   48
</b>
Next, let’s use <b>matplot</b> to plot the three rows of the matrix as individual lines on a plot:
<b>matplot(t(data), type = "l")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/matplot1.png">
Each line in the plot represents one of the three rows of data in the matrix.
<b>Note:</b> The matplot function is used to plot the columns of a matrix. Thus, we use <b>t()</b> to transpose the matrix so that we instead plot the rows.
We can also modify the width of the lines and add some labels to the plot:
<b>matplot(t(data),
        type = "l",
        lwd = 2,
        main="Plotting the Rows of a Matrix",
        ylab="Value")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/matplot2.png">
You can find more R tutorials on  this page .
<h2><span class="orange">How to Plot a Subset of a Data Frame in R</span></h2>
You can use the following methods to plot a subset of a data frame in R:
<b>Method 1: Plot Subset of Data Frame Based on One Condition</b>
<b>#plot var1 vs. var2 where var3 is less than 15
with(df[df$var3 &lt; 15,], plot(var1, var2))
</b>
<b>Method 2: Plot Subset of Data Frame Based on Multiple Conditions</b>
<b>#plot var1 vs. var2 where var3 is less than 15 and var4 is greater than 3
with(df[(df$var3 &lt; 15) & (df$var4 > 3),], plot(var1, var2))
</b>
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(A=c(1, 3, 3, 4, 5, 7, 8), B=c(3, 6, 9, 12, 15, 14, 10), C=c(10, 12, 14, 14, 17, 19, 20), D=c(5, 7, 4, 3, 3, 2, 1))
#view data frame
df
  A  B  C D
1 1  3 10 5
2 3  6 12 7
3 3  9 14 4
4 4 12 14 3
5 5 15 17 3
6 7 14 19 2
7 8 10 20 1</b>
<h2>Example 1: Plot Subset of Data Frame Based on One Condition</h2>
The following code shows how to create a scatter plot of variable A vs. variable B where variable C is less than 15:
<b>#plot A vs. B where C is less than 15
with(df[df$C &lt; 15,], plot(A, B))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/plotsubset1.jpg"462">
Notice that only the rows in the data frame where variable C is less than 15 are shown in the plot.
<h2>Example 2: Plot Subset of Data Frame Based on Multiple Conditions</h2>
The following code shows how to create a scatter plot of variable A vs. variable B where variable C is less than 15 <b>and</b> variable D is greater than 3:
<b>#plot A vs. B where C is less than 15 and D is greater than 3
with(df[(df$C&lt; 15) & (df$D> 3),], plot(A, B))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/plotsubset2.jpg"440">
Notice that only the rows in the data frame where variable C is less than 15 and variable D is greater than 3 are shown in the plot.
<b>Related</b>:  How to Use with() and within() Functions in R 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Create Scatter Plots by Group in R 
 How to Create a Scatterplot Matrix in R 
<h2><span class="orange">How to Plot SVM Object in R (With Example)</span></h2>
You can use the following basic syntax to plot an SVM (support vector machine) object in R:
<b>library(e1071)
plot(svm_model, df)
</b>
In this example, <b>df</b> is the name of the data frame and <b>svm_model</b> is a support vector machine fit using the <b>svm()</b> function.
The following example shows how to use this syntax in practice.
<h2>Example: How to Plot SVM Object in R</h2>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(points = c(4, 5, 5, 7, 8, 12, 15, 22, 25, 29), assists = c(3, 4, 6, 8, 5, 6, 5, 6, 8, 12), good = factor(c(0, 0, 0, 1, 0, 1, 0, 1, 1, 1)))
#view data frame
df
   points assists good
1       4       3    0
2       5       4    0
3       5       6    0
4       7       8    1
5       8       5    0
6      12       6    1
7      15       5    0
8      22       6    1
9      25       8    1
10     29      12    1</b>
Suppose we would like to create a support vector machine that uses the variables <b>points</b> and <b>assists</b> to predict whether or not a player is <b>good</b> (0 = no, 1 = yes).
We can use the following code to fit the support vector machine and then plot the results:
<b>library(e1071)
#fit support vector machine
model = svm(good ~ points + assists, data = df)
#plot support vector machine
plot(model, df)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/svm1.jpg"488">
The plot displays the values for the <b>assists</b> variable on the x-axis, the values for the <b>points</b> variable on the y-axis, and uses two different colors to display whether or not a player is predicted to be good (red) or not (yellow).
Note that you can use the <b>color.palette</b> argument within the <b>plot()</b> function to use a different color palette for the plot.
For example, we might choose to use the <b>heat.colors</b> color palette:
<b>library(e1071)
#fit support vector machine
model = svm(good ~ points + assists, data = df)
#plot support vector machine using different color palette
plot(model, df, color.palette = heat.colors)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/svm2.jpg"481">
Other popular choices for the <b>color.palette</b> argument include:
rainbow
terrain.colors
topo.colors
Each color palette will produce different colors for the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Filter for Unique Values Using dplyr 
 How to Filter by Multiple Conditions Using dplyr 
 How to Count Number of Occurrences in Columns in R 
<h2><span class="orange">How to Plot a t Distribution in R</span></h2>
To plot the probability density function for a t distribution in R, we can use the following functions:
<b>dt(x, df)</b> to create the probability density function
<b>curve(function, from = NULL, to = NULL)</b> to plot the probability density function
To plot the probability density function, we need to specify <b>df </b>(degrees of freedom) in the <b>dt() </b>function along with the <b>from </b>and <b>to </b>values in the <b>curve() </b>function.
For example, the following code illustrates how to plot a probability density function for a t distribution with 10 degrees of freedom where the x-axis of the plot ranges from -4 to 4:
<b>curve(dt(x, df=10), from=-4, to=4)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tDistR1.png">
Similar to  the normal distribution , the t distribution is symmetrical around a mean of 0.
We can add a title, change the y-axis label, increase the line width, and even change the line color to make the plot more aesthetically pleasing:
<b>curve(dt(x, df=10), from=-4, to=4, 
    main = 't Distribution (df = 10)', #add title
    ylab = 'Density', #change y-axis label
    lwd = 2, #increase line width to 2
    col = 'steelblue') #change line color to steelblue 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tDistR4.png">
We can also add more than one curve to the graph to compare t distributions with different degrees of freedom. For example, the following code creates t distribution plots with df = 6, df = 10, and df = 30:
<b>curve(dt(x, df=6), from=-4, to=4, col='blue') 
curve(dt(x, df=10), from=-4, to=4, col='red', add=TRUE)
curve(dt(x, df=30), from=-4, to=4, col='green', add=TRUE)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tDistR2.png">
We can add a legend to the plot by using the <b>legend() </b>function, which takes on the following syntax:
<b>legend(x, y=NULL, legend, fill, col, bg, lty, cex)</b>
where:
<b>x, y:</b> the x and y coordinates used to position the legend
<b>legend:</b> the text to go in the legend
<b>fill:</b> fill color inside the legend
<b>col: </b>the list of colors to be used for the lines inside the legend
<b>bg: </b>the background color for the legend
<b>lty: </b>line style
<b>cex: </b>text size in the legend
In our example we will use the following syntax to create a legend:
<b>#create density plots
curve(dt(x, df=6), from=-4, to=4, col='blue') 
curve(dt(x, df=10), from=-4, to=4, col='red', add=TRUE)
curve(dt(x, df=30), from=-4, to=4, col='green', add=TRUE)
#add legend
legend(-4, .3, legend=c("df=6", "df=10", "df=30"),
       col=c("blue", "red", "green"), lty=1, cex=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tDistR3.png">
<h2><span class="orange">How to Plot a Table in R (With Example)</span></h2>
Often you may want to plot a table along with a chart in R.
Fortunately this is easy to do using functions from the <b>gridExtra</b> package.
The following example shows how to use functions from this package to plot a table in practice.
<h2>Example: Plot a Table in R</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7), y=c(3, 4, 4, 8, 6, 10, 14))
#view data frame
df
  x  y
1 1  3
2 2  4
3 3  4
4 4  8
5 5  6
6 6 10
7 7 14</b>
Now suppose we would like to create a scatterplot to visualize the values in the data frame and also plot a table that shows the raw values.
We can use the following syntax to do so:
<b>library(gridExtra)
library(ggplot2)
#define scatterplot
my_plot &lt;- ggplot(df, aes(x=x, y=y)) +
  geom_point()
#define table
my_table &lt;- tableGrob(df)
#create scatterplot and add table underneath it
grid.arrange(my_plot, my_table)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/plottable1.jpg">
Here is how this code worked:
We used <b>ggplot()</b> to generate the scatterplot
We used<b> tableGrob()</b> to convert the data frame to a table object
We used <b>grid.arrange()</b> to plot both the scatterplot and the table
By default, the <b>grid.arrange()</b> function arranges the scatterplot and the table in the same column.
However, you can use the <b>ncol</b> argument to display the scatterplot and table in two columns, i.e. side by side:
<b>library(gridExtra)
library(ggplot2)
#define scatterplot
my_plot &lt;- ggplot(df, aes(x=x, y=y)) +
  geom_point()
#define table
my_table &lt;- tableGrob(df)
#create scatterplot and add table next to it
grid.arrange(arrangeGrob(my_plot, my_table, ncol=2))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/plottable2.jpg">
The table is now shown to the side of the plot rather than underneath it.
<b>Note</b>: In this example we only plotted one table, but you can specify multiple tables within the <b>grid.arrange()</b> function to plot multiple tables at once.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Label Points on a Scatterplot in R 
 How to Add Text Outside of a Plot in R 
 How to Create a Scatterplot with a Regression Line in R 
<h2><span class="orange">How to Plot a Time Series in Excel (With Example)</span></h2>
This tutorial provides a step-by-step example of how to plot the following time series in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time7.jpg"545">
Let’s jump in!
<h2>Step 1: Enter the Time Series Data</h2>
First, let’s enter the following values for a time series dataset in Excel:
<h2>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time1.jpg"430">
<b>Step 2: Plot the Time Series</b>
</h2>
Next, highlight the values in the range <b>A2:B20</b>: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time2.jpg"446">
Then click the <b>Insert</b> tab along the top ribbon, then click the icon called <b>Scatter with Smooth Lines and Markers</b> within the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time3.jpg"670">
The following chart will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time4.jpg"636">
The x-axis shows the date and the y-axis shows the sales.
<h2>Step 3: Customize the Time Series Plot</h2>
Lastly, we can customize the plot to make it easier to read.
Double click any of the values on the x-axis. In the <b>Format Axis</b> panel that appears, click the icon called <b>Size & Properties</b> and type <b>-45</b> in the box titled <b>Custom angle</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time5.jpg"314">
The x-axis labels will be rotated at a 45 degree angle to make them easier to read:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time6.jpg"581">
Lastly, click on the Chart Title and change it to whatever you’d like.
Then click the labels on each axis and make them bold:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/time7.jpg"545">
The time series plot is now complete.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Add Labels to Scatterplot Points in Excel 
 How to Change Axis Scales in Excel Plots 
 How to Add a Vertical Line to Charts in Excel 
<h2><span class="orange">How to Plot a Time Series in R (With Examples)</span></h2>
Often you may want to plot a time series in R to visualize how the values of the time series are changing over time.
This tutorial explains how to quickly do so using the data visualization library  ggplot2 .
<h3>Basic Time Series Plot in R</h3>
Suppose we have the following dataset in R:
<b>#create dataset
df &lt;- data.frame(date = as.Date("2021-01-01") - 0:99, sales = runif(100, 10, 500) + seq(50, 149)^2)
#view first six rows
head(df)
        date    sales
1 2021-01-01 2845.506
2 2020-12-31 2837.849
3 2020-12-30 3115.517
4 2020-12-29 2847.161
5 2020-12-28 3374.619
6 2020-12-27 3182.005</b>
We can use the following code to create a basic time series plot for this dataset using ggplot2:
<b>library(ggplot2)
#create time series plot
p &lt;- ggplot(df, aes(x=date, y=sales)) +
       geom_line()
#display time series plot
p
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/timeseriesR1.png">
<h3>Format the Dates on the X-Axis</h3>
We can use the <b>scale_x_date() </b>function* to format the dates shown along the x-axis of the plot. This function takes the following arguments:
<b>%d: </b>Day as a number between 0 and 31
<b>%a: </b>Abbreviated weekday (e.g. “Tue”)
<b>%A: </b>Unabbreviated weekday (e.g. “Tuesday”)
<b>%m</b>: Month between 0 and 12
<b>%b:</b> Abbreviated month (e.g. “Jan”)
<b>%B: </b>Unabbreviated month (e.g. “January”)
<b>%y:</b> 2-digit year (e.g. “21”)
<b>%Y:</b> 4-digit year (e.g. “2021”)
<b>%W: </b>Week of the year between 0 and 52
<em><b>*</b>In order for this function to work, the x-axis variable must be a date variable. If it is not already one, you can quickly convert it to one by using <b>as.Date(variable_name)</b>.</em>
The following code shows how to use one of these formats in practice:
<b>p + scale_x_date(date_labels = "%b %Y")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/timeseriesR2.png">
You can also add more frequent (or infrequent) breaks along the x-axis by using the <b>date_breaks</b> argument. For example, we could display the dates for every two weeks along the x-axis:
<b>p + scale_x_date(date_breaks = "2 week")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/timeseriesR3.png">
We can also easily angle the x-axis labels by using the following argument:
<b>p + theme(axis.text.x=element_text(angle=50, hjust=1)) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/timeseriesR4.png">
Lastly, we can change the theme, the axes labels, and the title to make the time series plot more visually appealing:
<b>p &lt;- ggplot(df, aes(x=date, y=sales)) +
       geom_line(color="turquoise4") +
       theme_minimal() + 
       labs(x="", y="Sales", title="Total Sales (Past 100 Days)") +
       theme(plot.title = element_text(hjust=0.5, size=20, face="bold"))
p</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/timeseriesR5.png">
<h3>
<b>Additional Resources</b>
</h3>
 A Complete Guide to the Best ggplot2 Themes 
 The Complete Guide to ggplot2 Titles 
 How to Create Side-by-Side Plots in ggplot2 
<h2><span class="orange">How to Plot a Uniform Distribution in R</span></h2>
The  uniform distribution  is a probability distribution in which every value between an interval from <em>a</em> to <em>b</em> is equally likely to occur.
If a  random variable  <em>X</em> follows a uniform distribution, then the probability that <em>X</em> takes on a value between <em>x<sub>1</sub></em> and <em>x</em><sub>2</sub> can be found by the following formula:
<b>P(x<sub>1</sub> &lt; X &lt; x<sub>2</sub>) = (x<sub>2 </sub>– x<sub>1</sub>) / (b – a)</b>
where:
<b>x<sub>1</sub>:</b> the lower value of interest
<b>x<sub>2</sub>:</b> the upper value of interest
<b>a: </b>the minimum possible value
<b>b: </b>the maximum possible value
The following examples show how to plot a uniform distribution in R.
<h3>Example 1: Plot Basic Uniform Distribution in R</h3>
The following code shows how to plot a basic uniform distribution in R:
<b>#define x-axis
x &lt;- seq(-4, 4, length=100)
#calculate uniform distribution probabilities
y &lt;- dunif(x, min = -3, max = 3)
#plot uniform distribution
plot(x, y, type = 'l')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotUniform1.png">
The x-axis displays the potential values for a random variable that follows a uniform distribution while the y-axis shows the probability that the random variable takes on those values.
<b>Note</b>: The <b>dunif()</b> function in R is used to calculate the density of a uniform distribution, given a minimum and maximum value.
<h3>Example 2: Plot Custom Uniform Distribution in R</h3>
The following code shows how to plot a basic uniform distribution in R along with how to modify the title, axes labels, and colors:
<b>#define x-axis
x &lt;- seq(-4, 4, length=100)
#calculate uniform distribution probabilities
y &lt;- dunif(x, min = -3, max = 3)
#plot uniform distribution
plot(x, y, type = 'l', lwd = 3, ylim = c(0, .2), col='blue',
     xlab='x', ylab='Probability', main='Uniform Distribution Plot')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/plotUniform2.png">
<h2><span class="orange">How to Plot a Weibull Distribution in R</span></h2>
To plot the probability density function for a Weibull distribution in R, we can use the following functions:
<b>dweibull(x, shape, scale = 1)</b> to create the probability density function.
<b>curve(function, from = NULL, to = NULL)</b> to plot the probability density function.
To plot the probability density function, we need to specify the value for the <b>shape</b> and <b>scale</b> parameter in the <b>dweibull </b>function along with the <b>from </b>and <b>to </b>values in the <b>curve() </b>function.
For example, the following code illustrates how to plot a probability density function for a Weibull distribution with parameters shape = 2 and scale = 1 where the x-axis of the plot ranges from 0 to 4:
<b>curve(dweibull(x, shape=2, scale = 1), from=0, to=4)
</b>
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/weibullR1.png">
We can add a title, change the y-axis label, increase the line width, and even change the line color to make the plot more aesthetically pleasing:
<b>curve(dweibull(x, shape=2, scale = 1), from=0, to=4, 
    main = 'Weibull Distribution (shape = 2, scale = 1)', #add title
    ylab = 'Density', #change y-axis label
    lwd = 2, #increase line width to 2
    col = 'steelblue') #change line color to steelblue 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/weibullR2.png">
We can also add more than one curve to the graph to compare Weibull distributions with different shape and scale parameters:
<b>curve(dweibull(x, shape=2, scale = 1), from=0, to=4, col='red')
curve(dweibull(x, shape=1.5, scale = 1), from=0, to=4, col='blue', add=TRUE)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/weibullR3.png">
We can add a legend to the plot by using the <b>legend() </b>function, which takes on the following syntax:
<b>legend(x, y=NULL, legend, fill, col, bg, lty, cex)</b>
where:
<b>x, y:</b> the x and y coordinates used to position the legend
<b>legend:</b> the text to go in the legend
<b>fill:</b> fill color inside the legend
<b>col: </b>the list of colors to be used for the lines inside the legend
<b>bg: </b>the background color for the legend
<b>lty: </b>line style
<b>cex: </b>text size in the legend
In our example we will use the following syntax to create a legend:
<b>#create density plots
curve(dweibull(x, shape=2, scale = 1), from=0, to=4, col='red')
curve(dweibull(x, shape=1.5, scale = 1), from=0, to=4, col='blue', add=TRUE)
#add legend
legend(2, .7, legend=c("shape=2, scale=1", "shape=1.5, scale=1"),
       col=c("red", "blue"), lty=1, cex=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/weibullR4.png">
<h2><span class="orange">What is a Probability Mass Function (PMF) in Statistics?</span></h2>
A <b>probability mass function</b>, often abbreviated <b>PMF</b>, tells us the probability that a  discrete random variable  takes on a certain value.
For example, suppose we roll a dice one time. If we let x denote the number that the dice lands on, then the probability that the <em>x</em> is equal to different values can be described as follows:
P(X=1): 1/6
P(X=2): 1/6
P(X=3): 1/6
P(X=4): 1/6
P(X=5): 1/6
P(X=6): 1/6
There is an equal chance that the dice could land on any number between 1 and 6.
Here’s how we would write these probabilities as a probability mass function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pmf1-1.png">
The left side of the diagram shows the probability associated with the outcomes on the right side:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/pmf2.png">
One characteristic of a probability mass function is that all of the probabilities must add up to 1. You’ll notice that this PMF satisfies that condition:
Sum of probabilities = 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
The <b>support</b> of a probability mass function refers to the set of values that the discrete random variable can take. In this example, the support would be {1, 2, 3, 4, 5, 6} since the value of the dice can take on any of these values.
Outside of the support, the value for the PMF is equal to zero. For example, the probability that the dice lands on “0” or “7” or “8” is equal to zero since none of these numbers are included in the support.
<h3>Probability Mass Functions in Practice</h3>
The two most common examples of probability mass functions in practice are for the  Binomial distribution  and the  Poisson distribution .
<b>Binomial Distribution</b>
If a random variable <em>X</em> follows a Binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0, 1, 2, and 3 heads during these 3 flips:
<b>P(X=0) </b>= <sub>3</sub>C<sub>0</sub> * .5<sup>0</sup> * (1-.5)<sup>3-0</sup> = 1 * 1 * (.5)<sup>3</sup> = <b>0.125</b>
<b>P(X=1) </b>= <sub>3</sub>C<sub>1</sub> * .5<sup>1</sup> * (1-.5)<sup>3-1</sup> = 1 * 1 * (.5)<sup>2</sup> = <b>0.375</b>
<b>P(X=2) </b>= <sub>3</sub>C<sub>2</sub> * .5<sup>2</sup> * (1-.5)<sup>3-2</sup> = 1 * 1 * (.5)<sup>1</sup> = <b>0.375</b>
<b>P(X=3) </b>= <sub>3</sub>C<sub>3</sub> * .5<sup>3</sup> * (1-.5)<sup>3-3</sup> = 1 * 1 * (.5)<sup>0</sup> = <b>0.125</b>
<b>Poisson Distribution</b>
If a random variable <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
For example, suppose a particular hospital experiences an average of 2 births per hour. We can use the formula above to determine the probability of experiencing 0, 1, 2, 3 births, etc. in a given hour:
<b>P(X=0) </b>= 2<sup>0</sup> * e<sup>– 2</sup> / 0! = <b>0.1353</b>
P(X=1) </b>= 2<sup style="color: #000000;">1</sup> * e<sup style="color: #000000;">– 2</sup> / 1! = 0.2707</b>
P(X=2) </b>= 2<sup style="color: #000000;">2</sup> * e<sup style="color: #000000;">– 2</sup> / 2! = 0.2707</b>
P(X=3) </b>= 2<sup style="color: #000000;">3</sup> * e<sup style="color: #000000;">– 2</sup> / 3! = 0.1805</b>
<h3>Visualizing a PMF</h3>
We often visualize probability mass functions with bar charts.
For example, the following bar chart shows the probabilities associated with the number of births per hour for the Poisson distribution described in the previous example:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonDist1.png">
Note that the number of births could extend to infinity, but the probabilities become so low after 10 that we can’t even see them on a bar chart.
<h3>Properties of a PMF</h3>
A probability mass function has the following properties:
<b>1. All probabilities are positive in the support.</b> For example, the probability that a dice lands between 1 and 6 is positive, while the probability of all other outcomes is equal to zero.
<b>2. All outcomes have a probability between 0 and 1.</b> For example, the probability that a dice lands between 1 and 6 is 1/6, or 0.1666666 for each outcome.
<b>3. The sum of all probabilities must add up to 1.</b> For example, the sum of probabilities that a dice lands on a certain number is 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
<h2><span class="orange">How to Use the PMT Function in Google Sheets (3 Examples)</span></h2>
The <b>PMT</b> function in Google Sheets can be used to find the periodic payment for a loan.
This function uses the following basic syntax:
<b>PMT(rate, number_of_periods, present_value)</b>
where:
<b>rate</b>: The annual interest rate
<b>number_of_periods</b>: Number of payments to be made
<b>present_value</b>: The total amount of the loan
The following examples show how to use this function in different scenarios.
<h3>Example 1: Calculate Loan Payments for Mortgage</h3>
Suppose a family takes out a mortgage loan for a house with the following details:
Mortgage Amount: $200,000
Number of Months: 360
Annual Interest Rate: 4%
The following screenshot shows how to use the PMT function in Google Sheets to calculate the necessary monthly loan payment:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pmt1.png">
The monthly loan payment is <b>$954.83</b>. This is how much the family must pay each month in order to pay off the $200,000 loan in 360 months.
<b>Note</b>: When using the PMT function, we divided the annual interest rate by 12 (since we’re paying monthly) and we placed a negative sign in front of the mortgage amount since the family technically started with a value of -$200,000 and are trying to get back to zero.
<h3>Example 2: Calculate Loan Payments for Car Loan</h3>
Suppose an individual takes out a loan for a car with the following details:
Loan Amount: $20,000
Number of Months: 60
Annual Interest Rate: 3%
The following screenshot shows how to use the PMT function in Google Sheets to calculate the necessary monthly loan payment:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pmt2.png">
The monthly loan payment is <b>$359.37</b>. This is how much the individual must pay each month in order to pay off the $20,000 loan in 60 months.
<h3>Example 3: Calculate Loan Payments for Student Loan</h3>
Suppose a student takes out a loan for university with the following details:
Loan Amount: $40,000
Number of Months: 120
Annual Interest Rate: 5.2%
The following screenshot shows how to use the PMT function in Google Sheets to calculate the necessary monthly loan payment:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/pmt3.png">
The monthly loan payment is <b>$428.18</b>. This is how much the individual must pay each month in order to pay off the $40,000 loan in 120 months.
<b>Note</b>: You can find the complete online documentation for the PMT function  here .
<h2><span class="orange">How to Calculate Point-Biserial Correlation in Excel</span></h2>
<b>Point-biserial correlation</b> is used to measure the relationship between a binary variable, x, and a continuous variable, y.
Similar to the  Pearson correlation coefficient , the point-biserial correlation coefficient takes on a value between -1 and 1 where:
-1 indicates a perfectly negative correlation between two variables
0 indicates no correlation between two variables
1 indicates a perfectly positive correlation between two variables
This tutorial explains how to calculate the point-biserial correlation between two variables in Excel.
<h3>Example: Point-Biserial Correlation in Excel</h3>
Suppose we have the following binary variable, x, and a continuous variable, y:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/pointBiExcel1.png">
To calculate the <b>point-biserial correlation</b> between x and y, we can simply use the <b>=CORREL()</b> function as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/pointBiExcel2.png">
The point-biserial correlation between x and y is <b>0.218163</b>.
Since this number is positive, this indicates that when the variable x takes on the value “1” that the variable y tends to take on higher values compared to when the variable x takes on the value “0.”
We can easily verify this by calculating the average value of y when x is 0 and when x is 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/pointBiExcel3.png">
When x = 0, the average value of y is <b>14.2</b>. When x = 1, the average value of y is <b>16.2</b>. This confirms the fact that the point-biserial correlation between the two variables should be positive.
We can also use the following formulas to calculate the p-value for this correlation coefficient:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/pointBiExcel4.png">
The p-value turns out to be <b>0.5193</b>. Thus, although the correlation coefficient between the two variables is slightly positive it turns out to not be a statistically significant correlation.
<h2><span class="orange">How to Calculate Point-Biserial Correlation in Python</span></h2>
<b>Point-biserial correlation</b> is used to measure the relationship between a binary variable, x, and a continuous variable, y.
Similar to the  Pearson correlation coefficient , the point-biserial correlation coefficient takes on a value between -1 and 1 where:
-1 indicates a perfectly negative correlation between two variables
0 indicates no correlation between two variables
1 indicates a perfectly positive correlation between two variables
This tutorial explains how to calculate the point-biserial correlation between two variables in Python.
<h3>Example: Point-Biserial Correlation in Python</h3>
Suppose we have a binary variable, x, and a continuous variable, y:
<b>x = [0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0]
y = [12, 14, 17, 17, 11, 22, 23, 11, 19, 8, 12]
</b>
We can use the  pointbiserialr()  function from the scipy.stats library to calculate the point-biserial correlation between the two variables.
Note that this function returns a correlation coefficient along with a corresponding p-value:
<b>import scipy.stats as stats
#calculate point-biserial correlation
stats.pointbiserialr(x, y)
PointbiserialrResult(correlation=0.21816, pvalue=0.51928)
</b>
The point-biserial correlation coefficient is <b>0.21816 </b>and the corresponding p-value is <b>0.51928</b>.
Since the correlation coefficient is positive, this indicates that when the variable x takes on the value “1” that the variable y tends to take on higher values compared to when the variable x takes on the value “0.”
Since the p-value of this correlation is not less than .05, this correlation is not statistically significant. 
<em>You can find the exact details of how this correlation is calculated in the scipy.stats  documentation .</em>
<h2><span class="orange">How to Calculate Point-Biserial Correlation in R</span></h2>
<b>Point-biserial correlation</b> is used to measure the relationship between a binary variable, x, and a continuous variable, y.
Similar to the  Pearson correlation coefficient , the point-biserial correlation coefficient takes on a value between -1 and 1 where:
-1 indicates a perfectly negative correlation between two variables
0 indicates no correlation between two variables
1 indicates a perfectly positive correlation between two variables
This tutorial explains how to calculate the point-biserial correlation between two variables in R.
<h2>Example: Point-Biserial Correlation in R</h2>
Suppose we have a binary variable, x, and a continuous variable, y:
<b>x &lt;- c(0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0)
y &lt;- c(12, 14, 17, 17, 11, 22, 23, 11, 19, 8, 12)
</b>
We can use the built-in R function <b>cor.test() </b>to calculate the point-biserial correlation between the two variables:
<b>#calculate point-biserial correlation
cor.test(x, y)
Pearson's product-moment correlation
data:  x and y
t = 0.67064, df = 9, p-value = 0.5193
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.4391885  0.7233704
sample estimates:
      cor 
0.2181635 
</b>
From the output we can observe the following:
The point-biserial correlation coefficient is <b>0.218</b>
The corresponding p-value is <b>0.5193</b>
Since the correlation coefficient is positive, this indicates that when the variable x takes on the value “1” that the variable y tends to take on higher values compared to when the variable x takes on the value “0.”
However, since the p-value of this correlation is not less than .05, this correlation is not statistically significant. 
Note that the output also provides a 95% confidence interval for the true correlation coefficient, which turns out to be:
<b>95% C.I. = (-0.439, 0.723)</b>
Since this confidence interval contains zero, this is further evidence that the correlation coefficient is not statistically significant. 
<b>Note</b>: You can find the complete documentation for the<b> cor.test()</b> function  here .
<h2 data-slot-rendered-dynamic="true"><b>Additional Resources</h2>
The following tutorials explain how to calculate other correlation coefficients in R:
 How to Calculate Partial Correlation in R 
 How to Calculate Rolling Correlation in R 
 How to Calculate Spearman Rank Correlation in R 
 How to Calculate Polychoric Correlation in R 
<h2><span class="orange">Point Estimate Calculator</span></h2>
A <b>point estimate</b> represents our “best guess” of a population parameter.
For example, a sample mean can be used as a point estimate of a population mean.
Similarly, a sample proportion can be used as a point estimate of a population proportion. However, there are several ways to calculate the point estimate of a population proportion, including:
<b>MLE Point Estimate</b>: x / n
<b>Wilson Point Estimate</b>: (x + z<sup>2</sup>/2) / (n + z<sup>2</sup>)
<b>Jeffrey Point Estimate</b>: (x + 0.5) / (n + 1)
<b>Laplace Point Estimate</b>: (x + 1) / (n + 2)
where <b>x</b> is the number of “successes” in the sample, <b>n</b> is the sample size or number of trials, and <b>z</b> is the z-score associated with the confidence level.
To find the best point estimate, simply enter in the values for the number of successes, number of trials, and confidence level in the boxes below and then click the “Calculate” button.
<label for="x"><b>Number of successes (x)</b></label>
<input type="number" id="x" value="14"><label for="n"><b>Number of trials (n)</b></label>
<input type="number" id="n" value="31"><label for="conf"><b>Confidence level (%)</b></label>
<input type="number" id="conf" value="95">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
Best Estimate = <b>0.45695</b>
MLE Point Estimate = 0.45161
Wilson Point Estimate = 0.45695
Jeffrey Point Estimate = 0.45313
Laplace Point Estimate = 0.45455
This calculator uses the following logic to determine which point estimate is best to use:
If <b>x / n ≤ 0.5</b>, use the Wilson Point Estimate.
Otherwise, if <b>x / n &lt; 0.9</b>, use the MLE Point Estimate.
Otherwise, if <b>x / n &lt; 1.0</b>, use the smaller of the Jeffrey Point Estimate or the Laplace Point Estimate.
Otherwise, if <b>x / n = 1.0</b>, use the Laplace Point Estimate.
<script>
function calc() {
//get input values
var x = document.getElementById('x').value*1;
var n = document.getElementById('n').value*1;
var conf = (1 - document.getElementById('conf').value/100)/2;
var z = Math.abs(jStat.normal.inv(conf, 0, 1));
//find estimates
var mle = x/n;
var wilson = (x - ((-z*z)/2)) / (n - (-z*z));
var jeffrey = (x - (-.5)) / (n - (-1));
var laplace = (x - (-1)) / (n - (-2));
//find best estimate
var val = x/n;
var best = x/n;
if (val <= 0.5) {
  best = wilson;
} else if (val < 0.9) {
  best = mle;
} else if (val < 1) {
  best = Math.min(laplace, jeffrey);
} else {
  best = laplace;
}
//output results
document.getElementById('best').innerHTML = best.toFixed(5);
document.getElementById('mle').innerHTML = mle.toFixed(5);
document.getElementById('wilson').innerHTML = wilson.toFixed(5);
document.getElementById('jeffrey').innerHTML = jeffrey.toFixed(5);
document.getElementById('laplace').innerHTML = laplace.toFixed(5);
}
</script>
<h2><span class="orange">How to Calculate Point Estimates in Excel (With Examples)</span></h2>
A  point estimate  represents a number that we calculate from sample data to estimate some population parameter. This serves as our best possible estimate of what the true population parameter may be.
The following table shows the point estimate that we use to estimate population parameters:
<table><tbody>
<tr>
<th style="text-align: left;"><b>Measurement</b></th>
<th><b>Population parameter</b></th>
<th><b>Point estimate</b></th>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: left;">μ (population mean)</td>
<td style="text-align: left;">x (sample mean)</td>
</tr>
<tr>
<td style="text-align: left;">Proportion</td>
<td style="text-align: left;">π (population proportion)</td>
<td style="text-align: left;">p (sample proportion)</td>
</tr>
</tbody></table>
Although a point estimate represents our best guess of a population parameter, it’s not guaranteed to exactly match the true population parameter.
For this reason, we often calculate confidence intervals as well – intervals that are likely to contain a population parameter with a certain level of confidence.
The following examples explain how to calculate point estimates and confidence intervals in Excel.
<h3>Example 1: Point Estimate for a Population Mean</h3>
Suppose we’re interested in calculating the mean weight of a population of turtles. To do so, we collect a random sample of 20 turtles:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/point1.png">
Our <b>point estimate</b> for the population mean is simply the sample mean, which turns out to be <b>300.3</b> pounds:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/point2.png">
We can then use the following formula to calculate a 95% confidence interval for the population mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/point3.png">
We are 95% confident that the true mean weight of turtles in this population is in the range <b>[296.96, 303.64]</b>.
We can confirm these results by using the  Confidence Interval Calculator .
<h3>Example 2: Point Estimate for a Population Proportion</h3>
Suppose we’re interested in calculating the proportion of turtles in a population that have spots on their shell. To do so, we collect a random sample of 20 turtles and find that 13 have spots.
Our <b>point estimate</b> for the proportion of turtles that have spots is <b>0.65</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/point4.png">
We can then use the following formula to calculate a 95% confidence interval for the population proportion:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/point5.png">
We are 95% confident that the true proportion of turtles in this population that have spots is in the range <b>[0.44, 0.86]</b>.
We can confirm these results by using the  Confidence Interval for Proportion Calculator .
<h2><span class="orange">How to Calculate Point Estimates in R (With Examples)</span></h2>
A  point estimate  represents a number that we calculate from sample data to estimate some population parameter. This serves as our best possible estimate of what the true population parameter may be.
The following table shows the point estimate that we use to estimate population parameters:
<table><tbody>
<tr>
<th style="text-align: left;"><b>Measurement</b></th>
<th><b>Population parameter</b></th>
<th><b>Point estimate</b></th>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: left;">μ (population mean)</td>
<td style="text-align: left;">x (sample mean)</td>
</tr>
<tr>
<td style="text-align: left;">Proportion</td>
<td style="text-align: left;">π (population proportion)</td>
<td style="text-align: left;">p (sample proportion)</td>
</tr>
</tbody></table>
The following examples explain how to calculate point estimates for a population mean and a population proportion in R.
<h3>Example 1: Point Estimate of Population Mean</h3>
Suppose we would like to estimate the mean height (in inches) of a certain type of plant in a certain field. We gather a simple random sample of 13 plants and measure the height of each plant.
The following code shows how to calculate the sample mean:
<b>#define data
data &lt;- c(8, 8, 9, 12, 13, 13, 14, 15, 19, 22, 23, 23, 24)
#calculate sample mean
mean(data, na.rm = TRUE)
[1] 15.61538</b>
The sample mean is <b>15.6 </b>inches. This represents our point estimate for the population mean.
We can also use the following code to calculate a 95% confidence interval for the population mean:
<b>#find sample size, sample mean, and sample standard deviation
n &lt;- length(data)
xbar &lt;- mean(data, na.rm = TRUE)
s &lt;- sd(data)
#calculate margin of error
margin &lt;- qt(0.975,df=n-1)*s/sqrt(n)
#calculate lower and upper bounds of confidence interval
low &lt;- xbar - margin
low
[1] 12.03575
high &lt;- xbar + margin
high
[1] 19.19502
</b>
The 95% confidence interval for the population mean is <b>[12.0, 19.2]</b> inches.
<h3>Example 2: Point Estimate of Population Proportion</h3>
Suppose we would like to estimate the proportion of people in a certain city that support a certain law. We survey a simple random sample of 20 citizens.
The following code shows how to calculate the sample proportion:
<b>#define data
data &lt;- c('Y', 'Y', 'Y', 'N', 'N', 'Y', 'Y', 'Y', 'N', 'Y',
          'N', 'Y', 'Y', 'N', 'N', 'Y', 'Y', 'Y', 'N', 'N')
#find total sample size
n &lt;- length(data)
#find number who responded 'Yes'
k &lt;- sum(data == 'Y') 
#find sample proportion
p &lt;- k/n
p
[1] 0.6
</b>
The sample proportion of citizens who support the law is <b>0.6</b>. This represents our point estimate for the population proportion.
We can also use the following code to calculate a 95% confidence interval for the population mean:
<b>#find total sample size
n &lt;- length(data)
#find number who responded 'Yes'
k &lt;- sum(data == 'Y') 
#find sample proportion
p &lt;- k/n
#calculate margin of error
margin &lt;- qnorm(0.975)*sqrt(p*(1-p)/n)
#calculate lower and upper bounds of confidence interval
low &lt;- p - margin
low
[1] 0.3852967
high &lt;- p + margin
high
[1] 0.8147033
</b>
The 95% confidence interval for the population proportion is <b>[0.39, 0.81]</b>.
<h2><span class="orange">What is a Point Estimate in Statistics?</span></h2>
Often in statistics we’re interested in measuring  population parameters  – numbers that describe some characteristic of an entire population.
Two of the most common population parameters are:
<b>1. Population mean:</b> the mean value of some variable in a population (e.g. the mean height of males in a certain city)
<b>2. Population proportion:</b> the proportion of some variable in a population (e.g. the proportion of residents in a county who support a certain law)
Although we’re interested in measuring these parameters, it’s usually too costly and time-consuming to actually go around and collect data on every individual in a population.
Instead, we take a random  sample  from the population and use data from the sample to estimate the population parameter.
The number that we use from the sample to estimate the population parameter is known as the <b>point estimate</b>. This serves as our best possible estimate of what the true population parameter may be.
The following table shows the point estimate that we use to <em>estimate</em> the population parameters:
<table><tbody>
<tr>
<th style="text-align: left;"><b>Measurement</b></th>
<th><b>Population parameter</b></th>
<th><b>Point estimate</b></th>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: left;">μ (population mean)</td>
<td style="text-align: left;">x (sample mean)</td>
</tr>
<tr>
<td style="text-align: left;">Proportion</td>
<td style="text-align: left;">π (population proportion)</td>
<td style="text-align: left;">p (sample proportion)</td>
</tr>
</tbody></table>
We are interested in calculating the population parameters but since it’s too time-consuming and costly to do, we instead use samples to calculate point estimates.
For example, suppose we want to estimate the mean weight of a certain species of turtle in Florida. Since there are thousands of turtles in Florida, it would be extremely time-consuming and costly to go around and weigh each individual turtle. Instead, we might take a  simple random sample  of 50 turtles and use the mean weight of the turtles in this sample to estimate the true population mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CImean1.png">
If the  sample mean  is 150.4 pounds, then our point estimate for the true population mean of the entire species would be 150.4 pounds.
<h3>The Importance of Representative Samples</h3>
When we collect a sample from a population, we ideally want the sample to be like a “mini version” of our population.
We say that a sample is  representative of a population  if the characteristics of the individuals in the sample closely matches the characteristics of the individuals in the overall population.
When this occurs, we can generalize the findings from the sample to the overall population with confidence and we can say that the point estimate from the sample is an <b>unbiased estimate</b> of the true population parameter.
<h3>Point Estimates & Confidence Intervals</h3>
Although a point estimate represents our best possible estimate of some true population parameter, it’s unlikely that it will <em>exactly</em> match the population parameter.
In our previous example, the mean weight of turtles in the sample is not guaranteed to exactly match the mean weight of turtles in the whole population. For example, we might just happen to pick a sample full of low-weight turtles or perhaps a sample full of heavy turtles.
So, to capture this uncertainty we can create a <b>confidence interval</b> – a range of values that is likely to contain a population parameter with a certain level of confidence.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/cie1.png">
For example, we may use our sample mean of 150.4 pounds to estimate the true average weight of a species of turtles. Our confidence interval would then be a range of values – perhaps 145 pounds to 155.8 pounds.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/cie2.png">
Our point estimate is our best estimate of the true population mean weight and the confidence interval provides a range of values that is likely to contain the true population mean weight.
You can read more about confidence intervals  here .
<h2><span class="orange">Poisson Confidence Interval Calculator</span></h2>
This calculator constructs a confidence interval for the mean of a  Poisson distribution  based on the number of observed events.
Simply fill in the values below and then click the “Calculate” button.
<label for="n">Observed Events</label> <input id="n" min="0" type="number" value="14">
<label for="sig">Confidence Level</label> <input id="sig" min="0" type="number" value="0.95">
<input id="buttonCalc" type="button" value="Calculate" onclick="pvalue()">
95% Confidence Interval = [7.65393, 23.48962]
<script>
function pvalue() {
//get input values
var n = document.getElementById('n').value*1;
var sig = document.getElementById('sig').value*1;
var confOut = sig*100;
var alpha = 1-sig;
//find C.I. boundaries
var lower = 0.5*jStat.chisquare.inv(alpha/2, 2*n);
var upper = 0.5*jStat.chisquare.inv(1-(alpha/2), 2*n-(-2));
document.getElementById('confOut').innerHTML = confOut.toFixed(0);
document.getElementById('lower').innerHTML = lower.toFixed(5);
document.getElementById('upper').innerHTML = upper.toFixed(5);
}
</script>
<h2><span class="orange">How to Calculate a Poisson Confidence Interval (Step-by-Step)</span></h2>
The  Poisson distribution  is a probability distribution that is used to model the probability that a certain number of events occur during a fixed time interval when the events are known to occur independently and with a constant mean rate.
While it’s helpful to know the mean number of occurrences of some Poisson process, it can be even more helpful to have a  confidence interval  around the mean number of occurrences.
For example, suppose we collect data at a call center on a random day and find that the mean number of calls per hour is 15.
Since we only collected data on one single day, we can’t be certain that the call center receives 15 calls per hour, on average, throughout the entire year.
However, we can use the following formula to calculate a confidence interval for the mean number of calls per hour:
<b>Poisson Confidence Interval Formula</b>
 
Confidence Interval = [0.5*X<sup>2</sup><sub>2N, α/2</sub>,  0.5*X<sup>2</sup><sub>2(N+1), 1-α/2</sub>]
 
where:
 
X<sup>2</sup>: Chi-Square Critical Value
N: The number of observed events
α: The significance level
The following step-by-step example illustrates how to calculate a 95% Poisson confidence interval in practice.
<h3>Step 1: Count the Observed Events</h3>
Suppose we calculate the mean number of calls per hour at a call center to be 15. Thus, <b>N = 15</b>.
And since we’re calculating a 95% confidence interval, we’ll use <b>α = .05</b> in the following calculations.
<h3>Step 2: Find the Lower Confidence Interval Bound</h3>
The lower confidence interval bound is calculated as:
Lower bound = 0.5*X<sup>2</sup><sub>2N, α/2</sub>
Lower bound = 0.5*X<sup>2</sup><sub>2(15), .975</sub>
Lower bound = 0.5*X<sup>2</sup><sub>30, .975</sub>
Lower bound = 0.5*16.791
Lower bound = <b>8.40</b>
<em><b>Note:</b> We used the  Chi-Square Critical Value Calculator  to compute X<sup>2</sup><sub>30, .975</sub>.</em>
<h3>Step 3: Find the Upper Confidence Interval Bound</h3>
The upper confidence interval bound is calculated as:
Upper bound = 0.5*X<sup>2</sup><sub>2(N+1), 1-α/2</sub>
Upper bound = 0.5*X<sup>2</sup><sub>2(15+1), .025</sub>
Upper bound = 0.5*X<sup>2</sup><sub>32, .025</sub>
Upper bound = 0.5*49.48
Upper bound = <b>24.74</b>
<em><b>Note:</b> We used the  Chi-Square Critical Value Calculator  to compute X<sup>2</sup><sub>32, .025</sub>.</em>
<h3>Step 4: Find the Confidence Interval</h3>
Using the lower and upper bounds previously computed, our 95% Poisson confidence interval turns out to be:
95% C.I. = <b>[8.40, 24.74]</b>
This means we are 95% confident that the true mean number of calls per hour that the call center receives is between 8.40 calls and 24.74 calls.
<h3>Bonus: Poisson Confidence Interval Calculator</h3>
Feel free to use this  Poisson Confidence Interval Calculator  to automatically compute a Poisson confidence interval.
For example, here’s how to use this calculator to find the Poisson confidence interval we just computed manually:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/poissonCI1.png">
Notice that the results match the confidence interval that we computed manually.
<h2><span class="orange">The Four Assumptions of the Poisson Distribution</span></h2>
The  Poisson distribution  is a probability distribution that is used to model the probability that a certain number of events occur during a fixed time interval.
The Poisson distribution is appropriate to use if the following four assumptions are met:
<b>Assumption 1: The number of events can be counted.</b>
We assume that the number of “events” that can occur during a given time interval can be counted and can take on the values of 0, 1, 2, 3, … etc.
<b>Assumption 2: The occurrence of events are independent.</b>
We assume that the occurrence of one event does not affect the probability that another event will occur.
<b>Assumption 3: The average rate at which events occur can be calculated.</b>
We assume that the average rate at which events occur during a given time interval can be calculated and that it is constant over each sub-interval.
<b>Assumption 4: Two events cannot occur at exactly the same instant in time.</b>
We assume that at each extremely small sub-interval exactly one event occurs or does not occur.
The following examples show various scenarios that meet the assumptions of a Poisson distribution.
<h3>Example 1: Number of Arrivals at a Restaurant</h3>
The number of customers that arrive at a restaurant each day can be modeled using a Poisson distribution.
This scenario meets each of the assumptions of a Poisson distribution:
<b>Assumption 1: The number of events can be counted.</b>
The number of customers that arrive at a restaurant each day can be counted (e.g. 200 customers).
<b>Assumption 2: The occurrence of events are independent.</b>
The arrival of one customer does not affect the arrival of another customer.
<b>Assumption 3: The average rate at which events occur can be calculated.</b>
We can easily collect data on the average number of customers that enter the restaurant each day.
<b>Assumption 4: Two events cannot occur at exactly the same instant in time.</b>
Two customers cannot technically enter a restaurant at <em>exactly</em> the same moment in time.
<h3>Example 2: Number of Network Failures per Week</h3>
The number of network failures that a tech company experiences each week can be modeled using a Poisson distribution.
This scenario meets each of the assumptions of a Poisson distribution:
<b>Assumption 1: The number of events can be counted.</b>
The number of network failures each week can be counted (e.g. 3 network failures).
<b>Assumption 2: The occurrence of events are independent.</b>
It’s assumed that the occurrence of one network failure does not affect the probability that another network failure will occur.
<b>Assumption 3: The average rate at which events occur can be calculated.</b>
We can easily collect data on the average number of network failures that occur each week.
<b>Assumption 4: Two events cannot occur at exactly the same instant in time.</b>
Two network failures cannot occur at the exact same moment in time – only one network failure can occur at once.
<h2><span class="orange">Poisson Distribution Calculator</span></h2>
The  Poisson distribution  is one of the most commonly used distributions in statistics.
This calculator finds Poisson probabilities associated with a provided Poisson mean and a value for a random variable.
<label for="mean"><b>λ</b> (average rate of success)</label>
<input type="number" id="mean" value="5">
<label for="x"><b>x</b> (random variable)</label>
<input type="number" id="x" value="3">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
P(X = 3): 0.14037
P(X &lt; 3): 0.12465
P(X ≤ 3): 0.26503
P(X > 3): 0.73497
P(X ≥ 3): 0.87535
<script>
function calc() {
//get input values
var x = +document.getElementById('x').value;
var mean = +document.getElementById('mean').value;
//calculate SE
var exactP = jStat.poisson.pdf(x, mean);
var lessP = jStat.poisson.cdf(x-1, mean);
var lessEP = jStat.poisson.cdf(x, mean);
var greatP = 1-jStat.poisson.cdf(x, mean);
var greatEP = 1-jStat.poisson.cdf(x-1, mean);
//output probabilities
document.getElementById('exactP').innerHTML = exactP.toFixed(5);
document.getElementById('lessP').innerHTML = lessP.toFixed(5);
document.getElementById('lessEP').innerHTML = lessEP.toFixed(5);
document.getElementById('greatP').innerHTML = greatP.toFixed(5);
document.getElementById('greatEP').innerHTML = greatEP.toFixed(5);
document.getElementById('x1').innerHTML = x;
document.getElementById('x2').innerHTML = x;
document.getElementById('x3').innerHTML = x;
document.getElementById('x4').innerHTML = x;
document.getElementById('x5').innerHTML = x;
}
</script>
<h2><span class="orange">How to Use the Poisson Distribution in Excel</span></h2>
The  Poisson distribution  is one of the most commonly used distributions in statistics.
In Excel, we can use the <b>POISSON.DIST()</b> function to find the probability that an event occurs a certain number of times during a given interval, based on knowing the mean number of times the event occurs during the given interval.
The syntax for <b>POISSON.DIST </b>is as follows:
<b>POISSON.DIST</b>(x, mean, cumulative)
<b>x:</b> number of occurrences during a given interval
<b>mean: </b>mean number of occurrences during a given interval
<b>cumulative: </b>TRUE returns the cumulative probability; FALSE returns the exact probability
The following examples illustrate how to solve Poisson probability questions using <b>POISSON.DIST</b>.
<h3>Example 1</h3>
<em>A hardware store sells 3 hammers per day on average. What is the probability that they will sell 5 hammers on a given day?</em>
In this example, we can plug in the following numbers to the <b>POISSON.DIST</b> function:
<b>x:</b> number of occurrences during a given interval (selling <b>5 </b>hammers)
<b>mean: </b>mean number of occurrences during a given interval (they sell <b>3 </b>on average)
<b>cumulative: FALSE</b> (we want an exact probability, not a cumulative probability)
To answer this question, we can use the following formula in Excel: <b>POISSON.DIST(5, 3, FALSE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/poisson1.jpg">
The probability that the store sells 5 hammers in a given day is <b>0.100819</b>.
<h3>Example 2</h3>
<em>A certain store sells 15 cans of tuna per day on average. What is the probability that this store sells more than 20 cans of tuna in a given day?</em>
In this example, we can plug in the following numbers to the <b>POISSON.DIST</b> function:
<b>x:</b> number of occurrences during a given interval (selling <b>20 </b>cans)
<b>mean: </b>mean number of occurrences during a given interval (they sell <b>15</b> cans on average)
<b>cumulative: TRUE </b>(we want a cumulative probability, not an exact probability)
To answer this question, we can use the following formula in Excel: <b>1 – POISSON.DIST(20, 15, TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/poisson2.jpg">
The probability that the store sells more than 20 cans of tuna in a given day is <b>0.082971</b>.
<em><b>Note:</b> In this example, POISSON.DIST(20, 15, TRUE) returns the probability that the store sells 20 or fewer cans of tuna. So, to find the probability that the store sells more than 20 cans, we simply use 1 – POISSON.DIST(20, 15, TRUE).</em>
<h3>Example 3</h3>
<em>A certain sporting goods store sells seven basketballs per day on average. What is the probability that this store sells four or less basketballs in a given day?</em>
In this example, we can plug in the following numbers to the <b>POISSON.DIST</b> function:
<b>x:</b> number of occurrences during a given interval (selling <b>4 </b>basketballs)
<b>mean: </b>mean number of occurrences during a given interval (they sell <b>7 </b>on average)
<b>cumulative: TRUE </b>(we want a cumulative probability, not an exact probability)
To answer this question, we can use the following formula in Excel: <b>POISSON.DIST(4, 7, TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/poisson3.jpg">
The probability that the store sells 4 or fewer basketballs in a given day is <b>0.172992</b>.
<h3>Example 4</h3>
<em>A certain store sells twelve pineapples per day on average. What is the probability that this store sells between 12 and 14 pineapples in a given day?</em>
In this example, we can plug in the following numbers to the <b>POISSON.DIST</b> function:
<b>x:</b> number of occurrences during a given interval (selling <b>between 12 and 14 </b>pineapples)
<b>mean: </b>mean number of occurrences during a given interval (they sell <b>12 </b>on average)
<b>cumulative: TRUE </b>(we want a cumulative probability, not an exact probability)
To answer this question, we can use the following formula in Excel:
<b>POISSON.DIST(14, 12, TRUE) – POISSON.DIST(11, 12, TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/poisson4.jpg">
The probability that the store sells between 12 and 14 pineapples in a given day is <b>0.310427</b>.
<em><b>Note:</b> In this example, POISSON.DIST(14, 12, TRUE) returns the probability that the store sells 14 or fewer pineapples and POISSON.DIST(11, 12, TRUE) returns the probability that the store sells 11 or fewer pineapples . So, to find the probability that the store sells between 12 and 14 titles, we subtract the difference so that we are actually finding the probability that the store sells either 12, 13, or 14 pineapples.</em>
An alternative way to solve this problem is to simply find the individual probabilities of selling 12, 13, and 14 pineapples, and then add up these probabilities:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/poisson5.jpg">
This gives us the same probability as the previous method.
<h2><span class="orange">How to Create a Poisson Distribution Graph in Excel</span></h2>
The  Poisson distribution  describes the probability of obtaining <em>k</em> successes during a given time interval.
If a  random variable  <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
The following example explains how to create a Poisson distribution graph in Excel.
<h3>Example: Poisson Distribution Graph in Excel</h3>
To create a Poisson distribution graph, we need to first decide on a value for λ (mean number of successes):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph1.png">
Next, we need to create a column for each possible number of successes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph2.png">
Note that we chose k = 10 possible successes. We could have chosen more, but the probabilities become very small for values greater than 10, as we’ll see later in this post.
Next, we can use the <b>POISSON.DIST()</b> function to calculate the Poisson probability for the first number of successes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph3.png">
We can then copy and paste this formula to the remaining cells in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph4.png">
Lastly, we can highlight each of the Poisson probabilities, then click the <b>Insert</b> tab along the top ribbon, then click the <b>Insert Column or Bar Chart</b> icon in the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph5-1.png">
The x-axis of the graph shows the number of successes and the y-axis shows the corresponding probability of that many successes.
Note that if you change the value for λ (mean number of successes), the graph will automatically change to reflect the new probabilities.
For example, if we change the λ value to 4, the Poisson probabilities and the graph will automatically update:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/poissonGraph6.png">
<h2><span class="orange">How to Use the Poisson Distribution in Python</span></h2>
The  Poisson distribution  describes the probability of obtaining <em>k</em> successes during a given time interval.
If a  random variable  <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
This tutorial explains how to use the Poisson distribution in Python.
<h3>How to Generate a Poisson Distribution</h3>
You can use the <b>poisson.rvs(mu, size)</b> function to generate random values from a Poisson distribution with a specific mean value and sample size:
<b>from scipy.stats import poisson
#generate random values from Poisson distribution with mean=3 and sample size=10
poisson.rvs(mu=3, size=10)
array([2, 2, 2, 0, 7, 2, 1, 2, 5, 5])
</b>
<h3>How to Calculate Probabilities Using a Poisson Distribution</h3>
You can use the <b>poisson.pmf(k, mu)</b> and <b>poisson.cdf(k, mu)</b> functions to calculate probabilities related to the Poisson distribution.
<b>Example 1: Probability Equal to Some Value</b>
A store sells 3 apples per day on average. What is the probability that they will sell 5 apples on a given day? 
<b>from scipy.stats import poisson
#calculate probability
poisson.pmf(k=5, mu=3)
0.100819
</b>
The probability that the store sells 5 apples in a given day is <b>0.100819</b>.
<b>Example 2: Probability Less than Some Value</b>
A certain store sells seven footballs per day on average. What is the probability that this store sells four or less footballs in a given day?
<b>from scipy.stats import poisson
#calculate probability
poisson.cdf(k=4, mu=7)
0.172992</b>
The probability that the store sells four or less footballs in a given day is <b>0.172992</b>.
<b>Example 3: Probability Greater than Some Value</b>
A certain store sells 15 cans of tuna per day on average. What is the probability that this store sells more than 20 cans of tuna in a given day?
<b>from scipy.stats import poisson
#calculate probability
1-poisson.cdf(k=20, mu=15)
0.082971</b>
The probability that the store sells more than 20 cans of tuna in a given day is <b>0.082971</b>.
<h3>How to Plot a Poisson Distribution</h3>
You can use the following syntax to plot a Poisson distribution with a given mean:
<b>from scipy.stats import poisson
import matplotlib.pyplot as plt
#generate Poisson distribution with sample size 10000
x = poisson.rvs(mu=3, size=10000)
#create plot of Poisson distribution
plt.hist(x, density=True, edgecolor='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/python1.png">
<h2><span class="orange">5 Real-Life Examples of the Poisson Distribution</span></h2>
The  Poisson distribution  is a probability distribution that is used to model the probability that a certain number of events occur during a fixed time interval when the events are known to occur independently and with a constant mean rate.
In this article we share 5 examples of how the Poisson distribution is used in the real world.
<h3>Example 1: Calls per Hour at a Call Center</h3>
Call centers use the Poisson distribution to model the number of expected calls per hour that they’ll receive so they know how many call center reps to keep on staff.
For example, suppose a given call center receives 10 calls per hour. We can use a  Poisson distribution calculator  to find the probability that a call center receives 0, 1, 2, 3 … calls in a given hour:
P(X = 0 calls) = <b>0.00005</b>
P(X = 1 call) = <b>0.00045</b>
P(X = 2 calls) = <b>0.00227</b>
P(X = 3 calls) = <b>0.00757</b>
And so on.
This gives call center managers an idea of how many calls they’re likely to receive per hour and enables them to manage employee schedules based on the number of expected calls.
<h3>Example 2: Number of Arrivals at a Restaurant</h3>
Restaurants use the Poisson distribution to model the number of expected customers that will arrive at the restaurant per day.
For example, suppose a given restaurant receives an average of 100 customers per day. We can use the  Poisson distribution calculator  to find the probability that the restaurant receives more than a certain number of customers:
P(X > 110 customers) = <b>0.14714</b>
P(X > 120 customers) = <b>0.02267</b>
P(X > 130 customers) = <b>0.00171</b>
And so on.
This gives restaurant managers an idea of the likelihood that they’ll receive more than a certain number of customers in a given day.
<h3>Example 3: Number of Website Visitors per Hour</h3>
Website hosting companies use the Poisson distribution to model the number of expected visitors per hour that websites will receive.
For example, suppose a given website receives an average of 20 visitors per hour. We can use the  Poisson distribution calculator  to find the probability that the website receives more than a certain number of visitors in a given hour:
P(X > 25 visitors) = <b>0.11218</b>
P(X > 30 visitors) = <b>0.01347</b>
P(X > 35 visitors) = <b>0.00080</b>
And so on.
This gives hosting companies an idea of how much bandwidth to provide to different websites to ensure that they’ll be able to handle a certain number of visitors each hour.
<h3>Example 4: Number of Bankruptcies Filed per Month</h3>
Banks use the Poisson distribution to model the number of expected customer bankruptcies per month.
For example, suppose a given bank has an average of 3 bankruptcies filed by customers each month. We can use the  Poisson distribution calculator  to find the probability that the bank receives a specific number of bankruptcy files in a given month:
P(X = 0 bankruptcies) = <b>0.04979</b>
P(X = 1 bankruptcy) = <b>0.14936</b>
P(X = 2 bankruptcies) = <b>0.22404</b>
And so on.
This gives banks an idea of how much reserve cash to keep on hand in case a certain number of bankruptcies occur in a given month.
<h3>Example 5: Number of Network Failures per Week</h3>
Technology companies use the Poisson distribution to model the number of expected network failures per week.
For example, suppose a given company experiences an average of 1 network failure per week. We can use the  Poisson distribution calculator  to find the probability that the company experiences a certain number of network failures in a given week:
P(X = 0 failures) = <b>0.36788</b>
P(X = 1 failure) = <b>0.36788</b>
P(X = 2 failures) = <b>0.18394</b>
And so on.
This gives the company an idea of how many failures are likely to occur each week.
<h2><span class="orange">An Introduction to the Poisson Distribution</span></h2>
The <b>Poisson distribution </b>is one of the most popular distributions in statistics.
To understand the Poisson distribution, it helps to first understand Poisson experiments.
<h2>Poisson Experiments</h2>
A <b>Poisson experiment</b> is an experiment that has the following properties:
The number of successes in the experiment can be counted.
The mean number of successes that occurs during a specific interval of time (or space) is known.
Each outcome is independent.
The probability that a success will occur is proportional to the size of the interval.
One example of a Poisson experiment is the number of births per hour at a given hospital. For example, suppose a particular hospital experiences an average of 10 births per hour. This is a Poisson experiment because it has the following four properties:
The number of successes in the experiment can be counted – We can count the number of births.
The mean number of successes that occurs during a specific interval of time is known – It is known that an average of 10 births per hour occur.
Each outcome is independent – The probability that one mother gives birth during a given hour is independent of the probability of another mother giving birth.
The probability that a success will occur is proportional to the size of the interval – the longer the interval of time, the higher the probability that a birth will occur.
We can use the Poisson distribution to answer questions about probabilities regarding this Poisson experiment such as:
What is the probability that more than 12 births occur in a given hour?
What is the probability that less than 5 births occur in a given hour?
What is the probability that between 8 to 11 births occur in a given hour?
<h2>The Poisson Distribution</h2>
The <b>Poisson distribution</b> describes the probability of obtaining <em>k</em> successes during a given time interval.
If a  random variable  <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
For example, suppose a particular hospital experiences an average of 2 births per hour. We can use the formula above to determine the probability of experiencing 0, 1, 2, 3 births, etc. in a given hour:
<b>P(X=0) </b>= 2<sup>0</sup> * e<sup>– 2</sup> / 0! = <b>0.1353</b>
<b>P(X=1) </b>= 2<sup>1</sup> * e<sup>– 2</sup> / 1! = <b>0.2707</b>
<b>P(X=2) </b>= 2<sup>2</sup> * e<sup>– 2</sup> / 2! = <b>0.2707</b>
<b>P(X=3) </b>= 2<sup>3</sup> * e<sup>– 2</sup> / 3! = <b>0.1805</b>
We can calculate the probability for any number of births up to infinity. We create then create a simple histogram to visualize this probability distribution:
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonDist1.png"></figure>
<h2>Calculating Cumulative Poisson Probabilities</h2>
It’s straightforward to calculate a single Poisson probability (e.g. the probability of a hospital experiencing 3 births during a given hour) using the formula above, but to calculate cumulative Poisson probabilities we need to add individual probabilities.
For example, suppose we want to know the probability that the hospital experiences 1 or fewer births in a given hour. We would use the following formula to calculate this probability:
<b>P(X≤1)</b> = P(X=0) + P(X=1) = 0.1353 + 0.2707 = <b>0.406</b>
This is known as a <b>cumulative probability</b> because it involves adding more than one probability. We can calculate the cumulative probability of experiencing <em>k</em> or less births in a given hour using a similar formula:
<b>P(X≤0)</b> = P(X=0) = <b>0.1353</b>
<b>P(X≤1)</b> = P(X=0) + P(X=1) = 0.1353 + 0.2707 = <b>0.406</b>
<b>P(X≤2)</b> = P(X=0) + P(X=1) + P(X=2) =0.1353 + 0.2707 + 0.2707 = <b>0.6767</b>
We can calculate these cumulative probabilities for any number of births up to infinity. We can then create a histogram to visualize this cumulative probability distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonDist2.png">
 
<h2>Properties of the Poisson Distribution</h2>
The Poisson distribution has the following properties:
The mean of the distribution is<b> λ</b>.
The variance of the distribution is also <b>λ</b>.
The standard deviation of the distribution is <b>√λ</b>.
For example, suppose a hospital experiences an average of 2 births per hour.
The mean number of births we would expect in a given hour is λ = 2 births.
The variance in the number of births we would expect is λ = 2 births.
<h2>Poisson Distribution Practice Problems</h2>
Use the following practice problems to test your knowledge of the Poisson distribution.
<em><b>Note: </b>We will use the  Poisson Distribution Calculator  to calculate the answers to these questions.</em>
<b>Problem 1</b>
<b>Question: </b>It is known that a certain website makes 10 sales per hour. In a given hour, what is the probability that the site makes exactly 8 sales?
<b>Answer:</b> Using the Poisson Distribution Calculator with λ = 10 and x = 8, we find that P(X=8) = <b>0.1126</b>.
<b>Problem 2</b>
<b>Question: </b>It is known that a certain realtor makes an average of 5 sales per month. In a given month, what is the probability that she makes more than 7 sales?
<b>Answer:</b> Using the Poisson Distribution Calculator with λ = 5 and x = 7, we find that P(X>7) = <b>0.13337</b>.
<b>Problem 3</b>
<b>Question: </b>It is known that a certain hospital experience 4 births per hour. In a given hour, what is the probability that 4 or less births occur?
<b>Answer:</b> Using the Poisson Distribution Calculator with λ = 4 and x = 4, we find that P(X≤4) = <b>0.62884</b>.
<h2>Additional Resources</h2>
The following articles explain how to work with the Poisson distribution in different statistical software:
 How to Use the Poisson Distribution in R 
 How to Use the Poisson Distribution in Excel 
 How to Calculate Poisson Probabilities on a TI-84 Calculator 
 Real-Life Examples of the Poisson Distribution 
 Poisson Distribution Calculator 
<h2><span class="orange">How to Calculate Poisson Probabilities on a TI-84 Calculator</span></h2>
The  Poisson distribution  is one of the most commonly used distributions in all of statistics. This tutorial explains how to use the following functions on a TI-84 calculator to find Poisson probabilities:
<b>poissonpdf(mean, x) </b>returns the probability associated with the Poisson pdf.
<b>poissoncdf(mean, x) </b>returns the cumulative probability associated with the Poisson cdf.
where:
<b>mean </b>= mean number of “successes”
<b>x </b>= specific number of “successes”
Both of these functions can be accessed on a TI-84 calculator by pressing 2nd and then pressing vars. This will take you to a <b>DISTR </b>screen where you can then use <b>poissonpdf() </b>and <b>poissoncdf()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/poissonTI1.png">
The following examples illustrate how to use these functions to answer different questions.
<h3>Example 1: Poisson probability of exactly x successes</h3>
<b>Question: </b>A hardware store sells 3 hammers per day on average. What is the probability that they will sell 5 hammers on a given day?
<b>Answer: </b>Use the function poissonpdf(mean, x):
<b>poissonpdf(3, 5) = 0.1008</b>
<h3>Example 2: Poisson probability of less than x successes</h3>
<b>Question: </b>A hardware store sells 3 hammers per day on average. What is the probability that they will sell less than 5 hammers on a given day?
<b>Answer: </b>Use the function poissoncdf(mean, x-1):
<b>poissoncdf(3, 4) = 0.8153</b>
<h3>Example 3: Poisson probability of at most x successes</h3>
<b>Question: </b>A hardware store sells 3 hammers per day on average. What is the probability that they will sell at most 5 hammers on a given day?
<b>Answer: </b>Use the function poissoncdf(mean, x):
<b>poissoncdf(3, 5) = 0.9161</b>
<h3>Example 4: Poisson probability of more than x successes</h3>
<b>Question: </b>A hardware store sells 3 hammers per day on average. What is the probability that they will sell more than 5 hammers on a given day?
<b>Answer: </b>Use the function 1 – poissoncdf(mean, x):
<b>1 – poissoncdf(3, 5) = 0.0839</b>
<h3>Example 5: Poisson probability of at least x successes</h3>
<b>Question: </b>A hardware store sells 3 hammers per day on average. What is the probability that they will sell at least 5 hammers on a given day?
<b>Answer: </b>Use the function 1 – poissoncdf(mean, x-1):
<b>1 – poissoncdf(3, 4) = 0.1847</b>
<h2><span class="orange">A Gentle Introduction to Poisson Regression for Count Data</span></h2>
<b>Regression</b> is a statistical method that can be used to determine the relationship between one or more predictor variables and a  response variable .
<b>Poisson regression</b> is a special type of regression in which the response variable consists of “count data.” The following examples illustrate cases where Poisson regression could be used:
<b>Example 1: </b>Poisson regression can be used to examine the number of students who graduate from a specific college program based on their GPA upon entering the program and their gender. In this case, “number of students who graduate” is the response variable, “GPA upon entering the program” is a continuous predictor variable, and “gender” is a categorical predictor variable.
<b>Example 2: </b>Poisson regression can be used to examine the number of traffic accidents at a particular intersection based on weather conditions (“sunny”, “cloudy”, “rainy”) and whether or not a special event is taking place in the city (“yes” or “no”). In this case, “number of traffic accidents” is the response variable, while “weather conditions” and “special event” are both categorical predictor variables.
<b>Example 3: </b>Poisson regression can be used to examine the number of people ahead of you in line at a store based on time of day, day of the week, and whether or not there is a sale taking place (“yes” or “no”). In this case, “number of people ahead of you in line” is the response variable, “time of day” and “day of week” are both continuous predictor variables, and “sale taking place” is a categorical predictor variable.
<b>Example 4: </b>Poisson regression can be used to examine the number of people who finish a triathlon based on weather conditions (“sunny”, “cloudy”, “rainy”) and difficulty of the course (“easy”, “moderate”, “difficult”). In this case, “number of people who finish” is the response variable, while “weather conditions” and “difficulty of the course” are both categorical predictor variables.
Conducting a Poisson regression will allow you to see which predictor variables (if any) have a statistically significant effect on the response variable.
For continuous predictor variables you will be able to interpret how a one unit increase or decrease in that variable is associated with a percentage change in the counts of the response variable (e.g. “each additional point increase in GPA is associated with a 12.5% increase in the number of students who graduate”).
For categorical predictor variables you will be able to interpret the percentage change in counts of one group (e.g. number of people who finish a triathlon in sunny weather) compared to another group (e.g. number of people who finish a triathlon in rainy weather).
<h2>Assumptions of Poisson Regression</h2>
Before we can conduct a Poisson regression, we need to make sure the following assumptions are met so that our results from the Poisson regression are valid:
<b>Assumption 1:</b> <b>The response variable consists of count data.</b> In traditional linear regression, the response variable consists of continuous data. To use Poisson regression, however, our response variable needs to consists of count data that include integers of 0 or greater (e.g. 0, 1, 2, 14, 34, 49, 200, etc.). Our response variable cannot contain negative values.
<b>Assumption 2: Observations are independent. </b>Each  observation  in the dataset should be independent of one another. This means that one observation should not be able to provide any information about a different observation.
<b>Assumption 3: The distribution of counts follows a Poisson distribution.</b> As a result, the observed and expected counts should be similar. One simple way to test for this is to plot the expected and observed counts and see if they are similar.
<b>Assumption 4: The mean and variance of the model are equal.</b> This is a result of the assumption that the distribution of counts follows a Poisson distribution. For a Poisson distribution the variance has the same value as the mean. If this assumption is satisfied, then you have <b>equidispersion</b>. However, this assumption is often violated as overdispersion is a common problem.
<h2>Example: Poisson Regression in R</h2>
Now we will walk through an example of how to conduct Poisson regression in R.
<h3>Background</h3>
Suppose we want to know how many scholarship offers a high school baseball player in a given county receives based on their school division (“A”, “B”, or “C”) and their college entrance exam score (measured from 0 to 100).
The following code creates the dataset we will work with, which includes data on 100 baseball players:
<b>#make this example reproducible
set.seed(1)
#create dataset
data &lt;- data.frame(offers = c(rep(0, 50), rep(1, 30), rep(2, 10), rep(3, 7), rep(4, 3)),   division = sample(c("A", "B", "C"), 100, replace = TRUE),   exam = c(runif(50, 60, 80), runif(30, 65, 95), runif(20, 75, 95)))</b>
<h3>Understanding the Data</h3>
Before we actually fit the Poisson regression model to this dataset, we can get a better understanding of the data by viewing the first few lines of the dataset and by using the <b> dplyr  </b>library to run some summary statistics:
<b>#view dimensions of dataset
dim(data)
#[1] 100   3
#view first six lines of dataset
head(data)
#  offers division     exam
#1      0        A 73.09448
#2      0        B 67.06395
#3      0        B 65.40520
#4      0        C 79.85368
#5      0        A 72.66987
#6      0        C 64.26416
#view summary of each variable in dataset
summary(data)
#     offers     division      exam      
# Min.   :0.00   A:27     Min.   :60.26  
# 1st Qu.:0.00   B:38     1st Qu.:69.86  
# Median :0.50   C:35     Median :75.08  
# Mean   :0.83            Mean   :76.43  
# 3rd Qu.:1.00            3rd Qu.:82.87  
# Max.   :4.00            Max.   :93.87  
#view mean exam score by number of offers
library(dplyr)
data %>%
  group_by(offers) %>%
  summarise(mean_exam = mean(exam))
#  A tibble: 5 x 2
#  offers mean_exam
#        
#1      0      70.0
#2      1      80.8
#3      2      86.8
#4      3      83.9
#5      4      87.9</b>
From the output above we can observe the following:
There are 100 rows and 3 columns in the dataset
The minimum number of offers received by a player was zero, the max was four, and the mean was 0.83.
In this dataset, there are 27 players from division “A”, 38 players from division “B”, and 35 players from division “C.”
The minimum exam score was a 60.26, the max was 93.87, and the mean was 76.43.
In general, players who received more scholarship offers tended to earn higher exam scores (e.g. the mean exam score for players who received 0 offers was 70.0 and the mean exam score for players who received 4 offers was 87.9).
We can also create a histogram to visualize the number of offers received by players based on division:
<b>#load <em>ggplot2</em> package
library(ggplot2)
#create histogram
ggplot(data, aes(offers, fill = division)) +
  geom_histogram(binwidth=.5, position="dodge")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/poissonReg1.jpg">
We can see that most players received either zero or one offer. This is typical for datasets that follow  Poisson distributions : a decent chunk of response values are zero.
<h3>Fitting the Poisson Regression Model</h3>
Next, we can fit the model using the <b>glm()</b> function and specifying that we’d like to use <b>family = “poisson”</b> for the model:
<b>#fit the model
model &lt;- glm(offers ~ division + exam, family = "poisson", data = data)
#view model output
summary(model)
#Call:
#glm(formula = offers ~ division + exam, family = "poisson", data = data)
#
#Deviance Residuals: 
#    Min       1Q   Median       3Q      Max  
#-1.2562  -0.8467  -0.5657   0.3846   2.5033  
#
#Coefficients:
#            Estimate Std. Error z value Pr(>|z|)    
#(Intercept) -7.90602    1.13597  -6.960 3.41e-12 ***
#divisionB    0.17566    0.27257   0.644    0.519    
#divisionC   -0.05251    0.27819  -0.189    0.850    
#exam         0.09548    0.01322   7.221 5.15e-13 ***
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#(Dispersion parameter for poisson family taken to be 1)
#
#    Null deviance: 138.069  on 99  degrees of freedom
#Residual deviance:  79.247  on 96  degrees of freedom
#AIC: 204.12
#
#Number of Fisher Scoring iterations: 5
</b>
From the output we can observe the following:
The Poisson regression coefficients, the standard error of the estimates, the z-scores, and the corresponding p-values are all provided.
The coefficient for <em>exam </em>is <b>0.09548</b>, which indicates that the expected log count for number of offers for a one-unit increase in <em>exam </em>is <b>0.09548</b>. An easier way to interpret this is to take the exponentiated value, that is <b>e<sup>0.09548</sup></b> = <b>1.10</b>. This means there is a 10% increase in the number of offers received for each additional point scored on the entrance exam.
The coefficient for <em>divisionB </em>is <b>0.1756</b>, which indicates that the expected log count for number of offers for a player in division B is <b>0.1756 </b>higher than for a player in division A. An easier way to interpret this is to take the exponentiated value, that is <b>e<sup>0.1756 </sup></b> = <b>1.19</b>. This means players in division B receive 19% more offers than players in division A. Note that this difference is not statistically significant (p = 0.519).
The coefficient for <em>divisionC </em>is <b>-0.05251</b>, which indicates that the expected log count for number of offers for a player in division C is <b>0.05251 </b><i>lower </i>than for a player in division A. An easier way to interpret this is to take the exponentiated value, that is <b>e<sup>0.05251</sup></b> = <b>0.94</b>. This means players in division C receive 6% fewer offers than players in division A. Note that this difference is not statistically significant (p = 850).
Information on the deviance of the model is also provided. We are most interested in the <em>residual deviance</em>, which has a value of <b>79.247</b> on <b>96</b> degrees of freedom. Using these numbers, we can conduct a Chi-Square goodness of fit test to see if the model fits the data. The following code illustrates how to conduct this test:
<b>pchisq(79.24679, 96, lower.tail = FALSE)
#[1] 0.8922676
</b>
The p-value for this test is <b>0.89</b>, which is much larger than the significance level of 0.05. We can conclude that the data fits the model reasonably well.
<h3>Visualizing the Results</h3>
We can also create a plot that shows the predicted number of scholarship offers received based on division and entrance exam score using the following code:
<b>#find predicted number of offers using the fitted Poisson regression model
data$phat &lt;- predict(model, type="response")
#create plot that shows number of offers based on division and exam score
ggplot(data, aes(x = exam, y = phat, color = division)) +
  geom_point(aes(y = offers), alpha = .7, position = position_jitter(h = .2)) +
  geom_line() +
  labs(x = "Entrance Exam Score", y = "Expected number of scholarship offers")</b>
<h3><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/poissonReg2.jpg"></h3>
The plot shows the highest number of expected scholarship offers for players who score high on the entrance exam score. In addition, we can see that players from division B (the green line) are expected to get more offers in general than players from either division A or division C.
<h3>Reporting the Results</h3>
Lastly, we can report the results of the regression in such a way that summarizes our findings:
A Poisson regression was run to predict the number of scholarship offers received by baseball players based on division and entrance exam scores. For each additional point scored on the entrance exam, there is a 10% increase in the number of offers received (<em>p &lt; 0.0001)</em>. Division was found to not be statistically significant.
<h2><span class="orange">Poisson vs. Normal Distribution: What’s the Difference?</span></h2>
The <b>Poisson distribution</b> and the <b>normal distribution</b> are two of the most commonly used probability distributions in statistics.
This tutorial provides a quick explanation of each distribution along with two key differences between the distributions.
<h3>An Overview: The Poisson Distribution</h3>
The <b>Poisson distribution</b> describes the probability of obtaining <em>k</em> successes during a given time interval.
If a  random variable  <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
For example, suppose a particular hospital experiences an average of 2 births per hour. We can use the formula above to determine the probability of experiencing 3 births in a given hour:
<b>P(X=3) </b>= 2<sup>3</sup> * e<sup>– 2</sup> / 3! = <b>0.1805</b>
The probability of experiencing 3 births in a given hour is <b>0.1805</b>.
<h3>An Overview: The Normal Distribution</h3>
The <b>normal distribution</b> describes the probability that a random variable takes on a value within a given interval.
The  probability density function  of a normal distribution can be written as:
<b>P(X=x) = (1/σ√2π)e<sup>-1/2((x-μ)/σ)<sup>2</sup></sup></b>
where:
<b>σ: </b>Standard deviation of the distribution
<b>μ: </b>Mean of the distribution
<b>x: </b>Value for the random variable
For example, suppose the weight of a certain species of otters is normally distributed with μ = 40 pounds and σ = 5 pounds.
If we randomly select an otter from this population, we can use the following formula to find the probability that it weighs between 38 and 42 pounds:
P(38 &lt; X &lt; 42) = (1/σ√2π)e<sup>-1/2((42-40)/5)<sup>2</sup></sup> – (1/σ√2π)e<sup>-1/2((38-40)/5)<sup>2</sup></sup> = <b>0.3108</b>
The probability that the randomly selected otter weighs between 38 and 42 pounds is <b>0.3108</b>.
<h3>Difference #1: Discrete vs. Continuous Data</h3>
The first difference between the Poisson and normal distribution is the type of data that each probability distribution models.
A <b>Poisson distribution</b> is used when you’re working with <b>discrete data</b> that can only take on integer values equal to or greater than zero. Some examples include:
Number of calls received per hour at a call center
Number of customers per day at a restaurant
Number of car accidents per month
In each scenario, the random variable can only take on a value of 0, 1, 2, 3, etc.
A <b>normal distribution</b> is used when you’re working with <b>continuous data</b> that can take on <em>any</em> value from negative infinity to positive infinity. Some examples include:
Weight of a certain animal
Height of a certain plant
Marathon times of females
Temperature in Celsius
In these scenarios, the random variables can take on <em>any</em> value like -11.3, 21.343435, 85, etc.
<h3>Difference #2: Shape of the Distributions</h3>
The second difference between the Poisson and normal distribution is the shape of the distributions.
A <b>normal distribution</b> will always exhibit a bell shape:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/normPoisson1.jpg"416">
However, the shape of the Poisson distribution will vary based on the mean value of the distribution. 
For example, a Poisson distribution with a small value for the mean like <b>μ = 3</b> will be highly  right skewed :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/normPoisson3.jpg"410">
However, a Poisson distribution with a larger value for the mean like <b>μ = 20</b> will exhibit a bell shape just like the normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/normPoisson4-1.jpg"403">
Notice that the lower bound for a Poisson distribution will always be zero no matter what the value for the mean is because a Poisson distribution can only be used with integer values that are equal to or greater than zero.
<h2><span class="orange">How to Calculate Polychoric Correlation in R</span></h2>
<b>Polychoric correlation</b> is used to calculate the correlation between ordinal variables.
Recall that  ordinal variables  are variables whose possible values are categorical and have a natural order.
Some examples of variables measured on an ordinal scale include:
<b>Satisfaction</b>: Very unsatisfied, unsatisfied, neutral, satisfied, very satisfied
<b>Income level</b>: Low income, medium income, high income
<b>Workplace status</b>: Entry Analyst, Analyst I, Analyst II, Lead Analyst
<b>Degree of pain</b>: Small amount, medium amount, high amount 
The value for polychoric correlation ranges from -1 to 1 where:
<b> -1</b> indicates a perfect negative correlation
<b>0</b> indicates no correlation
<b>1</b> indicates a perfect positive correlation
We can use the <b>polychor(x, y)</b> function from the <b>polycor</b> package to calculate the polychoric correlation between two ordinal variables in R.
The following examples show how to use this function in practice.
<h3>Example 1: Calculate Polychoric Correlation for Movie Ratings</h3>
Suppose want to know whether or not two different movie ratings agencies have a high correlation between their movie ratings.
We ask each agency to rate 20 different movies on a scale of 1 to 3 where:
<b>1</b> indicates “bad”
<b>2</b> indicates “mediocre”
<b>3</b> indicates “good”
We can use the following code in R to calculate the polychoric correlation between the ratings of the two agencies:
<b>library(polycor)
#define movie ratings for each agency
agency1 &lt;- c(1, 1, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 1, 2, 2, 1, 1, 1, 2, 2)
agency2 &lt;- c(1, 1, 2, 1, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 1, 2, 1, 3, 3)
#calculate polychoric correlation between ratings
polychor(agency1, agency2)
[1] 0.7828328
</b>
The polychoric correlation turns out to be <b>0.78</b>.
This value is quite high, which indicates that there is a strong positive association between the ratings from each agency.
<h3>Example 2: Calculate Polychoric Correlation for Restaurant Ratings</h3>
Suppose want to know whether or not two different neighborhood restaurants have any correlation between their restaurant ratings from customers.
We randomly survey 20 customers who ate at both restaurants and ask them to rate their overall satisfaction a scale of 1 to 5 where:
<b>1</b> indicates “very unsatisfied”
<b>2</b> indicates “unsatisfied”
<b>3</b> indicates “neutral”
<b>4</b> indicates “satisfied”
<b>5</b> indicates “very satisfied”
We can use the following code in R to calculate the polychoric correlation between the ratings of the two restaurants:
<b>library(polycor)
#define ratings for each restaurant
restaurant1 &lt;- c(1, 1, 2, 2, 2, 3, 3, 3, 2, 2, 3, 4, 4, 5, 5, 4, 3, 4, 5, 5)
restaurant2 &lt;- c(4, 3, 3, 4, 3, 3, 4, 5, 4, 4, 4, 5, 5, 4, 2, 1, 1, 2, 1, 4)
#calculate polychoric correlation between ratings
polychor(restaurant1, restaurant2)
[1] -0.1322774
</b>
The polychoric correlation turns out to be <b>-0.13</b>.
This value is close to zero, which indicates that there is very little (if any) association between the ratings of the restaurants.
<h2><span class="orange">How to Perform Polynomial Regression in Excel</span></h2>
Regression analysis is used to quantify the relationship between one or more explanatory variables and a  response variable .
The most common type of regression analysis is  simple linear regression , which is used when an explanatory variable and a response variable have a linear relationship.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel1.png">
However, sometimes the relationship between an explanatory variable and a response variable is nonlinear. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel2.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel3.png">
In these cases it makes sense to use <b>polynomial regression</b>, which can account for the nonlinear relationship between the variables.
This tutorial explains how to perform polynomial regression in Excel.
<h3>Example: Polynomial Regression in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel4.png">
Use the following steps to fit a polynomial regression equation to this dataset:
<b>Step 1: Create a scatterplot.</b>
First, we need to create a scatterplot. Go to the <b>Charts </b>group in the <b>Insert </b>tab and click the first chart type in <b>Scatter</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel5.png">
A scatterplot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel6.png">
<b>Step 2: Add a trendline.</b>
Next, we need to add a trendline to the scatterplot. To do so, click on any of the individual points in the scatterplot. Then, right click and select <b>Add Trendline…</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel7.png">
A new window will pop up with the option to specify a trendline. Choose <b>Polynomial </b>and choose the number you’d like to use for <b>Order</b>. We will use 3. Then, check the box near the bottom that says <b>Display Equation on chart</b>. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel8.png">
A trendline with a polynomial regression equation will automatically appear on the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel9.png">
<b>Step 3: Interpret the regression equation.</b>
For this particular example, our fitted polynomial regression equation is:
y = -0.1265x<sup>3</sup> + 2.6482x<sup>2</sup> – 14.238x + 37.213
This equation can be used to find the expected value for the response variable based on a given value for the explanatory variable. For example, suppose x = 4. The expected value for the response variable, y, would be:
y = -0.1265(4)<sup>3</sup> + 2.6482(4)<sup>2</sup> – 14.238(4) + 37.213 = <b>14.5362</b>.
<h2><span class="orange">Polynomial Regression in Google Sheets (Step-by-Step)</span></h2>
Regression analysis is used to quantify the relationship between one or more predictor variables and a  response variable .
The most common type of regression analysis is  simple linear regression , which is used when a predictor variable and a response variable have a linear relationship.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel1.png">
However, sometimes the relationship between a predictor variable and a response variable is nonlinear. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel2.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel3.png">
In these cases it makes sense to use <b>polynomial regression</b>, which can account for the nonlinear relationship between the variables.
This tutorial provides a step-by-step example of how to perform polynomial regression in Google Sheets
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset with the following values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets1.png">
<h3>Step 2: Create a Scatterplot</h3>
Next, we’ll create a scatterplot to visualize the data.
First, highlight cells <b>A2:B11</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets2.png">
Next, click the <b>Insert</b> tab and then click <b>Chart</b> from the dropdown menu:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets3.png">
By default, Google Sheets will insert a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets4.png">
<h3>Step 3: Find the Polynomial Regression Equation</h3>
Next, double click anywhere on the scatterplot to bring up the <b>Chart Editor</b> window on the right:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets5.png">
Next, click <b>Series</b>. Then, scroll down and check the box next to <b>Trendline</b> and change the Type to <b>Polynomial</b>. For Label, choose <b>Use Equation</b> and then check the box next to <b>Show R<sup>2</sup></b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets6.png">
This will cause the following formula to be displayed above the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets7.png">
We can see that the fitted polynomial regression equation is:
<b>y = 9.45 + 2.1x – 0.0188x<sup>2</sup></b>
The R-squared for this model is <b>0.718</b>.
<em>Recall that  R-squared  tells us the percentage of variation in the response variable that can be explained by the predictor variables. The higher the value, the better the model.</em>
Next, change the Polynomial degree to 3 in the Chart Editor:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets8.png">
This will cause the following formula to be displayed above the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets9.png">
This causes the fitted polynomial regression equation to change to:
<b>y = 37.2 – 14.2x + 2.64x<sup>2</sup> – 0.126x<sup>3</sup></b>
The R-squared for this model is <b>0.976</b>.
Notice that the R-squared for this model is significantly higher than the polynomial regression model with a degree of 2. This suggests that this regression model is significantly better at capturing the trend in the underlying data.
If you change the degree of the polynomial to 4, the R-squared increases just barely to <b>0.981</b>. This suggests that a polynomial regression model with a degree of 3 is sufficient to capture the trend for this data.
We can use the fitted regression equation to find the expected value for the response variable based on a given value for the predictor variable. For example, if <em>x</em> = 4 then the expected value for <em>y</em> would be:
y = 37.2 – 14.2(4) + 2.64(4)<sup>2</sup> – 0.126(4)<sup>3</sup> = <b>14.576</b>
<em>You can find more Google Sheets tutorials on  this page .</em>
<h2><span class="orange">How to Perform Polynomial Regression in SAS</span></h2>
The most common type of regression analysis is  simple linear regression , which is used when a predictor variable and a  response variable  have a linear relationship.
However, sometimes the relationship between a predictor variable and a response variable is nonlinear. 
In these cases it makes sense to use <b>polynomial regression</b>, which can account for the nonlinear relationship between the variables.
The following example shows how to perform polynomial regression in SAS.
<h2>Example: Polynomial Regression in SAS</h2>
Suppose we have the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input x y;
    datalines;
2 18
4 14
4 16
5 17
6 18
7 23
7 25
8 28
9 32
12 29
;
run;
/*view dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/polysas1.jpg"118">
Now suppose we create a scatter plot to visualize the relationship between the variables x and y in the dataset:
<b>/*create scatter plot of x vs. y*/
proc sgplot data=my_data;
    scatter x=x y=y;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/polysas2.jpg"592">
From the plot we can see that the relationship between x and y appears to be cubic.
Thus, we can define two new predictor variables in our dataset (x<sup>2</sup> and x<sup>3</sup>) and then use <b>proc reg</b> to fit a polynomial regression model using these predictor variables:
<b>/*create dataset with new predictor variables*/
data my_data;
    input x y;
    x2 = x**2;
    x3 = x**3;
    datalines;
2 18
4 14
4 16
5 17
6 18
7 23
7 25
8 28
9 32
12 29
;
run;
/*fit polynomial regression model*/
proc reg data=my_data;
    model y = x x2 x3;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/polysas3.jpg"413">
From the <b>Parameter Estimates</b> table we can find the coefficient estimates and write our fitted polynomial regression equation as:
y = 37.213 – 14.238x + 2.648x<sup>2</sup> – 0.126x<sup>3</sup>
This equation can be used to find the expected value for the response variable based on a given value for the predictor variable.
 For example if x has a value of 4 then y is expected to have a value of 14.565:
y = 37.213 – 14.238(4) + 2.648(4)<sup>2</sup> – 0.126(4)<sup>3</sup> = <b>14.565</b>
We can also see the polynomial regression model has an adjusted R-squared value of <b>0.9636</b>, which is extremely close to one and tells us that the model does an excellent job of fitting the dataset.
<b>Related:</b>  How to Interpret Adjusted R-Squared (With Examples) 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Perform Simple Linear Regression in SAS 
 How to Perform Multiple Linear Regression in SAS 
 How to Perform Quantile Regression in SAS 
<h2><span class="orange">How to Perform Polynomial Regression in Python</span></h2>
Regression analysis is used to quantify the relationship between one or more explanatory variables and a response variable.
The most common type of regression analysis is  simple linear regression , which is used when a predictor variable and a response variable have a linear relationship.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/polynomialpython1.png">
However, sometimes the relationship between a predictor variable and a response variable is nonlinear.
For example, the true relationship may be quadratic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/polynomialpython2.png">
Or it may be cubic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/polynomialpython3.png">
In these cases it makes sense to use <b>polynomial regression</b>, which can account for the nonlinear relationship between the variables.
This tutorial explains how to perform polynomial regression in Python.
<h3>Example: Polynomial Regression in Python</h3>
Suppose we have the following predictor variable (x) and response variable (y) in Python:
<b>x = [2, 3, 4, 5, 6, 7, 7, 8, 9, 11, 12]
y = [18, 16, 15, 17, 20, 23, 25, 28, 31, 30, 29]
</b>
If we create a simple scatterplot of this data, we can see that the relationship between x and y is clearly not linear:
<b>import matplotlib.pyplot as plt
#create scatterplot 
plt.scatter(x, y)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/polynomialpython4.png">
Thus, it wouldn’t make sense to fit a linear regression model to this data. Instead, we can attempt to fit a polynomial regression model with a degree of 3 using the  numpy.polyfit()  function:
<b>import numpy as np
#polynomial fit with degree = 3
model = np.poly1d(np.polyfit(x, y, 3))
#add fitted polynomial line to scatterplot
polyline = np.linspace(1, 12, 50)
plt.scatter(x, y)
plt.plot(polyline, model(polyline))
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/polynomialpython5.png">
We can obtain the fitted polynomial regression equation by printing the model coefficients:
<b>print(model)
poly1d([ -0.10889554,   2.25592957, -11.83877127,  33.62640038])
</b>
The fitted polynomial regression equation is:
<b>y = -0.109x<sup>3</sup> + 2.256x<sup>2</sup> – 11.839x + 33.626</b>
This equation can be used to find the expected value for the response variable based on a given value for the explanatory variable. For example, suppose x = 4. The expected value for the response variable, y, would be:
y = -0.109(4)<sup>3</sup> + 2.256(4)<sup>2</sup> – 11.839(4) + 33.626= <b>15.39</b>.
We can also write a short function to obtain the R-squared of the model, which is the proportion of the variance in the response variable that can be explained by the predictor variables.
<b>#define function to calculate r-squared
def polyfit(x, y, degree):
    results = {}
    coeffs = numpy.polyfit(x, y, degree)
    p = numpy.poly1d(coeffs)
    #calculate r-squared
    yhat = p(x)
    ybar = numpy.sum(y)/len(y)
    ssreg = numpy.sum((yhat-ybar)**2)
    sstot = numpy.sum((y - ybar)**2)
    results['r_squared'] = ssreg / sstot
    return results
#find r-squared of polynomial model with degree = 3
polyfit(x, y, 3)
{'r_squared': 0.9841113454245183}
</b>
In this example, the R-squared of the model is <b>0.9841</b>. This means that 98.41% of the variation in the response variable can be explained by the predictor variables.
<h2><span class="orange">Polynomial Regression in R (Step-by-Step)</span></h2>
 Polynomial regression  is a technique we can use when the relationship between a predictor variable and a  response variable  is nonlinear. 
This type of regression takes the form:
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + … + β<sub>h</sub>X<sup>h</sup> + ε
where <em>h</em> is  the “degree” of the polynomial.
This tutorial provides a step-by-step example of how to perform polynomial regression in R.
<h3>Step 1: Create the Data</h3>
For this example we’ll create a dataset that contains the number of hours studied and final exam score for a class of 50 students:
<b>#make this example reproducible
set.seed(1)
#create dataset
df &lt;- data.frame(hours = runif(50, 5, 15), score=50)
df$score = df$score + df$hours^3/150 + df$hours*runif(50, 1, 2)
#view first six rows of data
head(data)
      hours    score
1  7.655087 64.30191
2  8.721239 70.65430
3 10.728534 73.66114
4 14.082078 86.14630
5  7.016819 59.81595
6 13.983897 83.60510
</b>
<h3>Step 2: Visualize the Data</h3>
Before we fit a regression model to the data, let’s first create a scatterplot to visualize the relationship between hours studied and exam score:
<b>library(ggplot2)
ggplot(df, aes(x=hours, y=score)) +
  geom_point()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/03/poly1-1.png">
We can see that the data exhibits a bit of a quadratic relationship, which indicates that polynomial regression could fit the data better than simple linear regression.
<h3>Step 3: Fit the Polynomial Regression Models</h3>
Next, we’ll fit five different polynomial regression models with degrees <em>h</em> = 1…5 and use k-fold cross-validation with k=10 folds to calculate the test MSE for each model:
<b>#randomly shuffle data
df.shuffled &lt;- df[sample(nrow(df)),]
#define number of folds to use for k-fold cross-validation
K &lt;- 10 
#define degree of polynomials to fit
degree &lt;- 5
#create k equal-sized folds
folds &lt;- cut(seq(1,nrow(df.shuffled)),breaks=K,labels=FALSE)
#create object to hold MSE's of models
mse = matrix(data=NA,nrow=K,ncol=degree)
#Perform K-fold cross validation
for(i in 1:K){
    
    #define training and testing data
    testIndexes &lt;- which(folds==i,arr.ind=TRUE)
    testData &lt;- df.shuffled[testIndexes, ]
    trainData &lt;- df.shuffled[-testIndexes, ]
    
    #use k-fold cv to evaluate models
    for (j in 1:degree){
        fit.train = lm(score ~ poly(hours,j), data=trainData)
        fit.test = predict(fit.train, newdata=testData)
        mse[i,j] = mean((fit.test-testData$score)^2) 
    }
}
#find MSE for each degree 
colMeans(mse)
[1]  9.802397  8.748666  9.601865 10.592569 13.545547
</b>
From the output we can see the test MSE for each model:
Test MSE with degree h = 1: <b>9.80</b>
Test MSE with degree h = 2: <b>8.75</b>
Test MSE with degree h = 3: <b>9.60</b>
Test MSE with degree h = 4: <b>10.59</b>
Test MSE with degree h = 5: <b>13.55</b>
The model with the lowest test MSE turned out to be the polynomial regression model with degree <em>h</em> =2.
This matches our intuition from the original scatterplot: A quadratic regression model fits the data best.
<h3>Step 4: Analyze the Final Model</h3>
Lastly, we can obtain the coefficients of the best performing model:
<b>#fit best model
best = lm(score ~ poly(hours,2, raw=T), data=df)
#view summary of best model
summary(best)
Call:
lm(formula = score ~ poly(hours, 2, raw = T), data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-5.6589 -2.0770 -0.4599  2.5923  4.5122 
Coefficients:         Estimate Std. Error t value Pr(>|t|)    
(Intercept)              54.00526    5.52855   9.768 6.78e-13 ***
poly(hours, 2, raw = T)1 -0.07904    1.15413  -0.068  0.94569    
poly(hours, 2, raw = T)2  0.18596    0.05724   3.249  0.00214 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
From the output we can see that the final fitted model is:
Score = 54.00526 – .07904*(hours) + .18596*(hours)<sup>2</sup>
We can use this equation to estimate the score that a student will receive based on the number of hours they studied.
For example, a student who studies for 10 hours is expected to receive a score of <b>71.81</b>:
Score = 54.00526 – .07904*(10) + .18596*(10)<sup>2</sup> = 71.81
We can also plot the fitted model to see how well it fits the raw data:
<b>ggplot(df, aes(x=hours, y=score)) + 
          geom_point() +
          stat_smooth(method='lm', formula = y ~ poly(x,2), size = 1) + 
          xlab('Hours Studied') +
          ylab('Score')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/03/poly2.png">
You can find the complete R code used in this example  here .
<h2><span class="orange">An Introduction to Polynomial Regression</span></h2>
When we have a dataset with one predictor variable and one  response variable , we often use  simple linear regression  to quantify the relationship between the two variables.
However, simple linear regression (SLR) assumes that the relationship between the predictor and response variable is linear. Written in mathematical notation, SLR assumes that the relationship takes the form:
Y = β<sub>0</sub> + β<sub>1</sub>X + ε
But in practice the relationship between the two variables can actually be nonlinear and attempting to use linear regression can result in a poorly fit model.
One way to account for a nonlinear relationship between the predictor and response variable is to use <b>polynomial regression</b>, which takes the form:
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + … + β<sub>h</sub>X<sup>h</sup> + ε
In this equation, <em>h</em> is referred to as the <b>degree</b> of the polynomial.
As we increase the value for <em>h</em>, the model is able to fit nonlinear relationships better, but in practice we rarely choose <em>h</em> to be greater than 3 or 4. Beyond this point, the model becomes too flexible and  overfits the data .
<b>Technical Notes</b>
 
Although polynomial regression can fit nonlinear data, it is still considered to be a form of <em>linear</em> regression because it is linear in the coefficients β<sub>1</sub>, β<sub>2</sub>, …, β<sub>h</sub>.
Polynomial regression can be used for multiple predictor variables as well but this creates interaction terms in the model, which can make the model extremely complex if more than a few predictor variables are used.
<h3>When to Use Polynomial Regression</h3>
We use polynomial regression when the relationship between a predictor and response variable is nonlinear.
There are three common ways to detect a nonlinear relationship:
<b>1. Create a Scatterplot.</b>
The easiest way to detect a nonlinear relationship is to create a  scatterplot  of the response vs. predictor variable.
For example, if we create the following scatterplot then we can see that the relationship between the two variables is roughly linear, thus simple linear regression would likely perform fine on this data.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel1.png">
However, if our scatterplot looks like one of the following plots then we could see that the relationship is nonlinear and thus polynomial regression would be a good idea:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel2.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/polynomialExcel3.png">
<b>2. Create a residuals vs. fitted plot.</b>
Another way to detect nonlinearity is to fit a simple linear regression model to the data and then produce a  residuals vs. fitted values plot .
If the residuals of the plot are roughly evenly distributed around zero with no clear pattern, then simple linear regression is likely sufficient.
However, if the residuals display a nonlinear pattern in the plot then this is a sign that the relationship between the predictor and the response is likely nonlinear.
<b>3. Calculate the R<sup>2</sup> of the model.</b>
The R<sup>2</sup> value of a regression model tells you the percentage of the variation in the response variable that can be explained by the predictor variable(s).
If you fit a simple linear regression model to a dataset and the R<sup>2</sup> value of the model is quite low, this could be an indication that the relationship between the predictor and response variable is more complex than just a simple linear relationship.
This could be a sign that you may need to try polynomial regression instead.
<b>Related:</b>  What is a Good R-squared Value? 
<h3>How to Choose the Degree of the Polynomial</h3>
A polynomial regression model takes the following form:
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + … + β<sub>h</sub>X<sup>h</sup> + ε
In this equation, <em>h</em> is the degree of the polynomial.
But how do we choose a value for <em>h</em>?
In practice, we fit several different models with different values of <em>h</em> and perform  k-fold cross-validation  to determine which model produces the lowest test mean squared error (MSE).
For example, we may fit the following models to a given dataset:
Y = β<sub>0</sub> + β<sub>1</sub>X
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup>
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + β<sub>3</sub>X<sup>3</sup>
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + β<sub>3</sub>X<sup>3</sup> + β<sub>4</sub>X<sup>4</sup>
We can then use k-fold cross-validation to calculate the test MSE of each model, which will tell us how well each model performs on data it hasn’t seen before.
<h3>The Bias-Variance Tradeoff of Polynomial Regression</h3>
There exists a  bias-variance tradeoff  when using polynomial regression. As we increase the degree of the polynomial, the bias decreases (as the model becomes more flexible) but the variance increases.
As with all machine learning models, we must find an optimal tradeoff between bias and variance.
In most cases it helps to increase the degree of the polynomial to an extent, but beyond a certain value the model begins to fit the noise of the data and the test MSE begins to decrease.
To ensure that we fit a model that is flexible but not <em>too</em> flexible, we use k-fold cross-validation to find the model that produces the lowest test MSE.
<h3>How to Perform Polynomial Regression</h3>
The following tutorials provide examples of how to perform polynomial regression in different softwares:
 How to Perform Polynomial Regression in Excel 
 How to Perform Polynomial Regression in R 
 How to Perform Polynomial Regression in Python 
<h2><span class="orange">Pooled Standard Deviation Calculator</span></h2>
The <b>pooled standard deviation</b> is a weighted average of two standard deviations from two different groups. It is typically used in a  two sample t-test .
To calculate the pooled standard deviation for two groups, simply fill in the information below and then click the “Calculate” button.
<label for="raw">Enter raw data</label>
<input type="radio" id="raw" name="tails" onclick="check()" checked><label for="summary">Enter summary data</label>
<input type="radio" id="summary" name="tails" onclick="check()">
<b>Sample 1</b>
<textarea id="rawData1" rows="5" cols="40">301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304</textarea>
<b>Sample 2</b>
<textarea id="rawData2" rows="5" cols="40">302, 309, 324, 313, 312, 310, 305, 298, 299, 300, 289, 294</textarea>
<label><b>s<sub>1</sub></b> (sample 1 standard deviation)</label>
<input type="number" id="s1" value="18.5">
<label><b>n<sub>1</sub></b> (sample 1 size)</label>
<input type="number" id="n1" value="40">
<label><b>s<sub>2</sub></b> (sample 2 standard deviation)</label>
<input type="number" id="s2" value="16.7">
<label><b>n<sub>2</sub></b> (sample 2 size)</label>
<input type="number" id="n2" value="38">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Pooled standard deviation = </b> 7.739852
<script>
//set summary table to hidden to start
var summary_display = document.getElementById("summary_table");
summary_display.style.display = "none";
//find which radio button is checked
function check() {
if (document.getElementById('raw').checked) {
var table_display = document.getElementById("words_table");
        table_display.style.display = "block";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "none";
} else {
var table_display = document.getElementById("words_table");
        table_display.style.display = "none";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "block";
}
} //end check
//perform one-sample t-test
function calc() {
if (document.getElementById('summary').checked) {
var s1 = +document.getElementById('s1').value;
var n1 = +document.getElementById('n1').value;
var s2 = +document.getElementById('s2').value;
var n2 = +document.getElementById('n2').value;
var df = n1-(-1*n2)-2;
var pooled = Math.sqrt(((n1-1)*Math.pow(s1,2) - (-1*((n2-1)*Math.pow(s2,2))))/df);
document.getElementById('pooled').innerHTML = pooled.toFixed(6);
} else {
var raw1 = document.getElementById('rawData1').value.split(',').map(Number);
var raw2 = document.getElementById('rawData2').value.split(',').map(Number);
var s1 = math.std(raw1);
var n1 = raw1.length;
var s2 = math.std(raw2);
var n2 = raw2.length;
var df = n1-(-1*n2)-2;
var pooled = Math.sqrt(((n1-1)*Math.pow(s1,2) - (-1*((n2-1)*Math.pow(s2,2))))/df);
document.getElementById('pooled').innerHTML = pooled.toFixed(6);
}
//output results
}
</script>
<h2><span class="orange">How to Calculate Pooled Standard Deviation in R</span></h2>
A <b>pooled standard deviation</b> is simply a weighted average of standard deviations from two or more independent groups.
In statistics it appears most often in the  two sample t-test , which is used to test whether or not the means of two populations are equal.
The formula to calculate a pooled standard deviation for two groups is as follows:
<b>Pooled standard deviation = √ (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> /  (n<sub>1</sub>+n<sub>2</sub>-2)</b>
where:
<b>n<sub>1</sub>, n<sub>2</sub>:</b> Sample size for group 1 and group 2, respectively.
<b>s<sub>1</sub>, s<sub>2</sub>:</b> Standard deviation for group 1 and group 2, respectively.
The following examples show two methods for calculating a pooled standard deviation between two groups in R.
<h3>Method 1: Calculate Pooled Standard Deviation Manually</h3>
Suppose we have the following data values for two samples:
<b>Sample 1</b>: 6, 6, 7, 8, 8, 10, 11, 13, 15, 15, 16, 17, 19, 19, 21
<b>Sample 2</b>: 10, 11, 13, 13, 15, 17, 17, 19, 20, 22, 24, 25, 27, 29, 29
The following code shows how to calculate the pooled standard deviation between these two samples:
<b>#define two samples
data1 &lt;- c(6, 6, 7, 8, 8, 10, 11, 13, 15, 15, 16, 17, 19, 19, 21)
data2 &lt;- c(10, 11, 13, 13, 15, 17, 17, 19, 20, 22, 24, 25, 27, 29, 29)
#find sample standard deviation of each sample
s1 &lt;- sd(data1)
s2 &lt;- sd(data2)
#find sample size of each sample
n1 &lt;- length(data1)
n2 &lt;- length(data2)
#calculate pooled standard deviation
pooled &lt;- sqrt(((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n1-2))
#view pooled standard deviation
pooled
[1] 5.789564
</b>
The pooled standard deviation turns out to be <b>5.789564</b>.
<h3>Method 2: Calculate Pooled Standard Deviation Using a Package</h3>
Another way to calculate the pooled standard deviation between two samples in R is to use the <b>sd_pooled()</b> function from the <b>effectsize</b> package.
The following code shows how to use this function in practice:
<b>library(effectsize)
#define two samples
data1 &lt;- c(6, 6, 7, 8, 8, 10, 11, 13, 15, 15, 16, 17, 19, 19, 21)
data2 &lt;- c(10, 11, 13, 13, 15, 17, 17, 19, 20, 22, 24, 25, 27, 29, 29)
#calculate pooled standard deviation between two samples
sd_pooled(data1, data2)
[1] 5.789564
</b>
The pooled standard deviation turns out to be <b>5.789564</b>.
Note that this matches the value that we calculated manually in the previous example.
<h2><span class="orange">How to Calculate a Pooled Standard Deviation (With Example)</span></h2>
A <b>pooled standard deviation </b>is simply a weighted average of standard deviations from two or more independent groups.
In statistics it appears most often in the  two sample t-test , which is used to test whether or not the means of two populations are equal.
The formula to calculate a pooled standard deviation for two groups is as follows:
<b>Pooled standard deviation = √ (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> /  (n<sub>1</sub>+n<sub>2</sub>-2)</b>
where:
<b>n<sub>1</sub>, n<sub>2</sub>:</b> Sample size for group 1 and group 2, respectively.
<b>s<sub>1</sub>, s<sub>2</sub>:</b> Standard deviation for group 1 and group 2, respectively.
Note that the pooled standard deviation should only be used when the standard deviations between the two groups can be assumed to be roughly equal.
Also note that because the pooled standard deviation is a weighted average, it will give more “weight” to the group with the larger sample size.
<h3>Example: Calculating the Pooled Standard Deviation</h3>
Suppose we have two different groups with the following information:
<b>Group 1: </b>
Sample size (n<sub>1</sub>): 15
Sample standard deviation (s<sub>1</sub>): 6.4
<b>Group 2: </b>
Sample size (n<sub>2</sub>): 19
Sample standard deviation (s<sub>2</sub>): 8.2
We can calculated the pooled standard deviation for these two groups as:
Pooled standard deviation = √ (15-1)6.4<sup>2</sup> +  (19-1)8.2<sup>2</sup> /  (15+19-2) = <b>7.466</b>
Notice how the value for the pooled standard deviation (7.466) is between the values for the standard deviation of group 1 (6.4) and group 2 (8.2).
This should make sense considering the pooled standard deviation is just a weighted average between the two groups.
<h3>Bonus: Pooled Standard Deviation Calculator</h3>
You can also use the  Pooled Standard Deviation Calculator  to quickly calculate the pooled standard deviation between two groups.
For example, we could plug in the values from the previous example to come up with the same pooled standard deviation that we calculated by hand:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/pooledCalc1.png">
Note that you can also use the “Enter raw data” option on the calculator to enter the raw data values for the two groups and calculate the pooled standard deviation in that manner.
<h2><span class="orange">Pooled Variance Calculator</span></h2>
When performing a  two sample t-test , we typically assume that the variances between the two samples are equal. Under this assumption, we can calculate the  pooled variance  to use in the two sample t-test.
To calculate the pooled variance for two samples, simply fill in the information below and then click the “Calculate” button.
<label for="raw">Enter raw data</label>
<input type="radio" id="raw" name="tails" onclick="check()" checked><label for="summary">Enter summary data</label>
<input type="radio" id="summary" name="tails" onclick="check()">
<b>Sample 1</b>
<textarea id="rawData1" rows="5" cols="40">301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304</textarea>
<b>Sample 2</b>
<textarea id="rawData2" rows="5" cols="40">302, 309, 324, 313, 312, 310, 305, 298, 299, 300, 289, 294</textarea>
<label><b>s<sub>1</sub></b> (sample 1 standard deviation)</label>
<input type="number" id="s1" value="18.5">
<label><b>n<sub>1</sub></b> (sample 1 size)</label>
<input type="number" id="n1" value="40">
<label><b>s<sub>2</sub></b> (sample 2 standard deviation)</label>
<input type="number" id="s2" value="16.7">
<label><b>n<sub>2</sub></b> (sample 2 size)</label>
<input type="number" id="n2" value="38">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Pooled variance = </b> 59.905303
<script>
//set summary table to hidden to start
var summary_display = document.getElementById("summary_table");
summary_display.style.display = "none";
//find which radio button is checked
function check() {
if (document.getElementById('raw').checked) {
var table_display = document.getElementById("words_table");
        table_display.style.display = "block";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "none";
} else {
var table_display = document.getElementById("words_table");
        table_display.style.display = "none";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "block";
}
} //end check
//perform one-sample t-test
function calc() {
if (document.getElementById('summary').checked) {
var s1 = +document.getElementById('s1').value;
var n1 = +document.getElementById('n1').value;
var s2 = +document.getElementById('s2').value;
var n2 = +document.getElementById('n2').value;
var df = n1-(-1*n2)-2;
var pooled = Math.sqrt(((n1-1)*Math.pow(s1,2) - (-1*((n2-1)*Math.pow(s2,2))))/df);
var pooled2 = pooled*pooled;
document.getElementById('pooled').innerHTML = pooled2.toFixed(6);
} else {
var raw1 = document.getElementById('rawData1').value.split(',').map(Number);
var raw2 = document.getElementById('rawData2').value.split(',').map(Number);
var s1 = math.std(raw1);
var n1 = raw1.length;
var s2 = math.std(raw2);
var n2 = raw2.length;
var df = n1-(-1*n2)-2;
var pooled = Math.sqrt(((n1-1)*Math.pow(s1,2) - (-1*((n2-1)*Math.pow(s2,2))))/df);
var pooled2 = pooled*pooled;
document.getElementById('pooled').innerHTML = pooled2.toFixed(6);
}
//output results
}
</script>
<h2><span class="orange">How to Calculate Pooled Variance in Excel (Step-by-Step)</span></h2>
In statistics,  pooled variance  refers to the average of two or more group variances.
We use the word “pooled” to indicate that we’re “pooling” two or more group variances to come up with a single number for the common variance between the groups.
In practice, pooled variance is used most often in a  two sample t-test , which is used to determine whether or not two population means are equal.
The pooled variance between two samples is typically denoted as s<sub>p</sub><sup>2</sup> and is calculated as:
s<sub>p</sub><sup>2</sup> = ( (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup>  )  /  (n<sub>1</sub>+n<sub>2</sub>-2)
This tutorial provides a step-by-step example of how to calculate the pooled variance between two groups in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create two datasets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pooledVarExcel1.png">
<h3>Step 2: Calculate the Sample Size & Sample Variance</h3>
Next, let’s calculate the sample size and sample variance for each dataset.
Cells E17:F18 show the formulas we used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pooledVarExcel2.png">
<h3>Step 3: Calculate the Pooled Variance</h3>
Lastly, we can use the following formula to calculate the pooled variance:
<b>=((B17-1)*B18 + (C17-1)*C18) / (B17+C17-2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pooledVarExcel3.png">
The pooled variance between these two groups turns out to be <b>46.97</b>.
<b>Bonus:</b> You can use this  Pooled Variance Calculator  to automatically calculate the pooled variance between two groups.
<h2><span class="orange">How to Calculate Pooled Variance in R</span></h2>
In statistics,  pooled variance  refers to the average of two or more group variances.
We use the word “pooled” to indicate that we’re “pooling” two or more group variances to come up with a single number for the common variance between the groups.
In practice, pooled variance is used most often in a  two sample t-test , which is used to determine whether or not two population means are equal.
The pooled variance between two samples is typically denoted as s<sub>p</sub><sup>2</sup> and is calculated as:
s<sub>p</sub><sup>2</sup> = ( (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup>  )  /  (n<sub>1</sub>+n<sub>2</sub>-2)
Unfortunately there is no built-in function to calculate the pooled variance between two groups in R, but we can calculate it fairly easily.
For example, suppose we want to calculate the pooled variance between the following two groups:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/pooledvar1.png">
The following code shows how to calculate the pooled variance between these groups in R:
<b>#define groups of data
x1 &lt;- c(6, 7, 7, 8, 10, 11, 13, 14, 14, 16, 18, 19, 19, 19, 20)
x2 &lt;- c(5, 7, 7, 8, 10, 13, 14, 15, 19, 20, 20, 23, 25, 28, 32)
#calculate sample size of each group
n1 &lt;- length(x1)
n2 &lt;- length(x2)
#calculate sample variance of each group
var1 &lt;- var(x1)
var2 &lt;- var(x2)
#calculate pooled variance between the two groups
pooled &lt;- ((n1-1)*var1 + (n2-1)*var2) / (n1+n2-2)
#display pooled variance
pooled
[1] 46.97143
</b>
The pooled variance between these two groups turns out to be <b>46.97143</b>.
<h2><span class="orange">What is Pooled Variance? (Definition & Example)</span></h2>
In statistics, <b>pooled variance</b> simply refers to the average of two or more group variances.
We use the word “pooled” to indicate that we’re “pooling” two or more group variances to come up with a single number for the common variance between the groups.
In practice, pooled variance is used most often in a  two sample t-test , which is used to determine whether or not two population means are equal.
The pooled variance between two samples is typically denoted as s<sub>p</sub><sup>2</sup> and is calculated as:
s<sub>p</sub><sup>2</sup> = ( (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> )  /  (n<sub>1</sub>+n<sub>2</sub>-2)
When the two sample sizes (n<sub>1</sub> and n<sub>2</sub>) are equal, the formula simplifies to:
s<sub>p</sub><sup>2</sup> = (s<sub>1</sub><sup>2</sup> + s<sub>2</sub><sup>2</sup> ) / 2
<h3>When to Calculate the Pooled Variance</h3>
When we want to compare two population means, there are two statistical tests we could potentially use:
<b>1.</b>  Two sample t-test : This test assumes the variances between the two samples are approximately equal. If we use this test, then we calculate the pooled variance.
<b>2.</b>  Welch’s t-test : This test <em>does not</em> assume the variances between the two samples are approximately equal. If we use this test, we <em>do not</em> calculate the pooled variance. Instead, we use a different formula.
To determine which test to use, we use the following rule of thumb:
<b>Rule of Thumb:</b> If the ratio of the larger variance to the smaller variance is less than 4, then we can assume the variances are approximately equal and use the two sample t-test.
For example, suppose sample 1 has a variance of 24.5 and sample 2 has a variance of 15.2. The ratio of the larger sample variance to the smaller sample variance would be calculated as:
<b>Ratio:</b> 24.5 / 15.2 = 1.61
Since this ratio is less than 4, we could assume that the variances between the two groups are approximately equal. Thus, we would use the two sample t-test which means we would calculate the pooled variance.
<h3>Example of Calculating the Pooled Variance</h3>
Suppose we want to know whether or not the mean weight between two different species of turtles is equal. To test this, we collect a random sample of turtles from each population with the following information:
<b>Sample 1:</b>
Sample size n<sub>1</sub> = 40
Sample variance s<sub>1</sub><sup>2</sup> = 18.5
<b>Sample 2:</b>
Sample size n<sub>2</sub> = 38
Sample variance s<sub>2</sub><sup>2</sup> = 6.7
Here is how to calculate the pooled variance between the two samples:
s<sub>p</sub><sup>2</sup> = ( (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> )  /  (n<sub>1</sub>+n<sub>2</sub>-2)
s<sub>p</sub><sup>2</sup> = ( (40-1)*18.5 + (38-1)*6.7 )  /  (40+38-2)
s<sub>p</sub><sup>2</sup> = (39*18.5 + 37*6.7 )  /  (76) = 12.755
The pooled variance is <b>12.755</b>.
Notice that the value for the pooled variance is located between the two original variances of 18.5 and 6.7. This makes sense considering the pooled variance is just a weighted average of the two sample variances.
<b>Bonus Resource:</b> Use this  Pooled Variance Calculator  to automatically calculate the pooled variance between two samples.
<h2><span class="orange">What is a Population Proportion?</span></h2>
In statistics, a <b>population proportion</b> refers to the fraction of individuals in a  population  with a certain characteristic.
For example, suppose 43.8% of individuals in a certain city support a new law. The value <b>0.438</b> represents a population proportion.
<h3>Formula for a Population Proportion</h3>
A population proportion always ranges between 0 and 1 (or 0% to 100% in percentage terms) and it is calculated as follows:
<b>p = X / N</b>
where:
<b>p:</b> The population proportion
<b>X:</b> The count of individuals in a population with a certain characteristic.
<b>N:</b> The total number of individuals in a population.
<h3>How to Estimate a Population Proportion</h3>
Since it is usually too time-consuming and costly to collect data for every individual in a population, we often collect data for a sample instead.
For example, suppose we want to know what proportion of residents in a certain city support a new law. If the population consists of 50,000 total residents, we may take a  simple random sample  of 1,000 residents:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample3.png">
We would then calculate the sample proportion as follows:
<b>p<U+0302> = x / n</b>
where:
<b>p<U+0302>:</b> The sample proportion
<b>x:</b> The count of individuals in the sample with a certain characteristic.
<b>n:</b> The total number of individuals in the sample.
We would then use this sample proportion to <em>estimate</em> the population proportion. For example, if 367 of the 1,000 residents in the sample supported the new law, the sample proportion would be calculated as 367 / 1,000 = <b>0.367</b>.
Thus, our best estimate for the proportion of residents in the population who supported the law would be <b>0.367</b>.
<h3>Confidence Interval for a Population Proportion</h3>
Although the sample proportion provides us with an estimate of the true population proportion, there’s no guarantee that the sample proportion will exactly match the population proportion.
For this reason, we typically construct a confidence interval – a range of values that are likely to contain the true population proportion with a high degree of confidence.
The formula to calculate a  confidence interval for a population proportion  is:
<b>Confidence Interval = p<U+0302>  +/-  z*√p<U+0302>(1-p<U+0302>) / n</b>
where:
<b>p<U+0302>: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
Notice that higher confidence levels correspond to larger z-values, which leads to wider confidence intervals. This means that, for example, a 95% confidence interval will be wider than a 90% confidence interval for the same set of data.
<h3>Example: Confidence Interval for a Population Proportion</h3>
Suppose we want to estimate the proportion of residents in a city  that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p<U+0302> = 0.56</b>
Here is how to find various confidence intervals for the population proportion:
<b>90% Confidence Interval: </b>0.56  +/-  1.645*(√.56(1-.56) / 100) = <b>[0.478, 0.642]</b>
<b>95% Confidence Interval: </b>0.56  +/-  1.96*(√.56(1-.56) / 100) = <b>[0.463, 0.657]</b>
<b>99% Confidence Interval: </b>0.56  +/-  2.58*(√.56(1-.56) / 100) = <b>[0.432, 0.688]</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Confidence Interval for Proportion Calculator .</em>
<h2><span class="orange">How to Create a Population Pyramid in Excel</span></h2>
A <b>population pyramid</b> is a graph that shows the age and gender distribution of a given population. It’s useful for understanding the composition of a population and the trend in population growth.
This tutorial explains how to create the following population pyramid in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel8.png">
<h3>Example: Population Pyramid in Excel</h3>
Use the following steps to create a population pyramid in Excel.
<b>Step 1: Input the data.</b>
First, input the population counts (by age bracket) for males and females in separate columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel1.png">
<b>Step 2: Calculate the percentages.</b>
Next, use the following formulas to calculate the percentages for both males and females:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel2.png">
<b>Step 3: Insert a 2-D Stacked Bar Chart.</b>
Next, highlight cells D2:E:11. In the <b>Charts </b>group within the <b>Insert </b>tab, click on the option that says <b>2-D stacked bar chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel.png">
The following chart will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel3.png">
<b>Step 4: Modify the appearance of the population pyramid.</b>
Lastly, we will modify the appearance of the population pyramid to make it look better.
<b>Remove the gap width.</b>
Right click any bar on the chart. Then click <b>Format Data Series…</b>
Change <b>Gap Width </b>to 0%.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel4.png">
<b>Add a black border to each bar.</b>
Click the paint bucket icon.
Click <b>Border</b>. Then click <b>Solid line</b>.
Change the Color to black.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel5.png">
<b>Display the x-axis labels as positive numbers.</b>
Right click on the x-axis. Then click <b>Format Axis…</b>
Click <b>Number</b>.
Under <b>Format Code</b>, type <b>0.0;[Black]0.0 </b>and click <b>Add</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel6.png">
<b>Move the vertical axis to the left-hand side of the chart.</b>
Right click on the y-axis. Then click <b>Format Axis…</b>
Click <b>Labels</b>. Set <b>Label Position </b>to <b>Low</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel7.png">
<b>Change the title of the graph and the colors as needed. Also click on any of the vertical grid lines and click delete.</b>
The final result should look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/popPyramidExcel8.png">
<h2><span class="orange">How to Create a Population Pyramid in R</span></h2>
A <b>population pyramid</b> is a graph that shows the age and gender distribution of a given population. It is a useful chart for easily understanding the make-up of a population as well as the current trend in population growth.
If a population pyramid has a rectangular shape, it’s an indication that a population is growing at a slower rate; older generations are being replaced by new generations of roughly the same size.
If a population pyramid has a pyramid shape, it’s an indication that a population is growing at a faster rate; older generations are producing larger new generations.
Within the chart, the gender is shown on the left and right sides, the age is shown on the y-axis, and the percentage or amount of the population is shown on the x-axis.
This tutorial explains how to create a population pyramid in R.
<h3>Creating a Population Pyramid in R</h3>
Suppose we have the following dataset that shows the percentage make-up of a population according to age (0 to 100 years) and gender(M = “Male”, F = “Female”):
<b>#make this example reproducible
set.seed(1)
#create data frame
data &lt;- data.frame(age = rep(1:100, 2), gender = rep(c("M", "F"), each = 100))
#add population variable
data$population &lt;- 1/sqrt(data$age) * runif(200, 10000, 15000)
#convert population variable to percentage
data$population &lt;- data$population / sum(data$population) * 100
#view first six rows of dataset
head(data)
#  age gender population
#1   1      M   2.424362
#2   2      M   1.794957
#3   3      M   1.589594
#4   4      M   1.556063
#5   5      M   1.053662
#6   6      M   1.266231
</b>
We can create a basic population pyramid for this dataset using the <b>ggplot2 </b>library:
<b>#load <em>ggplot2
</em>library(ggplot2)
#create population pyramid
ggplot(data, aes(x = age, fill = gender, y = ifelse(test = gender == "M",            yes = -population, no = population))) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(data$population) * c(-1,1)) +
  coord_flip()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/pop_pyramid1_1.jpg">
<h3>Adding Titles & Labels</h3>
We can add both titles and axis labels to the population pyramid using the<b> labs()</b> argument:
<b>ggplot(data, aes(x = age, fill = gender, y = ifelse(test = gender == "M",            yes = -population, no = population))) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(data$population) * c(-1,1)) +
  labs(title = "Population Pyramid", x = "Age", y = "Percent of population") +
  coord_flip()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/pop_pyramid2.jpg">
<h3>Modifying the Colors</h3>
We can modify the two colors used to represent the genders by using the <b>scale_colour_manual() </b>argument:
<b>ggplot(data, aes(x = age, fill = gender, y = ifelse(test = gender == "M",            yes = -population, no = population))) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(data$population) * c(-1,1)) +
  labs(title = "Population Pyramid", x = "Age", y = "Percent of population") +
  scale_colour_manual(values = c("pink", "steelblue"),      aesthetics = c("colour", "fill")) +
  coord_flip()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/pop_pyramid3_1.jpg">
<h2>Multiple Population Pyramids</h2>
It’s also possible to plot several population pyramids together using the <b>facet_wrap()</b> argument. For example, suppose we have demographic data for countries <em>A, B, </em>and <em>C. </em>The following code illustrates how to create one population pyramid for each country:
<b>#make this example reproducible
set.seed(1)
#create data frame
data_multiple &lt;- data.frame(age = rep(1:100, 6),   gender = rep(c("M", "F"), each = 300),   country = rep(c("A", "B", "C"), each = 100, times = 2))
#add population variable
data_multiple$population &lt;- round(1/sqrt(data_multiple$age)*runif(200, 10000, 15000), 0)
#view first six rows of dataset
head(data_multiple)
#  age gender country population
#1   1      M       A      11328
#2   2      M       A       8387
#3   3      M       A       7427
#4   4      M       A       7271
#5   5      M       A       4923
#6   6      M       A       5916
#create one population pyramid per country
ggplot(data_multiple, aes(x = age, fill = gender,          y = ifelse(test = gender == "M",                     yes = -population, no = population))) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(data_multiple$population) * c(-1,1)) +
  labs(y = "Population Amount") + 
  coord_flip() +
  facet_wrap(~ country) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) #rotate x-axis labels</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/pop_pyramid4.jpg">
<h2>Modifying the Theme</h2>
Lastly, we can modify the theme of the charts. For example, the following code uses <b>theme_classic()</b> to give the charts a more minimalist look:
<b>ggplot(data_multiple, aes(x = age, fill = gender,          y = ifelse(test = gender == "M",                     yes = -population, no = population))) + 
  geom_bar(stat = "identity") +
  scale_y_continuous(labels = abs, limits = max(data_multiple$population) * c(-1,1)) +
  labs(y = "Population Amount") + 
  coord_flip() +
  facet_wrap(~ country) +
  theme_classic() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/pop_pyramid5.jpg">
Or you can use custom ggthemes. For a complete list of ggthemes, check out  the documentation page .
<h2><span class="orange">How to Create a Population Pyramid in Python</span></h2>
A <b>population pyramid</b> is a graph that shows the age and gender distribution of a given population. It’s useful for understanding the composition of a population and the trend in population growth.
This tutorial explains how to create the following population pyramid in Python:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/popPyramidPython1.png">
<h3>Population Pyramid in Python</h3>
Suppose we have the following dataset that displays the total population of males and females by age group for a given country:
<b>#import libraries 
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
#create dataframe
df = pd.DataFrame({'Age': ['0-9','10-19','20-29','30-39','40-49','50-59','60-69','70-79','80-89','90+'],     'Male': [9000, 14000, 22000, 26000, 34000, 32000, 29000, 22000, 14000, 3000],     'Female': [8000, 15000, 19000, 28000, 35000, 34000, 28000, 24000, 17000, 5000]})
#view dataframe 
df
    Age  Male Female
0   0-9  9000   8000
1 10-19 14000  15000
2 20-29 22000  19000
3 30-39 26000  28000
4 40-49 34000  35000
5 50-59 32000  34000
6 60-69 29000  28000
7 70-79 22000  24000
8 80-89 14000  17000
9   90+  3000   5000
</b>
We can use the following code to create a population pyramid for the data:
<b>#define x and y limits
y = range(0, len(df))
x_male = df['Male']
x_female = df['Female']
#define plot parameters
fig, axes = plt.subplots(ncols=2, sharey=True, figsize=(9, 6))
#specify background color and plot title
fig.patch.set_facecolor('xkcd:light grey')
plt.figtext(.5,.9,"Population Pyramid ", fontsize=15, ha='center')
    
#define male and female bars
axes[0].barh(y, x_male, align='center', color='royalblue')
axes[0].set(title='Males')
axes[1].barh(y, x_female, align='center', color='lightpink')
axes[1].set(title='Females')
#adjust grid parameters and specify labels for y-axis
axes[1].grid()
axes[0].set(yticks=y, yticklabels=df['Age'])
axes[0].invert_xaxis()
axes[0].grid()
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/popPyramidPython1.png">
From the plot we can see that the distribution of males and females is fairly symmetrical, with most of the population falling in the middle-age brackets. By simply looking at this one plot, we can get a decent idea about the demographics of this particular country.
Note that you can adjust the colors of the plot background and the individual bars by specifying colors from the  matplotlib color list .
For example, we could specify ‘hotpink’ and ‘dodgerblue’ to be used with a ‘beige’ background:
<b>fig.patch.set_facecolor('xkcd:beige')
    
axes[0].barh(y, x_male, align='center', color='dodgerblue')
axes[1].barh(y, x_female, align='center', color='hotpink')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/popPyramidPython2.png">
Feel free to modify the color scheme based on what you think looks best.
<h2><span class="orange">Population vs. Sample Standard Deviation: When to Use Each</span></h2>
<b>Standard deviation</b> is one of the most common ways to measure the  spread of values  in a dataset.
It turns out that there are two different types of standard deviations you can calculate, depending on the type of data you’re working with.
<b>1. Population standard deviation</b>
You should calculate the population standard deviation when the dataset you’re working with represents an entire population, i.e. every value that you’re interested in.
The formula to calculate a population standard deviation, denoted as σ, is:
<b>σ = √Σ(x<sub>i</sub> – μ)<sup>2</sup> / N</b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in a dataset
<b>μ</b>: The population mean
<b>N</b>: The population size
<b>2. Sample standard deviation</b>
You should calculate the sample standard deviation when the dataset you’re working with represents a a sample taken from a larger population of interest.
The formula to calculate a sample standard deviation, denoted as <em>s</em>, is:
<b>s = √Σ(x<sub>i</sub> – x<U+0304>)<sup>2</sup> / (n – 1)</b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in a dataset
<b>x<U+0304></b>: The sample mean
<b>n</b>: The sample size
<h3>Population vs. Sample Standard Deviation: The Difference</h3>
From the formulas above, we can see that there is one tiny difference between the population and the sample standard deviation: <b>When calculating the sample standard deviation, we divided by n-1 instead of N.</b>
The reason for this is because when we calculate the sample standard deviation, we tend to underestimate the true variability in the population. In other words, our estimate of the true population standard deviation is biased.*
To correct this bias, we divide by n-1. This has been shown to make the sample standard deviation an unbiased estimate of the population standard deviation.
*Proof of this is beyond the scope of this article. For a mathematical proof, refer to  this post  from Stack Exchange.
<h3>Population vs. Sample Standard Deviation: When to Use Each</h3>
Use the following practice problems to gain a better understanding of when you should use population vs sample standard deviation.
<b>Practice Problem 1: Sports</b>
Suppose a basketball coach wants to summarize the mean and standard deviation of points scored by the 12 players on his team.
When calculating the standard deviation of points scored, should he use the population or sample standard deviation formula?
Answer: He should use the <b>population standard deviation</b> because he is only interested in the points scored by his players and not any other players on any other team.
<b>Practice Problem 2: Height</b>
Suppose a gym teacher wants to summarize the mean and standard deviation of heights of students in his class.
When calculating the standard deviation of height, should he use the population or sample standard deviation formula?
Answer: He should use the <b>population standard deviation</b> because he is only interested in the height of students in this one particular class.
<b>Practice Problem 3: Biology</b>
Suppose a biologist wants to summarize the mean and standard deviation of the weight of a particular species of turtles. She decides to go out and collect a simple random sample of 20 turtles from the population.
When calculating the standard deviation of weights, should she use the population or sample standard deviation formula?
Answer: She should use the <b>sample standard deviation</b> because she is interested in the weights of the entire population of turtles, not just the weights of the turtles in her sample.
<b>Practice Problem 4: Manufacturing</b>
Suppose an inspector wants to summarize the mean and standard deviation of the weight of tires produced at a certain factory. He decides to collect a simple random sample of 40 tires from the factory and weighs each of them.
When calculating the standard deviation of weights, should he use the population or sample standard deviation formula?
Answer: He should use the <b>sample standard deviation</b> because he is interested in the weights of all tires produced at this factory, not just the weights of the tires in his sample.
<h2><span class="orange">Population vs. Sample: What’s the Difference?</span></h2>
Often in statistics we’re interested in collecting data so that we can answer some research question.
For example, we might want to answer the following questions:
<b>1.</b> What is the median household income in Miami, Florida?
<b>2.</b> What is the mean weight of a certain population of turtles?
<b>3.</b> What percentage of residents in a certain county support a certain law?
In each scenario, we are interested in answering some question about a <b>population</b>, which represents every possible individual element that we’re interested in measuring.
However, instead of collecting data on every individual in a population we instead collect data on a <b>sample</b> of the population, which represents a portion of the population.
<b>Population</b>: Every possible individual element that we are interested in measuring.
 
<b>Sample:</b> A portion of the population.
Here is an example of a population vs. a sample in the three intro examples.
<b>Example 1: What is the median household income in Miami, Florida?</b>
The entire <b>population</b> might include 500,000 households, but we might only collect data on a <b>sample</b> of 2,000 total households.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample1.png">
<b>2. What is the mean weight of a certain population of turtles?</b>
The entire <b>population</b> might include 800 turtles, but we might only collect data on a <b>sample</b> of 30 turtles.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample2.png">
<b>3. What percentage of residents in a certain county support a certain law?</b>
The entire <b>population</b> might include 50,000 residents, but we might only collect data on a <b>sample</b> of 1,000 residents.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample3.png">
<h3>Why Use Samples?</h3>
There are several reasons that we typically collect data on samples instead of entire populations, including:
<b>1. It is too time-consuming to collect data on an entire population</b>. For example, if we want to know the median household income in Miami, Florida, it might take months or even years to go around and gather income for each household. By the time we collect all of this data, the population may have changed or the research question of interest might no longer be of interest.
<b>2. It is too costly to collect data on an entire population.</b> It is often too expensive to go around and collect data for every individual in a population, which is why we instead choose to collect data on a sample instead.
<b>3. It is unfeasible to collect data on an entire population.</b> In many cases it’s simply not possible to collect data for <em>every</em> individual in a population. For example, it may be extraordinarily difficult to track down and weigh every turtle in a certain population that we’re interested in. 
By collecting data on samples, we’re able to gather information about a given population much faster and cheaper.
And if our sample is  representative of the population , then we can generalize the findings from a sample to the larger population with a high level of confidence.
<h3>The Importance of Representative Samples</h3>
When we collect a sample from a population, we ideally want the sample to be like a “mini version” of our population.
For example, suppose we want to understand the movie preferences of students in a certain school district that has a population of 5,000 total students. Since it would take too long to survey every individual student, we might instead take a sample of 100 students and ask them about their preferences. 
If the overall student population is composed of 50% girls and 50% boys, our sample would not be representative if it included 90% boys and only 10% girls.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/rep_sample1.jpg">
Or if the overall population is composed of equal parts freshman, sophomores, juniors, and seniors, then our sample would not be representative if it only included freshman. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/rep_sample2.jpg"423">
A sample is representative of a population if the characteristics of the individuals in the sample closely matches the characteristics of the individuals in the overall population.
When this occurs, we can generalize the findings from the sample to the overall population with confidence. 
<h3>How to Obtain Samples</h3>
There are  many different methods  we can use to obtain samples from populations. 
To maximize the chances that we obtain a representative sample, we can use one of the three following methods:
<b>Simple random sampling:</b> Randomly select individuals through the use of a random number generator or some means of random selection.
<b>Systematic random sampling:</b> Put every member of a population into some order. Choose a random starting point and select every n<sup>th</sup> member to be in the sample.
<b>Stratified random sampling:</b> Split a population into groups. Randomly select some members from each group to be in the sample. 
In each of these methods, every individual in the population has an equal probability of being included in the sample. This maximizes the chances that we obtain a sample that is a “mini version” of the population.
<h2><span class="orange">How to Fix: SyntaxError: positional argument follows keyword argument</span></h2>
One error you may encounter in Python is:
<b>SyntaxError: positional argument follows keyword argument
</b>
This error occurs when you use a <b>positional argument</b> in a function after using a <b>keyword argument</b>.
Here’s the difference between the two:
<b>Positional arguments</b> are ones that have no “keyword” in front of them.
Example: <b>my_function(2, 2)</b>
<b>Keyword arguments</b> are ones that do have a “keyword” in front of them.
Example: <b>my_function(a=2, b=2)</b>
If you use a positional argument after a keyword argument then Python will throw an error.
Example: <b>my_function(a=2, 2)</b>
The following example shows how this error may occur in practice.
<h3>Example: Positional Argument Follows Keyword Argument</h3>
Suppose we have the following function in Python that multiplies two values and then divides by a third:
<b>def do_stuff(a, b):
    return a * b / c
</b>
The following examples show valid and invalid ways to use this function:
<b>Valid Way #1: All Positional Arguments</b>
The following code shows how to use our function with all positional arguments:
<b>do_stuff(4, 10, 5)
8.0
</b>
No error is thrown because Python knows exactly which values to use for each argument in the function.
<b>Valid Way #2: All Keyword Arguments</b>
The following code shows how to use our function with all keyword arguments:
<b>do_stuff(a=4, b=10, c=5)
8.0
</b>
Once again no error is thrown because Python knows exactly which values to use for each argument in the function.
<b>Valid Way #3: Positional Arguments Before Keyword Arguments</b>
The following code shows how to use our function with positional arguments used <em>before</em> keyword arguments:
<b>do_stuff(4, b=10, c=5)
8.0
</b>
No error is thrown because Python knows that the value <b>4</b> must be assigned to the <em>a</em> argument.
<b>Invalid Way: Positional Arguments After Keyword Arguments</b>
The following code shows how we may attempt to use the function with positional arguments used <i>after </i>keyword arguments:
<b>do_stuff(a=4, 10, 5)
SyntaxError: positional argument follows keyword argument
</b>
An error is thrown because we used positional arguments after keyword arguments.
Specifically, Python doesn’t know if the <b>10</b> and <b>5</b> values should be assigned to arguments <em>b</em> or <em>c</em> so it’s unable to execute the function.
<h2><span class="orange">Positive Predictive Value vs. Sensitivity: What’s the Difference?</span></h2>
One of the most common ways to assess the performance of a  classification model  is to create a confusion matrix, which summarizes the predicted outcomes from the model vs. the actual outcomes from the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/pospred1-2.jpg"360">
Two metrics that we’re often interested in within a confusion matrix are <b>positive predictive value</b> and <b>sensitivity</b>.
<b>Positive predictive value</b> is the probability that an observation with a positive predicted outcome actually <em>has</em> a positive outcome.
It is calculated as:
<b>Positive predictive value </b>= True Positives / (True Positives + False Positives)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/pospred2.jpg"548">
<b>Sensitivity</b> is the probability that an observation with a positive outcome actually has a positive predicted outcome.
It is calculated as:
<b>Sensitivity </b>= True Positives / (True Positives + False Negatives)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/pospred3.jpg"385">
The following example shows how to calculate both of these metrics in practice.
<h3>Example: Calculating Positive Predictive Value & Sensitivity</h3>
Suppose a doctor uses a  logistic regression model  to predict whether or not 400 individuals have a certain disease.
The following confusion matrix summarizes the predictions made by the model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/pospred5.jpg"520">
We would calculate the <b>positive predictive value</b> as:
<b>Positive predictive value </b>= True Positives / (True Positives + False Positives)
<b>Positive predictive value </b>= 15 / (15 + 10)
<b>Positive predictive value </b>= 0.60
This tells us that the probability that an individual who receives a positive test result actually <em>has</em> the disease is <b>0.60</b>.
We would calculate the <b>sensitivity</b> as:
<b>Sensitivity</b> = True Positives / (True Positives + False Negatives)
<b><b>Sensitivity</b> </b>= 15 / (15 + 5)
<b>Sensitivity</b> = 0.75
This tells us that the probability that an individual who has the disease will actually receive a positive test result is <b>0.75</b>.

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
