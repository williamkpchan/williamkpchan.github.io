<base target="_blank"><html><head><title>statologyContents 2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 2"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 2</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">Bubble Chart in Google Sheets (Step-by-Step)</span></h2>
A <b>bubble chart</b> is a type of chart that allows you to visualize three variables in a dataset at once.
The first two variables are used as (x,y) coordinates on a scatterplot and the third variable is used to depict size.
This tutorial provides a step-by-step example of how to create the following bubble chart in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets8.png">
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset that shows various statistics for 10 different professional basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets1.png">
Note that we have to use the following format in order to create a bubble chart in Google Sheets:
<b>Column A:</b> Labels
<b>Column B:</b> X-axis value
<b>Column C:</b> Y-axis value
<b>Column D:</b> Color
<b>Column E:</b> Size
<h3>Step 2: Create the Bubble Chart</h3>
Next, highlight each of the columns of data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets2.png">
Next, click the <b>Insert</b> tab and then click <b>Chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets3.png">
Google Sheets will insert a histogram by default. To convert this into a bubble chart, simply click <b>Chart type</b> in the Chart editor that appears on the right of the screen. Then scroll down and click <b>Bubble chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets4.png">
This will automatically produce the following bubble chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets5.png">
<h3>Step 3: Modify the Bubble Chart</h3>
Next, we can modify the appearance of the bubble chart to make it easier to read.
First, double click the vertical axis. In the Chart editor that appears to the right, change the Min and Max axis values to 75 and 115, respectively.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets6.png">
Next, double click the horizontal axis of the chart and change the Min and Max values to 90 and 115, respectively. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets7.png">
Next, double click the chart again. In the Chart editor, select the <b>Bubble</b> tab and change the Opacity to 20% and the Border Color to black.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets7-1.png">
Next, click the <b>Chart & axis titles</b> tab and change the text for the title, subtitle, horizontal axis title, and vertical axis title.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets7-2.png">
Here is what the final bubble chart will look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bubbleSheets8.png">
<h2><span class="orange">How to Create a Bubble Chart in R</span></h2>
A <b>bubble chart</b> is a type of chart that allows you to visualize three variables in a dataset at once.
The first two variables are used as (x,y) coordinates on a scatterplot and the third variable is used to depict size.
You can use the following basic syntax to create a bubble chart in R:
<b>library(ggplot2)
#create bubble chart
ggplot(df, aes(x=x_var, y=y_var, size=size_var)) +
  geom_point(alpha=0.5) +
  scale_size(range=c(2, 10), name='Legend Name')
</b>
The following example shows how to use this syntax to create a bubble chart in practice.
<b>Note</b>: The <b>alpha</b> argument specifies that the circles in the chart should be partially transparent. The <b>range</b> argument allows you to set the minimum and maximum radius values for the circles in the chart.
<h2>Example: Create a Bubble Chart in R</h2>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'), points=c(8, 11, 13, 13, 15, 18, 22, 27, 32), assists=c(4, 3, 6, 5, 4, 7, 8, 11, 6), minutes=c(9, 12, 15, 20, 36, 30, 31, 40, 43))
#view data frame
df
  team points assists minutes
1    A      8       4       9
2    A     11       3      12
3    A     13       6      15
4    B     13       5      20
5    B     15       4      36
6    B     18       7      30
7    C     22       8      31
8    C     27      11      40
9    C     32       6      43
</b>
We can use the following syntax to create a bubble chart that displays <b>assists</b> on the x-axis, <b>points</b> on the y-axis, and uses <b>minutes</b> to determine the size of the circles:
<b>library(ggplot2)
#create bubble chart
ggplot(df, aes(x=assists, y=points, size=minutes)) +
  geom_point(alpha=0.5) +
  scale_size(range=c(2, 10), name='Minutes Played')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/bubble1.jpg"587">
You can change the color of all of the circles by using the <b>color</b> argument within the <b>geom_point()</b> function:
<b>library(ggplot2)
#create bubble chart with blue circles
ggplot(df, aes(x=assists, y=points, size=minutes)) +
  geom_point(alpha=0.5, color='steelblue')  +
  scale_size(range=c(2, 10), name='Minutes Played')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/bubble2.jpg">
Alternatively, you can use the <b>color</b> argument within <b>aes()</b> to make the color of each circle based on the value of another variable in the data frame:
<b>library(ggplot2)
#create bubble chart and color circles based on value of team variable
ggplot(df, aes(x=assists, y=points, size=minutes, color=team)) +
  geom_point(alpha=0.5)  +
  scale_size(range=c(2, 10), name='Minutes Played')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/bubble3.jpg">
The color of each circle in the plot is now dependent on the value for the <b>team</b> variable.
<b>Note</b>: Feel free to play around with the minimum and maximum values in the <b>range</b> argument to increase or decrease the size of the circles in the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to create other common charts in R:
 How to Create a Pareto Chart in R 
 How to Plot Multiple Lines in One Chart in R 
 How to Plot Multiple Boxplots in One Chart in R 
<h2><span class="orange">How to Easily Create a Bump Chart in R Using ggplot2</span></h2>
A <b>bump chart</b> is a type of chart that shows rankings of different groups over time instead of absolute values to emphasize the order of the groups instead of the magnitude of change.
This tutorial explains how to easily create a bump chart in R using ggplot2.
<h2>Example: Creating a Bump Chart</h2>
To create a bump chart in R, we first need to load two packages: <b>dplyr</b> and <b>ggplot2</b>:
<b>library(ggplot2) #for creating bump chart
library(dplyr) #for manipulating data</b>
Next, we’ll create some data to work with:
<b>#set the seed to make this example reproducible
set.seed(10)
data &lt;- data.frame(team = rep(LETTERS[1:5], each = 10),   random_num = runif(50),   day = rep(1:10, 5))
data &lt;- data %>%
  group_by(day) %>%
  arrange(day, desc(random_num), team) %>% 
  mutate(rank = row_number()) %>%
  ungroup()
head(data)
#  team  random_num   day  rank          
#1 C          0.865     1     1
#2 B          0.652     1     2
#3 D          0.536     1     3
#4 A          0.507     1     4
#5 E          0.275     1     5
#6 C          0.615     2     1</b>
This data frame simply shows the “rank” of five different teams across a time span of 10 days.
We can use ggplot2 to create a bump chart to visualize the rank of each team during each day over this time span:
<b>ggplot(data, aes(x = day, y = rank, group = team)) +
  geom_line(aes(color = team, alpha = 1), size = 2) +
  geom_point(aes(color = team, alpha = 1), size = 4) +
  scale_y_reverse(breaks = 1:nrow(data))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/bumpChart2.jpg">
This bump chart shows the data in the format that we want, but it’s fairly ugly. With some aesthetic changes, we can make this chart look much better.
<h2>Styling the Bump Chart</h2>
To make the chart look better, we can use the following theme created by  Dominik Koch :
<b>my_theme &lt;- function() {
  # Colors
  color.background = "white"
  color.text = "#22211d"
  # Begin construction of chart
  theme_bw(base_size=15) +
    # Format background colors
    theme(panel.background = element_rect(fill=color.background,                          color=color.background)) +
    theme(plot.background  = element_rect(fill=color.background,                          color=color.background)) +
    theme(panel.border     = element_rect(color=color.background)) +
    theme(strip.background = element_rect(fill=color.background,                          color=color.background)) +
    # Format the grid
    theme(panel.grid.major.y = element_blank()) +
    theme(panel.grid.minor.y = element_blank()) +
    theme(axis.ticks       = element_blank()) +
    # Format the legend
    theme(legend.position = "none") +
    # Format title and axis labels
    theme(plot.title       = element_text(color=color.text, size=20, face = "bold")) +
    theme(axis.title.x     = element_text(size=14, color="black", face = "bold")) +
    theme(axis.title.y     = element_text(size=14, color="black", face = "bold",                          vjust=1.25)) +
    theme(axis.text.x      = element_text(size=10, vjust=0.5, hjust=0.5,                          color = color.text)) +
    theme(axis.text.y      = element_text(size=10, color = color.text)) +
    theme(strip.text       = element_text(face = "bold")) +
    # Plot margins
    theme(plot.margin = unit(c(0.35, 0.2, 0.3, 0.35), "cm"))
}</b>
We’ll create the bump chart again, but this time we’ll remove the legend, add some chart labels, and use the theme defined in the code above:
<b>ggplot(data, aes(x = as.factor(day), y = rank, group = team)) +
  geom_line(aes(color = team, alpha = 1), size = 2) +
  geom_point(aes(color = team, alpha = 1), size = 4) +
  geom_point(color = "#FFFFFF", size = 1) +
  scale_y_reverse(breaks = 1:nrow(data)) + 
  scale_x_discrete(breaks = 1:10) +
  theme(legend.position = 'none') +
  geom_text(data = data %>% filter(day == "1"),
            aes(label = team, x = 0.5) , hjust = .5,
            fontface = "bold", color = "#888888", size = 4) +
  geom_text(data = data %>% filter(day == "10"),
            aes(label = team, x = 10.5) , hjust = 0.5,
            fontface = "bold", color = "#888888", size = 4) +
  labs(x = 'Day', y = 'Rank', title = 'Team Ranking by Day') +
  my_theme() </b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/bumpChart4.jpg">
We can also easily highlight one of the lines by adding a <b>scale_color_manual()</b> argument. For example, in the following code we make the line for team A purple and the line for all of the other lines grey:
<b>ggplot(data, aes(x = as.factor(day), y = rank, group = team)) +
  geom_line(aes(color = team, alpha = 1), size = 2) +
  geom_point(aes(color = team, alpha = 1), size = 4) +
  geom_point(color = "#FFFFFF", size = 1) +
  scale_y_reverse(breaks = 1:nrow(data)) + 
  scale_x_discrete(breaks = 1:10) +
  theme(legend.position = 'none') +
  geom_text(data = data %>% filter(day == "1"),
            aes(label = team, x = 0.5) , hjust = .5,
            fontface = "bold", color = "#888888", size = 4) +
  geom_text(data = data %>% filter(day == "10"),
            aes(label = team, x = 10.5) , hjust = 0.5,
            fontface = "bold", color = "#888888", size = 4) +
  labs(x = 'Day', y = 'Rank', title = 'Team Ranking by Day') +
  my_theme() +
  scale_color_manual(values = c('purple', 'grey', 'grey', 'grey', 'grey'))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/bumpChart5.jpg">
We could also highlight more than one line if we’d like:
<b>ggplot(data, aes(x = as.factor(day), y = rank, group = team)) +
  geom_line(aes(color = team, alpha = 1), size = 2) +
  geom_point(aes(color = team, alpha = 1), size = 4) +
  geom_point(color = "#FFFFFF", size = 1) +
  scale_y_reverse(breaks = 1:nrow(data)) + 
  scale_x_discrete(breaks = 1:10) +
  theme(legend.position = 'none') +
  geom_text(data = data %>% filter(day == "1"),
            aes(label = team, x = 0.5) , hjust = .5,
            fontface = "bold", color = "#888888", size = 4) +
  geom_text(data = data %>% filter(day == "10"),
            aes(label = team, x = 10.5) , hjust = 0.5,
            fontface = "bold", color = "#888888", size = 4) +
  labs(x = 'Day', y = 'Rank', title = 'Team Ranking by Day') +
  my_theme() +
  scale_color_manual(values = c('purple', 'steelblue', 'grey', 'grey', 'grey'))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/bumpChart6.jpg">
<h2><span class="orange">An Introduction to the c() Function in R</span></h2>
You can use the <b>c()</b> function in R to perform three common tasks:
<b>1.</b> Create a vector.
<b>2.</b> Concatenate multiple vectors.
<b>3.</b> Create columns in a data frame.
This function uses the following basic syntax:
<b>my_vector &lt;- c(value1, value2, value3, ...)</b>
Note that <b>c()</b> stands for “combine” because it is used to <em>combine</em> several values or objects into one.
The following examples show how to use this function in practice.
<h3>Example 1: Use c() to Create a Vector</h3>
The following code shows how to use <b>c()</b> to create a numeric vector:
<b>#create numeric vector
numeric_vector &lt;- c(4, 7565, 15, 93.22, 100, 50, 0)
#display numeric vector
numeric_vector 
[1]    4.00 7565.00   15.00   93.22  100.00   50.00    0.00
</b>
We can also use <b>c()</b> to create a character vector:
<b>#create character vector
char_vector &lt;- c('A', 'C', 'L', 'M', 'O')
#display character vector
char_vector 
[1] "A" "C" "L" "M" "O"
</b>
<h3>Example 2: Use c() to Concatenate Multiple Vectors</h3>
The following code shows how to use <b>c()</b> to concatenate multiple vectors into one:
<b>#define two vectors
vec1 &lt;- c(4, 15, 19, 18)
vec2 &lt;- c(10, 100, 40, 20, 80, 85)
#concatenate vectors into one
vec3 &lt;- c(vec1, vec2)
#view concatenated vector
vec3
[1]   4  15  19  18  10 100  40  20  80  85
</b>
<h3>Example 3: Use c() to Create Columns in a Data Frame</h3>
The following code shows how to use <b>c()</b> to create columns in a data frame in R:
<b>#create data frame with three columns
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 90, 86, 88, 95), assists=c(33, 28, 31, 39, 34))
#view data frame
df
  team points assists
1    A     99      33
2    B     90      28
3    C     86      31
4    D     88      39
5    E     95      34</b>
The result is a data frame with three columns, each created by using the <b>c()</b> function.  
<h2><span class="orange">How to Calculate CAGR in Google Sheets (Step-by-Step)</span></h2>
The acronym <b>CAGR</b> stands for <b>compound annual growth rate</b>, which is the average annualized revenue growth rate during a certain time period.
The formula to calculate CAGR is as follows:
CAGR = (future value / present value)<sup>1/periods</sup> – 1
The following examples show two equivalent ways to calculate CAGR in Google Sheets.
<h3>Method 1: Calculate CAGR Manually</h3>
We can use the following formula to calculate CAGR manually in Google Sheets:
<b>=(ENDING_VALUE/STARTING_VALUE)^(1/PERIODS)-1
</b>
The following screenshot shows how to use this formula to calculate CAGR for an investment that started at $1,000 and ended at $5,000 after 9 investment periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cagr1.png">
The CAGR is <b>19.58%</b>. This represents the compound annual growth rate of the investment during these 9 investment periods.
We can confirm this answer is correct by calculating the growth of an initial $1,000 investment if it grew consistently at 19.58% each year for 9 years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cagr2.png">
<h3>Method 2: Calculate CAGR Using RRI Function</h3>
Another way to calculate CAGR in Google Sheets is by using the <b>RRI</b> function, which uses the following syntax:
<b>RRI(number of periods, starting value, ending value)</b>
The following screenshot shows how to use this function in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cagr3.png">
The CAGR is <b>19.58%</b>.
This matches the value that we calculated manually using the previous method.
<h2><span class="orange">How to Calculate Degrees of Freedom for Any T-Test</span></h2>
In statistics, there are three commonly used t-tests:
 One Sample t-test : Used to compare a population mean to some value.
 Two Sample t-test : Used to compare two population means.
 Paired Samples t-test : Used to compare two population means when each observation in one sample can be paired with an observation in the other sample.
When performing each t-test, you’ll have to calculate a test statistic and a corresponding <b>degrees of freedom</b>.
Here is how to calculate the degrees of freedom for each type of test:
<b>One Sample t-test: df = n-1</b> where <em>n</em> is the total number of observations.
<b>Two Sample t-test: df = n<sub>1</sub> + n<sub>2</sub> – 2</b> where <em>n<sub>1</sub></em>, <em>n<sub>2</sub></em> are the total observations from each sample.
<b>Paired Samples t-test: n-1</b> where <em>n</em> is the total number of pairs.
The following examples show how to calculate the degrees of freedom for each type of t-test in practice.
<h2>Example 1: Degrees of Freedom for One Sample t-test</h2>
Suppose we want to know whether or not the mean weight of a certain species of turtle is equal to 310 pounds.
Suppose we collect a random sample of turtles with the following information:
Sample size n = 40
Sample mean weight x = 300
Sample standard deviation s = 18.5
We will perform a one sample t-test with the following hypotheses:
H<sub>0</sub>: μ = 310 (population mean is equal to 310 pounds)
H<sub>A</sub>: μ ≠ 310 (population mean is not equal to 310 pounds)
First, we’ll calculate the test statistic:
t = (x – μ) / (s/√n) = (300-310) / (18.5/√40) = -3.4187
Next, <b>we’ll calculate the degrees of freedom:</b>
df = n -1 = 40 – 1 = 39
Lastly, we’ll plug in the test statistic and degrees of freedom into the  T Score to P Value Calculator  to find that the p-value is <b>0.00149</b>.
Since this p-value is less than our significance level α = 0.05, we reject the null hypothesis. We have sufficient evidence to say that the mean weight of this species of turtle is not equal to 310 pounds.
<h2>Example 2: Degrees of Freedom for Two Sample t-test</h2>
Suppose we want to know whether or not the mean weight between two different species of turtles is equal.
Suppose we collect a random sample of turtles from each population with the following information:
<b>Sample 1:</b>
Sample size n<sub>1</sub> = 40
Sample mean weight x<sub>1</sub> = 300
Sample standard deviation s<sub>1</sub> = 18.5
<b>Sample 2:</b>
Sample size n<sub>2</sub> = 38
Sample mean weight x<sub>2</sub> = 305
Sample standard deviation s<sub>2</sub> = 16.7
We will perform a two sample t-test with the following hypotheses:
H<sub>0</sub>: μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
H<sub>A</sub>: μ<sub>1</sub> ≠ μ<sub>2</sub> (the two population means are not equal)
First, we will calculate the  pooled standard deviation  s<sub>p</sub>:
s<sub>p</sub> = √ (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> /  (n<sub>1</sub>+n<sub>2</sub>-2) = √ (40-1)18.5<sup>2</sup> +  (38-1)16.7<sup>2</sup> /  (40+38-2) = 17.647
Next, we will calculate the test statistic <em>t</em>:
t = (x<sub>1</sub> – x<sub>2</sub>)  /  s<sub>p</sub>(√1/n<sub>1</sub> + 1/n<sub>2</sub>) =  (300-305) / 17.647(√1/40 + 1/38) = -1.2508
Next, <b>we’ll calculate the degrees of freedom:</b>
df = n<sub>1</sub> + n<sub>2</sub> – 2 = 40 + 38 – 2 = 76
Lastly, we’ll plug in the test statistic and degrees of freedom into the  T Score to P Value Calculator  to find that the p-value is <b>0.21484</b>.
Since this p-value is not less than our significance level α = 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the mean weight of turtles between these two populations is different.
<h2>Example 3: Degrees of Freedom for Paired Samples t-test</h2>
Suppose we want to know whether or not a certain training program is able to increase the max vertical jump (in inches) of college basketball players.
To test this, we may recruit a  simple random sample  of 20 college basketball players and measure each of their max vertical jumps. Then, we may have each player use the training program for one month and then measure their max vertical jump again at the end of the month.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/pairedT1.png">
To determine whether or not the training program actually had an effect on max vertical jump, we will perform a paired samples t-test.
First, we’ll calculate the following summary data for the differences:
x<sub>diff</sub>: sample mean of the differences = -0.95
s: sample standard deviation of the differences = 1.317
n: sample size (i.e. number of pairs) = 20
We will perform a paired samples t-test with the following hypotheses:
H<sub>0</sub>: μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
H<sub>A</sub>: μ<sub>1</sub> ≠ μ<sub>2</sub> (the two population means are not equal)
Next, we’ll calculate the test statistic:
t = x<sub>diff</sub> / (s<sub>diff</sub>/√n)  = -0.95 / (1.317/√20) = -3.226
Next, <b>we’ll calculate the degrees of freedom</b>:
df = n – 1 = 20 – 1 = 19
According to the  T Score to P Value Calculator , the p-value associated with t = -3.226 and degrees of freedom = n-1 = 20-1 = 19 is <b>0.00445</b>.
Since this p-value is less than our significance level α = 0.05, we reject the null hypothesis. We have sufficient evidence to say that the mean max vertical jump of players is different before and after participating in the training program.
<h2>Additional Resources</h2>
The following calculators can be used to automatically perform t-tests based on data that you provide:
 One Sample t-test Calculator 
 Two Sample t-test Calculator 
 Paired Samples t-test Calculator 
<h2><span class="orange">How to Calculate the Mean of Multiple Columns in R</span></h2>
Often you may want to calculate the mean of multiple columns in R. Fortunately you can easily do this by using the <b>colMeans() </b>function.
<b>colMeans(df)</b>
The following examples show how to use this function in practice.
<h3>Using colMeans() to Find the Mean of Multiple Columns</h3>
The following code shows how to use the <b>colMeans() </b>function to find the mean of every column in a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#find mean of each column
colMeans(df)
var1 var2 var3 var4 
 3.2  5.4  5.2  4.2 
</b>
We can also specify <em>which </em>columns to find the mean for:
<b>#find the mean of columns 2 and 3
colMeans(df[ , c(2, 3)])
var2 var3 
 5.4  5.2 
#find the mean of the first three columns
colMeans(df[ , 1:3])
var1 var2 var3 
 3.2  5.4  5.2</b>
If there happen to be some columns that aren’t numeric, you can use  sapply()  to specify that you’d only like to find the mean of columns that are numeric:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9), var5=c('a', 'a', 'b', 'b', 'c'))
#find mean of <em>only </em>numeric columns
colMeans(df[sapply(df, is.numeric)])
var1 var2 var3 var4 
 3.2  5.4  5.2  4.2 </b>
And if there happen to be missing values in any columns, you can use the argument <b>na.rm=TRUE </b>to ignore missing values when calculating the means:
<b>#create data frame with some missing values
df &lt;- data.frame(var1=c(1, 3, NA, NA, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, NA))
#find mean of each column and ignore missing values
colMeans(df, na.rm=TRUE)
var1 var2 var3 var4 
 3.0  5.4  5.2  3.0</b>
<h2><span class="orange">How to Calculate Percentiles from Mean & Standard Deviation</span></h2>
You can use the following formula to calculate the percentile of a normal distribution based on a mean and standard deviation:
<b>Percentile Value = μ + zσ</b>
where:
<b>μ</b>: Mean
<b>z</b>: z-score from  z table  that corresponds to percentile value
<b>σ</b>: Standard deviation
The following examples show how to use this formula in practice.
<h3>Example 1: Calculate 15th Percentile Using Mean & Standard Deviation</h3>
Suppose the weight of a certain species of otters is normally distributed with a mean of μ = 60 pounds and standard deviation of σ = 12 pounds.
What is the weight of an otter at the 15th percentile?
To answer this, we must find the z-score that is closest to the value <b>0.15</b> in the  z table . This value turns out to be <b>-1.04</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/perce1-1.jpg"614">
We can then plug this value into the percentile formula:
Percentile Value = μ + zσ
15th percentile = 60 + (-1.04)*12
15th percentile = 47.52
An otter at the 15th percentile weighs about <b>47.52</b> pounds.
<b>Note</b>: We could also use the  Percentile to Z-Score Calculator  to find that the exact z-score that corresponds to the 15th percentile is -1.0364.
Pugging this value into the percentile formula, we get:
Percentile Value = μ + zσ
15th percentile = 60 + (-1.0364)*12
15th percentile = 47.5632
<h3>
<b>Example 2: Calculate 93rd Percentile Using Mean & Standard Deviation</b>
</h3>
Suppose the exam scores on a certain test are normally distributed with a mean of μ = 85 and standard deviation of σ = 5.
What is the exam score of a student who scores at the 93rd percentile?
To answer this, we must find the z-score that is closest to the value <b>0.93 </b>in the  z table . This value turns out to be <b>1.48</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/perce2.jpg"631">
We can then plug this value into the percentile formula:
Percentile Value = μ + zσ
93rd percentile = 85 + (1.48)*5
93rd percentile = 92.4
A student who scores at the 93rd percentile would receive an exam score of about <b>92.4</b>.
<b>Note</b>: We could also use the  Percentile to Z-Score Calculator  to find that the exact z-score that corresponds to the 93rd percentile is 1.4758.
Pugging this value into the percentile formula, we get:
Percentile Value = μ + zσ
93rd percentile = 85+ (1.4758)*5
93rd percentile = 92.379
<h2><span class="orange">How to Calculate R-Squared by Hand</span></h2>
In statistics, <b>R-squared</b> (R<sup>2</sup>) measures the proportion of the variance in the  response variable  that can be explained by the predictor variable in a regression model.
We use the following formula to calculate R-squared:
R<sup>2</sup> =  [ (nΣxy – (Σx)(Σy)) / (√nΣx<sup>2</sup>-(Σx)<sup>2</sup> * √nΣy<sup>2</sup>-(Σy)<sup>2</sup>) ]<sup>2</sup>
The following step-by-step example shows how to calculate R-squared by hand for a given regression model.
<h3>Step 1: Create a Dataset</h3>
First, let’s create a dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/r2hand1.png">
<h3>Step 2: Calculate Necessary Metrics</h3>
Next, let’s calculate each metric that we need to use in the R<sup>2</sup> formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/r2hand2.png">
<h3>Step 3: Calculate R-Squared</h3>
Lastly, we’ll plug in each metric into the formula for R<sup>2</sup>:
R<sup>2</sup> =  [ (nΣxy – (Σx)(Σy)) / (√nΣx<sup>2</sup>-(Σx)<sup>2</sup> * √nΣy<sup>2</sup>-(Σy)<sup>2</sup>) ]<sup>2</sup>
R<sup>2</sup> =  [ (8*(2169) – (72)(223)) / (√8*(818)-(72)<sup>2</sup> * √8*(6447)-(223)<sup>2</sup>) ]<sup>2</sup>
R<sup>2</sup> =  0.6686
<b>Note:</b> The <em>n</em> in the formula represents the number of observations in the dataset and turns out to be n = 8 observations in this example.
Assuming <em>x</em> is the predictor variable and <em>y</em> is the response variable in this regression model, the R-squared for the model is <b>0.6686</b>.
This tells us that 66.86% of the variation in the variable <em>y</em> can be explained by variable <em>x</em>.
<h2><span class="orange">How to Calculate Sample Size in Excel (With Example)</span></h2>
You can use the <b>COUNTA</b> function to calculate the sample size of a dataset in Excel.
This function uses the following basic syntax:
<b>=COUNTA(A2:A16)
</b>
This particular formula counts all of the non-blank cells in the range <b>A2:A16</b>.
The following example shows how to use this function to calculate a sample size in Excel in practice.
<h2>Example: Calculating Sample Size in Excel</h2>
Suppose we have the following dataset that shows the points scored by basketball players on various teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/samp1.jpg"466">
We can type the following formula into cell <b>E1</b> to calculate the sample size of this dataset:
<b>=COUNTA(A2:A16)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/samp2.jpg">
From the output we can see that the sample size is <b>15</b>.
Note that if we’d like to calculate the sample size using a criteria, we could use the <b>COUNTIF</b> function instead.
For example, we could use the following formula to calculate the sample size only for the players on the Hawks team:
<b>=COUNTIF(A2:A16, "Hawks")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/samp3.jpg"553">
From the output we can see that the sample size for the players on the Hawks team is <b>5</b>.
We could also use the &lt;> symbols to calculate the sample size for the players who are <em>not</em> on the Hawks team:
<b>=COUNTIF(A2:A16, "&lt;>Hawks")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/samp4.jpg"542">
From the output we can see that the sample size for the players not on the Hawks team is <b>10</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Count Filtered Rows in Excel 
 How to Count Duplicates in Excel 
 How to Count by Group in Excel 
<h2><span class="orange">How to Calculate Tenure in Excel (With Example)</span></h2>
You can use one of the following formulas to calculate employee tenure in Excel:
<b>Formula 1: Calculate Tenure in Years and Months (e.g. 14 years, 2 months)</b>
<b>=DATEDIF(B2,C2,"y") & " years , "& DATEDIF(B2,C2,"ym") & " months"</b>
<b>Formula 2: Calculate Tenure in Years as Decimal (e.g. 14.16944 years)</b>
<b>=YEARFRAC(B2, C2)</b>
Both formulas assume that the start date is in cell <b>B2</b> and the end date is in cell <b>C2</b>.
The following examples show how to use each formula in practice with the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/tenure1.jpg"478">
<h2>Example 1: Calculate Tenure in Years and Months</h2>
We can type the following formula into cell <b>D2</b> to calculate the tenure for the first employee in terms of years and months:
<b>=DATEDIF(B2,C2,"y") & " years , "& DATEDIF(B2,C2,"ym") & " months"</b>
We can then drag and fill this formula down to each remaining cell in column D to calculate the tenure for each employee:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/tenure2.jpg">
The values in column D display the tenure for each employee in terms of years and months.
If you simply want to display the tenure in terms of total days, you can use the following formula instead:
<b>=DATEDIF(B2, C2, "d")</b>
This will display the tenure in terms of days instead of years and months.
For example, the tenure for the first employee will be shown as 5,173 days.
<h2>Example 2: Calculate Tenure in Years as Decimal</h2>
We can type the following formula into cell <b>D2</b> to calculate the tenure for the first employee in terms of years as a decimal:
<b>=YEARFRAC(B2, C2)</b>
We can then drag and fill this formula down to each remaining cell in column D to calculate the tenure for each employee:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/tenure3.jpg"549">
The values in column D display the tenure for each employee in terms of years as a decimal.
For example:
Andy has a tenure of <b>14.169 years</b>.
Ben has a tenure of <b>3.894 years</b>.
Charles has a tenure of <b>9.281 years</b>.
And so on.
<b>Note</b>: You can find the complete documentation for the Excel <b>YEARFRAC</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Calculate the Difference Between Two Dates in Excel 
 How to Calculate the Number of Months Between Dates in Excel 
 How to Apply Conditional Formatting Based on Date in Excel 
<h2><span class="orange">How to Calculate a Trimmed Mean (Step-by-Step)</span></h2>
A <b>trimmed mean</b> is the mean of a dataset that has been calculated after removing a specific percentage of the smallest and largest values from the dataset.
To calculate a X% trimmed mean, you can use the following steps:
<b>Step 1</b>: Order each value in a dataset from smallest to largest.
<b>Step 2</b>: Remove the values in the bottom X% and top X% of the dataset.
<b>Step 3</b>: Calculate the mean of the remaining values.
The following examples show how to calculate a trimmed mean in practice.
<h3>Example 1: Calculate a 10% Trimmed Mean</h3>
Suppose we have the following dataset:
Dataset: 4, 8, 12, 15, 9, 6, 14, 18, 12, 9
Here is how to calculate the 10% trimmed mean for the dataset:
<b>Step 1</b>: Order each value in a dataset from smallest to largest.
Ordered Dataset: 4, 6, 8, 9, 9, 12, 12, 14, 15, 18
<b>Step 2</b>: Remove the values in the bottom 10% and top 10% of the dataset.
There are 10 total values in the dataset. Thus, 10% * 10 = 1. This means we need to remove the one smallest value and one largest value from the dataset:
Trimmed Dataset: 6, 8, 9, 9, 12, 12, 14, 15
<b>Step 3</b>: Calculate the mean of the remaining values.
10% trimmed mean = (6+8+9+9+12+12+14+15) / 8 = 10.625
The 10% trimmed mean is <b>10.625</b>.
<h3>Example 2: Calculate a 20% Trimmed Mean</h3>
Suppose we have the following dataset:
Dataset: 22, 25, 29, 11, 14, 18, 13, 13, 17, 11, 8, 8, 7, 12, 15, 6, 8, 7, 9, 12
Here is how to calculate the 20% trimmed mean for the dataset:
<b>Step 1</b>: Order each value in a dataset from smallest to largest.
Ordered Dataset: 6, 7, 7, 8, 8, 8, 9, 11, 11, 12, 12, 13, 13, 14, 15, 17, 18, 22, 25, 29
<b>Step 2</b>: Remove the values in the bottom 20% and top 20% of the dataset.
There are 20 total values in the dataset. Thus, 20% * 20 = 4. This means we need to remove the four smallest values and four largest values from the dataset:
Trimmed Dataset: 8, 8, 9, 11, 11, 12, 12, 13, 13, 14, 15, 17
<b>Step 3</b>: Calculate the mean of the remaining values.
20% trimmed mean = (8+8+9+11+11+12+12+13+13+14+15+17) / 12 = 11.9167
The 20% trimmed mean is <b>11.9167</b>.
<h3>Bonus Resource: Trimmed Mean Calculator</h3>
If you have an extremely large dataset and do not want to calculate a trimmed mean by hand, feel free to use this  Trimmed Mean Calculator .
For example, here’s how to use this calculator to find the 20% trimmed mean from the previous dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/trimmed1.png">
Notice that the 20% trimmed mean matches the value that we calculated by hand.
<h2><span class="orange">Can a Z-Score Be Negative?</span></h2>
In statistics, a <b>z-score </b>tells us how many standard deviations away a value is from  the mean . We use the following formula to calculate a z-score:
<b>z</b> = (X – μ) / σ
where X is the value we are analyzing, μ is the mean, and σ is the standard deviation.
<b>A z-score can be positive, negative, or equal to zero.</b>
A positive z-score indicates that a particular value is greater than the mean, a negative z-score indicates that a particular value is less than the mean, and a z-score of zero indicates that a particular value is equal to the mean.
A few examples should make this clear.
<h2>Examples: Calculating a Z-Score</h2>
Suppose we have the following dataset that shows the height (in inches) of a certain group of plants:
5, 7, 7, 8, 9, 10, 13, 17, 17, 18, 19, 19, 20
The sample mean of this dataset is <b>13</b> and the sample standard deviation is <b>5.51</b>.
<b>1. Find the z-score for the value “8” in this dataset.</b>
Here is how to calculate the z-score:
<b>z</b>  =  (X – μ) / σ  =  (8 – 13) / 5.51  =  <b>-0.91</b>
This means that the value “8” is 0.91 standard deviations <em>below </em>the mean.
<b>2. Find the z-score for the value “13” in this dataset.</b>
Here is how to calculate the z-score:
<b>z</b>  =  (X – μ) / σ  =  (13 – 13) / 5.46  =  <b>0</b>
This means that the value “13” is exactly equal to the mean.
<b>3. Find the z-score for the value “20” in this dataset.</b>
Here is how to calculate the z-score:
<b>z</b>  =  (X – μ) / σ  =  (20 – 13) / 5.46  =  <b>1.28</b>
This means that the value “20” is 1.28 standard deviations <em>above</em> the mean.
<h2>How to Interpret Z-Scores</h2>
A  Z Table  tells us what percentage of values fall below certain Z-scores. A few examples should make this clear.
<b>Example 1: Negative Z-Scores</b>
Earlier, we found that the raw value “8” in our dataset had a z-score of <b>-0.91</b>. According to the Z Table, 18.14% of values fall below this value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/zScoreNegative1.jpg">
<b>Example 2: Z-Scores equal to zero</b>
Earlier, we found that the raw value “13” in our dataset had a z-score of <b>0</b>. According to the Z Table, 50.00% of values fall below this value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/zScoreNegative2.jpg">
<b>Example 3: Positive Z-Scores</b>
Earlier, we found that the raw value “20” in our dataset had a z-score of <b>1.28</b>. According to the Z Table, 89.97% of values fall below this value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/zScoreNegative3.jpg">
<h2>Conclusion</h2>
Z-scores can take on any value between negative infinity and positive infinity, but most z-scores fall within 2 standard deviations of the mean. There’s actually a rule in statistics known as  the Empirical Rule , which states that for a given dataset with a normal distribution:
<b>68%</b> of data values fall within one standard deviation of the mean.
<b>95%</b> of data values fall within two standard deviations of the mean.
<b>99.7%</b> of data values fall within three standard deviations of the mean.
The higher the absolute value of a z-score, the further away a raw value is from the mean of the dataset. The lower the absolute value of a z-score, the closer a raw value is to the mean of the dataset.
<b>Related Topics:</b>
 Empirical Rule Calculator 
 How to Apply the Empirical Rule in Excel 
<h2><span class="orange">Can Kurtosis Be Negative?</span></h2>
In statistics, <b>kurtosis </b>is used to describe the shape of a probability distribution.
Specifically, it tells us the degree to which data values cluster in the tails or the peak of a distribution.
<b>The kurtosis for a distribution can be negative, equal to zero, or positive.</b>
<h2>Zero Kurtosis</h2>
If a distribution has a kurtosis of 0, then it is equal to the normal distribution which has the following bell-shape:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kurtosis1.png">
<h2>Positive Kurtosis</h2>
If a distribution has positive kurtosis, it is said to be <b>leptokurtic</b>, which means that it has a sharper peak and heavier tails compared to a normal distribution.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kurtosis2.png">
This simply means that fewer data values are located near the mean and more data values are located on the tails.
The most well-known distribution that has a positive kurtosis is the t distribution, which has a sharper peak and heaver tails compared to the normal distribution.
<h2>Negative Kurtosis</h2>
If a distribution has negative kurtosis, it is said to be <b>platykurtic</b>, which means that it has a flatter peak and thinner tails compared to a normal distribution.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kurtosis3.png">
This simply means that more data values are located near the mean and less data values are located on the tails.
One extreme example of a distribution that has a negative kurtosis is the  uniform distribution , which has no peak at all and is a completely flat distribution.
<h2>When to Use Kurtosis in Practice</h2>
In practice, we often measure the <b>kurtosis </b>of a distribution in the exploratory phase of analysis when we’re just trying to get a better understanding of the data.
So, if we see that the kurtosis is positive then we know we’re working with a distribution that has fewer data values located near the center and more data values that are spread out along the tails.
Conversely, if we see that the kurtosis is negative then we know we’re working with a distribution that has more data values located near the center and less data values in the tails.
<h2><span class="orange">How to Fix: Can only use .str accessor with string values</span></h2>
One error you may encounter when using Python is:
<b>AttributeError: Can only use .str accessor with string values!
</b>
This error usually occurs when you attempt to replace a pattern in a string column of a pandas DataFrame, but the column you’re working with isn’t actually a string.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [6.5, 7.8, 8.0, 9.0, 7.5, 3.4, 6.6, 6.8],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A6.5511
1A7.878
2A8.0710
3A9.096
4B7.5126
5B3.495
6B6.699
7B6.8412</b>
Now suppose we attempt to replace every decimal in the “points” column with a blank:
<b>#attempt to replace decimal in "points" column with a blank
df['points'] = df['points'].str.replace('.', '')
AttributeError: Can only use .str accessor with string values! 
</b>
We receive an error because the “points” column is not a string column.
<h3>How to Fix the Error</h3>
The easiest way to get around this error is to use the <b>.astype(str)</b> function before attempting to replace any values in the “points” column:
<b>#replace decimal in "points" column with a blank
df['points'] = df['points'].astype(str).str.replace('.', '')
#view updated DataFrame
df
teampointsassistsrebounds
0A65511
1A7878
2A80710
3A9096
4B75126
5B3495
6B6699
7B68412</b>
Notice that each decimal in the “points” column has been replaced and we don’t receive any error since we used the <b>.astype(str)</b> function before attempting to replace any values in the “points” column.
<b>Note</b>: You can find the complete documentation for the <b>replace()</b> function  here .
<h2><span class="orange">Can Variance Be Negative?</span></h2>
In statistics, the term <b>variance</b> refers to how spread out values are in a given dataset.
One common question students often have about variance is:
<em>Can variance be negative?</em>
The answer: <b>No, variance cannot be negative.</b> The lowest value it can take on is zero.
To find out why this is the case, we need to understand how variance is actually calculated.
<h3>How to Calculate Variance</h3>
The formula to find the variance of a sample (denoted as <b>s<sup>2</sup></b>) is:
<b>s<sup>2</sup></b> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>x</b>: The sample mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> observation in the sample
<b>N</b>: The sample size
<b> Σ</b>: A Greek symbol that means “sum”
For example, suppose we have the following dataset with 10 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/varByHand1.png">
We can use the following steps to calculate the variance of this sample:
<b>Step 1: Find the Mean</b>
The mean is simply the average. This turns out to be <b>14.7</b>.
<b>Step 2: Find the Squared Deviations</b>
Next, we can calculate the squared deviation of each individual value from the mean.
For example, the first squared deviation is calculated as (6-14.7)<sup>2</sup> = 75.69.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/varByHand2.png">
<b>Step 3: Find the Sum of Squared Deviations</b>
Next, we can take the sum of all the squared deviations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/varByHand3.png">
<b>Step 4: Calculate the Sample Variance</b>
Lastly, we can calculate the sample variance as the sum of squared deviations divided by (n-1):
s<sup>2</sup> = 330.1 / (10-1) = 330.1 / 9 = 36.678
The sample variance turns out to be <b>36.678</b>.
<h3>An Example of Zero Variance</h3>
The only way that a dataset can have a variance of zero is if <b>all of the values in the dataset are the same</b>.
For example, the following dataset has a sample variance of zero:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/varByHand4.png">
The mean of the dataset is 15 and none of the individual values deviate from the mean. Thus, the sum of the squared deviations will be zero and the sample variance will simply be zero.
<h3>Can Standard Deviation Be Negative?</h3>
A more common way to measure the spread of values in a dataset is to use the standard deviation, which is simply the square root of the variance.
For example, if the variance of a given sample is s<sup>2</sup> = <b>36.678</b>, then the standard deviation (written as <em>s</em>) is calculated as:
s = √s<sup>2</sup> = √36.678 = <b>6.056</b>
Since we already know that variance is always zero or a positive number, then this means that <b>the standard deviation can never be negative since</b> the square root of zero or a positive number can’t be negative.
<h2><span class="orange">How to Calculate Canberra Distance in Python (With Example)</span></h2>
The <b>Canberra distance</b> between two vectors, A and B, is calculated as:
<b>Canberra distance = Σ |A<sub>i</sub>-B<sub>i</sub>| / (|A<sub>i</sub>| + |B<sub>i</sub>|)</b>
where:
<b>A<sub>i</sub></b>: The i<sup>th</sup> value in vector A
<b>B<sub>i</sub></b>: The i<sup>th</sup> value in vector B
For example, suppose we have the following two vectors:
A = [2, 4, 4, 6]
B = [5, 5, 7, 8]
We would calculate the Canberra distance between A and B as:
Canberra Distance = |2-5|/(2+5) + |4-5|/(4+5) + |4-7|/(4+7) + |6-8|/(6+8)
Canberra Distance = 3/7 + 1/9 + 3/11 + 2/14
Canberra Distance = 0.95527
The Canberra distance between these two vectors is <b>0.95527</b>.
The following example shows how to calculate the Canberra distance between these exact two vectors in Python.
<h3>Example: Calculating Canberra Distance in Python</h3>
First, let’s create a NumPy array to hold each of our vectors:
<b>import numpy as np
#define two arrays
array1 = np.array([2, 4, 4, 6])
array2 = np.array([5, 5, 7, 8])</b>
Next, we can use the <b>canberra()</b> function from the <b>SciPy</b> package in Python to calculate the Canberra distance between the two vectors:
<b>from scipy.spatial import distance
#calculate Canberra distance between the arrays
distance.canberra(array1, array2)
0.9552669552
</b>
The Canberra distance between the two vectors is <b>0.95527</b>.
Notice that this value matches the one we calculated earlier by hand.
<b>Note</b>: You can find the complete documentation for the <b>canberra()</b> function from the <b>SciPy</b> package  here .
<h2><span class="orange">How to Create a Candlestick Chart in Excel (Step-by-Step)</span></h2>
A  candlestick chart  is a type of financial chart that displays the price movements of securities over time.
The following step-by-step example shows how to create a candlestick chart in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the prices for a dataset that show the open, high, low, and close price for a certain stock during an 8-day period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/candleExcel1.png">
<h3>Step 2: Create the Candlestick Chart</h3>
Next, highlight all of the values in the range A1:E9 as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/candleExcel2.png">
Then click the <b>Insert</b> tab along the top ribbon. Within the <b>Charts</b> group, click the <b>Waterfall</b> icon and then click the <b>Open-High-Low-Close</b> icon:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/candleExcel3.png">
This will automatically create the following candlestick chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/candleExcel4.png">
<h3>Step 3: Modify the Candlestick Chart</h3>
Feel free to add a title to the chart and delete the legend at the bottom that says ‘Open High Low Close.’
Also feel free to click on the individual candlesticks and change their fill colors.
For example, we can change the ‘up’ candles to have a color of black and the ‘down’ candles to have a color of red:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/candleExcel5.png">
We can now quickly see on which days the stock price ended higher (black) and on which days it ended lower (red).
<h2><span class="orange">Create a Candlestick Chart in Google Sheets (Step-by-Step)</span></h2>
A  candlestick chart  is a type of financial chart that displays the price movements of securities over time.
The following step-by-step example shows how to create a candlestick chart in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the prices for a dataset that show the low, open, close, and high price for a certain stock during an 8-day period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cand1.png">
<b>Note</b>: The Date column should be in Plain text format. If it is not, simply highlight the Date column, then click the <b>Format</b> tab, then click <b>Number</b>, then click <b>Plain text</b>.
<h3>Step 2: Create the Candlestick Chart</h3>
Next, highlight all of the values in the range A1:E9 as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cand2.png">
Then click the <b>Insert</b> tab and then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cand3.png">
By default, Google Sheets should create the following candlestick chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cand4.png">
If Google Sheets creates a different type of chart for some reason, simply click the <b>Chart type</b> tab within the <b>Chart editor</b> and scroll down until you see the <b>Candlestick chart</b> option and click it.
<h3>Step 3: Modify the Candlestick Chart</h3>
Feel free to use the <b>Customize</b> tab within the <b>Chart editor</b> to modify the title of the chart and change the background color.
For example, we can change the title to ‘Daily Stock Price’ and modify the background color to be a light shade of grey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/cand5.png">
<h2><span class="orange">How to Fix: cannot compare a dtyped [float64] array with a scalar of type [bool]</span></h2>
One error you may encounter when using pandas is:
<b>TypeError: cannot compare a dtyped [object] array with a scalar of type [bool]
</b>
This error usually occurs when you attempt to subset a DataFrame based on multiple conditions and fail to place parenthesis around each individual condition.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we create the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'C', 'G', 'F', 'F', 'C'],   'points': [21, 30, 26, 29, 14, 29, 22, 16]})
#view DataFrame
print(df)
  team position  points
0    A        G      21
1    A        G      30
2    A        F      26
3    A        C      29
4    B        G      14
5    B        F      29
6    B        F      22
7    B        C      16
</b>
Now suppose we attempt to use the <b>.loc</b> function to only display the rows where the team is equal to ‘A’ and the position is equal to ‘G’:
<b>#attempt to only show rows where team='A' and position='G'
df.loc[df.team == 'A' & df.position == 'G']
TypeError: cannot compare a dtyped [object] array with a scalar of type [bool]
</b>
We receive a <b>ValueError</b> because we didn’t place parenthesis around the individual conditions.
Since the <b>&</b> operator takes precedence over the <b>==</b> operator, pandas fails to interpret this statement in the correct order.
<h3>How to Fix the Error</h3>
The easiest way to fix this error is to simply add parenthesis around the individual conditions as follows:
<b>#only show rows where team='A' and position='G'
df.loc[(df.team == 'A') & (df.position == 'G')]
teamposition  points
0AG  21
1AG  30
</b>
Notice that we don’t receive any <b>ValueError</b> and we are able to successfully subset the DataFrame.
<h2><span class="orange">How to Fix: ValueError: Cannot mask with non-boolean array containing NA / NaN values</span></h2>
One error you may encounter when using pandas is:
<b>ValueError: Cannot mask with non-boolean array containing NA / NaN values
</b>
This error usually occurs when you attempt to find the rows in a pandas DataFrame that contain a particular string, but the column you’re searching in has NaN values.
The following example shows how to fix this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B'],   'position': ['Guard', 'Guard', np.nan, 'Guard', 'Forward'],   'points': [22, 28, 14, 13, 19]})
#view DataFrame
print(df)
  team position  points
0    A    Guard      22
1    A    Guard      28
2    A      NaN      14
3    B    Guard      13
4    B  Forward      19</b>
Now suppose we attempt to access all rows in the DataFrame where the <b>position</b> column contains the string “Guard”:
<b>#access all rows where position column contains 'Guard'
df[df['position'].str.contains('Guard')]
ValueError: Cannot mask with non-boolean array containing NA / NaN values
</b>
We receive an error because there is a NaN value in the <b>position</b> column.
<h2>How to Fix the Error</h2>
To avoid this error, we simply need to use the argument <b>na=False</b> within the <b>str.contains()</b> function:
<b>#access all rows where position column contains 'Guard', ignore NaN
df[df['position'].str.contains('Guard', na=False)]
        teamposition  points
0AGuard  22
1AGuard  28
3BGuard  13
</b>
This time we’re able to access all rows that contain “Guard” in the <b>position</b> column without any errors.
Another way to avoid this error is to use<b> .fillna(False)</b> as follows:
<b>#access all rows where position column contains 'Guard', ignore NaN
df[df['position'].str.contains('Guard').fillna(False)]
        teamposition  points
0AGuard  22
1AGuard  28
3BGuard  13</b>
Once again we’re able to access all rows that contain “Guard” in the <b>position</b> column without any errors.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in Python:
 How to Fix KeyError in Pandas 
 How to Fix: ValueError: cannot convert float NaN to integer 
 How to Fix: ValueError: operands could not be broadcast together with shapes 
<h2><span class="orange">How to Fix: Cannot perform ‘rand_’ with a dtyped [int64] array and scalar of type [bool]</span></h2>
One error you may encounter in Python is the following:
<b>TypeError:Cannot perform 'rand_' with a dtyped [int64] array and scalar of type [bool]
</b>
This error usually occurs when you attempt to filter a pandas DataFrame using multiple conditions but fail to use parenthesis around each individual condition.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    A      22        7         8
2    A      19        7        10
3    A      14        9         6
4    B      14       12         6
5    B      11        9         5
6    B      20        9         9
7    B      28        4        12
</b>
Now suppose we attempt to filter the DataFrame to only show rows where the team column is equal to ‘A’ and the points column is greater than 15:
<b>#attempt to filter DataFrame
df.loc[df.team == 'A' & df.points > 15]
TypeError:Cannot perform 'rand_' with a dtyped [int64] array and scalar of type [bool]
</b>
We receive an error because we didn’t place parenthesis around each individual condition.
<h3>How to Fix the Error</h3>
To fix this error, we just need to make sure we place parenthesis around each individual condition when performing the filter:
<b>#filter DataFrame
df.loc[(df.team == 'A') & (df.points > 15)]
teampointsassistsrebounds
0A18511
1A2278
2A19710
</b>
Notice that we’re able to successfully filter the DataFrame to only show the rows where team is equal to ‘A’ and where points is greater than 15.
Note that we also need to place parenthesis around each individual condition if we’re using an or “|” operator instead:
<b>#filter rows where team is equal to 'A' <em>or</em> points is greater than 15
df.loc[(df.team == 'A') | (df.points > 15)]
teampointsassistsrebounds
0A18511
1A2278
2A19710
3A1496
6B2099
7B28412
</b>
Notice that we avoid any errors once again.
<h2><span class="orange">Carryover Effects: Definition & Example</span></h2>
A <b>carryover effect</b> is an effect that “carries over” from one experimental treatment to another.
This type of effect occurs most often in <em>within-subjects research designs</em> in which the same participants are exposed to each treatment condition.
For example, suppose we recruit subjects to participate in an experiment in which they use three different techniques to memorize the order of cards in a deck.
Since each participant uses each technique, there’s a good chance that they will improve their ability to memorize the cards simply due to practice rather than the actual technique they’re using.
This is an example of a carryover effect – the increased ability of the participant to memorize cards “carries over” to each subsequent technique.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/carryover1-1.png">
This type of effect is problematic because it makes it difficult to know whether or not the differences in performance between the experimental treatments is due to a carryover effect or the actual treatments.
For example, a participant might perform far better using technique 3, but is that because technique 3 is better or is it because the participant had time to practice and improve before using technique 3?
<h3>Types of Carryover Effects</h3>
There are two main types of carryover effects:
<b>1. Practice Effect</b>
A practice effect refers to a carryover effect in which the participant simply gets better at some task due to practice.
This means they’re likely to show better results during later experimental treatments because they’ve had time to practice and improve.
<b>2. Fatigue Effect</b>
A fatigue effect refers to a carryover effect in which the participant gets worse at some task because they get fatigued from performing previous experimental treatments.
For example, it’s possible that participants actually get worse at memorizing cards as they use more and more techniques because they simply get tired or mentally drained.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/carryover2.png">
<h3>How to Minimize Carryover Effects</h3>
There are several ways to minimize carryover effects in an experiment, including:
<b>1. Give participants time to warm up.</b>
One way to prevent practice effects is to give participants time to warm up with the task to prevent them from getting better at the task <em>during</em> the actual experiment.
<b>2. Make a task shorter.</b>
One way to prevent fatigue effects is to make a task shorter or less intense to perform. This makes it less likely that the participants will get fatigued and tired as they perform more and more tasks.
<b>3. Use counterbalancing.</b>
Counterbalancing is when researchers assign experimental treatments in different orders to different participants.
For example, researchers might have 10 participants use three techniques in the order of 123, another 10 participants use the techniques in order of 213, another 10 participants use the techniques in order of 312, and so on.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/carryover3.png">
By using each order the same number of times, we can “counterbalance” any order effects.
The downside of this method is that it can be too time-consuming or costly to actually implement every order an equal number of times.  
<h2><span class="orange">How to Use a Case Sensitive VLOOKUP in Google Sheets</span></h2>
You can use the following basic formula to perform a case sensitive VLOOKUP in Google Sheets:
<b>=INDEX(B2:B10, MATCH(TRUE, EXACT(G2, A2:A10), 0))
</b>
This particular formula finds the exact value in cell <b>G2</b> in the range <b>A2:A10</b> and returns the corresponding value in the range <b>B2:B10</b>.
The following example shows how to use this formula in practice.
<h3>Example: Case Sensitive VLOOKUP in Google Sheets</h3>
Suppose we have the following dataset that shows the number of sales made by various salesmen at a company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/vlookup11.jpg"477">
Now suppose we attempt to use the following VLOOKUP formula to look up “Andy” and return his number of sales:
<b>=VLOOKUP(D2, A2:B10, 2)
</b>
This formula incorrectly returns the number of sales for <b>ANDY</b> instead of <b>Andy</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/vlookup12.jpg"556">
Instead, we need to use the following formula that can perform a case-sensitive VLOOKUP:
<b>=INDEX(B2:B10, MATCH(TRUE, EXACT(G2, A2:A10), 0))</b>
This formula correctly returns the number of sales for <b>Andy</b>, which turns out to be <b>29</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/vlookup13.jpg">
The formula correctly returns the number of sales for <b>Andy</b> instead of <b>ANDY</b>.
<h2><span class="orange">How to Write a Case Statement in Google Sheets</span></h2>
A <b>case statement</b> is a type of statement that goes through conditions and returns a value when the first condition is met.
The easiest way to implement a case statement in Google Sheets is by using the <b>SWITCH()</b> function, which uses the following basic syntax:
<b>=SWITCH(A2, "G", "Guard", "F", "Forward", "C", "Center", "None")
</b>
This particular function looks at cell A2 and returns the following value:
“<b>Guard</b>” if cell A2 contains “G”
“<b>Forward</b>” if cell A2 contains “F”
“<b>Center</b>” if cell A2 contains “C”
“<b>None</b>” if cell A2 does not contain any of the previous values
The following example shows how to use this function in practice in Google Sheets.
<h3>Example: Case Statement in Google Sheets</h3>
Suppose we have the following list of basketball positions:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/casesheets1.jpg"467">
We’ll use the following <b>SWITCH()</b> function to return a specific position name in column B based on the value in column A:
<b>=SWITCH(A2, "G", "Guard", "F", "Forward", "C", "Center", "None")</b>
We’ll type this formula into cell <b>B2</b> and then copy and paste it down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/casesheets2.jpg"630">
This formula returns the following values in column B:
“<b>Guard</b>” if column A contains “G”
“<b>Forward</b>” if column A contains “F”
“<b>Center</b>” if column A contains “C”
“<b>None</b>” if column A does not contain any of the previous values
Notice that the last value in column B returns a value of “<b>None</b>” since we didn’t specify a specific value to return for “Z” in the formula.
We could also use the following formula to simply return the original value from column A if it does not contain any of the specified values in the <b>SWITCH</b> formula:
<b>=SWITCH(A2, "G", "Guard", "F", "Forward", "C", "Center", A2)</b>
We’ll type this formula into cell <b>B2</b> and then copy and paste it down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/casesheets3.jpg"586">
Notice that the last value in column B simply returns a value of <b>Z</b> since the last value in column A didn’t contain any of the specified values in the <b>SWITCH</b> formula.
<b>Note</b>: You can find the complete documentation for the <b>SWITCH</b> function in Google Sheets  here .
<h2><span class="orange">How to Write a Case Statement in Excel (With Example)</span></h2>
A <b>case statement</b> is a type of statement that goes through conditions and returns a value when the first condition is met.
The easiest way to implement a case statement in Excel is by using the <b>SWITCH()</b> function, which uses the following basic syntax:
<b>=SWITCH(A2, "G", "Guard", "F", "Forward", "C", "Center", "None")
</b>
This particular function looks at cell A2 and returns the following value:
“<b>Guard</b>” if cell A2 contains “G”
“<b>Forward</b>” if cell A2 contains “F”
“<b>Center</b>” if cell A2 contains “C”
“<b>None</b>” if cell A2 does not contain any of the previous values
The following example shows how to use this function in practice.
<h3>Example: Case Statement in Excel</h3>
Suppose we have the following list of basketball positions:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/casestatement1.jpg"479">
We’ll use the following <b>SWITCH()</b> function to return a specific position name in column B based on the value in column A:
<b>=SWITCH(A2, "G", "Guard", "F", "Forward", "C", "Center", "None")</b>
We’ll type this formula into cell <b>B2</b> and then copy and paste it down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/casestatement2.jpg">
Notice that this formula returns the following values in column B:
“<b>Guard</b>” if column A contains “G”
“<b>Forward</b>” if column A contains “F”
“<b>Center</b>” if column A contains “C”
“<b>None</b>” if column A does not contain any of the previous values
Notice that the last value in column B returns a value of “<b>None</b>” since we didn’t specify a specific value to return for “Z” in the formula.
<h2><span class="orange">How to Write a Case Statement in R (With Example)</span></h2>
A <b>case statement</b> is a type of statement that goes through conditions and returns a value when the first condition is met.
The easiest way to implement a case statement in R is by using the <b>case_when()</b> function from the <b>dplyr</b> package:
<b>library(dplyr)
df %>% 
  mutate(new_column = case_when(
    col1 &lt; 9 ~ 'value1',
    col1 &lt; 12 ~ 'value2',
    col1 &lt; 15 ~ 'value3',
    TRUE ~ 'Great'))
</b>
This particular function looks at the value in the column called <b>col1</b> and returns:
“<b>value1</b>” if the value in col1 is less than 9
“<b>value2</b>” if the value in col1 is less than 12
“<b>value3</b>” if the value in col2 is less than 15
“<b>value4</b>” if none of the previous conditions are true
Note that <b>TRUE</b> is equivalent to an “else” statement.
The following example shows how to use this function in practice.
<h2>Example: Case Statement in R</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(player=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), points=c(6, 8, 9, 9, 12, 14, 15, 17, 19, 22))
#view data frame
df
   player points
1       1      6
2       2      8
3       3      9
4       4      9
5       5     12
6       6     14
7       7     15
8       8     17
9       9     19
10     10     22</b>
We can use the following syntax to write a case statement that creates a new column called <b>class</b> whose values are determined by the values in the <b>points</b> column:
<b>library(dplyr) 
#create new column using case statement
df %>% 
  mutate(class = case_when(
    points &lt; 9 ~ 'Bad',
    points &lt; 12 ~ 'OK',
    points &lt; 15 ~ 'Good',
    TRUE ~ 'Great'))
   player points class
1       1      6   Bad
2       2      8   Bad
3       3      9    OK
4       4      9    OK
5       5     12  Good
6       6     14  Good
7       7     15 Great
8       8     17 Great
9       9     19 Great
10     10     22 Great</b>
The case statement looked at the value in the <b>points</b> column and returned:
“<b>Bad</b>” if the value in the points column was less than 9
“<b>OK</b>” if the value in the points column was less than 12
“<b>Good</b>” if the value in the points column was less than 15
“<b>Great</b>” if none of the previous conditions are true
The new column is called <b>class</b>, since this is the name we specified in the <b>mutate()</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use If Statement with Multiple Conditions in R 
 How to Write a Nested If Else Statement in R 
 How to Write Your First tryCatch() Function in R 
<h2><span class="orange">How to Write a Case Statement in Pandas (With Example)</span></h2>
A <b>case statement</b> is a type of statement that goes through conditions and returns a value when the first condition is met.
The easiest way to implement a case statement in a Pandas DataFrame is by using the NumPy <b>where()</b> function, which uses the following basic syntax:
<b>df['new_column'] = np.where(df['col2']&lt;9, 'value1',   np.where(df['col2']&lt;12, 'value2',   np.where(df['col2']&lt;15, 'value3', 'value4')))
</b>
This particular function looks at the value in the column called <b>col2</b> and returns:
“<b>value1</b>” if the value in col2 is less than 9
“<b>value2</b>” if the value in col2 is less than 12
“<b>value3</b>” if the value in col2 is less than 15
“<b>value4</b>” if none of the previous conditions are true
The following example shows how to use this function in practice.
<h3>Example: Case Statement in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'player': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],   'points': [6, 8, 9, 9, 12, 14, 15, 17, 19, 22]})
#view DataFrame
df
playerpoints
016
128
239
349
4512
5614
6715
7817
8919
91022
</b>
We can use the following syntax to write a case statement that creates a new column called <b>class</b> whose values are determined by the values in the <b>points</b> column:
<b>#add 'class' column using case-statement logic
df['class'] = np.where(df['points']&lt;9, 'Bad',
              np.where(df['points']&lt;12, 'OK',
              np.where(df['points']&lt;15, 'Good', 'Great')))
#view updated DataFrame
df
playerpointsclass
016Bad
128Bad
239OK
349OK
4512Good
5614Good
6715Great
7817Great
8919Great
91022Great
</b>
The case statement looked at the value in the <b>points</b> column and returned:
“<b>Bad</b>” if the value in the points column was less than 9
“<b>OK</b>” if the value in the points column was less than 12
“<b>Good</b>” if the value in the points column was less than 15
“<b>Great</b>” if none of the previous conditions are true
<b>Note</b>: You can find the complete documentation for the NumPy <b>where()</b> function  here .
<h2><span class="orange">What is a Categorical Distribution?</span></h2>
A <b>categorical distribution</b> is a discrete probability distribution that describes the probability that a  random variable  will take on a value that belongs to one of <em>K</em> categories, where each category has a probability associated with it.
For a distribution to be classified as a categorical distribution, it must meet the following criteria:
The categories are discrete.
There are two or more potential categories.
The probability that the random variable takes on a value in each category must be between 0 and 1.
The sum of the probabilities for all categories must sum to 1.
The most obvious example of a categorical distribution is the distribution of outcomes associated with rolling a dice. There are <em>K</em> = 6 potential outcomes and the probability for each outcome is 1/6:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/categoricalDist1.png">
This distribution satisfies all of the criteria to be classified as a categorical distribution:
The categories are discrete (e.g. the random variable can only take on discrete values – 1, 2, 3, 4, 5, 6)
There are two or more potential categories.
The probability of each category is between 0 and 1.
The sum of the probabilities add up to 1: 1/6 + 1/6 + 1/6 + 1/6 + 1/6 + 1/6 = 1.
<b>Rule of Thumb:</b>
 
If you can <i>count </i>the number of outcomes, then you are working with a discrete random variable – e.g. counting the number of times a coin lands on heads.
 
But if you can <i>measure </i>the outcome, you are working with a continuous random variable – e.g. measuring height, weight, time, etc.
<h3>Other Examples of Categorical Distributions</h3>
There are plenty of categorical distributions in the real world, including:
<b>Example 1: Flipping a Coin.</b>
When we flip a coin there are 2 potential discrete outcomes, the probability of each outcome is between 0 and 1, and the sum of the probabilities is equal to 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/categoricalDist2.png">
<b>Example 2: Selecting Marbles from an Urn.</b>
Suppose an urn contains 5 red marbles, 3 green marbles, and 2 purple marbles. If we randomly select one marble from the urn, there are 3 potential discrete outcomes, the probability of each outcome is between 0 and 1, and the sum of the probabilities is equal to 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/categoricalDist3.png">
<b>Example 3: Selecting a Card from a Deck.</b>
If we randomly select a card from a standard 52-card deck, there are 13 potential discrete outcomes, the probability of each outcome is between 0 and 1, and the sum of the probabilities is equal to 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/categoricalDist4.png">
<h3>Relation to Other Distributions</h3>
For a distribution to be classified as a <b>categorical distribution</b>, it must have <em>K</em> ≥ 2 potential outcomes and <em>n</em> = 1 trial.
Using this terminology, a categorical distribution is similar to the following distributions:
<b>Bernoulli distribution:</b> <em>K</em> = 2 outcomes, <em>n</em> = 1 trial
<b>Binomial distribution:</b> <em>K</em> = 2 outcomes, n ≥ 1 trial
<b>Multinomial distribution:</b> <em>K</em> ≥ 2 outcomes, n ≥ trial
<h2><span class="orange">Categorical vs. Quantitative Variables: Definition + Examples</span></h2>
In statistics, variables can be classified as either <b>categorical</b> or <b>quantitative</b>.
<b>Categorical Variables: </b>Variables that take on names or labels. Examples include:
Marital status (“married”, “single”, “divorced”)
Smoking status (“smoker”, “non-smoker”)
Eye color (“blue”, “green”, “hazel”)
Level of education (e.g. “high school”,  “Bachelor’s degree”, “Master’s degree”)
<b>Quantitative Variables: </b>Variables that take on numerical values. Examples include:
Height of an individual
Population size of a city
Number of students in a class
Number of square feet in a house
The following table summarizes the difference between these two types of variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cat2.png">
<h2>Examples: Categorical vs. Quantitative Variables</h2>
Use the following examples to gain a better understanding of categorical vs. quantitative variables.
<b>Example 1: Plant Height</b>
A botanist walks around a local forest and measures the height of a certain species of plant. The variable <b>plant height</b> is a <b>quantitative variable</b> because it takes on numerical values. For example, the height could be 15 inches, 17.5 inches, 19.2 inches, etc.
<b>Example 2: Vacation Locations</b>
A researcher surveys 200 people and asks them about their favorite vacation location. The variable <b>vacation location</b> is a <b>categorical variable</b> because it takes on names. For example, responses could include “Miami”, “San Francisco”, “Hilton Head”, etc.
<b>Example 3: Political Party</b>
A political scientists surveys 50 people in a certain town and asks them which political party they identify with. The variable <b>political party</b> is a <b>categorical variable</b> because it takes on labels. For example, responses could include “Democrat”, “Republican”, “Independent”, etc.
<b>Example 4: Running Times</b>
A coach records the running times of his 20 track runners. The variable <b>running time</b> is a <b>quantitative variable</b> because it takes on numerical values. For example, running time could be 58 seconds, 60.343 seconds, 65.4 seconds, etc.
<b>Example 5: House Prices</b>
An economist collects data about house prices in a certain city. The variable <b>house price</b> is a <b>quantitative variable</b> because it takes on numerical values. For example, house price could be $149,000, $289,000, $560,000, etc.
<h2>How to Describe Categorical & Quantitative Variables</h2>
We can summarize <b>categorical variables</b> by using frequency tables.
For example, suppose we collect data on the eye color of 100 individuals. Since “eye color” is a categorical variable, we might use the following frequency table to summarize its values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cat1.png">
We can summarize <b>quantitative variables</b> using a variety of descriptive statistics.
For example, suppose we collect data on the square footage of 100 homes. Since “square footage” is a quantitative variable, we might use the following descriptive statistics to summarize its values:
<b>Mean: </b>1,800
<b>Median: </b>2,150
<b>Mode: </b>1,600
<b>Range: </b>6,500
<b>Interquartile Range: </b>890
<b>Standard Deviation: </b>235
These metrics give us an idea of where the  center value  is located as well as how  spread out  the values are for this variable.
<b>Related:</b>  How to Plot Categorical Data in R 
<h2><span class="orange">How to Use cbind in Python (Equivalent to R)</span></h2>
The <b>cbind</b> function in R, short for <em>column-bind</em>, can be used to combine data frames together by their columns.
We can use the  concat()  function from pandas to perform the equivalent function in Python:
<b>df3 = pd.concat([df1, df2], axis=1)
</b>
The following examples shows how to use this function in practice.
<h3>Example 1: Use cbind in Python with Equal Index Values</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [99, 91, 104, 88, 108]})
print(df1)
  team  points
0    A      99
1    B      91
2    C     104
3    D      88
4    E     108
df2 = pd.DataFrame({'assists': ['A', 'B', 'C', 'D', 'E'],    'rebounds': [22, 19, 25, 33, 29]})
print(df2)
  assists  rebounds
0       A        22
1       B        19
2       C        25
3       D        33
4       E        29
</b>
We can use the <b>concat()</b> function to quickly bind these two DataFrames together by their columns:
<b>#column-bind two DataFrames into new DataFrame
df3 = pd.concat([df1, df2], axis=1)
#view resulting DataFrame
df3
teampointsassistsrebounds
0A99A22
1B91B19
2C104C25
3D88D33
4E108E29</b>
<h3>Example 2: Use cbind in Python with Unequal Index Values</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [99, 91, 104, 88, 108]})
print(df1)
  team  points
0    A      99
1    B      91
2    C     104
3    D      88
4    E     108
df2 = pd.DataFrame({'assists': ['A', 'B', 'C', 'D', 'E'],    'rebounds': [22, 19, 25, 33, 29]})
df2.index = [6, 7, 8, 9, 10]
print(df2)
   assists  rebounds
6        A        22
7        B        19
8        C        25
9        D        33
10       E        29
</b>
Notice that the two DataFrames do not have the same index values.
If we attempt to use the <b>concat()</b> function to cbind them together, we’ll get the following result:
<b>#attempt to column-bind two DataFrames
df3 = pd.concat([df1, df2], axis=1)
#view resulting DataFrame
df3
teampointsassistsrebounds
0A99.0NaNNaN
1B91.0NaNNaN
2C104.0NaNNaN
3D88.0NaNNaN
4E108.0NaNNaN
6NaNNaNA22.0
7NaNNaNB19.0
8NaNNaNC25.0
9NaNNaND33.0
10NaNNaNE29.0</b>
This is not the result we wanted. 
To fix this, we need to first reset the index of each DataFrame before concatenating them together:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [99, 91, 104, 88, 108]})
df2 = pd.DataFrame({'assists': ['A', 'B', 'C', 'D', 'E'],    'rebounds': [22, 19, 25, 33, 29]})
df2.index = [6, 7, 8, 9, 10]
#reset index of each DataFrame
df1.reset_index(drop=True, inplace=True)
df2.reset_index(drop=True, inplace=True)
#column-bind two DataFrames
df3 = pd.concat([df1, df2], axis=1)
#view resulting DataFrame
df3
teampointsassistsrebounds
0A99A22
1B91B19
2C104C25
3D88D33
4E108E29</b>
Notice that this DataFrame matches the one we got in the previous example.
<h2><span class="orange">How to Use cbind in R (With Examples)</span></h2>
The <b>cbind</b> function in R, short for <em>column-bind</em>, can be used to combine vectors, matrices and data frames by column. 
The following examples show how to use this function in practice.
<h3>Example 1: Cbind Vectors into a Matrix</h3>
The following code shows how to use cbind to column-bind two vectors into a single matrix:
<b>#create two vectors
a &lt;- c(1, 3, 3, 4, 5)
b &lt;- c(7, 7, 8, 3, 2)
#cbind the two vectors into a matrix
new_matrix &lt;- cbind(a, b)
#view matrix
new_matrix
     a b
[1,] 1 7
[2,] 3 7
[3,] 3 8
[4,] 4 3
[5,] 5 2
#view class of new_matrix
class(new_matrix)
[1] "matrix" "array" 
</b>
<h3>Example 2: Cbind Vector to a Data Frame</h3>
The following code shows how to use cbind to column-bind a vector to an existing data frame:
<b>#create data frame
df &lt;- data.frame(a=c(1, 3, 3, 4, 5), b=c(7, 7, 8, 3, 2), c=c(3, 3, 6, 6, 8))
#define vector
d &lt;- c(11, 14, 16, 17, 22)
#cbind vector to data frame
df_new &lt;- cbind(df, d)
#view data frame
df_new
  a b c  d
1 1 7 3 11
2 3 7 3 14
3 3 8 6 16
4 4 3 6 17
5 5 2 8 22
</b>
Note that R will throw an error if the length of the vector is not the same as the length of the columns in the existing data frame.
<h3>Example 3: Cbind Multiple Vectors to a Data Frame</h3>
The following code shows how to use cbind to column-bind multiple vectors to an existing data frame:
<b>#create data frame
df &lt;- data.frame(a=c(1, 3, 3, 4, 5), b=c(7, 7, 8, 3, 2), c=c(3, 3, 6, 6, 8))
#define vectors
d &lt;- c(11, 14, 16, 17, 22)
e &lt;- c(34, 35, 36, 36, 40) 
#cbind vectors to data frame
df_new &lt;- cbind(df, d, e)
#view data frame
df_new
  a b c  d  e
1 1 7 3 11 34
2 3 7 3 14 35
3 3 8 6 16 36
4 4 3 6 17 36
5 5 2 8 22 40
</b>
<h3>Example 4: Cbind Two Data Frames</h3>
The following code shows how to use cbind to column-bind two data frames into one data frame:
<b>#create two data frames
df1 &lt;- data.frame(a=c(1, 3, 3, 4, 5),  b=c(7, 7, 8, 3, 2),  c=c(3, 3, 6, 6, 8))
df2 &lt;- data.frame(d=c(11, 14, 16, 17, 22),  e=c(34, 35, 36, 36, 40))
#cbind two data frames into one data frame
df_new &lt;- cbind(df1, df2)
#view data frame
df_new
  a b c  d  e
1 1 7 3 11 34
2 3 7 3 14 35
3 3 8 6 16 36
4 4 3 6 17 36
5 5 2 8 22 40
</b>
<b>Bonus:</b> If you want to bind together vectors, matrices, or data frames by rows, you can used the  rbind  function instead.
<h2><span class="orange">How to Plot a CDF in Excel</span></h2>
A <b>cumulative distribution function (CDF) </b>describes the probability that a random variable takes on a value less than or equal to some number.
We can use the following function in Excel to calculate cumulative distribution probabilities:
<b>=NORM.DIST(x, MEAN, STANDARD_DEVIATION, TRUE)
</b>
The following example shows how to calculate and plot a CDF in Excel.
<h3>Example: Calculate & Plot CDF in Excel</h3>
First, let’s create the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfExcel1.png">
Next, let’s specify the mean and standard deviation of the distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfExcel2.png">
Next, we can calculate the cumulative distribution probability for the first value in the dataset by using the following formula:
<b>=NORM.DIST(A2, $F$1, $F$2, TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfExcel3.png">
Next, we can copy and paste this formula down to every other cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfExcel4.png">
The CDF is now complete. The way we interpret the values is as follows:
The probability that the random variable will take on a value equal to or less than 6 is .<b>00135</b>.
The probability that the random variable will take on a value equal to or less than 7 is .<b>00383</b>.
The probability that the random variable will take on a value equal to or less than 8 is .<b>00982</b>.
And so on.
To visualize this CDF, we can highlight every value in column B. Then, we can click the <b>Insert</b> tab along the top ribbon and click <b>Insert Line Chart</b> to produce the following chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfExcel5.png">
The values along the x-axis show the values from the dataset and the values along the y-axis show the CDF values.
<h2><span class="orange">How to Calculate & Plot a CDF in R</span></h2>
You can use the following basic syntax to calculate and plot a cumulative distribution function (CDF) in R:
<b>#calculate empirical CDF of data
p = ecdf(data)
#plot CDF
plot(p)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculate & Plot CDF of Raw Data</h3>
The following code shows how to calculate and plot a CDF of a random dataset in R:
<b>#create some data
data = rnorm(100)
#calculate empirical CDF of data
p = ecdf(data)
#plot CDF
plot(p, xlab='x', ylab='CDF', main='CDF of Data') </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfR1.png">
The x-axis shows the raw data values and the y-axis shows the corresponding CDF values.
<h3>Example 2: Calculate & Plot CDF of Known Distribution</h3>
The following code shows how to calculate and plot a CDF of the standard normal distribution:
<b>curve(pnorm, from = -3, to = 3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfR2.png">
Alternatively, you can create the same plot using ggplot2:
<b>library(ggplot2)
ggplot(data.frame(x = c(-3, 3)), aes(x = x)) +
  stat_function(fun = pnorm)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfR3.png">
<h2><span class="orange">How to Calculate & Plot a CDF in Python</span></h2>
You can use the following basic syntax to calculate the cumulative distribution function (CDF) in Python:
<b>#sort data
x = np.sort(data)
#calculate CDF values
y = 1. * np.arange(len(data)) / (len(data) - 1)
#plot CDF
plt.plot(x, y)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: CDF of Random Distribution</h3>
The following code shows how to calculate and plot a cumulative distribution function (CDF) for a random sample of data in Python:
<b>import numpy as np
import matplotlib.pyplot as plt
#define random sample of data
data = np.random.randn(10000)
#sort data
x = np.sort(data)
#calculate CDF values
y = 1. * np.arange(len(data)) / (len(data) - 1)
#plot CDF
plt.plot(x, y)
plt.xlabel('x')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfPython1.png">
The x-axis displays the raw data values and the y-axis displays the corresponding CDF values.
<h3>Example 2: CDF of Normal Distribution</h3>
If you’d like to plot the cumulative distribution function of a known distribution (such as the  normal distribution ) then you can use the following functions from the  SciPy  library:
<b>import numpy as np
import scipy
import matplotlib.pyplot as plt
#generate data from normal distribution
data = np.random.randn(1000)
#sort data
x = np.sort(data)
#calculate CDF values
y = scipy.stats.norm.cdf(x)
#plot CDF
plt.plot(data_sorted, norm_cdf)
#plot CDF
plt.plot(x, y)
plt.xlabel('x')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cdfPython2.png">
<h2><span class="orange">CDF vs. PDF: What’s the Difference?</span></h2>
This tutorial provides a simple explanation of the difference between a PDF (probability density function) and a CDF (cumulative distribution function) in statistics.
<h3>Random Variables</h3>
Before we can define a PDF or a CDF, we first need to understand random variables.
A  <b>random variable</b> , usually denoted as X, is a variable whose values are numerical outcomes of some random process. There are two types of random variables: discrete and continuous.
<b>Discrete Random Variables</b>
A <b>discrete random variable</b> is one which can take on only a countable number of distinct values like 0, 1, 2, 3, 4, 5…100, 1 million, etc. Some examples of discrete random variables include:
The number of times a coin lands on tails after being flipped 20 times.
The number of times a dice lands on the number <em>4 </em>after being rolled 100 times.
<h3>Continuous Random Variables</h3>
A <b>continuous random variable</b> is one which can take on an infinite number of possible values. Some examples of continuous random variables include:
Height of a person
Weight of an animal
Time required to run a mile
For example, the height of a person could be 60.2 inches, 65.2344 inches, 70.431222 inches, etc. There are an infinite amount of possible values for height.
Rule of Thumb: If you can <i>count </i>the number of outcomes, then you are working with a discrete random variable (e.g. counting the number of times a coin lands on heads). But if you can <i>measure </i>the outcome, you are working with a continuous random variable (e.g. measuring, height, weight, time, etc.)
<h3>Probability Density Functions</h3>
A <b>probability density function </b>(pdf) tells us the probability that a random variable takes on a certain value.
For example, suppose we roll a dice one time. If we let <em>x </em>denote the number that the dice lands on, then the probability density function for the outcome can be described as follows:
<b>P(x &lt; 1)</b> : 0
<b>P(x = 1)</b> : 1/6
<b>P(x = 2)</b> : 1/6
<b>P(x = 3)</b> : 1/6
<b>P(x = 4)</b> : 1/6
<b>P(x = 5)</b> : 1/6
<b>P(x = 6)</b> : 1/6
<b>P(x > 6)</b> : 0
Note that this is an example of a discrete random variable, since <em>x </em>can only take on integer values.
For a continuous random variable, we cannot use a PDF directly, since the probability that <em>x </em>takes on any exact value is zero. 
For example, suppose we want to know the probability that a burger from a particular restaurant weighs a quarter-pound (0.25 lbs). Since <em>weight </em>is a continuous variable, it can take on an infinite number of values.
For example, a given burger might actually weight 0.250001 pounds, or 0.24 pounds, or 0.2488 pounds. The probability that a given burger weights exactly .25 pounds is essentially zero.
<h3>Cumulative Distribution Functions</h3>
A <b>cumulative distribution function </b>(cdf) tells us the probability that a random variable takes on a value less than or equal to <em>x</em>.
For example, suppose we roll a dice one time. If we let <em>x </em>denote the number that the dice lands on, then the cumulative distribution function for the outcome can be described as follows:
<b>P(x ≤ 0)</b> : 0
<b>P(x ≤ 1)</b> : 1/6
<b>P(x ≤ 2)</b> : 2/6
<b>P(x ≤ 3)</b> : 3/6
<b>P(x ≤ 4)</b> : 4/6
<b>P(x ≤ 5)</b> : 5/6
<b>P(x ≤ 6)</b> : 6/6
<b>P(x > 6)</b> : 0
Notice that the probability that <em>x </em>is less than or equal to <em>6 </em>is 6/6, which is equal to 1. This is because the dice will land on either 1, 2, 3, 4, 5, or 6 with 100% probability.
This example uses a discrete random variable, but a continuous density function can also be used for a continuous random variable.
Cumulative distribution functions have the following properties:
The probability that a random variable takes on a value less than the smallest possible value is zero. For example, the probability that a dice lands on a value less than 1 is zero.
The probability that a random variable takes on a value less than or equal to the largest possible value is one. For example, the probability that a dice lands on a value of 1, 2, 3, 4, 5, or 6 is one. It must land on one of those numbers.
The cdf is always non-decreasing. That is, the probability that a dice lands on a number less than or equal to 1 is 1/6, the probability that it lands on a number less than or equal to 2 is 2/6, the probability that it lands on a number less than or equal to 3 is 3/6, etc. The cumulative probabilities are always non-decreasing.
<b>Related: </b>You can use an  ogive graph  to visualize a cumulative distribution function.
<h3>The Relationship Between a CDF and a PDF</h3>
In technical terms, a probability density function (pdf) is the derivative of a cumulative distribution function (cdf). 
Furthermore, the area under the curve of a pdf between negative infinity and <em>x </em>is equal to the value of <em>x </em>on the cdf.
For an in-depth explanation of the relationship between a pdf and a cdf, along with the proof for why the pdf is the derivative of the cdf, refer to a statistical textbook.
<h2><span class="orange">What is a Ceiling Effect? (Explanation & Example)</span></h2>
In research, a <b>ceiling effect</b> occurs when there is some upper limit on a survey or questionnaire and a large percentage of respondents score near this upper limit.
The opposite of this is known as a  floor effect .
A ceiling effect can cause a variety of problems including:
It makes it difficult to get an accurate  measure of central tendency .
It makes it difficult to get an accurate  measure of dispersion .
It makes it difficult to rank individuals according to score.
It makes it difficult to compare the means between two groups.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/ceiling2.png">
This tutorial provides several examples of ceiling effects, details on why they’re problematic, and ways to prevent them from occurring.
<h3>Ceiling Effect Examples</h3>
The following examples illustrate scenarios where ceiling effects may occur in research.
<b>Example 1: A Questionnaire on Income.</b>
Suppose researchers want to understand the distribution of household incomes in a particular neighborhood so they create a questionnaire to give to each household. Since they want to prevent  nonresponse bias , they decide to ask households “which income bracket they fall in” and make the highest bracket <em>$120k or more</em>.
In this case, even if households make far more than $120k per year (e.g. suppose some make $150k, $180k, $250k or more) they will simply be grouped in the <em>$120k or more </em>group. If there are many households that make far more than $120k, the researchers will have no clue and they’ll likely underestimate the true household income average for the neighborhood.
<b>Example 2: A Survey on Alcohol Consumption</b>
Suppose researchers want to understand the alcohol consumption habits of students on a particular campus. They decide to email a short survey to each student asking how many drinks they consume per week. To prevent  nonresponse bias , they make the highest category <em>10 drinks or more</em>.
In this case, many of the students may actually consume far more than 10 drinks per week but the highest category they can select to be in is <em>10 drinks or more</em>. This creates an artificial ceiling and it’s likely that a high percentage of respondents will fall in this category.
<b>Example 3: An Easy Exam</b>
Suppose a teacher administers an IQ exam that is measured on a scale of 1 to 50. Without realizing it, she makes the exam a bit too easy and a large percentage of the class scores at or near a perfect score of 50.
Because of this, it will be difficult for her to rank the scores of the students in any type of order and she won’t be able to distinguish which students could have performed even higher on a harder exam.
<h3>Problems Caused by Ceiling Effects</h3>
Ceiling effects cause a variety of problems including:
<b>1. It makes it difficult to get an accurate measure of central tendency.</b>
If a large percentage of respondents score at or near the highest possible value in an exam, questionnaire, or survey, it will become difficult to get an accurate measure of what the “average” score should be.
<b>2. It makes it difficult to get an accurate measure of dispersion.</b>
Similarly, if many respondents score near the highest possible value on an exam or survey, it will make it seem as if there is less dispersion than there really is because it’s not possible for respondents to score abnormally high.
<b>3. It makes it difficult to rank individuals according to score.</b>
If many individuals receive a perfect score on an exam, it becomes impossible to rank the individuals in any way since many of them received the same score.
<b>4. It makes it difficult to differentiate between two groups.</b>
Suppose a researcher wants to know if two different studying techniques lead to different average exam scores. If the exam is too easy then most of the students in each group will score near the max possible value, which will make it impossible to compare the average exam scores between each group to determine if the studying technique made any difference.
<h3>How to Prevent Ceiling Effects</h3>
There are two common ways to prevent ceiling effects:
<b>1. In surveys and questionnaires, provide anonymity and don’t set artificial ceilings on responses.</b>
For example, in a questionnaire about household incomes researchers need to reassure respondents that their answers will be completely  anonymous  and allow respondents to fill in their actual income instead of selecting from brackets.
This will increase the likelihood that respondents will provide their true income since their answer will be anonymous and it will allow researchers to understand the actual income distribution without extremely high incomes being masked from the responses.
<b>2. Increase the difficulty of exams or tests.</b>
For exams and tests, it’s important that researchers increase the difficulty so that a smaller percentage of individuals are able to score at or near a perfect score.
This will allow researchers to gain an accurate understanding of the mean and the dispersion of the data. This will also allow researchers to be able to rank the scores of individuals since fewer individuals are likely to receive the same score.
<h2><span class="orange">How to Find the Center and Spread of a Dot Plot</span></h2>
A <b>dot plot</b> is a type of plot that displays the frequencies of values in a dataset by using stacked dots.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/dotcenter1.png">
Often when we create a dot plot, we’re interested in quantifying the center and the spread:
<b>Center</b>: The center point of the dataset. We often use the median to measure this.
<b>Spread</b>: The spread of values in the dataset. We often use the range to measure this.
By knowing just these two values, we can get a good idea of how the values are distributed in a given dataset.
The following examples show how to find the center and spread of a dot plot in practice.
<h3>Example 1: Dot Plot of Fouls Committed</h3>
The following dot plot shows the number of fouls committed in a certain game by basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/dotcenter2.png">
Here’s how to find the center and spread of values in this dataset:
<b>Center</b>: We can use the median to measure the “center” of the dataset. The median represents the middle value of the dataset. To find the median for this particular dataset, we can list out each value and identify the middle value:
Data values: 1, 1, 1, 1, 2, <b>2</b>, 2, 3, 4, 5, 5
The median value in this dataset is <b>2</b>.
<b>Spread</b>: We can use the range to measure the “spread” of values in the dataset. The range represents the difference between the largest and smallest value.
In this dataset, we can see that the largest value is 5 and the smallest value is 1 so the range can be calculated as 5 – 1 = <b>4</b>.
<h3>Example 2: Dot Plot of Test Scores</h3>
The following dot plot shows the test scores of students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/dotcenter3.png">
Here’s how to find the center and spread of values in this dataset:
<b>Center</b>: We can use the median to measure the “center” of the dataset. To find the median for this particular dataset, we can list out each value and identify the middle value:
Data values: 85, 85, 85, 85, 86, 86, 86, <b>87</b>, <b>87</b>, 87, 88, 88, 89, 89, 90, 91
This dataset has an even number of values, so the median will be the average of the middle two values. The median turns out to be <b>87</b>.
<b>Spread</b>: We can use the range to measure the “spread” of values in the dataset, which represents the difference between the largest and smallest value.
In this dataset, we can see that the largest value is 91 and the smallest value is 85 so the range can be calculated as 91 – 85 = <b>6</b>.
<h2><span class="orange">How to Center Data in Python (With Examples)</span></h2>
To <b>center</b> a dataset means to subtract the  mean value  from each individual  observation  in the dataset.
Once you’ve centered a dataset, the mean value of the dataset becomes zero.
The following examples show how to center data in Python.
<h3>Example 1: Center the Values of a NumPy Array</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([4, 6, 9, 13, 14, 17, 18, 19, 19, 21])
#display mean of array
print(data.mean())
14.0</b>
We can define a  function  to subtract the mean value of the array from each individual observation:
<b>#create function to center data
center_function = lambda x: x - x.mean()
#apply function to original NumPy array
data_centered = center_function(data)
#view updated Array
print(data_centered)
array([-10.,  -8.,  -5.,  -1.,   0.,   3.,   4.,   5.,   5.,   7.])
</b>
The resulting values are the centered values of the dataset.
Since the mean of the original array was 14, this function simply subtracted 14 from each individual value in the original array.
For example:
1st value in centered array = 4 – 14 = <b>-10</b>
2nd value in centered array = 6 – 14 = <b>-8</b>
3rd value in centered array = 9 – 14 = <b>-5</b>
And so on.
We can also verify that the mean of the centered array is zero:
<b>#display mean of centered array
print(data_centered.mean())
0.0</b>
<h3>Example 2: Center the Columns of a Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [1, 4, 5, 6, 6, 8, 9],   'y': [7, 7, 8, 8, 8, 9, 12],   'z': [3, 3, 4, 4, 6, 7, 7]})
#view DataFrame
print(df)
   x   y  z
0  1   7  3
1  4   7  3
2  5   8  4
3  6   8  4
4  6   8  6
5  8   9  7
6  9  12  7</b>
We can use the pandas <b>apply()</b> function to center the values of each column in the DataFrame:
<b>#center the values in each column of the DataFrame
df_centered = df.apply(lambda x: x-x.mean())
#view centered DataFrame
print(df_centered)
        x        y        z
0-4.571429-1.428571-1.857143
1-1.571429-1.428571-1.857143
2-0.571429-0.428571-0.857143
3 0.428571-0.428571-0.857143
4 0.428571-0.428571 1.142857
5 2.428571 0.571429 2.142857
6 3.428571 3.571429 2.142857 
</b>
We can then verify that the mean value of each column is zero:
<b>#display mean of each column in the DataFrame
df_centered.mean()
x    2.537653e-16
y   -2.537653e-16
z    3.806479e-16
dtype: float64
</b>
The column means are shown in scientific notation, but each value is essentially equal to zero.
<h2><span class="orange">How to Center Data in R (With Examples)</span></h2>
To <b>center</b> a dataset means to subtract the mean value from each individual observation in the dataset.
For example, suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/center1.png">
It turns out that the mean value is 14. Thus, to center this dataset we would subtract 14 from each individual observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/center2.png">
Note that the mean value of the centered dataset is zero.
This tutorial provides several examples of how to center data in R.
<h3>Example 1: Center the Values of a Vector</h3>
The following code shows how to use the  scale()  function from base R to center the values in a vector:
<b>#create vector
data &lt;- c(4, 6, 9, 13, 14, 17, 18, 19, 19, 21)
#subtract the mean value from each observation in the vector
scale(data, scale=FALSE)
      [,1]
 [1,]  -10
 [2,]   -8
 [3,]   -5
 [4,]   -1
 [5,]    0
 [6,]    3
 [7,]    4
 [8,]    5
 [9,]    5
[10,]    7
attr(,"scaled:center")
[1] 14
</b>
The resulting values are the centered values of the dataset. The scale() function also tells us that the mean value of the dataset is 14.
Note that the <b>scale()</b> function, by default, subtracts the mean from each individual observation and then divides by the standard deviation.
By specifying <b>scale=FALSE</b>, we tell R not to divide by the standard deviation.
<h3>Example 2: Center the Columns in a Data Frame</h3>
The following code shows how to use the  sapply()  function and the  scale()  function from base R to center the values of each column of a data frame:
<b>#create data frame
df &lt;- data.frame(x = c(1, 4, 5, 6, 6, 8, 9), y = c(7, 7, 8, 8, 8, 9, 12), z = c(3, 3, 4, 4, 6, 7, 7))
#center each column in the data frame
df_new &lt;- sapply(df, function(x) scale(x, scale=FALSE))
#display data frame
df_new
              x          y          z
[1,] -4.5714286 -1.4285714 -1.8571429
[2,] -1.5714286 -1.4285714 -1.8571429
[3,] -0.5714286 -0.4285714 -0.8571429
[4,]  0.4285714 -0.4285714 -0.8571429
[5,]  0.4285714 -0.4285714  1.1428571
[6,]  2.4285714  0.5714286  2.1428571
[7,]  3.4285714  3.5714286  2.1428571
</b>
We can verify that the mean of each column in the new data frame is equal  to zero by using the <b>colMeans()</b> function:
<b>colMeans(df_new)
            x             y             z 
 2.537653e-16 -2.537653e-16  3.806479e-16 
</b>
The values are shown in scientific notation, but each value is essentially equal to zero.
<h2><span class="orange">Central Limit Theorem Calculator</span></h2>
The  central limit theorem  states that the sampling distribution of a sample mean is approximately normal if the sample size is large enough, <i>even if the population distribution is not normal.</i> The central limit theorem also states that the sampling distribution will have the following properties:
<b>1.</b> The <b>mean</b> of the sampling distribution will be equal to the mean of population distribution:
x = μ
<b>2.</b> The <b>standard deviation</b> of the sampling distribution will be equal to the standard deviation of the population distribution divided by the sample size:
s = σ / √n
To find the sample mean and sample standard deviation of a given sample, simply enter the necessary values below and then click the “Calculate” button.
<label for="pop_mean"><b>Population mean (μ)</b></label>
<input type="number" id="pop_mean" min="0" value="17">
<label for="pop_sd"><b>Population standard deviation (σ)</b></label>
<input type="number" id="pop_sd" min="0" value="4">
<label for="n"><b>Sample size (n)</b></label>
<input type="number" id="n" min="0" value="25">
<input type="button" id="button" onclick="calc()" value="Calculate">
<div>
Sample mean (x) = <b>17</b>
<div>
Sample standard deviation (s) = <b>0.8</b>
<script>
function calc() {
    
//get input degrees of freedom, t-value
var n = document.getElementById('n').value*1;
var pop_mean = document.getElementById('pop_mean').value*1;
var pop_sd = document.getElementById('pop_sd').value*1;
//calculate sample mean and sample standard deviation
var sample_mean = pop_mean;
var sample_sd = pop_sd / Math.sqrt(n);
//output values
document.getElementById('sample_mean').innerHTML = sample_mean.toFixed(5);
document.getElementById('sample_sd').innerHTML = sample_sd.toFixed(5);
  }
</script>
<h2><span class="orange">Central Limit Theorem: The Four Conditions to Meet</span></h2>
The  central limit theorem  states that  the sampling distribution of a sample mean  is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>.
In order to apply the central limit theorem, there are four conditions that must be met:
<b>1.</b> <b>Randomization</b>: The data must be sampled randomly such that every member in a population has an equal probability of being selected to be in the sample.
<b>2. Independence:</b> The sample values must be independent of each other.
<b>3. The 10% Condition:</b> When the sample is drawn without replacement, the sample size should be no larger than 10% of the population.
<b>4. Large Sample Condition:</b> The sample size needs to be sufficiently large.
This tutorial provides a brief explanation of each condition.
<h3>Condition 1: Randomization</h3>
In order to apply the central limit theorem, the data that we use must be sampled randomly from the population by using a <b>probability sampling method</b>.
In statistics, there are two types of  sampling methods :
<b>1. Probability sampling methods:</b> Sampling methods in which every member in a population has an equal probability of being selected to be in the sample. Examples include:
Simple random sample
Stratified random sample
Cluster random sample
Systematic random sample
<b>2. Non-probability sampling methods:</b> Sampling methods in which every member in a population does not have an equal probability of being selected to be in the sample. Examples include:
Convenience sample
 Voluntary response sample 
 Snowball sample 
Purposive sample
It’s important that a probability sampling method is used to obtain the sample because this maximizes the chances that we obtain a sample that is  representative of the population .
<h3>Condition 2: Independence</h3>
In order to apply the central limit theorem, we must also assume that each of the sample values is independent of each other. That is, the occurrence of one event does not affect the occurrence of any other event.
This assumption is often met if we use a probability sampling method because these types of sampling methods choose  observations  to be included in the sample completely independently of each other. 
<h3>Condition 3: The 10% Condition</h3>
When the sample is drawn without replacement (which is almost always the case), the sample size must be no larger than 10% of the total population.
For example:
If our population size is 500, then our sample size should be no larger than 50.
If our population size is 1,000 then our sample size should be no larger than 100.
If our population size is 50,000, then our sample size should be no larger than 5,000.
And so on.
<h3>Condition 4: Large Sample Condition</h3>
Lastly, in order to apply the central limit theorem our sample size must be sufficiently large.
In general, we consider “sufficiently large” to be 30 or larger. However, this number can vary a bit based on the underlying shape of the population distribution.
In particular:
If the population distribution is symmetric, sometimes a sample size as small as 15 is sufficient.
If the population distribution is skewed, generally a sample size of at least 30 is needed.
If the population distribution is extremely skewed, then a sample size of 40 or higher may be necessary.
Depending on the shape of the population distribution, you may require more or less than a sample size of 30 in order for the Central Limit Theorem to apply.
<h2><span class="orange">How to Apply the Central Limit Theorem in Excel</span></h2>
The  central limit theorem  states that the sampling distribution of a sample mean is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>.
The central limit theorem also states that the sampling distribution will have the following properties:
<b>1.</b> The mean of the sampling distribution will be equal to the mean of the population distribution:
<b>x = μ</b>
<b>2. </b>The standard deviation of the sampling distribution will be equal to the standard deviation of the population divided by the sample size:
<b>s = σ / √n</b>
In this tutorial, we explain how to apply the central limit theorem in Excel to a given distribution.
<h2>Applying the Central Limit Theorem in Excel</h2>
Suppose we have a distribution with a mean of <b>8 </b>and a standard deviation of <b>4</b>. We can use the following formulas in Excel to find both the mean and the standard deviation of the sampling distribution with a sample size of <b>15</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/centralLimExcel1.png">
The mean of the sampling distribution is simply equal to the mean of the population distribution, which is <b>8</b>.
The standard deviation of the sampling distribution is equal to the population standard deviation divided by the sample size, which is: 4 /√15 = <b>1.0328</b>.
We can also use the central limit theorem to answer questions about probabilities. For example, if a given population has a mean of <b>8</b> and a standard deviation of <b>4</b>, what is the probability that a given sample of size <b>15 </b>has a mean less than or equal to <b>7</b>?
To answer this question, we can use the <b>NORM.DIST() </b>function in Excel, which uses the following syntax:
<b>NORM.DIST(x, mean, standard_dev, cumulative)</b>
where:
<b>x: </b>the sample mean you’d like to test
<b>mean: </b>expected mean of sampling distribution
<b>standard_dev: </b>expected standard deviation of sampling distribution
<b>cumulative: </b>TRUE returns the value of the normal CDF; FALSE returns the value of the normal PDF. In our case, we will always use TRUE.
This function will return the probability that the sample mean is less than or equal to a certain value.
Here is the formula we would use in this example:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/centralLimExcel2.png">
This tells us that for a population with a mean of <b>8 </b>and a standard deviation of <b>4</b>, the probability that a given sample of size <b>15 </b>has a mean less than or equal to <b>7 </b>is <b>0.1665</b>.
We can also find the probability that a given sample size has a mean <em>greater </em>than a certain number by simply using the formula <b>1 – NORM.DIST()</b>.
For example, the following formula shows how to find the probability that a given sample size of 15 has a mean <em>greater than </em>7:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/centralLimExcel3.png">
Lastly, we can find the probability that a given sample size will have a mean <em>between </em>two numbers by using the formula <b>NORM.DIST(larger number) – NORM.DIST(smaller number)</b>.
For example, the following formula shows how to find the probability that a given sample size of 15 has a mean between 7 and 9:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/centralLimExcel4.png">
<h2><span class="orange">How to Apply the Central Limit Theorem in R (With Examples)</span></h2>
The <b>central limit theorem </b>states that the  sampling distribution  of a sample mean is approximately normal if the sample size is large enough, even if the population distribution is not normal.
The central limit theorem also states that the sampling distribution will have the following properties:
<b>1.</b> The mean of the sampling distribution will be equal to the mean of the population distribution:
<b>x = μ</b>
<b>2.</b> The standard deviation of the sampling distribution will be equal to the standard deviation of the population distribution divided by the sample size:
<b>s = σ / n</b>
The following example demonstrates how to apply the central limit theorem in R.
<h3>Example: Applying the Central Limit Theorem in R</h3>
Suppose the width of a turtle’s shell follows a  uniform distribution  with a minimum width of 2 inches and a maximum width of 6 inches.
That is, if we randomly selected a turtle and measured the width of its shell, it’s equally likely to be <em>any </em>width between 2 and 6 inches.
The following code shows how to create a dataset in R that contains the measurements of shell widths for 1,000 turtles, uniformally distributed between 2 and 6 inches:
<b>#make this example reproducible
set.seed(0)
#create random variable with sample size of 1000 that is uniformally distributed
data &lt;- runif(n=1000, min=2, max=6)
#create histogram to visualize distribution of turtle shell widths
hist(data, col='steelblue', main='Histogram of Turtle Shell Widths')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/central1.jpg"386">
Notice that the distribution of turtle shell widths is not normally distributed at all.
Now, imagine that we take repeated random samples of 5 turtles from this population and measure the sample mean over and over again.
The following code shows how to perform this process in R and create a histogram to visualize the distribution of sample means:
<b>#create empty vector to hold sample means
sample5 &lt;- c()
#take 1,000 random samples of size n=5
n = 1000
for (i in 1:n){
sample5[i] = mean(sample(data, 5, replace=TRUE))
}
#calculate mean and standard deviation of sample means
mean(sample5)
[1] 4.008103
sd(sample5)
[1] 0.5171083 
#create histogram to visualize sampling distribution of sample means
hist(sample5, col ='steelblue', xlab='Turtle Shell Width', main='Sample size = 5')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/central2.jpg"404">
Notice that the sampling distribution of sample means appears normally distributed, even though the distribution that the samples came from was not normally distributed.
Also notice the sample mean and sample standard deviation for this sampling distribution:
<b>x<U+0304></b>: 4.008
<b>s</b>: 0.517
Now suppose we increase the sample size that we use from n=5 to n=30 and recreate the histogram of sample means:
<b>#create empty vector to hold sample means
sample30 &lt;- c()
#take 1,000 random samples of size n=30
n = 1000
for (i in 1:n){
sample30[i] = mean(sample(data, 30, replace=TRUE))
}
#calculate mean and standard deviation of sample means
mean(sample30)
[1] 4.000472
sd(sample30)
[1] 0.2003791
#create histogram to visualize sampling distribution of sample means
hist(sample30, col ='steelblue', xlab='Turtle Shell Width', main='Sample size = 30')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/central3.jpg"427">
The sampling distribution is  normally distributed  once again, but the sample standard deviation is even smaller:
<b>s</b>: 0.200
This is because we used a larger sample size (n = 30) compared to the previous example (n = 5) so the standard deviation of sample means is even smaller.
If we keep using larger and larger sample sizes, we’ll find that the sample standard deviation gets smaller and smaller.
This illustrates the central limit theorem in practice.
<h2><span class="orange">5 Examples of Using the Central Limit Theorem in Real Life</span></h2>
The <b>central limit theorem </b>states that if we take repeated random samples from a population and calculate the mean value of each sample, then the distribution of the sample means will be approximately  normally distributed , <em>even if the population the samples came from is not normal</em>.
The central limit theorem also states that the mean of the sampling distribution will be equal to the mean of the population distribution:
<b>x = μ</b>
The central limit theorem is useful because it allows us to use a sample mean to draw conclusions about a larger  population mean .
The following examples show how the central limit theorem is used in different real-life situations.
<h3>Example 1: Economics</h3>
Economists often use the central limit theorem when using sample data to draw conclusions about a population.
For example, an economist may collect a  simple random sample  of 50 individuals in a town and use the average annual income of the individuals in the sample to estimate the average annual income of individuals in the entire town.
If the economist finds that the average annual income of the individuals in the sample is $58,000, then her best guess for the true average annual income of individuals in the entire town will be $58,000.
<h3>Example 2: Biology</h3>
Biologists use the central limit theorem whenever they use data from a sample of organisms to draw conclusions about the overall population of organisms.
For example, a biologist may measure the height of 30 randomly selected plants and then use the sample mean height to estimate the population mean height.
If the biologist finds that the sample mean height of the 30 plants is 10.3 inches, then her best guess for the population mean height will also be 10.3 inches.
<h3>Example 3: Manufacturing</h3>
Manufacturing plants often use the central limit theorem to estimate how many products produced by the plant are defective.
For example, the manager of the plant may randomly select 60 products produced by the plant in a given day and count how many of the products are defective. He can use the proportion of defective products in the sample to estimate the proportion of all products that are defective that are produced by the entire plant.
If he finds that 2% of products are defective in the sample, then his best guess for the proportion of defective products produced by the entire plant is also 2%.
<h3>Example 4: Surveys</h3>
Human Resources departments often use the central limit theorem when using surveys to draw conclusions about overall employee satisfaction at companies.
For example, the HR department of some company may randomly select 50 employees to take a survey that assesses their overall satisfaction on a scale of 1 to 10.
If it’s found that the average satisfaction among employees in the survey is 8.5 then the best guess for the average satisfaction rating of all employees at the company is also 8.5.
<h3>Example 5: Agriculture</h3>
Agricultural scientists use the central limit theorem whenever they use data from samples to draw conclusions about a larger population.
For example, an agricultural scientist may test a new fertilizer on 15 different fields and measure the average crop yield of each field.
If it’s found that the average field produces 400 pounds of wheat, then the best guess for the average crop yield for all fields will also be 400 pounds.
<h2><span class="orange">How to Apply the Central Limit Theorem on TI-84 Calculator</span></h2>
The  central limit theorem  states that the sampling distribution of a sample mean is approximately normal if the sample size is large enough, even if the population distribution is not normal.
The central limit theorem also states that the sampling distribution will have the following properties:
<b>1.</b> The mean of the sampling distribution will be equal to the mean of the population distribution:
<b>x = μ</b>
<b>2. </b>The standard deviation of the sampling distribution will be equal to the standard deviation of the population divided by the sample size:
<b>s = σ / √n</b>
To find probabilities related to the sample mean on a TI-84 calculator, we can use the <b>normalcdf()</b> function with the following syntax:
<b>normalcdf(lower value, upper value, x, s/√n)
</b>
where:
<b>x</b>: sample mean
<b>s</b>: sample standard deviation
<b>n</b>: sample size
To access this function on a TI-84 calculator, simply press 2nd then press VARS then scroll down to normalcdf( and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/clt1.png">
The following examples show how to use this function in practice.
<h3>Example 1: Find Probability Between Two Values</h3>
A distribution has a mean of 70 and a standard deviation of 7. If we select a random sample of size n = 35, find the probability that the sample mean is between 68 and 72.
We can use the following syntax on the TI-84:
<b>normalcdf(68, 72, 70, 7/√35)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/clt2.png">
The probability that the sample mean is between 68 and 72 is <b>0.909</b>.
<h3>Example 2: Find Probability Greater Than One Value</h3>
A distribution has a mean of 50 and a standard deviation of 4. If we select a random sample of size n = 30, find the probability that the sample mean is greater than 48.
We can use the following syntax on the TI-84:
<b>normalcdf(48, E99, 50, 4/√30)</b>
<b>Note:</b> You can access the “E” symbol by pressing 2nd and then pressing the , button.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/clt3.png">
The probability that the sample mean is greater than 48 is <b>0.9969</b>.
<h3>Example 3: Find Probability Less Than One Value</h3>
A distribution has a mean of 20 and a standard deviation of 3. If we select a random sample of size n = 40, find the probability that the sample mean is less than 19.
We can use the following syntax on the TI-84:
<b>normalcdf(-E99, 19, 20, 3/√40)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/clt4.png">
The probability that the sample mean is less than 19 is <b>0.0175</b>.
<h2><span class="orange">Central Limit Theorem: Definition + Examples</span></h2>
The <b>central limit theorem </b>states that  the sampling distribution of a sample mean  is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>.
The central limit theorem also states that the sampling distribution will have the following properties:
<b>1.</b> The mean of the sampling distribution will be equal to the mean of the population distribution:
<b>x = μ</b>
<b>2.</b> The variance of the sampling distribution will be equal to the variance of the population distribution divided by the sample size:
<b>s<sup>2</sup> = σ<sup>2</sup> / n</b>
<h2>Examples of the Central Limit Theorem</h2>
Here are a few examples to illustrate the central limit theorem in practice.
<h3>The Uniform Distribution</h3>
Suppose the width of a turtle’s shell follows a uniform distribution with a minimum width of 2 inches and a maximum width of 6 inches. That is, if we randomly selected a turtle and measured the width of its shell, it’s equally likely to be <em>any </em>width between 2 and 6 inches.
If we made a histogram to represent the distribution of turtle shell widths, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_uniform1.jpg"> 
The mean of a uniform distribution is <b>μ</b> = (b+a) / 2 where <em>b </em>is the largest possible value and <em>a </em>is the smallest possible value. In this case, it’s (6+2) / 2 = 4.
The variance of a uniform distribution is <b>σ<sup>2</sup></b> = (b-a)<sup>2</sup> / 12. In this case, it’s  (6-2)<sup>2</sup> / 12 = <b>1.33</b>
<h4><b>Taking random samples of 2 from the uniform distribution</b></h4>
Now, imagine that we take a random sample of 2 turtles from this population and measure the width of each turtles shell. Suppose the first turtle’s shell has a width of 3 inches and the second has a width of 6 inches. The mean width for this sample of 2 turtles is 4.5 inches.
Then, imagine that we take another random sample of 2 turtles from this population and again measure the width of each turtles shell. Suppose the first turtle’s shell has a width of 2.5 inches and the second also has a width of 2.5 inches. The mean width for this sample of 2 turtles is 2.5 inches.
Imagine that we just keep taking random samples of 2 turtles over and over again and keep finding the mean shell width each time.
If we made a histogram to represent the mean shell width of all these samples of 2 turtles, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_uniform2-2.jpg"> 
This is known as <b>the sampling distribution for the sample mean</b> because it shows the distribution of sample means. 
The mean of this sampling distribution is<b> x = μ = 4</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 1.33 / 2 = .665</b>
<h4><b>Taking random samples of 5 from the uniform distribution</b></h4>
Now, imagine that we repeated the same experiment, but this time we take random samples of 5 turtles over and over again and find the mean shell width each time.
If we made a histogram to represent the mean shell width of all these samples of 5 turtles, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_uniform4-2.jpg"> 
Notice how this distribution has more of a “bell” shape that resembles  the normal distribution . This is because when we take samples of 5, the variance among our sample means is much lower, so we’re less likely to obtain samples where the mean is close to 2 inches or close to 6 inches and more likely to obtain samples where the mean is closer to the true population mean of 4 inches.
The mean of this sampling distribution is<b> x = μ = 4</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 1.33 / 5 = .266</b>
<h4><b>Taking random samples of 30 from the uniform distribution</b></h4>
Now, imagine that we repeated the same experiment, but this time we take random samples of 30 turtles over and over again and find the mean shell width each time.
If we made a histogram to represent the mean shell width of all these samples of 30 turtles, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_uniform5-2.jpg"> 
Notice how this sampling distribution has even more of a bell shape and is much narrower than the previous two distributions.
The mean of this sampling distribution is<b> x = μ = 4</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 1.33 / 30 = .044</b>
<h3>The Chi-Square Distribution</h3>
Suppose the number of pets per family in a certain city follows a chi-square distribution with three degrees of freedom. If we made a histogram to represent the distribution of pets per family, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_chi1.jpg"> 
The mean of a chi-square distribution is simply the number of degrees of freedom (df). In this case, <b>μ</b> = <b>3</b>.
The variance of a chi-square distribution is  2 * df. In this case, <b>σ<sup>2</sup></b> = 2 * 3 = <b>6</b>.
<h4><b>Taking random samples of 2</b></h4>
Imagine that we take a random sample of 2 families from this population and count the number of pets in each family. Suppose the first family has 4 pets and the second family has 1 pet. The mean number of pets for this sample of 2 families is 2.5.
Then, imagine that we take another random sample of 2 families from this population and again count the number of pets in each family. Suppose the first family has 6 pets and the second family has 4 pets. The mean number of pets for this sample of 2 families is 5.
Imagine that we just keep taking random samples of 2 families over and over again and keep finding the mean number of pets each time.
If we made a histogram to represent the mean number of pets of all these samples of 2 families, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_chi2.jpg"> 
The mean of this sampling distribution is<b> x = μ = 3</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 6 / 2 = 3</b>
<h4><b>Taking random samples of 10</b></h4>
Now, imagine that we repeated the same experiment, but this time we take random samples of 10 families over and over again and find the mean number of pets per family each time.
If we made a histogram to represent the mean number of pets per family in all these samples of 10 families, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_chi3.jpg"> 
The mean of this sampling distribution is<b> x = μ = 3</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 6 / 10 = 0.6</b>
<h4><b>Taking random samples of 30</b></h4>
Now, imagine that we repeated the same experiment, but this time we take random samples of 30 families over and over again and find the mean number of pets per family each time.
If we made a histogram to represent the mean number of pets per family in all these samples of 30 families, it would look like this:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/CLT_chi4.jpg"> 
The mean of this sampling distribution is<b> x = μ = 3</b>
The variance of this sampling distribution is <b>s<sup>2</sup> = σ<sup>2</sup> / n = 6 / 30 = 0.2</b>
<h2>Summary</h2>
Here are the key takeaways from these two examples:
The sampling distribution of a sample mean is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>. In the two examples above, neither the uniform distribution nor the chi-square distribution were normal (they didn’t have a “bell” shape at all), yet when we took a large enough sample size, the distribution of the sample mean turned out to be normal.
The larger the sample size, the smaller the variance of the sample mean.
<h2>Defining “Large Enough”</h2>
Recall that the central limit theorem states that the sampling distribution of a sample mean is approximately normal if the sample size is <b>“large enough”</b>, even if the population distribution is not normal.
There is no exact definition for how large a sample size needs to be in order for the central limit theorem to apply, but in general it depends on the skewness of the population distribution that the sample comes from:
If the population distribution is symmetric, sometimes a sample size as small as 15 is sufficient.
If the population distribution is skewed, generally a sample size of at least 30 is needed.
If the population distribution is extremely skewed, then a sample size of 40 or higher may be necessary.
Check out this tutorial on  The Large Sample Condition  for more information on this topic.
<h2><span class="orange">What is Central Tendency Bias?</span></h2>
<b>Central tendency bias</b> refers to the tendency of an individual to rate most items on a survey in the middle of a rating scale.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/centraltendencybias1.png">
This type of bias occurs most often with internal surveys – when managers have to rate their employees. For example, on a 10-point scale managers may be likely to rate employees between 6 and 8 for most categories.
Managers may do this because they want to avoid showing preferential treatment to certain employees or to simply avoid backlash from employees for making ratings on the extreme ends of the scale.
This type of bias occurs less frequently when customers are filling out surveys for a company because there are lower stakes.
Customers aren’t directly rating people they know and interact with on a daily basis, so they’re more willing to provide extremely high or extremely low ratings when they feel the need to do so.
<h3>The Problem with Central Tendency Bias</h3>
There are two problems caused by central tendency bias:
<b>1. The data may be inaccurate.</b>
If managers rate each employee in the middle of a rating scale simply because they’re afraid of providing extreme ratings, this means the data collected in the survey will be inaccurate and not reflect true employee performance.
<b>2. The data will be unhelpful.</b>
If managers rate each employee in the middle of a rating scale for each category, then the performance of all of the employees will appear mostly equal. This makes it difficult to identify the individuals who should be promoted or given a bonus.
<h3>How to Avoid Central Tendency Bias</h3>
There are three ways to avoid central tendency bias in surveys:
<b>1. Don’t require managers to provide justification for their ratings.</b>
Sometimes managers don’t want to provide ratings on the low end or high end of a scale simply because they’re required to provide justification for unusually low or high ratings.
By not requiring managers to provide justification for their ratings, they’re more likely to be honest about whether or not an employee should be rated high or lower because they don’t have to do additional work in providing a justification.
The benefit of this approach is that you’re likely to get more accurate data, but the downside is that you don’t actually get the reasoning behind particularly low or high ratings.
<b>2. Allow for rank ordering.</b>
Instead of asking a manager to provide a 1-10 rating for each individual employee on overall productivity, you could instead ask them to rank the employees from least productive to most productive.
This will force managers to identify low-productivity and high-productivity employees simply because not each employee can be the least productive or most productive.
<b>3. Ensure that questions are clear.</b>
Another obvious way to avoid central tendency bias is to make sure that the survey questions are clear. Often when questions are unclear, managers simply assign a rating in the middle because they’re unsure of the question being asked.
For example, consider the following survey with unclear questions:
How responsible is employee <em>X</em> on a scale of 1-10?
How would you rate the leadership of employee <em>X</em> on a scale of 1-10?
Now, consider the revised survey with clear questions:
Rate the responsibility of employee <em>X</em> on a scale of 1-10, with 1 indicating that they’re not responsible in any manner and 10 indicating that they’re completely responsible for their actions and their work.
Rate the leadership of employee <em>X</em> on a scale of 1-10, with 1 indicating that they have never taken a leadership role on a project and do not exhibit any leadership traits and 10 indicating that they always take on leadership roles and exhibit leadership traits on all projects when needed.
The revised survey is more likely to produce accurate data because they’re much more clear about what the manager is supposed to provide ratings for.
<h2><span class="orange">How to Change Bin Width of Histograms in Excel</span></h2>
A <b>histogram</b> is a plot that can be used to quickly visualize the distribution of values in a dataset.
This tutorial provides a step-by-step example of how to create a histogram in Excel and how to modify the <b>bin width</b> so that the histogram looks exactly how you’d like.
<h3>Step 1: Create the Data</h3>
First, we’ll create the following dataset that shows the annual income of 26 different people:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth1.png">
<h3>Step 2: Create the Histogram</h3>
Next, we’ll highlight the two columns of data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth2.png">
Then we’ll click the <b>INSERT</b> tab along the top ribbon, then we’ll click the <b>Histogram</b> icon within the <b>Charts</b> section.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth3.png">
This creates the following histogram by default:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth4.png">
<h3>Step 3: Adjust the Bin Width</h3>
To adjust the bin width, right click the horizontal axis on the histogram and then click <b>Format Axis</b> from the dropdown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth5.png">
In the window that appears to the right, we can see that Excel chose the bin width to be <b>29,000.</b> We can change this to any number we’d like.
For example, we could increase the bin width to <b>50,000</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth6.png">
Notice how this increases the width of each bin and reduces the total number of bins.
We could also decrease the bin width to <b>10,000:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/binWidth7.png">
This decreases the width of each bin and increases the total number of bins in the histogram.
Feel free to modify the bin width to any value you’d like and keep in mind the following rules of thumb:
<b>Increasing the bin width</b> often decreases the number of total bins in the histogram and leads to fatter and fewer bins.
<b>Decreasing the bin width</b> often increases the number of total bins in the histogram and leads to narrower and more bins.
<em>You can find more Excel tutorials on  this page .</em>
<h2><span class="orange">How to Change Font Sizes on a Matplotlib Plot</span></h2>
Often you may want to change the font sizes of various elements on a Matplotlib plot. Fortunately this is easy to do using the following code:
<b>import matplotlib.pyplot as plt
plt.rc('font', size=10) #controls default text size
plt.rc('axes', titlesize=10) #fontsize of the title
plt.rc('axes', labelsize=10) #fontsize of the x and y labels
plt.rc('xtick', labelsize=10) #fontsize of the x tick labels
plt.rc('ytick', labelsize=10) #fontsize of the y tick labels
plt.rc('legend', fontsize=10) #fontsize of the legend
</b>
The following examples illustrates how to change the font sizes of various elements in the following matplotlib scatterplot:
<b>import matplotlib.pyplot as plt
x = [3, 4, 6, 7, 8]
y = [12, 14, 15, 19, 24]
plt.scatter(x, y)
plt.title('title')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/matplotlibChangeFont1.png">
<em><b>Note: </b>The default font size for all elements is <b>10</b>.</em>
<h3>Example 1: Change the Font Size of All Elements</h3>
The following code shows how to change the font size of every element in the plot:
<b>#set font of all elements to size 15
plt.rc('font', size=15) 
#create plot
plt.scatter(x, y)
plt.title('title')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/matplotlibChangeFont2.png">
<h3>Example 2: Change the Font Size of the Title</h3>
The following code shows how to change the font size of the title of the plot:
<b>#set title font to size 50
plt.rc('axes', titlesize=50) 
#create plot
plt.scatter(x, y)
plt.title('title')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/matplotlibChangeFont3.png">
<h3>Example 3: Change the Font Size of the Axes Labels</h3>
The following code shows how to change the font size of the axes labels of the plot:
<b>#set axes labels font to size 20
plt.rc('axes', labelsize=20) 
#create plot
plt.scatter(x, y)
plt.title('title')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/matplotlibChangeFont4.png">
<h3>Example 4: Change the Font Size of the Tick Labels</h3>
The following code shows how to change the font size of the tick labels of the plot:
<b>#set tick labels font to size 20
plt.rc('xtick', labelsize=20) 
plt.rc('ytick', labelsize=20) 
#create plot
plt.scatter(x, y)
plt.title('title')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/matplotlibChangeFont5.png">
<h3>Bonus: Restore the Default Font Sizes</h3>
You can use the following code to restore all fonts to their default size at any point:
<b>plt.rcParams.update(plt.rcParamsDefault)
</b>
<em>You can find more Matplotlib tutorials  here .</em>
<h2><span class="orange">How to Change the Legend Title in ggplot2 (With Examples)</span></h2>
There are two easy ways to change the legend title in a ggplot2 chart:
<b>Method 1: Use labs()</b>
<b>ggplot(data, aes(x=x_var, y=y_var, fill=fill_var)) + 
  geom_boxplot() + 
  labs(fill='Legend Title')
</b>
<b>Method 2: Use scale_fill_manual()</b>
<b>ggplot(data, aes(x=x_var, y=y_var, fill=fill_var)) + 
  geom_boxplot() +
  scale_fill_manual('Legend Title', values=c('color1', 'color2'))
</b>
This tutorial shows examples of how to use these two methods in practice.
<h3>Method 1: Change Legend Title Using labs()</h3>
The following code shows how to create a  grouped boxplot  for a given dataset:
<b>library(ggplot2) 
#create dataset
data &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=50),   program=rep(c('low', 'high'), each=25),   values=seq(1:150)+sample(1:100, 150, replace=TRUE))
#create boxplot
ggplot(data, aes(x=team, y=values, fill=program)) + 
  geom_boxplot()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/legendTitleggplot1.png">
By default, ggplot2 uses the variable name in the dataset as the legend title. However, we can use the <b>labs()</b> function to easily change it:
<b>library(ggplot2) 
#create dataset
data &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=50),   program=rep(c('low', 'high'), each=25),   values=seq(1:150)+sample(1:100, 150, replace=TRUE))
#create boxplot
ggplot(data, aes(x=team, y=values, fill=program)) + 
  geom_boxplot() + 
  labs(fill='Program Type')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/legendTitleggplot2.png">
We can also add a line break in the legend title by placing <b>\n </b>wherever we’d like the new line to start:
<b>library(ggplot2) 
#create dataset
data &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=50),   program=rep(c('low', 'high'), each=25),   values=seq(1:150)+sample(1:100, 150, replace=TRUE))
#create boxplot
ggplot(data, aes(x=team, y=values, fill=program)) + 
  geom_boxplot() + 
  labs(fill='Program\nType')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/legendTitleggplot3.png">
<h3>Method 2: Change Legend Title Using scale_fill_manual()</h3>
We can also use the <b>scale_fill_manual() </b>function to simultaneously specify a legend title and a vector of color values to use:
<b>library(ggplot2) 
#create dataset
data &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=50),   program=rep(c('low', 'high'), each=25),   values=seq(1:150)+sample(1:100, 150, replace=TRUE))
#create boxplot
ggplot(data, aes(x=team, y=values, fill=program)) + 
  geom_boxplot() + 
  scale_fill_manual('Program Type', values=c('pink','blue'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/legendTitleggplot4.png">
Note that the values for the colors can either be names or hex color codes.
<h2><span class="orange">How to Change Row Names in R (With Examples)</span></h2>
You can use the <b>row.names()</b> function to quickly get and set the row names of a data frame in R.
This tutorial provides several examples of how to use this function in practice on the built-in <b>mtcars</b> dataset in R:
<b>#view first six rows of <em>mtcars</em>
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</b>
<h3>How to Get Row Names</h3>
You can use the following syntax to view the first few row names of the <em>mtcars</em> data frame:
<b>#view first six row names of <em>mtcars</em>
head(row.names(mtcars))
[1] "Mazda RX4"         "Mazda RX4 Wag"     "Datsun 710"       
[4] "Hornet 4 Drive"    "Hornet Sportabout" "Valiant" 
</b>
<h3>How to Change One Row Name</h3>
You can use the following syntax to change on specific row name:
<b>#change the row name called <em>Datsun710</em> to <em>710</em>
row.names(mtcars)[row.names(mtcars) == "Datsun 710"] &lt;- "710"
#view first six row names of <em>mtcars</em> 
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
710               22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
<h3>How to Change All Row Names</h3>
You can use the following syntax to change all of the row names to a list of integers starting at 1:
<b>#change row names to a list of integers
row.names(mtcars) &lt;- 1:nrow(mtcars)
#view first six row names of <em>mtcars</em> 
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
You can also use the <b>paste()</b> function to append a word in front of each row name:
<b>#change row names
row.names(mtcars) &lt;- paste("row", 1:nrow(mtcars))
#view first six row names of <em>mtcars</em> 
head(mtcars)
       mpg cyl disp  hp drat    wt  qsec vs am gear carb
row 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
row 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
row 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
row 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
row 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
row 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
Note that each row now has the word “row” appended to the front.
<h2><span class="orange">How to Fix: character string is not in a standard unambiguous format</span></h2>
One common error you may encounter in R is:
<b>Error in as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format
</b>
This error usually occurs when you attempt to convert an object in R to a date format, but the object is currently either a character or factor.
To fix this error, you must first convert the object to numeric. 
This tutorial explains how to fix this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(date=c('1459397140', '1464397220', '1513467142'), sales=c(140, 199, 243))
#view data frame
df
        date sales
1 1459397140   140
2 1464397220   199
3 1513467142   243</b>
Now suppose we attempt to convert the values in the <b>date</b> column to a date format:
<b>#attempt to convert values in date column to date
df$date &lt;- as.POSIXct(df$date, origin='1970-01-01')
Error in as.POSIXlt.character(x, tz, ...) : 
  character string is not in a standard unambiguous format
</b>
We receive an error because the values in the <b>date</b> column are currently in a character format, which the <b>as.POSIXct()</b> function doesn’t know how to handle.
<h2>How to Fix the Error</h2>
To fix this error, we need to use<b> as.numeric()</b> to first convert the values in the <b>date</b> column to a numeric format, which is one that <b>as.POSIXct</b> can handle:
<b>#convert values in date column to date
df$date &lt;- as.POSIXct(as.numeric(as.character(df$date)), origin='1970-01-01')
#view updated data frame
df
 date sales
1 2016-03-31 04:05:40   140
2 2016-05-28 01:00:20   199
3 2017-12-16 23:32:22   243</b>
This time we don’t receive an error and we’re able to successfully convert the values in the <b>date</b> column to a date format because we first converted the values to a numeric format.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in R:
 How to Fix: (list) object cannot be coerced to type ‘double’ 
 How to Fix in R: invalid model formula in ExtractVars 
 How to Fix in R: replacement has length zero 
<h2><span class="orange">How to Convert Character to Factor in R (With Examples)</span></h2>
We can use the following syntax to convert a character vector to a factor vector in R:
<b>factor_vector &lt;- as.factor(character_vector)
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Convert a Vector from Character to Factor</h3>
The following code shows how to convert a character vector to a factor vector:
<b>#create character vector
character_vector &lt;- c('First', 'Second', 'Third')
#convert character vector to factor vector
factor_vector &lt;- as.factor(character_vector)
#view factor vector
factor_vector
[1] First  Second Third 
Levels: First Second Third
#confirm class of factor vector
class(factor_vector)
[1] "factor"
</b>
<h3>Example 2: Convert a Column from Character to Factor</h3>
The following code shows how to convert a specific column in a data frame from character to factor:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c(28, 34, 35, 36, 40))
#convert column 'a' from character to factor
df$a &lt;- as.factor(df$a)
#view new data frame
df
       a  b
1  First 28
2 Second 34
3  Third 40
#confirm class of factor vector
class(df$a)
[1] "factor"
</b>
<h3>Example 3: Convert Several Columns from Character to Factor</h3>
The following code shows how to convert all character columns in a data frame from character to factor:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c('28', '34', '35', '36', '40'), c = as.factor(c(1, 2, 3, 4, 5)), d = c(45, 56, 54, 57, 59))
#display classes of each column
sapply(df, class)
          a           b           c           d 
"character" "character"    "factor"   "numeric" 
#convert all character columns to factor
df &lt;- as.data.frame(unclass(df), stringsAsFactors = TRUE)
#display classes of each column
sapply(df, class)
        a         b         c         d 
 "factor"  "factor"  "factor" "numeric" </b>
This code made the following changes to the data frame columns:
<b>Column a:</b> From character to factor
<b>Column b:</b> From character to factor
<b>Column c:</b> Unchanged (since it was already a factor)
<b>Column d:</b> Unchanged (since it was numeric)
By using the  apply()  and  sapply()  functions, we were able to convert only the character columns to factor columns and leave all other columns unchanged.
<h2><span class="orange">How to Convert Character to Numeric in R (With Examples)</span></h2>
We can use the following syntax to convert a character vector to a numeric vector in R:
<b>numeric_vector &lt;- as.numeric(character_vector)
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Convert a Vector from Character to Numeric</h3>
The following code shows how to convert a character vector to a numeric vector:
<b>#create character vector
chars &lt;- c('12', '14', '19', '22', '26')
#convert character vector to numeric vector
numbers &lt;- as.numeric(chars)
#view numeric vector
numbers
[1] 12 14 19 22 26
#confirm class of numeric vector
class(numbers)
[1] "numeric"
</b>
<h3>Example 2: Convert a Column from Character to Numeric</h3>
The following code shows how to convert a specific column in a data frame from character to numeric:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c(28, 34, 35, 36, 40))
#convert column 'a' from character to numeric
df$a &lt;- as.numeric(df$a)
#view new data frame
df
   a  b
1 12 28
2 14 34
3 19 35
4 22 36
5 26 40
#confirm class of numeric vector
class(df$a)
[1] "numeric"
</b>
<h3>Example 3: Convert Several Columns from Character to Numeric</h3>
The following code shows how to convert all character columns in a data frame from character to numeric:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c('28', '34', '35', '36', '40'), c = as.factor(c(1, 2, 3, 4, 5)), d = c(45, 56, 54, 57, 59))
#display classes of each column
sapply(df, class)
          a           b           c           d 
"character" "character"    "factor"   "numeric" 
#identify all character columns
chars &lt;- sapply(df, is.character)
#convert all character columns to numeric
df[ , chars] &lt;- as.data.frame(apply(df[ , chars], 2, as.numeric))
#display classes of each column
sapply(df, class)
        a         b         c         d 
"numeric" "numeric"  "factor" "numeric" </b>
This code made the following changes to the data frame columns:
<b>Column a:</b> From character to numeric
<b>Column b:</b> From character to numeric
<b>Column c:</b> Unchanged (since it was a factor)
<b>Column d:</b> Unchanged (since it was already numeric)
By using the  apply()  and  sapply()  functions, we were able to convert only the character columns to numeric columns and leave all other columns unchanged.
<h2><span class="orange">Chauvenet’s Criterion: Definition & Example</span></h2>
An <b>outlier</b> is an  observation  that lies abnormally far away from other values in a dataset. Outliers can be problematic because they can affect the results of an analysis.
One way to identify outliers in a dataset is to use <b>Chauvenet’s Criterion</b>, which uses the following process:
<b>1.</b> For each individual value x<sub>i</sub> in the dataset, calculate the deviation from the mean as:
<b>Deviation = |x<sub>i</sub> – x| / s</b> 
where x is the sample mean and <em>s</em> is the sample standard deviation.
<b>2. </b>Compare the deviations of each individual value to the critical values of Chauvenet’s Criterion Table below. For individual data values with deviations greater than those found in the table, declare those data values to be outliers.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/chauvenet1.png">
<h3>Chauvenet’s Criterion: An Example</h3>
Suppose we have the following dataset of 15 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/chauvenet2.png">
The sample mean for this dataset is <b>x =</b> <b>17.067 </b>and the sample standard deviation is <b>s =</b> <b>10.096</b>. For each individual data value, we can calculate calculate its deviation as:
Deviation = |x<sub>i</sub> – x| / s
For example:
The first data value would have a deviation of |4 – 17.067| / 10.096 = <b>1.294</b>.
The first data value would have a deviation of |6 – 17.067| / 10.096 = <b>1.096</b>.
And so on.
We can use the same formula to calculate the deviation of each individual data value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/chauvenet3.png">
We can then refer to Chauvenet’s Criterion Table and find that the critical value that corresponds to a sample size of n=15 is <b>2.128</b>. Thus, any value with a deviation greater than 2.128 can be considered an outlier.
It turns out that the value <b>42 </b>has a deviation greater than 2.128:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/chauvenet5.png">
Thus, the value 42 is the only outlier in this dataset.
<h3>Cautions on Using Chauvenet’s Criterion</h3>
Chauvenet’s Criterion makes the assumption that the values in a dataset are  normally distributed . If this assumption is not met, then using Chauvenet’s Criterion to identify outliers is likely not valid.
If you do use this method and find that a value is an outlier, you should first verify that the value is not a result of a data entry error. Sometimes data is simply entered incorrectly. 
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just be sure to mention that you removed an outlier when you report your results.
Also, this method should only be used on a given dataset once. For example, suppose we use this criterion to identify the value <b>42 </b>as an outlier in the previous example and remove this value from the dataset.
We then shouldn’t recalculate the  sample mean  and sample standard deviation and calculate the deviations once again to find more outliers.
<h2><span class="orange">How to Apply Chebyshev’s Theorem in Excel</span></h2>
<b>Chebyshev’s Theorem</b> states that for any number k greater than 1, at least <b>1 – 1/k<sup>2</sup> </b>of the data values in any shaped distribution lie within k standard deviations of the mean.
For example, for any shaped distribution at least 1 – 1/3<sup>2</sup> = 88.89% of the values in the distribution will lie within 3 standard deviations of the mean.
This tutorial illustrates several examples of how to apply Chebyshev’s Theorem in Excel.
<b>Example 1: Use Chebyshev’s Theorem to find what percentage of values will fall between 30 and 70 for a dataset with a mean of 50 and standard deviation of 10.</b>
First, determine the value for k. We can do this by finding out how many standard deviations away 30 and 70 are from the mean:
(30 – mean) / standard deviation = (30 – 50) / 10 = -20 / 10 =<b> -2</b>
(70 – mean) / standard deviation = (70 – 50) / 10 = 20 / 10 = <b>2</b>
The values 30 and 70 are 2 standard deviations below and above the mean, respectively. Thus, <b>k = 2</b>.
We can then use the following formula in Excel to find the minimum percentage of values that fall within 2 standard deviations of the mean for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chebExcel1.png">
The percentage of values that fall within 30 and 70 for this dataset will be <b>at least 75%</b>.
<b>Example 2: Use Chebyshev’s Theorem to find what percentage of values will fall between 20 and 50 for a dataset with a mean of 35 and standard deviation of 5.</b>
First, determine the value for k. We can do this by finding out how many standard deviations away 20 and 50 are from the mean:
(20 – mean) / standard deviation = (20 – 35) / 5 = -15 / 5 =<b> -3</b>
(50 – mean) / standard deviation = (50 – 35) / 5 = 15 / 5 = <b>3</b>
The values 20 and 50 are 3 standard deviations below and above the mean, respectively. Thus, <b>k = 3</b>.
We can then use the following formula in Excel to find the minimum percentage of values that fall within 3 standard deviations of the mean for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chebExcel2.png">
The percentage of values that fall within 20 and 50 for this dataset will be <b>at least 88.89%</b>.
<b>Example 3: Use Chebyshev’s Theorem to find what percentage of values will fall between 80 and 120 for a dataset with a mean of 100 and standard deviation of 5.</b>
First, determine the value for k. We can do this by finding out how many standard deviations away 80 and 120 are from the mean:
(80 – mean) / standard deviation = (80 – 100) / 5 = -20 / 5 =<b> -4</b>
(120 – mean) / standard deviation = (120 – 100) / 5 = 20 / 5 = <b>4</b>
The values 80 and 120 are 4 standard deviations below and above the mean, respectively. Thus, <b>k = 4</b>.
We can then use the following formula in Excel to find the minimum percentage of values that fall within 4 standard deviations of the mean for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chebExcel3.png">
The percentage of values that fall within 80 and 120 for this dataset will be <b>at least 93.75%</b>.
<h2><span class="orange">How to Check if Cell is Empty in Google Sheets</span></h2>
You can use the following formulas to check if cells are empty in Google Sheets:
<b>Method 1: Check if One Cell is Empty</b>
<b>=IF(ISBLANK(A1),"Empty","Not Empty")
</b>
If cell <b>A1</b> is empty, this formula returns “Empty” and if it’s not then the formula returns “Not Empty.”
<b>Method 2: Check if Multiple Cells are Empty</b>
<b>=IF(AND(ISBLANK(A1), ISBLANK(B1)),"Empty","Not Empty")
</b>
If cells <b>A1</b> and <b>B1</b> are both empty, this formula returns “Empty.” Otherwise, it returns “Not Empty.”
The following examples show how to use each method in Google Sheets.
<h3>Example 1: Check if One Cell is Empty</h3>
Suppose we have the following dataset in Google Sheets that shows the number of points scored by various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/emptycheck1.jpg"468">
We’ll use the following formula to check if each cell in column A is empty:
<b>=IF(ISBLANK(A2),"Empty","Not Empty") </b>
We’ll type this formula into cell <b>B2</b> and then copy and paste it down to every remaining cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/emptycheck2.jpg">
The values in column B tell us whether each corresponding value in column A is empty or not.
<h3>Example 2: Check if Multiple Cells are Empty</h3>
Suppose we have the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/emptycheck3-1.jpg"448">
We’ll use the following formula to check if the cell in both column A and column B are empty:
<b>=IF(AND(ISBLANK(A2), ISBLANK(B2)),"Empty","Not Empty")
</b>
We’ll type this formula into cell <b>C2</b> and then copy and paste it down to every remaining cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/emptycheck4.jpg">
The values in column C tell us whether or not both corresponding values in column A and column B are empty.
<b>Note</b>: You can find the complete documentation for the <b>ISBLANK </b>function in Google Sheets  here .
<h2><span class="orange">Chi-Square Critical Value Calculator</span></h2>
This calculator finds the <b>Chi-Square critical value</b> for a given degrees of freedom and significance level.
<label for="mean">Degrees of freedom</label>
<input type="number" id="df" value="13">
<label for="x">Significance level</label>
<input type="number" id="p" value=".05" min="0" max="1" step=".01">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
X<sup>2</sup> critical value = 22.36203
<script>
function calc() {
//get input values
var p = +document.getElementById('p').value;
var df = +document.getElementById('df').value;
//calculate critical value
var x2 = jStat.chisquare.inv(1-p, df);
//output probabilities
document.getElementById('x2').innerHTML = x2.toFixed(5);
}
</script>
<h2><span class="orange">How to Find the Chi-Square Critical Value in Excel</span></h2>
When you conduct a Chi-Square test, you will get a test statistic as a result. To determine if the results of the Chi-Square test are statistically significant, you can compare the test statistic to a<b> Chi-Square critical value</b>. If the test statistic is greater than the Chi-Square critical value, then the results of the test are statistically significant.
The Chi-Square critical value can be found by using a  Chi-Square distribution table  or by using statistical software.
To find the Chi-Square critical value, you need:
A significance level (common choices are 0.01, 0.05, and 0.10)
Degrees of freedom
Using these two values, you can determine the Chi-Square value to be compared with the test statistic.
<h2>How to Find the Chi-Square Critical Value in Excel</h2>
To find the Chi-Square critical value in Excel, you can use the<b> CHISQ.INV.RT()</b> function, which uses the following syntax:
<b>CHISQ.INV.RT</b>(probability, deg_freedom)
<b>probability: </b>The significance level to use
<b>deg_freedom</b>: The degrees of freedom
This function returns the critical value from the Chi-Square distribution based on the significance level and the degrees of freedom provided.
For example, suppose we would like to find the Chi-square critical value for a significance level of 0.05 and degrees of freedom = 11. 
In Excel, we can type the following formula: <b>CHISQ.INV.RT(0.05, 11)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/chiExcel1.jpg">
This returns the value <b>19.67514</b>. This is the critical value for a significance level of 0.05 and degrees of freedom = 11.
Note that this also matches the number we would find in the  Chi-Square distribution table  with α = 0.05, DF <em>(degrees of freedom)</em> = 11.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/chiExcel2.jpg">
<h2>Cautions on Finding the Chi-Square Critical Value in Excel</h2>
Note that the<b> CHISQ.INV.RT()</b> function in Excel will throw an error if any of the following occur:
If any argument is non-numeric.
If the value for <em>probability </em>is less than zero or greater than 1.
If the value for <em>deg_freedom</em><em> </em>is less than 1.
<h2><span class="orange">How to Find the Chi-Square Critical Value in Python</span></h2>
When you conduct a Chi-Square test, you will get a test statistic as a result. To determine if the results of the Chi-Square test are statistically significant, you can compare the test statistic to a <b>Chi-Square critical value</b>. If the test statistic is greater than the Chi-Square critical value, then the results of the test are statistically significant.
The Chi-Square critical value can be found by using a  Chi-Square distribution table  or by using statistical software.
To find the Chi-Square critical value, you need:
A significance level (common choices are 0.01, 0.05, and 0.10)
Degrees of freedom
Using these two values, you can determine the Chi-Square value to be compared with the test statistic.
<h3>How to Find the Chi-Square Critical Value in Python</h3>
To find the Chi-Square critical value in Python, you can use the  scipy.stats.chi2.ppf() function , which uses the following syntax:
<b>scipy.stats.chi2.ppf(q, df)</b>
where:
<b>q: </b>The significance level to use
<b>df</b>: The degrees of freedom
This function returns the critical value from the Chi-Square distribution based on the significance level and degrees of freedom provided.
For example, suppose we would like to find the Chi-Square critical value for a significance level of 0.05 and degrees of freedom = 11.
<b>import scipy.stats
#find Chi-Square critical value
scipy.stats.chi2.ppf(1-.05, df=11)
19.67514</b>
The Chi-Square critical value for a significance level of 0.05 and degrees of freedom = 11 is <b>19.67514</b>.
Thus, if we’re conducting some type of Chi-Square test then we can compare the Chi-Square test statistic to <b>19.67514</b>. If the test statistic is greater than 19.67514, then the results of the test are statistically significant.
Note that smaller values of alpha will lead to larger Chi-Square critical values. For example, consider the Chi-Square critical value for a significance level of <b>0.01</b>, and degrees of freedom = 11. 
<b>scipy.stats.chi2.ppf(1-.01, df=11)
24.72497</b>
And consider the Chi-Square critical value with the exact same degrees of freedom, but with a significance level of <b>0.005</b>:
<b>scipy.stats.chi2.ppf(1-.005 df=11) 
26.75685</b>
<em>Refer to the  SciPy documentation  for the exact details of the chi2.ppf() function.</em>
<h2><span class="orange">How to Find the Chi-Square Critical Value in R</span></h2>
When you conduct a Chi-Square test, you will get a test statistic as a result. To determine if the results of the Chi-Square test are statistically significant, you can compare the test statistic to a <b>Chi-Square critical value</b>. If the test statistic is greater than the Chi-Square critical value, then the results of the test are statistically significant.
The Chi-Square critical value can be found by using a  Chi-Square distribution table  or by using statistical software.
To find the Chi-Square critical value, you need:
A significance level (common choices are 0.01, 0.05, and 0.10)
Degrees of freedom
Using these two values, you can determine the Chi-Square value to be compared with the test statistic.
<h3>How to Find the Chi-Square Critical Value in R</h3>
To find the Chi-Square critical value in R, you can use the qchisq() function, which uses the following syntax:
<b>qchisq(p, df, lower.tail=TRUE)</b>
where:
<b>p: </b>The significance level to use
<b>df</b>: The degrees of freedom
<b>lower.tail: </b>If TRUE, the probability to the left of <b>p </b>in the F distribution is returned. If FALSE, the probability to the right is returned. Default is TRUE.
This function returns the critical value from the Chi-Square distribution based on the significance level and degrees of freedom provided.
For example, suppose we would like to find the Chi-Square critical value for a significance level of 0.05 and degrees of freedom = 11.
<b>#find Chi-Square critical value
qchisq(p=.05, df=11, lower.tail=FALSE)
[1] 19.67514</b>
The Chi-Square critical value for a significance level of 0.05 and degrees of freedom = 11 is <b>19.67514</b>.
Thus, if we’re conducting some type of Chi-Square test then we can compare the Chi-Square test statistic to <b>19.67514</b>. If the test statistic is greater than 19.67514, then the results of the test are statistically significant.
Note that smaller values of alpha will lead to larger Chi-Square critical values. For example, consider the Chi-Square critical value for a significance level of <b>0.01</b>, and degrees of freedom = 11. 
<b>#find Chi-Square critical value
qchisq(p=.01, df=11, lower.tail=FALSE)
[1] 24.72497</b>
And consider the Chi-Square critical value with the exact same degrees of freedom, but with a significance level of <b>0.005</b>:
<b>#find Chi-Square critical value
qchisq(p=.005, df=11, lower.tail=FALSE)
[1] 26.75685</b>
<i>You can find more R tutorials  here .</i>
<h2><span class="orange">The Chi-Square Distribution in R: dchisq, pchisq, qchisq, rchisq</span></h2>
This tutorial explains how to work with the Chi-Square distribution in R using the following functions:
<b>dchisq</b>: returns the value of the Chi-Square probability density function.
<b>pchisq</b>: returns the value of the Chi-Square cumulative density function.
<b>qchisq</b>: returns the value of the Chi-Square quantile function.
<b>rchisq</b>: generates a vector of Chi-Square distributed random variables.
The following examples show how to use each of these functions in practice.
<h2>dchisq</h2>
We often use the <b>dchisq()</b> function with the <b>curve()</b> function to plot a Chi-Square distribution with a certain number of degrees of freedom.
For example, we can use the following code to plot a Chi-Square distribution with 5 degrees of freedom:
<b>#plot Chi_Square distribution with 5 degrees of freedom
curve(dchisq(x, df=5), from=0, to=20)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/chisq1.jpg"441">
The x-axis shows the values of a Chi-Square test statistic and the y-axis shows the corresponding value of the probability density function.
<b>Related:</b>  How to Easily Plot a Chi-Square Distribution in R 
<h2>pchisq</h2>
We often use the <b>pchisq() </b>function to find the  p-value  that corresponds to a given Chi-Square test statistic.
For example, suppose we perform a  Chi-Square Test of Independence  and end up with a test statistic of X<sup>2</sup> = 0.86404 with 2 degrees of freedom.
We can use the <b>pchisq()</b> function to find the p-value that corresponds to this test statistic:
<b>#calculate p-value for given test statistic with 2 degrees of freedom
1-pchisq(0.86404, df=2)
[1] 0.6491964
</b>
The p-value turns out to be <b>0.6491964</b>.
We can also confirm this is correct by using the  Chi-Square Score to P-Value Calculator .
<h2>qchisq</h2>
We often use the <b>qchisq() </b>function to find the Chi-Square critical value that corresponds to a given significance level and degrees of freedom.
For example, we can use the following code to find the Chi-Square critical value that corresponds to a significance level of .05 with 13 degrees of freedom:
<b>qchisq(p=.95, df=13)
[1] 22.36203
</b>
The critical value turns out to be <b>22.36203</b>.
We can also confirm this is correct by using the  Chi-Square Critical Value Calculator .
<h2>rchisq</h2>
We often use the <b>rchisq() </b>function to generate a list of <em>n</em> random values that follow a Chi-Square distribution with a given degrees of freedom.
For example, we can use the following code to generate a list of 1,000 random values that follow a Chi-Square distribution with 5 degrees of freedom:
<b>#make this example reproducible
set.seed(0) 
#generate 1000 random values that follow Chi-Square dist with df=5
values &lt;- rchisq(n=1000, df=5)
#view first five values 
head(values)
[1]  8.369701  3.130487  1.985623  5.258747 10.578594  6.360859
</b>
We can also use the <b>hist(</b>) function to generate a histogram to visualize this distribution of values:
<b>#create histogram to visualize distribution of values
hist(values)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/chisq2.jpg"429">
The x-axis shows the data values and the y-axis shows the frequency of those values.
<h2><span class="orange">Chi-square Distribution Table</span></h2>
The chi-square distribution table below shows the critical values for different probability levels (P) and degrees of freedom (DF).
  
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in Stata</span></h2>
A <b> Chi-Square Goodness of Fit Test  </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test in Stata.
<h3>Example: Chi-Square Goodness of Fit Test in Stata</h3>
To illustrate how to perform this test, we will use a dataset called <em>nlsw88</em>, which contains information about labor statistics for women in the U.S. in 1988.
Use the following steps to perform a Chi-Square Goodness of Fit test to determine if the true distribution of race in this dataset is as follows: 70% White, 20% Black, 10% Other.
<b>Step 1: Load and view the raw data.</b>
First, we will load the data by typing in the following command:
<b>sysuse nlsw88</b>
We can view the raw data by typing in the following command:
<b>br</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/chiSquareGoodnessStata1.png">
Each line displays information for an individual including their age, race, marital status, education level, and a variety of other factors.
<b>Step 2: Load the goodness of fit package.</b>
To perform a Goodness of Fit Test, we will need to install the <em>csgof </em>package. We can do so by typing in the following command:
<b>findit csgof</b>
A new window will pop up. Click the link that says <em>csgof from https://stats.idre.ucla.edu/stat/stata/ado/analysis</em>.
Another window will pop up. Click the link that says <em>click here to install</em>. 
The package should only take a few seconds to install.
<b>Step 3: Perform the Goodness-of-Fit Test.</b>
Once the package is installed, we can perform the Goodness of Fit Test on the data to determine if the true distribution of race is as follows: 70% White, 20% Black, 10% Other.
We will use the following syntax to perform the test:
<b>csgof variable_of_interest, expperc(list_of_expected_percentages)</b>
Here is the exact syntax we’ll use in our case:
<b>csgof race, expperc(70, 20, 10)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/chiSquareGoodnessStata2.png">
Here is how to interpret the output:
<b>Summary box: </b>This box shows us the expected percent, expected frequency, and observed frequency for each race. For example:
The expected percent of white individuals was 70%. This is the percentage that we specified. 
The expected frequency of white individuals was 1,572.2. This is calculated using the fact that there were 2,246 individuals in the dataset, so 70% of that number is 1,572.2.
The observed frequency of white individuals was 1,637. This is the actual number of white individuals in the dataset.
<b>Chisq(2): </b>This is the Chi-Square test statistic for the Goodness of Fit Test. It turns out to be 218.13.
<b>p: </b>This is the p-value associated with the Chi-Square test statistic. It turns out to be 0. Since this is less than 0.05, we fail to reject the null hypothesis that the true distribution of race is 70% White, 20% Black, 10% Other. We have sufficient evidence to conclude that the true distribution of race is different from this hypothesized distribution.
<h2><span class="orange">Chi-Square Goodness of Fit Test Calculator</span></h2>
</style>
A  Chi-Square Goodness of Fit Test  is used to determine whether or not a categorical variable follows a hypothesized distribution.
To perform a Chi-Square Goodness of Fit Test, simply enter a list of observed and expected values for up to 10 categories in the boxes below, then click the “Calculate” button:
<table><tbody>
<tr>
<th><b>Category</b></th>
<th><b><span>Observed</b></th>
<th><b><span>Expected</b></th>
</tr>
<tr>
<td>Category 1</td>
<td><input type="text" id="o1" value="50"></td>
<td><input type="text" id="e1" value="50"></td>
</tr>
<tr>
<td>Category 2</td>
<td><input type="text" id="o2" value="60"></td>
<td><input type="text" id="e2" value="50"></td>
</tr>
<tr>
<td>Category 3</td>
<td><input type="text" id="o3" value="40"></td>
<td><input type="text" id="e3" value="50"></td>
</tr>
<tr>
<td>Category 4</td>
<td><input type="text" id="o4"></td>
<td><input type="text" id="e4"></td>
</tr>
<tr>
<td>Category 5</td>
<td><input type="text" id="o5"></td>
<td><input type="text" id="e5"></td>
</tr>
<tr>
<td>Category 6</td>
<td><input type="text" id="o6"></td>
<td><input type="text" id="e6"></td>
</tr>
<tr>
<td>Category 7</td>
<td><input type="text" id="o7"></td>
<td><input type="text" id="e7"></td>
</tr>
<tr>
<td>Category 8</td>
<td><input type="text" id="o8"></td>
<td><input type="text" id="e8"></td>
</tr>
</tbody></table>
<input type="button" id="button" onclick="calc()" value="Calculate">
X<sup>2</sup> Test Statistic: <b>4.360000</b>
p-value: <b>0.359472</b>
<script>
function calc() {
//get input data
var o1 = document.getElementById('o1').value;
var o2 = document.getElementById('o2').value;
var o3 = document.getElementById('o3').value;
var o4 = document.getElementById('o4').value;
var o5 = document.getElementById('o5').value;
var o6 = document.getElementById('o6').value;
var o7 = document.getElementById('o7').value;
var o8 = document.getElementById('o8').value;
var e1 = document.getElementById('e1').value;
var e2 = document.getElementById('e2').value;
var e3 = document.getElementById('e3').value;
var e4 = document.getElementById('e4').value;
var e5 = document.getElementById('e5').value;
var e6 = document.getElementById('e6').value;
var e7 = document.getElementById('e7').value;
var e8 = document.getElementById('e8').value;
var obs = [o1, o2, o3, o4, o5, o6, o7, o8];
var empties = obs.length - obs.filter(String).length;
var df = 7 - empties;
//do calculations
var diff1 = 0;
if (o1) {
o1 = +o1;
e1 = +e1;
diff1 = Math.pow(o1-e1,2) / e1;
}
var diff2 = 0;
if (o2) {
o2 = +o2;
e2 = +e2;
diff2 = Math.pow(o2-e2,2) / e2;
}
var diff3 = 0;
if (o3) {
o3 = +o3;
e3 = +e3;
diff3 = Math.pow(o3-e3,2) / e3;
}
var diff4 = 0;
if (o4) {
o4 = +o4;
e4 = +e4;
diff4 = Math.pow(o4-e4,2) / e4;
}
var diff5 = 0;
if (o5) {
o5 = +o5;
e5 = +e5;
diff5 = Math.pow(o5-e5,2) / e5;
}
var diff6 = 0;
if (o6) {
o6 = +o6;
e6 = +e6;
diff6 = Math.pow(o6-e6,2) / e6;
}
var diff7 = 0;
if (o7) {
o7 = +o7;
e7 = +e7;
diff7 = Math.pow(o7-e7,2) / e7;
}
var diff8 = 0;
if (o8) {
o8 = +o8;
e8 = +e8;
diff8 = Math.pow(o8-e8,2) / e8;
}
var errors = [diff1, diff2, diff3, diff4, diff5, diff6, diff7, diff8];
var X2 = math.sum(errors);
var p = 1-jStat.chisquare.cdf(X2, df);
//output results
document.getElementById('X2').innerHTML = X2.toFixed(6);
document.getElementById('p').innerHTML = p.toFixed(6);
  
} //end calc function
</script>
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in Excel</span></h2>
A <b> Chi-Square Goodness of Fit Test  </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test in Excel.
<h3>Example: Chi-Square Goodness of Fit Test in Excel</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
We will use the following steps to perform a Chi-Square goodness of fit test to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Input the data.</b>
First, we will input the data values for the expected number of customers each day in one column and the observed number of customers each day in another column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/GOFexcel1.png">
<em><b>Note: </b>There were 250 customers total. Thus, if the shop owner expects an equal number to come into the shop each day then he would expect 50 customers per day.</em>
<b>Step 2: Find the difference between the observed and expected values.</b>
The Chi-Square test statistic for the Goodness of Fit test is<b> X<sup>2</sup> = Σ(O-E)<sup>2</sup> / E</b>
where:
<b>Σ:</b> is a fancy symbol that means “sum”
<b>O: </b>observed value
<b>E: </b>expected value
The following formula shows how to calculate <b>(O-E)<sup>2</sup> / E</b> for each row:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/GOFexcel2.png">
<b>Step 3: Calculate the Chi-Square test statistic and the corresponding p-value.</b>
Lastly, we will calculate the Chi-Square test statistic along with the corresponding p-value using the following formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/GOFexcel3.png">
<b>Note: </b>The Excel function <b>CHISQ.DIST.RT(x, deg_freedom) </b>returns the right-tailed probability of the Chi-Square distribution associated with a test statistic <b>x </b>and a certain degrees of freedom. The degrees of freedom is calculated as n-1. In this case, deg_freedom = 5 – 1 = 4.
<b>Step 4: Interpret the results.</b>
The X<sup>2</sup> test statistic for the test is <b>4.36 </b>and the corresponding p-value is <b>0.3595</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in R</span></h2>
A <b> Chi-Square Goodness of Fit Test  </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test in R.
<h3>Example: Chi-Square Goodness of Fit Test in R</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, a researcher records the number of customers that come into the shop in a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
Use the following steps to perform a Chi-Square goodness of fit test in R to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Create the data.</b>
First, we will create two arrays to hold our observed frequencies and our expected proportion of customers for each day:
<b>observed &lt;- c(50, 60, 40, 47, 53) 
expected &lt;- c(.2, .2, .2, .2, .2) #must add up to 1
</b>
<b>Step 2: Perform the Chi-Square Goodness of Fit Test.</b>
Next, we can perform the Chi-Square Goodness of Fit Test using the <b>chisq.test()</b> function, which uses the following syntax:
<b>chisq.test(x, p) </b>
where:
<b>x: </b>A numerical vector of observed frequencies.
<b>p: </b>A numerical vector of expected proportions.
The following code shows how to use this function in our example:
<b>#perform Chi-Square Goodness of Fit Test
chisq.test(x=observed, p=expected)
Chi-squared test for given probabilities
data:  observed
X-squared = 4.36, df = 4, p-value = 0.3595</b>
The Chi-Square test statistic is found to be <b>4.36 </b>and the corresponding p-value is <b>0.3595</b>.
Note that the p-value corresponds to a Chi-Square value with n-1 degrees of freedom (dof), where n is the number of different categories. In this case, dof = 5-1 = 4.
You can use the  Chi-Square to P Value Calculator  to confirm that the p-value that corresponds to X<sup>2</sup> = 4.36 with dof = 4 is <b>0.35947</b>.
Recall that a Chi-Square Goodness of Fit Test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>A variable follows a hypothesized distribution.
<b>H<sub>1</sub>: (alternative hypothesis) </b>A variable does not follow a hypothesized distribution.
Since the p-value (.35947) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in SAS</span></h2>
A  Chi-Square goodness of fit test  is used to determine whether or not a  categorical variable  follows a hypothesized distribution.
The following example explains how to perform a Chi-Square Goodness of Fit Test in SAS.
<h3>Example: Chi-Square Goodness of Fit Test in SAS</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, a researcher records the number of customers that come into the shop in a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
Use the following steps to perform a Chi-Square goodness of fit test in SAS to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Create the dataset.</b>
First, we’ll create a dataset and name it <b>my_data</b>:
<b>/*create dataset*/
data my_data;
input Day $ Customers;
datalines;
Mon 50
Tue 60
Wed 40
Thur 47
Fri 53
;
run;
/*print dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/good1.jpg"202">
<b>Step 2: Perform the Chi-Square Goodness of Fit Test.</b>
Next, we’ll use the following code to perform a Chi-Square Goodness of Fit test:
<b>/*perform Chi-Square Goodness of Fit test*/
proc freq data=my_data;
tables Day / chisq;
weight Customers;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/good2.jpg"365">
From the output we can see:
The Chi-Square test statistic: <b>4.36</b>
The corresponding p-value: <b>0.3595</b>
Recall that a Chi-Square Goodness of Fit Test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>: </b>A variable follows a hypothesized distribution.
<b>H<sub>A</sub>: </b>A variable does not follow a hypothesized distribution.
Since the  p-value  (.3595) is not less than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in Python</span></h2>
A <b> Chi-Square Goodness of Fit Test  </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test in Python.
<h3>Example: Chi-Square Goodness of Fit Test in Python</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, a researcher records the number of customers that come into the shop in a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
Use the following steps to perform a Chi-Square goodness of fit test in Python to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Create the data.</b>
First, we will create two arrays to hold our observed and expected number of customers for each day:
<b>expected = [50, 50, 50, 50, 50]
observed = [50, 60, 40, 47, 53]
</b>
<b>Step 2: Perform the Chi-Square Goodness of Fit Test.</b>
Next, we can perform the Chi-Square Goodness of Fit Test using the  chisquare function  from the SciPy library, which uses the following syntax:
<b>chisquare(f_obs, f_exp) </b>
where:
<b>f_obs: </b>An array of observed counts.
<b>f_exp: </b>An array of expected counts. By default, each category is assumed to be equally likely.
The following code shows how to use this function in our specific example:
<b>import scipy.stats as stats
#perform Chi-Square Goodness of Fit Test
stats.chisquare(f_obs=observed, f_exp=expected)
(statistic=4.36, pvalue=0.35947)
</b>
The Chi-Square test statistic is found to be <b>4.36 </b>and the corresponding p-value is <b>0.35947</b>.
Note that the p-value corresponds to a Chi-Square value with n-1 degrees of freedom (dof), where n is the number of different categories. In this case, dof = 5-1 = 4. You can use the  Chi-Square to P Value Calculator  to confirm that the p-value that corresponds to X<sup>2</sup> = 4.36 with dof = 4 is <b>0.35947</b>.
Recall that a Chi-Square Goodness of Fit Test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>A variable follows a hypothesized distribution.
<b>H<sub>1</sub>: (alternative hypothesis) </b>A variable does not follow a hypothesized distribution.
Since the p-value (.35947) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">How to Perform a Chi-Square Goodness of Fit Test in SPSS</span></h2>
A <b> Chi-Square Goodness of Fit Test  </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test in SPSS.
<h3>Example: Chi-Square Goodness of Fit Test in SPSS</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, a researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
Use the following steps to perform a Chi-Square goodness of fit test in SPSS to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Input the data.</b>
First, enter the data into SPSS in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss1.png">
<b>Step 2: Use weighted cases.</b>
In order for the test to work correctly, we need to tell SPSS that the variable “Day” should be weighted by the variable “Count.”
Click the <b>Data </b>tab, then <b>Weight Cases</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss2.png">
In the new window that pops up, drag the variable <b>Count </b>into the box labelled Test Variable List. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss4.png">
<b>Step 3: Perform the Chi-Square Goodness of Fit Test.</b>
Click on the <b>Analyze </b>tab, then <b>Nonparametric Tests</b>, then <b>Legacy Dialogs</b>, then <b>Chi-Square</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss3.png">
In the new window that pops up, drag the variable <b>Count </b>into the box labelled Test Variable List.
Leave the label checked next to <b>All categories equal </b>since each of our categories (i.e. days of the week) have the same expected number of visitors each day. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss4.png">
<b>Step 4: Interpret the results</b>.
Once you click <b>OK</b>, the results of the Chi-Square Goodness of Fit Test will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/GFspss5.png">
The first table shows the observed and expected number of customers each day of the week, along with the residual (i.e. the difference) between observed and expected.
The second table displays the following numbers:
<b>Chi-Square: </b>The Chi-Square test statistic, found to be 4.36.
<b>df:</b> The degrees of freedom, calculated as #categories-1 = 5-1 = 4.
<b>Asymp. Sig: </b>The p-value that corresponds to a Chi-Square value of 4.36 with 4 degrees of freedom, found to be .359. This value can also be found by using the  Chi-Square Score to P Value Calculator .
Since the p-value (.359) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">Chi-Square Goodness of Fit Test on a TI-84 Calculator</span></h2>
A <b>Chi-Square Goodness of Fit Test </b>is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains how to perform a Chi-Square Goodness of Fit Test on a TI-84 calculator.
<h3>Example: Chi-Square Goodness of Fit Test on a TI-84 Calculator</h3>
A shop owner claims that an equal number of customers come into his shop during each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
We will use the following steps to perform a Chi-Square goodness of fit test to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Input the data.</b>
First, we will input the data values for the expected number of customers each day and the observed number of customers each day. Press Stat  and then press EDIT . Enter the following values for the observed number of customers in column L1 and the values for the expected number of customers in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiGoodnessTI3.png">
<em><b>Note: </b>There were 250 customers total. Thus, if the shop owner expects an equal number to come into the shop each day then that would be 50 customers per day.</em>
<b>Step 2: Perform the Chi-Square goodness of fit test.</b>
Next, we will perform the Chi-Square goodness of fit test. Press Stat and then scroll over to <b>TESTS</b>. Then scroll down to <b>X<sup>2</sup>GOF-Test</b> and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiGoodnessTI2.png">
For <b>Observed</b>, choose list L1. For <b>Expected</b>, choose list L2. For <b>df</b> (degrees of freedom), enter # categories – 1. In our case, we have 5-1 = 4. Then highlight <b>Calculate </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiGoodnessTI4.png">
The following output will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiGoodnessTI5.png">
<b>Step 3: Interpret the results.</b>
The X<sup>2</sup> test statistic for the test is <b>4.36 </b>and the corresponding p-value is <b>0.3595</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">Chi-Square Goodness of Fit Test: Definition, Formula, and Example</span></h2>
A <b>Chi-Square goodness of fit test</b> is used to determine whether or not a categorical variable follows a hypothesized distribution.
This tutorial explains the following:
The motivation for performing a Chi-Square goodness of fit test.
The formula to perform a Chi-Square goodness of fit test.
An example of how to perform a Chi-Square goodness of fit test.
<h3>Chi-Square Goodness of Fit Test: Motivation</h3>
A Chi-Square goodness of fit test can be used in a wide variety of settings. Here are a few examples:
We want to know if a die is fair, so we roll it 50 times and record the number of times it lands on each number.
We want to know if an equal number of people come into a shop each day of the week, so we count the number of people who come in each day during a random week.
We want to know if the percentage of M&M’s that come in a bag are as follows: 20% yellow, 30% blue, 30% red, 20% other. To test this, we open a random bag of M&M’s and count how many of each color appear.
In each of these scenarios, we want to know if some variable follows a hypothesized distribution. In each scenario, we can use a Chi-Square goodness of fit test to determine if there is a statistically significant difference in the number of expected counts for each level of a variable compared to the observed counts.
<h3>Chi-Square Goodness of Fit Test: Formula</h3>
A Chi-Square goodness of fit test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>A variable follows a hypothesized distribution.
<b>H<sub>1</sub>: (alternative hypothesis) </b>A variable does not follow a hypothesized distribution.
We use the following formula to calculate the Chi-Square test statistic X<sup>2</sup>:
<b>X<sup>2</sup> = Σ(O-E)<sup>2</sup> / E</b>
where:
<b>Σ:</b> is a fancy symbol that means “sum”
<b>O: </b>observed value
<b>E: </b>expected value
If the p-value that corresponds to the test statistic X<sup>2</sup> with n-1 degrees of freedom (where n is the number of categories) is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h3>Chi-Square Goodness of Fit Test: Example</h3>
A shop owner claims that an equal number of customers come into his shop each weekday. To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
We will use the following steps to perform a Chi-Square goodness of fit test to determine if the data is consistent with the shop owner’s claim.
<b>Step 1: Define the hypotheses.</b>
We will perform the Chi-Square goodness of fit test using the following hypotheses:
<b>H<sub>0</sub>: </b>An equal number of customers come into the shop each day.
<b>H<sub>1</sub>: </b>An equal number of customers do not come into the shop each day.
<b>Step 2: Calculate (O-E)<sup>2</sup> / E for each day.</b>
There were a total of 250 customers that came into the shop during the week. Thus, if we expected an equal amount to come in each day then the expected value “E” for each day would be 50.
<b>Monday: </b>(50-50)<sup>2</sup> / 50 = 0
<b>Tuesday: </b>(60-50)<sup>2</sup> / 50 = 2
<b>Wednesday: </b>(40-50)<sup>2</sup> / 50 = 2
<b>Thursday: </b>(47-50)<sup>2</sup> / 50 = 0.18
<b>Friday: </b>(53-50)<sup>2</sup> / 50 = 0.18
<b>Step 3: Calculate the test statistic X<sup>2</sup>.</b>
<b>X<sup>2 </sup></b>= Σ(O-E)<sup>2</sup> / E = 0 + 2 + 2 + 0.18 + 0.18 = <b>4.36</b>
<b>Step 4: Calculate the p-value of the test statistic X<sup>2</sup>.</b>
According to the  Chi-Square Score to P Value Calculator , the p-value associated with X<sup>2</sup> = 4.36 and n-1 = 5-1 = 4 degrees of freedom is <b>0.359472</b>.
<b>Step 5: Draw a conclusion.</b>
Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<em><b>Note: </b>You can also perform this entire test by simply using the  Chi-Square Goodness of Fit Test Calculator .</em>
<h2><span class="orange">Chi-Square Score to P Value Calculator</span></h2>
This calculator finds the p-value associated with a given Chi-Square score and a specified degrees of freedom.
<label><b>X<sup>2</sup></b> (Chi-Square Score)</label>
<input type="number" id="x2" value="4.36">
<label><b>df</b> (Degrees of freedom)</label>
<input type="number" id="df" value="4">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
P-value: 0.359472
<script>
function calc() {
//get input values
var x2 = +document.getElementById('x2').value;
var df = +document.getElementById('df').value;
//calculate SE
var p = 1-jStat.chisquare.cdf(x2, df);
//output probabilities
document.getElementById('p').innerHTML = p.toFixed(6);
}
</script>
<h2><span class="orange">The Four Assumptions of a Chi-Square Test</span></h2>
A  Chi-Square test of independence  is used to determine whether or not there is a significant association between two categorical variables.
This test makes four assumptions:
<b>Assumption 1: Both variables are categorical.</b>
It’s assumed that both variables are categorical. That is, both variables take on values that are names or labels.
Examples of categorical variables include:
Marital status (“married”, “single”, “divorced”)
Political preference (“republican”, “democrat”, “independent”)
Smoking status (“smoker”, “non-smoker”)
<b>Assumption 2: All observations are independent.</b>
It’s assumed that every observation in the dataset is independent. That is, the value of one observation in the dataset does not affect the value of any other observation.
<b>Assumption 3: Cells in the contingency table are mutually exclusive.</b>
It’s assumed that individuals can only belong to one cell in the contingency table. That is, cells in the table are mutually exclusive – an individual cannot belong to more than one cell.
<b>Assumption 4: Expected value of cells should be 5 or greater in at least 80% of cells.</b>
It’s assumed that the expected value of cells in the contingency table should be 5 or greater in at least 80% of cells and that no cell should have an expected value less than 1.
The following example shows how to check each of these four assumptions in practice.
<h3>Example: Checking the Assumptions of a Chi-Square Test</h3>
Suppose we want to know whether or not gender is associated with political party preference.
We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
Before performing a Chi-Square test of independence, let’s verify that the four assumptions of the test are met.
<b>Assumption 1: Both variables are categorical.</b>
This assumption is easy to verify. We can see that the two variables in the contingency table are both categorical:
<b>Gender</b>: This variable can only take on two categories – Male or Female.
<b>Political Party Preference</b>: This variable can take on three categories – Republican, Democrat, or Independent.
<b>Assumption 2: All observations are independent.</b>
The only way to check this assumption is to verify that each individual included in this dataset was surveyed independently of every other individual.
If we used a  random sampling method  (like simple random sampling) then this assumption is likely met.
<b>Assumption 3: Cells in the contingency table are mutually exclusive.</b>
We can verify that this assumption is met by checking that no individual has been counted in more than one cell.
Assuming each individual in the dataset was only surveyed once, this assumption should be met because it’s not possible for an individual to be, say, a Male Republican <em>and</em> a Female Democrat simultaneously.
<b>Assumption 4: Expected value of cells should be 5 or greater in at least 80% of cells.</b>
We can use the following formula to calculate the expected values for each cell in the contingency table:
Expected value = (row sum * column sum) / table sum.
For example, the expected value for Male Republicans is: (230*250) / 500 = <b>115</b>.
We can repeat this formula to obtain the expected value for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
We can see that no cell in the table has an expected value less than 5, so this assumption is met.
Once we’ve verified that the four assumptions are met, we can then use  this calculator  to perform a Chi-Square Test of Independence:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chi_assumptions.png">
The p-value of the test is <b>0.649198</b>. Since this p-value is not less than .05, we do not have sufficient evidence to say that there is an association between gender and political party preference.
<h2><span class="orange">How to Perform a Chi-Square Test by Hand (Step-by-Step)</span></h2>
A <b>Chi-Square goodness of fit test</b> is used to determine whether or not a  categorical variable  follows a hypothesized distribution.
The following step-by-step example shows how to perform a Chi-Square goodness of fit test by hand.
<h2>Chi-Square Goodness of Fit Test By Hand</h2>
Suppose we believe that a certain dice is fair. In other words, we believe the dice is equally likely to land on a 1, 2, 3, 4, 5, or 6 on a given roll.
To test this, we we roll it 60 times and record the number that it lands on each time. The results are as follows:
<b>1</b>: 8 times
<b>2</b>: 12 times
<b>3</b>: 18 times
<b>4</b>: 9 times
<b>5</b>: 7 times
<b>6</b>: 6 times
Use the following steps to perform a Chi-Square goodness of fit test to determine if the dice is fair.
<h3>Step 1: Define the Null and Alternative Hypotheses</h3>
<b>H<sub>0</sub></b> (null): The dice is equally likely to land on each number.
<b>H<sub>1</sub></b> (alternative) : The dice is not equally likely to land on each number.
<h3>Step 2: Calculate the Observed and Expected Frequencies</h3>
Next, let’s create a table of observed and expected frequencies for each number on the dice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/chi_hand1.png">
<b>Note</b>: If we believe the dice is fair, this means we expect it to land on each number an equal amount of times – in this case, 10 times each. 
<h3>Step 3: Calculate the Test Statistic</h3>
The Chi-Square test statistic, X<sup>2</sup>, is calculated as:
X<sup>2</sup> = Σ(O-E)<sup>2</sup> / E
The following table shows how to calculate this test statistic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/chi_hand2.png">
In this case, X<sup>2</sup> turns out to be <b>9.8</b>.
<h3>Step 4: Find the Critical Value</h3>
Next, we need to find the critical value in the  Chi-Square distribution table  that corresponds to α = <b>.05</b> and df = (#categories – 1).
In this case, there are 6 categories, so we will use df = 6 – 1 = <b>5</b>.
We can see that the critical value is <b>11.07</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/chi_hand3.png">
<h3>Step 5: Reject or Fail to Reject the Null Hypothesis</h3>
Since our test statistic is less than the critical value, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the dice is unfair.
<h2><span class="orange">Chi-Square Goodness of Fit Test in Google Sheets (Step-by-Step)</span></h2>
A  Chi-Square Goodness of Fit Test  is used to determine whether or not a categorical variable follows a hypothesized distribution.
For example, suppose a shop owner claims that an equal number of customers come into his shop each weekday.
To test this hypothesis, an independent researcher records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
We can perform a Chi-Square Goodness of Fit Test to determine if the data is consistent with the shop owner’s claim.
This following step-by-step example shows how to perform a Chi-Square Goodness of Fit Test in Google Sheets.
<h3>Step 1: Create the Data</h3>
First, let’s input the data into Google Sheets in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/chiSheets1.png">
<b>Note: </b>There were 250 customers total. If the shop owner expects an equal number to come into the shop each day then he would expect 50 customers per day.
<h3>Step 2: Calculate the Difference Between Observed and Expected Values</h3>
The Chi-Square test statistic for the Goodness of Fit test is<b> X<sup>2</sup> = Σ(O-E)<sup>2</sup> / E</b>
where:
<b>Σ:</b> is a fancy symbol that means “sum”
<b>O: </b>observed value
<b>E: </b>expected value
The following formula shows how to calculate <b>(O-E)<sup>2</sup> / E</b> for each row:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/chiSheets2.png">
<h3>Step 3: Calculate the P-Value</h3>
Lastly, we will calculate the Chi-Square test statistic along with the corresponding p-value using the following formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/chiSheets3.png">
<b>Note: </b>The function <b>CHISQ.DIST.RT(x, deg_freedom) </b>returns the right-tailed probability of the Chi-Square distribution associated with a test statistic <b>x </b>and a certain degrees of freedom. The degrees of freedom is calculated as n-1. In this case, deg_freedom = 5 – 1 = 4.
The X<sup>2</sup> test statistic for the test is <b>4.36 </b>and the corresponding p-value is <b>0.3595</b>.
Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h2><span class="orange">Chi-Square Test of Independence on a TI-84 Calculator</span></h2>
A <b> Chi-Square Test of Independence  </b>is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains how to perform a Chi-Square Test of Independence on a TI-84 Calculator.
<h3>Example: Chi-Square Test of Independence on a TI-84 Calculator</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td></td>
<td><b>Republican</b></td>
<td><b>Democrat</b></td>
<td><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td>120</td>
<td>90</td>
<td>40</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td>110</td>
<td>95</td>
<td>45</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td>230</td>
<td>185</td>
<td>85</td>
<td>500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square test of independence to determine if gender is associated with political party preference.
<b>Step 1: Input the data.</b>
First, we will input the data into a matrix. Press 2nd  and then press  x<sup>-1</sup> . Scroll over to <b>Edit</b>, highlight any matrix that is blank and press Enter. Then, choose the number of rows (2 in our case) and columns (3 in our case) to use in the matrix and enter the raw data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepTI1.png">
<b>Step 2: Perform a Chi-Square Test of Independence.</b>
Next, we will perform a Chi-Square test of independence on the matrix we just created. Press stat and scroll over to <b>TESTS</b>. Then scroll down to <b>X<sup>2</sup>-Test</b> and Press Enter. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepTI2.png">
For <b>Observed</b>, choose the matrix you entered the data in. In our case, we used matrix A. For <b>Expected</b>, this can be any empty matrix (the calculator will automatically produce the expected values for us). In our case, we’ll leave this as matrix B.
Then, highlight <b>Calculate </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepTI4.png">
The following output will automatically display:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepTI3.png">
<b>Step 3: Interpret the results.</b>
The X<sup>2</sup> test statistic is <b>0.8640 </b>and the corresponding p-value is <b>0.6492</b>. Since this p-value is not less than .05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to state that there is an association between gender and political party preference.
<h2><span class="orange">Chi-Square Test of Independence Calculator</span></h2>
svg:not(:root) {
  overflow: visible;
}
td input {
  max-width:60px;
  max-height:30px;
}
</style>
A <b>Chi-Square Test of Independence</b> is used to determine whether or not there is a significant association between two categorical variables.
To perform a Chi-Square Test of Independence, simply fill in the cells below for a contingency table of up to 5 rows and 5 columns. If your table is smaller than 5×5, simply leave the other cells blank.
<table><tbody>
<tr style="max-height:10px">
<th style="min-width:120px"></th>
<th><b><span>Group 1</b></th>
<th><b><span>Group 2</b></th>
<th><b><span>Group 3</b></th>
<th><b><span>Group 4</b></th>
<th><b><span>Group 5</b></th>
</tr>
<tr>
<td>Category 1</td>
<td><input type="text" id="o11" value="120"></td>
<td><input type="text" id="o12" value="90"></td>
<td><input type="text" id="o13" value="40"></td>
<td><input type="text" id="o14"></td>
<td><input type="text" id="o15"></td>
</tr>
<tr>
<td>Category 2</td>
<td><input type="text" id="o21" value="110"></td>
<td><input type="text" id="o22" value="95"></td>
<td><input type="text" id="o23" value="45"></td>
<td><input type="text" id="o24"></td>
<td><input type="text" id="o25"></td>
</tr>
<tr>
<td>Category 3</td>
<td><input type="text" id="o31"></td>
<td><input type="text" id="o32"></td>
<td><input type="text" id="o33"></td>
<td><input type="text" id="o34"></td>
<td><input type="text" id="o35"></td>
</tr>
<tr>
<td>Category 4</td>
<td><input type="text" id="o41"></td>
<td><input type="text" id="o42"></td>
<td><input type="text" id="o43"></td>
<td><input type="text" id="o44"></td>
<td><input type="text" id="o45"></td>
</tr>
<tr>
<td>Category 5</td>
<td><input type="text" id="o51"></td>
<td><input type="text" id="o52"></td>
<td><input type="text" id="o53"></td>
<td><input type="text" id="o54"></td>
<td><input type="text" id="o55"></td>
</tr>
</tbody></table>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
X<sup>2</sup> Test Statistic: <b>0.864035</b>
p-value: <b>0.649198</b>
<script>
function calc() {
//get input data
var o11 = document.getElementById('o11').value;
var o12 = document.getElementById('o12').value;
var o13 = document.getElementById('o13').value;
var o14 = document.getElementById('o14').value;
var o15 = document.getElementById('o15').value;
var o21 = document.getElementById('o21').value;
var o22 = document.getElementById('o22').value;
var o23 = document.getElementById('o23').value;
var o24 = document.getElementById('o24').value;
var o25 = document.getElementById('o25').value;
var o31 = document.getElementById('o31').value;
var o32 = document.getElementById('o32').value;
var o33 = document.getElementById('o33').value;
var o34 = document.getElementById('o34').value;
var o35 = document.getElementById('o35').value;
var o41 = document.getElementById('o41').value;
var o42 = document.getElementById('o42').value;
var o43 = document.getElementById('o43').value;
var o44 = document.getElementById('o44').value;
var o45 = document.getElementById('o45').value;
var o51 = document.getElementById('o51').value;
var o52 = document.getElementById('o52').value;
var o53 = document.getElementById('o53').value;
var o54 = document.getElementById('o54').value;
var o55 = document.getElementById('o55').value;
var cols = [o11, o12, o13, o14, o15];
var colCount = cols.filter(String).length;
var rows = [o11, o21, o31, o41, o51];
var rowCount = rows.filter(String).length;
var df = (colCount-1)*(rowCount-1);
var col1 = [o11, o21, o31, o41, o51];
var col2 = [o12, o22, o32, o42, o52];
var col3 = [o13, o23, o33, o43, o53];
var col4 = [o14, o24, o34, o44, o54];
var col5 = [o15, o25, o35, o45, o55];
var row1 = [o11, o12, o13, o14, o15];
var row2 = [o21, o22, o23, o24, o25];
var row3 = [o31, o32, o33, o34, o35];
var row4 = [o41, o42, o43, o44, o45];
var row5 = [o51, o52, o53, o54, o55];
var col1sum = math.sum(col1);
var col2sum = math.sum(col2);
var col3sum = math.sum(col3);
var col4sum = math.sum(col4);
var col5sum = math.sum(col5);
var row1sum = math.sum(row1);
var row2sum = math.sum(row2);
var row3sum = math.sum(row3);
var row4sum = math.sum(row4);
var row5sum = math.sum(row5);
var n = math.sum(row1sum, row2sum, row3sum, row4sum, row5sum);
//do calculations
var diff11 = 0;
if (o11) {
o11 = +o11;
var e11 = row1sum*col1sum/n;
diff11 = Math.pow(o11-e11,2) / e11;
}
var diff12 = 0;
if (o12) {
o12 = +o12;
var e12 = row1sum*col2sum/n;
diff12 = Math.pow(o12-e12,2) / e12;
}
var diff13 = 0;
if (o13) {
o13 = +o13;
var e13 = row1sum*col3sum/n;
diff13 = Math.pow(o13-e13,2) / e13;
}
var diff14 = 0;
if (o14) {
o14 = +o14;
var e14 = row1sum*col4sum/n;
diff14 = Math.pow(o14-e14,2) / e14;
}
var diff15 = 0;
if (o15) {
o15 = +o15;
var e15 = row1sum*col5sum/n;
diff15 = Math.pow(o15-e15,2) / e15;
}
var diff21 = 0;
if (o21) {
o21 = +o21;
var e21 = row2sum*col1sum/n;
diff21 = Math.pow(o21-e21,2) / e21;
}
var diff22 = 0;
if (o22) {
o22 = +o22;
var e22 = row2sum*col2sum/n;
diff22 = Math.pow(o22-e22,2) / e22;
}
var diff23 = 0;
if (o23) {
o23 = +o23;
var e23 = row2sum*col3sum/n;
diff23 = Math.pow(o23-e23,2) / e23;
}
var diff24 = 0;
if (o24) {
o24 = +o24;
var e24 = row2sum*col4sum/n;
diff24 = Math.pow(o24-e24,2) / e24;
}
var diff25 = 0;
if (o25) {
o25 = +o25;
var e25 = row2sum*col5sum/n;
diff25 = Math.pow(o25-e25,2) / e25;
}
var diff31 = 0;
if (o31) {
o31 = +o31;
var e31 = row3sum*col1sum/n;
diff31 = Math.pow(o31-e31,2) / e31;
}
var diff32 = 0;
if (o32) {
o32 = +o32;
var e32 = row3sum*col2sum/n;
diff32 = Math.pow(o32-e32,2) / e32;
}
var diff33 = 0;
if (o33) {
o33 = +o33;
var e33 = row3sum*col3sum/n;
diff33 = Math.pow(o33-e33,2) / e33;
}
var diff34 = 0;
if (o34) {
o34 = +o34;
var e34 = row3sum*col4sum/n;
diff34 = Math.pow(o34-e34,2) / e34;
}
var diff35 = 0;
if (o35) {
o35 = +o35;
var e35 = row3sum*col5sum/n;
diff35 = Math.pow(o35-e35,2) / e35;
}
var diff41 = 0;
if (o41) {
o41 = +o41;
var e41 = row4sum*col1sum/n;
diff41 = Math.pow(o41-e41,2) / e41;
}
var diff42 = 0;
if (o42) {
o42 = +o42;
var e42 = row4sum*col2sum/n;
diff42 = Math.pow(o42-e42,2) / e42;
}
var diff43 = 0;
if (o43) {
o43 = +o43;
var e43 = row4sum*col3sum/n;
diff43 = Math.pow(o43-e43,2) / e43;
}
var diff44 = 0;
if (o44) {
o44 = +o44;
var e44 = row4sum*col4sum/n;
diff44 = Math.pow(o44-e44,2) / e44;
}
var diff45 = 0;
if (o45) {
o45 = +o45;
var e45 = row4sum*col5sum/n;
diff45 = Math.pow(o45-e45,2) / e45;
}
var diff51 = 0;
if (o51) {
o51 = +o51;
var e51 = row5sum*col1sum/n;
diff51 = Math.pow(o51-e51,2) / e51;
}
var diff52 = 0;
if (o52) {
o52 = +o52;
var e52 = row5sum*col2sum/n;
diff52 = Math.pow(o52-e52,2) / e52;
}
var diff53 = 0;
if (o53) {
o53 = +o53;
var e53 = row5sum*col3sum/n;
diff53 = Math.pow(o53-e53,2) / e53;
}
var diff54 = 0;
if (o54) {
o54 = +o54;
var e54 = row5sum*col4sum/n;
diff54 = Math.pow(o54-e54,2) / e54;
}
var diff55 = 0;
if (o55) {
o55 = +o55;
var e55 = row5sum*col5sum/n;
diff55 = Math.pow(o55-e55,2) / e55;
}
console.log(o31, o32, o33, e31, e32, e33, diff31, diff32, diff33);
var errors = [diff11, diff12, diff13, diff14, diff15, diff21, diff22, diff23, diff24, diff25, diff31, diff32, diff33, diff34, diff35, diff41, diff42, diff43, diff44, diff45, diff51, diff52, diff53, diff54, diff55];
var X2 = math.sum(errors);
var p = 1-jStat.chisquare.cdf(X2, df);
//output results
document.getElementById('X2').innerHTML = X2.toFixed(6);
document.getElementById('p').innerHTML = p.toFixed(6);
  
} //end calc function
</script>
<h2><span class="orange">How to Perform a Chi-Square Test of Independence in Excel</span></h2>
A  <b>Chi-Square Test of Independence</b>  is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains how to perform a Chi-Square Test of Independence in Excel.
<h3>Example: Chi-Square Test of Independence in Excel</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepExcel1.png">
Use the following steps to perform a Chi-Square test of independence to determine if gender is associated with political party preference.
<b>Step 1: Define the hypotheses.</b>
We will perform the Chi-Square test of independence using the following hypotheses:
<b>H<sub>0</sub>: </b>Gender and political party preference are independent.
<b>H<sub>1</sub>:</b> Gender and political party preference are <em>not</em> independent.
<b>Step 2: Calculate the expected values.</b>
Next, we will calculate the expected values for each cell in the contingency table using the following formula:
Expected value = (row sum * column sum) / table sum.
For example, the expected value for Male Republicans is: (230*250) / 500 = <b>115</b>.
We can repeat this formula to obtain the expected value for each cell in the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepExcel2.png">
<b>Step 3: Calculate (O-E)<sup>2</sup> / E for each cell in the table.</b>
Next we will calculate <b>(O-E)<sup>2</sup> / E </b>for each cell in the table where:
<b>O: </b>observed value
<b>E: </b>expected value
For example, Male Republicans would have a value of: (120-115)<sup>2</sup> /115 = <b>0.2174</b>.
We can repeat this formula for each cell in the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepExcel3.png">
<b>Step 4: Calculate the test statistic X<sup>2</sup> and the corresponding p-value.</b>
The test statistic X<sup>2</sup> is simply the sum of the values in the last table.
The p-value that corresponds to the test statistic X<sup>2</sup> can be found by using the formula :
<b>=CHISQ.DIST.RT(x, deg_freedom)</b>
where:
<b>x: </b>test statistic X<sup>2</sup>
<b>deg_freedom:</b> degrees of freedom, calculated as (#rows-1) * (#columns-1)
The test statistic X<sup>2</sup> turns out to be <b>0.8640 </b>and the corresponding p-value is <b>0.649198</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/chiIndepExcel4.png">
<b>Step 5: Draw a conclusion.</b>
Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
<em><b>Note: </b>You can also perform this entire test by using the  Chi-Square Test of Independence Calculator .</em>
<h2><span class="orange">Chi-Square Test of Independence in R (With Examples)</span></h2>
A  Chi-Square Test of Independence  is used to determine whether or not there is a significant association between two  categorical variables .
This tutorial explains how to perform a Chi-Square Test of Independence in R.
<h3>Example: Chi-Square Test of Independence in R</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td style="text-align: center;"><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square Test of Independence in R to determine if gender is associated with political party preference.
<b>Step 1: Create the data.</b>
First, we will  create a table  to hold our data:
<b>#create table
data &lt;- matrix(c(120, 90, 40, 110, 95, 45), ncol=3, byrow=TRUE)
colnames(data) &lt;- c("Rep","Dem","Ind")
rownames(data) &lt;- c("Male","Female")
data &lt;- as.table(data)
#view table
data
       Rep Dem Ind
Male   120  90  40
Female 110  95  45
</b>
<b>Step 2: Perform the Chi-Square Test of Independence.</b>
Next, we can perform the Chi-Square Test of Independence using the <b>chisq.test()</b> function:
<b>#Perform Chi-Square Test of Independence
chisq.test(data)
Pearson's Chi-squared test
data:  data
X-squared = 0.86404, df = 2, p-value = 0.6492
</b>
The way to interpret the output is as follows:
Chi-Square Test Statistic: <b>0.86404</b>
Degrees of freedom: <b>2 </b>(calculated as #rows-1 * #columns-1)
p-value: <b>0.6492</b>
Recall that the  Chi-Square Test of Independence  uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>The two variables are independent.
<b>H<sub>1</sub>: (alternative hypothesis) </b>The two variables are <em>not</em> independent.
Since the p-value (0.6492) of the test is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
In other words, gender and political party preference are independent.
<h2><span class="orange">How to Perform a Chi-Square Test of Independence in SAS</span></h2>
A  Chi-Square Test of Independence  is used to determine whether or not there is a significant association between two  categorical variables .
The following example shows how to perform a Chi-Square Test of Independence in SAS.
<h3>Example: Chi-Square Test of Independence in SAS</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a  simple random sample  of 500 voters and survey them on their political party preference.
The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td style="text-align: center;"><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square Test of Independence in SAS to determine if gender is associated with political party preference.
<b>Step 1: Create the data.</b>
First, we will create a dataset in SAS to hold the survey responses:
<b>/*create dataset*/
data my_data;
input Gender $ Party $ Count;
datalines;
Male Rep 120
Male Dem 90
Male Ind 40
Female Rep 110
Female Dem 95
Female Ind 45
;
run;
/*print dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/ind1.jpg"215">
<b>Step 2: Perform the Chi-Square Test of Independence.</b>
Next, we can use the following code to perform the Chi-Square Test of Independence:
<b>/*perform Chi-Square Test of Independence*/
proc freq data=my_data;
tables Gender*Party / chisq;
weight Count;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/ind2.jpg">
There are two values of interest in the output:
Chi-Square Test Statistic: <b>0.8640</b>
Corresponding p-value: <b>0.6492</b>
Recall that the Chi-Square Test of Independence uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>The two variables are independent.
<b>H<sub>A</sub>: </b>The two variables are <em>not</em> independent.
Since the  p-value  (0.6492) of the test is not less than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
In other words, gender and political party preference are independent.
<h2><span class="orange">How to Perform a Chi-Square Test of Independence in Python</span></h2>
A <b> Chi-Square Test of Independence  </b>is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains how to perform a Chi-Square Test of Independence in Python.
<h3>Example: Chi-Square Test of Independence in Python</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td></td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td style="text-align: center;"><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square Test of Independence in Python to determine if gender is associated with political party preference.
<b>Step 1: Create the data.</b>
First, we will create a table to hold our data:
<b>data = [[120, 90, 40],
        [110, 95, 45]]</b>
<b>Step 2: Perform the Chi-Square Test of Independence.</b>
Next, we can perform the Chi-Square Test of Independence using the  chi2_contingency function  from the SciPy library, which uses the following syntax:
<b>chi2_contingency(observed) </b>
where:
<b>observed: </b>A contingency table of observed values.
The following code shows how to use this function in our specific example:
<b>import scipy.stats as stats
#perform the Chi-Square Test of Independence
stats.chi2_contingency(data)
(0.864,
 0.649,
 2,
 array([[115. ,  92.5,  42.5],
        [115. ,  92.5,  42.5]]))
</b>
The way to interpret the output is as follows:
Chi-Square Test Statistic: <b>0.864</b>
p-value: <b>0.649</b>
Degrees of freedom: <b>2 </b>(calculated as #rows-1 * #columns-1)
Array: The last array displays the expected values for each cell in the contingency table.
Recall that the  Chi-Square Test of Independence  uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>The two variables are independent.
<b>H<sub>1</sub>: (alternative hypothesis) </b>The two variables are <em>not</em> independent.
Since the p-value (.649) of the test is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
In other words, gender and political party preference are independent.
<h2><span class="orange">How to Perform a Chi-Square Test of Independence in SPSS</span></h2>
A  Chi-Square Test of Independence  is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains how to perform a Chi-Square Test of Independence in SPSS.
<h2>Example: Chi-Square Test of Independence in SPSS</h2>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td style="text-align: center;"><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td style="text-align: center;">500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square Test of Independence in SPSS to determine if gender is associated with political party preference.
<b>Step 1: Enter the data.</b>
First, enter the data in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS1.png">
<b>Step 2: Use weighted cases.</b>
In order for the test to work correctly, we need to tell SPSS that the variables Party and Gender should be weighted by the variable Count.
Click the <b>Data </b>tab, then <b>Weight Cases</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS2.png">
In the new window that pops up, drag the variable <b>Count </b>into the box labelled Test Variable List. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS6.png">
<b>Step 3: Perform the Chi-Square Goodness of Fit Test.</b>
Click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Crosstabs</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS3.png">
In the new window that pops up, drag the variable <b>Gender </b>into the box labelled Rows and the variable <b>Party </b>into the box labelled Columns. Then click <b>Statistics </b>and make sure the box next to <b>Chi-square </b>is checked. Click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS4.png">
<b>Step 4: Interpret the results</b>.
Once you click <b>OK</b>, the results of the Chi-Square Test of Independence will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/indepSPSS5.png">
The first table displays the number of missing cases in the dataset. We can see that there are 0 missing cases in this example.
The second table displays a crosstab of the total number of individuals by gender and political party preference.
The third table shows the results of the Chi-Square Test of Independence. The test statistic is <b>.864 </b>and the corresponding two-sided p-value is <b>.649</b>.
The null hypothesis for the Chi-Square Test of Independence is that the two variables are independent. In this case, our null hypothesis is that gender and political party preference are independent.
Since the p-value (.649) of the test is not less than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
<h2><span class="orange">How to Perform a Chi-Square Test of Independence in Stata</span></h2>
A <b> Chi-Square Test of Independence  </b>is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains how to perform a Chi-Square Test of Independence in Stata.
<h3>Example: Chi-Square Test of Independence in Stata</h3>
For this example we will use a dataset called <em>auto</em>, which contains information about 74 different automobiles from 1978.
Use the following steps to perform a Chi-Square Test of Independence to determine if there is a significant association between the following two variables:
<b>rep78:</b> the number of times the car received a repair in 1978 (ranges from  1 to 5)
<b>foreign: </b>whether or not the car type is foreign (0 = no, 1 = yes)
<b>Step 1: Load and view the raw data.</b>
First, we will load the data by typing in the following command:
<b>sysuse auto</b>
We can view the raw data by typing in the following command:
<b>br</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/chiSquareIndependenceStata1.png">
Each line displays information for an individual car including price, mpg, weight, length, and a variety of other variables. The only two variables that we care about are <em>rep78 </em>and <em>foreign</em>.
<b>Step 3: Perform the Chi-Square Test of Independence.</b>
We will use the following syntax to perform the test:
<b>tab first_variable second_variable, chi2</b>
Here is the exact syntax we’ll use in our case:
<b>tab rep78 foreign, chi2</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/chiSquareIndependenceStata2.png">
Here is how to interpret the output:
<b>Summary table: </b>This table shows the total counts for each combination of <em>rep78 </em>and <em>foreign</em>. For example
There were 2 cars that were domestic and received 1 repair in 1978.
There were 8 cars that were domestic and received 2 repairs in 1978.
There were 27 cars that were domestic and received 3 repairs in 1978.
And so on.
<b>Pearson chisq(4): </b>This is the Chi-Square test statistic for the test. It turns out to be 27.2640.
<b>Pr: </b>This is the p-value associated with the Chi-Square test statistic. It turns out to be 0.000. Since this is less than 0.05, we fail to reject the null hypothesis that the two variables are independent. We have sufficient evidence to conclude that there is a statistically significant association between whether or not a car was foreign and the total number of repairs it received.
<h2><span class="orange">Chi-Square Test of Independence: Definition, Formula, and Example</span></h2>
A <b>Chi-Square Test of Independence</b> is used to determine whether or not there is a significant association between two categorical variables.
This tutorial explains the following:
The motivation for performing a Chi-Square Test of Independence.
The formula to perform a Chi-Square Test of Independence.
An example of how to perform a Chi-Square Test of Independence.
<h3>Chi-Square Test of Independence: Motivation</h3>
A Chi-Square test of independence can be used to determine if there is an association between two categorical variables in a many different settings. Here are a few examples:
We want to know if gender is associated with political party preference so we survey 500 voters and record their gender and political party preference.
We want to know if a person’s favorite color is associated with their favorite sport so we survey 100 people and ask them about their preferences for both.
We want to know if education level and marital status are associated so we collect data about these two variables on a simple random sample of 50 people.
In each of these scenarios we want to know if two categorical variables are associated with each other. In each scenario, we can use a Chi-Square test of independence to determine if there is a statistically significant association between the variables. 
<h3>Chi-Square Test of Independence: Formula</h3>
A Chi-Square test of independence uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: (null hypothesis) </b>The two variables are independent.
<b>H<sub>1</sub>: (alternative hypothesis) </b>The two variables are <em>not</em> independent. (i.e. they are associated)
We use the following formula to calculate the Chi-Square test statistic X<sup>2</sup>:
<b>X<sup>2</sup> = Σ(O-E)<sup>2</sup> / E</b>
where:
<b>Σ:</b> is a fancy symbol that means “sum”
<b>O: </b>observed value
<b>E: </b>expected value
If the p-value that corresponds to the test statistic X<sup>2</sup> with (#rows-1)*(#columns-1) degrees of freedom is less than your chosen significance level then you can reject the null hypothesis.
<h3>Chi-Square Test of Independence: Example</h3>
Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
Use the following steps to perform a Chi-Square test of independence to determine if gender is associated with political party preference.
<b>Step 1: Define the hypotheses.</b>
We will perform the Chi-Square test of independence using the following hypotheses:
<b>H<sub>0</sub>: </b>Gender and political party preference are independent.
<b>H<sub>1</sub>:</b> Gender and political party preference are <em>not</em> independent.
<b>Step 2: Calculate the expected values.</b>
Next, we will calculate the expected values for each cell in the contingency table using the following formula:
Expected value = (row sum * column sum) / table sum.
For example, the expected value for Male Republicans is: (230*250) / 500 = <b>115</b>.
We can repeat this formula to obtain the expected value for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">115</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">42.5</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
<b>Step 3: Calculate (O-E)<sup>2</sup> / E for each cell in the table.</b>
Next we will calculate <b>(O-E)<sup>2</sup> / E </b>for each cell in the table where:
<b>O: </b>observed value
<b>E: </b>expected value
For example, Male Republicans would have a value of: (120-115)<sup>2</sup> /115 = <b>0.2174</b>.
We can repeat this formula for each cell in the table:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">0.2174</td>
<td style="text-align: center;">0.0676</td>
<td style="text-align: center;">0.1471</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">0.2174</td>
<td style="text-align: center;">0.0676</td>
<td style="text-align: center;">0.1471</td>
</tr>
</tbody></table>
<b>Step 4: Calculate the test statistic X<sup>2</sup> and the corresponding p-value.</b>
<b>X<sup>2 </sup></b>= Σ(O-E)<sup>2</sup> / E = 0.2174 + 0.2174 + 0.0676 + 0.0676 + 0.1471 + 0.1471 = <b>0.8642</b>
According to the  Chi-Square Score to P Value Calculator , the p-value associated with X<sup>2</sup> = 0.8642 and (2-1)*(3-1) = 2 degrees of freedom is <b>0.649198</b>.
<b>Step 5: Draw a conclusion.</b>
Since this p-value is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that there is an association between gender and political party preference.
<em><b>Note: </b>You can also perform this entire test by simply using the  Chi-Square Test of Independence Calculator .</em>
<h2><span class="orange">4 Examples of Using Chi-Square Tests in Real Life</span></h2>
In statistics, there are two different types of <b>Chi-Square tests:</b>
<b>1.</b>  The Chi-Square Goodness of Fit Test  – Used to determine whether or not a categorical variable follows a hypothesized distribution.
<b>2.</b>  The Chi-Square Test of Independence  – Used to determine whether or not there is a significant association between two categorical variables.
In this article, we share several examples of how each of these types of Chi-Square tests are used in real-life situations.
<h3>Example 1: Chi-Square Goodness of Fit Test</h3>
Suppose a shop owner claims that an equal number of customers come into his shop each weekday.
To test this hypothesis, he records the number of customers that come into the shop on a given week and finds the following:
<b>Monday: </b>50 customers
<b>Tuesday: </b>60 customers
<b>Wednesday: </b>40 customers
<b>Thursday: </b>47 customers
<b>Friday: </b>53 customers
He can use a <b>Chi-Square Goodness of Fit Test</b> to determine if the distribution of the customers that come in each day is consistent with his hypothesized distribution.
Using the  Chi-Square Goodness of Fit Test Calculator , he can find that the p-value of the test is <b>0.359</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal1.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal2.png">
Since this p-value is not less than .05, there is not sufficient evidence to say that the true distribution of customers is different from the distribution that the shop owner claimed.
<h3>Example 2: Chi-Square Goodness of Fit Test</h3>
Suppose a biologist claims that an equal number of four different species of deer enter a certain wooded area in a forest each week.
To test this hypothesis, she records the number of each species of deer that enter the wooded area over the course of one week:
<b>Species #1: </b>22 
<b>Species #2: </b>20
<b>Species #3: </b>23
<b>Species #4: </b>35
She can use a <b>Chi-Square Goodness of Fit Test</b> to determine if the distribution of the deer species that enter the wooded area in the forest each week is consistent with his hypothesized distribution.
Using the  Chi-Square Goodness of Fit Test Calculator , she can find that the p-value of the test is <b>0.137</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal3.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal4.png">
Since this p-value is not less than .05, there is not sufficient evidence to say that the true distribution of deer is different from the distribution that the biologist claimed.
<h3>Example 3: Chi-Square Test of Independence</h3>
Suppose a policy maker in a certain town wants to know whether or not gender is associated with political party preference.
He decides to take a simple random sample of 500 voters and survey them on their political party preference. The following table shows the results of the survey:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>Republican</b></td>
<td style="text-align: center;"><b>Democrat</b></td>
<td style="text-align: center;"><b>Independent</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Male</b></td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">90</td>
<td style="text-align: center;">40</td>
<td>250</td>
</tr>
<tr>
<td><b>Female</b></td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">95</td>
<td style="text-align: center;">45</td>
<td>250</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">230</td>
<td style="text-align: center;">185</td>
<td style="text-align: center;">85</td>
<td>500</td>
</tr>
</tbody></table>
He can use a <b>Chi-Square Test of Independence</b> to determine if there is a statistically significant association between the two variables.
Using the  Chi-Square Test of Independence Calculator , he can find that the p-value of the test is <b>0.649</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal5.png">
Since the p-value is not less than .05, there is not sufficient evidence to say that there is an association between gender and political party preference.
<h3>Example 4: Chi-Square Test of Independence</h3>
Suppose a researcher wants to know whether or not marital status is associated with education level.
He decides to take a simple random sample of 300 individuals and obtains the following results:
<table><tbody>
<tr>
<td> </td>
<td style="text-align: center;"><b>High School</b></td>
<td style="text-align: center;"><b>Bachelor’s</b></td>
<td style="text-align: center;"><b>Master’s or Higher</b></td>
<td><b>Total</b></td>
</tr>
<tr>
<td><b>Married</b></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">35</td>
<td>155</td>
</tr>
<tr>
<td><b>Single</b></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">15</td>
<td>145</td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">70</td>
<td style="text-align: center;">180</td>
<td style="text-align: center;">50</td>
<td>300</td>
</tr>
</tbody></table>
He can use a <b>Chi-Square Test of Independence</b> to determine if there is a statistically significant association between the two variables.
Using the  Chi-Square Test of Independence Calculator , he can find that the p-value of the test is <b>0.000011</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/chiReal6.png">
Since the p-value is less than .05, there is sufficient evidence to say that there is an association between marital status and education level.
 Chi-Square Test of Independence 
 Chi-Square Goodness of Fit Test 
The following tutorials explain the difference between Chi-Square tests and other statistical tests:
 Chi-Square Test vs. T-Test 
 Chi-Square Test vs. ANOVA 
<h2><span class="orange">Chi-Square Test vs. t-Test: What’s the Difference?</span></h2>
<b>Chi-Square Tests</b> and <b>t-Tests </b>are two of the most common types of statistical tests. Thus, it’s important to understand the difference between these two tests and how to know when to use each one based on the problem you want to answer.
This tutorial provides a simple explanation of the difference between the two tests, along with when to use each one.
<h2>Chi-Square Test</h2>
There are actually a few different versions of the chi-square test, but the most common one is the  Chi-Square Test of Independence .
<h3>Definition</h3>
We use a<b> chi-square test for independence</b> when we want to formally test whether or not there is a statistically significant association between two categorical variables.
The hypotheses of the test are as follows:
<b>Null hypothesis</b> (H<sub>0</sub>): There is no significant association between the two variables.
<b>Alternative hypothesis:</b> (Ha): There <em>is</em> a significant association between the two variables.
<h3>Examples</h3>
Here are some examples of when we might use a chi-square test for independence:
<b>Example 1: </b>We want to know if there is a statistically significant association between gender (male, female) and political party preference (republican, democrat, independent). To test this, we might survey 100 random people and record their gender and political party preference. Then, we can conduct a chi-square test for independence to determine if there is a statistically significant association between gender and political party preference.
<b>Example 2: </b>We want to know if there is a statistically significant association between class level (freshman, sophomore, junior, senior) and favorite movie genre (thriller, drama, western). To test this, we might survey 100 random students from each grade level at a certain school and record their favorite movie genre. Then, we can conduct a chi-square test for independence to determine if there is a statistically significant association between class level and favorite movie genre.
<b>Example 3: </b>We want to know if there is a statistically significant association between a person’s favorite sport (basketball, baseball, football) and where they grew up (urban, rural). To test this, we might survey 100 random people and ask them what type of place they grew up in and what their favorite sport is. Then, we can conduct a chi-square test for independence to determine if there is a statistically significant association between a person’s favorite sport and where they grew up.
<h3>Assumptions</h3>
Before we can conduct a chi-square test for independence, we first need to make sure the following assumptions are met to ensure that our test will be valid:
<b>Random:</b> A random sample or random experiment should be used to collect the data for both samples.
<b>Categorical: </b>The variables we are studying should be categorical.
<b>Size:</b> The expected number of observations at each level of the variable should be at least 5.
If these assumptions are met, then we can then conduct the test. 
<h2>t-Test</h2>
There are also a few different versions of the t-test, but the most common one is the  t-test for a difference in means .
<h3>Definition</h3>
We use a <b>t-test for a difference in means </b>when we want to formally test whether or not there is a statistically significant difference between two population means.
The hypotheses of the test are as follows:
<b>Null hypothesis</b> (H<sub>0</sub>): The two population means are equal.
<b>Alternative hypothesis:</b> (Ha): The two population means are not equal.
<em>Note: It’s possible to test whether one population mean is greater or less than the other, but the most common null hypothesis is that both means are equal.</em>
<h3>Examples</h3>
Here are some examples of when we might use a t-test for a difference in means:
<b>Example 1: </b>We want to know if diet <em>A </em>or diet <em>B </em>leads to greater weight loss. We randomly assign 100 people to follow diet <em>A </em>for two months and another 100 people to follow diet <em>B </em>for two months. We can conduct a t-test for a difference in means to determine if there is a statistically significant difference in average weight loss between the two groups. 
<b>Example 2: </b>We want to know if two different study plans lead to different exam scores for students. We randomly assign 50 students to use one study plan and 50 students to use another study plan for one month leading up to an exam. We can conduct a t-test for a difference in means to determine if there is a statistically significant difference in average exam scores between the two study plans.
<b>Example 3: </b>We want to know if students from two different schools have the same average height. We measure the height of 100 random students from one school and 100 random students from another school. We can conduct a t-test for a difference in means to determine if there is a statistically significant difference in average height of students between the two schools.
<h3>Assumptions</h3>
Before we can conduct a hypothesis test for a difference between two population means, we first need to make sure the following conditions are met to ensure that our hypothesis test will be valid:
<b>Random:</b> A random sample or random experiment should be used to collect data for both samples.
<b>Normal:</b> The sampling distribution is normal or approximately normal.
<b>Independence:</b> The two samples are independent. 
If these assumptions are met, then we can then conduct the hypothesis test.
<h2>How to Know When to Use Each Test</h2>
Here is a brief summary of each test:
<b>Chi-Square Test for independence: </b>Allows you to test whether or not not there is a statistically significant association between two <em>categorical</em> variables. When you reject the null hypothesis of a chi-square test for independence, it means there is a significant association between the two variables.
<b>t-Test for a difference in means: </b>Allows you to test whether or not there is a statistically significant difference between two population means. When you reject the null hypothesis of a t-test for a difference in means, it means the two population means are not equal.
The easiest way to know whether or not to use a chi-square test vs. a t-test is to simply look at the types of variables you are working with.
If you have two variables that are both categorical, i.e. they can be placed in categories like <em>male</em>, <em>female</em> and <em>republican</em>, <em>democrat</em>, <em>independent</em>, then you should use a chi-square test.
But if one variable is  categorical  (e.g. type of study plan – either plan 1 or plan 2) and the other is continuous (e.g. exam score – measured from 0 to 100), then you should use a t-test.
<h2><span class="orange">Chi-Square Test vs. ANOVA: What’s the Difference?</span></h2>
<b>Chi-Square tests</b> and <b>ANOVA</b> (“Analysis of Variance”) are two commonly used statistical tests.
Thus, it’s important to understand the difference between these two tests and how to know when you should use each.
This tutorial provides a simple explanation of the difference between the two tests, along with when to use each one.
<h3>Explanation of Chi-Square Tests</h3>
In statistics, there are two different types of <b>Chi-Square tests:</b>
<b>1.</b>  The Chi-Square Goodness of Fit Test  – Used to determine whether or not a categorical variable follows a hypothesized distribution.
For example:
We want to know if a die is fair, so we roll it 50 times and record the number of times it lands on each number.
We want to know if an equal number of people come into a shop each day of the week, so we count the number of people who come in each day during a random week.
<b>2.</b>  The Chi-Square Test of Independence  – Used to determine whether or not there is a significant association between two categorical variables.
For example:
We want to know if gender is associated with political party preference so we survey 500 voters and record their gender and political party preference.
We want to know if a person’s favorite color is associated with their favorite sport so we survey 100 people and ask them about their preferences for both.
Note that both of these tests are only appropriate to use when you’re working with <b>categorical variables</b>. These are variables that take on names or labels and can fit into categories.
<h3>Explanation of ANOVA</h3>
In statistics, an <b>ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
For example:
We want to know if three different studying techniques lead to different mean exam scores.
We want to know if four different types of fertilizer lead to different mean crop yields.
Note that it’s appropriate to use an ANOVA when there is at least one categorical variable and one continuous dependent variable.
<h3>When to Use Chi-Square Tests vs. ANOVA</h3>
As a basic rule of thumb:
<b>Use Chi-Square Tests</b> when every variable you’re working with is categorical.
<b>Use ANOVA</b> when you have at least one categorical variable and one continuous dependent variable.
Use the following practice problems to improve your understanding of when to use Chi-Square Tests vs. ANOVA:
<b>Practice Problem 1</b>
Suppose a researcher want to know if education level and marital status are associated so she collects data about these two variables on a simple random sample of 50 people.
To test this, she should use a <b>Chi-Square Test of Independence</b> because she is working with two categorical variables – “education level” and “marital status.”
<b>Practice Problem 2</b>
Suppose an economist wants to determine if the proportion of residents who support a certain law differ between the three cities.
To test this, he should use a <b>Chi-Square Goodness of Fit Test</b> because he is only analyzing the distribution of one categorical variable.
<b>Practice Problem 3</b>
Suppose a basketball trainer wants to know if three different training techniques lead to different mean jump height among his players.
To test this, he should use a <b>one-way ANOVA</b> because he is analyzing one categorical variable (training technique) and one continuous dependent variable (jump height).
<b>Practice Problem 4: </b>
Suppose a botanist wants to know if two different amounts of sunlight exposure and three different watering frequencies lead to different mean plant growth.
To test this, she should use a <b>two-way ANOVA</b> because she is analyzing two categorical variables (sunlight exposure and watering frequency) and one continuous dependent variable (plant growth).
 Chi-Square Test of Independence 
 Chi-Square Goodness of Fit Test 
The following tutorials provide an introduction to the different types of ANOVA tests:
 One-Way ANOVA 
 Two-Way ANOVA 
 Repeated Measures ANOVA 
The following tutorials explain the difference between other statistical tests:
 One-Way vs. Two-Way ANOVA 
 Chi-Square Test vs. T-Test 
 F-Test vs. T-Test 
<h2><span class="orange">How to Perform a Chow Test in Python</span></h2>
A  Chow test  is used to test whether the coefficients in two different regression models on different datasets are equal.
This test is typically used in the field of econometrics with time series data to determine if there is a structural break in the data at some point.
The following a step-by-step example shows how to perform a Chow test in Python.
<h3>Step 1: Create the Data</h3>
First, we’ll create some fake data:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [1, 1, 2, 3, 4, 4, 5, 5, 6, 7, 7, 8, 8, 9, 10, 10,         11, 12, 12, 13, 14, 15, 15, 16, 17, 18, 18, 19, 20, 20],   'y': [3, 5, 6, 10, 13, 15, 17, 14, 20, 23, 25, 27, 30, 30, 31,         33, 32, 32, 30, 32, 34, 34, 37, 35, 34, 36, 34, 37, 38, 36]})
#view first five rows of DataFrame
df.head()
        xy
013
115
226
3310
4413</b>
<h3>Step 2: Visualize the Data</h3>
Next, we’ll create a simple  scatterplot  to visualize the data:
<b>import matplotlib.pyplot as plt
#create scatterplot
plt.plot(df.x, df.y, 'o')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/chow11.png">
From the scatterplot we can see that the pattern in the data appears to change at x = 10.
Thus, we can perform the Chow test to determine if there is a structural break point in the data at x = 10.
<h3>Step 3: Perform the Chow Test</h3>
We can use the chowtest function from the  chowtest  package in Python to perform a Chow test.
First, we need to install this package using pip:
<b>pip install chowtest
</b>
Next, we can use the following syntax to perform the Chow test:
<b>from chow_test import chowtest
chowtest(y=df[['y']], X=df[['x']],
         last_index_in_model_1=15,
         first_index_in_model_2=16,
         significance_level=.05)
***********************************************************************************
Reject the null hypothesis of equality of regression coefficients in the 2 periods.
***********************************************************************************
Chow Statistic: 118.14097335479373 p value: 0.0
***********************************************************************************
(118.14097335479373, 1.1102230246251565e-16)
</b>
Here’s what the individual arguments mean in the <b>chowtest()</b> function:
<b>y</b>: The response variable in the DataFrame
<b>x</b>: The predictor variable in the DataFrame
<b>last_index_in_model_1</b>: The index value for the last point before the structural break
<b>first_index_in_model_2</b>: The index value for the first point after the structural break
<b>significance_level</b>: The significance level to use for the hypothesis test
From the output of the test we can see:
<b>F test</b> <b>statistic</b>: 118.14
<b>p-value:</b> &lt;.0000
Since the p-value is less than .05, we can reject the null hypothesis of the test. This means we have sufficient evidence to say that a structural break point is present in the data.
In other words, two regression lines can fit the pattern in the data more effectively than a single regression line.
<h2><span class="orange">How to Perform a Chow Test in R</span></h2>
A  Chow test  is used to test whether the coefficients in two different regression models on different datasets are equal.
This test is typically used in the field of econometrics with time series data to determine if there is a structural break in the data at some point.
This tutorial provides a step-by-step example of how to perform a Chow test in R.
<h3>Step 1: Create the Data</h3>
First, we’ll create some fake data:
<b>#create data
data &lt;- data.frame(x = c(1, 1, 2, 3, 4, 4, 5, 5, 6, 7, 7, 8, 8, 9, 10, 10,         11, 12, 12, 13, 14, 15, 15, 16, 17, 18, 18, 19, 20, 20),   y = c(3, 5, 6, 10, 13, 15, 17, 14, 20, 23, 25, 27, 30, 30, 31,         33, 32, 32, 30, 32, 34, 34, 37, 35, 34, 36, 34, 37, 38, 36))
#view first six rows of data
head(data)
  x  y
1 1  3
2 1  5
3 2  6
4 3 10
5 4 13
6 4 15
</b>
<h3>Step 2: Visualize the Data</h3>
Next, we’ll create a simple  scatterplot  to visualize the data:
<b>#load ggplot2 visualization package
library(ggplot2)
#create scatterplot
ggplot(data, aes(x = x, y = y)) +
    geom_point(col='steelblue', size=3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/chowR1.png">
From the scatterplot we can see that the pattern in the data appears to change at x = 10. Thus, we can perform the Chow test to determine if there is a structural break point in the data at x = 10.
<h3>Step 3: Perform the Chow Test</h3>
We can use the  sctest  function from the <b>strucchange</b> package to perform a Chow test:
<b>#load strucchange package
library(strucchange)
#perform Chow test
sctest(data$y ~ data$x, type = "Chow", point = 10)
Chow test
data:  data$y ~ data$x
F = 110.14, p-value = 2.023e-13
</b>
From the output of the test we can see:
<b>F test</b> <b>statistic</b>: 110.14
<b>p-value:</b> &lt;.0000
Since the p-value is less than .05, we can reject the null hypothesis of the test. This means we have sufficient evidence to say that a structural break point is present in the data.
In other words, two regression lines can fit the pattern in the data more effectively than a single regression line.
<h2><span class="orange">What is a Chow Test? (Explanation & Example)</span></h2>
A <b>Chow test</b> is a statistical test developed by economist  Gregory Chow  that is used to test whether the coefficients in two different regression models on different datasets are equal.
The Chow test is typically used in the field of econometrics with time series data to determine if there is a structural break in the data at some point.
For example, consider the following scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/chow1.png">
If we used one regression line to summarize the pattern in the data, it may look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/chow2.png">
And if we used two separate regression lines to summarize the pattern in the data, it may look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/chow3.png">
The Chow test allows us to test for whether or not the regression coefficients of each regression line are equal.
If the test determines that the coefficients are not equal between the regression lines, this means there is significant evidence that a structural break exists in the data. In other words, the pattern in the data is significantly different before and after that structural break point.
<h3>When to use the Chow Test</h3>
The following examples illustrate situations where you may wish to perform a Chow test:
<b>1. </b>To determine if stock prices change at different rates before and after an election.
<b>2. </b>To determine if housing prices change before and after an interest rate change.
<b>3.</b> To determine if the average profit of public companies is different before and after a new tax law is passed.
In each situation, we could use a Chow test to determine if there is a structural break point in the data at a certain point in time.
<h3>Steps to Perform a Chow Test</h3>
We can use the following steps to perform a Chow test.
<b>Step 1: Define the null and alternative hypotheses.</b>
Suppose we fit the following regression model to our entire dataset:
y<sub>t</sub> = a + bx<sub>1t</sub> + cx<sub>t2</sub> + ε
Then suppose we split our data into two groups based on some structural break point and fit the following regression models to each group:
y<sub>t</sub> = a<sub>1</sub> + b<sub>1</sub>x<sub>1t</sub> + c<sub>1</sub>x<sub>t2</sub> + ε
y<sub>t</sub> = a<sub>2</sub> + b<sub>2</sub>x<sub>1t</sub> + c<sub>2</sub>x<sub>t2</sub> + ε
We would use the following null and alternative hypotheses for the Chow test:
<b>Null (H<sub>0</sub>):</b> a<sub>1</sub> = a<sub>2</sub>, b<sub>1</sub> = b<sub>2</sub>, and c<sub>1</sub> = c<sub>2</sub>
<b>Alternative (H<sub>A</sub>):</b> At least one of the comparisons in the Null is not equal.
If we reject the null hypothesis, we have sufficient evidence to say that there is a structural break point in the data and two regression lines can fit the data better than one.
If we fail to reject the null hypothesis, we do not have sufficient evidence to say that there is a structural break point in the data. In this case, we say that the regression lines can be “pooled” into a single regression line that represents the pattern in the data sufficiently well.
<b>Step 2: Calculate the test statistic.</b>
If we define the following terms:
<b>S<sub>T</sub>:</b> The sum of squared residuals from the total data
<b>S<sub>1</sub>, S<sub>2</sub></b>:The sum of squared residuals from each group
<b>N<sub>1</sub>, N<sub>2</sub>:</b> The number of observations in each group
<b>k:</b> The number of parameters
Then we can say that the Chow test statistic is:
Chow test statistic = [(S<sub>T </sub>– (S<sub>1</sub>+S<sub>2</sub>))/k]  /  [(S<sub>1</sub>+S<sub>2</sub>)/ (N<sub>1</sub>+N<sub>2</sub>-2k)]
This test statistic follows the F-distribution with <em>k</em> and and N<sub>1</sub>+N<sub>2</sub>-2k degrees of freedom. 
<b>Step 3: Reject or fail to reject the null hypothesis.</b>
If the p-value associated with this test statistic is less than a certain  significance level , we can reject the null hypothesis and conclude that there is a structural break point in the data.
Fortunately, most statistical software is capable of performing a Chow test so you will likely never have to perform the test by hand.
<h3>Example of Performing a Chow Test</h3>
Refer to  this tutorial  to see a step-by-step example of how to perform a Chow test for a given dataset in R.
<h3>Notes on the Chow Test</h3>
Here are a couple notes to keep in mind in regards to the Chow test:
<b>1.</b> The test assumes that the residuals of the regression models are independently and identically distributed from a  normal distribution  with unknown variance.
<b>2. </b>The Chow test should only be used when the structural break that you’d like to test for is at a <em>known</em> time. In other words, the test shouldn’t be used repeatedly to determine if any point in time can be considered a structural break.
<h2><span class="orange">How to Find Class Midpoints in a Frequency Distribution</span></h2>
A frequency distribution table is a table that displays the frequencies of different data classes.
For example, the following frequency distribution table shows the frequency for five different classes:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Class</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">1 – 10</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">11 – 20</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">21 – 30</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">31 – 40</td>
<td style="text-align: center;">5</td>
</tr>
<tr>
<td style="text-align: center;">41 – 50</td>
<td style="text-align: center;">4</td>
</tr>
</tbody></table>
You can find the midpoint of each class by adding the lower class limit and the upper class limit, then dividing by two:
<b>Class midpoint </b> = (lower class limit + upper class limit) / 2
The following table shows how to calculate the midpoint of each class:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Class</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Midpoint</b></th>
</tr>
<tr>
<td style="text-align: center;">1 – 10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">(1+10) / 2</td>
</tr>
<tr>
<td style="text-align: center;">11 – 20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">(11+20) / 2</td>
</tr>
<tr>
<td style="text-align: center;">21 – 30</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">(21+30) / 2</td>
</tr>
<tr>
<td style="text-align: center;">31 – 40</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">(31+40) / 2</td>
</tr>
<tr>
<td style="text-align: center;">41 – 50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">(41+50) / 2</td>
</tr>
</tbody></table>
Thus, we’re left with the following midpoints:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Class</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Midpoint</b></th>
</tr>
<tr>
<td style="text-align: center;">1 – 10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">5.5</td>
</tr>
<tr>
<td style="text-align: center;">11 – 20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">15.5</td>
</tr>
<tr>
<td style="text-align: center;">21 – 30</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">25.5</td>
</tr>
<tr>
<td style="text-align: center;">31 – 40</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">35.5</td>
</tr>
<tr>
<td style="text-align: center;">41 – 50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">45.5</td>
</tr>
</tbody></table>
<h2>When Are Class Midpoints Used?</h2>
Class midpoints are often used when you want to create a histogram to visualize the values in a frequency table.
A histogram lists the classes along the x-axis of a graph and uses bars to represent the frequency of each class along the y-axis. Each bar is centered at its <b>class midpoint</b>. 
The following histogram provides a visual representation of the data in the previous frequency table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/midpoint1.png">
Notice how each bar is centered at its class midpoint:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/midpoint2.png">
When you’re creating a histogram by hand, it’s especially useful to know the class midpoints so that you know where to place the bars.
However, most statistical softwares are able to automatically center the bars of a histogram around the class midpoints, so you typically won’t have to manually find these midpoints yourself.
<h2><span class="orange">Class Width Calculator</span></h2>
In a frequency distribution, <b>class width</b> refers to the difference between the upper and lower boundaries of any class or category. It is calculated as:
Class width = (max – min) / n
where:
<i>max</i> is the maximum value in a dataset
<i>min</i> is the minimum value in a dataset
<i>n</i> is the number of classes
To calculate class width, simply fill in the values below and then click the “Calculate” button.
<label for="min"><b>Minimum value</b></label>
<input type="number" id="min" value="4"><label for="max"><b>Maximum value</b></label>
<input type="number" id="max" value="36"><label for="n"><b>Number of classes (n)</b></label>
<input type="number" id="n" value="9">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
Class Width: <b>3.5556</b>
<b>Explanation:</b>
Class Width = (max – min) / n
Class Width = (36 – 4) / 9 = <b>3.5556</b>
<script>
function calc() {
//get input values
var min = document.getElementById('min').value*1;
var max = document.getElementById('max').value*1;
var n = document.getElementById('n').value*1;
var cWidth = (max-min) / n;
//output
document.getElementById('cWidth').innerHTML = cWidth.toFixed(4);
document.getElementById('min2').innerHTML = min;
document.getElementById('max2').innerHTML = max;
document.getElementById('n2').innerHTML = n;
document.getElementById('cWidth2').innerHTML = cWidth.toFixed(4);
}
</script>
<h2><span class="orange">How to Calculate Class Width in Excel</span></h2>
In a frequency distribution, <b>class width</b> refers to the difference between the upper and lower boundaries of any class or category.
For example, the following frequency distribution has a class width of <b>4</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel1.png">
<b>e.g. </b>– the class width for the first class is 5-1 = <b>4</b>. The class width for the second class is 10-6 = <b>4</b>, and so on.
And the following frequency distribution has a class width of <b>9:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel2.png">
<b>e.g. </b>– the class width for the first class is 10-1 = <b>9</b>. The class width for the second class is 20-11 = <b>9</b>, and so on.
If you have a raw dataset of values, you can calculate the class width by using the following formula:
<b>Class width</b> = (max – min) / n
where:
<b>max</b> is the maximum value in a dataset
<b>min</b> is the minimum value in a dataset
<b>n</b> is the number of classes you want to use
The following example illustrates how to calculate class width for a dataset in Excel.
<h3>Example: How to Calculate Class Width in Excel</h3>
Suppose we have the following dataset of 20 values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel3.png">
To calculate the class width for a frequency distribution of this dataset, we simply need to decide how many classes we want to use. Suppose we want to use n = 5. Then we can use the following formula to find the class width:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel4.png">
The class width is <b>4.6</b>. For convenience, we typically round up to the nearest integer so in this case we will use <b>5</b>.
Lastly, we can create a frequency table that uses a class width of 5:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel5-1.png">
Notice that the width for each class is 5 and the sum of the values in the “Frequency” column adds up to 20, which matches the total number of values in our dataset.
<h2><span class="orange">How to Calculate Class Width in Google Sheets</span></h2>
In a frequency distribution, <b>class width</b> refers to the difference between the upper and lower boundaries of any class or category.
For example, the following frequency distribution has a class width of <b>4</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel1.png">
For example:
The class width for the first class is 5 – 1 = <b>4</b>
The class width for the second class is 10 – 6 = <b>4</b>
The class width for the third class is 15 – 11 = <b>4</b>
And so on.
You can calculate the class width for any dataset by using the following formula:
<b>Class width</b> = (max – min) / n
where:
<b>max:</b> The maximum value in the dataset
<b>min:</b> The minimum value in the dataset
<b>n: </b>The number of classes you want to use
The following example illustrates how to calculate class width for a dataset in Google Sheets.
<h3>Example: How to Calculate Class Width in Google Sheets</h3>
Suppose we have the following dataset of 20 values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/classWidthSheets1.png">
Suppose we’d like to create a frequency distribution for this dataset using n = 5 classes.
We can use the following formula in Google Sheets to find the class width for this frequency distribution:
<b>=(MAX(A2:21)-MIN(A2:A21))/5</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/classWidthSheets2.png">
The class width turns out to be <b>4.6</b>.
As a rule of thumb, we always round up to the nearest integer so in this case we would use a class width of <b>5</b>.
Lastly, we can create a frequency table that uses a class width of 5:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/classWidthExcel5-1.png">
Notice the following about our frequency distribution:
The width for each class is 5.
The sum of the values in the “Frequency” column adds up to 20. This matches the total number of values in our dataset.
We can use this general approach to calculate the class width of a frequency distribution for any dataset in Google Sheets.
<h2><span class="orange">How to Fit Classification and Regression Trees in R</span></h2>
When the relationship between a set of predictor variables and a  response variable  is linear, methods like  multiple linear regression  can produce accurate predictive models.
However, when the relationship between a set of predictors and a response is more complex, then non-linear methods can often produce more accurate models.
One such method is  classification and regression trees  (CART), which use a set of predictor variable to build decision trees that predict the value of a response variable.
If the response variable is continuous then we can build regression trees and if the response variable is categorical then we can build classification trees. 
This tutorial explains how to build both regression and classification trees in R.
<h3>Example 1: Building a Regression Tree in R</h3>
For this example, we’ll use the <b>Hitters</b> dataset from the <b>ISLR</b> package, which contains various information about 263 professional baseball players.
We will use this dataset to build a regression tree that uses the predictor variables <em>home runs</em> and <em>years played</em> to predict the <em>Salary</em> of a given player.
Use the following steps to build this regression tree.
<b>Step 1: Load the necessary packages.</b>
First, we’ll load the necessary packages for this example:
<b>library(ISLR) #contains Hitters dataset
library(rpart) #for fitting decision trees
library(rpart.plot) #for plotting decision trees
</b>
<b>Step 2: Build the initial regression tree.</b>
First, we’ll build a large initial regression tree. We can ensure that the tree is large by using a small value for <b>cp</b>, which stands for “complexity parameter.”
This means we will perform new splits on the regression tree as long as the overall R-squared of the model increases by at least the value specified by cp. 
We’ll then use the <b>printcp()</b> function to print the results of the model:
<b>#build the initial tree
tree &lt;- rpart(Salary ~ Years + HmRun, data=Hitters, control=rpart.control(cp=.0001))
#view results
printcp(tree)
Variables actually used in tree construction:
[1] HmRun Years
Root node error: 53319113/263 = 202734
n=263 (59 observations deleted due to missingness)
           CP nsplit rel error  xerror    xstd
1  0.24674996      0   1.00000 1.00756 0.13890
2  0.10806932      1   0.75325 0.76438 0.12828
3  0.01865610      2   0.64518 0.70295 0.12769
4  0.01761100      3   0.62652 0.70339 0.12337
5  0.01747617      4   0.60891 0.70339 0.12337
6  0.01038188      5   0.59144 0.66629 0.11817
7  0.01038065      6   0.58106 0.65697 0.11687
8  0.00731045      8   0.56029 0.67177 0.11913
9  0.00714883      9   0.55298 0.67881 0.11960
10 0.00708618     10   0.54583 0.68034 0.11988
11 0.00516285     12   0.53166 0.68427 0.11997
12 0.00445345     13   0.52650 0.68994 0.11996
13 0.00406069     14   0.52205 0.68988 0.11940
14 0.00264728     15   0.51799 0.68874 0.11916
15 0.00196586     16   0.51534 0.68638 0.12043
16 0.00016686     17   0.51337 0.67577 0.11635
17 0.00010000     18   0.51321 0.67576 0.11615
n=263 (59 observations deleted due to missingness)
</b>
<b>Step 3: Prune the tree.</b>
Next, we’ll prune the regression tree to find the optimal value to use for cp (the complexity parameter) that leads to the lowest test error.
Note that the optimal value for cp is the one that leads to the lowest <b>xerror</b> in the previous output, which represents the error on the observations from the cross-validation data.
<b>#identify best cp value to use
best &lt;- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
#produce a pruned tree based on the best cp value
pruned_tree &lt;- prune(tree, cp=best)
#plot the pruned tree
prp(pruned_tree,
    faclen=0, #use full names for factor labels
    extra=1, #display number of obs. for each terminal node
    roundint=F, #don't round to integers in output
    digits=5) #display 5 decimal places in output
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree3.png">
We can see that the final pruned tree has six terminal nodes. Each terminal node shows the predicted salary of player’s in that node along with the number of observations from the original dataset that belong to that note.
For example, we can see that in the original dataset there were 90 players with less than 4.5 years of experience and their average salary was $225.83k.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree4.png">
<b>Step 4: Use the tree to make predictions.</b>
We can use the final pruned tree to predict a given player’s salary based on their years of experience and average home runs.
For example, a player who has 7 years of experience and 4 average home runs has a predicted salary of <b>$502.81k</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree5.png">
We can use the <b>predict()</b> function in R to confirm this:
<b>#define new player
new &lt;- data.frame(Years=7, HmRun=4)
#use pruned tree to predict salary of this player
predict(pruned_tree, newdata=new)
502.8079 </b>
<h3>Example 2: Building a Classification Tree in R</h3>
For this example, we’ll use the <b>ptitanic</b> dataset from the <b>rpart.plot</b> package, which contains various information about passengers aboard the Titanic.
We will use this dataset to build a classification tree that uses the predictor variables <em>class</em>, <em>sex</em>, and <em>age </em>to predict whether or not a given passenger survived.
Use the following steps to build this classification tree.
<b>Step 1: Load the necessary packages.</b>
First, we’ll load the necessary packages for this example:
<b>library(rpart) #for fitting decision trees
library(rpart.plot) #for plotting decision trees
</b>
<b>Step 2: Build the initial classification tree.</b>
First, we’ll build a large initial classification tree. We can ensure that the tree is large by using a small value for <b>cp</b>, which stands for “complexity parameter.”
This means we will perform new splits on the classification tree as long as the overall fit of the model increases by at least the value specified by cp. 
We’ll then use the <b>printcp()</b> function to print the results of the model:
<b>#build the initial tree
tree &lt;- rpart(survived~pclass+sex+age, data=ptitanic, control=rpart.control(cp=.0001))
#view results
printcp(tree)
Variables actually used in tree construction:
[1] age    pclass sex   
Root node error: 500/1309 = 0.38197
n= 1309 
      CP nsplit rel error xerror     xstd
1 0.4240      0     1.000  1.000 0.035158
2 0.0140      1     0.576  0.576 0.029976
3 0.0095      3     0.548  0.578 0.030013
4 0.0070      7     0.510  0.552 0.029517
5 0.0050      9     0.496  0.528 0.029035
6 0.0025     11     0.486  0.532 0.029117
7 0.0020     19     0.464  0.536 0.029198
8 0.0001     22     0.458  0.528 0.029035
</b>
<b>Step 3: Prune the tree.</b>
Next, we’ll prune the regression tree to find the optimal value to use for cp (the complexity parameter) that leads to the lowest test error.
Note that the optimal value for cp is the one that leads to the lowest <b>xerror</b> in the previous output, which represents the error on the observations from the cross-validation data.
<b>#identify best cp value to use
best &lt;- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]
#produce a pruned tree based on the best cp value
pruned_tree &lt;- prune(tree, cp=best)
#plot the pruned tree
prp(pruned_tree,
    faclen=0, #use full names for factor labels
    extra=1, #display number of obs. for each terminal node
    roundint=F, #don't round to integers in output
    digits=5) #display 5 decimal places in output
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree6.png">
We can see that the final pruned tree has 10 terminal nodes. Each terminal node shows the number of passengers that died along with the number that survived.
For example, in the far left node we see that 664 passengers died and 136 survived.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree7.png">
<b>Step 4: Use the tree to make predictions.</b>
We can use the final pruned tree to predict the probability that a given passenger will survive based on their class, age, and sex.
For example, a male passenger who is in 1st class and is 8 years old has a survival probability of 11/29 = 37.9%.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree8.png">
You can find the complete R code used in these examples  here .
<h2><span class="orange">An Introduction to Classification and Regression Trees</span></h2>
When the relationship between a set of predictor variables and a  response variable  is linear, methods like  multiple linear regression  can produce accurate predictive models.
However, when the relationship between a set of predictors and a response is highly non-linear and complex then non-linear methods can perform better.
One such example of a non-linear method is <b>classification and regression trees</b>, often abbreviated <b>CART</b>. 
As the name implies, CART models use a set of predictor variables to build <em>decision trees</em> that predict the value of a response variable.
For example, suppose we have a dataset that contains the predictor variables <em>Years played </em>and <em>average home runs</em> along with the response variable <em>Yearly Salary</em> for hundreds of professional baseball players. 
Here’s what a regression tree might look like for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree1.png">
The way to interpret the tree is as follows:
Players with less than 4.5 years played have a predicted salary of $225.8k.
Players with greater than or equal to 4.5 years played and less than 16.5 average home runs have a predicted salary of $577.6k.
Players with greater than or equal to 4.5 years played and greater than or equal to 16.5 average home runs have a predicted salary of $975.6k.
The results of this model should intuitively make sense: Players with more years of experience and more average home runs tend to earn higher salaries.
We can then use this model to predict the salary of a new player.
For example, suppose a given player has played 8 years and averages 10 home runs per year. According to our model, we would predict that this player has an annual salary of $577.6k.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree2.png">
A couple notes about the tree:
The first predictor variable at the top of the tree is the most important, i.e. the most influential in predicting the value of the response variable. In this case, <em>years played</em> is able to predict salary better than <em>average home runs</em>.
The regions at the bottom of the tree are known as <em>terminal nodes</em>. This particular tree has three terminal nodes.
<h3>Steps to Build CART Models</h3>
We can use the following steps to build a CART model for a given dataset:
<b>Step 1: Use recursive binary splitting to grow a large tree on the training data.</b>
First, we use a <em>greedy</em> algorithm known as recursive binary splitting to grow a regression tree using the following method:
Consider all predictor variables X<sub>1</sub>, X<sub>2</sub>, … , X<sub>p</sub> and all possible values of the cut points for each of the predictors, then choose the predictor and the cut point such that the resulting tree has the lowest RSS (residual standard error).
<em>For classification trees, we choose the predictor and cut point such that the resulting tree has the lowest misclassification rate.</em>
Repeat this process, stopping only when each terminal node has less than some minimum number of observations.
This algorithm is <em>greedy</em> because at each step of the tree-building process it determines the best split to make based only on that step, rather than looking ahead and picking a split that will lead to a better overall tree in some future step.
<b>Step 2: Apply cost complexity pruning to the large tree to obtain a sequence of best trees, as a function of α.</b>
Once we’ve grown the large tree, we then need to <em>prune</em> the tree using a method known as cost complexity pruning, which works as follows:
For each possible tree with T terminal nodes, find the tree that minimizes RSS + α|T|.
Note that as we increase the value of α, trees with more terminal nodes are penalized. This ensures that the tree doesn’t become too complex.
This process results in a sequence of best trees for each value of α.
<b>Step 3: Use k-fold cross-validation to choose α. </b>
Once we’ve found the best tree for each value of α, we can apply  k-fold cross-validation  to choose the value of α that minimizes the test error.
<b>Step 4: Choose the final model.</b>
Lastly, we choose the final model to be the one that corresponds to the chosen value of α.
<h3>Pros & Cons of CART Models</h3>
CART models offer the following <b>pros</b>:
They are easy to interpret.
They are easy to explain.
They are easy to visualize.
They can be applied to both  regression and classification  problems.
However, CART models come with the following <b>con:</b>
They tend to not have as much predictive accuracy as other non-linear machine learning algorithms. However, by aggregating many decision trees with methods like bagging, boosting, and random forests, their predictive accuracy can be improved.
<b>Related: </b> How to Fit Classification and Regression Trees in R 
<h2><span class="orange">How to Clear the Environment in R (3 Methods)</span></h2>
There are three methods you can use to quickly clear the environment in R:
<b>Method 1: Clear Environment Using rm()</b>
<b>rm(list=ls())</b>
<b>Method 2: Clear Environment Using the Broom Icon</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/broom1.png">
<b>Method 3: Clear Specific Types of Objects Using lm() and class</b>
<b>#clear all data frames from environment
rm(list=ls(all=TRUE)[sapply(mget(ls(all=TRUE)), class) == "data.frame"])
#clear all lists from environment
rm(list=ls(all=TRUE)[sapply(mget(ls(all=TRUE)), class) == "list"])
</b>
The following examples shows how to use each of these methods in practice.
<h3>Method 1: Clear Environment Using rm()</h3>
Suppose we have an R environment with two data frames, two lists, two matrices, and two vectors:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir1.png">
We can use the following code to remove all objects from the envinroment:
<b>rm(list=ls())</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir2.png">
Notice that every object in the R environment is now cleared.
<h3>Method 2: Clear Environment Using the Broom Icon</h3>
Once again suppose we have an R environment with the following objects:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir1.png">
We can click the broom icon to clear the entire environment:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir3.png">
Once we click <b>Yes</b>, the environment will be cleared:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir4.png">
<h3>Method 3: Clear Specific Types of Objects</h3>
Occasionally we may only want to clear specific types of objects from the environment in R.
For example, suppose we have an R environment with the following objects:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir1.png">
We can use the following code to clear only the data frames from the environment:
<b>#clear all data frames from environment
rm(list=ls(all=TRUE)[sapply(mget(ls(all=TRUE)), class) == "data.frame"])</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/envir5.png">
Notice that all of the data frames have been cleared from the environment but all of the other objects remain.
<h2><span class="orange">How to Clear All Plots in RStudio (With Example)</span></h2>
You can use the following basic syntax to clear all plots in RStudio:
<b>dev.off(dev.list()["RStudioGD"])
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Clear All Plots in RStudio</h3>
Suppose we use the following code to create three different scatterplots in RStudio:
<b>#create some vectors
x &lt;- c(1, 1, 3, 4, 6, 7, 9, 10, 14, 19)
y &lt;- c(3, 5, 5, 4, 6, 9, 10, 14, 13, 14)
z &lt;- c(14, 14, 13, 10, 6, 9, 5, 4, 3, 5)
#create several scatterplots
plot(x, y)
plot(x, z)
plot(y, z)
</b>
We can view each of these scatterplots in the plotting window in RStudio: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/clearplot1.jpg"559">
We can use the blue arrows in the top left corner of the plotting window to scroll through the various plots we created.
We can then use the following code to clear all plots from the RStudio environment:
<b>#clear all plots
dev.off(dev.list()["RStudioGD"]) </b>
The plotting window will now be cleared of all plots:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/clearplot2.jpg"516">
<h3>Example 2: Clear All Plots in RStudio (And Suppress Any Errors)</h3>
If there are no plots in RStudio and we attempt to clear all the plots, we will receive an error:
<b>#attempt to clear all plots
dev.off(dev.list()["RStudioGD"])
Error in if (which == 1) stop("cannot shut down device 1 (the null device)") : 
  argument is of length zeroan>))</b>
However, we can use a <b>try()</b> statement to suppress this error:
<b>#attempt to clear all plots (suppress error if not plots exist)
try(dev.off(dev.list()["RStudioGD"]), silent=TRUE)</b>
This code will attempt to clear all plots from RStudio and if no plots exist, no error will be shown.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/clearplot3.jpg"603">
When this code is run in the console window, we receive no error even though there are no plots to clear.
<h2><span class="orange">5 Examples of Cluster Analysis in Real Life</span></h2>
<b>Cluster analysis</b> is a technique used in  machine learning  that attempts to find clusters of observations within a dataset.
The goal of cluster analysis is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.
The following examples show how cluster analysis is used in various real-life situations.
<h3>Example 1: Retail Marketing</h3>
Retail companies often use clustering to identify groups of households that are similar to each other.
For example, a retail company may collect the following information on households:
Household income
Household size
Head of household Occupation
Distance from nearest urban area
They can then feed these variables into a clustering algorithm to perhaps identify the following clusters:
Cluster 1: Small family, high spenders
Cluster 2: Larger family, high spenders
Cluster 3: Small family, low spenders
Cluster 4: Large family, low spenders
The company can then send personalized advertisements or sales letters to each household based on how likely they are to respond to specific types of advertisements.
<h3>Example 2: Streaming Services</h3>
Streaming services often use clustering analysis to identify viewers who have similar behavior.
For example, a streaming service may collect the following data about individuals:
Minutes watched per day
Total viewing sessions per week
Number of unique shows viewed per month
Using these metrics, a streaming service can perform cluster analysis to identify high usage and low usage users so that they can know who they should spend most of their advertising dollars on.
<h3>Example 3: Sports Science</h3>
Data scientists for sports teams often use clustering to identify players that are similar to each other. 
For example, professional basketball teams may collect the following information about players:
Points per game
Rebounds per game
Assists per game
Steals per game
They can then feed these variables into a clustering algorithm to identify players that are similar to each other so that they can have these players practice with each other and perform specific drills based on their strengths and weaknesses.
<h3>
<b>Example 4: Email Marketing</b>
</h3>
Many businesses use cluster analysis to identify consumers who are similar to each other so they can tailor their emails sent to consumers in such a way that maximizes their revenue.
For example, a business may collect the following information about consumers:
Percentage of emails opened
Number of clicks per email
Time spent viewing email
Using these metrics, a business can perform cluster analysis to identify consumers who use email in similar ways and tailor the types of emails and frequency of emails they send to different clusters of customers.
<h3>Example 5: Health Insurance</h3>
Actuaries at health insurance companies often used cluster analysis to identify “clusters” of consumers that use their health insurance in specific ways.
For example, an actuary may collect the following information about households:
Total number of doctor visits per year
Total household size
Total number of chronic conditions per household
Average age of household members
An actuary can then feed these variables into a clustering algorithm to identify households that are similar. The health insurance company can then set monthly premiums based on how often they expect households in specific clusters to use their insurance.
<h2><span class="orange">How to Perform Cluster Sampling in Excel (Step-by-Step)</span></h2>
In statistics, we often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>cluster sampling</b>, in which a population is split into clusters and all members of <em>some</em> clusters are chosen to be included in the sample.
The following step-by-step example shows how to perform cluster sampling in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following dataset into Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster1.png">
Next, we’ll perform cluster sampling in which we randomly select two teams and choose to include every player from those two teams in the final sample.
<h3>Step 2: Find Unique Values</h3>
Next, type in <b>=UNIQUE(B2:B21)</b> to produce an array of unique values from the <b>Team</b> column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster2.png">
Next, we’ll type an integer (starting at 1) next to each unique team name:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster3.png">
<h3>Step 3: Select Random Clusters</h3>
Next, we’ll type <b>=RANDBETWEEN(G2, G6)</b> to randomly select one of the integers from the list:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster4.png">
Once we click<b> ENTER</b>, we can see that the value <b>5</b> was randomly selected. The team associated with this value is team E, which represents the first team we’ll include in our final sample.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster5.png">
Next, double click any cell and press <b>Enter</b>. A new number will be selected from the <b>=RANDBETWEEN(G2, G6)</b> function.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster6.png">
We can see that the value <b>3</b> was randomly selected. The team associated with this value is team C, which represents the second team we’ll include in our final sample.
<h3>Step 4: Filter the Final Sample</h3>
The final sample will simply include all players who belong to either team C or team E.
To filter for just these players, highlight all of the data. Then click the <b>Data</b> tab along the top ribbon and then click the <b>Filter</b> button within the <b>Sort & Filter</b> group.
When the filter appears above each column, click the dropdown arrow next to the Team column and check the boxes next to teams C and E only:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster7.png">
Once you click OK, the dataset will be filtered to only show players on team C or team E:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/cluster8.png">
This represents our final sample.
Our cluster sampling is complete because we randomly chose two teams and included each player from those two teams in our final sample.
<h2><span class="orange">Cluster Sampling in Pandas (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>cluster sampling</b>, in which a population is split into clusters and all members of <em>some </em>clusters are chosen to be included in the sample.
This tutorial explains how to perform cluster sampling on a pandas DataFrame in Python.
<h2>Example: Cluster Sampling in Pandas</h2>
Suppose a company that gives city tours wants to survey its customers. Out of ten tours they give one day, they randomly select four tours and ask every customer to rate their experience on a scale of 1 to 10.
The following code shows how to create a pandas DataFrame to work with:
<b>import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(0)
#create DataFrame
df = pd.DataFrame({'tour': np.repeat(np.arange(1,11), 20),   'experience': np.random.normal(loc=7, scale=1, size=200)})
#view first six rows of DataFrame
df.head()
  tour experience
1    1   6.373546
2    1   7.183643
3    1   6.164371
4    1   8.595281
5    1   7.329508
6    1   6.179532
</b>
And the following code shows how obtain a sample of customers by randomly selecting four tours and including every member in those tours in the sample:
<b>#randomly choose 4 tour groups out of the 10
clusters = np.random.choice(np.arange(1,11), size=4, replace=False)
#define sample as all members who belong to one of the 4 tour groups
cluster_sample = df[df['tour'].isin(clusters)]
#view first six rows of sample
cluster_sample.head()
tourexperience
4035.951447
4135.579982
4235.293730
4338.950775
4436.490348 
#find how many observations came from each tour group
cluster_sample['tour'].value_counts()
10    20
6     20
5     20
3     20
Name: tour, dtype: int64</b>
From the output we can see that:
20 customers from tour group #10 were included in the sample.
20 customers from tour group #6 were included in the sample.
20 customers from tour group #5 were included in the sample.
20 customers from tour group #3 were included in the sample.
Thus, this sample is composed of 80 total customers that came from 4 different tour groups.
<h2><span class="orange">Cluster Sampling in R (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>cluster sampling</b>, in which a population is split into clusters and all members of <em>some </em>clusters are chosen to be included in the sample.
This tutorial explains how to perform cluster sampling in R.
<h2>Example: Cluster Sampling in R</h2>
Suppose a company that gives city tours wants to survey its customers. Out of ten tours they give one day, they randomly select four tours and ask every customer to rate their experience on a scale of 1 to 10.
The following code shows how to create a fake data frame in R to work with:
<b>#make this example reproducible
set.seed(1)
#create data frame
df &lt;- data.frame(tour = rep(1:10, each=20), experience = rnorm(200, mean=7, sd=1))
#view first six rows of data frame
head(df)
  tour experience
1    1   6.373546
2    1   7.183643
3    1   6.164371
4    1   8.595281
5    1   7.329508
6    1   6.179532
</b>
And the following code shows how obtain a sample of customers by randomly selecting four tours and including every member in those tours in the sample:
<b>#randomly choose 4 tour groups out of the 10
clusters &lt;- sample(unique(df$tour), size=4, replace=F)
#define sample as all members who belong to one of the 4 tour groups
cluster_sample &lt;- df[df$tour %in% clusters, ]
#view how many customers came from each tour
table(cluster_sample$tour)
 2  7  8 10 
20 20 20 20 
</b>
From the output we can see that:
20 customers from tour group #2 were included in the sample.
20 customers from tour group #7 were included in the sample.
20 customers from tour group #8 were included in the sample.
20 customers from tour group #10 were included in the sample.
Thus, this sample is composed of 80 total customers that came from 4 different tour groups.
<b>Related: </b> How to Use %in% Operator in R 
<h2><span class="orange">Cluster Sampling vs. Stratified Sampling: What’s the Difference?</span></h2>
In statistics, two of the most common methods used to obtain  samples  from a population are <b>cluster sampling</b> and <b>stratified sampling</b>.
This tutorial provides a brief explanation of both sampling methods along with the similarities and differences between them.
<h3>Cluster Sampling</h3>
<b>Cluster sampling</b> is a type of sampling method in which we split a population into clusters, then randomly select some of the clusters and include all members from those clusters in the sample.
For example, suppose a company that gives whale-watching tours wants to survey its customers. Out of ten tours they give one day, they randomly select four tours and ask every customer about their experience.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/twostage_cluster1.png">
This is an example of <b>cluster sampling</b>. 
<h3>Stratified Sampling</h3>
<b>Stratified sampling</b> is a type of sampling method in which we split a population into groups, then randomly select some members from each group to be in the sample.
For example, suppose a high school principal wants to conduct a survey to collect the opinions of students. He first splits the students into four stratums based on their grade – Freshman, Sophomore, Junior, and Senior – then selects a simple random sample of 50 students from each grade to be included in the survey.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/cluster_vs_stratified1.png">
This is an example of <b>stratified sampling</b>.
<h3>Similarities & Differences</h3>
Cluster sampling and stratified sampling share the following <b>similarities:</b>
Both methods are examples of <em>probability sampling methods </em>– every member in the population has an equal probability of being selected to be in the sample.
Both methods divide a population into distinct groups (either clusters or stratums).
Both methods tend to be quicker and more cost-effective ways of obtaining a sample from a population compared to a simple random sample.
Cluster sampling and stratified sampling share the following <b>differences:</b>
Cluster sampling divides a population into groups, then includes <em>all</em> members of <i>some</i> randomly chosen groups.
Stratified sampling divides a population into groups, then includes <em>some</em> members of <em>all</em> of the groups.
<h3>When to Use Each Sampling Method</h3>
There is a simple rule of thumb we can use to decide whether to use cluster sampling or stratified sampling:
If a population is <b>heterogeneous</b> (i.e. there are natural differences between individuals) then it’s best to use <b>stratified sampling</b> to obtain a random sample.
In our previous example with high school students, the students could naturally be divided into four groups based on grade. Thus, it made sense to include some students from each grade in the sample to get a representative sample of all students in the school.
If a population is <b>homogeneous</b> (i.e. there are no noticeable differences between individuals) then it’s best to use <b>cluster sampling</b> to obtain a sample.
In our previous example with whale-watching tours, there were no clear differences between one group of customers and the next. Thus, it made sense to just randomly choose some groups and include all customers from those chosen groups to be included in the sample.
Keep this rule of thumb in mind when deciding whether to use stratified sampling or clustering sampling.
<h2><span class="orange">How to Create a Clustered Stacked Bar Chart in Excel</span></h2>
A <b>clustered stacked bar chart</b> is a type of bar chart that is both clustered and stacked.
It’s particularly useful for visualizing data values that have multiple groups and span several time periods.
This tutorial provides a step-by-step example of how to create the following clustered stacked bar chart in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack1.jpg">
<h2>Step 1: Enter the Data</h2>
First, let’s enter the following dataset that shows the sales of various products at different retail stores during different years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack2.jpg"557">
<h2>Step 2: Create the Clustered Stacked Bar Chart</h2>
Next, highlight the cell range <b>C1:E16</b>, then click the <b>Insert</b> tab along the top ribbon, then click the <b>Stacked Column</b> icon within the <b>Charts</b> group to create the following clustered stacked bar chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack3.jpg"671">
<h2>
<b>Step 3: Customize the Clustered Stacked Bar Chart</b>
</h2>
Next, we need to insert custom labels on the x-axis.
Before we do so, click on cell <b>A17</b> and type a couple empty spaces. This will be necessary for the next step.
Next, right click anywhere on the chart and then click <b>Select Data</b>.
In the window that appears, click the <b>Edit</b> button under <b>Horizontal (Category) Axis Labels</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack4.jpg"607">
For the Axis label range, highlight the range <b>A2:B17</b> and then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack5.jpg"355">
The following labels will appear on the x-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack6.jpg"563">
Next, click on any of the bars in the graph. In the <b>Format Data Series</b> panel that appears, adjust the <b>Gap Width</b> to be <b>0%</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusters7.jpg"328">
The gap between the bars in the same clusters will be removed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusters8.jpg"570">
Next, click on any bar in the chart. In the <b>Format Data Series</b> panel that appears, click the paint can icon, then click <b>Border</b> and choose <b>Solid line</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusters9.jpg"305">
Repeat for each bar in the chart until there is a solid black border around each bar:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusters10.jpg"582">
Lastly, click on the title text and change it to “Sales by Store and Year”.
Then click on any individual text elements in the plot that you’d like and make them bold:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/clusterstack1.jpg"570">
The clustered stacked bar chart is now complete.
<h2>Additional Resources</h2>
The following tutorials explain how to create other common visualizations in Excel:
 How to Create a Quadrant Chart in Excel 
 How to Create a Bubble Chart in Excel 
 How to Create a Double Doughnut Chart in Excel 
<h2><span class="orange">What are Clustered Standard Errors? (Definition & Example)</span></h2>
<b>Clustered standard errors</b> are used in  regression models  when some observations in a dataset are naturally “clustered” together or related in some way.
To understand when to use clustered standard errors, it helps to take a step back and understand the goal of regression analysis.
In statistics, regression models are used to quantify the relationship between one or more predictor variables and a  response variable .
Whenever you fit a regression model, your output will be displayed in a  regression table  that looks like the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/clusterStandardError1.png">
Here’s how to interpret the values in the table:
<b>Coefficient:</b> The average increase in the response variable associated with a one unit increase in a specific predictor variable, assuming all other predictor variables are held constant.
<b>Standard Error:</b> A measure of the precision of the estimate of the coefficient.
<b>t Stat:</b> The t-statistic for the predictor variable, calculated as Coefficient / Standard Error.
<b>p-value:</b> The p-value associated with the t-statistic. If this value is less than a certain significance level (e.g. 0.05), we say that there is a statistically significant relationship between the predictor variable and the response variable.
One of the key assumptions of regression analysis is the  assumption of independence . This assumptions states that each  observation  in the dataset should be independent of every other observation.
In practice, this assumption is sometimes violated.
For example, suppose a researcher wants to fit a regression model using hours studied as the predictor variable and exam score as the response variable. He decides to collect data for 50 students spread across five different classrooms.
In this scenario, students are naturally clustered together into classrooms, which means the data collected for each student will not be independent.
For example, some classrooms may have an excellent teacher while other classrooms have a sub-par teacher who does a poor job of teaching their subject.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/clusterStandardError2.png">
If the researcher fits a regression model without accounting for this clustered nature of the data, the <b>standard errors of the regression coefficients will be smaller than they should be</b>.
This will result in the following errors:
The t-statistics will be too large.
The p-values will be too small.
The  confidence intervals  will be too narrow.
Simply put, the results of the regression analysis will not be reliable.
To account for this, we can use <b>clustered standard errors</b>. Fortunately, in most statistical software you can explicitly tell the software to use clustered standard errors when fitting a regression model. 
For example, in Stata you can use the <b>cluster(variable name)</b> command to tell Stata to use clustered standard errors when fitting a regression model.
In practice, you can use the following syntax to fit a regression model in Stata with clustered standard errors:
regress x y, cluster(variable_name)
where:
<b>x:</b> The predictor variable
<b>y:</b> The response variable
<b>variable_name:</b> The name of the variable that the data should be clustered based on
This will return a regression table with clustered standard errors.
<h2><span class="orange">What is Cochran’s Q Test? (Definition & Example)</span></h2>
<b>Cochran’s Q test</b> is a statistical test that is used to determine whether the proportion of “successes” is equal across three or more groups in which the same individuals appear in each group.
For example, we may use Cochran’s Q test to determine if the proportion of students who pass a test is equal when using three different studying techniques.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/cochranQ2.png">
<h3>Steps to Perform Cochran’s Q Test</h3>
Cochran’s Q test uses the following null and alternative hypotheses:
<b>Null Hypothesis (H<sub>0</sub>):</b> The proportion of “successes” is the same in all groups
<b>Alternatve Hypothesis(H<sub>A</sub>):</b> The proportion of “successes” is different in at least one of the groups
The test statistic is calculated as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/cochranQ.png">
where:
<b>k:</b> The number of treaments (or “groups”)
<b>X.j:</b> The column total for the j<sup>th</sup> treatment
<b>b:</b> The number of blocks
<b>Xi.:</b> The row total for the i<sup>th</sup> block
<b>N:</b> The grand total
The test statistic <em>T</em> follows a Chi-Square distribution with <em>k-1</em> degrees of freedom.
If the  p-value  associated with the test statistic is less than a certain significance level (like α = .05), we can reject the null hypothesis and conclude that we have sufficient evidence to say the proportion of “successes” is different in at least one of the groups.
<h3>Example: Cochran’s Q Test</h3>
Suppose a researcher wants to know if three different studying techniques lead to different proportions of pass rates among students.
To test this, she recruits 20 students to each take an exam of equal difficulty using three different studying techniques. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/cochranQ2.png">
To perform Cochran’s Q test, we can use statistical software since it can be cumbersome to perform by hand.
Here’s the code we can use to create this dataset and perform Cochran’s Q test in the statistical programming language R:
<b>#load DescTools package
library(DescTools)
#create dataset
df &lt;- data.frame(student=rep(1:20, each=3), technique=rep(c('A', 'B', 'C'), times=20), outcome=c(1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1,           1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,           1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,           1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1))
#perform Cochran's Q test
CochranQTest(outcome ~ technique| student, data=df)
Cochran's Q test
data:  outcome and technique and student
Q = 0.33333, df = 2, p-value = 0.8465</b>
From the output of the test we can observe the following:
The test statistic is <b>0.333</b>
The corresponding p-value is <b>0.8465</b>
Since this p-value is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the studying technique used by students leads to different proportions of passing rates.
<h2><span class="orange">Coefficient of Determination Calculator</span></h2>
The <b>coefficient of determination</b>, often denoted R<sup>2</sup>, is the proportion of variance in the response variable that can be explained by the predictor variables in a regression model. 
This calculator finds the coefficient of determination for a given regression model.
Simply enter a list of values for x (the predictor variable) and y (the response variable) in the boxes below, then click the “Calculate” button:
<b>x (Predictor Variable)</b>
<textarea id="x" rows="5" cols="40">12, 13, 14, 15, 15, 22, 27</textarea>
<b>y (Response Variable)</b>
<textarea id="y" rows="5" cols="40">11, 13, 14, 14, 15, 16, 18</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Coefficient of Determination (R<sup>2</sup>): 0.8368
Interpretation: 83.68% of the variation in the response variable can be explained by the predictor variable.
Helpful Resource:  What is Considered a Good R-Squared Value? 
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        lr['r2'] = Math.pow((n*sum_xy - sum_x*sum_y)/Math.sqrt((n*sum_xx-sum_x*sum_x)*(n*sum_yy-sum_y*sum_y)),2);
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope;
var b = lr.intercept;
var r2 = lr.r2;
var r2p = r2*100;
document.getElementById('r2').innerHTML = r2.toFixed(4);
document.getElementById('r2_explain').innerHTML = r2p.toFixed(2);
}
//output error message if boths lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate the Coefficient of Variation in Excel</span></h2>
A <b>coefficient of variation</b>, often abbreviated as CV, is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>CV</b> = σ / μ
where:
σ = standard deviation of dataset
μ = mean of dataset
In its simplest terms, the coefficient of variation is simply the ratio between the standard deviation and the mean.
<h2>When is the Coefficient of Variation Used?</h2>
The coefficient of variation is often used to compare the variation between two different datasets.
In the real world, it’s often used in finance to compare the mean expected return of an investment relative to the expected standard deviation of the investment. This allows investors to compare the risk-return trade-off between investments.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 7%, standard deviation  = 12.4%
Mutual Fund B: mean = 5%, standard deviation  = 8.2%
Upon calculating the coefficient of variation for each fund, the investor finds:
CV for Mutual Fund A = 12.4% / 7% = <b>1.77</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund B has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
<h2>How to Calculate the Coefficient of Variation in Excel</h2>
There is no built-in formula in Excel to calculate the coefficient of variation for a dataset, but fortunately it’s relatively easy to calculate using a couple simple formulas. The following example illustrates how to calculate the coefficient of variation for a given dataset.
Suppose we have the following dataset that contains the exam scores of 20 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/CV0.png">
To calculate the coefficient of variation for this dataset, we only need to know two numbers: the mean and the standard deviation. These can be calculated using the following formulas:
Mean: <b>=AVERAGE(A2:A21)</b>
Standard deviation: <b>=STDEV(A2:A21)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/CV1.png">
To calculate the coefficient of variation, we then divide the standard deviation by the mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/CV2.png">
The coefficient of variation turns out to be <b>0.0864</b>.
Note that we also could have used just one formula to calculate the CV:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/CV3.png">
This results in the same CV of <b>0.0864</b>.
<h2><span class="orange">How to Calculate the Coefficient of Variation in Google Sheets</span></h2>
A <b>coefficient of variation</b>, often abbreviated as <em>CV</em>, is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>CV = σ / μ</b>
where:
<b>σ:</b> The standard deviation of dataset
<b>μ:</b> The mean of dataset
In plain English, the coefficient of variation is simply the ratio between the standard deviation and the mean.
<h3>When to Use the Coefficient of Variation</h3>
The coefficient of variation is often used to compare the variation between two different datasets.
In the real world, it’s often used in finance to compare the mean expected return of an investment relative to the expected standard deviation of the investment. This allows investors to compare the risk-return trade-off between investments.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 7%, standard deviation  = 12.4%
Mutual Fund B: mean = 5%, standard deviation  = 8.2%
Upon calculating the coefficient of variation for each fund, the investor finds:
CV for Mutual Fund A = 12.4% / 7% = <b>1.77</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund B has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
<h3>Example: Calculating the Coefficient of Variation in Google Sheets</h3>
There is no built-in function in Google Sheets to calculate the coefficient of variation for a dataset, but it’s relatively easy to calculate using simple formulas.
Suppose we have the following dataset that contains 20 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/cvSheets1-1.png">
To calculate the coefficient of variation for this dataset, we only need to know two numbers: the mean and the standard deviation. These can be calculated using the following formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/cvSheets2.png">
To calculate the coefficient of variation, we then divide the standard deviation by the mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/cvSheets3.png">
The coefficient of variation turns out to be <b>0.0864</b>.
<h2><span class="orange">How to Calculate the Coefficient of Variation in Python</span></h2>
A <b>coefficient of variation</b>, often abbreviated as <em>CV</em>, is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>CV = σ / μ</b>
where:
<b>σ:</b> The standard deviation of dataset
<b>μ:</b> The mean of dataset
In plain English, the coefficient of variation is simply the ratio between the standard deviation and the mean.
<h3>When to Use the Coefficient of Variation</h3>
The coefficient of variation is often used to compare the variation between two different datasets.
In the real world, it’s often used in finance to compare the mean expected return of an investment relative to the expected standard deviation of the investment. This allows investors to compare the risk-return trade-off between investments.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 9%, standard deviation  = 12.4%
Mutual Fund B: mean = 5%, standard deviation  = 8.2%
Upon calculating the coefficient of variation for each fund, the investor finds:
CV for Mutual Fund A = 12.4% /9% = <b>1.38</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund A has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
<h3>How to Calculate the Coefficient of Variation in Python</h3>
To calculate the coefficient of variation for a dataset in Python, you can use the following syntax:
<b>import numpy as np
cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100
</b>
The following examples show how to use this syntax in practice.
<b>Example 1: Coefficient of Variation for a Single Array</b>
The following code shows how to calculate CV for a single array:
<b>#create vector of data
data = [88, 85, 82, 97, 67, 77, 74, 86, 81, 95, 77, 88, 85, 76, 81, 82]
#define function to calculate cv
cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100 
#calculate CV
cv(data)
9.234518
</b>
The coefficient of variation turns out to be <b>9.23</b>.
<b>Example 2: Coefficient of Variation for Several Vectors</b>
The following code shows how to calculate the CV for several columns in a pandas DataFrame:
<b>import numpy as np
import pandas as pd
#define function to calculate cv
cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100 
#create pandas DataFrame
df = pd.DataFrame({'a': [88, 85, 82, 97, 67, 77, 74, 86, 81, 95],   'b': [77, 88, 85, 76, 81, 82, 88, 91, 92, 99],   'c': [67, 68, 68, 74, 74, 76, 76, 77, 78, 84]})
#calculate CV for each column in data frame
df.apply(cv)
a    11.012892
b     8.330843
c     7.154009
dtype: float64</b>
Note that missing values will simply be ignored when calculating the coefficient of variation:
<b>import numpy as np
import pandas as pd
#define function to calculate cv
cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100 
#create pandas DataFrame
df = pd.DataFrame({'a': [88, 85, 82, 97, 67, 77, 74, 86, 81, 95],   'b': [77, 88, 85, 76, 81, 82, 88, 91, np.nan, 99],   'c': [67, 68, 68, 74, 74, 76, 76, 77, 78, np.nan]})
#calculate CV for each column in data frame
df.apply(cv)
a    11.012892
b     8.497612
c     5.860924
dtype: float64</b>
<h2><span class="orange">How to Calculate the Coefficient of Variation in R</span></h2>
A <b>coefficient of variation</b>, often abbreviated as <em>CV</em>, is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>CV = σ / μ</b>
where:
<b>σ:</b> The standard deviation of dataset
<b>μ:</b> The mean of dataset
In plain English, the coefficient of variation is simply the ratio between the standard deviation and the mean.
<h3>When to Use the Coefficient of Variation</h3>
The coefficient of variation is often used to compare the variation between two different datasets.
In the real world, it’s often used in finance to compare the mean expected return of an investment relative to the expected standard deviation of the investment. This allows investors to compare the risk-return trade-off between investments.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 9%, standard deviation  = 12.4%
Mutual Fund B: mean = 5%, standard deviation  = 8.2%
Upon calculating the coefficient of variation for each fund, the investor finds:
CV for Mutual Fund A = 12.4% /9% = <b>1.38</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund A has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
<h3>How to Calculate the Coefficient of Variation in R</h3>
To calculate the coefficient of variation for a dataset in R, you can use the following syntax:
<b>cv &lt;- sd(data) / mean(data) * 100
</b>
The following examples show how to use this syntax in practice.
<b>Example 1: Coefficient of Variation for a Single Vector</b>
The following code shows how to calculate CV for a single vector:
<b>#create vector of data
data &lt;- c(88, 85, 82, 97, 67, 77, 74, 86, 81, 95, 77, 88, 85, 76, 81, 82)
#calculate CV
cv &lt;- sd(data) / mean(data) * 100
#display CV
cv
[1] 9.234518
</b>
The coefficient of variation turns out to be <b>9.23</b>.
<b>Example 2: Coefficient of Variation for Several Vectors</b>
The following code shows how to calculate the CV for several vectors in a data frame by using the  sapply()  function:
<b>#create data frame
data &lt;- data.frame(a=c(88, 85, 82, 97, 67, 77, 74, 86, 81, 95),   b=c(77, 88, 85, 76, 81, 82, 88, 91, 92, 99),   c=c(67, 68, 68, 74, 74, 76, 76, 77, 78, 84))
#calculate CV for each column in data frame
sapply(data, function(x) sd(x) / mean(x) * 100)
        a         b         c 
11.012892  8.330843  7.154009</b>
Be sure to use <b>na.rm=T</b> if there happen to be missing values in your data as well. This tells R to simply ignore the missing values when calculating the coefficient of variation:
<b>#create data frame
data &lt;- data.frame(a=c(88, 85, 82, 97, 67, 77, 74, 86, 81, 95),   b=c(77, 88, 85, 76, 81, 82, 88, 91, NA, 99),   c=c(67, 68, 68, 74, 74, 76, 76, 77, 78, NA))
#calculate CV for each column in data frame
sapply(data, function(x) sd(x, na.rm=T) / mean(x, na.rm=T) * 100)
        a         b         c 
11.012892  8.497612  5.860924
</b>
<h2><span class="orange">How to Calculate the Coefficient of Variation in SPSS</span></h2>
The <b>coefficient of variation</b> is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>Coefficient of variation</b> = σ / μ
where:
<b>σ</b> = standard deviation of dataset
<b>μ</b> = mean of dataset
This tutorial explains how to calculate the coefficient of variation for a dataset in SPSS
<h3>Example: Coefficient of Variation in SPSS</h3>
Suppose we have the following dataset that displays the annual income (in thousands) for 15 individuals:
Use the following steps to calculate the coefficient of variation for this dataset in SPSS:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/outliersSPSS1.png">
<b>Step 1: Create a column of 1’s.</b>
First, we need to create a column of all 1’s next to the original dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/cvspss1.png">
<b>Step 2: Calculate the coefficient of variation.</b>
Next, click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Ratio</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/cvspss2.png">
In the new window that pops up, drag the variable <b>income </b>into the box labelled Numerator and drag the variable <b>one </b>into the box labelled Denominator:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/cvspss3.png">
Next, click <b>Statistics</b>. Make sure the boxes are checked next to <b>Mean</b>, <b>Standard deviation</b>, and <b>Mean Centered COV</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/cvspss6.png">
Lastly, click <b>OK</b>.
<b>Step 3: Interpret the coefficient of variation.</b>
Once you click <b>OK</b>, the coefficient of variation for this dataset will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/cvspss7.png">
We can see that the coefficient of variation for this dataset is <b>49.3%</b>. This was calculated using the following formula:
CV =  σ / μ * 100 = (29.060/58.933) * 100 = <b>49.3%</b>.
<h2><span class="orange">How to Find Coefficient of Variation on a TI-84 Calculator</span></h2>
A <b>coefficient of variation</b>, often abbreviated as <em>CV</em>, is a way to measure how spread out values are in a dataset relative to the mean. It is calculated as:
<b>CV = σ / μ</b>
where:
<b>σ:</b> The standard deviation of dataset
<b>μ:</b> The mean of dataset
In simple terms, the coefficient of variation is the ratio between the standard deviation and the mean.
It is often used to compare the variation between two different datasets. For example, in finance it is used to compare the mean expected return of an investment relative to the expected standard deviation of the investment.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 9%, standard deviation = 12.4%
Mutual Fund B: mean = 5%, standard deviation = 8.2%
The investor can calculate the coefficient of variation for each fund:
CV for Mutual Fund A = 12.4% / 9% = <b>1.38</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund A has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
The following step-by-step example explains how to calculate the coefficient of variation for the following dataset on a TI-84 calculator:
<b>Dataset:</b> 3, 8, 8, 13, 16, 11
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values.
Press Stat, then press EDIT. Then enter the values of the dataset in column L1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cvTI1.png">
<h3>Step 2: Find the Coefficient of Variation</h3>
Next, press Stat and then scroll over to the right and press CALC.
Then press 1-Var Stats.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum2.png">
In the new screen that appears, press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum3.png">
Once you press Enter, a list of summary statistics will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cvTI2.png">
From this screen we can observe the values for the mean and the sample standard deviation:
Mean (x): <b>9.8333</b>
Sample standard deviation (Sx): <b>4.535</b>
We can then calculate the coefficient of variation as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cvTI3.png">
The coefficient of variation for this dataset turns out to be <b>0.4611</b>. In percentage terms, this is equal to <b>46.11%</b>.
<h2><span class="orange">Coefficient of Variation vs. Standard Deviation: The Difference</span></h2>
The <b>standard deviation</b> of a dataset is a way to measure how far the average value lies from the mean.
To find the standard deviation of a given  sample , we can use the following formula:
<b>s = √(Σ(x<sub>i</sub> – x)<sup>2</sup> / (n-1))</b>
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The value of the i<sup>th</sup> observation in the sample
<b>x:</b> The mean of the sample
<b>n:</b> The sample size
The higher the value for the standard deviation, the more spread out the values are in a sample. However, it’s hard to say if a given value for a standard deviation is “high” or “low” because it depends on the type of data we’re working with.
For example, a standard deviation of 500 may be considered low if we’re talking about annual income of residents in a certain city. Conversely, a standard deviation of 50 may be considered high if we’re talking about exam scores of students on a certain test.
One way to understand whether or not a certain value for the standard deviation is high or low is to find the <b>coefficient of variation</b>, which is calculated as:
CV = s / x
where:
<b>s:</b> The sample standard deviation
<b>x:</b> The sample mean
In simple terms, the coefficient of variation is the ratio between the standard deviation and the mean. 
The higher the coefficient of variation, the higher the standard deviation of a sample <em>relative</em> to the mean.
<h3>Example: Calculating the Standard Deviation & Coefficient of Variation</h3>
Suppose we have the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
Using a calculator, we can find the following metrics for this dataset:
Sample mean (x): 19.29
Sample standard deviation (s): 9.25
We can then use these values to calculate the coefficient of variation:
CV = s / x
CV = 9.25 / 19.29
CV = 0.48
Both the standard deviation and the coefficient of variation are useful to know for this dataset.
The standard deviation tells us that the typical value in this dataset lies 9.25 units away from the mean. The coefficient of variation then tells us that the standard deviation is about half the size of the sample mean.
<h3>Standard Deviation vs. Coefficient of Variation: When to Use Each</h3>
The standard deviation is most commonly used when we want to know the spread of values in a single dataset.
However, the coefficient of variation is more commonly used when we want to compare the variation between two datasets.
For example, in finance the coefficient of variation is used to compare the mean expected return of an investment relative to the expected standard deviation of the investment.
For example, suppose an investor is considering investing in the following two mutual funds:
Mutual Fund A: mean = 9%, standard deviation = 12.4%
Mutual Fund B: mean = 5%, standard deviation = 8.2%
The investor can calculate the coefficient of variation for each fund:
CV for Mutual Fund A = 12.4% / 9% = <b>1.38</b>
CV for Mutual Fund B = 8.2% / 5% = <b>1.64</b>
Since Mutual Fund A has a lower coefficient of variation, it offers a better mean return relative to the standard deviation.
<h3>Summary</h3>
Here’s a brief summary of the main points in this article:
Both the standard deviation and the coefficient of variation measure the spread of values in a dataset.
The standard deviation measures how far the average value lies from the mean.
The coefficient of variation measures the ratio of the standard deviation to the mean.
The standard deviation is used more often when we want to measure the spread of values in a single dataset.
The coefficient of variation is used more often when we want to compare the variation between two different datasets.
<h2><span class="orange">How to Use the coeftest() Function in R</span></h2>
You can use the <b>coeftest()</b> function from the <b>lmtest</b> package in R to perform a t-test for each estimated coefficient in a regression model.
This function uses the following basic syntax:
<b>coeftest(x)</b>
where:
<b>x</b>: Name of the fitted regression model
The following example shows how to use this function in practice.
<h2>Example: How to Use coeftest() Function in R</h2>
Suppose we have the following data frame in R that shows the number of hours spent studying, number of practice exams taken, and final exam score for 10 students in some class:
<b>#create data frame
df &lt;- data.frame(score=c(77, 79, 84, 85, 88, 99, 95, 90, 92, 94), hours=c(1, 1, 2, 3, 2, 4, 4, 2, 3, 3), prac_exams=c(2, 3, 3, 2, 4, 5, 4, 3, 5, 4))
#view data frame
df
   score hours prac_exams
1     77     1          2
2     79     1          3
3     84     2          3
4     85     3          2
5     88     2          4
6     99     4          5
7     95     4          4
8     90     2          3
9     92     3          5
10    94     3          4
</b>
Now suppose we would like to fit the following multiple linear regression model in R:
Exam score = β<sub>0</sub> + β<sub>1</sub>(hours) + β<sub>2</sub>(practice exams)
We can use the  lm()  function to fit this model:
<b>#fit multiple linear regression model
fit &lt;- lm(score ~ hours + prac_exams, data=df)
</b>
We can then use the <b>coeftest()</b> function to perform a t-test for each fitted regression coefficient in the model:
<b>library(lmtest)
#perform t-test for each coefficient in model
coeftest(fit)
t test of coefficients:
            Estimate Std. Error t value  Pr(>|t|)    
(Intercept) 68.40294    2.87227 23.8150 5.851e-08 ***
hours        4.19118    0.99612  4.2075  0.003998 ** 
prac_exams   2.69118    0.99612  2.7017  0.030566 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
The t test statistic and corresponding p-value is shown for each t-test:
<b>Intercept</b>: t = 23.8150, p = &lt;0.000
<b>hours</b>: t = 4.2075, p = .003998
<b>prac_exams</b>: t = 2.7017, p = .030566
Note that we use the following null and alternative hypotheses for each t-test:
<b>H<sub>0</sub></b>: β<sub>i</sub> = 0 (the slope is equal to zero)
<b>H<sub>A</sub></b>: β<sub>i</sub> ≠ 0 (the slope is not equal to zero)
If the p-value of the t-test is less than some threshold (e.g. α = .05) then we reject the null hypothesis and conclude that there is a statistically significant relationship between the predictor variable and the response variable.
Since the p-value for each t-test is less than .05, we would conclude that each predictor variable in the model has a statistically significant relationship with the response variable.
In the context of this example, we would say that hours spent studying and number of practice exams taken are both statistically significant predictors of final exam score for students.
<h2>Additional Resources</h2>
The following tutorials provide additional information about linear regression in R:
 How to Interpret Regression Output in R 
 How to Perform Simple Linear Regression in R 
 How to Perform Multiple Linear Regression in R 
 How to Perform Logistic Regression in R 
<h2><span class="orange">How to Calculate Cohen’s d in Excel</span></h2>
In statistics, when we’re interested in determining whether or not there is a significant difference between two groups we often perform a hypothesis test, which results in a  p-value .
If this p-value is less than some significance level (common choices are 0.10, 0.05, and 0.01), we conclude that there is a statistically significant difference between the two groups.
However, while a p-value can tell us whether or not there is a statistically significant difference between two groups, an  effect size  can tell us how large this difference actually is.
One of the most common measurements of effect size is <b>Cohen’s d</b>, which is calculated as:
Cohen’s d = (x<sub>1</sub> – x<sub>2</sub>) / pooled SD
where:
x<sub>1</sub> = mean of group 1
x<sub>2</sub> = mean of group 2
pooled SD = √(s<sub>1</sub><sup>2 </sup>+ s<sub>2</sub><sup>2</sup>) / 2
This tutorial explains how to calculate Cohen’s d in Excel.
<h3>Example: Cohen’s d in Excel</h3>
Perform the following steps to calculate Cohen’s d in Excel.
<b>Step 1: Enter the data.</b>
First, we will enter the values for the mean, standard deviation, and sample size (n) for two groups.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/cohenDExcel1.png">
<b>Step 2: Calculate the difference in means.</b>
Next, we will calculate the difference between the group means.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/cohenDExcel2.png">
<b>Step 3: Calculate the pooled standard deviation.</b>
Next, we will calculate the pooled standard deviation.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/cohenDExcel5.png">
<b>Step 4: Calculate Cohen’s d.</b>
Lastly, we will calculate Cohen’s d.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/cohenDExcel6.png">
Cohen’s d turns out to be <b>0.29851</b> for this example.
<h3>How to Interpret Cohen’s d</h3>
As a rule of thumb, here is how to interpret Cohen’s d:
<b>0.2 </b>= Small effect size
<b>0.5</b> = Medium effect size
<b>0.8 </b>= Large effect size
In our example, an effect size of <b>0.29851 </b>would likely be considered a small effect size.
This means that even if the difference between the two group means is statistically significantly different, the actual difference between the group means is trivial.
<h2><span class="orange">How to Calculate Cohen’s d in R (With Example)</span></h2>
In statistics, we often use  p-values  to determine if there is a statistically significant difference between the mean of two groups.
However, while a p-value can tell us whether or not there is a statistically significant difference between two groups, an  effect size  can tell us how large this difference actually is.
One of the most common measurements of effect size is <b>Cohen’s d</b>, which is calculated as:
Cohen’s d = (x<sub>1</sub> – x<sub>2</sub>) / √(s<sub>1</sub><sup>2 </sup>+ s<sub>2</sub><sup>2</sup>) / 2
where:
x<sub>1</sub> , x<sub>2</sub>: mean of sample 1 and sample 2, respectively
s<sub>1</sub><sup>2</sup>, s<sub>2</sub><sup>2</sup>: variance of sample 1 and sample 2, respectively
Using this formula, here is how we interpret Cohen’s d:
A <em>d </em>of <b>0.5</b> indicates that the two group means differ by 0.5 standard deviations.
A <em>d </em>of <b>1</b> indicates that the group means differ by 1 standard deviation.
A <em>d</em> of <b>2</b> indicates that the group means differ by 2 standard deviations.
And so on.
Here’s another way to interpret cohen’s d: An effect size of 0.5 means the value of the average person in group 1 is 0.5 standard deviations above the average person in group 2.
We often use the following rule of thumb when interpreting Cohen’s d:
A value of <b>0.2</b> represents a small effect size.
A value of <b>0.5</b> represents a medium effect size.
A value of <b>0.8</b> represents a large effect size.
The following example shows how to calculate Cohen’s d in R.
<h3>Example: How to Calculate Cohen’s d in R</h3>
Suppose a botanist applies two different fertilizers to plants to determine if there is a significant difference in average plant growth (in inches) after one month.
There are two methods we can use to quickly calculate Cohen’s d in R:
<b>Method 1: Use lsr Package</b>
<b>library(lsr)
#define plant growth values for each group
group1 &lt;- c(8, 9, 11, 11, 12, 14, 15, 16, 16, 18, 20, 21)
group2 &lt;- c(7, 9, 10, 10, 11, 11, 12, 14, 14, 16, 20, 23)
#calculate Cohen's d
cohensD(group1, group2)
[1] 0.2635333
</b>
<b>Method 2: Use effsize Package</b>
<b>library(effsize)
#define plant growth values for each group
group1 &lt;- c(8, 9, 11, 11, 12, 14, 15, 16, 16, 18, 20, 21)
group2 &lt;- c(7, 9, 10, 10, 11, 11, 12, 14, 14, 16, 20, 23)
#calculate Cohen's d
cohen.d(group1, group2)
Cohen's d
d estimate: 0.2635333 (small)
95 percent confidence interval:
     lower      upper 
-0.5867889  1.1138555 </b>
Notice that both methods produce the same result: Cohen’s d is <b>0.2635</b>.
We interpret this to mean that the average height of plants that received fertilizer #1 is <b>0.2635</b> standard deviations greater than the average height of plants that received fertilizer #2.
Using the rule of thumb mentioned earlier, we would interpret this to be a small effect size.
In other words, whether or not there is a statistically significant difference in the mean plant growth between the two fertilizers, the actual difference between the group means is trivial.
<h2><span class="orange">Cohen’s Kappa Calculator</span></h2>
<b> Cohen’s kappa </b> measures the level of agreement between two raters or judges who each classify items into mutually exclusive categories. The formula for Cohen’s kappa is calculated as:
k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)
where:
p<sub>o</sub>: Relative observed agreement among raters 
p<sub>e</sub>:  Hypothetical probability of chance agreement
To find Cohen’s kappa between two raters, simply fill in the boxes below and then click the “Calculate” button.
<label for="bothyes"><b>Both raters said ‘Yes’</b></label>
<input type="number" id="bothyes" value="25">
<label for="bothno"><b>Both raters saids ‘No’</b></label>
<input type="number" id="bothno" value="20">
<label for="yes1"><b>Only the first rater said ‘Yes’</b></label>
<input type="number" id="yes1" value="10">
<label for="yes2"><b>Only the second rater said ‘Yes’</b></label>
<input type="number" id="yes2" value="15">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Cohen’s Kappa: </b> 0.2857
<script>
function calc() {
//get input values
var bothyes  = document.getElementById('bothyes').value*1;
var bothno  = document.getElementById('bothno').value*1;
var yes1  = document.getElementById('yes1').value*1;
var yes2  = document.getElementById('yes2').value*1;
//calculate stuff
var n = bothyes-(-1*bothno)-(-1*yes1)-(-1*yes2)
var po = (bothyes-(-1*bothno))/n;
var pe_1 = ((bothyes-(-1*yes1))/n) * ((bothyes-(-1*yes2))/n);
var pe_2 =  ((n-yes1-bothyes)/n)*((n-yes2-bothyes)/n);
var pe = pe_1 - (-1*pe_2);
var k = (po-pe)/(1-pe);
//output
document.getElementById('k').innerHTML = k.toFixed(4);
}
</script>
<h2><span class="orange">How to Calculate Cohen’s Kappa in Excel</span></h2>
<b>Cohen’s Kappa</b> is used to measure the level of agreement between two raters or judges who each classify items into mutually exclusive categories.
The formula for Cohen’s kappa is calculated as:
<b>k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)</b>
where:
<b>p<sub>o</sub>:</b> Relative observed agreement among raters
<b>p<sub>e</sub>:</b> Hypothetical probability of chance agreement
Rather than just calculating the percentage of items that the raters agree on, Cohen’s Kappa attempts to account for the fact that the raters may happen to agree on some items purely by chance.
The value for Cohen’s Kappa always ranges between 0 and 1, with 0 indicating no agreement between the two raters and 1 indicating perfect agreement between the two raters.
The following table summarizes how to interpret different values for Cohen’s Kappa:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/kappa1.png">
The following example shows how to calculate Cohen’s Kappa in Excel.
<h3>Example: Calculating Cohen’s Kappa in Excel</h3>
Suppose two art museum curators are asked to rate 70 paintings on whether they’re good enough to be shown in a new exhibit.
The following 2×2 table shows the results of the ratings:
<b><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/cohenExcel1.jpg"507"></b>
The following screenshot shows how to calculate Cohen’s Kappa for the two raters, including the formulas used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/cohenExcel2.jpg">
The <b>p<sub>0</sub></b> value represents the relative agreement between the raters. This is the proportion of total ratings that the raters both said “Yes” or both said “No” on. 
This turns out to be <b>0.6429</b>.
The <b>p<sub>e</sub></b> value represents the probability that the raters could have agreed purely by chance. 
This turns out to be <b>0.5</b>.
The <b>k</b> value represents Cohen’s Kappa, which is calculated as:
k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)
k = (0.6429 – 0.5) / (1 – 0.5)
k = <b>0.2857</b>
Cohen’s Kappa turns out to be <b>0.2857</b>.
Based on the table from earlier, we would say that the two raters only had a “fair” level of agreement.
<h2><span class="orange">How to Calculate Cohen’s Kappa in R</span></h2>
In statistics,  Cohen’s Kappa  is used to measure the level of agreement between two raters or judges who each classify items into mutually exclusive categories.
The formula for Cohen’s kappa is calculated as:
<b>k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)</b>
where:
<b>p<sub>o</sub>:</b> Relative observed agreement among raters
<b>p<sub>e</sub>:</b> Hypothetical probability of chance agreement
Rather than just calculating the percentage of items that the raters agree on, Cohen’s Kappa attempts to account for the fact that the raters may happen to agree on some items purely by chance.
The value for Cohen’s Kappa always ranges between 0 and 1where:
<b>0 </b>indicates no agreement between the two raters
<b>1</b> indicates perfect agreement between the two raters
The following table summarizes how to interpret different values for Cohen’s Kappa:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/kappa1.png">
The easiest way to calculate Cohen’s Kappa in R is by using the <b>cohen.kappa()</b> function from the <b>psych</b> package.
The following example shows how to use this function in practice.
<h2>Example: Calculating Cohen’s Kappa in R</h2>
Suppose two art museum curators are asked to rate 15 paintings on whether they’re good enough to be shown in a new exhibit.
The following code shows how to use the <b>cohen.kappa()</b> function from the <b>psych </b>package to calculate Cohen’s Kappa for the two raters:
<b>library(psych)
#define vector of ratings for both raters
rater1 = [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0]
rater2 = [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0]
#calculate Cohen's Kappa
cohen.kappa(x=cbind(rater1,rater2))
Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries  lower estimate upper
unweighted kappa -0.14     0.34  0.81
weighted kappa   -0.14     0.34  0.81
 Number of subjects = 15 
</b>
The <b>estimate</b> column displays the value for Cohen’s Kappa.
From the output we can see that Cohen’s Kappa turns out to be <b>0.34</b>.
Based on the table from earlier, we would say that the two raters only had a “fair” level of agreement.
If you want to calculate the level of agreement between three or more raters, it’s recommended to use Fleiss’ Kappa instead.
<h2>Additional Resources</h2>
The following tutorials offer additional resources on Cohen’s Kappa:
 Introduction to Cohen’s Kappa 
 Online Cohen’s Kappa Calculator 
 How to Calculate Cohen’s Kappa in Excel 
 How to Calculate Cohen’s Kappa in Python 
<h2><span class="orange">How to Calculate Cohen’s Kappa in Python</span></h2>
In statistics,  Cohen’s Kappa  is used to measure the level of agreement between two raters or judges who each classify items into mutually exclusive categories.
The formula for Cohen’s kappa is calculated as:
<b>k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)</b>
where:
<b>p<sub>o</sub>:</b> Relative observed agreement among raters
<b>p<sub>e</sub>:</b> Hypothetical probability of chance agreement
Rather than just calculating the percentage of items that the raters agree on, Cohen’s Kappa attempts to account for the fact that the raters may happen to agree on some items purely by chance.
The value for Cohen’s Kappa always ranges between 0 and 1where:
<b>0 </b>indicates no agreement between the two raters
<b>1</b> indicates perfect agreement between the two raters
The following table summarizes how to interpret different values for Cohen’s Kappa:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/kappa1.png">
The following example shows how to calculate Cohen’s Kappa in Python.
<h3>Example: Calculating Cohen’s Kappa in Python</h3>
Suppose two art museum curators are asked to rate 15 paintings on whether they’re good enough to be shown in a new exhibit.
The following code shows how to use the <b>cohen_kappa_score()</b> function from the <b>sklearn</b> library to calculate Cohen’s Kappa for the two raters:
<b>from sklearn.metrics import cohen_kappa_score
#define array of ratings for both raters
rater1 = [0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0]
rater2 = [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0]
#calculate Cohen's Kappa
cohen_kappa_score(rater1, rater2)
0.33628318584070793
</b>
Cohen’s Kappa turns out to be <b>0.33628</b>.
Based on the table from earlier, we would say that the two raters only had a “fair” level of agreement.
If you want to calculate the level of agreement between three or more raters, it’s recommended to use Fleiss’ Kappa instead.
You can use the  fleiss_kappa()  function from the <b>statsmodels</b> library to calculate this metric.
<b>Note</b>: You can find the complete documentation for the <b>cohen_kappa_score()</b> function  here .
<h2><span class="orange">Cohen’s Kappa Statistic: Definition & Example</span></h2>
<b>Cohen’s Kappa Statistic</b> is used to measure the level of agreement between two raters or judges who each classify items into mutually exclusive categories.
The formula for Cohen’s kappa is calculated as:
<b>k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)</b>
where:
<b>p<sub>o</sub>:</b> Relative observed agreement among raters
<b>p<sub>e</sub>:</b> Hypothetical probability of chance agreement
Rather than just calculating the percentage of items that the raters agree on, Cohen’s Kappa attempts to account for the fact that the raters may happen to agree on some items purely by chance.
<h3>How to Interpret Cohen’s Kappa</h3>
Cohen’s Kappa always ranges between 0 and 1, with 0 indicating no agreement between the two raters and 1 indicating perfect agreement between the two raters.
The following table summarizes how to interpret different values for Cohen’s Kappa:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/kappa1.png">
The following step-by-step example shows how to calculate Cohen’s Kappa by hand.
<h3>Calculating Cohen’s Kappa: Step-by-Step Example</h3>
Suppose two museum curators are asked to rate 70 paintings on whether they’re good enough to be hung in a new exhibit.
The following 2×2 table shows the results of the ratings:
<b><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/kappa2.png"></b>
<b>Step 1: Calculate relative agreement (p<sub>o</sub>) between raters.</b>
First, we’ll calculate the relative agreement between the raters. This is simply the proportion of total ratings that the raters both said “Yes” or both said “No” on. 
We can calculate this as:
p<sub>o</sub> = (Both said Yes + Both said No) / (Total Ratings)
p<sub>o</sub> = (25 + 20) / (70) = <b>0.6429</b>
<b>Step 2: Calculate the hypothetical probability of chance agreement (p<sub>e</sub>) between raters.</b>
Next, we’ll calculate the probability that the raters could have agreed purely by chance.
This is calculated as the total number of times that Rater 1 said “Yes” divided by the total number of responses, multiplied by the total number of times that Rater 2 said “Yes” divided by the total number of responses, added to the total number of times that Rater 1 said “No” multiplied by the total number of times that Rater 2 said “No.”
For our example, this is calculated as:
P(“Yes”) = ((25+10)/70) * ((25+15)/70) = 0.285714
P(“No”) = ((15+20)/70) * ((10+20)/70) = 0.214285
p<sub>e</sub> = 0.285714 + 0.214285 = <b>0.5</b>
<b>Step 3: Calculate Cohen’s Kappa</b>
Lastly, we’ll use p<sub>o</sub> and p<sub>e</sub> to calculate Cohen’s Kappa:
k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)
k = (0.6429 – 0.5) / (1 – 0.5)
k = 0.2857
Cohen’s Kappa turns out to be <b>0.2857</b>. Based on the table from earlier, we would say that the two raters only had a “fair” level of agreement.
<h2><span class="orange">How to Use colClasses to Quickly Import Data in R</span></h2>
You can use the <b>colClasses</b> argument when importing a file into R to specify the classes of each column:
<b>df &lt;- read.csv('my_data.csv',
               colClasses=c('character', 'numeric', 'numeric'))
</b>
The benefit of using <b>colClasses</b> is that you can import data much faster, especially when the files are extremely large.
The following example shows how to use this argument in practice.
<h3>Example: Use colClasses When Importing Files</h3>
Suppose I have some CSV file called <b>my_data.csv</b> with three columns that I’d like to import into R:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/col1.png">
I can use the following syntax to do so:
<b>#import CSV file
df &lt;- read.csv('my_data.csv',
               colClasses=c('character', 'numeric', 'numeric'))
#view class of each column in data frame
str(df)
'data.frame':14 obs. of  3 variables:
 $ team    : chr  "Mavs" "Spurs" "Hornets" "Rockets" ...
 $ points  : num  91 99 104 103 105 88 89 93 96 99 ...
 $ rebounds: num  33 23 26 25 25 26 29 30 34 23 ...
</b>
Note that the number of values in the <b>colClasses</b> argument should match the number of columns in the data frame.
For example, if you only supply one value to the <b>colClasses</b> argument then each column in the data frame will have the same class:
<b>#import CSV file
df &lt;- read.csv('my_data.csv',
               colClasses=c('character'))
#view class of each column in data frame
str(df)
'data.frame':14 obs. of  3 variables:
 $ team    : chr  "Mavs" "Spurs" "Hornets" "Rockets" ...
 $ points  : chr  "91" "99" "104" "103" ...
 $ rebounds: chr  "33" "23" "26" "25" ...</b>
Notice that each column in the resulting data frame has a “character” class since we only supplied one value to the <b>colClasses</b> argument.
 Note that you can specify the following potential classes in the <b>colClasses</b> argument:
<b>character</b>: “hey”, “there”, “world”
<b>complex</b>: as.complex(-1), 4i
<b>numeric</b>: as.integer(20), 3L
<b>integer</b>: 4, 12, 158
<b>logical</b>: TRUE, FALSE
<h2><span class="orange">Collectively Exhaustive Events: Definition & Example</span></h2>
A set of events is <b>collectively exhaustive</b> if at least one of the events <em>must</em> occur.
For example, if we roll a die then it must land on one of the following values:
1
2
3
4
5
6
Thus, we would say that the set of events <b>{1, 2, 3, 4, 5, 6}</b> is <b>collectively exhaustive</b> because the die <em>must</em> land on one of those values.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/collectively_exhaustive.png">
In other words, that set of events, as a <em>collection</em>, <em>exhausts</em> all possible outcomes.
The following examples show some more situations that illustrate collectively exhaustive events:
<h3>Example 1: Flipping a Coin</h3>
Suppose we flip a coin one time. We know that the coin must land on one of the following values:
Heads
Tails
Thus, the set of events <b>{Heads, Tails}</b> would be collectively exhaustive.
<h3>Example 2: Spinning a Spinner</h3>
Suppose we have a spinner that has three different colors: red, blue and green.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/spinner.png">
If we spin it one time then it must land on one of the following values:
Red
Blue
Green
Thus, the set of events <b>{Red, Blue, Green}</b> would be collectively exhaustive.
However, the set of events <b>{Red, Green}</b> would <em>not</em> be collectively exhaustive because it does not contain all possible outcomes.
<h3>Example 3: Types of Basketball Players</h3>
Suppose we have a survey that asks individuals to select their favorite basketball player position. The only potential responses are:
Point Guard
Shooting Guard
Small Forward
Power Forward
Center
Thus, the set of events <b>{Point Guard, Shooting Guard, Small Forward, Power Forward, Center}</b> would be collectively exhaustive.
However, the set of events <b>{Point Guard, Shooting Guard, Small Forward}</b> would <em>not</em> be collectively exhaustive because it does not contain all possible outcomes.
<h3>The Importance of Collectively Exhaustive Events in Surveys</h3>
When designing surveys, it’s particularly important that the responses to the questions are collectively exhaustive. 
For example, suppose a survey asks the following question:
<em><b>What is you favorite basketball player position?</b></em>
And suppose the potential responses were:
Point Guard
Shooting Guard
Small Forward
Power Forward
Since the position <em>Center</em> was left out, these responses are not collectively exhaustive.
This means that someone who prefers <em>Center</em> as their favorite position will have to pick one of the other options, which means the responses to the survey won’t reflect the true opinions of the respondents.
<h3>Collectively Exhaustive vs. Mutually Exclusive</h3>
Events are  mutually exclusive  if they cannot occur at the same time.
For example, let event A be the event that a die lands on an even number and let event B be the event that a die lands on an odd number.
We would define the  sample space  for the events as follows:
A = {2, 4, 6}
B = {1, 3, 5}
Notice that there is no overlap between the two sample spaces, which means they’re mutually exclusive. They also happen to be collectively exhaustive because combined they’re able to account for all the potential outcomes of the die roll.
However, suppose we define event A and event B as follows:
A = {1, 2, 3, 4}
B = {3, 4, 5, 6}
In this case, there is some overlap between A and B so they are not mutually exclusive. However, combined they’re still able to account for all the potential outcomes of the die roll.
This illustrates an important point: <b>A set of events can be collectively exhaustive without being mutually exclusive</b>.
<h2><span class="orange">How to Use colMeans() Function in R</span></h2>
The <b>colMeans()</b> function in R can be used to calculate the mean of several columns of a matrix or data frame in R.
This function uses the following basic syntax:
<b>#calculate column means of every column
colMeans(df)
#calculate column means and exclude NA values
colMeans(df, na.rm=T)
#calculate column means of specific columns
colMeans(df[c('col1', 'col3', 'col4')])</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculate Mean of Every Column</h3>
The following code shows how to calculate the mean of every column in a data frame:
<b>#create data frame
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28), blocks=c(1, 4, 11, 0, 2))
#calculate column means
colMeans(df)
  points  assists rebounds   blocks 
    91.8     33.0     26.8      3.6 </b>
<h3>
<b>Example 2: Calculate Mean of Every Column & Exclude NA’s</b>
</h3>
The following code shows how to calculate the mean of every column and exclude NA values:
<b>#create data frame with some NA values
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, NA, 31, 39, 34), rebounds=c(30, 28, NA, NA, 28), blocks=c(1, 4, 11, 0, 2))
#calculate column means
colMeans(df, na.rm=T)
  points  assists rebounds   blocks 
91.80000 34.25000 28.66667  3.60000</b>
<h3>Example 3: Calculate Mean of Specific Columns</h3>
The following code shows how to calculate the mean values of specific columns in the data frame:
<b>#create data frame
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28), blocks=c(1, 4, 11, 0, 2))
#calculate column means for 'points' and 'blocks' columns
colMeans(df[c('points', 'blocks')])
points blocks 
  91.8    3.6 </b>
Note that we can also use index values to calculate the mean of specific columns:
<b>#create data frame
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28), blocks=c(1, 4, 11, 0, 2))
#calculate column means for columns in position 1 and 4
colMeans(df[c(1, 4)])
points blocks 
  91.8    3.6 </b>
<h2><span class="orange">How to Assign Colors by Factor in ggplot2 (With Examples)</span></h2>
Often you may want to assign colors to points in a ggplot2 plot based on some categorical variable. 
Fortunately this is easy to do using the following syntax:
<b>ggplot(df, aes(x=x_variable, y=y_variable, color=color_variable)) +
  geom_point()
</b>
This tutorial provides several examples of how to use this syntax in practice using the built-in R dataset titled <b>iris</b>:
<b>#view first six rows of iris dataset
head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
</b>
<h3>Example 1: Use Default Colors</h3>
The following code shows how to assign default colors to the points in a ggplot2 plot based on the factor variable <em>Species</em>:
<b>library(ggplot2)
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/colorgg1.png">
Since we didn’t specify a color scale or a list of custom colors, ggplot2 simply assigned a list of default red, green, and blue colors to the points.
<h3>Example 2: Use Custom Colors</h3>
The following code shows how to assign custom colors to the points in a ggplot2 plot by using <b>scale_color_manual()</b>:
<b>library(ggplot2)
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() +
  scale_color_manual(values = c("setosa" = "purple",                "versicolor="orange",                "virginica"="steelblue")) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/colorgg2.png">
Note that we could have used hex color codes as well to specify the colors.
<h3>Example 3: Use a Custom Color Scale</h3>
The following code shows how to assign custom colors to the points in a ggplot2 plot by using a custom color scale from the  RColorBrewer  package:
<b>library(ggplot2)
library(RColorBrewer)
#define custom color scale
myColors &lt;- brewer.pal(3, "Spectral")
names(myColors) &lt;- levels(iris$Species)
custom_colors &lt;- scale_colour_manual(name = "Species Names", values = myColors)
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point() +
  custom_colors
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/colorgg3.png">
<h2><span class="orange">How to Use colSums() Function in R</span></h2>
The <b>colSums()</b> function in R can be used to calculate the sum of the values in each column of a matrix or data frame in R.
This function uses the following basic syntax:
<b>colSums(x, na.rm=FALSE)</b>
where:
<b>x</b>: Name of the matrix or data frame.
<b>na.rm</b>: Whether to ignore NA values. Default is FALSE.
The following examples show how to use this function in practice.
<h3>Example 1: Use colSums() with Data Frame</h3>
The following code shows how to use <b>colSums()</b> to find the sum of the values in each column of a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 2, 5, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 14, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    2    3    1
3    3    5    6    2
4    4    3    6   14
5    5    2    8    9
#find sum of each column
colSums(df)
var1 var2 var3 var4 
  16   19   26   27 
</b>
Here’s how to interpret the output:
The sum of values in the ‘var1’ column is <b>16</b>.
The sum of values in the ‘var2’ column is <b>19</b>.
The sum of values in the ‘var3’ column is <b>26</b>.
The sum of values in the ‘var4’ column is <b>27</b>.
<h3>Example 2: Use colSums() with NA Values in Data Frame</h3>
The following code shows how to use <b>colSums()</b> to find the sum of the values in each column of a data frame when there are NA values in some columns:
<b>#create data frame with some NA values
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, NA, NA, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, NA, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3   NA    3    1
3    3   NA    6    2
4    4    3    6   NA
5    5    2    8    9
#find sum of each column
colSums(df, na.rm=TRUE)
var1 var2 var3 var4 
  16   12   26   13 </b>
<h3>Example 3: Use colSums() with Specific Columns</h3>
The following code shows how to use <b>colSums()</b> to find the sum of the values in specific columns of a data frame:
<b>#create data frame with some NA values
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, NA, NA, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, NA, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3   NA    3    1
3    3   NA    6    2
4    4    3    6   NA
5    5    2    8    9
#find sum of columns 1, 3, and 4
colSums(df[, c(1, 3, 4)], na.rm=TRUE)
var1 var3 var4 
  16   26   13</b>
<h2><span class="orange">How to Fix: columns overlap but no suffix specified</span></h2>
One error you may encounter when using pandas is:
<b>ValueError: columns overlap but no suffix specified: Index(['column'], dtype='object')
</b>
This error occurs when you attempt to join together two data frames that share at least one common column name and a suffix is not provided for either the left or right data frame to distinguish between the columns in the new data frame.
There are two ways to fix this error:
<b>Solution 1: Provide suffix names.</b>
<b>df1.join(df2, how = 'left', lsuffix='left', rsuffix='right')</b>
<b>Solution 2: Use the merge function instead.</b>
<b>df1.merge(df2, how = 'left')</b>
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to join together the following two data frames:
<b>import pandas as pd
#create first data frame
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F'],    'points': [5, 7, 7, 9, 12, 9],    'assists': [11, 8, 10, 6, 6, 5]})
#create second data frame
df2 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F'],    'rebounds': [4, 4, 6, 9, 13, 16],    'steals': [2, 2, 1, 4, 3, 2]})
#attempt to perform left join on data frames
df1.join(df2, how = 'left')
ValueError: columns overlap but no suffix specified: Index(['player'], dtype='object')
</b>
We receive an error because the two data frames both share the “player” column, but there is no suffix provided for either the left or right data frame to distinguish between the columns in the new data frame.
<h3>How to Fix the Error</h3>
One way to fix this error is to provide a suffix name for either the left or right data frame:
<b>#perform left join on data frames with suffix provided
df1.join(df2, how = 'left', lsuffix='left', rsuffix='right')
        playerleft points assists playerright reboundssteals
0A   5  11  A      4        2
1B   7  8  B      4        2
2C   7  10  C      6        1
3D   9  6  D      9        4
4E   12  6  E     13        3
5F   9  5  F     16        2
</b>
Another way to fix this error is to simply use the <b>merge()</b> function, which doesn’t encounter this problem when joining two data frames together:
<b>#merge two data frames
df1.merge(df2, how = 'left')
playerpointsassistsrebounds steals
0A5114 2
1B784 2
2C7106 1
3D969 4
4E12613 3
5F9516 2
</b>
Notice that the <b>merge()</b> function simply drops any names from the second data frame that already belong to the first data frame.
<h2><span class="orange">How to Calculate Combinations & Permutations in R</span></h2>
You can use the following functions to calculate combinations and permutations in R:
<b>#calculate total combinations of size <em>r</em> from <em>n</em> total objects
choose(n, r)
#calculate total permutations of size <em>r</em> from <em>n</em> total objects
choose(n, r) * factorial(r)
</b>
The following examples show how to use each of these functions in practice.
<h2>Example 1: Calculate Total Combinations</h2>
<b>Combinations</b> represent ways of selecting a sample from a group of objects in which the <em>order of the objects does not matter</em>.
For example, suppose we have a bag of four marbles: red, blue, green, and yellow. Suppose we’d like to select two marbles randomly from the bag, without replacement.
Here are the different combinations of marbles we could select:
{red, blue}
{red, green}
{red, yellow}
{blue, green}
{blue, yellow}
{green, yellow}
There are <b>6</b> total combinations.
Here is how to calculate the total number of combinations in R:
<b>#calculate total combinations of size <em>2</em> from <em>4</em> total objects
choose(4, 2)
[1] 6
</b>
Our answer matches the number of combinations that we calculated by hand.
<h2>Example 2: Calculate Total Permutations</h2>
<b>Permutations</b> represent ways of selecting a sample from a group of objects in which the <em>order of the objects does matter</em>.
For example, suppose we have a bag of four marbles: red, blue, green, and yellow.
Suppose we’d like to select two marbles randomly from the bag, without replacement.
Here are the different permutations of marbles we could select:
{red, blue}, {blue, red}
{red, green}, {green, red}
{red, yellow}, {yellow, red}
{blue, green}, {green, blue}
{blue, yellow}, {yellow, blue}
{green, yellow}, {yellow, green}
There are <b>12</b> total permutations.
Here is how to calculate the total number of permutations in R:
<b>#calculate total permutations of size <em>2</em> from <em>4</em> total objects
choose(4, 2) * factorial(2)
[1] 12
</b>
Our answer matches the number of permutations that we calculated by hand.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform Linear Interpolation in R (With Example) 
 How to Select Unique Rows in a Data Frame in R 
 How to Replicate Rows in Data Frame in R 
<h2><span class="orange">How to Combine a List of Matrices in R</span></h2>
You can use the following methods to combine a list of matrices in R:
<b>Method 1: Combine List of Matrices by Rows</b>
<b>do.call(rbind, list_of_matrices)
</b>
<b>Method 2: Combine List of Matrices by Columns</b>
<b>do.call(cbind, list_of_matrices)
</b>
The following examples show how to use each method in practice with the following two matrices in R:
<b>#define matrices
matrix1 &lt;- matrix(1:6, nrow=3)
matrix2 &lt;- matrix(7:12, nrow=3)
#view first matrix
matrix1
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
#view second matrix 
matrix2
     [,1] [,2]
[1,]    7   10
[2,]    8   11
[3,]    9   12</b>
<h2>Example 1: Combine List of Matrices by Rows</h2>
The following code shows how to use the  rbind  function to combine a list of matrices by rows:
<b>#create list of matrices
matrix_list &lt;- list(matrix1, matrix2)
#combine into one matrix by rows
do.call(rbind, matrix_list)
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
[4,]    7   10
[5,]    8   11
[6,]    9   12</b>
The two matrices have been combined into a single matrix by rows. 
<h2>Example 2: Combine List of Matrices by Columns</h2>
The following code shows how to use the  cbind  function to combine a list of matrices by columns:
<b>#create list of matrices
matrix_list &lt;- list(matrix1, matrix2)
#combine into one matrix by columns
do.call(cbind, matrix_list)
     [,1] [,2] [,3] [,4]
[1,]    1    4    7   10
[2,]    2    5    8   11
[3,]    3    6    9   12</b>
The two matrices have been combined into a single matrix by columns.
<b>Related:</b>  An Introduction to do.call in R 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Create an Empty Matrix in R 
 How to Convert Data Frame to Matrix in R 
 How to Convert List to Matrix in R 
<h2><span class="orange">How to Combine Lists in R (With Examples)</span></h2>
You can use either the <b>c()</b> function or the <b>append()</b> function to combine two or more lists in R:
<b>#combine two lists using c()
combined &lt;- c(list1, list2)
#combine two lists using append()
combined &lt;- append(list1, list2)</b>
Both functions will produce the same result.
The following examples show how to use this syntax in practice.
<h3>Example 1: Combine Two Lists</h3>
The following code shows how to combine two lists in R:
<b>#define lists
list1 &lt;- list(2, 5, 6, 8)
list2 &lt;- list(A = 1:5, B = 3)
#combine two lists into one
combined &lt;- c(list1, list2)
#view combined list
combined
[[1]]
[1] 2
[[2]]
[1] 5
[[3]]
[1] 6
[[4]]
[1] 8
$A
[1] 1 2 3 4 5
$B
[1] 3
</b>
We can also use the <b>length()</b> function to get the length of the combined list:
<b>#get length of combined list
length(combined)
[1] 6
</b>
We can also use the <b>class()</b> function to get the class of the combined list:
<b>#get class of combined list
class(combined)
[1] "list"
</b>
<h3>Example 2: Combine More Than Two Lists</h3>
We can use similar syntax to combine more than two lists in R:
<b>#define lists
list1 &lt;- list(2, 5, 6, 8)
list2 &lt;- list(A = 1:5, B = 3)
list3 &lt;- list(X = 'A', Y = 'B')
#combine three lists into one
combined &lt;- c(list1, list2, list3)
#view combined list
combined
[[1]]
[1] 2
[[2]]
[1] 5
[[3]]
[1] 6
[[4]]
[1] 8
$A
[1] 1 2 3 4 5
$B
[1] 3
$X
[1] "A"
$Y
[1] "B"</b>
<h2><span class="orange">How to Combine Multiple Excel Sheets in Pandas</span></h2>
Often you may want to import and combine multiple Excel sheets into a single pandas DataFrame.
For example, suppose you have the following Excel workbook called <b>data.xlsx</b> with three different sheets that all contain two columns of data about basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/combinepd1.png">
We can easily import and combine each sheet into a single pandas DataFrame using the pandas functions <b>concat() </b>and <b>read_excel()</b>, but first we need to make sure that <b>xlrd </b>is installed:
<b>pip install xlrd
</b>
Once this is installed, we can use the following code to import and combine these three sheets into one pandas DataFrame:
<b>#load pandas library
import pandas as pd
#import and combine the three sheets into one pandas DataFrame
df = pd.concat(pd.read_excel('data.xlsx', sheet_name=None), ignore_index=True)
#view DataFrame
df
        playerpoints
0A12
1B5
2C13
3D17
4E27
5F24
6G26
7H27
8I27
9J12
10K9
11L5
12M5
13N13
14O17
</b>
<h3>How This Code Works</h3>
There are only two pieces to understanding how this single line of code is able to import and combine multiple Excel sheets:
<b>1. Read in all sheets.</b>
<b>pd.read_excel('data.xlsx', sheet_name=None)</b>
This chunk of code reads in all sheets of an Excel workbook. By default, the <b>read_excel()</b> function only reads in the first sheet, but through specifying <b>sheet_name=None</b> we are able to read in every single sheet in the Excel workbook.
<b>2. Concatenate all sheets.</b>
<b>pd.concat(<em>DataFrames to concatenate</em>, ignore_index=True)</b>
This chunk of code simply concatenates all of the DataFrames from each Excel sheet into one single pandas DataFrame. By specifying <b>ignore_index=True</b>, we’re telling pandas that the names of the individual sheets are not important.
Note that this code only works if each of the Excel sheets has the same format. In this example, each sheet had two columns of data and each column had the same name, which is why this single line of code worked so easily to combine each of the Excel sheets into one pandas DataFrame.
<h2><span class="orange">How to Combine Two Vectors in R (With Examples)</span></h2>
You can use one of the following methods to combine two vectors in R:
<b>Method 1: Combine Two Vectors Into One Vector</b>
<b>new_vector &lt;- c(vector1, vector2)</b>
<b>Method 2: Combine Two Vectors Into a Matrix</b>
<b>new_matrix &lt;- cbind(vector1, vector2)</b>
<b>Method 3: Combine Two Vectors Into a Data Frame</b>
<b>new_df &lt;- data.frame(vector1, vector2)</b>
The following examples show how to use each method in practice.
<h3>Method 1: Combine Two Vectors Into One Vector</h3>
The following code shows how to combine two vectors into one new vector:
<b>#define vectors
vector1 &lt;- c(1, 2, 3, 4, 5)
vector2 &lt;- c(6, 7, 8, 9, 10)
#combine two vectors into one vector
new_vector &lt;- c(vector1, vector2)
#view resulting vector
new_vector
[1]  1  2  3  4  5  6  7  8  9 10
</b>
<h3>Method 2: Combine Two Vectors Into a Matrix</h3>
The following code shows how to combine two vectors into a matrix:
<b>#define vectors
vector1 &lt;- c(1, 2, 3, 4, 5)
vector2 &lt;- c(6, 7, 8, 9, 10)
#combine two vectors into matrix
new_matrix &lt;- cbind(vector1, vector2)
#view resulting matrix
new_matrix
     vector1 vector2
[1,]       1       6
[2,]       2       7
[3,]       3       8
[4,]       4       9
[5,]       5      10
</b>
<b>Related:</b>  How to Use cbind in R (With Examples) 
<h3>Method 3: Combine Two Vectors Into a Data Frame</h3>
The following code shows how to combine two vectors into a data frame:
<b>#define vectors
vector1 &lt;- c(1, 2, 3, 4, 5)
vector2 &lt;- c(6, 7, 8, 9, 10)
#combine two vectors into data frame
new_df &lt;- data.frame(vector1, vector2)
#view resulting data frame
new_df
  vector1 vector2
1       1       6
2       2       7
3       3       8
4       4       9
5       5      10</b>
Notice that each original vector is now a unique column in the resulting data frame.
<h2><span class="orange">How to Compare Box Plots (With Examples)</span></h2>
A <b>box plot</b> is a type of plot that displays the five number summary of a dataset, which includes:
The minimum value
The first quartile (the 25th percentile)
The median value
The third quartile (the 75th percentile)
The maximum value
To make a box plot, we draw a box from the first to the third quartile. Then we draw a vertical line at the median. Lastly, we draw “whiskers” from the quartiles to the minimum and maximum value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/boxplot.png">
Box plots are useful because they allow us to gain a quick understanding of the distribution of values in a dataset. They’re also useful for comparing two different datasets.
When comparing two or more box plots, we can answer four different questions:
<b>1. How do the median values compare? </b>We can compare the vertical line in each box to determine which dataset has a higher median value.
<b>2. How does the dispersion compare?</b> We can compare the length of each box (which represents the distance between Q1 and Q3 – the interquartile range) to determine which dataset is more spread out.
<b>3. How does the skewness compare?</b> The closer the vertical line is to Q1, the more positively skewed the dataset. The closer the vertical line is to Q3, the more negatively skewed the dataset.
<b>4. Are outliers present?</b> In box plots, outliers are typically represented by tiny circles that extend beyond either whisker. An observation is defined to be an outlier if it meets one of the following criteria:
An observation is less than Q1 – 1.5*IQR
An observation is greater than Q3 + 1.5*IQR
The following example shows how to compare two different box plots and answer these four questions.
<h3>Example: Comparing Box Plots</h3>
The following datasets display the exam scores for students who used one of two studying techniques to prepare for the exam:
<b>Method 1:</b> 78, 78, 79, 80, 80, 82, 82, 83, 83, 86, 86, 86, 86, 87, 87, 87, 88, 88, 88, 91
<b>Method 2:</b> 66, 66, 66, 67, 68, 70, 72, 75, 75, 78, 82, 83, 86, 88, 89, 90, 93, 94, 95, 98
If we create box plots for each dataset, here’s what they would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/compareBox1.png">
We can compare these two box plots and answer the following four questions:
<b>1. How do the median values compare? </b>The line in the middle of the box plot for Study Method 1 is higher than the line for Study Method 2, which indicates that the students who used Study Method 1 had a higher median exam score.
<b>2. How does the dispersion compare?</b> The box plot for Study Method 2 is much longer than Study Method 1, which indicates that the exam scores are much more spread out among students who used Study Method 2.
<b>3. How does the skewness compare?</b> The line in the middle of the box plot for Study Method 1 is close to Q3, which indicates that the distribution of exam scores for students who used Study Method 1 is negatively skewed. Conversely, the line in the middle of the box plot for Study Method 2 is near the center of the box, which means the distribution of scores has little skew at all.
<b>4. Are outliers present?</b> Neither box plot has tiny circles that extend beyond the top or bottom whiskers, which means neither dataset had any clear outliers.
<h2><span class="orange">How to Compare Two Excel Sheets for Differences</span></h2>
Occasionally you may want to compare two different Excel sheets to identify the differences between them.
Fortunately this is fairly easy to do and this tutorial explains how.
<h3>How to Identify Differences Between Two Excel Sheets</h3>
Suppose we have the following two sheets in Excel with some information about basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel1.png"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel2.png">
 
 
 
 
 
 
 
To compare the differences between the two sheets, we can create a third sheet and use the following formula in cell <b>A2:</b>
<b>=IF(Sheet1!A1 &lt;> Sheet2!A1, "Sheet1:"&Sheet1!A1&", Sheet2:"&Sheet2!A1, "")
</b>
We can then copy this formula to each cell, which results in the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel3.png">
If the corresponding cells in Sheet1 and Sheet2 are identical, then the cell in Sheet3 will be blank. However, if the cells are different between the two sheets then the differences will be shown in Sheet3.
For example, cell A9 in the first sheet has a value of <b>G </b>while cell A9 in the second sheet has a value of <b>X</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel4.png">
<h3>How to Highlight Differences Between Two Excel Sheets</h3>
In addition to identifying the differences between the two sheets, you can also highlight the differences using conditional formatting.
For example, suppose we want to highlight each cell in Sheet2 that has a different value from the corresponding cell in Sheet1. To do this, we can use the following steps:
<b>Step 1: Select the range of cells.</b>
First, select the entire range of cells that we’re interested in applying conditional formatting to:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel5.png">
<b>Step 2: Choose conditional formatting.</b>
Next, on the <b>Home </b>tab within the <b>Styles </b>group, click <b>Conditional Formatting </b>and then click <b>New Rule</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel6.png">
<b>Step 3: Choose conditional formatting.</b>
Choose the option titled <b>Use a formula to determine which cells to format</b>. Then type in the following formula:
<b>=A1&lt;>Sheet1!A1</b>
Then click <b>Format </b>and choose a color you’d like to use to highlight the cells that are different. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel7-1.png">
Once you click <b>OK</b>, the cells in Sheet2 that have different values than the corresponding cells in Sheet1 will be highlighted:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/compareSheetsExcel8.png">
<em>You can find more Excel tutorials  here .</em>
<h2><span class="orange">How to Compare Histograms (With Examples)</span></h2>
A <b>histogram</b> is a type of chart that allows us to visualize the distribution of values in a dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape1.png">
The x-axis displays the values in the dataset and the y-axis shows the frequency of each value.
Histograms are useful because they allow us to gain a quick understanding of the distribution of values in a dataset. They’re also useful for comparing two different datasets.
When comparing two or more histograms, we can answer three different questions:
<b>1. How do the median values compare?</b>
We can roughly estimate the median to be located near the middle of each histogram, which allows us to compare the median values of the distributions.
<b>2. How does the dispersion compare?</b>
We can visually see which histogram is more spread out, which gives us an idea of which distribution has values that are more dispersed.
<b>3. How does the skewness compare?</b>
If a histogram has a “tail” on the left side of the plot, it is said to be negatively skewed. Conversely, if a histogram has a “tail” on the right side of the plot, it is said to be positively skewed. We can visually check each histogram to compare the  skewness .
The following example shows how to compare two different histograms and answer these three questions.
<h2>Example: Comparing Histograms</h2>
Suppose 200 students use one study method to prepare for an exam and another 200 students use a different study method to prepare for the same exam.
Suppose we create the following histograms to compare the exam scores for each group of students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compareHistograms2.jpg"652">
We can compare these histograms and answer the following three questions:
<b>1. How do the median values compare?</b>
Although we don’t know the exact median values of each distribution just by looking at the histograms, it’s obvious that the median exam score for students who used Method 1 is higher than the median exam score for students who used Method 2.
We might estimate that the median value for Method 1 is about 84 and the median value for Method 2 is about 78.
<b>2. How does the dispersion compare?</b>
The values in the histogram for Method 2 are much more spread out compared to the values for Method 1, which tells us that there is much greater dispersion in the exam scores for students who used Method 2.
<b>3. How does the skewness compare?</b>
From looking at the histograms, it appears that the distribution of exam scores for Method 1 is slightly right skewed, as indicated by the “tail” that extends to the right of the histogram.
There doesn’t appear to be any “tail” in the distribution of exam scores for Method 2, though, which tells us that the distribution has little to no skew.
<b>Bonus</b>: Here is the code that we used in R to create these two histograms:
<b>library(ggplot2)
#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(method=rep(c('Method 1', 'Method 2'), each=200), Score=c(rnorm(200, mean=84, sd=2),         rnorm(200, mean=78, sd=4)))
#create histogram of scores for each method
ggplot(df, aes(x=Score)) +
  geom_histogram(fill='steelblue', color='black') +
  facet_wrap(.~method, nrow=2) +
  labs(title='Exam Scores by Study Method')
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks with histograms:
 How to Estimate the Mean and Median of Any Histogram 
 How to Estimate the Standard Deviation of Any Histogram 
 How to Describe the Shape of Histograms 
<h2><span class="orange">How to Compare Strings in R (3 Examples)</span></h2>
You can use the following methods to compare strings in R:
<b>Method 1: Compare Two Strings</b>
<b>#case-sensitive comparison
string1 == string2
#case-insensitive comparison
tolower(string1) == tolower(string2)
</b>
<b>Method 2: Compare Two Vectors of Strings</b>
<b>#case-sensitive comparison
identical(vector1, vector2)
#case-insensitive comparison
identical(tolower(vector1), tolower(vector2))</b>
<b>Method 3: Find Similarities Between Two Vectors of Strings</b>
<b>#find which strings in vector1 are also in vector2
vector1[vector1 %in% vector2]  
</b>
The following examples show how to use each method in practice.
<h3>Example 1: Check if Two Vectors Are Identical</h3>
The following code shows how to compare two strings in R to determine if they’re equal:
<b>#define two strings
string1 &lt;- "Mavericks"
string2 &lt;- "mavericks"
#case-sensitive comparison
string1 == string2
[1] FALSE
#case-insensitive comparison
tolower(string1) == tolower(string2)
[1] TRUE
</b>
The case-sensitive comparison returns a value of <b>FALSE</b> since the two strings are not perfectly identical.
However, the case-insensitive comparison returns a value of <b>TRUE</b> since the two strings contain the same characters in the same order, regardless of case.
<h3>Example 2: Compare Two Vectors of Strings</h3>
The following code shows how to use the <b>identical()</b> function to determine if two vectors of strings are equal:
<b>#define two vectors of strings
vector1 &lt;- c("hey", "hello", "HI")
vector2 &lt;- c("hey", "hello", "hi")
#case-sensitive comparison
identical(vector1, vector2)
[1] FALSE
#case-insensitive comparison
identical(tolower(vector1), tolower(vector2))
[1] TRUE
</b>
The case-sensitive comparison returns a value of <b>FALSE</b> since the two vectors don’t contain the exact same strings in the same case.
However, the case-insensitive comparison returns a value of <b>TRUE</b> since the two vectors contain the same strings, regardless of case.
<h3>Example 3: Find Similarities Between Two Vectors of Strings</h3>
The following code shows how to use the <b>%in%</b> operator to find which strings in one vector belong to another vector:
<b>#define two vectors of strings
vector1 &lt;- c("hey", "hello", "greetings")
vector2 &lt;- c("hey", "hello", "hi")
#find which strings in vector1 are also in vector2
vector1[vector1 %in% vector2]
[1] "hey"   "hello"
</b>
From the output we can see that the strings “hey” and “hello” exist in both vector1 and vector2.
<b>Related:</b>  How to Use %in% Operator in R 
<h2><span class="orange">How to Compare Three Columns in Excel (With Example)</span></h2>
You can use the following basic formula to compare three columns in Excel:
<b>=IF(AND(B2=C2,C2=D2),"Equal","Not Equal")
</b>
This particular formula compares the values in cells <b>B2</b>, <b>C2</b>, and <b>D2</b>.
If all of the values are equal, the formula returns <b>Equal</b>. Otherwise, it returns <b>Not Equal</b>.
The following example shows how to use this formula in practice.
<h2>Example: Compare Three Columns in Excel</h2>
Suppose we have the following dataset that shows the highest scorer on various basketball teams during three different games:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree1.jpg"483">
We can type the following formula into cell <b>E2</b> to test if the three values in the first row are all equal:
<b>=IF(AND(B2=C2,C2=D2),"Equal","Not Equal")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree2.jpg">
The formula returns <b>Not Equal</b> since the three names in the first row don’t all match.
We can then drag this formula in cell <b>E2</b> down to the remaining cells in column E to test for matches in each row:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree3.jpg"468">
Notice that the formula only returns <b>Equal</b> for the rows where the cell values are equal across each of the three columns.
You can also apply conditional formatting to the rows where all three cell values are equal by highlighting the cell range <b>E2:E11</b>, then clicking on the <b>Conditional Formatting</b> button on the Home tab, then clicking <b>Highlight Cell Rules</b>, then clicking <b>Equal To</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree3-1.jpg"592">
In the new window that appears, type <b>Equal</b> into the box and choose a fill color, then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree4.jpg"674">
The rows that have matching values across all three columns will now be filled with green:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/excelthree5.jpg"493">
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Compare Two Lists in Excel Using VLOOKUP 
 How to Compare Two Excel Sheets for Differences 
 How to Compare Dates Without Time in Excel 
<h2><span class="orange">How to Compare Three Columns in Pandas (With Example)</span></h2>
You can use the following basic syntax to compare the values in three columns in pandas:
<b>df['all_matching'] = df.apply(lambda x: x.col1 == x.col2 == x.col3, axis = 1)
</b>
This syntax creates a new column called <b>all_matching</b> that returns a value of <b>True</b> if all of the columns have matching values, otherwise it returns <b>False</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Compare Three Columns in Pandas</h2>
Suppose we have the following pandas DataFrame with three columns:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A': [4, 0, 3, 3, 6, 8, 7, 9, 12],   'B': [4, 2, 3, 5, 6, 4, 7, 7, 12],   'C': [4, 0, 3, 5, 5, 10, 7, 9, 12]})
#view DataFrame
print(df)
    A   B   C
0   4   4   4
1   0   2   0
2   3   3   3
3   3   5   5
4   6   6   5
5   8   4  10
6   7   7   7
7   9   7   9
8  12  12  12</b>
We can use the following code to create a new column called <b>all_matching</b> that returns <b>True</b> if all three columns match in a given row and <b>False</b> if they do not:
<b>#create new column that displays whether or not all column values match
df['all_matching'] = df.apply(lambda x: x.A == x.B == x.C, axis = 1)
#view updated DataFrame
print(df)
    A   B   C  all_matching
0   4   4   4          True
1   0   2   0         False
2   3   3   3          True
3   3   5   5         False
4   6   6   5         False
5   8   4  10         False
6   7   7   7          True
7   9   7   9         False
8  12  12  12          True
</b>
The new column called <b>all_matching</b> shows whether or not the values in all three columns match in a given row.
For example:
All three values match in the first row, so <b>True</b> is returned.
Not every value matches in the second row, so <b>False</b> is returned.
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Compare Two Columns in Pandas (With Examples)</span></h2>
Often you may want to compare two columns in a Pandas DataFrame and write the results of the comparison to a third column.
You can easily do this by using the following syntax:
<b>conditions=[(condition1),(condition2)]
choices=["choice1","choice2"]
df["new_column_name"]=np.select(conditions, choices, default)
</b>
Here’s what this code does:
<b>conditions</b> are the conditions to check for between the two columns
<b>choices</b> are the results to return based on the conditions
<b>np.select</b> is used to return the results to the new column
The following example shows how to use this code in practice.
<h2>Example: Compare Two Columns in Pandas</h2>
Suppose we have the following DataFrame that shows the number of goals scored by two soccer teams in five different matches:
<b>import numpy as np
import pandas as pd
#create DataFrame
df = pd.DataFrame({'A_points': [1, 3, 3, 3, 5],   'B_points': [4, 5, 2, 3, 2]})
             
#view DataFrame      
df
          A_points  B_points
0         1         4
1         3         5
2         3         2
3         3         3
4         5         2
</b>
We can use the following code to compare the number of goals by row and output the winner of the match in a third column:
<b>#define conditions
conditions = [df['A_points'] > df['B_points'], 
              df['A_points'] &lt; df['B_points']]
#define choices
choices = ['A', 'B']
#create new column in DataFrame that displays results of comparisons
df['winner'] = np.select(conditions, choices, default='Tie')
#view the DataFrame
df
          A_points  B_points  winner
0         1         4         B
1         3         5         B
2         3         2         A
3         3         3         Tie
4         5         2         A</b>
The results of the comparison are shown in the new column called <em>winner</em>.
<h2>Notes</h2>
Here are a few things to keep in mind when comparing two columns in a pandas DataFrame:
The number of <b>conditions</b> and <b>choices</b> should be equal.
The <b>default</b> value specifies the value to display in the new column if none of the conditions are met.
Both <b>NumPy</b> and <b>Pandas</b> are required to make this code work.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Rename Columns in Pandas 
 How to Add a Column to a Pandas DataFrame 
 How to Change the Order of Columns in Pandas DataFrame 
<h2><span class="orange">How to Compare Two Columns in R (With Examples)</span></h2>
Often you may want to compare two columns in R and write the results of the comparison to a third column.
You can easily do this by using the following syntax:
<b>df$new_col &lt;- ifelse(df$col1 > df$col2, 'A',
                ifelse(df$col1 &lt; df$col2, 'B', 'C'))
</b>
This single line of code does the following:
If column 1 is greater than column 2 then write ‘A’ as the output to the third column.
Otherwise, if column 1 is less than column 2 then write ‘B’ as the output..
Otherwise, write ‘C’ as the output.
The following example shows how to use this code in practice.
<h3>Example: Compare Two Columns in R</h3>
Suppose we have the following data frame that shows the number of goals scored by two soccer teams in five different matches:
<b>#create data frame
df &lt;- data.frame(A_points=c(1, 3, 3, 3, 5), B_points=c(4, 5, 2, 3, 2))
#view data frame
df
  A_points B_points
1        1        4
2        3        5
3        3        2
4        3        3
5        5        2
</b>
We can use the following code to compare the number of goals by row and output the winner of the match in a third column:
<b>#compare <em>A_points</em> and <em>B_points</em> and output results to new column titled <em>winner</em>
df$winner &lt;- ifelse(df$A_points > df$B_points, 'A',
               ifelse(df$A_points &lt; df$B_points, 'B', 'Tie'))
#view data frame
df
  A_points B_points winner
1        1        4      B
2        3        5      B
3        3        2      A
4        3        3    Tie
5        5        2      A</b>
The results of the comparison are shown in the new column called <em>winner</em>.
<h2><span class="orange">How to Compare Two DataFrames in Pandas</span></h2>
Often you might be interested in comparing the values between two pandas DataFrames to spot their similarities and differences.
This tutorial explains how to do so.
<h3>Example: Comparing Two DataFrames in Pandas</h3>
Suppose we have the following two pandas DataFrames that each contain data about four basketball players:
<b>import pandas as pd
#define DataFrame 1
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D'],   'points': [12, 15, 17, 24],   'assists': [4, 6, 7, 8]})
df1
        playerpointsassists
0A124
1B156
2C177
3D2488
#define DataFrame 2
df2 = pd.DataFrame({'player': ['A', 'B', 'C', 'D'],    'points': [12, 24, 26, 29],    'assists': [7, 8, 10, 13]})
df2
playerpointsassists
0A127
1B248
2C2610
3D2913
</b>
<b>Example 1: Find out if the two DataFrames are identical.</b>
We can first find out if the two DataFrames are identical by using the  DataFrame.equals()  function:
<b>#see if two DataFrames are identical
df1.equals(df2)
False</b>
The two DataFrames do not contain the exact same values, so this function correctly returns <b>False</b>.
<b>Example 2: Find the differences in player stats between the two DataFrames.</b>
We can find the differences between the assists and points for each player by using the pandas  subtract()  function:
<b>#subtract df1 from df2
df2.set_index('player').subtract(df1.set_index('player'))
pointsassists
player
A03
B92
C93
D55
</b>
The way to interpret this is as follows:
Player A had the same amount of points in both DataFrames, but they had 3 more assists in DataFrame 2.
Player B had 9 more points and 2 more assists in DataFrame 2 compared to DataFrame 1.
Player C had 9 more points and 3 more assists in DataFrame 2 compared to DataFrame 1.
Player D had 5 more points and 5 more assists in DataFrame 2 compared to DataFrame 1.
<b>Example 3: Find all rows that only exist in one DataFrame.</b>
We can use the following code to obtain a complete list of rows that only appear in one DataFrame:
<b>#outer merge the two DataFrames, adding an indicator column called 'Exist'
diff_df = pd.merge(df1, df2, how='outer', indicator='Exist')
#find which rows don't exist in both DataFrames
diff_df = diff_df.loc[diff_df['Exist'] != 'both']
diff_df
playerpointsassistsExist
0A124left_only
1B156left_only
2C177left_only
3D248left_only
4A127right_only
5B248right_only
6C2610right_only
7D2913right_only
</b>
In this case, the two DataFrames share no identical rows so there are 8 total rows that only appear in one of the DataFrames.
The column titled “Exist” conveniently tells us which DataFrame each row uniquely appears in.
<h2><span class="orange">How to Compare Two NumPy Arrays (With Examples)</span></h2>
You can use the following methods to compare the values of two NumPy arrays:
<b>Method 1: Test if Two NumPy Arrays are Element-wise Equal</b>
<b>#test if array A and array B are element-wise equal
np.array_equal(A,B)
</b>
<b>Method 2: Test if Two NumPy Arrays are Element-wise Equal (Within a tolerance)</b>
<b>#test if array A and array B are element-wise equal (within absolute tolerance of 2)
np.allclose(A, B, atol=2)
</b>
The following examples show how to use each method in practice.
<h3>Example 1: Test if Two NumPy Arrays are Element-wise Equal</h3>
The following code shows how to use the <b>array_equal()</b> function to test if two NumPy arrays are element-wise equal:
<b>import numpy as np
#create two NumPy arrays
A = np.array([1, 4, 5, 7, 10])
B = np.array([1, 4, 5, 7, 10])
#test if arrays are element-wise equal
np.array_equal(A,B)
True</b>
The function returns <b>True</b> since the two NumPy arrays have the same length with the same values in the same positions.
However, the function will return <b>False</b> if the two NumPy arrays have the same values but in different positions:
<b>import numpy as np
#create two NumPy arrays with same values but in different positions
A = np.array([1, 4, 5, 7, 10])
B = np.array([1, 4, 7, 5, 10])
#test if arrays are element-wise equal
np.array_equal(A,B)
False</b>
<h3>Example 2: Test if Two NumPy Arrays are Element-wise Equal (Within Tolerance)</h3>
The following code shows how to use the <b>allclose()</b> function to test if two NumPy arrays are element-wise equal within a tolerance value of <b>2</b>:
<b>import numpy as np
#create two NumPy arrays
A = np.array([1, 4, 5, 7, 10])
B = np.array([1, 4, 7, 8, 10])
#test if arrays are element-wise equal (within absolute tolerance of 2)
np.allclose(A, B, atol=2)
True</b>
The function returns <b>True</b> since the corresponding elements between each NumPy array are all within 2 of each other.
For example, we see that elements in the third and fourth positions of each array are different, but since each pair is within 2 values of each other, the function returns true.
However, if we change the absolute tolerance (atol) argument to <b>1</b>, then the function will return <b>False</b>:
<b>import numpy as np
#create two NumPy arrays
A = np.array([1, 4, 5, 7, 10])
B = np.array([1, 4, 7, 8, 10])
#test if arrays are element-wise equal (within absolute tolerance of 1)
np.allclose(A, B, atol=1)
False</b>
The function returns <b>False </b>since the corresponding elements in the third position of each NumPy array are not within 1 of each other.
<b>Note</b>: Refer to the NumPy documentation to find a complete explanation of the  array_equal  and  allclose  functions.
<h2><span class="orange">How to Compare Two Vectors in R (With Examples)</span></h2>
You can use the following basic syntax to compare two vectors in R:
<b>#check if two vectors are identical
identical(vector_1, vector_2)
#display items that are in both vectors
intersect(vector_1, vector_2)
#display items that are only in first vector, but not in second vector
setdiff(vector_1, vector_2)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Check if Two Vectors Are Identical</h3>
The following code shows how to use the <b>identical()</b> function to check if two vectors are identical:
<b>#define vectors
vector_1 &lt;- c('Andy', 'Bob', 'Carl', 'Doug')
vector_2 &lt;- c('Bob', 'Carl', 'Doug', 'Ethan', 'Fred')
#check if two vectors are identical
identical(vector_1, vector_2)
[1] FALSE
</b>
The two vectors are not identical, so a value of <b>FALSE</b> is returned.
<h3>Example 2: Find Items that Exist in Both Vectors</h3>
The following code shows how to use the <b>intersect()</b> function to display the items that exist in both vectors:
<b>#define vectors
vector_1 &lt;- c('Andy', 'Bob', 'Carl', 'Doug')
vector_2 &lt;- c('Bob', 'Carl', 'Doug', 'Ethan', 'Fred')
#display items that exist in both vectors
intersect(vector_1, vector_2)
[1] "Bob"  "Carl" "Doug"
</b>
The three items that exist in both vectors are displayed.
We can also use the <b>length()</b> function if we simply want to know <em>how many</em> items exist in both vectors:
<b>#find how many items exist in both vectors
length(intersect(vector_1, vector_2))
[1] 3
</b>
Three items exist in both vectors.
<h3>Example 3: Find Items that Only Exist in One Vector</h3>
The following code shows how to use the <b>setdiff()</b> function to display the items that exist in the first vector, but not the second:
<b>#define vectors
vector_1 &lt;- c('Andy', 'Bob', 'Carl', 'Doug')
vector_2 &lt;- c('Bob', 'Carl', 'Doug', 'Ethan', 'Fred')
#display items that exist in first vector, but not in second vector
setdiff(vector_1, vector_2)
[1] "Andy"
</b>
Exactly one item exists in the first vector that does not exist in the second vector.
We can switch the two vectors around to identify the items that exist in the second vector, but not the first:
<b>#define vectors
vector_1 &lt;- c('Andy', 'Bob', 'Carl', 'Doug')
vector_2 &lt;- c('Bob', 'Carl', 'Doug', 'Ethan', 'Fred')
#display items that exist in second vector, but not in first vector
setdiff(vector_2, vector_1)
[1] "Ethan" "Fred"
</b>
Two items exist in the second vector that do not exist in the first.
<h2><span class="orange">Compare Z Scores Calculator</span></h2>
A <b>z-score</b> tells you how many standard deviations away an individual data value falls from the mean. It is calculated as:
<b>z-score</b> = (x – μ) / σ
where:
x: individual data value
μ: population mean
σ: population standard deviation
This calculator allows you to compare the relative standing of two data points using their z-scores. Simply fill in the boxes below and then click the “Calculate” button.
<label><b>μ<sub>1</sub></b> (population 1 mean)</label>
<input type="number" id="x1" value="15">
<label><b>σ<sub>1</sub></b> (population 1 standard deviation)</label>
<input type="number" id="s1" value="4.2">
<label><b>x<sub>1</sub></b> (individual value 1)</label>
<input type="number" id="v1" value="19">
<label><b>μ<sub>2</sub></b> (population 2 mean)</label>
<input type="number" id="x2" value="14">
<label><b>σ<sub>2</sub></b> (population 2 standard deviation)</label>
<input type="number" id="s2" value="2.6">
<label><b>x<sub>2</sub></b> (individual value 2)</label>
<input type="number" id="v2" value="17">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
z-score of first value = (19–15) / 4.2 = <b>0.9524</b>
z-score of second value = (17–14) / 2.6 = <b>1.1538</b>
<script>
function calc() {
//get input values
var x1 = +document.getElementById('x1').value;
var s1 = +document.getElementById('s1').value;
var v1 = +document.getElementById('v1').value;
var x2 = +document.getElementById('x2').value;
var s2 = +document.getElementById('s2').value;
var v2 = +document.getElementById('v2').value;
//calculate stuff
var z1 = (v1-x1)/s1;
var z2 = (v2-x2)/s2;
//output results
document.getElementById('v1out').innerHTML = v1;
document.getElementById('x1out').innerHTML = x1;
document.getElementById('s1out').innerHTML = s1;
document.getElementById('z1').innerHTML = z1.toFixed(4);
document.getElementById('v2out').innerHTML = v2;
document.getElementById('x2out').innerHTML = x2;
document.getElementById('s2out').innerHTML = s2;
document.getElementById('z2').innerHTML = z2.toFixed(4);
}
</script>
<h2><span class="orange">Comparing Z-Scores from Different Distributions</span></h2>
A <b>z-score </b>tells you how many standard deviations away an individual data value falls from the mean. It is calculated as:
<b>z-score = (x – μ) / σ</b>
where:
<b>x: </b>individual data value
<b>μ: </b>population mean
<b>σ: </b>population standard deviation
A z-score for an individual value can be interpreted as follows:
<b>Positive z-score: </b>The individual value is greater than the mean.
<b>Negative z-score: </b>The individual value is less than the mean.
<b>A z-score of 0:</b> The individual value is equal to the mean.
Z-scores are particularly useful for when we want to compare the relative standing of two data points from two different distributions. To illustrate this, consider the following example.
<h3>Example: Comparing Z-Scores</h3>
The scores on a certain college exam are normally distributed with mean μ = 80 and standard deviation σ = 4. Duane scores an 84 on this exam.
The scores on another college exam are normally distributed with mean μ = 85 and standard deviation σ = 8. Debbie scores an 90 on this exam.
<b>Relative to their own exam score distributions, who scored higher on their exam?</b>
To answer this question, we can calculate the z-score of each person’s exam score:
Duane’s z-score = (x – μ) / σ = (84 – 80) / 4 = 4 / 4 = <b>1</b>
Debbie’s z-score = (x – μ) / σ = (90 – 85) / 8 = 5 / 8 = <b>0.625</b>
Although Debbie scored higher, Duane’s score is actually higher relative to the distribution of his particular exam.
To understand this, it helps to visualize the situation. Here is Duane’s exam score relative to the distribution of his particular exam:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/comparingZ1.png">
And here is Debbie’s exam score relative to the distribution of her exam:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/comparingZ2.png">
Notice how Debbie’s score is closer to her population mean compared to Duane. Although she has an overall higher score, her z-score is lower simply because the mean score on her particular exam is higher.
This example illustrates why z-scores are so useful for comparing data values from different distributions: z-scores take into account the mean and standard deviations of distributions, which allows us to compare data values from different distributions and see which one is higher relative to their own distributions.
<h2><span class="orange">How to Use complete.cases in R (With Examples)</span></h2>
You can use the <b>complete.cases()</b> function in R to remove missing values in a vector, matrix, or data frame.
This function uses the following basic syntax:
<b>#remove missing values from vector
x &lt;- x[complete.cases(x)]
#remove rows with missing values in any column of data frame
df &lt;- df[complete.cases(df), ]
#remove rows with NA in specific columns of data frame
df &lt;- df[complete.cases(df[ , c('col1', 'col2', ...)]), ] 
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Remove Missing Values from Vector</h3>
The following code shows how to remove all NA values from a vector:
<b>#define vector
x &lt;- c(1, 24, NA, 6, NA, 9)
#remove NA values from vector
x &lt;- x[complete.cases(x)]
x
[1]  1 24  6  9
</b>
<h3>Example 2: Remove Rows with NA in Any Column of Data Frame</h3>
The following code shows how to remove rows with NA values in any column of a data frame :
<b>#define data frame
df &lt;- data.frame(x=c(1, 24, NA, 6, NA, 9), y=c(NA, 3, 4, 8, NA, 12), z=c(NA, 7, 5, 15, 7, 14))
#view data frame
df
   x  y  z
1  1 NA NA
2 24  3  7
3 NA  4  5
4  6  8 15
5 NA NA  7
6  9 12 14
#remove rows with NA value in any column data frame
df &lt;- df[complete.cases(df), ]
#view data frame 
df
   x  y  z
2 24  3  7
4  6  8 15
6  9 12 14
</b>
<h3>Example 3: Remove Rows with NA in Specific Columns of Data Frame</h3>
The following code shows how to remove rows with NA values in specific columns of a data frame :
<b>#define data frame
df &lt;- data.frame(x=c(1, 24, NA, 6, NA, 9), y=c(NA, 3, 4, 8, NA, 12), z=c(NA, 7, 5, 15, 7, 14))
#view data frame
df
   x  y  z
1  1 NA NA
2 24  3  7
3 NA  4  5
4  6  8 15
5 NA NA  7
6  9 12 14
#remove rows with NA value in <em>y </em>or <em>z</em> column
df &lt;- df[complete.cases(df[ , c('y', 'z')]), ]
#view data frame 
df
   x  y  z
2 24  3  7
3 NA  4  5
4  6  8 15
6  9 12 14</b>
<h2><span class="orange">How to Calculate Compound Interest in Google Sheets (3 Examples)</span></h2>
We can use the following compound interest formula to find the ending value of some investment after a certain amount of time:
<b>A = P(1 + r/n)<sup>nt</sup></b>
where:
<b>A:</b> Final Amount
<b>P:</b> Initial Principal
<b>r:</b> Annual Interest Rate
<b>n:</b> Number of compounding periods per year
<b>t:</b> Number of years
The following examples show how to use this formula in Google Sheets to calculate the ending value of investments in different scenarios.
<h3>Example 1: Compound Interest Formula with Annual Compounding</h3>
Suppose we invest $5,000 into an investment that compounds at 6% annually.
The following screenshot shows how to use the compound interest formula in Google Sheets to calculate the ending value of this investment after 10 years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/compoundSheets1.png">
This investment will be worth <b>$8,954.24</b> after 10 years.
The following screenshot shows how to calculate the ending investment after each year during the 10-year period.
Note that Column F shows the formula we used in each corresponding cell in Column E:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/compoundSheets2-1.png">
<h3>Example 2: Compound Interest Formula with Monthly Compounding</h3>
Suppose we invest $1,000 into an investment that compounds at 6% annually and is compounded on a monthly basis (12 times per year).
The following screenshot shows how to use the compound interest formula in Google Sheets to calculate the ending value of this investment after 5 years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/compoundSheets3.png">
This investment will be worth <b>$1,348.85 </b>after 5 years.
<h3>Example 3: Compound Interest Formula with Daily Compounding</h3>
Suppose we invest $5,000 into an investment that compounds at 8% annually and is compounded on a daily basis (365 times per year).
The following screenshot shows how to use the compound interest formula in Google Sheets to calculate the ending value of this investment after 15 years:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/compoundSheets4.png">
This investment will be worth <b>$16,598.39 </b>after 15 years.
<h2><span class="orange">How to Calculate Compound Interest in Python (3 Examples)</span></h2>
We can use the following compound interest formula to find the ending value of some investment after a certain amount of time:
<b>A = P(1 + r/n)<sup>nt</sup></b>
where:
<b>A:</b> Final Amount
<b>P:</b> Initial Principal
<b>r:</b> Annual Interest Rate
<b>n:</b> Number of compounding periods per year
<b>t:</b> Number of years
We can use the following formula to calculate the ending value of some investment in Python:
<b>P*(pow((1+r/n), n*t))
</b>
And we can use the following function to display the ending value of some investment at the end of each period:
<b>def each_year(P, r, n, t):
    for period in range(t):
        amount = P*(pow((1+r/n), n*(period+1)))
        print('Period:', period+1, amount)
    return amount</b>
The following examples show how to use these formulas in Python to calculate the ending value of investments in different scenarios.
<h3>Example 1: Compound Interest Formula with Annual Compounding</h3>
Suppose we invest $5,000 into an investment that compounds at 6% annually.
The following code shows how to calculate the ending value of this investment after 10 years:
<b>#define principal, interest rate, compounding periods per year, and total years
P = 5000
r = .06
n = 1
t = 10
#calculate final amount
P*(pow((1+r/n), n*t))
8954.238482714272
</b>
This investment will be worth <b>$8,954.24</b> after 10 years.
We can use the function we defined earlier to display the ending investment after each year during the 10-year period:
<b>#display ending investment after each year during 10-year period
each_year(P, r, n, t)
Period: 1 5300.0
Period: 2 5618.000000000001
Period: 3 5955.08
Period: 4 6312.384800000002
Period: 5 6691.127888000002
Period: 6 7092.595561280002
Period: 7 7518.151294956803
Period: 8 7969.240372654212
Period: 9 8447.394795013464
Period: 10 8954.238482714272
</b>
This tells us:
The ending value after year 1 was <b>$5,300</b>.
The ending value after year 2 was <b>$5,618</b>.
The ending value after year 3 was <b>$5,955.08</b>.
And so on.
<h3>Example 2: Compound Interest Formula with Monthly Compounding</h3>
Suppose we invest $1,000 into an investment that compounds at 6% annually and is compounded on a monthly basis (12 times per year).
The following code shows how to calculate the ending value of this investment after 5 years:
<b>#define principal, interest rate, compounding periods per year, and total years
P = 1000
r = .06
n = 12
t = 5
#calculate final amount
P*(pow((1+r/n), n*t))
1348.8501525493075</b>
This investment will be worth <b>$1,348.85 </b>after 5 years.
<h3>Example 3: Compound Interest Formula with Daily Compounding</h3>
Suppose we invest $5,000 into an investment that compounds at 8% annually and is compounded on a daily basis (365 times per year).
The following code shows how to calculate the ending value of this investment after 15 years:
<b>#define principal, interest rate, compounding periods per year, and total years
P = 5000
r = .08
n = 365
t = 15
#calculate final amount
P*(pow((1+r/n), n*t))
16598.40198554521</b>
This investment will be worth <b>$16,598.40 </b>after 15 years.
<h2><span class="orange">How to Concatenate Arrays in Python (With Examples)</span></h2>
The easiest way to concatenate arrays in Python is to use the <b>numpy.concatenate</b> function, which uses the following syntax:
<b>numpy.concatenate((a1, a2, ….), axis = 0)</b>
where:
<b>a1, a2 …:</b> The sequence of arrays
<b>axis:</b> The axis along which the arrays will be joined. Default is 0.
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Concatenate Two Arrays</h3>
The following code shows how to concatenate two 1-dimensional arrays:
<b>import numpy as np
#create two arrays
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.array([6, 7, 8])
#concatentate the two arrays
np.concatenate((arr1, arr2))
[1, 2, 3, 4, 5, 6, 7, 8]</b>
The following code shows how to concatenate two 2-dimensional arrays:
<b>import numpy as np
#create two arrays
arr1 = np.array([[3, 5], [9, 9], [12, 15]])
arr2 = np.array([[4, 0]])
#concatentate the two arrays
np.concatenate((arr1, arr2), axis=0)
array([[3, 5],
       [9, 9],
       [12, 15],
       [4, 0]])
#concatentate the two arrays and flatten the result
np.concatenate((arr1, arr2), axis=None)
array([3, 5, 9, 9, 12, 15, 4, 0])</b>
<h3>Example 2: Concatenate More Than Two Arrays</h3>
We can use similar code to concatenate more than two arrays:
<b>import numpy as np
#create four arrays
arr1 = np.array([[3, 5], [9, 9], [12, 15]])
arr2 = np.array([[4, 0]])
arr3 = np.array([[1, 1]])
arr4 = np.array([[8, 8]])
#concatentate all the arrays
np.concatenate((arr1, arr2, arr3, arr4), axis=0)
array([[3, 5],
       [9, 9],
       [12, 15],
       [4, 0],
       [1, 1],
       [8, 8]])
#concatentate all the arrays and flatten the result
np.concatenate((arr1, arr2, arr3, arr4), axis=None)
array([3, 5, 9, 9, 12, 15, 4, 0, 1, 1, 8, 8])</b>
<h2><span class="orange">How to Concatenate Two Pandas DataFrames (With Examples)</span></h2>
You can use the following basic syntax to concatenate two pandas DataFrames:
<b>df3 = pd.concat([df1, df2], ignore_index=True)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: How to Concatenate Two Pandas DataFrames</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'A', 'A', 'A'],    'assists': [5, 7, 7, 9],    'points': [11, 8, 10, 6]})
df2 = pd.DataFrame({'team': ['B', 'B', 'B', 'B'],    'assists': [4, 4, 3, 7],    'points': [14, 11, 7, 6]})
#view DataFrames
print(df1)
  team  assists  points
0    A        5      11
1    A        7       8
2    A        7      10
3    A        9       6
print(df2)
  team  assists  points
0    B        4      14
1    B        4      11
2    B        3       7
3    B        7       6</b>
We can use the following syntax to concatenate the two DataFrames:
<b>#concatenate the DataFrames
df3 = pd.concat([df1, df2])
#view resulting DataFrame
print(df3)
  team  assists  points
0    A        5      11
1    A        7       8
2    A        7      10
3    A        9       6
0    B        4      14
1    B        4      11
2    B        3       7
3    B        7       6</b>
The result is one DataFrame that contains the data from both DataFrames.
If you’d like to create a new index when concatenating the DataFrames, you must use the <b>ignore_index</b> argument:
<b>#concatenate the DataFrames and ignore index
df3 = pd.concat([df1, df2], ignore_index=True)
#view resulting DataFrame
print(df3)
  team  assists  points
0    A        5      11
1    A        7       8
2    A        7      10
3    A        9       6
4    B        4      14
5    B        4      11
6    B        3       7
7    B        7       6</b>
Notice that the index of the resulting DataFrame ranges from 0 to 7.
<b>Note #1:</b> In this example we concatenated two pandas DataFrames, but you can use this exact syntax to concatenate any number of DataFrames that you’d like.
<b>Note #2:</b> You can find the complete documentation for the pandas <b>concat()</b> function  here .
<h2><span class="orange">What is a Conceptual Variable? (Definition & Examples)</span></h2>
In statistics, a <b>conceptual variable</b> represents some abstract construct or entity that we want to measure.
However, we can’t directly measure a conceptual variable so we instead use an <b>actual measure</b> to quantify the conceptual variable.
For example, suppose a researcher wants to assess the “overall happiness” of individuals. This represents a conceptual variable because there is no way to directly assign a value of “overall happiness” to individuals.
Instead, the researcher may have individuals respond to survey questions that have possible responses such as:
I am extremely dissatisfied with my life situation.
I am somewhat dissatisfied with my life situation.
I feel neutral about my life situation.
I am somewhat satisfied with my life situation.
I am extremely satisfied with my life situation.
The researcher can then use these responses to assign some type of “overall happiness” score to individuals.
The variable “overall happiness” is the true variable of interest but since it is conceptual we must use the survey responses as actual measures that can be used to assess overall happiness.
Read through the following scenarios for more examples of conceptual variables and actual measures.
<h3>Example 1: Workplace Performance</h3>
Suppose a Human Resources department at a certain company wants to assess workplace performance for each individual in a company so they use manager performance ratings to assign a performance score to each individual.
The <b>conceptual variable</b> is workplace performance and the <b>actual measure</b> is the manager performance rating, which may be measured on a scale of 0 to 10.
<h3>Example 2: Athleticism</h3>
Suppose a track coach wants to measure overall athleticism of his runners so he uses a combination of their record times in the 200 meter run, 400 meter run, and 800 meter run.
The <b>conceptual variable</b> is athleticism and the <b>actual measure</b> is the individual time for each runner at the various distances, which may be measured in minutes and seconds.
<h3>Example 3: Strength</h3>
Suppose a lifting coach wants to measure the strength of athletes in his gym so he uses a combination of their personal best records in squat, bench press, and shoulder press.
The <b>conceptual variable</b> is strength and the <b>actual measure</b> is the actual weight in pounds that each athlete was able to lift for the various exercises.
<h3>Example 4: Depression</h3>
Suppose a psychologist wants to measure depression levels in individuals so he uses a self-reported survey that asks individuals to rate their overall satisfaction in different areas of their life.
The <b>conceptual variable</b> is depression and the <b>actual measure</b> is the score that individuals receive on the self-reported survey.
<h3>Summary: Conceptual Variables vs. Actual Measures</h3>
The following table summarizes the difference between conceptual variables and actual measures:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/conceptualVar1.png">
Note that in each example, the <b>conceptual variable</b> is the variable of interest and the <b>actual measure</b> is the thing we use to assign a quantitative value to the conceptual variable.
<h2><span class="orange">Concomitant Variable: Definition & Examples</span></h2>
A <b>concomitant variable</b> (sometimes called a “covariate”) is a variable that is not of primary interest in a study, but nonetheless may have some interaction with the variable(s) of interest being studied.
Not accounting for these types of variables can lead to biased or misleading results in an analysis, so it’s important to deal with them when possible.
In observational studies, it’s important to be aware of the fact that concomitant variables could cause unusual interpretations of data and the relationships between variables. In experimental studies, it’s important to design the experiment in such a way that eliminates or reduces the risk of concomitant variables.
The following examples illustrate several cases in which concomitant variables could be present in a study:
<h3>Example 1</h3>
Researchers want to understand the relationship between population density and ice cream sales. However, a concomitant variable that likely affects ice cream sales is weather.
Thus, if researchers want to perform a  linear regression  to quantify the relationship between population density and ice cream sales, they should also attempt to collect data about weather so that they can control for that variable in the regression and be able to obtain an accurate estimate of the effect that population density has on ice cream sales.
<h3>Example 2</h3>
Researchers want to understand the relationship between hours spent practicing and average points scored per game by basketball players. However, a concomitant variable that likely affects average points scored is minutes played per game.
Thus, researchers should also track how many minutes a player plays per game so that they can include it as a variable in regression analysis and isolate the effect that hours spent practicing has on average points scored per game.
<b>Related: </b> How to Interpret Regression Coefficients 
<h3>Example 3</h3>
Researchers want to know whether or not a certain fertilizer leads to increased plant growth. However, sunlight exposure and watering frequency are both potential concomitant variables that likely affect plant growth.
Thus, researchers should also collect data on sunlight exposure and watering frequency so that they can include them as variables in regression analysis and be able to understand the effect that the fertilizer has on plant growth, <em>after accounting for sunlight exposure and watering frequency.</em>
<h3>How to Identify & Eliminate Concomitant Variables</h3>
To discover concomitant variables, it helps to have domain expertise in the area under study. By knowing what potential variables could be affecting the relationship between the variables in the study that aren’t included explicitly in the study, you may be able to uncover potential concomitant variables.
In observational studies, it can be very difficult to eliminate the risk of concomitant variables. In most cases, the best you can do is simply identify, rather then prevent, potential concomitant variables that may be impacting the study.
In experimental studies, however, the impact of concomitant variables can mostly be eliminated with good experimental design.
For example, suppose we want to know whether two pills have a different impact on blood pressure. We know that concomitant variables such as <em>diet</em> and <em>smoking habits </em>also impact blood pressure, so we can attempt to control for these concomitant variables by using a randomized design. This means we randomly assign patients to take either the first or second pill.
Since we randomly assign patients to groups, we can assume that the concomitant variables will affect both groups roughly equally. This means any differences in blood pressure can be attributed to the pill, rather than the effect of a concomitant variable.
<h2><span class="orange">What is Concurrent Validity? (Definition & Examples)</span></h2>
In statistics, we’re often interested in understanding if the value of some  explanatory variable  can predict the value of some response variable. This response variable is sometimes called a  criterion variable .
For example, we might want to know how well some college entrance exam is able to predict the first semester grade point average of students.
The entrance exam would be the explanatory variable and the criterion variable would be the first semester GPA.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity1.png"(max-width: 589px) 100vw, 589px">
We want to know if it’s <em>valid</em> to use this particular explanatory variable as a way to predict the criterion variable. If it is valid, then we say that  criterion validity  exists.
There are two types of criterion validity:
<b>1. Predictive Validity</b> – This tells us if it’s valid to use the value of one variable to predict the value of some other variable in the future.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity2.png">
<b>2. Concurrent Validity</b> – This tells us if it’s valid to use the value of one variable to predict the value of some other variable measured <em>concurrently </em>(i.e. at the same time).
For example, a company might administer some type of test to see if the scores on the test are correlated with current employee productivity levels.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity3.png">
The benefit of this approach is that we don’t have to wait until some point in the future to take a measurement on the criterion variable we’re interested in.
Note that we usually measure both types of validity using the  Pearson Correlation Coefficient , which takes on value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the association between the two variables.
<h3>Examples of Concurrent Validity</h3>
The following examples illustrate more scenarios in which we can use concurrent validity to determine whether or not some explanatory variable can be used to predict the value of some criterion variable.
<b>Example 1: A Test of Knowledge</b>
A researcher creates a new test that is designed to assess the knowledge of college students in the subject of biology.
The researcher gives out the test to all biology majors at a certain university and compares the scores of his test with their current GPA.
If there is a high correlation between the grades on his test and the current GPA of the students, we can say that concurrent validity exists.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/concurrent1.png">
<b>Example 2: A Test of Endurance</b>
A track coach creates a new endurance challenge that is designed to assess the endurance levels of his athletes. He lets each of his athletes perform the challenge and compares their scores to their current performance levels.
If there is a high correlation between the endurance challenge and the current performance levels, then he can say that concurrent validity exists.
In other words, it would be valid to use the endurance challenge to assess the performance levels of the athletes.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/concurrent2.png">
<b>Example 3: A Test of Leadership</b>
A business executive creates a new test to assess the leadership ability of employees at a company. She gives out the test to each employee at a company and compares their scores to current peer-assessed levels of leadership.
If there is a high correlation between the test and current peer-assessed levels of leadership, then she can say that concurrent validity exists.
In other words, it would be valid to use the test to assess leadership levels of the various employees at the company.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/concurrent3.png">
<h2><span class="orange">What is a Conditional Distribution in Statistics?</span></h2>
If <em>X</em> and <em>Y</em> are two jointly distributed  random variables , then the <b>conditional distribution</b> of <em>Y</em> given <em>X</em> is the probability distribution of <em>Y</em> when <em>X</em> is known to be a certain value.
For example, the following two-way table shows the results of a survey that asked 100 people which sport they liked best: baseball, basketball, or football.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/marginal1.png">
If we want to know the probability that a person prefers a certain sport <em>given</em> that they are male, then this is an example of a conditional distribution.
The value of one random variable is known (the person is male), but the value of the other random variable is unknown (we don’t know their favorite sport).
To find the conditional distribution of sports preference among males, we would simply look at the values in the row for <b>Male</b> in the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/conditionalDist1.png">
The conditional distribution would be calculated as:
Males who prefer baseball: 13/48 = <b>.2708</b>
Males who prefer basketball: 15/48 = <b>.3125</b>
Males who prefer football: 20/48 = <b>.4167</b>
Notice that the sum of the probabilities adds up to 1: 13/48 + 15/48 + 20/48 = 48/48 = 1.
We can use this conditional distribution to answer questions like: <em>Given that an individual is male, what is the probability that baseball is their favorite sport?</em>
From the conditional distribution we calculated earlier, we can see that the probability is <b>.2708</b>.
In technical terms, when we calculate a conditional distribution we say that we’re interested in a particular <b>subpopulation</b> of the overall population. The subpopulation in the previous example was males:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/conditionalDist2.png">
And when we want to calculate a probability related to this subpopulation, we say that we’re interested in a particular <b>character of interest</b>. The character of interest in the previous example was baseball:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/conditionalDist3.png">
To find the probability that the character of interest occurs in the subpopulation, we simply divide the value of the character of interest (e.g. 13) by the total values in the subpopulation (e.g. 48) to get 13/48 = <b>.2708</b>.
<h3>Conditional Distributions & Independence</h3>
We can say that random variables <em>X</em> and <em>Y</em> are independent if and only if the conditional distribution of <em>Y</em> given <em>X</em> is, for all possible realizations of <em>X</em>, equal to the unconditional distribution of <em>Y</em>.
For example, in the previous table can we see that the events “prefers baseball” and “male” are independent?
To answer this, let’s calculate the following probabilites:
P(prefers baseball)
P(prefers baseball | male)  “prefers baseball, given that they are male
The probability that a given individual prefers baseball is:
P(prefers baseball) = 36/100 =<b> .36</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/conditionalDist4.png">
The probability that a given individual prefers baseball, given that they are male is
P(prefers baseball | male) = 13/48 = <b>.2708</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/conditionalDist5.png">
Since P(prefers baseball) is not equal to P(prefers baseball | male), the random variables of Sports Preference and Gender are <em>not</em> independent.
<h3>Why Use Conditional Distributions?</h3>
Conditional probability distributions are useful because we often collect data for two variables (like Gender and Sports Preference) but we’re interested in answering questions about probability when we happen to <em>know</em> the value of one of the variables.
In the previous example, we considered the scenario where we knew that a given individiual was male and we simply wanted to know the probability that the individual preferred baseball.
There are many instances in real life where we happen to know the value of one variable and we can use a conditional distribution to find the probability of another variable taking on a certain value.
<h2><span class="orange">How to Calculate Conditional Mean in Excel (With Examples)</span></h2>
You can use the <b>AVERAGEIF</b> function in Excel to calculate a conditional mean.
This function uses the following basic syntax:
<b>=AVERAGEIF(A2:A7, "some value", B2:B7)</b>
This particular formula calculates the average value in the range <b>B2:B7</b> where the corresponding value in the range <b>A2:A7</b> is equal to “some value.”
The following examples show how to use this function to calculate a conditional mean using the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/conditional1.jpg"501">
<h2>Example 1: Calculate Conditional Mean for Categorical Data</h2>
We can use the following formula to calculate the mean of the <b>Points</b> column only for the rows where the <b>Team</b> column has a value of “A.”
<b>=AVERAGEIF(A2:A7, "A", B2:B7)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/conditional2.jpg">
The mean value in the Points column for the rows where Team is equal to “A” is <b>94</b>.
We can manually verify this is correct by calculating the average of the points values for only the rows where Team is equal to “A”:
Average of Points: (99 + 90 + 93) / 3 = <b>94</b>
This matches the value calculated by the AVERAGEIF function.
<h2>Example 2: Calculate Conditional Mean for Numeric Data</h2>
We can use the following formula to calculate the mean of the <b>Assists</b> column only for the rows where the <b>Points</b> column has a value greater than or equal to 90:
<b>=AVERAGEIF(B2:B7, ">=90", C2:C7)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/conditional3.jpg"539">
The mean value in the Assists column for the rows where Points is greater than or equal to 90 is <b>30.66667</b>.
We can manually verify this is correct by calculating the average of the assists values for only the rows where points is greater than or equal to 90:
Average of Assists: (33 + 28 + 31) / 3 = <b>30.66667</b>
This matches the value calculated by the AVERAGEIF function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Find Mean, Median & Mode in Excel 
 How to Calculate Standard Error of the Mean in Excel 
 How to Calculate Mean Squared Error (MSE) in Excel 
<h2><span class="orange">How to Calculate Conditional Mean in R (With Examples)</span></h2>
You can use the following syntax to calculate a conditional mean in R:
<b>mean(df[df$team == 'A', 'points'])</b>
This calculates the mean of the ‘points’ column for every row in the data frame where the ‘team’ column is equal to ‘A.’
The following examples show how to use this syntax in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'B'), points=c(99, 90, 93, 86, 88, 82), assists=c(33, 28, 31, 39, 34, 30))
#view data frame
df
  team points assists
1    A     99      33
2    A     90      28
3    A     93      31
4    B     86      39
5    B     88      34
6    B     82      30
</b>
<h2>Example 1: Calculate Conditional Mean for Categorical Variable</h2>
The following code shows how to calculate the mean of the ‘points’ column for only the rows in the data frame where the ‘team’ column has a value of ‘A.’
<b>#calculate mean of 'points' column for rows where team equals 'A'
mean(df[df$team == 'A', 'points'])
[1] 94
</b>
The mean value in the ‘points’ column for the rows where ‘team’ is equal to ‘A’ is <b>94</b>.
We can manually verify this by calculating the average of the points values for only the rows where ‘team’ is equal to ‘A’:
Average of Points: (99 + 90 + 93) / 3 = <b>94</b>
<h2>Example 2: Calculate Conditional Mean for Numeric Variable</h2>
The following code shows how to calculate the mean of the ‘assists’ column for only the rows in the data frame where the ‘points’ column has a value greater than or equal to 90.
<b>#calculate mean of 'assists' column for rows where 'points' >= 90
mean(df[df$points >= 90, 'assists'])
[1] 30.66667
</b>
The mean value in the ‘assists’ column for the rows where ‘points’ is greater than or equal to 90 is <b>30.66667</b>.
We can manually verify this by calculating the average of the assists values for only the rows where points is greater than or equal to 90:
Average of Assists: (33 + 28 + 31) / 3 = <b>30.66667</b>
<h2>Additional Resources</h2>
The following tutorials explain how to calculate other mean values in R:
 How to Calculate a Trimmed Mean in R 
 How to Calculate Geometric Mean in R 
 How to Calculate a Weighted Mean in R 
<h2><span class="orange">How to Calculate Conditional Mean in Pandas (With Examples)</span></h2>
You can use the following syntax to calculate a conditional mean in pandas:
<b>df.loc[df['team'] == 'A', 'points'].mean()
</b>
This calculates the mean of the ‘points’ column for every row in the DataFrame where the ‘team’ column is equal to ‘A.’
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [99, 90, 93, 86, 88, 82],   'assists': [33, 28, 31, 39, 34, 30]})
#view DataFrame
print(df)
  team  points  assists
0    A      99       33
1    A      90       28
2    A      93       31
3    B      86       39
4    B      88       34
5    B      82       30</b>
<h3>Example 1: Calculate Conditional Mean for Categorical Variable</h3>
The following code shows how to calculate the mean of the ‘points’ column for only the rows in the DataFrame where the ‘team’ column has a value of ‘A.’
<b>#calculate mean of 'points' column for rows where team equals 'A'
df.loc[df['team'] == 'A', 'points'].mean()
94.0
</b>
The mean value in the ‘points’ column for the rows where ‘team’ is equal to ‘A’ is <b>94</b>.
We can manually verify this by calculating the average of the points values for only the rows where ‘team’ is equal to ‘A’:
Average of Points: (99 + 90 + 93) / 3 = <b>94</b>
<h3>Example 2: Calculate Conditional Mean for Numeric Variable</h3>
The following code shows how to calculate the mean of the ‘assists’ column for only the rows in the DataFrame where the ‘points’ column has a value greater than or equal to 90.
<b>#calculate mean of 'assists' column for rows where 'points' >= 90
df.loc[df['points'] >= 90, 'assists'].mean()
30.666666666666668
</b>
The mean value in the ‘assists’ column for the rows where ‘points’ is greater than or equal to 90 is <b>30.66667</b>.
We can manually verify this by calculating the average of the points values for only the rows where ‘team’ is equal to ‘A’:
Average of Assists: (33 + 28 + 31) / 3 = <b>30.66667</b>
<h2><span class="orange">Create New Variables in R with mutate() and case_when()</span></h2>
Often you may want to create a new variable in a data frame in R based on some condition. Fortunately this is easy to do using the <b>mutate()</b> and <b>case_when()</b> functions from the  dplyr  package.
This tutorial shows several examples of how to use these functions with the following data frame:
<b>#create data frame
df &lt;- data.frame(player = c('a', 'b', 'c', 'd', 'e'), position = c('G', 'F', 'F', 'G', 'G'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#view data frame
df
  player position points rebounds
1      a        G     12        5
2      b        F     15        7
3      c        F     19        7
4      d        G     22       12
5      e        G     32       11
</b>
<h3>Example 1: Create New Variable Based on One Existing Variable</h3>
The following code shows how to create a new variable called ‘scorer’ based on the value in the points column:
<b>library(dplyr)
#define new variable 'scorer' using mutate() and case_when()
df %>%
  mutate(scorer = case_when(points &lt; 15 ~ 'low',           points &lt; 25 ~ 'med',           points &lt; 35 ~ 'high'))
  player position points rebounds scorer
1      a        G     12        5    low
2      b        F     15        7    med
3      c        F     19        7    med
4      d        G     22       12    med
5      e        G     32       11   high</b>
<h3>Example 2: Create New Variable Based on Several Existing Variables</h3>
The following code shows how to create a new variable called ‘type’ based on the value in the player and position column:
<b>library(dplyr)
#define new variable 'type' using mutate() and case_when()
df %>%
  mutate(type = case_when(player == 'a' | player == 'b' ~ 'starter',            player == 'c' | player == 'd' ~ 'backup',            position == 'G' ~ 'reserve'))
  player position points rebounds    type
1      a        G     12        5 starter
2      b        F     15        7 starter
3      c        F     19        7  backup
4      d        G     22       12  backup
5      e        G     32       11 reserve</b>
The following code shows how to create a new variable called ‘valueAdded’ based on the value in the points and rebounds columns:
<b>library(dplyr)
#define new variable 'valueAdded' using mutate() and case_when()
df %>%
  mutate(valueAdded = case_when(points &lt;= 15 & rebounds &lt;=5 ~ 2,                points &lt;=15 & rebounds > 5 ~ 4,                points &lt; 25 & rebounds &lt; 8 ~ 6,                points &lt; 25 & rebounds > 8 ~ 7,                points >=25 ~ 9))
  player position points rebounds valueAdded
1      a        G     12        5          2
2      b        F     15        7          4
3      c        F     19        7          6
4      d        G     22       12          7
5      e        G     32       11          9</b>
<h2><span class="orange">How to Calculate Conditional Probability in Excel</span></h2>
The <b>conditional probability </b>that event <em>A </em>occurs, given that event <em>B </em>has occurred, is calculated as follows:
P(A|B) = P(A∩B) / P(B)
where:
P(A∩B) = the probability that event <em>A </em>and event <em>B </em>both occur. 
P(B) = the probability that event B occurs.
This formula is particularly useful when calculating probabilities for a two-way table, which is a table that displays the frequencies (or “counts”) for two categorical variables.
For example, the following two-way table shows the results of a survey that asked 300 people which sport they liked best: baseball, basketball, football, or soccer. The rows display the gender of the respondent and the columns show which sport they chose:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/condProb0.png">
This is a <em>two-way </em>table because we have two  categorical variables : gender and favorite sport.
Next, we’ll show how to calculate conditional probabilities for two-way tables in Excel.
<h2>How to Calculate Conditional Probability in Excel</h2>
Suppose we’re interested in answering questions like:
<em>“What is the probability that a respondent is male, given their favorite sport is baseball?”</em>
We can find the answer by using the conditional probability formula:
P(male|baseball) = P(male∩baseball) / P(baseball) = (34/300) / (68/300) = <b>0.5</b>
Thus, the probability that a respondent is male, given their favorite sport is baseball, is 0.5 (or 50%). 
We can calculate conditional probabilities for other scenarios in the table using a similar formula. The image below shows how to calculate every conditional probability in the table, along with the formula used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/condProb1.png">
Notice that for every conditional probability calculation, we’re simply using the conditional probability formula of P(A|B) = P(A∩B) / P(B).
For example, the probability that a respondent’s favorite sport is soccer, given they are female, is calculated as:
P(soccer|female) = P(soccer∩female) / P(female)
Out of the 300 respondents, there are exactly 44 who are female <em>and </em>prefer soccer as their favorite sport, thus P(soccer∩female) = 44/300.
And out of the 300 respondents, there are 150 who are female, thus P(female) = 150/300.
Thus, P(soccer|female) = P(soccer∩female) / P(female) = (44/300) / (150/300) = <b>0.2933</b>.
We perform a similar calculation for every conditional probability scenario.
<h2><span class="orange">How to Calculate Conditional Probability in Python</span></h2>
The <b>conditional probability</b> that event <em>A</em> occurs, given that event <em>B</em> has occurred, is calculated as follows:
P(A|B) = P(A∩B) / P(B)
where:
P(A∩B) = the probability that event <em>A </em>and event <em>B </em>both occur. 
P(B) = the probability that event B occurs.
The following example shows how to use this formula to calculate conditional probabilities in Python.
<h3>Example: Calculate Conditional Probability in Python</h3>
Suppose we send out a survey to 300 individuals asking them which sport they like best: baseball, basketball, football, or soccer.
We can create the following table in Python to hold the survey responses:
<b>import pandas as pd
import numpy as np
#create pandas DataFrame with raw data
df = pd.DataFrame({'gender': np.repeat(np.array(['Male', 'Female']), 150),   'sport': np.repeat(np.array(['Baseball', 'Basketball', 'Football',                                'Soccer', 'Baseball', 'Basketball',                                'Football', 'Soccer']),                     (34, 40, 58, 18, 34, 52, 20, 44))})
#produce contingency table to summarize raw data
survey_data = pd.crosstab(index=df['gender'], columns=df['sport'], margins=True)
#view contingency table
survey_data
sportBaseballBasketballFootballSoccer All
gender
Female      34        52      20    44 150
Male      34        40      58    18 150
All      68        92      78    62 300</b>
<b>Related</b>:  How to Use pd.crosstab() to Create Contingency Tables in Python 
We can use the following syntax to extract values from the table:
<b>#extract value in second row and first column 
survey_data.iloc[1, 0]
[1] 34
</b>
We can use the following syntax to calculate the probability that an individual is male, given that they prefer baseball as their favorite sport:
<b>#calculate probability of being male, given that individual prefers baseball
survey_data.iloc[1, 0] / survey_data.iloc[2, 0]
0.5
</b>
And we can use the following syntax to calculate the probability that an individual prefers basketball as their favorite sport, given that they’re female:
<b>#calculate probability of preferring basketball, given that individual is female
survey_data.iloc[0, 1] / survey_data.iloc[0, 4]
0.3466666666666667
</b>
We can use this basic approach to calculate any conditional probability we’d like from the contingency table.
<h2><span class="orange">How to Calculate Conditional Probability in R</span></h2>
The <b>conditional probability</b> that event <em>A</em> occurs, given that event <em>B</em> has occurred, is calculated as follows:
P(A|B) = P(A∩B) / P(B)
where:
P(A∩B) = the probability that event <em>A </em>and event <em>B </em>both occur. 
P(B) = the probability that event B occurs.
The following examples show how to use this formula to calculate conditional probabilities in R.
<h3>Example 1: Calculate Conditional Probability Using Values</h3>
Suppose we send out a survey to 300 individuals asking them which sport they like best: baseball, basketball, football, or soccer.
Suppose we know that the probability that an individual is male <em>and</em> prefers baseball as their favorite sport is <b>0.113</b>.
Suppose we also know that the probability that any individual prefers baseball as their favorite sport is <b>0.226</b>.
Given that an individual prefers baseball, we could calculate the probability that they’re male to be:
P(Male|Prefers Baseball) = P(Male∩Prefers Baseball) / P(Prefers Baseball)
P(Male|Prefers Baseball) = 0.113 / 0.226
P(Male|Prefers Baseball) = 0.5
Given that an individual prefers baseball, the probability that they’re male is <b>0.5</b>.
Here’s how we can calculate this probability in R:
<b>#define probability of being male <em>and</em> preferring baseball
p_male_baseball &lt;- 0.113
#define probability of preferring baseball
p_baseball &lt;- 0.226
#calculate probability of being male, given that individual prefers baseball
p_male_baseball / p_baseball
[1] 0.5
</b>
<h3>Example 2: Calculate Conditional Probability Using a Table</h3>
Suppose we send out a survey to 300 individuals asking them which sport they like best: baseball, basketball, football, or soccer.
We can create the following table in R to hold the survey responses:
<b>#create data frame to hold survey responses
df &lt;- data.frame(gender=rep(c('Male', 'Female'), each=150), sport=rep(c('Baseball', 'Basketball', 'Football', 'Soccer',             'Baseball', 'Basketball', 'Football', 'Soccer'),              times=c(34, 40, 58, 18, 34, 52, 20, 44)))
#create two-way table from data frame
survey_data &lt;- addmargins(table(df$gender, df$sport))
#view table
survey_data
         Baseball Basketball Football Soccer  Sum
  Female       34         52       20     44  150
  Male         34         40       58     18  150
  Sum          68         92       78     62  300
</b>
We can use the following syntax to extract values from the table:
<b>#extract value in second row and first column 
survey_data[2, 1]
[1] 34
</b>
We can use the following syntax to calculate the probability that an individual is male, given that they prefer baseball as their favorite sport:
<b>#calculate probability of being male, given that individual prefers baseball
survey_data[2, 1] / survey_data[3, 1]
[1] 0.5
</b>
And we can use the following syntax to calculate the probability that an individual prefers basketball as their favorite sport, given that they’re female:
<b>#calculate probability of preferring basketball, given that individual is female
survey_data[1, 2] / survey_data[1, 5]
[1] 0.3466667
</b>
We can use this basic approach to calculate any conditional probability we’d like from the table.
<h2><span class="orange">4 Examples of Using Conditional Probability in Real Life</span></h2>
The <b>conditional probability </b>that event <em>A </em>occurs, given that event <em>B </em>has occurred, is calculated as follows:
<b>P(A|B) = P(A∩B) / P(B)</b>
where:
<b>P(A∩B)</b> = the probability that event <em>A </em>and event <em>B </em>both occur. 
<b>P(B)</b> = the probability that event B occurs.
Conditional probability is used in all types of areas in real life including weather forecasting, sports betting, sales forecasting, and more.
The following examples share how conditional probability is used in 4 real-life situations on a regular basis.
<h3>Example 1: Weather Forecasting</h3>
One of the most common real life examples of using conditional probability is <b>weather forecasting</b>.
Weather forecasters use conditional probability to predict the likelihood of future weather conditions, given current conditions.
For example, suppose the following two probabilities are known:
P(cloudy) = 0.25
P(rainy∩cloudy) = 0.15
A weather forecaster could use these values to calculate the probability that it will rain on a particular day, given that it is cloudy out:
P(rain|cloudy) = P(rainy∩cloudy) / P(cloudy)
P(rain|cloudy) = 0.15 / 0.25
P(rain|cloudy) = 0.6
The probability that it will rain <em>given</em> that it is cloudy out, is 0.6 or <b>60%</b>.
This is a simplified example, but in real life weather forecasters use computer programs to take in data on current weather conditions and use conditional probability to calculate the likelihood of future weather conditions.
<h3>Example 2: Sports Betting</h3>
Conditional probability is frequently used by sports betting companies to determine the odds they should set for certain teams to win certain games.
For example, suppose the following two probabilities are known about some basketball team:
P(Team A star player is hurt) = 0.15
P(Team A wins∩Team A start player is hurt) = 0.02
The company could use these values to calculate the probability that team A will win, given that their star player is hurt:
P(Team A Wins|star is hurt) = P(Team A Wins∩star is hurt) / P(star is hurt)
P(Team A Wins|star is hurt) = 0.02 / 0.15
P(Team A Wins|star is hurt) = 0.13
The probability that Team A will win <em>given</em> that their star player is hurt is is 0.13 or <b>13%</b>.
If the sports betting company finds out ahead of the game that the star player is hurt, then they can use conditional probability to update their odds and payouts accordingly.
This happens all the time with sports betting companies when they calculate various odds for basketball, football, baseball, hockey matches, and more.
<h3>Example 3: Sales Forecasting</h3>
Retail companies use conditional probability to predict the chances that they’ll sell out of a certain product based on product promotions.
For example, suppose the following two probabilities are known:
P(promotion) = 0.35
P(sell out∩promotion) = 0.15
A retail company could use these values to calculate the probability that they’ll sell out of a certain product, given that a product promotion is ran that day:
P(sell out|promotion) = P(sell out∩promotion) / P(promotion)
P(sell out|promotion) = 0.15 / 0.35
P(sell out|promotion) = 0.428
The probability that the retail company sells out of the product <em>given</em> that a promotion is ran that day is 0.428 or <b>42.8%</b>.
If the retail company knows ahead of time that a promotion will be ran, they can increase their inventory ahead of time so they reduce the chances of selling out.
<h3>Example 4: Traffic</h3>
Traffic engineers use conditional probability to predict the likelihood of traffic jams based on stop light failures.
For example, suppose the following two probabilities are known:
P(stop light failure) = 0.001
P(traffic jam∩stop light failure) = 0.0004
A retail company could use these values to calculate the probability that they’ll sell out of a certain product, given that a product promotion is ran that day:
P(traffic jam|stop light failure) = P(traffic jam∩stop light failure) / P(stop light failure)
P(traffic jam|stop light failure) = 0.0004 / 0.001
P(traffic jam|stop light failure) = 0.4
The probability that there will be a traffic jam <em>given</em> that there is a stop light failure is 0.4 or <b>40%</b>.
Traffic engineers can use this conditional probability to decide if they need to design a different route to redirect traffic since a traffic jam is likely to occur if there is a traffic light failure.
<h2><span class="orange">How to Find Conditional Relative Frequency in a Two-Way Table</span></h2>
A <b>two-way frequency table</b> is a table that displays the frequencies (or “counts”) for two categorical variables.
For example, the following two-way table shows the results of a survey that asked 100 people which sport they liked best: baseball, basketball, or football. The rows display the gender of the respondent and the columns show which sport they chose:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay1.jpg">
This is a <em>two-way </em>table because we have two categorical variables: <em>gender</em> and <em>favorite sport</em>.
The numbers in the body of the table are called <b>joint frequencies</b> and the numbers that display the total row and column frequencies are called <b>marginal frequencies</b>.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay2-1.jpg">
Here is how to interpret this table:
A total of 100 people responded to this survey.
Of the 100 total respondents, 48 were males and 52 were females.
A total of 36 respondents said they like baseball the most, 31 said they like basketball the most, and 33 said they like football the most.
A total of 13 males said they like baseball the most, 23 females said they like baseball the most, 15 males said they like basketball the most, 16 females said they like basketball the most, 20 males said they like football the most, and 13 females said they like football the most.
<h2>How to Find Conditional Relative Frequencies Using a Two-Way Table</h2>
A two-way frequency table is useful for helping us find <b>conditional relative frequencies</b>. These are frequencies that are based on some <em>condition</em>. 
The following examples illustrate how to use a two-way frequency table to find conditional relative frequencies.
<h3>Example 1</h3>
What is the probability that a survey respondent likes basketball the most, <em>given that the respondent is male</em>?
Since we are given the condition that the respondent is male, we only want to look at the row that contains the male responses. To find the probability that the respondent likes basketball the most, we can simply divide the number of male respondents who like basketball the most by the total number of males:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay3.jpg">
Thus, the probability that a survey respondent likes basketball the most, <em>given that the respondent is male</em> is 0.3125, or <b>31.25%</b>.
<h3>Example 2</h3>
What is the probability that a survey respondent likes baseball the most, <em>given that the respondent is female</em>?
Since we are given the condition that the respondent is female, we only want to look at the row that contains the female responses. To find the probability that the respondent likes baseball the most, we can simply divide the number of female respondents who like baseball the most by the total number of females:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay4.jpg">
Thus, the probability that a survey respondent likes baseball the most, <em>given that the respondent is female</em> is 0.4423, or <b>44.23%</b>.
<h3>Example 3</h3>
What is the probability that a survey respondent is male, <em>given that the respondent likes football the most</em>?
Since we are given the condition that the respondent likes football the most, we only want to look at the column that contains the responses of people who like football the most. To find the probability that the respondent is male, we can simply divide the number of males who like football the most by the total number of respondents who like football the most:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay5.jpg">
Thus, the probability that a survey respondent is male, <em>given that the respondent likes football the most</em><em> </em>is 0.606, or <b>60.6%</b>.
<h3>Example 4</h3>
What is the probability that a survey respondent is female, <em>given that the respondent likes baseball the most</em>?
Since we are given the condition that the respondent likes baseball the most, we only want to look at the column that contains the responses of people who like baseball the most. To find the probability that the respondent is female, we can simply divide the number of females who like baseball the most by the total number of respondents who like baseball the most:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay6.jpg">
Thus, the probability that a survey respondent is female, <em>given that the respondent likes baseball the most</em><em> </em>is 0.6389, or <b>63.89%</b>.
<h3>Example 5</h3>
What is the probability that a survey respondent likes baseball <em>or </em>football the most, <em>given that the respondent is male</em>?
Since we are given the condition that the respondent is male, we only want to look at the row that contains the responses of males. To find the probability that the respondent likes baseball <em>or </em>football the most, we can simply divide the number of males who like baseball or football the most by the total number of male respondents:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay7.jpg">
Thus, the probability that a survey respondent likes baseball <em>or </em>football the most, <em>given that the respondent is male</em><em> </em>is 0.6875, or <b>68.75%</b>.
<h3>Example 6</h3>
What is the probability that a survey respondent likes baseball <em>or </em>basketball the most, <em>given that the respondent is female</em>?
Since we are given the condition that the respondent is female, we only want to look at the row that contains the responses of females. To find the probability that the respondent likes baseball <em>or </em>basketball the most, we can simply divide the number of females who like baseball or basketball the most by the total number of female respondents:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay8.jpg">
Thus, the probability that a survey respondent likes baseball <em>or </em>basketball the most, <em>given that the respondent is female</em><em> </em>is 0.75, or <b>75%</b>.
<h3>Example 7</h3>
What is the probability that a survey respondent does <em>not </em>like football the most, <em>given that the respondent is male</em>?
Since we are given the condition that the respondent is male, we only want to look at the row that contains the responses of males. To find the probability that the respondent does not like football the most, we can simply divide the number of males who like baseball or basketball the most by the total number of male respondents:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/twoWay9.jpg">
Thus, the probability that a survey respondent does <em>not </em>like football the most, <em>given that the respondent is male</em><em> </em>is 0.5833, or <b>58.33%</b>.
<h2><span class="orange">How to Calculate a Conditional Running Total in Excel</span></h2>
Often you may want to calculate a running total of values in a column in Excel that is conditional on the values in some other column.
The following example shows how to do so.
<h3>Example: Conditional Running Total in Excel</h3>
Suppose we have the following dataset in Excel that shows the total sales made by some company on various days:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/rt1.jpg"477">
Suppose we’d like to calculate a running total of sales that restarts on each new day.
First, we can enter the first value of sales in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/rt2.jpg"506">
Next, we can use the following formula to calculate a running total of sales that restarts on each new day:
<b>=IF(A3=A2, C2+B3, B3)
</b>
This formula checks if the current date in column A is equal to the date in the previous row.
If the dates match, then the sales from the current row is added to the sales from the previous row.
If the dates don’t match, then the sales from the current row is used as the new starting amount for sales.
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/rt3.jpg"524">
Column C shows the running total of sales for each day.
Notice that the running total of sales in column C restarts for each new day in column A.
<h2><span class="orange">The 6 Confidence Interval Assumptions to Check</span></h2>
When constructing confidence intervals, it’s important that certain assumptions are met. If these assumptions are violated, then the confidence interval can become unreliable.
Here are the six assumptions you should check when constructing a confidence interval:
<h3>Assumption #1: Random Sampling</h3>
The data should be collected using a  random sampling method  (a method in which each individual in a population is equally likely to be included in the sample) so that the sample data you’re working with is  representative  of the overall population of interest.
<h3>Assumption #2: Independence</h3>
Each observation in the sample data should be independent of every other observation. This means that no two observations in a sample are related to each other or affect each other in any way.
If you use a random sampling method to collect the data, this assumption is typically met.
<h3>Assumption #3: Large Sample</h3>
In order to apply the  Central Limit Theorem , our sample size must be sufficiently large. In general, we consider “sufficiently large” to be 30 or larger. However, this number can vary based on the underlying shape of the population distribution.
In particular:
If the population distribution is symmetric, sometimes a sample size as small as 15 is sufficient.
If the population distribution is skewed, generally a sample size of at least 30 is needed.
If the population distribution is extremely skewed, then a sample size of 40 or higher may be necessary.
<h3>Assumption #4: The 10% Condition</h3>
The sample size should be less than or equal to 10% of the population size. This further ensures that the observations in the data are independent.
<h3>Assumption #5: The Success / Failure Condition</h3>
When working with confidence intervals that involve proportions, there should be at least 10 expected successes and 10 expected failures in a sample in order to use the normal distribution as an approximation. 
<h3>Assumption #6: Homogeneity of Variances</h3>
When working with confidence intervals that involve two samples, it’s assumed that the two populations that the samples came from have equal variances.
As a rule of thumb, if the ratio of the larger variance to the smaller variance is less than 4, then we can assume the variances are approximately equal and use the two sample t-test.
For example, if sample 1 has a variance of 24.5 and sample 2 has a variance of 15.2 then the ratio of the larger sample variance to the smaller would be calculated as 24.5 / 15.2 = 1.61.
Since this ratio is less than 4, we could assume that the variances between the two groups are approximately equal.
<h2><span class="orange">How to Interpret a Confidence Interval that Contains Zero</span></h2>
In statistics, a <b>confidence interval</b> is a range of values that is likely to contain a  population parameter  with a certain level of confidence.
If we calculate a  confidence interval for the difference between two population means  and find that the confidence interval contains the value zero, this means we think that zero is a reasonable value for the true difference between the two population means.
In other words, if a confidence interval contains zero then we would say there is strong evidence that there is not a ‘significant’ difference between the two population means.
The following examples explain how to interpret confidence intervals with and without the value zero in them.
<h3>Example 1: Confidence Interval Contains Zero</h3>
Suppose a biologist wants to estimate the difference in mean weight between two different species of turtles. She goes out and gathers a random sample of 15 turtles from each population.
Here is the summary data for each sample:
<b>Sample 1:</b>
x<sub>1</sub> = 310
s<sub>1</sub> = 18.5
n<sub>1</sub> = 15
<b>Sample 2:</b>
x<sub>2</sub> = 300
s<sub>2</sub> = 16.4
n<sub>2</sub> = 15
We can plug these numbers into the  Confidence Interval for the Difference in Population Means Calculator  to find the following 95% confidence interval for the true difference in mean weights between the two species:
<b>95% Confidence interval =  [-3.0757, 23.0757]</b>
Since this confidence interval contains the value zero, this means we think that zero is a reasonable value for the true difference in mean weights between the two species of turtles.
In other words, at a 95% confidence level, we would say that there is not a significant difference in the mean weight between the two species.
<h3>Example 2: Confidence Interval Does Not Contain Zero</h3>
Suppose a professor wants to estimate the difference in mean exam score between two different studying techniques. He recruits 20 random students to use technique A and 20 random students to use technique B, then has each student take the same final exam.
Here is the summary of exam scores for each group:
<b>Technique A:</b>
x<sub>1</sub> = 91
s<sub>1</sub> = 4.4
n<sub>1</sub> = 20
<b>Technique B:</b>
x<sub>2</sub> = 86
s<sub>2</sub> = 3.5
n<sub>2</sub> = 20
We can plug these numbers into the  Confidence Interval for the Difference in Population Means Calculator  to find the following 95% confidence interval for the true difference in mean exam scores:
<b>95% Confidence interval =  [2.4550, 7.5450]</b>
Since this confidence interval does not contain the value zero, this means we think that zero is not a reasonable value for the true difference in mean exam scores between the two two groups.
In other words, at a 95% confidence level, we would say that there is a significant difference in the mean exam score between the two groups.
<h2><span class="orange">Confidence Interval for a Correlation Coefficient Calculator</span></h2>
A <b>confidence interval for a correlation coefficient</b> is a range of values that is likely to contain a population correlation coefficient with a certain level of confidence.
To find a confidence interval for a population correlation coefficient, simply fill in the boxes below and then click the “Calculate” button.
<label><b>r</b> (sample correlation coefficient)</label>
<input type="number" id="r" value="0.56" min="-1" max="1" step=".01">
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="30">
<label>Confidence level</label>
<input type="number" id="conf" value=".95" min="0" max="1" step=".01">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
95% C.I. = [0.2502, 0.7658]
You can be 95% confident that the interval [0.2502, 0.7658] contains the true population correlation coefficient.
<script>
function calc() {
//get input values
var r = +document.getElementById('r').value;
var n = +document.getElementById('n').value;
var conf = +document.getElementById('conf').value;
var confUse = conf - (-(1-conf)/2);
var confOut = conf*100;
//calculate stuff
var zr = Math.log((1-(-1*r))/(1-r))/2;
var z = Math.abs(jStat.normal.inv(confUse, 0, 1));
var L = zr - (z/Math.sqrt(n-3));
var U = zr - -1*((z/Math.sqrt(n-3)));
var low = (Math.exp(2*L)-1) / (Math.exp(2*L)-(-1*1));
var high = (Math.exp(2*U)-1) / (Math.exp(2*U)-(-1*1));
//output results
document.getElementById('confOut').innerHTML = confOut;
document.getElementById('confOut2').innerHTML = confOut;
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('low2').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
document.getElementById('high2').innerHTML = high.toFixed(4);
}
</script>
<h2><span class="orange">Confidence Interval for a Correlation Coefficient</span></h2>
A <b>confidence interval for a correlation coefficient </b>is a range of values that is likely to contain a population correlation coefficient with a certain level of confidence.
This tutorial explains the following:
The motivation for creating this type of confidence interval.
The formula to create this type of confidence interval.
An example of how to create this type of confidence interval.
How to interpret this type of confidence interval.
<h3>Confidence Interval for a Correlation Coefficient: Motivation</h3>
The reason to create a  confidence interval  for a correlation coefficient is to capture our uncertainty when estimating a population correlation coefficient.
For example, suppose we want to estimate the correlation coefficient between height and weight of residents in a certain county. Since there are thousands of residents in the county, it would be too costly and time-consuming to go around and gather information on every resident’s height and weight.
Instead, we might select a  simple random sample  of residents and simply gather information about them.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CIprop1.png">
Since we select a random sample of residents, there is no guarantee that the correlation coefficient between height and weight for these residents in the sample will exactly match the correlation coefficient in the larger population.
So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true correlation coefficient between height and weight of residents in this county.
<h3>Confidence Interval for a Correlation Coefficient: Formula</h3>
We use the following steps to calculate a confidence interval for a population correlation coefficient, based on sample size <b>n </b>and sample correlation coefficient <b>r</b>.
<b>Step 1:  Perform Fisher transformation.</b>
Let z<sub>r</sub> = ln((1+r) / (1-r)) / 2
<b>Step 2: Find log upper and lower bounds.</b>
Let L = z<sub>r</sub>  –  (z<sub>1-α/2</sub> /√n-3)
Let U = z<sub>r</sub>  +  (z<sub>1-α/2</sub> /√n-3)
<b>Step 3: Find confidence interval.</b>
The final confidence interval can be found using the following formula:
Confidence interval = [(e<sup>2L</sup>-1)/(e<sup>2L</sup>+1),  (e<sup>2U</sup>-1)/(e<sup>2U</sup>+1)]
<h3>Confidence Interval for a Correlation Coeffficient: Example</h3>
Suppose we want to estimate the correlation coefficient between height and weight of residents in a certain county. We select a random sample of 30 residents and find the following information:
Sample size <b>n = 30</b>
Correlation coefficient between height and weight <b>r = 0.56</b>
Here is how to find a 95% confidence interval for the population correlation coefficient:
<b>Step 1:  Perform Fisher transformation.</b>
Let z<sub>r</sub> = ln((1+r) / (1-r)) / 2 = ln((1+.56) / (1-.56)) / 2 = <b>0.6328</b>
<b>Step 2: Find log upper and lower bounds.</b>
Let L = z<sub>r</sub>  –  (z<sub>1-α/2</sub> /√n-3) = .6328  –  (1.96 /√30-3) = <b>.2556</b>
Let U = z<sub>r</sub>  +  (z<sub>1-α/2</sub> /√n-3) = .6328  +  (1.96 /√30-3) = <b>1.01</b>
<b>Step 3: Find confidence interval.</b>
Confidence interval = [(e<sup>2L</sup>-1)/(e<sup>2L</sup>+1),  (e<sup>2U</sup>-1)/(e<sup>2U</sup>+1)] 
Confidence interval = [(e<sup>2(.2556)</sup>-1)/(e<sup>2(.2556)</sup>+1),  (e<sup>2(1.01)</sup>-1)/(e<sup>2(1.01)</sup>+1)] = <b>[.2502, .7658]</b>
<b>Note:</b> You can also find this confidence interval by using the  Confidence Interval for a Correlation Coefficient Calculator .
<h3>Confidence Interval for a Correlation Coefficient: Interpretation</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [.2502, .7658] contains the true population correlation coefficient between height and weight of residents in this county.
Another way of saying the same thing is that there is only a 5% chance that the true population correlation coefficient lies outside of the 95% confidence interval.
That is, there’s only a 5% chance that the true population correlation coefficient between height and weight of residents in this county is less than .2502 or greater than .7658.
<h2><span class="orange">Confidence Interval for the Difference Between Means Calculator</span></h2>
A <b>confidence interval for a difference between means</b> is a range of values that is likely to contain the true difference between two population means with a certain level of confidence.
The formula to calculate the confidence interval is:
Confidence interval = (x<sub>1</sub>–x<sub>2</sub>)  +/-  t*√((s<sub>p</sub><sup>2</sup>/n<sub>1</sub>) + (s<sub>p</sub><sup>2</sup>/n<sub>2</sub>))
where:
x<sub>1</sub>, x<sub>2</sub>: sample 1 mean, sample 2 mean
t: the t-critical value based on the confidence level
s<sub>p</sub><sup>2</sup>: pooled variance
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
To find a confidence interval for a difference between two means, simply fill in the boxes below and then click the “Calculate” button.
<label><b>x<sub>1</sub></b> (sample 1 mean)</label>
<input type="number" id="x1" value="15.1">
<label><b>s<sub>1</sub></b> (sample 1 standard deviation)</label>
<input type="number" id="s1" value="4.3">
<label><b>n<sub>1</sub></b> (sample 1 size)</label>
<input type="number" id="n1" value="22">
<label><b>x<sub>2</sub></b> (sample 2 mean)</label>
<input type="number" id="x2" value="14.3">
<label><b>s<sub>2</sub></b> (sample 2 standard deviation)</label>
<input type="number" id="s2" value="4.9">
<label><b>n<sub>2</sub></b> (sample 2 size)</label>
<input type="number" id="n2" value="22">
<label><b>Confidence level</b></label>
<input type="number" id="conf" value="0.95" step="0.01" min="0" max="1">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
95% C.I. = [-2.0049, 3.6049]
You can be 95% confident that the interval [-2.0049, 3.6049] contains the true difference between the population means μ<sub>1</sub> and μ<sub>2</sub>.
<script>
function calc() {
//get input values
var x1 = +document.getElementById('x1').value;
var s1 = +document.getElementById('s1').value;
var n1 = +document.getElementById('n1').value;
var x2 = +document.getElementById('x2').value;
var s2 = +document.getElementById('s2').value;
var n2 = +document.getElementById('n2').value;
var conf = +document.getElementById('conf').value;
var confUse = conf - (-(1-conf)/2);
var confOut = conf*100;
var df = n1 - (-1*n2) - 2;
var s2p = ((n1-1)*(Math.pow(s1,2)) - ((-1*(n2-1))*Math.pow(s2,2))) / df;
//calculate stuff
var meanDiff = x1-x2;
var t = jStat.studentt.inv(confUse, df);
var se = Math.sqrt((s2p/n1) - (-1*s2p/n2));
var low = meanDiff - (t*se);
var high = meanDiff - (-1*t*se);
console.log(s2p);
//output results
document.getElementById('confOut').innerHTML = confOut;
document.getElementById('confOut2').innerHTML = confOut;
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('low2').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
document.getElementById('high2').innerHTML = high.toFixed(4);
}
</script>
<h2><span class="orange">Confidence Interval for the Difference Between Means</span></h2>
A <b>confidence interval (C.I.) for a difference between means </b>is a range of values that is likely to contain the true difference between two population means with a certain level of confidence.
This tutorial explains the following:
The motivation for creating this confidence interval.
The formula to create this confidence interval.
An example of how to calculate this confidence interval.
How to interpret this confidence interval.
<h3>C.I. for the Difference Between Means: Motivation</h3>
Often researchers are interested in estimating the difference between two population means. To estimate this difference, they’ll go out and gather a random sample from each population and calculate the mean for each sample. Then, they can compare the difference between the two means.
However, they can’t know for sure if the difference in the sample means matches the true difference in the population means which is why they may create a  confidence interval  for the difference between the two means. This provides a range of values that is likely to contain the true difference between the population means.
For example, suppose we want to estimate the difference in mean weight between two different species of turtles. Since there are thousands of turtles in each population, it would be too time-consuming and costly to go around and weigh each individual turtle.
Instead, we might take a  simple random sample  of 15 turtles from each population and use the mean weight in each sample to estimate the true difference in mean weight between the two populations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/confIntmeans1.png">
The problem is that our samples are random, so the difference in mean weights between the two samples is not guaranteed to exactly match the difference in mean weights between the two populations. So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true difference in mean weight between the two populations.
<h3>
<b>C.I. for the Difference Between Means: Formula</b>
</h3>
We use the following formula to calculate a confidence interval for a difference between two means:
<b>Confidence interval</b> = (x<sub>1</sub>–x<sub>2</sub>) +/- t*√((s<sub>p</sub><sup>2</sup>/n<sub>1</sub>) + (s<sub>p</sub><sup>2</sup>/n<sub>2</sub>))
where:
x<sub>1</sub>, x<sub>2</sub>: sample 1 mean, sample 2 mean
t: the t-critical value based on the confidence level and (n<sub>1</sub>+n<sub>2</sub>-2) degrees of freedom
s<sub>p</sub><sup>2</sup>: pooled variance
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
where:
The pooled variance is calculated as: s<sub>p</sub><sup>2</sup></b> = ((n<sub style="color: #000000;">1</sub>-1)s<sub style="color: #000000;">1</sub><sup style="color: #000000;">2</sup> + (n<sub style="color: #000000;">2</sub>-1)s<sub style="color: #000000;">2</sub><sup style="color: #000000;">2</sup>) / (n<sub style="color: #000000;">1</sub>+n<sub style="color: #000000;">2</sub>-2)
The t-critical value <b>t </b>can be found using  the Inverse t Distribution calculator 
<h3>
<b>C.I. for the Difference Between Means: Example</b>
</h3>
Suppose we want to estimate the difference in mean weight between two different species of turtles, so we go out and gather a random sample of 15 turtles from each population. Here is the summary data for each sample:
<b>Sample 1:</b>
x<sub>1</sub> = 310
s<sub>1</sub> = 18.5
n<sub>1</sub> = 15
<b>Sample 2:</b>
x<sub>2</sub> = 300
s<sub>2</sub> = 16.4
n<sub>2</sub> = 15
Here is how to find various confidence intervals for the true difference in population mean weights:
<b>90% Confidence Interval:</b>
(310-300) +/- 1.70*√((305.61/15) + (305.61/15)) =<b>  [-0.8589, 20.8589]
</b>
<b>95% Confidence Interval:</b>
(310-300) +/- 2.05*√((305.61/15) + (305.61/15)) =<b>  [-3.0757, 23.0757]
</b>
<b>99% Confidence Interval:</b>
(310-300) +/- 2.76*√((305.61/15) + (305.61/15)) =<b>   [-7.6389, 27.6389]
</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Statology Confidence Interval for the Difference Between Means Calculator .</em>
You’ll notice that the higher the confidence level, the wider the confidence interval. This should make sense because wider intervals are more likely to contain the true population mean, thus we’re more “confident” that the interval contains the true population mean.
<h3>
<b>C.I. for the Difference Between Means: Interpretation</b>
</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [-3.0757, 23.0757] contains the true difference in mean weight between the two turtle populations.
Since this interval contains the value “0” it means that it’s possible that there is no difference in the mean weight between the turtles in these two populations. In other words, we cannot say with 95% confidence that there is a difference in mean weight between the turtles in these two populations.
<h2><span class="orange">Confidence Interval for the Difference in Proportions Calculator</span></h2>
A <b>confidence interval for a difference in proportions</b> is a range of values that is likely to contain the true difference between two population proportions with a certain level of confidence.
The formula to calculate the confidence interval is:
Confidence interval = (p<sub>1</sub> – p<sub>2</sub>)  +/-  z*√(p<sub>1</sub>(1-p<sub>1</sub>)/n<sub>1</sub> + p<sub>2</sub>(1-p<sub>2</sub>)/n<sub>2</sub>)
where:
p<sub>1</sub>, p<sub>2</sub>: sample 1 proportion, sample 2 proportion
z: the z-critical value based on the confidence level
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
To find a confidence interval for a difference between two population proportions, simply fill in the boxes below and then click the “Calculate” button.
<label><b>n<sub>1</sub></b> (sample 1 size)</label>
<input type="number" id="n1" value="100">
<label><b>p<sub>1</sub></b> (sample 1 proportion)</label>
<input type="number" id="p1" value="0.62">
<label><b>n<sub>2</sub></b> (sample 2 size)</label>
<input type="number" id="n2" value="100">
<label><b>p<sub>2</sub></b> (sample 2 proportion)</label>
<input type="number" id="p2" value="0.46">
<label><b>Confidence level</b></label>
<input type="number" id="conf" value="0.95" step="0.01" min="0" max="1">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
95% C.I. = [0.0236, 0.2964]
You can be 95% confident that the interval [0.0236, 0.2964] contains the true difference between the population proportions.
<script>
function calc() {
//get input values
var n1 = +document.getElementById('n1').value;
var p1 = +document.getElementById('p1').value;
var n2 = +document.getElementById('n2').value;
var p2 = +document.getElementById('p2').value;
var conf = +document.getElementById('conf').value;
var confUse = conf - (-(1-conf)/2);
var confOut = conf*100;
//calculate stuff
var meanDiff = p1-p2;
var z = Math.abs(jStat.normal.inv(confUse, 0, 1));
var se = Math.sqrt(p1*(1-p1)/n1 - (-1*(p2*(1-p2)/n2)));
var low = meanDiff - (z*se);
var high = meanDiff - (-1*z*se);
//output results
document.getElementById('confOut').innerHTML = confOut;
document.getElementById('confOut2').innerHTML = confOut;
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('low2').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
document.getElementById('high2').innerHTML = high.toFixed(4);
}
</script>
<h2><span class="orange">Confidence Interval for the Difference in Proportions</span></h2>
A <b>confidence interval (C.I.) for a difference in proportions </b>is a range of values that is likely to contain the true difference between two population proportions with a certain level of confidence.
This tutorial explains the following:
The motivation for creating this confidence interval.
The formula to create this confidence interval.
An example of how to calculate this confidence interval.
How to interpret this confidence interval.
<h3>C.I. for the Difference in Proportions: Motivation</h3>
Often researchers are interested in estimating the difference between two population proportions. To estimate this difference, they’ll go out and gather a random sample from each population and calculate the proportion for each sample. Then, they can compare the difference between the two proportions.
However, they can’t know for sure if the difference in the sample proportons matches the true difference in the population proportions which is why they may create a  confidence interval  for the difference between the two proportions. This provides a range of values that is likely to contain the true difference between the population proportions.
For example, suppose we want to estimate the difference in the proportion of residents who support a certain law in county A compared to the proportion who support the law in county B.
Since there are thousands of residents in each county, it would take too long and be too costly to go around and survey every individual resident in each county.
Instead, we might take a  simple random sample  of residents from each county and use the proportion in favor of the law in each sample to estimate the true difference in proportions between the two counties:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/diffProps1.png">
Since our samples are random, the difference in proportions between the two samples is not guaranteed to exactly match the difference in proportions between the two populations. So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true difference in proportions between the two populations.
<h3>
<b>C.I. for the Difference in Proportions: Formula</b>
</h3>
We use the following formula to calculate a confidence interval for a difference between two population proportions:
<b>Confidence interval = (p<sub>1</sub>–p<sub>2</sub>)  +/-  z*√(p<sub>1</sub>(1-p<sub>1</sub>)/n<sub>1 </sub>+ p<sub>2</sub>(1-p<sub>2</sub>)/n<sub>2</sub>)</b>
where:
p<sub>1</sub>, p<sub>2</sub>: sample 1 proportion, sample 2 proportion
z: the z-critical value based on the confidence level
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
Notice that higher confidence levels correspond to larger z-values, which leads to wider confidence intervals. This means that, for example, a 95% confidence interval will be wider than a 90% confidence interval for the same set of data.
<h3>
<b>C.I. for the Difference in Proportions: Example</b>
</h3>
Suppose we want to estimate the difference in the proportion of residents who support a certain law in county A compared to the proportion who support the law in county B. Here is the summary data for each sample:
<b>Sample 1:</b>
n<sub>1</sub> = 100
p<sub>1</sub> = 0.62 (i.e. 62 out of 100 residents support the law)
<b>Sample 2:</b>
n<sub>2</sub> = 100
p<sub>2</sub> = 0.46 (i.e. 46 our of 100 residents support the law)
Here is how to find various confidence intervals for the difference in population proportions:
<b>90% Confidence Interval:</b>
(.62-.46) +/- 1.645*√(.62(1-.62)/100 + .46(1-.46)/100) =<b>  [.0456, .2744]
</b>
<b>95% Confidence Interval:</b>
(.62-.46) +/- 1.96*√(.62(1-.62)/100 + .46(1-.46)/100) = <b> [.0236, .2964]</b>
<b>99% Confidence Interval:</b>
(.62-.46) +/- 2.58*√(.62(1-.62)/100 + .46(1-.46)/100) =<b>  [-0.0192, 0.3392]</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Confidence Interval for the Difference in Proportions Calculator .</em>
<h3>
<b>C.I. for the Difference in Proportions: Interpretation</b>
</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [.0236, .2964] contains the true difference in the proportion of residents who favor the law between the two counties.
Since this interval does not contain the value “0” it means that it’s highly likely that there is a true difference in the proportion of residents who support this law in County A compared to county B.
<h2><span class="orange">How to Create a Confidence Interval Using the F Distribution</span></h2>
To determine if the variances of two populations are equal, we can calculate the variance ratio <b>σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub></b>, where σ<sup>2</sup><sub>1</sub> is the variance of population 1 and σ<sup>2</sup><sub>2</sub> is the variance of population 2.
To estimate the true population variance ratio, we typically take a  simple random sample  from each population and calculate the sample variance ratio, <b>s<sub>1</sub><sup>2</sup> / s<sub>2</sub><sup>2</sup></b>, where s<sub>1</sub><sup>2</sup>  and s<sub>2</sub><sup>2</sup> are the sample variances for sample 1 and sample 2, respectively.
This test assumes that both s<sub>1</sub><sup>2</sup>  and s<sub>2</sub><sup>2</sup> are computed from independent samples of size n<sub>1</sub> and n<sub>2</sub>, both drawn from normally distributed populations.
The further this ratio is from one, the stronger the evidence for unequal population variances. 
<b>The (1-α)100% confidence interval</b> for σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub> is defined as:
(s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n<sub>1</sub>-1, n<sub>2</sub>-1, α/2   </sub>≤  σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub>  ≤  (s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n<sub>2</sub>-1, n<sub>1</sub>-1, </sub><sub>α/2</sub>
where F<sub>n<sub>2</sub>-1, n<sub>1</sub>-1, α/2 </sub>and F<sub>n<sub>1</sub>-1, n<sub>2</sub>-1, </sub><sub>α/2</sub><sub> </sub>are the critical values from the F distribution for the chosen significance level α.
The following examples illustrate how to create a confidence interval for σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub> using three different methods:
By hand
Using Microsoft Excel
Using the statistical software <em>R</em>
For each of the following examples, we will use the following information:
<b>α</b> = 0.05
<b>n<sub>1</sub></b> = 16
<b>n<sub>2</sub></b> = 11
<b>s<sub>1</sub><sup>2</sup></b> =28.2
<b>s<sub>2</sub><sup>2</sup> </b>= 19.3
<h2>Creating a Confidence Interval By Hand</h2>
To calculate a confidence interval for <b>σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub></b> by hand, we’ll simply plug in the numbers we have into the confidence interval formula:
(s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n1-1, n2-1,α/2   </sub>≤  σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub>  ≤  (s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n2-1, n1-1, </sub><sub>α/2</sub>
The only numbers we’re missing are the critical values. Luckily, we can locate these critical values in  the F distribution table :
F<sub>n2-1, n1-1, α/2   </sub>= F<sub>10, 15, 0.025   </sub>= <b>3.0602</b>
 F<sub>n1-1, n2-1, </sub><sub>α/2  </sub>=  1/ F<sub>15, 10, 0.025</sub> = 1 / 3.5217 = <b>0.2839</b>
<em>(Click to zoom in on the table)</em>
  
Now we can plug all of the numbers into the confidence interval formula:
(s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n1-1, n2-1,α/2   </sub>≤  σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub>  ≤  (s<sub>1</sub><sup>2</sup>  / s<sub>2</sub><sup>2</sup>) * F<sub>n2-1, n1-1, </sub><sub>α/2</sub>
(28.2 / 19.3) * (0.2839) ≤  σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub>  ≤  (28.2 / 19.3) * (3.0602)
0.4148 ≤  σ<sup>2</sup><sub>1</sub> / σ<sup>2</sup><sub>2</sub>  ≤  4.4714
Thus, the 95% confidence interval for the ratio of the population variances is <b>(0.4148, 4.4714)</b>.
<h2>Creating a Confidence Interval Using Excel</h2>
The following image shows how to calculate a 95% confidence interval for the ratio of population variances in Excel. The lower and upper bounds of the confidence interval are displayed in column E and the formula used to find the lower and upper bounds are displayed in column F:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/fDistExcel2.jpg">
Thus, the 95% confidence interval for the ratio of the population variances is <b>(0.4148, 4.4714)</b>. This matches what we got when we calculated the confidence interval by hand.
<h2>Creating a Confidence Interval Using R</h2>
The following code illustrates how to calculate a 95% confidence interval for the ratio of population variances in R:
<b>#define significance level, sample sizes, and sample variances
alpha &lt;- .05
n1    &lt;- 16
n2    &lt;- 11
var1  &lt;- 28.2
var2  &lt;- 19.3
#define F critical values
upper_crit &lt;- 1/qf(alpha/2, n1-1, n2-1)
lower_crit &lt;- qf(alpha/2, n2-1, n1-1)
#find confidence interval
lower_bound &lt;- (var1/var2) * lower_crit
upper_bound &lt;- (var1/var2) * upper_crit
#output confidence interval
paste0("(", lower_bound, ", ", upper_bound, " )")
#[1] "(0.414899337980266, 4.47137571035219 )"
</b>
Thus, the 95% confidence interval for the ratio of the population variances is <b>(0.4148, 4.4714)</b>. This matches what we got when we calculated the confidence interval by hand.
<h2>Additional Resources</h2>
<b> How to Read the F-Distribution Table 
 How to Find the F Critical Value in Excel </b>
<h2><span class="orange">How to Calculate a Confidence Interval for a Regression Intercept</span></h2>
<b>Simple linear regression</b> is used to quantify the relationship between a predictor variable and a response variable.
This method finds a line that best “fits” a dataset and takes on the following form:
<b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x</b>
where:
<b><U+0177></b>: The estimated response value
<b>b<sub>0</sub></b>: The intercept of the regression line
<b>b<sub>1</sub></b>: The slope of the regression line
<b>x</b>: The value of the predictor variable
Often we’re interested in the value for b<sub>1</sub>, which tells us the average change in the  response variable  associated with a one unit increase in the predictor variable.
However, in rare circumstances we’re also interested in the value for b<sub>0</sub>, which tells us the average value of the response variable when the predictor variable is equal to zero.
We can use the following formula to calculate a confidence interval for the value of β<sub>0</sub>, the true population intercept:
<b>Confidence Interval for β<sub>0</sub>: b<sub>0</sub> ± t<sub>α/2, n-2</sub> * se(b<sub>0</sub>)</b>
The following example shows how to calculate a confidence interval for an intercept in practice.
<h3>Example: Confidence Interval for Regression Intercept</h3>
Suppose we’d like to fit a simple linear regression model using hours studied as a predictor variable and exam score as a response variable for 15 students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/intercept1.png">
The following code shows how to fit this simple linear regression model in R:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14), score=c(64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89))
#fit simple linear regression model
fit &lt;- lm(score ~ hours, data=df)
#view summary of model
summary(fit)
Call:
lm(formula = score ~ hours, data = df)
Residuals:
   Min     1Q Median     3Q    Max 
-5.140 -3.219 -1.193  2.816  5.772 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   65.334      2.106  31.023 1.41e-13 ***
hours          1.982      0.248   7.995 2.25e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.641 on 13 degrees of freedom
Multiple R-squared:  0.831,Adjusted R-squared:  0.818 
F-statistic: 63.91 on 1 and 13 DF,  p-value: 2.253e-06
</b>
Using the coefficient estimates in the output, we can write the fitted simple linear regression model as:
Score = 65.334 + 1.982*(Hours Studied)
The intercept value is 65.334. This tells us that the mean estimated exam score for a student who studies for zero hours is <b>65.334</b>.
We can use the following formula to calculate a 95% confidence interval for the intercept:
95% C.I. for β<sub>0</sub>: b<sub>0</sub> ± t<sub>α/2, n-2</sub> * se(b<sub>0</sub>)
95% C.I. for β<sub>0</sub>: 65.334 ± t<sub>.05/2, 15-2</sub> * 2.106
95% C.I. for β<sub>0</sub>: 65.334 ± 2.1604 * 2.106
95% C.I. for β<sub>0</sub>: [60.78, 69.88]
We interpret this to mean that we’re 95% confident that the true population mean exam score for students who study for zero hours is between 60.78 and 69.88.
<b>Note</b>: We used the  Inverse t Distribution Calculator  to find the t critical value that corresponds to a 95% confidence level with 13 degrees of freedom.
<h3>Cautions on Calculating a Confidence Interval for a Regression Intercept</h3>
We often don’t calculate a confidence interval for a regression intercept in practice because it usually doesn’t make sense to interpret the value of the intercept in a regression model.
For example, suppose we fit a regression model that uses height of a basketball player as a predictor variable and average points per game as a response variable.
It’s not possible for a player to be zero feet tall, so it wouldn’t make sense to interpret the intercept literally in this model.
There are countless scenarios like this where a predictor variable can’t take on a value of zero so it doesn’t make sense to interpret the intercept value of the model or create a confidence interval for the intercept.
For example, consider the following potential predictor variables in a model:
Square footage of a house
Length of a car
Weight of a person
Each of these predictor variables can’t take on a value of zero, so it wouldn’t make sense to calculate a confidence interval for the intercept of a regression model in any of these circumstances.
<h2><span class="orange">How to Find a Confidence Interval for a Median (Step-by-Step)</span></h2>
We can use the following formula to calculate the upper and lower bounds of a  confidence interval  for a population median:
<b>j:</b> nq  –  z√nq(1-q)
<b>k:</b> nq  +  z√nq(1-q)
where:
<b>n:</b> The sample size
<b>q:</b> The quantile of interest. For a median, we will use q = 0.5.
<b>z:</b> The z-critical value
We round j and k up to the next integer. The resulting confidence interval is between the j<sup>th</sup> and k<sup>th</sup> observations in the ordered sample data.
Note that the z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
<b>Source:</b> This formula comes from <em> Practical Nonparametric Statistics, 3rd Edition by W.J. Conover .</em>
The following step-by-step example shows how to calculate a confidence interval for a population median using the following sample data of 15 values:
<b>Sample data: </b>8, 11, 12, 13, 15, 17, 19, 20, 21, 21, 22, 23, 25, 26, 28
<h3>Step 1: Find the Median</h3>
First, we need to find the median of the sample data. This turns out to be the middle value of <b>20</b>:
8, 11, 12, 13, 15, 17, 19, <b>20</b>, 21, 21, 22, 23, 25, 26, 28
<h3>Step 2: Find <em>j</em> and <em>k</em></h3>
Suppose we would like to find a 95% confidence interval for the population median. To do so, we need to first find <em>j</em> and <em>k</em>:
<b>j:</b> nq – z√nq(1-q) = (15)(.5) – 1.96√(15)(.5)(1-.5) = 3.7
<b>k:</b> nq + z√nq(1-q) = (15)(.5) + 1.96√(15)(.5)(1-.5) = 11.3
We will round both <em>j</em> and <em>k</em> up to the nearest integer:
<b>j:</b> 4
<b>k:</b> 12
<h3>Step 3: Find the Confidence Interval</h3>
The 95% confidence interval for the median will be between the j = 4<sup>th</sup> and k = 12<sup>th</sup> observation in the sample dataset.
The 4<sup>th</sup> observation is equal to 13 and the 12<sup>th</sup> observation is equal to 23:
8, 11, 12, <b>13</b>, 15, 17, 19, 20, 21, 21, 22, <b>23</b>, 25, 26, 28
Thus, the 95% confidence interval for the median turns out to be <b>[13, 23]</b>.
<h2><span class="orange">How to Calculate a Confidence Interval for an Odds Ratio</span></h2>
We often calculate an <b>odds ratio </b>when analyzing a 2×2 table, which takes on the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
The <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group to the odds of an event occurring in a control group. It is calculated as:
<b>Odds ratio</b> = (A*D) / (B*C)
We can then use the following formula to calculate a confidence interval for the odds ratio:
<b>Lower 95% CI</b> = e<sup>ln(OR) – 1.96√(1/a + 1/b + 1/c + 1/d)</sup>
<b>Upper 95% CI</b> = e<sup>ln(OR) + 1.96√(1/a + 1/b + 1/c + 1/d)</sup>
The following example shows how to calculate an odds ratio and a corresponding confidence interval in practice.
<h3>Example: Calculating a Confidence Interval for an Odds Ratio</h3>
Suppose a basketball coach uses a new training program to see if it increases the number of players who are able to pass a certain skills test, compared to an old training program.
The coach recruits 50 players to use each program. The following table shows the number of players who passed and failed the skills test, based on the program they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/odds_CI1.png">
We can calculate the odds ratio as (34*11) / (16*39) = <b>0.599</b>
We would interpret this to mean that the odds that a player passes the test by using the new program are just 0.599 times the odds that a player passes the test by using the old program.
In other words, the odds that a player passes the test are actually lowered by 40.1% by using the new program.
We can then use the following formulas to calculate the 95% confidence interval for the odds ratio:
Lower 95% CI = e<sup>ln(.599) – 1.96√(1/34 + 1/16 + 1/39 + 1/11)</sup> = <b>0.245</b>
Upper 95% CI = e<sup>ln(.599) + 1.96√(1/34 + 1/16 + 1/39 + 1/11)</sup> = <b>1.467</b>
Thus, the 95% confidence interval for the odds ratio is <b>[0.245, 1.467]</b>.
We are 95% confident that the true odds ratio between the new and old training program is contained in this interval.
Since this confidence interval contains the value 1, it is not statistically significant.
This should make sense if we consider the following:
An odds ratio greater than 1 would mean that the odds that a player passes the test by using the new program are <em>higher</em> than the odds that a player passes the test by using the old program.
An odds ratio less than 1 would mean that the odds that a player passes the test by using the new program are <em>lower</em> than the odds that a player passes the test by using the old program.
So, since our 95% confidence interval for the odds ratio contains the value 1, it means the odds of a player passing the skills test using the new program may or may not be higher than the odds of the same player passing the test using the old program.
<h2><span class="orange">How to Calculate Confidence Interval for Regression Coefficient in R</span></h2>
In a linear regression model, a regression coefficient tells us the average change in the  response variable  associated with a one unit increase in the predictor variable.
We can use the following formula to calculate a confidence interval for a regression coefficient:
<b>Confidence Interval for β<sub>1</sub>: b<sub>1</sub> ± t<sub>1-α/2, n-2</sub> * se(b<sub>1</sub>)</b>
where:
<b> b<sub>1</sub></b> = Regression coefficient shown in the regression table
<b>t<sub>1-∝/2, n-2</sub></b> = The t critical value for confidence level 1-∝ with n-2 degrees of freedom where <em>n </em>is the total number of observations in our dataset
<b>se(b<sub>1</sub>)</b> = The standard error of b<sub>1</sub> shown in the regression table
The following example shows how to calculate a confidence interval for a regression slope in practice.
<h2>Example: Confidence Interval for Regression Coefficient in R</h2>
Suppose we’d like to fit a simple linear regression model using <b>hours studied</b> as a predictor variable and <b>exam score</b> as a response variable for 15 students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/intercept1.png">
We can use the  lm()  function to fit this simple linear regression model in R:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14), score=c(64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89))
#fit linear regression model
fit &lt;- lm(score ~ hours, data=df)
#view model summary
summary(fit)
Call:
lm(formula = score ~ hours, data = df)
Residuals:
   Min     1Q Median     3Q    Max 
-5.140 -3.219 -1.193  2.816  5.772 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   65.334      2.106  31.023 1.41e-13 ***
hours          1.982      0.248   7.995 2.25e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.641 on 13 degrees of freedom
Multiple R-squared:  0.831,Adjusted R-squared:  0.818 
F-statistic: 63.91 on 1 and 13 DF,  p-value: 2.253e-06</b>
Using the coefficient estimates in the output, we can write the fitted simple linear regression model as:
Score = 65.334 + 1.982*(Hours Studied)
Notice that the regression coefficient for hours is <b>1.982</b>.
This tells us that each additional one hour increase in studying is associated with an average increase of <b>1.982</b> in exam score.
We can use the <b>confint()</b> function to calculate a 95% confidence interval for the regression coefficient:
<b>#calculate confidence interval for regression coefficient for 'hours'
confint(fit, 'hours', level=0.95)
         2.5 %   97.5 %
hours 1.446682 2.518068
</b>
The 95% confidence interval for the regression coefficient is <b>[1.446, 2.518]</b>.
Since this confidence interval doesn’t contain the value 0, we can conclude that there is a statistically significant association between hours studied and exam score.
We can also confirm this is correct by calculating the 95% confidence interval for the regression coefficient by hand:
95% C.I. for β<sub>1</sub>: b<sub>1</sub> ± t<sub>1-α/2, n-2</sub> * se(b<sub>1</sub>)
95% C.I. for β<sub>1</sub>: 1.982 ± t<sub>.975, 15-2</sub> * .248
95% C.I. for β<sub>1</sub>: 1.982 ± 2.1604 * .248
95% C.I. for β<sub>1</sub>: [1.446, 2.518]
The 95% confidence interval for the regression coefficient is <b>[1.446, 2.518]</b>.
<b>Note #1</b>: We used the  Inverse t Distribution Calculator  to find the t critical value that corresponds to a 95% confidence level with 13 degrees of freedom.
<b>Note #2</b>: To calculate a confidence interval with a different confidence level, simply change the value for the <b>level</b> argument in the <b>confint()</b> function.
<h2>Additional Resources</h2>
The following tutorials provide additional information about linear regression in R:
 How to Interpret Regression Output in R 
 How to Perform Simple Linear Regression in R 
 How to Perform Multiple Linear Regression in R 
 How to Perform Logistic Regression in R 
<h2><span class="orange">How to Calculate Confidence Interval for Regression Slope</span></h2>
<b>Simple linear regression</b> is used to quantify the relationship between a predictor variable and a response variable.
This method finds a line that best “fits” a dataset and takes on the following form:
<b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x</b>
where:
<b><U+0177></b>: The estimated response value
<b>b<sub>0</sub></b>: The intercept of the regression line
<b>b<sub>1</sub></b>: The slope of the regression line
<b>x</b>: The value of the predictor variable
Often we’re interested in the value for b<sub>1</sub>, which tells us the average change in the  response variable  associated with a one unit increase in the predictor variable.
We can use the following formula to calculate a confidence interval for the value of β<sub>1</sub>, the value of the slope for the overall population:
<b>Confidence Interval for β<sub>1</sub>: b<sub>1</sub> ± t<sub>1-α/2, n-2</sub> * se(b<sub>1</sub>)</b>
where:
<b> b<sub>1</sub></b> = Slope coefficient shown in the regression table
<b>t<sub>1-∝/2, n-2</sub></b> = The t critical value for confidence level 1-∝ with n-2 degrees of freedom where <em>n </em>is the total number of observations in our dataset
<b>se(b<sub>1</sub>)</b> = The standard error of b<sub>1</sub> shown in the regression table
The following example shows how to calculate a confidence interval for a regression slope in practice.
<h2>Example: Confidence Interval for Regression Slope</h2>
Suppose we’d like to fit a simple linear regression model using hours studied as a predictor variable and exam score as a response variable for 15 students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/intercept1.png">
We can perform  simple linear regression in Excel  and receive the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/confslope1.jpg"603">
Using the coefficient estimates in the output, we can write the fitted simple linear regression model as:
Score = 65.334 + 1.982*(Hours Studied)
The value for the regression slope is <b>1.982</b>.
This tells us that each additional one hour increase in studying is associated with an average increase of <b>1.982</b> in exam score.
We can use the following formula to calculate a 95% confidence interval for the slope:
95% C.I. for β<sub>1</sub>: b<sub>1</sub> ± t<sub>1-α/2, n-2</sub> * se(b<sub>1</sub>)
95% C.I. for β<sub>1</sub>: 1.982 ± t<sub>.975, 15-2</sub> * .248
95% C.I. for β<sub>1</sub>: 1.982 ± 2.1604 * .248
95% C.I. for β<sub>1</sub>: [1.446, 2.518]
The 95% confidence interval for the regression slope is <b>[1.446, 2.518]</b>.
Since this confidence interval doesn’t contain the value 0, we can conclude that there is a statistically significant association between hours studied and exam score.
<b>Note</b>: We used the  Inverse t Distribution Calculator  to find the t critical value that corresponds to a 95% confidence level with 13 degrees of freedom.
<h2>Additional Resources</h2>
The following tutorials provide additional information about linear regression:
 Introduction to Simple Linear Regression 
 Introduction to Multiple Linear Regression 
 How to Read and Interpret a Regression Table 
 How to Report Regression Results 
<h2><span class="orange">How to Calculate Confidence Intervals in Google Sheets</span></h2>
A <b>confidence interval for a mean </b>is a range of values that is likely to contain a population mean with a certain level of confidence.
It is calculated as:
<b>Confidence Interval = x +/- t*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t: </b>t-value that corresponds to the confidence level
<b>s: </b>sample standard deviation
<b>n: </b>sample size
This tutorial explains how to calculate confidence intervals in Google Sheets.
<h3>Confidence Intervals Using the t Distribution</h3>
If we’re working with a small sample (n &lt; 30), we can use the t-Distribution to calculate a confidence interval for a population mean.
For example, suppose we want to calculate a confidence interval for the true population mean height (in inches) of a certain species of plant, using a sample of 15 plants:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/confGoogle1.png">
First, we can calculate the sample mean, sample standard deviation, and sample size:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/confGoogle2.png">
Next, we can use the following formulas to calculate the lower and upper bound for the 95% confidence interval:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/confGoogle3.png">
The 95% confidence interval for the true population mean height is <b>(13.877, 19.457)</b>.
<h3>Confidence Intervals Using the Normal Distribution</h3>
If we’re working with larger samples ( n≥ 30), we can assume that the sampling distribution of the sample mean is normally distributed thanks to the  Central Limit Theorem .
This means we can instead use the <b>NORM.S.INV()</b> function to calculate the critical value to use for the confidence interval.
The following example shows how to calculate a confidence interval for the true population mean height (in inches) of a certain species of plant, using a sample of 30 plants:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/confGoogle4.png">
The 95% confidence interval for the true population mean height is <b>(20.571, 26.429)</b>.
Note that larger confidence levels lead to wider confidence intervals. For example, here’s how to calculate a 99% C.I. for the exact same data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/confGoogle5.png">
The 99% confidence interval for the true population mean height is <b>(19.650, 27.350)</b>.
Notice that this 99% confidence interval is wider than the 95% confidence interval we calculated earlier.
<b>Related:</b>  Confidence Level vs. Confidence Interval: What’s the Difference? 
<h2><span class="orange">How to Find Confidence Intervals in R (With Examples)</span></h2>
A  confidence interval  is a range of values that is likely to contain a  population parameter  with a certain level of confidence.
It is calculated using the following general formula:
<b>Confidence Interval</b> = (point estimate)  +/-  (critical value)*(standard error)
This formula creates an interval with a lower bound and an upper bound, which likely contains a population parameter with a certain level of confidence:
<b>Confidence Interval </b> = [lower bound, upper bound]
This tutorial explains how to calculate the following confidence intervals in R:
<b>1.</b> Confidence Interval for a Mean
<b>2.</b> Confidence Interval for a Difference in Means
<b>3.</b> Confidence Interval for a Proportion
<b>4.</b> Confidence Interval for a Difference in Proportions
Let’s jump in!
<h3>Example 1: Confidence Interval for a Mean</h3>
We use the following formula to calculate a  confidence interval for a mean :
<b>Confidence Interval = x  +/-  t<sub>n-1, 1-α/2</sub>*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t: </b>the t-critical value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
<b>Example: </b>Suppose we collect a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
The following code shows how to calculate a 95% confidence interval for the true population mean weight of turtles:
<b>#input sample size, sample mean, and sample standard deviation
n &lt;- 25
xbar &lt;- 300 
s &lt;- 18.5
#calculate margin of error
margin &lt;- qt(0.975,df=n-1)*s/sqrt(n)
#calculate lower and upper bounds of confidence interval
low &lt;- xbar - margin
low
[1] 292.3636
high &lt;- xbar + margin
high
[1] 307.6364
</b>
The 95% confidence interval for the true population mean weight of turtles is <b>[292.36, 307.64]</b>.
<h3>Example 2: Confidence Interval for a Difference in Means</h3>
We use the following formula to calculate a confidence interval for a  difference in population means :
<b>Confidence interval</b> = (x<sub>1</sub>–x<sub>2</sub>) +/- t*√((s<sub>p</sub><sup>2</sup>/n<sub>1</sub>) + (s<sub>p</sub><sup>2</sup>/n<sub>2</sub>))
where:
x<sub>1</sub>, x<sub>2</sub>: sample 1 mean, sample 2 mean
t: the t-critical value based on the confidence level and (n<sub>1</sub>+n<sub>2</sub>-2) degrees of freedom
s<sub>p</sub><sup>2</sup>: pooled variance, calculated as ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup>) / (n<sub>1</sub>+n<sub>2</sub>-2)
t: the t-critical value
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
<b>Example: </b>Suppose we want to estimate the difference in mean weight between two different species of turtles, so we go out and gather a random sample of 15 turtles from each population. Here is the summary data for each sample:
<b>Sample 1:</b>
x<sub>1</sub> = 310
s<sub>1</sub> = 18.5
n<sub>1</sub> = 15
<b>Sample 2:</b>
x<sub>2</sub> = 300
s<sub>2</sub> = 16.4
n<sub>2</sub> = 15
The following code shows how to calculate a 95% confidence interval for the true difference in population means:
<b>#input sample size, sample mean, and sample standard deviation
n1 &lt;- 15
xbar1 &lt;- 310 
s1 &lt;- 18.5
n2 &lt;- 15
xbar2 &lt;- 300
s2 &lt;- 16.4
#calculate pooled variance
sp = ((n1-1)*s1^2 + (n2-1)*s2^2) / (n1+n2-2)
#calculate margin of error
margin &lt;- qt(0.975,df=n1+n2-1)*sqrt(sp/n1 + sp/n2)
#calculate lower and upper bounds of confidence interval
low &lt;- (xbar1-xbar2) - margin
low
[1] -3.055445
high &lt;- (xbar1-xbar2) + margin
high
[1] 23.05544
</b>
The 95% confidence interval for the true difference in population means is <b>[-3.06, 23.06]</b>.
<h3>Example 3: Confidence Interval for a Proportion</h3>
We use the following formula to calculate a  confidence interval for a proportion :
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
<b>Example: </b>Suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p = 0.56</b>
The following code shows how to calculate a 95% confidence interval for the true proportion of residents in the entire county who are in favor of the law:
<b>#input sample size and sample proportion
n &lt;- 100
p &lt;- .56
#calculate margin of error
margin &lt;- qnorm(0.975)*sqrt(p*(1-p)/n)
#calculate lower and upper bounds of confidence interval
low &lt;- p - margin
low
[1] 0.4627099
high &lt;- p + margin
high
[1] 0.6572901
</b>
The 95% confidence interval for the true proportion of residents in the entire county who are in favor of the law is <b>[.463, .657]</b>.
<h3>Example 4: Confidence Interval for a Difference in Proportions</h3>
We use the following formula to calculate a  confidence interval for a difference in proportions :
<b>Confidence interval = (p<sub>1</sub>–p<sub>2</sub>)  +/-  z*√(p<sub>1</sub>(1-p<sub>1</sub>)/n<sub>1 </sub>+ p<sub>2</sub>(1-p<sub>2</sub>)/n<sub>2</sub>)</b>
where:
p<sub>1</sub>, p<sub>2</sub>: sample 1 proportion, sample 2 proportion
z: the z-critical value based on the confidence level
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
<b>Example: </b>Suppose we want to estimate the difference in the proportion of residents who support a certain law in county A compared to the proportion who support the law in county B. Here is the summary data for each sample:
<b>Sample 1:</b>
n<sub>1</sub> = 100
p<sub>1</sub> = 0.62 (i.e. 62 out of 100 residents support the law)
<b>Sample 2:</b>
n<sub>2</sub> = 100
p<sub>2</sub> = 0.46 (i.e. 46 our of 100 residents support the law)
The following code shows how to calculate a 95% confidence interval for the true difference in proportion of residents who support the law between the counties:
<b>#input sample sizes and sample proportions
n1 &lt;- 100
p1 &lt;- .62
n2 &lt;- 100
p2 &lt;- .46
#calculate margin of error
margin &lt;- qnorm(0.975)*sqrt(p1*(1-p1)/n1 + p2*(1-p2)/n2)
#calculate lower and upper bounds of confidence interval
low &lt;- (p1-p2) - margin
low
[1] 0.02364509
high &lt;- (p1-p2) + margin
high
[1] 0.2963549
</b>
The 95% confidence interval for the true difference in proportion of residents who support the law between the counties is <b>[.024, .296]</b>.
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">Confidence Interval Calculator</span></h2>
This calculator creates a  confidence interval for a population mean  using the following formula:
Confidence Interval = x  +/-  z*(s/√n)
where:
<b>x:</b> sample mean
<b>z:</b> z-value that corresponds to confidence level
<b>s:</b> sample standard deviation
<b>n:</b> sample size
To create a confidence interval for a population mean, simply fill in the values below and then click the “Calculate” button:
<label for="mean"><b>Sample mean</b></label>
<input type="number" id="mean" min="0" value="17">
<label for="sd"><b>Sample standard deviation</b></label>
<input type="number" id="sd" min="0" value="13.5">
<label for="n"><b>Sample size (n)</b></label>
<input type="number" id="n" min="0" value="4">
<input type="button" id="button" onclick="calc()" value="Calculate">
90% Confidence Interval: <b>(5.896, 28.104)</b>
95% Confidence Interval: <b>(3.770, 30.230)</b>
99% Confidence Interval: <b>(-0.388, 34.388)</b>
<script>
function calc() {
    
//get input degrees of freedom, t-value
var mean = document.getElementById('mean').value*1;
var sd = document.getElementById('sd').value*1;
var n = document.getElementById('n').value*1;
//define z-scores to use
var z_90 = 1.645;
var z_95 = 1.96;
var z_99 = 2.576;
//calculate intervals
var low_90 = mean - (z_90 * (sd/(Math.sqrt(n))));
var high_90 = mean - (-1*(z_90 * (sd/(Math.sqrt(n)))));
var low_95 = mean - (z_95 * (sd/(Math.sqrt(n))));
var high_95 = mean - (-1*(z_95 * (sd/(Math.sqrt(n)))));
var low_99 = mean - (z_99 * (sd/(Math.sqrt(n))));
var high_99 = mean - (-1*(z_99 * (sd/(Math.sqrt(n)))));
//output values
    document.getElementById('low90').innerHTML = low_90.toFixed(3);
document.getElementById('high90').innerHTML = high_90.toFixed(3);
document.getElementById('low95').innerHTML = low_95.toFixed(3);
document.getElementById('high95').innerHTML = high_95.toFixed(3);
document.getElementById('low99').innerHTML = low_99.toFixed(3);
document.getElementById('high99').innerHTML = high_99.toFixed(3);
  }
</script>
<h2><span class="orange">Confidence Interval for a Mean</span></h2>
A <b>confidence interval for a mean </b>is a range of values that is likely to contain a population mean with a certain level of confidence.
This tutorial explains the following:
The motivation for creating a confidence interval for a mean.
The formula to create a confidence interval for a mean.
An example of how to calculate a confidence interval for a mean.
How to interpret a confidence interval for a mean.
<h3>Confidence Interval for a Mean: Motivation</h3>
The reason that we would even want to create  a confidence interval  for a mean is because we want to capture our uncertainty when estimating a population mean.
For example, suppose we want to estimate the mean weight of a certain species of turtle in Florida. Since there are thousands of turtles in Florida, it would be extremely time-consuming and costly to go around and weigh each individual turtle.
Instead, we might take a  simple random sample  of 50 turtles and use the mean weight of the turtles in this sample to estimate the true population mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CImean1.png">
The problem is that the mean weight in the sample is not guaranteed to exactly match the mean weight of the whole population. So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true mean weight of the turtles in the population.
<h3>Confidence Interval for a Mean: Formula</h3>
We use the following formula to calculate a confidence interval for a mean:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the chosen z-value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
Notice that higher confidence levels correspond to larger z-values, which leads to wider confidence intervals. This means that, for example, a 99% confidence interval will be wider than a 95% confidence interval for the same set of data.
<h3>Confidence Interval for a Mean: Example</h3>
Suppose we collect a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
Here is how to find various confidence intervals for the true population mean weight:
<b>90% Confidence Interval: </b>300 +/-  1.645*(18.5/√25) = <b>[293.91, 306.09]</b>
<b>95% Confidence Interval: </b>300 +/-  1.96*(18.5/√25) = <b>[292.75, 307.25]</b>
<b>99% Confidence Interval: </b>300 +/-  2.58*(18.5/√25) = [<b>290.47, 309.53]</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Statology Confidence Interval Calculator .</em>
<h3>Confidence Interval for a Mean: Interpretation</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [292.75, 307.25] contains the true population mean weight of turtles.
Another way of saying the same thing is that there is only a 5% chance that the true population mean lies outside of the 95% confidence interval. That is, there’s only a 5% chance that the true population mean weight of turtles is greater than 307.25 pounds or less than 292.75 pounds.
<h2><span class="orange">Confidence Interval for Proportion Calculator</span></h2>
A  confidence interval for a population proportion  is a range of values that is likely to contain a population proportion with a certain level of confidence.
The formula to calculate this confidence interval is:
Confidence interval = p  +/-  z*(√p(1-p)/n)
where:
p: sample proportion
z: the z-critical value based on the confidence level
n: sample proportion
To find a confidence interval for a population proportion, simply fill in the boxes below and then click the “Calculate” button.
<label><b>p</b> (sample proportion)</label>
<input type="number" id="p" value="0.56" min="0" max="1">
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="100">
<label>Confidence level</label>
<input type="number" id="conf" value=".95" min="0" max="1" step=".01">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
95% C.I. = [0.4627, 0.6573]
You can be 95% confident that the interval [0.4627, 0.6573] contains the true population proportion.
<script>
function calc() {
//get input values
var p = +document.getElementById('p').value;
var n = +document.getElementById('n').value;
var conf = +document.getElementById('conf').value;
var confUse = conf - (-(1-conf)/2);
var confOut = conf*100;
//calculate stuff
var z = Math.abs(jStat.normal.inv(confUse, 0, 1));
var se = Math.sqrt(p*(1-p)/n);
var low = p - (z*se);
var high = p - (-1*z*se);
//output results
document.getElementById('confOut').innerHTML = confOut;
document.getElementById('confOut2').innerHTML = confOut;
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('low2').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
document.getElementById('high2').innerHTML = high.toFixed(4);
}
</script>
<h2><span class="orange">Confidence Interval for a Proportion</span></h2>
A <b>confidence interval for a proportion </b>is a range of values that is likely to contain a population proportion with a certain level of confidence.
This tutorial explains the following:
The motivation for creating a confidence interval for a proportion.
The formula to create a confidence interval for a proportion.
An example of how to calculate a confidence interval for a proportion.
How to interpret a confidence interval for a proportion.
<h3>Confidence Interval for a Proportion: Motivation</h3>
The reason to create a  confidence interval  for a proportion is to capture our uncertainty when estimating a population proportion.
For example, suppose we want to estimate the proportion of people in a certain county that are in favor of a certain law. Since there are thousands of residents in the county, it would be too costly and time-consuming to go around and ask each resident about their stance on the law.
Instead, we might select a  simple random sample  of residents and ask each one whether or not they support the law:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CIprop1.png">
Since we select a random sample of residents, there is no guarantee that the proportion of residents in the sample who are in favor of the law will exactly match the proportion of residents in the entire county who are in favor of the law.
So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true proportion of residents who are in favor of the law in the entire county.
<h3>Confidence Interval for a Proportion: Formula</h3>
We use the following formula to calculate a confidence interval for a population proportion:
<b>Confidence Interval = p  +/-  z*√p(1-p) / n</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
Notice that higher confidence levels correspond to larger z-values, which leads to wider confidence intervals.
This means that, for example, a 95% confidence interval will be wider than a 90% confidence interval for the same set of data.
<b>Related:</b>  What is Considered a Good Confidence Interval? 
<h3>Confidence Interval for a Proportion: Example</h3>
Suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p = 0.56</b>
Here is how to find various confidence intervals for the population proportion:
<b>90% Confidence Interval: </b>0.56  +/-  1.645*(√.56(1-.56) / 100) = <b>[0.478, 0.642]</b>
<b>95% Confidence Interval: </b>0.56  +/-  1.96*(√.56(1-.56) / 100) = <b>[0.463, 0.657]</b>
<b>99% Confidence Interval: </b>0.56  +/-  2.58*(√.56(1-.56) / 100) = <b>[0.432, 0.688]</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Confidence Interval for Proportion Calculator .</em>
<h3>Confidence Interval for a Proportion: Interpretation</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [0.463, 0.657] contains the true population proportion of residents who are in favor of this certain law.
Another way of saying the same thing is that there is only a 5% chance that the true population proportion lies outside of the 95% confidence interval.
That is, there’s only a 5% chance that the true proportion of residents in the county that support the law is less than 46.3% or greater than 65.7%.
<h2><span class="orange">4 Examples of Confidence Intervals in Real Life</span></h2>
In statistics,  confidence intervals  are used to represent a range of values that is likely to contain a  population parameter  with a certain level of confidence.
The following general formula is used to calculate confidence intervals:
<b>Confidence Interval</b> = (point estimate)  +/-  (critical value)*(standard error)
This formula creates an interval with a lower bound and an upper bound, which likely contains a population parameter with a certain level of confidence.
<b>Confidence Interval </b> = [lower bound, upper bound]
The following examples provide several situations where confidence intervals are used in the real world.
<h3>Example 1: Biology</h3>
Confidence intervals are often used in biology to estimate the mean height, weight, width, diameter, etc. of different plant and animal species.
For example, a biologist may be interested in measuring the mean weight of a certain species of frog in Australia. Since it would take too long to go around and weigh thousands of individual frogs, the biologist may instead collect a  simple random sample  of 50 frogs and measure the mean and standard deviation of the frogs in the sample.
She could then use the sample mean and sample standard deviation to construct an interval for the true mean of the frogs in the entire population.
<h3>Example 2: Clinical Trials</h3>
Confidence intervals are often used in clinical trials to determine the mean change in blood pressure, heart rate, cholesterol, etc. produced by some new drug or treatment.
For example, a doctor may believe that a new drug is able to reduce blood pressure in patients. To test this, he may recruit 20 patients to participate in a trial in which they used the new drug for one month. At the end of the month, the doctor may record the mean decrease in blood pressure and the standard deviation of the decrease in each patient in the sample.
He could then use the sample mean and sample standard deviation to construct an interval for the true mean change in blood pressure that patients are likely to experience in the population.
<h3>Example 3: Advertising</h3>
Confidence intervals are often used by marketing departments within companies to determine if some new advertising technique, method, tactic, etc. produces significantly higher revenue.
For example, a marketing team at a grocery retailer may run two different advertising campaigns at 20 different stores each during one quarter and measure the average sales produced by each campaign at each store at the end of the quarter.
They could then use the sample mean and sample standard deviation of sales from each campaign to construct a confidence interval for the difference between mean sales. This will tell the marketing team if there is any meaningful difference in sales that occurs as a result of the two campaigns.
<h3>Example 4: Manufacturing</h3>
Confidence intervals are often used by engineers in manufacturing plants to determine if some new process, technique, method, etc. causes a meaningful change in the number of defective products produced by the plant.
For example, an engineer may believe that a new process will change the number of defective widgets produced per day, which is currently 50. To test this, he may implement the new process and record the number of defective products produced each day for one month at the plant.
He could then use the sample mean and sample standard deviation of the number of daily defects to construct a confidence interval for the true mean number of defective products produced by the new process.
If the confidence interval does not contain the value “50” then the engineer can be confident that the new process produces a different number of daily defective products compared to the current process.
<h2><span class="orange">How to Calculate Confidence Intervals in SAS</span></h2>
A  confidence interval  is a range of values that is likely to contain a  population parameter  with a certain level of confidence.
This tutorial explains how to calculate the following confidence intervals in R:
<b>1.</b> Confidence Interval for a Population Mean
<b>2.</b> Confidence Interval for a Difference in Population Means
Let’s jump in!
<h2>Example 1: Confidence Interval for Population Mean in SAS</h2>
Suppose we have the following dataset that contains the height (in inches) of a random sample of 12 plants that all belong to the same species:
<b>/*create dataset*/
data my_data;
    input Height;
    datalines;
14
14
16
13
12
17
15
14
15
13
15
14
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/confsas1.jpg"122">
Suppose we would like to calculate a 95% confidence for the true population mean height of this species.
We can use the following code in SAS to do so:
<b>/*generate 95% confidence interval for population mean*/
proc ttest data=my_data alpha=0.05;
    var Height;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/confsas2.jpg"388">
The value for <b>Mean</b> shows the sample mean and the values under<b> 95% CL Mean</b> show the 95% confidence interval for the population mean.
From the output we can see that the 95% confidence interval for the mean weight of plants in this population is <b>[13.4624 inches, 15.2042 inches]</b>.
<h2>Example 2: Confidence Interval for Difference in Population Means in SAS</h2>
Suppose we have the following dataset that contains the height (in inches) of a random sample of plants that belong to two different species:
<b>/*create dataset*/
data my_data2;
    input Species $ Height;
    datalines;
A 14
A 14
A 16
A 13
A 12
A 17
A 15
A 14
A 15
A 13
B 15
B 14
B 19
B 19
B 17
B 18
B 20
B 19
B 17
B 15
;
run;
/*view dataset*/
proc print data=my_data2;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/confsas3.jpg"174">
Suppose we would like to calculate a 95% confidence for difference in population mean height between species A and species B.
We can use the following code in SAS to do so:
<b>/*sort data by Species to ensure confidence interval is calculated correctly*/
proc sort data=my_data2;
    by Species;
run;
/*generate 95% confidence interval for difference in population means*/
proc ttest data=my_data2 alpha=0.05;
    class Species;
    var Height;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/confsas4.jpg"533">
The first table we need to look at in the output is <b>Equality of Variances</b>, which tests whether or not the variance between each sample is equal.
Since the p-value is not less than .05 in this table, we can assume that the variances between the two groups is equal.
Thus, we can look at the row that uses <b>Pooled</b> variance to find the 95% confidence interval for difference in population means.
From the output we can see that the 95% confidence interval for the difference in population means is <b>[-4.6895 inches, -1.1305 inches]</b>.
This tells us we can be 95% confident that the true difference between the mean height of plants in species A compared to species B is between -4.6895 inches and -1.1305 inches.
Since  0 is not in this confidence interval , this indicates that there is a statistically significant difference between the two population means.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Perform a One Sample t-Test in SAS 
 How to Perform a Two Sample t-Test in SAS 
 How to Perform a Paired Samples t-Test in SAS 
<h2><span class="orange">Confidence Interval for a Standard Deviation Calculator</span></h2>
A <b>confidence interval for a population standard deviation</b> is a range of values that is likely to contain a population standard deviation with a certain level of confidence.
The formula to calculate this confidence interval is:
Confidence interval = [√(n-1)s<sup>2</sup>/X<sup>2</sup><sub>α/2</sub>, √(n-1)s<sup>2</sup>/X<sup>2</sup><sub>1-α/2</sub>]
where:
n: sample size
s<sup>2</sup>: sample variance
X<sup>2</sup>: Chi-Square critical value with n-1 degrees of freedom
To find a confidence interval for a population standard deviation, simply fill in the boxes below and then click the “Calculate” button.
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="27">
<label><b>s</b> (sample standard deviation)</label>
<input type="number" id="s" value="6.43">
<label>Confidence level</label>
<input type="number" id="conf" value=".95" min="0" max="1" step=".01">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
95% C.I. = [5.0637, 8.8119]
You can be 95% confident that the interval [5.0637, 8.8119] contains the true population standard deviation.
<script>
function calc() {
//get input values
var n = +document.getElementById('n').value;
var s = +document.getElementById('s').value;
var conf = +document.getElementById('conf').value;
var s2 = Math.pow(s,2);
var df = n-1;
var confUse = 1-conf;
var confOut = conf*100;
var x2high = jStat.chisquare.inv(confUse/2, df);
var x2low = jStat.chisquare.inv(1-(confUse/2), df);
//calculate stuff
var low = Math.sqrt(df*s2/x2low);
var high = Math.sqrt(df*s2/x2high);
//output results
document.getElementById('confOut').innerHTML = confOut;
document.getElementById('confOut2').innerHTML = confOut;
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('low2').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
document.getElementById('high2').innerHTML = high.toFixed(4);
}
</script>

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
