<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #001010; font-size: 18px;}
pre { color: gray; background-color: #001010; font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
img{
	margin-top:1%;
	margin-bottom:2%;
}
.topic{
    color: lime;
}
.goldsha {
    color: white;
    border: 1px solid gold;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px gold inset;
}
.redsha {
    color: gold;
    border: 1px solid red;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px red inset;
}
.whitesha {
    color: red;
    border: 1px solid white;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -3px -2px 3px white inset;
}
.orangesha {
    color: yellow;
    border: 1px solid orange;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px orange inset;
}
.yellowsha {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
	display: inline-block;
}
.greensha {
    color: lightblue;
    border: 1px solid green;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px green inset;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.yellowbord {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
}
.bluebord {
    color: white;
    border: 1px solid lightblue;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px silver inset;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 15%;
	margin-right: 15%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head>
<body>

<center><h1>Text Classification using Vowpal Wabbit</h1></center>
<div id="toc"><ul></ul></div>
<br>
<br>
<br>

<h2>Introduction</h2>
<p>A large number of E-Commerce and tech companies rely on real time training and predictions for their products. Google predicts real time click-through rates for their ads. This is used as an input to their auction mechanism, apart from a bid from the advertiser to decide which ads to show to the user.</p>
<p>Stackoverflow uses real time predictions to automatically tag a question with the correct programming language so that they reach the right asker. An election management team might want to predict real time sentiment using Twitter to assess the impact of their campaign.</p>
<p>Such datasets are also characterized by their large size. For training the model, we can always take a sub-sample of the data, but its a rather unreliable method. There&#8217;s a good chance we might miss out on a significant amount of information.</p>
<p>Is there a solution that tackles both these problems? As it turns out, there is. In this article, we will discuss a comparison of batch learning and online learning. In the second section, we&#8217;ll look at an example of text classification using an online learning framework called Vowpal Wabbit (VW).</p>
<p>&nbsp;</p>
<h2>Table of Contents</h2>
<ol>
<li>Online Learning Vs Batch Learning</li>
<li>Vowpal Wabbit (VW) &#8211; A framework for online learning
<ul>
<li>Input Format</li>
<li>VW for text classification
<ul>
<li>Training</li>
<li>Testing</li>
<li>n-grams</li>
<li>Model Interpretability</li>
<li>Regularization</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>&nbsp;</p>
<h2>Online Learning Vs Batch Learning</h2>
<ul>
<li>Online learning is a method of machine learning that <strong>stores and processes only one training example</strong> at a time sequentially. It assumes an initial predictive model and updates its parameters for future predictions at each step. Other algorithms based on batch learning perform <strong>training on the whole batch.</strong></li>
<li>Batch learning requires the <strong>whole batch to be stored in the memory</strong> and once we have some new training data, the model has to retrain on the entire set again.  Since online learning processes only one training example at a time, its storage requirements are met even by an average system that is able to <strong>fit at least one training example</strong> in memory.</li>
<li>In online learning, we update our parameters by adding (minus sign) the gradient computed on a single instance of the dataset. Since it&#8217;s based on one random data point, it&#8217;s <strong>very noisy</strong> and may go off in a direction far from the batch gradient. Batch learning updates are based on a batch of training examples and hence intermediate <strong>updates are much more stable</strong>.</li>
</ul>
<p>Letâ€™s say we want to build a model that identifies spam emails. One way is to download a large corpus of emails, train a model on it and subsequently test it on unseen examples. This is called offline learning. Another way is to take in each email sequentially and continuously update the parameters in a classifier. You guessed it &#8211; this is online learning.</p>
<p><img class="aligncenter wp-image-12517 " src="https://www.analyticsvidhya.com/wp-content/uploads/2015/01/schematics.png" alt="schematics" width="494" height="371" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/01/schematics.png 1009w, https://www.analyticsvidhya.com/wp-content/uploads/2015/01/schematics-300x225.png 300w" sizes="(max-width: 494px) 100vw, 494px" /></p>
<p>Going back to our example of spam classification, imagine a situation where the spammers have found a work-around and started bypassing the existing spam classifier. Two teams have been tasked to solve the problem &#8211; The Batch Learning Team and the Online Learning team.</p>
<ul>
<li>This scenario will be a nightmare for the Batch Learning Team. They will first have to label the new spam emails as spam. Following that, they will retrain the model on the entire dataset to get the new model and then deploy it in production.</li>
<li>The Online Learning Team will still have to label the new spam, but they can do away with retraining the model on the whole dataset. Instead, they can feed the stream of freshly labelled data to an already existing online learning model. They can sit back as it updates the existing model and learns new parameters from each new sample sequentially.</li>
</ul>
<p>&nbsp;</p>
<h2>Vowpal Wabbit &#8211; A Framework for Online Learning</h2>
<p>SkLearn has SGDClassifier and SGDRegressor from sklearn.linear_model. These are nice implementations of SGD/online learning but we&#8217;ll focus on Vowpal Wabbit as it&#8217;s superior to sklearn&#8217;s SGD models in many aspects, including computational performance. The Vowpal Wabbit (VW) project is a fast online learning framework sponsored by Microsoft Research and (previously) Yahoo! Research.</p>
<p>When it comes to categorical variables, label encoding or one hot encoding are popular methods of dealing with it. These are good, if we are able to store and process the whole dataset. But online learning does not have the liberty of looking at the complete dataset. Also, real data can be volatile and we cannot guarantee that new values of categorical features will not be added at some point. This issue hampers the use of the trained model when some new data is introduced,</p>
<p>This is where Vowpal Wabbit comes into the picture.</p>
<p>The idea is very simple: convert data into a vector of features. When this is done using hashing, we call the method &#8220;feature hashing&#8221; or &#8220;the hashing trick&#8221;.</p>
<p>I&#8217;ll explain how it works with a simple example using text as data.</p>
<p>Let&#8217;s say our text is:</p>
<p><em>&#8220;the great blue whale&#8221;</em></p>
<p>We would like to represent this as a vector. The first thing we need to do is fix the length of the vector, i.e., the number of dimensions we are going to use. For the purpose of this example, we will take 5 dimensions.</p>
<p>Once we fix the number of dimensions, we will need a hash function that will take a string and return a number between 0 and n-1 (in our case between 0 and 4). Any good hash function can be used. We will use <em>h(string) mod n</em> to make it return a number between 0 and n-1. <em>h(string)</em> generates a large number based on the number of bits allotted.</p>
<p>I&#8217;ll compute the results for each word in our text:</p>
<p style="text-align: center;">h(the) mod 5 = 0</p>
<p style="text-align: center;">h(great) mod 5 = 1</p>
<p style="text-align: center;">h(blue) mod 5 = 1</p>
<p style="text-align: center;">h(whale) mod 5 = 3</p>
<p>Once we have this, we can simply construct our vector as:</p>
<p style="text-align: center;">(1,2,0,1,0)</p>
<p>Notice that we just add 1 to the nth dimension of the vector each time our hash function returns that dimension for a word in the text.</p>
<p>Vowpal Wabbit is so incredibly fast in part due to the hashing trick. With many features and a small-sized hash, collisions (i.e. when we have the same hash for 2 different features) start occurring. These collisions may influence the results. Often for the worse, but not necessarily: Multiple features sharing the same hash can have a <a href="https://www.analyticsvidhya.com/blog/2016/03/practical-guide-principal-component-analysis-python/">PCA-like</a> effect of dimensionality reduction.</p>
<p>If you feel hashing is really hurting the performance of your model, you could increase the number of bits required to produce the hash. Read more about hash functions <a href="https://en.wikipedia.org/wiki/Hash_function">here</a>.</p>
<p>Itâ€™s friendly to online learning where you can train on a dataset that doesnâ€™t fit in memory because you need to see each example only once. One-hot encoding/Label Encoding will not work with online learning because to prepare dictionaries you need to see whole dataset first.</p>
<p>&nbsp;</p>
<h3>Input Format</h3>
<p>Vowpal Wabbit reads data from files or from standard input stream (stdin) assuming the following format:</p>
[Label] [Importance] [Tag]|Namespace Features |Namespace Features &#8230; |Namespace Features</p>
<p>Namespace=String[:Value]
<p>Features=(String[:Value] )*</p>
<p>here [] denotes non-mandatory elements, and (&#8230;)* means some repeats.</p>
<ul>
<li><b>Label</b> is a number. In case of classification it is usually 1 and -1, in case of regression it is some real float value</li>
<li><b>Importance</b> is a number, it denotes sample weight during training. Setting this helps when working with imbalanced data</li>
<li><b>Tag</b> is some string without spaces &#8211; it is a &#8220;name&#8221; of sample, VW saves it upon prediction. In order to separate Tag and Importance it is better to start Tag with the &#8216; character</li>
<li><b>Namespace</b> is for creation of separate feature spaces</li>
<li><b>Features</b> are object features inside <b>Namespace</b>. Features have weight 1.0 by default, but it can be changed, for example &#8211; feature:0.1</li>
</ul>
<p>&nbsp;</p>
<h3>VW for text classification</h3>
<p>Consider a setting where a movie production company wants to build a real time IMDB review extraction and prediction system. This is a binary classification problem and the task is to predict whether a particular review is positive or negative. The <a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">data</a> provided has only the actual reviews as text. Here, I will show you how easy it is to work with text data using Vowpal Wabbit.</p>
<p>Installing Vowpal Wabbit is easy and can be done using the documentation provided at this <a href="https://github.com/JohnLangford/vowpal_wabbit">link.</a></p>
<p>VW can only be used via command line syntax. Help related to all the functions can be seen using the following command:</p>
<pre><span class="c1">!vw --help
</span></pre>
<p>The review dataset is provided in the form of text files at the provided link. The following piece of python code helps to read all the text files and combine it into a single file.</p>
<pre>import os
import numpy as np
import re
from sklearn.datasets import load_files
from sklearn.metrics import accuracy_score, roc_auc_score

# Read the train Data
path_to_movies = os.path.expanduser('/home/analyticsvidhya/IMDB/')
reviews_train = load_files(os.path.join(path_to_movies, 'train'))
text_train, y_train = reviews_train.data, reviews_train.target</pre>
<p>Next, we will convert the text to the Vowpal Wabbit format described above.</p>
<p>Here, it is difficult to put the words (features) in different namespaces and hence we will just consider all the words separately without considering the interaction features.</p>
<p>We have done a very basic removal of all the words with less than three characters (like &#8216;is&#8217;, &#8216;in&#8217; etc.) because these do not add any valuable information to the model.</p>
<pre>def to_vw_format(document, label=None):
      return str(label or '') + ' |text ' + ' '.join(re.findall('\w{3,}', document.lower())) + '\n'
to_vw_format(str(text_train[1]), 1 if y_train[0] == 1 else -1)

<img class="aligncenter wp-image-40587 " src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_1.png" alt="" width="922" height="268" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_1.png 1326w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_1-300x87.png 300w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_1-768x224.png 768w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_1-850x247.png 850w" sizes="(max-width: 922px) 100vw, 922px" />
# Splitting train data to train and validation sets
train_size = int(0.7 * len(text_train))
train, train_labels = text_train[:train_size], y_train[:train_share]
valid, valid_labels = text_train[train_size:], y_train[train_share:]

# Convert and save in vowpal wabbit format
with open('movie_reviews_train.vw', 'w') as vw_train_data:
   for text, target in zip(train, train_labels):
   vw_train_data.write(to_vw_format(str(text), 1 if target == 1 else -1))
with open('movie_reviews_valid.vw', 'w') as vw_train_data:
   for text, target in zip(valid, valid_labels):
   vw_train_data.write(to_vw_format(str(text), 1 if target == 1 else -1))</pre>
<p>VW can be used at the terminal or command line. If you prefer using the python notebook, you can use &#8216;!&#8217; before the command to run it like any other python code.</p>
<p>&nbsp;</p>
<h3><strong>Training</strong></h3>
<pre># Fitting a logistic regression for predicting the sentiment of a review
!vw -d movie_reviews_train.vw --loss_function logistic -f movie_reviews_model.vw
</pre>
<p>Now, let&#8217;s examine each of the syntax elements in the above command line function.</p>
<ul>
<li><strong>-d</strong> This is used to include the  path to train the data for the command line.</li>
<li><strong>&#8211;loss_function</strong> This is used to declare the loss function (e.g. squared, logistic, hinge etc.).</li>
<li><strong>-f </strong> This is used to save the model for using it on a different dataset.</li>
</ul>
<p><img class="aligncenter wp-image-40588 " src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2.png" alt="" width="933" height="606" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2.png 1129w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2-300x195.png 300w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2-768x499.png 768w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2-850x552.png 850w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_2-258x169.png 258w" sizes="(max-width: 933px) 100vw, 933px" /></p>
<p>&nbsp;</p>
<h3><strong>Testing</strong></h3>
<pre>!vw -i movie_reviews_model.vw -t -d movie_reviews_valid.vw -p movie_valid_pred.txt --quiet</pre>
<ul>
<li><strong>-i</strong> Read the model from the given file</li>
<li><strong>-t -d</strong> Declare that we are dealing with dataset without labels (test) at this path</li>
<li><strong>-p</strong> Save predictions to this file</li>
<li><strong>&#8211;quiet</strong> Do not print any steps taken for prediction</li>
</ul>
<pre>with open('movie_valid_pred.txt') as pred_file:
     valid_prediction = [float(label) for label in pred_file.readlines()]
     print("Accuracy: {}".format(round(accuracy_score(valid_labels, [int(pred_prob &gt; 0) for pred_prob in valid_prediction]), 5)))
     print("AUC: {}".format(round(roc_auc_score(valid_labels, valid_prediction), 5)))
<img class="aligncenter wp-image-40589 " src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_3.png" alt="" width="996" height="161" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_3.png 1130w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_3-300x49.png 300w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_3-768x124.png 768w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_3-850x138.png 850w" sizes="(max-width: 996px) 100vw, 996px" />
</pre>
<p>That is quite an impressive accuracy for such a simple model and it only gets better as we stream more data to VW.</p>
<p>We have built a decent model by just feeding it the actual words and a label. And we did this in a fraction of a second on an average system! Now, let&#8217;s try hinge loss as the loss function to check if we can improve the results.</p>
<p><img class="aligncenter wp-image-40591 " src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_4.png" alt="" width="954" height="213" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_4.png 1128w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_4-300x67.png 300w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_4-768x172.png 768w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_4-850x190.png 850w" sizes="(max-width: 954px) 100vw, 954px" /></p>
<p>Hinge loss did not show any significant improvement. Let&#8217;s move on to adding more features via n-grams.</p>
<p>&nbsp;</p>
<h3>n-grams</h3>
<p>VW also supports other functionalities specifically for text data.</p>
<p>For those new to the Natural Language Processing landscape, n-grams are a continuous sequence of n words inside a text. For example, in the sentence:</p>
<p>&#8220;Hi, I don&#8217;t like dark places&#8221;</p>
<p>&#8216;don&#8217;t like&#8217; and &#8216;dark places&#8217; are examples of bi-grams (n =2).</p>
<p><img class="transparent aligncenter" src="https://i.stack.imgur.com/8ARA1.png" alt="https://i.stack.imgur.com/8ARA1.png" width="424" height="175" /></p>
<p>Let us briefly discuss how n-grams can provide a boost to accuracy. We&#8217;ll take the phrase &#8211; &#8216;not the best experience&#8217;. This phrase has a definite negative sentiment, but if we don&#8217;t use n-grams, a positive weight will be learnt owing to the presence of the word &#8216;best&#8217;. However, if we include bi-grams, &#8216;not best&#8217; will be a separate feature and will have a more negative weight which is appropriate in this case.</p>
<p>VW supports n-grams via command line tags. Let us try including bi-grams in our model to see whether they improve the overall accuracy.</p>
<pre>!vw -d movie_reviews_train.vw --loss_function logistic --ngram 2 -f movie_reviews_model_bigram.vw --quiet</pre>
<p><img class="aligncenter wp-image-40592 size-full" src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5.png" alt="" width="1104" height="38" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5.png 1104w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5-300x10.png 300w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5-768x26.png 768w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5-850x29.png 850w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/im_5-1080x38.png 1080w" sizes="(max-width: 1104px) 100vw, 1104px" /></p>
<p>Indeed, we get a better accuracy and a higher AUC after including the bi-grams in the model. We can also try including tri-grams or higher to see if we can get a further boost in the accuracy.</p>
<p>&nbsp;</p>
<h3>Model Interpretability</h3>
<p>How do we interpret the model we just created? VW has a one line solution for it. We just need to replace <em>-f</em>  with <em>&#8211;invert_hash</em> and it will produce a model file that is human readable.</p>
<pre>!vw -d movie_reviews_train.vw --loss_function logistic --ngram 2 --invert_hash movie_reviews_readable_model_bigram.vw</pre>
<p><img class="aligncenter wp-image-40593 " src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screenshot-from-2018-01-15-11-36-46.png" alt="" width="503" height="469" srcset="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screenshot-from-2018-01-15-11-36-46.png 570w, https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/01/Screenshot-from-2018-01-15-11-36-46-300x279.png 300w" sizes="(max-width: 503px) 100vw, 503px" /></p>
<p>We can see the weights learnt for various bi-grams in the above screenshot. &#8216;abominably, being a strictly negative word, produces mostly negative weights except in a few cases where the other word is highly positive, like &#8216;best&#8217;.</p>
<p>&nbsp;</p>
<h3>Regularization</h3>
<p>L1 and L2 regularization may be added with the <em>&#8211;l1</em> and <em>&#8211;l2</em> options respectively.</p>
<pre>!vw -d movie_reviews_train.vw --l1 0.00005 --l2 0.00005 --loss_function logistic --ngram 2 -f movie_reviews_model_bigram.vw</pre>
<p>&nbsp;</p>
<h2>End Notes</h2>
<p>Text data is not the only domain where online learning is useful. It performs really well for CTR (Click-through Rate) predictions and supports many different optimization methods. I plan to cover these in a future article.</p>
<p>Meanwhile, please let me know in the comments if you have ever used VW or online learning to solve a problem. Detailed documentation on VW can be viewed <a href="https://github.com/JohnLangford/vowpal_wabbit/wiki">here</a>.</p>
<br>
<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2,h3').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
