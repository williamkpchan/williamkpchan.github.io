var ImgList = [
'<h2>nodejs server</h2>\n<center><div id="nodejs_servertoc" class="toc"><a href="#nodejs_servertopic-0" target="_self">What will we need?</a><br><a href="#nodejs_servertopic-1" target="_self">Project Setup.</a><br><a href="#nodejs_servertopic-2" target="_self">Setting up the Request</a><br><a href="#nodejs_servertopic-3" target="_self">Make the Request</a><br><a href="#nodejs_servertopic-4" target="_self">Selectors</a><br><a href="#nodejs_servertopic-5" target="_self">Looping</a><br><a href="#nodejs_servertopic-6" target="_self">Finding</a><br><a href="#nodejs_servertopic-7" target="_self">Children</a><br><a href="#nodejs_servertopic-8" target="_self">Text &amp; HTML</a><br><a href="#nodejs_servertopic-9" target="_self">Additional Methods</a><br><a href="#nodejs_servertopic-10" target="_self">Chrome Developer Tools</a><br><a href="#nodejs_servertopic-11" target="_self">Limitations</a><br></div></center><br><br>\n<md>var http = require(\'http\');\nhttp.createServer(\n function (req, res) {\n  res.writeHead(200, {\'Content-Type\': \'text/html\'});\n  res.end(\'Hello World!\');\n}).listen(8080);</md>\n\n<h3 id="nodejs_servertopic-0">What will we need?</h3>For this project we’ll be using <a href="https://nodejs.org/en/" target="_blank">Node.js</a>. \n\nWe’ll also be using two open-sourced <a href="https://www.npmjs.com/" target="_blank"><i>npm</i></a><i> </i>modules to make today’s task a little easier:\n<a href="https://github.com/request/request-promise" target="_blank"><i>request-promise </i></a>— Request is a simple HTTP client that allows us to make quick and easy HTTP calls.\n<a href="https://github.com/cheeriojs/cheerio" target="_blank"><i>cheerio</i></a> — jQuery for Node.js. \nCheerio makes it easy to select, edit, and view DOM elements.\n<h3 id="nodejs_servertopic-1">Project Setup.</h3>Create a new project folder. \nWithin that folder create an <k>index.js</k> file. \nWe’ll need to install and require our dependencies. \nOpen up your command line, and install and save:<i> request, request-promise, and cheerio</i>\nnpm install --save request request-promise cheerio\nThen require them in our <k>index.js</k> file:\nconst rp = require("request-promise");\nconst cheerio = require("cheerio");\n<h3 id="nodejs_servertopic-2">Setting up the Request</h3><k>request-promise</k> accepts an object as input, and returns a promise. \nThe <k>options</k> object needs to do two things:\nPass in the url we want to scrape.\nTell Cheerio to load the returned HTML so that we can use it.\n\nHere’s what that looks like:\nconst options = {\n uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`" target="_blank">https://www.yourURLhere.com`</a>,\n transform: function (body) { return cheerio.load(body); }\n};\nThe <k>uri</k> key is simply the website we want to scrape.\nThe <k>transform</k> key tells <k>request-promise</k> to take the returned body and load it into Cheerio before returning it to us.\nAwesome. \nWe’ve successfully set up our HTTP request options! Here’s what your code should look like so far:\nconst rp = require("request-promise");\nconst cheerio = require("cheerio");const options = {\n uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`" target="_blank">https://www.yourURLhere.com`</a>,\n transform: function (body) { return cheerio.load(body); }\n};\n<h3 id="nodejs_servertopic-3">Make the Request</h3>Now that the options are taken care of, we can actually make our request. \nThe boilerplate in the documentation for that looks like this:\nrp(<i>OPTIONS</i>)\n  .then(function (data) {\n    // <i>REQUEST SUCCEEDED:</i> <i>DO SOMETHING</i>\n  })\n  .catch(function (err) {\n    // <i>REQUEST FAILED: ERROR OF SOME KIND</i>\n  });\nWe pass in our <k>options</k> object to <k>request-promise</k>, then wait to see if our request succeeds or fails. \nEither way, we do something with the returned data.\nKnowing what the documentation says to do, lets create our own version:\nrp(options)\n .then(($) =&gt; { console.log($); })\n .catch((err) =&gt; { console.log(err); });\nThe code is pretty similar. \nThe big difference is I’ve used arrow functions. \nI’ve also logged out the returned data from our HTTP request. \nWe’re going to test to make sure everything is working so far.\nReplace the placeholder <k>uri</k> with the website you want to scrape. \nThen, open up your console and type:\n<i>node index.js</i>// LOGS THE FOLLOWING:\n{ [Function: initialize]\n fn:\n  initialize {\n   constructor: [Circular],\n   _originalRoot:\n   { type: "root",\n    name: "root",\n    namespace: "<a href="http://www.w3.org/1999/xhtml" "="" target="_blank">http://www.w3.org/1999/xhtml"</a>,\n    attribs: {},\n    ...\nIf you don’t see an error, then everything is working so far — and you just made your first scrape!\n\nHere is the full code of our boilerplate:\nconst rp = require(\'request-promise\');\nconst cheerio = require(\'cheerio\');\nconst options = {\n uri: `https://www.google.com`,\n transform: function (body) { return cheerio.load(body); }\n};\n\nrp(options)\n .then(($) =&gt; {\n  console.log($);\n })\n .catch((err) =&gt; {\n  console.log(err);\n });\n\nBoilerplate web scraping code\n<h3 id="nodejs_servertopic-4">Selectors</h3>First and foremost, Cheerio’s selector implementation is nearly identical to jQuery’s. \nSo if you know jQuery, this will be a breeze. \nThe selector method allows you to traverse and select elements in the document. \nYou can get data and set data using a selector. \nImagine we have the following HTML in the website we want to scrape:\n<i>&lt;ul id="cities"&gt;\n &lt;li class="large"&gt;New York&lt;/li&gt;\n &lt;li id="medium"&gt;Portland&lt;/li&gt;\n &lt;li class="small"&gt;Salem&lt;/li&gt;\n&lt;/ul&gt;</i>\nWe can select id’s using (<k>#</k>), classes using (<k>.</k>), and elements by their tag names, ex: <k>div</k>.\n$(".large").text() // New York\n$("#medium").text() // Portland\n$("li[class=small]").html() // &lt;li class="small"&gt;Salem&lt;/li&gt;\n<h3 id="nodejs_servertopic-5">Looping</h3>Just like jQuery, we can also iterate through multiple elements with the <k>each()</k> function. \nUsing the same HTML code as above, we can return the inner text of each <k>li</k> with the following code:\n\n$("li").each(function(i, elem) {\n cities[i] = $(this).text();\n});// New York Portland Salem\n<h3 id="nodejs_servertopic-6">Finding</h3><k>find()</k>\nImagine we have two lists on our web site:\n<i>&lt;ul id="cities"&gt;\n &lt;li class="large"&gt;New York&lt;/li&gt;\n &lt;li id="c-medium"&gt;Portland&lt;/li&gt;\n &lt;li class="small"&gt;Salem&lt;/li&gt;\n&lt;/ul&gt;\n&lt;ul id="towns"&gt;\n &lt;li class="large"&gt;Bend&lt;/li&gt;\n &lt;li id="t-medium"&gt;Hood River&lt;/li&gt;\n &lt;li class="small"&gt;Madras&lt;/li&gt;\n&lt;/ul&gt;</i>\nWe can select each list using their respective ID’s, then find the <em>small </em>city/town within each list:\n$("#cities").find(".small").text() // Salem\n$("#towns").find(".small").text() // Madras\n<blockquote>Finding will search all descendant DOM elements, not just immediate children as shown in this example.</blockquote>\n<h3 id="nodejs_servertopic-7">Children</h3><k>.children()</k>\nChildren is similar to find. \nThe difference is that children <i>only </i>searches for immediate children of the selected element.\n$("#cities").children("#c-medium").text();\n// Portland\n<h3 id="nodejs_servertopic-8">Text &amp; HTML</h3>Up until this point, all of my examples have included the <k>.text()</k> function. \nHopefully you’ve been able to figure out that this function is what gets the text of the selected element. \nYou can also use <k>.html()</k> to return the html of the given element:\n$(".large")<i>.text()</i> // Bend\n$(".large")<i>.html()</i> // &lt;li class="large"&gt;Bend&lt;/li&gt;\n<h4 id="nodejs_servertopic-9">Additional Methods</h4>There are more methods than I can count, and the documentation for all of them is available <a href="https://github.com/cheeriojs/cheerio" target="_blank"><i>here</i></a>.\n<h3 id="nodejs_servertopic-10">Chrome Developer Tools</h3>Don’t forget, the Chrome Developer Tools are your friend. \nIn Google Chrome, you can easily find element, class, and ID names using: <i>CTRL + SHIFT + F</i>\n<h3 id="nodejs_servertopic-11">Limitations</h3>MOST websites modify the DOM using JavaScript. \nUnfortunately Cheerio doesn’t resolve parsing a modified DOM. \nDynamically generated content from procedures leveraging AJAX, client-side logic, and other async procedures are not available to Cheerio.\nRemember this is an introduction to basic scraping. \nIn order to get started you’ll need to find a static website with minimal DOM manipulation.\n\n<a href="https://www.freecodecamp.org/news/the-ultimate-guide-to-web-scraping-with-node-js-daa2027dcd3/" target="_blank">The Ultimate Guide to Web Scraping with Node.js</a>\n<a href="https://stackabuse.com/web-scraping-with-node-js/" target="_blank">Web Scraping with Node.js</a>\n<a href="https://levelup.gitconnected.com/web-scraping-with-node-js-c93dcf76fe2b" target="_blank">Web Scraping with Node.js</a>\n<a href="https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27" target="_blank">Debugging Node.js with Chrome DevTools</a>\n<a href="https://medium.com/the-node-js-collection/debugging-node-js-with-google-chrome-4965b5f910f4" target="_blank">Debugging Node.js with Google Chrome</a>\n<a href="https://www.google.com/search?q=chrome+node.js&amp;oq=chrome+node.js&amp;aqs=chrome..69i57.5657j0j7&amp;sourceid=chrome&amp;ie=UTF-8" target="_blank">chrome node.js</a>\n',
'<h2>web scraping</h2>acquiring the data using an HTML request library or a headless browser,\nand parsing the data to get the exact information you want.\nThis guide will walk you through the process with the popular Node.js request-promise module, CheerioJS, and Puppeteer.\n<h3>Making your first request</h3>npm install --save request request-promise cheerio puppeteer\n\nNext, let’s open a new text file (name the file potusScraper.js), and write a quick function to get the HTML of the Wikipedia “List of Presidents” page.\nconst rp = require(\'request-promise\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log(html);\n  })\n  .catch(function(err){\n    //handle error\n  });',
'<h3>Using Chrome DevTools</h3>Cool, we got the raw HTML from the web page! But now we need to make sense of this giant blob of text.\nTo do that, we’ll need to use Chrome DevTools to allow us to easily search through the HTML of a web page.\nUsing Chrome DevTools is easy: simply open Google Chrome, and right click on the element you would like to scrape (in this case I am right clicking on George Washington, because we want to get links to all of the individual presidents’ Wikipedia pages):\n\nNow, simply click inspect, and Chrome will bring up its DevTools pane, allowing you to easily inspect the page’s source HTML.',
'<h3>Parsing HTML with Cheerio.js</h3>Let’s use Cheerio.js to parse the HTML we received earlier to return a list of links to the individual Wikipedia pages of U.S.presidents.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log($(\'big > a\', html).length);\n    console.log($(\'big > a\', html));\n  })\n  .catch(function(err){\n    //handle error\n  });\n\nOutput:\n{ \'0\':\n  { type: \'tag\',\n    name: \'a\',\n    attribs: { href: \'/wiki/George_Washington\', title: \'George Washington\' },\n    children: [ [Object] ],\n    next: null,\n    prev: null,\n    parent:\n      { type: \'tag\',\n        name: \'big\',\n        attribs: {},\n        children: [Array],\n        next: null,\n        prev: null,\n        parent: [Object] } },\n  \'1\':\n    { type: \'tag\'\n  ...\n\nWe check to make sure there are exactly 45 elements returned (the number of U.S.presidents), meaning there aren’t any extra hidden “big” tags elsewhere on the page.\nNow, we can go through and grab a list of links to all 45 presidential Wikipedia pages by getting them from the “attribs” section of each element.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    const wikiUrls = [];\n    for (let i = 0; i < 45; i++) {\n      wikiUrls.push($(\'big > a\', html)[i].attribs.href);\n    }\n    console.log(wikiUrls);\n  })\n  .catch(function(err){\n    //handle error\n  });\n\nNow we have a list of all 45 presidential Wikipedia pages.\nLet’s create a new file (named potusParse.js), which will contain a function to take a presidential Wikipedia page and return the president’s name and birthday.\nFirst things first, let’s get the raw HTML from George Washington’s Wikipedia page.\n\nconst rp = require(\'request-promise\');\nconst url = \'https://en.wikipedia.org/wiki/George_Washington\';\n\nrp(url)\n  .then(function(html) {\n    console.log(html);\n  })\n  .catch(function(err) {\n    //handle error\n  });\nLet’s once again use Chrome DevTools to find the syntax of the code we want to parse, so that we can extract the name and birthday with Cheerio.js.\n\nSo we see that the name is in a class called “firstHeading” and the birthday is in a class called “bday”.\nLet’s modify our code to use Cheerio.js to extract these two classes.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/George_Washington\';\n\nrp(url)\n  .then(function(html) {\n    console.log($(\'.firstHeading\', html).text());\n    console.log($(\'.bday\', html).text());\n  })\n  .catch(function(err) {\n    //handle error\n  });<h3>Putting it all together</h3>Perfect! Now let’s wrap this up into a function and export it from this module.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\n\nconst potusParse = function(url) {\n  return rp(url)\n    .then(function(html) {\n      return {\n        name: $(\'.firstHeading\', html).text(),\n        birthday: $(\'.bday\', html).text(),\n      };\n    })\n    .catch(function(err) {\n      //handle error\n    });\n};\n\nmodule.exports = potusParse;\n\nNow let’s return to our original file potusScraper.js and require the potusParse.js module.\nWe’ll then apply it to the list of wikiUrls we gathered earlier.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst potusParse = require(\'./potusParse\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html) {\n    //success!\n    const wikiUrls = [];\n    for (let i = 0; i < 45; i++) {\n      wikiUrls.push($(\'big > a\', html)[i].attribs.href);\n    }\n    return Promise.all(\n      wikiUrls.map(function(url) {\n        return potusParse(\'https://en.wikipedia.org\' + url);\n      })\n    );\n  })\n  .then(function(presidents) {\n    console.log(presidents);\n  })\n  .catch(function(err) {\n    //handle error\n    console.log(err);\n  });',
'<h3>Rendering JavaScript Pages</h3>Voilà! A list of the names and birthdays of all 45 U.S.presidents.\nUsing just the request-promise module and Cheerio.js should allow you to scrape the vast majority of sites on the internet.\nRecently, however, many sites have begun using JavaScript to generate dynamic content on their websites.\nThis causes a problem for request-promise and other similar HTTP request libraries (such as axios and fetch), because they only get the response from the initial request, but they cannot execute the JavaScript the way a web browser can.\nThus, to scrape sites that require JavaScript execution, we need another solution.\nIn our next example, we will get the titles for all of the posts on the front page of Reddit.\nLet’s see what happens when we try to use request-promise as we did in the previous example.\nOutput:\n\nconst rp = require(\'request-promise\');\nconst url = \'https://www.reddit.com\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log(html);\n  })\n  .catch(function(err){\n    //handle error\n  });\n}\n\nHmmm…not quite what we want.\nThat’s because getting the actual content requires you to run the JavaScript on the page! With Puppeteer, that’s no problem.\nPuppeteer is an extremely popular new module brought to you by the Google Chrome team that allows you to control a headless browser.\nThis is perfect for programmatically scraping pages that require JavaScript execution.\nLet’s get the HTML from the front page of Reddit using Puppeteer instead of request-promise.\n\nconst puppeteer = require(\'puppeteer\');\nconst url = \'https://www.reddit.com\';\n\npuppeteer\n  .launch()\n  .then(function(browser) {\n    return browser.newPage();\n  })\n  .then(function(page) {\n    return page.goto(url).then(function() {\n      return page.content();\n    });\n  })\n  .then(function(html) {\n    console.log(html);\n  })\n  .catch(function(err) {\n    //handle error\n  });\n\nNice! The page is filled with the correct content!\nNow we can use Chrome DevTools like we did in the previous example.\nIt looks like Reddit is putting the titles inside “h2” tags.\nLet’s use Cheerio.js to extract the h2 tags from the page.\n\nconst puppeteer = require(\'puppeteer\');\nconst $ = require(\'cheerio\');\nconst url = \'https://www.reddit.com\';\n\npuppeteer\n  .launch()\n  .then(function(browser) {\n    return browser.newPage();\n  })\n  .then(function(page) {\n    return page.goto(url).then(function() {\n      return page.content();\n    });\n  })\n  .then(function(html) {\n    $(\'h2\', html).each(function() {\n      console.log($(this).text());\n    });\n  })\n  .catch(function(err) {\n    //handle error\n  });',
'<h3>Additional Resources</h3>And there’s the list! At this point you should feel comfortable writing your first web scraper to gather data from any website.\nHere are a few additional resources that you may find helpful during your web scraping journey:\n\n<a href="https://www.scraperapi.com/blog/the-10-best-rotating-proxy-services-for-web-scraping" rel="noopener" target="_blank">List of web scraping proxy services</a>\n<a href="https://www.scraperapi.com/blog/the-10-best-web-scraping-tools" rel="noopener" target="_blank">List of handy web scraping tools</a>\n<a href="https://www.scraperapi.com/blog/5-tips-for-web-scraping" target="_blank">List of web scraping tips</a>\n',
'<h2>build a web scraping application</h2>\n<center><div id="scrapingapplicationtoc" class="toc"><a href="#scrapingapplicationtopic-0" target="_self">Step 1 — Setting Up the Web Scraper</a><br><a href="#scrapingapplicationtopic-1" target="_self">Step 2 — Setting Up the Browser Instance</a><br><a href="#scrapingapplicationtopic-2" target="_self">Step 3 — Scraping Data from a Single Page</a><br><a href="#scrapingapplicationtopic-3" target="_self">Step 4 — Scraping Data From Multiple Pages</a><br><a href="#scrapingapplicationtopic-4" target="_self">Step 5 — Scraping Data by Category</a><br><a href="#scrapingapplicationtopic-5" target="_self">Step 6 — Scraping Data from Multiple Categories and Saving the Data as JSON</a><br></div></center><br><br>\n\n<h2>Introduction</h2>build a web scraping application using Node.js and Puppeteer.\nFirst, you will code your app to open <a href="https://www.chromium.org/getting-involved/download-chromium">Chromium</a> and load a special website designed as a web-scraping sandbox: <a href="http://books.toscrape.com">books.toscrape.com</a>.\nIn the next two steps, you will scrape all the books on a single page of books.toscrape and then all the books across multiple pages. In the remaining steps, you will filter your scraping by book category and then save your data as a JSON file.\n<i>Warning: Scraping any other domain falls outside the scope of this tutorial.</i>\n\n<h3 id="scrapingapplicationtopic-0">Step 1 — Setting Up the Web Scraper</h3>With Node.js installed, you can begin setting up your web scraper. First, you will create a project root directory and then install the required dependencies. This tutorial requires just one dependency, and you will install it using Node.js’s default package manager <a href="https://www.digitalocean.com/community/tutorial_series/how-to-code-in-node-js">npm</a>. npm comes preinstalled with Node.js, so you don’t need to install it.\nCreate a folder for this project and then move inside:\n<k>mkdir book-scraper\ncd book-scraper</k>\nYou will run all subsequent commands from this directory.\nWe need to install one package using npm, or the node package manager. First initialize npm in order to create a <k>packages.json</k> file, which will manage your project’s dependencies and metadata.\nInitialize npm for your project:\n<k>npm init</k>\nnpm will present a sequence of prompts. You can press <k>ENTER</k> to every prompt, or you can add personalized descriptions. Make sure to press <k>ENTER</k> and leave the default values in place when prompted for <k>entry point:</k> and <k>test command:</k>. Alternately, you can pass the <k>y</k> flag to <k>npm</k>—<k>npm init -y</k>—and it will submit all the default values for you.\nYour output will look something like this:\n<k>Output{\n  "name": "<k>sammy_scraper</k>",\n  "version": "<k>1.0.0</k>",\n  "description": "<k>a web scraper</k>",\n  "main": "index.js",\n  "scripts": {\n    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"\n  },\n  "keywords": [],\n  "author": "<k>sammy the shark</k>",\n  "license": "<k>ISC</k>"\n}\n<k>Is this OK? (yes) yes</k></k>\nType <k>yes</k> and press <k>ENTER</k>. npm will save this output as your <k>package.json</k> file.\nNow use npm to install Puppeteer:\n<k>npm install --save puppeteer</k>\nThis command installs both Puppeteer and a version of Chromium that the Puppeteer team knows will work with their API.\nOn Linux machines, Puppeteer might require some additional dependencies.\nIf you are using Ubuntu 18.04, <a href="https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md#chrome-headless-doesnt-launch-on-unix">check the ‘Debian Dependencies’ dropdown inside the ‘Chrome headless doesn’t launch on UNIX’ section of Puppeteer’s troubleshooting docs</a>. You can use the following command to help find any missing dependencies:\n<k>ldd chrome | grep not</k>\nWith npm, Puppeteer, and any additional dependencies installed, your <k>package.json</k> file requires one last configuration before you start coding. In this tutorial, you will launch your app from the command line with <k>npm run start</k>. You must add some information about this <k>start</k> script to <k>package.json</k>. Specifically, you must add one line under the <k>scripts</k> directive regarding your <k>start</k> command.\nOpen the file in your preferred text editor:\n<k>nano package.json</k>\nFind the <k>scripts:</k> section and add the following configurations. Remember to place a comma at the end of the <k>test</k> script line, or your file will not parse correctly.\n<k>Output{\n  . . .\n  "scripts": {\n    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"<k>,</k>\n  <k>"start": "node index.js"</k>\n  },\n  . . .\n  "dependencies": {\n    "puppeteer": "^5.2.1"\n  }\n}</k>\nYou will also notice that <k>puppeteer</k> now appears under <k>dependencies</k> near the end of the file. Your <k>package.json</k> file will not require any more revisions. Save your changes and close your editor.\nYou are now ready to start coding your scraper. In the next step, you will set up a browser instance and test your scraper’s basic functionality.\n\n<h3 id="scrapingapplicationtopic-1">Step 2 — Setting Up the Browser Instance</h3>When you open a traditional browser, you can do things like click buttons, navigate with your mouse, type, open the dev tools, and more. A headless browser like Chromium allows you to do these same things, but programmatically and without a user interface. In this step, you will set up your scraper’s browser instance. When you launch your application, it will automatically open Chromium and navigate to <a href="http://books.toscrape.com">books.toscrape.com</a>. These initial actions will form the basis of your program.\nYour web scraper will require four <k>.js</k> files: <k>browser.js</k>, <k>index,js</k>, <k>pageController.js</k>, and <k>pageScraper.js</k>. In this step, you will create all four files and then continually update them as your program grows in sophistication. Start with <k>browser.js</k>; this file will contain the script that starts your browser.\nFrom your project’s root directory, create and open <k>browser.js</k> in a text editor:\n<k>nano browser.js</k>\nFirst, you will <k>require</k> Puppeteer and then create an <k>async</k> function called <k>startBrowser()</k>. This function will start the browser and return an instance of it. Add the following code:\n./book-scraper/browser.js\n<k>const puppeteer = require(\'puppeteer\');\nasync function startBrowser(){\n  let browser;\n  try {\n      console.log("Opening the browser......");\n      browser = await puppeteer.launch({\n          headless: false,\n          args: ["--disable-setuid-sandbox"],\n          \'ignoreHTTPSErrors\': true\n      });\n  } catch (err) {\n      console.log("Could not create a browser instance =&gt; : ", err);\n  }\n  return browser;\n}\nmodule.exports = {\n  startBrowser\n};</k>\n<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#puppeteerlaunchoptions">Puppeteer has a <k>.launch()</k> method</a> that launches an instance of a browser. This method returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>, so you have to <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Async_await">make sure the Promise resolves by using a <k>.then</k> or <k>await</k> block</a>.\nYou are using <k>await</k> to make sure the Promise resolves, wrapping this instance around <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch">a <k>try-catch</k> code block</a>, and then returning an instance of the browser.\nNotice that the <k>.launch()</k> method takes a JSON parameter with several values:\n<i>headless</i> - <k>false</k> means the browser will run with an Interface so you can watch your script execute, while <k>true</k> means the browser will run in headless mode. Note well, however, that if you want to deploy your scraper to the cloud, set <k>headless</k> back to <k>true</k>. Most virtual machines are headless and do not include a user interface, and hence can only run the browser in headless mode. Puppeteer also includes a <k>headful</k> mode, but that should be used solely for testing purposes.\n<i>ignoreHTTPSErrors</i> - <k>true</k> allows you to visit websites that aren’t hosted over a secure HTTPS protocol and ignore any HTTPS-related errors.\nSave and close the file.\nNow create your second <k>.js</k> file, <k>index.js</k>:\n<k>nano index.js</k>\nHere you will <k>require browser.js</k> and <k>pageController.js</k>. You will then call the <k>startBrowser()</k> function and pass the created browser instance to our page controller, which will direct its actions. Add the following code:\n./book-scraper/index.js\n<k>const browserObject = require(\'./browser\');\nconst scraperController = require(\'./pageController\');\n//Start the browser and create a browser instance\nlet browserInstance = browserObject.startBrowser();\n// Pass the browser instance to the scraper controller\nscraperController(browserInstance)</k>\nSave and close the file.\nCreate your third <k>.js</k> file, <k>pageController.js</k>:\n<k>nano pageController.js</k>\n<k>pageController.js</k> controls your scraping process. It uses the browser instance to control the <k>pageScraper.js</k> file, which is where all the scraping scripts execute. Eventually, you will use it to specify what book category you want to scrape. For now, however, you just want to make sure that you can open Chromium and navigate to a web page:\n./book-scraper/pageController.js\n<k>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    await pageScraper.scraper(browser);  \n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</k>\nThis code exports a function that takes in the browser instance and passes it to a function called <k>scrapeAll()</k>. This function, in turn, passes this instance to <k>pageScraper.scraper()</k> as an argument which uses it to scrape pages.\nSave and close the file.\nFinally, create your last <k>.js</k> file, <k>pageScraper.js</k>:\n<k>nano pageScraper.js</k>\nHere you will create an object literal with a <k>url</k> property and a <k>scraper()</k> method. The <k>url</k> is the web URL of the web page you want to scrape, while the <k>scraper()</k> method contains the code that will perform your actual scraping, although at this stage it merely navigates to a URL. Add the following code:\n./book-scraper/pageScraper.js\n<k>const scraperObject = {\n  url: \'http://books.toscrape.com\',\n  async scraper(browser){\n    let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    await page.goto(this.url);\n  }\n}\nmodule.exports = scraperObject;</k>\n<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#browsernewpage">Puppeteer has a <k>newPage()</k> method</a> that creates a new page instance in the browser, and these page instances can do quite a few things. In our <k>scraper()</k> method, you created a page instance and then used the <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagegotourl-options"><k>page.goto()</k> method</a> to navigate to <a href="http://books.toscrape.com">the books.toscrape.com homepage</a>.\nSave and close the file.\nYour program’s file-structure is now complete. The first level of your project’s directory tree will look like this:\n<k>Output.\n├── browser.js\n├── index.js\n├── node_modules\n├── package-lock.json\n├── package.json\n├── pageController.js\n└── pageScraper.js</k>\nNow run the command <k>npm run start</k> and watch your scraper application execute:\n<k>npm run start</k>\nIt will automatically open a Chromium browser instance, open a new page in the browser, and navigate to <a href="http://books.toscrape.com">books.toscrape.com</a>.\nIn this step, you created a Puppeteer application that opened Chromium and loaded the homepage for a dummy online bookstore—books.toscrape.com. In the next step, you will scrape the data for every book on that homepage.\n\n<h3 id="scrapingapplicationtopic-2">Step 3 — Scraping Data from a Single Page</h3>Before adding more functionality to your scraper application, open your preferred web browser and manually navigate to the <a href="http://books.toscrape.com/">books to scrape homepage</a>. Browse the site and get a sense of how data is structured.\n<img src="https://assets.digitalocean.com/articles/67187/web_scraper.png" alt="Books to scrape websites image">\nYou will find a category section on the left and books displayed on the right. When you click on a book, the browser navigates to a new URL that displays relevant information regarding that particular book.\nIn this step, you will replicate this behavior, but with code; you will automate the business of navigating the website and consuming its data.\nFirst, if you inspect the source code for the homepage using the Dev Tools inside your browser, you will notice that the page lists each book’s data under a <k>section</k> tag. Inside the <k>section</k> tag every book is under a <k>list</k> (<k>li</k>) tag, and it is here that you find the link to the book’s dedicated page, the price, and the in-stock availability.\n<img src="https://assets.digitalocean.com/articles/67187/bookstoscrape_devtools.png" alt="books.toscrape source code viewed in dev tools">\nYou’ll be scraping these book URLs, filtering for books that are in-stock, navigating to each individual book page, and scraping that book’s data.\nReopen your <k>pageScraper.js</k> file:\n<k>nano pageScraper.js</k>\nAdd the following highlighted content. You will nest another <k>await</k> block inside <k>await page.goto(this.url);</k>:\n./book-scraper/pageScraper.js\n<k>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n        let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    <k>// Wait for the required DOM to be rendered</k>\n    <k>await page.waitForSelector(\'.page_inner\');</k>\n    <k>// Get the link to all the required books</k>\n    <k>let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {</k>\n      <k>// Make sure the book to be scraped is in stock</k>\n      <k>links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")</k>\n      <k>// Extract the links from the data</k>\n      <k>links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)</k>\n      <k>return links;</k>\n    <k>});</k>\n    <k>console.log(urls);</k>\n    }\n}\nmodule.exports = scraperObject;</k>\nIn this code block, you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagewaitforselectorselector-options">the <k>page.waitForSelector()</k> method</a>. This waited for the div that contains all the book-related information to be rendered in the DOM, and then you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pageevalselector-pagefunction-args">the <k>page.$$eval()</k> method</a>. This method gets the URL element with the selector <k>section ol li</k> (be sure that you always return only a string or a number from the <k>page.$eval()</k> and <k>page.$$eval()</k> methods).\nEvery book has two statuses; a book is either <k>In Stock</k> or <k>Out of stock</k>. You only want to scrape books that are <k>In Stock</k>. Because <k>page.$$eval()</k> returns an array of all matching elements, you have filtered this array to ensure that you are only working with in-stock books. You did this by searching for and evaluating the class <k>.instock.availability</k>. You then mapped out the <k>href</k> property of the book links and returned it from the method.\nSave and close the file.\nRe-run your application:\n<k>npm run start</k>\nThe browser will open, navigate to the web page, and then close once the task completes. Now check your console; it will contain all the scraped URLs:\n<k>Output&gt; book-scraper@1.0.0 start <k>/Users/sammy/book-scraper</k>\n&gt; node index.js\nOpening the browser......\nNavigating to http://books.toscrape.com...\n[\n  \'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\',\n  ...]</k>\nThis is a great start, but you want to scrape all the relevant data for a particular book and not only its URL. You will now use these URLs to open each page and scrape the book’s title, author, price, availability, UPC, description, and image URL.\nReopen <k>pageScraper.js</k>:\n<k>nano pageScraper.js</k>\nAdd the following code, which will loop through each scraped link, open a new page instance, and then retrieve the relevant data:\n./book-scraper/pageScraper.js\n<k>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n        let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    // Wait for the required DOM to be rendered\n    await page.waitForSelector(\'.page_inner\');\n    // Get the link to all the required books\n    let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n      // Make sure the book to be scraped is in stock\n      links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n      // Extract the links from the data\n      links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n      return links;\n    });\n        <k>// Loop through each of those links, open a new page instance and get the relevant data from them</k>\n    <k>let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {</k>\n      <k>let dataObj = {};</k>\n      <k>let newPage = await browser.newPage();</k>\n      <k>await newPage.goto(link);</k>\n      <k>dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);</k>\n      <k>dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);</k>\n      <k>dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {</k>\n        <k>// Strip new line and tab spaces</k>\n        <k>text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");</k>\n        <k>// Get the number of stock available</k>\n        <k>let regexp = /^.*\((.*)\).*$/i;</k>\n        <k>let stockAvailable = regexp.exec(text)[1].split(\' \')[0];</k>\n        <k>return stockAvailable;</k>\n      <k>});</k>\n      <k>dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);</k>\n      <k>dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);</k>\n      <k>dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);</k>\n      <k>resolve(dataObj);</k>\n      <k>await newPage.close();</k>\n    <k>});</k>\n    <k>for(link in urls){</k>\n      <k>let currentPageData = await pagePromise(urls[link]);</k>\n      <k>// scrapedData.push(currentPageData);</k>\n      <k>console.log(currentPageData);</k>\n    <k>}</k>\n    }\n}\nmodule.exports = scraperObject; </k>\nYou have an array of all URLs. You want to loop through this array, open up the URL in a new page, scrape data on that page, close that page, and open a new page for the next URL in the array. Notice that you wrapped this code in a Promise. This is because you want to be able to wait for each action in your loop to complete. Therefore each Promise opens a new URL and won’t resolve until the program has scraped all the data on the URL, and then that page instance has closed.\n<i>Warning:</i> note well that you waited for the Promise using a <k>for-in</k> loop. Any other loop will be sufficient but avoid iterating over your URL arrays using an array-iteration method like <k>forEach</k>, or any other method that uses a callback function. This is because the callback function will have to go through the callback queue and event loop first, hence, multiple page instances will open all at once. This will place a much larger strain on your memory.\nTake a closer look at your <k>pagePromise</k> function. Your scraper first created a new page for each URL, and then you used the <k>page.$eval()</k> function to target selectors for relevant details that you wanted to scrape on the new page. Some of the texts contain whitespaces, tabs, newlines, and other non-alphanumeric characters, which you stripped off using a regular expression. You then appended the value for every piece of data scraped in this page to an Object and resolved that object.\nSave and close the file.\nRun the script again:\n<k>npm run start</k>\nThe browser opens the homepage and then opens each book page and logs the scraped data from each of those pages. This output will print to your console:\n<k>OutputOpening the browser......\nNavigating to http://books.toscrape.com...\n{\n  bookTitle: \'A Light in the Attic\',\n  bookPrice: \'£51.77\',\n  noAvailable: \'22\',\n  imageUrl: \'http://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg\',\n  bookDescription: "It\'s hard to imagine a world without A Light in the Attic. [...]\',\n  upc: \'a897fe39b1053632\'\n}\n{\n  bookTitle: \'Tipping the Velvet\',\n  bookPrice: \'£53.74\',\n  noAvailable: \'20\',\n  imageUrl: \'http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg\',\n  bookDescription: `"Erotic and absorbing...Written with starling power."--"The New York Times Book Review " Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler [...]`,\n  upc: \'90fa61229261140a\'\n}\n{\n  bookTitle: \'Soumission\',\n  bookPrice: \'£50.10\',\n  noAvailable: \'20\',\n  imageUrl: \'http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg\',\n  bookDescription: \'Dans une France assez proche de la nôtre, [...]\',\n  upc: \'6957f44c3847a760\'\n}\n...</k>\nIn this step, you scraped relevant data for every book on the homepage of <a href="http://books.toscrape.com">books.toscrape.com</a>, but you could add much more functionality. Each page of books, for instance, is paginated; how do you get books from these other pages? Also, on the left side of the website you found book categories; what if you don’t want all the books, but you just want books from a particular genre? You will now add these features.\n\n<h3 id="scrapingapplicationtopic-3">Step 4 — Scraping Data From Multiple Pages</h3>Pages on <a href="http://books.toscrape.com">books.toscrape.com</a> that are paginated have a <k>next</k> button beneath their content, while pages that are not paginated do not.\nYou will use the presence of this button to determine if the page is paginated or not. Since the data on each page is of the same structure and has the same markup, you won’t be writing a scraper for every possible page. Rather, you will use the practice of <a href="https://www.digitalocean.com/community/tutorials/js-understanding-recursion">recursion</a>.\nFirst, you need to change the structure of your code a bit to accommodate recursively navigating to several pages.\nReopen <k>pagescraper.js</k>:\n<k>nano pagescraper.js</k>\nYou will add a new function called <k>scrapeCurrentPage()</k> to your <k>scraper()</k> method. This function will contain all the code that scrapes data from a particular page and then click the next button if it exists. Add the following highlighted code:\n./book-scraper/pageScraper.js scraper()\n<k>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n    let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    <k>let scrapedData = [];</k>\n    // Wait for the required DOM to be rendered\n    <k>async function scrapeCurrentPage(){</k>\n      await page.waitForSelector(\'.page_inner\');\n      // Get the link to all the required books\n      let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n        // Make sure the book to be scraped is in stock\n        links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n        // Extract the links from the data\n        links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n        return links;\n      });\n      // Loop through each of those links, open a new page instance and get the relevant data from them\n      let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {\n        let dataObj = {};\n        let newPage = await browser.newPage();\n        await newPage.goto(link);\n        dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);\n        dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);\n        dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {\n          // Strip new line and tab spaces\n          text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");\n          // Get the number of stock available\n          let regexp = /^.*\((.*)\).*$/i;\n          let stockAvailable = regexp.exec(text)[1].split(\' \')[0];\n          return stockAvailable;\n        });\n        dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);\n        dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);\n        dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);\n        resolve(dataObj);\n        await newPage.close();\n      });\n      for(link in urls){\n        let currentPageData = await pagePromise(urls[link]);\n        <k>scrapedData.push(currentPageData);</k>\n        // console.log(currentPageData);\n      }\n      // When all the data on this page is done, click the next button and start the scraping of the next page\n      // You are going to check if this button exist first, so you know if there really is a next page.\n      <k>let nextButtonExist = false;</k>\n      <k>try{</k>\n        <k>const nextButton = await page.$eval(\'.next &gt; a\', a =&gt; a.textContent);</k>\n        <k>nextButtonExist = true;</k>\n      <k>}</k>\n      <k>catch(err){</k>\n        <k>nextButtonExist = false;</k>\n      }\n      <k>if(nextButtonExist){</k>\n        <k>await page.click(\'.next &gt; a\');  </k>\n        <k>return scrapeCurrentPage(); // Call this function recursively</k>\n      <k>}</k>\n      <k>await page.close();</k>\n      <k>return scrapedData;</k>\n    <k>}</k>\n    <k>let data = await scrapeCurrentPage();</k>\n    <k>console.log(data);</k>\n    <k>return data;</k>\n  }\n}\nmodule.exports = scraperObject;</k>\nYou set the <k>nextButtonExist</k> variable to false initially, and then check if the button exists. If the <k>next</k> button exists, you set <k>nextButtonExists</k> to <k>true</k> and proceed to click the <k>next</k> button, and then call this function recursively.\nIf <k>nextButtonExists</k> is false, it returns the <k>scrapedData</k> array as usual.\nSave and close the file.\nRun your script again:\n<k>npm run start</k>\nThis might take a while to complete; your application, after all, is now scraping the data from over 800 books. Feel free to either close the browser or press <k>CTRL + C</k> to cancel the process.\nYou have now maximized your scraper’s capabilities, but you’ve created a new problem in the process. Now the issue is not too little data but too much data. In the next step, you will fine-tune your application to filter your scraping by book category.\n\n<h3 id="scrapingapplicationtopic-4">Step 5 — Scraping Data by Category</h3>To scrape data by category, you will need to modify both your <k>pageScraper.js</k> file and your <k>pageController.js</k> file.\nOpen <k>pageController.js</k> in a text editor. Call the scraper so that it only scrapes travel books. Add the following code:\n./book-scraper/pageController.js\n<k>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    <k>let scrapedData = {};</k>\n    // Call the scraper for different set of books to be scraped\n    <k>scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');</k>\n    <k>await browser.close();</k>\n    <k>console.log(scrapedData)</k>\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</k>\nYou are now passing two parameters into your <k>pageScraper.scraper()</k> method, with the second parameter being the category of books you want to scrape, which in this example is <k>Travel</k>. But your <k>pageScraper.js</k> file does not recognize this parameter yet. You will need to adjust this file, too.\nSave and close the file.\nOpen <k>pageScraper.js</k>:\n<k>nano pageScraper.js</k>\nAdd the following code, which will add your category parameter, navigate to that category page, and then begin scraping through the paginated results:\n./book-scraper/pageScraper.js\n<k>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    <k>async scraper(browser, category){</k>\n        let page = await browser.newPage();\n        console.log(`Navigating to ${this.url}...`);\n        // Navigate to the selected page\n        await page.goto(this.url);\n        // Select the category of book to be displayed\n    <k>let selectedCategory = await page.$$eval(\'.side_categories &gt; ul &gt; li &gt; ul &gt; li &gt; a\', (links, _category) =&gt; {</k>\n      // Search for the element that has the matching text\n      <k>links = links.map(a =&gt; a.textContent.replace(/(\r\n\t|\n|\r|\t|^\s|\s$|\B\s|\s\B)/gm, "") === _category ? a : null);</k>\n      <k>let link = links.filter(tx =&gt; tx !== null)[0];</k>\n      <k>return link.href;</k>\n    <k>}, category);</k>\n    // Navigate to the selected category\n    <k>await page.goto(selectedCategory);</k>\n        let scrapedData = [];\n        // Wait for the required DOM to be rendered\n        async function scrapeCurrentPage(){\n            await page.waitForSelector(\'.page_inner\');\n            // Get the link to all the required books\n            let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n                // Make sure the book to be scraped is in stock\n                links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n                // Extract the links from the data\n                links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n                return links;\n            });\n            // Loop through each of those links, open a new page instance and get the relevant data from them\n            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {\n                let dataObj = {};\n                let newPage = await browser.newPage();\n                await newPage.goto(link);\n                dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);\n                dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);\n                dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {\n                    // Strip new line and tab spaces\n                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");\n                    // Get the number of stock available\n                    let regexp = /^.*\((.*)\).*$/i;\n                    let stockAvailable = regexp.exec(text)[1].split(\' \')[0];\n                    return stockAvailable;\n                });\n                dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);\n                dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);\n                dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);\n                resolve(dataObj);\n                await newPage.close();\n            });\n            for(link in urls){\n                let currentPageData = await pagePromise(urls[link]);\n                scrapedData.push(currentPageData);\n                // console.log(currentPageData);\n            }\n            // When all the data on this page is done, click the next button and start the scraping of the next page\n            // You are going to check if this button exist first, so you know if there really is a next page.\n            let nextButtonExist = false;\n            try{\n                const nextButton = await page.$eval(\'.next &gt; a\', a =&gt; a.textContent);\n                nextButtonExist = true;\n            }\n            catch(err){\n                nextButtonExist = false;\n            }\n            if(nextButtonExist){\n                await page.click(\'.next &gt; a\');   \n                return scrapeCurrentPage(); // Call this function recursively\n            }\n            await page.close();\n            return scrapedData;\n        }\n        let data = await scrapeCurrentPage();\n        console.log(data);\n        return data;\n    }\n}\nmodule.exports = scraperObject;</k>\nThis code block uses the category that you passed in to get the URL where the books of that category reside.\nThe <k>page.$$eval()</k> can take in arguments by passing the argument as a third parameter to the <k>$$eval()</k> method, and defining it as the third parameter in the callback as such:\nexample page.$$eval() function\n<k>page.$$eval(\'selector\', function(elem, args){\n  // .......\n}, args)</k>\nThis was what you did in your code; you passed the category of books you wanted to scrape, mapped through all the categories to check which one matches, and then returned the URL of this category.\nThis URL is then used to navigate to the page that displays the category of books you want to scrape using the <k>page.goto(selectedCategory)</k> method.\nSave and close the file.\nRun your application again. You will notice that it navigates to the <k>Travel</k> category, recursively opens books in that category page by page, and logs the results:\n<k>npm run start</k>\nIn this step, you scraped data across multiple pages and then scraped data across multiple pages from one particular category. In the final step, you will modify your script to scrape data across multiple categories and then save this scraped data to a stringified JSON file.\n\n<h3 id="scrapingapplicationtopic-5">Step 6 — Scraping Data from Multiple Categories and Saving the Data as JSON</h3>In this final step, you will make your script scrape data off of as many categories as you want and then change the manner of your output. Rather than logging the results, you will save them in a structured file called <k>data.json</k>.\nYou can quickly add more categories to scrape; doing so requires only one additional line per genre.\nOpen <k>pageController.js</k>:\n<k>nano pageController.js</k>\nAdjust your code to include additional categories. The example below adds <k>HistoricalFiction</k> and <k>Mystery</k> to our existing <k>Travel</k> category:\n./book-scraper/pageController.js\n<k>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n    let browser;\n    try{\n    browser = await browserInstance;\n    let scrapedData = {};\n    // Call the scraper for different set of books to be scraped\n    <k>scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');</k>\n    <k>scrapedData[\'HistoricalFiction\'] = await pageScraper.scraper(browser, \'Historical Fiction\');</k>\n    <k>scrapedData[\'Mystery\'] = await pageScraper.scraper(browser, \'Mystery\');</k>\n    await browser.close();\n    console.log(scrapedData)\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</k>\nSave and close the file.\nRun the script again and watch it scrape data for all three categories:\n<k>npm run start</k>\nWith the scraper fully-functional, your final step involves saving your data in a more useful format. You will now store it in a JSON file using <a href="https://nodejs.org/api/fs.html">the <k>fs</k> module in Node.js</a>.\nFirst, reopen <k>pageController.js</k>:\n<k>nano pageController.js</k>\nAdd the following highlighted code:\n./book-scraper/pageController.js\n<k>const pageScraper = require(\'./pageScraper\');\n<k>const fs = require(\'fs\');</k>\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    let scrapedData = {};\n    // Call the scraper for different set of books to be scraped\n    scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');\n    scrapedData[\'HistoricalFiction\'] = await pageScraper.scraper(browser, \'Historical Fiction\');\n    scrapedData[\'Mystery\'] = await pageScraper.scraper(browser, \'Mystery\');\n    await browser.close();\n    <k>fs.writeFile("data.json", JSON.stringify(scrapedData), \'utf8\', function(err) {</k>\n        <k>if(err) {</k>\n            <k>return console.log(err);</k>\n        }\n        <k>console.log("The data has been scraped and saved successfully! View it at \'./data.json\'");</k>\n    <k>});</k>\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</k>\nFirst, you are requiring Node,js’s <k>fs</k> module in <k>pageController.js</k>. This ensures that you can save your data as a JSON file. Then you are adding code so that when the scraping completes and the browser closes, the program will create a new file called <k>data.json</k>. Note that the contents of <k>data.json</k> are <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify">stringified JSON</a>. Therefore, when reading the content of <k>data.json</k>, always <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON">parse it as JSON</a> before reusing the data.\nSave and close the file.\nYou have now built a web-scraping application that scrapes books across multiple categories and then stores your scraped data in a JSON file. As your application grows in complexity, you might want to store this scraped data in a database or serve it over an API. How this data is consumed is really up to you.\n',
'<h2>to build a Local Proxy Server</h2>\n<div id="LocalProxyServertoc" class="toc"><a href="#LocalProxyServertopic-0" target="_self">Using Node.js</a><br><a href="#LocalProxyServertopic-1" target="_self"> **Initialize a Node.js Project**:</a><br><a href="#LocalProxyServertopic-2" target="_self"> **Install Required Packages**:</a><br><a href="#LocalProxyServertopic-3" target="_self"> **Create the Proxy Server Script**:</a><br><a href="#LocalProxyServertopic-4" target="_self"> **Run the Server**:</a><br><a href="#LocalProxyServertopic-5" target="_self">Testing the Proxy Server</a><br><a href="#LocalProxyServertopic-6" target="_self">Customizing the Proxy Server</a><br><a href="#LocalProxyServertopic-7" target="_self">Why Use a Proxy Server?</a><br></div></center><br><br>\n\nBuilding a local proxy server is a great way to serve external resources (like images) as if they are hosted locally.\nBelow is a step-by-step guide to creating a simple local proxy server using Node.js.\n\n<o id="LocalProxyServertopic-0">Using Node.js</o>\n#### Steps:\n<y id="LocalProxyServertopic-1"> **Initialize a Node.js Project**:</y>\n   - Open a terminal in the project folder and run:\n     npm init -y\n\n<y id="LocalProxyServertopic-2"> **Install Required Packages**:</y>\n   - Install the `http-proxy` package to handle proxy functionality:\n     npm install http-proxy\n\n<y id="LocalProxyServertopic-3"> **Create the Proxy Server Script**:</y>\n   - Create a file named `server.js` and add the following code:\n     const http = require(\'http\');\n     const httpProxy = require(\'http-proxy\');\n\n     // Create a proxy server\n     const proxy = httpProxy.createProxyServer({});\n\n     // Create the main server\n     const server = http.createServer((req, res) =&gt; {\n         // Proxy all requests to an external URL\n         proxy.web(req, res, { target: \'https://example.com\' });\n     });\n\n     // Start the server\n     const PORT = 8000;\n     server.listen(PORT, () =&gt; {\n         console.log(`Proxy server running on http://localhost:${PORT}`);\n     });\n\n<y id="LocalProxyServertopic-4"> **Run the Server**:</y>\n   - Start the server by running:\n     node server.js\n   - The server will now proxy requests to `https://example.com`.\nFor example, if you visit `http://localhost:8000/images/your-image.jpg`, it will fetch the image from `https://example.com/images/your-image.jpg`.\n\n<o id="LocalProxyServertopic-5">Testing the Proxy Server</o>\nOpen your browser and navigate to `http://localhost:8000`.\n\nAppend the path to the external resource you want to fetch.\nFor example:\n   - `http://localhost:8000/images/your-image.jpg` will fetch `https://example.com/images/your-image.jpg`.\n\n<o id="LocalProxyServertopic-6">Customizing the Proxy Server</o>\n- **Change the Target URL**: Replace `https://example.com` with the external URL you want to proxy.\n- **Add Caching**: To improve performance, you can cache responses locally on your hard disk.\n- **Handle Errors**: Add error handling to manage cases where the external resource is unavailable.\n\n<o id="LocalProxyServertopic-7">Why Use a Proxy Server?</o>\n- **Local Testing**: Simulate external resources as if they are hosted locally.\n- **Security**: Avoid exposing direct external URLs in your code.\n- **Control**: Modify or filter requests/responses as needed.\n',
'<h2>finance data from yahoo</h2>\n//1wk 5m 1d\nhttps://query2.finance.yahoo.com/v8/finance/chart/TIGR?period1=1747957800&period2=1748649600&interval=5m',
'<h2>Infinite Scroll</h2>\n<center><div id="InfiniteScrolltoc" class="toc"><a href="#InfiniteScrolltopic-0" target="_self">Basic JavaScript Implementation</a><br><a href="#InfiniteScrolltopic-1" target="_self">React Implementation</a><br><a href="#InfiniteScrolltopic-2" target="_self">Vue Implementation</a><br><a href="#InfiniteScrolltopic-3" target="_self">Key Considerations</a><br><a href="#InfiniteScrolltopic-4" target="_self">Modern Approach: Intersection Observer API</a><br><a href="#InfiniteScrolltopic-5" target="_self"><pk>how to find out the url of the fetch action</pk></a><br><a href="#InfiniteScrolltopic-6" target="_self">Method 1: Using the Network Tab</a><br><a href="#InfiniteScrolltopic-7" target="_self">Method 2: Using the Console</a><br><a href="#InfiniteScrolltopic-8" target="_self">Method 3: Using Breakpoints</a><br><a href="#InfiniteScrolltopic-9" target="_self">Method 4: Using Performance Monitoring</a><br><a href="#InfiniteScrolltopic-10" target="_self">Tips for Finding Specific Fetch Requests</a><br><a href="#InfiniteScrolltopic-11" target="_self"><pk>With XMLHttpRequest or WebSockets</pk></a><br><a href="#InfiniteScrolltopic-12" target="_self">Monitoring XMLHttpRequest (XHR)</a><br><a href="#InfiniteScrolltopic-13" target="_self">Method 1: Using Network Tab</a><br><a href="#InfiniteScrolltopic-14" target="_self">Method 2: Console Override (for debugging)</a><br><a href="#InfiniteScrolltopic-15" target="_self"><pk>Monitoring WebSocket Connections</pk></a><br><a href="#InfiniteScrolltopic-16" target="_self">Method 1: Using Network Tab</a><br><a href="#InfiniteScrolltopic-17" target="_self">Method 2: Console Override (for debugging)</a><br><a href="#InfiniteScrolltopic-18" target="_self">Advanced Techniques</a><br><a href="#InfiniteScrolltopic-19" target="_self">For Both XHR and WebSockets:</a><br><a href="#InfiniteScrolltopic-20" target="_self">For Single Page Applications (SPAs):</a><br><a href="#InfiniteScrolltopic-21" target="_self">For Encrypted WebSockets (WSS):</a><br></div></center><br><br>\n\n# Implementing Infinite Scroll\n\nInfinite scroll is a technique where content is continuously loaded as the user scrolls down the page, creating a seamless browsing experience. Here\'s how to implement it:\n<h3 id="InfiniteScrolltopic-0">Basic JavaScript Implementation</h3>\n\nwindow.addEventListener(\'scroll\', function() {\n // Check if user has scrolled near the bottom of the page\n if (window.innerHeight + window.scrollY &gt;= document.body.offsetHeight - 500) {\n  // Load more content\n  loadMoreContent();\n }\n});\n\nfunction loadMoreContent() {\n // Prevent multiple simultaneous loads\n if (isLoading) return;\n isLoading = true;\n \n // Show loading indicator\n document.getElementById(\'loader\').style.display = \'block\';\n \n // Fetch more content (AJAX call or similar)\n fetch(\'/api/more-content?page=\' + currentPage)\n  .then(response =&gt; response.json())\n  .then(data =&gt; {\n   // Append new content\n   const container = document.getElementById(\'content-container\');\n   data.items.forEach(item =&gt; {\n    container.appendChild(createContentElement(item));\n   });\n   \n   // Update page counter and hide loader\n   currentPage++;\n   isLoading = false;\n   document.getElementById(\'loader\').style.display = \'none\';\n  });\n}\n<h3 id="InfiniteScrolltopic-1">React Implementation</h3>\njsx\nimport { useState, useEffect, useRef } from \'react\';\n\nfunction InfiniteScrollList() {\n const [items, setItems] = useState([]);\n const [page, setPage] = useState(1);\n const [loading, setLoading] = useState(false);\n const loaderRef = useRef(null);\n\n useEffect(() =&gt; {\n  const observer = new IntersectionObserver(\n   (entries) =&gt; {\n    if (entries[0].isIntersecting &amp;&amp; !loading) {\n     loadMoreItems();\n    }\n   },\n   { threshold: 1.0 }\n  );\n\n  if (loaderRef.current) {\n   observer.observe(loaderRef.current);\n  }\n\n  return () =&gt; {\n   if (loaderRef.current) {\n    observer.unobserve(loaderRef.current);\n   }\n  };\n }, [loading]);\n\n const loadMoreItems = async () =&gt; {\n  setLoading(true);\n  const newItems = await fetchItems(page);\n  setItems([...items, ...newItems]);\n  setPage(page + 1);\n  setLoading(false);\n };\n\n return (\n  &lt;div&gt;\n   {items.map((item) =&gt; (\n    &lt;div key={item.id}&gt;{item.content}&lt;/div&gt;\n   ))}\n   &lt;div ref={loaderRef} style={{ height: \'20px\' }}&gt;\n    {loading &amp;&amp; &lt;p&gt;Loading more items...&lt;/p&gt;}\n   &lt;/div&gt;\n  &lt;/div&gt;\n );\n}\n<h3 id="InfiniteScrolltopic-2">Vue Implementation</h3>\nvue\n&lt;template&gt;\n &lt;div&gt;\n  &lt;div v-for="item in items" :key="item.id"&gt;\n   {{ item.content }}\n  &lt;/div&gt;\n  &lt;div ref="loader" v-if="loading"&gt;\n   Loading more items...\n  &lt;/div&gt;\n &lt;/div&gt;\n&lt;/template&gt;\n\n&lt;script&gt;\nexport default {\n data() {\n  return {\n   items: [],\n   page: 1,\n   loading: false,\n   observer: null\n  };\n },\n mounted() {\n  this.observer = new IntersectionObserver((entries) =&gt; {\n   if (entries[0].isIntersecting &amp;&amp; !this.loading) {\n    this.loadMoreItems();\n   }\n  });\n  \n  this.observer.observe(this.$refs.loader);\n },\n beforeDestroy() {\n  this.observer.disconnect();\n },\n methods: {\n  async loadMoreItems() {\n   this.loading = true;\n   const newItems = await fetchItems(this.page);\n   this.items = [...this.items, ...newItems];\n   this.page++;\n   this.loading = false;\n  }\n }\n};\n&lt;/script&gt;\n<h3 id="InfiniteScrolltopic-3">Key Considerations</h3>\n\n1. **Performance**: Debounce scroll events to avoid excessive function calls\n2. **Loading State**: Show loading indicators and prevent duplicate requests\n3. **Error Handling**: Implement proper error handling for failed requests\n4. **Mobile Optimization**: Test on mobile devices where scroll behavior differs\n5. **Accessibility**: Ensure keyboard navigation still works with infinite scroll\n6. **SEO**: Consider how infinite scroll affects search engine indexing\n<h3 id="InfiniteScrolltopic-4">Modern Approach: Intersection Observer API</h3>\n\nThe newer Intersection Observer API is more efficient than listening to scroll events:\n\nconst observer = new IntersectionObserver((entries) =&gt; {\n if (entries[0].isIntersecting) {\n  loadMoreContent();\n }\n});\n\nobserver.observe(document.querySelector(\'.footer\'));\n\nThis approach is more performant as it doesn\'t continuously check scroll position.\n<h3 id="InfiniteScrolltopic-5"><pk>how to find out the url of the fetch action</pk></h3>\n# How to Find the URL of Fetch Requests in Chrome\n\nThere are several ways to inspect fetch request URLs in Chrome\'s Developer Tools:\n<h3 id="InfiniteScrolltopic-6">Method 1: Using the Network Tab</h3>\n\n1. Open Chrome Developer Tools:\n- Right-click on the page and select "Inspect"\n- Or press `Ctrl+Shift+I` (Windows/Linux) or `Cmd+Opt+I` (Mac)\n\n2. Go to the "Network" tab\n\n3. Filter for fetch requests:\n- Click the "XHR" or "Fetch/XHR" filter button\n- Or type "fetch" in the filter box\n\n4. Perform the action that triggers the fetch request\n\n5. Look for the request in the list - the URL will be shown in:\n- The "Name" column (main part of the URL)\n- The "Headers" tab (full URL)\n<h3 id="InfiniteScrolltopic-7">Method 2: Using the Console</h3>\n\n1. Open Developer Tools and go to the "Console" tab\n\n2. Monitor all fetch requests by pasting this code:\n// Monitor all fetch requests\nconst originalFetch = window.fetch;\nwindow.fetch = function(...args) {\n console.log(\'Fetch request to:\', args[0]);\n return originalFetch.apply(this, args);\n};\n\n3. Now all fetch URLs will be logged to the console when they occur\n<h3 id="InfiniteScrolltopic-8">Method 3: Using Breakpoints</h3>\n\n1. Go to the "Sources" tab in Developer Tools\n\n2. Press `Ctrl+Shift+F` (Windows/Linux) or `Cmd+Opt+F` (Mac) to search all files\n\n3. Search for "fetch(" to find fetch calls in the JavaScript code\n\n4. Set a breakpoint on the fetch call by clicking the line number\n\n5. When the breakpoint hits, you can inspect the URL parameter\n<h3 id="InfiniteScrolltopic-9">Method 4: Using Performance Monitoring</h3>\n\n1. Go to the "Performance" tab in Developer Tools\n\n2. Start recording\n\n3. Perform the action that triggers the fetch\n\n4. Stop recording and look for network requests in the timeline\n<h3 id="InfiniteScrolltopic-10">Tips for Finding Specific Fetch Requests</h3>\n\n- Clear the network log before triggering the action (click the 🚫 icon)\n- Look for the "initiator" column to see what code triggered the request\n- Check the "Preview" or "Response" tabs to see what data was returned\n- For API requests, look for common patterns like `/api/` in the URL\n\nRemember that some sites may use other methods like XMLHttpRequest or WebSockets instead of fetch, so you might need to check those as well.\n\n<h3 id="InfiniteScrolltopic-11"><pk>With XMLHttpRequest or WebSockets</pk></h3>\n# Monitoring XMLHttpRequest and WebSocket Connections in Chrome\n\nFor websites that use XMLHttpRequest (XHR) or WebSockets instead of the Fetch API, here\'s how to inspect their URLs and activity:\n<h3 id="InfiniteScrolltopic-12">Monitoring XMLHttpRequest (XHR)</h3>\n<h3 id="InfiniteScrolltopic-13">Method 1: Using Network Tab</h3>\n1. Open Chrome DevTools (`F12` or `Ctrl+Shift+I`/`Cmd+Opt+I`)\n2. Go to the **Network** tab\n3. Filter by **XHR** (click the XHR button or type "xhr" in the filter)\n4. Perform the action that triggers the request\n5. Click on any XHR request to see:\n- Full URL in the **Headers** tab under "Request URL"\n- Request and response data\n- Initiator (what code triggered it)\n<h3 id="InfiniteScrolltopic-14">Method 2: Console Override (for debugging)</h3>\n// Override XMLHttpRequest to log all requests\nconst originalXHR = window.XMLHttpRequest;\nwindow.XMLHttpRequest = function() {\n const xhr = new originalXHR();\n const originalOpen = xhr.open;\n \n xhr.open = function(method, url) {\n  console.log(\'XHR to:\', url, \'Method:\', method);\n  originalOpen.apply(this, arguments);\n };\n \n return xhr;\n};\n<h3 id="InfiniteScrolltopic-15"><pk>Monitoring WebSocket Connections</pk></h3>\n<h3 id="InfiniteScrolltopic-16">Method 1: Using Network Tab</h3>\n1. Open Chrome DevTools\n2. Go to the **Network** tab\n3. Filter by **WS** (WebSocket) or type "ws" in the filter\n4. Look for entries with type "websocket"\n5. Click on a WebSocket connection to see:\n- The initial connection URL\n- Frames exchanged (in the **Messages** tab)\n- Timing information\n<h3 id="InfiniteScrolltopic-17">Method 2: Console Override (for debugging)</h3>\n// Override WebSocket to monitor connections\nconst originalWebSocket = window.WebSocket;\nwindow.WebSocket = function(url, protocols) {\n console.log(\'WebSocket connecting to:\', url);\n \n const ws = new originalWebSocket(url, protocols);\n \n ws.addEventListener(\'message\', function(event) {\n  console.log(\'WebSocket message received:\', event.data);\n });\n \n ws.addEventListener(\'close\', function(event) {\n  console.log(\'WebSocket closed:\', event.code, event.reason);\n });\n \n return ws;\n};\n<h3 id="InfiniteScrolltopic-18">Advanced Techniques</h3>\n<h3 id="InfiniteScrolltopic-19">For Both XHR and WebSockets:</h3>\n1. **Set breakpoints** in the Sources tab:\n- Search for `new XMLHttpRequest()` or `new WebSocket()`\n- Add breakpoints to examine the URL and parameters\n\n2. **Performance monitoring**:\n- Record performance in the Performance tab\n- Look for network activity in the timeline\n\n3. **Event listeners**:\n- In the Elements panel, check for event listeners that might trigger these requests\n<h3 id="InfiniteScrolltopic-20">For Single Page Applications (SPAs):</h3>\n- Use Chrome\'s **Performance monitor** (in More Tools) to track network activity\n- Check the **Application** tab for WebSocket connections under "WebSockets"\n<h3 id="InfiniteScrolltopic-21">For Encrypted WebSockets (WSS):</h3>\n- The URL will be visible in the Network tab\n- Message contents can be viewed unless they\'re encrypted at the application level\n\nRemember that some sites may use libraries like Axios or Socket.IO that wrap these native APIs, but the underlying connections will still appear in the Network tab as XHR or WebSocket traffic.\n',
'<h2>javascript to select substring start with "<img" and then until the last "/"</h2>\n/&lt;img[^>]*\//',
'<h2>javascript convert 1748649600 to date and time</h2>\n// Unix timestamp in seconds: 1748649600\nconst unixTimestamp = 1748649600;\n\n// Create a new Date object convert to milliseconds\nconst date = new Date(unixTimestamp * 1000);\n\n// Extract the date and time components\nconst year = date.getFullYear();\nconst month = (date.getMonth() + 1).toString().padStart(2, \'0\'); // Month is zero-based\nconst day = date.getDate().toString().padStart(2, \'0\');\nconst hours = date.getHours().toString().padStart(2, \'0\');\nconst minutes = date.getMinutes().toString().padStart(2, \'0\');\nconst seconds = date.getSeconds().toString().padStart(2, \'0\');\n\n// Format the date and time\nconst formattedDateTime = `${year}-${month}-${day} ${hours}:${minutes}:${seconds}`;\n',
'<h2>Axios Application Samples</h2>\n<center><div id="AxiosApplicationtoc" class="toc"><a href="#AxiosApplicationtopic-0" target="_self">1. Basic GET Request</a><br><a href="#AxiosApplicationtopic-1" target="_self">2. GET Request with Parameters</a><br><a href="#AxiosApplicationtopic-2" target="_self">3. POST Request with Data</a><br><a href="#AxiosApplicationtopic-3" target="_self">4. PUT Request (Update)</a><br><a href="#AxiosApplicationtopic-4" target="_self">5. DELETE Request</a><br><a href="#AxiosApplicationtopic-5" target="_self">6. Multiple Concurrent Requests</a><br><a href="#AxiosApplicationtopic-6" target="_self">7. Using Async/Await Syntax</a><br><a href="#AxiosApplicationtopic-7" target="_self">8. Custom Axios Instance with Defaults</a><br><a href="#AxiosApplicationtopic-8" target="_self">9. Handling Errors with Response Interceptors</a><br><a href="#AxiosApplicationtopic-9" target="_self">10. File Upload with Progress Tracking</a><br></div></center><br><br>\n\nAxios is promise-based and works well with both async/await and traditional promise syntax.\n\nHere are several practical examples of using Axios in different scenarios:\n<h3 id="AxiosApplicationtopic-0">1. Basic GET Request</h3>\nconst axios = require(\'axios\');\n\n// Make a GET request\naxios.get(\'https://jsonplaceholder.typicode.com/posts\')\n .then(response =&gt; {\n  console.log(\'Response data:\', response.data);\n })\n .catch(error =&gt; {\n  console.error(\'Error:\', error);\n });\n<h3 id="AxiosApplicationtopic-1">2. GET Request with Parameters</h3>\naxios.get(\'https://jsonplaceholder.typicode.com/comments\', {\n params: {\n  postId: 1\n }\n})\n.then(response =&gt; {\n console.log(\'Comments for post 1:\', response.data);\n})\n.catch(error =&gt; {\n console.error(\'Error:\', error);\n});\n<h3 id="AxiosApplicationtopic-2">3. POST Request with Data</h3>\nconst newPost = {\n title: \'foo\',\n body: \'bar\',\n userId: 1\n};\n\naxios.post(\'https://jsonplaceholder.typicode.com/posts\', newPost)\n .then(response =&gt; {\n  console.log(\'Created post:\', response.data);\n })\n .catch(error =&gt; {\n  console.error(\'Error:\', error);\n });\n<h3 id="AxiosApplicationtopic-3">4. PUT Request (Update)</h3>\nconst updatedPost = {\n id: 1,\n title: \'updated title\',\n body: \'updated body\',\n userId: 1\n};\n\naxios.put(\'https://jsonplaceholder.typicode.com/posts/1\', updatedPost)\n .then(response =&gt; {\n  console.log(\'Updated post:\', response.data);\n })\n .catch(error =&gt; {\n  console.error(\'Error:\', error);\n });\n<h3 id="AxiosApplicationtopic-4">5. DELETE Request</h3>\naxios.delete(\'https://jsonplaceholder.typicode.com/posts/1\')\n .then(response =&gt; {\n  console.log(\'Post deleted\');\n })\n .catch(error =&gt; {\n  console.error(\'Error:\', error);\n });\n<h3 id="AxiosApplicationtopic-5">6. Multiple Concurrent Requests</h3>\naxios.all([\n axios.get(\'https://jsonplaceholder.typicode.com/posts/1\'),\n axios.get(\'https://jsonplaceholder.typicode.com/comments?postId=1\')\n])\n.then(axios.spread((postResponse, commentsResponse) =&gt; {\n console.log(\'Post:\', postResponse.data);\n console.log(\'Comments:\', commentsResponse.data);\n}))\n.catch(error =&gt; {\n console.error(\'Error:\', error);\n});\n<h3 id="AxiosApplicationtopic-6">7. Using Async/Await Syntax</h3>\nasync function fetchData() {\n try {\n  const response = await axios.get(\'https://jsonplaceholder.typicode.com/users/1\');\n  console.log(\'User data:\', response.data);\n } catch (error) {\n  console.error(\'Error:\', error);\n }\n}\n\nfetchData();\n<h3 id="AxiosApplicationtopic-7">8. Custom Axios Instance with Defaults</h3>\nconst apiClient = axios.create({\n baseURL: \'https://jsonplaceholder.typicode.com\',\n timeout: 5000,\n headers: {\n  \'Content-Type\': \'application/json\',\n  \'Authorization\': \'Bearer your-token-here\'\n }\n});\n\n// Now you can use apiClient for all requests\napiClient.get(\'/posts\')\n .then(response =&gt; {\n  console.log(response.data);\n });\n<h3 id="AxiosApplicationtopic-8">9. Handling Errors with Response Interceptors</h3>\naxios.interceptors.response.use(\n response =&gt; response,\n error =&gt; {\n  if (error.response) {\n   // Server responded with a status other than 2xx\n   console.error(\'Error response:\', error.response.status);\n  } else if (error.request) {\n   // Request was made but no response received\n   console.error(\'No response received:\', error.request);\n  } else {\n   // Something happened in setting up the request\n   console.error(\'Request setup error:\', error.message);\n  }\n  return Promise.reject(error);\n }\n);\n<h3 id="AxiosApplicationtopic-9">10. File Upload with Progress Tracking</h3>\nconst formData = new FormData();\nformData.append(\'file\', fileInput.files[0]);\n\naxios.post(\'https://example.com/upload\', formData, {\n headers: {\n  \'Content-Type\': \'multipart/form-data\'\n },\n onUploadProgress: progressEvent =&gt; {\n  const percentCompleted = Math.round(\n   (progressEvent.loaded * 100) / progressEvent.total\n  );\n  console.log(`${percentCompleted}% uploaded`);\n }\n})\n.then(response =&gt; {\n console.log(\'Upload complete:\', response.data);\n});\n',
'<h2>Cheerio Application Samples</h2>\n<center><div id="CheerioSamplestoc" class="toc"><a href="#CheerioSamplestopic-0" target="_self">1. Basic HTML Parsing</a><br><a href="#CheerioSamplestopic-1" target="_self">2. Web Scraping Example</a><br><a href="#CheerioSamplestopic-2" target="_self">3. Modifying HTML</a><br><a href="#CheerioSamplestopic-3" target="_self">4. Extracting Attributes</a><br><a href="#CheerioSamplestopic-4" target="_self">5. Filtering and Traversing</a><br><a href="#CheerioSamplestopic-5" target="_self">6. Form Data Extraction</a><br><a href="#CheerioSamplestopic-6" target="_self">7. Table Data Extraction</a><br></div></center><br><br>\nCheerio is a fast, flexible, and lean implementation of core jQuery designed specifically for server-side DOM manipulation with Node.js. Here are several practical examples of how to use Cheerio:\n<h3 id="CheerioSamplestopic-0">1. Basic HTML Parsing</h3>\nconst cheerio = require(\'cheerio\');\nconst html = `\n &lt;ul id="fruits">\n  &lt;li class="apple">Apple&lt;/li>\n  &lt;li class="orange">Orange&lt;/li>\n  &lt;li class="pear">Pear&lt;/li>\n &lt;/ul>\n`;\n\nconst $ = cheerio.load(html);\n\nconsole.log($(\'#fruits\').length); // 1\nconsole.log($(\'.apple\').text()); // "Apple"\n<h3 id="CheerioSamplestopic-1">2. Web Scraping Example</h3>\nconst cheerio = require(\'cheerio\');\nconst axios = require(\'axios\');\n\nasync function scrapeWebsite() {\n try {\n  const { data } = await axios.get(\'https://example.com/news\');\n  const $ = cheerio.load(data);\n  \n  const headlines = [];\n  $(\'h2.news-title\').each((i, element) =&gt; {\n   headlines.push({\n    title: $(element).text().trim(),\n    link: $(element).find(\'a\').attr(\'href\')\n   });\n  });\n  \n  console.log(headlines);\n } catch (error) {\n  console.error(\'Error scraping website:\', error);\n }\n}\n\nscrapeWebsite();\n<h3 id="CheerioSamplestopic-2">3. Modifying HTML</h3>\nconst cheerio = require(\'cheerio\');\nconst html = \'&lt;div class="container">&lt;/div>\';\n\nconst $ = cheerio.load(html);\n$(\'.container\')\n .append(\'&lt;p>Hello World!&lt;/p>\')\n .addClass(\'main-content\');\n\nconsole.log($.html());\n// Outputs: &lt;div class="container main-content">&lt;p>Hello World!&lt;/p>&lt;/div>\n<h3 id="CheerioSamplestopic-3">4. Extracting Attributes</h3>\nconst cheerio = require(\'cheerio\');\nconst html = `\n &lt;a href="https://example.com" title="Example Site">Link&lt;/a>\n &lt;img src="image.jpg" alt="Sample Image">\n`;\n\nconst $ = cheerio.load(html);\n\nconsole.log($(\'a\').attr(\'href\')); // "https://example.com"\nconsole.log($(\'img\').attr(\'src\')); // "image.jpg"\n<h3 id="CheerioSamplestopic-4">5. Filtering and Traversing</h3>\nconst cheerio = require(\'cheerio\');\nconst html = `\n &lt;div class="products">\n  &lt;div class="product" data-price="10">Product A&lt;/div>\n  &lt;div class="product" data-price="20">Product B&lt;/div>\n  &lt;div class="product" data-price="30">Product C&lt;/div>\n &lt;/div>\n`;\n\nconst $ = cheerio.load(html);\n\n// Get products with price &gt; 15\nconst expensiveProducts = $(\'.product\').filter((i, el) =&gt; {\n return parseInt($(el).attr(\'data-price\')) &gt; 15;\n});\n\nexpensiveProducts.each((i, el) =&gt; {\n console.log($(el).text());\n});\n// Outputs: Product B, Product C\n<h3 id="CheerioSamplestopic-5">6. Form Data Extraction</h3>\nconst cheerio = require(\'cheerio\');\nconst html = `\n &lt;form id="login-form">\n  &lt;input type="text" name="username" value="admin">\n  &lt;input type="password" name="password" value="secret">\n  &lt;input type="hidden" name="csrf" value="abc123">\n  &lt;button type="submit">Login&lt;/button>\n &lt;/form>\n`;\n\nconst $ = cheerio.load(html);\n\nconst formData = {};\n$(\'#login-form input\').each((i, el) =&gt; {\n const name = $(el).attr(\'name\');\n const value = $(el).attr(\'value\');\n if (name) formData[name] = value;\n});\n\nconsole.log(formData);\n// Outputs: { username: \'admin\', password: \'secret\', csrf: \'abc123\' }\n<h3 id="CheerioSamplestopic-6">7. Table Data Extraction</h3>\nconst cheerio = require(\'cheerio\');\nconst html = `\n &lt;table>\n  &lt;thead>\n   &lt;tr>&lt;th>Name&lt;/th>&lt;th>Age&lt;/th>&lt;th>Job&lt;/th>&lt;/tr>\n  &lt;/thead>\n  &lt;tbody>\n   &lt;tr>&lt;td>John&lt;/td>&lt;td>25&lt;/td>&lt;td>Developer&lt;/td>&lt;/tr>\n   &lt;tr>&lt;td>Jane&lt;/td>&lt;td>30&lt;/td>&lt;td>Designer&lt;/td>&lt;/tr>\n  &lt;/tbody>\n &lt;/table>\n`;\n\nconst $ = cheerio.load(html);\nconst tableData = [];\n\n$(\'table tbody tr\').each((i, row) =&gt; {\n const rowData = {};\n $(row).find(\'td\').each((j, cell) =&gt; {\n  const header = $(\'table thead th\').eq(j).text();\n  rowData[header] = $(cell).text();\n });\n tableData.push(rowData);\n});\n\nconsole.log(tableData);\n/*\nOutputs: \n[\n { Name: \'John\', Age: \'25\', Job: \'Developer\' },\n { Name: \'Jane\', Age: \'30\', Job: \'Designer\' }\n]\n*/\n',
'<h2>alternative scraping dynamic content packages</h2>\nWhen web scraping dynamic content (such as content on Pinterest that may load after initial page rendering), in addition to <lg>RSelenium</lg>, several other R packages can be used.\nHere\'s an overview of alternative packages and their characteristics, along with applicable scenarios and usage examples:\n<h3>1. rvest + phantomjs (or other headless browsers)</h3>\n<lg>Characteristics</lg>\n- <lg>rvest</lg> is a lightweight web scraping package, but it primarily parses static HTML.\n- Combined with <lg>phantomjs</lg> (a headless browser), it can render dynamic pages before scraping.\n\n<lg>Applicable scenarios</lg>\n- Suitable for websites where content is loaded via simple JavaScript (e.g., initial page rendering with subsequent content loading).\n- Note: Phantomjs is no longer actively maintained; consider alternatives like <lg>headless Chrome</lg> with other tools.\n\n<lg>Example code</lg>\n# Install and load packages\n# install.packages("rvest")\n# install.packages("xml2")\nlibrary(rvest)\nlibrary(xml2)\n\n# Use phantomjs to render the page (needs to install phantomjs executable first)\nrender_page <- function(url) {\n # Execute phantomjs script to render the page and save HTML\n phantom_script <- \'\n  var page = require("webpage").create();\n  page.open("{{URL}}", function(status) {\n   if (status === "success") {\n    page.render("temp_rendered.png");\n    console.log(page.content);\n   }\n   phantom.exit();\n  });\n \'\n\n # Replace URL and save the script\n script_path <- "render_page.js"\n writeLines(gsub("{{URL}}", url, phantom_script), script_path)\n\n # Execute phantomjs via system command\n system(paste0("phantomjs ", script_path))\n\n # Read the rendered HTML\n rendered_html <- readLines("temp_rendered.html")\n return(rendered_html)\n}\n\n# Read URLs from file\nurls <- readLines("inputurls.txt")\n\n# Create a list to store results\nall_results <- list()\n\n# Loop through each URL\nfor (url in urls) {\n try {\n  cat("Processing:", url, "\n")\n  # Render the page with phantomjs\n  html_content <- render_page(url)\n \n  # Parse HTML with rvest\n  doc <- read_html(html_content)\n \n  # Extract img src and srcset\n  img_src <- doc %>% html_elements("img") %>% html_attr("src")\n  img_srcset <- doc %>% html_elements("img") %>% html_attr("srcset")\n \n  # Extract iframe sources\n  iframe_src <- doc %>% html_elements("iframe") %>% html_attr("src")\n \n  # Store results\n  results <- list(\n   url = url,\n   img_src = img_src,\n   img_srcset = img_srcset,\n   iframe_src = iframe_src\n  )\n  all_results <- c(all_results, list(results))\n \n  cat("Completed:", url, "\n\n")\n } catch (e) {\n  cat("Error processing", url, ":", e$message, "\n\n")\n }\n}\n\n# Save all results to a file\nsaveRDS(all_results, "pinterest_resources.rds")\n# Or convert to CSV (for simpler viewing)\nlibrary(tidyr)\nlibrary(dplyr)\nflatten_results <- lapply(all_results, function(x) {\n data.frame(\n  url = x$url,\n  img_src = paste(x$img_src, collapse = "|"),\n  img_srcset = paste(x$img_srcset, collapse = "|"),\n  iframe_src = paste(x$iframe_src, collapse = "|"),\n  stringsAsFactors = FALSE\n )\n})\nresult_df <- bind_rows(flatten_results)\nwrite.csv(result_df, "pinterest_resources.csv", row.names = FALSE)\n<h3>2. RSelenium alternative: remDrv (using Selenium with headless Chrome)</h3>\n<lg>Characteristics</lg>\n- <lg>remDrv</lg> (from the <lg>RSelenium</lg> package) can be configured to use headless Chrome, which is more modern and stable than Phantomjs.\n- It supports full JavaScript rendering and is suitable for complex dynamic pages.\n\n<lg>Applicable scenarios</lg>\n- Websites with heavy JavaScript usage (e.g., Pinterest\'s infinite scrolling, modal pop-ups, etc.).\n\n<lg>Example code</lg>\n# Install and load packages\n# install.packages("RSelenium")\n# install.packages("dplyr")\n# install.packages("stringr")\nlibrary(RSelenium)\nlibrary(dplyr)\nlibrary(stringr)\n\n# Start Selenium server (needs to install ChromeDriver matching your Chrome version)\nrD <- rsDriver(browser = "chrome", \n        chromever = "114", # Replace with your Chrome version\n        extraCapabilities = list("goog:chromeOptions" = list(\n         args = c("--headless", "--disable-gpu", "--window-size=1920,1080")\n        )))\nremDrv <- rD[["client"]]\n\n# Read URLs from file\nurls <- readLines("inputurls.txt")\n\n# Create a list to store results\nall_results <- list()\n\n# Loop through each URL\nfor (url in urls) {\n try {\n  cat("Processing:", url, "\n")\n  # Navigate to the URL\n  remDrv$navigate(url)\n \n  # Wait for page loading (adjust waiting time as needed)\n  Sys.sleep(5) # Simple waiting; better to use explicit waits\n \n  # Scroll to load more content (for infinite scrolling pages)\n  for (i in 1:10) { # Scroll 10 times\n   remDrv$executeScript("window.scrollTo(0, document.body.scrollHeight);")\n   Sys.sleep(2)\n  }\n \n  # Get page source\n  page_source <- remDrv$getPageSource()[[1]]\n  doc <- read_html(page_source)\n \n  # Extract img src and srcset\n  img_src <- doc %>% html_elements("img") %>% html_attr("src")\n  img_srcset <- doc %>% html_elements("img") %>% html_attr("srcset")\n \n  # Extract iframe sources\n  iframe_src <- doc %>% html_elements("iframe") %>% html_attr("src")\n \n  # Store results\n  results <- list(\n   url = url,\n   img_src = img_src,\n   img_srcset = img_srcset,\n   iframe_src = iframe_src\n  )\n  all_results <- c(all_results, list(results))\n \n  cat("Completed:", url, "\n\n")\n } catch (e) {\n  cat("Error processing", url, ":", e$message, "\n\n")\n }\n}\n\n# Close the browser\nremDrv$close()\nrD$server$stop()\n\n# Save results (same as previous example)\nsaveRDS(all_results, "pinterest_resources_remdrv.rds")\n<h3>3. playr: Lightweight browser automation</h3>\n<lg>Characteristics</lg>\n- <lg>playr</lg> is a lightweight package for browser automation, built on top of <lg>Phantomjs</lg> or <lg>Slimerjs</lg>.\n- It provides a simpler API than RSelenium, suitable for basic dynamic content scraping.\n\n<lg>Example code</lg>\n# install.packages("playr")\nlibrary(playr)\n\n# Start the browser\nbrowse()\n\n# Read URLs from file\nurls <- readLines("inputurls.txt")\nall_results <- list()\n\nfor (url in urls) {\n try {\n  cat("Processing:", url, "\n")\n  go(url)\n  wait(5) # Wait for page loading\n \n  # Scroll to load more content\n  for (i in 1:10) {\n   scroll_to("bottom")\n   wait(2)\n  }\n \n  # Get page HTML\n  html_content <- html()\n  doc <- read_html(html_content)\n \n  # Extract resources\n  img_src <- doc %>% html_elements("img") %>% html_attr("src")\n  img_srcset <- doc %>% html_elements("img") %>% html_attr("srcset")\n  iframe_src <- doc %>% html_elements("iframe") %>% html_attr("src")\n \n  results <- list(\n   url = url,\n   img_src = img_src,\n   img_srcset = img_srcset,\n   iframe_src = iframe_src\n  )\n  all_results <- c(all_results, list(results))\n \n  cat("Completed:", url, "\n\n")\n } catch (e) {\n  cat("Error:", e$message, "\n")\n }\n}\n\n# Close the browser\nclose()\n<h3>4. httr2: For APIs and AJAX requests (advanced approach)</h3>\n<lg>Characteristics</lg>\n- Instead of scraping the front-end, directly call the backend API (if discoverable).\n- Suitable for websites that load content via AJAX requests (e.g., Pinterest\'s API endpoints).\n\n<lg>Applicable scenarios</lg>\n- When you can analyze network requests to find API endpoints (requires browser developer tools).\n- More efficient than rendering the entire page, but requires understanding of the website\'s API structure.\n\n<lg>Example思路 (not complete code)</lg>\n1. Use browser developer tools to capture AJAX requests when browsing Pinterest.\n2. Identify API endpoints that return image/iframe data (e.g., `https://api.pinterest.com/...`).\n3. Use <lg>httr2</lg> to simulate these requests:\n# install.packages("httr2")\nlibrary(httr2)\n\n# Example: Send a request to a Pinterest API endpoint\napi_url <- "https://api.pinterest.com/v1/boards/.../pins/"\nreq <- request(api_url) %>% \n req_header("Authorization" = "Bearer YOUR_TOKEN") %>% # If authentication is required\n req_perform()\n\n# Parse response\nres <- req_body_json(req)\n# Extract img src from the response data\n<h3>5. Comparison of packages</h3>\nPackage : RSelenium (remDrv) \nAdvantages : Full browser functionality, supports headless Chrome, stable for complex pages. \nDisadvantages : Requires setting up Selenium server and ChromeDriver. \nSuitability : Complex dynamic pages (e.g., Pinterest). \n\nPackage : rvest + phantomjs \nAdvantages : Lightweight, simple setup. \nDisadvantages : Phantomjs is deprecated; limited support for modern websites. \nSuitability : Simple dynamic pages. \n\nPackage : playr \nAdvantages : Simple API, lightweight. \nDisadvantages : Limited functionality, relies on Phantomjs/Slimerjs. \nSuitability : Basic automation tasks. \n\nPackage : httr2 \nAdvantages : Direct API calls, high efficiency. \nDisadvantages : Requires analyzing API endpoints, may need authentication. \nSuitability : Websites with discoverable APIs. \n\n<h3>Notes for scraping Pinterest\n1. <lg>Anti-scraping measures</lg>: Pinterest may detect automated scraping and block IPs. Consider:\n  - Adding random delays between requests.\n  - Using a proxy IP pool.\n  - Limiting the scraping frequency.\n',
'<h2>using cheerio and puppeteer to get webpage and parse doms</h2>\n步骤：\n1. <lg>启动无头浏览器</lg>：使用 Puppeteer 加载目标网页\n2. <lg>获取页面内容</lg>：等待页面完全加载后提取 HTML\n3. <lg>解析 HTML</lg>：将内容传递给 Cheerio 进行 DOM 操作\n4. <lg>筛选锚点</lg>：使用 Cheerio 选择器提取带有特定类的 `&lt;a>` 标签\n\n以下是完整的实现代码：\nconst puppeteer = require(\'puppeteer\');\nconst cheerio = require(\'cheerio\');\n\nasync function extractAnchorsByClass(url, targetClass) {\n const browser = await puppeteer.launch({ headless: \'new\' }); // 启动无头浏览器\n try {\n  const page = await browser.newPage();\n  await page.goto(url, { waitUntil: \'networkidle2\' }); // 等待网络请求完成\n  \n  // 获取完整渲染后的 HTML\n  const html = await page.content();\n  \n  // 使用 Cheerio 加载 HTML\n  const $ = cheerio.load(html);\n  \n  // 提取所有带有目标类的锚点元素\n  const anchors = $(`a.${targetClass}`).map((index, element) => {\n   return {\n    text: $(element).text().trim(),\n    href: $(element).attr(\'href\'),\n    class: $(element).attr(\'class\')\n   };\n  }).get();\n  \n  return anchors;\n } catch (error) {\n  console.error(\'Error occurred:\', error);\n  throw error;\n } finally {\n  await browser.close(); // 确保浏览器始终关闭\n }\n}\n\n// 使用示例\n(async () => {\n try {\n  const url = \'https://example.com\'; // 替换为实际URL\n  const targetClass = \'special-link\'; // 替换为实际类名\n  \n  const result = await extractAnchorsByClass(url, targetClass);\n  console.log(\'提取的锚点元素:\', result);\n } catch (error) {\n  console.error(\'运行时错误:\', error);\n }\n})();\n使用说明：\n1. 安装依赖：`npm install puppeteer cheerio`\n2. 修改代码中的 `url` 和 `targetClass` 变量\n3. 运行脚本：`node script.js`\n\n代码特点：\n- 自动处理浏览器启动与关闭\n- 等待页面完全加载后再提取内容\n- 使用 CSS 选择器 `a.special-link` 精确匹配目标元素\n- 返回包含文本、链接和类的对象数组\n- 包含完整的错误处理机制\n\n如需提取其他类型的元素，只需修改选择器即可。\n例如，提取带有特定类的 `&lt;div>` 元素可使用 `div.special-class`。\n\n',
'<h2>collect elements in infinite scroll web pages</h2>\n实现方案：\nconst puppeteer = require(\'puppeteer\');\nconst cheerio = require(\'cheerio\');\n\nasync function extractAnchorsFromInfiniteScroll(url, targetClass, scrollTimeout = 2000, maxScrolls = 10) {\n const browser = await puppeteer.launch({ headless: \'new\' });\n try {\n  const page = await browser.newPage();\n  await page.goto(url, { waitUntil: \'networkidle2\' });\n  \n  // 存储已收集的元素数量，用于检测内容是否加载\n  let previousCount = 0;\n  let scrolls = 0;\n  \n  // 循环滚动页面直到内容不再加载或达到最大滚动次数\n  while (scrolls < maxScrolls) {\n   // 滚动到页面底部\n   await page.evaluate(() => {\n    window.scrollTo(0, document.body.scrollHeight);\n   });\n   \n   // 等待页面加载新内容\n   await page.waitForTimeout(scrollTimeout);\n   \n   // 计算当前已加载的目标元素数量\n   const currentCount = await page.$$eval(`a.${targetClass}`, elements => elements.length);\n   \n   // 如果没有新内容加载，则停止滚动\n   if (currentCount === previousCount) {\n    console.log(\'内容加载完成，停止滚动\');\n    break;\n   }\n   \n   previousCount = currentCount;\n   scrolls++;\n   console.log(`已滚动 ${scrolls}/${maxScrolls}，当前元素数量: ${currentCount}`);\n  }\n  \n  // 获取完整渲染后的 HTML\n  const html = await page.content();\n  const $ = cheerio.load(html);\n  \n  // 提取所有目标锚点元素\n  const anchors = $(`a.${targetClass}`).map((index, element) => ({\n   text: $(element).text().trim(),\n   href: $(element).attr(\'href\'),\n   class: $(element).attr(\'class\')\n  })).get();\n  \n  return anchors;\n } catch (error) {\n  console.error(\'Error occurred:\', error);\n  throw error;\n } finally {\n  await browser.close();\n }\n}\n\n// 使用示例\n(async () => {\n try {\n  const url = \'https://example-infinite-scroll.com\'; // 替换为实际无限滚动页面URL\n  const targetClass = \'special-link\';\n  const maxScrolls = 15; // 最大滚动次数\n  const scrollTimeout = 3000; // 每次滚动后等待时间（毫秒）\n  \n  const result = await extractAnchorsFromInfiniteScroll(url, targetClass, scrollTimeout, maxScrolls);\n  console.log(`共提取 ${result.length} 个锚点元素`);\n  console.log(\'示例结果:\', result.slice(0, 3)); // 仅显示前3个结果\n } catch (error) {\n  console.error(\'运行时错误:\', error);\n }\n})();\n<h3>主要改进点：</h3>\n1. <lg>滚动机制</lg>：使用 `window.scrollTo(0, document.body.scrollHeight)` 滚动到页面底部\n2. <lg>内容检测</lg>：通过比较每次滚动前后目标元素数量，判断是否有新内容加载\n3. <lg>终止条件</lg>：设置最大滚动次数和内容加载超时机制，避免无限循环\n4. <lg>进度反馈</lg>：控制台输出滚动进度和元素计数，便于调试\n5. <lg>参数配置</lg>：\n  - `scrollTimeout`：调整每次滚动后的等待时间（毫秒）\n  - `maxScrolls`：设置最大滚动次数上限\n<h3>使用建议：</h3>\n1. 根据目标网站加载速度调整 `scrollTimeout`（默认 2000ms）\n2. 调整 `maxScrolls` 以平衡数据完整性和爬取效率\n3. 对于动态加载较慢的页面，可增加 `scrollTimeout` 或使用更精确的加载检测方法\n4. 如需更复杂的加载检测，可改用 `page.waitForSelector()` 等待特定元素出现\n\n此方案能有效处理大多数基于滚动的无限加载页面，确保捕获全部目标元素。\n',
'<h2>Parsing HTML with Node.js and Cheerio</h2>\n<center><div id="ParsingHTMLtoc" class="toc"><a href="#ParsingHTMLtopic-0" target="_self">Basic Setup</a><br><a href="#ParsingHTMLtopic-1" target="_self">Example 1: Simple HTML Parsing and Saving</a><br><a href="#ParsingHTMLtopic-2" target="_self">Example 2: Parsing from a File and Saving Changes</a><br><a href="#ParsingHTMLtopic-3" target="_self">Example 3: Extracting Specific Content and Saving</a><br><a href="#ParsingHTMLtopic-4" target="_self">Example 4: Advanced Manipulation with Saving</a><br></div></center><br><br>\n\nHere are some examples of how to parse HTML using Node.js and Cheerio, then save the output to an HTML file.\n<h3 id="ParsingHTMLtopic-0">Basic Setup</h3>\nFirst, install the required packages:\nnpm install cheerio fs\n<h3 id="ParsingHTMLtopic-1">Example 1: Simple HTML Parsing and Saving</h3>\nconst cheerio = require(\'cheerio\');\nconst fs = require(\'fs\');\n// Sample HTML input\nconst html = `\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n &lt;title&gt;Test Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n &lt;h1&gt;Welcome&lt;/h1&gt;\n &lt;div class="content"&gt;\n  &lt;p&gt;First paragraph&lt;/p&gt;\n  &lt;p&gt;Second paragraph&lt;/p&gt;\n &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n`;\n\n// Load HTML into Cheerio\nconst $ = cheerio.load(html);\n\n// Modify the HTML (example: add a class to h1)\n$(\'h1\').addClass(\'header\');\n\n// Get the modified HTML\nconst modifiedHtml = $.html();\n\n// Save to file\nfs.writeFile(\'output.html\', modifiedHtml, (err) =&gt; {\n if (err) throw err;\n console.log(\'HTML saved to output.html\');\n});\n<h3 id="ParsingHTMLtopic-2">Example 2: Parsing from a File and Saving Changes</h3>\n\nconst cheerio = require(\'cheerio\');\nconst fs = require(\'fs\').promises;\n\nasync function parseAndSaveHtml() {\n try {\n  // Read HTML file\n  const html = await fs.readFile(\'input.html\', \'utf8\');\n\n  // Load HTML into Cheerio\n  const $ = cheerio.load(html);\n\n  // Example modifications:\n  // 1. Add a class to all images\n  $(\'img\').addClass(\'responsive-img\');\n\n  // 2. Wrap all paragraphs in a div\n  $(\'p\').wrap(\'&lt;div class="paragraph-container"&gt;&lt;/div&gt;\');\n\n  // 3. Add a footer\n  $(\'body\').append(\'&lt;footer&gt;© 2023 My Website&lt;/footer&gt;\');\n\n  // Get the modified HTML\n  const modifiedHtml = $.html();\n\n  // Save to new file\n  await fs.writeFile(\'modified.html\', modifiedHtml);\n  console.log(\'Modified HTML saved to modified.html\');\n } catch (err) {\n  console.error(\'Error:\', err);\n }\n}\n\nparseAndSaveHtml();\n<h3 id="ParsingHTMLtopic-3">Example 3: Extracting Specific Content and Saving</h3>\n\nconst cheerio = require(\'cheerio\');\nconst fs = require(\'fs\');\n\n// Sample HTML with articles\nconst html = `\n&lt;div class="articles"&gt;\n &lt;article&gt;\n  &lt;h2&gt;Article 1&lt;/h2&gt;\n  &lt;p&gt;Content for article 1&lt;/p&gt;\n &lt;/article&gt;\n &lt;article&gt;\n  &lt;h2&gt;Article 2&lt;/h2&gt;\n  &lt;p&gt;Content for article 2&lt;/p&gt;\n &lt;/article&gt;\n&lt;/div&gt;\n`;\n\nconst $ = cheerio.load(html);\n\n// Extract articles and create a new HTML structure\nlet newHtml = \'&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Extracted Articles&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Featured Articles&lt;/h1&gt;&lt;div class="featured"&gt;\';\n\n$(\'article\').each((index, element) =&gt; {\n const title = $(element).find(\'h2\').text();\n const content = $(element).find(\'p\').text();\n\n newHtml += `\n &lt;div class="article"&gt;\n  &lt;h3&gt;${title}&lt;/h3&gt;\n  &lt;p&gt;${content}&lt;/p&gt;\n &lt;/div&gt;\n `;\n});\n\nnewHtml += \'&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;\';\n\n// Save the extracted content\nfs.writeFile(\'articles.html\', newHtml, (err) =&gt; {\n if (err) throw err;\n console.log(\'Extracted articles saved to articles.html\');\n});\n<h3 id="ParsingHTMLtopic-4">Example 4: Advanced Manipulation with Saving</h3>\n\nconst cheerio = require(\'cheerio\');\nconst fs = require(\'fs\').promises;\n\nasync function processHtml() {\n // Load HTML from a website or file\n const response = await fetch(\'https://example.com\');\n const html = await response.text();\n // Or from a local file:\n // const html = await fs.readFile(\'input.html\', \'utf8\');\n\n const $ = cheerio.load(html);\n\n // Remove all scripts and styles\n $(\'script, style\').remove();\n\n // Add custom CSS link\n $(\'head\').append(\'&lt;link rel="stylesheet" href="styles.css"&gt;\');\n\n // Add meta tag\n $(\'head\').append(\'&lt;meta name="description" content="Processed page"&gt;\');\n\n // Modify all links to open in new tab\n $(\'a\').attr(\'target\', \'_blank\');\n\n // Pretty print the HTML (requires html-formatter or similar)\n const modifiedHtml = $.html();\n // Or use a formatter like: const modifiedHtml = html.format($.html());\n\n // Save the processed HTML\n await fs.writeFile(\'processed.html\', modifiedHtml);\n console.log(\'Processed HTML saved to processed.html\');\n}\n\nprocessHtml().catch(console.error);\nFor formatting, you might want to use a package like `html-formatter`.\n',
'<h2>Using Puppeteer to Collect Web Pages and Extract Specific Content</h2>\n<center><div id="ExtractContenttoc" class="toc"><a href="#ExtractContenttopic-0" target="_self">Setup</a><br><a href="#ExtractContenttopic-1" target="_self">Complete Example</a><br><a href="#ExtractContenttopic-2" target="_self">How This Works</a><br><a href="#ExtractContenttopic-3" target="_self">Customization Options</a><br></div></center><br><br>\n<h3 id="ExtractContenttopic-0">Setup</h3>\nFirst, install the required packages:\nnpm install puppeteer cheerio fs\n<h3 id="ExtractContenttopic-1">Complete Example</h3>\nconst puppeteer = require(\'puppeteer\');\nconst cheerio = require(\'cheerio\');\nconst fs = require(\'fs\').promises;\n\nasync function fetchPage(url) {\n // Launch Puppeteer browser\n const browser = await puppeteer.launch({\n  headless: \'new\' // Use the new Headless mode\n });\n const page = await browser.newPage();\n \n // Configure Puppeteer to mimic a regular browser\n await page.setUserAgent(\'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\');\n await page.setViewport({ width: 1366, height: 768 });\n \n try {\n  console.log(`Fetching ${url}...`);\n  await page.goto(url, {\n   waitUntil: \'networkidle2\', // Wait for page to fully load\n   timeout: 30000 // 30 seconds timeout\n  });\n  \n  // Get the page content\n  const html = await page.content();\n  return html;\n } catch (error) {\n  console.error(\'Error fetching page:\', error);\n  return null;\n } finally {\n  await browser.close();\n }\n}\n\nasync function processHtml(html, targetClass) {\n if (!html) {\n  throw new Error(\'No HTML content provided\');\n }\n\n const $ = cheerio.load(html);\n \n // Find the target div\n const targetDiv = $(`.${targetClass}`);\n \n if (targetDiv.length === 0) {\n  throw new Error(`No div with class "${targetClass}" found`);\n }\n \n // Create a new HTML structure with the extracted content\n const newHtml = `&lt;!DOCTYPE html&gt;\n&lt;html lang="en"&gt;\n&lt;head&gt;\n &lt;meta charset="UTF-8"&gt;\n &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;\n &lt;title&gt;Extracted Content&lt;/title&gt;\n &lt;style&gt;\n  body {\n   font-family: Arial, sans-serif;\n   line-height: 1.6;\n   max-width: 1200px;\n   margin: 0 auto;\n   padding: 20px;\n  }\n  .extracted-content {\n   margin: 20px 0;\n  }\n &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n &lt;h1&gt;Extracted Content from ${targetClass}&lt;/h1&gt;\n &lt;div class="extracted-content"&gt;\n  ${targetDiv.html()}\n &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;`;\n \n return newHtml;\n}\n\nasync function main() {\n const url = \'https://example.com\'; // Replace with your target URL\n const targetClass = \'content\'; // Replace with the class you want to extract\n \n try {\n  // Step 1: Fetch the page with Puppeteer\n  const html = await fetchPage(url);\n  \n  if (!html) {\n   console.log(\'Failed to fetch page\');\n   return;\n  }\n  \n  // Step 2: Process the HTML to extract specific content\n  const extractedHtml = await processHtml(html, targetClass);\n  \n  // Step 3: Save the extracted content\n  await fs.writeFile(\'extracted-content.html\', extractedHtml);\n  console.log(\'Successfully saved extracted content to extracted-content.html\');\n } catch (error) {\n  console.error(\'Error:\', error.message);\n }\n}\n\n// Run the main function\nmain();\n<h3 id="ExtractContenttopic-2">How This Works</h3>\n\n1. **Puppeteer Setup**:\n- Launches a headless browser\n- Configures realistic browser settings to avoid detection\n- Navigates to the target URL and waits for the page to fully load\n\n2. **HTML Processing**:\n- Uses Cheerio to parse the collected HTML\n- Extracts the specific div with the target class\n- Creates a new clean HTML document with just the extracted content\n- Adds basic styling for better presentation\n\n3. **Saving Output**:\n- Writes the extracted content to a new HTML file\n<h3 id="ExtractContenttopic-3">Customization Options</h3>\n\n1. **To extract a different element**:\n- Modify the `processHtml` function to look for different selectors\n- For example, to extract by ID: `$(\'#someId\')`\n\n2. **To handle multiple pages**:\n   const urls = [\'https://example.com/page1\', \'https://example.com/page2\'];\n   for (const url of urls) {\n    const html = await fetchPage(url);\n    // Process each page\n   }\n   \n3. **To add more processing**:\n- Clean up the extracted content further (remove unwanted elements)\n- Transform the content (modify links, images, etc.)\n- Add additional metadata\n\n4. **To handle dynamic content**:\n   // In fetchPage(), add this before getting content:\n   await page.waitForSelector(`.${targetClass}`, { timeout: 5000 });\n   \nThis approach gives you a powerful combination of Puppeteer for reliable page fetching (even for JavaScript-rendered content) and Cheerio for efficient HTML processing.\n',
'<h2>Make Puppeteer Wait Until the DOM is Complete</h2>\n<center><div id="WaitUntilDOMCompletetoc" class="toc"><a href="#WaitUntilDOMCompletetopic-0" target="_self">1. Basic Wait Until Options</a><br><a href="#WaitUntilDOMCompletetopic-1" target="_self">2. Explicit Wait for Specific Elements</a><br><a href="#WaitUntilDOMCompletetopic-2" target="_self">3. Custom Wait Function</a><br><a href="#WaitUntilDOMCompletetopic-3" target="_self">4. Combination Approach (Recommended)</a><br><a href="#WaitUntilDOMCompletetopic-4" target="_self">5. Handling Single Page Applications (SPAs)</a><br><a href="#WaitUntilDOMCompletetopic-5" target="_self">Best Practices</a><br></div></center><br><br>\n\nTo ensure Puppeteer waits until all DOM elements are fully loaded and ready, you can use several different approaches depending on your specific needs.\nHere are the best methods:\n<h3 id="WaitUntilDOMCompletetopic-0">1. Basic Wait Until Options</h3>\nThe simplest way is to use the `waitUntil` option in `page.goto()`:\n\nawait page.goto(url, {\n waitUntil: \'domcontentloaded\' // waits for DOMContentLoaded event\n});\n\n// OR\nawait page.goto(url, {\n waitUntil: \'load\' // waits for load event (DOM and all resources)\n});\n\n// OR for single page applications\nawait page.goto(url, {\n waitUntil: \'networkidle0\' // waits until there are no more than 0 network connections for at least 500ms\n});\n\n// OR\nawait page.goto(url, {\n waitUntil: \'networkidle2\' // waits until there are no more than 2 network connections for at least 500ms\n});\n<h3 id="WaitUntilDOMCompletetopic-1">2. Explicit Wait for Specific Elements</h3>\nFor more reliability, wait for specific selectors:\n\nawait page.goto(url);\nawait page.waitForSelector(\'#main-content\'); // waits for specific element\nawait page.waitForSelector(\'.dynamic-content\', { visible: true }); // waits until visible\n<h3 id="WaitUntilDOMCompletetopic-2">3. Custom Wait Function</h3>\nFor complex pages, create a custom wait function:\n\nasync function waitForDOMComplete(page) {\n await page.evaluate(async () =&gt; {\n  // Check if document is ready\n  const checkReadyState = () =&gt; document.readyState === \'complete\';\n  \n  // Check if there are pending network requests\n  const checkPendingRequests = () =&gt; {\n   return window.performance\n    .getEntriesByType(\'resource\')\n    .filter(r =&gt; r.initiatorType !== \'beacon\')\n    .every(r =&gt; r.responseEnd &gt; 0);\n  };\n\n  // Poll until both conditions are met\n  return new Promise(resolve =&gt; {\n   const check = () =&gt; {\n    if (checkReadyState() &amp;&amp; checkPendingRequests()) {\n     resolve();\n    } else {\n     setTimeout(check, 100);\n    }\n   };\n   check();\n  });\n });\n}\n\n// Usage\nawait page.goto(url);\nawait waitForDOMComplete(page);\n<h3 id="WaitUntilDOMCompletetopic-3">4. Combination Approach (Recommended)</h3>\nFor most reliable results, combine multiple methods:\n\nasync function loadPageCompletely(page, url) {\n try {\n  // Set longer timeout (default is 30 seconds)\n  await page.setDefaultNavigationTimeout(60000);\n  \n  // First try the standard navigation\n  await page.goto(url, {\n   waitUntil: [\'domcontentloaded\', \'networkidle0\'],\n   timeout: 45000\n  });\n\n  // Wait for either body or a specific element\n  await Promise.race([\n   page.waitForSelector(\'body\', { timeout: 5000 }),\n   page.waitForSelector(\'#app-root\', { timeout: 5000 })\n  ]);\n\n  // Additional check for dynamic content\n  await page.waitForFunction(() =&gt; {\n   return document.readyState === \'complete\' &amp;&amp;\n    (window.jQuery ? jQuery.active === 0 : true) &amp;&amp;\n    (window.angular ? angular.element(document).injector().get(\'$http\').pendingRequests.length === 0 : true);\n  }, { timeout: 10000 });\n\n  console.log(\'Page fully loaded\');\n } catch (error) {\n  console.warn(\'Loading checks failed:\', error);\n  // Continue anyway as some content might be loaded\n }\n}\n\n// Usage\nawait loadPageCompletely(page, \'https://example.com\');\n<h3 id="WaitUntilDOMCompletetopic-4">5. Handling Single Page Applications (SPAs)</h3>\nFor SPAs with dynamic content loading:\n\nasync function loadSPA(page, url) {\n await page.goto(url, { waitUntil: \'networkidle0\' });\n \n // Wait for framework-specific indicators\n try {\n  // For React\n  await page.waitForFunction(() =&gt; {\n   return !document.querySelector(\'[data-reactroot], [data-reactid]\') || \n    (window.React &amp;&amp; React.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED);\n  }, { timeout: 10000 });\n  \n  // OR for Angular\n  await page.waitForFunction(() =&gt; {\n   return window.angular &amp;&amp; angular.element(document).injector();\n  }, { timeout: 10000 });\n  \n  // OR generic approach\n  await page.waitForFunction(() =&gt; {\n   return document.querySelectorAll(\'[aria-busy="true"], .loading, .progress\').length === 0;\n  }, { timeout: 15000 });\n } catch (e) {\n  console.log(\'Framework detection timeout, proceeding...\');\n }\n \n // Final content check\n await page.waitForSelector(\'.main-content\', { visible: true, timeout: 5000 });\n}\n<h3 id="WaitUntilDOMCompletetopic-5">Best Practices</h3>\n1. **Set appropriate timeouts**:\n   page.setDefaultNavigationTimeout(60000); // 60 seconds\n  page.setDefaultTimeout(30000); // 30 seconds for other operations\n\n2. **Handle failures gracefully**:\n   try {\n   await page.waitForSelector(\'.content\', { timeout: 10000 });\n  } catch (e) {\n   console.log(\'Content not found after 10 seconds, proceeding...\');\n  }\n\n3. **Monitor network activity**:\n   await page.waitForNetworkIdle({ idleTime: 1000, timeout: 30000 });\n\n4. **Check for visual completeness**:\n   await page.waitForFunction(() =&gt; {\n   return document.readyState === \'complete\' &amp;&amp; \n    window.performance.timing.loadEventEnd &gt; 0;\n  });\n',
'<h2>Enhanced `processHtml()` to Handle All Elements with Target Class</h2>\n<center><div id="HandleTargetClasstoc" class="toc"><a href="#HandleTargetClasstopic-0" target="_self" onclick="jumpto(0)">Updated Function</a><br><a href="#HandleTargetClasstopic-1" target="_self" onclick="jumpto(1)">Key Improvements:</a><br><a href="#HandleTargetClasstopic-2" target="_self" onclick="jumpto(2)">Usage Example:</a><br><a href="#HandleTargetClasstopic-3" target="_self" onclick="jumpto(3)">Handling Edge Cases:</a><br></div></center><br><br>\n\nTo extract all elements (not just divs) that have the specified class:\n<h3 id="HandleTargetClasstopic-0">Updated Function</h3> <a href="#top" target="_self"><b>⇧</b></a><br>\nasync function processHtml(html, targetClass) {\n if (!html) {\n  throw new Error(\'No HTML content provided\');\n }\n\n const $ = cheerio.load(html);\n \n // Find all elements with the target class (not just divs)\n const targetElements = $(`.${targetClass}`);\n \n if (targetElements.length === 0) {\n  throw new Error(`No elements with class "${targetClass}" found`);\n }\n \n // Create a container for all extracted elements\n let extractedContent = \'\';\n \n // Process each matching element\n targetElements.each((index, element) =&gt; {\n  const tagName = element.tagName.toLowerCase();\n  const elementHtml = $(element).html();\n  \n  // Preserve the original element type and attributes\n  extractedContent += `\n  &lt;div class="extracted-element" data-original-tag="${tagName}"&gt;\n   &lt;${tagName} class="${targetClass}"&gt;\n    ${elementHtml}\n   &lt;/${tagName}&gt;\n  &lt;/div&gt;\n  `;\n });\n \n // Create a new HTML structure with all extracted content\n const newHtml = `&lt;!DOCTYPE html&gt;\n&lt;html lang="en"&gt;\n&lt;head&gt;\n &lt;meta charset="UTF-8"&gt;\n &lt;meta name="viewport" content="width=device-width, initial-scale=1.0"&gt;\n &lt;title&gt;Extracted Content&lt;/title&gt;\n &lt;style&gt;\n  body {\n   font-family: Arial, sans-serif;\n   line-height: 1.6;\n   max-width: 1200px;\n   margin: 0 auto;\n   padding: 20px;\n  }\n  .extracted-content {\n   margin: 20px 0;\n  }\n  .extracted-element {\n   margin-bottom: 30px;\n   padding: 15px;\n   border: 1px solid #eee;\n   border-radius: 4px;\n  }\n  .extracted-element::before {\n   content: attr(data-original-tag);\n   display: block;\n   font-size: 12px;\n   color: #666;\n   margin-bottom: 5px;\n  }\n &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n &lt;h1&gt;Extracted Content from ${targetClass} class&lt;/h1&gt;\n &lt;div class="extracted-content"&gt;\n  ${extractedContent}\n &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;`;\n \n return newHtml;\n}\n<h3 id="HandleTargetClasstopic-1">Key Improvements:</h3> <a href="#top" target="_self"><b>⇧</b></a><br>\n\n1. <lg>Finds all elements</lg> with the specified class, regardless of tag type (div, p, span, etc.)\n2. <lg>Preserves original tag information</lg> by:\n- Storing the original tag name in a data attribute\n- Wrapping each extracted element in a container that shows its origin\n3. <lg>Maintains the original HTML structure</lg> within each extracted element\n4. <lg>Adds visual indicators</lg> to help identify different element types\n<h3 id="HandleTargetClasstopic-2">Usage Example:</h3> <a href="#top" target="_self"><b>⇧</b></a><br>\n// Example usage with Puppeteer\nconst html = await fetchPage(\'https://example.com\');\nconst extractedHtml = await processHtml(html, \'article\'); // Will find all elements with class "article"\n\nawait fs.writeFile(\'extracted.html\', extractedHtml);\n<h3 id="HandleTargetClasstopic-3">Handling Edge Cases:</h3> <a href="#top" target="_self"><b>⇧</b></a><br>\n\nFor a more robust solution, you might want to add:\n1. <lg>Attribute preservation</lg>:// Preserve all original attributes\nconst attributes = Object.entries(element.attribs)\n .map(([name, value]) =&gt; `${name}="${value}"`)\n .join(\' \');\n\n2. <lg>Nested class handling</lg>:// Handle elements that might have multiple classes\nconst exactClassMatch = $(`[class="${targetClass}"]`); // Exact match\nconst containsClass = $(`.${targetClass}`); // Contains class\n\n3. <lg>Special element processing</lg>:// Special handling for specific element types\nif (tagName === \'img\') {\n // Ensure absolute URLs for images\n const src = $(element).attr(\'src\');\n if (src &amp;&amp; !src.startsWith(\'http\')) {\n  $(element).attr(\'src\', new URL(src, baseUrl).href);\n }\n}\n',
];
nodejsServerTips = ImgList
tipsList = ImgList
