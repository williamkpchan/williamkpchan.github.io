<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #002010; font-size: 18px;}
pre { color: gray; background-color: #001010; font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
strong{color: lightblue;}
.topic{
    color: lime;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 10%;
	margin-right: 10%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.grayword{
    color: gray;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>
</head><body>

<center><h4>Simple Guide to Logistic Regression 逻辑回归法 in R</h4></center>
<div id="toc"><ul></ul></div>
<br>

<h2>Introduction</h2>
<p style="text-align: justify;">Every machine learning algorithm works best under a given set of conditions. Making sure your algorithm fits the assumptions / requirements ensures superior performance. You can&#8217;t use any algorithm in any condition. For example: Have you ever tried using linear regression on a categorical dependent variable? Don&#8217;t even try! Because you won&#8217;t be appreciated for getting extremely low values of adjusted R² and F statistic.</p>
<p style="text-align: justify;">Instead, in such situations, you should try using algorithms such as Logistic Regression, Decision Trees, SVM, Random Forest etc. To get a quick overview of these algorithms, I&#8217;ll recommend <a href="https://www.analyticsvidhya.com/blog/2015/08/common-machine-learning-algorithms/" target="_blank">Essentials of Machine Learning Algorithms</a>.</p>
<p style="text-align: justify;">With this post, I give you useful knowledge on Logistic Regression in R. After you&#8217;ve mastered <a href="https://www.analyticsvidhya.com/blog/2015/10/regression-python-beginners/" target="_blank">linear regression</a>, this comes as the natural following step in your journey. It&#8217;s also easy to learn and implement, but you must know the science behind this algorithm.</p>
<p style="text-align: justify;">I&#8217;ve tried to explain these concepts in simplest possible manner. Let&#8217;s get started.</p>

<h2 style="text-align: justify;">What is Logistic Regression ?</h2>
<p style="text-align: justify;">Logistic Regression is a classification algorithm. It is used to predict a binary outcome (1 / 0, Yes / No, True / False) given a set of independent variables. To represent binary / categorical outcome, we use dummy variables. You can also think of logistic regression as a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function.</p>
<p>&nbsp;</p>
<h2 style="text-align: justify;">Derivation of Logistic Regression Equation</h2>
<p style="text-align: justify;">Logistic Regression is part of a larger class of algorithms known as Generalized Linear Model (glm). In 1972, Nelder and Wedderburn proposed this model with an effort to provide a means of using linear regression to the problems which were not directly suited for application of linear regression. Infact, they proposed a class of different models (linear regression, ANOVA, Poisson Regression etc) which included logistic regression as a special case.</p>
<p>The fundamental equation of generalized linear model is:</p>
<pre style="text-align: center;">g(E(y)) = α + βx1 + γx2</pre>
<p style="text-align: justify;">Here, g() is the link function, E(y) is the expectation of target variable and α + βx1 + γx2 is the linear predictor ( α,β,γ to be predicted). The role of link function is to &#8216;link&#8217; the expectation of y to linear predictor.</p>
<p style="text-align: justify;"><span style="text-decoration: underline;">Important Points</span></p>
<ol>
<li style="text-align: justify;">GLM does not assume a linear relationship between dependent and independent variables. However, it assumes a linear relationship between link function and independent variables in logit model.</li>
<li style="text-align: justify;"> The dependent variable need not to be normally distributed.</li>
<li style="text-align: justify;">It does not uses OLS (Ordinary Least Square) for parameter estimation. Instead, it uses maximum likelihood estimation (MLE).</li>
<li style="text-align: justify;">Errors need to be independent but not normally distributed.</li>
</ol>
<p>&nbsp;</p>
<p>Let&#8217;s understand it further using an example:</p>
<p style="text-align: justify;">We are provided a sample of 1000 customers. We need to predict the probability whether a customer will buy (y) a particular magazine or not. As you can see, we&#8217;ve a categorical outcome variable, we&#8217;ll use logistic regression.</p>
<p style="text-align: justify;">To start with logistic regression, I&#8217;ll first write the simple linear regression equation with dependent variable enclosed in a link function:</p>
<pre style="text-align: justify;">                           g(y) = βo + β(Age)         ---- (a)</pre>
<p style="text-align: justify;">Note: For ease of understanding, I&#8217;ve considered &#8216;Age&#8217; as independent variable.</p>
<p style="text-align: justify;">In logistic regression, we are only concerned about the probability of outcome dependent variable ( success or failure). As described above, g() is the link function. This function is established using two things: Probability of Success(p) and Probability of Failure(1-p). p should meet following criteria:</p>
<ol style="text-align: justify;">
<li>It must always be positive (since p &gt;= 0)</li>
<li>It must always be less than equals to 1 (since p &lt;= 1)</li>
</ol>
<p style="text-align: justify;">Now, we&#8217;ll simply satisfy these 2 conditions and get to the core of logistic regression. To establish link function, we&#8217;ll denote g() with &#8216;p&#8217; initially and eventually end up deriving this function.</p>
<p style="text-align: justify;">Since probability must always be positive, we&#8217;ll put the linear equation in exponential form. For any value of slope and dependent variable, exponent of this equation will never be negative.</p>
<pre style="text-align: center;">p = exp(βo + β(Age)) = e^(βo + β(Age))    ------- (b)</pre>
<p style="text-align: justify;">To make the probability less than 1, we must divide p by a number greater than p. This can simply be done by:</p>
<pre style="text-align: center;">p  =  exp(βo + β(Age)) / exp(βo + β(Age)) + 1   =   e^(βo + β(Age)) / e^(βo + β(Age)) + 1    ----- (c)</pre>
<p>Using (a), (b) and (c), we can redefine the probability as:</p>
<pre style="text-align: center;">              p = e^y/ 1 + e^y           --- (d)</pre>
<p style="text-align: center;"><em>where </em>p is the probability of success.<em> This (d) is the Logit Function</em></p>
<p>If p is the probability of success, 1-p will be the probability of failure which can be written as:</p>
<pre style="text-align: center;">q = 1 - p = 1 - (e^y/ 1 + e^y)    --- (e)</pre>
<p style="text-align: center;"><em>where</em> q is the probability of failure</p>
<p>On dividing, (d) / (e), we get,</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1.png"><img class="size-full wp-image-21197 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1.png" alt="1" width="167" height="74" /></a></p>
<p>After taking log on both side, we get,<br />
<a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/2.png"><img class="size-full wp-image-21198 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/2.png" alt="2" width="197" height="110" /></a></p>
<p style="text-align: justify;">log(p/1-p) is the link function. Logarithmic transformation on the outcome variable allows us to model a non-linear association in a linear way.</p>
<p style="text-align: justify;">After substituting value of y, we&#8217;ll get:</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/3.png"><img class="size-full wp-image-21199 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/3.png" alt="logistic regression equation" width="314" height="104" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/3.png 314w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/3-300x99.png 300w" sizes="(max-width: 314px) 100vw, 314px" /></a></p>
<p style="text-align: justify;">This is the equation used in Logistic Regression. Here (p/1-p) is the odd ratio. Whenever the log of odd ratio is found to be positive, the probability of success is always more than 50%. A typical logistic model plot is shown below. You can see probability never goes below 0 and above 1.</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/plot.png"><img class="size-full wp-image-21200 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/plot.png" alt="logistic regression model" width="355" height="270" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/plot.png 355w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/plot-300x228.png 300w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/plot-83x63.png 83w" sizes="(max-width: 355px) 100vw, 355px" /></a></p>
<p>&nbsp;</p>
<h2>Performance of Logistic Regression Model</h2>
<p style="text-align: justify;">To evaluate the performance of a logistic regression model, we must consider few metrics. Irrespective of tool (SAS, R, Python) you would work on, always look for:</p>
<p style="text-align: justify;">1. <strong><span style="text-decoration: underline;">AIC (Akaike Information Criteria)</span></strong> &#8211; The analogous metric of adjusted R² in logistic regression is AIC. AIC is the measure of fit which penalizes model for the number of model coefficients. Therefore, we always prefer model with minimum AIC value.</p>
<p>&nbsp;</p>
<p style="text-align: justify;">2. <strong><span style="text-decoration: underline;">Null Deviance and Residual Deviance</span></strong> &#8211; Null Deviance indicates the response predicted by a model with nothing but an intercept. Lower the value, better the model. Residual deviance indicates the response predicted by a model on adding independent variables. Lower the value, better the model.</p>
<p>&nbsp;</p>
<p>3. <strong><span style="text-decoration: underline;">Confusion Matrix</span>:</strong> It is nothing but a tabular representation of Actual vs Predicted values. This helps us to find the accuracy of the model and avoid overfitting. This is how it looks like:</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1111.png"><img class="size-full wp-image-21219 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1111.png" alt="1111" width="618" height="198" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1111.png 618w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/1111-300x96.png 300w" sizes="(max-width: 618px) 100vw, 618px" /></a>                                                                                            Source: (plug &#8211; n &#8211; score)</p>
<p>You can calculate the <strong>accuracy</strong> of your model with:</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/7.png"><img class="size-full wp-image-21204 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/7.png" alt="accuracy of confusion matrix" width="593" height="100" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/7.png 593w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/7-300x51.png 300w" sizes="(max-width: 593px) 100vw, 593px" /></a></p>
<p>From confusion matrix, Specificity and Sensitivity can be derived as illustrated below:</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/9.png"><img class="size-full wp-image-21206 aligncenter" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/9.png" alt="9" width="542" height="265" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/9.png 542w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/9-300x147.png 300w" sizes="(max-width: 542px) 100vw, 542px" /></a></p>
<p>Specificity and Sensitivity plays a crucial role in deriving ROC curve.</p>
<p>&nbsp;</p>
<p style="text-align: justify;">4. <strong><span style="text-decoration: underline;">ROC Curve:</span></strong> Receiver Operating Characteristic(ROC) summarizes the model&#8217;s performance by evaluating the trade offs between true positive rate (sensitivity) and false positive rate(1- specificity). For plotting ROC, it is advisable to assume p &gt; 0.5 since we are more concerned about success rate. ROC summarizes the predictive power for all possible values of p &gt; 0.5.  The area under curve (AUC), referred to as index of accuracy(A) or concordance index, is a perfect performance metric for ROC curve. Higher the area under curve, better the prediction power of the model. Below is a sample ROC curve. The ROC of a perfect predictive model has TP equals 1 and FP equals 0. This curve will touch the top left corner of the graph.</p>
<p><a href="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/logit_roc.png"><img class="aligncenter wp-image-21207 size-medium" src="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/logit_roc-300x292.png" alt="logit_roc" width="300" height="292" srcset="https://www.analyticsvidhya.com/wp-content/uploads/2015/11/logit_roc-300x292.png 300w, https://www.analyticsvidhya.com/wp-content/uploads/2015/11/logit_roc.png 708w" sizes="(max-width: 300px) 100vw, 300px" /></a></p>
<p style="text-align: justify;"><strong><span style="text-decoration: underline;">Note:</span></strong> For model performance, you can also consider likelihood function. It is called so, because it selects the coefficient values which maximizes the likelihood of explaining the observed data. It indicates goodness of fit as its value approaches one, and a poor fit of the data as its value approaches zero.</p>
<p>&nbsp;</p>
<h2>Logistic Regression Model in R</h2>
<p style="text-align: justify;">Considering the availability, I&#8217;ve build this model on our practice problem &#8211; Dressify data set. You can download it <a href="http://datahack.analyticsvidhya.com/contest/practice-problem-1" target="_blank">here</a>. Without going deep into feature engineering, here&#8217;s the script of simple logistic regression model:</p>
<pre>setwd('C:/Users/manish/Desktop/dressdata')</pre>
<pre>#load data
train &lt;- read.csv('Train_Old.csv')

</pre>
<pre>#create training and validation data from given data
install.packages('caTools')
library(caTools)

</pre>
<pre>set.seed(88)
split &lt;- sample.split(train$Recommended, SplitRatio = 0.75)

</pre>
<pre>#get training and test data
dresstrain &lt;- subset(train, split == TRUE)
dresstest &lt;- subset(train, split == FALSE)

</pre>
<pre>#logistic regression model
model &lt;- glm (Recommended ~ .-ID, data = dresstrain, family = binomial)
summary(model)</pre>
<pre>predict &lt;- predict(model, type = 'response')</pre>
<pre>#confusion matrix
table(dresstrain$Recommended, predict &gt; 0.5)</pre>
<pre>#ROCR Curve
library(ROCR)
ROCRpred &lt;- prediction(predict, dresstrain$Recommended)
ROCRperf &lt;- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf, colorize = TRUE, text.adj = c(-0.2,1.7))</pre>
<pre>#plot glm
library(ggplot2)
ggplot(dresstrain, aes(x=Rating, y=Recommended)) + geom_point() + 
stat_smooth(method="glm", family="binomial", se=FALSE)</pre>
<p style="text-align: justify;">This data require lots of cleaning and feature engineering. The scope of this article restricted me to keep the example focused on building logistic regression model. This data is <a href="http://datahack.analyticsvidhya.com/contest/practice-problem-1" target="_blank">available</a> for practice. I&#8217;d recommend you to work on this problem. There&#8217;s a lot to learn.</p>
<p>&nbsp;</p>
<h2>End Notes</h2>
<p style="text-align: justify;">By now, you would know the science behind logistic regression. I&#8217;ve seen many times that people know the use of this algorithm without actually having knowledge about its core concepts. I&#8217;ve tried my best to explain this part in simplest possible manner. The example above only shows the skeleton of using logistic regression in R. Before actually approaching to this stage, you must invest your crucial time in feature engineering.</p>
<p style="text-align: justify;">Furthermore, I&#8217;d recommend you to work on this problem set. You&#8217;d explore things which you might haven&#8217;t faced before.</p>
<p style="text-align: justify;">Did I miss out on anything important ? Did you find this article helpful? Please share your opinions / thoughts in the comments section below.</p>
<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2, h3').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
