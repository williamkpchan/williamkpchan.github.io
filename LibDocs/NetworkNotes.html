<base target="_blank"><html><head><title>Network Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script>
  var showTopicNumber = true;
  var bookid = "Network Notes" 
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Network Notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="https://www.guru99.com/best-free-proxy.html" class="whitebut ">Proxy Server Service</a>

<a href="https://www.youtube.com/watch?v=-cRMD40Se80" class="whitebut ">Xender 無線傳輸APP</a>
<a href="https://www.youtube.com/watch?v=156s1hphwHg" class="whitebut ">免費創建自己的私人云盤</a>
<a href="https://www.youtube.com/watch?v=O4Y6UXjVlFQ" class="whitebut ">隱藏自己真實的IP地址5種方法</a>

</div>
<pre>
<br>
<br>
<h2>5 Tools to Manage Multiple Network Connection Profiles</h2>
Every home or work network you connect to can conceivably have different settings for the connection. 
At home you might <a href="https://www.raymond.cc/blog/how-do-i-know-if-someone-is-using-my-wireless-network-wifi/">use a WiFi connection</a> with default or automatic settings, for work you might have a manually set IP address and gateway or <a href="https://www.raymond.cc/blog/test-change-dns-servers-quickly-dns-jumper/">custom DNS servers</a>. 
Each different network configuration you come across requires the settings in Windows to be adjusted to match. 
While this isn&#8217;t a major problem because Windows allows you to change these types of settings, it isn&#8217;t the quickest thing to get at and change regularly.
Manual reconfiguration each time through <a href="https://www.raymond.cc/blog/repair-xp-and-vista-internet-connection-problems-with-icr/">Windows Network Connections</a> is both time consuming and inefficient. 
It requires you to remember each individual setting or the network might not connect properly. 
An easy solution is being able to create different network profiles for different scenarios, so you can save <a href="https://www.raymond.cc/blog/6-free-websites-to-map-ip-addresses-to-geographical-locations/">IP address</a>, DNS, default gateway and other settings for each network connection you encounter. 
When you need to change to a different network, a different profile can easily be applied which changes all the required settings automatically.
Here are 5 free tools for you to create and apply network profiles.

<h3>1. TCP/IP Manager</h3>
TCP/IP Manager has a good mix of the ability to easily save network settings into a series of profiles and enough features and functions to cater for most users. 
The program is open source and available in both setup installer and portable versions. 
Make sure to get the correct 32-bit or 64-bit version for your system.
<img src="https://img.raymond.cc/blog/wp-content/uploads/2012/05/tcpip_manager.png">

After launching the program click Create a new profile and give it a name. 
Choose a network adapter from the drop down and configure the IP address, subnet, gateway and DNS servers. 
Automatic options are also available like in Windows. 
Optionally go to the Profile settings tab and choose to show the profile in the tray menu and give it a keyboard shortcut so you can launch the profile by a key combination. 
Finally click Save current profile. 
When you want to launch a profile click Apply in the window, press the hotkey combination or select from the tray icon context menu.
Proxy servers can be configured from the corresponding tab, advanced settings include changing the computer name, changing the workgroup name and possibly a unique feature of allowing MAC spoofing on the network adapter. 
TCP/IP Manager was quite reasonable on memory usage consuming around 4MB while sitting in the tray.
<a href="https://www.raymond.cc/blog/download/did/2089/" rel="nofollow">Download TCP/IP Manager</a>

<h3>2. IP Shifter</h3>
If you just want a nice and simple network connection changer, IP Shifter is relatively easy to use and doesn&#8217;t require tons of knowledge to configure. 
It also has a portable version so installation isn&#8217;t necessary either.
<img src="https://img.raymond.cc/blog/wp-content/uploads/2012/05/ip_shifter.png">

Start the program, click the button to create a new profile and give it a name. 
The standard options allow you to select the network adapter from the drop down and then obtain an automatic or manual IP address and DNS server. 
This window is similar to the Windows Internet Protocol 4 Properties dialog and has boxes for IP address, subnet mask, gateway and DNS servers. 
Proxies for Internet Explorer and Firefox can be setup by clicking on Settings and the check box near the bottom.
Once all the profiles are setup you can switch between them by selecting and clicking Apply in the main window or minimize the program to the tray and right click on the tray icon. 
IP Shifter used around 3MB of memory while in the tray. 
A couple of useful extras are in the Tools menu to Ping an address, scan the LAN for computers and obtain your public IP address.
<a href="https://www.raymond.cc/blog/download/did/3631/" rel="nofollow">Download IP Shifter</a>

<h3>3. NetSetMan</h3>
In contrast to IP Shifter, NetSetMan is loaded with tons of options and may be a bit too much for the average user. 
For geeks and advanced users though, it&#8217;s one of the most feature rich network profiling tools around. 
Only a setup installer is available but it can create a portable version because the program can be extracted to the folder of your choice.
<img src="https://img.raymond.cc/blog/wp-content/uploads/2012/05/netsetman.png">

Most of the settings can be left alone if you only want a simple IP and DNS changer. 
Choose a renamable SET tab to edit the profile and enter enter the IP, gateway and DNS information or leave what you don&#8217;t need on automatic. 
Use the Activate button or the tray context menu to choose between them. 
The IP+ button takes you to an advanced settings window where extra functions such as routing tables, DNS suffixes and expert settings like running Windows ipconfig commands are available.
Other more advanced networking options like a built in WiFi connection manager, computer name and workgroup changer, create network drives, append to the HOSTS file and a dedicated IPv6 settings window could all prove useful. 
Other options like changing the default printer, changing dozens of system settings or running a script/program are nice additions but not strictly necessary. 
NetSetMan uses around 8MB of memory in the background. 
The free personal use only version cannot change proxies, browser home pages and network domains.
<a href="https://www.raymond.cc/blog/download/did/3632/" rel="nofollow">Download NetSetMan</a>

<h3>4. Net Profiles Mod</h3>
This is a modified and forked version of the discontinued Net Profiles tool which has not been updated since 2011. 
Luckily this open source modded version is still in active development so there&#8217;s a fair chance bugs and issues will be fixed in future.
<img src="https://img.raymond.cc/blog/wp-content/uploads/2012/05/net_profiles_mod.png">

Setting up a new profile is easy and for basic usage you only have to give it a name, select a network (if there are multiple) and enter the IP and DNS details manually if required. 
Use the Get Current Settings button to create a profile of your current network configuration. 
Additional options include proxies, default browser homepage, mapped drives, default printer, running an application, desktop resolution/wallpaper and connection to a specific SSID. 
File &gt; Create Desktop Shortcut allows launching a profile via shortcut without having the program running in the background.
There are a couple of issues we had with Net Profiles Mod. 
Firstly, our WiFi adapter was not recognized unless it was connected to a wireless network. 
Secondly, you cannot obtain an IP address automatically through DHCP and set the DNS servers manually, or vice versa. 
This is easily possible through Windows and a drawback if you want to change the DNS but leave the IP alone.
<a href="https://www.raymond.cc/blog/download/did/3633/" rel="nofollow">Download Net Profiles Mod</a>

<h3>5. Argon Network Switcher</h3>
Argon Network Switcher is a middle of the road type of tool in terms of features. 
It has enough to satisfy all but the most advanced users but not too many to confuse people.
<img src="https://img.raymond.cc/blog/wp-content/uploads/2012/05/argon_network_switcher.png">

Usage is similar to the other tools here. 
Click New to create a new profile, enter a name, select the network adapter and then enter the IP, subnet, gateway and DNS addresses. 
Click Save to add the profile. 
Additional options include assigning a specific WiFi SSID, proxy settings, map a drive, set the default printer, start and stop system services, run scripts and applications and also disabling a specific network adapter on running the profile. 
An interesting feature is Autorun which leaves it up to the program to determine the best profile to launch.
We did notice a couple of bugs during usage. 
One was the WiFi SSID profiles are not displayed for everyone so you can&#8217;t associate a wireless SSID with the network profile. 
Another was using the Test button in the Drive Map tab freezes the program. 
However, mounting and unmounting networked drives does work fine. 
Network Switcher consumes about 10-15MB of RAM when minimized to the tray.
<a href="https://www.raymond.cc/blog/download/did/3634/" rel="nofollow">Download Argon Network Switcher</a>
<em>Final Note: </em>We did also look at a few other network connection profiling tools, one we almost included was <a href="https://www.raymond.cc/blog/download/did/3635/" rel="nofollow">Eusing Free IP Switcher</a>. 
This tool is like an easier to use version of NetSetMan but a major issue is a donate popup nag every time the program launches. 
This is a shame as it has a good blend of ease of use and features to make it useful.

<h2>开源的社区网盘</h2>
在gitee上发现了开源的 KodExplorer 可道云（https://gitee.com/kalcaddle/KODExplorer），发现非常适合作为小型文件共享，也很适合作为社区网盘。

它不需要安装任何程序或者插件，也不需要特殊的软件或者权限，只要有一个浏览器（主流的网络浏览器都可以，推荐chrome或firefox），就可以在线查看pdf、图片，编辑文件，共享文件等，使用方式和windows的文件管理器类似，无需学习就可以掌握。

https://www.micropython.org.cn/pan


<h2>Find the IP address</h2>
Windows
Type in “ipconfig” and hit Enter.
Look for the line that reads “IPv4 Address.”

192.168.0.100

Find the IP address of an iPad or Android Tablet

Go to setting on your iPad
Select ” WiFi ” from the sidebar.
Tap on the arrow next to the network name.
Your IP address will be displayed to the right of "IP address"

<h2>Accessing WAMP From Computers on Your LAN</h2>
<a href="https://john-dugan.com/access-wamp-from-lan-computers/" class="whitebut ">Accessing WAMP From Computers on Your LAN</a>

<a href="https://stackoverflow.com/questions/24005828/how-to-enable-local-network-users-to-access-my-wamp-sites" class="whitebut ">enable local network users to access my WAMP sites</a>

<a href="https://stackoverflow.com/questions/36810669/why-wamp-server-put-online-offline-option-is-missing/36825283" class="whitebut ">wamp server put online/ offline</a>

Right click Wampmanager -> WAMPSetting -> Menu Item: Online/Offline

If you click it so there is a Tick beside it, you will see the Online/Offline menu on the left click menu.

However it was made optional as its use is defunct.

You should create Virtual Hosts for each of your projects, then you can amend each of those individually to control the Apache access rules.

In fact in WAMPServer 3 or greater, there is a Virtual Host defined for localhost so this old Online/Offline process wont actually do what you want.

You now have to go to the wamp\bin\apache\apache{version}\conf\extra\httpd-vhosts.conf file and manually amend that entry

&lt;VirtualHost *:80>
    ServerName localhost
    DocumentRoot D:/wamp/www
    &lt;Directory  "D:/wamp/www/">
        Options Indexes FollowSymLinks MultiViews
        AllowOverride All
        Require all granted                  #&lt;-- changed line
    &lt;/Directory>
&lt;/VirtualHost>

This file can be edited using the wampmanager menus like this
wampmanager -> Apache -> httpd-vhosts.conf

However it is not recommended to allow this sort of access to localhost. It is better to create a Virtual Hosts for each of your projects eg

&lt;VirtualHost *:80>
    ServerName localhost
    DocumentRoot D:/wamp/www
    &lt;Directory  "D:/wamp/www/">
        Options Indexes FollowSymLinks MultiViews
        AllowOverride All
        Require local
    &lt;/Directory>
&lt;/VirtualHost>

&lt;VirtualHost *:80>
    ServerName project1.dev
    DocumentRoot D:/wamp/www/project1
    &lt;Directory  "D:/wamp/www/project1">
        Options Indexes FollowSymLinks MultiViews
        AllowOverride All
        Require all granted
    &lt;/Directory>
&lt;/VirtualHost>

<h2>Setup a Virtual Host</h2>
<a href="https://john-dugan.com/wamp-vhost-setup/" class="whitebut ">Setup a Virtual Host on WAMP in 3 Steps</a>
<a href="https://www.techrepublic.com/blog/smb-technologist/create-virtual-hosts-in-a-wamp-server/" class="whitebut ">Create virtual hosts in a WAMP server</a>

<a href="http://forum.wampserver.com/read.php?2,146746" class="whitebut ">WAMPServer 3 Create a Virtual Host, the easy way</a>

<h2>Sci-Hub 用上了分布式网络</h2>
在网站域名屡次被撤销之后， Sci-Hub 创始人 Alexandra Elbakyan 在分布式域名网络 Handshake 上注册了新的网站。

现在，每个用户都可以直接通过服务门户和 NextDNS 直接访问 Sci-Hub。

<img class="lazy" data-src="https://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW9mkBqdBCp94269UtFWSXwVKf5bzAj8gh8YzGa7V5KvCricAp7ViaI0O9qrEv3Q8s5YrVOHiafdAdHKQ/640">

NextDNS：https://learn.namebase.io/starting-from-zero/how-to-access-handshake-sites#level-3-dns
HNS 网关 http://sci-hub.hns.hns.to/
这些年来，一直是 Alexandra Elbakyan 在维护 Sci-Hub，目前所有用户只能通过俄罗斯的 Yandex 和比特币赞助网站的运营。

此前，Sci-Hub 被多次撤销域名，推特账户又被封禁且无法申诉，传统域名系统显然不那么满足这个「盗版学术论文数据库」的需求，能够对抗审查的 DNS 成为 Sci-Hub 保持可访问状态的方法之一。
Handshake 工作原理Tieshun Roquerre 介绍说，Handshake 实际上是一个分布式的域名服务器。
它不使用 web 标准证书颁发机构来验证用户与服务器的连接，而是存储对在其系统中注册的网站的 IP 地址的引用。
这样一来，如果证书颁发机构公司试图通过旧版系统审查 Sci-Hub 的域名，那么想要访问网站的人仍然可以通过 Handshake 的记录访问。
Namebase 则是为用户提供访问 Handshake 网络的平台，这个名字对于国内开发者来说不算陌生。

去年，关于「Namebase 羊毛现金福利」的消息一度引起热议，「满足条件的 GitHub 开发者可以获得大约 4200 个 HNS 代币的奖励」，折合人民币 5000 元左右，还有成功领取者写出了「从天上接馅饼」的教程。
Namebase CEO Tieshun Roquerre 介绍说：「DNS 就像互联网的电话簿，电话簿中的地址是服务器 IP 地址。
DNS 的创建是为了给 IP 地址提供人类可读的名称，因此在这一平台上，用户可以通过 Handshake 找到 IP 地址，而不是通过证书颁发机构。
」Roquerre 提到：「如果你的服务器级别受到限制，则可以切换服务器。
但如果你的域名被删除，就没人能访问你的网站了。

只要名称完好无损，就可以将其指向任何服务器。
」即使 band 网站找到了一个新域名，但用户也不确定其真实性（例如 Sci-Hub 也经常被冒名顶替）。
要注册一个 Handshake 域名，任何人都可以提出一个网站名称，并在 Handshake 的市场上用其同名 HNS token 对该网站进行竞标。
据外媒 CoinDesk 统计，目前共有 6818 个 Handshake 域名处于活跃使用状态，已注册的域名达到了 375000 个。
数据显示，其市场交易量每月平均增长 60%，近期将突破 14 万美元。
像 Handshake 这样的分布式（decentralized）域名系统，可能会成为分布式网络的标志性胜利。
该项目是众多所谓「Web 3.0」应用程序的一部分，当然，「Web 3.0」这个概念还存在很多争议，比如创建某些未经审查的互联网搜索，是否会带来更多的隐患？正如 Sci-Hub 的经历所证明的问题一样，分布式的网络是出于去平台化的担忧而构建的。
随着互联网访问接入点越来越集中在「少数玩家」的手上，最近也有一些应用受到了 Web 服务器提供、应用商店和 DNS 证书颁发机构的审查。
参考链接：https://www.nasdaq.com/articles/pirated-academic-database-sci-hub-is-now-on-the-uncensorable-web-2021-01-11https://www.coindesk.com/pirated-academic-sci-hub-handshake

<h2>create simple file server</h2>
C:\Windows\System32
run ipconfig to find ip address

<a href="https://stackoverflow.com/questions/15328623/simple-file-server-to-serve-current-directory" class="whitebut ">Simple file server to serve current directory</a>

python -m SimpleHTTPServer 8000
will serve the contents of the current working directory over HTTP on port 8000.

If you use Python 3, you should instead write
python -m http.server 8000

acer:
http://192.168.128.93:8000/

For Node, there's http-server:
$ npm install -g http-server
$ http-server Downloads -a localhost -p 8080
Starting up http-server, serving Downloads on port: 8080
Hit CTRL-C to stop the server

<h2>enable FTP through Chrome on all Windows devices</h2>

In Chrome 81, FTP support is disabled by default, but you can enable it using the # enable-ftp flag.

Open Chrome and type “chrome://flags” in the address bar.
Once in the flags area, type “enable-ftp” in the search bar stating “search flags”.
When you see the “Enable support for FTP URLs” option tap where it says “Default”.
Tap “Enable” option.
Hit “Relaunch Now” option at the bottom of the page.


<b>To transfer files via FTP using your web browser:</b>

From the File menu, choose Open Location....
In the "Location" field, type a URL like the following:
  ftp://username@name-of-server
For example, if your username is dvader, and you want to reach your account on deathstar.empire.gov, enter:

  ftp://dvader@deathstar.empire.gov
Note: Do not close the URL with a /, or you will connect to the root directory rather than your home directory.

You will be prompted for your password.
After you supply the password, you will see the contents of your home directory on the remote machine.
To change directories, click the appropriate yellow folder icon.
To download a file, drag the file from the browser window to the desktop.
You can also double-click the filename, and you will be prompted to either save or open the file.
To upload a file, drag the file from your hard drive to the browser window.


<h2>Net Command Syntax</h2>
The command takes the following general form:
net [accounts | computer | config | continue | file | group | help | helpmsg | localgroup | name | pause | print | send | session | share | start | statistics | stop | time | use | user | view]
Net Command Options
<table>
<tr><td><b>Option</b></td><td><b>Explanation</b></td></tr>
<tr><td><b>net</b></td><td>Execute the net command alone to show information about how to use the command which, in this case, is simply a list of the net subset commands.</td></tr>
<tr><td><b>accounts</b></td><td>The net accounts command is used to set password and logon requirements for users. For example, the net accounts command can be used to set the minimum number of characters that users can set their password to. Also supported is password expiration, minimum number of days before a user can change their password again, and the unique password count before the user can use the same old password.</td></tr>
<tr><td><b>computer</b></td><td>The net computer command is used to add or remove a computer from a domain.</td></tr>
<tr><td><b>config</b></td><td>Use the net config command to show information about the configuration of the<em>Server</em> or<em>Workstation</em> service.</td></tr>
<tr><td><b>continue</b></td><td>The net continue command is used to restart a service that was put on hold by the net pause command.</td></tr>
<tr><td><b>file</b></td><td>Net file is used to show a list of open files on a server. The command can also be used to close a shared file and remove a file lock.</td></tr>
<tr><td><b>group</b></td><td>The net group command is used to add, delete, and manage global groups on servers.</td></tr>
<tr><td><b>localgroup</b></td><td>The net localgroup command is used to add, delete, and manage local groups on computers.</td></tr>
<tr><td><b>name</b></td><td>Net name is used to add or delete a messaging alias at a computer. The net name command was removed in conjunction with the removal of net send beginning in Windows Vista. See the net send command for more information.</td></tr>
<tr><td><b>pause</b></td><td>The net pause command puts on hold a Windows resource or service.</td></tr>
<tr><td><b>print</b></td><td>Net print is used to display and manage network print jobs. The net print command was removed beginning in Windows 7. According to Microsoft, the tasks performed with net print can be performed in Windows 10, Windows 8, and Windows 7 using the<em>prnjobs.vbs</em> and other cscript commands, Windows PowerShell cmdlets, or Windows Management Instrumentation (WMI).</td></tr>
<tr><td><b>send</b></td><td>
<a href="https://www.lifewire.com/net-send-2618095" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">Net send</a> is used to send messages to other users, computers, or net name created messaging aliases. The net send command is not available in Windows 10, Windows 8, Windows 7, or Windows Vista but the
<a href="https://www.lifewire.com/msg-command-2618093" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="2">msg command</a> accomplishes the same thing.</td></tr>
<tr><td><b>session</b></td><td>The net session command is used to list or disconnect sessions between the computer and others on the network.</td></tr>
<tr><td><b>share</b></td><td>The net share command is used to create, remove, and otherwise manage shared resources on the computer.</td></tr>
<tr><td><b>start</b></td><td>The net start command is used to start a network service or list running network services.</td></tr>
<tr><td><b>statistics</b></td><td>Use the net statistics command to show the network statistics log for the<em>Server</em> or<em>Workstation</em> service.</td></tr>
<tr><td><b>stop</b></td><td>The net stop command is used to stop a network service.</td></tr>
<tr><td><b>time</b></td><td>Net time can be used to display the current time and date of another computer on the network.</td></tr>
<tr><td><b>use</b></td><td>The<a href="https://www.lifewire.com/net-use-command-2618096" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">net use command</a> is used to display information about shared resources on the network that you're currently connected to, as well as connect to new resources and disconnect from connected ones.
In other words, the net use command can be used to show the shared drives you've mapped to as well as allow you to manage those mapped drives.</td></tr>
<tr><td><b>user</b></td><td>The<a href="https://www.lifewire.com/net-user-command-2618097" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">net user command</a> is used to add, delete, and otherwise manage the users on a computer.</td></tr>
<tr><td><b>view</b></td><td>Net view is used to show a list of computers and network devices on the network.</td></tr>
<tr><td><b>helpmsg</b></td><td>The net helpmsg is used to display more information about the numerical network messages you might receive when using net commands. For example, when executing
<b>net group</b> on a standard Windows workstation, you'll receive a
<em>3515</em> help message. To decode this message, type
<b>net helpmsg 3515</b> which displays
<em>"This command can be used only on a Windows Domain Controller."</em> on screen.</td></tr>
<tr><td><b>/?</b></td><td>Use the
<a href="https://www.lifewire.com/help-switch-2625896" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">help switch</a> with the net command to show detailed help about the command's several options.</td></tr>
</tbody>
</table>
Save to a file whatever a <strong>net </strong>command shows on screen using a <a href="https://www.lifewire.com/redirection-operator-2625979" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">redirection operator</a> with the command. Learn&nbsp;<a href="https://www.lifewire.com/how-to-redirect-command-output-to-a-file-2618084" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="2">how to redirect command output to a file</a> or see&nbsp;our list of&nbsp;<a href="https://www.lifewire.com/command-prompt-tricks-and-hacks-2618104" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="3">command prompt tricks</a>&nbsp;for more tips.
Only in Windows NT and Windows 2000 was there a difference in the <strong>net </strong>command and the <strong>net1</strong> command. The net1 command was made available in these two operating systems as a temporary fix for a Y2K problem that affected the <strong>net </strong>command.
<h2>  Net Command Examples  </h2>
net view
This is one of the simplest net commands that lists all the networked devices.
net share Downloads=Z:\Downloads&nbsp;/GRANT:everyone,FULL
In the above example, I'm sharing the&nbsp;<em>Z:\Downloads</em>&nbsp;folder with&nbsp;everyone&nbsp;on the network and giving all of them&nbsp;full&nbsp;read/write access. You could modify this one by replacing&nbsp;<em>FULL</em>&nbsp;with&nbsp;<em>READ</em>&nbsp;or&nbsp;<em>CHANGE</em>&nbsp;for those rights only, as well as replace&nbsp;<em>everyone</em>&nbsp;with a specific username to give share access to just that one user account.
net accounts /MAXPWAGE:180
This example of the net accounts command forces a user's password to expire after 180 days. This number can be anywhere from&nbsp;<em>1</em>&nbsp;to&nbsp;<em>49,710</em>, or <em>UNLIMITED</em>&nbsp;can be used so that the password never expires. Default is 90 days.
net stop&nbsp;"print spooler"
The above net command example is how you'd stop the Print Spooler service from the command line.&nbsp;Services can also be started, stopped, and restarted via the Services graphical tool in Windows (services.msc), but using the net stop command lets you control them from places like Command Prompt and <a href="https://www.lifewire.com/bat-file-2619796" data-component="link" data-source="inlineLink" data-type="internalLink" data-ordinal="1">BAT&nbsp;files</a>.

net start
Executing the net start command without any options following it (e.g. net start "print spooler")&nbsp;is useful if you want to see a list of currently running services. This list can be helpful when managing services because you don't have to leave the command line to see which services are running.

<h2>send a pop up message to another computer</h2>
<h3>Send a Message to Another PC on a Local Network</h3>
run cmd
type <b>Net send</b> followed by the name of the computer to which you wish to send the message. Next, enter the message.

msg /SERVER:DestinationPC * /TIME:60 “This is the message to be sent to a PC named DestinationPC and closes in 60 seconds."

You will need to replace DestinationPC with the name of the desired PC (you can find this in the list of computers that are currently sharing your network, if you don't already know the PC name).
Now, replace the value for TIME with how long you want the message to appear on the other screen for. For example TIME:30 for 30seconds
Then replace the text between the quotation marks with the message that you want to send.
Finally, you can hit enter, and the message will be sent.

<h3>to list all computers in a windows wi-fi network</h3>
run cmd
<b>net view</b> will probably show most of them.

Ping the server if you know or your gateway.
Type the command <b>arp -a</b>
It will usually list down all the IP's and Computers with their Mac Addresses.

If you want to use a GUI tool. I recommend <b>IPScan</b>.
Although it is a light application (433KB), it is freeware that's always worked for me.
You can also use another GUI Tool, Advance IP Scanner.

<h3>how do I send a pop up message</h3>
To send a popup message:
Select one or more users from the user list.
Issue the Send popup message command by right-clicking on any selected user and selecting the 'Popup Message' menu item.

The 'Send popup message' window will appear.
Enter message text and click on the 'Send' button.

<h3>can I send a message to an IP address? </h3>
The IP Message no longer works.
The idea behind it is that you type a message and sort of password protect using some IP address.
Only the person with that IP address will be able to see the message.

Likewise, people ask, how can I see messages from another computer using CMD?
Start command prompt
Type the command as follows:
Hit enter and voila, the message is sent.
How can I communicate with another computer on the same network?

Method 1 Sharing Internet from Windows

Connect the two computers with an Ethernet cable.
Open Start.
Open Control Panel.
Click Network and Internet.
Click Network and Sharing Center.
Click Change adapter settings.
Select both the Wi-Fi connection and the Ethernet connection.
Right-click the Wi-Fi connection.

Alternatives for Older Versions of Windows
Alternative 1
Here is the first alternative way of sending messages that may work if you have an older version of Windows. Here's how:

Click Start > Run.
Type cmd, and press Enter.
In the window that opens, type Net send followed by the name of the computer to which you wish to send the message.
Next, enter the message. For example, the format should resemble "Net send PC01 can you read this message?"
Alternative 2
It is easy to send messages through cmd prompt to other systems here is the answer first we have to set our systems messenger ACTIVE. For it, follow these steps:

1. Go to RUN
2. Type services.msc
3. Scroll down and right click on MESSENGER
4. Select PROPERTIES
5. Then for enabling it go to STARTUP TYPE and select AUTOMATIC
6. Then OK
And this should be performed on both sides (SENDER & RECEIVER). After that if you want to send message then do the following steps:

1. Go to cmd prompt
2. Type syntax as follows: net send <ipaddress of reciever> <message to be send>
Ex:

net send 172.16.6.99 "hello"

<h2>FileZilla</h2>
<a href="https://filezilla-project.org/download.php?show_all=1" class="whitebut ">downoad filezilla client</a>

<a href="https://filezilla-project.org/download.php?type=server" class="whitebut ">Download FileZilla Server for Windows</a>
<a href="https://dl4.cdn.filezilla-project.org/server/FileZilla_Server-0_9_60_2.exe?h=Q49drOcs8bOUZF_nd8ry1A&x=1626777411" class="whitebut ">FileZilla Server file</a>

<h2>Turn Wi-Fi Router USB Port into a NAS Server</h2>
<div id="routertoc">0 <a href="#routertopic-0" target="_self">Turn Wi-Fi Router USB Port into a NAS Server</a><br>1 <a href="#routertopic-1" target="_self">What's the use of a Wi-Fi router USB port?</a><br>2 <a href="#routertopic-2" target="_self">Host that (old) printer</a><br>3 <a href="#routertopic-3" target="_self">Cellular connection</a><br>4 <a href="#routertopic-4" target="_self">Network-attached storage (NAS) server</a><br>5 <a href="#routertopic-5" target="_self">How to best turn a Wi-Fi router USB port into a NAS server</a><br>6 <a href="#routertopic-6" target="_self">Get expectations straight</a><br>7 <a href="#routertopic-7" target="_self">Get a good external drive</a><br>8 <a href="#routertopic-8" target="_self">Get the right router</a><br>9 <a href="#routertopic-9" target="_self">Use the correct settings</a><br>10 <a href="#routertopic-10" target="_self">How to access your router-based NAS server</a><br>11 <a href="#routertopic-11" target="_self">Accessing your NAS server on a Windows computer</a><br>12 <a href="#routertopic-12" target="_self">Accessing your NAS server on a Mac</a><br>13 <a href="#routertopic-13" target="_self">Best USB-enabled routers that can work as a NAS server</a><br></div></center>

When it comes to network storage, I'd recommend a <a href="https://dongknows.com/dong-ngos-most-important-gadget-is-a-synology-nas-server/#Whats_a_NAS_server" target="_blank">NAS server</a>. 
But a good server can be expensive; plus, not everyone wants or has time to configure all the features. 
So the second-best option is to make use of what you likely already have: the Wi-Fi router USB port.

Indeed, there are many routers on the market that can simultaneously deliver both Wi-Fi <em>and</em> storage space for your entire home. 
Specifically, they allow you to share files stored on an external drive with the rest of the network.
This post, among other things, talks about the storage-related use of a USB-ready Wi-Fi router. 
You'll also find the link to my list of recommended routers and tips on how to best set up one as a NAS server.

<img class="lazy loaded" data-src="https://dongknows.com/wp-content/uploads/2020/12/Asus-RT-AX89X-USB-768x1024.jpg" src="https://dongknows.com/wp-content/uploads/2020/12/Asus-RT-AX89X-USB-768x1024.jpg" data-was-processed="true">
Router USB port: These little ports can bring about extra values.

<h3>What's the use of a Wi-Fi router USB port?</h3>
Not every Wi-Fi router has a USB port, but if yours happens to have one, chances are you can use it for (at least one of) the followings:
<h3>Host that (old) printer</h3>

Print serving is the original function of a router USB port. 
Connect a USB printer to this port, and it's now available to the entire network. 
There's no need to buy a printer for each person anymore.
Five or six years ago, this feature was a big deal since printers at the time were mostly USB-only. 

Nowadays, those with a built-in network port or Wi-Fi are commonplace. 
With that, some new Wi-Fi routers don't offer the print serving feature anymore, though many still do.
<h3>Cellular connection</h3>
This feature allows the router to host a cellular USB modem and share the mobile Internet to the entire network. 

A cellular connection is a great way to have a backup Internet when your broadband service, like DSL or cable, is down.
Note that a router with this feature only supports specific cellular modems. 
Make sure you check the manual to know which one to get.
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2019/08/Wi-Fi-Router-USB-NAS.jpg">

Picking the right external storage device is the first step to turn a Wi-Fi router USB port into a mini NAS server.
<h3>Network-attached storage (NAS) server</h3>
This feature is, by far, the most common and useful. 
Similar to the case of printing, plugging an external hard drive into the router USB port can also make its storage available to the entire network.

On top of that, you can use that public storage space for other applications, such as a backup destination (including Time Machine backup, in some cases,) PC-less downloading, or even a personal cloud.<a href="https://dongknows.com/how-to-turn-your-wi-fi-router-into-a-time-capsule/">See also How to Turn your USB-enabled Wi-Fi Router into a Time Capsule</a>
<h3>How to best turn a Wi-Fi router USB port into a NAS server</h3>
There are a couple of things to keep in mind about using a router as a NAS server.
<h3>Get expectations straight</h3>

The first and most important thing to remember is a router's primary function is to host your network. 
For this reason, even a high-end router tends to have limited processing power for non-networking tasks. 
<h4>It's a router you're using!</h4>
Naturally, a router is not as capable as a dedicated NAS server when hosting storage space. 

Also, just because the router USB port or ports support a few functions -- like NAS, printing, or cellular modem, and so on -- doesn't mean you should expect to use <em>all of them at the same time</em>, nor should you expect the top performance of each when you use them all together. 
(For the same reason, you can't, either, expect to have the same storage performance via Wi-Fi as via a wired connection. 
In the former, the router has to use its power to broadcast the Wi-Fi signals at the same time.)
By the way, if a router has multiple USB ports, chances are they all share a single USB hub. 

So, you can't use more than one bus-powered devices with it, and each port only has its share of the hub's total bandwidth.
Again, it's a router you're looking at. 
Just because there are ports doesn't mean you can use them all at your expected performance.
<h4>Security</h4>

Security can also be a concern. 
For example, some routers still use <a rel="noopener noreferrer nofollow" href="https://en.wikipedia.org/wiki/Server_Message_Block#SMB_/_CIFS_/_SMB1" target="_blank">SMBv1</a>, which is the original and ancient version of the popular Server Message Block protocol used in the Windows environment for network file and printer sharing. 
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2019/09/SMBv1-required.png">
Router USB port: Some routers still require the old and vulnerable SMBv1 protocol for the USB-based file sharing.

Due to security holes, for about a decade now, SMBv1 has been replaced by SMBv2 and newer versions and recently even disabled by default in most modern operating systems. 
That doesn't mean using SMBv1 will get you in trouble immediately, but it sure is not ideal. 
(Note, though, that many Asus routers might have the warning about enabling SMBv1, but they don't require it to work. 
In my experience, all Asus Wi-Fi 6 routers can work with newer SMB versions.)

Another security concern is when you use the NAS feature via the Internet. 
In this case, make sure you create an account for each user access. 
But if you're not sure, just don't turn on any “cloud” feature or FTP access. 
Use those only when you know what you're doing.

The bottom line is, if you want to do a lot of things with your network storage, it's a good idea to get a real dedicated <a href="https://dongknows.com/dong-ngos-most-important-gadget-is-a-synology-nas-server/" target="_blank">NAS server</a>. 
But if you only wish to use some casual network storage, it's quite fun and sensible to get even more use out of our router.
<h3>Get a good external drive</h3>
Any good external storage device, namely desktop or laptop (portable) USB drives, will work. 

You don't need to get a NAS-specific drive. 
So, if you want the fastest possible speed, get a fast SSD-based portable drive, such as one of those on this list. 
However, keep in mind that the performance depends on the network connection or the router's processing power.
That said, a fast external storage device doesn't always translate into better performance. 

In most cases, an affordable hard-drive-based portable drive, like the <a href="https://dongknows.com/wd-my-passport-2019-portable-drive-review/" class="rank-math-link">WD My Passport</a> or the <a href="https://dongknows.com/g-tech-g-drive-mobile-usb-c-review/" class="rank-math-link">G-Tech Mobile</a>, will do. 
Generally, a router USB port has enough juice to power one bus-powered drive. 
But you can also use desktop external drives that have a power adapter of their own. 
In this case, you can use one with each of a router's USB ports.

<a href="https://dongknows.com/best-portable-drives/">See also Best Portable Drives of 2021: The Ultra-secure, Extra-rugged, and Super-fast</a>
When it comes to storage space, the more, the better, so get the drive with the most capacity you can afford. 
If you're serious about your data, you can also choose an external drive with <a href="https://dongknows.com/why-you-would-want-a-synology-nas-server/#Redundancy_via_the_use_of_RAID_explained" target="_blank">redundancy</a>, such as a dual-drive RAID 1 external storage device, like the <a rel="noopener noreferrer nofollow" href="https://amzn.to/2lpFRrN" target="_blank">WD My Book Duo</a>.
<strong>Note: </strong>You will need to configure the hardware RAID setup <em>before</em> plugging it into the router. 

So do that on a computer first. 
<h3>Get the right router</h3>
Not all routers are equal, especially when it comes to raw power. 
That said, get a router that has a lot of processing power. 

Generally, the higher the specs, the better.<a href="https://dongknows.com/usb-c-vs-thunderbolt-3-explained/">See also Device Connections Explained: Thunderbolt or Not, It's All about USB-C</a>
Also, make sure you get a router that supports <a href="https://dongknows.com/peripheral-connection-explained-usb-c-vs-thunderbolt-3/" target="_blank">USB 3.2 Gen 1</a>, a.k.a USB 3.0, or faster. 
Some router also has an eSATA or USB-C port. 
So, find one that suits your needs. 

And finally, get the router that includes the storage features you want, such as the <a href="https://dongknows.com/how-to-turn-your-wi-fi-router-into-a-time-capsule/" target="_blank">support for Time Machine backup</a>.
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2019/01/Asus-RT-AX88U-USB-mode-1024x858.png">
Router USB port: Steps to make an Asus Wi-Fi router USB port work in USB 3.0 (a.k.a USB 3.2 Gen 1) mode.
<h3>Use the correct settings</h3>

By default, many routers -- especially those from Asus and Synology -- automatically set the connected drive to work in USB 2.0 mode. 
This mode won't affect the router's NAS functionality but has a theoretical cap speed of just 480 Mbps (60 MB/s) -- the real-world rate will be just about half of that. 
The USB 3.2 Gen 1 (formerly USB 3.0) mode, which the cap of 5 Gbps (625 MB/s), unfortunately, can adversely affect the router's 2.4GHz Wi-Fi band. 
That said, if you want to get the most out of the router's storage feature, you'll need to enable the faster USB mode manually -- we use mostly the 5GHz band these days anyway.

Also, make sure you use the external drive with the right setting. 
For one, use it in the <a href="https://dongknows.com/file-system-explained-and-how-to-format-your-drive/" target="_blank">correct file system</a> that the router supports. 
Most, if not all routers, support NTFS. 
<a href="https://dongknows.com/disk-partition-and-file-system-explained/">See also File System and Partition Explained: Take Control of Your Storage</a>

By the way, it's worth noting that you only need to use the file system that the router supports, and not the one your computer supports. 
That's because the file system used by the server has nothing to do with the client. 
So, for example, if you use an NTFS (Windows) external drive with your router and share its storage, over the network, your Mac will be able to read, write to the shared folder, and use the space for Time Machine backup (if supported) just fine. 
Finally, don't turn on the external storage device's security feature if it has one. 

A router has no mechanism to unlock it.
<h3>How to access your router-based NAS server</h3>
Once you've connected a storage device to a router and turned on the data sharing feature -- often referred to as Windows-based, or SAMBA (SMB), file sharing -- it's easy to access that share space from any computer within the network. 
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2021/01/Asus-RT-AX92U-USB-Features-836x1024.jpg">

The USB-related features of an Asus router. 
For NAS, the Samba option is in the second (Servers Center) from top.
A couple of things to note here:
<ul><li>Depending on the router, there might be more features than just data sharing. 

Another popular option is the media server -- where the router shares video and audio files stored on the connected drive via media streaming protocol. 
In this case, just follow the instruction to turn the desired feature on.</li><li>Here I assume you know how to set up a router, access its web interface, etc -- enabling the NAS feature is part of working with the router's interface. 
If not, this <a aria-label="undefined (opens in a new tab)" rel="noreferrer noopener" href="https://dongknows.com/home-wi-fi-router-setup/" target="_blank">post on how to build a network from scratch</a> will help you with that.</li></ul>
But data sharing is the most useful and popular, and I'll cover it here. 

It's fairly easy. 
To make it work, the only next thing you need is the router's IP address, which is the same one you've used to access its interface. 
Alternatively, you can also use the router's network name. 
But the IP is always the <em>sure</em> way. 

For this post, let's say the IP address in question is <em>192.168.1.1</em>. 
(Chances are yours is a different one. 
If you don't know what it is, this post on IP addresses includes detailed steps to figure that out.)
Once you've got the IP address, the steps below are the standard ways to access your newly-minted NAS server from a Windows or Mac computer within your local network hosted by the router.

<h3>Accessing your NAS server on a Windows computer</h3>
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2020/07/Access-Server-Windows.png">
You can access the shared folder via the router's IP address.
1. 

Open Explorer.
2. 
On the address bar type in this command then press Enter:
<strong><em>\\192.168.1.1</em></strong>

(Alternatively, you can also use <strong>\\RouterName</strong> and the Windows search field under the Start Menu instead of Explorer. 
Don't forget the \\ (<strong>not</strong> //) and remember there's no space in the command.)
3. 
Enter the username and password if prompted. 

If you haven't set up an account for the data sharing, or if the router doesn't support that, you can just use the<em> admin username and password of the router</em>‘s web interface.
<h3>Accessing your NAS server on a Mac</h3>
<img class="lazy" data-src="https://dongknows.com/wp-content/uploads/2020/07/Access-Server-Mac-1024x768.jpg">
1. 

Click on an empty spot on the desktop then press&nbsp;<em>Command + K</em>, and the “Connect to Server” window will appear.
2. 
Under <em>Server address</em>&nbsp;type in
<strong><em> smb://192.168.1.1&nbsp;</em></strong>

(Again, you can also substitute the IP address with the router's network name.)
3. 
Click on <em>Connect</em> and enter username and password (of the account you've created or the router's admin account) if prompted.
And that's it. 

Happy data sharing!
<h3>Best USB-enabled routers that can work as a NAS server</h3>
Now that you know how to turn a USB-enabled router into a NAS server, you probably wonder which router or routers you should get for the job. 
I addressed that big question in this separate, frequently updated post on <a href="https://dongknows.com/best-wi-fi-routers-for-nas/">the best Wi-Fi routers for NAS features</a>. 

Check it out!
<a href="https://dongknows.com/best-wi-fi-router-nas-solutions/">
See also Best 13 Router NAS Options: Add Some Cool Storage to Your Wi-Fi Today!</a>
By the way, again, many routers can also work as a <a href="https://dongknows.com/all-you-need-to-know-about-macs-time-machine-backup/" target="_blank" rel="noreferrer noopener">Time Machine backup</a> destination. 
For more, check out this post on how to <a href="https://dongknows.com/how-to-turn-your-wi-fi-router-into-a-time-capsule/">turn a router into a Time Capsule.</a>

'<h2>Wi-Fi Direct</h2><br>Windows 10 boasts another feature that most people don\'t know about, called <strong>Wi-Fi Direct</strong>, a wireless connectivity system that helps you effortlessly hook devices up and transfer huge amounts of data.<br><br><h3>How Does Wi-Fi Direct Work?</h3><br>For Wi-Fi Direct technology to work, you\'ll need at least a single device that\'s compatible with its protocols.3<br><br>Wi-Fi Direct is built on top of Wi-Fi. <br>The only thing that separates it from regular Wi-Fi is that while you need a router to connect your devices to the internet, Wi-Fi Direct doesn\'t have any limitations.<br><br><h3>Check If Your Windows 10 PC Is Wi-Fi Direct Compatible</h3><br>You can do this by pressing<strong> Windows Key +R</strong>, entering <strong>CMD</strong> to <a href="https://www.makeuseof.com/tag/a-beginners-guide-to-the-windows-command-line/">open the Command Prompt</a> then entering <strong>ipconfig /all</strong>.<br><img src="https://static1.makeuseofimages.com/wordpress/wp-content/uploads/2021/06/windows-command-prompt.png"><br>If Wi-Fi Direct is available, you should see an entry labeled <strong>Microsoft Wi-Fi Direct Virtual Adapter.</strong><br><br>Next, you\'ll need to start transferring data over Wi-Fi Direct. <br><br><h3>How to Transfer Files From Android to Windows With Wi-Fi Direct</h3><br><br>As you\'ll need a third-party app to use Wi-Fi Direct, choosing the right option is important.<br><a href="https://feem.io/">Feem</a> is a software that has provided Wi-Fi Direct support to Windows PC and laptop users since the days of Windows 7 and Windows 8.<br>Feem is free to use, although it has various premium options. <br><br>Wi-Fi Direct in Feem is free, as is live chat. <br><br>Set your Android device as a mobile hotspot via <strong>Settings &gt; Network &amp; Internet &gt; Hotspot &amp; tethering</strong>. <br>Connect your Windows computer to this network.<br>Launch Feem on Android and Windows. <br><br>You\'ll notice that both devices are given unusual names by the app (e.g., Junior Raccoon) and a password. <br>Keep a note of the password, as you\'ll need it to establish the initial connection.<br>Send a file from Android to Windows using Wi-Fi Direct, choose the destination device, and tap <strong>Send File</strong>. <br><br>Browse for the file or files, then tap <strong>Send</strong>.<br>Moments later, the data will be sent to your PC. <br>It\'s as simple as that—and it works backwards, too.<br><br><strong>Download:</strong> <a href="https://feem.io/#download">Feem</a> (for Windows, macOS, Linux, Android, iOS, Windows Phone)<br><h3>Don\'t Have Wi-Fi Direct? Transfer Files With Bluetooth!</h3><br>If your devices don\'t support Wi-Fi Direct, a smart solution (in the absence of a USB cable) is Bluetooth. <br><br>This is particularly useful if you\'re trying to use Wi-Fi Direct on Windows 7 or 8 and find that the feature isn\'t there or it doesn\'t work.<br><br>First, ensure your computer is paired to a suitable Bluetooth device (phone, tablet, computer, etc.) before sending a file to it. <br>The methodology for this is largely the same across devices and requires that both are set to "discoverable."<br><br>Both devices will then search for one another and, if successful, connect following input of a confirmation code.<br>For more information, here\'s a list of how you can <a href="https://www.makeuseof.com/tag/transfer-files-android-pc/">transfer data between a PC and Android</a>.<br>If you\'re not sure where the controls for Bluetooth can be found on your Windows 10 computer, open <strong>Settings &gt; Devices. <br><br></strong>After you\'re in the <strong>Bluetooth &amp; other devices</strong> section, turn on the Bluetooth, and pair your device with the computer. <br>For that, click on <strong>Add Bluetooth or other device </strong>and go ahead with the pairing up.<br><br>Then click on <strong>Send or receive files via Bluetooth &gt; Send Files. <br><br>Next, select</strong> a device that you want to share files with, choose the file to be sent, and click on <strong>Next </strong>to go ahead with the transmission.<br><img src="https://static1.makeuseofimages.com/wordpress/wp-content/uploads/2021/06/send-a-file-by-bluetooth.png"><br>On sending the file, the device receiving your data file will ask you to confirm that you wish to save the data. <br><br>Agree to this, and wait for the transfer to complete.<br>Note that due to the shorter range of Bluetooth, the best results will be enjoyed by keeping both devices close together.<br><h3>No Wi-Fi Direct? Transfer Files From Android to Windows PC With FTP</h3><br><br>FTP is another handy file transfer option for Android users attempting to transfer files to their Windows 10 PC (or other operating systems, for that matter).<br><a href="http://www.estrongs.com/#/">ES File Explorer</a> is a popular third-party <a href="https://www.makeuseof.com/tag/es-file-explorer-the-best-file-manager-for-android-android/">file manager for Android</a>. <br>This comes with several file management features for local and network use. <br><br>Among these is FTP, which provides a direct network connection between two devices.<br>Use ES File Explorer\'s <strong>Network &gt; FTP</strong> feature to display your Android device\'s IP address.<br>Paste this into a file transfer program such as <strong>FileZilla</strong> to browse the contents. <br><br>You can then effortlessly transfer files between the two devices.<br>So, try ES File Explorer if you want to transfer data from a mobile device to your laptop through Wi-Fi and don\'t have Wi-Fi Direct.<br><h3>Data Transfer Speeds: Which Is Best?</h3><br><br>You will probably notice while trying these two methods that Wi-Fi Direct is considerably quicker than Bluetooth. <br>Indeed, recent tests have demonstrated that <a href="https://www.dignited.com/23330/bluetooth-5-vs-wifi-direct-which-is-the-best-for-sharing-files-between-smartphones/#:~:text=Bluetooth%20is%20a%20peer%2Dto,a%20phone%20for%20a%20while.&amp;text=With%20WiFi%20Direct%2C%20you%20can,point%20or%20Router%20or%20Mifi.">Bluetooth speed is like a tortoise in comparison</a>.<br>While Wi-Fi Direct isn\'t quicker than any cable data transfer (such as USB 2.0 or USB 3.0), it is certainly capable of transferring a 1.5 GB file within 10 minutes; in contrast, Bluetooth takes almost 125 minutes to shift the same data.<br>',

<h2>The share option is greyed out</h2>
From the Properties window, set the Advanced Shared Permissions
a. 
From the Properties window, Click the Advanced Sharing button.
b. 
The Advanced Sharing window appears. 
First, select the check box beside the Share this folder option. 
Next, you need to type in a share name. 
By default, Windows 7 uses the name of the folder as the share name, but you can change the name by typing a new name in the Share name field.
c. 
Now you’re ready to set the permissions. 
Click the Permissions button from the Advanced Sharing window
d. 
Select the Everyone group in the Group or user names list, and then click Remove. 
Click Add to display the Select users or groups window. 
Within the Enter the object names to select text box, type the name of the user or users you want to give permission to access the shared folder (separate multiple usernames with semicolons). 
Click OK when you’re done to return to the Permissions window.
e. 
Next select the appropriate user in the Group or user names list. 
You can now use the permissions list to allow or deny one of the following permissions: Read, Change, or Full Control. 
Click OK if you’re done. 
If you want to assign permissions to additional users or group, repeat these steps to assign them the appropriate permissions.
f. 
Click OK to return to the Advanced Sharing window and click OK again to return to the Sharing tab. 
Finally, when you click Close, the folder or file is accessible to others on the network.
g. 
Click OK to return to the Sharing tab, and then click Close to share the resource with the network.


<h2>Find all Wi-Fi passwords with only 1 command</h2>
netsh wlan show profile
netsh wlan export profile folder=C:\ key=clear


<h2>Github Actions</h2>
<div id="GitActionstoc"><a href="#GitActionstopic-0" target="_self" >Terminology</a><br><a href="#GitActionstopic-1" target="_self" >Documentation</a><br><a href="#GitActionstopic-2" target="_self" >Experimenting</a><br><a href="#GitActionstopic-3" target="_self" >Workflows</a><br><a href="#GitActionstopic-4" target="_self" >Events</a><br><a href="#GitActionstopic-5" target="_self" >Environment</a><br><a href="#GitActionstopic-6" target="_self" >Example Job</a><br><a href="#GitActionstopic-7" target="_self" >Environment Variables</a><br><a href="#GitActionstopic-8" target="_self" >Secrets</a><br><a href="#GitActionstopic-9" target="_self" >Third Party Actions</a><br><a href="#GitActionstopic-10" target="_self" >Conditions</a><br><a href="#GitActionstopic-11" target="_self" >Persisting Data</a><br><a href="#GitActionstopic-12" target="_self" > Using shared job data to determine if subsequent job should run</a><br><a href="#GitActionstopic-13" target="_self" > Persist data using <code>strategy.matrix</code></a><br><a href="#GitActionstopic-14" target="_self" > Clarify the cache action</a><br><a href="#GitActionstopic-15" target="_self" >Reusable Workflows</a><br><a href="#GitActionstopic-16" target="_self" >Conclusion</a><br></div>

<a href="https://github.com/features/actions">GitHub Actions</a> makes it easy to automate all your software workflows, now with world-class CI/CD. 
Build, test, and deploy your code right from GitHub.
I've been using GitHub Actions a lot recently and I've found it to be immensely flexible and feature rich. 
I think it's well worth your time learning how to run your CI/CD pipelines via GitHub Actions, and in this post that's exactly what we're going to dig into.
<h3 id="GitActionstopic-0">Terminology</h3>
It all starts with a 'workflow'.
A workflow is a yaml configuration file that defines:
<strong>Jobs</strong>: a job represents a collection of 'steps'.
<strong>Steps</strong>: each 'step' does something useful (e.g. executes a piece of code).
<strong>Events</strong>: an event determines when your 'jobs' should run.
GitHub has a nice visualisation of this..
<img src="https://www.integralist.co.uk/images/overview-actions-simple.png">
<strong>NOTE</strong>: Each job you define will run in parallel. 
If you need jobs to run sequentially, then you'll need to configure a job to depend on another job using the <code>needs</code> property (we'll see an example of this later).
<h3 id="GitActionstopic-1">Documentation</h3>
I would strongly suggest bookmarking the <a href="https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions">official documentation</a> because it's very thorough. 
Unfortunately it's so thorough that it can be a bit overwhelming, but don't worry, once you've gotten familiar with the various concepts you'll start to remember where the important information is located.
The pages I use the most are:
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/expressions">Expressions</a></strong>: explains how to use GitHub's builtin functions, like <code>contains()</code>, <code>fromJSON()</code>, and functions like <code>success()</code> and <code>failure()</code> that tell you the state of previous steps that have run.
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/contexts">Contexts</a></strong>: there are lots of contextual objects your jobs can use, such as objects for getting information about git (what branch we're dealing with, the commit SHA etc), environment variables, secrets, data exposed by other jobs and lots more.
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions">Syntax</a></strong>: this is the most important page as it details all the yaml configuration syntax (so I come here often).
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/workflow-commands-for-github-actions">Commands</a></strong>: when executing a shell command (or script) you can use <code>echo</code> with a specific format and, depending on the format used, it'll take on special meaning to the workflow runner and can be used (among other things) for displaying rich messaging in the GitHub UI.
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/environment-variables">Environment variables</a></strong>: most of the 'context' properties are also exposed as environment variables to make it easier for your shell commands/scripts to utilise.
<strong><a href="https://docs.github.com/en/actions/learn-github-actions/reusing-workflows">Reusing workflows</a></strong>: If you have a bunch of jobs all doing a similar thing (e.g. you've three jobs and each of them setup a specific set of environment variables before installing the rust programming language), then you can move that duplication into a separate workflow file that your main workflow can call out to.
<h3 id="GitActionstopic-2">Experimenting</h3>
The best thing to do when it comes to learning GitHub Actions is to create a test repo as a playground, like I did here: <a href="https://github.com/Integralist/actions-testing">https://github.com/Integralist/actions-testing</a>.
You can of course use an existing repo if you want, but I prefer to use something completely decoupled when I'm testing a new tool.
<h3 id="GitActionstopic-3">Workflows</h3>
As I mentioned earlier: everything starts with a workflow.
To create a workflow you must save a yaml file into the <code>.github/workflows</code> directory of your project's git repo.
In the following example you'll see I have created three separate workflows:
<code>├── .github
│   └── workflows
│       ├── my-first-workflow.yaml
│       ├── my-second-workflow.yaml
│       └── my-third-workflow.yaml
</code>
You can create as many workflows as you need to, and you can name a workflow file anything you like.
<h3 id="GitActionstopic-4">Events</h3>
Once we create a workflow file, let's see how we can trigger any jobs that are defined within the workflow. 
Be aware that in the following example workflow I have not defined any jobs, I'm focusing on the events configuration only:
<code>name: My Workflow
on:
  push:
  schedule:
    - cron: "0 0 1 * *"   # https://crontab.guru/every-month
    - cron: "*/5 * * * *" # https://crontab.guru/every-5-minutes
</code>
So we can see I've given my workflow a name using the <code>name</code> key, and I've defined some events using the <code>on</code> key:
<code>push</code>: any push to your repo, whether it be to your <code>main</code> branch or a pull-request branch, will trigger your job(s) to run.
<code>schedule</code>: run your job(s) on a schedule using cron syntax, which in this example triggers job(s) every five minutes and monthly.
<strong>NOTE</strong>: <a href="https://crontab.guru/">https://crontab.guru/</a> makes dealing with cron syntax easy.
Refer to GitHub's <a href="https://docs.github.com/en/actions/learn-github-actions/events-that-trigger-workflows">events documentation</a> to learn more about the various events you can configure. 
For example, you can restrict a workflow to only execute against a specific branch.
<h3 id="GitActionstopic-5">Environment</h3>
Each job runs inside its own virtual machine 'runner', which means (just as an example) files you create, or data you produce, will not persist across jobs. 
This includes things like environment variables.
<h3 id="GitActionstopic-6">Example Job</h3>
Let's take a look at a very simple job:
<code>name: Test Workflow
on: push
jobs:
  simple-job:
    runs-on: ubuntu-latest
    steps:
      - name: Say Hello
        run: echo 'hello'
</code>
Let's now break apart this workflow to understand what it's doing..
<code>name</code>: the name for the workflow.
<code>on</code>: the event(s) we want to have trigger our job(s).
<code>jobs</code>: a list of jobs we want to be executed under this workflow.
<code>simple-job</code>: a job that consists of nested configuration.
<code>runs-on</code>: the environment I want the job to run in.
<code>steps</code>: a list of steps to run under the job.
<code>name</code>: the name of the step, which can be omitted but it makes the output in the GitHub UI nicer.
<code>run</code>: the shell command/script I want to run (in this example, my command prints the string <code>hello</code>).
<strong>NOTE</strong>: Refer to the GitHub <a href="https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#github-hosted-runners">runner documentation</a> to see what other environments are available. 
Also refer to the <a href="https://github.com/actions/virtual-environments">virtual-environments</a> repo to see what is installed on each runner's operating system.
As you can see, defining a workflow and its jobs/steps is actually very simple and intuitive. 
Now we can start looking at using more features and how to take advantage of the platform.
<h3 id="GitActionstopic-7">Environment Variables</h3>
GitHub Actions let you configure environment variables in multiple places depending on the scope they should have.
You can define environment variables globally, so they're available to all jobs (and all steps within those jobs), or at a job level (so they can be accessed by all steps within the specific job) or at a step level, meaning only a specific step will have access to them.
I'll demonstrate environment variable configuration in the following example:
<code>name: Testing Environment Variables
on: push
env:
  FOO: bar
jobs:
  validate-env-vars:
    runs-on: ubuntu-latest
    env:
      LITERAL: whatever
      INTERPOLATION: ${{ github.ref_name }}
      EXPRESSION: $(echo ${{ github.ref_name }} | perl -pe 's/[^a-zA-Z0-9]+/-/g' | perl -pe 's/(\A-|-\Z)//g' | awk '{print tolower($0)}')
    steps:
      - name: Print Global Environment Variables
        run: echo $FOO
      - name: Print Job Environment Variables
        run: |
          echo ${{ env.LITERAL }}
          echo ${{ env.INTERPOLATION }}
          echo ${{ env.EXPRESSION }}
          echo $LITERAL
          echo $INTERPOLATION
          echo $EXPRESSION          
      - name: Print Step Environment Variables:
        env:
          STEPVAR: my step
        run: |
          echo ${{ env.STEPVAR }}
          echo $STEPVAR          
</code>
OK, so there's a few things to unpack from the above example workflow.
The first thing I want to clarify is that when running shell commands/scripts using <code>steps.run</code> you can either define multiple steps like so:
<code>- run: echo hello
- run: echo world
</code>
Or alternatively you can use the pipe <code>|</code> character to indicate the value spans multiple lines, like so:
<code>- run: |
  echo hello
  echo world  
</code>
I would tend towards using separate <code>run</code> steps rather than one long multi-line <code>run</code> because it's harder to handle errors (or know where an error occurred) in the latter approach.
The next thing to clarify is that there are two ways to access an environment variable:
<ol>
<strong>Interpolation</strong>: <code>${{ ... 
}}</code> (e.g. <code>echo ${{ env.LITERAL }}</code>).
<strong>Variable reference</strong>: <code>$VARNAME</code> (e.g. <code>echo $LITERAL</code>).
</ol>
It's important to understand that although the output between the two approaches is the same, there is still a distinction worth being aware of.
With interpolation the value is acquired by looking up the relevant key within the <a href="https://docs.github.com/en/actions/learn-github-actions/contexts#env-context"><code>env</code> context object</a>, and then the value is injected into the shell command to be executed, while the more traditional variable reference approach works by the shell instance looking up the environment variable to access the value.
So when you look at the GitHub Actions GUI (e.g. <code>https://github.com/&lt;USER&gt;/&lt;REPO&gt;/actions/runs/&lt;ID&gt;</code>) and you look through the job output for the <code>LITERAL</code> example, then you'll see something like:
<code data-lang="bash">echo whatever
echo $LITERAL
</code>
This is because the first line used interpolation, so really the shell command was just echo'ing the literal value, where as the second line was echo'ing the <em>result</em> of looking up the environment variable.
In our example workflow you can see we defined a global environment variable called <code>FOO</code>, we also defined a job level set of environment variables (<code>LITERAL</code>, <code>INTERPOLATED</code> and <code>EXPRESSION</code>), and finally we defined a step level environment variable called <code>STEPVAR</code>.
Let's take a moment to clarify the job level environment variables as these demonstrate something important, which is although the <code>EXPRESSION</code> variable was assigned a shell command, that command isn't <em>evaluated</em> and so the literal characters are assigned as the value.
If you thought the <em>result</em> of the shell command (e.g. acquire the branch name from the <code>github</code> context object, and then do some normalisation of the name using a combination of <code>perl</code> and <code>awk</code>) would be assigned to the environment variable, then you would have been wrong.
Only the <a href="https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idstepsrun"><code>steps.run</code></a> key will actually evaluate a given shell command/script.
Now if the only way to evaluate a shell command is via <code>steps.run</code>, then how can we dynamically assign a value to an environment variable?
Consider this example, you're a <a href="https://nodejs.org/en/">Node.js</a> developer and you're using the Node version manager <code>nvm</code> along with a <a href="https://github.com/nvm-sh/nvm#nvmrc"><code>.nvmrc</code> file</a> to control which version of Node your project uses.
You're also using GitHub actions and you want to use a pre-existing action to manage installing Node on your job runner (we'll dig into third-party actions later, but for now just know that they are a thing).
This is where things get a little funky, because third-party actions can't evaluate shell commands given as inputs. 
So the most popular third-party action for installing Node is <a href="https://github.com/actions/setup-node"><code>actions/setup-node</code></a> and it allows you to specify the version of Node to install but it has to be a literal value.
The following example demonstrates how to side-step this restriction by using a <code>steps.run</code> to dynamically access the value inside the <code>.nvmrc</code> file and to then update the local environment with a new variable holding that value. 
This means you can then use interpolation to access that value and pass it into the <code>actions/setup-node</code> action:
<code>name: Testing Dynamic Environment Variable Value
on: push
jobs:
  validate-env-vars:
    runs-on: ubuntu-latest
    steps:
      - name: Generate Dynamic Environment Variable
        run: echo "NODE_VERSION=$(cat .nvmrc)" &gt;&gt; $GITHUB_ENV
      - name: Print NODE_VERSION
        run: |
          echo ${{ env.NODE_VERSION }}
          echo $NODE_VERSION          
      - uses: actions/setup-node@v2
        with:
          node-version: "${{ env.NODE_VERSION }}"
</code>
The trick is to update a GitHub Actions provided environment variable called <code>$GITHUB_ENV</code>. 
All the following steps will then have an environment that includes whatever is in <code>$GITHUB_ENV</code>.
<strong>NOTE</strong>: You could also use the output of a step as the input to the <code>actions/setup-node</code> action, but we'll look at that feature later.
<h3 id="GitActionstopic-8">Secrets</h3>
The previous Node.js example workflow actually leads us quite nicely into this section about <a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets#using-encrypted-secrets-in-a-workflow">secrets</a>.
Let's see a common solution to accessing a private NPM repository..
<code>name: Testing GITHUB_TOKEN access restrictions
on: push
jobs:
  testing:
    runs-on: ubuntu-latest
    steps:
      - name: Acquire Node Version
        run: echo "NODE_VERSION=$(cat .nvmrc)" &gt;&gt; $GITHUB_ENV
      - uses: actions/setup-node@v2
        with:
          node-version: "${{ env.NODE_VERSION }}"
      - name: Authorize access to private packages
        run: echo "//npm.pkg.github.com/:_authToken=${{ secrets.GITHUB_TOKEN }}" &gt; ~/.npmrc
</code>
What we're doing here is installing Node.js and then modifying a <code>.npmrc</code> file so that it knows, when we do an <code>npm install</code>, to use an authentication token because the repository we want to use to get at our dependencies is otherwise private.
You'll see we're using a <a href="https://docs.github.com/en/actions/security-guides/encrypted-secrets"><code>secrets</code> context object</a> to access a <code>GITHUB_TOKEN</code> value to use as our authentication token. 
This should be fine, but it doesn't work and our authentication with the private NPM repository fails.
Let's take a moment to learn a bit about GitHub 'secrets'..
In the GitHub UI for a repo/project you can add secrets that are encrypted and made accessible to your workflow via the <code>secrets</code> context object. 
But additionally GitHub Actions provides access to a secret called <a href="https://docs.github.com/en/actions/security-guides/automatic-token-authentication#about-the-github_token-secret"><code>GITHUB_TOKEN</code></a>.
I'll just refer you to the GitHub documentation, as it explains it best:
At the start of each workflow run, GitHub automatically creates a unique GITHUB_TOKEN secret to use in your workflow. 
When you enable GitHub Actions, GitHub installs a GitHub App on your repository. 
The GITHUB_TOKEN secret is a GitHub App installation access token. 
You can use the installation access token to authenticate on behalf of the GitHub App installed on your repository. 
The token's permissions are limited to the repository that contains your workflow.
That last sentence is the important bit! What essentially it means is that you can't use <code>secrets.GITHUB_TOKEN</code> to access things outside of the project repo.
So to solve our problem we still need to use the <code>secrets</code> context object, but instead of using the default <code>GITHUB_TOKEN</code> we're going to need to create a new <a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token">Personal Access Token</a> (PAT).
Once you create a PAT, as long as the user account that creates the PAT has access to the private repository, we can paste the PAT into the GitHub secrets UI.
For the sake of an example let's say you create a new secret called <code>NPM_AUTH_TOKEN</code> and paste the PAT value into it. 
We can now reference the secret token value via the <code>secrets</code> context object in our workflow file:
<code>name: Testing with Personal Access Token
on: push
jobs:
  testing:
    runs-on: ubuntu-latest
    steps:
      - name: Acquire Node Version
        run: echo "NODE_VERSION=$(cat .nvmrc)" &gt;&gt; $GITHUB_ENV
      - uses: actions/setup-node@v2
        with:
          node-version: "${{ env.NODE_VERSION }}"
      - name: Authorize access to private packages
        run: echo "//npm.pkg.github.com/:_authToken=${{ secrets.NPM_AUTH_TOKEN }}" &gt; ~/.npmrc
</code>
Notice that the workflow file basically hasn't changed, except we swapped <code>secrets.GITHUB_TOKEN</code> for <code>secrets.NPM_AUTH_TOKEN</code>.
<h3 id="GitActionstopic-9">Third Party Actions</h3>
An action is just code that interacts with your repository. 
It's possible to write your own custom actions and to share them with the community.
If you want to learn more about creating your own actions, then refer to GitHub's “<a href="https://docs.github.com/en/actions/creating-actions/about-custom-actions">About custom actions</a>”.
There are a bunch of third-party actions that you'll see used a lot..
<a href="https://github.com/actions/checkout"><code>actions/checkout</code></a>
<a href="https://github.com/actions/cache"><code>actions/cache</code></a>
<a href="https://github.com/actions/setup-go"><code>actions/setup-go</code></a>
<a href="https://github.com/actions/setup-node"><code>actions/setup-node</code></a>
<a href="https://github.com/actions-rs/toolchain"><code>actions-rs/toolchain</code></a>
All of the above actions, with the exception of the last, are official GitHub maintained actions. 
This means they are considered safe to use in your workflows (remember that an action runner will be able to use your <code>secrets.GITHUB_TOKEN</code>). 
See also <a href="https://github.com/actions">https://github.com/actions</a> for more official/verified actions you can use.
<strong>NOTE</strong>: I've no idea why they don't provide a Rust action.
<h3 id="GitActionstopic-10">Conditions</h3>
You can use an <code>if</code> conditional statement to control whether a job or step is run. 
It's best to use this feature if you need to <em>skip</em> a job/step apposed to using an exit code from within a <code>run</code> shell command/script to stop a job/step that has already started to run.
One important caveat to using <code>if</code> in a workflow is that you <em>must</em> use single quotes and not double quotes. 
Consider the following example, which you might expect to only run the job if the GitHub branch affected is <code>main</code>..
<code>name: Testing with Double Quotes
on: push
jobs:
  run-if-main:
    if: ${{ github.ref_name == "main" }}
    runs-on: ubuntu-latest
    steps:
      - run: echo hello
</code>
<strong>NOTE</strong>: Expressions can omit the surrounding <code>${{ ... 
}}</code> but I tend to include it.
The above example won't work simply because the string <code>"main"</code> is using double quotes. 
You'll find the requirement for using single quotes is mentioned in the GitHub documentation for “<a href="https://docs.github.com/en/actions/learn-github-actions/expressions#literals">Literals</a>”.
If you want to learn more about the available operators, like <code>==</code>, <code>!=</code> and <code>&amp;&amp;</code> etc, then refer to the <a href="https://docs.github.com/en/actions/learn-github-actions/expressions#operators">operators documentation</a>.
<h3 id="GitActionstopic-11">Persisting Data</h3>
A GitHub Action job does not persist data by default, as each job runs within its own 'runner'. 
To persist data we can use either a:
<ol>
Cache (<a href="https://github.com/actions/cache"><code>actions/cache</code></a>)
Artifact (<a href="https://github.com/actions/upload-artifact"><code>actions/upload-artifact</code></a>, <a href="https://github.com/actions/download-artifact"><code>actions/download-artifact</code></a>)
Job Output (<a href="https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idoutputs"><code>outputs</code> documenation</a>)
</ol>
Caching is quick but isn't always ideal as it requires the use of I/O to create files to be cached, and then also we have to coordinate reading values back from disk and parsing the data contained within those files etc. 
Caching is best used for simple situations such as caching installed programming language dependencies.
Artifacts are slow as they need to upload and download files from GitHub's servers and also this requires two separate external actions to configure, making them not ideal for quick data persistence (and also makes 'simple' data persistence scenarios tedious).
To persist data using a job's output requires a job to produce some data and to expose that data via the job's <code>outputs</code> field. 
A consumer can then use and parse that data however they see fit. 
This approach can also be useful for dynamically generating job matrix variants using a job's <a href="https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions#jobsjob_idstrategy"><code>strategy.matrix</code></a> field.
<strong>NOTE</strong>: A job has an <code>outputs</code> field, but also individual steps can access the output from a previous step by way of the same mechanism, which is a step needs to set an <code>id</code> field which either another step or the job itself can reference.
The way your <code>run</code> code (or an external shell script) can produce data that the GitHub job can reference is to <code>echo</code> a specially formatted string:
<code data-lang="bash">echo "::set-output name=&lt;name&gt;::&lt;data&gt;"
</code>
So in the following example we produce the data <code>bar</code> and we make it accessible via the name <code>foo</code>:
<code data-lang="bash">echo "::set-output name=foo::bar"
</code>
Now in the following example our GitHub job has two steps, and the latter step is accessing data produced by the first step:
<code>name: Produce Data
on: push
jobs:
  example:
    runs-on: ubuntu-latest
    steps:
      - id: produce-data
        run: echo "::set-output name=foo::bar"
      - run: echo ${{ steps.produce-data.outputs.foo }}
</code>
<strong>NOTE</strong>: Be sure to use the <code>id</code> field so the second step can use it to reference the first step's output!
<h3 id="GitActionstopic-12"> Using shared job data to determine if subsequent job should run</h3>
In the following example the <code>bar</code> job will not run if the required fields <code>foo</code> and <code>baz</code> aren't set to <code>true</code>.
Notice the data I produce within job <code>foo</code> is a simple array/list whose elements are strings who have a format of <code>key=value</code>:
<code>name: Produce Data To Control Job Run
on: push
jobs:
  foo:
    runs-on: ubuntu-latest
    outputs:
      data: ${{ steps.footest.outputs.data }}
    steps:
      - run: |
          echo "FOO=true" &gt;&gt; $GITHUB_ENV
          echo "BAR=false" &gt;&gt; $GITHUB_ENV
          echo "BAZ=true" &gt;&gt; $GITHUB_ENV          
      - id: footest
        run: echo ::set-output name=data::[\"foo=$FOO\", \"bar=$BAR\", \"baz=$BAZ\"]
  bar:
    needs: foo
    if: ${{ contains(needs.foo.outputs.data, 'foo=true') &amp;&amp; contains(needs.foo.outputs.data, 'baz=true') }}
    runs-on: ubuntu-latest
    steps:
      - run: echo 'yay! we ran because the fields were set to true'
  build:
    needs: bar
    runs-on: ubuntu-latest
    steps:
      - run: echo 'yay! this job ran as the bar job was successful'
</code>
The key part to getting job <code>bar</code> to determine if it should run is to use an <code>if</code> along with one of GitHub's native functions called <a href="https://docs.github.com/en/actions/learn-github-actions/expressions#contains"><code>contains()</code></a>. 
You can see I use it twice to check if the persisted data contains the values I'm looking for.
<strong>NOTE</strong>: You'll likely want to use the <code>needs</code> property to help make the jobs run sequentially. 
Otherwise without it the jobs will run in parallel and this means there would be a data race in <code>bar</code> trying to access the <code>foo</code> job's output which might not yet be available and this would cause both the <code>bar</code> job and its dependant <code>build</code> job to not be run.
<h3 id="GitActionstopic-13"> Persist data using <code>strategy.matrix</code></h3>
Below is an example of using JSON data from <code>job1</code> and exposing it to <code>job2</code>. 
This approach of using <a href="https://docs.github.com/en/actions/learn-github-actions/expressions#fromjson"><code>fromJSON</code></a> with data from another job to create a matrix is really cool, but otherwise this approach (as far as data persistence is concerned) isn't ideal because it requires the JSON data produced by the first job to have a very specific structure that the <code>strategy.matrix</code> expects. 
So to reiterate I'm not using <code>strategy.matrix</code> in this example because I want a matrix but just to demonstrate a clever way to persist data without having to resort to using either caching or artifacts.
With this in mind I needed to ensure I set the value of each matrix field to be a <em>list</em> type that contains a <em>single</em> entry.
If I didn't use a list type then the <code>strategy.matrix</code> would fail to parse my JSON data. 
I purposely ensure there is only a single value within the list because I don't actually need my data to cause the job to be run multiple times. 
This is because a <code>strategy.matrix</code> is typically used to generate multiple 'variants' of a job. 
By setting a single value inside a list, we ensure there is only ever one job variant generated (i.e. only one job is created), and that single job can simply reference the fields within the matrix as data points of interest.
<code>name: example
on: push
jobs:
  job1:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: echo "::set-output name=matrix::{\"FOO\":["abc"],\"BAR\":["xyz"]}"
  job2:
    needs: job1
    runs-on: ubuntu-latest
    strategy:
      matrix: ${{fromJSON(needs.job1.outputs.matrix)}}
    env:
      FOO: ${{ matrix.FOO }}
      BAR: ${{ matrix.BAR }}
    steps:
      - run: echo ${{ matrix.FOO }} # abc
      - run: echo ${{ matrix.BAR }} # xyz
</code>
<h3 id="GitActionstopic-14"> Clarify the cache action</h3>
OK, so as far as data persistence is concerned, we have the <code>actions/cache</code> action we can use as an option. 
It's actually not very obvious how this action works and so I thought I'd take a brief moment to clarify my understanding, which then helped me to better understand how I could use it for data persistence outside of just caching language dependencies (which is what most examples are based on).
The <code>actions/cache@v2</code> works like so, you have to define a new step like so:
<code>- uses: actions/cache@v2
  with:
    path: path/to/be/cached
    key: ${{ runner.os }}-my-cache-key
</code>
When the step that implements the action is executed (see above snippet), the cache action simply looks up the cache key (e.g. <code>Linux-my-cache-key</code>) and if it finds something in the cache, then it restores the cache to the path you specified (e.g. <code>path/to/be/cached</code>).
If the cache action doesn't find anything in the cache, then nothing happens.
Now the important bit: the cache action has a 'post run' event that executes once your job has finished successfully. 
The cache action will be run again and this time it stores whatever was in your given path into the cache using the key you said it should be stored under.
This means, when it comes to running another job, you need to ensure you define the cache action <em>again</em> (the same as you defined it in your first job). 
This is so all of what I've just explained will happen again in your second job (i.e. it'll lookup the key but this time it'll find something in the cache thanks to the 'post run' step from the first job). 
The only difference now in the second job is that in the 'post run' event, when the action gets run again, you'll now see something like..
Cache hit occurred on the primary key <code>Linux-my-cache-key</code>, not saving cache.
Meaning there was nothing else to do. 
I imagine if there were changes to the files in the given path then it would indicate the cache was updated with the latest files.
<h3 id="GitActionstopic-15">Reusable Workflows</h3>
If you have a bunch of setup configuration that is the same between jobs, then you can move that configuration into a separate workflow file that can then be imported and used by each job in your main workflow file.
The following example, demonstrates how to <em>call</em> (i.e. import) a reusable workflow:
<code>jobs:
  build:
    ...
  deploy:
    ...
  validate-foo:
    uses: integralist/actions-testing/.github/workflows/resuable-setup@main # install node, rust, setup env vars etc
    steps: 
      - ...
  validate-bar:
    uses: integralist/actions-testing/.github/workflows/resuable-setup@main # install node, rust, setup env vars etc
    steps: 
      - ...
</code>
An example implementation of a reusable workflow would be:
<code>name: Reusable workflow for validation scripts
on:
  workflow_call:
    inputs:
      install_node:
        type: bool
      name:
        required: true
        type: string
      description:
        required: true
        type: string
      script:
        required: true
        type: string
jobs:
  validation:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - if: ${{ github.event_name != 'schedule' }}
        run: exit 1
      - if: ${{ inputs.install_node }}
        name: Environment
        run: |
                    echo "NODE_VERSION=$(cat .nvmrc)" &gt;&gt; $GITHUB_ENV
      - if: ${{ inputs.install_node }}
        uses: actions/setup-node@v2
        id: node-yarn
        with:
          node-version: "${{ env.NODE_VERSION }}"
          cache: yarn
      - name: status update
        uses: ouzi-dev/commit-status-updater@v1.1.2
        with:
          name: ${{ inputs.name }}
          description: ${{ inputs.description }}
          status: pending
      - id: validator
        run: ${{ inputs.script }}
      - if: ${{ success() }}
        name: status update
        uses: ouzi-dev/commit-status-updater@v1.1.2
        with:
          name: ${{ inputs.name }}
          description: ${{ steps.validator.outputs.description }}
          status: ${{ steps.validator.outputs.status }}
</code>
Notice that in the reusable workflow we have a new event <code>workflow_call</code> that helps us to define <em>inputs</em> for the job being called.
In this reusable job we define a bunch of inputs which helps us to control whether the steps in the job are run (making the reusable job even more flexible) by utilising a step's <code>if</code> condition and interpolating the input value.
For example, we don't always install the Node.js programming language, only if the caller requests it:
<code>- if: ${{ inputs.install_node }}
  uses: actions/setup-node@v2
  id: node-yarn
  with:
    node-version: "${{ env.NODE_VERSION }}"
    cache: yarn
</code>
Now one important consideration is that the reusable workflow will not inherit the parent workflow's environment. 
This means secrets and environment variables need to be passed in via either <code>workflow_call.inputs</code> or <code>workflow_call.secrets</code>.
Annoyingly you can't just set an input with <code>${{ env.FOO }}</code> because the <code>env</code> context object can't be referenced. 
So if your reusable job is used a lot then you either have to hardcode the value as an <code>input</code> to the reusable workflow or you store the environment variable as a secret in the GitHub UI so that you can reference it from the <code>workflow_call.secrets</code> property.
<h3 id="GitActionstopic-16">Conclusion</h3>
There is so much more to explore with GitHub Actions. 
The bits I've mentioned here are just the tip of the iceberg. 
I strongly recommend you read the documentation and have a play around with these, and other features. 
Let me know on twitter what you think of GitHub Actions and whether you're using it in your projects.


<h2>Using branches in Git</h2>
<div id="Usingbranchestoc"><a href="#Usingbranchestopic-0" target="_self">Using branches in Git</a><br><a href="#Usingbranchestopic-1" target="_self">Why and how branches are used in Git</a><br><a href="#Usingbranchestopic-3" target="_self">Pre-requisites:</a><br><a href="#Usingbranchestopic-4" target="_self">What are Git branches</a><br><a href="#Usingbranchestopic-5" target="_self">Commands used with branches</a><br><a href="#Usingbranchestopic-6" target="_self">Using branches for pull requests</a><br><a href="#Usingbranchestopic-7" target="_self">Resources:</a><br><a href="#Usingbranchestopic-8" target="_self">Glossary:</a><br><a href="#Usingbranchestopic-9" target="_self">Links:</a><br></div>
<h3 id="Usingbranchestopic-1">Why and how branches are used in Git</h3>
Version control systems like Git help manage changes to files. 
Sometimes, you’ll want (or need) to make some ‘feature’ or ‘patches’ to a collaborative research project. 
Or maybe you want to make some experimental changes to your code, but don’t want to touch your main code. 

This, and more, is where branches come into play.
In this code-along we’ll go over what branches are, and how and why you would use them.
<h3 id="Usingbranchestopic-3">Pre-requisites:</h3>
Obviously, have <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">Git installed</a>
Make sure to <a href="http://codeasmanuscript.org/lessons/git/cheatsheet/">configure your git</a>
<h3 id="Usingbranchestopic-4">What are Git branches</h3>
In very simple terms, git branches are individual projects within a git repository. 
Different branches within a repository can have completely different files and folders, or it could have everything the same except for some lines of code in a file.

Let’s use a few real world examples (at least that I’ve used before, others may have used them differently):
Pretend you submitted a research article to a journal and they want you to revise it based on some reviewers comments. 
There are several ways to deal with the comments, so instead of changing your main manuscript, you create a
<code>revision</code> branch in your manuscript git repository. 
In that branch you make the changes to your manuscript in response to the reviewers. 
Once you are satisfied,
you merge the branch into the <code>master</code> branch and resubmit the article.
Imagine you have a dataset that multiple people work off of but that is also often updated with more data. 

You think you found a problem with the dataset, but aren’t sure. 
So you create a new branch <code>fixing</code> to fix the problems without messing with the master dataset. 
After you confirm the problem is real and that you have the solution, you submit a pull request of the <code>fixing</code> branch to be merged with the <code>master</code> branch.
What is often the case in software development, a bug or missing feature in the software gets identified. 
Because the software is already in production use (fairly stable, other people rely on it, etc), you can’t just make changes to the main software code. 

So a <code>hotfix</code> or <code>feature</code> branch is created to address these problems, which will eventually get merged in with the <code>master</code> branch for the next version of the software. 
This ensures that other people’s code isn’t broken everytime a bug gets fixed.
There are many uses of branches in Git. 
The nice (and very powerful) thing about Git is the fact that branches are very cheap compared to other version control systems. 

By cheap, I mean they don’t take up much disk space, it’s computationally easy to move between branches, and it’s (relatively) easy to merge branches together. 
This is because of how Git represents branches, since they are simply <em>pointers</em> or an individual commit. 
<em>That’s it.</em> Just a pointer… Git commit history is a 
<a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graph</a>, 
which means that every single commit always has a ‘parent’ commit (the previous commit in the history, or multiple parents when a merge happens), and any individual commit can have multiple ‘children’. 
This history can be traced back through the ‘lineage’ or ‘ancestry’. 
The branch just gives a name to each ‘lineage’ when a commit has multiple children.

When you merge two branches together, the commit histories get merged together as well. 
Which means that all the changes you made in each branch gets combined back into a single lineage, rather than two. 
This makes it easier to work collaboratively on a project, since each individual could work on their own branches, without dealing with the messiness that could come from working all on one branch.
<h3 id="Usingbranchestopic-5">Commands used with branches</h3>
Branches are best understood visually. 
So let’s first start with using 
<a href="https://onlywei.github.io/explain-git-with-d3/">this website</a> to see what the 
<code>branch</code>, <code>checkout</code>, and <code>merge</code> commands are doing.
After we’ve tried that, let’s do it locally (on your own computer). 
Here is a sequence of commands to try out:
<div><div><code>cd ~/Desktop mkdir 
git-branches
cd git-branches 
git init # start a repo 
git add .
git commit -m "First commit" # make the first commit 
git branch testBranch # create branch 
git checkout testBranch # move to branch
## can also do git checkout -b testBranch
echo "Some text" > file.txt 
git add file.txt
git commit -m "Added a file with text"
git checkout master
echo "Text in another file" > new-file.txt 
git add new-file.txt 
git commit -m "Added another file"
git log --graph --oneline --decorate --all
# This command is long, so shorten it using aliases 
git config --global alias.lg 'log --graph --oneline --decorate --all'
git merge testBranch 
git lg 
git branch -d testBranch # delete the branch</code>

<h3 id="Usingbranchestopic-6">Using branches for pull requests</h3>
I mentioned this already, but branches are best used when doing a 
<a href="https://help.github.com/articles/using-pull-requests/">pull request</a> (unless the pull request is very small or few people work on the repository).
The steps to take would be:
Fork a repository on GitHub Clone it onto your computer Make a branch and move to it: <code>git checkout -b fixingBranch</code>
Make changes to the files Commit the changes to the history Push the branch up to your forked version: <code>git push origin fixingBranch</code>
On GitHub, submit a Pull Request of your <code>fixingBranch</code>
Once the pull request is merged, 
<a href="https://github.com/blog/1377-create-and-delete-branches">delete</a> 
the <code>fixingBranch</code> on your forked repo on GitHub and on your computer 
(<code>git checkout master &amp;&amp; 
git pull upstream master &amp;&amp; 
git branch -d fixingBranch</code>)

<h3 id="Usingbranchestopic-7">Resources:</h3>
If you have any questions, often one of the best places to start is either<br>
<a href="https://stackoverflow.com/questions/tagged/git">StackOverflow</a> or Google (which more likely links to StackOverflow).
<h3 id="Usingbranchestopic-8">Glossary:</h3>
<code>cd</code> - change directory directory - the same thing as a folder
<code>mkdir</code> - make a directory
<code>echo</code> - print a message to the screen or to a file if <code>></code> (redirect) is present.
<code>git init</code> - start or initialize a git repository
<code>git add</code> - put a file into the staging area, so that git starts tracking it
<code>git commit</code> - send files in the staging/index area into the history (the git repository)
<code>git log --graph --oneline --decorate --all</code> - view the commit history in the git repository and the branches, with each commit as one line.
<code>git branch</code> - An individual line of commit history that contains files that may differ from other branches.
<code>git checkout</code> - A way to move across the git commits and branches.
<code>git merge</code> - Combine a branch into the current checked out branch (i.e. the branch you are on).
<h3 id="Usingbranchestopic-9">Links:</h3>
<a href="https://pcottle.github.io/learnGitBranching/">Interactive, visual tutorial on branching</a>
<a href="https://www.atlassian.com/git/tutorials/using-branches/git-branch">Brief explanation of branching</a>


<h2>create an orphan branch</h2>
git checkout --orphan <branchname>
This will create a new branch with no parents.
Then, clear everything in the orphan branch

git rm --cached -r
or
git rm -rf

and add the documentation files, commit them and push them up to github.

to check log
git log

A pull or fetch will always update the local information about all the remote branches.
If you only want to pull/fetch the information for a single remote branch, you need to specify it.

To switch back to your master branch
git checkout master

return to the orphan branch
git checkout mybranch

added first file in the new branch
git commit -m

git push origin new_branch_name






<h2>What are SSH Keys?</h2>
<div id="SSHKeystoc"><a href="#SSHKeystopic-1" target="_self">The History of The SSH Protocol</a> <a href="#SSHKeystopic-2" target="_self">What are SSH keys?</a> <a href="#SSHKeystopic-3" target="_self">How User Keys Work</a> <a href="#SSHKeystopic-4" target="_self">First Steps – SSH Key Generation</a> <a href="#SSHKeystopic-5" target="_self">Linux/Mac Instructions:</a> <a href="#SSHKeystopic-6" target="_self">Windows Instructions:</a> <a href="#SSHKeystopic-7" target="_self">How SSH Key Authentication Works</a> <a href="#SSHKeystopic-8" target="_self">Managing SSH Keys</a> <a href="#SSHKeystopic-9" target="_self">Cloud IAM offers SSH Key Management</a> </div>
If you spend enough time in an IT environment and with the rise of cloud infrastructure such as AWS, you will likely come across the term SSH keys. 
If you’ve already come across this IT term, then you might find yourself wondering, what are SSH keys?

SSH (Secure Shell) keys are an access credential that is used in the SSH protocol and they are foundational to modern Infrastructure-as-a-Service platforms such as AWS, Google Cloud, and Azure.
Before this post delves into an explanation on what are SSH keys, let’s take a quick look at the SSH protocol.

<h3 id="SSHKeystopic-1">The History of The SSH Protocol</h3>
The first version of the SSH protocol was developed in the summer of 1995 by Tatu Ylonen. 
Tatu was a researcher at the University of Helsinki when a sniffing attack was discovered on the university network. 

A sniffing attack intercepts and logs the traffic that takes place on a network and can provide attackers with usernames and passwords which can then be used to gain access to critical IT assets.
Thousands of credentials were impacted, including those belonging to community partnerships. 
This sniffing attack motivated Tatu to figure out how to make networks more secure, and this ultimately led to the creation of the SSH protocol (<a href="https://youtu.be/OHBdKM7s5V4">SSH.com</a>).

Today, the SSH protocol is widely used to login remotely from one system into another, and its strong encryption makes it ideal to carry out tasks such as issuing remote commands and remotely managing network infrastructure and other vital system components. 
This is especially important in the era of cloud infrastructure and remote work.
To use the SSH protocol, a couple pieces of software need to be installed. 

The remote systems need to have a piece of software called an SSH daemon, and the system used to issue commands and manage the remote servers needs to have a piece of software called the SSH client. 
These pieces of software are necessary to create a proper communication channel using the SSH protocol (<a href="https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys#ssh-overview">DigitalOcean</a>).
Essentially, SSH keys are an authentication method used to gain access to an encrypted connection between systems and then ultimately use that connection to manage the remote system.

<h3 id="SSHKeystopic-2">What are SSH keys?</h3>
SSH keys come in many sizes, but a popular choice is an<a href="https://www.linkedin.com/pulse/2048-bit-encryption-what-why-does-matter-srini-vasan"> RSA 2048-bit encryption</a>, which is comparable to a 617 digit long password. 
On Windows systems, it is possible to<a href="http://statistics.berkeley.edu/computing/ssh-keys"> generate your own SSH key pair</a> by downloading and using an SSH client like PuTTY. 

On Mac and Linux systems, it is possible to generate an SSH key pair using a terminal window. 
Watch the video below to find out how to generate your own RSA key pair on Mac and Linux.
<iframe src="https://www.youtube.com/embed/FocgH8gTFVw?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>

SSH keys always come in pairs, and every pair is made up of a private key and a public key. 
Who or what possesses these keys determines the type of SSH key pair. 
If the private key and the public key remain with the user, this set of SSH keys is referred to as user keys.

If the private and public keys are on a remote system, then this key pair is referred to as<a href="https://www.ssh.com/ssh/host-key"> host keys</a>. 
Another type of SSH key is<a href="https://www.ssh.com/ssh/session-key"> a session key</a>. 
When a large amount of data is being transmitted, session keys are used to encrypt this information.

Now let’s take a closer look at how a private key and public key work. 
To keep things simple, we will focus on how user keys work.

<h3 id="SSHKeystopic-3">How User Keys Work</h3>
In a user key set, the private key remains on the system being used to access the remote system (i.e. the user’s desktop or laptop) and is used to decrypt information that is exchanged in the SSH protocol.
Private keys should never be shared with anyone and should be secured on a system – i.e. the system is secured (full disk encryption,<a href="https://jumpcloud.com/platform/multi-factor-authentication-mfa"> MFA</a>), in the user’s possession, and the private key is secured via passphrase.
A public key is used to encrypt information, can be shared, and is used by the user and the remote server. 
On the server end, the public key is saved in a file that contains a list of authorized public keys. 

<h3 id="SSHKeystopic-4">First Steps – SSH Key Generation</h3>
Before you can start using SSH keys, first you need to generate your own SSH key pair on the system you would like to use to access a remote system. 
Please see the instructions below.

<h3 id="SSHKeystopic-5">Linux/Mac Instructions:</h3>
<strong>Recommended</strong>: Install or Update OpenSSH
<code>sudo apt-get update
sudo apt-get install openssh-server</code>

First, load a command line terminal and type the following command:
<img src="https://lh4.googleusercontent.com/j4-CVJ1I-xRjdZOaZ8cXqN2hSUpCCYWDFe2oQK9i4XFt84BhrimIatRnpfRcKwMgVsJwCoMReFzODKRTPt5sHAy0oHyWN3Vgt7NrqBpDJ-CgBmYfTYdO2kj8gdGJtPw0juumpuQt">
Your two key files will be created in the /HOME/.ssh/ directory by default, including your private key, but you may specify a direct location. 

<strong>Never share your private key.</strong>
<strong>$HOME/.ssh/id_rsa.pub (public key</strong>
<strong>$HOME/.ssh/id_rsa (private net)</strong>

<img src="https://lh3.googleusercontent.com/8ve6RT8yo_KzV422Ute-qa_mZoI6eJVAZQXqu0SSKnbCZGDLVSKyAPcyipPYWSE-m0nRbemjldlHlASXaMhmq9ShIEJWydAfgvw8EgZmNTqhUrcY4S1MfXVW48c-H_j_lFvKVE98">

Per above, you’ll be given the option to set a passphrase to make it more difficult for unauthorized users to log into your accounts by protecting the confidentiality of your keypairs.
Your public key is uploaded to your server to use SSH key authentication for access control. 

The <a href="https://linuxhint.com/use-ssh-copy-id-command/">ssh-copy-id command</a>, which is part of the OpenSSH package, can be used to automate the transfer process in the syntax of:
<code>ssh-copy-id username@host</code>
You may also add the key to your account using<a href="https://support.jumpcloud.com/support/s/article/how-do-i-add-an-ssh-key-to-my-jumpcloud-account"> JumpCloud</a>, or by manually placing the public SSH key on the remote server (<a href="https://www.digitalocean.com/community/tutorials/ssh-essentials-working-with-ssh-servers-clients-and-keys#basic-connection-instructions">DigitalOcean</a>).
<a href="https://jumpcloud-1.wistia.com/medias/5usy6ga9lo?wvideo=5usy6ga9lo"><img src="https://embedwistia-a.akamaihd.net/deliveries/c8719e08ca0f214b1cab32d7d9f7cf71.jpg?image_play_button_size=2x&amp;image_crop_resized=960x540&amp;image_play_button=1&amp;image_play_button_color=41c8c6e0"></a>

<a href="https://jumpcloud-1.wistia.com/medias/5usy6ga9lo?wvideo=5usy6ga9lo">Mac and Linux with CLI – jumpcloud-1</a>
<h3 id="SSHKeystopic-6">Windows Instructions:</h3>
<img src="https://lh4.googleusercontent.com/Xxs3ElpyXtbw_A92tJBEaHIr0rfnQ0klXz5DV5HRYCPQpDgYQYY-ulqCj7kfNek5m5B2fVBv3jNsUahOQhWZoIV3nS1YFYvVuzlHqnrWvMwb_eBa_PsV3XDweV1BPE3TBYSXyR4z">

Download and install the PuTTY SSH keygen program.

Load the PuTTYgen program
Click the <strong>Generate</strong> button and select RSA (SSH-2)
You may opt for a 4096 bit key, but some applications may not accept it and it may result in increased CPU usage during the ‘handshake’ process.
Key in a passphrase for additional security
You must select <strong>Save private key</strong> and specify a location
<strong>Do not share this file with unauthorized individuals</strong>
You can copy and paste your public key from the field above with <strong>Select All </strong>or hit the <strong>Save public key</strong> button.
You may add the key to your account using<a href="https://support.jumpcloud.com/support/s/article/how-do-i-add-an-ssh-key-to-my-jumpcloud-account"> JumpCloud</a>
<a href="https://jumpcloud-1.wistia.com/medias/gi8wc7hlbe?wvideo=gi8wc7hlbe"><img src="https://embed-fastly.wistia.com/deliveries/af124ab6985c9e4c76abe3183720f6cb.jpg?image_play_button_size=2x&amp;image_crop_resized=960x540&amp;image_play_button=1&amp;image_play_button_color=41c8c6e0"></a>
<a href="https://jumpcloud-1.wistia.com/medias/gi8wc7hlbe?wvideo=gi8wc7hlbe">Windows SSH Pair Keygen using PuTTY – jumpcloud-1</a>

<h3 id="SSHKeystopic-7">How SSH Key Authentication Works</h3>
After completing the steps mentioned above, use your terminal to enter in your ssh username and the IP address of the remote system in this format: <code>ssh username@my_ip_address</code>. 
This will initiate a connection to the remote system using the SSH protocol. 
The protocol and specified username will then tell the remote server which public key to use to authenticate you. 
Then the remote server will use that public key to encrypt a random challenge message that is sent back to the client. 

This challenge message is decrypted using the private key on your system.
Once the message is decrypted, it is combined with a previously arranged session ID and then sent back to the server. 
If the message matches with what the server sent out, the client is authenticated, and you will gain access to the remote server. 

This process proves to the server that you have the corresponding private key to the public key it has on file.
However, the security that this authentication process provides can be undermined when SSH keys are not properly managed. 

<h3 id="SSHKeystopic-8">Managing SSH Keys</h3>
It is imperative that proper SSH key management is in place because they often grant access to mission-critical digital assets. 
Also, companies tend to have a lot of SSH keys. 
In fact, Fortune 500 companies will often have <em>several millions</em> of these.

Despite the difficulty in trying to manually manage millions of SSH keys, having an SSH key management system in place is continuously overlooked. 
SSH.com did some digging and discovered a company that had 3 million SSH keys “that granted access to live production servers.
Of those, 90% were no longer used. 

Root access was granted by 10% of the keys ”. 
An effective SSH key management system in place would have gone a long way in reducing this concerning security risk.
IT has a couple options to<a href="https://jumpcloud.com/blog/identity-service-function-ssh-key-management/"> gain control over SSH keys</a> in their environment. 

One of these includes using an SSH key management tool. 
However this means having to manage one more platforms in addition to managing an SSO provider, a directory service, and maybe a system management solution. 
A new solution has emerged that is providing IT with a second option:<a href="https://jumpcloud.com/platform"> JumpCloud Directory Platform</a>.

<h3 id="SSHKeystopic-9">Cloud IAM offers SSH Key Management</h3>
This <a href="https://jumpcloud.com/blog/cloud-identity-and-access-management/">cloud-based identity and access management solution</a> provides IT with one central place to manage SSH keys. 
Furthermore, IT can also centralize user authentication to Mac, Linux, and Windows <a href="https://jumpcloud.com/blog/centralized-system-and-user-management/">devices</a>, cloud <a href="https://jumpcloud.com/blog/idaas-and-aws-cloud-servers/">servers</a>, wired and WiFi <a href="https://jumpcloud.com/blog/secure-wifi-networks-cloud-radius/">networks</a>, web-based and on-prem <a href="https://jumpcloud.com/blog/true-single-sign-on-systems-apps-network/">applications</a>, and virtual and on-prem storage. 
With one central place to manage a user’s authentication to all of their resources, it becomes a simple matter of a few clicks to deprovision users from all of their resources, including SSH key access to remote systems.






<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>

<script type='text/javascript' src='readbookNewMarker.js'></script>
</body>
</html>
