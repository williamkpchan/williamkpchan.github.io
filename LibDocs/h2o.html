<base target="_blank"><html><head><title>H2O Tutorials</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script src="D:/Dropbox/Public/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "H2O Tutorials"
  var markerName = "h2"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>H2O Tutorials</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="https://docs.h2o.ai/h2o-tutorials/latest-stable/index.html" class="whitebut ">H2O Tutorials</a>

</div>
<pre>
<br>
<br>
<h2><span class="orange">H2O Tutorials</span></h2>
This document contains tutorials and training materials for H2O-3. 
 
If you find any problems with the tutorial code, please open an issue in this repository.

For general H2O questions, please post those to <a href="http://stackoverflow.com/questions/tagged/h2o" target="_blank">Stack Overflow using the "h2o" tag</a> or join the <a href="https://groups.google.com/forum/#!forum/h2ostream" target="_blank">H2O Stream Google Group</a> for questions that don&apos;t fit into the Stack Overflow format.

<h2>Finding tutorial material in Github</h2>
There are a number of tutorials on all sorts of topics in this repo. 
 
To help you get started, here are some of the most useful topics in both R and Python.

<h3>R Tutorials</h3>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R" target="_blank">Intro to H2O in R</a>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/grid-search-model-selection.R" target="_blank">H2O Grid Search & Model Selection in R</a>
<a href="http://htmlpreview.github.io/?https://github.com/ledell/sldm4-h2o/blob/master/sldm4-deeplearning-h2o.html" target="_blank">H2O Deep Learning in R</a>
<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html" target="_blank">H2O Stacked Ensembles in R</a>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/README.md" target="_blank">H2O AutoML in R</a>
<a href="https://github.com/ledell/LatinR-2019-h2o-tutorial" target="_blank">LatinR 2019 H2O Tutorial</a> (broad overview of all the above topics)

<h3>Python Tutorials</h3>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.ipynb" target="_blank">Intro to H2O in Python</a>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/grid-search-model-selection.ipynb" target="_blank">H2O Grid Search & Model Selection in Python</a>
<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html" target="_blank">H2O Stacked Ensembles in Python</a>
<a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/README.md" target="_blank">H2O AutoML in Python</a>

<h3>Most current material</h3>
Tutorials in the master branch are intended to work with the lastest stable version of H2O.

<table><thead>
<tr><th></th><th>URL</th></tr></thead>
<tbody>
<tr><td>Training material</td><td><a href="https://github.com/h2oai/h2o-tutorials/blob/master/SUMMARY.md" target="_blank">https://github.com/h2oai/h2o-tutorials/blob/master/SUMMARY.md</a></td></tr>
<tr><td>Latest stable H2O release</td><td><a href="http://h2o.ai/download" target="_blank">http://h2o.ai/download</a></td></tr>
</tbody>
</table>
<h3>Historical events</h3>
Tutorial versions in named branches are snapshotted for specific events. 
 
Scripts should work unchanged for the version of H2O used at that time.

<h4>H2O World 2017 Training</h4>
<table><thead>
<tr><th></th><th>URL</th></tr></thead>
<tbody>
<tr><td>Training material</td><td><a href="https://github.com/h2oai/h2o-tutorials/tree/master/h2o-world-2017/README.md" target="_blank">https://github.com/h2oai/h2o-tutorials/tree/master/h2o-world-2017/README.md</a></td></tr>
<tr><td>Wheeler-2 H2O release</td><td><a href="http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/2/index.html" target="_blank">http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/2/index.html</a></td></tr>
</tbody>
</table>
<h4>H2O World 2015 Training</h4>
<table><thead>
<tr><th></th><th>URL</th></tr></thead>
<tbody>
<tr><td>Training material</td><td><a href="https://github.com/h2oai/h2o-tutorials/blob/h2o-world-2015-training/SUMMARY.md" target="_blank">https://github.com/h2oai/h2o-tutorials/blob/h2o-world-2015-training/SUMMARY.md</a></td></tr>
<tr><td>Tibshirani-3 H2O release</td><td><a href="http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/3/index.html" target="_blank">http://h2o-release.s3.amazonaws.com/h2o/rel-tibshirani/3/index.html</a></td></tr>
</tbody>
</table>
<h3>Requirements:</h3>
For most tutorials using Python you can install dependent modules to your environment by running the following commands.

<code># As current user
pip install -r requirements.txt
</code>
<code># As root user
sudo -E pip install -r requirements.txt
</code>
<strong>Note:</strong> If you are behind a corporate proxy you may need to set environment variables for <code>https_proxy</code> accordingly.

<code># If you are behind a corporate proxy
export https_proxy=https://&lt;user&gt;:&lt;password&gt;@&lt;proxy_server&gt;:&lt;proxy_port&gt;

# As current user
pip install -r requirements.txt
</code>
<code># If you are behind a corporate proxy
export https_proxy=https://&lt;user&gt;:&lt;password&gt;@&lt;proxy_server&gt;:&lt;proxy_port&gt;

# As root user
sudo -E pip install -r requirements.txt
</code>

<h2><span class="orange">What is H2O?</span></h2>
H2O is fast, scalable, open-source machine learning and deep learning for Smarter Applications. 
With H2O, enterprises like PayPal, Nielsen Catalina, Cisco and others can use all of their data without sampling and get accurate predictions faster. 
Advanced algorithms, like Deep Learning, Boosting, and Bagging Ensembles are readily available for application designers to build smarter applications through elegant APIs. 
Some of our earliest customers have built powerful domain-specific predictive engines for Recommendations, Customer Churn, Propensity to Buy, Dynamic Pricing and Fraud Detection for the Insurance, Healthcare, Telecommunications, AdTech, Retail and Payment Systems.

Using in-memory compression techniques, H2O can handle billions of data rows in-memory, even with a fairly small cluster. 
The platform includes interfaces for R, Python, Scala, Java, JSON and Coffeescript/JavaScript, along with a built-in  web interface, Flow, that make it easier for non-engineers to stitch together complete analytic workflows. 
The platform was built alongside (and on top of) both Hadoop and Spark Clusters and is typically deployed within minutes.

H2O implements almost all common machine learning algorithms, such as generalized linear modeling (linear regression, logistic regression, etc.), Na&#xEF;ve Bayes, principal components analysis, time series, k-means clustering, and others. 
H2O also implements best-in-class algorithms such as Random Forest, Gradient Boosting, and Deep Learning at scale. 
Customers can build thousands of models and compare them to get the best prediction results.

H2O is nurturing a grassroots movement of physicists, mathematicians, computer and data scientists to herald the new wave of discovery with data science. 
Academic researchers and Industrial data scientists collaborate closely with our team to make this possible. 
Stanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise the H2O team to build scalable machine learning algorithms. 
With 100s of meetups over the past two years, H2O has become a word-of-mouth phenomenon growing amongst the data community by a 100-fold and is now used by 12,000+ users, deployed in 2000+ corporations using R, Python, Hadoop and Spark.

<strong>Try it out</strong>

H2O offers an R package that can be installed from CRAN, and a python package that can be installed from PyPI.

H2O can also be downloaded directly from <a href="http://h2o.ai/download" target="_blank">http://h2o.ai/download</a>. 


<strong>Join the community</strong>

Visit the open source community forum at <a href="https://groups.google.com/d/forum/h2ostream" target="_blank">https://groups.google.com/d/forum/h2ostream</a>. 


To learn about our meetups, training sessions, hackathons, and product updates, visit <a href="http://h2o.ai" target="_blank">http://h2o.ai</a>. 


<h2>Intro to Data Science</h2>
<h3>Slides</h3>
<a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/intro-to-datascience/intro-to-datascience.pdf" target="_blank">PDF</a>
<a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/intro-to-datascience/intro-to-datascience.key" target="_blank">Keynote</a>

<h2>Building a Smarter Application</h2>
<h3>Slides</h3>
<a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/building-a-smarter-application/BuildingASmarterApplication.pdf" target="_blank">PDF</a>
<a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/building-a-smarter-application/BuildingASmarterApplication.pptx" target="_blank">PowerPoint</a>

<h3>Code</h3>
The source code for this example is here: <a href="https://github.com/h2oai/app-consumer-loan" target="_blank">https://github.com/h2oai/app-consumer-loan</a>

<h2><span class="orange">Classification and Regression with H2O Deep Learning</span></h2>
IntroductionInstallation and Startup
Decision Boundaries

Cover Type DatasetExploratory Data Analysis
Deep Learning Model
Hyper-Parameter Search
Checkpointing
Cross-Validation
Model Save & Load

Regression and Binary Classification
Deep Learning Tips & Tricks

<h2>Introduction</h2>
This tutorial shows how a H2O <a href="http://en.wikipedia.org/wiki/Deep_learning" target="_blank">Deep Learning</a> model can be used to do supervised classification and regression. 
A great tutorial about Deep Learning is given by Quoc Le <a href="http://cs.stanford.edu/~quocle/tutorial1.pdf" target="_blank">here</a> and <a href="http://cs.stanford.edu/~quocle/tutorial2.pdf" target="_blank">here</a>. 
This tutorial covers usage of H2O from R. 
A python version of this tutorial will be available as well in a separate document. 
This file is available in plain R, R markdown and regular markdown formats, and the plots are available as PDF files. 
All documents are available <a href="https://github.com/h2oai/h2o-tutorials/tree/master/tutorials/deeplearning" target="_blank">on Github</a>.

If run from plain R, execute R in the directory of this script. 
If run from RStudio, be sure to setwd() to the location of this script. 
h2o.init() starts H2O in R&apos;s current working directory. 
h2o.importFile() looks for files from the perspective of where H2O was started.

More examples and explanations can be found in our <a href="http://h2o.ai/resources/" target="_blank">H2O Deep Learning booklet</a> and on our <a href="http://github.com/h2oai/h2o-3/" target="_blank">H2O Github Repository</a>. 
The PDF slide deck can be found <a href="https://github.com/h2oai/h2o-tutorials/tree/master/tutorials/deeplearning/H2ODeepLearning.pdf" target="_blank">on Github</a>.

<h3>H2O R Package</h3>
Load the H2O R package:

<code class="lang-r"><span class="hljs-comment">## R installation instructions are at http://h2o.ai/download</span>
<span class="hljs-keyword">library</span>(h2o)
</code>

<h3>Start H2O</h3>
Start up a 1-node H2O server on your local machine, and allow it to use all CPU cores and up to 2GB of memory:

<code class="lang-r">h2o.init(nthreads=-<span class="hljs-number">1</span>, max_mem_size=<span class="hljs-string">"2G"</span>)
h2o.removeAll() <span class="hljs-comment">## clean slate - just in case the cluster was already running</span>
</code>

The <code>h2o.deeplearning</code> function fits H2O&apos;s Deep Learning models from within R.
We can run the example from the man page using the <code>example</code> function, or run a longer demonstration from the <code>h2o</code> package using the <code>demo</code> function:

<code class="lang-r">args(h2o.deeplearning)
help(h2o.deeplearning)
example(h2o.deeplearning)
<span class="hljs-comment">#demo(h2o.deeplearning)  #requires user interaction</span>
</code>

While H2O Deep Learning has many parameters, it was designed to be just as easy to use as the other supervised training methods in H2O. 
Early stopping, automatic data standardization and handling of categorical variables and missing values and adaptive learning rates (per weight) reduce the amount of parameters the user has to specify. 
Often, it&apos;s just the number and sizes of hidden layers, the number of epochs and the activation function and maybe some regularization techniques.

<h3>Let&apos;s have some fun first: Decision Boundaries</h3>
We start with a small dataset representing red and black dots on a plane, arranged in the shape of two nested spirals. 
Then we task H2O&apos;s machine learning methods to separate the red and black dots, i.e., recognize each spiral as such by assigning each point in the plane to one of the two spirals.

We visualize the nature of H2O Deep Learning (DL), H2O&apos;s tree methods (GBM/DRF) and H2O&apos;s generalized linear modeling (GLM) by plotting the decision boundary between the red and black spirals:

<code class="lang-r">setwd(<span class="hljs-string">"~/h2o-tutorials/tutorials/deeplearning"</span>) <span class="hljs-comment">##For RStudio</span>
spiral &lt;- h2o.importFile(path = normalizePath(<span class="hljs-string">"../data/spiral.csv"</span>))
grid   &lt;- h2o.importFile(path = normalizePath(<span class="hljs-string">"../data/grid.csv"</span>))
<span class="hljs-comment"># Define helper to plot contours</span>
plotC &lt;- <span class="hljs-keyword">function</span>(name, model, data=spiral, g=grid) {
  data &lt;- as.data.frame(data) <span class="hljs-comment">#get data from into R</span>
  pred &lt;- as.data.frame(h2o.predict(model, g))
  n=<span class="hljs-number">0.5</span>*(sqrt(nrow(g))-<span class="hljs-number">1</span>); d &lt;- <span class="hljs-number">1.5</span>; h &lt;- d*(-n:n)/n
  plot(data[,-<span class="hljs-number">3</span>],pch=<span class="hljs-number">19</span>,col=data[,<span class="hljs-number">3</span>],cex=<span class="hljs-number">0.5</span>,
       xlim=c(-d,d),ylim=c(-d,d),main=name)
  contour(h,h,z=array(ifelse(pred[,<span class="hljs-number">1</span>]==<span class="hljs-string">"Red"</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>),
          dim=c(<span class="hljs-number">2</span>*n+<span class="hljs-number">1</span>,<span class="hljs-number">2</span>*n+<span class="hljs-number">1</span>)),col=<span class="hljs-string">"blue"</span>,lwd=<span class="hljs-number">2</span>,add=<span class="hljs-literal">T</span>)
}
</code>

We build a few different models:

<code class="lang-r"><span class="hljs-comment">#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window</span>
par(mfrow=c(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) <span class="hljs-comment">#set up the canvas for 2x2 plots</span>
plotC( <span class="hljs-string">"DL"</span>, h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,epochs=<span class="hljs-number">1e3</span>))
plotC(<span class="hljs-string">"GBM"</span>, h2o.gbm         (<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral))
plotC(<span class="hljs-string">"DRF"</span>, h2o.randomForest(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral))
plotC(<span class="hljs-string">"GLM"</span>, h2o.glm         (<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,family=<span class="hljs-string">"binomial"</span>))
</code>

Let&apos;s investigate some more Deep Learning models. 
First, we explore the evolution over training time (number of passes over the data), and we use checkpointing to continue training the same model:

<code class="lang-r"><span class="hljs-comment">#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window</span>
par(mfrow=c(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) <span class="hljs-comment">#set up the canvas for 2x2 plots</span>
ep &lt;- c(<span class="hljs-number">1</span>,<span class="hljs-number">250</span>,<span class="hljs-number">500</span>,<span class="hljs-number">750</span>)
plotC(paste0(<span class="hljs-string">"DL "</span>,ep[<span class="hljs-number">1</span>],<span class="hljs-string">" epochs"</span>),
      h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,epochs=ep[<span class="hljs-number">1</span>],
      model_id=<span class="hljs-string">"dl_1"</span>))
plotC(paste0(<span class="hljs-string">"DL "</span>,ep[<span class="hljs-number">2</span>],<span class="hljs-string">" epochs"</span>),
      h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,epochs=ep[<span class="hljs-number">2</span>],
            checkpoint=<span class="hljs-string">"dl_1"</span>,model_id=<span class="hljs-string">"dl_2"</span>))
plotC(paste0(<span class="hljs-string">"DL "</span>,ep[<span class="hljs-number">3</span>],<span class="hljs-string">" epochs"</span>),
      h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,epochs=ep[<span class="hljs-number">3</span>],
            checkpoint=<span class="hljs-string">"dl_2"</span>,model_id=<span class="hljs-string">"dl_3"</span>))
plotC(paste0(<span class="hljs-string">"DL "</span>,ep[<span class="hljs-number">4</span>],<span class="hljs-string">" epochs"</span>),
      h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,epochs=ep[<span class="hljs-number">4</span>],
            checkpoint=<span class="hljs-string">"dl_3"</span>,model_id=<span class="hljs-string">"dl_4"</span>))
</code>

You can see how the network learns the structure of the spirals with enough training time. 
We explore different network architectures next:

<code class="lang-r"><span class="hljs-comment">#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window</span>
par(mfrow=c(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) <span class="hljs-comment">#set up the canvas for 2x2 plots</span>
<span class="hljs-keyword">for</span> (hidden <span class="hljs-keyword">in</span> list(c(<span class="hljs-number">11</span>,<span class="hljs-number">13</span>,<span class="hljs-number">17</span>,<span class="hljs-number">19</span>),c(<span class="hljs-number">42</span>,<span class="hljs-number">42</span>,<span class="hljs-number">42</span>),c(<span class="hljs-number">200</span>,<span class="hljs-number">200</span>),c(<span class="hljs-number">1000</span>))) {
  plotC(paste0(<span class="hljs-string">"DL hidden="</span>,paste0(hidden, collapse=<span class="hljs-string">"x"</span>)),
        h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,hidden=hidden,epochs=<span class="hljs-number">500</span>))
}
</code>

It is clear that different configurations can achieve similar performance, and that tuning will be required for optimal performance. 
Next, we compare between different activation functions, including one with 50% dropout regularization in the hidden layers:

<code class="lang-r"><span class="hljs-comment">#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window</span>
par(mfrow=c(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)) <span class="hljs-comment">#set up the canvas for 2x2 plots</span>
<span class="hljs-keyword">for</span> (act <span class="hljs-keyword">in</span> c(<span class="hljs-string">"Tanh"</span>,<span class="hljs-string">"Maxout"</span>,<span class="hljs-string">"Rectifier"</span>,<span class="hljs-string">"RectifierWithDropout"</span>)) {
  plotC(paste0(<span class="hljs-string">"DL "</span>,act,<span class="hljs-string">" activation"</span>), 
        h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,spiral,
              activation=act,hidden=c(<span class="hljs-number">100</span>,<span class="hljs-number">100</span>),epochs=<span class="hljs-number">1000</span>))
}
</code>

Clearly, the dropout rate was too high or the number of epochs was too low for the last configuration, which often ends up performing the best on larger datasets where generalization is important.

More information about the parameters can be found in the <a href="http://h2o.ai/resources/" target="_blank">H2O Deep Learning booklet</a>.

<h2>Cover Type Dataset</h2>
We import the full cover type dataset (581k rows, 13 columns, 10 numerical, 3 categorical).
We also split the data 3 ways: 60% for training, 20% for validation (hyper parameter tuning) and 20% for final testing.

<code class="lang-r">df &lt;- h2o.importFile(path = normalizePath(<span class="hljs-string">"../data/covtype.full.csv"</span>))
dim(df)
df
splits &lt;- h2o.splitFrame(df, c(<span class="hljs-number">0.6</span>,<span class="hljs-number">0.2</span>), seed=<span class="hljs-number">1234</span>)
train  &lt;- h2o.assign(splits[[<span class="hljs-number">1</span>]], <span class="hljs-string">"train.hex"</span>) <span class="hljs-comment"># 60%</span>
valid  &lt;- h2o.assign(splits[[<span class="hljs-number">2</span>]], <span class="hljs-string">"valid.hex"</span>) <span class="hljs-comment"># 20%</span>
test   &lt;- h2o.assign(splits[[<span class="hljs-number">3</span>]], <span class="hljs-string">"test.hex"</span>)  <span class="hljs-comment"># 20%</span>
</code>

Here&apos;s a scalable way to do scatter plots via binning (works for categorical and numeric columns) to get more familiar with the dataset.

<code class="lang-r"><span class="hljs-comment">#dev.new(noRStudioGD=FALSE) #direct plotting output to a new window</span>
par(mfrow=c(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)) <span class="hljs-comment"># reset canvas</span>
plot(h2o.tabulate(df, <span class="hljs-string">"Elevation"</span>,                       <span class="hljs-string">"Cover_Type"</span>))
plot(h2o.tabulate(df, <span class="hljs-string">"Horizontal_Distance_To_Roadways"</span>, <span class="hljs-string">"Cover_Type"</span>))
plot(h2o.tabulate(df, <span class="hljs-string">"Soil_Type"</span>,                       <span class="hljs-string">"Cover_Type"</span>))
plot(h2o.tabulate(df, <span class="hljs-string">"Horizontal_Distance_To_Roadways"</span>, <span class="hljs-string">"Elevation"</span> ))
</code>

<h3>First Run of H2O Deep Learning</h3>
Let&apos;s run our first Deep Learning model on the covtype dataset. 

We want to predict the <code>Cover_Type</code> column, a categorical feature with 7 levels, and the Deep Learning model will be tasked to perform (multi-class) classification. 
It uses the other 12 predictors of the dataset, of which 10 are numerical, and 2 are categorical with a total of 44 levels. 
We can expect the Deep Learning model to have 56 input neurons (after automatic one-hot encoding).

<code class="lang-r">response &lt;- <span class="hljs-string">"Cover_Type"</span>
predictors &lt;- setdiff(names(df), response)
predictors
</code>

To keep it fast, we only run for one epoch (one pass over the training data).

<code class="lang-r">m1 &lt;- h2o.deeplearning(
  model_id=<span class="hljs-string">"dl_model_first"</span>, 
  training_frame=train, 
  validation_frame=valid,   <span class="hljs-comment">## validation dataset: used for scoring and early stopping</span>
  x=predictors,
  y=response,
  <span class="hljs-comment">#activation="Rectifier",  ## default</span>
  <span class="hljs-comment">#hidden=c(200,200),       ## default: 2 hidden layers with 200 neurons each</span>
  epochs=<span class="hljs-number">1</span>,
  variable_importances=<span class="hljs-literal">T</span>    <span class="hljs-comment">## not enabled by default</span>
)
summary(m1)
</code>

Inspect the model in <a href="http://localhost:54321/" target="_blank">Flow</a> for more information about model building etc. 
by issuing a cell with the content <code>getModel "dl_model_first"</code>, and pressing Ctrl-Enter.

<h3>Variable Importances</h3>
Variable importances for Neural Network models are notoriously difficult to compute, and there are many <a href="ftp://ftp.sas.com/pub/neural/importance.html" target="_blank">pitfalls</a>. 
H2O Deep Learning has implemented the method of <a href="http://cs.anu.edu.au/~./Tom.Gedeon/pdfs/ContribDataMinv2.pdf" target="_blank">Gedeon</a>, and returns relative variable importances in descending order of importance.

<code class="lang-r">head(as.data.frame(h2o.varimp(m1)))
</code>

<h3>Early Stopping</h3>
Now we run another, smaller network, and we let it stop automatically once the misclassification rate converges (specifically, if the moving average of length 2 does not improve by at least 1% for 2 consecutive scoring events). 
We also sample the validation set to 10,000 rows for faster scoring.

<code class="lang-r">m2 &lt;- h2o.deeplearning(
  model_id=<span class="hljs-string">"dl_model_faster"</span>, 
  training_frame=train, 
  validation_frame=valid,
  x=predictors,
  y=response,
  hidden=c(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>),                  <span class="hljs-comment">## small network, runs faster</span>
  epochs=<span class="hljs-number">1000000</span>,                      <span class="hljs-comment">## hopefully converges earlier...</span>
  score_validation_samples=<span class="hljs-number">10000</span>,      <span class="hljs-comment">## sample the validation dataset (faster)</span>
  stopping_rounds=<span class="hljs-number">2</span>,
  stopping_metric=<span class="hljs-string">"misclassification"</span>, <span class="hljs-comment">## could be "MSE","logloss","r2"</span>
  stopping_tolerance=<span class="hljs-number">0.01</span>
)
summary(m2)
plot(m2)
</code>

<h3>Adaptive Learning Rate</h3>
By default, H2O Deep Learning uses an adaptive learning rate (<a href="http://arxiv.org/pdf/1212.5701v1.pdf" target="_blank">ADADELTA</a>) for its stochastic gradient descent optimization. 
There are only two tuning parameters for this method: <code>rho</code> and <code>epsilon</code>, which balance the global and local search efficiencies. 
<code>rho</code> is the similarity to prior weight updates (similar to momentum), and <code>epsilon</code> is a parameter that prevents the optimization to get stuck in local optima. 
Defaults are <code>rho=0.99</code> and <code>epsilon=1e-8</code>. 
For cases where convergence speed is very important, it might make sense to perform a few runs to optimize these two parameters (e.g., with <code>rho in c(0.9,0.95,0.99,0.999)</code> and <code>epsilon in c(1e-10,1e-8,1e-6,1e-4)</code>). 
Of course, as always with grid searches, caution has to be applied when extrapolating grid search results to a different parameter regime (e.g., for more epochs or different layer topologies or activation functions, etc.).

If <code>adaptive_rate</code> is disabled, several manual learning rate parameters become important: <code>rate</code>, <code>rate_annealing</code>, <code>rate_decay</code>, <code>momentum_start</code>, <code>momentum_ramp</code>, <code>momentum_stable</code> and <code>nesterov_accelerated_gradient</code>, the discussion of which we leave to <a href="http://h2o.ai/resources/" target="_blank">H2O Deep Learning booklet</a>.

<h3>Tuning</h3>
With some tuning, it is possible to obtain less than 10% test set error rate in about one minute. 
Error rates of below 5% are possible with larger models. 
Note that deep tree methods can be more effective for this dataset than Deep Learning, as they directly partition the space into sectors, which seems to be needed here.

<code class="lang-r">m3 &lt;- h2o.deeplearning(
  model_id=<span class="hljs-string">"dl_model_tuned"</span>, 
  training_frame=train, 
  validation_frame=valid, 
  x=predictors, 
  y=response, 
  overwrite_with_best_model=<span class="hljs-literal">F</span>,    <span class="hljs-comment">## Return the final model after 10 epochs, even if not the best</span>
  hidden=c(<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>),          <span class="hljs-comment">## more hidden layers -&gt; more complex interactions</span>
  epochs=<span class="hljs-number">10</span>,                      <span class="hljs-comment">## to keep it short enough</span>
  score_validation_samples=<span class="hljs-number">10000</span>, <span class="hljs-comment">## downsample validation set for faster scoring</span>
  score_duty_cycle=<span class="hljs-number">0.025</span>,         <span class="hljs-comment">## don&apos;t score more than 2.5% of the wall time</span>
  adaptive_rate=<span class="hljs-literal">F</span>,                <span class="hljs-comment">## manually tuned learning rate</span>
  rate=<span class="hljs-number">0.01</span>, 
  rate_annealing=<span class="hljs-number">2e-6</span>,            
  momentum_start=<span class="hljs-number">0.2</span>,             <span class="hljs-comment">## manually tuned momentum</span>
  momentum_stable=<span class="hljs-number">0.4</span>, 
  momentum_ramp=<span class="hljs-number">1e7</span>, 
  l1=<span class="hljs-number">1e-5</span>,<span class="hljs-comment">## add some L1/L2 regularization</span>
  l2=<span class="hljs-number">1e-5</span>,
  max_w2=<span class="hljs-number">10</span>                       <span class="hljs-comment">## helps stability for Rectifier</span>
) 
summary(m3)
</code>

Let&apos;s compare the training error with the validation and test set errors

<code class="lang-r">h2o.performance(m3, train=<span class="hljs-literal">T</span>)          <span class="hljs-comment">## sampled training data (from model building)</span>
h2o.performance(m3, valid=<span class="hljs-literal">T</span>)          <span class="hljs-comment">## sampled validation data (from model building)</span>
h2o.performance(m3, newdata=train)    <span class="hljs-comment">## full training data</span>
h2o.performance(m3, newdata=valid)    <span class="hljs-comment">## full validation data</span>
h2o.performance(m3, newdata=test)     <span class="hljs-comment">## full test data</span>
</code>

To confirm that the reported confusion matrix on the validation set (here, the test set) was correct, we make a prediction on the test set and compare the confusion matrices explicitly:

<code class="lang-r">pred &lt;- h2o.predict(m3, test)
pred
test$Accuracy &lt;- pred$predict == test$Cover_Type
<span class="hljs-number">1</span>-mean(test$Accuracy)
</code>

<h3>Hyper-parameter Tuning with Grid Search</h3>
Since there are a lot of parameters that can impact model accuracy, hyper-parameter tuning is especially important for Deep Learning:

For speed, we will only train on the first 10,000 rows of the training dataset:

<code class="lang-r">sampled_train=train[<span class="hljs-number">1</span>:<span class="hljs-number">10000</span>,]
</code>

The simplest hyperparameter search method is a brute-force scan of the full Cartesian product of all combinations specified by a grid search:

<code class="lang-r">hyper_params &lt;- list(
  hidden=list(c(<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">32</span>),c(<span class="hljs-number">64</span>,<span class="hljs-number">64</span>)),
  input_dropout_ratio=c(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>),
  rate=c(<span class="hljs-number">0.01</span>,<span class="hljs-number">0.02</span>),
  rate_annealing=c(<span class="hljs-number">1e-8</span>,<span class="hljs-number">1e-7</span>,<span class="hljs-number">1e-6</span>)
)
hyper_params
grid &lt;- h2o.grid(
  algorithm=<span class="hljs-string">"deeplearning"</span>,
  grid_id=<span class="hljs-string">"dl_grid"</span>, 
  training_frame=sampled_train,
  validation_frame=valid, 
  x=predictors, 
  y=response,
  epochs=<span class="hljs-number">10</span>,
  stopping_metric=<span class="hljs-string">"misclassification"</span>,
  stopping_tolerance=<span class="hljs-number">1e-2</span>,        <span class="hljs-comment">## stop when misclassification does not improve by &gt;=1% for 2 scoring events</span>
  stopping_rounds=<span class="hljs-number">2</span>,
  score_validation_samples=<span class="hljs-number">10000</span>, <span class="hljs-comment">## downsample validation set for faster scoring</span>
  score_duty_cycle=<span class="hljs-number">0.025</span>,         <span class="hljs-comment">## don&apos;t score more than 2.5% of the wall time</span>
  adaptive_rate=<span class="hljs-literal">F</span>,                <span class="hljs-comment">## manually tuned learning rate</span>
  momentum_start=<span class="hljs-number">0.5</span>,             <span class="hljs-comment">## manually tuned momentum</span>
  momentum_stable=<span class="hljs-number">0.9</span>, 
  momentum_ramp=<span class="hljs-number">1e7</span>, 
  l1=<span class="hljs-number">1e-5</span>,
  l2=<span class="hljs-number">1e-5</span>,
  activation=c(<span class="hljs-string">"Rectifier"</span>),
  max_w2=<span class="hljs-number">10</span>,                      <span class="hljs-comment">## can help improve stability for Rectifier</span>
  hyper_params=hyper_params
)
grid
</code>

Let&apos;s see which model had the lowest validation error:

<code class="lang-r">grid &lt;- h2o.getGrid(<span class="hljs-string">"dl_grid"</span>,sort_by=<span class="hljs-string">"err"</span>,decreasing=<span class="hljs-literal">FALSE</span>)
grid

<span class="hljs-comment">## To see what other "sort_by" criteria are allowed</span>
<span class="hljs-comment">#grid &lt;- h2o.getGrid("dl_grid",sort_by="wrong_thing",decreasing=FALSE)</span>

<span class="hljs-comment">## Sort by logloss</span>
h2o.getGrid(<span class="hljs-string">"dl_grid"</span>,sort_by=<span class="hljs-string">"logloss"</span>,decreasing=<span class="hljs-literal">FALSE</span>)

<span class="hljs-comment">## Find the best model and its full set of parameters</span>
grid@summary_table[<span class="hljs-number">1</span>,]
best_model &lt;- h2o.getModel(grid@model_ids[[<span class="hljs-number">1</span>]])
best_model

print(best_model@allparameters)
print(h2o.performance(best_model, valid=<span class="hljs-literal">T</span>))
print(h2o.logloss(best_model, valid=<span class="hljs-literal">T</span>))
</code>

<h3>Random Hyper-Parameter Search</h3>
Often, hyper-parameter search for more than 4 parameters can be done more efficiently with random parameter search than with grid search. 
Basically, chances are good to find one of many good models in less time than performing an exhaustive grid search. 
We simply build up to <code>max_models</code> models with parameters drawn randomly from user-specified distributions (here, uniform). 
For this example, we use the adaptive learning rate and focus on tuning the network architecture and the regularization parameters. 
We also let the grid search stop automatically once the performance at the top of the leaderboard doesn&apos;t change much anymore, i.e., once the search has converged.

<code class="lang-r">hyper_params &lt;- list(
  activation=c(<span class="hljs-string">"Rectifier"</span>,<span class="hljs-string">"Tanh"</span>,<span class="hljs-string">"Maxout"</span>,<span class="hljs-string">"RectifierWithDropout"</span>,<span class="hljs-string">"TanhWithDropout"</span>,<span class="hljs-string">"MaxoutWithDropout"</span>),
  hidden=list(c(<span class="hljs-number">20</span>,<span class="hljs-number">20</span>),c(<span class="hljs-number">50</span>,<span class="hljs-number">50</span>),c(<span class="hljs-number">30</span>,<span class="hljs-number">30</span>,<span class="hljs-number">30</span>),c(<span class="hljs-number">25</span>,<span class="hljs-number">25</span>,<span class="hljs-number">25</span>,<span class="hljs-number">25</span>)),
  input_dropout_ratio=c(<span class="hljs-number">0</span>,<span class="hljs-number">0.05</span>),
  l1=seq(<span class="hljs-number">0</span>,<span class="hljs-number">1e-4</span>,<span class="hljs-number">1e-6</span>),
  l2=seq(<span class="hljs-number">0</span>,<span class="hljs-number">1e-4</span>,<span class="hljs-number">1e-6</span>)
)
hyper_params

<span class="hljs-comment">## Stop once the top 5 models are within 1% of each other (i.e., the windowed average varies less than 1%)</span>
search_criteria = list(strategy = <span class="hljs-string">"RandomDiscrete"</span>, max_runtime_secs = <span class="hljs-number">360</span>, max_models = <span class="hljs-number">100</span>, seed=<span class="hljs-number">1234567</span>, stopping_rounds=<span class="hljs-number">5</span>, stopping_tolerance=<span class="hljs-number">1e-2</span>)
dl_random_grid &lt;- h2o.grid(
  algorithm=<span class="hljs-string">"deeplearning"</span>,
  grid_id = <span class="hljs-string">"dl_grid_random"</span>,
  training_frame=sampled_train,
  validation_frame=valid, 
  x=predictors, 
  y=response,
  epochs=<span class="hljs-number">1</span>,
  stopping_metric=<span class="hljs-string">"logloss"</span>,
  stopping_tolerance=<span class="hljs-number">1e-2</span>,        <span class="hljs-comment">## stop when logloss does not improve by &gt;=1% for 2 scoring events</span>
  stopping_rounds=<span class="hljs-number">2</span>,
  score_validation_samples=<span class="hljs-number">10000</span>, <span class="hljs-comment">## downsample validation set for faster scoring</span>
  score_duty_cycle=<span class="hljs-number">0.025</span>,         <span class="hljs-comment">## don&apos;t score more than 2.5% of the wall time</span>
  max_w2=<span class="hljs-number">10</span>,                      <span class="hljs-comment">## can help improve stability for Rectifier</span>
  hyper_params = hyper_params,
  search_criteria = search_criteria
)        
grid &lt;- h2o.getGrid(<span class="hljs-string">"dl_grid_random"</span>,sort_by=<span class="hljs-string">"logloss"</span>,decreasing=<span class="hljs-literal">FALSE</span>)
grid

grid@summary_table[<span class="hljs-number">1</span>,]
best_model &lt;- h2o.getModel(grid@model_ids[[<span class="hljs-number">1</span>]]) <span class="hljs-comment">## model with lowest logloss</span>
best_model
</code>

Let&apos;s look at the model with the lowest validation misclassification rate:

<code class="lang-r">grid &lt;- h2o.getGrid(<span class="hljs-string">"dl_grid"</span>,sort_by=<span class="hljs-string">"err"</span>,decreasing=<span class="hljs-literal">FALSE</span>)
best_model &lt;- h2o.getModel(grid@model_ids[[<span class="hljs-number">1</span>]]) <span class="hljs-comment">## model with lowest classification error (on validation, since it was available during training)</span>
h2o.confusionMatrix(best_model,valid=<span class="hljs-literal">T</span>)
best_params &lt;- best_model@allparameters
best_params$activation
best_params$hidden
best_params$input_dropout_ratio
best_params$l1
best_params$l2
</code>

<h3>Checkpointing</h3>
Let&apos;s continue training the manually tuned model from before, for 2 more epochs. 
Note that since many important parameters such as <code>epochs, l1, l2, max_w2, score_interval, train_samples_per_iteration, input_dropout_ratio, hidden_dropout_ratios, score_duty_cycle, classification_stop, regression_stop, variable_importances, force_load_balance</code> can be modified between checkpoint restarts, it is best to specify as many parameters as possible explicitly.

<code class="lang-r">max_epochs &lt;- <span class="hljs-number">12</span> <span class="hljs-comment">## Add two more epochs</span>
m_cont &lt;- h2o.deeplearning(
  model_id=<span class="hljs-string">"dl_model_tuned_continued"</span>, 
  checkpoint=<span class="hljs-string">"dl_model_tuned"</span>, 
  training_frame=train, 
  validation_frame=valid, 
  x=predictors, 
  y=response, 
  hidden=c(<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">128</span>),          <span class="hljs-comment">## more hidden layers -&gt; more complex interactions</span>
  epochs=max_epochs,              <span class="hljs-comment">## hopefully long enough to converge (otherwise restart again)</span>
  stopping_metric=<span class="hljs-string">"logloss"</span>,      <span class="hljs-comment">## logloss is directly optimized by Deep Learning</span>
  stopping_tolerance=<span class="hljs-number">1e-2</span>,        <span class="hljs-comment">## stop when validation logloss does not improve by &gt;=1% for 2 scoring events</span>
  stopping_rounds=<span class="hljs-number">2</span>,
  score_validation_samples=<span class="hljs-number">10000</span>, <span class="hljs-comment">## downsample validation set for faster scoring</span>
  score_duty_cycle=<span class="hljs-number">0.025</span>,         <span class="hljs-comment">## don&apos;t score more than 2.5% of the wall time</span>
  adaptive_rate=<span class="hljs-literal">F</span>,                <span class="hljs-comment">## manually tuned learning rate</span>
  rate=<span class="hljs-number">0.01</span>, 
  rate_annealing=<span class="hljs-number">2e-6</span>,            
  momentum_start=<span class="hljs-number">0.2</span>,             <span class="hljs-comment">## manually tuned momentum</span>
  momentum_stable=<span class="hljs-number">0.4</span>, 
  momentum_ramp=<span class="hljs-number">1e7</span>, 
  l1=<span class="hljs-number">1e-5</span>,<span class="hljs-comment">## add some L1/L2 regularization</span>
  l2=<span class="hljs-number">1e-5</span>,
  max_w2=<span class="hljs-number">10</span>                       <span class="hljs-comment">## helps stability for Rectifier</span>
) 
summary(m_cont)
plot(m_cont)
</code>

Once we are satisfied with the results, we can save the model to disk (on the cluster). 
In this example, we store the model in a directory called <code>mybest_deeplearning_covtype_model</code>, which will be created for us since <code>force=TRUE</code>.

<code class="lang-r">path &lt;- h2o.saveModel(m_cont, 
          path=<span class="hljs-string">"./mybest_deeplearning_covtype_model"</span>, force=<span class="hljs-literal">TRUE</span>)
</code>

It can be loaded later with the following command:

<code class="lang-r">print(path)
m_loaded &lt;- h2o.loadModel(path)
summary(m_loaded)
</code>

This model is fully functional and can be inspected, restarted, or used to score a dataset, etc. 
Note that binary compatibility between H2O versions is currently not guaranteed.

<h3>Cross-Validation</h3>
For N-fold cross-validation, specify <code>nfolds&gt;1</code> instead of (or in addition to) a validation frame, and <code>N+1</code> models will be built: 1 model on the full training data, and N models with each 1/N-th of the data held out (there are different holdout strategies). 
Those N models then score on the held out data, and their combined predictions on the full training data are scored to get the cross-validation metrics.

<code class="lang-r">dlmodel &lt;- h2o.deeplearning(
  x=predictors,
  y=response, 
  training_frame=train,
  hidden=c(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),
  epochs=<span class="hljs-number">1</span>,
  nfolds=<span class="hljs-number">5</span>,
  fold_assignment=<span class="hljs-string">"Modulo"</span> <span class="hljs-comment"># can be "AUTO", "Modulo", "Random" or "Stratified"</span>
  )
dlmodel
</code>

N-fold cross-validation is especially useful with early stopping, as the main model will pick the ideal number of epochs from the convergence behavior of the cross-validation models.

<h2>Regression and Binary Classification</h2>
Assume we want to turn the multi-class problem above into a binary classification problem. 
We create a binary response as follows:

<code class="lang-r">train$bin_response &lt;- ifelse(train[,response]==<span class="hljs-string">"class_1"</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
</code>

Let&apos;s build a quick model and inspect the model:

<code class="lang-r">dlmodel &lt;- h2o.deeplearning(
  x=predictors,
  y=<span class="hljs-string">"bin_response"</span>, 
  training_frame=train,
  hidden=c(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),
  epochs=<span class="hljs-number">0.1</span>
)
summary(dlmodel)
</code>

Instead of a binary classification model, we find a regression model (<code>H2ORegressionModel</code>) that contains only 1 output neuron (instead of 2). 
The reason is that the response was a numerical feature (ordinal numbers 0 and 1), and H2O Deep Learning was run with <code>distribution=AUTO</code>, which defaulted to a Gaussian regression problem for a real-valued response.
H2O Deep Learning supports regression for distributions other than <code>Gaussian</code> such as <code>Poisson</code>, <code>Gamma</code>, <code>Tweedie</code>, <code>Laplace</code>. 
It also supports <code>Huber</code> loss and per-row offsets specified via an <code>offset_column</code>. 
We refer to our <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/deeplearning" target="_blank">H2O Deep Learning regression code examples</a> for more information.

To perform classification, the response must first be turned into a categorical (factor) feature:

<code class="lang-r">train$bin_response &lt;- as.factor(train$bin_response) <span class="hljs-comment">##make categorical</span>
dlmodel &lt;- h2o.deeplearning(
  x=predictors,
  y=<span class="hljs-string">"bin_response"</span>, 
  training_frame=train,
  hidden=c(<span class="hljs-number">10</span>,<span class="hljs-number">10</span>),
  epochs=<span class="hljs-number">0.1</span>
  <span class="hljs-comment">#balance_classes=T    ## enable this for high class imbalance</span>
)
summary(dlmodel) <span class="hljs-comment">## Now the model metrics contain AUC for binary classification</span>
plot(h2o.performance(dlmodel)) <span class="hljs-comment">## display ROC curve</span>
</code>

Now the model performs (binary) classification, and has multiple (2) output neurons.

<h2>Unsupervised Anomaly detection</h2>
For instructions on how to build unsupervised models with H2O Deep Learning, we refer to our previous <a href="https://www.youtube.com/watch?v=fUSbljByXak" target="_blank">Tutorial on Anomaly Detection with H2O Deep Learning</a> and our <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_anomaly_large.R" target="_blank">MNIST Anomaly detection code example</a>, as well as our <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_stacked_autoencoder_large.R" target="_blank">Stacked AutoEncoder R code example</a> and another one for 
<a href="https://github.com/h2oai/h2o-3/blob/master/h2o-r/tests/testdir_algos/deeplearning/runit_deeplearning_autoencoder_large.R" target="_blank">Unsupervised Pretraining with an AutoEncoder R code example</a>.

<h2>H2O Deep Learning Tips & Tricks</h2>
<h4>Performance Tuning</h4>
The <a href="http://blog.h2o.ai/2015/08/deep-learning-performance-august/" target="_blank">Definitive H2O Deep Learning Performance Tuning</a> blog post covers many of the following points that affect the computational efficiency, so it&apos;s highly recommended.

<h4>Activation Functions</h4>
While sigmoids have been used historically for neural networks, H2O Deep Learning implements <code>Tanh</code>, a scaled and shifted variant of the sigmoid which is symmetric around 0. 
Since its output values are bounded by -1..1, the stability of the neural network is rarely endangered. 
However, the derivative of the tanh function is always non-zero and back-propagation (training) of the weights is more computationally expensive than for rectified linear units, or <code>Rectifier</code>, which is <code>max(0,x)</code> and has vanishing gradient for <code>x&lt;=0</code>, leading to much faster training speed for large networks and is often the fastest path to accuracy on larger problems. 
In case you encounter instabilities with the <code>Rectifier</code> (in which case model building is automatically aborted), try a limited value to re-scale the weights: <code>max_w2=10</code>. 
The <code>Maxout</code> activation function is computationally more expensive, but can lead to higher accuracy. 
It is a generalized version of the Rectifier with two non-zero channels. 
In practice, the <code>Rectifier</code> (and <code>RectifierWithDropout</code>, see below) is the most versatile and performant option for most problems.

<h4>Generalization Techniques</h4>
L1 and L2 penalties can be applied by specifying the <code>l1</code> and <code>l2</code> parameters. 
Intuition: L1 lets only strong weights survive (constant pulling force towards zero), while L2 prevents any single weight from getting too big. 
<a href="http://arxiv.org/pdf/1207.0580.pdf" target="_blank">Dropout</a> has recently been introduced as a powerful generalization technique, and is available as a parameter per layer, including the input layer. 
<code>input_dropout_ratio</code> controls the amount of input layer neurons that are randomly dropped (set to zero), while <code>hidden_dropout_ratios</code> are specified for each hidden layer. 
The former controls overfitting with respect to the input data (useful for high-dimensional noisy data), while the latter controls overfitting of the learned features. 
Note that <code>hidden_dropout_ratios</code> require the activation function to end with <code>...WithDropout</code>.

<h4>Early stopping and optimizing for lowest validation error</h4>
By default, Deep Learning training stops when the <code>stopping_metric</code> does not improve by at least <code>stopping_tolerance</code> (0.01 means 1% improvement) for <code>stopping_rounds</code> consecutive scoring events on the training (or validation) data. 
By default, <code>overwrite_with_best_model</code> is enabled and the model returned after training for the specified number of epochs (or after stopping early due to convergence) is the model that has the best training set error (according to the metric specified by <code>stopping_metric</code>), or, if a validation set is provided, the lowest validation set error. 
Note that the training or validation set errors can be based on a subset of the training or validation data, depending on the values for <code>score_validation_samples</code> or <code>score_training_samples</code>, see below. 
For early stopping on a predefined error rate on the <em>training data</em> (accuracy for classification or MSE for regression), specify <code>classification_stop</code> or <code>regression_stop</code>.

<h4>Training Samples per MapReduce Iteration</h4>
The parameter <code>train_samples_per_iteration</code> matters especially in multi-node operation. 
It controls the number of rows trained on for each MapReduce iteration. 
Depending on the value selected, one MapReduce pass can sample observations, and multiple such passes are needed to train for one epoch. 
All H2O compute nodes then communicate to agree on the best model coefficients (weights/biases) so far, and the model may then be scored (controlled by other parameters below). 
The default value of <code>-2</code> indicates auto-tuning, which attemps to keep the communication overhead at 5% of the total runtime. 
The parameter <code>target_ratio_comm_to_comp</code> controls this ratio. 
This parameter is explained in more detail in the <a href="http://h2o.ai/resources/" target="_blank">H2O Deep Learning booklet</a>,

<h4>Categorical Data</h4>
For categorical data, a feature with K factor levels is automatically one-hot encoded (horizontalized) into K-1 input neurons. 
Hence, the input neuron layer can grow substantially for datasets with high factor counts. 
In these cases, it might make sense to reduce the number of hidden neurons in the first hidden layer, such that large numbers of factor levels can be handled. 
In the limit of 1 neuron in the first hidden layer, the resulting model is similar to logistic regression with stochastic gradient descent, except that for classification problems, there&apos;s still a softmax output layer, and that the activation function is not necessarily a sigmoid (<code>Tanh</code>). 
If variable importances are computed, it is recommended to turn on <code>use_all_factor_levels</code> (K input neurons for K levels). 
The experimental option <code>max_categorical_features</code> uses feature hashing to reduce the number of input neurons via the hash trick at the expense of hash collisions and reduced accuracy. 
Another way to reduce the dimensionality of the (categorical) features is to use <code>h2o.glrm()</code>, we refer to the GLRM tutorial for more details.

<h4>Sparse Data</h4>
If the input data is sparse (many zeros), then it might make sense to enable the <code>sparse</code> option. 
This will result in the input not being standardized (0 mean, 1 variance), but only de-scaled (1 variance) and 0 values remain 0, leading to more efficient back-propagation. 
Sparsity is also a reason why CPU implementations can be faster than GPU implementations, because they can take advantage of if/else statements more effectively.

<h4>Missing Values</h4>
H2O Deep Learning automatically does mean imputation for missing values during training (leaving the input layer activation at 0 after standardizing the values). 
For testing, missing test set values are also treated the same way by default. 
See the <code>h2o.impute</code> function to do your own mean imputation.

<h4>Loss functions, Distributions, Offsets, Observation Weights</h4>
H2O Deep Learning supports advanced statistical features such as multiple loss functions, non-Gaussian distributions, per-row offsets and observation weights.
In addition to <code>Gaussian</code> distributions and <code>Squared</code> loss, H2O Deep Learning supports <code>Poisson</code>, <code>Gamma</code>, <code>Tweedie</code> and <code>Laplace</code> distributions. 
It also supports <code>Absolute</code> and <code>Huber</code> loss and per-row offsets specified via an <code>offset_column</code>. 
Observation weights are supported via a user-specified <code>weights_column</code>.

We refer to our <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/deeplearning" target="_blank">H2O Deep Learning R test code examples</a> for more information.

<h4>Exporting Weights and Biases</h4>
The model parameters (weights connecting two adjacent layers and per-neuron bias terms) can be stored as H2O Frames (like a dataset) by enabling <code>export_weights_and_biases</code>, and they can be accessed as follows:

<code class="lang-r">iris_dl &lt;- h2o.deeplearning(<span class="hljs-number">1</span>:<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,as.h2o(iris),
             export_weights_and_biases=<span class="hljs-literal">T</span>)
h2o.weights(iris_dl, matrix_id=<span class="hljs-number">1</span>)
h2o.weights(iris_dl, matrix_id=<span class="hljs-number">2</span>)
h2o.weights(iris_dl, matrix_id=<span class="hljs-number">3</span>)
h2o.biases(iris_dl,  vector_id=<span class="hljs-number">1</span>)
h2o.biases(iris_dl,  vector_id=<span class="hljs-number">2</span>)
h2o.biases(iris_dl,  vector_id=<span class="hljs-number">3</span>)
<span class="hljs-comment">#plot weights connecting `Sepal.Length` to first hidden neurons</span>
plot(as.data.frame(h2o.weights(iris_dl,  matrix_id=<span class="hljs-number">1</span>))[,<span class="hljs-number">1</span>])
</code>

<h4>Reproducibility</h4>
Every run of DeepLearning results in different results since multithreading is done via <a href="http://www.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf" target="_blank">Hogwild!</a> that benefits from intentional lock-free race conditions between threads. 
To get reproducible results for small datasets and testing purposes, set <code>reproducible=T</code> and set <code>seed=1337</code> (pick any integer). 
This will not work for big data for technical reasons, and is probably also not desired because of the significant slowdown (runs on 1 core only).

<h4>Scoring on Training/Validation Sets During Training</h4>
The training and/or validation set errors <em>can</em> be based on a subset of the training or validation data, depending on the values for <code>score_validation_samples</code> (defaults to 0: all) or <code>score_training_samples</code> (defaults to 10,000 rows, since the training error is only used for early stopping and monitoring). 
For large datasets, Deep Learning can automatically sample the validation set to avoid spending too much time in scoring during training, especially since scoring results are not currently displayed in the model returned to R.

Note that the default value of <code>score_duty_cycle=0.1</code> limits the amount of time spent in scoring to 10%, so a large number of scoring samples won&apos;t slow down overall training progress too much, but it will always score once after the first MapReduce iteration, and once at the end of training.

Stratified sampling of the validation dataset can help with scoring on datasets with class imbalance. 
 
Note that this option also requires <code>balance_classes</code> to be enabled (used to over/under-sample the training dataset, based on the max. 
relative size of the resulting training dataset, <code>max_after_balance_size</code>):

<h3>More information can be found in the <a href="http://h2o.ai/resources/" target="_blank">H2O Deep Learning booklet</a>, in our <a href="http://www.slideshare.net/0xdata/presentations" target="_blank">H2O SlideShare Presentations</a>, our <a href="https://www.youtube.com/user/0xdata/" target="_blank">H2O YouTube channel</a>, as well as on our <a href="https://github.com/h2oai/h2o-3/" target="_blank">H2O Github Repository</a>, especially in our <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-r/tests/testdir_algos/deeplearning" target="_blank">H2O Deep Learning R tests</a>, and <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-py/tests/testdir_algos/deeplearning" target="_blank">H2O Deep Learning Python tests</a>.</h3>
<h3>All done, shutdown H2O</h3>
<code class="lang-r">h2o.shutdown(prompt=<span class="hljs-literal">FALSE</span>)
</code>

<h2><span class="orange">GBM and Random Forest in H2O</span></h2>
<h2>Slides</h2>
<a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_in_H2O.pdf" target="_blank">PDF</a>

<h2>Code</h2>
The source code for this example is here: <a href="https://github.com/h2oai/h2o-world-2015-training/blob/master/tutorials/gbm-randomforest/GBM_RandomForest_Example.R" target="_blank">R script</a>

IntroductionInstallation and Startup

Cover Type Dataset
Multinomial Model
Binomial ModelAdding extra features

Multinomial Model Revisited

<h2>Introduction</h2>
This tutorial shows how a H2O <a href="http://en.wikipedia.org/wiki/Generalized_linear_model" target="_blank">GLM</a> model can be used to do binary and multi-class classification. 
This tutorial covers usage of H2O from R. 
A python version of this tutorial will be available as well in a separate document. 
This file is available in plain R, R markdown and regular markdown formats, and the plots are available as PDF files. 
All documents are available <a href="https://github.com/h2oai/h2o-world-2015-training/raw/master/tutorials/glm/" target="_blank">on Github</a>.

If run from plain R, execute R in the directory of this script. 
If run from RStudio, be sure to <code>setwd()</code> to the location of this script. 
<code>h2o.init()</code> starts H2O in R&apos;s current working directory. 
h2o.importFile() looks for files from the perspective of where H2O was started.

More examples and explanations can be found in our <a href="http://h2o.ai/resources/" target="_blank">H2O GLM booklet</a> and on our <a href="http://github.com/h2oai/h2o-3/" target="_blank">H2O Github Repository</a>. 


<h3>H2O R Package</h3>
Load the H2O R package:

<code class="lang-{r}">## R installation instructions are at http://h2o.ai/download
library(h2o)
</code>

<h3>Start H2O</h3>
Start up a 1-node H2O server on your local machine, and allow it to use all CPU cores and up to 2GB of memory:

<code class="lang-{r}">h2o.init(nthreads=-1, max_mem_size="2G")
h2o.removeAll() ## clean slate - just in case the cluster was already running
</code>

<h2>Cover Type Data</h2>
Predicting forest cover type from cartographic variables only (no remotely sensed data).
Let&apos;s import the dataset:

<code class="lang-{r}">D = h2o.importFile(path = normalizePath("../data/covtype.full.csv"))
h2o.summary(D)
</code>

We have 11 numeric and two categorical features. 
Response is "Cover_Type" and has 7 classes.
Let&apos;s split the data into Train/Test/Validation with train having 70% and Test and Validation 15% each:

<code class="lang-{r}">data = h2o.splitFrame(D,ratios=c(.7,.15),destination_frames = c("train","test","valid"))
names(data) </code>

<h2>Multinomial Model</h2>
We imported our data, so let&apos;s run GLM. 
As we mentioned previously, Cover_Type is the response and we use all other columns as predictors.
We have multi-class problem so we pick family=multinomial. 
L-BFGS solver tends to be faster on multinomial problems, so we pick L-BFGS for our first try. 

The rest can use the default settings.

<code class="lang-{r}">m1 = h2o.glm(training_frame = data$Train, validation_frame = data$Valid, x = x, y = y,family=&apos;multinomial&apos;,solver=&apos;L_BFGS&apos;)
h2o.confusionMatrix(m1, valid=TRUE)
</code>

The model predicts only the majority class so it&apos;s not useful at all! Maybe we regularized it too much, let&apos;s try again without regularization: 

<code class="lang-{r}">m2 = h2o.glm(training_frame = data$Train, validation_frame = data$Valid, x = x, y = y,family=&apos;multinomial&apos;,solver=&apos;L_BFGS&apos;, lambda = 0)
h2o.confusionMatrix(m2, valid=FALSE) # get confusion matrix in the training data
h2o.confusionMatrix(m2, valid=TRUE)  # get confusion matrix in the validation data
</code>

No overfitting (as train and test performance are the same), regularization is not needed in this case. 


This model is actually useful. 
It got 28% classification error, down from 51% obtained by predicting majority class only.

<h2>Binomial Model</h2>
Since multinomial models are difficult and time consuming, let&apos;s try a simpler binary classification. 

We&apos;ll take a subset of the data with only <code>class_1</code> and <code>class_2</code> (the two majority classes) and build a binomial model deciding between them.

<code class="lang-{r}">D_binomial = D[D$Cover_Type %in% c("class_1","class_2"),]
h2o.setLevels(D_binomial$Cover_Type,c("class_1","class_2"))
# split to train/test/validation again
data_binomial = h2o.splitFrame(D_binomial,ratios=c(.7,.15),destination_frames = c("train_b","test_b","valid_b"))
names(data_binomial) </code>

We can run a binomial model now: 

<code class="lang-{r}">m_binomial = h2o.glm(training_frame = data_binomial$Train, validation_frame = data_binomial$Valid, x = x, y = y, family=&apos;binomial&apos;,lambda=0)
h2o.confusionMatrix(m_binomial, valid = TRUE)
h2o.confusionMatrix(m_binomial, valid = TRUE)
</code>

The output for a binomial problem is slightly different from multinomial. 
The confusion matrix now has a threshold attached to it.

The model produces probability of <code>class_1</code> and <code>class_2</code> similarly to multinomial example earlier. 
However, this time we only have two classes and we can tune the classification to our needs. 


The classification errors in binomial cases have a particular meaning: we call them false-positive and false negative. 
In reality, each can have a different cost associated with it, so we want to tune our classifier accordingly. 


The common way to evaluate a binary classifier performance is to look at its <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" target="_blank">ROC curve</a>. 
The ROC curve plots the true positive rate versus false positive rate. 
We can plot it from the H2O model output:

<code class="lang-{r}">fpr = m_binomial@model$training_metrics@metrics$thresholds_and_metric_scores$fpr
tpr = m_binomial@model$training_metrics@metrics$thresholds_and_metric_scores$tpr
fpr_val = m_binomial@model$validation_metrics@metrics$thresholds_and_metric_scores$fpr
tpr_val = m_binomial@model$validation_metrics@metrics$thresholds_and_metric_scores$tpr
plot(fpr,tpr, type=&apos;l&apos;)
title(&apos;AUC&apos;)
lines(fpr_val,tpr_val,type=&apos;l&apos;,col=&apos;red&apos;)
legend("bottomright",c("Train", "Validation"),col=c("black","red"),lty=c(1,1),lwd=c(3,3))
</code>

The area under the ROC curve (AUC) is a common "good fit" metric for binary classifiers. 
For this example, the results were:

<code class="lang-{r}">h2o.auc(m_binomial,valid=FALSE) # on train                   
h2o.auc(m_binomial,valid=TRUE)  # on test
</code>

The default confusion matrix is computed at thresholds that optimize the <a href="https://en.wikipedia.org/wiki/F1_score" target="_blank">F1 score</a>. 
We can choose different thresholds - the H2O output shows optimal thresholds for some common metrics.

<code class="lang-{r}">m_binomial@model$training_metrics@metrics$max_criteria_and_metric_scores
</code>

The model we just built gets 23% classification error at the F1-optimizing threshold, so there is still room for improvement.
Let&apos;s add some features:

There are 11 numerical predictors in the dataset, we will cut them into intervals and add a categorical variable for each
We can add interaction terms capturing interactions between categorical variables

Let&apos;s make a convenience function to cut the column into intervals working on all three of our datasets (Train/Validation/Test). 

We&apos;ll use <code>h2o.hist</code> to determine interval boundaries (but there are many more ways to do that!) on the Train set.<br>We&apos;ll take only the bins with non-trivial support:  

<code class="lang-{r}">cut_column <- function(data,="" col)="" {="" #="" need="" lower="" upper="" bound="" due="" to="" h2o.cut="" behavior="" (points="" <="" the="" first="" break="" or=""> the last break are replaced with missing value) 
  min_val = min(data$Train[,col])-1
  max_val = max(data$Train[,col])+1
  x = h2o.hist(data$Train[, col])
  # use only the breaks with enough support
  breaks = x$breaks[which(x$counts &gt; 1000)]
  # assign level names 
  lvls = c("min",paste("i_",breaks[2:length(breaks)-1],sep="),"max")
  col_cut </-></code>

Now let&apos;s make a convenience function generating interaction terms on all three of our datasets. 
We&apos;ll use <code>h2o.interaction</code>:

<code class="lang-{r}">interactions </code>

Finally, let&apos;s wrap addition of the features into a separate function call, as we will use it again later.
We&apos;ll add intervals for each numeric column and interactions between each pair of binary columns.

<code class="lang-{r}"># add features to our cover type example
# let&apos;s cut all the numerical columns into intervals and add interactions between categorical terms
add_features </code>

Now we generate new features and add them to the dataset. 
We&apos;ll also need to generate column names again, as we added more columns:

<code class="lang-{r}"># Add Features
data_binomial_ext </code>

Let&apos;s build the model! We should add some regularization this time because we added correlated variables, so let&apos;s try the default:

<code class="lang-{r}">m_binomial_1_ext = try(h2o.glm(training_frame = data_binomial_ext$Train, validation_frame = data_binomial_ext$Valid, x = x, y = y, family=&apos;binomial&apos;))
</code>

Oops, doesn&apos;t run - well, we know have more features than the default method can solve with 2GB of RAM. 
Let&apos;s try L-BFGS instead.

<code class="lang-{r}">m_binomial_1_ext = h2o.glm(training_frame = data_binomial_ext$Train, validation_frame = data_binomial_ext$Valid, x = x, y = y, family=&apos;binomial&apos;, solver=&apos;L_BFGS&apos;)
h2o.confusionMatrix(m_binomial_1_ext)
h2o.auc(m_binomial_1_ext,valid=TRUE)
</code>

Not much better, maybe too much regularization? Let&apos;s pick a smaller lambda and try again.

<code class="lang-{r}">m_binomial_2_ext = h2o.glm(training_frame = data_binomial_ext$Train, validation_frame = data_binomial_ext$Valid, x = x, y = y, family=&apos;binomial&apos;, solver=&apos;L_BFGS&apos;, lambda=1e-4)
h2o.confusionMatrix(m_binomial_2_ext, valid=TRUE)
h2o.auc(m_binomial_2_ext,valid=TRUE)
</code>

Way better, we got an AUC of .91 and classification error of 0.180838. 

We picked our regularization strength arbitrarily. 
Also, we used only the l2 penalty but we added lot of extra features, some of which may be useless. 

Maybe we can do better with an l1 penalty. 

So now we want to run a lambda search to find optimal penalty strength and we want to have a non-zero l1 penalty to get sparse solution.
We&apos;ll use the IRLSM solver this time as it does much better with lambda search and l1 penalty. 

Recall we were not able to use it before. 
We can use it now as we are running a lambda search that will filter out a large portion of the inactive (coefficient==0) predictors. 


<code class="lang-{r}">m_binomial_3_ext = h2o.glm(training_frame = data_binomial_ext$Train, validation_frame = data_binomial_ext$Valid, x = x, y = y, family=&apos;binomial&apos;, lambda_search=TRUE)
h2o.confusionMatrix(m_binomial_3_ext, valid=TRUE)
h2o.auc(m_binomial_3_ext,valid=TRUE)
</code>

Better yet, we have 17% error and we used only 3000 out of 7000 features.
Ok, our new features improved the binomial model significantly, so let&apos;s revisit our former multinomial model and see if they make a difference there (they should!):

<code class="lang-{r}"># Multinomial Model 2
# let&apos;s revisit the multinomial case with our new features
data_ext </code>

Improved considerably, 21% instead of 28%.

<h2><span class="orange">Generalized Low Rank Models</span></h2>
Overview
What is a Low Rank Model?
Why use Low Rank Models?Memory
Speed
Feature Engineering
Missing Data Imputation

Example 1: Visualizing Walking StancesBasic Model Building
Plotting Archetypal Features
Imputing Missing Values

Example 2: Compressing Zip CodesCondensing Categorical Data
Runtime and Accuracy Comparison

References

<h2>Overview</h2>
This tutorial introduces the Generalized Low Rank Model (GLRM) [<a href="#references">1</a>], a new machine learning approach for reconstructing missing values and identifying important features in heterogeneous data. 
It demonstrates how to build a GLRM in H2O and integrate it into a data science pipeline to make better predictions.

<h2>What is a Low Rank Model?</h2>
Across business and research, analysts seek to understand large collections of data with numeric and categorical values. 
Many entries in this table may be noisy or even missing altogether. 
Low rank models facilitate the understanding of tabular data by producing a condensed vector representation for every row and column in the data set.

Specifically, given a data table A with m rows and n columns, a GLRM consists of a decomposition of A into numeric matrices X and Y. 
The matrix X has the same number of rows as A, but only a small, user-specified number of columns k. 
The matrix Y has k rows and d columns, where d is equal to the total dimension of the embedded features in A. 
For example, if A has 4 numeric columns and 1 categorical column with 3 distinct levels (e.g., <em>red</em>, <em>blue</em> and <em>green</em>), then Y will have 7 columns. 
When A contains only numeric features, the number of columns in A and Y are identical, as shown below.

<img src="images/glrm_matrix_decomposition.png" alt="GLRM Matrix Decomposition">

Both X and Y have practical interpretations. 
Each row of Y is an archetypal feature formed from the columns of A, and each row of X corresponds to a row of A projected into this reduced dimension feature space. 
We can approximately reconstruct A from the matrix product XY, which has rank k. 
The number k is chosen to be much less than both m and n: a typical value for 1 million rows and 2,000 columns of numeric data is k = 15. 
The smaller k is, the more compression we gain from our low rank representation.

GLRMs are an extension of well-known matrix factorization methods such as Principal Components Analysis (PCA). 
While PCA is limited to numeric data, GLRMs can handle mixed numeric, categorical, ordinal and Boolean data with an arbitrary number of missing values. 
It allows the user to apply regularization to X and Y, imposing restrictions like non-negativity appropriate to a particular data science context. 
Thus, it is an extremely flexible approach for analyzing and interpreting heterogeneous data sets.

<h2>Why use Low Rank Models?</h2>
<strong>Memory:</strong> By saving only the X and Y matrices, we can significantly reduce the amount of memory required to store a large data set. 
A file that is 10 GB can be compressed down to 100 MB. 
When we need the original data again, we can reconstruct it on the fly from X and Y with minimal loss in accuracy.
<strong>Speed:</strong> We can use GLRM to compress data with high-dimensional, heterogeneous features into a few numeric columns. 
This leads to a huge speed-up in model building and prediction, especially by machine learning algorithms that scale poorly with the size of the feature space. 
Below, we will see an example with 10x speed-up and no accuracy loss in deep learning.
<strong>Feature Engineering:</strong> The Y matrix represents the most important combination of features from the training data. 
These condensed features, called archetypes, can be analyzed, visualized and incorporated into various data science applications. 

<strong>Missing Data Imputation:</strong> Reconstructing a data set from X and Y will automatically impute missing values. 
This imputation is accomplished by intelligently leveraging the information contained in the known values of each feature, as well as user-provided parameters such as the loss function.

<h2>Example 1: Visualizing Walking Stances</h2>
For our first example, we will use data on <a href="https://simtk.org/project/xml/downloads.xml?group_id=603" target="_blank">Subject 01&apos;s walking stances</a> from an experiment carried out by <em>Hamner and Delp (2013)</em> [<a href="#references">2</a>]. 
Each of the 151 rows of the data set contains the (x, y, z) coordinates of major body parts recorded at a specific point in time.

<h4>Basic Model Building</h4>
<h6>Initialize the H2O server and import our walking stance data. 
We use all available cores on our computer and allocate a maximum of 2 GB of memory to H2O.</h6>
<code>library(h2o)
h2o.init(nthreads = -1, max_mem_size = "2G")
gait.hex &lt;- h2o.importFile(path = normalizePath("../data/subject01_walk1.csv"), destination_frame = "gait.hex")
</code>
<h6>Get a summary of the imported data set.</h6>
<code>dim(gait.hex)
summary(gait.hex)
</code>
<h6>Build a basic GLRM using quadratic loss and no regularization. 
Since this data set contains only numeric features and no missing values, this is equivalent to PCA. 
We skip the first column since it is the time index, set the rank k = 10, and allow the algorithm to run for a maximum of 1,000 iterations.</h6>
<code>gait.glrm &lt;- h2o.glrm(training_frame = gait.hex, cols = 2:ncol(gait.hex), k = 10, loss = "Quadratic", 
                      regularization_x = "None", regularization_y = "None", max_iterations = 1000)
</code>
<h6>To ensure our algorithm converged, we should always plot the objective function value per iteration after model building is complete.</h6>
<code>plot(gait.glrm)
</code>
<h4>Plotting Archetypal Features</h4>
<h6>The rows of the Y matrix represent the principal stances that Subject 01 took while walking. 
We can visualize each of the 10 stances by plotting the (x, y) coordinate weights of each body part.</h6>
<code>gait.y &lt;- gait.glrm@model$archetypes
gait.y.mat &lt;- as.matrix(gait.y)
x_coords &lt;- seq(1, ncol(gait.y), by = 3)
y_coords &lt;- seq(2, ncol(gait.y), by = 3)
feat_nams &lt;- sapply(colnames(gait.y), function(nam) { substr(nam, 1, nchar(nam)-1) })
feat_nams &lt;- as.character(feat_nams[x_coords])
for(k in 1:10) {
    plot(gait.y.mat[k,x_coords], gait.y.mat[k,y_coords], xlab = "X-Coordinate Weight", ylab = "Y-Coordinate Weight", main = paste("Feature Weights of Archetype", k), col = "blue", pch = 19, lty = "solid")
    text(gait.y.mat[k,x_coords], gait.y.mat[k,y_coords], labels = feat_nams, cex = 0.7, pos = 3)
    cat("Press [Enter] to continue")
    line &lt;- readline()
}
</code>
<h6>The rows of the X matrix decompose each bodily position Subject 01 took at a specific time into a combination of the principal stances. 
Let&apos;s plot each principal stance over time to see how they alternate.</h6>
<code>gait.x &lt;- h2o.getFrame(gait.glrm@model$representation_name)
time.df &lt;- as.data.frame(gait.hex$Time[1:150])[,1]
gait.x.df &lt;- as.data.frame(gait.x[1:150,])
matplot(time.df, gait.x.df, xlab = "Time", ylab = "Archetypal Projection", main = "Archetypes over Time", type = "l", lty = 1, col = 1:5)
legend("topright", legend = colnames(gait.x.df), col = 1:5, pch = 1)
</code>
<h6>We can reconstruct our original training data from X and Y.</h6>
<code>gait.pred &lt;- predict(gait.glrm, gait.hex)
head(gait.pred)
</code>
<h6>For comparison, let&apos;s plot the original and reconstructed data of a specific feature over time: the x-coordinate of the left acromium.</h6>
<code>lacro.df &lt;- as.data.frame(gait.hex$L.Acromium.X[1:150])
lacro.pred.df &lt;- as.data.frame(gait.pred$reconstr_L.Acromium.X[1:150])
matplot(time.df, cbind(lacro.df, lacro.pred.df), xlab = "Time", ylab = "X-Coordinate of Left Acromium", main = "Position of Left Acromium over Time", type = "l", lty = 1, col = c(1,4))
legend("topright", legend = c("Original", "Reconstructed"), col = c(1,4), pch = 1)
</code>
<h4>Imputing Missing Values</h4>
Suppose that due to a sensor malfunction, our walking stance data has missing values randomly interspersed. 
We can use GLRM to reconstruct these missing values from the existing data.

<h6>Import walking stance data containing 15% missing values and get a summary.</h6>
<code>gait.miss &lt;- h2o.importFile(path = normalizePath("../data/subject01_walk1_miss15.csv"), destination_frame = "gait.miss")
dim(gait.miss)
summary(gait.miss)
</code>
<h6>Count the total number of missing values in the data set.</h6>
<code>sum(is.na(gait.miss))
</code>
<h6>Build a basic GLRM with quadratic loss and no regularization, validating on our original data set that has no missing values. 
We change the algorithm initialization method, increase the maximum number of iterations to 2,000, and reduce the minimum step size to 1e-6 to ensure convergence.</h6>
<code>gait.glrm2 &lt;- h2o.glrm(training_frame = gait.miss, validation_frame = gait.hex, cols = 2:ncol(gait.miss), k = 10, init = "SVD", svd_method = "GramSVD",
                      loss = "Quadratic", regularization_x = "None", regularization_y = "None", max_iterations = 2000, min_step_size = 1e-6)
plot(gait.glrm2)
</code>
<h6>Impute missing values in our training data from X and Y.</h6>
<code>gait.pred2 &lt;- predict(gait.glrm2, gait.miss)
head(gait.pred2)
sum(is.na(gait.pred2))
</code>
<h6>Plot the original and reconstructed values of the x-coordinate of the left acromium. 
Red x&apos;s mark the points where the training data contains a missing value, so we can see how accurate our imputation is.</h6>
<code>lacro.pred.df2 &lt;- as.data.frame(gait.pred2$reconstr_L.Acromium.X[1:150])
matplot(time.df, cbind(lacro.df, lacro.pred.df2), xlab = "Time", ylab = "X-Coordinate of Left Acromium", main = "Position of Left Acromium over Time", type = "l", lty = 1, col = c(1,4))
legend("topright", legend = c("Original", "Imputed"), col = c(1,4), pch = 1)
lacro.miss.df &lt;- as.data.frame(gait.miss$L.Acromium.X[1:150])
idx_miss &lt;- which(is.na(lacro.miss.df))
points(time.df[idx_miss], lacro.df[idx_miss,1], col = 2, pch = 4, lty = 2)
</code>
<h2>Example 2: Compressing Zip Codes</h2>
For our second example, we will be using two data sets. 
The first is compliance actions carried out by the U.S. 
Labor Department&apos;s <a href="http://ogesdw.dol.gov/views/data_summary.php" target="_blank">Wage and Hour Division (WHD)</a> from 2014-2015. 
This includes information on each investigation, including the zip code tabulation area (ZCTA) where the firm is located, number of violations found and civil penalties assessed. 
We want to predict whether a firm is a repeat and/or willful violator. 
In order to do this, we need to encode the categorical ZCTA column in a meaningful way. 
One common approach is to replace ZCTA with indicator variables for every unique level, but due to its high cardinality (there are over 32,000 ZCTAs!), this is slow and leads to overfitting.

Instead, we will build a GLRM to condense ZCTAs into a few numeric columns representing the demographics of that area. 
Our second data set is the 2009-2013 <a href="http://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?src=bkmk" target="_blank">American Community Survey (ACS)</a> 5-year estimates of household characteristics. 
Each row contains information for a unique ZCTA, such as average household size, number of children and education. 
By transforming the WHD data with our GLRM, we not only address the speed and overfitting issues, but also transfer knowledge between similar ZCTAs in our model.

<h4>Condensing Categorical Data</h4>
<h6>Initialize the H2O server and import the ACS data set. 
We use all available cores on our computer and allocate a maximum of 2 GB of memory to H2O.</h6>
<code>library(h2o)
h2o.init(nthreads = -1, max_mem_size = "2G")
acs_orig &lt;- h2o.importFile(path = "../data/ACS_13_5YR_DP02_cleaned.zip", col.types = c("enum", rep("numeric", 149)))
</code>
<h6>Separate out the zip code tabulation area column.</h6>
<code>acs_zcta_col &lt;- acs_orig$ZCTA5
acs_full &lt;- acs_orig[,-which(colnames(acs_orig) == "ZCTA5")]
</code>
<h6>Get a summary of the ACS data set.</h6>
<code>dim(acs_full)
summary(acs_full)
</code>
<h6>Build a GLRM to reduce ZCTA demographics to k = 10 archetypes. 
We standardize the data before model building to ensure a good fit. 
For the loss function, we select quadratic again, but this time, apply regularization to X and Y in order to sparsify the condensed features.</h6>
<code>acs_model &lt;- h2o.glrm(training_frame = acs_full, k = 10, transform = "STANDARDIZE", 
                      loss = "Quadratic", regularization_x = "Quadratic", 
                      regularization_y = "L1", max_iterations = 100, gamma_x = 0.25, gamma_y = 0.5)
plot(acs_model)
</code>
<h6>The rows of the X matrix map each ZCTA into a combination of demographic archetypes.</h6>
<code>zcta_arch_x &lt;- h2o.getFrame(acs_model@model$representation_name)
head(zcta_arch_x)
</code>
<h6>Plot a few interesting ZCTAs on the first two archetypes. 
We should see cities with similar demographics, such as Sunnyvale and Cupertino, grouped close together, while very different cities, such as the rural town McCune and the upper east side of Manhattan, fall far apart on the graph.</h6>
<code>idx &lt;- ((acs_zcta_col == "10065") |   # Manhattan, NY (Upper East Side)
        (acs_zcta_col == "11219") |   # Manhattan, NY (East Harlem)
        (acs_zcta_col == "66753") |   # McCune, KS
        (acs_zcta_col == "84104") |   # Salt Lake City, UT
        (acs_zcta_col == "94086") |   # Sunnyvale, CA
        (acs_zcta_col == "95014"))    # Cupertino, CA

city_arch &lt;- as.data.frame(zcta_arch_x[idx,1:2])
xeps &lt;- (max(city_arch[,1]) - min(city_arch[,1])) / 10
yeps &lt;- (max(city_arch[,2]) - min(city_arch[,2])) / 10
xlims &lt;- c(min(city_arch[,1]) - xeps, max(city_arch[,1]) + xeps)
ylims &lt;- c(min(city_arch[,2]) - yeps, max(city_arch[,2]) + yeps)
plot(city_arch[,1], city_arch[,2], xlim = xlims, ylim = ylims, xlab = "First Archetype", ylab = "Second Archetype", main = "Archetype Representation of Zip Code Tabulation Areas")
text(city_arch[,1], city_arch[,2], labels = c("Upper East Side", "East Harlem", "McCune", "Salt Lake City", "Sunnyvale", "Cupertino"), pos = 1)
</code>
<h4>Runtime and Accuracy Comparison</h4>
We now build a deep learning model on the WHD data set to predict repeat and/or willful violators. 
For comparison purposes, we train our model using the original data, the data with the ZCTA column replaced by the compressed GLRM representation (the X matrix), and the data with the ZCTA column replaced by all the demographic features in the ACS data set.

<h6>Import the WHD data set and get a summary.</h6>
<code>whd_zcta &lt;- h2o.importFile(path = "../data/whd_zcta_cleaned.zip", col.types = c(rep("enum", 7), rep("numeric", 97)))
dim(whd_zcta)
summary(whd_zcta)
</code>
<h6>Split the WHD data into test and train with a 20/80 ratio.</h6>
<code>split &lt;- h2o.runif(whd_zcta)
train &lt;- whd_zcta[split &lt;= 0.8,]
test &lt;- whd_zcta[split &gt; 0.8,]
</code>
<h6>Build a deep learning model on the WHD data set to predict repeat/willful violators. 
Our response is a categorical column with four levels: N/A = neither repeat nor willful, R = repeat, W = willful, and RW = repeat and willful violator. 
Thus, we specify a multinomial distribution. 
We skip the first four columns, which consist of the case ID and location information that is already captured by the ZCTA.</h6>
<code>myY &lt;- "flsa_repeat_violator"
myX &lt;- setdiff(5:ncol(train), which(colnames(train) == myY))
orig_time &lt;- system.time(dl_orig &lt;- h2o.deeplearning(x = myX, y = myY, training_frame = train, 
     validation_frame = test, distribution = "multinomial",
     epochs = 0.1, hidden = c(50,50,50)))
</code>
<h6>Replace each ZCTA in the WHD data with the row of the X matrix corresponding to its condensed demographic representation. 
In the end, our single categorical column will be replaced by k = 10 numeric columns.</h6>
<code>zcta_arch_x$zcta5_cd &lt;- acs_zcta_col
whd_arch &lt;- h2o.merge(whd_zcta, zcta_arch_x, all.x = TRUE, all.y = FALSE)
whd_arch$zcta5_cd &lt;- NULL
summary(whd_arch)
</code>
<h6>Split the reduced WHD data into test/train and build a deep learning model to predict repeat/willful violators.</h6>
<code>train_mod &lt;- whd_arch[split &lt;= 0.8,]
test_mod  &lt;- whd_arch[split &gt; 0.8,]
myX &lt;- setdiff(5:ncol(train_mod), which(colnames(train_mod) == myY))
mod_time &lt;- system.time(dl_mod &lt;- h2o.deeplearning(x = myX, y = myY, training_frame = train_mod, 
   validation_frame = test_mod, distribution = "multinomial",
   epochs = 0.1, hidden = c(50,50,50)))
</code>
<h6>Replace each ZCTA in the WHD data with the row of ACS data containing its full demographic information.</h6>
<code>colnames(acs_orig)[1] &lt;- "zcta5_cd"
whd_acs &lt;- h2o.merge(whd_zcta, acs_orig, all.x = TRUE, all.y = FALSE)
whd_acs$zcta5_cd &lt;- NULL
summary(whd_acs)
</code>
<h6>Split the combined WHD-ACS data into test/train and build a deep learning model to predict repeat/willful violators.</h6>
<code>train_comb &lt;- whd_acs[split &lt;= 0.8,]
test_comb &lt;- whd_acs[split &gt; 0.8,]
myX &lt;- setdiff(5:ncol(train_comb), which(colnames(train_comb) == myY))
comb_time &lt;- system.time(dl_comb &lt;- h2o.deeplearning(x = myX, y = myY, training_frame = train_comb,
     validation_frame = test_comb, distribution = "multinomial",
     epochs = 0.1, hidden = c(50,50,50)))
</code>
<h6>Compare the performance between the three models. 
We see that the model built on the reduced WHD data set finishes almost 10 times faster than the model using the original data set, and it yields a lower log-loss error. 
The model with the combined WHD-ACS data set does not improve significantly on this error. 
We can conclude that our GLRM compressed the ZCTA demographics with little informational loss.</h6>
<code>data.frame(original = c(orig_time[3], h2o.logloss(dl_orig, train = TRUE), h2o.logloss(dl_orig, valid = TRUE)),
              reduced  = c(mod_time[3], h2o.logloss(dl_mod, train = TRUE), h2o.logloss(dl_mod, valid = TRUE)),
           combined = c(comb_time[3], h2o.logloss(dl_comb, train = TRUE), h2o.logloss(dl_comb, valid = TRUE)),
           row.names = c("runtime", "train_logloss", "test_logloss"))
</code>
<h2>References</h2>
[1] M. 
Udell, C. 
Horn, R. 
Zadeh, S. 
Boyd (2014). 
<a href="http://arxiv.org/abs/1410.0342" target="_blank">Generalized Low Rank Models</a>. 
Unpublished manuscript, Stanford Electrical Engineering Department.

[2] Hamner, S.R., Delp, S.L. 
<a href="http://nmbl.stanford.edu/publications/pdf/Hamner2012.pdf" target="_blank">Muscle contributions to fore-aft and vertical body mass center accelerations over a range of running speeds</a>. 
Journal of Biomechanics, vol 46, pp 780-787. 
(2013)

<h2><span class="orange">H2O AutoML Tutorial</span></h2>
AutoML is a function in H2O that automates the process of building a large number of models, with the goal of finding the "best" model without any prior knowledge or effort by the Data Scientist. 
 

The current version of AutoML (in H2O 3.16.*) trains and cross-validates a default Random Forest, an Extremely-Randomized Forest, a random grid of Gradient Boosting Machines (GBMs), a random grid of Deep Neural Nets, a fixed grid of GLMs, and then trains two Stacked Ensemble models at the end. 
One ensemble contains all the models (optimized for model performance), and the second ensemble contains just the best performing model from each algorithm class/family (optimized for production use).

More information and code examples are available in the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html" target="_blank">AutoML User Guide</a>.

New features and improvements planned for AutoML are listed <a href="https://0xdata.atlassian.net/issues/?filter=21603" target="_blank">here</a>.

<h2>Part 1: Binary Classification</h2>
For the AutoML binary classification demo, we use a subset of the <a href="https://www.kaggle.com/tiredgeek/predict-bo-trial/data" target="_blank">Product Backorders</a> dataset. 
 
The goal here is to predict whether or not a product will be put on backorder status, given a number of product metrics such as current inventory, transit time, demand forecasts and prior sales.

In this tutorial, you will:

Specify a training frame.
Specify the response variable and predictor variables.
Run AutoML where stopping is based on max number of models.
View the leaderboard (based on cross-validation metrics).
Explore the ensemble composition.
Save the leader model (binary format & MOJO format).

Demo Notebooks:

<a href="R/automl_binary_classification_product_backorders.Rmd">R/automl_binary_classification_product_backorders.Rmd</a> <a href="http://htmlpreview.github.io/?https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/R/automl_binary_classification_product_backorders.html" target="_blank">(html)</a> <a href="https://www.r-project.org//favicon-16x16.png" target="_blank"><img src="https://www.r-project.org//favicon-16x16.png" width="18"></a>
<a href="Python/automl_binary_classification_product_backorders.ipynb">Python/automl_binary_classification_product_backorders.ipynb</a>  <a href="https://www.python.org/static/favicon.ico" target="_blank"><img src="https://www.python.org/static/favicon.ico" width="16"></a>

<h2>Part 2: Regression</h2>
For the AutoML regression demo, we use the <a href="http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant" target="_blank">Combined Cycle Power Plant</a> dataset. 
 
The goal here is to predict the energy output (in megawatts), given the temperature, ambient pressure, relative humidity and exhaust vacuum values. 
 
In this demo, you will use H2O&apos;s AutoML to outperform the <a href="https://www.sciencedirect.com/science/article/pii/S0142061514000908" target="_blank">state-of-the-art results</a> on this task.

In this tutorial, you will:

Split the data into train/test sets.
Specify a training frame and leaderboard (test) frame.
Specify the response variable.
Run AutoML where stopping is based on max runtime, using training frame (80%).
Run AutoML where stopping is based on max runtime, using original frame (100%).
View leaderboard (based on test set metrics).
Compare the leaderboards of the two AutoML runs.
Predict using the AutoML leader model.
Compute performance of the AutoML leader model on a test set.

Demo Notebooks:

<a href="R/automl_regression_powerplant_output.Rmd">R/automl_regression_powerplant_output.Rmd</a> <a href="http://htmlpreview.github.io/?https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/R/automl_regression_powerplant_output.html" target="_blank">(html)</a> <a href="https://www.r-project.org//favicon-16x16.png" target="_blank"><img src="https://www.r-project.org//favicon-16x16.png" width="18"></a>
<a href="Python/automl_regression_powerplant_output.ipynb">Python/automl_regression_powerplant_output.ipynb</a> <a href="https://www.python.org/static/favicon.ico" target="_blank"><img src="https://www.python.org/static/favicon.ico" width="16"></a>

<h2><span class="orange">NLP with H2O Tutorial</span></h2>
The focus of this tutorial is to provide an introduction to H2O&apos;s Word2Vec algorithm. 
Word2Vec is an algorithm that trains a shallow neural network model to learn vector representations of words. 
These vector representations are able to capture the meanings of words. 
During the tutorial, we will use H2O&apos;s Word2Vec implementation to understand relationships between words in our text data. 
We will use the model results to find similar words and synonyms. 
We will also use it to showcase how to effectively represent text data for machine learning problems where we will highlight the impact this representation can have on accuracy. 


More information and code examples are available in the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/word2vec.html" target="_blank">Word2Vec Documentation</a>

<h2>Supervised Learning with Text Data</h2>
For the demo, we use a subset of the <a href="https://www.kaggle.com/snap/amazon-fine-food-reviews" target="_blank">Amazon Reviews</a> dataset. 
 
The goal here is to predict whether or not an Amazon review is positive or negative. 


The tutorial is split up into three parts. 
 
In the first part, we will train a model using non-text predictor variables. 
 
In the second and third part, we will train a model using our text columns. 
 
The text columns in this dataset are the review of the product and the summary of the review. 
 
In order to leverage our text columns, we will train a Word2Vec model to convert text into numeric vectors.

<h3>Initial Model - No Text</h3>
In this section, you will see how accurate your model is if you do not use any text columns. 
 
You will: 

Specify a training frame.
Specify a test frame.
Train a GBM model on non-text predictor variables such as: <code>ProductId</code>, <code>UserId</code>, <code>Time</code>, etc.
Analyze our initial model - AUC, confusion matrix, variable importance, partial dependency plots

<h3>Second Model - Word Embeddings of Reviews</h3>
In this section, you will see how much your model improves if you include the word embeddings from the reviews. 
You will:

Tokenize words in the review.
Train a Word2Vec model (or import the already trained Word2Vec model: <a href="https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex" target="_blank">https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex</a>)
Find synonyms using the Word2Vec model.
Aggregate word embeddings - one word embedding per review.
Train a GBM model using our initial predictors plus the word embeddings of the reviews.
Analyze our second model - AUC, confusion matrix

<h3>Third Model - Word Embeddings of Summaries</h3>
In this section, you will see if you can improve your model even more by also adding the word embeddings from the summary of the review. 
You will:

Aggregate word embeddings of summaries - one word embedding per summary.
Train a GBM model now including the word embeddings of the summary.
Analyze our final model - AUC, confusion matrix, variable importance, partial dependency plot
Predict on new reviews using our third and final model.

<h2>Resources</h2>
Demo Notebooks: <a href="AmazonReviews.ipynb">AmazonReviews.ipynb</a>
The subset of the Amazon Reviews data used for this demo can be found here: <a href="https://s3.amazonaws.com/tomk/h2o-world/megan/AmazonReviews.csv" target="_blank">https://s3.amazonaws.com/tomk/h2o-world/megan/AmazonReviews.csv</a>
The word2vec model that was trained on this data can be found here: <a href="https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex" target="_blank">https://s3.amazonaws.com/tomk/h2o-world/megan/w2v.hex</a>

<h2>Hive UDF POJO Example</h2>
This tutorial describes how to use a model created in H2O to create a Hive UDF (user-defined function) for scoring data. 
 
 While the fastest scoring typically results from ingesting data files in HDFS directly into H2O for scoring, there may be several motivations not to do so. 
 
For example, the clusters used for model building may be research clusters, and the data to be scored may be on "production" clusters. 
 
In other cases, the final data set to be scored may be too large to reasonably score in-memory. 
 
To help with these kinds of cases, this document walks through how to take a scoring model from H2O, plug it into a template UDF project, and use it to score in Hive. 
 
All the code needed for this walkthrough can be found in this repository branch.

<h2>The Goal</h2>
The desired work flow for this task is:

<ol>
Load training and test data into H2O
Create several models in H2O
Export the best model as a <a href="https://en.wikipedia.org/wiki/Plain_Old_Java_Object" target="_blank">POJO</a>
Compile the H2O model as a part of the UDF project
Copy the UDF to the cluster and load into Hive
Score with your UDF
</ol>
For steps 1-3, we will give instructions scoring the data through R. 
 
We will add a step between 4 and 5 to load some test data for this example.

<h2>Requirements</h2>
This tutorial assumes the following:

<ol>
Some familiarity with using H2O in R. 
 
Getting started tutorials can be found <a href="http://docs.0xdata.com/newuser/top.html" target="_blank">here</a>.
The ability to compile Java code. 
 
The repository provides a pom.xml file, so using Maven will be the simplest way to compile, but IntelliJ IDEA will also read in this file. 
 
If another build system is preferred, it is left to the reader to figure out the compilation details.
A working Hive install to test the results.
</ol>
<h2>The Data</h2>
For this post, we will be using a 0.1% sample of the Person-Level 2013 Public Use Microdata Sample (PUMS) from United States Census Bureau. 
 
75% of that sample is designated as the training data set and 25% as the test data set. 
This data set is intended as an update to the <a href="https://archive.ics.uci.edu/ml/datasets/Adult" target="_blank">UCI Adult Data Set</a>. 
 
The two datasets are available <a href="https://h2o-training.s3.amazonaws.com/pums2013/adult_2013_train.csv.gz" target="_blank">here</a> and <a href="https://h2o-training.s3.amazonaws.com/pums2013/adult_2013_test.csv.gz" target="_blank">here</a>.

The goal of the analysis in this demo is to predict if an income exceeds $50K/yr based on census data. 
 
The columns we will be using are:

AGEP:  age
COW: class of worker
SCHL: educational attainment
MAR: marital status
INDP: Industry code
RELP: relationship
RAC1P: race
SEX: gender
WKHP: hours worked per week
POBP: Place of birth code
LOG_CAPGAIN: log of capital gains
LOG_CAPLOSS: log of capital losses
LOG_WAGP: log of wages or salary

<h2>Building the Model in R</h2>
No need to cut and paste code: the complete R script described below is part of this git repository (GBM-example.R).

<h3>Load the training and test data into H2O</h3>
Since we are playing with a small data set for this example, we will start H2O locally and load the datasets:

<h2>Building the Model in R</h2>
No need to cut and paste code: the complete R script described below is part of this git repository (GBM-example.R).

<h3>Load the training and test data into H2O</h3>
Since we are playing with a small data set for this example, we will start H2O locally and load the datasets:

<code class="lang-r">&gt; <span class="hljs-keyword">library</span>(h2o)
&gt; h2o.init(nthreads = -<span class="hljs-number">1</span>)

&gt; <span class="hljs-comment"># Download the data into the pums2013 directory if necessary.</span>
&gt; pumsdir &lt;- <span class="hljs-string">"pums2013"</span>
&gt; <span class="hljs-keyword">if</span> (! file.exists(pumsdir)) {
&gt;   dir.create(pumsdir)
&gt; }

&gt; trainfile &lt;- file.path(pumsdir, <span class="hljs-string">"adult_2013_train.csv.gz"</span>)
&gt; <span class="hljs-keyword">if</span> (! file.exists(trainfile)) {
&gt;   download.file(<span class="hljs-string">"http://h2o-training.s3.amazonaws.com/pums2013/adult_2013_train.csv.gz"</span>, trainfile)
&gt; }

&gt; testfile  &lt;- file.path(pumsdir, <span class="hljs-string">"adult_2013_test.csv.gz"</span>)
&gt; <span class="hljs-keyword">if</span> (! file.exists(testfile)) {
&gt;   download.file(<span class="hljs-string">"http://h2o-training.s3.amazonaws.com/pums2013/adult_2013_test.csv.gz"</span>, testfile)
&gt; }
</code>

Load the datasets (change the directory to reflect where you stored these files):

<code class="lang-r">&gt; adult_2013_train &lt;- h2o.importFile(trainfile, destination_frame = <span class="hljs-string">"adult_2013_train"</span>)

&gt; adult_2013_test &lt;- h2o.importFile(testfile, destination_frame = <span class="hljs-string">"adult_2013_test"</span>)
</code>

Looking at the data, we can see that 8 columns are using integer codes to represent different categorical levels. 
 
Let&apos;s tell H2O to treat those columns as factors.

<code class="lang-r">&gt; actual_log_wagp &lt;- h2o.assign(adult_2013_test[, <span class="hljs-string">"LOG_WAGP"</span>], key = <span class="hljs-string">"actual_log_wagp"</span>)

&gt; <span class="hljs-keyword">for</span> (j <span class="hljs-keyword">in</span> c(<span class="hljs-string">"COW"</span>, <span class="hljs-string">"SCHL"</span>, <span class="hljs-string">"MAR"</span>, <span class="hljs-string">"INDP"</span>, <span class="hljs-string">"RELP"</span>, <span class="hljs-string">"RAC1P"</span>, <span class="hljs-string">"SEX"</span>, <span class="hljs-string">"POBP"</span>)) {
&gt;   adult_2013_train[[j]] &lt;- as.factor(adult_2013_train[[j]])
&gt;   adult_2013_test[[j]]  &lt;- as.factor(adult_2013_test[[j]])
&gt; }
</code>

<h3>Creating several models in H2O</h3>
Now that the data has been prepared, let&apos;s build a set of models using <a href="http://h2o-release.s3.amazonaws.com/h2o/rel-simons/7/docs-website/h2o-docs/index.html#Data%20Science%20Algorithms-GBM" target="_blank">GBM</a>. 
 
Here we will select the columns used as predictors and results, specify the validation data set, and then build a model.

<code class="lang-r">&gt; predset &lt;- c(<span class="hljs-string">"RELP"</span>, <span class="hljs-string">"SCHL"</span>, <span class="hljs-string">"COW"</span>, <span class="hljs-string">"MAR"</span>, <span class="hljs-string">"INDP"</span>, <span class="hljs-string">"RAC1P"</span>, <span class="hljs-string">"SEX"</span>, <span class="hljs-string">"POBP"</span>, <span class="hljs-string">"AGEP"</span>, <span class="hljs-string">"WKHP"</span>, <span class="hljs-string">"LOG_CAPGAIN"</span>, <span class="hljs-string">"LOG_CAPLOSS"</span>)

&gt; log_wagp_gbm_grid &lt;- h2o.gbm(x = predset,
     y = <span class="hljs-string">"LOG_WAGP"</span>,
     training_frame = adult_2013_train,
     model_id  = <span class="hljs-string">"GBMModel"</span>,
     distribution = <span class="hljs-string">"gaussian"</span>,
     max_depth = <span class="hljs-number">5</span>,
     ntrees = <span class="hljs-number">110</span>,
     validation_frame = adult_2013_test)
&gt; log_wagp_gbm

Model Details:
==============

H2ORegressionModel: gbm
Model ID:  GBMModel
Model Summary:
  number_of_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
<span class="hljs-number">1</span>      <span class="hljs-number">110.000000</span>       <span class="hljs-number">111698.000000</span>  <span class="hljs-number">5.000000</span>  <span class="hljs-number">5.000000</span>    <span class="hljs-number">5.00000</span>  <span class="hljs-number">14.000000</span>  <span class="hljs-number">32.000000</span>    <span class="hljs-number">27.93636</span>

H2ORegressionMetrics: gbm
** Reported on training data. 
**

MSE:  <span class="hljs-number">0.4626122</span>
R2 :  <span class="hljs-number">0.7362828</span>
Mean Residual Deviance :  <span class="hljs-number">0.4626122</span>

H2ORegressionMetrics: gbm
** Reported on validation data. 
**

MSE:  <span class="hljs-number">0.6605266</span>
R2 :  <span class="hljs-number">0.6290677</span>
Mean Residual Deviance :  <span class="hljs-number">0.6605266</span>
</code>

<h3>Export the best model as a POJO</h3>
From here, we can download this model as a Java <a href="https://en.wikipedia.org/wiki/Plain_Old_Java_Object" target="_blank">POJO</a> to a local directory called <code>generated_model</code>.

<code class="lang-r">&gt; tmpdir_name &lt;- <span class="hljs-string">"generated_model"</span>
&gt; dir.create(tmpdir_name)
&gt; h2o.download_pojo(log_wagp_gbm, tmpdir_name)
[<span class="hljs-number">1</span>] <span class="hljs-string">"POJO written to: generated_model/GBMModel.java"</span>
</code>

At this point, the Java POJO is available for scoring data outside of H2O. 
 
As the last step in R, let&apos;s take a look at the scores this model gives on the test data set. 
We will use these to confirm the results in Hive.

<code class="lang-r">&gt; h2o.predict(log_wagp_gbm, adult_2013_test)
H2OFrame with <span class="hljs-number">37345</span> rows and <span class="hljs-number">1</span> column

First <span class="hljs-number">10</span> rows:
     predict
<span class="hljs-number">1</span>  <span class="hljs-number">10.432787</span>
<span class="hljs-number">2</span>  <span class="hljs-number">10.244159</span>
<span class="hljs-number">3</span>  <span class="hljs-number">10.432688</span>
<span class="hljs-number">4</span>   <span class="hljs-number">9.604912</span>
<span class="hljs-number">5</span>  <span class="hljs-number">10.285979</span>
<span class="hljs-number">6</span>  <span class="hljs-number">10.356251</span>
<span class="hljs-number">7</span>  <span class="hljs-number">10.261413</span>
<span class="hljs-number">8</span>  <span class="hljs-number">10.046026</span>
<span class="hljs-number">9</span>  <span class="hljs-number">10.766078</span>
<span class="hljs-number">10</span>  <span class="hljs-number">9.502004</span>
</code>

<h2>Compile the H2O model as a part of UDF project</h2>
All code for this section can be found in this git repository. 
 
To simplify the build process, I have included a pom.xml file. 
 
For Maven users, this will automatically grab the dependencies you need to compile.

To use the template:

<ol>
Copy the Java from H2O into the project
Update the POJO to be part of the UDF package
Update the pom.xml to reflect your version of Hadoop and Hive
Compile
</ol>
<h3>Copy the java from H2O into the project</h3>
<code class="lang-bash">$ cp generated_model/h2o-genmodel.jar localjars
$ cp generated_model/GBMModel.java src/main/java/ai/h2o/hive/udf/GBMModel.java
</code>

<h3>Update the POJO to Be a Part of the Same Package as the UDF</h3>
To the top of <code>GBMModel.java</code>, add:

<code class="lang-Java"><span class="hljs-keyword">package</span> ai.h2o.hive.udf;
</code>

<h3>Update the pom.xml to Reflect Hadoop and Hive Versions</h3>
Get your version numbers using:

<code class="lang-bash">$ hadoop version
$ hive --version
</code>

And plug these into the <code>&lt;properties&gt;</code>  section of the <code>pom.xml</code> file. 
 
Currently, the configuration is set for pulling the necessary dependencies for Hortonworks. 
 
For other Hadoop distributions, you will also need to update the <code>&lt;repositories&gt;</code> section to reflect the respective repositories (a commented-out link to a Cloudera repository is included).

<h3>Compile</h3>
<blockquote>
Caution:  This tutorial was written using Maven 3.0.4. 
 
Older 2.x versions of Maven may not work.

</blockquote>
<code class="lang-bash">$ mvn compile
$ mvn package
</code>

As with most Maven builds, the first run will probably seem like it is downloading the entire Internet. 
 
It is just grabbing the needed compile dependencies. 
 
In the end, this process should create the file <code>target/ScoreData-1.0-SNAPSHOT.jar</code>.

As a part of the build process, Maven is running a unit test on the code. 
If you are looking to use this template for your own models, you either need to modify the test to reflect your own data, or run Maven without the test (<code>mvn package -Dmaven.test.skip=true</code>). 
 

<h2>Loading test data in Hive</h2>
Now load the same test data set into Hive. 
 
This will allow us to score the data in Hive and verify that the results are the same as what we saw in H2O.

<code class="lang-bash">$ hadoop fs -mkdir hdfs://my-name-node:/user/myhomedir/UDF<span class="hljs-built_in">test</span>
$ hadoop fs -put adult_2013_test.csv.gz  hdfs://my-name-node:/user/myhomedir/UDF<span class="hljs-built_in">test</span>/.
$ hive
</code>

Here we mark the table as <code>EXTERNAL</code> so that Hive doesn&apos;t make a copy of the file needlessly. 
 
We also tell Hive to ignore the first line, since it contains the column names.

<code class="lang-hive">&gt; CREATE EXTERNAL TABLE adult_data_set (AGEP INT, COW STRING, SCHL STRING, MAR STRING, INDP STRING, RELP STRING, RAC1P STRING, SEX STRING, WKHP INT, POBP STRING, WAGP INT, CAPGAIN INT, CAPLOSS INT, LOG_CAPGAIN DOUBLE, LOG_CAPLOSS DOUBLE, LOG_WAGP DOUBLE, CENT_WAGP STRING, TOP_WAG2P INT, RELP_SCHL STRING) COMMENT &apos;PUMS 2013 test data&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE location &apos;/user/myhomedir/UDFtest&apos; tblproperties ("skip.header.line.count"="1");
&gt; ANALYZE TABLE adult_data_set COMPUTE STATISTICS;
</code>

<h2>Copy the UDF to the cluster and load into Hive</h2>
<code class="lang-bash">$ hadoop fs -put localjars/h2o-genmodel.jar  hdfs://my-name-node:/user/myhomedir/
$ hadoop fs -put target/ScoreData-<span class="hljs-number">1.0</span>-SNAPSHOT.jar  hdfs://my-name-node:/user/myhomedir/
$ hive
</code>

Note that for correct class loading, you will need to load the h2o-model.jar before the ScoredData jar file.

<code class="lang-hive">&gt; ADD JAR h2o-genmodel.jar;
&gt; ADD JAR ScoreData-1.0-SNAPSHOT.jar;
&gt; CREATE TEMPORARY FUNCTION scoredata AS &apos;ai.h2o.hive.udf.ScoreDataUDF&apos;;
</code>

Keep in mind that your UDF is only loaded in Hive for as long as you are using it. 
 
If you <code>quit;</code> and then join Hive again, you will have to re-enter the last three lines.

<h2>Score with your UDF</h2>
Now the moment we&apos;ve been working towards:

<code class="lang-r"> hive&gt; SELECT scoredata(AGEP, COW, SCHL, MAR, INDP, RELP, RAC1P, SEX, WKHP, POBP, LOG_CAPGAIN, LOG_CAPLOSS) FROM adult_data_set LIMIT <span class="hljs-number">10</span>;
OK
<span class="hljs-number">10.476669</span>
<span class="hljs-number">10.201586</span>
<span class="hljs-number">10.463915</span>
<span class="hljs-number">9.709603</span>
<span class="hljs-number">10.175115</span>
<span class="hljs-number">10.3576145</span>
<span class="hljs-number">10.256757</span>
<span class="hljs-number">10.050725</span>
<span class="hljs-number">10.759903</span>
<span class="hljs-number">9.316141</span>
Time taken: <span class="hljs-number">0.063</span> seconds, Fetched: <span class="hljs-number">10</span> row(s)
</code>

<a name="Limitations"></a>

<h2>Limitations</h2>
This solution is fairly quick and easy to implement. 
 
Once you&apos;ve run through things once, going through steps 1-5 should be pretty painless. 
 
There are, however, a few things to be desired here.

The major trade-off made in this template has been a more generic design over strong input checking. 
 
 To be applicable for any POJO, the code only checks that the user-supplied arguments have the correct count and they are all at least primitive types. 
 
Stronger type checking could be done by generating Hive UDF code on a per-model basis.

Also, while the template isn&apos;t specific to any given model, it isn&apos;t completely flexible to the incoming data either. 
 
If you used 12 of 19 fields as predictors (as in this example), then you must feed the scoredata() UDF only those 12 fields, and in the order that the POJO expects. 
This is fine for a small number of predictors, but can be messy for larger numbers of predictors. 
 
Ideally, it would be nicer to say <code>SELECT scoredata(*) FROM adult_data_set;</code> and let the UDF pick out the relevant fields by name. 
 
While the H2O POJO does have utility functions for this, Hive, on the other hand, doesn&apos;t provide UDF writers the names of the fields (as mentioned in <a href="https://issues.apache.org/jira/browse/HIVE-3491" target="_blank">this</a> Hive feature request) from which the arguments originate.

Finally, as written, the UDF only returns a single prediction value. 
 
The H2O POJO actually returns an array of float values. 
 
The first value is the main prediction and the remaining values hold probability distributions for classifiers. 
 
This code can easily be expanded to return all values if desired.

<h2>A Look at the UDF Template</h2>
The template code starts with some basic annotations that define the nature of the UDF and display some simple help output when the user types <code>DESCRIBE scoredata</code> or <code>DESCRIBE EXTENDED scoredata</code>.

<code class="lang-Java"><span class="hljs-annotation">@UDFType</span>(deterministic = <span class="hljs-keyword">true</span>, stateful = <span class="hljs-keyword">false</span>)
<span class="hljs-annotation">@Description</span>(name=<span class="hljs-string">"scoredata"</span>, value=<span class="hljs-string">"_FUNC_(*) - Returns a score for the given row"</span>,
        extended=<span class="hljs-string">"Example:\n"</span>+<span class="hljs-string">"&gt; SELECT scoredata(*) FROM target_data;"</span>)
</code>

Rather than extend the plain UDF class, this template extends GenericUDF. 
 
The plain UDF requires that you hard code each of your input variables. 
 
This is fine for most UDFs, but for a function like scoring the number of columns used in scoring may be large enough to make this cumbersome. 
 
 Note the declaration of an array to hold ObjectInspectors for each argument, as well as the instantiation of the model POJO.

<code class="lang-Java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScoreDataUDF</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">GenericUDF</span> </span>{
  <span class="hljs-keyword">private</span> PrimitiveObjectInspector[] inFieldOI;
  GBMModel p = <span class="hljs-keyword">new</span> GBMModel();

  <span class="hljs-annotation">@Override</span>
  <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getDisplayString</span><span class="hljs-params">(String[] args)</span> </span>{
    <span class="hljs-keyword">return</span> <span class="hljs-string">"scoredata("</span>+Arrays.asList(p.getNames())+<span class="hljs-string">")."</span>;
  }
</code>

All GenericUDF children must implement initialize() and evaluate(). 
 
In initialize(), we see very basic argument type checking, initialization of ObjectInspectors for each argument, and declaration of the return type for this UDF. 
 
The accepted primitive type list here could easily be expanded if needed. 
BOOLEAN, CHAR, VARCHAR, and possibly TIMESTAMP and DATE might be useful to add.

<code class="lang-Java">  <span class="hljs-annotation">@Override</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> ObjectInspector <span class="hljs-title">initialize</span><span class="hljs-params">(ObjectInspector[] args)</span> <span class="hljs-keyword">throws</span> UDFArgumentException </span>{
    <span class="hljs-comment">// Basic argument count check</span>
    <span class="hljs-comment">// Expects one less argument than model used; results column is dropped</span>
    <span class="hljs-keyword">if</span> (args.length != p.getNumCols()) {
      <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentLengthException(<span class="hljs-string">"Incorrect number of arguments."</span> +
              <span class="hljs-string">"  scoredata() requires: "</span>+ Arrays.asList(p.getNames())
              +<span class="hljs-string">", in the listed order. 
Received "</span>+args.length+<span class="hljs-string">" arguments."</span>);
    }

    <span class="hljs-comment">//Check input types</span>
    inFieldOI = <span class="hljs-keyword">new</span> PrimitiveObjectInspector[args.length];
    PrimitiveObjectInspector.PrimitiveCategory pCat;
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; args.length; i++) {
      <span class="hljs-keyword">if</span> (args[i].getCategory() != ObjectInspector.Category.PRIMITIVE)
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Only takes primitive field types as parameters"</span>);
      pCat = ((PrimitiveObjectInspector) args[i]).getPrimitiveCategory();
      <span class="hljs-keyword">if</span> (pCat != PrimitiveObjectInspector.PrimitiveCategory.STRING
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.DOUBLE
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.FLOAT
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.LONG
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.INT
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.SHORT)
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Cannot accept type: "</span> + pCat.toString());
      inFieldOI[i] = (PrimitiveObjectInspector) args[i];
    }

    <span class="hljs-comment">// the return type of our function is a double, so we provide the correct object inspector</span>
    <span class="hljs-keyword">return</span> PrimitiveObjectInspectorFactory.javaDoubleObjectInspector;
  }
</code>

The real work is done in the evaluate() method. 
 
Again, some quick sanity checks are made on the arguments, then each argument is converted to a double. 
 
All H2O models take an array of doubles as their input. 
 
For integers, a simple casting is enough. 
 
For strings/enumerations, the double quotes are stripped, then the enumeration value for the given string/field index is retrieved, and then it is cast to a double. 
 
 Once all the arguments have been made into doubles, the model&apos;s predict() method is called to get a score. 
 
The main prediction for this row is then returned.

<code class="lang-Java">  <span class="hljs-annotation">@Override</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> Object <span class="hljs-title">evaluate</span><span class="hljs-params">(DeferredObject[] record)</span> <span class="hljs-keyword">throws</span> HiveException </span>{
    <span class="hljs-comment">// Expects one less argument than model used; results column is dropped</span>
    <span class="hljs-keyword">if</span> (record != <span class="hljs-keyword">null</span>) {
      <span class="hljs-keyword">if</span> (record.length == p.getNumCols()) {
        <span class="hljs-keyword">double</span>[] data = <span class="hljs-keyword">new</span> <span class="hljs-keyword">double</span>[record.length];
        <span class="hljs-comment">//Sadly, HIVE UDF doesn&apos;t currently make the field name available.</span>
        <span class="hljs-comment">//Thus this UDF must depend solely on the arguments maintaining the same</span>
        <span class="hljs-comment">// field order seen by the original H2O model creation.</span>
        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; record.length; i++) {
          <span class="hljs-keyword">try</span> {
            Object o = inFieldOI[i].getPrimitiveJavaObject(record[i].get());
            <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> java.lang.String) {
              <span class="hljs-comment">// Hive wraps strings in double quotes, remove</span>
              data[i] = p.mapEnum(i, ((String) o).replace(<span class="hljs-string">"\"</span>, <span class="hljs-string">"</span>));
              <span class="hljs-keyword">if</span> (data[i] == -<span class="hljs-number">1</span>)
                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): The value "</span> + (String) o
                    + <span class="hljs-string">" is not a known category for column "</span> + p.getNames()[i]);
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Double) {
              data[i] = ((Double) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Float) {
              data[i] = ((Float) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Long) {
              data[i] = ((Long) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Integer) {
              data[i] = ((Integer) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Short) {
              data[i] = ((Short) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o == <span class="hljs-keyword">null</span>) {
              <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;
            } <span class="hljs-keyword">else</span> {
              <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Cannot accept type: "</span>
                  + o.getClass().toString() + <span class="hljs-string">" for argument # "</span> + i + <span class="hljs-string">"."</span>);
            }
          } <span class="hljs-keyword">catch</span> (Throwable e) {
            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"Unexpected exception on argument # "</span> + i + <span class="hljs-string">". 
"</span> + e.toString());
          }
        }
        <span class="hljs-comment">// get the predictions</span>
        <span class="hljs-keyword">try</span> {
          <span class="hljs-keyword">double</span>[] preds = <span class="hljs-keyword">new</span> <span class="hljs-keyword">double</span>[p.getPredsSize()];
          p.score0(data, preds);
          <span class="hljs-keyword">return</span> preds[<span class="hljs-number">0</span>];
        } <span class="hljs-keyword">catch</span> (Throwable e) {
          <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"H2O predict function threw exception: "</span> + e.toString());
        }
      } <span class="hljs-keyword">else</span> {
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"Incorrect number of arguments."</span> +
            <span class="hljs-string">"  scoredata() requires: "</span> + Arrays.asList(p.getNames()) + <span class="hljs-string">", in order. 
Received "</span>
            +record.length+<span class="hljs-string">" arguments."</span>);
      }
    } <span class="hljs-keyword">else</span> { <span class="hljs-comment">// record == null</span>
      <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>; <span class="hljs-comment">//throw new UDFArgumentException("scoredata() received a NULL row.");</span>
    }
  }
</code>

Really, almost all the work is in type detection and conversion.

<h2>Summary</h2>
That&apos;s it. 
 
The given template should work for most cases. 
 
As mentioned in the <a href="#Limitations">limitations</a> section, two major modifications could be done. 
 
Some users may desire handling for a few more primitive types. 
 
Other users might want stricter type checking. 
 
There are two options for the latter: either use the template as the basis for auto-generating type checking UDF code on a per model basis, or create a Hive client application and call the UDF from the client. 
 
A Hive client could handle type checking and field alignment, since it would both see the table level information and invoke the UDF.

<h2>Hive UDF MOJO Example</h2>
This tutorial describes how to use a <a href="https://maven.apache.org/developers/mojo-api-specification.html" target="_blank">MOJO</a> model created in H2O to create a Hive UDF (user-defined function) for scoring data. 
 
 While the fastest scoring typically results from ingesting data files in HDFS directly into H2O for scoring, there may be several motivations not to do so. 
 
For example, the clusters used for model building may be research clusters, and the data to be scored may be on "production" clusters. 
 
In other cases, the final data set to be scored may be too large to reasonably score in-memory. 
 
To help with these kinds of cases, this document walks through how to take a scoring model from H2O, plug it into a template UDF project, and use it to score in Hive. 
 
All the code needed for this walkthrough can be found in this repository branch.

<h2>The Goal</h2>
The desired work flow for this task is:

<ol>
Load training and test data into H2O
Create several models in H2O
Export the best model as a <a href="https://maven.apache.org/developers/mojo-api-specification.html" target="_blank">MOJO</a>
Compile the H2O model as a part of the UDF project
Copy the UDF to the cluster and load into Hive
Score with your UDF
</ol>
For steps 1-3, we will give instructions scoring the data through R. 
 
We will add a step between 4 and 5 to load some test data for this example.

<h2>Requirements</h2>
This tutorial assumes the following:

<ol>
Some familiarity with using H2O in R. 
 
Getting started tutorials can be found <a href="http://docs.0xdata.com/newuser/top.html" target="_blank">here</a>.
The ability to compile Java code. 
 
The repository provides a pom.xml file, so using Maven will be the simplest way to compile, but IntelliJ IDEA will also read in this file. 
 
If another build system is preferred, it is left to the reader to figure out the compilation details.
A working Hive install to test the results.
</ol>
<h2>The Data</h2>
For this post, we will be using a 0.1% sample of the Person-Level 2013 Public Use Microdata Sample (PUMS) from United States Census Bureau. 
 
75% of that sample is designated as the training data set and 25% as the test data set. 
This data set is intended as an update to the <a href="https://archive.ics.uci.edu/ml/datasets/Adult" target="_blank">UCI Adult Data Set</a>. 
 
The two datasets are available <a href="https://h2o-training.s3.amazonaws.com/pums2013/adult_2013_train.csv.gz" target="_blank">here</a> and <a href="https://h2o-training.s3.amazonaws.com/pums2013/adult_2013_test.csv.gz" target="_blank">here</a>.

The goal of the analysis in this demo is to predict if an income exceeds $50K/yr based on census data. 
 
The columns we will be using are:

AGEP:  age
COW: class of worker
SCHL: educational attainment
MAR: marital status
INDP: Industry code
RELP: relationship
RAC1P: race
SEX: gender
WKHP: hours worked per week
POBP: Place of birth code
LOG_CAPGAIN: log of capital gains
LOG_CAPLOSS: log of capital losses
LOG_WAGP: log of wages or salary

<h2>Building the Model in R</h2>
No need to cut and paste code: the complete R script described below is part of this git repository (GBM-example.R).

<h3>Load the training and test data into H2O</h3>
Since we are playing with a small data set for this example, we will start H2O locally and load the datasets:

<code class="lang-r">&gt; <span class="hljs-keyword">library</span>(h2o)
&gt; h2o.init(nthreads = -<span class="hljs-number">1</span>)

&gt; <span class="hljs-comment"># Download the data into the pums2013 directory if necessary.</span>
&gt; pumsdir &lt;- <span class="hljs-string">"pums2013"</span>
&gt; <span class="hljs-keyword">if</span> (! file.exists(pumsdir)) {
&gt;   dir.create(pumsdir)
&gt; }

&gt; trainfile &lt;- file.path(pumsdir, <span class="hljs-string">"adult_2013_train.csv.gz"</span>)
&gt; <span class="hljs-keyword">if</span> (! file.exists(trainfile)) {
&gt;   download.file(<span class="hljs-string">"http://h2o-training.s3.amazonaws.com/pums2013/adult_2013_train.csv.gz"</span>, trainfile)
&gt; }

&gt; testfile  &lt;- file.path(pumsdir, <span class="hljs-string">"adult_2013_test.csv.gz"</span>)
&gt; <span class="hljs-keyword">if</span> (! file.exists(testfile)) {
&gt;   download.file(<span class="hljs-string">"http://h2o-training.s3.amazonaws.com/pums2013/adult_2013_test.csv.gz"</span>, testfile)
&gt; }
</code>

Load the datasets (change the directory to reflect where you stored these files):

<code class="lang-r">&gt; adult_2013_train &lt;- h2o.importFile(trainfile, destination_frame = <span class="hljs-string">"adult_2013_train"</span>)

&gt; adult_2013_test &lt;- h2o.importFile(testfile, destination_frame = <span class="hljs-string">"adult_2013_test"</span>)
</code>

Looking at the data, we can see that 8 columns are using integer codes to represent different categorical levels. 
 
Let&apos;s tell H2O to treat those columns as factors.

<code class="lang-r">&gt; actual_log_wagp &lt;- h2o.assign(adult_2013_test[, <span class="hljs-string">"LOG_WAGP"</span>], key = <span class="hljs-string">"actual_log_wagp"</span>)

&gt; <span class="hljs-keyword">for</span> (j <span class="hljs-keyword">in</span> c(<span class="hljs-string">"COW"</span>, <span class="hljs-string">"SCHL"</span>, <span class="hljs-string">"MAR"</span>, <span class="hljs-string">"INDP"</span>, <span class="hljs-string">"RELP"</span>, <span class="hljs-string">"RAC1P"</span>, <span class="hljs-string">"SEX"</span>, <span class="hljs-string">"POBP"</span>)) {
&gt;   adult_2013_train[[j]] &lt;- as.factor(adult_2013_train[[j]])
&gt;   adult_2013_test[[j]]  &lt;- as.factor(adult_2013_test[[j]])
&gt; }
</code>

<h3>Creating several models in H2O</h3>
Now that the data has been prepared, let&apos;s build a set of models using <a href="http://h2o-release.s3.amazonaws.com/h2o/rel-simons/7/docs-website/h2o-docs/index.html#Data%20Science%20Algorithms-GBM" target="_blank">GBM</a>. 
 
Here we will select the columns used as predictors and results, specify the validation data set, and then build a model.

<code class="lang-r">&gt; predset &lt;- c(<span class="hljs-string">"RELP"</span>, <span class="hljs-string">"SCHL"</span>, <span class="hljs-string">"COW"</span>, <span class="hljs-string">"MAR"</span>, <span class="hljs-string">"INDP"</span>, <span class="hljs-string">"RAC1P"</span>, <span class="hljs-string">"SEX"</span>, <span class="hljs-string">"POBP"</span>, <span class="hljs-string">"AGEP"</span>, <span class="hljs-string">"WKHP"</span>, <span class="hljs-string">"LOG_CAPGAIN"</span>, <span class="hljs-string">"LOG_CAPLOSS"</span>)

&gt; log_wagp_gbm_grid &lt;- h2o.gbm(x = predset,
     y = <span class="hljs-string">"LOG_WAGP"</span>,
     training_frame = adult_2013_train,
     model_id  = <span class="hljs-string">"GBMModel"</span>,
     distribution = <span class="hljs-string">"gaussian"</span>,
     max_depth = <span class="hljs-number">5</span>,
     ntrees = <span class="hljs-number">110</span>,
     validation_frame = adult_2013_test)
&gt; log_wagp_gbm

Model Details:
==============

H2ORegressionModel: gbm
Model ID:  GBMModel
Model Summary:
  number_of_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves mean_leaves
<span class="hljs-number">1</span>      <span class="hljs-number">110.000000</span>       <span class="hljs-number">111698.000000</span>  <span class="hljs-number">5.000000</span>  <span class="hljs-number">5.000000</span>    <span class="hljs-number">5.00000</span>  <span class="hljs-number">14.000000</span>  <span class="hljs-number">32.000000</span>    <span class="hljs-number">27.93636</span>

H2ORegressionMetrics: gbm
** Reported on training data. 
**

MSE:  <span class="hljs-number">0.4626122</span>
R2 :  <span class="hljs-number">0.7362828</span>
Mean Residual Deviance :  <span class="hljs-number">0.4626122</span>

H2ORegressionMetrics: gbm
** Reported on validation data. 
**

MSE:  <span class="hljs-number">0.6605266</span>
R2 :  <span class="hljs-number">0.6290677</span>
Mean Residual Deviance :  <span class="hljs-number">0.6605266</span>
</code>

<h3>Export the best model as a MOJO</h3>
From here, we can download this model as a Java <a href="https://maven.apache.org/developers/mojo-api-specification.html" target="_blank">MOJO</a> to a local directory called <code>generated_model</code>.

<code class="lang-r">&gt; tmpdir_name &lt;- <span class="hljs-string">"generated_model"</span>
&gt; dir.create(tmpdir_name)
&gt; h2o.download_mojo(log_wagp_gbm, tmpdir_name)
[<span class="hljs-number">1</span>] <span class="hljs-string">"MOJO written to: generated_model/GBMModel.zip"</span>
</code>

At this point, the Java MOJO is available for scoring data outside of H2O. 
 
As the last step in R, let&apos;s take a look at the scores this model gives on the test data set. 
We will use these to confirm the results in Hive.

<code class="lang-r">&gt; h2o.predict(log_wagp_gbm, adult_2013_test)
H2OFrame with <span class="hljs-number">37345</span> rows and <span class="hljs-number">1</span> column

First <span class="hljs-number">10</span> rows:
     predict
<span class="hljs-number">1</span>  <span class="hljs-number">10.432787</span>
<span class="hljs-number">2</span>  <span class="hljs-number">10.244159</span>
<span class="hljs-number">3</span>  <span class="hljs-number">10.432688</span>
<span class="hljs-number">4</span>   <span class="hljs-number">9.604912</span>
<span class="hljs-number">5</span>  <span class="hljs-number">10.285979</span>
<span class="hljs-number">6</span>  <span class="hljs-number">10.356251</span>
<span class="hljs-number">7</span>  <span class="hljs-number">10.261413</span>
<span class="hljs-number">8</span>  <span class="hljs-number">10.046026</span>
<span class="hljs-number">9</span>  <span class="hljs-number">10.766078</span>
<span class="hljs-number">10</span>  <span class="hljs-number">9.502004</span>
</code>

<h2>Compile the H2O model as a part of UDF project</h2>
All code for this section can be found in this git repository. 
 
To simplify the build process, I have included a pom.xml file. 
 
For Maven users, this will automatically grab the dependencies you need to compile.

To use the template:

<ol>
Copy the Java from H2O into the project
Update the MOJO to be part of the UDF package
Update the pom.xml to reflect your version of Hadoop and Hive
Compile
</ol>
<h3>Copy the java from H2O into the project</h3>
<code class="lang-bash">$ cp generated_model/h2o-genmodel.jar localjars
$ <span class="hljs-built_in">cd</span> src/main/
$ mkdir resources
$ cp generated_model/GBMModel.zip src/main/java/resources/ai/h2o/hive/udf/GBMModel.zip
</code>

<h3>Verify File Structure</h3>
Ensure that your file structure looks exactly like this repository. 
Your MOJO model needs to be in
a new resources folder with the file path as shown above or else the project will not compile.

<h3>Update the pom.xml to Reflect Hadoop and Hive Versions</h3>
Get your version numbers using:

<code class="lang-bash">$ hadoop version
$ hive --version
</code>

And plug these into the <code>&lt;properties&gt;</code>  section of the <code>pom.xml</code> file. 
 
Currently, the configuration is set for pulling the necessary dependencies for Hortonworks. 
 
For other Hadoop distributions, you will also need to update the <code>&lt;repositories&gt;</code> section to reflect the respective repositories (a commented-out link to a Cloudera repository is included).

<h3>Compile</h3>
<blockquote>
Caution:  This tutorial was written using Maven 3.5.0. 
 
Older 2.x versions of Maven may not work.

</blockquote>
<code class="lang-bash">$ mvn compile
$ mvn package -Dmaven.test.skip=<span class="hljs-literal">true</span>
</code>

As with most Maven builds, the first run will probably seem like it is downloading the entire Internet. 
 
It is just grabbing the needed compile dependencies. 
 
In the end, this process should create the file <code>target/ScoreData-1.0-SNAPSHOT.jar</code>.

As a part of the build process, Maven is running a unit test on the code. 
If you are looking to use this template for your own models, you either need to modify the test to reflect your own data, or run Maven without the test (<code>mvn package -Dmaven.test.skip=true</code>). 
 

<h2>Loading test data in Hive</h2>
Now load the same test data set into Hive. 
 
This will allow us to score the data in Hive and verify that the results are the same as what we saw in H2O.

<code class="lang-bash">$ hadoop fs -mkdir hdfs://my-name-node:/user/myhomedir/UDF<span class="hljs-built_in">test</span>
$ hadoop fs -put adult_2013_test.csv.gz  hdfs://my-name-node:/user/myhomedir/UDF<span class="hljs-built_in">test</span>/.
$ hive
</code>

Here we mark the table as <code>EXTERNAL</code> so that Hive doesn&apos;t make a copy of the file needlessly. 
 
We also tell Hive to ignore the first line, since it contains the column names.

<code class="lang-hive">&gt; CREATE EXTERNAL TABLE adult_data_set (AGEP INT, COW STRING, SCHL STRING, MAR STRING, INDP STRING, RELP STRING, RAC1P STRING, SEX STRING, WKHP INT, POBP STRING, WAGP INT, CAPGAIN INT, CAPLOSS INT, LOG_CAPGAIN DOUBLE, LOG_CAPLOSS DOUBLE, LOG_WAGP DOUBLE, CENT_WAGP STRING, TOP_WAG2P INT, RELP_SCHL STRING) COMMENT &apos;PUMS 2013 test data&apos; ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;,&apos; STORED AS TEXTFILE location &apos;/user/myhomedir/UDFtest&apos; tblproperties ("skip.header.line.count"="1");
&gt; ANALYZE TABLE adult_data_set COMPUTE STATISTICS;
</code>

<h2>Copy the UDF to the cluster and load into Hive</h2>
<code class="lang-bash">$ hadoop fs -put localjars/h2o-genmodel.jar  hdfs://my-name-node:/user/myhomedir/
$ hadoop fs -put target/ScoreData-<span class="hljs-number">1.0</span>-SNAPSHOT.jar  hdfs://my-name-node:/user/myhomedir/
$ hive
</code>

Note that for correct class loading, you will need to load the h2o-model.jar before the ScoredData jar file.

<code class="lang-hive">&gt; ADD JAR h2o-genmodel.jar;
&gt; ADD JAR ScoreData-1.0-SNAPSHOT.jar;
&gt; CREATE TEMPORARY FUNCTION scoredata AS &apos;ai.h2o.hive.udf.ScoreDataUDF&apos;;
</code>

Keep in mind that your UDF is only loaded in Hive for as long as you are using it. 
 
If you <code>quit;</code> and then join Hive again, you will have to re-enter the last three lines.

<h2>Score with your UDF</h2>
Now the moment we&apos;ve been working towards:

<code class="lang-r"> hive&gt; SELECT scoredata(AGEP, COW, SCHL, MAR, INDP, RELP, RAC1P, SEX, WKHP, POBP, LOG_CAPGAIN, LOG_CAPLOSS) FROM adult_data_set LIMIT <span class="hljs-number">10</span>;
OK
<span class="hljs-number">10.476669</span>
<span class="hljs-number">10.201586</span>
<span class="hljs-number">10.463915</span>
<span class="hljs-number">9.709603</span>
<span class="hljs-number">10.175115</span>
<span class="hljs-number">10.3576145</span>
<span class="hljs-number">10.256757</span>
<span class="hljs-number">10.050725</span>
<span class="hljs-number">10.759903</span>
<span class="hljs-number">9.316141</span>
Time taken: <span class="hljs-number">0.063</span> seconds, Fetched: <span class="hljs-number">10</span> row(s)
</code>

<a name="Limitations"></a>

<h2>Limitations</h2>
This solution is fairly quick and easy to implement. 
 
Once you&apos;ve run through things once, going through steps 1-5 should be pretty painless. 
 
There are, however, a few things to be desired here.

The major trade-off made in this template has been a more generic design over strong input checking. 
 
 To be applicable for any MOJO, the code only checks that the user-supplied arguments have the correct count and they are all at least primitive types. 
 
Stronger type checking could be done by generating Hive UDF code on a per-model basis.

Also, while the template isn&apos;t specific to any given model, it isn&apos;t completely flexible to the incoming data either. 
 
If you used 12 of 19 fields as predictors (as in this example), then you must feed the scoredata() UDF only those 12 fields, and in the order that the MOJO expects. 
This is fine for a small number of predictors, but can be messy for larger numbers of predictors. 
 
Ideally, it would be nicer to say <code>SELECT scoredata(*) FROM adult_data_set;</code> and let the UDF pick out the relevant fields by name. 
 
While the H2O MOJO does have utility functions for this, Hive, on the other hand, doesn&apos;t provide UDF writers the names of the fields (as mentioned in <a href="https://issues.apache.org/jira/browse/HIVE-3491" target="_blank">this</a> Hive feature request) from which the arguments originate.

Finally, as written, the UDF only returns a single prediction value. 
 
The H2O MOJO actually returns an array of float values. 
 
The first value is the main prediction and the remaining values hold probability distributions for classifiers. 
 
This code can easily be expanded to return all values if desired.

<h2>A Look at the UDF Template</h2>
The template code starts with some basic annotations that define the nature of the UDF and display some simple help output when the user types <code>DESCRIBE scoredata</code> or <code>DESCRIBE EXTENDED scoredata</code>.

<code class="lang-Java"><span class="hljs-annotation">@UDFType</span>(deterministic = <span class="hljs-keyword">true</span>, stateful = <span class="hljs-keyword">false</span>)
<span class="hljs-annotation">@Description</span>(name=<span class="hljs-string">"scoredata"</span>, value=<span class="hljs-string">"_FUNC_(*) - Returns a score for the given row"</span>,
        extended=<span class="hljs-string">"Example:\n"</span>+<span class="hljs-string">"&gt; SELECT scoredata(*) FROM target_data;"</span>)
</code>

Rather than extend the plain UDF class, this template extends GenericUDF. 
 
The plain UDF requires that you hard code each of your input variables. 
 
This is fine for most UDFs, but for a function like scoring the number of columns used in scoring may be large enough to make this cumbersome. 
 
 Note the declaration of an array to hold ObjectInspectors for each argument, as well as the instantiation of the model MOJO.

<code class="lang-Java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ScoreDataUDF</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">GenericUDF</span> </span>{
  <span class="hljs-keyword">private</span> PrimitiveObjectInspector[] inFieldOI;

  MojoModel p;

  <span class="hljs-annotation">@Override</span>
  <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getDisplayString</span><span class="hljs-params">(String[] args)</span> </span>{
    <span class="hljs-keyword">return</span> <span class="hljs-string">"scoredata("</span>+Arrays.asList(p.getNames())+<span class="hljs-string">")."</span>;
  }
</code>

All GenericUDF children must implement initialize() and evaluate(). 
 
In initialize(), we see very basic argument type checking, initialization of ObjectInspectors for each argument, and declaration of the return type for this UDF. 
 
The accepted primitive type list here could easily be expanded if needed. 
BOOLEAN, CHAR, VARCHAR, and possibly TIMESTAMP and DATE might be useful to add.

<code class="lang-Java">  <span class="hljs-annotation">@Override</span>
  <span class="hljs-function"><span class="hljs-keyword">public</span> ObjectInspector <span class="hljs-title">initialize</span><span class="hljs-params">(ObjectInspector[] args)</span> <span class="hljs-keyword">throws</span> UDFArgumentException </span>{
    <span class="hljs-comment">// Get the MOJO as a resource</span>
    URL mojoURL = ScoreDataUDF.class.getResource(<span class="hljs-string">"GBMModel.zip"</span>);
    <span class="hljs-comment">// Declare r as a MojoReaderBackend</span>
    MojoReaderBackend r;
    <span class="hljs-comment">// Read the MOJO and assign it to p</span>
    <span class="hljs-keyword">try</span> {
      r = MojoReaderBackendFactory.createReaderBackend(mojoURL, CachingStrategy.MEMORY);
      p = ModelMojoReader.readFrom(r);
    } <span class="hljs-keyword">catch</span> (IOException e) {
      <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> RuntimeException(e);
    }
    <span class="hljs-comment">// Basic argument count check</span>
    <span class="hljs-comment">// Expects one less argument than model used; results column is dropped</span>
    <span class="hljs-keyword">if</span> (args.length != p.getNumCols()) {
      <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentLengthException(<span class="hljs-string">"Incorrect number of arguments."</span> +
              <span class="hljs-string">"  scoredata() requires: "</span>+ Arrays.asList(p.getNames())
              +<span class="hljs-string">", in the listed order. 
Received "</span>+args.length+<span class="hljs-string">" arguments."</span>);
    }

    <span class="hljs-comment">//Check input types</span>
    inFieldOI = <span class="hljs-keyword">new</span> PrimitiveObjectInspector[args.length];
    PrimitiveObjectInspector.PrimitiveCategory pCat;
    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; args.length; i++) {
      <span class="hljs-keyword">if</span> (args[i].getCategory() != ObjectInspector.Category.PRIMITIVE)
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Only takes primitive field types as parameters"</span>);
      pCat = ((PrimitiveObjectInspector) args[i]).getPrimitiveCategory();
      <span class="hljs-keyword">if</span> (pCat != PrimitiveObjectInspector.PrimitiveCategory.STRING
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.DOUBLE
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.FLOAT
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.LONG
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.INT
              & pCat != PrimitiveObjectInspector.PrimitiveCategory.SHORT)
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Cannot accept type: "</span> + pCat.toString());
      inFieldOI[i] = (PrimitiveObjectInspector) args[i];
    }

    <span class="hljs-comment">// the return type of our function is a double, so we provide the correct object inspector</span>
    <span class="hljs-keyword">return</span> PrimitiveObjectInspectorFactory.javaDoubleObjectInspector;
  }
</code>

The real work is done in the evaluate() method. 
 
Again, some quick sanity checks are made on the arguments, then each argument is converted to a double. 
 
All H2O models take an array of doubles as their input. 
 
For integers, a simple casting is enough. 
 
For strings/enumerations, the double quotes are stripped, then the enumeration value for the given string/field index is retrieved, and then it is cast to a double. 
 
 Once all the arguments have been made into doubles, the model&apos;s predict() method is called to get a score. 
 
The main prediction for this row is then returned.

<code class="lang-Java">  <span class="hljs-annotation">@Override</span>
    <span class="hljs-function"><span class="hljs-keyword">public</span> Object <span class="hljs-title">evaluate</span><span class="hljs-params">(DeferredObject[] record)</span> <span class="hljs-keyword">throws</span> HiveException </span>{
    <span class="hljs-comment">// Expects one less argument than model used; results column is dropped</span>
    <span class="hljs-keyword">if</span> (record != <span class="hljs-keyword">null</span>) {
      <span class="hljs-keyword">if</span> (record.length == p.getNumCols()) {
        <span class="hljs-keyword">double</span>[] data = <span class="hljs-keyword">new</span> <span class="hljs-keyword">double</span>[record.length];
        <span class="hljs-comment">//Sadly, HIVE UDF doesn&apos;t currently make the field name available.</span>
        <span class="hljs-comment">//Thus this UDF must depend solely on the arguments maintaining the same</span>
        <span class="hljs-comment">// field order seen by the original H2O model creation.</span>
        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; record.length; i++) {
          <span class="hljs-keyword">try</span> {
            Object o = inFieldOI[i].getPrimitiveJavaObject(record[i].get());
            <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> java.lang.String) {
              <span class="hljs-comment">// Hive wraps strings in double quotes, remove</span>
              data[i] = p.mapEnum(i, ((String) o).replace(<span class="hljs-string">"\"</span>, <span class="hljs-string">"</span>));
              <span class="hljs-keyword">if</span> (data[i] == -<span class="hljs-number">1</span>)
                <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): The value "</span> + (String) o
                    + <span class="hljs-string">" is not a known category for column "</span> + p.getNames()[i]);
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Double) {
              data[i] = ((Double) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Float) {
              data[i] = ((Float) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Long) {
              data[i] = ((Long) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Integer) {
              data[i] = ((Integer) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o <span class="hljs-keyword">instanceof</span> Short) {
              data[i] = ((Short) o).doubleValue();
            } <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (o == <span class="hljs-keyword">null</span>) {
              <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>;
            } <span class="hljs-keyword">else</span> {
              <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"scoredata(...): Cannot accept type: "</span>
                  + o.getClass().toString() + <span class="hljs-string">" for argument # "</span> + i + <span class="hljs-string">"."</span>);
            }
          } <span class="hljs-keyword">catch</span> (Throwable e) {
            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"Unexpected exception on argument # "</span> + i + <span class="hljs-string">". 
"</span> + e.toString());
          }
        }
        <span class="hljs-comment">// get the predictions</span>
        <span class="hljs-keyword">try</span> {
          <span class="hljs-keyword">double</span>[] preds = <span class="hljs-keyword">new</span> <span class="hljs-keyword">double</span>[p.getPredsSize()];
          p.score0(data, preds);
          <span class="hljs-keyword">return</span> preds[<span class="hljs-number">0</span>];
        } <span class="hljs-keyword">catch</span> (Throwable e) {
          <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"H2O predict function threw exception: "</span> + e.toString());
        }
      } <span class="hljs-keyword">else</span> {
        <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> UDFArgumentException(<span class="hljs-string">"Incorrect number of arguments."</span> +
            <span class="hljs-string">"  scoredata() requires: "</span> + Arrays.asList(p.getNames()) + <span class="hljs-string">", in order. 
Received "</span>
            +record.length+<span class="hljs-string">" arguments."</span>);
      }
    } <span class="hljs-keyword">else</span> { <span class="hljs-comment">// record == null</span>
      <span class="hljs-keyword">return</span> <span class="hljs-keyword">null</span>; <span class="hljs-comment">//throw new UDFArgumentException("scoredata() received a NULL row.");</span>
    }
  }
</code>

Really, almost all the work is in type detection and conversion.

<h2>Summary</h2>
That&apos;s it. 
 
The given template should work for most cases. 
 
As mentioned in the <a href="#Limitations">limitations</a> section, two major modifications could be done. 
 
Some users may desire handling for a few more primitive types. 
 
Other users might want stricter type checking. 
 
There are two options for the latter: either use the template as the basis for auto-generating type checking UDF code on a per model basis, or create a Hive client application and call the UDF from the client. 
 
A Hive client could handle type checking and field alignment, since it would both see the table level information and invoke the UDF.

<h2><span class="orange">Ensembles: Stacking, Super Learner</span></h2>
<a href="#overview">Overview</a>
<a href="#what-is-ensemble-learning">What is Ensemble Learning?</a><a href="#bagging">Bagging</a>
<a href="#boosting">Boosting</a>
<a href="#stacking--super-learning">Stacking / Super Learning</a>

<a href="#h2o-stacked-ensemble">H2O Stacked Ensemble</a>

<h2><span class="orange">Overview</span></h2>
In this tutorial, we will discuss ensemble learning with a focus on a type of ensemble learning called stacking or Super Learning. 
In this tutorial, we present an H2O implementation of the Super Learner algorithm (aka Stacking, Stacked Ensembles).

H2O&#x2019;s Stacked Ensemble method is a supervised ensemble machine learning algorithm that finds the optimal combination of a collection of prediction algorithms using a process called stacking. 
like all supervised models in H2O, Stacked Ensemble supports regression, binary classification, and multiclass classification. 
The documentation for H2O Stacked Ensembles, including R and Python code examples, can be found <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.html" target="_blank">here</a>.

<h2><span class="orange">What is Ensemble Learning?</span></h2>
Ensemble machine learning methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms.

Many of the popular modern machine learning algorithms are actually ensembles. 
 
For example, <a href="https://en.wikipedia.org/wiki/Random_forest" target="_blank">Random Forest</a> and <a href="https://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">Gradient Boosting Machine</a> are both ensemble learners.

Common types of ensembles:

<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="_blank">Bagging</a>
<a href="https://en.wikipedia.org/wiki/Boosting_%28machine_learning%29" target="_blank">Boosting</a>
<a href="https://en.wikipedia.org/wiki/Ensemble_learning#Stacking" target="_blank">Stacking</a>

<h2>Bagging</h2>
Bootstrap aggregating, or bagging, is an ensemble method designed to improve the stability and accuracy of machine learning algorithms. 
 
It reduces variance and helps to avoid overfitting. 
 
Bagging is a special case of the model averaging approach and is relatively robust against noisy data and outliers.

One of the most well known bagging ensembles is the Random Forest algorithm, which applies bagging to decision trees.

<h2>Boosting</h2>
Boosting is an ensemble method designed to reduce bias and variance. 
 
A boosting algorithm iteratively learns weak classifiers and adds them to a final strong classifier. 
 
 

After a weak learner is added, the data is reweighted: examples that are misclassified gain weight and examples that are classified correctly lose weight. 
Thus, future weak learners focus more on the examples that previous weak learners misclassified. 
 
This causes boosting methods to be not very robust to noisy data and outliers.

Both bagging and boosting are ensembles that take a collection of weak learners and forms a single, strong learner.

<h2>Stacking / Super Learning</h2>
Stacking is a broad class of algorithms that involves training a second-level "metalearner" to ensemble a group of base learners. 
The type of ensemble learning implemented in H2O is called "super learning", "stacked regression" or "stacking."  Unlike bagging and boosting, the goal in stacking is to ensemble strong, diverse sets of learners together.

<h3>Some Background</h3>
<a href="https://en.wikipedia.org/wiki/Leo_Breiman" target="_blank">Leo Breiman</a>, known for his work on classification and regression trees and the creator of the Random Forest algorithm, formalized stacking in his 1996 paper, <a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/367.pdf" target="_blank">"Stacked Regressions"</a>. 
 
Although the idea originated with <a href="https://en.wikipedia.org/wiki/David_Wolpert" target="_blank">David Wolpert</a> in 1992 under the name <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.1533" target="_blank">"Stacked Generalization"</a>, the modern form of stacking that uses internal k-fold cross-validation was Dr. 
Breiman&apos;s contribution.

However, it wasn&apos;t until 2007 that the theoretical background for stacking was developed, which is when the algorithm took on the name, "Super Learner". 
 
Until this time, the mathematical reasons for why stacking worked were unknown and stacking was considered a "black art."  The Super Learner algorithm learns the optimal combination of the base learner fits. 
In an article titled, <a href="http://dx.doi.org/10.2202/1544-6115.1309" target="_blank">"Super Learner"</a>, by <a href="http://www.stat.berkeley.edu/~laan/Laan/laan.html" target="_blank">Mark van der Laan</a> et al., proved that the Super Learner ensemble represents an asymptotically optimal system for learning.

<h3>Super Learner Algorithm</h3>
Here is an outline of the tasks involved in training and testing a Super Learner ensemble:

<h4>Set up the ensemble</h4>
Specify a list of L base algorithms (with a specific set of model parameters).
Specify a metalearning algorithm.

<h4>Train the ensemble</h4>
Train each of the L base algorithms on the training set.
Perform k-fold cross-validation on each of these learners and collect the cross-validated predicted values from each of the L algorithms.
The N cross-validated predicted values from each of the L algorithms can be combined to form a new N x L matrix. 
 
This matrix, along wtih the original response vector, is called the "level-one" data. 
 
(N = number of rows in the training set)
Train the metalearning algorithm on the level-one data.
The "ensemble model" consists of the L base learning models and the metalearning model, which can then be used to generate predictions on a test set.

<h4>Predict on new data</h4>
To generate ensemble predictions, first generate predictions from the base learners.
Feed those predictions into the metalearner to generate the ensemble prediction.

<h2><span class="orange">H2O Stacked Ensemble in R</span></h2>
<h2>Install H2O R Package</h2>
First you need to install the H2O R package if you don&#x2019;t already have it installed. 
It an be downloaded from CRAN or from the H2O website at: <a href="http://h2o.ai/download" target="_blank">http://h2o.ai/download</a>. 


<h2>Higgs Demo</h2>
This is an example of binary classification using the <code>h2o.stackedEnsemble</code> function. 
This demo uses a subset of the <a href="https://archive.ics.uci.edu/ml/datasets/HIGGS" target="_blank">HIGGS dataset</a>, which has 28 numeric features and a binary response. 
 
The machine learning task in this example is to distinguish between a signal process which produces Higgs bosons (Y = 1) and a background process which does not (Y = 0). 
 
The dataset contains approximately the same number of positive vs negative examples. 
 
In other words, this is a balanced, rather than imbalanced, dataset.

To run this script, be sure to <code>setwd()</code> to the location of this script. 
<code>h2o.init()</code> starts H2O in R&#x2019;s current working directory. 
<code>h2o.importFile()</code> looks for files from the perspective of where H2O was started.

<h3>Start H2O Cluster</h3>
<code class="lang-r"><span class="hljs-keyword">library</span>(h2o)
h2o.init()
</code>

<h3>Load Data into H2O Cluster</h3>
First, import a sample binary outcome train and test set into the H2O cluster.

<code class="lang-r"><span class="hljs-comment"># Import a sample binary outcome train/test set into H2O</span>
train &lt;- h2o.importFile(<span class="hljs-string">"https://s3.amazonaws.com/erin-data/higgs/higgs_train_10k.csv"</span>)
test &lt;- h2o.importFile(<span class="hljs-string">"https://s3.amazonaws.com/erin-data/higgs/higgs_test_5k.csv"</span>)
</code>

Identify predictors and response:

<code>y &lt;- "response"
x &lt;- setdiff(names(train), y)
</code>
For binary classification, the response should be encoded as a <a href="http://stat.ethz.ch/R-manual/R-patched/library/base/html/factor.html" target="_blank">factor</a> type (also known as the <a href="https://docs.oracle.com/javase/tutorial/java/javaOO/enum.html" target="_blank">enum</a> type in Java or <a href="http://pandas.pydata.org/pandas-docs/stable/categorical.html" target="_blank">categorial</a> in Python Pandas). 
 
The user can specify column types in the <code>h2o.importFile</code> command, or you can convert the response column as follows:

<code class="lang-r">train[,y] &lt;- as.factor(train[,y])  
test[,y] &lt;- as.factor(test[,y])
</code>

Number of CV folds (to generate level-one data for stacking):

<code class="lang-r">nfolds &lt;- <span class="hljs-number">5</span>
</code>

<h3>Train an Ensemble</h3>
There are a few ways to assemble a list of models to stack together: 

<ol>
Train individual models and put them in a list 
Train a grid of models
Train several grids of models 
</ol>
We demonstrate some of these methods below.

<strong>Note:</strong> In order to use a model for stacking you must set <code>keep_cross_validation_predctions = TRUE</code> because the Stacked Ensemble algorithm requires the cross-validation predictions to train the metalaerner algorithm (unless you use a <a href="docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/blending_frame.html">blending frame</a>.

<h4>1. 
Generate a 2-model ensemble (GBM + RF)</h4>
<code class="lang-r"><span class="hljs-comment"># Train & cross-validate a GBM:</span>
my_gbm &lt;- h2o.gbm(x = x,
                  y = y,
                  training_frame = train,
                  distribution = &#x201C;bernoulli&#x201D;,
                  ntrees = <span class="hljs-number">10</span>,
                  max_depth = <span class="hljs-number">3</span>,
                  min_rows = <span class="hljs-number">2</span>,
                  learn_rate = <span class="hljs-number">0.2</span>,
                  nfolds = nfolds,
                  keep_cross_validation_predictions = <span class="hljs-literal">TRUE</span>,
                  seed = <span class="hljs-number">1</span>)

<span class="hljs-comment"># Train & cross-validate a RF:</span>
my_rf &lt;- h2o.randomForest(x = x,
  y = y,
  training_frame = train,
  ntrees = <span class="hljs-number">50</span>,
  nfolds = nfolds,
  keep_cross_validation_predictions = <span class="hljs-literal">TRUE</span>,
  seed = <span class="hljs-number">1</span>)

<span class="hljs-comment"># Train a stacked ensemble using the GBM and RF above:</span>
ensemble &lt;- h2o.stackedEnsemble(x = x,
        y = y,
        training_frame = train,
        base_models = list(my_gbm, my_rf))
</code>

<h5>Eval the ensemble performance on a test set:</h5>
Since the the response is binomial, we can use Area Under the ROC Curve (AUC) to evaluate the model performance. 
Compute test set performance, and sort by AUC (the default metric that is printed for a binomial classification):

<code class="lang-r">perf &lt;- h2o.performance(ensemble, newdata = test)
ensemble_auc_test &lt;- h2o.auc(perf)
</code>

<h5>Compare to the base learner performance on the test set.</h5>
We can compare the performance of the ensemble to the performance of the individual learners in the ensemble.

<code class="lang-r">perf_gbm_test &lt;- h2o.performance(my_gbm, newdata = test)
perf_rf_test &lt;- h2o.performance(my_rf, newdata = test)
baselearner_best_auc_test &lt;- max(h2o.auc(perf_gbm_test), 
         h2o.auc(perf_rf_test))

print(sprintf(&#x201C;Best Base-learner Test AUC: %s&#x201D;, baselearner_best_auc_test))
print(sprintf(&#x201C;Ensemble Test AUC: %s&#x201D;, ensemble_auc_test))
<span class="hljs-comment"># [1] "Best Base-learner Test AUC:  0.76979821502548"</span>
<span class="hljs-comment"># [1] "Ensemble Test AUC:  0.773501212640419"</span>
</code>

So we see the best individual algorithm in this group is the GBM with a test set AUC of 0.7735, as compared to 0.7698 for the ensemble. 
At first thought, this might not seem like much, but in many industries like medicine or finance, this small advantage can be highly valuable.

To increase the performance of the ensemble, we have several options. 
One of them is to increase the number of cross-validation folds using the <code>nfolds</code> argument. 
The other options are to change the base learner library or the metalearning algorithm.

<h5>Generate predictions on a test set (if necessary):</h5>
<code class="lang-r">pred &lt;- h2o.predict(ensemble, newdata = test)
</code>

<h3>2. 
Generate a Random Grid of Models and Stack Them Together</h3>
<code class="lang-r"><span class="hljs-comment"># GBM Hyperparamters</span>
learn_rate_opt &lt;- c(<span class="hljs-number">0.01</span>, <span class="hljs-number">0.03</span>)
max_depth_opt &lt;- c(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>)
sample_rate_opt &lt;- c(<span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>, <span class="hljs-number">0.9</span>, <span class="hljs-number">1.0</span>)
col_sample_rate_opt &lt;- c(<span class="hljs-number">0.2</span>, <span class="hljs-number">0.3</span>, <span class="hljs-number">0.4</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.6</span>, <span class="hljs-number">0.7</span>, <span class="hljs-number">0.8</span>)
hyper_params &lt;- list(learn_rate = learn_rate_opt,
                     max_depth = max_depth_opt,
                     sample_rate = sample_rate_opt,
                     col_sample_rate = col_sample_rate_opt)

search_criteria &lt;- list(strategy = <span class="hljs-string">"RandomDiscrete"</span>,
max_models = <span class="hljs-number">3</span>,
seed = <span class="hljs-number">1</span>)

gbm_grid &lt;- h2o.grid(algorithm = <span class="hljs-string">"gbm"</span>,
                     grid_id = <span class="hljs-string">"gbm_grid_binomial"</span>,
                     x = x,
                     y = y,
                     training_frame = train,
                     ntrees = <span class="hljs-number">10</span>,
                     seed = <span class="hljs-number">1</span>,
                     nfolds = nfolds,
                     keep_cross_validation_predictions = <span class="hljs-literal">TRUE</span>,
                     hyper_params = hyper_params,
                     search_criteria = search_criteria)

<span class="hljs-comment"># Train a stacked ensemble using the GBM grid</span>
ensemble &lt;- h2o.stackedEnsemble(x = x,
        y = y,
        training_frame = train,
        base_models = gbm_grid@model_ids)

<span class="hljs-comment"># Eval ensemble performance on a test set</span>
perf &lt;- h2o.performance(ensemble, newdata = test)

<span class="hljs-comment"># Compare to base learner performance on the test set</span>
.getauc &lt;- <span class="hljs-keyword">function</span>(mm) h2o.auc(h2o.performance(h2o.getModel(mm), newdata = test))
baselearner_aucs &lt;- sapply(gbm_grid@model_ids, .getauc)
baselearner_best_auc_test &lt;- max(baselearner_aucs)
ensemble_auc_test &lt;- h2o.auc(perf)
print(sprintf(<span class="hljs-string">"Best Base-learner Test AUC:  %s"</span>, baselearner_best_auc_test))
print(sprintf(<span class="hljs-string">"Ensemble Test AUC:  %s"</span>, ensemble_auc_test))
<span class="hljs-comment"># [1] "Best Base-learner Test AUC:  0.748146530400473"</span>
<span class="hljs-comment"># [1] "Ensemble Test AUC:  0.773501212640419"</span>

<span class="hljs-comment"># Generate predictions on a test set (if neccessary)</span>
pred &lt;- h2o.predict(ensemble, newdata = test)
</code>

<h3>All done, shutdown H2O</h3>
<code class="lang-r">h2o.shutdown()
</code>

<h2>Roadmap for H2O Stacked Ensemble</h2>
Open tickets for the native H2O version of Stacked Ensembles can be found <a href="https://0xdata.atlassian.net/issues/?filter=19301" target="_blank">here</a> (JIRA tickets with the "StackedEnsemble" tag).

<h2><span class="orange">Real-time Predictions With H2O on Storm</span></h2>
This tutorial shows how to create a <a href="https://storm.apache.org/" target="_blank">Storm</a> topology can be used to make real-time predictions with <a href="http://h2o.ai" target="_blank">H2O</a>.

<h2>1. 
 
What this tutorial covers</h2>
<br>

In this tutorial, we explore a combined modeling and streaming workflow as seen in the picture below:

<img src="images/h2o_storm.png" alt="">

We produce a GBM model by running H2O and emitting a Java POJO used for scoring. 
 
The POJO is very lightweight and does not depend on any other libraries, not even H2O. 
 
As such, the POJO is perfect for embedding into third-party environments, like a Storm bolt.

This tutorial walks you through the following sequence:

Installing the required software
A brief discussion of the data
Using R to build a gbm model in H2O
Exporting the gbm model as a Java POJO
Copying the generated POJO files into a Storm bolt build environment
Building Storm and the bolt for the model
Running a Storm topology with your model deployed
Watching predictions in real-time

(Note that R is not strictly required, but is used for convenience by this tutorial.)

<h2>2. 
 
Installing the required software</h2>
<h3>2.1. 
 
Clone the required repositories from Github</h3>
<code>git clone https://github.com/apache/storm.git 
git clone https://github.com/h2oai/h2o-world-2015-training.git
</code>
<em>NOTE</em>: Building storm (c.f. 
<a href="#BuildStorm">Section 5</a>) requires <a href="http://maven.apache.org/" target="_blank">Maven</a>. 
You can install Maven (version 3.x) by following the <a href="http://maven.apache.org/download.cgi" target="_blank">Maven installation instructions</a>.

Navigate to the directory for this tutorial inside the h2o-world-2015-training repository:

<code>cd h2o-world-2015-training/tutorials/streaming/storm
</code>
You should see the following files in this directory:

<strong><em>README.md</em></strong> (This document)
<strong><em>example.R</em></strong> (The R script that builds the GBM model and exports it as a Java POJO)
<strong><em>training_data.csv</em></strong> (The data used to build the GBM model)
<strong><em>live_data.csv</em></strong> (The data that predictions are made on; used to feed the spout in the Storm topology)
<strong><em>H2OStormStarter.java</em></strong> (The Storm topology with two bolts:  a prediction bolt and a classifying bolt)
<strong><em>TestH2ODataSpout.java</em></strong> (The Storm spout which reads data from the live_data.csv file and passes each observation to the prediction bolt one observation at a time; this simulates the arrival of data in real-time)

And the following directories:

<strong><em>premade_generated_model</em></strong> (For those people who have trouble building the model but want to try running with Storm anyway; you can ignore this directory if you successfully build your own generated_model later in the tutorial)
<strong><em>images</em></strong> (Images for the tutorial documentation, you can ignore these)
<strong><em>web</em></strong> (Contains the html and image files for watching the real-time prediction output (c.f. 
<a href="#real_time">Section 8</a>))

<h3>2.2. 
 
Install R</h3>
Get the <a href="http://www.r-project.org/index.html" target="_blank">latest version of R from CRAN</a> and install it on your computer.

<h3>2.3. 
 
Install the H2O package for R</h3>
<blockquote>
Note:  The H2O package for R includes both the R code as well as the main H2O jar file. 
 
This is all you need to run H2O locally on your laptop.

</blockquote>
Step 1:  Start R (at the command line or via RStudio)   

Step 2:  Install H2O from CRAN  

<code>install.packages("h2o")
</code>
Note:  For convenience, this tutorial was created with the <a href="http://h2o-release.s3.amazonaws.com/h2o/rel-slater/3/index.html" target="_blank">Slater</a> stable release of H2O (3.2.0.3) from CRAN, as shown above. 
 
Later versions of H2O will also work.

<h3>2.4. 
 
Development environment</h3>
This tutorial was developed with the following software environment. 
 
(Other environments will work, but this is what we used to develop and test this tutorial.)

H2O 3.3.0.99999 (Slater)
MacOS X (Mavericks)
java version "1.7.0_79"
R 3.2.2
Storm git hash: 99285bb719357760f572d6f4f0fb4cd02a8fd389
curl 7.30.0 (x86_64-apple-darwin13.0) libcurl/7.30.0 SecureTransport zlib/1.2.5
Maven (Apache Maven 3.3.3)

For viewing predictions in real-time (<a href="#real_time">Section 8</a>) you will need the following:

npm (1.3.11)  (<code>brew install npm</code>)
http-server   (<code>npm install http-server -g</code>)
A modern web browser (animations depend on <a href="http://d3js.org/" target="_blank">D3</a>)

<h2>3. 
 
A brief discussion of the data</h2>
Let&apos;s take a look at a small piece of the training_data.csv file for a moment. 
 
This is a synthetic data set created for this tutorial.

<code>head -n 20 training_data.csv
</code>
<table><thead>
<tr><th>Label</th><th>Has4Legs</th><th>CoatColor</th><th>HairLength</th><th>TailLength</th><th>EnjoysPlay</th><th>StaresOutWindow</th><th>HoursSpentNapping</th><th>RespondsToCommands</th><th>EasilyFrightened</th><th>Age</th><th>Noise1</th><th>Noise2</th><th>Noise3</th><th>Noise4</th><th>Noise5</th></tr></thead>
<tbody>
<tr><td>dog</td><td>1</td><td>Brown</td><td>0</td><td>2</td><td>1</td><td>1</td><td>2</td><td>1</td><td>0</td><td>4</td><td>0.852352328598499</td><td>0.229839221341535</td><td>0.576096264412627</td><td>0.0105558061040938</td><td>0.470826978096738</td></tr>
<tr><td>dog</td><td>1</td><td>Brown</td><td>1</td><td>1</td><td>1</td><td>1</td><td>5</td><td>0</td><td>0</td><td>16</td><td>0.928460991941392</td><td>0.98618565662764</td><td>0.553872474469244</td><td>0.932764369761571</td><td>0.435074317501858</td></tr>
<tr><td>dog</td><td>1</td><td>Grey</td><td>1</td><td>10</td><td>1</td><td>1</td><td>2</td><td>1</td><td>0</td><td>5</td><td>0.658247262472287</td><td>0.379703616956249</td><td>0.767817151267081</td><td>0.840509128058329</td><td>0.538852979661897</td></tr>
<tr><td>dog</td><td>1</td><td>Grey</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td>2</td><td>0.210346511797979</td><td>0.912498287158087</td><td>0.757371880114079</td><td>0.915149037493393</td><td>0.27393517526798</td></tr>
<tr><td>dog</td><td>1</td><td>Brown</td><td>1</td><td>5</td><td>1</td><td>0</td><td>10</td><td>1</td><td>0</td><td>20</td><td>0.770219849422574</td><td>0.999768516747281</td><td>0.482816896401346</td><td>0.904691722244024</td><td>0.232283475110307</td></tr>
<tr><td>cat</td><td>1</td><td>Grey</td><td>1</td><td>6</td><td>1</td><td>1</td><td>3</td><td>0</td><td>1</td><td>10</td><td>0.499049366684631</td><td>0.690937616396695</td><td>0.00580681697465479</td><td>0.516113663092256</td><td>0.161103375256062</td></tr>
<tr><td>dog</td><td>1</td><td>Spotted</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td>17</td><td>0.980622073402628</td><td>0.193929805886</td><td>0.50500241224654</td><td>0.848579460754991</td><td>0.750856031663716</td></tr>
<tr><td>cat</td><td>1</td><td>Spotted</td><td>1</td><td>7</td><td>0</td><td>1</td><td>5</td><td>0</td><td>1</td><td>9</td><td>0.298585452139378</td><td>0.425832540960982</td><td>0.816698056645691</td><td>0.0246927759144455</td><td>0.692579888971522</td></tr>
<tr><td>dog</td><td>1</td><td>Grey</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td><td>3</td><td>0.724013194208965</td><td>0.120883409865201</td><td>0.754467910155654</td><td>0.43663241318427</td><td>0.0592612794134766</td></tr>
<tr><td>cat</td><td>1</td><td>Black</td><td>0</td><td>7</td><td>0</td><td>1</td><td>5</td><td>0</td><td>1</td><td>5</td><td>0.849093642551452</td><td>0.0961945767048746</td><td>0.588080670218915</td><td>0.0478771082125604</td><td>0.211781785823405</td></tr>
<tr><td>dog</td><td>1</td><td>Grey</td><td>1</td><td>1</td><td>1</td><td>0</td><td>2</td><td>0</td><td>1</td><td>1</td><td>0.362678906414658</td><td>0.54775956296362</td><td>0.522148486692458</td><td>0.903857592027634</td><td>0.496479033492506</td></tr>
<tr><td>dog</td><td>1</td><td>Spotted</td><td>0</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>0</td><td>3</td><td>0.745238043367863</td><td>0.0181446429342031</td><td>0.33444849960506</td><td>0.550831729080528</td><td>0.625747208483517</td></tr>
<tr><td>dog</td><td>1</td><td>Spotted</td><td>1</td><td>4</td><td>1</td><td>1</td><td>2</td><td>1</td><td>0</td><td>20</td><td>0.693285189568996</td><td>0.69526576064527</td><td>0.386858200887218</td><td>0.235119538847357</td><td>0.401590927504003</td></tr>
<tr><td>cat</td><td>1</td><td>Spotted</td><td>1</td><td>8</td><td>1</td><td>1</td><td>3</td><td>0</td><td>0</td><td>3</td><td>0.695167713565752</td><td>0.81692309374921</td><td>0.530564708868042</td><td>0.081766308285296</td><td>0.277844901895151</td></tr>
<tr><td>cat</td><td>1</td><td>White</td><td>1</td><td>8</td><td>0</td><td>1</td><td>5</td><td>0</td><td>0</td><td>3</td><td>0.0237249641213566</td><td>0.867370987776667</td><td>0.855278167175129</td><td>0.284646768355742</td><td>0.566314383875579</td></tr>
<tr><td>cat</td><td>1</td><td>Black</td><td>1</td><td>5</td><td>1</td><td>1</td><td>2</td><td>0</td><td>1</td><td>16</td><td>0.281967194052413</td><td>0.798100406536832</td><td>0.306403951486573</td><td>0.681048742029816</td><td>0.237810888560489</td></tr>
<tr><td>cat</td><td>1</td><td>Grey</td><td>1</td><td>7</td><td>1</td><td>1</td><td>3</td><td>1</td><td>1</td><td>16</td><td>0.178538456792012</td><td>0.566589535912499</td><td>0.297640548087656</td><td>0.634627313353121</td><td>0.677242929581553</td></tr>
<tr><td>cat</td><td>1</td><td>Spotted</td><td>1</td><td>8</td><td>0</td><td>0</td><td>10</td><td>0</td><td>1</td><td>3</td><td>0.219212393043563</td><td>0.482533045113087</td><td>0.739678716054186</td><td>0.132942436495796</td><td>0.100684949662536</td></tr>
</tbody>
</table>
Note that the first row in the training data set is a header row specifying the column names.

The response column (i.e. 
the "y" column) we want to make predictions for is Label. 
 
It&apos;s a binary column, so we want to build a classification model. 
 
The response column is categorical, and contains two levels, &apos;cat&apos; and &apos;dog&apos;. 
 
Note that the ratio of dogs to cats is 3:1.

The remaining columns are all input features (i.e. 
the "x" columns) we use to predict whether each new observation is a &apos;cat&apos; or a &apos;dog&apos;. 
 
The input features are a mix of integer, real, and categorical columns.

<h2>4. 
 
Using R to build a gbm model in H2O and export it as a Java POJO</h2>
<a name="RPOJO"></a>

<h3>4.1. 
 
Build and export the model</h3>
The <code>example.R</code> script builds the model and exports the Java POJO to the generated_model temporary directory. 
 
Run <code>example.R</code> at the command line as follows:

<code class="lang-bash">R <span class="hljs-operator">-f</span> example.R
</code>

You will see the following output:

<code class="lang-{r}">
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin13.4.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type &apos;license()&apos; or &apos;licence()&apos; for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type &apos;contributors()&apos; for more information and
&apos;citation()&apos; on how to cite R or R packages in publications.

Type &apos;demo()&apos; for some demos, &apos;help()&apos; for on-line help, or
&apos;help.start()&apos; for an HTML browser interface to help.
Type &apos;q()&apos; to quit R.

&gt; #
&gt; # Example R code for generating an H2O Scoring POJO.
&gt; #
&gt; 
&gt; # "Safe" system. 
 
Error checks process exit status code. 
 
stop() if it failed.
&gt; safeSystem <- function(x)="" {="" +="" print(sprintf("+="" CMD:="" %s",="" x))="" res="" <-="" system(x)="" print(res)="" if="" (res="" !="0)" msg="" sprintf("SYSTEM="" COMMAND="" FAILED="" (exit="" status="" %d)",="" res)="" stop(msg)="" }=""> 
&gt; library(h2o)
Loading required package: statmod

----------------------------------------------------------------------

Your next step is to start H2O:
    &gt; h2o.init()

For H2O package documentation, ask for help:
    &gt; ??h2o

After starting H2O, you can use the Web UI at http://localhost:54321
For more information visit http://docs.h2o.ai

----------------------------------------------------------------------

Attaching package: &#x2018;h2o&#x2019;

The following objects are masked from &#x2018;package:stats&#x2019;:

    sd, var

The following objects are masked from &#x2018;package:base&#x2019;:

    %*%, %in%, apply, as.factor, as.numeric, colnames, colnames<-, ifelse,="" is.factor,="" is.numeric,="" log,="" range,="" trunc=""> 
&gt; cat("Starting H2O\n")
Starting H2O
&gt; myIP <- "localhost"=""> myPort <- 54321=""> h <- 1="" 2="" 8="" 738="" h2o.init(ip="myIP," port="myPort," startH2O="TRUE)" H2O="" is="" not="" running="" yet,="" starting="" it="" now...="" Note:="" In="" case="" of="" errors="" look="" at="" the="" following="" log="" files:="" var="" folders="" ct="" mv0lk53d5lq6bkvm_2snjgm00000gn="" T="" RtmpkEUbAR="" h2o_ludirehak_started_from_r.out="" h2o_ludirehak_started_from_r.err="" java="" version="" "1.7.0_79"="" Java(TM)="" SE="" Runtime="" Environment="" (build="" 1.7.0_79-b15)="" Java="" HotSpot(TM)="" 64-Bit="" Server="" VM="" 24.79-b02,="" mixed="" mode)="" ..Successfully="" connected="" to="" http:="" localhost:54321="" R="" cluster:="" cluster="" uptime:="" seconds="" milliseconds="" version:="" 3.3.0.99999="" name:="" H2O_started_from_R_ludirehak_dwh703="" total="" nodes:="" memory:="" 3.56="" GB="" cores:="" allowed="" healthy:="" TRUE="" As="" started,="" limited="" CRAN="" default CPUs.="" Shut="" down="" and="" restart="" as="" shown="" below="" use="" all="" your=""> h2o.shutdown()
           &gt; h2o.init(nthreads = -1)

&gt; 
&gt; cat("Building GBM model\n")
Building GBM model
&gt; df <- h2o.importFile(path="normalizePath("./training_data.csv"));" |="=====================================================================|" 100%=""> y <- "Label"=""> x <- c("Has4Legs","CoatColor","HairLength","TailLength","EnjoysPlay","StaresOutWindow","HoursSpentNapping","RespondsToCommands","EasilyFrightened","Age",="" "Noise1",="" "Noise2",="" "Noise3",="" "Noise4",="" "Noise5")=""> gbm.h2o.fit <- h2o.gbm(training_frame="df," y="y," x="x," model_id="GBMPojo" ,="" ntrees="10)" |="=====================================================================|" 100%=""> 
&gt; cat("Downloading Java prediction model code from H2O\n")
Downloading Java prediction model code from H2O
&gt; model_id <- gbm.h2o.fit@model_id=""> 
&gt; tmpdir_name <- "generated_model"=""> cmd <- sprintf("rm="" -fr="" %s",="" tmpdir_name)=""> safeSystem(cmd)
[1] "+ CMD: rm -fr generated_model"
[1] 0
&gt; cmd <- sprintf("mkdir="" %s",="" tmpdir_name)=""> safeSystem(cmd)
[1] "+ CMD: mkdir generated_model"
[1] 0
&gt; 
&gt; h2o.download_pojo(gbm.h2o.fit, "./generated_model/")
[1] "POJO written to: ./generated_model//GBMPojo.java"
&gt; 
&gt; cat("Note: H2O will shut down automatically if it was started by this R script and the script exits\n")
Note: H2O will shut down automatically if it was started by this R script and the script exits
&gt;
</-></-></-></-></-></-></-></-></-></-></-></-,></-></code>

<h3>4.2. 
 
Look at the output</h3>
The generated_model directory is created and now contains two files:

<code>ls -l generated_model</code>  

<code>ls -l generated_model/
total 72
-rw-r--r--  1 ludirehak  staff  19764 Sep 25 12:36 GBMPojo.java
-rw-r--r--  1 ludirehak  staff  23655 Sep 25 12:36 h2o-genmodel.jar
</code>
The h2o-genmodel.jar file contains the interface definition, and the GBMPojo.java file contains the Java code for the POJO model.

The following three sections from the generated model are of special importance.

<h4>4.2.1. 
 
Class name</h4>
<code>public class GBMPojo extends GenModel {
</code>
This is the class to instantiate in the Storm bolt to make predictions. 


<h4>4.2.2. 
 
Predict method</h4>
<code>public final double[] score0( double[] data, double[] preds )
</code>
score0() is the method to call to make a single prediction for a new observation. 
 
<strong><em>data</em></strong> is the input, and <strong><em>preds</em></strong> is the output. 
 
The return value is just <strong><em>preds</em></strong>, and can be ignored.

Inputs and Outputs must be numerical. 
 
Categorical columns must be translated into numerical values using the DOMAINS mapping on the way in. 
 
Even if the response is categorical, the result will be numerical. 
 
It can be mapped back to a level string using DOMAINS, if desired. 
 
When the response is categorical, the preds response is structured as follows:

<code>preds[0] contains the predicted level number
preds[1] contains the probability that the observation is level0
preds[2] contains the probability that the observation is level1
...
preds[N] contains the probability that the observation is levelN-1

sum(preds[1] ... 
preds[N]) == 1.0
</code>
In this specific case, that means:

<code>preds[0] contains 0 or 1
preds[1] contains the probability that the observation is ColInfo_15.VALUES[0]
preds[2] contains the probability that the observation is ColInfo_15.VALUES[1]
</code>
<h4>4.2.3. 
 
DOMAINS array</h4>
<code>  // Column domains. 
The last array contains domain of response column.
  public static final String[][] DOMAINS = new String[][] {
    /* Has4Legs */ null,
    /* CoatColor */ GBMPojo_ColInfo_1.VALUES,
    /* HairLength */ null,
    /* TailLength */ null,
    /* EnjoysPlay */ null,
    /* StaresOutWindow */ null,
    /* HoursSpentNapping */ null,
    /* RespondsToCommands */ null,
    /* EasilyFrightened */ null,
    /* Age */ null,
    /* Noise1 */ null,
    /* Noise2 */ null,
    /* Noise3 */ null,
    /* Noise4 */ null,
    /* Noise5 */ null,
    /* Label */ GBMPojo_ColInfo_15.VALUES
  };
</code>
The DOMAINS array contains information about the level names of categorical columns. 
 
Note that Label (the column we are predicting) is the last entry in the DOMAINS array.

<a name="BuildStorm"></a>

<h2>5. 
 
Building Storm and the bolt for the model</h2>
<h3>5.1 Build storm and import into IntelliJ</h3>
To build storm navigate to the cloned repo and install via Maven:

<code>cd storm & mvn clean install -DskipTests=true</code>  

Once storm is built, open up your favorite IDE to start building the h2o streaming topology. 
In this tutorial, we will be using <a href="https://www.jetbrains.com/idea/" target="_blank">IntelliJ</a>.

To import the storm-starter project into your IntelliJ please follow these screenshots:

Click on "Import Project" and find the storm repo. 
Select storm-starter and click "OK"<br><img src="images/ij_1.png" alt="">

Import the project from extrenal model using Maven, click "Next"<br><img src="images/ij_2.png" alt="">

Ensure that "Import Maven projects automatically" check box is clicked (it&apos;s off by default), click "Next"<br><img src="images/ij_3.png" alt="">

That&apos;s it! Now click through the remaining prompts (Next -&gt; Next -&gt; Next -&gt; Finish).

Once inside the project, open up <em>storm-starter/test/jvm/storm.starter</em>. 
Yes, we&apos;ll be working out of the test directory.

<h3>5.2  Build the topology</h3>
The topology we&apos;ve prepared has one spout TestH2ODataSpout and two bolts (a "Predict Bolt" and a "Classifier Bolt"). 
Please copy the pre-built bolts and spout into the <em>test</em> directory in IntelliJ. 


Edit L100 of H2OStormStarter.java so that the file path is: <code>PATH_TO_H2O_WORLD_2015_TRAINING/h2o-world-2015-training/tutorials/streaming/storm/web/out</code>

Likewise, edit L46 of TestH2ODataSpout.java so that the file path is: <code>PATH_TO_H2O_WORLD_2015_TRAINING/h2o-world-2015-training/tutorials/streaming/storm/live_data.csv</code>

Now copy.

<code>cp H2OStormStarter.java /PATH_TO_STORM/storm/examples/storm-starter/test/jvm/storm/starter/</code>  

<code>cp TestH2ODataSpout.java /PATH_TO_STORM/storm/examples/storm-starter/test/jvm/storm/starter/</code>  

Your project should now look like this:

<img src="images/ij_4.png" alt="">

<h2>6. 
 
Copying the generated POJO files into a Storm bolt build environment</h2>
We are now ready to import the H2O pieces into the IntelliJ project. 
We&apos;ll need to add the <em>h2o-genmodel.jar</em> and the scoring POJO.

To import the <em>h2o-genmodel.jar</em> into your IntelliJ project, please follow these screenshots:

File &gt; Project Structure&#x2026;<br><img src="images/ij_6.png" alt="">

Click the "+" to add a new dependency<br><img src="images/ij_7.png" alt="">

Click on Jars or directories&#x2026;<br><img src="images/ij_8.png" alt="">

Find the h2o-genmodel.jar that we previously downloaded with the R script in <a href="#RPOJO">section 4</a><br><img src="images/ij_9.png" alt="">

Click "OK", then "Apply", then "OK".

You now have the h2o-genmodel.jar as a dependency in your project.

Modify GBMPojo.java to add <code>package storm.starter;</code> as the first line.

<code>sed -i -e &apos;1i\&apos;$&apos;\n&apos;&apos;package storm.starter;&apos;$&apos;\n&apos; ./generated_model/GBMPojo.java</code>

We now copy over the POJO from <a href="#RPOJO">section 4</a> into our storm project.

<code>cp ./generated_model/GBMPojo.java /PATH_TO_STORM/storm/examples/storm-starter/test/jvm/storm/starter/</code>  

<strong><em>OR</em></strong> if you were not able to build the GBMPojo, copy over the pre-made version:

<code>cp ./premade_generated_model/GBMPojo.java /PATH_TO_STORM/storm/examples/storm-starter/test/jvm/storm/starter/</code>

If copying over the pre-made version of GBMPojo, also repeat the above steps in this section to import the pre-made <em>h2o-genmodel.jar</em> from the <em>premade_generated_model</em> directory.

Your storm-starter project directory should now look like this:

<img src="images/ij_10.png" alt="">

In order to use the GBMPojo class, our <strong><em>PredictionBolt</em></strong> in H2OStormStarter has the following "execute" block:

<code>    @Override public void execute(Tuple tuple) {

      GBMPojo p = new GBMPojo();

      // get the input tuple as a String[]
      ArrayList&lt;String&gt; vals_string = new ArrayList&lt;String&gt;();
      for (Object v : tuple.getValues()) vals_string.add((String)v);
      String[] raw_data = vals_string.toArray(new String[vals_string.size()]);

      // the score pojo requires a single double[] of input.
      // We handle all of the categorical mapping ourselves
      double data[] = new double[raw_data.length-1]; //drop the Label

      String[] colnames = tuple.getFields().toList().toArray(new String[tuple.size()]);

      // if the column is a factor column, then look up the value, otherwise put the double
      for (int i = 1; i &lt; raw_data.length; ++i) {
        data[i-1] = p.getDomainValues(colnames[i]) == null
                ? Double.valueOf(raw_data[i])
                : p.mapEnum(p.getColIdx(colnames[i]), raw_data[i]);
      }

      // get the predictions
      double[] preds = new double [GBMPojo.NCLASSES+1];
      //p.predict(data, preds);
      p.score0(data, preds);

      // emit the results
      _collector.emit(tuple, new Values(raw_data[0], preds[1]));
      _collector.ack(tuple);
    }
</code>
The probability emitted is the probability of being a &apos;dog&apos;. 
We use this probability to decide whether the observation is of type &apos;cat&apos; or &apos;dog&apos; depending on some threshold. 
This threshold was chosen such that the F1 score was maximized for the testing data (please see AUC and/or h2o.performance() from R). 


The <strong><em>ClassifierBolt</em></strong> then looks like:

<code>  public static class ClassifierBolt extends BaseRichBolt {
    OutputCollector _collector;
    final double _thresh = 0.54;

    @Override
    public void prepare(Map conf, TopologyContext context, OutputCollector collector) {
      _collector = collector;
    }

    @Override
    public void execute(Tuple tuple) {
      String expected=tuple.getString(0);
      double dogProb = tuple.getDouble(1);
      String content = expected + "," + (dogProb &lt;= _thresh ? "dog" : "cat");
      try {
        File file = new File("/Users/ludirehak/other_h2o/h2o-world-2015-training/tutorials/streaming/storm/web/out");
        if (!file.exists())  file.createNewFile();
        FileWriter fw = new FileWriter(file.getAbsoluteFile());
        BufferedWriter bw = new BufferedWriter(fw);
        bw.write(content);
        bw.close();
      } catch (IOException e) {
        e.printStackTrace();
      }
      _collector.emit(tuple, new Values(expected, dogProb &lt;= _thresh ? "dog" : "cat"));
      _collector.ack(tuple);
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {
      declarer.declare(new Fields("expected_class", "class"));
    }
  }
</code>
<h2>7. 
 
Running a Storm topology with your model deployed</h2>
Finally, we can run the topology by right-clicking on H2OStormStarter and running. 
Here&apos;s a screen shot of what that looks like:

<img src="images/ij_11.png" alt="">

<a name="real_time"></a>

<h2>8. 
 
Watching predictions in real-time</h2>
<img src="images/cats_n_dogs.png" alt="">

To watch the predictions in real time, we start up an http-server on port 4040 and navigate to <a href="http://localhost:4040" target="_blank">http://localhost:4040</a>.

In order to get http-server, install <em>npm</em> (you may need sudo):

<code>brew install npm</code><br><code>npm install http-server -g</code>  

Once these are installed, you may navigate to the <em>web</em> directory and start the server:

<code>cd web</code><br><code>http-server -p 4040 -c-1</code>  

Now open up your browser and navigate to <a href="http://localhost:4040" target="_blank">http://localhost:4040</a>. 
Requires a modern browser (depends on <a href="http://d3js.org/" target="_blank">D3</a> for animation). 
 

Here&apos;s a <a href="https://www.youtube.com/watch?v=NTUo5MyS6qE" target="_blank">short video</a> showing what it looks like all together.

Enjoy!

<h2>References</h2>
<a href="http://cran.r-project.org" target="_blank">CRAN</a>
GBM

  <a href="http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf" target="_blank">The Elements of Statistical Learning. 
Vol.1. 
N.p., page 339</a><br>  Hastie, Trevor, Robert Tibshirani, and J Jerome H Friedman.<br>  Springer New York, 2001.

  <a href="http://docs.0xdata.com/datascience/gbm.html" target="_blank">Data Science with H2O (GBM)</a>

  <a href="http://en.wikipedia.org/wiki/Gradient_boosting" target="_blank">Gradient Boosting (Wikipedia)</a>

<a href="http://h2o.ai" target="_blank">H2O</a>

<a href="http://h2o-release.s3.amazonaws.com/h2o/rel-markov/1/index.html" target="_blank">H2O Markov stable release</a>
<a href="http://en.wikipedia.org/wiki/Plain_Old_Java_Object" target="_blank">Java POJO</a>
<a href="http://www.r-project.org" target="_blank">R</a>
<a href="https://storm.apache.org" target="_blank">Storm</a>

<h2><span class="orange">H2OWorld - Building Machine Learning Applications with Sparkling Water</span></h2>
<h2>Requirements</h2>
Oracle Java 7+ (<a href="../..">USB</a>)
<a href="http://spark.apache.org/downloads.html" target="_blank">Spark 1.5.1</a> (<a href="../../Spark">USB</a>)
<a href="http://h2o-release.s3.amazonaws.com/sparkling-water/rel-1.5/6/index.html" target="_blank">Sparkling Water 1.5.6</a> (<a href="../../SparklingWater">USB</a>)
<a href="https://raw.githubusercontent.com/h2oai/sparkling-water/master/examples/smalldata/smsData.txt" target="_blank">SMS dataset</a> (<a href="../data/smsData.txt">USB</a>)

<h2>Provided on USB</h2>
<a href="../..">Binaries</a>
<a href="../data/smsData.txt">SMS dataset</a>
<a href="SparklingWater.pdf">Slides</a>
<a href="h2oworld.script.scala">Scala Script</a>

<h2>Machine Learning Workflow</h2>
<strong>Goal</strong>: For a given text message, identify if it is spam or not.

<ol>
Extract data
Transform & tokenize messages
Build Spark&apos;s <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">Tf-IDF model</a> and expand messages to feature vectors
Create and evaluate <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/dl/dl.md" target="_blank">H2O&apos;s Deep Learning model</a>
Use the models to detect spam messages
</ol>
<h3>Prepare environment</h3>
<ol>
Run Sparkling shell with an embedded Spark cluster:

<code class="lang-bash"><span class="hljs-built_in">cd</span> <span class="hljs-string">"path/to/sparkling/water"</span>
<span class="hljs-built_in">export</span> SPARK_HOME=<span class="hljs-string">"/path/to/spark/installation"</span>
<span class="hljs-built_in">export</span> MASTER=<span class="hljs-string">"local-cluster[3,2,4096]"</span>
bin/sparkling-shell --conf spark.executor.memory=<span class="hljs-number">2</span>G
</code>

<blockquote>
Note: To avoid flooding output with Spark INFO messages, I recommend editing your <code>$SPARK_HOME/conf/log4j.properties</code> and configuring the log level to <code>WARN</code>.

</blockquote>

Open Spark UI: Go to <a href="http://localhost:4040/" target="_blank">http://localhost:4040/</a> to see the Spark status.

Prepare the environment:

<code class="lang-scala"><span class="hljs-comment">// Input data</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">DATAFILE=</span>"..<span class="hljs-title">/data/smsData</span>.<span class="hljs-title">txt</span>"
</span><span class="hljs-comment">// Common imports from H2O and Sparks</span>
<span class="hljs-keyword">import</span> _root_.hex.deeplearning.{<span class="hljs-type">DeepLearningModel</span>, <span class="hljs-type">DeepLearning</span>}
<span class="hljs-keyword">import</span> _root_.hex.deeplearning.<span class="hljs-type">DeepLearningParameters</span>
<span class="hljs-keyword">import</span> org.apache.spark.examples.h2o.<span class="hljs-type">DemoUtils</span>._
<span class="hljs-keyword">import</span> org.apache.spark.h2o._
<span class="hljs-keyword">import</span> org.apache.spark.mllib
<span class="hljs-keyword">import</span> org.apache.spark.mllib.feature.{<span class="hljs-type">IDFModel</span>, <span class="hljs-type">IDF</span>, <span class="hljs-type">HashingTF</span>}
<span class="hljs-keyword">import</span> org.apache.spark.rdd.<span class="hljs-type">RDD</span>
<span class="hljs-keyword">import</span> water.<span class="hljs-type">Key</span>
</code>

Define the representation of the training message:

<code class="lang-scala"><span class="hljs-comment">// Representation of a training message</span>
<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SMS</span>(</span>target: <span class="hljs-type">String</span>, fv: mllib.linalg.<span class="hljs-type">Vector</span>)
</code>

Define the data loader and parser:

<code class="lang-scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load</span>(</span>dataFile: <span class="hljs-type">String</span>): <span class="hljs-type">RDD</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">String</span>]] = {
 <span class="hljs-comment">// Load file into memory, split on TABs and filter all empty lines</span>
 sc.textFile(dataFile).map(l =&gt; l.split(<span class="hljs-string">"\t"</span>)).filter(r =&gt; !r(<span class="hljs-number">0</span>).isEmpty)
}
</code>

Define the input messages tokenizer:

<code class="lang-scala">// Tokenizer
// For each sentence in input RDD it provides array of string representing individual interesting words in the sentence
def tokenize(dataRDD: RDD[String]): RDD[Seq[String]] = {
 // Ignore all useless words
 val ignoredWords = Seq("the", "a", ", "in", "on", "at", "as", "not", "for")
 // Ignore all useless characters
 val ignoredChars = Seq(&apos;,&apos;, &apos;:&apos;, &apos;;&apos;, &apos;/&apos;, &apos;&lt;&apos;, &apos;&gt;&apos;, &apos;"&apos;, &apos;.&apos;, &apos;(&apos;, &apos;)&apos;, &apos;?&apos;, &apos;-&apos;, &apos;\&apos;&apos;,&apos;!&apos;,&apos;0&apos;, &apos;1&apos;)

 // Invoke RDD API and transform input data
 val textsRDD = dataRDD.map( r =&gt; {
   // Get rid of all useless characters
   var smsText = r.toLowerCase
   for( c &lt;- ignoredChars) {
     smsText = smsText.replace(c, &apos; &apos;)
   }
   // Remove empty and uninteresting words
   val words = smsText.split(" ").filter(w =&gt; !ignoredWords.contains(w) & w.length&gt;2).distinct

   words.toSeq
 })
 textsRDD
}
</code>

Configure Spark&apos;s Tf-IDF model builder: 

<code class="lang-scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">buildIDFModel</span>(</span>tokensRDD: <span class="hljs-type">RDD</span>[<span class="hljs-type">Seq</span>[<span class="hljs-type">String</span>]],
                 minDocFreq:<span class="hljs-type">Int</span> = <span class="hljs-number">4</span>,
                 hashSpaceSize:<span class="hljs-type">Int</span> = <span class="hljs-number">1</span> &lt;&lt; <span class="hljs-number">10</span>): (<span class="hljs-type">HashingTF</span>, <span class="hljs-type">IDFModel</span>, <span class="hljs-type">RDD</span>[mllib.linalg.<span class="hljs-type">Vector</span>]) = {
 <span class="hljs-comment">// Hash strings into the given space</span>
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">hashingTF</span> =</span> <span class="hljs-keyword">new</span> <span class="hljs-type">HashingTF</span>(hashSpaceSize)
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">tf</span> =</span> hashingTF.transform(tokensRDD)

 <span class="hljs-comment">// Build term frequency-inverse document frequency model</span>
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">idfModel</span> =</span> <span class="hljs-keyword">new</span> <span class="hljs-type">IDF</span>(minDocFreq = minDocFreq).fit(tf)
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">expandedTextRDD</span> =</span> idfModel.transform(tf)
 (hashingTF, idfModel, expandedTextRDD)
}
</code>

<blockquote>
<strong>Wikipedia</strong> defines TF-IDF as: "tf&#x2013;idf, short for term frequency&#x2013;inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. 
It is often used as a weighting factor in information retrieval and text mining. 
The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general."

</blockquote>

Configure H2O&apos;s DeepLearning model builder:

<code class="lang-scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">buildDLModel</span>(</span>trainHF: <span class="hljs-type">Frame</span>, validHF: <span class="hljs-type">Frame</span>,
              epochs: <span class="hljs-type">Int</span> = <span class="hljs-number">10</span>, l1: <span class="hljs-type">Double</span> = <span class="hljs-number">0.001</span>, l2: <span class="hljs-type">Double</span> = <span class="hljs-number">0.0</span>,
              hidden: <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>] = <span class="hljs-type">Array</span>[<span class="hljs-type">Int</span>](<span class="hljs-number">200</span>, <span class="hljs-number">200</span>))
             (<span class="hljs-keyword">implicit</span> h2oContext: <span class="hljs-type">H2OContext</span>): <span class="hljs-type">DeepLearningModel</span> = {
 <span class="hljs-keyword">import</span> h2oContext._
 <span class="hljs-keyword">import</span> _root_.hex.deeplearning.<span class="hljs-type">DeepLearning</span>
 <span class="hljs-keyword">import</span> _root_.hex.deeplearning.<span class="hljs-type">DeepLearningParameters</span>
 <span class="hljs-comment">// Create algorithm parameres</span>
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">dlParams</span> =</span> <span class="hljs-keyword">new</span> <span class="hljs-type">DeepLearningParameters</span>()
 <span class="hljs-comment">// Name for target model</span>
 dlParams._model_id = <span class="hljs-type">Key</span>.make(<span class="hljs-string">"dlModel.hex"</span>)
 <span class="hljs-comment">// Training dataset</span>
 dlParams._train = trainHF
 <span class="hljs-comment">// Validation dataset</span>
 dlParams._valid = validHF
 <span class="hljs-comment">// Column used as target for training</span>
 dlParams._response_column = <span class="hljs-symbol">&apos;target</span>
 <span class="hljs-comment">// Number of passes over data</span>
 dlParams._epochs = epochs
 <span class="hljs-comment">// L1 penalty</span>
 dlParams._l1 = l1
 <span class="hljs-comment">// Number internal hidden layers</span>
 dlParams._hidden = hidden

 <span class="hljs-comment">// Create a DeepLearning job</span>
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">dl</span> =</span> <span class="hljs-keyword">new</span> <span class="hljs-type">DeepLearning</span>(dlParams)
 <span class="hljs-comment">// And launch it</span>
 <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">dlModel</span> =</span> dl.trainModel.get

 <span class="hljs-comment">// Force computation of model metrics on both datasets</span>
 dlModel.score(trainHF).delete()
 dlModel.score(validHF).delete()

 <span class="hljs-comment">// And return resulting model</span>
 dlModel
}
</code>

Initialize <code>H2OContext</code> and start H2O services on top of Spark:

<code class="lang-scala"><span class="hljs-comment">// Create SQL support</span>
<span class="hljs-keyword">import</span> org.apache.spark.sql._
<span class="hljs-keyword">implicit</span> <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">sqlContext</span> =</span> <span class="hljs-type">SQLContext</span>.getOrCreate(sc)
<span class="hljs-keyword">import</span> sqlContext.implicits._

<span class="hljs-comment">// Start H2O services</span>
<span class="hljs-keyword">import</span> org.apache.spark.h2o._
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">h2oContext</span> =</span> <span class="hljs-keyword">new</span> <span class="hljs-type">H2OContext</span>(sc).start()
</code>

Open H2O UI and verify that H2O is running: 

<code class="lang-scala">h2oContext.openFlow
</code>

<blockquote>
At this point, you can use the H2O UI and see the status of the H2O cloud by typing <code>getCloud</code>.

</blockquote>

Build the final workflow using all building pieces:

<code class="lang-scala"><span class="hljs-comment">// Data load</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">dataRDD</span> =</span> load(<span class="hljs-type">DATAFILE</span>)
<span class="hljs-comment">// Extract response column from dataset</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">hamSpamRDD</span> =</span> dataRDD.map( r =&gt; r(<span class="hljs-number">0</span>))
<span class="hljs-comment">// Extract message from dataset</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">messageRDD</span> =</span> dataRDD.map( r =&gt; r(<span class="hljs-number">1</span>))
<span class="hljs-comment">// Tokenize message content</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">tokensRDD</span> =</span> tokenize(messageRDD)

<span class="hljs-comment">// Build IDF model on tokenized messages</span>
<span class="hljs-comment">// It returns</span>
<span class="hljs-comment">//   - hashingTF: hashing function to hash a word to a vector space</span>
<span class="hljs-comment">//   - idfModel: a model to transform hashed sentence to a feature vector</span>
<span class="hljs-comment">//   - tfidf: transformed input messages</span>
<span class="hljs-keyword">var</span> (hashingTF, idfModel, tfidfRDD) = buildIDFModel(tokensRDD)

<span class="hljs-comment">// Merge response with extracted vectors</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">resultDF</span> =</span> hamSpamRDD.zip(tfidfRDD).map(v =&gt; <span class="hljs-type">SMS</span>(v._1, v._2)).toDF

<span class="hljs-comment">// Publish Spark DataFrame as H2OFrame  </span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">tableHF</span> =</span> h2oContext.asH2OFrame(resultDF, <span class="hljs-string">"messages_table"</span>)

<span class="hljs-comment">// Transform target column into categorical!</span>
tableHF.replace(tableHF.find(<span class="hljs-string">"target"</span>), tableHF.vec(<span class="hljs-string">"target"</span>).toCategoricalVec()).remove()
tableHF.update(<span class="hljs-literal">null</span>)

<span class="hljs-comment">// Split table into training and validation parts</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">keys</span> =</span> <span class="hljs-type">Array</span>[<span class="hljs-type">String</span>](<span class="hljs-string">"train.hex"</span>, <span class="hljs-string">"valid.hex"</span>)
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">ratios</span> =</span> <span class="hljs-type">Array</span>[<span class="hljs-type">Double</span>](<span class="hljs-number">0.8</span>)
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">frs</span> =</span> split(tableHF, keys, ratios)
<span class="hljs-function"><span class="hljs-keyword">val</span> (</span>trainHF, validHF) = (frs(<span class="hljs-number">0</span>), frs(<span class="hljs-number">1</span>))
tableHF.delete()

<span class="hljs-comment">// Build final DeepLearning model</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">dlModel</span> =</span> buildDLModel(trainHF, validHF)(h2oContext)
</code>

Evaluate the model&apos;s quality:

<code class="lang-scala"><span class="hljs-comment">// Collect model metrics and evaluate model quality</span>
<span class="hljs-keyword">import</span> water.app.<span class="hljs-type">ModelMetricsSupport</span>
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">trainMetrics</span> =</span> <span class="hljs-type">ModelMetricsSupport</span>.binomialMM(dlModel, trainHF)
<span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">validMetrics</span> =</span> <span class="hljs-type">ModelMetricsSupport</span>.binomialMM(dlModel, validHF)
println(trainMetrics.auc._auc)
println(validMetrics.auc._auc)
</code>

<blockquote>
You can also open the H2O UI and type <code>getPredictions</code> to visualize the model&apos;s performance or type <code>getModels</code> to see model output.

</blockquote>

Create a spam detector:

<code class="lang-scala"><span class="hljs-comment">// Spam detector</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">isSpam</span>(</span>msg: <span class="hljs-type">String</span>,
       dlModel: <span class="hljs-type">DeepLearningModel</span>,
       hashingTF: <span class="hljs-type">HashingTF</span>,
       idfModel: <span class="hljs-type">IDFModel</span>,
       h2oContext: <span class="hljs-type">H2OContext</span>,
       hamThreshold: <span class="hljs-type">Double</span> = <span class="hljs-number">0.5</span>):<span class="hljs-type">String</span> = {
  <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">msgRdd</span> =</span> sc.parallelize(<span class="hljs-type">Seq</span>(msg))
  <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">msgVector</span>:</span> <span class="hljs-type">DataFrame</span> = idfModel.transform(
      hashingTF.transform (
        tokenize (msgRdd))).map(v =&gt; <span class="hljs-type">SMS</span>(<span class="hljs-string">"?"</span>, v)).toDF
  <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">msgTable</span>:</span> <span class="hljs-type">H2OFrame</span> = h2oContext.asH2OFrame(msgVector)
  msgTable.remove(<span class="hljs-number">0</span>) <span class="hljs-comment">// remove first column</span>
  <span class="hljs-function"><span class="hljs-keyword">val</span> <span class="hljs-title">prediction</span> =</span> dlModel.score(msgTable)

  <span class="hljs-keyword">if</span> (prediction.vecs()(<span class="hljs-number">1</span>).at(<span class="hljs-number">0</span>) &lt; hamThreshold) <span class="hljs-string">"SPAM DETECTED!"</span> <span class="hljs-keyword">else</span> <span class="hljs-string">"HAM"</span>
}
</code>

Try to detect spam:

<code class="lang-scala">isSpam(<span class="hljs-string">"Michal, h2oworld party tonight in MV?"</span>, dlModel, hashingTF, idfModel, h2oContext)
<span class="hljs-comment">// </span>
isSpam(<span class="hljs-string">"We tried to contact you re your reply to our offer of a Video Handset? 750 anytime any networks mins? UNLIMITED TEXT?"</span>, dlModel, hashingTF, idfModel, h2oContext)
</code>

At this point, you have finished your 1st Sparkling Water Machine Learning application. 
Hack and enjoy! Thank you!   

</ol>

<h2>1. Define Spark Context</h2>
<code>sc</code>

<code>&lt;pyspark.context.SparkContext at 0x102cea1d0&gt;</code>

<h2>2. Start H2O Context</h2>
<code>
    from pysparkling import *
    sc
    hc= H2OContext(sc).start()

    Warning: Version mismatch. 
H2O is version 3.6.0.2, but the python package is version 3.7.0.99999.
</code>
<div style="overflow:auto"><table style="width:50%"><tr><td>H2O cluster uptime: </td><td>2 seconds 217 milliseconds </td></tr>
<tr><td>H2O cluster version: </td><td>3.6.0.2</td></tr>
<tr><td>H2O cluster name: </td><td>sparkling-water-nidhimehta</td></tr>
<tr><td>H2O cluster total nodes: </td><td>2</td></tr>
<tr><td>H2O cluster total memory: </td><td>3.83 GB</td></tr>
<tr><td>H2O cluster total cores: </td><td>16</td></tr>
<tr><td>H2O cluster allowed cores: </td><td>16</td></tr>
<tr><td>H2O cluster healthy: </td><td>True</td></tr>
<tr><td>H2O Connection ip: </td><td>172.16.2.98</td></tr>
<tr><td>H2O Connection port: </td><td>54329</td></tr></table></div>

<h2>3. Define H2O Context</h2>
<code>    hc

    H2OContext: ip=172.16.2.98, port=54329
</code>
<h2>4. Import H2O Python library</h2>
<code>import h2o
</code>
<h2>5. View all available H2O Python functions</h2>
<code>#dir(h2o)
</code>
<h2>6. Parse Chicago Crime dataset into H2O</h2>
<code>column_type = [&apos;Numeric&apos;,&apos;String&apos;,&apos;String&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Enum&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Enum&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Enum&apos;,&apos;Numeric&apos;,&apos;Numeric&apos;,&apos;Enum&apos;]
f_crimes = h2o.import_file(path ="../data/chicagoCrimes10k.csv",col_types =column_type)

print(f_crimes.shape)
f_crimes.summary()

Parse Progress: [##################################################] 100%
(9999, 22)
</code>
<table>
<tr><th>       </th><th>ID           </th><th>Case Number  </th><th>Date                  </th><th>Block               </th><th>IUCR  </th><th>Primary Type   </th><th>Description                 </th><th>Location Description  </th><th>Arrest        </th><th>Domestic      </th><th>Beat         </th><th>District     </th><th>Ward         </th><th>Community Area  </th><th>FBI Code  </th><th>X Coordinate  </th><th>Y Coordinate  </th><th>Year  </th><th>Updated On            </th><th>Latitude       </th><th>Longitude      </th><th>Location                     </th></tr>
<tr><td>type   </td><td>int          </td><td>string       </td><td>string                </td><td>enum                </td><td>enum  </td><td>enum           </td><td>enum</td><td>enum                  </td><td>enum          </td><td>enum          </td><td>int          </td><td>int          </td><td>int          </td><td>int             </td><td>enum      </td><td>int           </td><td>int           </td><td>int   </td><td>enum                  </td><td>real           </td><td>real           </td><td>enum </td></tr>
<tr><td>mins   </td><td>21735.0      </td><td>NaN          </td><td>NaN                   </td><td>0.0                 </td><td>0.0   </td><td>0.0            </td><td>0.0 </td><td>0.0                   </td><td>0.0           </td><td>0.0           </td><td>111.0        </td><td>1.0          </td><td>1.0          </td><td>1.0             </td><td>0.0       </td><td>1100317.0     </td><td>1814255.0     </td><td>2015.0</td><td>0.0                   </td><td>41.64507243    </td><td>-87.906463888  </td><td>0.0  </td></tr>
<tr><td>mean   </td><td>9931318.73737</td><td>NaN          </td><td>NaN                   </td><td>NaN                 </td><td>NaN   </td><td>NaN            </td><td>NaN </td><td>NaN                   </td><td>0.292829282928</td><td>0.152315231523</td><td>1159.61806181</td><td>11.3489885128</td><td>22.9540954095</td><td>37.4476447645   </td><td>NaN       </td><td>1163880.59815 </td><td>1885916.14984 </td><td>2015.0</td><td>NaN                   </td><td>41.8425652247  </td><td>-87.6741405221 </td><td>NaN  </td></tr>
<tr><td>maxs   </td><td>9962898.0    </td><td>NaN          </td><td>NaN                   </td><td>6517.0              </td><td>212.0 </td><td>26.0           </td><td>198.0                       </td><td>90.0                  </td><td>1.0           </td><td>1.0           </td><td>2535.0       </td><td>25.0         </td><td>50.0         </td><td>77.0            </td><td>24.0      </td><td>1205069.0     </td><td>1951533.0     </td><td>2015.0</td><td>32.0                  </td><td>42.022646183   </td><td>-87.524773286  </td><td>8603.0                       </td></tr>
<tr><td>sigma  </td><td>396787.564221</td><td>NaN          </td><td>NaN                   </td><td>NaN                 </td><td>NaN   </td><td>NaN            </td><td>NaN </td><td>NaN                   </td><td>0.455083515588</td><td>0.35934414686 </td><td>695.76029875 </td><td>6.94547493301</td><td>13.6495661144</td><td>21.2748762223   </td><td>NaN       </td><td>16496.4493681 </td><td>31274.0163199 </td><td>0.0   </td><td>NaN                   </td><td>0.0860186579358</td><td>0.0600357970653</td><td>NaN  </td></tr>
<tr><td>zeros  </td><td>0            </td><td>0            </td><td>0                     </td><td>3                   </td><td>16    </td><td>11             </td><td>933 </td><td>19                    </td><td>7071          </td><td>8476          </td><td>0            </td><td>0            </td><td>0            </td><td>0               </td><td>16        </td><td>0             </td><td>0             </td><td>0     </td><td>603                   </td><td>0              </td><td>0              </td><td>1    </td></tr>
<tr><td>missing</td><td>0            </td><td>0            </td><td>0                     </td><td>0                   </td><td>0     </td><td>0              </td><td>0   </td><td>6                     </td><td>0             </td><td>0             </td><td>0            </td><td>162          </td><td>0            </td><td>0               </td><td>0         </td><td>162           </td><td>162           </td><td>0     </td><td>0                     </td><td>162            </td><td>162            </td><td>162  </td></tr>
<tr><td>0      </td><td>9955810.0    </td><td>HY144797     </td><td>02/08/2015 11:43:40 PM</td><td>081XX S COLES AVE   </td><td>1811  </td><td>NARCOTICS      </td><td>POSS: CANNABIS 30GMS OR LESS</td><td>STREET                </td><td>true          </td><td>false         </td><td>422.0        </td><td>4.0          </td><td>7.0          </td><td>46.0            </td><td>18        </td><td>1198273.0     </td><td>1851626.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.747693646   </td><td>-87.549035389  </td><td>(41.747693646, -87.549035389)</td></tr>
<tr><td>1      </td><td>9955861.0    </td><td>HY144838     </td><td>02/08/2015 11:41:42 PM</td><td>118XX S STATE ST    </td><td>0486  </td><td>BATTERY        </td><td>DOMESTIC BATTERY SIMPLE     </td><td>APARTMENT             </td><td>true          </td><td>true          </td><td>522.0        </td><td>5.0          </td><td>34.0         </td><td>53.0            </td><td>08B       </td><td>1178335.0     </td><td>1826581.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.679442289   </td><td>-87.622850758  </td><td>(41.679442289, -87.622850758)</td></tr>
<tr><td>2      </td><td>9955801.0    </td><td>HY144779     </td><td>02/08/2015 11:30:22 PM</td><td>002XX S LARAMIE AVE </td><td>2026  </td><td>NARCOTICS      </td><td>POSS: PCP                   </td><td>SIDEWALK              </td><td>true          </td><td>false         </td><td>1522.0       </td><td>15.0         </td><td>29.0         </td><td>25.0            </td><td>18        </td><td>1141717.0     </td><td>1898581.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.87777333    </td><td>-87.755117993  </td><td>(41.87777333, -87.755117993) </td></tr>
<tr><td>3      </td><td>9956197.0    </td><td>HY144787     </td><td>02/08/2015 11:30:23 PM</td><td>006XX E 67TH ST     </td><td>1811  </td><td>NARCOTICS      </td><td>POSS: CANNABIS 30GMS OR LESS</td><td>STREET                </td><td>true          </td><td>false         </td><td>321.0        </td><td>nan          </td><td>6.0          </td><td>42.0            </td><td>18        </td><td>nan           </td><td>nan           </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>nan            </td><td>nan            </td><td>     </td></tr>
<tr><td>4      </td><td>9955846.0    </td><td>HY144829     </td><td>02/08/2015 11:30:58 PM</td><td>0000X S MAYFIELD AVE</td><td>0610  </td><td>BURGLARY       </td><td>FORCIBLE ENTRY              </td><td>APARTMENT             </td><td>false         </td><td>false         </td><td>1513.0       </td><td>15.0         </td><td>29.0         </td><td>25.0            </td><td>05        </td><td>1137239.0     </td><td>1899372.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.880025548   </td><td>-87.771541324  </td><td>(41.880025548, -87.771541324)</td></tr>
<tr><td>5      </td><td>9955835.0    </td><td>HY144778     </td><td>02/08/2015 11:30:21 PM</td><td>010XX W 48TH ST     </td><td>0486  </td><td>BATTERY        </td><td>DOMESTIC BATTERY SIMPLE     </td><td>APARTMENT             </td><td>false         </td><td>true          </td><td>933.0        </td><td>9.0          </td><td>3.0          </td><td>61.0            </td><td>08B       </td><td>1169986.0     </td><td>1873019.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.807059405   </td><td>-87.65206589   </td><td>(41.807059405, -87.65206589) </td></tr>
<tr><td>6      </td><td>9955872.0    </td><td>HY144822     </td><td>02/08/2015 11:27:24 PM</td><td>015XX W ARTHUR AVE  </td><td>1320  </td><td>CRIMINAL DAMAGE</td><td>TO VEHICLE                  </td><td>STREET                </td><td>false         </td><td>false         </td><td>2432.0       </td><td>24.0         </td><td>40.0         </td><td>1.0             </td><td>14        </td><td>1164732.0     </td><td>1943222.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.999814056   </td><td>-87.669342967  </td><td>(41.999814056, -87.669342967)</td></tr>
<tr><td>7      </td><td>21752.0      </td><td>HY144738     </td><td>02/08/2015 11:26:12 PM</td><td>060XX W GRAND AVE   </td><td>0110  </td><td>HOMICIDE       </td><td>FIRST DEGREE MURDER         </td><td>STREET                </td><td>true          </td><td>false         </td><td>2512.0       </td><td>25.0         </td><td>37.0         </td><td>19.0            </td><td>01A       </td><td>1135910.0     </td><td>1914206.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.920755683   </td><td>-87.776067514  </td><td>(41.920755683, -87.776067514)</td></tr>
<tr><td>8      </td><td>9955808.0    </td><td>HY144775     </td><td>02/08/2015 11:20:33 PM</td><td>001XX W WACKER DR   </td><td>0460  </td><td>BATTERY        </td><td>SIMPLE                      </td><td>OTHER                 </td><td>false         </td><td>false         </td><td>122.0        </td><td>1.0          </td><td>42.0         </td><td>32.0            </td><td>08B       </td><td>1175384.0     </td><td>1902088.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.886707818   </td><td>-87.631396356  </td><td>(41.886707818, -87.631396356)</td></tr>
<tr><td>9      </td><td>9958275.0    </td><td>HY146732     </td><td>02/08/2015 11:15:36 PM</td><td>001XX W WACKER DR   </td><td>0460  </td><td>BATTERY        </td><td>SIMPLE                      </td><td>HOTEL/MOTEL           </td><td>false         </td><td>false         </td><td>122.0        </td><td>1.0          </td><td>42.0         </td><td>32.0            </td><td>08B       </td><td>1175384.0     </td><td>1902088.0     </td><td>2015.0</td><td>02/15/2015 12:43:39 PM</td><td>41.886707818   </td><td>-87.631396356  </td><td>(41.886707818, -87.631396356)</td></tr>
</table>

<h2>7. Look at the distribution of the IUCR column</h2>
<code>f_crimes["IUCR"].table()
</code>
<table>
<tr><th>IUCR  </th><th style="text-align: right;">  Count</th></tr>
<tr><td>0110  </td><td style="text-align: right;">     16</td></tr>
<tr><td>0261  </td><td style="text-align: right;">      2</td></tr>
<tr><td>0263  </td><td style="text-align: right;">      2</td></tr>
<tr><td>0265  </td><td style="text-align: right;">      5</td></tr>
<tr><td>0266  </td><td style="text-align: right;">      2</td></tr>
<tr><td>0281  </td><td style="text-align: right;">     41</td></tr>
<tr><td>0291  </td><td style="text-align: right;">      3</td></tr>
<tr><td>0312  </td><td style="text-align: right;">     18</td></tr>
<tr><td>0313  </td><td style="text-align: right;">     20</td></tr>
<tr><td>031A  </td><td style="text-align: right;">    136</td></tr>
</table>

<h2>8. Look at the distribution of the Arrest column</h2>
<code>f_crimes["Arrest"].table()
</code>
<table>
<tr><th>Arrest  </th><th style="text-align: right;">  Count</th></tr>
<tr><td>false   </td><td style="text-align: right;">   7071</td></tr>
<tr><td>true    </td><td style="text-align: right;">   2928</td></tr>
</table>

<h2>9. Modify column names to replace blank spaces with underscores</h2>
<code>col_names = map(lambda s: s.replace(&apos; &apos;, &apos;_&apos;), f_crimes.col_names)
f_crimes.set_names(col_names)
</code>
<table>
<tr><th style="text-align: right;">             ID</th><th>Case_Number  </th><th>Date                  </th><th>Block               </th><th style="text-align: right;">  IUCR</th><th>Primary_Type   </th><th>Description                 </th><th>Location_Description  </th><th>Arrest  </th><th>Domestic  </th><th style="text-align: right;">  Beat</th><th style="text-align: right;">  District</th><th style="text-align: right;">  Ward</th><th style="text-align: right;">  Community_Area</th><th>FBI_Code  </th><th style="text-align: right;">  X_Coordinate</th><th style="text-align: right;">  Y_Coordinate</th><th style="text-align: right;">  Year</th><th>Updated_On            </th><th style="text-align: right;">  Latitude</th><th style="text-align: right;">  Longitude</th><th>Location                     </th></tr>
<tr><td style="text-align: right;">    9.95581e+06</td><td>HY144797     </td><td>02/08/2015 11:43:40 PM</td><td>081XX S COLES AVE   </td><td style="text-align: right;">  1811</td><td>NARCOTICS      </td><td>POSS: CANNABIS 30GMS OR LESS</td><td>STREET                </td><td>true    </td><td>false     </td><td style="text-align: right;">   422</td><td style="text-align: right;">         4</td><td style="text-align: right;">     7</td><td style="text-align: right;">              46</td><td>18        </td><td style="text-align: right;">   1.19827e+06</td><td style="text-align: right;">   1.85163e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.7477</td><td style="text-align: right;">   -87.549 </td><td>(41.747693646, -87.549035389)</td></tr>
<tr><td style="text-align: right;">    9.95586e+06</td><td>HY144838     </td><td>02/08/2015 11:41:42 PM</td><td>118XX S STATE ST    </td><td style="text-align: right;">  0486</td><td>BATTERY        </td><td>DOMESTIC BATTERY SIMPLE     </td><td>APARTMENT             </td><td>true    </td><td>true      </td><td style="text-align: right;">   522</td><td style="text-align: right;">         5</td><td style="text-align: right;">    34</td><td style="text-align: right;">              53</td><td>08B       </td><td style="text-align: right;">   1.17834e+06</td><td style="text-align: right;">   1.82658e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.6794</td><td style="text-align: right;">   -87.6229</td><td>(41.679442289, -87.622850758)</td></tr>
<tr><td style="text-align: right;">    9.9558e+06 </td><td>HY144779     </td><td>02/08/2015 11:30:22 PM</td><td>002XX S LARAMIE AVE </td><td style="text-align: right;">  2026</td><td>NARCOTICS      </td><td>POSS: PCP                   </td><td>SIDEWALK              </td><td>true    </td><td>false     </td><td style="text-align: right;">  1522</td><td style="text-align: right;">        15</td><td style="text-align: right;">    29</td><td style="text-align: right;">              25</td><td>18        </td><td style="text-align: right;">   1.14172e+06</td><td style="text-align: right;">   1.89858e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.8778</td><td style="text-align: right;">   -87.7551</td><td>(41.87777333, -87.755117993) </td></tr>
<tr><td style="text-align: right;">    9.9562e+06 </td><td>HY144787     </td><td>02/08/2015 11:30:23 PM</td><td>006XX E 67TH ST     </td><td style="text-align: right;">  1811</td><td>NARCOTICS      </td><td>POSS: CANNABIS 30GMS OR LESS</td><td>STREET                </td><td>true    </td><td>false     </td><td style="text-align: right;">   321</td><td style="text-align: right;">       nan</td><td style="text-align: right;">     6</td><td style="text-align: right;">              42</td><td>18        </td><td style="text-align: right;"> nan          </td><td style="text-align: right;"> nan          </td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">  nan     </td><td style="text-align: right;">   nan     </td><td>     </td></tr>
<tr><td style="text-align: right;">    9.95585e+06</td><td>HY144829     </td><td>02/08/2015 11:30:58 PM</td><td>0000X S MAYFIELD AVE</td><td style="text-align: right;">  0610</td><td>BURGLARY       </td><td>FORCIBLE ENTRY              </td><td>APARTMENT             </td><td>false   </td><td>false     </td><td style="text-align: right;">  1513</td><td style="text-align: right;">        15</td><td style="text-align: right;">    29</td><td style="text-align: right;">              25</td><td>05        </td><td style="text-align: right;">   1.13724e+06</td><td style="text-align: right;">   1.89937e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.88  </td><td style="text-align: right;">   -87.7715</td><td>(41.880025548, -87.771541324)</td></tr>
<tr><td style="text-align: right;">    9.95584e+06</td><td>HY144778     </td><td>02/08/2015 11:30:21 PM</td><td>010XX W 48TH ST     </td><td style="text-align: right;">  0486</td><td>BATTERY        </td><td>DOMESTIC BATTERY SIMPLE     </td><td>APARTMENT             </td><td>false   </td><td>true      </td><td style="text-align: right;">   933</td><td style="text-align: right;">         9</td><td style="text-align: right;">     3</td><td style="text-align: right;">              61</td><td>08B       </td><td style="text-align: right;">   1.16999e+06</td><td style="text-align: right;">   1.87302e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.8071</td><td style="text-align: right;">   -87.6521</td><td>(41.807059405, -87.65206589) </td></tr>
<tr><td style="text-align: right;">    9.95587e+06</td><td>HY144822     </td><td>02/08/2015 11:27:24 PM</td><td>015XX W ARTHUR AVE  </td><td style="text-align: right;">  1320</td><td>CRIMINAL DAMAGE</td><td>TO VEHICLE                  </td><td>STREET                </td><td>false   </td><td>false     </td><td style="text-align: right;">  2432</td><td style="text-align: right;">        24</td><td style="text-align: right;">    40</td><td style="text-align: right;">               1</td><td>14        </td><td style="text-align: right;">   1.16473e+06</td><td style="text-align: right;">   1.94322e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.9998</td><td style="text-align: right;">   -87.6693</td><td>(41.999814056, -87.669342967)</td></tr>
<tr><td style="text-align: right;">21752          </td><td>HY144738     </td><td>02/08/2015 11:26:12 PM</td><td>060XX W GRAND AVE   </td><td style="text-align: right;">  0110</td><td>HOMICIDE       </td><td>FIRST DEGREE MURDER         </td><td>STREET                </td><td>true    </td><td>false     </td><td style="text-align: right;">  2512</td><td style="text-align: right;">        25</td><td style="text-align: right;">    37</td><td style="text-align: right;">              19</td><td>01A       </td><td style="text-align: right;">   1.13591e+06</td><td style="text-align: right;">   1.91421e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.9208</td><td style="text-align: right;">   -87.7761</td><td>(41.920755683, -87.776067514)</td></tr>
<tr><td style="text-align: right;">    9.95581e+06</td><td>HY144775     </td><td>02/08/2015 11:20:33 PM</td><td>001XX W WACKER DR   </td><td style="text-align: right;">  0460</td><td>BATTERY        </td><td>SIMPLE                      </td><td>OTHER                 </td><td>false   </td><td>false     </td><td style="text-align: right;">   122</td><td style="text-align: right;">         1</td><td style="text-align: right;">    42</td><td style="text-align: right;">              32</td><td>08B       </td><td style="text-align: right;">   1.17538e+06</td><td style="text-align: right;">   1.90209e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.8867</td><td style="text-align: right;">   -87.6314</td><td>(41.886707818, -87.631396356)</td></tr>
<tr><td style="text-align: right;">    9.95828e+06</td><td>HY146732     </td><td>02/08/2015 11:15:36 PM</td><td>001XX W WACKER DR   </td><td style="text-align: right;">  0460</td><td>BATTERY        </td><td>SIMPLE                      </td><td>HOTEL/MOTEL           </td><td>false   </td><td>false     </td><td style="text-align: right;">   122</td><td style="text-align: right;">         1</td><td style="text-align: right;">    42</td><td style="text-align: right;">              32</td><td>08B       </td><td style="text-align: right;">   1.17538e+06</td><td style="text-align: right;">   1.90209e+06</td><td style="text-align: right;">  2015</td><td>02/15/2015 12:43:39 PM</td><td style="text-align: right;">   41.8867</td><td style="text-align: right;">   -87.6314</td><td>(41.886707818, -87.631396356)</td></tr>
</table>

<h2>10. Set time zone to UTC for date manipulation</h2>
<code>h2o.set_timezone("Etc/UTC")
</code>
<h2>11. Refine the date column</h2>
<code>def refine_date_col(data, col, pattern):
    data[col]         = data[col].as_date(pattern)
    data["Day"]       = data[col].day()
    data["Month"]     = data[col].month()    # Since H2O indexes from 0
    data["Year"]      = data[col].year()
    data["WeekNum"]   = data[col].week()
    data["WeekDay"]   = data[col].dayOfWeek()
    data["HourOfDay"] = data[col].hour()

    # Create weekend and season cols
    data["Weekend"] = (data["WeekDay"] == "Sun" or data["WeekDay"] == "Sat").ifelse(1, 0)[0]
    data["Season"] = data["Month"].cut([0, 2, 5, 7, 10, 12], ["Winter", "Spring", "Summer", "Autumn", "Winter"])

refine_date_col(f_crimes, "Date", "%m/%d/%Y %I:%M:%S %p")
f_crimes = f_crimes.drop("Date")
</code>
<h2>12. Parse Census data into H2O</h2>
<code>f_census = h2o.import_file("../data/chicagoCensus.csv",header=1)

## Update column names in the table
col_names = map(lambda s: s.strip().replace(&apos; &apos;, &apos;_&apos;), f_census.col_names)
f_census.set_names(col_names)
f_census = f_census[1:78,:]
print(f_census.dim)
#f_census.summary()

Parse Progress: [##################################################] 100%
[77, 9]
</code>
<h2>13. Parse Weather data into H2O</h2>
<code>f_weather = h2o.import_file("../data/chicagoAllWeather.csv")
f_weather = f_weather[1:]
print(f_weather.dim)
#f_weather.summary()

Parse Progress: [##################################################] 100%
[5162, 6]
</code>
<h2>14. Look at all the null entires in the Weather table</h2>
<code>f_weather[f_weather["meanTemp"].isna()]
</code>
<table>
<tr><th style="text-align: right;">  month</th><th style="text-align: right;">  day</th><th style="text-align: right;">  year</th><th style="text-align: right;">  maxTemp</th><th style="text-align: right;">  meanTemp</th><th style="text-align: right;">  minTemp</th></tr>
<tr><td style="text-align: right;">      6</td><td style="text-align: right;">   19</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   23</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   24</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   25</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   26</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   27</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   28</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   29</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      9</td><td style="text-align: right;">   30</td><td style="text-align: right;">  2008</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
<tr><td style="text-align: right;">      3</td><td style="text-align: right;">    4</td><td style="text-align: right;">  2009</td><td style="text-align: right;">      nan</td><td style="text-align: right;">       nan</td><td style="text-align: right;">      nan</td></tr>
</table>

<h2>15. Look at the help on <code>as_h2o_frame</code></h2>
<code>hc.as_spark_frame?
f_weather

H2OContext: ip=172.16.2.98, port=54329
</code>
<table>
<tr><th style="text-align: right;">  month</th><th style="text-align: right;">  day</th><th style="text-align: right;">  year</th><th style="text-align: right;">  maxTemp</th><th style="text-align: right;">  meanTemp</th><th style="text-align: right;">  minTemp</th></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    1</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       23</td><td style="text-align: right;">        14</td><td style="text-align: right;">        6</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    2</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       18</td><td style="text-align: right;">        12</td><td style="text-align: right;">        6</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    3</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       28</td><td style="text-align: right;">        18</td><td style="text-align: right;">        8</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    4</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       30</td><td style="text-align: right;">        24</td><td style="text-align: right;">       19</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    5</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       36</td><td style="text-align: right;">        30</td><td style="text-align: right;">       21</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    6</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       33</td><td style="text-align: right;">        26</td><td style="text-align: right;">       19</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    7</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       34</td><td style="text-align: right;">        28</td><td style="text-align: right;">       21</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    8</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       26</td><td style="text-align: right;">        20</td><td style="text-align: right;">       14</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">    9</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       23</td><td style="text-align: right;">        16</td><td style="text-align: right;">       10</td></tr>
<tr><td style="text-align: right;">      1</td><td style="text-align: right;">   10</td><td style="text-align: right;">  2001</td><td style="text-align: right;">       34</td><td style="text-align: right;">        26</td><td style="text-align: right;">       19</td></tr>
</table>

<h2>16. Copy data frames to Spark from H2O</h2>
<code>df_weather = hc.as_spark_frame(f_weather,)
df_census = hc.as_spark_frame(f_census)
df_crimes = hc.as_spark_frame(f_crimes)
</code>
<h2>17. Look at the weather data as parsed in Spark</h2>
(only showing top 2 rows)

<code>df_weather.show(2)

+-----+---+----+-------+--------+-------+
|month|day|year|maxTemp|meanTemp|minTemp|
+-----+---+----+-------+--------+-------+
|    1|  1|2001|     23|      14|      6|
|    1|  2|2001|     18|      12|      6|
+-----+---+----+-------+--------+-------+
</code>
<h2>18. Join columns from Crime, Census and Weather DataFrames in Spark</h2>
<code>## Register DataFrames as tables in SQL context
sqlContext.registerDataFrameAsTable(df_weather, "chicagoWeather")
sqlContext.registerDataFrameAsTable(df_census, "chicagoCensus")
sqlContext.registerDataFrameAsTable(df_crimes, "chicagoCrime")

crimeWithWeather = sqlContext.sql("SELECT
a.Year, a.Month, a.Day, a.WeekNum, a.HourOfDay, a.Weekend, a.Season, a.WeekDay,
a.IUCR, a.Primary_Type, a.Location_Description, a.Community_Area, a.District,
a.Arrest, a.Domestic, a.Beat, a.Ward, a.FBI_Code,
b.minTemp, b.maxTemp, b.meanTemp,
c.PERCENT_AGED_UNDER_18_OR_OVER_64, c.PER_CAPITA_INCOME, c.HARDSHIP_INDEX,
c.PERCENT_OF_HOUSING_CROWDED, c.PERCENT_HOUSEHOLDS_BELOW_POVERTY,
c.PERCENT_AGED_16__UNEMPLOYED, c.PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA
FROM chicagoCrime a
JOIN chicagoWeather b
ON a.Year = b.year AND a.Month = b.month AND a.Day = b.day
JOIN chicagoCensus c
ON a.Community_Area = c.Community_Area_Number")
</code>
<h2>19. Print the <code>crimeWithWeather</code> data table from Spark</h2>
<code>crimeWithWeather.show(2)

+----+-----+---+-------+---------+-------+------+-------+----+-----------------+--------------------+--------------+--------+------+--------+----+----+--------+-------+-------+--------+--------------------------------+-----------------+--------------+--------------------------+--------------------------------+---------------------------+--------------------------------------------+
|Year|Month|Day|WeekNum|HourOfDay|Weekend|Season|WeekDay|IUCR|     Primary_Type|Location_Description|Community_Area|District|Arrest|Domestic|Beat|Ward|FBI_Code|minTemp|maxTemp|meanTemp|PERCENT_AGED_UNDER_18_OR_OVER_64|PER_CAPITA_INCOME|HARDSHIP_INDEX|PERCENT_OF_HOUSING_CROWDED|PERCENT_HOUSEHOLDS_BELOW_POVERTY|PERCENT_AGED_16__UNEMPLOYED|PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA|
+----+-----+---+-------+---------+-------+------+-------+----+-----------------+--------------------+--------------+--------+------+--------+----+----+--------+-------+-------+--------+--------------------------------+-----------------+--------------+--------------------------+--------------------------------+---------------------------+--------------------------------------------+
|2015|    1| 23|      4|       22|      0|Winter|    Fri|143A|WEAPONS VIOLATION|               ALLEY|            31|      12|  true|   false|1234|  25|      15|     29|     31|      30|    32.6|            16444|            76|         9.600000000000001|    25.8|                       15.8|                40.7|
|2015|    1| 23|      4|       19|      0|Winter|    Fri|4625|    OTHER OFFENSE|            SIDEWALK|            31|      10|  true|   false|1034|  25|      26|     29|     31|      30|    32.6|            16444|            76|         9.600000000000001|    25.8|                       15.8|                40.7|
+----+-----+---+-------+---------+-------+------+-------+----+-----------------+--------------------+--------------+--------+------+--------+----+----+--------+-------+-------+--------+--------------------------------+-----------------+--------------+--------------------------+--------------------------------+---------------------------+--------------------------------------------+
only showing top 2 rows
</code>
<h2>20. Copy table from Spark to H2O</h2>
<code>hc.as_h2o_frame?
crimeWithWeatherHF = hc.as_h2o_frame(crimeWithWeather,framename="crimeWithWeather")

H2OContext: ip=172.16.2.98, port=54329

crimeWithWeatherHF.summary()
</code>
<table>
<tr><th>       </th><th>Year  </th><th>Month         </th><th>Day          </th><th>WeekNum       </th><th>HourOfDay    </th><th>Weekend       </th><th>Season  </th><th>WeekDay  </th><th>IUCR  </th><th>Primary_Type              </th><th>Location_Description          </th><th>Community_Area  </th><th>District     </th><th>Arrest  </th><th>Domestic  </th><th>Beat         </th><th>Ward         </th><th>FBI_Code  </th><th>minTemp      </th><th>maxTemp      </th><th>meanTemp     </th><th>PERCENT_AGED_UNDER_18_OR_OVER_64  </th><th>PER_CAPITA_INCOME  </th><th>HARDSHIP_INDEX  </th><th>PERCENT_OF_HOUSING_CROWDED  </th><th>PERCENT_HOUSEHOLDS_BELOW_POVERTY  </th><th>PERCENT_AGED_16__UNEMPLOYED  </th><th>PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA  </th></tr>
<tr><td>type   </td><td>int   </td><td>int           </td><td>int          </td><td>int           </td><td>int          </td><td>int           </td><td>string  </td><td>string   </td><td>string</td><td>string                    </td><td>string</td><td>int             </td><td>int          </td><td>string  </td><td>string    </td><td>int          </td><td>int          </td><td>string    </td><td>int          </td><td>int          </td><td>int          </td><td>real      </td><td>int                </td><td>int             </td><td>real</td><td>real      </td><td>real </td><td>real                  </td></tr>
<tr><td>mins   </td><td>2015.0</td><td>1.0           </td><td>1.0          </td><td>4.0           </td><td>0.0          </td><td>0.0           </td><td>NaN     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>1.0             </td><td>1.0          </td><td>NaN     </td><td>NaN       </td><td>111.0        </td><td>1.0          </td><td>NaN       </td><td>-2.0         </td><td>15.0         </td><td>7.0          </td><td>13.5      </td><td>8201.0             </td><td>1.0             </td><td>0.3 </td><td>3.3       </td><td>4.7  </td><td>2.5                   </td></tr>
<tr><td>mean   </td><td>2015.0</td><td>1.41944194419 </td><td>17.6839683968</td><td>5.18081808181 </td><td>13.6319631963</td><td>0.159115911591</td><td>NaN     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>37.4476447645   </td><td>11.3489885128</td><td>NaN     </td><td>NaN       </td><td>1159.61806181</td><td>22.9540954095</td><td>NaN       </td><td>17.699669967 </td><td>31.7199719972</td><td>24.9408940894</td><td>35.0596759676                     </td><td>25221.3057306      </td><td>54.4786478648   </td><td>5.43707370737               </td><td>24.600750075                      </td><td>16.8288328833                </td><td>21.096639664          </td></tr>
<tr><td>maxs   </td><td>2015.0</td><td>2.0           </td><td>31.0         </td><td>6.0           </td><td>23.0         </td><td>1.0           </td><td>NaN     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>77.0            </td><td>25.0         </td><td>NaN     </td><td>NaN       </td><td>2535.0       </td><td>50.0         </td><td>NaN       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>51.5      </td><td>88669.0            </td><td>98.0            </td><td>15.8</td><td>56.5      </td><td>35.9 </td><td>54.8                  </td></tr>
<tr><td>sigma  </td><td>0.0   </td><td>0.493492406787</td><td>11.1801043358</td><td>0.738929830409</td><td>6.47321735807</td><td>0.365802434041</td><td>NaN     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>21.2748762223   </td><td>6.94547493301</td><td>NaN     </td><td>NaN       </td><td>695.76029875 </td><td>13.6495661144</td><td>NaN       </td><td>8.96118136438</td><td>6.93809913472</td><td>7.46302527062</td><td>7.95653388237                     </td><td>18010.0446225      </td><td>29.3247456472   </td><td>3.75289588494               </td><td>10.1450570661                     </td><td>7.58926327988                </td><td>11.3868817911         </td></tr>
<tr><td>zeros  </td><td>0     </td><td>0             </td><td>0            </td><td>0             </td><td>374          </td><td>8408          </td><td>0       </td><td>0        </td><td>0     </td><td>0 </td><td>0     </td><td>0               </td><td>0            </td><td>0       </td><td>0         </td><td>0            </td><td>0            </td><td>0         </td><td>0            </td><td>0            </td><td>0            </td><td>0         </td><td>0                  </td><td>0               </td><td>0   </td><td>0         </td><td>0    </td><td>0                     </td></tr>
<tr><td>missing</td><td>0     </td><td>0             </td><td>0            </td><td>0             </td><td>0            </td><td>0             </td><td>0       </td><td>0        </td><td>0     </td><td>0 </td><td>6     </td><td>0               </td><td>162          </td><td>0       </td><td>0         </td><td>0            </td><td>0            </td><td>0         </td><td>0            </td><td>0            </td><td>0            </td><td>0         </td><td>0                  </td><td>0               </td><td>0   </td><td>0         </td><td>0    </td><td>0                     </td></tr>
<tr><td>0      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>22.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>2820  </td><td>OTHER OFFENSE             </td><td>APARTMENT                     </td><td>31.0            </td><td>10.0         </td><td>false   </td><td>false     </td><td>1034.0       </td><td>25.0         </td><td>26        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>1      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>21.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1310  </td><td>CRIMINAL DAMAGE           </td><td>RESTAURANT                    </td><td>31.0            </td><td>12.0         </td><td>true    </td><td>false     </td><td>1233.0       </td><td>25.0         </td><td>14        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>2      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>18.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1750  </td><td>OFFENSE INVOLVING CHILDREN</td><td>RESIDENCE                     </td><td>31.0            </td><td>12.0         </td><td>false   </td><td>true      </td><td>1235.0       </td><td>25.0         </td><td>20        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>3      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>18.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0460  </td><td>BATTERY                   </td><td>OTHER </td><td>31.0            </td><td>10.0         </td><td>false   </td><td>false     </td><td>1023.0       </td><td>25.0         </td><td>08B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>4      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>13.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0890  </td><td>THEFT                     </td><td>CURRENCY EXCHANGE             </td><td>31.0            </td><td>10.0         </td><td>false   </td><td>false     </td><td>1023.0       </td><td>25.0         </td><td>06        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>5      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>9.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0560  </td><td>ASSAULT                   </td><td>OTHER </td><td>31.0            </td><td>12.0         </td><td>false   </td><td>false     </td><td>1234.0       </td><td>25.0         </td><td>08A       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>6      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>8.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0486  </td><td>BATTERY                   </td><td>RESIDENCE                     </td><td>31.0            </td><td>12.0         </td><td>true    </td><td>true      </td><td>1235.0       </td><td>25.0         </td><td>08B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>7      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>1.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0420  </td><td>BATTERY                   </td><td>SIDEWALK                      </td><td>31.0            </td><td>10.0         </td><td>false   </td><td>false     </td><td>1034.0       </td><td>25.0         </td><td>04B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>8      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>0.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1320  </td><td>CRIMINAL DAMAGE           </td><td>PARKING LOT/GARAGE(NON.RESID.)</td><td>31.0            </td><td>9.0          </td><td>false   </td><td>false     </td><td>912.0        </td><td>11.0         </td><td>14        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>9      </td><td>2015.0</td><td>1.0           </td><td>31.0         </td><td>5.0           </td><td>23.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0820  </td><td>THEFT                     </td><td>SIDEWALK                      </td><td>31.0            </td><td>12.0         </td><td>false   </td><td>false     </td><td>1234.0       </td><td>25.0         </td><td>06        </td><td>19.0         </td><td>36.0         </td><td>28.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
</table>

<h2>21. Assign column types to the <code>CrimeWeatherHF</code> data table in H2O</h2>
<code>crimeWithWeatherHF["Season"]= crimeWithWeatherHF["Season"].asfactor()
crimeWithWeatherHF["WeekDay"]= crimeWithWeatherHF["WeekDay"].asfactor()
crimeWithWeatherHF["IUCR"]= crimeWithWeatherHF["IUCR"].asfactor()
crimeWithWeatherHF["Primary_Type"]= crimeWithWeatherHF["Primary_Type"].asfactor()
crimeWithWeatherHF["Location_Description"]= crimeWithWeatherHF["Location_Description"].asfactor()
crimeWithWeatherHF["Arrest"]= crimeWithWeatherHF["Arrest"].asfactor()
crimeWithWeatherHF["Domestic"]= crimeWithWeatherHF["Domestic"].asfactor()
crimeWithWeatherHF["FBI_Code"]= crimeWithWeatherHF["FBI_Code"].asfactor()
crimeWithWeatherHF["Season"]= crimeWithWeatherHF["Season"].asfactor()

crimeWithWeatherHF.summary()
</code>
<table>
<tr><th>       </th><th>Year  </th><th>Month         </th><th>Day          </th><th>WeekNum       </th><th>HourOfDay    </th><th>Weekend       </th><th>Season  </th><th>WeekDay  </th><th>IUCR  </th><th>Primary_Type              </th><th>Location_Description          </th><th>Community_Area  </th><th>District     </th><th>Arrest        </th><th>Domestic      </th><th>Beat         </th><th>Ward         </th><th>FBI_Code  </th><th>minTemp      </th><th>maxTemp      </th><th>meanTemp     </th><th>PERCENT_AGED_UNDER_18_OR_OVER_64  </th><th>PER_CAPITA_INCOME  </th><th>HARDSHIP_INDEX  </th><th>PERCENT_OF_HOUSING_CROWDED  </th><th>PERCENT_HOUSEHOLDS_BELOW_POVERTY  </th><th>PERCENT_AGED_16__UNEMPLOYED  </th><th>PERCENT_AGED_25__WITHOUT_HIGH_SCHOOL_DIPLOMA  </th></tr>
<tr><td>type   </td><td>int   </td><td>int           </td><td>int          </td><td>int           </td><td>int          </td><td>int           </td><td>enum    </td><td>enum     </td><td>enum  </td><td>enum                      </td><td>enum  </td><td>int             </td><td>int          </td><td>enum          </td><td>enum          </td><td>int          </td><td>int          </td><td>enum      </td><td>int          </td><td>int          </td><td>int          </td><td>real      </td><td>int                </td><td>int             </td><td>real</td><td>real      </td><td>real </td><td>real                  </td></tr>
<tr><td>mins   </td><td>2015.0</td><td>1.0           </td><td>1.0          </td><td>4.0           </td><td>0.0          </td><td>0.0           </td><td>0.0     </td><td>0.0      </td><td>0.0   </td><td>0.0                       </td><td>0.0   </td><td>1.0             </td><td>1.0          </td><td>0.0           </td><td>0.0           </td><td>111.0        </td><td>1.0          </td><td>0.0       </td><td>-2.0         </td><td>15.0         </td><td>7.0          </td><td>13.5      </td><td>8201.0             </td><td>1.0             </td><td>0.3 </td><td>3.3       </td><td>4.7  </td><td>2.5                   </td></tr>
<tr><td>mean   </td><td>2015.0</td><td>1.41944194419 </td><td>17.6839683968</td><td>5.18081808181 </td><td>13.6319631963</td><td>0.159115911591</td><td>0.0     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>37.4476447645   </td><td>11.3489885128</td><td>0.292829282928</td><td>0.152315231523</td><td>1159.61806181</td><td>22.9540954095</td><td>NaN       </td><td>17.699669967 </td><td>31.7199719972</td><td>24.9408940894</td><td>35.0596759676                     </td><td>25221.3057306      </td><td>54.4786478648   </td><td>5.43707370737               </td><td>24.600750075                      </td><td>16.8288328833                </td><td>21.096639664          </td></tr>
<tr><td>maxs   </td><td>2015.0</td><td>2.0           </td><td>31.0         </td><td>6.0           </td><td>23.0         </td><td>1.0           </td><td>0.0     </td><td>6.0      </td><td>212.0 </td><td>26.0                      </td><td>90.0  </td><td>77.0            </td><td>25.0         </td><td>1.0           </td><td>1.0           </td><td>2535.0       </td><td>50.0         </td><td>24.0      </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>51.5      </td><td>88669.0            </td><td>98.0            </td><td>15.8</td><td>56.5      </td><td>35.9 </td><td>54.8                  </td></tr>
<tr><td>sigma  </td><td>0.0   </td><td>0.493492406787</td><td>11.1801043358</td><td>0.738929830409</td><td>6.47321735807</td><td>0.365802434041</td><td>0.0     </td><td>NaN      </td><td>NaN   </td><td>NaN                       </td><td>NaN   </td><td>21.2748762223   </td><td>6.94547493301</td><td>0.455083515588</td><td>0.35934414686 </td><td>695.76029875 </td><td>13.6495661144</td><td>NaN       </td><td>8.96118136438</td><td>6.93809913472</td><td>7.46302527062</td><td>7.95653388237                     </td><td>18010.0446225      </td><td>29.3247456472   </td><td>3.75289588494               </td><td>10.1450570661                     </td><td>7.58926327988                </td><td>11.3868817911         </td></tr>
<tr><td>zeros  </td><td>0     </td><td>0             </td><td>0            </td><td>0             </td><td>374          </td><td>8408          </td><td>9999    </td><td>1942     </td><td>16    </td><td>11</td><td>19    </td><td>0               </td><td>0            </td><td>7071          </td><td>8476          </td><td>0            </td><td>0            </td><td>16        </td><td>0            </td><td>0            </td><td>0            </td><td>0         </td><td>0                  </td><td>0               </td><td>0   </td><td>0         </td><td>0    </td><td>0                     </td></tr>
<tr><td>missing</td><td>0     </td><td>0             </td><td>0            </td><td>0             </td><td>0            </td><td>0             </td><td>0       </td><td>0        </td><td>0     </td><td>0 </td><td>6     </td><td>0               </td><td>162          </td><td>0             </td><td>0             </td><td>0            </td><td>0            </td><td>0         </td><td>0            </td><td>0            </td><td>0            </td><td>0         </td><td>0                  </td><td>0               </td><td>0   </td><td>0         </td><td>0    </td><td>0                     </td></tr>
<tr><td>0      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>22.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>2820  </td><td>OTHER OFFENSE             </td><td>APARTMENT                     </td><td>31.0            </td><td>10.0         </td><td>false         </td><td>false         </td><td>1034.0       </td><td>25.0         </td><td>26        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>1      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>21.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1310  </td><td>CRIMINAL DAMAGE           </td><td>RESTAURANT                    </td><td>31.0            </td><td>12.0         </td><td>true          </td><td>false         </td><td>1233.0       </td><td>25.0         </td><td>14        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>2      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>18.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1750  </td><td>OFFENSE INVOLVING CHILDREN</td><td>RESIDENCE                     </td><td>31.0            </td><td>12.0         </td><td>false         </td><td>true          </td><td>1235.0       </td><td>25.0         </td><td>20        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>3      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>18.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0460  </td><td>BATTERY                   </td><td>OTHER </td><td>31.0            </td><td>10.0         </td><td>false         </td><td>false         </td><td>1023.0       </td><td>25.0         </td><td>08B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>4      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>13.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0890  </td><td>THEFT                     </td><td>CURRENCY EXCHANGE             </td><td>31.0            </td><td>10.0         </td><td>false         </td><td>false         </td><td>1023.0       </td><td>25.0         </td><td>06        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>5      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>9.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0560  </td><td>ASSAULT                   </td><td>OTHER </td><td>31.0            </td><td>12.0         </td><td>false         </td><td>false         </td><td>1234.0       </td><td>25.0         </td><td>08A       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>6      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>8.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0486  </td><td>BATTERY                   </td><td>RESIDENCE                     </td><td>31.0            </td><td>12.0         </td><td>true          </td><td>true          </td><td>1235.0       </td><td>25.0         </td><td>08B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>7      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>1.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0420  </td><td>BATTERY                   </td><td>SIDEWALK                      </td><td>31.0            </td><td>10.0         </td><td>false         </td><td>false         </td><td>1034.0       </td><td>25.0         </td><td>04B       </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>8      </td><td>2015.0</td><td>1.0           </td><td>24.0         </td><td>4.0           </td><td>0.0          </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>1320  </td><td>CRIMINAL DAMAGE           </td><td>PARKING LOT/GARAGE(NON.RESID.)</td><td>31.0            </td><td>9.0          </td><td>false         </td><td>false         </td><td>912.0        </td><td>11.0         </td><td>14        </td><td>29.0         </td><td>43.0         </td><td>36.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
<tr><td>9      </td><td>2015.0</td><td>1.0           </td><td>31.0         </td><td>5.0           </td><td>23.0         </td><td>0.0           </td><td>Winter  </td><td>Sat      </td><td>0820  </td><td>THEFT                     </td><td>SIDEWALK                      </td><td>31.0            </td><td>12.0         </td><td>false         </td><td>false         </td><td>1234.0       </td><td>25.0         </td><td>06        </td><td>19.0         </td><td>36.0         </td><td>28.0         </td><td>32.6      </td><td>16444.0            </td><td>76.0            </td><td>9.6 </td><td>25.8      </td><td>15.8 </td><td>40.7                  </td></tr>
</table>

<h2>22. Split final H2O data table into train test and validation sets</h2>
<code>ratios = [0.6,0.2]
frs = crimeWithWeatherHF.split_frame(ratios,seed=12345)
train = frs[0]
train.frame_id = "Train"
valid = frs[2]
valid.frame_id = "Validation"
test = frs[1]
test.frame_id = "Test"
</code>
<h2>23. Import Model Builders from H2O Python</h2>
<code>from h2o.estimators.gbm import H2OGradientBoostingEstimator
from h2o.estimators.deeplearning import H2ODeepLearningEstimator
</code>
<h2>24. Inspect the availble GBM parameters</h2>
<code>H2OGradientBoostingEstimator?
</code>
<h2>25. Define Predictors</h2>
<code>predictors = crimeWithWeatherHF.names[:]
response = "Arrest"
predictors.remove(response)
</code>
<h2>26. Create a Simple GBM model to Predict Arrests</h2>
<code>model_gbm = H2OGradientBoostingEstimator(ntrees         =50,
                max_depth      =6,
                learn_rate     =0.1, 
                #nfolds         =2,
                distribution   ="bernoulli")

model_gbm.train(x               =predictors,
               y               ="Arrest",
               training_frame  =train,
               validation_frame=valid
               )
</code>
<h2>27. Create a Simple Deep Learning model to Predict Arrests</h2>
<code>model_dl = H2ODeepLearningEstimator(variable_importances=True,
           loss                ="Automatic")

model_dl.train(x                =predictors,
              y                ="Arrest",
              training_frame  =train,
              validation_frame=valid)

gbm Model Build Progress: [##################################################] 100%

deeplearning Model Build Progress: [##################################################] 100%
</code>
<h2>28. Print confusion matrices for the training and validation datasets</h2>
<code>print(model_gbm.confusion_matrix(train = True))
print(model_gbm.confusion_matrix(valid = True))
</code>
<strong>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.335827722991:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>false</b></td><td><b>true</b></td><td><b>Error</b></td><td><b>Rate</b></td></tr>
<tr><td>false</td><td>4125.0</td><td>142.0</td><td>0.0333</td><td> (142.0/4267.0)</td></tr>
<tr><td>true</td><td>251.0</td><td>1504.0</td><td>0.143</td><td> (251.0/1755.0)</td></tr>
<tr><td>Total</td><td>4376.0</td><td>1646.0</td><td>0.0653</td><td> (393.0/6022.0)</td></tr></table></div>

<strong>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.432844055866:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>false</b></td><td><b>true</b></td><td><b>Error</b></td><td><b>Rate</b></td></tr>
<tr><td>false</td><td>1362.0</td><td>61.0</td><td>0.0429</td><td> (61.0/1423.0)</td></tr>
<tr><td>true</td><td>150.0</td><td>443.0</td><td>0.253</td><td> (150.0/593.0)</td></tr>
<tr><td>Total</td><td>1512.0</td><td>504.0</td><td>0.1047</td><td> (211.0/2016.0)</td></tr></table></div>

<code>print(model_gbm.auc(train=True))
print(model_gbm.auc(valid=True))
model_gbm.plot(metric="AUC")

0.974667176776
0.92596751276
</code>
<h2>29. Print variable importances</h2>
<code>model_gbm.varimp(True)
</code>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>relative_importance</th>
      <th>scaled_importance</th>
      <th>percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>      <th>0</th>
      <td>IUCR</td>
      <td>4280.939453</td>
      <td>1.000000e+00</td>
      <td>8.234218e-01</td>
    </tr>
    <tr>      <th>1</th>
      <td>Location_Description</td>
      <td>487.323059</td>
      <td>1.138355e-01</td>
      <td>9.373466e-02</td>
    </tr>
    <tr>      <th>2</th>
      <td>WeekDay</td>
      <td>55.790558</td>
      <td>1.303232e-02</td>
      <td>1.073109e-02</td>
    </tr>
    <tr>      <th>3</th>
      <td>HourOfDay</td>
      <td>55.419220</td>
      <td>1.294557e-02</td>
      <td>1.065967e-02</td>
    </tr>
    <tr>      <th>4</th>
      <td>PERCENT_AGED_16__UNEMPLOYED</td>
      <td>34.422894</td>
      <td>8.040967e-03</td>
      <td>6.621107e-03</td>
    </tr>
    <tr>      <th>5</th>
      <td>Beat</td>
      <td>31.468222</td>
      <td>7.350775e-03</td>
      <td>6.052788e-03</td>
    </tr>
    <tr>      <th>6</th>
      <td>PERCENT_HOUSEHOLDS_BELOW_POVERTY</td>
      <td>29.103352</td>
      <td>6.798356e-03</td>
      <td>5.597915e-03</td>
    </tr>
    <tr>      <th>7</th>
      <td>PER_CAPITA_INCOME</td>
      <td>26.233143</td>
      <td>6.127894e-03</td>
      <td>5.045841e-03</td>
    </tr>
    <tr>      <th>8</th>
      <td>PERCENT_AGED_UNDER_18_OR_OVER_64</td>
      <td>24.077402</td>
      <td>5.624327e-03</td>
      <td>4.631193e-03</td>
    </tr>
    <tr>      <th>9</th>
      <td>Day</td>
      <td>23.472567</td>
      <td>5.483041e-03</td>
      <td>4.514855e-03</td>
    </tr>
    <tr>      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>      <th>15</th>
      <td>maxTemp</td>
      <td>11.300793</td>
      <td>2.639793e-03</td>
      <td>2.173663e-03</td>
    </tr>
    <tr>      <th>16</th>
      <td>Community_Area</td>
      <td>10.252146</td>
      <td>2.394835e-03</td>
      <td>1.971960e-03</td>
    </tr>
    <tr>      <th>17</th>
      <td>HARDSHIP_INDEX</td>
      <td>10.116072</td>
      <td>2.363049e-03</td>
      <td>1.945786e-03</td>
    </tr>
    <tr>      <th>18</th>
      <td>Domestic</td>
      <td>9.294327</td>
      <td>2.171095e-03</td>
      <td>1.787727e-03</td>
    </tr>
    <tr>      <th>19</th>
      <td>District</td>
      <td>8.304654</td>
      <td>1.939914e-03</td>
      <td>1.597367e-03</td>
    </tr>
    <tr>      <th>20</th>
      <td>minTemp</td>
      <td>6.243027</td>
      <td>1.458331e-03</td>
      <td>1.200822e-03</td>
    </tr>
    <tr>      <th>21</th>
      <td>WeekNum</td>
      <td>4.230102</td>
      <td>9.881246e-04</td>
      <td>8.136433e-04</td>
    </tr>
    <tr>      <th>22</th>
      <td>FBI_Code</td>
      <td>2.363182</td>
      <td>5.520241e-04</td>
      <td>4.545486e-04</td>
    </tr>
    <tr>      <th>23</th>
      <td>Month</td>
      <td>0.000018</td>
      <td>4.187325e-09</td>
      <td>3.447935e-09</td>
    </tr>
    <tr>      <th>24</th>
      <td>Weekend</td>
      <td>0.000000</td>
      <td>0.000000e+00</td>
      <td>0.000000e+00</td>
    </tr>
  </tbody>
</table>
25 rows &#xD7; 4 columns

</div>

<h2>30. Inspect Deep Learning model output</h2>
<code>model_dl

Model Details
=============
H2ODeepLearningEstimator :  Deep Learning
Model Key:  DeepLearning_model_python_1446861372065_4

Status of Neuron Layers: predicting Arrest, 2-class classification, bernoulli distribution, CrossEntropy loss, 118,802 weights/biases, 1.4 MB, 72,478 training samples, mini-batch size 1
</code>
<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>layer</b></td><td><b>units</b></td><td><b>type</b></td><td><b>dropout</b></td><td><b>l1</b></td><td><b>l2</b></td><td><b>mean_rate</b></td><td><b>rate_RMS</b></td><td><b>momentum</b></td><td><b>mean_weight</b></td><td><b>weight_RMS</b></td><td><b>mean_bias</b></td><td><b>bias_RMS</b></td></tr>
<tr><td></td><td>1</td><td>390</td><td>Input</td><td>0.0</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td></td><td>2</td><td>200</td><td>Rectifier</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.1</td><td>0.3</td><td>0.0</td><td>-0.0</td><td>0.1</td><td>-0.0</td><td>0.1</td></tr>
<tr><td></td><td>3</td><td>200</td><td>Rectifier</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.1</td><td>0.2</td><td>0.0</td><td>-0.0</td><td>0.1</td><td>0.8</td><td>0.2</td></tr>
<tr><td></td><td>4</td><td>2</td><td>Softmax</td><td></td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.4</td><td>-0.0</td><td>0.0</td></tr></table></div>

<strong>ModelMetricsBinomial: deeplearning</strong>

  Reported on train data. 


<code>MSE: 0.0737426129728
R^2: 0.642891439669
LogLoss: 0.242051500943
AUC: 0.950131166302
Gini: 0.900262332604
</code>
<strong>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.343997370612:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>false</b></td><td><b>true</b></td><td><b>Error</b></td><td><b>Rate</b></td></tr>
<tr><td>false</td><td>4003.0</td><td>264.0</td><td>0.0619</td><td> (264.0/4267.0)</td></tr>
<tr><td>true</td><td>358.0</td><td>1397.0</td><td>0.204</td><td> (358.0/1755.0)</td></tr>
<tr><td>Total</td><td>4361.0</td><td>1661.0</td><td>0.1033</td><td> (622.0/6022.0)</td></tr></table></div>

<strong>Maximum Metrics: Maximum metrics at their respective thresholds</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b>metric</b></td><td><b>threshold</b></td><td><b>value</b></td><td><b>idx</b></td></tr>
<tr><td>max f1</td><td>0.3</td><td>0.8</td><td>195.0</td></tr>
<tr><td>max f2</td><td>0.2</td><td>0.9</td><td>278.0</td></tr>
<tr><td>max f0point5</td><td>0.7</td><td>0.9</td><td>86.0</td></tr>
<tr><td>max accuracy</td><td>0.5</td><td>0.9</td><td>149.0</td></tr>
<tr><td>max precision</td><td>1.0</td><td>1.0</td><td>0.0</td></tr>
<tr><td>max absolute_MCC</td><td>0.3</td><td>0.7</td><td>195.0</td></tr>
<tr><td>max min_per_class_accuracy</td><td>0.2</td><td>0.9</td><td>247.0</td></tr></table></div>

<code>ModelMetricsBinomial: deeplearning
** Reported on validation data. 
**

MSE: 0.0843305429737
R^2: 0.593831388139
LogLoss: 0.280203809486
AUC: 0.930515181213
Gini: 0.861030362427
</code>
<strong>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.493462351545:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>false</b></td><td><b>true</b></td><td><b>Error</b></td><td><b>Rate</b></td></tr>
<tr><td>false</td><td>1361.0</td><td>62.0</td><td>0.0436</td><td> (62.0/1423.0)</td></tr>
<tr><td>true</td><td>158.0</td><td>435.0</td><td>0.2664</td><td> (158.0/593.0)</td></tr>
<tr><td>Total</td><td>1519.0</td><td>497.0</td><td>0.1091</td><td> (220.0/2016.0)</td></tr></table></div>

<strong>Maximum Metrics: Maximum metrics at their respective thresholds</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b>metric</b></td><td><b>threshold</b></td><td><b>value</b></td><td><b>idx</b></td></tr>
<tr><td>max f1</td><td>0.5</td><td>0.8</td><td>137.0</td></tr>
<tr><td>max f2</td><td>0.1</td><td>0.8</td><td>303.0</td></tr>
<tr><td>max f0point5</td><td>0.7</td><td>0.9</td><td>82.0</td></tr>
<tr><td>max accuracy</td><td>0.7</td><td>0.9</td><td>91.0</td></tr>
<tr><td>max precision</td><td>1.0</td><td>1.0</td><td>0.0</td></tr>
<tr><td>max absolute_MCC</td><td>0.7</td><td>0.7</td><td>91.0</td></tr>
<tr><td>max min_per_class_accuracy</td><td>0.2</td><td>0.8</td><td>236.0</td></tr></table></div>

<strong>Scoring History</strong>:

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>timestamp</b></td><td><b>duration</b></td><td><b>training_speed</b></td><td><b>epochs</b></td><td><b>samples</b></td><td><b>training_MSE</b></td><td><b>training_r2</b></td><td><b>training_logloss</b></td><td><b>training_AUC</b></td><td><b>training_classification_error</b></td><td><b>validation_MSE</b></td><td><b>validation_r2</b></td><td><b>validation_logloss</b></td><td><b>validation_AUC</b></td><td><b>validation_classification_error</b></td></tr>
<tr><td></td><td>2015-11-06 17:57:05</td><td> 0.000 sec</td><td>None</td><td>0.0</td><td>0.0</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td><td>nan</td></tr>
<tr><td></td><td>2015-11-06 17:57:09</td><td> 2.899 sec</td><td>2594 rows/sec</td><td>1.0</td><td>6068.0</td><td>0.1</td><td>0.3</td><td>0.6</td><td>0.9</td><td>0.1</td><td>0.1</td><td>0.3</td><td>0.6</td><td>0.9</td><td>0.1</td></tr>
<tr><td></td><td>2015-11-06 17:57:15</td><td> 9.096 sec</td><td>5465 rows/sec</td><td>7.3</td><td>43742.0</td><td>0.1</td><td>0.6</td><td>0.3</td><td>0.9</td><td>0.1</td><td>0.1</td><td>0.6</td><td>0.3</td><td>0.9</td><td>0.1</td></tr>
<tr><td></td><td>2015-11-06 17:57:19</td><td>12.425 sec</td><td>6571 rows/sec</td><td>12.0</td><td>72478.0</td><td>0.1</td><td>0.6</td><td>0.2</td><td>1.0</td><td>0.1</td><td>0.1</td><td>0.6</td><td>0.3</td><td>0.9</td><td>0.1</td></tr></table></div>

<strong>Variable Importances:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b>variable</b></td><td><b>relative_importance</b></td><td><b>scaled_importance</b></td><td><b>percentage</b></td></tr>
<tr><td>Domestic.false</td><td>1.0</td><td>1.0</td><td>0.0</td></tr>
<tr><td>Primary_Type.NARCOTICS</td><td>0.9</td><td>0.9</td><td>0.0</td></tr>
<tr><td>IUCR.0860</td><td>0.8</td><td>0.8</td><td>0.0</td></tr>
<tr><td>FBI_Code.18</td><td>0.8</td><td>0.8</td><td>0.0</td></tr>
<tr><td>IUCR.4625</td><td>0.7</td><td>0.7</td><td>0.0</td></tr>
<tr><td>---</td><td>---</td><td>---</td><td>---</td></tr>
<tr><td>Location_Description.missing(NA)</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>
<tr><td>Primary_Type.missing(NA)</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>
<tr><td>FBI_Code.missing(NA)</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>
<tr><td>WeekDay.missing(NA)</td><td>0.0</td><td>0.0</td><td>0.0</td></tr>
<tr><td>Domestic.missing(NA)</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></table></div>

<h2>31. Predict on the test set using the GBM model</h2>
<code>predictions = model_gbm.predict(test)
predictions.show()
</code>
<table>
<tr><th>predict  </th><th style="text-align: right;">    false</th><th style="text-align: right;">      true</th></tr>
<tr><td>false    </td><td style="text-align: right;">0.946415 </td><td style="text-align: right;">0.0535847 </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.862165 </td><td style="text-align: right;">0.137835  </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.938661 </td><td style="text-align: right;">0.0613392 </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.870186 </td><td style="text-align: right;">0.129814  </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.980488 </td><td style="text-align: right;">0.0195118 </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.972006 </td><td style="text-align: right;">0.0279937 </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.990995 </td><td style="text-align: right;">0.00900489</td></tr>
<tr><td>true     </td><td style="text-align: right;">0.0210692</td><td style="text-align: right;">0.978931  </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.693061 </td><td style="text-align: right;">0.306939  </td></tr>
<tr><td>false    </td><td style="text-align: right;">0.992097 </td><td style="text-align: right;">0.00790253</td></tr>
</table>

<h2>32. Look at test set performance  (if it includes true labels)</h2>
<code>test_performance = model_gbm.model_performance(test)
test_performance

ModelMetricsBinomial: gbm
** Reported on test data. 
**

MSE: 0.0893676876445
R^2: 0.57094394422
LogLoss: 0.294019576922
AUC: 0.922152238508
Gini: 0.844304477016
</code>
<strong>Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.365461652105:</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b></b></td><td><b>false</b></td><td><b>true</b></td><td><b>Error</b></td><td><b>Rate</b></td></tr>
<tr><td>false</td><td>1297.0</td><td>84.0</td><td>0.0608</td><td> (84.0/1381.0)</td></tr>
<tr><td>true</td><td>153.0</td><td>427.0</td><td>0.2638</td><td> (153.0/580.0)</td></tr>
<tr><td>Total</td><td>1450.0</td><td>511.0</td><td>0.1209</td><td> (237.0/1961.0)</td></tr></table></div>

<strong>Maximum Metrics: Maximum metrics at their respective thresholds</strong>

<div style="overflow:auto"><table style="width:50%"><tr><td><b>metric</b></td><td><b>threshold</b></td><td><b>value</b></td><td><b>idx</b></td></tr>
<tr><td>max f1</td><td>0.4</td><td>0.8</td><td>158.0</td></tr>
<tr><td>max f2</td><td>0.1</td><td>0.8</td><td>295.0</td></tr>
<tr><td>max f0point5</td><td>0.7</td><td>0.9</td><td>97.0</td></tr>
<tr><td>max accuracy</td><td>0.6</td><td>0.9</td><td>112.0</td></tr>
<tr><td>max precision</td><td>1.0</td><td>1.0</td><td>0.0</td></tr>
<tr><td>max absolute_MCC</td><td>0.6</td><td>0.7</td><td>112.0</td></tr>
<tr><td>max min_per_class_accuracy</td><td>0.2</td><td>0.8</td><td>235.0</td></tr></table></div>

<h2>33. Create Plots of Crime type vs Arrest Rate and Proportion of reported Crime</h2>
<code># Create table to report Crimetype, Arrest count per crime, total reported count per Crime  
sqlContext.registerDataFrameAsTable(df_crimes, "df_crimes")
allCrimes = sqlContext.sql("SELECT Primary_Type, count(*) as all_count FROM df_crimes GROUP BY Primary_Type")
crimesWithArrest = sqlContext.sql("SELECT Primary_Type, count(*) as crime_count FROM chicagoCrime WHERE Arrest = &apos;true&apos; GROUP BY Primary_Type")

sqlContext.registerDataFrameAsTable(crimesWithArrest, "crimesWithArrest")
sqlContext.registerDataFrameAsTable(allCrimes, "allCrimes")

crime_type = sqlContext.sql("Select a.Primary_Type as Crime_Type, a.crime_count, b.all_count \
FROM crimesWithArrest a \
JOIN allCrimes b \
ON a.Primary_Type = b.Primary_Type ")

crime_type.show(12)

+--------------------+-----------+---------+
|          Crime_Type|crime_count|all_count|
+--------------------+-----------+---------+
|       OTHER OFFENSE|        183|      720|
|   WEAPONS VIOLATION|         96|      118|
|  DECEPTIVE PRACTICE|         25|      445|
|            BURGLARY|         14|      458|
|             BATTERY|        432|     1851|
|             ROBBERY|         17|      357|
| MOTOR VEHICLE THEFT|         17|      414|
|        PROSTITUTION|        106|      106|
|     CRIMINAL DAMAGE|         76|     1003|
|          KIDNAPPING|          1|        7|
|            GAMBLING|          3|        3|
|LIQUOR LAW VIOLATION|         12|       12|
+--------------------+-----------+---------+
only showing top 12 rows
</code>
<h2>34. Copy Crime_type table from Spark to H2O</h2>
<code>crime_typeHF = hc.as_h2o_frame(crime_type,framename="crime_type")
</code>
<h2>35. Create Additional columns Arrest_rate and Crime_propotion</h2>
<code>crime_typeHF["Arrest_rate"] = crime_typeHF["crime_count"]/crime_typeHF["all_count"]
crime_typeHF["Crime_proportion"] = crime_typeHF["all_count"]/crime_typeHF["all_count"].sum()
crime_typeHF["Crime_Type"] = crime_typeHF["Crime_Type"].asfactor()
# h2o.assign(crime_typeHF,crime_type)
crime_typeHF.frame_id = "Crime_type"

crime_typeHF
</code>
<table>
<tr><th>Crime_Type         </th><th style="text-align: right;">  crime_count</th><th style="text-align: right;">  all_count</th><th style="text-align: right;">  Arrest_rate</th><th style="text-align: right;">  Crime_proportion</th></tr>
<tr><td>OTHER OFFENSE      </td><td style="text-align: right;">          183</td><td style="text-align: right;">        720</td><td style="text-align: right;">    0.254167 </td><td style="text-align: right;">       0.0721226  </td></tr>
<tr><td>WEAPONS VIOLATION  </td><td style="text-align: right;">           96</td><td style="text-align: right;">        118</td><td style="text-align: right;">    0.813559 </td><td style="text-align: right;">       0.0118201  </td></tr>
<tr><td>DECEPTIVE PRACTICE </td><td style="text-align: right;">           25</td><td style="text-align: right;">        445</td><td style="text-align: right;">    0.0561798</td><td style="text-align: right;">       0.0445758  </td></tr>
<tr><td>BURGLARY           </td><td style="text-align: right;">           14</td><td style="text-align: right;">        458</td><td style="text-align: right;">    0.0305677</td><td style="text-align: right;">       0.045878   </td></tr>
<tr><td>BATTERY            </td><td style="text-align: right;">          432</td><td style="text-align: right;">       1851</td><td style="text-align: right;">    0.233387 </td><td style="text-align: right;">       0.185415   </td></tr>
<tr><td>ROBBERY            </td><td style="text-align: right;">           17</td><td style="text-align: right;">        357</td><td style="text-align: right;">    0.047619 </td><td style="text-align: right;">       0.0357608  </td></tr>
<tr><td>MOTOR VEHICLE THEFT</td><td style="text-align: right;">           17</td><td style="text-align: right;">        414</td><td style="text-align: right;">    0.0410628</td><td style="text-align: right;">       0.0414705  </td></tr>
<tr><td>PROSTITUTION       </td><td style="text-align: right;">          106</td><td style="text-align: right;">        106</td><td style="text-align: right;">    1        </td><td style="text-align: right;">       0.0106181  </td></tr>
<tr><td>CRIMINAL DAMAGE    </td><td style="text-align: right;">           76</td><td style="text-align: right;">       1003</td><td style="text-align: right;">    0.0757727</td><td style="text-align: right;">       0.100471   </td></tr>
<tr><td>KIDNAPPING         </td><td style="text-align: right;">            1</td><td style="text-align: right;">          7</td><td style="text-align: right;">    0.142857 </td><td style="text-align: right;">       0.000701192</td></tr>
</table>

<code>hc

H2OContext: ip=172.16.2.98, port=54329
</code>
<h2>36. Plot in Flow</h2>
<code>plot (g) -&gt; g(
  g.rect(
    g.position "Crime_Type", "Arrest_rate"
    g.fillColor g.value &apos;blue&apos;
    g.fillOpacity g.value 0.75
  )
  g.rect(
    g.position "Crime_Type", "Crime_proportion"
    g.fillColor g.value &apos;red&apos;
    g.fillOpacity g.value 0.65
  )
  g.from inspect "data", getFrame "Crime_type"
)

#hc.stop()
</code>

<h2>Resources</h2>
More information about machine learning with H2O

<h2>H2O</h2>
<strong>Documentation for H2O and Sparkling Water</strong>: <a href="http://docs.h2o.ai/" target="_blank">http://docs.h2o.ai/</a>

<strong>Glossary of terms</strong>: <a href="https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/glossary.md" target="_blank">https://github.com/h2oai/h2o-3/blob/master/h2o-docs/src/product/tutorials/glossary.md</a>

<strong>Open forum for questions about H2O (Google account required)</strong>: <a href="https://groups.google.com/forum/#!forum/h2ostream" target="_blank">https://groups.google.com/forum/#!forum/h2ostream</a>

<strong>Track or file bug reports for H2O</strong>: <a href="https://jira.h2o.ai" target="_blank">https://jira.h2o.ai</a>

<strong>GitHub repository for H2O</strong>: <a href="https://github.com/h2oai" target="_blank">https://github.com/h2oai</a>

<h2>Python</h2>
<strong>About Python</strong>: <a href="https://www.python.org/" target="_blank">https://www.python.org/</a>

<strong>Latest Python H2O documentation</strong>: <a href="http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Pydoc.html" target="_blank">http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Pydoc.html</a>

<h2>R</h2>
<strong>About R</strong>: <a href="https://www.r-project.org/about.html" target="_blank">https://www.r-project.org/about.html</a>

<strong>Download R</strong>: <a href="https://cran.r-project.org/mirrors.html" target="_blank">https://cran.r-project.org/mirrors.html</a>

<strong>Latest R API H2O documentation</strong>: <a href="http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Rdoc.html" target="_blank">http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Rdoc.html</a> 

<h2>Sparkling Water</h2>
<strong>About Spark</strong>: <a href="http://spark.apache.org/" target="_blank">http://spark.apache.org/</a>

<strong>Download Spark</strong>: <a href="http://spark.apache.org/downloads.html" target="_blank">http://spark.apache.org/downloads.html</a>

<strong>Sparkling Water Developer documentation</strong>: <a href="https://github.com/h2oai/sparkling-water/blob/master/doc/devel/devel.rst" target="_blank">https://github.com/h2oai/sparkling-water/blob/master/doc/devel/devel.rst</a>
<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... 
more custom settings?
});
</script>
</pre></body></html>