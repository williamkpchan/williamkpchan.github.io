<base target="_blank"><html><head><title>Using Sparklyr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script>
  var showTopicNumber = true;
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
strong, h1, h2 {color: gold;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Using Sparklyr</h1>
https://spark.rstudio.com/guides/connections/
<br>
<div id="toc"></div></center>
<pre>
<br>
<br>
<h2>Configuring Spark Connections </h2>
<h2>Local mode</h2>
Local mode is an excellent way to learn and experiment with Spark. 
Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.

To work in local mode, you should <strong>first install a version of Spark for local use</strong>. 
You can do this using the <code>spark_install()</code> function, for example:

<h3>Recommended properties</h3>
The following are the recommended Spark properties to set when connecting via R:

<strong>sparklyr.cores.local</strong> - It defaults to using all of the available cores. 
Not a necessary property to set, unless there’s a reason to use less cores than available for a given Spark session.

<strong>sparklyr.shell.driver-memory</strong> - The limit is the amount of RAM available in the computer minus what would be needed for OS operations.

<strong>spark.memory.fraction</strong> - The default is set to 60% of the requested memory per executor. 
For more information, please see this <a href="https://spark.apache.org/docs/latest/tuning.html#memory-management-overview">Memory Management Overview</a> page in the official Spark website.

<h3>Connection example</h3><code> conf$`sparklyr.cores.local` &lt;- 4
conf$`sparklyr.shell.driver-memory` &lt;- &quot;16G&quot;
conf$spark.memory.fraction &lt;- 0.9

sc &lt;- spark_connect(master = &quot;local&quot;, 
      version = &quot;2.1.0&quot;,
      config = conf)</code>
<h4>Executors page</h4>
To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI available in <a href="http://localhost:4040/storage/" class="uri">http://localhost:4040/storage/</a>

<img src="/images/deployment/connections/local.png" />
<h2>Customizing connections</h2>
A connection to Spark can be customized by setting the values of certain Spark properties. 
In <code>sparklyr</code>, Spark properties can be set by using the <code>config</code> argument in the <code>spark_connect()</code> function.

By default, <code>spark_connect()</code> uses <code>spark_config()</code> as the default configuration. 
But that can be customized as shown in the example code below. 
Because of the unending number of possible combinations, <code>spark_config()</code> contains only a basic configuration, so it will be very likely that additional settings will be needed to properly connect to the cluster.
<code> conf &lt;- spark_config()   # Load variable with spark_config()

conf$spark.executor.memory &lt;- &quot;16G&quot; # Use `$` to add or set values

sc &lt;- spark_connect(master = &quot;yarn-client&quot;, 
      config = conf)  # Pass the conf variable </code>

<h3>Spark definitions</h3>
It may be useful to provide some simple definitions for the Spark nomenclature:

<strong>Node:</strong> A server

<strong>Worker Node:</strong> A server that is part of the cluster and are available to run Spark jobs

<strong>Master Node:</strong> The server that coordinates the Worker nodes.

<strong>Executor:</strong> A sort of virtual machine inside a node. 
<strong>One Node can have multiple Executors</strong>.

<strong>Driver Node:</strong> The Node that initiates the Spark session. 
Typically, this will be the server where <code>sparklyr</code> is located.

<strong>Driver (Executor):</strong> The Driver Node will also show up in the Executor list.

<h3>Useful concepts</h3>

<strong>Spark configuration properties passed by R are just requests</strong> - In most cases, the cluster has the final say regarding the resources apportioned to a given Spark session.

<strong>The cluster overrides ‘silently’</strong> - Many times, no errors are returned when more resources than allowed are requested, or if an attempt is made to change a setting fixed by the cluster.
<h2>YARN</h2>
<h3>Background</h3>
Using Spark and R inside a Hadoop based <em>Data Lake</em> is becoming a common practice at companies. 
Currently, <em>there is no good way to manage user connections to the Spark service centrally</em>. 
There are some caps and settings that can be applied, but in most cases there are configurations that the R user will need to customize.

The <a href="https://spark.apache.org/docs/latest/running-on-yarn.html">Running on YARN</a> page in Spark’s official website is the best place to start for configuration settings reference, please bookmark it. 
Cluster administrators and users can benefit from this document. 
If Spark is new to the company, the <a href="https://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/">YARN tunning</a> article, courtesy of Cloudera, does a great job at explaining how the Spark/YARN architecture works.

<h3>Recommended properties</h3>
The following are the recommended Spark properties to set when connecting via R:

<strong>spark.executor.memory</strong> - The maximum possible is managed by the YARN cluster. 
See the <a href="#yarn-executor">Executor Memory Error</a>

<strong>spark.executor.cores</strong> - Number of cores assigned per Executor.

<strong>spark.executor.instances</strong> - Number of executors to start. 
This property is acknowledged by the cluster if <em>spark.dynamicAllocation.enabled</em> is set to “false”.

<strong>spark.dynamicAllocation.enabled</strong> - Overrides the mechanism that Spark provides to dynamically adjust resources. 
Disabling it provides more control over the number of the Executors that can be started, which in turn impact the amount of storage available for the session. 
For more information, please see the <a href="https://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation">Dynamic Resource Allocation</a> page in the official Spark website.

<h3>Client mode</h3>
Using <code>yarn-client</code> as the value for the <code>master</code> argument in <code>spark_connect()</code> will make the server in which R is running to be the Spark’s session driver. 
Here is a sample connection:
<code> conf &lt;- spark_config()

conf$spark.executor.memory &lt;- &quot;300M&quot;
conf$spark.executor.cores &lt;- 2
conf$spark.executor.instances &lt;- 3
conf$spark.dynamicAllocation.enabled &lt;- &quot;false&quot;

sc &lt;- spark_connect(master = &quot;yarn-client&quot;, 
      spark_home = &quot;/usr/lib/spark/&quot;,
      version = &quot;1.6.0&quot;,
      config = conf)</code>
<h4>Executors page</h4>
To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI. 
Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040.

Notice that 155.3MB per executor are assigned instead of the 300MB requested. 
This is because the <em>spark.memory.fraction</em> has been fixed by the cluster, plus, there is fixed amount of memory designated for overhead.

<img src="/images/deployment/connections/yarnclient.png" />

<h3>Cluster mode</h3>
Running in cluster mode means that YARN will choose where the driver of the Spark session will run. 
This means that the server where R is running may not necessarily be the driver for that session. 
Here is a good write-up explaining how running Spark applications work: <a href="https://www.cloudera.com/documentation/enterprise/5-12-x/topics/cdh_ig_running_spark_on_yarn.html">Running Spark on YARN</a>

The server will need to have copies of at least two files: <code>yarn-site.xml</code> and <code>hive-site.xml</code>. 
There may be other files needed based on your cluster’s individual setup.

This is an example of connecting to a Cloudera cluster:
<code> library(sparklyr)

Sys.setenv(JAVA_HOME=&quot;/usr/lib/jvm/java-7-oracle-cloudera/&quot;)
Sys.setenv(SPARK_HOME = &#39;/opt/cloudera/parcels/CDH/lib/spark&#39;)
Sys.setenv(YARN_CONF_DIR = &#39;/opt/cloudera/parcels/CDH/lib/spark/conf/yarn-conf&#39;)

conf$spark.executor.memory &lt;- &quot;300M&quot;
conf$spark.executor.cores &lt;- 2
conf$spark.executor.instances &lt;- 3
conf$spark.dynamicAllocation.enabled &lt;- &quot;false&quot;
conf &lt;- spark_config()

sc &lt;- spark_connect(master = &quot;yarn-cluster&quot;, 
      config = conf)</code>

<h3>Executor memory error</h3>
Requesting more memory or CPUs for Executors than allowed will return an error. 
This is one of the exceptions to the cluster’s ‘silent’ overrides. 
It will return a message similar to this:
<code>     Failed during initialize_connection: java.lang.IllegalArgumentException: Required executor memory (16384+1638 MB) is above the max threshold (8192 MB) of this cluster! Please check the values of &#39;yarn.scheduler.maximum-allocation-mb&#39; and/or &#39;yarn.nodemanager.resource.memory-mb&#39;</code>

<strong>A cluster’s administrator</strong> is the only person who can make changes to the settings mentioned in the error. 
If the cluster is supported by a vendor, like Cloudera or Hortonworks, then the change can be made using the cluster’s web UI. 
Otherwise, changes to those settings are done directly in the <em>yarn-default.xml</em> file.

<h3>Kerberos</h3>
There are two options to access a “kerberized” data lake:

Use <em>kinit</em> to get and cache the ticket. 
After <em>kinit</em> is installed and configured. 
After <em>kinit</em> is setup, it can used in R via a <code>system()</code> call prior to connecting to the cluster:<code> system(&quot;echo &#39;&lt;password&gt;&#39; | kinit &lt;username&gt;&quot;)</code>

For more information visit this site: <a href="http://directory.apache.org/apacheds/kerberos-ug/4.1-authenticate-kinit.html">Apache - Authenticate with kinit</a>

A preferred option may be to use the out-of-the-box integration with Kerberos that the commercial version of <a href="https://www.rstudio.com/products/rstudio-server-pro/">RStudio Server</a> offers.
<h2>Standalone mode</h2>
<h3>Recommended properties</h3>
The following are the recommended Spark properties to set when connecting via R:

The default behavior in Standalone mode is to create one executor per worker. 
So in a 3 worker node cluster, there will be 3 executors setup. 
The basic properties that can be set are:

<strong>spark.executor.memory</strong> - The requested memory cannot exceed the actual RAM available.

<strong>spark.memory.fraction</strong> - The default is set to 60% of the requested memory per executor. 
For more information, please see this <a href="https://spark.apache.org/docs/latest/tuning.html#memory-management-overview">Memory Management Overview</a> page in the official Spark website.

<strong>spark.executor.cores</strong> - The requested cores cannot be higher than the cores available in each worker.
<h4>Dynamic Allocation</h4>
If dynamic allocation is disabled, then Spark will attempt to assign all of the available cores evenly across the cluster. 
The property used is <strong>spark.dynamicAllocation.enabled</strong>.

For example, the Standalone cluster used for this article has 3 worker nodes. 
Each node has 14.7GB in RAM and 4 cores. 
This means that there are a total of 12 cores (3 workers with 4 cores) and 44.1GB in RAM (3 workers with 14.7GB in RAM each).

If the <code>spark.executor.cores</code> property is set to 2, and dynamic allocation is disabled, then Spark will spawn 6 executors. 
The <code>spark.executor.memory</code> property should be set to a level that when the value is multiplied by 6 (number of executors) it will not be over total available RAM. 
In this case, the value can be safely set to 7GB so that the total memory requested will be 42GB, which is under the available 44.1GB.

<h3>Connection example</h3><code> conf &lt;- spark_config()
conf$spark.executor.memory &lt;- &quot;7GB&quot;
conf$spark.memory.fraction &lt;- 0.9
conf$spark.executor.cores &lt;- 2
conf$spark.dynamicAllocation.enabled &lt;- &quot;false&quot;

sc &lt;- spark_connect(master=&quot;spark://master-url:7077&quot;, 
version = &quot;2.1.0&quot;,
config = conf,
spark_home = &quot;/home/ubuntu/spark-2.1.0-bin-hadoop2.7/&quot;)</code>
<h4>Executors page</h4>
To see how the requested configuration affected the Spark connection, go to the <strong>Executors</strong> page in the Spark Web UI. 
Typically, the Spark Web UI can be found using the exact same URL used for RStudio but on port 4040:

<img src="/images/deployment/connections/standalone.png" />
<h2 id="troubleshooting">Troubleshooting</h2>
<h2 id="help-with-code-debugging">Help with code debugging</h2>
For general programming questions with <code>sparklyr</code>, please ask on <a href="http://stackoverflow.com">Stack Overflow</a>.
<h2 id="code-does-not-work-after-upgrading-to-the-latest-sparklyr-version">Code does not work after upgrading to the latest sparklyr version</h2>
Please refer to the <a href="/news">NEWS</a> section of the sparklyr package to find out if any of the updates listed may have changed the way your code needs to work.

If it seems that current version of the package has a bug, or the new functionality does not perform as stated, please refer to the sparklyr <a href="https://github.com/rstudio/sparklyr/issues">ISSUES</a> page. 
 If no existing issue matches to what your problem is, please open a new issue.
<h2 id="not-able-to-connect-or-the-jobs-take-a-long-time-when-working-with-a-data-lake">Not able to connect, or the jobs take a long time when working with a Data Lake</h2>
The <a href="/guides/connections">Configuration connections</a> contains an overview and recommendations for requesting resources form the cluster.

The articles in the <strong>Guides</strong> section provide best-practice information about specific operations that may match to the intent of your code.

To verify your infrastructure, please review the <strong>Deployment Examples</strong> section.
<h2 id="manipulating-data-with-dplyr">Manipulating Data with dplyr</h2>
<h2 id="overview">Overview</h2>
<a href="https://cran.r-project.org/web/packages/dplyr/index.html"><strong>dplyr</strong></a> is
an R package for working with structured data both in and outside of R.
dplyr makes data manipulation for R users easy, consistent, and
performant. 
With dplyr as an interface to manipulating Spark DataFrames,
you can:

Select, filter, and aggregate data
Use window functions (e.g. for sampling)
Perform joins on DataFrames
Collect data from Spark into R

Statements in dplyr can be chained together using pipes defined by the
<a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html">magrittr</a>
R package. 
dplyr also supports <a href="https://cran.r-project.org/web/packages/dplyr/vignettes/nse.html">non-standard
evalution</a>
of its arguments. 
For more information on dplyr, see the
<a href="https://cran.r-project.org/web/packages/dplyr/vignettes/introduction.html">introduction</a>,
a guide for connecting to
<a href="https://cran.r-project.org/web/packages/dplyr/vignettes/databases.html">databases</a>,
and a variety of
<a href="https://cran.r-project.org/web/packages/dplyr/index.html">vignettes</a>.
<h2 id="reading-data">Reading Data</h2>
You can read data into Spark DataFrames using the following
functions:

<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th></tr>
</thead>

<tbody>
<tr>
<td><a href="/reference/spark_read_csv/"><code>spark_read_csv</code></a></td>
<td>Reads a CSV file and provides a data source compatible with dplyr</td></tr>

<tr>
<td><a href="/reference/spark_read_json/"><code>spark_read_json</code></a></td>
<td>Reads a JSON file and provides a data source compatible with dplyr</td></tr>

<tr>
<td><a href="/reference/spark_read_parquet/"><code>spark_read_parquet</code></a></td>
<td>Reads a parquet file and provides a data source compatible with dplyr</td></tr>
</tbody>
</table>

Regardless of the format of your data, Spark supports reading data from
a variety of different data sources. 
These include data stored on HDFS
(<code>hdfs://</code> protocol), Amazon S3 (<code>s3n://</code> protocol), or local files
available to the Spark worker nodes (<code>file://</code> protocol)

Each of these functions returns a reference to a Spark DataFrame which
can be used as a dplyr table (<code>tbl</code>).

<h3 id="flights-data">Flights Data</h3>

This guide will demonstrate some of the basic data manipulation verbs of
dplyr by using data from the <code>nycflights13</code> R package. 
This package
contains data for all 336,776 flights departing New York City in 2013.
It also includes useful metadata on airlines, airports, weather, and
planes. 
The data comes from the US <a href="http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;Link=0">Bureau of Transportation
Statistics</a>,
and is documented in <code>?nycflights13</code>

Connect to the cluster and copy the flights data using the <code>copy_to</code>
function. 
Caveat: The flight data in <code>nycflights13</code> is convenient for
dplyr demonstrations because it is small, but in practice large data
should rarely be copied directly from R objects.

<code class="language-r">library(sparklyr)
library(dplyr)
library(nycflights13)
library(ggplot2)

sc &lt;- spark_connect(master=&quot;local&quot;)
flights &lt;- copy_to(sc, flights, &quot;flights&quot;)
airlines &lt;- copy_to(sc, airlines, &quot;airlines&quot;)
src_tbls(sc)
</code>
<code> ## [1] &quot;airlines&quot; &quot;flights&quot;
</code>
<h2 id="dplyr-verbs">dplyr Verbs</h2>
Verbs are dplyr commands for manipulating data. 
When connected to a
Spark DataFrame, dplyr translates the commands into Spark SQL
statements. 
Remote data sources use exactly the same five verbs as local
data sources. 
Here are the five verbs with their corresponding SQL
commands:
<code> select</code> ~ <code>SELECT</code><code> filter</code> ~ <code>WHERE</code><code> arrange</code> ~ <code>ORDER</code><code> summarise</code> ~ <code>aggregators: sum, min, sd, etc.</code><code> mutate</code> ~ <code>operators: +, *, log, etc.</code>

<!-- end list -->

<code class="language-r">select(flights, year:day, arr_delay, dep_delay)
</code>
<code> ## # Source: lazy query [?? x 5]
## # Database: spark_connection
##     year month   day arr_delay dep_delay
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;
##  1  2013     1     1     11.0       2.00
##  2  2013     1     1     20.0       4.00
##  3  2013     1     1     33.0       2.00
##  4  2013     1     1    -18.0      -1.00
##  5  2013     1     1    -25.0      -6.00
##  6  2013     1     1     12.0      -4.00
##  7  2013     1     1     19.0      -5.00
##  8  2013     1     1    -14.0      -3.00
##  9  2013     1     1    - 8.00     -3.00
## 10  2013     1     1      8.00     -2.00
## # ... 
with more rows
</code>

<code class="language-r">filter(flights, dep_delay &gt; 1000)
</code>
<code> ## # Source: lazy query [?? x 19]
## # Database: spark_connection
##    year month   day dep_t~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~
##   &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;
## 1  2013     1     9    641   900  1301  1242  1530  1272 HA       51 N384~
## 2  2013     1    10   1121  1635  1126  1239  1810  1109 MQ     3695 N517~
## 3  2013     6    15   1432  1935  1137  1607  2120  1127 MQ     3535 N504~
## 4  2013     7    22    845  1600  1005  1044  1815   989 MQ     3075 N665~
## 5  2013     9    20   1139  1845  1014  1457  2210  1007 AA      177 N338~
## # ... 
with 7 more variables: origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,
## #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dbl&gt;
</code>

<code class="language-r">arrange(flights, desc(dep_delay))
</code>
<code> ## # Source: table&lt;flights&gt; [?? x 19]
## # Database: spark_connection
## # Ordered by: desc(dep_delay)
##     year month   day dep_~ sche~ dep_~ arr_~ sche~ arr_~ carr~ flig~ tail~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt;
##  1  2013     1     9   641   900  1301  1242  1530  1272 HA       51 N384~
##  2  2013     6    15  1432  1935  1137  1607  2120  1127 MQ     3535 N504~
##  3  2013     1    10  1121  1635  1126  1239  1810  1109 MQ     3695 N517~
##  4  2013     9    20  1139  1845  1014  1457  2210  1007 AA      177 N338~
##  5  2013     7    22   845  1600  1005  1044  1815   989 MQ     3075 N665~
##  6  2013     4    10  1100  1900   960  1342  2211   931 DL     2391 N959~
##  7  2013     3    17  2321   810   911   135  1020   915 DL     2119 N927~
##  8  2013     6    27   959  1900   899  1236  2226   850 DL     2007 N376~
##  9  2013     7    22  2257   759   898   121  1026   895 DL     2047 N671~
## 10  2013    12     5   756  1700   896  1058  2020   878 AA      172 N5DM~
## # ... 
with more rows, and 7 more variables: origin &lt;chr&gt;, dest &lt;chr&gt;,
## #   air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour
## #   &lt;dbl&gt;
</code>

<code class="language-r">summarise(flights, mean_dep_delay = mean(dep_delay))
</code>
<code> ## Warning: Missing values are always removed in SQL.
## Use `AVG(x, na.rm = TRUE)` to silence this warning

## # Source: lazy query [?? x 1]
## # Database: spark_connection
##   mean_dep_delay
##            &lt;dbl&gt;
## 1           12.6
</code>

<code class="language-r">mutate(flights, speed = distance / air_time * 60)
</code>
<code> ## # Source: lazy query [?? x 20]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545
##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714
##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141
##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725
##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461
##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696
##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507
##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708
##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79
## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301
## # ... 
with more rows, and 9 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;, speed &lt;dbl&gt;
</code>
<h2 id="laziness">Laziness</h2>
When working with databases, dplyr tries to be as lazy as possible:

It never pulls data into R unless you explicitly ask for it.

It delays doing any work until the last possible moment: it collects
together everything you want to do and then sends it to the database
in one step.

For example, take the following
code:

<code class="language-r">c1 &lt;- filter(flights, day == 17, month == 5, carrier %in% c('UA', 'WN', 'AA', 'DL'))
c2 &lt;- select(c1, year, month, day, carrier, dep_delay, air_time, distance)
c3 &lt;- arrange(c2, year, month, day, carrier)
c4 &lt;- mutate(c3, air_time_hours = air_time / 60)
</code>

This sequence of operations never actually touches the database. 
It’s
not until you ask for the data (e.g. by printing <code>c4</code>) that dplyr
requests the results from the database.

<code class="language-r">c4
</code>
<code> ## # Source: lazy query [?? x 8]
## # Database: spark_connection
## # Ordered by: year, month, day, carrier
##     year month   day carrier dep_delay air_time distance air_time_hours
##    &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;
##  1  2013     5    17 AA          -2.00      294     2248           4.90
##  2  2013     5    17 AA          -1.00      146     1096           2.43
##  3  2013     5    17 AA          -2.00      185     1372           3.08
##  4  2013     5    17 AA          -9.00      186     1389           3.10
##  5  2013     5    17 AA           2.00      147     1096           2.45
##  6  2013     5    17 AA          -4.00      114      733           1.90
##  7  2013     5    17 AA          -7.00      117      733           1.95
##  8  2013     5    17 AA          -7.00      142     1089           2.37
##  9  2013     5    17 AA          -6.00      148     1089           2.47
## 10  2013     5    17 AA          -7.00      137      944           2.28
## # ... 
with more rows
</code>
<h2 id="piping">Piping</h2>
You can use
<a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html">magrittr</a>
pipes to write cleaner syntax. 
Using the same example from above, you
can write a much cleaner version like this:

<code class="language-r">c4 &lt;- flights %&gt;%
  filter(month == 5, day == 17, carrier %in% c('UA', 'WN', 'AA', 'DL')) %&gt;%
  select(carrier, dep_delay, air_time, distance) %&gt;%
  arrange(carrier) %&gt;%
  mutate(air_time_hours = air_time / 60)
</code>
<h2 id="grouping">Grouping</h2>
The <code>group_by</code> function corresponds to the <code>GROUP BY</code> statement in SQL.

<code class="language-r">c4 %&gt;%
  group_by(carrier) %&gt;%
  summarize(count = n(), mean_dep_delay = mean(dep_delay))
</code>
<code> ## Warning: Missing values are always removed in SQL.
## Use `AVG(x, na.rm = TRUE)` to silence this warning

## # Source: lazy query [?? x 3]
## # Database: spark_connection
##   carrier count mean_dep_delay
##   &lt;chr&gt;   &lt;dbl&gt;          &lt;dbl&gt;
## 1 AA       94.0           1.47
## 2 DL      136             6.24
## 3 UA      172             9.63
## 4 WN       34.0           7.97
</code>
<h2 id="collecting-to-r">Collecting to R</h2>
You can copy data from Spark into R’s memory by using <code>collect()</code>.

<code class="language-r">carrierhours &lt;- collect(c4)
</code>
<code> collect()</code> executes the Spark query and returns the results to R for
further analysis and visualization.

<code class="language-r"># Test the significance of pairwise differences and plot the results
with(carrierhours, pairwise.t.test(air_time, carrier))
</code>
<code> ## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  air_time and carrier 
## 
##    AA      DL      UA     
## DL 0.25057 -       -      
## UA 0.07957 0.00044 -      
## WN 0.07957 0.23488 0.00041
## 
## P value adjustment method: holm
</code>

<code class="language-r">ggplot(carrierhours, aes(carrier, air_time_hours)) + geom_boxplot()
</code>

<img src="guides-dplyr_files/figure-gfm/unnamed-chunk-12-1.png" alt="" /><!-- -->
<h2 id="sql-translation">SQL Translation</h2>
It’s relatively straightforward to translate R code to SQL (or indeed to
any programming language) when doing simple mathematical operations of
the form you normally use when filtering, mutating and summarizing.
dplyr knows how to convert the following R functions to Spark SQL:

<code class="language-r"># Basic math operators
+, -, *, /, %%, ^
  
# Math functions
abs, acos, asin, asinh, atan, atan2, ceiling, cos, cosh, exp, floor, log, log10, round, sign, sin, sinh, sqrt, tan, tanh

# Logical comparisons
&lt;, &lt;=, !=, &gt;=, &gt;, ==, %in%

# Boolean operations
&amp;, &amp;&amp;, |, ||, !

# Character functions
paste, tolower, toupper, nchar

# Casting
as.double, as.integer, as.logical, as.character, as.date

# Basic aggregations
mean, sum, min, max, sd, var, cor, cov, n
</code>
<h2 id="window-functions">Window Functions</h2>
dplyr supports Spark SQL window functions. 
Window functions are used in
conjunction with mutate and filter to solve a wide range of problems.
You can compare the dplyr syntax to the query it has generated by using<code> dbplyr::sql_render()</code>.

<code class="language-r"># Find the most and least delayed flight each day
bestworst &lt;- flights %&gt;%
  group_by(year, month, day) %&gt;%
  select(dep_delay) %&gt;% 
  filter(dep_delay == min(dep_delay) || dep_delay == max(dep_delay))
dbplyr::sql_render(bestworst)
## Warning: Missing values are always removed in SQL.
## Use `min(x, na.rm = TRUE)` to silence this warning
## Warning: Missing values are always removed in SQL.
## Use `max(x, na.rm = TRUE)` to silence this warning
## &lt;SQL&gt; SELECT `year`, `month`, `day`, `dep_delay`
## FROM (SELECT `year`, `month`, `day`, `dep_delay`, min(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz3`, max(`dep_delay`) OVER (PARTITION BY `year`, `month`, `day`) AS `zzz4`
## FROM (SELECT `year`, `month`, `day`, `dep_delay`
## FROM `flights`) `coaxmtqqbj`) `efznnpuovy`
## WHERE (`dep_delay` = `zzz3` OR `dep_delay` = `zzz4`)
bestworst
## Warning: Missing values are always removed in SQL.
## Use `min(x, na.rm = TRUE)` to silence this warning

## Warning: Missing values are always removed in SQL.
## Use `max(x, na.rm = TRUE)` to silence this warning
## # Source: lazy query [?? x 4]
## # Database: spark_connection
## # Groups: year, month, day
##     year month   day dep_delay
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;
##  1  2013     1     1     853  
##  2  2013     1     1   -  15.0
##  3  2013     1     1   -  15.0
##  4  2013     1     9    1301  
##  5  2013     1     9   -  17.0
##  6  2013     1    24   -  15.0
##  7  2013     1    24     329  
##  8  2013     1    29   -  27.0
##  9  2013     1    29     235  
## 10  2013     2     1   -  15.0
## # ... 
with more rows
</code>

<code class="language-r"># Rank each flight within a daily
ranked &lt;- flights %&gt;%
  group_by(year, month, day) %&gt;%
  select(dep_delay) %&gt;% 
  mutate(rank = rank(desc(dep_delay)))
dbplyr::sql_render(ranked)
</code>
<code> ## &lt;SQL&gt; SELECT `year`, `month`, `day`, `dep_delay`, rank() OVER (PARTITION BY `year`, `month`, `day` ORDER BY `dep_delay` DESC) AS `rank`
## FROM (SELECT `year`, `month`, `day`, `dep_delay`
## FROM `flights`) `mauqwkxuam`
</code>

<code class="language-r">ranked
</code>
<code> ## # Source: lazy query [?? x 5]
## # Database: spark_connection
## # Groups: year, month, day
##     year month   day dep_delay  rank
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;int&gt;
##  1  2013     1     1       853     1
##  2  2013     1     1       379     2
##  3  2013     1     1       290     3
##  4  2013     1     1       285     4
##  5  2013     1     1       260     5
##  6  2013     1     1       255     6
##  7  2013     1     1       216     7
##  8  2013     1     1       192     8
##  9  2013     1     1       157     9
## 10  2013     1     1       155    10
## # ... 
with more rows
</code>
<h2 id="peforming-joins">Peforming Joins</h2>
It’s rare that a data analysis involves only a single table of data. 
In
practice, you’ll normally have many tables that contribute to an
analysis, and you need flexible tools to combine them. 
In dplyr, there
are three families of verbs that work with two tables at a time:

Mutating joins, which add new variables to one table from matching
rows in another.

Filtering joins, which filter observations from one table based on
whether or not they match an observation in the other table.

Set operations, which combine the observations in the data sets as
if they were set elements.

All two-table verbs work similarly. 
The first two arguments are <code>x</code> and<code> y</code>, and provide the tables to combine. 
The output is always a new table
with the same type as <code>x</code>.

The following statements are equivalent:

<code class="language-r">flights %&gt;% left_join(airlines)
</code>
<code> ## Joining, by = &quot;carrier&quot;

## # Source: lazy query [?? x 20]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545
##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714
##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141
##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725
##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461
##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696
##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507
##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708
##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79
## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301
## # ... 
with more rows, and 9 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;, name &lt;chr&gt;
</code>

<code class="language-r">flights %&gt;% left_join(airlines, by = &quot;carrier&quot;)
</code>
<code> ## # Source: lazy query [?? x 20]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545
##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714
##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141
##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725
##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461
##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696
##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507
##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708
##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79
## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301
## # ... 
with more rows, and 9 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;, name &lt;chr&gt;
</code>

<code class="language-r">flights %&gt;% left_join(airlines, by = c(&quot;carrier&quot;, &quot;carrier&quot;))
</code>
<code> ## # Source: lazy query [?? x 20]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545
##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714
##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141
##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725
##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461
##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696
##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507
##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708
##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79
## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301
## # ... 
with more rows, and 9 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;, name &lt;chr&gt;
</code>
<h2 id="sampling">Sampling</h2>
You can use <code>sample_n()</code> and <code>sample_frac()</code> to take a random sample of
rows: use <code>sample_n()</code> for a fixed number and <code>sample_frac()</code> for a
fixed fraction.

<code class="language-r">sample_n(flights, 10)
</code>
<code> ## # Source: lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    517     515   2.00   830    819  11.0  UA     1545
##  2  2013     1     1    533     529   4.00   850    830  20.0  UA     1714
##  3  2013     1     1    542     540   2.00   923    850  33.0  AA     1141
##  4  2013     1     1    544     545  -1.00  1004   1022 -18.0  B6      725
##  5  2013     1     1    554     600  -6.00   812    837 -25.0  DL      461
##  6  2013     1     1    554     558  -4.00   740    728  12.0  UA     1696
##  7  2013     1     1    555     600  -5.00   913    854  19.0  B6      507
##  8  2013     1     1    557     600  -3.00   709    723 -14.0  EV     5708
##  9  2013     1     1    557     600  -3.00   838    846 - 8.00 B6       79
## 10  2013     1     1    558     600  -2.00   753    745   8.00 AA      301
## # ... 
with more rows, and 8 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;
</code>

<code class="language-r">sample_frac(flights, 0.01)
</code>
<code> ## # Source: lazy query [?? x 19]
## # Database: spark_connection
##     year month   day dep_t~ sched_~ dep_d~ arr_~ sched~ arr_d~ carr~ flig~
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;int&gt;
##  1  2013     1     1    655     655   0     1021   1030 - 9.00 DL     1415
##  2  2013     1     1    656     700 - 4.00   854    850   4.00 AA      305
##  3  2013     1     1   1044    1045 - 1.00  1231   1212  19.0  EV     4322
##  4  2013     1     1   1056    1059 - 3.00  1203   1209 - 6.00 EV     4479
##  5  2013     1     1   1317    1325 - 8.00  1454   1505 -11.0  MQ     4475
##  6  2013     1     1   1708    1700   8.00  2037   2005  32.0  WN     1066
##  7  2013     1     1   1825    1829 - 4.00  2056   2053   3.00 9E     3286
##  8  2013     1     1   1843    1845 - 2.00  1955   2024 -29.0  DL      904
##  9  2013     1     1   2108    2057  11.0     25     39 -14.0  UA     1517
## 10  2013     1     2    557     605 - 8.00   832    823   9.00 DL      544
## # ... 
with more rows, and 8 more variables: tailnum &lt;chr&gt;, origin &lt;chr&gt;,
## #   dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;,
## #   time_hour &lt;dbl&gt;
</code>
<h2 id="writing-data">Writing Data</h2>
It is often useful to save the results of your analysis or the tables
that you have generated on your Spark cluster into persistent storage.
The best option in many scenarios is to write the table out to a
<a href="https://parquet.apache.org/">Parquet</a> file using the
<a href="reference/sparklyr/spark_write_parquet.html">spark_write_parquet</a>
function. 
For example:

<code class="language-r">spark_write_parquet(tbl, &quot;hdfs://hdfs.company.org:9000/hdfs-path/data&quot;)
</code>

This will write the Spark DataFrame referenced by the tbl R variable to
the given HDFS path. 
You can use the
<a href="reference/sparklyr/spark_read_parquet.html">spark_read_parquet</a>
function to read the same table back into a subsequent Spark
session:

<code class="language-r">tbl &lt;- spark_read_parquet(sc, &quot;data&quot;, &quot;hdfs://hdfs.company.org:9000/hdfs-path/data&quot;)
</code>

You can also write data as CSV or JSON using the
<a href="reference/sparklyr/spark_write_csv.html">spark_write_csv</a> and
<a href="reference/sparklyr/spark_write_json.html">spark_write_json</a>
functions.
<h2 id="hive-functions">Hive Functions</h2>
Many of Hive’s built-in functions (UDF) and built-in aggregate functions
(UDAF) can be called inside dplyr’s mutate and summarize. 
The <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF">Languange
Reference
UDF</a>
page provides the list of available functions.

The following example uses the <strong>datediff</strong> and <strong>current_date</strong> Hive
UDFs to figure the difference between the flight_date and the current
system date:

<code class="language-r">flights %&gt;% 
  mutate(flight_date = paste(year,month,day,sep=&quot;-&quot;),
 days_since = datediff(current_date(), flight_date)) %&gt;%
  group_by(flight_date,days_since) %&gt;%
  tally() %&gt;%
  arrange(-days_since)
</code>
<code> ## # Source: lazy query [?? x 3]
## # Database: spark_connection
## # Groups: flight_date
## # Ordered by: -days_since
##    flight_date days_since     n
##    &lt;chr&gt;            &lt;int&gt; &lt;dbl&gt;
##  1 2013-1-1          1844   842
##  2 2013-1-2          1843   943
##  3 2013-1-3          1842   914
##  4 2013-1-4          1841   915
##  5 2013-1-5          1840   720
##  6 2013-1-6          1839   832
##  7 2013-1-7          1838   933
##  8 2013-1-8          1837   899
##  9 2013-1-9          1836   902
## 10 2013-1-10         1835   932
## # ... 
with more rows
</code>
<h2 id="spark-machine-learning-library-mllib">Spark Machine Learning Library (MLlib)</h2>
<h2 id="overview">Overview</h2>
<strong>sparklyr</strong> provides bindings to Spark's distributed <a href="https://spark.apache.org/docs/latest/mllib-guide.html">machine learning</a> library. 
In particular, sparklyr allows you to access the machine learning routines provided by the <a href="https://spark.apache.org/docs/latest/ml-guide">spark.ml</a> package. 
Together with sparklyr's <a href="/dplyr">dplyr</a> interface, you can easily create and tune machine learning workflows on Spark, orchestrated entirely within R.

sparklyr provides three families of functions that you can use with Spark machine learning:

Machine learning algorithms for analyzing data (<code>ml_*</code>)
Feature transformers for manipulating individual features (<code>ft_*</code>)
Functions for manipulating Spark DataFrames (<code>sdf_*</code>)

An analytic workflow with sparklyr might be composed of the following stages. 
For an example see <a href="#example-workflow">Example Workflow</a>.

<ol>
Perform SQL queries through the sparklyr <a href="/dplyr">dplyr</a> interface,
Use the <code>sdf_*</code> and <code>ft_*</code> family of functions to generate new columns, or partition your data set,
Choose an appropriate machine learning algorithm from the <code>ml_*</code> family of functions to model your data,
Inspect the quality of your model fit, and use it to make predictions with new data.
Collect the results for visualization and further analysis in R
</ol>
<h2 id="algorithms">Algorithms</h2>
Spark's machine learning library can be accessed from sparklyr through the <code>ml_*</code> set of functions:

<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th></tr>
</thead>

<tbody>
<tr>
<td><a href="/reference/ml_kmeans"><code>ml_kmeans</code></a></td>
<td>K-Means Clustering</td></tr>

<tr>
<td><a href="/reference/ml_linear_regression"><code>ml_linear_regression</code></a></td>
<td>Linear Regression</td></tr>

<tr>
<td><a href="/reference/ml_logistic_regression"><code>ml_logistic_regression</code></a></td>
<td>Logistic Regression</td></tr>

<tr>
<td><a href="/reference/ml_survival_regression"><code>ml_survival_regression</code></a></td>
<td>Survival Regression</td></tr>

<tr>
<td><a href="/reference/ml_generalized_linear_regression"><code>ml_generalized_linear_regression</code></a></td>
<td>Generalized Linear Regression</td></tr>

<tr>
<td><a href="/reference/ml_decision_tree"><code>ml_decision_tree</code></a></td>
<td>Decision Trees</td></tr>

<tr>
<td><a href="/reference/ml_random_forest"><code>ml_random_forest</code></a></td>
<td>Random Forests</td></tr>

<tr>
<td><a href="/reference/ml_gradient_boosted_trees"><code>ml_gradient_boosted_trees</code></a></td>
<td>Gradient-Boosted Trees</td></tr>

<tr>
<td><a href="/reference/ml_pca"><code>ml_pca</code></a></td>
<td>Principal Components Analysis</td></tr>

<tr>
<td><a href="/reference/ml_naive_bayes"><code>ml_naive_bayes</code></a></td>
<td>Naive-Bayes</td></tr>

<tr>
<td><a href="/reference/ml_multilayer_perceptron"><code>ml_multilayer_perceptron</code></a></td>
<td>Multilayer Perceptron</td></tr>

<tr>
<td><a href="/reference/ml_lda"><code>ml_lda</code></a></td>
<td>Latent Dirichlet Allocation</td></tr>

<tr>
<td><a href="/reference/ml_lda"><code>ml_one_vs_rest</code></a></td>
<td>One vs Rest</td></tr>
</tbody>
</table>

<h3 id="formulas">Formulas</h3>

The <code>ml_*</code> functions take the arguments <code>response</code> and <code>features</code>. 
But <code>features</code> can also be a formula with main effects (it currently does not accept interaction terms). 
The intercept term can be omitted by using <code>-1</code>.

<code class="language-r"># Equivalent statements
ml_linear_regression(z ~ -1 + x + y)
ml_linear_regression(intercept = FALSE, response = &quot;z&quot;, features = c(&quot;x&quot;, &quot;y&quot;))
</code>

<h3 id="options">Options</h3>

The Spark model output can be modified with the <code>ml_options</code> argument in the <code>ml_*</code> functions. 
The <code>ml_options</code> is an <em>experts only</em> interface for tweaking the model output. 
For example, <code>model.transform</code> can be used to mutate the Spark model object before the fit is performed.
<h2 id="transformers">Transformers</h2>
A model is often fit not on a dataset as-is, but instead on some transformation of that dataset. 
Spark provides <a href="http://spark.apache.org/docs/latest/ml-features">feature transformers</a>, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the <code>ft_*</code> family of functions. 
These routines generally take one or more input columns, and generate a new output column formed as a transformation of those columns.

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="/reference/ft_binarizer"><code>ft_binarizer</code></a></td>
<td>Threshold numerical features to binary (0/1) feature</td></tr>
<tr class="even">
<td><a href="/reference/ft_bucketizer"><code>ft_bucketizer</code></a></td>
<td>Bucketizer transforms a column of continuous features to a column of feature buckets</td></tr>
<tr class="odd">
<td><a href="/reference/ft_discrete_cosine_transform"><code>ft_discrete_cosine_transform</code></a></td>
<td>Transforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain</td></tr>
<tr class="even">
<td><a href="/reference/ft_elementwise_product"><code>ft_elementwise_product</code></a></td>
<td>Multiplies each input vector by a provided weight vector, using element-wise multiplication.</td></tr>
<tr class="odd">
<td><a href="/reference/ft_index_to_string"><code>ft_index_to_string</code></a></td>
<td>Maps a column of label indices back to a column containing the original labels as strings</td></tr>
<tr class="even">
<td><a href="/reference/ft_quantile_discretizer"><code>ft_quantile_discretizer</code></a></td>
<td>Takes a column with continuous features and outputs a column with binned categorical features</td></tr>
<tr class="odd">
<td><a href="/reference/sql-transformer"><code>sql_transformer</code></a></td>
<td>Implements the transformations which are defined by a SQL statement</td></tr>
<tr class="even">
<td><a href="/reference/ft_string_indexer"><code>ft_string_indexer</code></a></td>
<td>Encodes a string column of labels to a column of label indices</td></tr>
<tr class="odd">
<td><a href="/reference/ft_vector_assembler"><code>ft_vector_assembler</code></a></td>
<td>Combines a given list of columns into a single vector column</td></tr>
</tbody>
</table>

<h3>Examples</h3>

We will use the <code>iris</code> data set to examine a handful of learning algorithms and transformers. 
The iris data set measures attributes for 150 flowers in 3 different species of iris.

<code class="language-r">library(sparklyr)
</code>
<code> ## Warning: package 'sparklyr' was built under R version 3.4.3
</code>

<code class="language-r">library(ggplot2)
library(dplyr)
</code>
<code> ## 
## Attaching package: 'dplyr'

## The following objects are masked from 'package:stats':
## 
##     filter, lag

## The following objects are masked from 'package:base':
## 
##     intersect, setdiff, setequal, union
</code>

<code class="language-r">sc &lt;- spark_connect(master = &quot;local&quot;)
</code>
<code> ## * Using Spark: 2.1.0
</code>

<code class="language-r">iris_tbl &lt;- copy_to(sc, iris, &quot;iris&quot;, overwrite = TRUE)
iris_tbl
</code>
<code> ## # Source:   table&lt;iris&gt; [?? x 5]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1          5.1         3.5          1.4         0.2  setosa
##  2          4.9         3.0          1.4         0.2  setosa
##  3          4.7         3.2          1.3         0.2  setosa
##  4          4.6         3.1          1.5         0.2  setosa
##  5          5.0         3.6          1.4         0.2  setosa
##  6          5.4         3.9          1.7         0.4  setosa
##  7          4.6         3.4          1.4         0.3  setosa
##  8          5.0         3.4          1.5         0.2  setosa
##  9          4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... 
with more rows
</code>

<h3 id="k-means-clustering">K-Means Clustering</h3>

Use Spark's <a href="http://spark.apache.org/docs/latest/ml-clustering.html#k-means">K-means clustering</a> to partition a dataset into groups. 
K-means clustering partitions points into <code>k</code> groups, such that the sum of squares from points to the assigned cluster centers is minimized.

<code class="language-r">kmeans_model &lt;- iris_tbl %&gt;%
  select(Petal_Width, Petal_Length) %&gt;%
  ml_kmeans(centers = 3)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r"># print our model fit
kmeans_model
</code>
<code> ## K-means clustering with 3 clusters
## 
## Cluster centers:
##   Petal_Width Petal_Length
## 1    1.359259     4.292593
## 2    0.246000     1.462000
## 3    2.047826     5.626087
## 
## Within Set Sum of Squared Errors =  31.41289
</code>

<code class="language-r"># predict the associated class
predicted &lt;- sdf_predict(kmeans_model, iris_tbl) %&gt;%
  collect
table(predicted$Species, predicted$prediction)
</code>
<code> ##             
##               0  1  2
##   setosa      0 50  0
##   versicolor 48  0  2
##   virginica   6  0 44
</code>

<code class="language-r"># plot cluster membership
sdf_predict(kmeans_model) %&gt;%
  collect() %&gt;%
  ggplot(aes(Petal_Length, Petal_Width)) +
  geom_point(aes(Petal_Width, Petal_Length, col = factor(prediction + 1)),
     size = 2, alpha = 0.5) + 
  geom_point(data = kmeans_model$centers, aes(Petal_Width, Petal_Length),
     col = scales::muted(c(&quot;red&quot;, &quot;green&quot;, &quot;blue&quot;)),
     pch = 'x', size = 12) +
  scale_color_discrete(name = &quot;Predicted Cluster&quot;,
         labels = paste(&quot;Cluster&quot;, 1:3)) +
  labs(
    x = &quot;Petal Length&quot;,
    y = &quot;Petal Width&quot;,
    title = &quot;K-Means Clustering&quot;,
    subtitle = &quot;Use Spark.ML to predict cluster membership with the iris dataset.&quot;
  )
</code>

<img src="guides-mllib_files/figure-markdown_github/unnamed-chunk-5-1.png" alt="" />

<h3 id="linear-regression">Linear Regression</h3>

Use Spark's <a href="http://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression">linear regression</a> to model the linear relationship between a response variable and one or more explanatory variables.

<code class="language-r">lm_model &lt;- iris_tbl %&gt;%
  select(Petal_Width, Petal_Length) %&gt;%
  ml_linear_regression(Petal_Length ~ Petal_Width)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r">iris_tbl %&gt;%
  select(Petal_Width, Petal_Length) %&gt;%
  collect %&gt;%
  ggplot(aes(Petal_Length, Petal_Width)) +
    geom_point(aes(Petal_Width, Petal_Length), size = 2, alpha = 0.5) +
    geom_abline(aes(slope = coef(lm_model)[[&quot;Petal_Width&quot;]],
      intercept = coef(lm_model)[[&quot;(Intercept)&quot;]]),
  color = &quot;red&quot;) +
  labs(
    x = &quot;Petal Width&quot;,
    y = &quot;Petal Length&quot;,
    title = &quot;Linear Regression: Petal Length ~ Petal Width&quot;,
    subtitle = &quot;Use Spark.ML linear regression to predict petal length as a function of petal width.&quot;
  )
</code>

<img src="guides-mllib_files/figure-markdown_github/unnamed-chunk-6-1.png" alt="" />

<h3 id="logistic-regression">Logistic Regression</h3>

Use Spark's <a href="http://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression">logistic regression</a> to perform logistic regression, modeling a binary outcome as a function of one or more explanatory variables.

<code class="language-r"># Prepare beaver dataset
beaver &lt;- beaver2
beaver$activ &lt;- factor(beaver$activ, labels = c(&quot;Non-Active&quot;, &quot;Active&quot;))
copy_to(sc, beaver, &quot;beaver&quot;)
</code>
<code> ## # Source:   table&lt;beaver&gt; [?? x 4]
## # Database: spark_connection
##      day  time  temp      activ
##    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;chr&gt;
##  1   307   930 36.58 Non-Active
##  2   307   940 36.73 Non-Active
##  3   307   950 36.93 Non-Active
##  4   307  1000 37.15 Non-Active
##  5   307  1010 37.23 Non-Active
##  6   307  1020 37.24 Non-Active
##  7   307  1030 37.24 Non-Active
##  8   307  1040 36.90 Non-Active
##  9   307  1050 36.95 Non-Active
## 10   307  1100 36.89 Non-Active
## # ... 
with more rows
</code>

<code class="language-r">beaver_tbl &lt;- tbl(sc, &quot;beaver&quot;)

glm_model &lt;- beaver_tbl %&gt;%
  mutate(binary_response = as.numeric(activ == &quot;Active&quot;)) %&gt;%
  ml_logistic_regression(binary_response ~ temp)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r">glm_model
</code>
<code> ## Call: binary_response ~ temp
## 
## Coefficients:
## (Intercept)        temp 
##  -550.52331    14.69184
</code>

<h3 id="pca">PCA</h3>

Use Spark's <a href="https://spark.apache.org/docs/latest/mllib-dimensionality-reduction">Principal Components Analysis (PCA)</a> to perform dimensionality reduction. 
PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.

<code class="language-r">pca_model &lt;- tbl(sc, &quot;iris&quot;) %&gt;%
  select(-Species) %&gt;%
  ml_pca()
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r">print(pca_model)
</code>
<code> ## Explained variance:
## 
##         PC1         PC2         PC3         PC4 
## 0.924618723 0.053066483 0.017102610 0.005212184 
## 
## Rotation:
##                      PC1         PC2         PC3        PC4
## Sepal_Length -0.36138659 -0.65658877  0.58202985  0.3154872
## Sepal_Width   0.08452251 -0.73016143 -0.59791083 -0.3197231
## Petal_Length -0.85667061  0.17337266 -0.07623608 -0.4798390
## Petal_Width  -0.35828920  0.07548102 -0.54583143  0.7536574
</code>

<h3 id="random-forest">Random Forest</h3>

Use Spark's <a href="https://spark.apache.org/docs/latest/ml-classification-regression.html#random-forest-regression">Random Forest</a> to perform regression or multiclass classification.

<code class="language-r">rf_model &lt;- iris_tbl %&gt;%
  ml_random_forest(Species ~ Petal_Length + Petal_Width, type = &quot;classification&quot;)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r">rf_predict &lt;- sdf_predict(rf_model, iris_tbl) %&gt;%
  ft_string_indexer(&quot;Species&quot;, &quot;Species_idx&quot;) %&gt;%
  collect

table(rf_predict$Species_idx, rf_predict$prediction)
</code>
<code> ##    
##      0  1  2
##   0 49  1  0
##   1  0 50  0
##   2  0  0 50
</code>

<h3 id="sdf-partitioning">SDF Partitioning</h3>

Split a Spark DataFrame into training, test datasets.

<code class="language-r">partitions &lt;- tbl(sc, &quot;iris&quot;) %&gt;%
  sdf_partition(training = 0.75, test = 0.25, seed = 1099)

fit &lt;- partitions$training %&gt;%
  ml_linear_regression(Petal_Length ~ Petal_Width)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r">estimate_mse &lt;- function(df){
  sdf_predict(fit, df) %&gt;%
  mutate(resid = Petal_Length - prediction) %&gt;%
  summarize(mse = mean(resid ^ 2)) %&gt;%
  collect
}

sapply(partitions, estimate_mse)
</code>
<code> ## $training.mse
## [1] 0.2374596
## 
## $test.mse
## [1] 0.1898848
</code>

<h3 id="ft-string-indexing">FT String Indexing</h3>

Use <code>ft_string_indexer</code> and <code>ft_index_to_string</code> to convert a character column into a numeric column and back again.

<code class="language-r">ft_string2idx &lt;- iris_tbl %&gt;%
  ft_string_indexer(&quot;Species&quot;, &quot;Species_idx&quot;) %&gt;%
  ft_index_to_string(&quot;Species_idx&quot;, &quot;Species_remap&quot;) %&gt;%
  collect

table(ft_string2idx$Species, ft_string2idx$Species_remap)
</code>
<code> ##             
##              setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         50         0
##   virginica       0          0        50
</code>

<h3 id="sdf-mutate">SDF Mutate</h3>

<a href="/reference/sdf_mutate">sdf_mutate</a> is provided as a helper function, to allow you to use feature transformers. 
For example, the previous code snippet could have been written as:

<code class="language-r">ft_string2idx &lt;- iris_tbl %&gt;%
  sdf_mutate(Species_idx = ft_string_indexer(Species)) %&gt;%
  sdf_mutate(Species_remap = ft_index_to_string(Species_idx)) %&gt;%
  collect
  
ft_string2idx %&gt;%
  select(Species, Species_idx, Species_remap) %&gt;%
  distinct
</code>
<code> ## # A tibble: 3 x 3
##      Species Species_idx Species_remap
##        &lt;chr&gt;       &lt;dbl&gt;         &lt;chr&gt;
## 1     setosa           2        setosa
## 2 versicolor           0    versicolor
## 3  virginica           1     virginica
</code>

<h3 id="example-workflow">Example Workflow</h3>

Let's walk through a simple example to demonstrate the use of Spark's machine learning algorithms within R. 
We'll use <a href="/reference/ml_linear_regression">ml_linear_regression</a> to fit a linear regression model. 
Using the built-in <code>mtcars</code> dataset, we'll try to predict a car's fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>).

First, we will copy the <code>mtcars</code> dataset into Spark.

<code class="language-r">mtcars_tbl &lt;- copy_to(sc, mtcars, &quot;mtcars&quot;)
</code>

Transform the data with Spark SQL, feature transformers, and DataFrame functions.

<ol>
Use Spark SQL to remove all cars with horsepower less than 100
Use Spark feature transformers to bucket cars into two groups based on cylinders
Use Spark DataFrame functions to partition the data into test and training
</ol>

Then fit a linear model using spark ML. 
Model MPG as a function of weight and cylinders.

<code class="language-r"># transform our data set, and then partition into 'training', 'test'
partitions &lt;- mtcars_tbl %&gt;%
  filter(hp &gt;= 100) %&gt;%
  sdf_mutate(cyl8 = ft_bucketizer(cyl, c(0,8,12))) %&gt;%
  sdf_partition(training = 0.5, test = 0.5, seed = 888)

# fit a linear mdoel to the training dataset
fit &lt;- partitions$training %&gt;%
  ml_linear_regression(mpg ~ wt + cyl)
</code>
<code> ## * No rows dropped by 'na.omit' call
</code>

<code class="language-r"># summarize the model
summary(fit)
</code>
<code> ## Call: ml_linear_regression(., mpg ~ wt + cyl)
## 
## Deviance Residuals::
##     Min      1Q  Median      3Q     Max 
## -2.0947 -1.2747 -0.1129  1.0876  2.2185 
## 
## Coefficients:
##             Estimate Std. 
Error t value Pr(&gt;|t|)    
## (Intercept) 33.79558    2.67240 12.6462 4.92e-07 ***
## wt          -1.59625    0.73729 -2.1650  0.05859 . 
 
## cyl         -1.58036    0.49670 -3.1817  0.01115 *  
## ---
## Signif. 
codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## R-Squared: 0.8267
## Root Mean Squared Error: 1.437
</code>

The <code>summary()</code> suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. 
(The model suggests that, on average, heavier cars consume more fuel.)

Let's use our Spark model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. 
We'll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.

<code class="language-r"># Score the data
pred &lt;- sdf_predict(fit, partitions$test) %&gt;%
  collect

# Plot the predicted versus actual mpg
ggplot(pred, aes(x = mpg, y = prediction)) +
  geom_abline(lty = &quot;dashed&quot;, col = &quot;red&quot;) +
  geom_point() +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_fixed(ratio = 1) +
  labs(
    x = &quot;Actual Fuel Consumption&quot;,
    y = &quot;Predicted Fuel Consumption&quot;,
    title = &quot;Predicted vs. 
Actual Fuel Consumption&quot;
  )
</code>

<img src="guides-mllib_files/figure-markdown_github/unnamed-chunk-15-1.png" alt="" />

Although simple, our model appears to do a fairly good job of predicting a car's average fuel consumption.

As you can see, we can easily and effectively combine feature transformers, machine learning algorithms, and Spark DataFrame functions into a complete analysis with Spark and R.
<h2>Understanding Spark Caching </h2>
<h2>Introduction</h2>
Spark also supports pulling data sets into a cluster-wide in-memory cache. 
This is very useful when data is accessed repeatedly, such as when querying a small dataset or when running an iterative algorithm like random forests. 
Since operations in Spark are lazy, caching can help force computation. 
Sparklyr tools can be used to cache and uncache DataFrames. 
The Spark UI will tell you which DataFrames and what percentages are in memory.

By using a reproducible example, we will review some of the main configuration settings, commands and command arguments that can be used that can help you get the best out of Spark’s memory management options.
<h2>Preparation</h2>
<h3>Download Test Data</h3>
The 2008 and 2007 Flights data from the Statistical Computing site will be used for this exercise. 
The <strong>spark_read_csv</strong> supports reading compressed CSV files in a <strong>bz2</strong> format, so no additional file preparation is needed.
<code> if(!file.exists(&quot;2008.csv.bz2&quot;))
  {download.file(&quot;http://stat-computing.org/dataexpo/2009/2008.csv.bz2&quot;, &quot;2008.csv.bz2&quot;)}
if(!file.exists(&quot;2007.csv.bz2&quot;))
  {download.file(&quot;http://stat-computing.org/dataexpo/2009/2007.csv.bz2&quot;, &quot;2007.csv.bz2&quot;)}</code>

<h3>Start a Spark session</h3>
A local deployment will be used for this example.
<code> library(sparklyr)
library(dplyr)
library(ggplot2)

# Install Spark version 2
spark_install(version = &quot;2.0.0&quot;)

# Customize the connection configuration
conf &lt;- spark_config()
conf$`sparklyr.shell.driver-memory` &lt;- &quot;16G&quot;

# Connect to Spark
sc &lt;- spark_connect(master = &quot;local&quot;, config = conf, version = &quot;2.0.0&quot;)</code>
<h2>The Memory Argument</h2>
In the <em>spark_read_…</em> functions, the <strong>memory</strong> argument controls if the data will be loaded into memory as an RDD. 
Setting it to <strong>FALSE</strong> means that Spark will essentially map the file, but not make a copy of it in memory. 
This makes the <strong>spark_read_csv</strong> command run faster, but the trade off is that any data transformation operations will take much longer.
<code> spark_read_csv(sc, &quot;flights_spark_2008&quot;, &quot;2008.csv.bz2&quot;, memory = FALSE)</code>

In the RStudio IDE, the <strong>flights_spark_2008</strong> table now shows up in the Spark tab.

<center>
<a href="images/deployment/performance/latest-tab1.png">
<img src="images/deployment/performance/latest-tab1.png" width="400px"/>
</a>
</center>
To access the Spark Web UI, click the <strong>SparkUI</strong> button in the <strong>RStudio Spark Tab</strong>. 
As expected, the <strong>Storage</strong> page shows no tables loaded into memory.

<center>
<a href="images/deployment/performance/storage-1.png">
<img src="images/deployment/performance/storage-1.png" width="1000px"/>
</a>
</center>
<h2>Loading Less Data into Memory</h2>
Using the pre-processing capabilities of Spark, the data will be transformed before being loaded into memory. 
In this section, we will continue to build on the example started in the <strong>Spark Read</strong> section

<h3>Lazy Transform</h3>
The following <strong>dplyr</strong> script will not be immediately run, so the code is processed quickly. 
There are some check-ups made, but for the most part it is building a Spark SQL statement in the background.
<code> flights_table &lt;- tbl(sc,&quot;flights_spark_2008&quot;) %&gt;%
  mutate(DepDelay = as.numeric(DepDelay),
 ArrDelay = as.numeric(ArrDelay),
 DepDelay &gt; 15 , DepDelay &lt; 240,
 ArrDelay &gt; -60 , ArrDelay &lt; 360, 
 Gain = DepDelay - ArrDelay) %&gt;%
  filter(ArrDelay &gt; 0) %&gt;%
  select(Origin, Dest, UniqueCarrier, Distance, DepDelay, ArrDelay, Gain)</code>

<h3>Register in Spark</h3>
<strong>sdf_register</strong> will register the resulting Spark SQL in Spark. 
The results will show up as a table called <strong>flights_spark</strong>. 
But a table of the same name is still not loaded into memory in Spark.
<code> sdf_register(flights_table, &quot;flights_spark&quot;)</code>

<center>
<a href="images/deployment/performance/spark-tab-3.png">
<img src="images/deployment/performance/spark-tab-3.png" width="400px"/>
</a>
</center>

<h3>Cache into Memory</h3>
The <strong>tbl_cache</strong> command loads the results into an Spark RDD in memory, so any analysis from there on will not need to re-read and re-transform the original file. 
The resulting Spark RDD is smaller than the original file because the transformations created a smaller data set than the original file.
<code> tbl_cache(sc, &quot;flights_spark&quot;)</code>

<center>
<a href="images/deployment/performance/storage-new-3.png">
<img src="images/deployment/performance/storage-new-3.png" width="1000px"/>
</a>
</center>

<h3>Driver Memory</h3>
In the <strong>Executors</strong> page of the Spark Web UI, we can see that the Storage Memory is at about half of the 16 gigabytes requested. 
This is mainly because of a Spark setting called <strong>spark.memory.fraction</strong>, which reserves by default 40% of the memory requested.

<center>
<a href="images/deployment/performance/drivers-1.png">
<img src="images/deployment/performance/drivers-1.png" width="1000px"/>
</a>
</center>
<h2>Process on the fly</h2>
The plan is to read the Flights 2007 file, combine it with the 2008 file and summarize the data without bringing either file fully into memory.
<code> spark_read_csv(sc, &quot;flights_spark_2007&quot; , &quot;2007.csv.bz2&quot;, memory = FALSE)</code>

<h3>Union and Transform</h3>
The <strong>union</strong> command is akin to the <strong>bind_rows</strong> dyplyr command. 
It will allow us to append the 2007 file to the 2008 file, and as with the previous transform, this script will be evaluated lazily.
<code> all_flights &lt;- tbl(sc, &quot;flights_spark_2008&quot;) %&gt;%
  union(tbl(sc, &quot;flights_spark_2007&quot;)) %&gt;%
  group_by(Year, Month) %&gt;%
  tally()</code>

<h3>Collect into R</h3>
When receiving a <strong>collect</strong> command, Spark will execute the SQL statement and send the results back to R in a data frame. 
In this case, R only loads 24 observations into a data frame called <em>all_flights</em>.
<code> all_flights &lt;- all_flights %&gt;%
  collect()</code>

<center>
<a href="images/deployment/performance/all-flights.png">
<img src="images/deployment/performance/all-flights.png" width="400px"/>
</a>
</center>

<h3>Plot in R</h3>
Now the smaller data set can be plotted
<code> ggplot(data = all_flights, aes(x = Month, y = n/1000, fill = factor(Year))) +
  geom_area(position = &quot;dodge&quot;, alpha = 0.5) +
  geom_line(alpha = 0.4) +
  scale_fill_brewer(palette = &quot;Dark2&quot;, name = &quot;Year&quot;) +
  scale_x_continuous(breaks = 1:12, labels = c(&quot;J&quot;,&quot;F&quot;,&quot;M&quot;,&quot;A&quot;,&quot;M&quot;,&quot;J&quot;,&quot;J&quot;,&quot;A&quot;,&quot;S&quot;,&quot;O&quot;,&quot;N&quot;,&quot;D&quot;)) +
  theme_light() +
  labs(y=&quot;Number of Flights (Thousands)&quot;, title = &quot;Number of Flights Year-Over-Year&quot;)</code>

<center>
<a href="images/deployment/performance/new-plot.png">
<img src="images/deployment/performance/new-plot.png" width="600"/>
</a>
</center>
<h2>Deployment and Configuration </h2>
<h2>Deployment</h2>
There are two well supported deployment modes for <strong>sparklyr</strong>:

Local — Working on a local desktop typically with smaller/sampled datasets
Cluster — Working directly within or alongside a Spark cluster (<a href="http://spark.apache.org/docs/latest/spark-standalone.html">standalone</a>, <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html">YARN</a>, <a href="http://mesos.apache.org/">Mesos</a>, etc.)

<h3>Local Deployment</h3>
Local mode is an excellent way to learn and experiment with Spark. 
Local mode also provides a convenient development environment for analyses, reports, and applications that you plan to eventually deploy to a multi-node Spark cluster.

To work in local mode you should first install a version of Spark for local use. 
You can do this using the <a href="/reference/spark_install">spark_install</a> function, for example:
<code> sparklyr::spark_install(version = &quot;2.1.0&quot;)</code>

To connect to the local Spark instance you pass “local” as the value of the Spark master node to <a href="/reference/spark-connections/">spark_connect</a>:
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;)</code>

For the local development scenario, see the <a href="#configuration">Configuration</a> section below for details on how to have the same code work seamlessly in both development and production environments.

<h3>Cluster Deployment</h3>
A common deployment strategy is to submit your application from a gateway machine that is physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster). 
In this setup, client mode is appropriate. 
In client mode, the driver is launched directly within the spark-submit process which acts as a client to the cluster. 
The input and output of the application is attached to the console. 
Thus, this mode is especially suitable for applications that involve the REPL (e.g. Spark shell). 
For more information see <a href="http://spark.apache.org/docs/latest/submitting-applications.html">Submitting Applications</a>.

To use spaklyr with a Spark cluster you should locate your R session on a machine that is either directly on one of the cluster nodes or is close to the cluster (for networking performance). 
In the case where R is not running directly on the cluster you should also ensure that the machine has a Spark version and configuration <strong>identical</strong> to that of the cluster nodes.

The most straightforward way to run R within or near to the cluster is either a remote SSH session or via <a href="https://www.rstudio.com/products/rstudio/">RStudio Server</a>.

In cluster mode you use the version of Spark already deployed on the cluster node. 
This version is located via the <code>SPARK_HOME</code> environment variable, so you should be sure that this variable is correctly defined on your server before attempting a connection. 
This would typically be done within the <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html">Renviron.site</a> configuration file. 
For example:
<code> SPARK_HOME=/opt/spark/spark-2.0.0-bin-hadoop2.6</code>

To connect, pass the address of the master node to <a href="/reference/spark-connections/">spark_connect</a>, for example:
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;spark://local:7077&quot;)</code>

For a <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/index.html">Hadoop YARN</a> cluster, you can connect using the YARN master, for example:
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;yarn-client&quot;)</code>

If you are running on EC2 using the Spark <a href="http://spark.apache.org/docs/latest/ec2-scripts.html">EC2 deployment scripts</a> then you can read the master from <code>/root/spark-ec2/cluster-url</code>, for example:
<code> library(sparklyr)
cluster_url &lt;- system(&#39;cat /root/spark-ec2/cluster-url&#39;, intern=TRUE)
sc &lt;- spark_connect(master = cluster_url)</code>

<h3>Livy Connections</h3>
<a href="http://livy.io/">Livy</a>, <em>“An Open Source REST Service for Apache Spark (Apache License)”</em> , is available starting in <code>sparklyr 0.5</code> as an <strong>experimental</strong> feature. 
Among many scenarios, this enables connections from the RStudio desktop to Apache Spark when Livy is available and correctly configured in the remote cluster.

To work with Livy locally, <code>sparklyr</code> supports <code>livy_install()</code> which installs Livy in your local environment, this is similar to <code>spark_install()</code>. 
Since Livy is a service to enable remote connections into Apache Spark, the service needs to be started with <code>livy_service_start()</code>. 
Once the service is running, <code>spark_connect()</code> needs to reference the running service and use <code>method = &quot;Livy&quot;</code>, then <code>sparklyr</code> can be used as usual. 
A short example follows:
<code> livy_install()
livy_service_start()

sc &lt;- spark_connect(master = &quot;http://localhost:8998&quot;, method = &quot;livy&quot;)
copy_to(sc, iris)

spark_disconnect(sc)
livy_service_stop()</code>

<h3>Connection Tools</h3>
You can view the Spark web UI via the <a href="/reference/spark_web">spark_web</a> function, and view the Spark log via the <a href="/reference/spark_log">spark_log</a> function:
<code> spark_web(sc)
spark_log(sc)</code>

You can disconnect from Spark using the <a href="/reference/spark-connections/">spark_disconnect</a> function:
<code> spark_disconnect(sc)</code>

<h3>Collect</h3>
The <code>collect</code> function transfers data from Spark into R. 
The data are collected from a cluster environment and transfered into local R memory. 
In the process, all data is first transfered from executor nodes to the driver node. 
Therefore, the driver node must have enough memory to collect all the data.

Collecting data on the driver node is relatively slow. 
The process also inflates the data as it moves from the executor nodes to the driver node. 
Caution should be used when collecting large data.

The following parameters could be adjusted to avoid OutOfMemory and Timeout errors:

spark.executor.heartbeatInterval
spark.network.timeout
spark.driver.extraJavaOptions
spark.driver.memory
spark.yarn.driver.memoryOverhead
spark.driver.maxResultSize
<h2>Configuration</h2>
This section describes the various options available for configuring both the behavior of the <strong>sparklyr</strong> package as well as the underlying Spark cluster. 
Creating multiple configuration profiles (e.g. development, test, production) is also covered.

<h3>Config Files</h3>
The configuration for a Spark connection is specified via the <code>config</code> parameter of the <a href="/reference/spark-connections/">spark_connect</a> function. 
By default the configuration is established by calling the <a href="/reference/spark_config/">spark_config</a> function. 
This code represents the default behavior:
<code> spark_connect(master = &quot;local&quot;, config = spark_config())</code>

By default the <a href="reference/sparklyr/latest/spark_config.html">spark_config</a> function reads configuration data from a file named <code>config.yml</code> located in the current working directory (or in parent directories if not located in the working directory). 
This file is not required and only need be provided for overriding default behavior. 
You can also specify an alternate config file name and/or location.

The <code>config.yml</code> file is in turn processed using the <a href="https://github.com/rstudio/config">config</a> package, which enables support for multiple named configuration profiles.

<h3>Package Options</h3>
There are a number of options available to configure the behavior of the sparklyr package:

For example, this configuration file sets the number of local cores to 4 and the amount of memory allocated for the Spark driver to 4G:
<code> default:
  sparklyr.cores.local: 4
  sparklyr.shell.driver-memory: 4G</code>

Note that the use of <code>default</code> will be explained below in <a href="#multiple-profiles">Multiple Profiles</a>.
<h4>Spark</h4>
<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>sparklyr.shell.*</code></td>
<td>Command line parameters to pass to <code>spark-submit</code>. 
For example, <code>sparklyr.shell.executor-memory: 20G</code> configures <code>--executor-memory 20G</code> (see the <a href="https://spark.apache.org/docs/latest/submitting-applications.html">Spark documentation</a> for details on supported options).</td></tr>
</tbody>
</table>
<h4>Runtime</h4>
<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>sparklyr.cores.local</code></td>
<td>Number of cores to use when running in local mode (defaults to <code>parallel::detectCores</code>).</td></tr>
<tr class="even">
<td><code>sparklyr.sparkui.url</code></td>
<td>Configures the url to the Spark UI web interface when calling spark_web.</td></tr>
<tr class="odd">
<td><code>sparklyr.defaultPackages</code></td>
<td>List of default Spark packages to install in the cluster (defaults to “com.databricks:spark-csv_2.11:1.3.0” and “com.amazonaws:aws-java-sdk-pom:1.10.34”).</td></tr>
<tr class="even">
<td><code>sparklyr.sanitize.column.names</code></td>
<td>Allows Spark to automatically rename column names to conform to Spark naming restrictions.</td></tr>
</tbody>
</table>
<h4>Diagnostics</h4>
<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>sparklyr.backend.threads</code></td>
<td>Number of threads to use in the sparklyr backend to process incoming connections form the sparklyr client.</td></tr>
<tr class="even">
<td><code>sparklyr.app.jar</code></td>
<td>The application jar to be submitted in Spark submit.</td></tr>
<tr class="odd">
<td><code>sparklyr.ports.file</code></td>
<td>Path to the ports file used to share connection information to the sparklyr backend.</td></tr>
<tr class="even">
<td><code>sparklyr.ports.wait.seconds</code></td>
<td>Number of seconds to wait while for the Spark connection to initialize.</td></tr>
<tr class="odd">
<td><code>sparklyr.verbose</code></td>
<td>Provide additional feedback while performing operations. 
Currently used to communicate which column names are being sanitized in sparklyr.sanitize.column.names.</td></tr>
</tbody>
</table>

<h3>Spark Options</h3>
You can also use <code>config.yml</code> to specify arbitrary Spark configuration properties:

<table>
<colgroup>
<col width="20%" />
<col width="79%" />
</colgroup>
<thead>
<tr class="header">
<th>Option</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>spark.*</code></td>
<td>Configuration settings for the Spark context (applied by creating a <code>SparkConf</code> containing the specified properties). 
For example, <code>spark.executor.memory: 1g</code> configures the memory available in each executor (see <a href="http://spark.apache.org/docs/latest/configuration.html#application-properties">Spark Configuration</a> for additional options.)</td></tr>
<tr class="even">
<td><code>spark.sql.*</code></td>
<td>Configuration settings for the Spark SQL context (applied using SET). 
For instance, <code>spark.sql.shuffle.partitions</code> configures number of partitions to use while shuffling (see <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#other-configuration-options">SQL Programming Guide</a> for additional options).</td></tr>
</tbody>
</table>
For example, this configuration file sets a custom scratch directory for Spark and specifies 100 as the number of partitions to use when shuffling data for joins or aggregations:
<code> default:
  spark.local.dir: /tmp/spark-scratch
  spark.sql.shuffle.partitions: 100</code>

<h3>User Options</h3>
You can also include arbitrary custom user options within the <code>config.yml</code> file. 
These can be named anything you like so long as they <em>do not</em> use either <code>spark</code> or <code>sparklyr</code> as a prefix. 
For example, this configuration file defines <code>dataset</code> and <code>sample-size</code> options:
<code> default:
  dataset: &quot;observations.parquet&quot;
  sample-size: 10000</code>

<h3>Multiple Profiles</h3>
The <a href="https://github.com/rstudio/config">config</a> package enables the definition of multiple named configuration profiles for different environments (e.g. default, test, production). 
All environments automatically inherit from the <code>default</code> environment and can optionally also inherit from each other.

For example, you might want to use a distinct datasets for development and testing or might want to use custom Spark configuration properties that are only applied when running on a production cluster. 
Here’s how that would be expressed in <code>config.yml</code>:
<code> default:
  dataset: &quot;observations-dev.parquet&quot;
  sample-size: 10000
  
production:
  spark.memory.fraction: 0.9
  spark.rdd.compress: true
  dataset: &quot;observations.parquet&quot;
  sample-size: null</code>

You can also use this feature to specify distinct Spark master nodes for different environments, for example:
<code> default:
  spark.master: &quot;local&quot;
  
production:
  spark.master: &quot;spark://local:7077&quot;</code>

With this configuration, you can omit the <code>master</code> argument entirely from the call to <a href="/reference/spark-connections/">spark_connect</a>:
<code> sc &lt;- spark_connect()</code>

Note that the currently active configuration is determined via the value of <code>R_CONFIG_ACTIVE</code> environment variable. 
See the <a href="https://github.com/rstudio/config">config package documentation</a> for additional details.

<h3>Tuning</h3>
In general, you will need to tune a Spark cluster for it to perform well. 
Spark applications tend to consume a lot of resources. 
There are many knobs to control the performance of Yarn and executor (i.e. worker) nodes in a cluster. 
Some of the parameters to pay attention to are as follows:

spark.executor.heartbeatInterval
spark.network.timeout
spark.executor.extraJavaOptions
spark.executor.memory
spark.yarn.executor.memoryOverhead
spark.executor.cores
spark.executor.instances (if is not enabled)
<h4>Example Config</h4>
Here is an example spark configuration for an EMR cluster on AWS with 1 master and 2 worker nodes. 
Eache node has 8 vCPU and 61 GiB of memory.

<table>
<thead>
<tr class="header">
<th>Parameter</th>
<th>Value</th></tr>
</thead>
<tbody>
<tr class="odd">
<td>spark.driver.extraJavaOptions</td>
<td><em>append</em> -XX:MaxPermSize=30G</td></tr>
<tr class="even">
<td>spark.driver.maxResultSize</td>
<td>0</td></tr>
<tr class="odd">
<td>spark.driver.memory</td>
<td>30G</td></tr>
<tr class="even">
<td>spark.yarn.driver.memoryOverhead</td>
<td>4096</td></tr>
<tr class="odd">
<td>spark.yarn.executor.memoryOverhead</td>
<td>4096</td></tr>
<tr class="even">
<td>spark.executor.memory</td>
<td>4G</td></tr>
<tr class="odd">
<td>spark.executor.cores</td>
<td>2</td></tr>
<tr class="even">
<td>spark.dynamicAllocation.maxExecutors</td>
<td>15</td></tr>
</tbody>
</table>
Configuration parameters can be set in the config R object or can be set in the <code>config.yml</code>. 
Alternatively, they can be set in the <code>spark-defaults.conf</code>.

<h5>Configuration in R script</h5><code> config &lt;- spark_config()
config$spark.executor.cores &lt;- 2
config$spark.executor.memory &lt;- &quot;4G&quot;
sc &lt;- spark_connect(master = &quot;yarn-client&quot;, config = config, version = &#39;2.0.0&#39;)</code>

<h5>Configuration in YAML script</h5><code> default:
  spark.executor.cores: 2
  spark.executor.memory: 4G</code>
<h2>RStudio Server</h2>
RStudio Server provides a web-based IDE interface to a remote R session, making it ideal for use as a front-end to a Spark cluster. 
This section covers some additional configuration options that are useful for RStudio Server.

<h3>Connection Options</h3>
The RStudio IDE Spark pane provides a <strong>New Connection</strong> dialog to assist in connecting with both local instances of Spark and Spark clusters:

<img src="images/deployment/overview/connect-to-spark.png" width=486 />

You can configure which connection choices are presented using the <code>rstudio.spark.connections</code> option. 
By default, users are presented with possibility of both local and cluster connections, however, you can modify this behavior to present only one of these, or even a specific Spark master URL. 
Some commonly used combinations of connection choices include:

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Value</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>c(&quot;local&quot;, &quot;cluster&quot;)</code></td>
<td>Default. 
Present connections to both local and cluster Spark instances.</td></tr>
<tr class="even">
<td><code>&quot;local&quot;</code></td>
<td>Present only connections to local Spark instances.</td></tr>
<tr class="odd">
<td><code>&quot;spark://local:7077&quot;</code></td>
<td>Present only a connection to a specific Spark cluster.</td></tr>
<tr class="even">
<td><code>c(&quot;spark://local:7077&quot;, &quot;cluster&quot;)</code></td>
<td>Present a connection to a specific Spark cluster and other clusters.</td></tr>
</tbody>
</table>
This option should generally be set within <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html">Rprofile.site</a>. 
For example:
<code> options(rstudio.spark.connections = &quot;spark://local:7077&quot;)</code>

<h3>Spark Installations</h3>
If you are running within local mode (as opposed to cluster mode) you may want to provide pre-installed Spark version(s) to be shared by all users of the server. 
You can do this by installing Spark versions within a shared directory (e.g. <code> /opt/spark</code>) then designating it as the Spark installation directory.

For example, after installing one or more versions of Spark to <code>/opt/spark</code> you would add the following to <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html">Rprofile.site</a>:
<code> options(spark.install.dir = &quot;/opt/spark&quot;)</code>

If this directory is read-only for ordinary users then RStudio will not offer installation of additional versions, which will help guide users to a version that is known to be compatible with versions of Spark deployed on clusters in the same organization.
<h2 id="distributing-r-computations">Distributing R Computations</h2>
<h2 id="overview">Overview</h2>
<strong>sparklyr</strong> provides support to run arbitrary R code at scale within your Spark Cluster through <code>spark_apply()</code>. 
This is especially useful where there is a need to use functionality available only in R or R packages that is not available in Apache Spark nor <a href="https://spark-packages.org/">Spark Packages</a>.
<code> spark_apply()</code> applies an R function to a Spark object (typically, a Spark DataFrame). 
Spark objects are partitioned so they can be distributed across a cluster. 
You can use <code>spark_apply</code> with the default partitions or you can define your own partitions with the <code>group_by</code> argument. 
Your R function must return another Spark DataFrame. <code> spark_apply</code> will run your R function on each partition and output a single Spark DataFrame.

<h3 id="apply-an-r-function-to-a-spark-object">Apply an R function to a Spark Object</h3>

Lets run a simple example. 
We will apply the identify function, <code>I()</code>, over a list of numbers we created with the <code>sdf_len</code> function.

<code class="language-r">library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;)

sdf_len(sc, 5, repartition = 1) %&gt;%
  spark_apply(function(e) I(e))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c2e4fb50&gt; [?? x 1]
## # Database: spark_connection
##      id
##   &lt;dbl&gt;
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5
</code>

Your R function should be designed to operate on an R <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/data.frame.html">data frame</a>. 
The R function passed to <code>spark_apply</code> expects a DataFrame and will return an object that can be cast as a DataFrame. 
We can use the <code>class</code> function to verify the class of the data.

<code class="language-r">sdf_len(sc, 10, repartition = 1) %&gt;%
  spark_apply(function(e) class(e))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c7ce7618d&gt; [?? x 1]
## # Database: spark_connection
##           id
##        &lt;chr&gt;
## 1 data.frame
</code>

Spark will partition your data by hash or range so it can be distributed across a cluster. 
In the following example we create two partitions and count the number of rows in each partition. 
Then we print the first record in each partition.

<code class="language-r">trees_tbl &lt;- sdf_copy_to(sc, trees, repartition = 2)

trees_tbl %&gt;%
  spark_apply(function(e) nrow(e), names = &quot;n&quot;)
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c15c45eb1&gt; [?? x 1]
## # Database: spark_connection
##       n
##   &lt;int&gt;
## 1    16
## 2    15
</code>

<code class="language-r">trees_tbl %&gt;%
  spark_apply(function(e) head(e, 1))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c29215418&gt; [?? x 3]
## # Database: spark_connection
##   Girth Height Volume
##   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
## 1   8.3     70   10.3
## 2   8.6     65   10.3
</code>

We can apply any arbitrary function to the partitions in the Spark DataFrame. 
For instance, we can scale or jitter the columns. 
Notice that <code>spark_apply</code> applies the R function to all partitions and returns a single DataFrame.

<code class="language-r">trees_tbl %&gt;%
  spark_apply(function(e) scale(e))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c8922ba8&gt; [?? x 3]
## # Database: spark_connection
##         Girth      Height     Volume
##         &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;
##  1 -1.4482330 -0.99510521 -1.1503645
##  2 -1.3021313 -2.06675697 -1.1558670
##  3 -0.7469449  0.68891899 -0.6826528
##  4 -0.6592839 -1.60747764 -0.8587325
##  5 -0.6300635  0.53582588 -0.4735581
##  6 -0.5716229  0.38273277 -0.3855183
##  7 -0.5424025 -0.07654655 -0.5395880
##  8 -0.3670805 -0.22963966 -0.6661453
##  9 -0.1040975  1.30129143  0.1427209
## 10  0.1296653 -0.84201210 -0.3029809
## # ... 
with more rows
</code>

<code class="language-r">trees_tbl %&gt;%
  spark_apply(function(e) lapply(e, jitter))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c43237574&gt; [?? x 3]
## # Database: spark_connection
##        Girth   Height   Volume
##        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1  8.319392 70.04321 10.30556
##  2  8.801237 62.85795 10.21751
##  3 10.719805 81.15618 18.78076
##  4 11.009892 65.98926 15.58448
##  5 11.089322 80.14661 22.58749
##  6 11.309682 79.01360 24.18158
##  7 11.418486 75.88748 21.38380
##  8 11.982421 74.85612 19.09375
##  9 12.907616 84.81742 33.80591
## 10 13.691892 71.05309 25.70321
## # ... 
with more rows
</code>

By default <code>spark_apply()</code> derives the column names from the input Spark data frame. 
Use the <code>names</code> argument to rename or add new columns.

<code class="language-r">trees_tbl %&gt;%
  spark_apply(
    function(e) data.frame(2.54 * e$Girth, e),
    names = c(&quot;Girth(cm)&quot;, colnames(trees)))
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c14e015b5&gt; [?? x 4]
## # Database: spark_connection
##    `Girth(cm)` Girth Height Volume
##          &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1      21.082   8.3     70   10.3
##  2      22.352   8.8     63   10.2
##  3      27.178  10.7     81   18.8
##  4      27.940  11.0     66   15.6
##  5      28.194  11.1     80   22.6
##  6      28.702  11.3     79   24.2
##  7      28.956  11.4     76   21.4
##  8      30.480  12.0     75   19.1
##  9      32.766  12.9     85   33.8
## 10      34.798  13.7     71   25.7
## # ... 
with more rows
</code>

<h3 id="group-by">Group By</h3>

In some cases you may want to apply your R function to specific groups in your data. 
For example, suppose you want to compute regression models against specific subgroups. 
To solve this, you can specify a <code>group_by</code> argument. 
This example counts the number of rows in <code>iris</code> by species and then fits a simple linear model for each species.

<code class="language-r">iris_tbl &lt;- sdf_copy_to(sc, iris)

iris_tbl %&gt;%
  spark_apply(nrow, group_by = &quot;Species&quot;)
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c1b8155f3&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50
</code>

<code class="language-r">iris_tbl %&gt;%
  spark_apply(
    function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared,
    names = &quot;r.squared&quot;,
    group_by = &quot;Species&quot;)
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c30e6155&gt; [?? x 2]
## # Database: spark_connection
##      Species r.squared
##        &lt;chr&gt;     &lt;dbl&gt;
## 1 versicolor 0.6188467
## 2  virginica 0.1037537
## 3     setosa 0.1099785
</code>
<h2 id="distributing-packages">Distributing Packages</h2>
With <code>spark_apply()</code> you can use any R package inside Spark. 
For instance, you can use the <a href="https://cran.r-project.org/package=broom">broom</a> package to create a tidy data frame from linear regression output.

<code class="language-r">spark_apply(
  iris_tbl,
  function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)),
  names = c(&quot;term&quot;, &quot;estimate&quot;, &quot;std.error&quot;, &quot;statistic&quot;, &quot;p.value&quot;),
  group_by = &quot;Species&quot;)
</code>
<code> ## # Source:   table&lt;sparklyr_tmp_378c5502500b&gt; [?? x 6]
## # Database: spark_connection
##      Species        term  estimate std.error statistic      p.value
##        &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08
## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11
## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09
## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02
## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27
## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02
</code>

To use R packages inside Spark, your packages must be installed on the worker nodes. 
The first time you call <code>spark_apply</code> all of the contents in your local <code>.libPaths()</code> will be copied into each Spark worker node via the <code>SparkConf.addFile()</code> function. 
Packages will only be copied once and will persist as long as the connection remains open. 
It's not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. 
You can disable package distribution by setting <code>packages = FALSE</code>. 
Note: packages are not copied in local mode (<code>master=&quot;local&quot;</code>) because the packages already exist on the system.
<h2 id="handling-errors">Handling Errors</h2>
It can be more difficult to troubleshoot R issues in a cluster than in local mode. 
For instance, the following R code causes the distributed execution to fail and suggests you check the logs for details.

<code class="language-r">spark_apply(iris_tbl, function(e) stop(&quot;Make this fail&quot;))
</code>
<code>  Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details
</code>

In local mode, <code>sparklyr</code> will retrieve the logs for you. 
The logs point out the real failure as <code>ERROR sparklyr: RScript (4190) Make this fail</code> as you might expect.
<code> ---- Output Log ----
(17/07/27 21:24:18 ERROR sparklyr: Worker (2427) is shutting down with exception ,java.net.SocketException: Socket closed)
17/07/27 21:24:18 WARN TaskSetManager: Lost task 0.0 in stage 389.0 (TID 429, localhost, executor driver): 17/07/27 21:27:21 INFO sparklyr: RScript (4190) retrieved 150 rows 
17/07/27 21:27:21 INFO sparklyr: RScript (4190) computing closure 
17/07/27 21:27:21 ERROR sparklyr: RScript (4190) Make this fail 
</code>

It is worth mentioning that different cluster providers and platforms expose worker logs in different ways. 
Specific documentation for your environment will point out how to retrieve these logs.
<h2 id="requirements">Requirements</h2>
The <strong>R Runtime</strong> is expected to be pre-installed in the cluster for <code>spark_apply</code> to function. 
Failure to install the cluster will trigger a <code>Cannot run program, no such file or directory</code> error while attempting to use <code>spark_apply()</code>. 
Contact your cluster administrator to consider making the R runtime available throughout the entire cluster.

A <strong>Homogeneous Cluster</strong> is required since the driver node distributes, and potentially compiles, packages to the workers. 
For instance, the driver and workers must have the same processor architecture, system libraries, etc.
<h2 id="configuration">Configuration</h2>
The following table describes relevant parameters while making use of <code>spark_apply</code>.

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Value</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><code>spark.r.command</code></td>
<td>The path to the R binary. 
Useful to select from multiple R versions.</td></tr>
<tr class="even">
<td><code>sparklyr.worker.gateway.address</code></td>
<td>The gateway address to use under each worker node. 
Defaults to <code>sparklyr.gateway.address</code>.</td></tr>
<tr class="odd">
<td><code>sparklyr.worker.gateway.port</code></td>
<td>The gateway port to use under each worker node. 
Defaults to <code>sparklyr.gateway.port</code>.</td></tr>
</tbody>
</table>

For example, one could make use of an specific R version by running:

<code class="language-r">config &lt;- spark_config()
config[[&quot;spark.r.command&quot;]] &lt;- &quot;&lt;path-to-r-version&gt;&quot;

sc &lt;- spark_connect(master = &quot;local&quot;, config = config)
sdf_len(sc, 10) %&gt;% spark_apply(function(e) e)
</code>
<h2 id="limitations">Limitations</h2>
<h3 id="closures">Closures</h3>

Closures are serialized using <code>serialize</code>, which is described as &ldquo;A simple low-level interface for serializing to connections.&rdquo;. 
One of the current limitations of <code>serialize</code> is that it wont serialize objects being referenced outside of it's environment. 
For instance, the following function will error out since the closures references <code>external_value</code>:

<code class="language-r">external_value &lt;- 1
spark_apply(iris_tbl, function(e) e + external_value)
</code>

<h3 id="livy">Livy</h3>

Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, not operating systems that the cluster machines.

<h3 id="computing-over-groups">Computing over Groups</h3>

While performing computations over groups, <code>spark_apply()</code> will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. 
To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use <code>dplyr::do</code> which is currently optimized for large partitions.

<h3 id="package-installation">Package Installation</h3>

Since packages are copied only once for the duration of the <code>spark_connect()</code> connection, installing additional packages is not supported while the connection is active. 
Therefore, if a new package needs to be installed, <code>spark_disconnect()</code> the connection, modify packages and reconnect.
<h2>Data Science using a Data Lake </h2>
<h2>Audience</h2>
This article aims explain how to take advantage of Apache Spark inside organizations that have already implemented, or are in the process of implementing, a Hadoop based Big Data Lake.
<h2>Introduction</h2>
We have noticed that the types of questions we field after a demo of sparklyr to our customers were more about high-level architecture than how the package works. 
To answer those questions, we put together a set of slides that illustrate and discuss important concepts, to help customers see where Spark, R, and sparklyr fit in a Big Data Platform implementation. 
In this article, we’ll review those slides and provide a narrative that will help you better envision how you can take advantage of our products.
<h2>R for Data Science</h2>
It is very important to preface the Use Case review with some background information about where RStudio focuses its efforts when developing packages and products. 
Many vendors offer <em>R integration</em>, but in most cases, what this means is that they will add a model built in R to their pipeline or interface, and pass new inputs to that model to generate outputs that can be used in the next step in the pipeline, or in a calculation for the interface.

In contrast, our focus is on the process that happens before that: the discipline that produces the model, meaning <strong>Data Science</strong>.

<center>

<img src="images/deployment/data-lakes/slide-1.png" align='center'/>

</center>
In their <a href="http://r4ds.had.co.nz/">R for Data Science</a> book, Hadley Wickham and Garrett Grolemund provide a great diagram that nicely illustrates the Data Science process: We <strong>import</strong> data into memory with R and clean and <strong>tidy</strong> the data. 
Then we go into a cyclical process called <strong>understand</strong>, which helps us to get to know our data, and hopefully find the answer to the question we started with. 
This cycle typically involves making <strong>transformations</strong> to our tidied data, using the transformed data to fit <strong>models</strong>, and <strong>visualizing</strong> results. 
Once we find an answer to our question, we then <strong>communicate</strong> the results.

Data Scientists like using R because it allows them to complete a Data Science project from beginning to end inside the R environment, and in memory.
<h2>Hadoop as a Data Source</h2>
What happens when the data that needs to be analyzed is very large, like the data sets found in a Hadoop cluster? It would be impossible to fit these in memory, so workarounds are normally used. 
Possible workarounds include using a comparatively minuscule data sample, or download as much data as possible. 
This becomes disruptive to Data Scientists because either the small sample may not be representative, or they have to wait a long time in every iteration of importing a lot of data, exploring a lot of data, and modeling a lot of data.

<center>

<img src="images/deployment/data-lakes/slide-2.png" align='center'/>

</center>
<h2>Spark as an Analysis Engine</h2>
We noticed that a very important mental leap to make is to see Spark not just as a gateway to Hadoop (or worse, as an additional data source), but as a computing engine. 
As such, it is an excellent vehicle to scale our analytics. 
Spark has many capabilities that makes it ideal for Data Science in a data lake, such as close integration with Hadoop and Hive, the ability to cache data into memory across multiple nodes, data transformers, and its Machine Learning libraries.

The approach, then, is to <strong>push as much compute</strong> to the cluster as possible, using R primarily as an interface to Spark for the Data Scientist, which will then <strong>collect as few results</strong> as possible back into R memory, mostly to <strong>visualize</strong> and <strong>communicate</strong>. 
As shown in the slide, the more <strong>import</strong>, <strong>tidy</strong>, <strong>transform</strong> and <strong>modeling</strong> work we can push to Spark, the faster we can analyze very large data sets.

<center>

<img src="images/deployment/data-lakes/slide-3.png" align='center'/>

</center>
<h2>Cluster Setup</h2>
Here is an illustration of how R, RStudio, and sparklyr can be added to the YARN managed cluster. 
The highlights are:

R, RStudio, and sparklyr need to be installed on one node only, typically an edge node
The Data Scientist can access R, Spark, and the cluster via a web browser by navigating to the RStudio IDE inside the edge node
<center>

<img src="images/deployment/data-lakes/slide-4.png" align='center'/>

</center>
<h2>Considerations</h2>
There are some important considerations to keep in mind when combining your Data Lake and R for large scale analytics:

Spark’s Machine Learning libraries may not contain specific models that a Data Scientist needs. 
For those cases, workarounds would include using a sparklyr extension like <a href="http://spark.rstudio.com/extensions.html">H2O</a>, or collecting a sample of the data into R memory for modeling.

Spark does not have visualization functionality; currently, the best approach is to collect pre-calculated data into R for plotting. 
A good way to drastically reduce the number of rows being brought back into memory is to push as much computation as possible to Spark, and return just the results to be plotted. 
For example, the bins of a Histogram can be calculated in Spark, so that only the final bucket values would be returned to R for visualization. 
Here is sample code for such a scenario: <a href="https://github.com/rstudio/sparkDemos/blob/master/prod/presentations/cloudera/sqlvis_histogram.R">sparkDemos/Histogram</a>

A particular use case may require a different way of scaling analytics. 
We have published an article that provides a very good overview of the options that are available: <a href="https://www.rstudio.com/rviews/2016/12/21/r-for-enterprise-how-to-scale-your-analytics-using-r/">R for Enterprise: How to Scale Your Analytics Using R</a>
<h2>R for Data Science Toolchain with Spark</h2>
With sparklyr, the Data Scientist will be able to access the Data Lake’s data, and also gain an additional, very powerful <strong>understand</strong> layer via Spark. 
sparklyr, along with the <a href="https://www.rstudio.com/products/rstudio/">RStudio IDE</a> and the <a href="http://tidyverse.org/">tidyverse</a> packages, provides the Data Scientist with an excellent toolbox to analyze data, big and small.

<center>

<img src="images/deployment/data-lakes/slide-5.png" align='center'/>

</center>
<h2 id="spark-ml-pipelines">Spark ML Pipelines</h2>
Spark’s <strong>ML Pipelines</strong> provide a way to easily combine multiple
transformations and algorithms into a single workflow, or pipeline.

For R users, the insights gathered during the interactive sessions with
Spark can now be converted to a formal pipeline. 
This makes the hand-off
from Data Scientists to Big Data Engineers a lot easier, this is because
there should not be additional changes needed to be made by the later
group.

The final list of selected variables, data manipulation, feature
transformations and modeling can be easily re-written into a<code> ml_pipeline()</code> object, saved, and ultimately placed into a Production
environment. 
The <code>sparklyr</code> output of a saved Spark ML Pipeline object
is in Scala code, which means that the code can be added to the
scheduled Spark ML jobs, and without any dependencies in R.
<h2 id="introduction-to-ml-pipelines">Introduction to ML Pipelines</h2>
The official Apache Spark site contains a more complete overview of <a href="http://spark.apache.org/docs/latest/ml-pipeline.html">ML
Pipelines</a>. 
This
article will focus in introducing the basic concepts and steps to work
with ML Pipelines via <code>sparklyr</code>.

There are two important stages in building an ML Pipeline. 
The first one
is creating a <strong>Pipeline</strong>. 
A good way to look at it, or call it, is as
an <strong>“empty” pipeline</strong>. 
This step just builds the steps that the data
will go through. 
This is the somewhat equivalent of doing this in R:
<code> r_pipeline &lt;-  . 
%&gt;% mutate(cyl = paste0(&quot;c&quot;, cyl)) %&gt;% lm(am ~ cyl + mpg, data = .)
r_pipeline

## Functional sequence with the following components:
## 
##  1. 
mutate(., cyl = paste0(&quot;c&quot;, cyl))
##  2. 
lm(am ~ cyl + mpg, data = .)
## 
## Use 'functions' to extract the individual functions.
</code>

The <code>r_pipeline</code> object has all the steps needed to transform and fit
the model, but it has not yet transformed any data.

The second step, is to pass data through the pipeline, which in turn
will output a fitted model. 
That is called a <strong>PipelineModel</strong>. 
The
<strong>PipelineModel</strong> can then be used to produce predictions.
<code> r_model &lt;- r_pipeline(mtcars)
r_model

## 
## Call:
## lm(formula = am ~ cyl + mpg, data = .)
## 
## Coefficients:
## (Intercept)        cylc6        cylc8          mpg  
##    -0.54388      0.03124     -0.03313      0.04767
</code>

<h3 id="taking-advantage-of-pipelines-and-pipelinemodels">Taking advantage of Pipelines and PipelineModels</h3>

The two stage ML Pipeline approach produces two final data products:

A <strong>PipelineModel</strong> that can be added to the daily Spark jobs which
will produce new predictions for the incoming data, and again, with
no R dependencies.

A <strong>Pipeline</strong> that can be <strong>easily re-fitted</strong> on a regular
interval, say every month. 
All that is needed is to pass a new
sample to obtain the new coefficients.
<h2 id="pipeline">Pipeline</h2>
An additional goal of this article is that the reader can follow along,
so the data, transformations and Spark connection in this example will
be kept as easy to reproduce as possible.
<code> library(nycflights13)
library(sparklyr)
library(dplyr)
sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.2.0&quot;)

## * Using Spark: 2.2.0

spark_flights &lt;- sdf_copy_to(sc, flights)
</code>

<h3 id="feature-transformers">Feature Transformers</h3>

Pipelines make heavy use of <a href="http://spark.rstudio.com/reference/#section-spark-feature-transformers">Feature
Transformers</a>.
If new to Spark, and <code>sparklyr</code>, it would be good to review what these
transformers do. 
These functions use the Spark API directly to transform
the data, and may be faster at making the data manipulations that a<code> dplyr</code> (SQL) transformation.

In <code>sparklyr</code> the <code>ft</code> functions are essentially are wrappers to
original <a href="http://spark.apache.org/docs/latest/ml-features.html">Spark feature
transformer</a>.

<h3 id="ft-dplyr-transformer">ft_dplyr_transformer</h3>

This example will start with <code>dplyr</code> transformations, which are
ultimately SQL transformations, loaded into the <code>df</code> variable.

In <code>sparklyr</code>, there is one feature transformer that is not available in
Spark, <code>ft_dplyr_transformer()</code>. 
The goal of this function is to convert
the <code>dplyr</code> code to a SQL Feature Transformer that can then be used in a
Pipeline.
<code> df &lt;- spark_flights %&gt;%
  filter(!is.na(dep_delay)) %&gt;%
  mutate(
    month = paste0(&quot;m&quot;, month),
    day = paste0(&quot;d&quot;, day)
  ) %&gt;%
  select(dep_delay, sched_dep_time, month, day, distance) 
</code>

This is the resulting pipeline stage produced from the <code>dplyr</code> code:
<code> ft_dplyr_transformer(sc, df)
</code>

Use the <code>ml_param()</code> function to extract the “statement” attribute. 
That
attribute contains the finalized SQL statement. 
Notice that the<code> flights</code> table name has been replace with <code>__THIS__</code>. 
This allows the
pipeline to accept different table names as its source, making the
pipeline very modular.
<code> ft_dplyr_transformer(sc, df) %&gt;%
  ml_param(&quot;statement&quot;)

## [1] &quot;SELECT `dep_delay`, `sched_dep_time`, `month`, `day`, `distance`\nFROM (SELECT `year`, CONCAT(\&quot;m\&quot;, `month`) AS `month`, CONCAT(\&quot;d\&quot;, `day`) AS `day`, `dep_time`, `sched_dep_time`, `dep_delay`, `arr_time`, `sched_arr_time`, `arr_delay`, `carrier`, `flight`, `tailnum`, `origin`, `dest`, `air_time`, `distance`, `hour`, `minute`, `time_hour`\nFROM (SELECT *\nFROM `__THIS__`\nWHERE (NOT(((`dep_delay`) IS NULL)))) `bjbujfpqzq`) `axbwotqnbr`&quot;
</code>

<h3 id="creating-the-pipeline">Creating the Pipeline</h3>

The following step will create a 5 stage pipeline:

<ol>
SQL transformer - Resulting from the <code>ft_dplyr_transformer()</code>
transformation
Binarizer - To determine if the flight should be considered delay.
The eventual outcome variable.
Bucketizer - To split the day into specific hour buckets
R Formula - To define the model’s formula
Logistic Model
</ol>

<!-- -->
<code> flights_pipeline &lt;- ml_pipeline(sc) %&gt;%
  ft_dplyr_transformer(
    tbl = df
    ) %&gt;%
  ft_binarizer(
    input.col = &quot;dep_delay&quot;,
    output.col = &quot;delayed&quot;,
    threshold = 15
  ) %&gt;%
  ft_bucketizer(
    input.col = &quot;sched_dep_time&quot;,
    output.col = &quot;hours&quot;,
    splits = c(400, 800, 1200, 1600, 2000, 2400)
  )  %&gt;%
  ft_r_formula(delayed ~ month + day + hours + distance) %&gt;% 
  ml_logistic_regression()
</code>

Another nice feature for ML Pipelines in <code>sparklyr</code>, is the print-out.
It makes it really easy to how each stage is setup:
<code> flights_pipeline

## Pipeline (Estimator) with 5 stages
## &lt;pipeline_24044e4f2e21&gt; 
##   Stages 
##   |--1 SQLTransformer (Transformer)
##   |    &lt;dplyr_transformer_2404e6a1b8e&gt; 
##   |     (Parameters -- Column Names)
##   |--2 Binarizer (Transformer)
##   |    &lt;binarizer_24045c9227f2&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: dep_delay
##   |      output_col: delayed
##   |--3 Bucketizer (Transformer)
##   |    &lt;bucketizer_240412366b1e&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: sched_dep_time
##   |      output_col: hours
##   |--4 RFormula (Estimator)
##   |    &lt;r_formula_240442d75f00&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |     (Parameters)
##   |      force_index_label: FALSE
##   |      formula: delayed ~ month + day + hours + distance
##   |--5 LogisticRegression (Estimator)
##   |    &lt;logistic_regression_24044321ad0&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |      prediction_col: prediction
##   |      probability_col: probability
##   |      raw_prediction_col: rawPrediction
##   |     (Parameters)
##   |      aggregation_depth: 2
##   |      elastic_net_param: 0
##   |      family: auto
##   |      fit_intercept: TRUE
##   |      max_iter: 100
##   |      reg_param: 0
##   |      standardization: TRUE
##   |      threshold: 0.5
##   |      tol: 1e-06
</code>

Notice that there are no <em>coefficients</em> defined yet. 
That’s because no
data has been actually processed. 
Even though <code>df</code> uses<code> spark_flights()</code>, recall that the final SQL transformer makes that
name, so there’s no data to process yet.
<h2 id="pipelinemodel">PipelineModel</h2>
A quick partition of the data is created for this exercise.
<code> partitioned_flights &lt;- sdf_partition(
  spark_flights,
  training = 0.01,
  testing = 0.01,
  rest = 0.98
)
</code>

The <code>ml_fit()</code> function produces the PipelineModel. 
The <code>training</code>
partition of the <code>partitioned_flights</code> data is used to train the model:
<code> fitted_pipeline &lt;- ml_fit(
  flights_pipeline,
  partitioned_flights$training
)
fitted_pipeline

## PipelineModel (Transformer) with 5 stages
## &lt;pipeline_24044e4f2e21&gt; 
##   Stages 
##   |--1 SQLTransformer (Transformer)
##   |    &lt;dplyr_transformer_2404e6a1b8e&gt; 
##   |     (Parameters -- Column Names)
##   |--2 Binarizer (Transformer)
##   |    &lt;binarizer_24045c9227f2&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: dep_delay
##   |      output_col: delayed
##   |--3 Bucketizer (Transformer)
##   |    &lt;bucketizer_240412366b1e&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: sched_dep_time
##   |      output_col: hours
##   |--4 RFormulaModel (Transformer)
##   |    &lt;r_formula_240442d75f00&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |     (Transformer Info)
##   |      formula:  chr &quot;delayed ~ month + day + hours + distance&quot; 
##   |--5 LogisticRegressionModel (Transformer)
##   |    &lt;logistic_regression_24044321ad0&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |      prediction_col: prediction
##   |      probability_col: probability
##   |      raw_prediction_col: rawPrediction
##   |     (Transformer Info)
##   |      coefficient_matrix:  num [1, 1:43] 0.709 -0.3401 -0.0328 0.0543 -0.4774 ... 

##   |      coefficients:  num [1:43] 0.709 -0.3401 -0.0328 0.0543 -0.4774 ... 

##   |      intercept:  num -3.04 
##   |      intercept_vector:  num -3.04 
##   |      num_classes:  int 2 
##   |      num_features:  int 43 
##   |      threshold:  num 0.5
</code>

Notice that the print-out for the fitted pipeline now displays the
model’s coefficients.

The <code>ml_transform()</code> function can be used to run predictions, in other
words it is used instead of <code>predict()</code> or <code>sdf_predict()</code>.
<code> predictions &lt;- ml_transform(
  fitted_pipeline,
  partitioned_flights$testing
)

predictions %&gt;%
  group_by(delayed, prediction) %&gt;%
  tally()

## # Source:   lazy query [?? x 3]
## # Database: spark_connection
## # Groups:   delayed
##   delayed prediction     n
##     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;
## 1      0. 
1. 
  51.
## 2      0. 
0. 
2599.
## 3      1. 
0. 
 666.
## 4      1. 
1. 
  69.
</code>
<h2 id="save-the-pipelines-to-disk">Save the pipelines to disk</h2>
The <code>ml_save()</code> command can be used to save the Pipeline and
PipelineModel to disk. 
The resulting output is a folder with the
selected name, which contains all of the necessary Scala scripts:
<code> ml_save(
  flights_pipeline,
  &quot;flights_pipeline&quot;,
  overwrite = TRUE
)

## NULL

ml_save(
  fitted_pipeline,
  &quot;flights_model&quot;,
  overwrite = TRUE
)

## NULL
</code>
<h2 id="use-an-existing-pipelinemodel">Use an existing PipelineModel</h2>
The <code>ml_load()</code> command can be used to re-load Pipelines and
PipelineModels. 
The saved ML Pipeline files can only be loaded into an
open Spark session.
<code> reloaded_model &lt;- ml_load(sc, &quot;flights_model&quot;)
</code>

A simple query can be used as the table that will be used to make the
new predictions. 
This of course, does not have to done in R, at this
time the “flights_model” can be loaded into an independent Spark
session outside of R.
<code> new_df &lt;- spark_flights %&gt;%
  filter(
    month == 7,
    day == 5
  )

ml_transform(reloaded_model, new_df) 

## # Source:   table&lt;sparklyr_tmp_24041e052b5&gt; [?? x 12]
## # Database: spark_connection
##    dep_delay sched_dep_time month day   distance delayed hours features  
##        &lt;dbl&gt;          &lt;int&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;    
##  1       39. 
  2359 m7    d5       1617. 
     1. 
   4. 
&lt;dbl [43]&gt;
##  2      141. 
  2245 m7    d5       2475. 
     1. 
   4. 
&lt;dbl [43]&gt;
##  3        0. 
   500 m7    d5        529. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  4       -5. 
   536 m7    d5       1400. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  5       -2. 
   540 m7    d5       1089. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  6       -7. 
   545 m7    d5       1416. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  7       -3. 
   545 m7    d5       1576. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  8       -7. 
   600 m7    d5       1076. 
     0. 
   0. 
&lt;dbl [43]&gt;
##  9       -7. 
   600 m7    d5         96. 
     0. 
   0. 
&lt;dbl [43]&gt;
## 10       -6. 
   600 m7    d5        937. 
     0. 
   0. 
&lt;dbl [43]&gt;
## # ... 
with more rows, and 4 more variables: label &lt;dbl&gt;,
## #   rawPrediction &lt;list&gt;, probability &lt;list&gt;, prediction &lt;dbl&gt;
</code>
<h2 id="re-fit-an-existing-pipeline">Re-fit an existing Pipeline</h2>
First, reload the pipeline into an open Spark session:
<code> reloaded_pipeline &lt;- ml_load(sc, &quot;flights_pipeline&quot;)
</code>

Use <code>ml_fit()</code> again to pass new data, in this case, <code>sample_frac()</code> is
used instead of <code>sdf_partition()</code> to provide the new data. 
The idea
being that the re-fitting would happen at a later date than when the
model was initially fitted.
<code> new_model &lt;-  ml_fit(reloaded_pipeline, sample_frac(spark_flights, 0.01))

new_model

## PipelineModel (Transformer) with 5 stages
## &lt;pipeline_24044e4f2e21&gt; 
##   Stages 
##   |--1 SQLTransformer (Transformer)
##   |    &lt;dplyr_transformer_2404e6a1b8e&gt; 
##   |     (Parameters -- Column Names)
##   |--2 Binarizer (Transformer)
##   |    &lt;binarizer_24045c9227f2&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: dep_delay
##   |      output_col: delayed
##   |--3 Bucketizer (Transformer)
##   |    &lt;bucketizer_240412366b1e&gt; 
##   |     (Parameters -- Column Names)
##   |      input_col: sched_dep_time
##   |      output_col: hours
##   |--4 RFormulaModel (Transformer)
##   |    &lt;r_formula_240442d75f00&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |     (Transformer Info)
##   |      formula:  chr &quot;delayed ~ month + day + hours + distance&quot; 
##   |--5 LogisticRegressionModel (Transformer)
##   |    &lt;logistic_regression_24044321ad0&gt; 
##   |     (Parameters -- Column Names)
##   |      features_col: features
##   |      label_col: label
##   |      prediction_col: prediction
##   |      probability_col: probability
##   |      raw_prediction_col: rawPrediction
##   |     (Transformer Info)
##   |      coefficient_matrix:  num [1, 1:43] 0.258 0.648 -0.317 0.36 -0.279 ... 

##   |      coefficients:  num [1:43] 0.258 0.648 -0.317 0.36 -0.279 ... 

##   |      intercept:  num -3.77 
##   |      intercept_vector:  num -3.77 
##   |      num_classes:  int 2 
##   |      num_features:  int 43 
##   |      threshold:  num 0.5
</code>

The new model can be saved using <code>ml_save()</code>. 
A new name is used in this
case, but the same name as the existing PipelineModel to replace it.
<code> ml_save(new_model, &quot;new_flights_model&quot;, overwrite = TRUE)

## NULL
</code>

Finally, this example is complete by closing the Spark session.
<code> spark_disconnect(sc)
</code>
<h2 id="text-mining-with-spark-sparklyr">Text mining with Spark &amp; sparklyr</h2>
This article focuses on a set of functions that can be used for text mining with Spark and <code>sparklyr</code>. 
The main goal is to illustrate how to perform most of the data preparation and analysis with commands that will run inside the Spark cluster, as opposed to locally in R. 
Because of that, the amount of data used will be small.

<h3 id="data-source">Data source</h3>

For this example, there are two files that will be analyzed. 
They are both the full works of Sir Arthur Conan Doyle and Mark Twain. 
The files were downloaded from the <a href="https://www.gutenberg.org/">Gutenberg Project</a> site via the <code>gutenbergr</code> package. 
Intentionally, no data cleanup was done to the files prior to this analysis. 
See the appendix below to see how the data was downloaded and prepared.

<code class="language-r">readLines(&quot;arthur_doyle.txt&quot;, 10) 
</code>
<code> ##  [1] &quot;THE RETURN OF SHERLOCK HOLMES,&quot;   
##  [2] &quot;&quot;                                 
##  [3] &quot;A Collection of Holmes Adventures&quot;
##  [4] &quot;&quot;                                 
##  [5] &quot;&quot;                                 
##  [6] &quot;by Sir Arthur Conan Doyle&quot;        
##  [7] &quot;&quot;                                 
##  [8] &quot;&quot;                                 
##  [9] &quot;&quot;                                 
## [10] &quot;&quot;
</code>
<h2 id="data-import">Data Import</h2>
<h3 id="connect-to-spark">Connect to Spark</h3>

An additional goal of this article is to encourage the reader to try it out, so a simple Spark local mode session is used.

<code class="language-r">library(sparklyr)
library(dplyr)

sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.1.0&quot;)
</code>

<h3 id="spark-read-text">spark_read_text()</h3>

The <code>spark_read_text()</code> is a new function which works like <code>readLines()</code> but for <code>sparklyr</code>. 
It comes in handy when non-structured data, such as lines in a book, is what is available for analysis.

<code class="language-r"># Imports Mark Twain's file

# Setting up the path to the file in a Windows OS laptop
twain_path &lt;- paste0(&quot;file:///&quot;, getwd(), &quot;/mark_twain.txt&quot;)
twain &lt;-  spark_read_text(sc, &quot;twain&quot;, twain_path) 
</code>

<code class="language-r"># Imports Sir Arthur Conan Doyle's file
doyle_path &lt;- paste0(&quot;file:///&quot;, getwd(), &quot;/arthur_doyle.txt&quot;)
doyle &lt;-  spark_read_text(sc, &quot;doyle&quot;, doyle_path) 
</code>
<h2 id="data-transformation">Data transformation</h2>
The objective is to end up with a tidy table inside Spark with one row per word used. 
The steps will be:

<ol>
The needed data transformations apply to the data from both authors. 
The data sets will be appended to one another
Punctuation will be removed
The words inside each line will be separated, or tokenized
For a cleaner analysis, stop words will be removed
To tidy the data, each word in a line will become its own row
The results will be saved to Spark memory
</ol>

<h3 id="sdf-bind-rows">sdf_bind_rows()</h3>
<code> sdf_bind_rows()</code> appends the <code>doyle</code> Spark Dataframe to the <code>twain</code> Spark Dataframe. 
This function can be used in lieu of a <code>dplyr::bind_rows()</code> wrapper function. 
For this exercise, the column <code>author</code> is added to differentiate between the two bodies of work.

<code class="language-r">all_words &lt;- doyle %&gt;%
  mutate(author = &quot;doyle&quot;) %&gt;%
  sdf_bind_rows({
    twain %&gt;%
mutate(author = &quot;twain&quot;)}) %&gt;%
  filter(nchar(line) &gt; 0)
</code>

<h3 id="regexp-replace">regexp_replace</h3>

The Hive UDF, <strong>regexp_replace</strong>, is used as a sort of <code>gsub()</code> that works inside Spark. 
In this case it is used to remove punctuation. 
The usual <code>[:punct:]</code> regular expression did not work well during development, so a custom list is provided. 
For more information, see the <a href="https://spark.rstudio.com/articles/guides-dplyr.html#hive-functions">Hive Functions</a> section in the <code>dplyr</code> page.

<code class="language-r">all_words &lt;- all_words %&gt;%
  mutate(line = regexp_replace(line, &quot;[_\&quot;\'():;,.!?\\-]&quot;, &quot; &quot;)) 
</code>

<h3 id="ft-tokenizer">ft_tokenizer()</h3>
<code> ft_tokenizer()</code> uses the Spark API to separate each word. 
It creates a new list column with the results.

<code class="language-r">all_words &lt;- all_words %&gt;%
    ft_tokenizer(input.col = &quot;line&quot;,
 output.col = &quot;word_list&quot;)

head(all_words, 4)
</code>
<code> ## # Source:   lazy query [?? x 3]
## # Database: spark_connection
##   author                              line  word_list
##    &lt;chr&gt;                             &lt;chr&gt;     &lt;list&gt;
## 1  doyle    THE RETURN OF SHERLOCK HOLMES  &lt;list [5]&gt;
## 2  doyle A Collection of Holmes Adventures &lt;list [5]&gt;
## 3  doyle         by Sir Arthur Conan Doyle &lt;list [5]&gt;
## 4  doyle                         CONTENTS  &lt;list [1]&gt;
</code>

<h3 id="ft-stop-words-remover">ft_stop_words_remover()</h3>
<code> ft_stop_words_remover()</code> is a new function that, as its name suggests, takes care of removing stop words from the previous transformation. 
It expects a list column, so it is important to sequence it correctly after a <code>ft_tokenizer()</code> command. 
In the sample results, notice that the new <code>wo_stop_words</code> column contains less items than <code>word_list</code>.

<code class="language-r">all_words &lt;- all_words %&gt;%
  ft_stop_words_remover(input.col = &quot;word_list&quot;,
          output.col = &quot;wo_stop_words&quot;)

head(all_words, 4)
</code>
<code> ## # Source:   lazy query [?? x 4]
## # Database: spark_connection
##   author                              line  word_list wo_stop_words
##    &lt;chr&gt;                             &lt;chr&gt;     &lt;list&gt;        &lt;list&gt;
## 1  doyle    THE RETURN OF SHERLOCK HOLMES  &lt;list [5]&gt;    &lt;list [3]&gt;
## 2  doyle A Collection of Holmes Adventures &lt;list [5]&gt;    &lt;list [3]&gt;
## 3  doyle         by Sir Arthur Conan Doyle &lt;list [5]&gt;    &lt;list [4]&gt;
## 4  doyle                         CONTENTS  &lt;list [1]&gt;    &lt;list [1]&gt;
</code>

<h3 id="explode">explode</h3>

The Hive UDF <strong>explode</strong> performs the job of unnesting the tokens into their own row. 
Some further filtering and field selection is done to reduce the size of the dataset.

<code class="language-r">all_words &lt;- all_words %&gt;%
  mutate(word = explode(wo_stop_words)) %&gt;%
  select(word, author) %&gt;%
  filter(nchar(word) &gt; 2)
  
head(all_words, 4)
</code>
<code> ## # Source:   lazy query [?? x 2]
## # Database: spark_connection
##         word author
##        &lt;chr&gt;  &lt;chr&gt;
## 1     return  doyle
## 2   sherlock  doyle
## 3     holmes  doyle
## 4 collection  doyle
</code>

<h3 id="compute">compute()</h3>
<code> compute()</code> will operate this transformation and cache the results in Spark memory. 
It is a good idea to pass a name to <code>compute()</code> to make it easier to identify it inside the Spark environment. 
In this case the name will be <em>all_words</em>

<code class="language-r">all_words &lt;- all_words %&gt;%
  compute(&quot;all_words&quot;)
</code>

<h3 id="full-code">Full code</h3>

This is what the code would look like on an actual analysis:

<code class="language-r">all_words &lt;- doyle %&gt;%
  mutate(author = &quot;doyle&quot;) %&gt;%
  sdf_bind_rows({
    twain %&gt;%
mutate(author = &quot;twain&quot;)}) %&gt;%
  filter(nchar(line) &gt; 0) %&gt;%
  mutate(line = regexp_replace(line, &quot;[_\&quot;\'():;,.!?\\-]&quot;, &quot; &quot;)) %&gt;%
  ft_tokenizer(input.col = &quot;line&quot;,
 output.col = &quot;word_list&quot;) %&gt;%
  ft_stop_words_remover(input.col = &quot;word_list&quot;,
          output.col = &quot;wo_stop_words&quot;) %&gt;%
  mutate(word = explode(wo_stop_words)) %&gt;%
  select(word, author) %&gt;%
  filter(nchar(word) &gt; 2) %&gt;%
  compute(&quot;all_words&quot;)
</code>
<h2 id="data-analysis">Data Analysis</h2>
<h3 id="words-used-the-most">Words used the most</h3>

<code class="language-r">word_count &lt;- all_words %&gt;%
  group_by(author, word) %&gt;%
  tally() %&gt;%
  arrange(desc(n)) 
  
word_count
</code>
<code> ## # Source:     lazy query [?? x 3]
## # Database:   spark_connection
## # Groups:     author
## # Ordered by: desc(n)
##    author  word     n
##     &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;
##  1  twain   one 20028
##  2  doyle  upon 16482
##  3  twain would 15735
##  4  doyle   one 14534
##  5  doyle  said 13716
##  6  twain  said 13204
##  7  twain could 11301
##  8  doyle would 11300
##  9  twain  time 10502
## 10  doyle   man 10478
## # ... 
with more rows
</code>

<h3 id="words-used-by-doyle-and-not-twain">Words used by Doyle and not Twain</h3>

<code class="language-r">doyle_unique &lt;- filter(word_count, author == &quot;doyle&quot;) %&gt;%
  anti_join(filter(word_count, author == &quot;twain&quot;), by = &quot;word&quot;) %&gt;%
  arrange(desc(n)) %&gt;%
  compute(&quot;doyle_unique&quot;)

doyle_unique
</code>
<code> ## # Source:     lazy query [?? x 3]
## # Database:   spark_connection
## # Groups:     author
## # Ordered by: desc(n), desc(n)
##    author      word     n
##     &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;
##  1  doyle     nigel   972
##  2  doyle   alleyne   500
##  3  doyle      ezra   421
##  4  doyle     maude   337
##  5  doyle   aylward   336
##  6  doyle   catinat   301
##  7  doyle   sharkey   281
##  8  doyle  lestrade   280
##  9  doyle summerlee   248
## 10  doyle     congo   211
## # ... 
with more rows
</code>

<code class="language-r">doyle_unique %&gt;%
  head(100) %&gt;%
  collect() %&gt;%
  with(wordcloud::wordcloud(
    word, 
    n,
    colors = c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;,&quot;#56B4E9&quot;)))
</code>

<img src="guides-textmining_files/figure-markdown_github-ascii_identifiers/unnamed-chunk-14-1.png" alt="" />

<h3 id="twain-and-sherlock">Twain and Sherlock</h3>

The word cloud highlighted something interesting. 
The word <strong>lestrade</strong> is listed as one of the words used by Doyle but not Twain. 
Lestrade is the last name of a major character in the Sherlock Holmes books. 
It makes sense that the word &ldquo;sherlock&rdquo; appears considerably more times than &ldquo;lestrade&rdquo; in Doyle's books, so why is Sherlock not in the word cloud? Did Mark Twain use the word &ldquo;sherlock&rdquo; in his writings?

<code class="language-r">all_words %&gt;%
  filter(author == &quot;twain&quot;,
 word == &quot;sherlock&quot;) %&gt;%
  tally()
</code>
<code> ## # Source:   lazy query [?? x 1]
## # Database: spark_connection
##       n
##   &lt;dbl&gt;
## 1    16
</code>

The <code>all_words</code> table <strong>contains 16 instances</strong> of the word <strong>sherlock</strong> in the words used by Twain in his works. 
The <strong>instr</strong> Hive UDF is used to extract the lines that contain that word in the <code>twain</code> table. 
This Hive function works can be used instead of <code>base::grep()</code> or <code>stringr::str_detect()</code>. 
To account for any word capitalization, the <strong>lower</strong> command will be used in <code>mutate()</code> to make all words in the full text lower cap.

<h3 id="instr-lower">instr &amp; lower</h3>

Most of these lines are in a short story by Mark Twain called <a href="https://www.gutenberg.org/files/3180/3180-h/3180-h.htm#link2H_4_0008">A Double Barrelled Detective Story</a>. 
As per the <a href="https://en.wikipedia.org/wiki/A_Double_Barrelled_Detective_Story">Wikipedia</a> page about this story, this is a satire by Twain on the mystery novel genre, published in 1902.

<code class="language-r">twain %&gt;%
  mutate(line = lower(line)) %&gt;%
  filter(instr(line, &quot;sherlock&quot;) &gt; 0) %&gt;%
  pull(line)
</code>
<code> ##  [1] &quot;late sherlock holmes, and yet discernible by a member of a race charged&quot;  
##  [2] &quot;sherlock holmes.&quot;                                                         
##  [3] &quot;\&quot;uncle sherlock! the mean luck of it!--that he should come just&quot;         
##  [4] &quot;another trouble presented itself. 
\&quot;uncle sherlock 'll be wanting to talk&quot;
##  [5] &quot;flint buckner's cabin in the frosty gloom. 
they were sherlock holmes and&quot; 
##  [6] &quot;\&quot;uncle sherlock's got some work to do, gentlemen, that 'll keep him till&quot;
##  [7] &quot;\&quot;by george, he's just a duke, boys! three cheers for sherlock holmes,&quot;   
##  [8] &quot;he brought sherlock holmes to the billiard-room, which was jammed with&quot;   
##  [9] &quot;of interest was there--sherlock holmes. 
the miners stood silent and&quot;      
## [10] &quot;the room; the chair was on it; sherlock holmes, stately, imposing,&quot;       
## [11] &quot;\&quot;you have hunted me around the world, sherlock holmes, yet god is my&quot;    
## [12] &quot;\&quot;if it's only sherlock holmes that's troubling you, you needn't worry&quot;   
## [13] &quot;they sighed; then one said: \&quot;we must bring sherlock holmes. 
he can be&quot;   
## [14] &quot;i had small desire that sherlock holmes should hang for my deeds, as you&quot; 
## [15] &quot;\&quot;my name is sherlock holmes, and i have not been doing anything.\&quot;&quot;      
## [16] &quot;late sherlock holmes, and yet discernible by a member of a race charged&quot;
</code>

<code class="language-r">spark_disconnect(sc)
</code>
<h2 id="appendix">Appendix</h2>
<h3 id="gutenbergr-package">gutenbergr package</h3>

This is an example of how the data for this article was pulled from the Gutenberg site:

<code class="language-r">library(gutenbergr)

gutenberg_works()  %&gt;%
  filter(author == &quot;Twain, Mark&quot;) %&gt;%
  pull(gutenberg_id) %&gt;%
  gutenberg_download() %&gt;%
  pull(text) %&gt;%
  writeLines(&quot;mark_twain.txt&quot;)
</code>
<h2>Intro to Spark Streaming with sparklyr </h2>
<h2>The <code>sparklyr</code> interface</h2>
As stated in the Spark’s official site, <a href="https://spark.apache.org/streaming/">Spark Streaming</a> makes it easy to build scalable fault-tolerant streaming applications. 
Because is part of the Spark API, it is possible to re-use query code that queries the current state of the stream, as well as joining the streaming data with historical data. 
Please see <a href="https://spark.apache.org/docs/2.1.3/structured-streaming-programming-guide.html">Spark’s official documentation</a> for a deeper look into Spark Streaming.

The <code>sparklyr</code> interface provides the following:

Ability to run <a href="/dplyr">dplyr</a>, SQL, <a href="/guides/distributed-r/">spark_apply()</a>, and <a href="/guides/pipelines/#introduction-to-ml-pipelines">PipelineModels</a> against a stream
Read in multiple formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc
Write stream results to Spark memory and the following file formats: CSV, text, JSON, parquet, Kafka, JDBC, and orc
An out-of-the box <a href="#example-1---inputoutput">graph visualization</a> to monitor the stream
A new <a href="#example-4---shiny-integration">reactiveSpark()</a> function, that allows Shiny apps to poll the contents of the stream
create Shiny apps that are able to read the contents of the stream
<h2>Interacting with a stream</h2>
A good way of looking at the way how Spark streams update is as a three stage operation:

<ol style="list-style-type: decimal">
<strong>Input</strong> - Spark reads the data inside a given folder. 
The folder is expected to contain multiple data files, with new files being created containing the most current stream data.
<strong>Processing</strong> - Spark applies the desired operations on top of the data. 
These operations could be data manipulations (<code>dplyr</code>, SQL), data transformations (<code>sdf</code> operations, PipelineModel predictions), or native R manipulations (<code>spark_apply()</code>).
<strong>Output</strong> - The results of processing the input files are saved in a different folder.
</ol>
In the same way all of the read and write operations in <code>sparklyr</code> for Spark Standalone, or in <code>sparklyr</code>’s local mode, the input and output folders are actual OS file system folders. 
For Hadoop clusters, these will be folder locations inside the HDFS.
<h2>Example 1 - Input/Output</h2>
The first intro example is a small script that can be used with a local master. 
The result should be to see the <code>stream_view()</code> app showing live the number of records processed for each iteration of test data being sent to the stream.
<code> library(future)
library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 
write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)
invisible(future(stream_generate_test(interval = 0.5)))

stream_view(write_output)</code>

<center>

<img src="/guides/streaming/stream_view.png" align='center' width = 500/>

</center><code> stream_stop(write_output)
spark_disconnect(sc)</code>

<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
Open the Spark connection
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)</code>

Optional step. 
This resets the input and output folders. 
It makes it easier to run the code multiple times in a clean manner.
<code> if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)</code>

Produces a single test file inside the “source” folder. 
This allows the “read” function to infer CSV file definition.
<code> stream_generate_test(iterations = 1)
list.files(&quot;source&quot;)</code>
<code> [1] &quot;stream_1.csv&quot;</code>

Points the stream reader to the folder where the streaming files will be placed. 
Since it is primed with a single CSV file, it will use as the expected layout of subsequent files. 
By default, <code>stream_read_csv()</code> creates a single integer variable data frame.
<code> read_folder &lt;- stream_read_csv(sc, &quot;source&quot;)</code>

<strong>The output writer is what starts the streaming job</strong>. 
It will start monitoring the input folder, and then write the new results in the “source-out” folder. 
So as new records stream in, new files will be created in the “source-out” folder. 
Since there are no operations on the incoming data at this time, the output files will have the same exact raw data as the input files. 
The only difference is that the files and sub folders within “source-out” will be structured how Spark structures data folders.
<code> write_output &lt;- stream_write_csv(read_folder, &quot;source-out&quot;)
list.files(&quot;source-out&quot;)</code>
<code> [1] &quot;_spark_metadata&quot;                                     &quot;checkpoint&quot;
[3] &quot;part-00000-1f29719a-2314-40e1-b93d-a647a3d57154-c000.csv&quot;</code>

The test generation function will run 100 files every 0.2 seconds. 
To run the tests “out-of-sync” with the current R session, the <code>future</code> package is used.
<code> library(future)
invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))</code>

The <code>stream_view()</code> function can be used before the 50 tests are complete because of the use of the <code>future</code> package. 
It will monitor the status of the job that <code>write_output</code> is pointing to and provide information on the amount of data coming into the “source” folder and going out into the “source-out” folder.
<code> stream_view(write_output)</code>

The monitor will continue to run even after the tests are complete. 
To end the experiment, stop the Shiny app and then use the following to stop the stream and close the Spark session.
<code> stream_stop(write_output)
spark_disconnect(sc)</code>

</ol>
<h2>Example 2 - Processing</h2>
The second example builds on the first. 
It adds a processing step that manipulates the input data before saving it to the output folder. 
In this case, a new binary field is added indicating if the value from <code>x</code> is over 400 or not. 
This time, while run the second code chunk in this example a few times during the stream tests to see the aggregated values change.
<code> library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  mutate(x = as.double(x)) %&gt;%
  ft_binarizer(
    input_col = &quot;x&quot;,
    output_col = &quot;over&quot;,
    threshold = 400
  )

write_output &lt;- stream_write_csv(process_stream, &quot;source-out&quot;)
invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))</code>

Run this code a few times during the experiment:
<code> spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) %&gt;%
  group_by(over) %&gt;%
  tally()</code>

The results would look similar to this. 
The <code>n</code> totals will increase as the experiment progresses.
<code> # Source:   lazy query [?? x 2]
# Database: spark_connection
   over     n
  &lt;dbl&gt; &lt;dbl&gt;
1     0 40215
2     1 60006</code>

Clean up after the experiment
<code> stream_stop(write_output)
spark_disconnect(sc)</code>

<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
The processing starts with the <code>read_folder</code> variable that contains the input stream. 
It coerces the integer field <code>x</code>, into a type double. 
This is because the next function, <code>ft_binarizer()</code> does not accept integers. 
The binarizer determines if <code>x</code> is over 400 or not. 
This is a good illustration of how <code>dplyr</code> can help simplify the manipulation needed during the processing stage.
<code> process_stream &lt;- read_folder %&gt;%
  mutate(x = as.double(x)) %&gt;%
  ft_binarizer(
    input_col = &quot;x&quot;,
    output_col = &quot;over&quot;,
    threshold = 400
  )</code>

The output now needs to write-out the processed data instead of the raw input data. 
Swap <code>read_folder</code> with <code>process_stream</code>.
<code> write_output &lt;- stream_write_csv(process_stream, &quot;source-out&quot;)</code>

The “source-out” folder can be treated as a if it was a single table within Spark. 
Using <code>spark_read_csv()</code>, the data can be mapped, but not brought into memory (<code>memory = FALSE</code>). 
This allows the current results to be further analyzed using regular <code>dplyr</code> commands.
<code> spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) %&gt;%
  group_by(over) %&gt;%
  tally()</code>

</ol>
<h2>Example 3 - Aggregate in process and output to memory</h2>
Another option is to save the results of the processing into a in-memory Spark table. 
Unless intentionally saving it to disk, the table and its data will only exist while the Spark session is active.

The biggest advantage of using Spark memory as the target, is that it will allow for aggregation to happen during processing. 
This is an advantage because <em>aggregation is not allowed for any file output, expect Kafka, on the input/process stage</em>.

Using example 2 as the base, this example code will perform some aggregations to the current stream input and save only those summarized results into Spark memory:
<code> library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )

write_output &lt;- stream_write_memory(process_stream, name = &quot;stream&quot;)

invisible(future(stream_generate_test()))</code>

Run this command a different times while the experiment is running:
<code> tbl(sc, &quot;stream&quot;) </code>

Clean up after the experiment
<code> stream_stop(write_output)
spark_disconnect(sc)</code>

<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
The <code>stream_watermark()</code> functions add a new <code>timestamp</code> variable that is then used in the <code>group_by()</code> command. 
This is required by Spark Stream to accept summarized results as output of the stream. 
The second step is to simply decide what kinds of aggregations we need to perform. 
In this case, a simply max, min and count are performed.
<code> process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )</code>

The <code>spark_write_memory()</code> function is used to write the output to Spark memory. 
The results will appear as a table of the Spark session with the name assigned in the <code>name</code> argument, in this case the name selected is: “stream”.
<code> write_output &lt;- stream_write_memory(process_stream, name = &quot;stream&quot;)</code>

To query the current data in the “stream” table can be queried by using the <code>dplyr</code> <code>tbl()</code> command.
<code> tbl(sc, &quot;stream&quot;) </code>

</ol>
<h2>Example 4 - Shiny integration</h2><code> sparklyr</code> provides a new Shiny function called <code>reactiveSpark()</code>. 
It can take a Spark data frame, in this case the one created as a result of the stream processing, and then creates a Spark memory stream table, the same way a table is created in example 3.
<code> library(future)
library(sparklyr)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

stream_generate_test(iterations = 1)
read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) 

process_stream &lt;- read_folder %&gt;%
  stream_watermark() %&gt;%
  group_by(timestamp) %&gt;%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  )

invisible(future(stream_generate_test(interval = 0.2, iterations = 100)))

library(shiny)
ui &lt;- function(){
  tableOutput(&quot;table&quot;)
}
server &lt;- function(input, output, session){
  
  ps &lt;- reactiveSpark(process_stream)
  
  output$table &lt;- renderTable({
    ps() %&gt;%
mutate(timestamp = as.character(timestamp)) 
    })
}
runGadget(ui, server)</code>

<center>

<img src="/guides/streaming/shiny.png" align='center' width = 500/>

</center>

<h3>Code breakdown</h3>
<ol style="list-style-type: decimal">
Notice that there is no <code>stream_write_...</code> command. 
The reason is that <code>reactiveSpark()</code> function contains the <code>stream_write_memory()</code> function.

This very basic Shiny app simply displays the output of a table in the <code>ui</code> section
<code> library(shiny)

ui &lt;- function(){
  tableOutput(&quot;table&quot;)
}</code>

In the <code>server</code> section, the <code>reactiveSpark()</code> function will update every time there’s a change to the stream and return a data frame. 
The results are saved to a variable called <code>ps()</code> in this script. 
Treat the <code>ps()</code> variable as a regular table that can be piped from, as shown in the example. 
In this case, the <code>timestamp</code> variable is converted to string for to make it easier to read.
<code> server &lt;- function(input, output, session){

  ps &lt;- reactiveSpark(process_stream)

  output$table &lt;- renderTable({
    ps() %&gt;%
mutate(timestamp = as.character(timestamp)) 
  })
}</code>

Use <code>runGadget()</code> to display the Shiny app in the Viewer pane. 
This is optional, the app can be run using normal Shiny run functions.
<code> runGadget(ui, server)</code>

</ol>
<h2>Example 5 - ML Pipeline Model</h2>
This example uses a fitted <a href="/guides/pipelines/">Pipeline Model</a> to process the input, and saves the predictions to the output. 
This approach would be used to apply Machine Learning on top of streaming data.
<code> library(sparklyr)
library(dplyr, warn.conflicts = FALSE)

sc &lt;- spark_connect(master = &quot;local&quot;, spark_version = &quot;2.3.0&quot;)

if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE)
if(file.exists(&quot;source-out&quot;)) unlink(&quot;source-out&quot;, TRUE)

df &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))

stream_generate_test(df = df, iteration = 1)

model_sample &lt;- spark_read_csv(sc, &quot;sample&quot;, &quot;source&quot;)

pipeline &lt;- sc %&gt;%
  ml_pipeline() %&gt;%
  ft_r_formula(x ~ y) %&gt;%
  ml_linear_regression()

fitted_pipeline &lt;- ml_fit(pipeline, model_sample)

ml_stream &lt;- stream_read_csv(
    sc = sc, 
    path = &quot;source&quot;, 
    columns = c(x = &quot;integer&quot;, y = &quot;integer&quot;)
  )  %&gt;%
  ml_transform(fitted_pipeline, .)  %&gt;%
  select(- features) %&gt;%
  stream_write_csv(&quot;source-out&quot;)

stream_generate_test(df = df, interval = 0.5)</code>
<code> spark_read_csv(sc, &quot;stream&quot;, &quot;source-out&quot;, memory = FALSE) </code>
<code> ### Source: spark&lt;stream&gt; [?? x 4]
##       x     y label prediction
## * &lt;int&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;
## 1   276   277   276       276.
## 2   277   278   277       277.
## 3   278   279   278       278.
## 4   279   280   279       279.
## 5   280   281   280       280.
## 6   281   282   281       281.
## 7   282   283   282       282.
## 8   283   284   283       283.
## 9   284   285   284       284.
##10   285   286   285       285.
### ... 
with more rows</code>
<code> stream_stop(ml_stream)
spark_disconnect(sc)</code>

<h3>Code Breakdown</h3>
<ol style="list-style-type: decimal">
Creates and fits a pipeline
<code> df &lt;- data.frame(x = rep(1:1000), y = rep(2:1001))
stream_generate_test(df = df, iteration = 1)
model_sample &lt;- spark_read_csv(sc, &quot;sample&quot;, &quot;source&quot;)

pipeline &lt;- sc %&gt;%
  ml_pipeline() %&gt;%
  ft_r_formula(x ~ y) %&gt;%
  ml_linear_regression()

fitted_pipeline &lt;- ml_fit(pipeline, model_sample)</code>

This example pipelines the input, process and output in a single code segment. 
The <code>ml_transform()</code> function is used to create the predictions. 
Because the CSV format does not support <em>list</em> type fields, the <code>features</code> column is removed before the results are sent to the output.
<code> ml_stream &lt;- stream_read_csv(
    sc = sc, 
    path = &quot;source&quot;, 
    columns = c(x = &quot;integer&quot;, y = &quot;integer&quot;)
  )  %&gt;%
  ml_transform(fitted_pipeline, .)  %&gt;%
  select(- features) %&gt;%
  stream_write_csv(&quot;source-out&quot;)</code>

</ol>
<h2>Using Spark with AWS S3 buckets </h2>
<h2>AWS Access Keys</h2>
AWS Access Keys are needed to access S3 data. 
To learn how to setup a new keys, please review the AWS documentation: <a href="http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html" class="uri">http://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html</a> .We then pass the keys to R via Environment Variables:
<code> Sys.setenv(AWS_ACCESS_KEY_ID=&quot;[Your access key]&quot;)
Sys.setenv(AWS_SECRET_ACCESS_KEY=&quot;[Your secret access key]&quot;)</code>
<h2>Connecting to Spark</h2>
There are four key settings needed to connect to Spark and use S3:

A Hadoop-AWS package
Executor memory (key but not critical)
The master URL
The Spark Home
To connect to Spark, we first need to initialize a variable with the contents of sparklyr default config (<code>spark_config</code>) which we will then customize for our needs
<code> library(sparklyr)

conf &lt;- spark_config()</code>

<h3>Hadoop-AWS package:</h3>
A Spark connection can be enhanced by using packages, please note that these are not R packages. 
For example, there are packages that tells Spark how to read CSV files, Hadoop or Hadoop in AWS.

In order to read S3 buckets, our Spark connection will need a package called <code>hadoop-aws</code>. 
If needed, multiple packages can be used. 
We experimented with many combinations of packages, and determined that for reading data in S3 we only need the one. 
The version we used, 2.7.3, refers to the latest Hadoop version, so as this article ages, please make sure to check this site to ensure that you are using the latest version: <a href="https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws" class="uri">https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws</a>
<code> conf$sparklyr.defaultPackages &lt;- &quot;org.apache.hadoop:hadoop-aws:2.7.3&quot;</code>

<h3>Executor Memory</h3>
As mentioned above this setting <em>key but not critical</em>. 
There are two points worth highlighting about it is:

The only performance related setting in a Spark Stand Alone cluster that can be tweaked, and in most cases because Spark defaults to a fraction of what is available, we then need to increase it by manually passing a value to that setting.

If more than the available RAM is requested, then Spark will set the Cores to 0, thus rendering the session unusable.
<code> conf$spark.executor.memory &lt;- &quot;14g&quot;</code>

<h3>Master URL and Spark home</h3>
There are three important points to mention when executing the spark_connect command:

<ol style="list-style-type: decimal">
The master will be the Spark Master’s URL. 
To find the URL, please see the Spark Cluster section.
Point the Spark Home to the location where Spark was installed in this node
Make sure to the conf variable as the value for the config argument
</ol><code> sc &lt;- spark_connect(master = &quot;spark://ip-172-30-1-5.us-west-2.compute.internal:7077&quot;, 
      spark_home = &quot;/home/ubuntu/spark-2.1.0-bin-hadoop2.7/&quot;,
      config =  conf)</code>
<h2>Data Import/Wrangle approach</h2>
We experimented with multiple approaches. 
Most of the factors for settling on a recommended approach were made based on the speed of each step. 
The premise is that we would rather wait longer during Data Import, if it meant that we can much faster Register and Cache our data subsets during Data Wrangling, especially since we would expect to end up with many subsets as we explore and model.The selected combination was the second slowest during the Import stage, but the fastest when caching a subset, by a lot.

In our tests, it took 72 seconds to read and cache the 29 columns of the 41 million rows of data, the slowest was 77 seconds. 
But when it comes to registering and caching a considerably sizable subset of 3 columns and almost all of the 41 million records, this approach was 17X faster than the second fastest approach. 
It took 1/3 of a second to register and cache the subset, the second fastest was 5 seconds.

To implement this approach, we need to set three arguments in the <code>spark_csv_read()</code> step:
<code> memory</code><code> infer_schema</code><code> columns</code>
Again, this is a recommended approach. 
The columns argument is needed only if infer_schema is set to <code>FALSE.</code> When memory is set to <code>TRUE</code> it makes Spark load the entire dataset into memory, and setting infer_schema to FALSE prevents Spark from trying to figure out what the schema of the files are. 
By trying different combinations the memory and infer_schema arguments you may be able to find an approach that may better fits your needs.

<h3>Reading the schema</h3>
Surprisingly, another critical detail that can easily be overlooked is choosing the right <strong>s3 URI scheme</strong>. 
There are two options: <strong>s3n</strong> and <strong>s3a</strong>. 
In most examples and tutorials I found, there was no reason give of why or when to use which one. 
The article the finally clarified it was this one: <a href="https://wiki.apache.org/hadoop/AmazonS3" class="uri">https://wiki.apache.org/hadoop/AmazonS3</a>

The gist of it is that <strong>s3a</strong> is the recommended one going forward, especially for Hadoop versions 2.7 and above. 
This means that if we copy from older examples that used <em>Hadoop 2.6 we would more likely also used s3n</em> thus making data import much, much slower.
<h2>Data Import</h2>
After the long introduction in the previous section, there is only one point to add about the following code chunk. 
If there are any <code>NA</code> values in numeric fields, then define the column as character and then convert it on later subsets using dplyr. 
The data import will fail if it finds any NA values on numeric fields. 
This is a small trade off in this approach because the next fastest one does not have this issue but is 17X slower at caching subsets.
<code> flights &lt;- spark_read_csv(sc, &quot;flights_spark&quot;, 
            path =  &quot;s3a://flights-data/full&quot;, 
            memory = TRUE, 
            columns = list(
              Year = &quot;character&quot;,
              Month = &quot;character&quot;,
              DayofMonth = &quot;character&quot;,
              DayOfWeek = &quot;character&quot;,
              DepTime = &quot;character&quot;,
              CRSDepTime = &quot;character&quot;,
              ArrTime = &quot;character&quot;,
              CRSArrTime = &quot;character&quot;,
              UniqueCarrier = &quot;character&quot;,
              FlightNum = &quot;character&quot;,
              TailNum = &quot;character&quot;,
              ActualElapsedTime = &quot;character&quot;,
              CRSElapsedTime = &quot;character&quot;,
              AirTime = &quot;character&quot;,
              ArrDelay = &quot;character&quot;,
              DepDelay = &quot;character&quot;,
              Origin = &quot;character&quot;,
              Dest = &quot;character&quot;,
              Distance = &quot;character&quot;,
              TaxiIn = &quot;character&quot;,
              TaxiOut = &quot;character&quot;,
              Cancelled = &quot;character&quot;,
              CancellationCode = &quot;character&quot;,
              Diverted = &quot;character&quot;,
              CarrierDelay = &quot;character&quot;,
              WeatherDelay = &quot;character&quot;,
              NASDelay = &quot;character&quot;,
              SecurityDelay = &quot;character&quot;,
              LateAircraftDelay = &quot;character&quot;), 
           infer_schema = FALSE)</code>
<h2>Data Wrangle</h2>
There are a few points we need to highlight about the following simple dyplr code:

Because there were NAs in the original fields, we have to mutate them to a number. 
Try coercing any variable as integer instead of numeric, this will save a lot of space when cached to Spark memory. 
The sdf_register command can be piped at the end of the code. 
After running the code, a new table will appear in the RStudio IDE’s Spark tab
<code> tidy_flights &lt;- tbl(sc, &quot;flights_spark&quot;) %&gt;%
  mutate(ArrDelay = as.integer(ArrDelay),
 DepDelay = as.integer(DepDelay),
 Distance = as.integer(Distance)) %&gt;%
  filter(!is.na(ArrDelay)) %&gt;%
  select(DepDelay, ArrDelay, Distance) %&gt;%
  sdf_register(&quot;tidy_spark&quot;)</code>

After we use <code>tbl_cache()</code> to load the <code>tidy_spark</code> table into Spark memory. 
We can see the new table in the Storage page of our Spark session.
<code> tbl_cache(sc, &quot;tidy_spark&quot;)</code>
<h2>Using Apache Arrow </h2>
<h2>Introduction</h2>
<a href="https://arrow.apache.org/">Apache Arrow</a> is a cross-language development platform for in-memory data. 
Arrow is supported starting with <code>sparklyr 1.0.0</code> to improve performance when transferring data between Spark and R. 
You can find some performance benchmarks under:

<a href="https://blog.rstudio.com/2019/03/04/sparklyr-1-0/">sparklyr 1.0: Arrow, XGBoost, Broom and TFRecords</a>.
<a href="https://arrow.apache.org/blog/2019/01/25/r-spark-improvements/">Speeding up R and Apache Spark using Apache Arrow</a>.
<h2>Installation</h2>
Using Arrow from R requires installing:

<strong>The Arrow Runtime</strong>: Provides a cross-language runtime library.
<strong>The Arrow R Package</strong>: Provides support for using Arrow from R through an R package.

<h3>Runtime</h3>
<h4>OS X</h4>
Installing from OS X requires <a href="https://brew.sh/">Homebrew</a> and executing from a terminal:
<code> brew install apache-arrow</code>
<h4>Windows</h4>
Currently, installing Arrow in Windows requires <a href="https://conda.io/en/latest/">Conda</a> and executing from a terminal:
<code> conda install arrow-cpp=0.12.* -c conda-forge
conda install pyarrow=0.12.* -c conda-forge</code>
<h4>Linux</h4>
Please reference <a href="https://arrow.apache.org/install/">arrow.apache.org/install</a> when installing Arrow for Linux.

<h3>Package</h3>
As of this writing, the <code>arrow</code> R package is not yet available in CRAN; however, this package can be installed using the <code>remotes</code> package. 
First, install <code>remotes</code>:
<code> install.packages(&quot;remotes&quot;)</code>

Then install the R package from github as follows:
<code> remotes::install_github(&quot;apache/arrow&quot;, subdir = &quot;r&quot;, ref = &quot;apache-arrow-0.12.0&quot;)</code>

If you happen to have Arrow 0.11 installed, you will have to install
<code> remotes::install_github(&quot;apache/arrow&quot;, subdir = &quot;r&quot;, ref = &quot;dc5df8f&quot;)</code>
<h2>Use Cases</h2>
There are three main use cases for <code>arrow</code> in <code>sparklyr</code>:

<strong>Data Copying</strong>: When copying data with <code>copy_to()</code>, Arrow will be used.
<strong>Data Collection</strong>: Also, when collecting either, implicitly by printing datasets or explicitly calling <code>collect</code>.
<strong>R Transformations</strong>: When using <code>spark_apply()</code>, data will be transferred using Arrow when possible.
To use <code>arrow</code> in <code>sparklyr</code> one simply needs to import this library:
<code> library(arrow)</code>
<code> Attaching package: ‘arrow’

The following object is masked from ‘package:utils’:
timestamp

The following objects are masked from ‘package:base’:
array, table</code>
<h2>Considerations</h2>
<h3>Types</h3>
Some data types are mapped to slightly different, one can argue more correct, types when using Arrow. 
For instance, consider collecting 64 bit integers in <code>sparklyr</code>:
<code> library(sparklyr)

sc &lt;- spark_connect(master = &quot;local&quot;)
integer64 &lt;- sdf_len(sc, 2, type = &quot;integer64&quot;)
integer64</code>
<code> # Source: spark&lt;?&gt; [?? x 1]
     id
  &lt;dbl&gt;
1     1
2     2</code>

Notice that <code>sparklyr</code> collects 64 bit integers as <code>double</code>; however, using <code>arrow</code>:
<code> library(arrow)
integer64</code>
<code> # Source: spark&lt;?&gt; [?? x 1]
  id             
  &lt;S3: integer64&gt;
1 1              
2 2 </code>

64 bit integers are now being collected as proper 64 bit integer using the <code>bit64</code> package.

<h3>Fallback</h3>
The Arrow R package supports many data types; however, in cases where a type is unsupported, <code>sparklyr</code> will fallback to not using arrow and print a warning.
<code> library(sparklyr.nested)
library(sparklyr)
library(dplyr)
library(arrow)

sc &lt;- spark_connect(master = &quot;local&quot;)
cars &lt;- copy_to(sc, mtcars)

sdf_nest(cars, hp) %&gt;%
  group_by(cyl) %&gt;%
  summarize(data = collect_list(data))</code>
<code> # Source: spark&lt;?&gt; [?? x 2]
    cyl data       
  &lt;dbl&gt; &lt;list&gt;     
1     6 &lt;list [7]&gt; 
2     4 &lt;list [11]&gt;
3     8 &lt;list [14]&gt;
Warning message:
In arrow_enabled_object.spark_jobj(sdf) :
  Arrow disabled due to columns: data</code>
<h2>Creating Extensions for sparklyr </h2>
<h2>Introduction</h2>
The sparklyr package provides a <a href="dplyr.html">dplyr</a> interface to Spark DataFrames as well as an R interface to Spark’s distributed <a href="mllib.html">machine learning</a> pipelines. 
However, since Spark is a general-purpose cluster computing system there are many other R interfaces that could be built (e.g. interfaces to custom machine learning pipelines, interfaces to 3rd party Spark packages, etc.).

The facilities used internally by sparklyr for its dplyr and machine learning interfaces are available to extension packages. 
This guide describes how you can use these tools to create your own custom R interfaces to Spark.

<h3>Examples</h3>
Here’s an example of an extension function that calls the text file line counting function available via the SparkContext:
<code> library(sparklyr)
count_lines &lt;- function(sc, file) {
  spark_context(sc) %&gt;% 
    invoke(&quot;textFile&quot;, file, 1L) %&gt;% 
    invoke(&quot;count&quot;)
}</code>

The <code>count_lines</code> function takes a <code>spark_connection</code> (<code>sc</code>) argument which enables it to obtain a reference to the <code>SparkContext</code> object, and in turn call the <code>textFile().count()</code> method.

You can use this function with an existing sparklyr connection as follows:
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;)
count_lines(sc, &quot;hdfs://path/data.csv&quot;)</code>

Here are links to some additional examples of extension packages:

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Package</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="https://github.com/bnosac/spark.sas7bdat"><code>spark.sas7bdat</code></a></td>
<td>Read in SAS data in parallel into Apache Spark.</td></tr>
<tr class="even">
<td><a href="https://github.com/h2oai/rsparkling"><code>rsparkling</code></a></td>
<td>Extension for using <a href="h2o.ai">H2O</a> machine learning algorithms against Spark Data Frames.</td></tr>
<tr class="odd">
<td><a href="https://github.com/javierluraschi/sparkhello"><code>sparkhello</code></a></td>
<td>Simple example of including a custom JAR file within an extension package.</td></tr>
<tr class="even">
<td><a href="https://github.com/clarkfitzg/rddlist"><code>rddlist</code></a></td>
<td>Implements some methods of an R list as a Spark RDD (resilient distributed dataset).</td></tr>
<tr class="odd">
<td><a href="https://github.com/javierluraschi/sparkwarc"><code>sparkwarc</code></a></td>
<td>Load <a href="http://commoncrawl.org/the-data/get-started/">WARC files</a> into Apache Spark with sparklyr.</td></tr>
<tr class="even">
<td><a href="https://github.com/chezou/sparkavro"><code>sparkavro</code></a></td>
<td>Load Avro data into Spark with sparklyr. 
It is a wrapper of <a href="https://github.com/databricks/spark-avro">spark-avro</a></td></tr>
<tr class="odd">
<td><a href="https://github.com/AkhilNairAmey/crassy"><code>crassy</code></a></td>
<td>Connect to Cassandra with sparklyr using the <code>Spark-Cassandra-Connector</code>.</td></tr>
<tr class="even">
<td><a href="https://github.com/kevinykuo/sparklygraphs"><code>sparklygraphs</code></a></td>
<td>R interface for <a href="https://graphframes.github.io/">GraphFrames</a> which aims to provide the functionality of <a href="http://spark.apache.org/graphx/">GraphX</a>.</td></tr>
<tr class="odd">
<td><a href="https://mitre.github.io/sparklyr.nested/"><code>sparklyr.nested</code></a></td>
<td>Extension for working with nested data.</td></tr>
<tr class="even">
<td><a href="https://github.com/javierluraschi/sparklyudf"><code>sparklyudf</code></a></td>
<td>Simple example registering an Scala UDF within an extension package.</td></tr>
</tbody>
</table>
<h2>Core Types</h2>
Three classes are defined for representing the fundamental types of the R to Java bridge:

<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="/reference/spark-connections/"><code>spark_connection</code></a></td>
<td>Connection between R and the Spark shell process</td></tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_jobj.html"><code>spark_jobj</code></a></td>
<td>Instance of a remote Spark object</td></tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dataframe.html"><code>spark_dataframe</code></a></td>
<td>Instance of a remote Spark DataFrame object</td></tr>
</tbody>
</table>
S3 methods are defined for each of these classes so they can be easily converted to or from objects that contain or wrap them. 
Note that for any given <code>spark_jobj</code> it’s possible to discover the underlying <code>spark_connection</code>.
<h2>Calling Spark from R</h2>
There are several functions available for calling the methods of Java objects and static methods of Java classes:

<table>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke</code></a></td>
<td>Call a method on an object</td></tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke_new</code></a></td>
<td>Create a new object by invoking a constructor</td></tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/invoke.html"><code>invoke_static</code></a></td>
<td>Call a static method on an object</td></tr>
</tbody>
</table>
For example, to create a new instance of the <code>java.math.BigInteger</code> class and then call the <code>longValue()</code> method on it you would use code like this:
<code> billionBigInteger &lt;- invoke_new(sc, &quot;java.math.BigInteger&quot;, &quot;1000000000&quot;)
billion &lt;- invoke(billionBigInteger, &quot;longValue&quot;)</code>

Note the <code>sc</code> argument: that’s the <code>spark_connection</code> object which is provided by the front-end package (e.g. sparklyr).

The previous example can be re-written to be more compact and clear using <a href="https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html">magrittr</a> pipes:
<code> billion &lt;- sc %&gt;% 
  invoke_new(&quot;java.math.BigInteger&quot;, &quot;1000000000&quot;) %&gt;%
    invoke(&quot;longValue&quot;)</code>

This syntax is similar to the method-chaining syntax often used with Scala code so is generally preferred.

Calling a static method of a class is also straightforward. 
For example, to call the <code>Math::hypot()</code> static function you would use this code:
<code> hypot &lt;- sc %&gt;% 
  invoke_static(&quot;java.lang.Math&quot;, &quot;hypot&quot;, 10, 20) </code>
<h2>Wrapper Functions</h2>
Creating an extension typically consists of writing R wrapper functions for a set of Spark services. 
In this section we’ll describe the typical form of these functions as well as how to handle special types like Spark DataFrames.

Here’s the wrapper function for <code>textFile().count()</code> which we defined earlier:
<code> count_lines &lt;- function(sc, file) {
  spark_context(sc) %&gt;% 
    invoke(&quot;textFile&quot;, file, 1L) %&gt;% 
invoke(&quot;count&quot;)
}</code>

The <code>count_lines</code> function takes a <code>spark_connection</code> (<code>sc</code>) argument which enables it to obtain a reference to the <code>SparkContext</code> object, and in turn call the <code>textFile().count()</code> method.

The following functions are useful for implementing wrapper functions of various kinds:

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="/reference/spark-connections/"><code>spark_connection</code></a></td>
<td>Get the Spark connection associated with an object (S3)</td></tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_jobj.html"><code>spark_jobj</code></a></td>
<td>Get the Spark jobj associated with an object (S3)</td></tr>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dataframe.html"><code>spark_dataframe</code></a></td>
<td>Get the Spark DataFrame associated with an object (S3)</td></tr>
<tr class="even">
<td><a href="/reference/spark-api/"><code>spark_context</code></a></td>
<td>Get the SparkContext for a <code>spark_connection</code></td></tr>
<tr class="odd">
<td><a href="/reference/spark-api/"><code>hive_context</code></a></td>
<td>Get the HiveContext for a <code>spark_connection</code></td></tr>
<tr class="even">
<td><a href="reference/sparklyr/latest/spark_version.html"><code>spark_version</code></a></td>
<td>Get the version of Spark (as a <code>numeric_version</code>) for a <code>spark_connection</code></td></tr>
</tbody>
</table>
The use of these functions is illustrated in this simple example:
<code> analyze &lt;- function(x, features) {
  
  # normalize whatever we were passed (e.g. 
a dplyr tbl) into a DataFrame
  df &lt;- spark_dataframe(x)
  
  # get the underlying connection so we can create new objects
  sc &lt;- spark_connection(df)
  
  # create an object to do the analysis and call its `analyze` and `summary`
  # methods (note that the df and features are passed to the analyze function)
  summary &lt;- sc %&gt;%  
    invoke_new(&quot;com.example.tools.Analyzer&quot;) %&gt;% 
invoke(&quot;analyze&quot;, df, features) %&gt;% 
invoke(&quot;summary&quot;)

  # return the results
  summary
}</code>

The first argument is an object that can be accessed using the Spark DataFrame API (this might be an actual reference to a DataFrame or could rather be a dplyr <code>tbl</code> which has a DataFrame reference inside it).

After using the <code>spark_dataframe</code> function to normalize the reference, we extract the underlying Spark connection associated with the data frame using the <code>spark_connection</code> function. 
Finally, we create a new <code>Analyzer</code> object, call it’s <code>analyze</code> method with the DataFrame and list of features, and then call the <code>summary</code> method on the results of the analysis.

Accepting a <code>spark_jobj</code> or <code>spark_dataframe</code> as the first argument of a function makes it very easy to incorporate into magrittr pipelines so this pattern is highly recommended when possible.
<h2>Dependencies</h2>
When creating R packages which implement interfaces to Spark you may need to include additional dependencies. 
Your dependencies might be a set of <a href="https://spark-packages.org/">Spark Packages</a> or might be a custom JAR file. 
In either case, you’ll need a way to specify that these dependencies should be included during the initialization of a Spark session. 
A Spark dependency is defined using the <code>spark_dependency</code> function:

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>Function</th>
<th>Description</th></tr>
</thead>
<tbody>
<tr class="odd">
<td><a href="reference/sparklyr/latest/spark_dependency"><code>spark_dependency</code></a></td>
<td>Define a Spark dependency consisting of JAR files and Spark packages</td></tr>
</tbody>
</table>
Your extension package can specify it’s dependencies by implementing a function named <code>spark_dependencies</code> within the package (this function should <em>not</em> be publicly exported). 
For example, let’s say you were creating an extension package named <strong>sparkds</strong> that needs to include a custom JAR as well as the Redshift and Apache Avro packages:
<code> spark_dependencies &lt;- function(spark_version, scala_version, ...) {
  spark_dependency(
    jars = c(
system.file(
sprintf(&quot;java/sparkds-%s-%s.jar&quot;, spark_version, scala_version), 
package = &quot;sparkds&quot;
)
    ),
    packages = c(
sprintf(&quot;com.databricks:spark-redshift_%s:0.6.0&quot;, scala_version),
sprintf(&quot;com.databricks:spark-avro_%s:2.0.1&quot;, scala_version)
    )
  )
}

.onLoad &lt;- function(libname, pkgname) {
  sparklyr::register_extension(pkgname)
}</code>

The <code>spark_version</code> argument is provided so that a package can support multiple Spark versions for it’s JARs. 
Note that the argument will include just the major and minor versions (e.g. <code> 1.6</code> or <code>2.0</code>) and will not include the patch level (as JARs built for a given major/minor version are expected to work for all patch levels).

The <code>scala_version</code> argument is provided so that a single package can support multiple Scala compiler versions for it’s JARs and packages (currently Scala 1.6 downloadable binaries are compiled with Scala 2.10 and Scala 2.0 downloadable binaries are compiled with Scala 2.11).

The <code>...</code> argument is unused but nevertheless should be included to ensure compatibility if new arguments are added to <code>spark_dependencies</code> in the future.

The <code>.onLoad</code> function registers your extension package so that it’s <code>spark_dependencies</code> function will be automatically called when new connections to Spark are made via <code>spark_connect</code>:
<code> library(sparklyr)
library(sparkds)
sc &lt;- spark_connect(master = &quot;local&quot;)</code>

<h3>Compiling JARs</h3>
The <strong>sparklyr</strong> package includes a utility function (<code>compile_package_jars</code>) that will automatically compile a JAR file from your Scala source code for the required permutations of Spark and Scala compiler versions. 
To use the function just invoke it from the root directory of your R package as follows:
<code> sparklyr::compile_package_jars()</code>

Note that a prerequisite to calling <code>compile_package_jars</code> is the installation of the Scala 2.10 and 2.11 compilers to one of the following paths:

/opt/scala
/opt/local/scala
/usr/local/scala
~/scala (Windows-only)
See the <a href="https://github.com/jjallaire/sparkhello">sparkhello</a> repository for a complete example of including a custom JAR within an extension package.
<h4>CRAN</h4>
When including a JAR file within an R package distributed on CRAN, you should follow the guidelines provided in <a href="https://cran.r-project.org/doc/manuals/r-release/R-exts.html#Non_002dR-scripts-in-packages">Writing R Extensions</a>:

<blockquote>
Java code is a special case: except for very small programs, .java files should be byte-compiled (to a .class file) and distributed as part of a .jar file: the conventional location for the .jar file(s) is <code>inst/java</code>. 
It is desirable (and required under an Open Source license) to make the Java source files available: this is best done in a top-level <code>java</code> directory in the package – the source files should not be installed.

</blockquote>
<h2>Data Types</h2>
The <a href="http://spark.rstudio.com/reference/sparklyr/latest/ensure.html"><code>ensure_*</code></a> family of functions can be used to enforce specific data types that are passed to a Spark routine. 
For example, Spark routines that require an integer will not accept an R numeric element. 
Use these functions ensure certain parameters are scalar integers, or scalar doubles, and so on.

ensure_scalar_integer
ensure_scalar_double
ensure_scalar_boolean
ensure_scalar_character
In order to match the correct data types while calling Scala code from R, or retrieving results from Scala back to R, consider the following types mapping:

<table>
<thead>
<tr class="header">
<th>From R</th>
<th>Scala</th>
<th>To R</th></tr>
</thead>
<tbody>
<tr class="odd">
<td>NULL</td>
<td>void</td>
<td>NULL</td></tr>
<tr class="even">
<td>integer</td>
<td>Int</td>
<td>integer</td></tr>
<tr class="odd">
<td>character</td>
<td>String</td>
<td>character</td></tr>
<tr class="even">
<td>logical</td>
<td>Boolean</td>
<td>logical</td></tr>
<tr class="odd">
<td>double</td>
<td>Double</td>
<td>double</td></tr>
<tr class="even">
<td>numeric</td>
<td>Double</td>
<td>double</td></tr>
<tr class="odd">
<td></td>
<td>Float</td>
<td>double</td></tr>
<tr class="even">
<td></td>
<td>Decimal</td>
<td>double</td></tr>
<tr class="odd">
<td></td>
<td>Long</td>
<td>double</td></tr>
<tr class="even">
<td>raw</td>
<td>Array[Byte]</td>
<td>raw</td></tr>
<tr class="odd">
<td>Date</td>
<td>Date</td>
<td>Date</td></tr>
<tr class="even">
<td>POSIXlt</td>
<td>Time</td>
<td></td></tr>
<tr class="odd">
<td>POSIXct</td>
<td>Time</td>
<td>POSIXct</td></tr>
<tr class="even">
<td>list</td>
<td>Array[T]</td>
<td>list</td></tr>
<tr class="odd">
<td>environment</td>
<td>Map[String, T]</td>
<td></td></tr>
<tr class="even">
<td>jobj</td>
<td>Object</td>
<td>jobj</td></tr>
</tbody>
</table>
<h2>Compiling</h2>
Most Spark extensions won’t need to define their own compilation specification, and can instead rely on the default behavior of <code>compile_package_jars</code>. 
For users who would like to take more control over where the scalac compilers should be looked up, use the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_compilation_spec.html"><code>spark_compilation_spec</code></a> fucnction. 
The Spark compilation specification is used when compiling Spark extension Java Archives, and defines which versions of Spark, as well as which versions of Scala, should be used for compilation.
<h2 id="sparkling-water-h2o-machine-learning">Sparkling Water (H2O) Machine Learning</h2>
<h2 id="overview">Overview</h2>
The <strong>rsparkling</strong> extension package provides bindings to H2O's distributed <a href="http://www.h2o.ai/product/algorithms/">machine learning</a> algorithms via <strong>sparklyr</strong>. 
In particular, rsparkling allows you to access the machine learning routines provided by the <a href="http://www.h2o.ai/product/sparkling-water/">Sparkling Water</a> Spark package.

Together with sparklyr's <a href="dplyr.html">dplyr</a> interface, you can easily create and tune H2O machine learning workflows on Spark, orchestrated entirely within R.

<a href="https://github.com/h2oai/rsparkling">rsparkling</a> provides a few simple conversion functions that allow the user to transfer data between Spark DataFrames and H2O Frames. 
Once the Spark DataFrames are available as H2O Frames, the <strong>h2o</strong> R interface can be used to train H2O machine learning algorithms on the data.

A typical machine learning pipeline with rsparkling might be composed of the following stages. 
To fit a model, you might need to:

<ol>
Perform SQL queries through the sparklyr <a href="dplyr.html">dplyr</a> interface,
Use the <code>sdf_*</code> and <code>ft_*</code> family of functions to generate new columns, or partition your data set,
Convert your training, validation and/or test data frames into H2O Frames using the <code>as_h2o_frame</code> function,
Choose an appropriate H2O machine learning algorithm to model your data,
Inspect the quality of your model fit, and use it to make predictions with new data.
</ol>
<h2 id="installation">Installation</h2>
You can install the <strong>rsparkling</strong> package from CRAN as follows:

<code class="language-r">install.packages(&quot;rsparkling&quot;)
</code>

Then set the Sparkling Water version for rsparkling.:

<code class="language-r">options(rsparkling.sparklingwater.version = &quot;2.1.14&quot;)
</code>

For Spark <code>2.0.x</code> set <code>rsparkling.sparklingwater.version</code> to <code>2.0.3</code> instead, for Spark <code>1.6.2</code> use <code>1.6.8</code>.
<h2 id="using-h2o">Using H2O</h2>
Now let's walk through a simple example to demonstrate the use of H2O's machine learning algorithms within R. 
We'll use <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html">h2o.glm</a> to fit a linear regression model. 
Using the built-in <code>mtcars</code> dataset, we'll try to predict a car's fuel consumption (<code>mpg</code>) based on its weight (<code>wt</code>), and the number of cylinders the engine contains (<code>cyl</code>).

First, we will initialize a local Spark connection, and copy the <code>mtcars</code> dataset into Spark.

<code class="language-r">library(rsparkling)
library(sparklyr)
library(h2o)
library(dplyr)

sc &lt;- spark_connect(&quot;local&quot;, version = &quot;2.1.0&quot;)

mtcars_tbl &lt;- copy_to(sc, mtcars, &quot;mtcars&quot;)
</code>

Now, let's perform some simple transformations &ndash; we'll

<ol>
Remove all cars with horsepower less than 100,
Produce a column encoding whether a car has 8 cylinders or not,
Partition the data into separate training and test data sets,
Fit a model to our training data set,
Evaluate our predictive performance on our test dataset.
</ol>

<code class="language-r"># transform our data set, and then partition into 'training', 'test'
partitions &lt;- mtcars_tbl %&gt;%
  filter(hp &gt;= 100) %&gt;%
  mutate(cyl8 = cyl == 8) %&gt;%
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)
</code>

Now, we convert our training and test sets into H2O Frames using rsparkling conversion functions. 
We have already split the data into training and test frames using dplyr.

<code class="language-r">training &lt;- as_h2o_frame(sc, partitions$training, strict_version_check = FALSE)
test &lt;- as_h2o_frame(sc, partitions$test, strict_version_check = FALSE)
</code>

Alternatively, we can use the <code>h2o.splitFrame()</code> function instead of <code>sdf_partition()</code> to partition the data within H2O instead of Spark (e.g. <code> partitions &lt;- h2o.splitFrame(as_h2o_frame(mtcars_tbl), 0.5)</code>)

<code class="language-r"># fit a linear model to the training dataset
glm_model &lt;- h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), 
       y = &quot;mpg&quot;, 
       training_frame = training,
       lambda_search = TRUE)
</code>

For linear regression models produced by H2O, we can use either <code>print()</code> or <code>summary()</code> to learn a bit more about the quality of our fit. 
The <code>summary()</code> method returns some extra information about scoring history and variable importance.

<code class="language-r">glm_model
</code>
<code> ## Model Details:
## ==============
## 
## H2ORegressionModel: glm
## Model ID:  GLM_model_R_1510348062048_1 
## GLM Model: summary
##     family     link                               regularization
## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.05468 )
##                                                                 lambda_search
## 1 nlambda = 100, lambda.max = 5.4682, lambda.min = 0.05468, lambda.1se = -1.0
##   number_of_predictors_total number_of_active_predictors
## 1                          2                           2
##   number_of_iterations                                training_frame
## 1                  100 frame_rdd_32_929e407384e0082416acd4c9897144a0
## 
## Coefficients: glm coefficients
##       names coefficients standardized_coefficients
## 1 Intercept    32.997281                 16.625000
## 2       cyl    -0.906688                 -1.349195
## 3        wt    -2.712562                 -2.282649
## 
## H2ORegressionMetrics: glm
## ** Reported on training data. 
**
## 
## MSE:  2.03293
## RMSE:  1.425808
## MAE:  1.306314
## RMSLE:  0.08238032
## Mean Residual Deviance :  2.03293
## R^2 :  0.8265696
## Null Deviance :93.775
## Null D.o.F. 
:7
## Residual Deviance :16.26344
## Residual D.o.F. 
:5
## AIC :36.37884
</code>

The output suggests that our model is a fairly good fit, and that both a cars weight, as well as the number of cylinders in its engine, will be powerful predictors of its average fuel consumption. 
(The model suggests that, on average, heavier cars consume more fuel.)

Let's use our H2O model fit to predict the average fuel consumption on our test data set, and compare the predicted response with the true measured fuel consumption. 
We'll build a simple ggplot2 plot that will allow us to inspect the quality of our predictions.

<code class="language-r">library(ggplot2)

# compute predicted values on our test dataset
pred &lt;- h2o.predict(glm_model, newdata = test)
# convert from H2O Frame to Spark DataFrame
predicted &lt;- as_spark_dataframe(sc, pred, strict_version_check = FALSE)

# extract the true 'mpg' values from our test dataset
actual &lt;- partitions$test %&gt;%
  select(mpg) %&gt;%
  collect() %&gt;%
  `[[`(&quot;mpg&quot;)

# produce a data.frame housing our predicted + actual 'mpg' values
data &lt;- data.frame(
  predicted = predicted,
  actual    = actual
)
# a bug in data.frame does not set colnames properly; reset here 
names(data) &lt;- c(&quot;predicted&quot;, &quot;actual&quot;)

# plot predicted vs. 
actual values
ggplot(data, aes(x = actual, y = predicted)) +
  geom_abline(lty = &quot;dashed&quot;, col = &quot;red&quot;) +
  geom_point() +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_fixed(ratio = 1) +
  labs(
    x = &quot;Actual Fuel Consumption&quot;,
    y = &quot;Predicted Fuel Consumption&quot;,
    title = &quot;Predicted vs. 
Actual Fuel Consumption&quot;
  )
</code>

<img src="guides-h2o_files/figure-markdown_github-ascii_identifiers/unnamed-chunk-8-1.png" alt="" />

Although simple, our model appears to do a fairly good job of predicting a car's average fuel consumption.

As you can see, we can easily and effectively combine dplyr data transformation pipelines with the machine learning algorithms provided by H2O's Sparkling Water.
<h2 id="algorithms">Algorithms</h2>
Once the <code>H2OContext</code> is made available to Spark (as demonstrated below), all of the functions in the standard h2o R interface can be used with H2O Frames (converted from Spark DataFrames). 
Here is a table of the available algorithms:

<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th></tr>
</thead>

<tbody>
<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glm.html"><code>h2o.glm</code></a></td>
<td>Generalized Linear Model</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html"><code>h2o.deeplearning</code></a></td>
<td>Multilayer Perceptron</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html"><code>h2o.randomForest</code></a></td>
<td>Random Forest</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html"><code>h2o.gbm</code></a></td>
<td>Gradient Boosting Machine</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/naive-bayes.html"><code>h2o.naiveBayes</code></a></td>
<td>Naive-Bayes</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/pca.html"><code>h2o.prcomp</code></a></td>
<td>Principal Components Analysis</td></tr>

<tr>
<td><a href="https://www.rdocumentation.org/packages/h2o/versions/3.8.3.3/topics/h2o.svd"><code>h2o.svd</code></a></td>
<td>Singular Value Decomposition</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/glrm.html"><code>h2o.glrm</code></a></td>
<td>Generalized Low Rank Model</td></tr>

<tr>
<td><a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html"><code>h2o.kmeans</code></a></td>
<td>K-Means Clustering</td></tr>

<tr>
<td><a href="https://www.rdocumentation.org/packages/h2o/versions/3.8.3.3/topics/h2o.anomaly"><code>h2o.anomaly</code></a></td>
<td>Anomaly Detection via Deep Learning Autoencoder</td></tr>
</tbody>
</table>

Additionally, the <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-r/ensemble">h2oEnsemble</a> R package can be used to generate Super Learner ensembles of H2O algorithms:

<table>
<thead>
<tr>
<th>Function</th>
<th>Description</th></tr>
</thead>

<tbody>
<tr>
<td><a href="http://learn.h2o.ai/content/tutorials/ensembles-stacking/"><code>h2o.ensemble</code></a></td>
<td>Super Learner / Stacking</td></tr>

<tr>
<td><a href="https://github.com/h2oai/h2o-3/blob/master/h2o-r/ensemble/demos/h2o_stack_documentation_example.R"><code>h2o.stack</code></a></td>
<td>Super Learner / Stacking</td></tr>
</tbody>
</table>
<h2 id="transformers">Transformers</h2>
A model is often fit not on a dataset as-is, but instead on some transformation of that dataset. 
Spark provides <a href="http://spark.apache.org/docs/latest/ml-features.html">feature transformers</a>, facilitating many common transformations of data within a Spark DataFrame, and sparklyr exposes these within the <code>ft_*</code> family of functions. 
Transformers can be used on Spark DataFrames, and the final training set can be sent to the H2O cluster for machine learning.

<table>
<colgroup>
<col width="38%" />
<col width="61%" />
</colgroup>
<thead>
<tr class="header">
<th>
Function
</th>
<th>
Description
</th></tr>
</thead>
<tbody>
<tr class="odd">
<td>
<a href="reference/sparklyr/latest/ft_binarizer.html"><code>ft_binarizer</code></a></td>
<td>
Threshold numerical features to binary (0/1) feature</td></tr>
<tr class="even">
<td>
<a href="reference/sparklyr/latest/ft_bucketizer.html"><code>ft_bucketizer</code></a></td>
<td>
Bucketizer transforms a column of continuous features to a column of feature buckets</td></tr>
<tr class="odd">
<td>
<a href="reference/sparklyr/latest/ft_discrete_cosine_transform.html"><code>ft_discrete_cosine_transform</code></a></td>
<td>
Transforms a length NN real-valued sequence in the time domain into another length NN real-valued sequence in the frequency domain</td></tr>
<tr class="even">
<td>
<a href="reference/sparklyr/latest/ft_elementwise_product.html"><code>ft_elementwise_product</code></a></td>
<td>
Multiplies each input vector by a provided weight vector, using element-wise multiplication.</td></tr>
<tr class="odd">
<td>
<a href="reference/sparklyr/latest/ft_index_to_string.html"><code>ft_index_to_string</code></a></td>
<td>
Maps a column of label indices back to a column containing the original labels as strings</td></tr>
<tr class="even">
<td>
<a href="reference/sparklyr/latest/ft_quantile_discretizer.html"><code>ft_quantile_discretizer</code></a></td>
<td>
Takes a column with continuous features and outputs a column with binned categorical features</td></tr>
<tr class="odd">
<td>
<a href="reference/sparklyr/latest/ft_sql_transformer.html"><code>ft_sql_transformer</code></a></td>
<td>
Implements the transformations which are defined by a SQL statement</td></tr>
<tr class="even">
<td>
<a href="reference/sparklyr/latest/ft_string_indexer.html"><code>ft_string_indexer</code></a></td>
<td>
Encodes a string column of labels to a column of label indices</td></tr>
<tr class="odd">
<td>
<a href="reference/sparklyr/latest/ft_vector_assembler.html"><code>ft_vector_assembler</code></a></td>
<td>
Combines a given list of columns into a single vector column</td></tr>
</tbody>
</table>

<h3>Examples</h3>

We will use the <code>iris</code> data set to examine a handful of learning algorithms and transformers. 
The iris data set measures attributes for 150 flowers in 3 different species of iris.

<code class="language-r">iris_tbl &lt;- copy_to(sc, iris, &quot;iris&quot;, overwrite = TRUE)
iris_tbl
</code>
<code> ## # Source:   table&lt;iris&gt; [?? x 5]
## # Database: spark_connection
##    Sepal_Length Sepal_Width Petal_Length Petal_Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;chr&gt;
##  1          5.1         3.5          1.4         0.2  setosa
##  2          4.9         3.0          1.4         0.2  setosa
##  3          4.7         3.2          1.3         0.2  setosa
##  4          4.6         3.1          1.5         0.2  setosa
##  5          5.0         3.6          1.4         0.2  setosa
##  6          5.4         3.9          1.7         0.4  setosa
##  7          4.6         3.4          1.4         0.3  setosa
##  8          5.0         3.4          1.5         0.2  setosa
##  9          4.4         2.9          1.4         0.2  setosa
## 10          4.9         3.1          1.5         0.1  setosa
## # ... 
with more rows
</code>

Convert to an H2O Frame:

<code class="language-r">iris_hf &lt;- as_h2o_frame(sc, iris_tbl, strict_version_check = FALSE)
</code>

<h3 id="k-means-clustering">K-Means Clustering</h3>

Use H2O's <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/k-means.html">K-means clustering</a> to partition a dataset into groups. 
K-means clustering partitions points into <code>k</code> groups, such that the sum of squares from points to the assigned cluster centers is minimized.

<code class="language-r">kmeans_model &lt;- h2o.kmeans(training_frame = iris_hf, 
             x = 3:4,
             k = 3,
             seed = 1)
</code>

To look at particular metrics of the K-means model, we can use <code>h2o.centroid_stats()</code> and <code>h2o.centers()</code> or simply print out all the model metrics using <code>print(kmeans_model)</code>.

<code class="language-r"># print the cluster centers
h2o.centers(kmeans_model)
</code>
<code> ##   petal_length petal_width
## 1     1.462000     0.24600
## 2     5.566667     2.05625
## 3     4.296154     1.32500
</code>

<code class="language-r"># print the centroid statistics
h2o.centroid_stats(kmeans_model)
</code>
<code> ## Centroid Statistics: 
##   centroid     size within_cluster_sum_of_squares
## 1        1 50.00000                       1.41087
## 2        2 48.00000                       9.29317
## 3        3 52.00000                       7.20274
</code>

<h3 id="pca">PCA</h3>

Use H2O's <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/pca.html">Principal Components Analysis (PCA)</a> to perform dimensionality reduction. 
PCA is a statistical method to find a rotation such that the first coordinate has the largest variance possible, and each succeeding coordinate in turn has the largest variance possible.

<code class="language-r">pca_model &lt;- h2o.prcomp(training_frame = iris_hf,
          x = 1:4,
          k = 4,
          seed = 1)
</code>
<code> ## Warning in doTryCatch(return(expr), name, parentenv, handler): _train:
## Dataset used may contain fewer number of rows due to removal of rows with
## NA/missing values. 
If this is not desirable, set impute_missing argument in
## pca call to TRUE/True/true/... 
depending on the client language.
</code>

<code class="language-r">pca_model
</code>
<code> ## Model Details:
## ==============
## 
## H2ODimReductionModel: pca
## Model ID:  PCA_model_R_1510348062048_3 
## Importance of components: 
##                             pc1      pc2      pc3      pc4
## Standard deviation     7.861342 1.455041 0.283531 0.154411
## Proportion of Variance 0.965303 0.033069 0.001256 0.000372
## Cumulative Proportion  0.965303 0.998372 0.999628 1.000000
## 
## 
## H2ODimReductionMetrics: pca
## 
## No model metrics available for PCA
</code>

<h3 id="random-forest">Random Forest</h3>

Use H2O's <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/drf.html">Random Forest</a> to perform regression or classification on a dataset. 
We will continue to use the iris dataset as an example for this problem.

As usual, we define the response and predictor variables using the <code>x</code> and <code>y</code> arguments. 
Since we'd like to do a classification, we need to ensure that the response column is encoded as a factor (enum) column.

<code class="language-r">y &lt;- &quot;Species&quot;
x &lt;- setdiff(names(iris_hf), y)
iris_hf[,y] &lt;- as.factor(iris_hf[,y])
</code>

We can split the <code>iris_hf</code> H2O Frame into a train and test set (the split defaults to <sup>75</sup>&frasl;<sub>25</sub> train/test).

<code class="language-r">splits &lt;- h2o.splitFrame(iris_hf, seed = 1)
</code>

Then we can train a Random Forest model:

<code class="language-r">rf_model &lt;- h2o.randomForest(x = x, 
               y = y,
               training_frame = splits[[1]],
               validation_frame = splits[[2]],
               nbins = 32,
               max_depth = 5,
               ntrees = 20,
               seed = 1)
</code>

Since we passed a validation frame, the validation metrics will be calculated. 
We can retrieve individual metrics using functions such as <code>h2o.mse(rf_model, valid = TRUE)</code>. 
The confusion matrix can be printed using the following:

<code class="language-r">h2o.confusionMatrix(rf_model, valid = TRUE)
</code>
<code> ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa          7          0         0 0.0000 =  0 / 7
## versicolor      0         13         0 0.0000 = 0 / 13
## virginica       0          1        10 0.0909 = 1 / 11
## Totals          7         14        10 0.0323 = 1 / 31
</code>

To view the variable importance computed from an H2O model, you can use either the <code>h2o.varimp()</code> or <code>h2o.varimp_plot()</code> functions:

<code class="language-r">h2o.varimp_plot(rf_model)
</code>

<img src="guides-h2o_files/figure-markdown_github-ascii_identifiers/unnamed-chunk-20-1.png" alt="" />

<h3 id="gradient-boosting-machine">Gradient Boosting Machine</h3>

The <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html">Gradient Boosting Machine (GBM)</a> is one of H2O's most popular algorithms, as it works well on many types of data. 
We will continue to use the iris dataset as an example for this problem.

Using the same dataset and <code>x</code> and <code>y</code> from above, we can train a GBM:

<code class="language-r">gbm_model &lt;- h2o.gbm(x = x, 
       y = y,
       training_frame = splits[[1]],
       validation_frame = splits[[2]],                     
       ntrees = 20,
       max_depth = 3,
       learn_rate = 0.01,
       col_sample_rate = 0.7,
       seed = 1)
</code>

Since this is a multi-class problem, we may be interested in inspecting the confusion matrix on a hold-out set. 
Since we passed along a <code>validatin_frame</code> at train time, the validation metrics are already computed and we just need to retreive them from the model object.

<code class="language-r">h2o.confusionMatrix(gbm_model, valid = TRUE)
</code>
<code> ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##            setosa versicolor virginica  Error     Rate
## setosa          7          0         0 0.0000 =  0 / 7
## versicolor      0         13         0 0.0000 = 0 / 13
## virginica       0          1        10 0.0909 = 1 / 11
## Totals          7         14        10 0.0323 = 1 / 31
</code>

<h3 id="deep-learning">Deep Learning</h3>

Use H2O's <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.html">Deep Learning</a> to perform regression or classification on a dataset, extact non-linear features generated by the deep neural network, and/or detect anomalies using a deep learning model with auto-encoding.

In this example, we will use the <code>prostate</code> dataset available within the h2o package:

<code class="language-r">path &lt;- system.file(&quot;extdata&quot;, &quot;prostate.csv&quot;, package = &quot;h2o&quot;)
prostate_df &lt;- spark_read_csv(sc, &quot;prostate&quot;, path)
head(prostate_df)
</code>
<code> ## # Source:   lazy query [?? x 9]
## # Database: spark_connection
##      ID CAPSULE   AGE  RACE DPROS DCAPS   PSA   VOL GLEASON
##   &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;
## 1     1       0    65     1     2     1   1.4   0.0       6
## 2     2       0    72     1     3     2   6.7   0.0       7
## 3     3       0    70     1     1     2   4.9   0.0       6
## 4     4       0    76     2     2     1  51.2  20.0       7
## 5     5       0    69     1     1     1  12.3  55.9       6
## 6     6       1    71     1     3     2   3.3   0.0       8
</code>

Once we've done whatever data manipulation is required to run our model we'll get a reference to it as an h2o frame then split it into training and test sets using the <code>h2o.splitFrame</code> function:

<code class="language-r">prostate_hf &lt;- as_h2o_frame(sc, prostate_df, strict_version_check = FALSE)
splits &lt;- h2o.splitFrame(prostate_hf, seed = 1)
</code>

Next we define the response and predictor columns.

<code class="language-r">y &lt;- &quot;VOL&quot;
#remove response and ID cols
x &lt;- setdiff(names(prostate_hf), c(&quot;ID&quot;, y))
</code>

Now we can train a deep neural net.

<code class="language-r">dl_fit &lt;- h2o.deeplearning(x = x, y = y,
             training_frame = splits[[1]],
             epochs = 15,
             activation = &quot;Rectifier&quot;,
             hidden = c(10, 5, 10),
             input_dropout_ratio = 0.7)
</code>

Evaluate performance on a test set:

<code class="language-r">h2o.performance(dl_fit, newdata = splits[[2]])
</code>
<code> ## H2ORegressionMetrics: deeplearning
## 
## MSE:  253.7022
## RMSE:  15.92803
## MAE:  12.90077
## RMSLE:  1.885052
## Mean Residual Deviance :  253.7022
</code>

Note that the above metrics are not reproducible when H2O's Deep Learning is run on multiple cores, however, the metrics should be fairly stable across repeat runs.

<h3 id="grid-search">Grid Search</h3>

H2O's grid search capabilities currently supports traditional (Cartesian) grid search and random grid search. 
Grid search in R provides the following capabilities:
<code> H2OGrid</code> class: Represents the results of the grid search<code> h2o.getGrid(&lt;grid_id&gt;, sort_by, decreasing)</code>: Display the specified grid<code> h2o.grid</code>: Start a new grid search parameterized by

model builder name (e.g., <code>algorithm = &quot;gbm&quot;</code>)
model parameters (e.g., <code>ntrees = 100</code>)<code> hyper_parameters</code>: attribute for passing a list of hyper parameters (e.g., <code>list(ntrees=c(1,100), learn_rate=c(0.1,0.001))</code>)<code> search_criteria</code>: optional attribute for specifying more a advanced search strategy

<h4 id="cartesian-grid-search">Cartesian Grid Search</h4>

By default, <code>h2o.grid()</code> will train a Cartesian grid search &ndash; meaning, all possible models in the specified grid. 
In this example, we will re-use the prostate data as an example dataset for a regression problem.

<code class="language-r">splits &lt;- h2o.splitFrame(prostate_hf, seed = 1)

y &lt;- &quot;VOL&quot;
#remove response and ID cols
x &lt;- setdiff(names(prostate_hf), c(&quot;ID&quot;, y))
</code>

After prepping the data, we define a grid and execute the grid search.

<code class="language-r"># GBM hyperparamters
gbm_params1 &lt;- list(learn_rate = c(0.01, 0.1),
      max_depth = c(3, 5, 9),
      sample_rate = c(0.8, 1.0),
      col_sample_rate = c(0.2, 0.5, 1.0))

# Train and validate a grid of GBMs
gbm_grid1 &lt;- h2o.grid(&quot;gbm&quot;, x = x, y = y,
        grid_id = &quot;gbm_grid1&quot;,
        training_frame = splits[[1]],
        validation_frame = splits[[1]],
        ntrees = 100,
        seed = 1,
        hyper_params = gbm_params1)

# Get the grid results, sorted by validation MSE
gbm_gridperf1 &lt;- h2o.getGrid(grid_id = &quot;gbm_grid1&quot;, 
               sort_by = &quot;mse&quot;, 
               decreasing = FALSE)
</code>

<code class="language-r">gbm_gridperf1
</code>
<code> ## H2O Grid Details
## ================
## 
## Grid ID: gbm_grid1 
## Used hyper parameters: 
##   -  col_sample_rate 
##   -  learn_rate 
##   -  max_depth 
##   -  sample_rate 
## Number of models: 36 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate max_depth sample_rate          model_ids
## 1             1.0        0.1         9         1.0 gbm_grid1_model_35
## 2             0.5        0.1         9         1.0 gbm_grid1_model_34
## 3             1.0        0.1         9         0.8 gbm_grid1_model_17
## 4             0.5        0.1         9         0.8 gbm_grid1_model_16
## 5             1.0        0.1         5         0.8 gbm_grid1_model_11
##                  mse
## 1  88.10947523138782
## 2  102.3118989994892
## 3 102.78632321923726
## 4  126.4217260351778
## 5  149.6066650109763
## 
## ---
##    col_sample_rate learn_rate max_depth sample_rate          model_ids
## 31             0.5       0.01         3         0.8  gbm_grid1_model_1
## 32             0.2       0.01         5         1.0 gbm_grid1_model_24
## 33             0.5       0.01         3         1.0 gbm_grid1_model_19
## 34             0.2       0.01         5         0.8  gbm_grid1_model_6
## 35             0.2       0.01         3         1.0 gbm_grid1_model_18
## 36             0.2       0.01         3         0.8  gbm_grid1_model_0
##                   mse
## 31  324.8117304723162
## 32 325.10992525687294
## 33 325.27898443785045
## 34 329.36983845305735
## 35 338.54411936919456
## 36  339.7744828617712
</code>

<h4 id="random-grid-search">Random Grid Search</h4>

H2O's Random Grid Search samples from the given parameter space until a set of constraints is met. 
The user can specify the total number of desired models using (e.g. <code> max_models = 40</code>), the amount of time (e.g. <code> max_runtime_secs = 1000</code>), or tell the grid to stop after performance stops improving by a specified amount. 
Random Grid Search is a practical way to arrive at a good model without too much effort.

The example below is set to run fairly quickly &ndash; increase <code>max_runtime_secs</code> or <code>max_models</code> to cover more of the hyperparameter space in your grid search. 
Also, you can expand the hyperparameter space of each of the algorithms by modifying the definition of <code>hyper_param</code> below.

<code class="language-r"># GBM hyperparamters
gbm_params2 &lt;- list(learn_rate = seq(0.01, 0.1, 0.01),
      max_depth = seq(2, 10, 1),
      sample_rate = seq(0.5, 1.0, 0.1),
      col_sample_rate = seq(0.1, 1.0, 0.1))
search_criteria2 &lt;- list(strategy = &quot;RandomDiscrete&quot;, 
           max_models = 50)

# Train and validate a grid of GBMs
gbm_grid2 &lt;- h2o.grid(&quot;gbm&quot;, x = x, y = y,
        grid_id = &quot;gbm_grid2&quot;,
        training_frame = splits[[1]],
        validation_frame = splits[[2]],
        ntrees = 100,
        seed = 1,
        hyper_params = gbm_params2,
        search_criteria = search_criteria2)

# Get the grid results, sorted by validation MSE
gbm_gridperf2 &lt;- h2o.getGrid(grid_id = &quot;gbm_grid2&quot;, 
               sort_by = &quot;mse&quot;, 
               decreasing = FALSE)
</code>

To get the best model, as measured by validation MSE, we simply grab the first row of the <code>gbm_gridperf2@summary_table</code> object, since this table is already sorted such that the lowest MSE model is on top.

<code class="language-r">gbm_gridperf2@summary_table[1,]
</code>
<code> ## Hyper-Parameter Search Summary: ordered by increasing mse
##   col_sample_rate learn_rate max_depth sample_rate          model_ids
## 1             0.8       0.01         2         0.7 gbm_grid2_model_35
##                  mse
## 1 244.61196951586288
</code>

In the examples above, we generated two different grids, specified by <code>grid_id</code>. 
The first grid was called <code>grid_id = &quot;gbm_grid1&quot;</code> and the second was called <code>grid_id = &quot;gbm_grid2&quot;</code>. 
However, if we are using the same dataset &amp; algorithm in two grid searches, it probably makes more sense just to add the results of the second grid search to the first. 
If you want to add models to an existing grid, rather than create a new one, you simply re-use the same <code>grid_id</code>.
<h2 id="exporting-models">Exporting Models</h2>
There are two ways of exporting models from H2O &ndash; saving models as a binary file, or saving models as pure Java code.

<h4 id="binary-models">Binary Models</h4>

The more traditional method is to save a binary model file to disk using the <code>h2o.saveModel()</code> function. 
To load the models using <code>h2o.loadModel()</code>, the same version of H2O that generated the models is required. 
This method is commonly used when H2O is being used in a non-production setting.

A binary model can be saved as follows:

<code class="language-r">h2o.saveModel(my_model, path = &quot;/Users/me/h2omodels&quot;)
</code>

<h4 id="java-pojo-models">Java (POJO) Models</h4>

One of the most valuable features of H2O is it's ability to export models as pure Java code, or rather, a &ldquo;Plain Old Java Object&rdquo; (POJO). 
You can learn more about H2O POJO models in this <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/pojo-quick-start.html">POJO quickstart guide</a>. 
The POJO method is used most commonly when a model is deployed in a production setting. 
POJO models are ideal for when you need very fast prediction response times, and minimal requirements &ndash; the POJO is a standalone Java class with no dependencies on the full H2O stack.

To generate the POJO for your model, use the following command:

<code class="language-r">h2o.download_pojo(my_model, path = &quot;/Users/me/h2omodels&quot;)
</code>

Finally, disconnect with:

<code class="language-r">spark_disconnect_all()
</code>
<code> ## [1] 1
</code>

You can learn more about how to take H2O models to production in the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/productionizing.html">productionizing H2O models</a> section of the H2O docs.
<h2 id="additional-resources">Additional Resources</h2>
<a href="http://docs.h2o.ai">Main documentation site for Sparkling Water (and all H2O software projects)</a>
<a href="http://h2o.ai">H2O.ai website</a>

If you are new to H2O for machine learning, we recommend you start with the <a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/intro-to-h2o.R">Intro to H2O Tutorial</a>, followed by the <a href="https://github.com/h2oai/h2o-tutorials/blob/master/h2o-open-tour-2016/chicago/grid-search-model-selection.R">H2O Grid Search &amp; Model Selection Tutorial</a>. 
There are a number of other H2O R <a href="https://github.com/h2oai/h2o-tutorials">tutorials</a> and <a href="https://github.com/h2oai/h2o-3/tree/master/h2o-r/demos">demos</a> available, as well as the <a href="http://learn.h2o.ai/content/">H2O World 2015 Training Gitbook</a>, and the <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/RBooklet.pdf">Machine Learning with R and H2O Booklet (pdf)</a>.
<h2>R interface for GraphFrames </h2>
<h2>Highlights</h2>
Support for <a href="https://graphframes.github.io/">GraphFrames</a> which aims to provide the functionality of <a href="http://spark.apache.org/graphx/">GraphX</a>.
Perform graph algorithms like: <a href="https://graphframes.github.io/api/scala/index.html#org.graphframes.lib.PageRank">PageRank</a>, <a href="https://graphframes.github.io/api/scala/index.html#org.graphframes.lib.ShortestPaths">ShortestPaths</a> and many <a href="https://graphframes.github.io/api/scala/#package">others</a>.
Designed to work with <a href="https://spark.rstudio.com">sparklyr</a> and the <a href="http://spark.rstudio.com/extensions.html">sparklyr extensions</a>.
<h2>Installation</h2>
To install from CRAN, run:
<code> install.packages(&quot;graphframes&quot;)</code>

For the development version, run:
<code> devtools::install_github(&quot;rstudio/graphframes&quot;)</code>
<h2>Examples</h2>
The examples make use of the <code>highschool</code> dataset from the <code>ggplot</code> package.

<h3>Create a GraphFrame</h3>
The base for graph analyses in Spark, using <code>sparklyr</code>, will be a GraphFrame.

Open a new Spark connection using <code>sparklyr</code>, and copy the <code>highschool</code> data set
<code> library(graphframes)
library(sparklyr)
library(dplyr)

sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.1.0&quot;)

highschool_tbl &lt;- copy_to(sc, ggraph::highschool, &quot;highschool&quot;)

head(highschool_tbl)</code>
<code> ## # Source:   lazy query [?? x 3]
## # Database: spark_connection
##    from    to  year
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    1. 
  14. 
1957.
## 2    1. 
  15. 
1957.
## 3    1. 
  21. 
1957.
## 4    1. 
  54. 
1957.
## 5    1. 
  55. 
1957.
## 6    2. 
  21. 
1957.</code>

The vertices table is be constructed using <code>dplyr</code>. 
The variable name expected by the GraphFrame is <strong>id</strong>.
<code> from_tbl &lt;- highschool_tbl %&gt;% 
  distinct(from) %&gt;% 
  transmute(id = from)

to_tbl &lt;- highschool_tbl %&gt;% 
  distinct(to) %&gt;% 
  transmute(id = to)
  
  
vertices_tbl &lt;- from_tbl %&gt;%
  sdf_bind_rows(to_tbl)

head(vertices_tbl)</code>
<code> ## # Source:   lazy query [?? x 1]
## # Database: spark_connection
##      id
##   &lt;dbl&gt;
## 1    6.
## 2    7.
## 3   12.
## 4   13.
## 5   55.
## 6   58.</code>

The edges table can also be created using <code>dplyr</code>. 
In order for the GraphFrame to work, the <strong>from</strong> variable needs be renamed <strong>src</strong>, and the <strong>to</strong> variable <strong>dst</strong>.
<code> # Create a table with &lt;source, destination&gt; edges
edges_tbl &lt;- highschool_tbl %&gt;% 
  transmute(src = from, dst = to)</code>

The <code>gf_graphframe()</code> function creates a new GraphFrame
<code> gf_graphframe(vertices_tbl, edges_tbl) </code>
<code> ## GraphFrame
## Vertices:
##   $ id &lt;dbl&gt; 6, 7, 12, 13, 55, 58, 63, 41, 44, 48, 59, 1, 4, 17, 20, 22,...
## Edges:
##   $ src &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 6, 6, 6, 7, 8...
##   $ dst &lt;dbl&gt; 14, 15, 21, 54, 55, 21, 22, 9, 15, 5, 18, 19, 43, 19, 43, ...</code>

<h3>Basic Page Rank</h3>
We will calculate <a href="https://en.wikipedia.org/wiki/PageRank">PageRank</a> over this dataset. 
The <code>gf_graphframe()</code> command can easily be piped into the <code>gf_pagerank()</code> function to execute the Page Rank.
<code> gf_graphframe(vertices_tbl, edges_tbl) %&gt;%
  gf_pagerank(reset_prob = 0.15, max_iter = 10L, source_id = &quot;1&quot;)</code>
<code> ## GraphFrame
## Vertices:
##   $ id       &lt;dbl&gt; 12, 12, 59, 59, 1, 1, 20, 20, 45, 45, 8, 8, 9, 9, 26,...
##   $ pagerank &lt;dbl&gt; 1.216914e-02, 1.216914e-02, 1.151867e-03, 1.151867e-0...
## Edges:
##   $ src    &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,...
##   $ dst    &lt;dbl&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 22, 22,...
##   $ weight &lt;dbl&gt; 0.02777778, 0.02777778, 0.02777778, 0.02777778, 0.02777...</code>

Additionaly, one can calculate the degrees of vertices using <code>gf_degrees</code> as follows:
<code> gf_graphframe(vertices_tbl, edges_tbl) %&gt;% 
  gf_degrees()</code>
<code> ## # Source:   table&lt;sparklyr_tmp_27b034635ad&gt; [?? x 2]
## # Database: spark_connection
##       id degree
##    &lt;dbl&gt;  &lt;int&gt;
##  1   55. 
    25
##  2    6. 
    10
##  3   13. 
    16
##  4    7. 
     6
##  5   12. 
    11
##  6   63. 
    21
##  7   58. 
     8
##  8   41. 
    19
##  9   48. 
    15
## 10   59. 
    11
## # ... 
with more rows</code>

<h3>Visualizations</h3>
In order to visualize large <code>graphframe</code>s, one can use <code>sample_n</code> and then use <code>ggraph</code> with <code>igraph</code> to visualize the graph as follows:
<code> library(ggraph)
library(igraph)

graph &lt;- highschool_tbl %&gt;%
  sample_n(20) %&gt;%
  collect() %&gt;%
  graph_from_data_frame()

ggraph(graph, layout = &#39;kk&#39;) + 
    geom_edge_link(aes(colour = factor(year))) + 
    geom_node_point() + 
    ggtitle(&#39;An example&#39;)</code>

<img src="/graphframes_files/figure-html/unnamed-chunk-9-1.png" width="672" />
<h2>Additional functions</h2>
Apart from calculating <code>PageRank</code> using <code>gf_pagerank</code>, the following functions are available:
<code> gf_bfs()</code>: Breadth-first search (BFS).<code> gf_connected_components()</code>: Connected components.<code> gf_shortest_paths()</code>: Shortest paths algorithm.<code> gf_scc()</code>: Strongly connected components.<code> gf_triangle_count</code>: Computes the number of triangles passing through each vertex and others.
<h2>R interface for MLeap </h2>
<strong>mleap</strong> is a <a href="http://spark.rstudio.com/">sparklyr</a> extension that provides an interface to <a href="http://mleap-docs.combust.ml/">MLeap</a>, which allows us to take Spark pipelines to production.
<h2>Install <code>mleap</code></h2>
<strong>mleap</strong> can be installed from CRAN via
<code> install.packages(&quot;mleap&quot;)</code>

or, for the latest development version from GitHub, using
<code> devtools::install_github(&quot;rstudio/mleap&quot;)</code>
<h2>Setup</h2>
Once <code>mleap</code> has been installed, we can install the external dependencies using:
<code> library(mleap)
install_mleap()</code>

Another dependency of <code>mleap</code> is Maven. 
If it is already installed, just point <code>mleap</code> to its location:
<code> options(maven.home = &quot;path/to/maven&quot;)</code>

If Maven is not yet installed, which is the most likely case, use the following to install it:
<code> install_maven()</code>
<h2>Create an MLeap Bundle</h2>
<ol style="list-style-type: decimal">
Start Spark session using <code>sparklyr</code>
<code> library(sparklyr)
sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.2.0&quot;)
mtcars_tbl &lt;- sdf_copy_to(sc, mtcars, overwrite = TRUE)</code>

Create a fit an ML Pipeline
<code> pipeline &lt;- ml_pipeline(sc) %&gt;%
  ft_binarizer(&quot;hp&quot;, &quot;big_hp&quot;, threshold = 100) %&gt;%
  ft_vector_assembler(c(&quot;big_hp&quot;, &quot;wt&quot;, &quot;qsec&quot;), &quot;features&quot;) %&gt;%
  ml_gbt_regressor(label_col = &quot;mpg&quot;)

pipeline_model &lt;- ml_fit(pipeline, mtcars_tbl)</code>

A transformed data frame with the appropriate schema is required for exporting the Pipeline model
<code> transformed_tbl &lt;- ml_transform(pipeline_model, mtcars_tbl)</code>

Export the model using the <code>ml_write_bundle()</code> function from <code>mleap</code>
<code> model_path &lt;- file.path(tempdir(), &quot;mtcars_model.zip&quot;)
ml_write_bundle(pipeline_model, transformed_tbl, model_path)</code>
<code> ## Model successfully exported.</code>

Close Spark session
<code> spark_disconnect(sc)</code>

</ol>
At this point, we can share <code>mtcars_model.zip</code> with the deployment/implementation engineers, and they would be able to embed the model in another application. 
See the <a href="http://mleap-docs.combust.ml/">MLeap docs</a> for details.
<h2>Test the <code>mleap</code> bundle</h2>
The <code>mleap</code> package also provides R functions for testing that the saved models behave as expected. 
Here we load the previously saved model:
<code> model &lt;- mleap_load_bundle(model_path)
model</code>
<code> ## MLeap Transformer
## &lt;db23a9f1-7b3d-4d27-9eb0-8675125ab3a5&gt; 
##   Name: pipeline_fe6b8cb0028f 
##   Format: json 
##   MLeap Version: 0.10.0-SNAPSHOT</code>

To retrieve the schema associated with the model use the <code>mleap_model_schema()</code> function
<code> mleap_model_schema(model)</code>
<code> ## # A tibble: 6 x 4
##   name       type   nullable dimension
##   &lt;chr&gt;      &lt;chr&gt;  &lt;lgl&gt;    &lt;chr&gt;    
## 1 qsec       double TRUE     &lt;NA&gt;     
## 2 hp         double FALSE    &lt;NA&gt;     
## 3 wt         double TRUE     &lt;NA&gt;     
## 4 big_hp     double FALSE    &lt;NA&gt;     
## 5 features   double TRUE     (3)      
## 6 prediction double FALSE    &lt;NA&gt;</code>

Then, we create a new data frame to be scored, and make predictions using the model:
<code> newdata &lt;- tibble::tribble(
  ~qsec, ~hp, ~wt,
  16.2,  101, 2.68,
  18.1,  99,  3.08
)

# Transform the data frame
transformed_df &lt;- mleap_transform(model, newdata)
dplyr::glimpse(transformed_df)</code>
<code> ## Observations: 2
## Variables: 6
## $ qsec       &lt;dbl&gt; 16.2, 18.1
## $ hp         &lt;dbl&gt; 101, 99
## $ wt         &lt;dbl&gt; 2.68, 3.08
## $ big_hp     &lt;dbl&gt; 1, 0
## $ features   &lt;list&gt; [[[1, 2.68, 16.2], [3]], [[0, 3.08, 18.1], [3]]]
## $ prediction &lt;dbl&gt; 21.06529, 22.36667</code>
<h2>Examples </h2>
  <table cellpadding="0">
    <col width="300">
<tr>
  <td>
  <a id="menu-link-/examples/databricks-cluster-remote/"  class="side-menu__sub__item__text" href = "/examples/databricks-cluster-remote/">
    Option 1 - Connecting to Databricks remotely
  </td>
  <td> Overview With this configuration, RStudio Server Pro is installed outside of the Spark cluster and allows users to connect to Spark remotely using sparklyr with Databricks Connect.
This is the recommended configuration because it targets separate environments, involves a typical configuration process, avoids resource contention, and allows RStudio Server Pro to connect to Databricks as well as other remote storage and compute resources.
 Advantages and limitations Advantages:
 RStudio Server Pro will remain functional if Databricks clusters are terminated Provides the ability to communicate with one or more Databricks clusters as a remote compute resource Avoids resource contention between RStudio Server Pro and Databricks  Limitations: </td></tr>
   
<tr>
  <td>
  <a id="menu-link-/examples/databricks-cluster-local/"  class="side-menu__sub__item__text" href = "/examples/databricks-cluster-local/">
    Option 2 - Working inside of Databricks
  </td>
  <td> Overview If the recommended path of connecting to Spark remotely with Databricks Connect does not apply to your use case, then you can install RStudio Server Pro directly within a Databricks cluster as described in the sections below.
With this configuration, RStudio Server Pro is installed on the Spark driver node and allows users to work locally with Spark using sparklyr.
This configuration can result in increased complexity, limited connectivity to other storage and compute resources, resource contention between RStudio Server Pro and Databricks, and maintenance concerns due to the ephemeral nature of Databricks clusters. </td></tr>
   
<tr>
  <td>
  <a id="menu-link-/examples/stand-alone-aws/"  class="side-menu__sub__item__text" href = "/examples/stand-alone-aws/">
    Spark Standalone Deployment in AWS
  </td>
  <td> Overview The plan is to launch 4 identical EC2 server instances. 
One server will be the Master node and the other 3 the worker nodes. 
In one of the worker nodes, we will install RStudio server.
What makes a server the Master node is only the fact that it is running the master service, while the other machines are running the slave service and are pointed to that first master. 
This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them. </td></tr>
   
<tr>
  <td>
  <a id="menu-link-/examples/databricks-cluster/"  class="side-menu__sub__item__text" href = "/examples/databricks-cluster/">
    Using sparklyr with Databricks
  </td>
  <td> Overview This documentation demonstrates how to use sparklyr with Apache Spark in Databricks along with RStudio Team, RStudio Server Pro, RStudio Connect, and RStudio Package Manager.
 Using RStudio Team with Databricks RStudio Team is a bundle of our popular professional software for developing data science projects, publishing data products, and managing packages.
RStudio Team and sparklyr can be used with Databricks to work with large datasets and distributed computations with Apache Spark. </td></tr>
   
<tr>
  <td>
  <a id="menu-link-/examples/cloudera-aws/"  class="side-menu__sub__item__text" href = "/examples/cloudera-aws/">
    Using sparklyr with an Apache Spark cluster
  </td>
  <td> Summary This document demonstrates how to use sparklyr with an Cloudera Hadoop &amp; Spark cluster. 
Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. 
RStudio Server is installed on the master node and orchestrates the analysis in spark.
 Cloudera Cluster This demonstration is focused on adding RStudio integration to an existing Cloudera cluster. 
The assumption will be made that there no aid is needed to setup and administer the cluster. </td></tr>
   
<tr>
  <td>
  <a id="menu-link-/examples/yarn-cluster-emr/"  class="side-menu__sub__item__text" href = "/examples/yarn-cluster-emr/">
    Using sparklyr with an Apache Spark cluster
  </td>
  <td> This document demonstrates how to use sparklyr with an Apache Spark cluster. 
Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. 
RStudio Server is installed on the master node and orchestrates the analysis in spark. 
Here is the basic workflow.
Data preparation Set up the cluster This demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsot, Google, or any other provider. </td></tr>
   
  </table>
<h2>Spark Standalone Deployment in AWS </h2>
<h2>Overview</h2>
The plan is to launch 4 identical EC2 server instances. 
One server will be the Master node and the other 3 the worker nodes. 
In one of the worker nodes, we will install RStudio server.

What makes a server the Master node is only the fact that it is running the <strong>master</strong> service, while the other machines are running the <strong>slave</strong> service and are pointed to that first master. 
This simple setup, allows us to install the same Spark components on all 4 servers and then just add RStudio to one of them.

The topology will look something like this:

<img src="images/deployment/amazon-ec2/spark-sa-setup.png" width='600px' align='center'/>
<h2>AWS EC Instances</h2>
Here are the details of the EC2 instance, just deploy one at this point:

<strong>Type:</strong> t2.medium
<strong>OS:</strong> Ubuntu 16.04 LTS
<strong>Disk space:</strong> At least 20GB
<strong>Security group:</strong> Open the following ports: 8080 (Spark UI), 4040 (Spark Worker UI), 8088 (sparklyr UI) and 8787 (RStudio). 
Also open <em>All TCP</em> ports for the machines inside the security group.
<h2>Spark</h2>
Perform the steps in this section on all of the servers that will be part of the cluster.

<h3>Install Java 8</h3>

We will add the Java 8 repository, install it and set it as default<code> sudo apt-add-repository ppa:webupd8team/java
sudo apt-get update
sudo apt-get install oracle-java8-installer
sudo apt-get install oracle-java8-set-default
sudo apt-get update</code>

<h3>Download Spark</h3>

Download and unpack a pre-compiled version of Spark. 
Here’s is the link to the <a href="http://spark.apache.org/downloads.html">official Spark download page</a><code> wget http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz
tar -xvzf spark-2.1.0-bin-hadoop2.7.tgz
cd spark-2.1.0-bin-hadoop2.7</code>

<h3>Create and launch AMI</h3>

We will create an image of the server. 
In Amazon, these are called AMIs, for information please see the <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">User Guide</a>.

Launch 3 instances of the AMI
<h2>RStudio Server</h2>
Select one of the nodes to execute this section. 
Please check the <a href="https://www.rstudio.com/products/rstudio/download-server/">RStudio download page</a> for the latest version

<h3>Install R</h3>

In order to get the latest R core, we will need to update the source list in Ubuntu.<code> sudo sh -c &#39;echo &quot;deb http://cran.rstudio.com/bin/linux/ubuntu xenial/&quot; &gt;&gt; /etc/apt/sources.list&#39;
gpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9
gpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -
sudo apt-get update</code>

Now we can install R<code> sudo apt-get install r-base
sudo apt-get install gdebi-core</code>

<h3>Install RStudio</h3>

We will download and install 1.044 of RStudio Server. 
To find the latest version, please visit the <a href="https://www.rstudio.com/products/rstudio/download3/">RStudio website</a>. 
In order to get the enhanced integration with Spark, RStudio version 1.044 or later will be needed.<code> wget https://download2.rstudio.org/rstudio-server-1.0.153-amd64.deb
sudo gdebi rstudio-server-1.0.153-amd64.deb</code>

<h3>Install dependencies</h3>

Run the following commands<code> sudo apt-get -y install libcurl4-gnutls-dev
sudo apt-get -y install libssl-dev
sudo apt-get -y install libxml2-dev</code>

<h3>Add default user</h3>

Run the following command to add a default user<code> sudo adduser rstudio-user</code>

<h3>Start the Master node</h3>

Select one of the servers to become your Master node

Run the command that starts the master service<code> sudo spark-2.1.0-bin-hadoop2.7/sbin/start-master.sh</code>

Close the terminal connection (optional)

<h3>Start Worker nodes</h3>

Start the slave service. 
<strong>Important</strong>: Use dots not dashes as separators for the Spark Master node’s address<code> sudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://[Master node&#39;s IP address]:7077</code>

sudo spark-2.1.0-bin-hadoop2.7/sbin/start-slave.sh spark://ip-172-30-1-94.us-west-2.compute.internal:7077
- Close the terminal connection (optional)

<h3>Pre-load pacakges</h3>

Log into RStudio (port 8787)

Use ‘rstudio-user’<code> install.packages(&quot;sparklyr&quot;)</code>

<h3>Connect to the Spark Master</h3>

Navigate to the Spark Master’s UI, typically on port 8080 <img src="images/deployment/amazon-ec2/spark-master.png" class="screenshot" width=639 />

Note the <strong>Spark Master URL</strong>

Logon to RStudio

Run the following code
<code> 
library(sparklyr)

conf &lt;- spark_config()
conf$spark.executor.memory &lt;- &quot;2GB&quot;
conf$spark.memory.fraction &lt;- 0.9

sc &lt;- spark_connect(master=&quot;[Spark Master URL]&quot;, 
version = &quot;2.1.0&quot;,
config = conf,
spark_home = &quot;/home/ubuntu/spark-2.1.0-bin-hadoop2.7/&quot;)
</code>
<h2>Using sparklyr with an Apache Spark cluster </h2>
This document demonstrates how to use <code>sparklyr</code> with an Apache Spark cluster. 
Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. 
RStudio Server is installed on the master node and orchestrates the analysis in spark. 
Here is the basic workflow.

<img src="images/deployment/amazon-emr/workflowShare.png" width=600/>
<h2>Data preparation</h2>
<h2>Set up the cluster</h2>
This demonstration uses Amazon Web Services (AWS), but it could just as easily use Microsot, Google, or any other provider. 
We will use Elastic Map Reduce (EMR) to easily set up a cluster with two core nodes and one master node. 
Nodes use virtual servers from the Elastic Compute Cloud (EC2). 
<em>Note: There is no free tier for EMR, charges will apply.</em>

Before beginning this setup we assume you have:

Familiarity with and access to an AWS account
Familiarity with basic linux commands
Sudo privileges in order to install software from the command line

<img src="images/deployment/amazon-emr/emrArchitecture.png" width=600/>

<h3>Build an EMR cluster</h3>
Before beginning the EMR wizard setup, make sure you create the following in AWS:

An AWS key pair (.pem key) so you can SSH into the EC2 master node
A security group that gives you access to port 22 on your IP and port 8787 from anywhere
<img src="images/deployment/amazon-emr/awsNewSecurityGroup.png" width=600/>

<hr />
<h4>Step 1: Select software</h4>
Make sure to select Hive and Spark as part of the install. 
Note that by choosing Spark, R will also be installed on the master node as part of the distribution.

<img src="images/deployment/amazon-emr/emrConfigStep1.png" width=600/>

<hr />
<h4>Step 2: Select hardware</h4>
Install 2 core nodes and one master node with m3.xlarge 80 GiB storage per node. 
You can easily increase the number of nodes later.

<img src="images/deployment/amazon-emr/emrConfigStep2.png" width=600/>

<hr />
<h4>Step 3: Select general cluster settings</h4>
Click next on the general cluster settings.

<img src="images/deployment/amazon-emr/emrConfigStep3.png" width=600/>

<hr />
<h4>Step 4: Select security</h4>
Enter your EC2 key pair and security group. 
Make sure the security group has ports 22 and 8787 open.

<img src="images/deployment/amazon-emr/emrConfigStep4.png" width=600/>

<hr />

<h3>Connect to EMR</h3>
The cluster page will give you details about your EMR cluster and instructions on connecting.

<img src="images/deployment/amazon-emr/awsClusterConnect.png" width=600/>

Connect to the master node via SSH using your key pair. 
Once you connect you will see the EMR welcome.
<code> # Log in to master node
ssh -i ~/spark-demo.pem hadoop@ec2-52-10-102-11.us-west-2.compute.amazonaws.com</code>

<img src="images/deployment/amazon-emr/emrLogin.png" width=600/>

<h3>Install RStudio Server</h3>
EMR uses Amazon Linux which is based on Centos. 
Update your master node and install dependencies that will be used by R packages.
<code> # Update
sudo yum update
sudo yum install libcurl-devel openssl-devel # used for devtools</code>

The installation of RStudio Server is easy. 
Download the <a href="https://www.rstudio.com/products/rstudio/download/preview/">preview version</a> of RStudio and install on the master node.
<code> # Install RStudio Server
wget -P /tmp https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-rhel-0.99.1266-x86_64.rpm
sudo yum install --nogpgcheck /tmp/rstudio-server-rhel-0.99.1266-x86_64.rpm</code>

<h3>Create a User</h3>
Create a user called <code>rstudio-user</code> that will perform the data analysis. 
Create a user directory for <code>rstudio-user</code> on HDFS with the <code>hadoop fs</code> command.
<code> # Make User
sudo useradd -m rstudio-user
sudo passwd rstudio-user

# Create new directory in hdfs
hadoop fs -mkdir /user/rstudio-user
hadoop fs -chmod 777 /user/rstudio-user</code>
<h2>Download flights data</h2>
The <a href="http://stat-computing.org/dataexpo/2009/the-data.html">flights</a> data is a well known data source representing 123 million flights over 22 years. 
It consumes roughly 12 GiB of storage in uncompressed CSV format in yearly files.
<h4>Switch User</h4>
For data loading and analysis, make sure you are logged in as regular user.
<code> # create directories on hdfs for new user
hadoop fs -mkdir /user/rstudio-user
hadoop fs -chmod 777 /user/rstudio-user

# switch user
su rstudio-user</code>

<h3>Download data</h3>
Run the following script to download data from the web onto your master node. 
Download the yearly flight data and the airlines lookup table.
<code> # Make download directory
mkdir /tmp/flights

# Download flight data by year
for i in {1987..2008}
  do
    echo &quot;$(date) $i Download&quot;
    fnam=$i.csv.bz2
    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam
    echo &quot;$(date) $i Unzip&quot;
    bunzip2 /tmp/flights/$fnam
  done

# Download airline carrier data
wget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS

# Download airports data
wget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat</code>

<h3>Distribute into HDFS</h3>
Copy data into HDFS using the <code>hadoop fs</code> command.
<code> # Copy flight data to HDFS
hadoop fs -mkdir /user/rstudio-user/flights/
hadoop fs -put /tmp/flights /user/rstudio-user/

# Copy airline data to HDFS
hadoop fs -mkdir /user/rstudio-user/airlines/
hadoop fs -put /tmp/airlines.csv /user/rstudio-user/airlines

# Copy airport data to HDFS
hadoop fs -mkdir /user/rstudio-user/airports/
hadoop fs -put /tmp/airports.csv /user/rstudio-user/airports</code>
<h2>Create Hive tables</h2>
Launch Hive from the command line.
<code> # Open Hive prompt
hive</code>

Create the metadata that will structure the flights table. 
Load data into the Hive table.
<code> # Create metadata for flights
CREATE EXTERNAL TABLE IF NOT EXISTS flights
(
year int,
month int,
dayofmonth int,
dayofweek int,
deptime int,
crsdeptime int,
arrtime int, 
crsarrtime int,
uniquecarrier string,
flightnum int,
tailnum string, 
actualelapsedtime int,
crselapsedtime int,
airtime string,
arrdelay int,
depdelay int, 
origin string,
dest string,
distance int,
taxiin string,
taxiout string,
cancelled int,
cancellationcode string,
diverted int,
carrierdelay string,
weatherdelay string,
nasdelay string,
securitydelay string,
lateaircraftdelay string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
TBLPROPERTIES(&quot;skip.header.line.count&quot;=&quot;1&quot;);

# Load data into table
LOAD DATA INPATH &#39;/user/rstudio-user/flights&#39; INTO TABLE flights;</code>

Create the metadata that will structure the airlines table. 
Load data into the Hive table.
<code> # Create metadata for airlines
CREATE EXTERNAL TABLE IF NOT EXISTS airlines
(
Code string,
Description string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;
WITH SERDEPROPERTIES
(
&quot;separatorChar&quot; = &#39;\,&#39;,
&quot;quoteChar&quot;     = &#39;\&quot;&#39;
)
STORED AS TEXTFILE
tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;);

# Load data into table
LOAD DATA INPATH &#39;/user/rstudio-user/airlines&#39; INTO TABLE airlines;</code>

Create the metadata that will structure the airports table. 
Load data into the Hive table.
<code> # Create metadata for airports
CREATE EXTERNAL TABLE IF NOT EXISTS airports
(
id string,
name string,
city string,
country string,
faa string,
icao string,
lat double,
lon double,
alt int,
tz_offset double,
dst string,
tz_name string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;
WITH SERDEPROPERTIES
(
&quot;separatorChar&quot; = &#39;\,&#39;,
&quot;quoteChar&quot;     = &#39;\&quot;&#39;
)
STORED AS TEXTFILE;

# Load data into table
LOAD DATA INPATH &#39;/user/rstudio-user/airports&#39; INTO TABLE airports;</code>
<h2>Connect to Spark</h2>
Log in to RStudio Server by pointing a browser at your master node IP:8787.

<img src="images/deployment/amazon-emr/rstudioLogin.png" width=600/>

Set the environment variable <code>SPARK_HOME</code> and then run <code>spark_connect</code>. 
After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.
<code> # Connect to Spark
library(sparklyr)
library(dplyr)
library(ggplot2)
Sys.setenv(SPARK_HOME=&quot;/usr/lib/spark&quot;)
config &lt;- spark_config()
sc &lt;- spark_connect(master = &quot;yarn-client&quot;, config = config, version = &#39;1.6.2&#39;)</code>

Once you are connected, you will see the Spark pane appear along with your hive tables.

<img src="images/deployment/amazon-emr/rstudioSparkPane.png" width=600/>

You can inspect your tables by clicking on the data icon.

<img src="images/deployment/amazon-emr/rstudioData.png" width=600/>
<h2>Data analysis</h2>
Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier.

<img src="images/deployment/amazon-emr/rstudio.png" width=600/>
<h2>Cache the tables into memory</h2>
Use <code>tbl_cache</code> to load the flights table into memory. 
Caching tables will make analysis much faster. 
Create a dplyr reference to the Spark DataFrame.
<code> # Cache flights Hive table into Spark
tbl_cache(sc, &#39;flights&#39;)
flights_tbl &lt;- tbl(sc, &#39;flights&#39;)

# Cache airlines Hive table into Spark
tbl_cache(sc, &#39;airlines&#39;)
airlines_tbl &lt;- tbl(sc, &#39;airlines&#39;)

# Cache airports Hive table into Spark
tbl_cache(sc, &#39;airports&#39;)
airports_tbl &lt;- tbl(sc, &#39;airports&#39;)</code>
<h2>Create a model data set</h2>
Filter the data to contain only the records to be used in the fitted model. 
Join carrier descriptions for reference. 
Create a new variable called <code>gain</code> which represents the amount of time gained (or lost) in flight.
<code> # Filter records and create target variable &#39;gain&#39;
model_data &lt;- flights_tbl %&gt;%
  filter(!is.na(arrdelay) &amp; !is.na(depdelay) &amp; !is.na(distance)) %&gt;%
  filter(depdelay &gt; 15 &amp; depdelay &lt; 240) %&gt;%
  filter(arrdelay &gt; -60 &amp; arrdelay &lt; 360) %&gt;%
  filter(year &gt;= 2003 &amp; year &lt;= 2007) %&gt;%
  left_join(airlines_tbl, by = c(&quot;uniquecarrier&quot; = &quot;code&quot;)) %&gt;%
  mutate(gain = depdelay - arrdelay) %&gt;%
  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)

# Summarize data by carrier
model_data %&gt;%
  group_by(uniquecarrier) %&gt;%
  summarize(description = min(description), gain=mean(gain), 
    distance=mean(distance), depdelay=mean(depdelay)) %&gt;%
  select(description, gain, distance, depdelay) %&gt;%
  arrange(gain)</code>
<code> Source:   query [?? x 4]
Database: spark connection master=yarn-client app=sparklyr local=FALSE
  description       gain  distance depdelay
            &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1        ATA Airlines d/b/a ATA -3.3480120 1134.7084 56.06583
2  ExpressJet Airlines Inc. 
(1) -3.0326180  519.7125 59.41659
3                     Envoy Air -2.5434415  416.3716 53.12529
4       Northwest Airlines Inc. 
-2.2030586  779.2342 48.52828
5          Delta Air Lines Inc. 
-1.8248026  868.3997 50.77174
6   AirTran Airways Corporation -1.4331555  641.8318 54.96702
7    Continental Air Lines Inc. 
-0.9617003 1116.6668 57.00553
8        American Airlines Inc. 
-0.8860262 1074.4388 55.45045
9             Endeavor Air Inc. 
-0.6392733  467.1951 58.47395
10              JetBlue Airways -0.3262134 1139.0443 54.06156
# ... 
with more rows</code>
<h2>Train a linear model</h2>
Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier.
<code> # Partition the data into training and validation sets
model_partition &lt;- model_data %&gt;% 
  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)

# Fit a linear model
ml1 &lt;- model_partition$train %&gt;%
  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)

# Summarize the linear model
summary(ml1)</code>
<code> Deviance Residuals: (approximate):
     Min       1Q   Median       3Q      Max 
-305.422   -5.593    2.699    9.750  147.871 

Coefficients:
      Estimate  Std. 
Error  t value  Pr(&gt;|t|)    
(Intercept)      -1.24342576  0.10248281 -12.1330 &lt; 2.2e-16 ***
distance          0.00326600  0.00001670 195.5709 &lt; 2.2e-16 ***
depdelay         -0.01466233  0.00020337 -72.0977 &lt; 2.2e-16 ***
uniquecarrier_AA -2.32650517  0.10522524 -22.1098 &lt; 2.2e-16 ***
uniquecarrier_AQ  2.98773637  0.28798507  10.3746 &lt; 2.2e-16 ***
uniquecarrier_AS  0.92054894  0.11298561   8.1475 4.441e-16 ***
uniquecarrier_B6 -1.95784698  0.11728289 -16.6934 &lt; 2.2e-16 ***
uniquecarrier_CO -2.52618081  0.11006631 -22.9514 &lt; 2.2e-16 ***
uniquecarrier_DH  2.23287189  0.11608798  19.2343 &lt; 2.2e-16 ***
uniquecarrier_DL -2.68848119  0.10621977 -25.3106 &lt; 2.2e-16 ***
uniquecarrier_EV  1.93484736  0.10724290  18.0417 &lt; 2.2e-16 ***
uniquecarrier_F9 -0.89788137  0.14422281  -6.2257 4.796e-10 ***
uniquecarrier_FL -1.46706706  0.11085354 -13.2343 &lt; 2.2e-16 ***
uniquecarrier_HA -0.14506644  0.25031456  -0.5795    0.5622    
uniquecarrier_HP  2.09354855  0.12337515  16.9690 &lt; 2.2e-16 ***
uniquecarrier_MQ -1.88297535  0.10550507 -17.8473 &lt; 2.2e-16 ***
uniquecarrier_NW -2.79538927  0.10752182 -25.9983 &lt; 2.2e-16 ***
uniquecarrier_OH  0.83520117  0.11032997   7.5700 3.730e-14 ***
uniquecarrier_OO  0.61993842  0.10679884   5.8047 6.447e-09 ***
uniquecarrier_TZ -4.99830389  0.15912629 -31.4109 &lt; 2.2e-16 ***
uniquecarrier_UA -0.68294396  0.10638099  -6.4198 1.365e-10 ***
uniquecarrier_US -0.61589284  0.10669583  -5.7724 7.815e-09 ***
uniquecarrier_WN  3.86386059  0.10362275  37.2878 &lt; 2.2e-16 ***
uniquecarrier_XE -2.59658123  0.10775736 -24.0966 &lt; 2.2e-16 ***
uniquecarrier_YV  3.11113140  0.11659679  26.6828 &lt; 2.2e-16 ***
---
Signif. 
codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-Squared: 0.02385
Root Mean Squared Error: 17.74</code>
<h2>Assess model performance</h2>
Compare the model performance using the validation data.
<code> # Calculate average gains by predicted decile
model_deciles &lt;- lapply(model_partition, function(x) {
  sdf_predict(ml1, x) %&gt;%
    mutate(decile = ntile(desc(prediction), 10)) %&gt;%
    group_by(decile) %&gt;%
    summarize(gain = mean(gain)) %&gt;%
    select(decile, gain) %&gt;%
    collect()
})

# Create a summary dataset for plotting
deciles &lt;- rbind(
  data.frame(data = &#39;train&#39;, model_deciles$train),
  data.frame(data = &#39;valid&#39;, model_deciles$valid),
  make.row.names = FALSE
)

# Plot average gains by predicted decile
deciles %&gt;%
  ggplot(aes(factor(decile), gain, fill = data)) +
  geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;) +
  labs(title = &#39;Average gain by predicted decile&#39;, x = &#39;Decile&#39;, y = &#39;Minutes&#39;)</code>

<img src="images/deployment/amazon-emr/performance-1.png" width=600/>
<h2>Visualize predictions</h2>
Compare actual gains to predicted gains for an out of time sample.
<code> # Select data from an out of time sample
data_2008 &lt;- flights_tbl %&gt;%
  filter(!is.na(arrdelay) &amp; !is.na(depdelay) &amp; !is.na(distance)) %&gt;%
  filter(depdelay &gt; 15 &amp; depdelay &lt; 240) %&gt;%
  filter(arrdelay &gt; -60 &amp; arrdelay &lt; 360) %&gt;%
  filter(year == 2008) %&gt;%
  left_join(airlines_tbl, by = c(&quot;uniquecarrier&quot; = &quot;code&quot;)) %&gt;%
  mutate(gain = depdelay - arrdelay) %&gt;%
  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)

# Summarize data by carrier
carrier &lt;- sdf_predict(ml1, data_2008) %&gt;%
  group_by(description) %&gt;%
  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %&gt;%
  filter(freq &gt; 10000) %&gt;%
  collect

# Plot actual gains and predicted gains by airline carrier
ggplot(carrier, aes(gain, prediction)) + 
  geom_point(alpha = 0.75, color = &#39;red&#39;, shape = 3) +
  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = &#39;blue&#39;) +
  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +
  labs(title=&#39;Average Gains Forecast&#39;, x = &#39;Actual&#39;, y = &#39;Predicted&#39;)</code>

<img src="images/deployment/amazon-emr/forecast-1.png" width=600/>

Some carriers make up more time than others in flight, but the differences are relatively small. 
The average time gains between the best and worst airlines is only six minutes. 
The best predictor of time gained is not carrier but flight distance. 
The biggest gains were associated with the longest flights.
<h2>Share Insights</h2>
This simple linear model contains a wealth of detailed information about carriers, distances traveled, and flight delays. 
These detailed insights can be conveyed to a non-technical audiance via an interactive <a href="http://rmarkdown.rstudio.com/flexdashboard/index.html">flexdashboard</a>.
<h2>Build dashboard</h2>
Aggregate the scored data by origin, destination, and airline. 
Save the aggregated data.
<code> # Summarize by origin, destination, and carrier
summary_2008 &lt;- sdf_predict(ml1, data_2008) %&gt;%
  rename(carrier = uniquecarrier, airline = description) %&gt;%
  group_by(origin, dest, carrier, airline) %&gt;%
  summarize(
    flights = n(),
    distance = mean(distance),
    avg_dep_delay = mean(depdelay),
    avg_arr_delay = mean(arrdelay),
    avg_gain = mean(gain),
    pred_gain = mean(prediction)
    )

# Collect and save objects
pred_data &lt;- collect(summary_2008)
airports &lt;- collect(select(airports_tbl, name, faa, lat, lon))
ml1_summary &lt;- capture.output(summary(ml1))
save(pred_data, airports, ml1_summary, file = &#39;flights_pred_2008.RData&#39;)</code>
<h2>Publish dashboard</h2>
Use the saved data to build an R Markdown <a href="http://rmarkdown.rstudio.com/flexdashboard/index.html">flexdashboard</a>. 
Publish the flexdashboard to <a href="https://www.rstudio.com/products/shiny-server-pro/">Shiny Server</a>, <a href="https://www.rstudio.com/products/shinyapps/">Shinyapps.io</a> or <a href="https://www.rstudio.com/products/connect/">RStudio Connect</a>.

<a href="https://beta.rstudioconnect.com/content/1439/">
<img src="images/deployment/amazon-emr/flightsDashboard.png" width=600/>
</a>
<h2>Using sparklyr with an Apache Spark cluster </h2>
<h2>Summary</h2>
This document demonstrates how to use <code>sparklyr</code> with an Cloudera Hadoop &amp; Spark cluster. 
Data are downloaded from the web and stored in Hive tables on HDFS across multiple worker nodes. 
RStudio Server is installed on the master node and orchestrates the analysis in spark.
<h2>Cloudera Cluster</h2>
This demonstration is focused on adding RStudio integration to an existing Cloudera cluster. 
The assumption will be made that there no aid is needed to setup and administer the cluster.

##CDH 5

We will start with a Cloudera cluster CDH version 5.8.2 (free version) with an underlaying Ubuntu Linux distribution.

<img src="images/deployment/cdh/manager-landing-page.png" width=600/>

##Spark 1.6

The default Spark 1.6.0 parcel is in installed and running

<img src="images/deployment/cdh/spark-history-server-1.png" width=600/>
<h2>Hive data</h2>
For this demo, we have created and populated 3 tables in Hive. 
The table names are: flights, airlines and airports. 
Using Hue, we can see the loaded tables. 
For the links to the data files and their Hive import scripts please see <em>Appendix A</em>.

<img src="images/deployment/cdh/hue-metastore-1.png" width=600/>
<h2>Install RStudio</h2>
The latest version of R is needed. 
In Ubuntu, the default core R is not the latest so we have to update the source list. 
We will also install a few other dependencies.
<code> sudo sh -c &#39;echo &quot;deb http://cran.rstudio.com/bin/linux/ubuntu trusty/&quot; &gt;&gt; /etc/apt/sources.list&#39;
gpg --keyserver keyserver.ubuntu.com --recv-key 0x517166190x51716619e084dab9
gpg -a --export 0x517166190x51716619e084dab9 | sudo apt-key add -
sudo apt-get update
sudo apt-get install r-base
sudo apt-get install gdebi-core
sudo apt-get -y install libcurl4-gnutls-dev
sudo apt-get -y install libssl-dev</code>

We will install the preview version of RStudio Server
<code> wget https://s3.amazonaws.com/rstudio-dailybuilds/rstudio-server-1.0.40-amd64.deb
sudo gdebi rstudio-server-1.0.49-amd64.deb</code>

<h3>Create and configure a User</h3>
Create a user called <code>rstudio</code> that will perform the data analysis.
<code> sudo adduser rstudio</code>

To ease security restriction in this demo, we will add the new user to the default super group defined in the <strong>dfs.permissions.superusergroup</strong> setting in CDH
<code> sudo groupadd supergroup
sudo usermod -a -G supergroup rstudio</code>
<h2>Connect to Spark</h2>
Log in to RStudio Server by pointing a browser at your master node IP:8787.

<img src="images/deployment/cdh/sign-in-1.png" width=600/>

Set the environment variable <code>SPARK_HOME</code> and then run <code>spark_connect</code>. 
After connecting you will be able to browse the Hive metadata in the RStudio Server Spark pane.
<code> library(sparklyr)
library(dplyr)
library(ggplot2)

sc &lt;- spark_connect(master = &quot;yarn-client&quot;, version=&quot;1.6.0&quot;, spark_home = &#39;/opt/cloudera/parcels/CDH/lib/spark/&#39;)</code>

Once you are connected, you will see the Spark pane appear along with your hive tables.

<img src="images/deployment/cdh/spark-pane-1.png" width=600/>

You can inspect your tables by clicking on the data icon.

<img src="images/deployment/cdh/tables-1.png" width=600/>

This is what the tables look like loaded in Spark via the History Server Web UI (port 18088)

<img src="images/deployment/cdh/spark-rdd-1.png" width=600/>
<h2>Data analysis</h2>
Is there evidence to suggest that some airline carriers make up time in flight? This analysis predicts time gained in flight by airline carrier.

<img src="images/deployment/cdh/data-analysis-1.png" width=600/>
<h2>Cache the tables into memory</h2>
Use <code>tbl_cache</code> to load the flights table into memory. 
Caching tables will make analysis much faster. 
Create a dplyr reference to the Spark DataFrame.
<code> # Cache flights Hive table into Spark
tbl_cache(sc, &#39;flights&#39;)
flights_tbl &lt;- tbl(sc, &#39;flights&#39;)

# Cache airlines Hive table into Spark
tbl_cache(sc, &#39;airlines&#39;)
airlines_tbl &lt;- tbl(sc, &#39;airlines&#39;)

# Cache airports Hive table into Spark
tbl_cache(sc, &#39;airports&#39;)
airports_tbl &lt;- tbl(sc, &#39;airports&#39;)</code>
<h2>Create a model data set</h2>
Filter the data to contain only the records to be used in the fitted model. 
Join carrier descriptions for reference. 
Create a new variable called <code>gain</code> which represents the amount of time gained (or lost) in flight.
<code> # Filter records and create target variable &#39;gain&#39;
model_data &lt;- flights_tbl %&gt;%
  filter(!is.na(arrdelay) &amp; !is.na(depdelay) &amp; !is.na(distance)) %&gt;%
  filter(depdelay &gt; 15 &amp; depdelay &lt; 240) %&gt;%
  filter(arrdelay &gt; -60 &amp; arrdelay &lt; 360) %&gt;%
  filter(year &gt;= 2003 &amp; year &lt;= 2007) %&gt;%
  left_join(airlines_tbl, by = c(&quot;uniquecarrier&quot; = &quot;code&quot;)) %&gt;%
  mutate(gain = depdelay - arrdelay) %&gt;%
  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain)

# Summarize data by carrier
model_data %&gt;%
  group_by(uniquecarrier) %&gt;%
  summarize(description = min(description), gain=mean(gain), 
    distance=mean(distance), depdelay=mean(depdelay)) %&gt;%
  select(description, gain, distance, depdelay) %&gt;%
  arrange(gain)</code>
<code> Source:   query [?? x 4]
Database: spark connection master=yarn-client app=sparklyr local=FALSE
  description       gain  distance depdelay
            &lt;chr&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
1        ATA Airlines d/b/a ATA -5.5679651 1240.7219 61.84391
2       Northwest Airlines Inc. 
-3.1134556  779.1926 48.84979
3                     Envoy Air -2.2056576  437.0883 54.54923
4             PSA Airlines Inc. 
-1.9267647  500.6955 55.60335
5  ExpressJet Airlines Inc. 
(1) -1.5886314  537.3077 61.58386
6               JetBlue Airways -1.3742524 1087.2337 59.80750
7         SkyWest Airlines Inc. 
-1.1265678  419.6489 54.04198
8          Delta Air Lines Inc. 
-0.9829374  956.9576 50.19338
9        American Airlines Inc. 
-0.9631200 1066.8396 56.78222
10  AirTran Airways Corporation -0.9411572  665.6574 53.38363
# ... 
with more rows</code>
<h2>Train a linear model</h2>
Predict time gained or lost in flight as a function of distance, departure delay, and airline carrier.
<code> # Partition the data into training and validation sets
model_partition &lt;- model_data %&gt;% 
  sdf_partition(train = 0.8, valid = 0.2, seed = 5555)

# Fit a linear model
ml1 &lt;- model_partition$train %&gt;%
  ml_linear_regression(gain ~ distance + depdelay + uniquecarrier)

# Summarize the linear model
summary(ml1)</code>
<code> Call: ml_linear_regression(., gain ~ distance + depdelay + uniquecarrier)

Deviance Residuals: (approximate):
     Min       1Q   Median       3Q      Max 
-302.343   -5.669    2.714    9.832  104.130 

Coefficients:
      Estimate  Std. 
Error  t value  Pr(&gt;|t|)    
(Intercept)      -1.26566581  0.10385870 -12.1864 &lt; 2.2e-16 ***
distance          0.00308711  0.00002404 128.4155 &lt; 2.2e-16 ***
depdelay         -0.01397013  0.00028816 -48.4812 &lt; 2.2e-16 ***
uniquecarrier_AA -2.18483090  0.10985406 -19.8885 &lt; 2.2e-16 ***
uniquecarrier_AQ  3.14330242  0.29114487  10.7964 &lt; 2.2e-16 ***
uniquecarrier_AS  0.09210380  0.12825003   0.7182 0.4726598    
uniquecarrier_B6 -2.66988794  0.12682192 -21.0523 &lt; 2.2e-16 ***
uniquecarrier_CO -1.11611186  0.11795564  -9.4621 &lt; 2.2e-16 ***
uniquecarrier_DL -1.95206198  0.11431110 -17.0767 &lt; 2.2e-16 ***
uniquecarrier_EV  1.70420830  0.11337215  15.0320 &lt; 2.2e-16 ***
uniquecarrier_F9 -1.03178176  0.15384863  -6.7065 1.994e-11 ***
uniquecarrier_FL -0.99574060  0.12034738  -8.2739 2.220e-16 ***
uniquecarrier_HA -1.16970713  0.34894788  -3.3521 0.0008020 ***
uniquecarrier_MQ -1.55569040  0.10975613 -14.1741 &lt; 2.2e-16 ***
uniquecarrier_NW -3.58502418  0.11534938 -31.0797 &lt; 2.2e-16 ***
uniquecarrier_OH -1.40654797  0.12034858 -11.6873 &lt; 2.2e-16 ***
uniquecarrier_OO -0.39069404  0.11132164  -3.5096 0.0004488 ***
uniquecarrier_TZ -7.26285217  0.34428509 -21.0955 &lt; 2.2e-16 ***
uniquecarrier_UA -0.56995737  0.11186757  -5.0949 3.489e-07 ***
uniquecarrier_US -0.52000028  0.11218498  -4.6352 3.566e-06 ***
uniquecarrier_WN  4.22838982  0.10629405  39.7801 &lt; 2.2e-16 ***
uniquecarrier_XE -1.13836940  0.11332176 -10.0455 &lt; 2.2e-16 ***
uniquecarrier_YV  3.17149538  0.11709253  27.0854 &lt; 2.2e-16 ***
---
Signif. 
codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

R-Squared: 0.02301
Root Mean Squared Error: 17.83</code>
<h2>Assess model performance</h2>
Compare the model performance using the validation data.
<code> # Calculate average gains by predicted decile
model_deciles &lt;- lapply(model_partition, function(x) {
  sdf_predict(ml1, x) %&gt;%
    mutate(decile = ntile(desc(prediction), 10)) %&gt;%
    group_by(decile) %&gt;%
    summarize(gain = mean(gain)) %&gt;%
    select(decile, gain) %&gt;%
    collect()
})

# Create a summary dataset for plotting
deciles &lt;- rbind(
  data.frame(data = &#39;train&#39;, model_deciles$train),
  data.frame(data = &#39;valid&#39;, model_deciles$valid),
  make.row.names = FALSE
)

# Plot average gains by predicted decile
deciles %&gt;%
  ggplot(aes(factor(decile), gain, fill = data)) +
  geom_bar(stat = &#39;identity&#39;, position = &#39;dodge&#39;) +
  labs(title = &#39;Average gain by predicted decile&#39;, x = &#39;Decile&#39;, y = &#39;Minutes&#39;)</code>

<img src="images/deployment/cdh/performance-1.png" width=600/>
<h2>Visualize predictions</h2>
Compare actual gains to predicted gains for an out of time sample.
<code> # Select data from an out of time sample
data_2008 &lt;- flights_tbl %&gt;%
  filter(!is.na(arrdelay) &amp; !is.na(depdelay) &amp; !is.na(distance)) %&gt;%
  filter(depdelay &gt; 15 &amp; depdelay &lt; 240) %&gt;%
  filter(arrdelay &gt; -60 &amp; arrdelay &lt; 360) %&gt;%
  filter(year == 2008) %&gt;%
  left_join(airlines_tbl, by = c(&quot;uniquecarrier&quot; = &quot;code&quot;)) %&gt;%
  mutate(gain = depdelay - arrdelay) %&gt;%
  select(year, month, arrdelay, depdelay, distance, uniquecarrier, description, gain, origin,dest)

# Summarize data by carrier
carrier &lt;- sdf_predict(ml1, data_2008) %&gt;%
  group_by(description) %&gt;%
  summarize(gain = mean(gain), prediction = mean(prediction), freq = n()) %&gt;%
  filter(freq &gt; 10000) %&gt;%
  collect

# Plot actual gains and predicted gains by airline carrier
ggplot(carrier, aes(gain, prediction)) + 
  geom_point(alpha = 0.75, color = &#39;red&#39;, shape = 3) +
  geom_abline(intercept = 0, slope = 1, alpha = 0.15, color = &#39;blue&#39;) +
  geom_text(aes(label = substr(description, 1, 20)), size = 3, alpha = 0.75, vjust = -1) +
  labs(title=&#39;Average Gains Forecast&#39;, x = &#39;Actual&#39;, y = &#39;Predicted&#39;)</code>

<img src="images/deployment/cdh/forecast-1.png" width=600/>

Some carriers make up more time than others in flight, but the differences are relatively small. 
The average time gains between the best and worst airlines is only six minutes. 
The best predictor of time gained is not carrier but flight distance. 
The biggest gains were associated with the longest flights.
<h2>Share Insights</h2>
This simple linear model contains a wealth of detailed information about carriers, distances traveled, and flight delays. 
These detailed insights can be conveyed to a non-technical audiance via an interactive <a href="http://rmarkdown.rstudio.com/flexdashboard/index.html">flexdashboard</a>.
<h2>Build dashboard</h2>
Aggregate the scored data by origin, destination, and airline. 
Save the aggregated data.
<code> # Summarize by origin, destination, and carrier
summary_2008 &lt;- sdf_predict(ml1, data_2008) %&gt;%
  rename(carrier = uniquecarrier, airline = description) %&gt;%
  group_by(origin, dest, carrier, airline) %&gt;%
  summarize(
    flights = n(),
    distance = mean(distance),
    avg_dep_delay = mean(depdelay),
    avg_arr_delay = mean(arrdelay),
    avg_gain = mean(gain),
    pred_gain = mean(prediction)
    )

# Collect and save objects
pred_data &lt;- collect(summary_2008)
airports &lt;- collect(select(airports_tbl, name, faa, lat, lon))
ml1_summary &lt;- capture.output(summary(ml1))
save(pred_data, airports, ml1_summary, file = &#39;flights_pred_2008.RData&#39;)</code>
<h2>Publish dashboard</h2>
Use the saved data to build an R Markdown <a href="http://rmarkdown.rstudio.com/flexdashboard/index.html">flexdashboard</a>. 
Publish the flexdashboard

<img src="images/deployment/cdh/flex-1.png" width=600/>

#Appendix

<h3>Appendix A - Data files</h3>
Run the following script to download data from the web onto your master node. 
Download the yearly flight data and the airlines lookup table.
<code> # Make download directory
mkdir /tmp/flights

# Download flight data by year
for i in {2006..2008}
  do
    echo &quot;$(date) $i Download&quot;
    fnam=$i.csv.bz2
    wget -O /tmp/flights/$fnam http://stat-computing.org/dataexpo/2009/$fnam
    echo &quot;$(date) $i Unzip&quot;
    bunzip2 /tmp/flights/$fnam
  done

# Download airline carrier data
wget -O /tmp/airlines.csv http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_UNIQUE_CARRIERS

# Download airports data
wget -O /tmp/airports.csv https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat</code>

<h3>Hive tables</h3>
We used the Hue interface, logged in as ‘admin’ to load the data into HDFS and then into Hive.
<code> CREATE EXTERNAL TABLE IF NOT EXISTS flights
(
year int,
month int,
dayofmonth int,
dayofweek int,
deptime int,
crsdeptime int,
arrtime int, 
crsarrtime int,
uniquecarrier string,
flightnum int,
tailnum string, 
actualelapsedtime int,
crselapsedtime int,
airtime string,
arrdelay int,
depdelay int, 
origin string,
dest string,
distance int,
taxiin string,
taxiout string,
cancelled int,
cancellationcode string,
diverted int,
carrierdelay string,
weatherdelay string,
nasdelay string,
securitydelay string,
lateaircraftdelay string
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY &#39;,&#39;
LINES TERMINATED BY &#39;\n&#39;
TBLPROPERTIES(&quot;skip.header.line.count&quot;=&quot;1&quot;);</code>
<code> LOAD DATA INPATH &#39;/user/admin/flights/2006.csv/&#39; INTO TABLE flights;
LOAD DATA INPATH &#39;/user/admin/flights/2007.csv/&#39; INTO TABLE flights;
LOAD DATA INPATH &#39;/user/admin/flights/2008.csv/&#39; INTO TABLE flights;</code>
<code> # Create metadata for airlines
CREATE EXTERNAL TABLE IF NOT EXISTS airlines
(
Code string,
Description string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;
WITH SERDEPROPERTIES
(
&quot;separatorChar&quot; = &#39;\,&#39;,
&quot;quoteChar&quot;     = &#39;\&quot;&#39;
)
STORED AS TEXTFILE
tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;);</code>
<code> LOAD DATA INPATH &#39;/user/admin/L_UNIQUE_CARRIERS.csv&#39; INTO TABLE airlines;</code>
<code> CREATE EXTERNAL TABLE IF NOT EXISTS airports
(
id string,
name string,
city string,
country string,
faa string,
icao string,
lat double,
lon double,
alt int,
tz_offset double,
dst string,
tz_name string
)
ROW FORMAT SERDE &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;
WITH SERDEPROPERTIES
(
&quot;separatorChar&quot; = &#39;\,&#39;,
&quot;quoteChar&quot;     = &#39;\&quot;&#39;
)
STORED AS TEXTFILE;</code>
<code> LOAD DATA INPATH &#39;/user/admin/airports.dat&#39; INTO TABLE airports;</code>
<h2>Using sparklyr with Databricks </h2>
<h2>Overview</h2>
This documentation demonstrates how to use <code>sparklyr</code> with Apache Spark in
Databricks along with RStudio Team, RStudio Server Pro, RStudio Connect, and
RStudio Package Manager.
<h2>Using RStudio Team with Databricks</h2>
RStudio Team is a bundle of our popular professional software for developing
data science projects, publishing data products, and managing packages.

RStudio Team and <code>sparklyr</code> can be used with Databricks to work with large
datasets and distributed computations with Apache Spark. 
The most common use
case is to perform interactive analysis and exploratory development with RStudio
Server Pro and <code>sparklyr</code>; write out the results to a database, file system, or
cloud storage; then publish apps, reports, and APIs to RStudio Connect that
query and access the results.

<img src="/images/deployment/databricks/rstudio-databricks-architecture.png" width='800px' align='center'/>

The sections below describe best practices and different options for configuring
specific RStudio products to work with Databricks.
<h2>Best practices for working with Databricks</h2>
<strong>Maintain separate installation environments</strong> - Install RStudio Server Pro,
RStudio Connect, and RStudio Package Manager outside of the Databricks cluster
so that they are not limited to the compute resources or ephemeral nature of
Databricks clusters.
<strong>Connect to Databricks remotely</strong> - Work with Databricks as a remote compute
resource, similar to how you would connect remotely to external databases,
data sources, and storage systems. 
This can be accomplished using Databricks
Connect (as described in the
<a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a>
section below) or by performing SQL queries with JDBC/ODBC using the
Databricks Spark SQL Driver on
<a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html">AWS</a> or
<a href="https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/jdbc-odbc-bi">Azure</a>.
<strong>Restrict workloads to interactive analysis</strong> - Only perform workloads
related to exploratory or interactive analysis with Spark, then write the
results to a database, file system, or cloud storage for more efficient
retrieval in apps, reports, and APIs.
<strong>Load and query results efficiently</strong> - Because of the nature of Spark
computations and the associated overhead, Shiny apps that use Spark on the
backend tend to have performance and runtime issues; consider reading the
results from a database, file system, or cloud storage instead.
<h2>Using RStudio Server Pro with Databricks</h2>
There are two options for using <code>sparklyr</code> and RStudio Server Pro with
Databricks:

Option 1:
<a href="#option-1---connecting-to-databricks-remotely">Connecting to Databricks remotely</a>
(Recommended Option)
Option 2:
<a href="#option-2---working-inside-of-databricks">Working inside of Databricks</a>
(Alternative Option)

<h3>Option 1 - Connecting to Databricks remotely</h3>
With this configuration, RStudio Server Pro is installed outside of the Spark
cluster and allows users to connect to Spark remotely using <code>sparklyr</code> with
<a href="https://docs.databricks.com/dev-tools/databricks-connect.html">Databricks Connect</a>.

This is the recommended configuration because it targets separate environments,
involves a typical configuration process, avoids resource contention, and allows
RStudio Server Pro to connect to Databricks as well as other remote storage and
compute resources.

<a href="/examples/databricks-cluster-remote">
<h2>
View steps for connecting to Databricks remotely</h2>
</a>

<a href="/examples/databricks-cluster-remote">
<img src="/images/deployment/databricks/rstudio-databricks-remote.png" width='800px' align='center'/>
</a>

<h3>Option 2 - Working inside of Databricks</h3>
If you cannot work with Spark remotely, you should install RStudio Server Pro on
the Driver node of a long-running, persistent Databricks cluster as opposed to a
worker node or an ephemeral cluster.

With this configuration, RStudio Server Pro is installed on the Spark driver
node and allows users to connect to Spark locally using <code>sparklyr</code>.

This configuration can result in increased complexity, limited connectivity to
other storage and compute resources, resource contention between RStudio Server
Pro and Databricks, and maintenance concerns due to the ephemeral nature of
Databricks clusters.

<a href="/examples/databricks-cluster-local">
<h2>
View steps for working inside of Databricks</h2>
</a>

<a href="/examples/databricks-cluster-local">
<img src="/images/deployment/databricks/rstudio-databricks-local.png" width='800px' align='center'/>
</a>
<h2>Using RStudio Connect with Databricks</h2>
The server environment within Databricks clusters is not permissive enough to
support RStudio Connect or the process sandboxing mechanisms that it uses to
isolate published content.

Therefore, the only supported configuration is to install RStudio Connect
outside of the Databricks cluster and connect to Databricks remotely.

Whether RStudio Server Pro is installed outside of the Databricks cluster
(Recommended Option) or within the Databricks cluster (Alternative Option), you
can publish content to RStudio Connect as long as HTTP/HTTPS network traffic is
allowed from RStudio Server Pro to RStudio Connect.

There are two options for using RStudio Connect with Databricks:

<ol style="list-style-type: decimal">
Performing SQL queries with JDBC/ODBC using the Databricks Spark SQL Driver
on <a href="https://docs.databricks.com/integrations/bi/jdbc-odbc-bi.html">AWS</a> or
<a href="https://docs.microsoft.com/en-us/azure/databricks/integrations/bi/jdbc-odbc-bi">Azure</a>
(Recommended Option)
Adding calls in your R code to create and run Databricks jobs
<a href="https://github.com/RafiKurlansik/bricksteR/">with bricksteR and the Databricks Jobs API</a>
(Alternative Option)
</ol>
<img src="/images/deployment/databricks/rstudio-connect-databricks.png" width='800px' align='center'/>
<h2>Using RStudio Package Manager with Databricks</h2>
Whether RStudio Server Pro is installed outside of the Databricks cluster
(Recommended Option) or within the Databricks cluster (Alternative Option), you
can install packages from repositories in RStudio Package Manager as long as
HTTP/HTTPS network traffic is allowed from RStudio Server Pro to RStudio Package
Manager.
<h2>Development </h2>
<h2 class = "content-header">Function Reference</h2>
<table >

<colgroup>
<col class="alias" />
<col class="title" />
</colgroup>

<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-spark-operations" class="hasAnchor">Spark Operations</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_config">spark_config()</a></code> </td>
    <td>Read Spark Configuration</td></tr>
  <tr><td width = "90%"><code> <a href="spark-connections">spark_connect()</a></code> <code><a href="spark-connections">spark_connection_is_open()</a></code> <code><a href="spark-connections">spark_disconnect()</a></code> <code><a href="spark-connections">spark_disconnect_all()</a></code> <code><a href="spark-connections">spark_submit()</a></code> </td>
    <td>Manage Spark Connections</td></tr>
  <tr><td width = "90%"><code> <a href="spark_install">spark_install_find()</a></code> <code><a href="spark_install">spark_install()</a></code> <code><a href="spark_install">spark_uninstall()</a></code> <code><a href="spark_install">spark_install_dir()</a></code> <code><a href="spark_install">spark_install_tar()</a></code> <code><a href="spark_install">spark_installed_versions()</a></code> <code><a href="spark_install">spark_available_versions()</a></code> </td>
    <td>Find a given Spark installation by version.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_log">spark_log()</a></code> </td>
    <td>View Entries in the Spark Log</td></tr>
  <tr><td width = "90%"><code> <a href="spark_web">spark_web()</a></code> </td>
    <td>Open the Spark web interface</td></tr>
  <tr><td width = "90%"><code> <a href="connection_is_open">connection_is_open()</a></code> </td>
    <td>Check whether the connection is open</td></tr>
  <tr><td width = "90%"><code> <a href="connection_spark_shinyapp">connection_spark_shinyapp()</a></code> </td>
    <td>A Shiny app that can be used to construct a <code>spark_connect</code> statement</td></tr>
  <tr><td width = "90%"><code> <a href="spark_configuration">spark_session_config()</a></code> </td>
    <td>Runtime configuration interface for the Spark Session</td></tr>
  <tr><td width = "90%"><code> <a href="checkpoint_directory">spark_set_checkpoint_dir()</a></code> <code><a href="checkpoint_directory">spark_get_checkpoint_dir()</a></code> </td>
    <td>Set/Get Spark checkpoint directory</td></tr>
  <tr><td width = "90%"><code> <a href="spark_table_name">spark_table_name()</a></code> </td>
    <td>Generate a Table Name from Expression</td></tr>
  <tr><td width = "90%"><code> <a href="spark_version_from_home">spark_version_from_home()</a></code> </td>
    <td>Get the Spark Version Associated with a Spark Installation</td></tr>
  <tr><td width = "90%"><code> <a href="spark_versions">spark_versions()</a></code> </td>
    <td>Retrieves a dataframe available Spark versions that van be installed.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_config_kubernetes">spark_config_kubernetes()</a></code> </td>
    <td>Kubernetes Configuration</td></tr>
  <tr><td width = "90%"><code> <a href="spark_config_settings">spark_config_settings()</a></code> </td>
    <td>Retrieve Available Settings</td></tr>
  <tr><td width = "90%"><code> <a href="spark_connection_find">spark_connection_find()</a></code> </td>
    <td>Find Spark Connection</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dependency_fallback">spark_dependency_fallback()</a></code> </td>
    <td>Fallback to Spark Dependency</td></tr>
  <tr><td width = "90%"><code> <a href="spark_extension">spark_extension()</a></code> </td>
    <td>Create Spark Extension</td></tr>
  <tr><td width = "90%"><code> <a href="spark_load_table">spark_load_table()</a></code> </td>
    <td>Reads from a Spark Table into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_libsvm">spark_read_libsvm()</a></code> </td>
    <td>Read libsvm file into a Spark DataFrame.</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-read-write-data" class="hasAnchor">Read/Write data</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_read_csv">spark_read_csv()</a></code> </td>
    <td>Read a CSV file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_delta">spark_read_delta()</a></code> </td>
    <td>Read from Delta Lake into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_jdbc">spark_read_jdbc()</a></code> </td>
    <td>Read from JDBC connection into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_json">spark_read_json()</a></code> </td>
    <td>Read a JSON file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_libsvm">spark_read_libsvm()</a></code> </td>
    <td>Read libsvm file into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_orc">spark_read_orc()</a></code> </td>
    <td>Read a ORC file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_parquet">spark_read_parquet()</a></code> </td>
    <td>Read a Parquet file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_source">spark_read_source()</a></code> </td>
    <td>Read from a generic source into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_table">spark_read_table()</a></code> </td>
    <td>Reads from a Spark Table into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_text">spark_read_text()</a></code> </td>
    <td>Read a Text file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_csv">spark_write_csv()</a></code> </td>
    <td>Write a Spark DataFrame to a CSV</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_delta">spark_write_delta()</a></code> </td>
    <td>Writes a Spark DataFrame into Delta Lake</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_jdbc">spark_write_jdbc()</a></code> </td>
    <td>Writes a Spark DataFrame into a JDBC table</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_json">spark_write_json()</a></code> </td>
    <td>Write a Spark DataFrame to a JSON file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_orc">spark_write_orc()</a></code> </td>
    <td>Write a Spark DataFrame to a ORC file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_parquet">spark_write_parquet()</a></code> </td>
    <td>Write a Spark DataFrame to a Parquet file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_source">spark_write_source()</a></code> </td>
    <td>Writes a Spark DataFrame into a generic source</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_table">spark_write_table()</a></code> </td>
    <td>Writes a Spark DataFrame into a Spark table</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_text">spark_write_text()</a></code> </td>
    <td>Write a Spark DataFrame to a Text file</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-spark-tables" class="hasAnchor">Spark Tables</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="sdf-saveload">sdf_save_table()</a></code> <code><a href="sdf-saveload">sdf_load_table()</a></code> <code><a href="sdf-saveload">sdf_save_parquet()</a></code> <code><a href="sdf-saveload">sdf_load_parquet()</a></code> </td>
    <td>Save / Load a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf-transform-methods">sdf_predict()</a></code> <code><a href="sdf-transform-methods">sdf_transform()</a></code> <code><a href="sdf-transform-methods">sdf_fit()</a></code> <code><a href="sdf-transform-methods">sdf_fit_and_transform()</a></code> </td>
    <td>Spark ML -- Transform, fit, and predict methods (sdf_ interface)</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_along">sdf_along()</a></code> </td>
    <td>Create DataFrame for along Object</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_bind">sdf_bind_rows()</a></code> <code><a href="sdf_bind">sdf_bind_cols()</a></code> </td>
    <td>Bind multiple Spark DataFrames by row and column</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_broadcast">sdf_broadcast()</a></code> </td>
    <td>Broadcast hint</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_checkpoint">sdf_checkpoint()</a></code> </td>
    <td>Checkpoint a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_coalesce">sdf_coalesce()</a></code> </td>
    <td>Coalesces a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_collect">sdf_collect()</a></code> </td>
    <td>Collect a Spark DataFrame into R.</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_copy_to">sdf_copy_to()</a></code> <code><a href="sdf_copy_to">sdf_import()</a></code> </td>
    <td>Copy an Object into Spark</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_crosstab">sdf_crosstab()</a></code> </td>
    <td>Cross Tabulation</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_debug_string">sdf_debug_string()</a></code> </td>
    <td>Debug Info for Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_describe">sdf_describe()</a></code> </td>
    <td>Compute summary statistics for columns of a data frame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_dim">sdf_dim()</a></code> <code><a href="sdf_dim">sdf_nrow()</a></code> <code><a href="sdf_dim">sdf_ncol()</a></code> </td>
    <td>Support for Dimension Operations</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_is_streaming">sdf_is_streaming()</a></code> </td>
    <td>Spark DataFrame is Streaming</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_last_index">sdf_last_index()</a></code> </td>
    <td>Returns the last index of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_len">sdf_len()</a></code> </td>
    <td>Create DataFrame for Length</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_num_partitions">sdf_num_partitions()</a></code> </td>
    <td>Gets number of partitions of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_persist">sdf_persist()</a></code> </td>
    <td>Persist a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_pivot">sdf_pivot()</a></code> </td>
    <td>Pivot a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_project">sdf_project()</a></code> </td>
    <td>Project features onto principal components</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_quantile">sdf_quantile()</a></code> </td>
    <td>Compute (Approximate) Quantiles with a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_random_split">sdf_random_split()</a></code> <code><a href="sdf_random_split">sdf_partition()</a></code> </td>
    <td>Partition a Spark Dataframe</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_read_column">sdf_read_column()</a></code> </td>
    <td>Read a Column from a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_register">sdf_register()</a></code> </td>
    <td>Register a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_repartition">sdf_repartition()</a></code> </td>
    <td>Repartition a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_residuals">sdf_residuals()</a></code> </td>
    <td>Model Residuals</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sample">sdf_sample()</a></code> </td>
    <td>Randomly Sample Rows from a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_schema">sdf_schema()</a></code> </td>
    <td>Read the Schema of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_separate_column">sdf_separate_column()</a></code> </td>
    <td>Separate a Vector Column into Scalar Columns</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_seq">sdf_seq()</a></code> </td>
    <td>Create DataFrame for Range</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sort">sdf_sort()</a></code> </td>
    <td>Sort a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sql">sdf_sql()</a></code> </td>
    <td>Spark DataFrame from SQL</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_with_sequential_id">sdf_with_sequential_id()</a></code> </td>
    <td>Add a Sequential ID Column to a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_with_unique_id">sdf_with_unique_id()</a></code> </td>
    <td>Add a Unique ID Column to a Spark DataFrame</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-spark-machine-learning" class="hasAnchor">Spark Machine Learning</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="ml_decision_tree">ml_decision_tree_classifier()</a></code> <code><a href="ml_decision_tree">ml_decision_tree()</a></code> <code><a href="ml_decision_tree">ml_decision_tree_regressor()</a></code> </td>
    <td>Spark ML -- Decision Trees</td></tr>
  <tr><td width = "90%"><code> <a href="ml_generalized_linear_regression">ml_generalized_linear_regression()</a></code> </td>
    <td>Spark ML -- Generalized Linear Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_gradient_boosted_trees">ml_gbt_classifier()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gradient_boosted_trees()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gbt_regressor()</a></code> </td>
    <td>Spark ML -- Gradient Boosted Trees</td></tr>
  <tr><td width = "90%"><code> <a href="ml_kmeans">ml_kmeans()</a></code> <code><a href="ml_kmeans">ml_compute_cost()</a></code> </td>
    <td>Spark ML -- K-Means Clustering</td></tr>
  <tr><td width = "90%"><code> <a href="ml_lda">ml_lda()</a></code> <code><a href="ml_lda">ml_describe_topics()</a></code> <code><a href="ml_lda">ml_log_likelihood()</a></code> <code><a href="ml_lda">ml_log_perplexity()</a></code> <code><a href="ml_lda">ml_topics_matrix()</a></code> </td>
    <td>Spark ML -- Latent Dirichlet Allocation</td></tr>
  <tr><td width = "90%"><code> <a href="ml_linear_regression">ml_linear_regression()</a></code> </td>
    <td>Spark ML -- Linear Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_logistic_regression">ml_logistic_regression()</a></code> </td>
    <td>Spark ML -- Logistic Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_model_data">ml_model_data()</a></code> </td>
    <td>Extracts data associated with a Spark ML model</td></tr>
  <tr><td width = "90%"><code> <a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier()</a></code> <code><a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron()</a></code> </td>
    <td>Spark ML -- Multilayer Perceptron</td></tr>
  <tr><td width = "90%"><code> <a href="ml_naive_bayes">ml_naive_bayes()</a></code> </td>
    <td>Spark ML -- Naive-Bayes</td></tr>
  <tr><td width = "90%"><code> <a href="ml_one_vs_rest">ml_one_vs_rest()</a></code> </td>
    <td>Spark ML -- OneVsRest</td></tr>
  <tr><td width = "90%"><code> <a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </td>
    <td>Feature Transformation -- PCA (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_random_forest">ml_random_forest_classifier()</a></code> <code><a href="ml_random_forest">ml_random_forest()</a></code> <code><a href="ml_random_forest">ml_random_forest_regressor()</a></code> </td>
    <td>Spark ML -- Random Forest</td></tr>
  <tr><td width = "90%"><code> <a href="ml_aft_survival_regression">ml_aft_survival_regression()</a></code> <code><a href="ml_aft_survival_regression">ml_survival_regression()</a></code> </td>
    <td>Spark ML -- Survival Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_add_stage">ml_add_stage()</a></code> </td>
    <td>Add a Stage to a Pipeline</td></tr>
  <tr><td width = "90%"><code> <a href="ml_als">ml_als()</a></code> <code><a href="ml_als">ml_recommend()</a></code> </td>
    <td>Spark ML -- ALS</td></tr>
  <tr><td width = "90%"><code> <a href="ft_lsh_utils">ml_approx_nearest_neighbors()</a></code> <code><a href="ft_lsh_utils">ml_approx_similarity_join()</a></code> </td>
    <td>Utility functions for LSH models</td></tr>
  <tr><td width = "90%"><code> <a href="ml_fpgrowth">ml_fpgrowth()</a></code> <code><a href="ml_fpgrowth">ml_association_rules()</a></code> <code><a href="ml_fpgrowth">ml_freq_itemsets()</a></code> </td>
    <td>Frequent Pattern Mining -- FPGrowth</td></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </td>
    <td>Spark ML - Evaluators</td></tr>
  <tr><td width = "90%"><code> <a href="ml_bisecting_kmeans">ml_bisecting_kmeans()</a></code> </td>
    <td>Spark ML -- Bisecting K-Means Clustering</td></tr>
  <tr><td width = "90%"><code> <a href="ml_call_constructor">ml_call_constructor()</a></code> </td>
    <td>Wrap a Spark ML JVM object</td></tr>
  <tr><td width = "90%"><code> <a href="ml_chisquare_test">ml_chisquare_test()</a></code> </td>
    <td>Chi-square hypothesis testing for categorical data.</td></tr>
  <tr><td width = "90%"><code> <a href="ml_clustering_evaluator">ml_clustering_evaluator()</a></code> </td>
    <td>Spark ML - Clustering Evaluator</td></tr>
  <tr><td width = "90%"><code> <a href="ml-model-constructors">new_ml_model_prediction()</a></code> <code><a href="ml-model-constructors">new_ml_model()</a></code> <code><a href="ml-model-constructors">new_ml_model_classification()</a></code> <code><a href="ml-model-constructors">new_ml_model_regression()</a></code> <code><a href="ml-model-constructors">new_ml_model_clustering()</a></code> <code><a href="ml-model-constructors">ml_supervised_pipeline()</a></code> <code><a href="ml-model-constructors">ml_clustering_pipeline()</a></code> <code><a href="ml-model-constructors">ml_construct_model_supervised()</a></code> <code><a href="ml-model-constructors">ml_construct_model_clustering()</a></code> </td>
    <td>Constructors for `ml_model` Objects</td></tr>
  <tr><td width = "90%"><code> <a href="ml_corr">ml_corr()</a></code> </td>
    <td>Compute correlation matrix</td></tr>
  <tr><td width = "90%"><code> <a href="ml-tuning">ml_sub_models()</a></code> <code><a href="ml-tuning">ml_validation_metrics()</a></code> <code><a href="ml-tuning">ml_cross_validator()</a></code> <code><a href="ml-tuning">ml_train_validation_split()</a></code> </td>
    <td>Spark ML -- Tuning</td></tr>
  <tr><td width = "90%"><code> <a href="ml_default_stop_words">ml_default_stop_words()</a></code> </td>
    <td>Default stop words</td></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluate">ml_evaluate()</a></code> </td>
    <td>Evaluate the Model on a Validation Set</td></tr>
  <tr><td width = "90%"><code> <a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </td>
    <td>Spark ML - Feature Importance for Tree Models</td></tr>
  <tr><td width = "90%"><code> <a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </td>
    <td>Feature Transformation -- Word2Vec (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml-transform-methods">is_ml_transformer()</a></code> <code><a href="ml-transform-methods">is_ml_estimator()</a></code> <code><a href="ml-transform-methods">ml_fit()</a></code> <code><a href="ml-transform-methods">ml_transform()</a></code> <code><a href="ml-transform-methods">ml_fit_and_transform()</a></code> <code><a href="ml-transform-methods">ml_predict()</a></code> </td>
    <td>Spark ML -- Transform, fit, and predict methods (ml_ interface)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_gaussian_mixture">ml_gaussian_mixture()</a></code> </td>
    <td>Spark ML -- Gaussian Mixture clustering.</td></tr>
  <tr><td width = "90%"><code> <a href="ml-params">ml_is_set()</a></code> <code><a href="ml-params">ml_param_map()</a></code> <code><a href="ml-params">ml_param()</a></code> <code><a href="ml-params">ml_params()</a></code> </td>
    <td>Spark ML -- ML Params</td></tr>
  <tr><td width = "90%"><code> <a href="ml_isotonic_regression">ml_isotonic_regression()</a></code> </td>
    <td>Spark ML -- Isotonic Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </td>
    <td>Feature Transformation -- StringIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_linear_svc">ml_linear_svc()</a></code> </td>
    <td>Spark ML -- LinearSVC</td></tr>
  <tr><td width = "90%"><code> <a href="ml-persistence">ml_save()</a></code> <code><a href="ml-persistence">ml_load()</a></code> </td>
    <td>Spark ML -- Model Persistence</td></tr>
  <tr><td width = "90%"><code> <a href="ml_pipeline">ml_pipeline()</a></code> </td>
    <td>Spark ML -- Pipelines</td></tr>
  <tr><td width = "90%"><code> <a href="ml_stage">ml_stage()</a></code> <code><a href="ml_stage">ml_stages()</a></code> </td>
    <td>Spark ML -- Pipeline stage extraction</td></tr>
  <tr><td width = "90%"><code> <a href="ml_standardize_formula">ml_standardize_formula()</a></code> </td>
    <td>Standardize Formula Input for `ml_model`</td></tr>
  <tr><td width = "90%"><code> <a href="ml_summary">ml_summary()</a></code> </td>
    <td>Spark ML -- Extraction of summary metrics</td></tr>
  <tr><td width = "90%"><code> <a href="ml_uid">ml_uid()</a></code> </td>
    <td>Spark ML -- UID</td></tr>
  <tr><td width = "90%"><code> <a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </td>
    <td>Feature Transformation -- CountVectorizer (Estimator)</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-spark-feature-transformers" class="hasAnchor">Spark Feature Transformers</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="ft_binarizer">ft_binarizer()</a></code> </td>
    <td>Feature Transformation -- Binarizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_bucketizer">ft_bucketizer()</a></code> </td>
    <td>Feature Transformation -- Bucketizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_chisq_selector">ft_chisq_selector()</a></code> </td>
    <td>Feature Transformation -- ChiSqSelector (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </td>
    <td>Feature Transformation -- CountVectorizer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_dct">ft_dct()</a></code> <code><a href="ft_dct">ft_discrete_cosine_transform()</a></code> </td>
    <td>Feature Transformation -- Discrete Cosine Transform (DCT) (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_elementwise_product">ft_elementwise_product()</a></code> </td>
    <td>Feature Transformation -- ElementwiseProduct (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_feature_hasher">ft_feature_hasher()</a></code> </td>
    <td>Feature Transformation -- FeatureHasher (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_hashing_tf">ft_hashing_tf()</a></code> </td>
    <td>Feature Transformation -- HashingTF (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_idf">ft_idf()</a></code> </td>
    <td>Feature Transformation -- IDF (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_imputer">ft_imputer()</a></code> </td>
    <td>Feature Transformation -- Imputer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_index_to_string">ft_index_to_string()</a></code> </td>
    <td>Feature Transformation -- IndexToString (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_interaction">ft_interaction()</a></code> </td>
    <td>Feature Transformation -- Interaction (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_lsh">ft_bucketed_random_projection_lsh()</a></code> <code><a href="ft_lsh">ft_minhash_lsh()</a></code> </td>
    <td>Feature Transformation -- LSH (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_lsh_utils">ml_approx_nearest_neighbors()</a></code> <code><a href="ft_lsh_utils">ml_approx_similarity_join()</a></code> </td>
    <td>Utility functions for LSH models</td></tr>
  <tr><td width = "90%"><code> <a href="ft_max_abs_scaler">ft_max_abs_scaler()</a></code> </td>
    <td>Feature Transformation -- MaxAbsScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_min_max_scaler">ft_min_max_scaler()</a></code> </td>
    <td>Feature Transformation -- MinMaxScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_ngram">ft_ngram()</a></code> </td>
    <td>Feature Transformation -- NGram (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_normalizer">ft_normalizer()</a></code> </td>
    <td>Feature Transformation -- Normalizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_one_hot_encoder">ft_one_hot_encoder()</a></code> </td>
    <td>Feature Transformation -- OneHotEncoder (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator()</a></code> </td>
    <td>Feature Transformation -- OneHotEncoderEstimator (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </td>
    <td>Feature Transformation -- PCA (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_polynomial_expansion">ft_polynomial_expansion()</a></code> </td>
    <td>Feature Transformation -- PolynomialExpansion (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_quantile_discretizer">ft_quantile_discretizer()</a></code> </td>
    <td>Feature Transformation -- QuantileDiscretizer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_r_formula">ft_r_formula()</a></code> </td>
    <td>Feature Transformation -- RFormula (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_regex_tokenizer">ft_regex_tokenizer()</a></code> </td>
    <td>Feature Transformation -- RegexTokenizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_standard_scaler">ft_standard_scaler()</a></code> </td>
    <td>Feature Transformation -- StandardScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_stop_words_remover">ft_stop_words_remover()</a></code> </td>
    <td>Feature Transformation -- StopWordsRemover (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </td>
    <td>Feature Transformation -- StringIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_tokenizer">ft_tokenizer()</a></code> </td>
    <td>Feature Transformation -- Tokenizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_assembler">ft_vector_assembler()</a></code> </td>
    <td>Feature Transformation -- VectorAssembler (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_indexer">ft_vector_indexer()</a></code> </td>
    <td>Feature Transformation -- VectorIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_slicer">ft_vector_slicer()</a></code> </td>
    <td>Feature Transformation -- VectorSlicer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </td>
    <td>Feature Transformation -- Word2Vec (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="sql-transformer">ft_sql_transformer()</a></code> <code><a href="sql-transformer">ft_dplyr_transformer()</a></code> </td>
    <td>Feature Transformation -- SQLTransformer</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-spark-machine-learning-utilities" class="hasAnchor">Spark Machine Learning Utilities</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </td>
    <td>Spark ML - Evaluators</td></tr>
  <tr><td width = "90%"><code> <a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </td>
    <td>Spark ML - Feature Importance for Tree Models</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-extensions" class="hasAnchor">Extensions</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="compile_package_jars">compile_package_jars()</a></code> </td>
    <td>Compile Scala sources into a Java Archive (jar)</td></tr>
  <tr><td width = "90%"><code> <a href="connection_config">connection_config()</a></code> </td>
    <td>Read configuration values for a connection</td></tr>
  <tr><td width = "90%"><code> <a href="download_scalac">download_scalac()</a></code> </td>
    <td>Downloads default Scala Compilers</td></tr>
  <tr><td width = "90%"><code> <a href="find_scalac">find_scalac()</a></code> </td>
    <td>Discover the Scala Compiler</td></tr>
  <tr><td width = "90%"><code> <a href="spark-api">spark_context()</a></code> <code><a href="spark-api">java_context()</a></code> <code><a href="spark-api">hive_context()</a></code> <code><a href="spark-api">spark_session()</a></code> </td>
    <td>Access the Spark API</td></tr>
  <tr><td width = "90%"><code> <a href="hive_context_config">hive_context_config()</a></code> </td>
    <td>Runtime configuration interface for Hive</td></tr>
  <tr><td width = "90%"><code> <a href="invoke">invoke()</a></code> <code><a href="invoke">invoke_static()</a></code> <code><a href="invoke">invoke_new()</a></code> </td>
    <td>Invoke a Method on a JVM Object</td></tr>
  <tr><td width = "90%"><code> <a href="register_extension">register_extension()</a></code> <code><a href="register_extension">registered_extensions()</a></code> </td>
    <td>Register a Package that Implements a Spark Extension</td></tr>
  <tr><td width = "90%"><code> <a href="spark_compilation_spec">spark_compilation_spec()</a></code> </td>
    <td>Define a Spark Compilation Specification</td></tr>
  <tr><td width = "90%"><code> <a href="spark_default_compilation_spec">spark_default_compilation_spec()</a></code> </td>
    <td>Default Compilation Specification for Spark Extensions</td></tr>
  <tr><td width = "90%"><code> <a href="spark_connection">spark_connection()</a></code> </td>
    <td>Retrieve the Spark Connection Associated with an R Object</td></tr>
  <tr><td width = "90%"><code> <a href="spark_context_config">spark_context_config()</a></code> </td>
    <td>Runtime configuration interface for the Spark Context.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dataframe">spark_dataframe()</a></code> </td>
    <td>Retrieve a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dependency">spark_dependency()</a></code> </td>
    <td>Define a Spark dependency</td></tr>
  <tr><td width = "90%"><code> <a href="spark_home_set">spark_home_set()</a></code> </td>
    <td>Set the SPARK_HOME environment variable</td></tr>
  <tr><td width = "90%"><code> <a href="spark_jobj">spark_jobj()</a></code> </td>
    <td>Retrieve a Spark JVM Object Reference</td></tr>
  <tr><td width = "90%"><code> <a href="spark_version">spark_version()</a></code> </td>
    <td>Get the Spark Version Associated with a Spark Connection</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-distributed-computing" class="hasAnchor">Distributed Computing</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_apply">spark_apply()</a></code> </td>
    <td>Apply an R Function in Spark</td></tr>
  <tr><td width = "90%"><code> <a href="spark_apply_bundle">spark_apply_bundle()</a></code> </td>
    <td>Create Bundle for Spark Apply</td></tr>
  <tr><td width = "90%"><code> <a href="spark_apply_log">spark_apply_log()</a></code> </td>
    <td>Log Writer for Spark Apply</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-livy" class="hasAnchor">Livy</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="livy_config">livy_config()</a></code> </td>
    <td>Create a Spark Configuration for Livy</td></tr>
  <tr><td width = "90%"><code> <a href="livy_service">livy_service_start()</a></code> <code><a href="livy_service">livy_service_stop()</a></code> </td>
    <td>Start Livy</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h2 id="section-streaming" class="hasAnchor">Streaming</h2>
</th></tr>
  <tr><td width = "90%"><code> <a href="stream_find">stream_find()</a></code> </td>
    <td>Find Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_generate_test">stream_generate_test()</a></code> </td>
    <td>Generate Test Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_id">stream_id()</a></code> </td>
    <td>Spark Stream's Identifier</td></tr>
  <tr><td width = "90%"><code> <a href="stream_name">stream_name()</a></code> </td>
    <td>Spark Stream's Name</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_csv">stream_read_csv()</a></code> </td>
    <td>Read CSV Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_json">stream_read_json()</a></code> </td>
    <td>Read JSON Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_kafka">stream_read_kafka()</a></code> </td>
    <td>Read Kafka Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_orc">stream_read_orc()</a></code> </td>
    <td>Read ORC Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_parquet">stream_read_parquet()</a></code> </td>
    <td>Read Parquet Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_scoket">stream_read_scoket()</a></code> </td>
    <td>Read Socket Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_text">stream_read_text()</a></code> </td>
    <td>Read Text Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_render">stream_render()</a></code> </td>
    <td>Render Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_stats">stream_stats()</a></code> </td>
    <td>Stream Statistics</td></tr>
  <tr><td width = "90%"><code> <a href="stream_stop">stream_stop()</a></code> </td>
    <td>Stops a Spark Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_trigger_continuous">stream_trigger_continuous()</a></code> </td>
    <td>Spark Stream Continuous Trigger</td></tr>
  <tr><td width = "90%"><code> <a href="stream_trigger_interval">stream_trigger_interval()</a></code> </td>
    <td>Spark Stream Interval Trigger</td></tr>
  <tr><td width = "90%"><code> <a href="stream_view">stream_view()</a></code> </td>
    <td>View Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_watermark">stream_watermark()</a></code> </td>
    <td>Watermark Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_console">stream_write_console()</a></code> </td>
    <td>Write Console Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_csv">stream_write_csv()</a></code> </td>
    <td>Write CSV Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_json">stream_write_json()</a></code> </td>
    <td>Write JSON Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_kafka">stream_write_kafka()</a></code> </td>
    <td>Write Kafka Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_memory">stream_write_memory()</a></code> </td>
    <td>Write Memory Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_orc">stream_write_orc()</a></code> </td>
    <td>Write a ORC Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_parquet">stream_write_parquet()</a></code> </td>
    <td>Write Parquet Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_text">stream_write_text()</a></code> </td>
    <td>Write Text Stream</td></tr>
  <tr><td width = "90%"><code> <a href="reactiveSpark">reactiveSpark()</a></code> </td>
    <td>Reactive spark reader</td></tr>
</tbody>
</table>

  

  
<h2>Function Reference - version 1.04 </h2>

<table >

<colgroup>
<col class="alias" />
<col class="title" />
</colgroup>

<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-operations" class="hasAnchor">Spark Operations</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_config">spark_config()</a></code> </td>
    <td>Read Spark Configuration</td></tr>
  <tr><td width = "90%"><code> <a href="spark-connections">spark_connect()</a></code> <code><a href="spark-connections">spark_connection_is_open()</a></code> <code><a href="spark-connections">spark_disconnect()</a></code> <code><a href="spark-connections">spark_disconnect_all()</a></code> <code><a href="spark-connections">spark_submit()</a></code> </td>
    <td>Manage Spark Connections</td></tr>
  <tr><td width = "90%"><code> <a href="spark_install">spark_install_find()</a></code> <code><a href="spark_install">spark_install()</a></code> <code><a href="spark_install">spark_uninstall()</a></code> <code><a href="spark_install">spark_install_dir()</a></code> <code><a href="spark_install">spark_install_tar()</a></code> <code><a href="spark_install">spark_installed_versions()</a></code> <code><a href="spark_install">spark_available_versions()</a></code> </td>
    <td>Find a given Spark installation by version.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_log">spark_log()</a></code> </td>
    <td>View Entries in the Spark Log</td></tr>
  <tr><td width = "90%"><code> <a href="spark_web">spark_web()</a></code> </td>
    <td>Open the Spark web interface</td></tr>
  <tr><td width = "90%"><code> <a href="connection_is_open">connection_is_open()</a></code> </td>
    <td>Check whether the connection is open</td></tr>
  <tr><td width = "90%"><code> <a href="connection_spark_shinyapp">connection_spark_shinyapp()</a></code> </td>
    <td>A Shiny app that can be used to construct a <code>spark_connect</code> statement</td></tr>
  <tr><td width = "90%"><code> <a href="spark_configuration">spark_session_config()</a></code> </td>
    <td>Runtime configuration interface for the Spark Session</td></tr>
  <tr><td width = "90%"><code> <a href="checkpoint_directory">spark_set_checkpoint_dir()</a></code> <code><a href="checkpoint_directory">spark_get_checkpoint_dir()</a></code> </td>
    <td>Set/Get Spark checkpoint directory</td></tr>
  <tr><td width = "90%"><code> <a href="spark_table_name">spark_table_name()</a></code> </td>
    <td>Generate a Table Name from Expression</td></tr>
  <tr><td width = "90%"><code> <a href="spark_version_from_home">spark_version_from_home()</a></code> </td>
    <td>Get the Spark Version Associated with a Spark Installation</td></tr>
  <tr><td width = "90%"><code> <a href="spark_versions">spark_versions()</a></code> </td>
    <td>Retrieves a dataframe available Spark versions that van be installed.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_config_kubernetes">spark_config_kubernetes()</a></code> </td>
    <td>Kubernetes Configuration</td></tr>
  <tr><td width = "90%"><code> <a href="spark_config_settings">spark_config_settings()</a></code> </td>
    <td>Retrieve Available Settings</td></tr>
  <tr><td width = "90%"><code> <a href="spark_connection_find">spark_connection_find()</a></code> </td>
    <td>Find Spark Connection</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dependency_fallback">spark_dependency_fallback()</a></code> </td>
    <td>Fallback to Spark Dependency</td></tr>
  <tr><td width = "90%"><code> <a href="spark_extension">spark_extension()</a></code> </td>
    <td>Create Spark Extension</td></tr>
  <tr><td width = "90%"><code> <a href="spark_load_table">spark_load_table()</a></code> </td>
    <td>Reads from a Spark Table into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_libsvm">spark_read_libsvm()</a></code> </td>
    <td>Read libsvm file into a Spark DataFrame.</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-data" class="hasAnchor">Spark Data</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_read_csv">spark_read_csv()</a></code> </td>
    <td>Read a CSV file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_jdbc">spark_read_jdbc()</a></code> </td>
    <td>Read from JDBC connection into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_json">spark_read_json()</a></code> </td>
    <td>Read a JSON file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_parquet">spark_read_parquet()</a></code> </td>
    <td>Read a Parquet file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_source">spark_read_source()</a></code> </td>
    <td>Read from a generic source into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_table">spark_read_table()</a></code> </td>
    <td>Reads from a Spark Table into a Spark DataFrame.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_orc">spark_read_orc()</a></code> </td>
    <td>Read a ORC file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_read_text">spark_read_text()</a></code> </td>
    <td>Read a Text file into a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_save_table">spark_save_table()</a></code> </td>
    <td>Saves a Spark DataFrame as a Spark table</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_orc">spark_write_orc()</a></code> </td>
    <td>Write a Spark DataFrame to a ORC file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_text">spark_write_text()</a></code> </td>
    <td>Write a Spark DataFrame to a Text file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_csv">spark_write_csv()</a></code> </td>
    <td>Write a Spark DataFrame to a CSV</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_jdbc">spark_write_jdbc()</a></code> </td>
    <td>Writes a Spark DataFrame into a JDBC table</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_json">spark_write_json()</a></code> </td>
    <td>Write a Spark DataFrame to a JSON file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_parquet">spark_write_parquet()</a></code> </td>
    <td>Write a Spark DataFrame to a Parquet file</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_source">spark_write_source()</a></code> </td>
    <td>Writes a Spark DataFrame into a generic source</td></tr>
  <tr><td width = "90%"><code> <a href="spark_write_table">spark_write_table()</a></code> </td>
    <td>Writes a Spark DataFrame into a Spark table</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-tables" class="hasAnchor">Spark Tables</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="src_databases">src_databases()</a></code> </td>
    <td>Show database list</td></tr>
  <tr><td width = "90%"><code> <a href="tbl_cache">tbl_cache()</a></code> </td>
    <td>Cache a Spark Table</td></tr>
  <tr><td width = "90%"><code> <a href="tbl_change_db">tbl_change_db()</a></code> </td>
    <td>Use specific database</td></tr>
  <tr><td width = "90%"><code> <a href="tbl_uncache">tbl_uncache()</a></code> </td>
    <td>Uncache a Spark Table</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-dataframes" class="hasAnchor">Spark DataFrames</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="sdf_along">sdf_along()</a></code> </td>
    <td>Create DataFrame for along Object</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_bind">sdf_bind_rows()</a></code> <code><a href="sdf_bind">sdf_bind_cols()</a></code> </td>
    <td>Bind multiple Spark DataFrames by row and column</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_broadcast">sdf_broadcast()</a></code> </td>
    <td>Broadcast hint</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_checkpoint">sdf_checkpoint()</a></code> </td>
    <td>Checkpoint a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_coalesce">sdf_coalesce()</a></code> </td>
    <td>Coalesces a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_copy_to">sdf_copy_to()</a></code> <code><a href="sdf_copy_to">sdf_import()</a></code> </td>
    <td>Copy an Object into Spark</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_len">sdf_len()</a></code> </td>
    <td>Create DataFrame for Length</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_num_partitions">sdf_num_partitions()</a></code> </td>
    <td>Gets number of partitions of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_random_split">sdf_random_split()</a></code> <code><a href="sdf_random_split">sdf_partition()</a></code> </td>
    <td>Partition a Spark Dataframe</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_pivot">sdf_pivot()</a></code> </td>
    <td>Pivot a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf-transform-methods">sdf_predict()</a></code> <code><a href="sdf-transform-methods">sdf_transform()</a></code> <code><a href="sdf-transform-methods">sdf_fit()</a></code> <code><a href="sdf-transform-methods">sdf_fit_and_transform()</a></code> </td>
    <td>Spark ML -- Transform, fit, and predict methods (sdf_ interface)</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_read_column">sdf_read_column()</a></code> </td>
    <td>Read a Column from a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_register">sdf_register()</a></code> </td>
    <td>Register a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_repartition">sdf_repartition()</a></code> </td>
    <td>Repartition a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_residuals">sdf_residuals()</a></code> </td>
    <td>Model Residuals</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sample">sdf_sample()</a></code> </td>
    <td>Randomly Sample Rows from a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_separate_column">sdf_separate_column()</a></code> </td>
    <td>Separate a Vector Column into Scalar Columns</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_seq">sdf_seq()</a></code> </td>
    <td>Create DataFrame for Range</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sort">sdf_sort()</a></code> </td>
    <td>Sort a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_with_unique_id">sdf_with_unique_id()</a></code> </td>
    <td>Add a Unique ID Column to a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_collect">sdf_collect()</a></code> </td>
    <td>Collect a Spark DataFrame into R.</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_crosstab">sdf_crosstab()</a></code> </td>
    <td>Cross Tabulation</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_debug_string">sdf_debug_string()</a></code> </td>
    <td>Debug Info for Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_describe">sdf_describe()</a></code> </td>
    <td>Compute summary statistics for columns of a data frame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_dim">sdf_dim()</a></code> <code><a href="sdf_dim">sdf_nrow()</a></code> <code><a href="sdf_dim">sdf_ncol()</a></code> </td>
    <td>Support for Dimension Operations</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_is_streaming">sdf_is_streaming()</a></code> </td>
    <td>Spark DataFrame is Streaming</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_last_index">sdf_last_index()</a></code> </td>
    <td>Returns the last index of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf-saveload">sdf_save_table()</a></code> <code><a href="sdf-saveload">sdf_load_table()</a></code> <code><a href="sdf-saveload">sdf_save_parquet()</a></code> <code><a href="sdf-saveload">sdf_load_parquet()</a></code> </td>
    <td>Save / Load a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_persist">sdf_persist()</a></code> </td>
    <td>Persist a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_project">sdf_project()</a></code> </td>
    <td>Project features onto principal components</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_quantile">sdf_quantile()</a></code> </td>
    <td>Compute (Approximate) Quantiles with a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_schema">sdf_schema()</a></code> </td>
    <td>Read the Schema of a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_sql">sdf_sql()</a></code> </td>
    <td>Spark DataFrame from SQL</td></tr>
  <tr><td width = "90%"><code> <a href="sdf_with_sequential_id">sdf_with_sequential_id()</a></code> </td>
    <td>Add a Sequential ID Column to a Spark DataFrame</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-machine-learning" class="hasAnchor">Spark Machine Learning</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="ml_decision_tree">ml_decision_tree_classifier()</a></code> <code><a href="ml_decision_tree">ml_decision_tree()</a></code> <code><a href="ml_decision_tree">ml_decision_tree_regressor()</a></code> </td>
    <td>Spark ML -- Decision Trees</td></tr>
  <tr><td width = "90%"><code> <a href="ml_generalized_linear_regression">ml_generalized_linear_regression()</a></code> </td>
    <td>Spark ML -- Generalized Linear Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_gradient_boosted_trees">ml_gbt_classifier()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gradient_boosted_trees()</a></code> <code><a href="ml_gradient_boosted_trees">ml_gbt_regressor()</a></code> </td>
    <td>Spark ML -- Gradient Boosted Trees</td></tr>
  <tr><td width = "90%"><code> <a href="ml_kmeans">ml_kmeans()</a></code> <code><a href="ml_kmeans">ml_compute_cost()</a></code> </td>
    <td>Spark ML -- K-Means Clustering</td></tr>
  <tr><td width = "90%"><code> <a href="ml_lda">ml_lda()</a></code> <code><a href="ml_lda">ml_describe_topics()</a></code> <code><a href="ml_lda">ml_log_likelihood()</a></code> <code><a href="ml_lda">ml_log_perplexity()</a></code> <code><a href="ml_lda">ml_topics_matrix()</a></code> </td>
    <td>Spark ML -- Latent Dirichlet Allocation</td></tr>
  <tr><td width = "90%"><code> <a href="ml_linear_regression">ml_linear_regression()</a></code> </td>
    <td>Spark ML -- Linear Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_logistic_regression">ml_logistic_regression()</a></code> </td>
    <td>Spark ML -- Logistic Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_model_data">ml_model_data()</a></code> </td>
    <td>Extracts data associated with a Spark ML model</td></tr>
  <tr><td width = "90%"><code> <a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron_classifier()</a></code> <code><a href="ml_multilayer_perceptron_classifier">ml_multilayer_perceptron()</a></code> </td>
    <td>Spark ML -- Multilayer Perceptron</td></tr>
  <tr><td width = "90%"><code> <a href="ml_naive_bayes">ml_naive_bayes()</a></code> </td>
    <td>Spark ML -- Naive-Bayes</td></tr>
  <tr><td width = "90%"><code> <a href="ml_one_vs_rest">ml_one_vs_rest()</a></code> </td>
    <td>Spark ML -- OneVsRest</td></tr>
  <tr><td width = "90%"><code> <a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </td>
    <td>Feature Transformation -- PCA (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_random_forest">ml_random_forest_classifier()</a></code> <code><a href="ml_random_forest">ml_random_forest()</a></code> <code><a href="ml_random_forest">ml_random_forest_regressor()</a></code> </td>
    <td>Spark ML -- Random Forest</td></tr>
  <tr><td width = "90%"><code> <a href="ml_aft_survival_regression">ml_aft_survival_regression()</a></code> <code><a href="ml_aft_survival_regression">ml_survival_regression()</a></code> </td>
    <td>Spark ML -- Survival Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ml_add_stage">ml_add_stage()</a></code> </td>
    <td>Add a Stage to a Pipeline</td></tr>
  <tr><td width = "90%"><code> <a href="ml_als">ml_als()</a></code> <code><a href="ml_als">ml_recommend()</a></code> </td>
    <td>Spark ML -- ALS</td></tr>
  <tr><td width = "90%"><code> <a href="ft_lsh_utils">ml_approx_nearest_neighbors()</a></code> <code><a href="ft_lsh_utils">ml_approx_similarity_join()</a></code> </td>
    <td>Utility functions for LSH models</td></tr>
  <tr><td width = "90%"><code> <a href="ml_fpgrowth">ml_fpgrowth()</a></code> <code><a href="ml_fpgrowth">ml_association_rules()</a></code> <code><a href="ml_fpgrowth">ml_freq_itemsets()</a></code> </td>
    <td>Frequent Pattern Mining -- FPGrowth</td></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </td>
    <td>Spark ML - Evaluators</td></tr>
  <tr><td width = "90%"><code> <a href="ml_bisecting_kmeans">ml_bisecting_kmeans()</a></code> </td>
    <td>Spark ML -- Bisecting K-Means Clustering</td></tr>
  <tr><td width = "90%"><code> <a href="ml_call_constructor">ml_call_constructor()</a></code> </td>
    <td>Wrap a Spark ML JVM object</td></tr>
  <tr><td width = "90%"><code> <a href="ml_chisquare_test">ml_chisquare_test()</a></code> </td>
    <td>Chi-square hypothesis testing for categorical data.</td></tr>
  <tr><td width = "90%"><code> <a href="ml_clustering_evaluator">ml_clustering_evaluator()</a></code> </td>
    <td>Spark ML - Clustering Evaluator</td></tr>
  <tr><td width = "90%"><code> <a href="ml-model-constructors">new_ml_model_prediction()</a></code> <code><a href="ml-model-constructors">new_ml_model()</a></code> <code><a href="ml-model-constructors">new_ml_model_classification()</a></code> <code><a href="ml-model-constructors">new_ml_model_regression()</a></code> <code><a href="ml-model-constructors">new_ml_model_clustering()</a></code> <code><a href="ml-model-constructors">ml_supervised_pipeline()</a></code> <code><a href="ml-model-constructors">ml_clustering_pipeline()</a></code> <code><a href="ml-model-constructors">ml_construct_model_supervised()</a></code> <code><a href="ml-model-constructors">ml_construct_model_clustering()</a></code> </td>
    <td>Constructors for `ml_model` Objects</td></tr>
  <tr><td width = "90%"><code> <a href="ml_corr">ml_corr()</a></code> </td>
    <td>Compute correlation matrix</td></tr>
  <tr><td width = "90%"><code> <a href="ml-tuning">ml_sub_models()</a></code> <code><a href="ml-tuning">ml_validation_metrics()</a></code> <code><a href="ml-tuning">ml_cross_validator()</a></code> <code><a href="ml-tuning">ml_train_validation_split()</a></code> </td>
    <td>Spark ML -- Tuning</td></tr>
  <tr><td width = "90%"><code> <a href="ml_default_stop_words">ml_default_stop_words()</a></code> </td>
    <td>Default stop words</td></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluate">ml_evaluate()</a></code> </td>
    <td>Evaluate the Model on a Validation Set</td></tr>
  <tr><td width = "90%"><code> <a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </td>
    <td>Spark ML - Feature Importance for Tree Models</td></tr>
  <tr><td width = "90%"><code> <a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </td>
    <td>Feature Transformation -- Word2Vec (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml-transform-methods">is_ml_transformer()</a></code> <code><a href="ml-transform-methods">is_ml_estimator()</a></code> <code><a href="ml-transform-methods">ml_fit()</a></code> <code><a href="ml-transform-methods">ml_transform()</a></code> <code><a href="ml-transform-methods">ml_fit_and_transform()</a></code> <code><a href="ml-transform-methods">ml_predict()</a></code> </td>
    <td>Spark ML -- Transform, fit, and predict methods (ml_ interface)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_gaussian_mixture">ml_gaussian_mixture()</a></code> </td>
    <td>Spark ML -- Gaussian Mixture clustering.</td></tr>
  <tr><td width = "90%"><code> <a href="ml-params">ml_is_set()</a></code> <code><a href="ml-params">ml_param_map()</a></code> <code><a href="ml-params">ml_param()</a></code> <code><a href="ml-params">ml_params()</a></code> </td>
    <td>Spark ML -- ML Params</td></tr>
  <tr><td width = "90%"><code> <a href="ml_isotonic_regression">ml_isotonic_regression()</a></code> </td>
    <td>Spark ML -- Isotonic Regression</td></tr>
  <tr><td width = "90%"><code> <a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </td>
    <td>Feature Transformation -- StringIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ml_linear_svc">ml_linear_svc()</a></code> </td>
    <td>Spark ML -- LinearSVC</td></tr>
  <tr><td width = "90%"><code> <a href="ml-persistence">ml_save()</a></code> <code><a href="ml-persistence">ml_load()</a></code> </td>
    <td>Spark ML -- Model Persistence</td></tr>
  <tr><td width = "90%"><code> <a href="ml_pipeline">ml_pipeline()</a></code> </td>
    <td>Spark ML -- Pipelines</td></tr>
  <tr><td width = "90%"><code> <a href="ml_stage">ml_stage()</a></code> <code><a href="ml_stage">ml_stages()</a></code> </td>
    <td>Spark ML -- Pipeline stage extraction</td></tr>
  <tr><td width = "90%"><code> <a href="ml_standardize_formula">ml_standardize_formula()</a></code> </td>
    <td>Standardize Formula Input for `ml_model`</td></tr>
  <tr><td width = "90%"><code> <a href="ml_summary">ml_summary()</a></code> </td>
    <td>Spark ML -- Extraction of summary metrics</td></tr>
  <tr><td width = "90%"><code> <a href="ml_uid">ml_uid()</a></code> </td>
    <td>Spark ML -- UID</td></tr>
  <tr><td width = "90%"><code> <a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </td>
    <td>Feature Transformation -- CountVectorizer (Estimator)</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-feature-transformers" class="hasAnchor">Spark Feature Transformers</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="ft_binarizer">ft_binarizer()</a></code> </td>
    <td>Feature Transformation -- Binarizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_bucketizer">ft_bucketizer()</a></code> </td>
    <td>Feature Transformation -- Bucketizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_count_vectorizer">ft_count_vectorizer()</a></code> <code><a href="ft_count_vectorizer">ml_vocabulary()</a></code> </td>
    <td>Feature Transformation -- CountVectorizer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_dct">ft_dct()</a></code> <code><a href="ft_dct">ft_discrete_cosine_transform()</a></code> </td>
    <td>Feature Transformation -- Discrete Cosine Transform (DCT) (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_elementwise_product">ft_elementwise_product()</a></code> </td>
    <td>Feature Transformation -- ElementwiseProduct (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_index_to_string">ft_index_to_string()</a></code> </td>
    <td>Feature Transformation -- IndexToString (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_one_hot_encoder">ft_one_hot_encoder()</a></code> </td>
    <td>Feature Transformation -- OneHotEncoder (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_quantile_discretizer">ft_quantile_discretizer()</a></code> </td>
    <td>Feature Transformation -- QuantileDiscretizer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="sql-transformer">ft_sql_transformer()</a></code> <code><a href="sql-transformer">ft_dplyr_transformer()</a></code> </td>
    <td>Feature Transformation -- SQLTransformer</td></tr>
  <tr><td width = "90%"><code> <a href="ft_string_indexer">ft_string_indexer()</a></code> <code><a href="ft_string_indexer">ml_labels()</a></code> <code><a href="ft_string_indexer">ft_string_indexer_model()</a></code> </td>
    <td>Feature Transformation -- StringIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_assembler">ft_vector_assembler()</a></code> </td>
    <td>Feature Transformation -- VectorAssembler (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_tokenizer">ft_tokenizer()</a></code> </td>
    <td>Feature Transformation -- Tokenizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_regex_tokenizer">ft_regex_tokenizer()</a></code> </td>
    <td>Feature Transformation -- RegexTokenizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_lsh">ft_bucketed_random_projection_lsh()</a></code> <code><a href="ft_lsh">ft_minhash_lsh()</a></code> </td>
    <td>Feature Transformation -- LSH (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_chisq_selector">ft_chisq_selector()</a></code> </td>
    <td>Feature Transformation -- ChiSqSelector (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_feature_hasher">ft_feature_hasher()</a></code> </td>
    <td>Feature Transformation -- FeatureHasher (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_hashing_tf">ft_hashing_tf()</a></code> </td>
    <td>Feature Transformation -- HashingTF (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_idf">ft_idf()</a></code> </td>
    <td>Feature Transformation -- IDF (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_imputer">ft_imputer()</a></code> </td>
    <td>Feature Transformation -- Imputer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_interaction">ft_interaction()</a></code> </td>
    <td>Feature Transformation -- Interaction (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_max_abs_scaler">ft_max_abs_scaler()</a></code> </td>
    <td>Feature Transformation -- MaxAbsScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_min_max_scaler">ft_min_max_scaler()</a></code> </td>
    <td>Feature Transformation -- MinMaxScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_ngram">ft_ngram()</a></code> </td>
    <td>Feature Transformation -- NGram (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_normalizer">ft_normalizer()</a></code> </td>
    <td>Feature Transformation -- Normalizer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_one_hot_encoder_estimator">ft_one_hot_encoder_estimator()</a></code> </td>
    <td>Feature Transformation -- OneHotEncoderEstimator (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_pca">ft_pca()</a></code> <code><a href="ft_pca">ml_pca()</a></code> </td>
    <td>Feature Transformation -- PCA (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_polynomial_expansion">ft_polynomial_expansion()</a></code> </td>
    <td>Feature Transformation -- PolynomialExpansion (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_r_formula">ft_r_formula()</a></code> </td>
    <td>Feature Transformation -- RFormula (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_standard_scaler">ft_standard_scaler()</a></code> </td>
    <td>Feature Transformation -- StandardScaler (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_stop_words_remover">ft_stop_words_remover()</a></code> </td>
    <td>Feature Transformation -- StopWordsRemover (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_indexer">ft_vector_indexer()</a></code> </td>
    <td>Feature Transformation -- VectorIndexer (Estimator)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_vector_slicer">ft_vector_slicer()</a></code> </td>
    <td>Feature Transformation -- VectorSlicer (Transformer)</td></tr>
  <tr><td width = "90%"><code> <a href="ft_word2vec">ft_word2vec()</a></code> <code><a href="ft_word2vec">ml_find_synonyms()</a></code> </td>
    <td>Feature Transformation -- Word2Vec (Estimator)</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-spark-machine-learning-utilities" class="hasAnchor">Spark Machine Learning Utilities</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="ml_evaluator">ml_binary_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_binary_classification_eval()</a></code> <code><a href="ml_evaluator">ml_multiclass_classification_evaluator()</a></code> <code><a href="ml_evaluator">ml_classification_eval()</a></code> <code><a href="ml_evaluator">ml_regression_evaluator()</a></code> </td>
    <td>Spark ML - Evaluators</td></tr>
  <tr><td width = "90%"><code> <a href="ml_feature_importances">ml_feature_importances()</a></code> <code><a href="ml_feature_importances">ml_tree_feature_importance()</a></code> </td>
    <td>Spark ML - Feature Importance for Tree Models</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-extensions" class="hasAnchor">Extensions</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="compile_package_jars">compile_package_jars()</a></code> </td>
    <td>Compile Scala sources into a Java Archive (jar)</td></tr>
  <tr><td width = "90%"><code> <a href="connection_config">connection_config()</a></code> </td>
    <td>Read configuration values for a connection</td></tr>
  <tr><td width = "90%"><code> <a href="download_scalac">download_scalac()</a></code> </td>
    <td>Downloads default Scala Compilers</td></tr>
  <tr><td width = "90%"><code> <a href="find_scalac">find_scalac()</a></code> </td>
    <td>Discover the Scala Compiler</td></tr>
  <tr><td width = "90%"><code> <a href="spark-api">spark_context()</a></code> <code><a href="spark-api">java_context()</a></code> <code><a href="spark-api">hive_context()</a></code> <code><a href="spark-api">spark_session()</a></code> </td>
    <td>Access the Spark API</td></tr>
  <tr><td width = "90%"><code> <a href="hive_context_config">hive_context_config()</a></code> </td>
    <td>Runtime configuration interface for Hive</td></tr>
  <tr><td width = "90%"><code> <a href="invoke">invoke()</a></code> <code><a href="invoke">invoke_static()</a></code> <code><a href="invoke">invoke_new()</a></code> </td>
    <td>Invoke a Method on a JVM Object</td></tr>
  <tr><td width = "90%"><code> <a href="register_extension">register_extension()</a></code> <code><a href="register_extension">registered_extensions()</a></code> </td>
    <td>Register a Package that Implements a Spark Extension</td></tr>
  <tr><td width = "90%"><code> <a href="spark_compilation_spec">spark_compilation_spec()</a></code> </td>
    <td>Define a Spark Compilation Specification</td></tr>
  <tr><td width = "90%"><code> <a href="spark_default_compilation_spec">spark_default_compilation_spec()</a></code> </td>
    <td>Default Compilation Specification for Spark Extensions</td></tr>
  <tr><td width = "90%"><code> <a href="spark_connection">spark_connection()</a></code> </td>
    <td>Retrieve the Spark Connection Associated with an R Object</td></tr>
  <tr><td width = "90%"><code> <a href="spark_context_config">spark_context_config()</a></code> </td>
    <td>Runtime configuration interface for the Spark Context.</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dataframe">spark_dataframe()</a></code> </td>
    <td>Retrieve a Spark DataFrame</td></tr>
  <tr><td width = "90%"><code> <a href="spark_dependency">spark_dependency()</a></code> </td>
    <td>Define a Spark dependency</td></tr>
  <tr><td width = "90%"><code> <a href="spark_home_set">spark_home_set()</a></code> </td>
    <td>Set the SPARK_HOME environment variable</td></tr>
  <tr><td width = "90%"><code> <a href="spark_jobj">spark_jobj()</a></code> </td>
    <td>Retrieve a Spark JVM Object Reference</td></tr>
  <tr><td width = "90%"><code> <a href="spark_version">spark_version()</a></code> </td>
    <td>Get the Spark Version Associated with a Spark Connection</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-distributed-computing" class="hasAnchor">Distributed Computing</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="spark_apply">spark_apply()</a></code> </td>
    <td>Apply an R Function in Spark</td></tr>
  <tr><td width = "90%"><code> <a href="spark_apply_bundle">spark_apply_bundle()</a></code> </td>
    <td>Create Bundle for Spark Apply</td></tr>
  <tr><td width = "90%"><code> <a href="spark_apply_log">spark_apply_log()</a></code> </td>
    <td>Log Writer for Spark Apply</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-livy" class="hasAnchor">Livy</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="livy_install">livy_install()</a></code> <code><a href="livy_install">livy_available_versions()</a></code> <code><a href="livy_install">livy_install_dir()</a></code> <code><a href="livy_install">livy_installed_versions()</a></code> <code><a href="livy_install">livy_home_dir()</a></code> </td>
    <td>Install Livy</td></tr>
  <tr><td width = "90%"><code> <a href="livy_config">livy_config()</a></code> </td>
    <td>Create a Spark Configuration for Livy</td></tr>
  <tr><td width = "90%"><code> <a href="livy_service">livy_service_start()</a></code> <code><a href="livy_service">livy_service_stop()</a></code> </td>
    <td>Start Livy</td></tr>
</tbody>
<tbody>
<tr>
  <th colspan="2">
    <h3 id="section-streaming" class="hasAnchor">Streaming</h3>
</th></tr>
  <tr><td width = "90%"><code> <a href="stream_find">stream_find()</a></code> </td>
    <td>Find Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_generate_test">stream_generate_test()</a></code> </td>
    <td>Generate Test Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_id">stream_id()</a></code> </td>
    <td>Spark Stream's Identifier</td></tr>
  <tr><td width = "90%"><code> <a href="stream_name">stream_name()</a></code> </td>
    <td>Spark Stream's Name</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_csv">stream_read_csv()</a></code> </td>
    <td>Read CSV Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_json">stream_read_json()</a></code> </td>
    <td>Read JSON Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_kafka">stream_read_kafka()</a></code> </td>
    <td>Read Kafka Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_orc">stream_read_orc()</a></code> </td>
    <td>Read ORC Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_parquet">stream_read_parquet()</a></code> </td>
    <td>Read Parquet Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_scoket">stream_read_scoket()</a></code> </td>
    <td>Read Socket Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_read_text">stream_read_text()</a></code> </td>
    <td>Read Text Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_render">stream_render()</a></code> </td>
    <td>Render Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_stats">stream_stats()</a></code> </td>
    <td>Stream Statistics</td></tr>
  <tr><td width = "90%"><code> <a href="stream_stop">stream_stop()</a></code> </td>
    <td>Stops a Spark Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_trigger_continuous">stream_trigger_continuous()</a></code> </td>
    <td>Spark Stream Continuous Trigger</td></tr>
  <tr><td width = "90%"><code> <a href="stream_trigger_interval">stream_trigger_interval()</a></code> </td>
    <td>Spark Stream Interval Trigger</td></tr>
  <tr><td width = "90%"><code> <a href="stream_view">stream_view()</a></code> </td>
    <td>View Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_watermark">stream_watermark()</a></code> </td>
    <td>Watermark Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_console">stream_write_console()</a></code> </td>
    <td>Write Console Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_csv">stream_write_csv()</a></code> </td>
    <td>Write CSV Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_json">stream_write_json()</a></code> </td>
    <td>Write JSON Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_kafka">stream_write_kafka()</a></code> </td>
    <td>Write Kafka Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_memory">stream_write_memory()</a></code> </td>
    <td>Write Memory Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_orc">stream_write_orc()</a></code> </td>
    <td>Write a ORC Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_parquet">stream_write_parquet()</a></code> </td>
    <td>Write Parquet Stream</td></tr>
  <tr><td width = "90%"><code> <a href="stream_write_text">stream_write_text()</a></code> </td>
    <td>Write Text Stream</td></tr>
  <tr><td width = "90%"><code> <a href="reactiveSpark">reactiveSpark()</a></code> </td>
    <td>Reactive spark reader</td></tr>
</tbody>
</table>
<h2>Read Spark Configuration </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
Read Spark Configuration
<code class="sourceCode r">spark_config(file = "config.yml", use_default = TRUE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>file</td>
<td>Name of the configuration file</td>
    </tr>
<tr>
<td>use_default</td>
<td>TRUE to use the built-in defaults provided in this package</td>
    </tr>
    </table>
<h3>Value</h3>
Named list with configuration data
<h3>Details</h3>
Read Spark configuration using the <a href='https://rdrr.io/pkg/config/man/config.html'>config</a> package.
<h2>Manage Spark Connections </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#examples">Examples</a>
These routines allow you to manage your connections to Spark.
<code class="sourceCode r">spark_connect(master, spark_home = <a href='https://rdrr.io/r/base/Sys.getenv.html'>Sys.getenv</a>("SPARK_HOME"),
  method = <a href='https://rdrr.io/r/base/c.html'>c</a>("shell", "livy", "databricks", "test", "qubole"),
  app_name = "sparklyr", version = NULL, config = <a href='spark_config.html'>spark_config</a>(),
  extensions = sparklyr::<a href='https://rdrr.io/pkg/sparklyr/man/register_extension.html'>registered_extensions</a>(), packages = NULL, ...)

spark_connection_is_open(sc)

spark_disconnect(sc, ...)

spark_disconnect_all()

spark_submit(master, file, spark_home = <a href='https://rdrr.io/r/base/Sys.getenv.html'>Sys.getenv</a>("SPARK_HOME"),
  app_name = "sparklyr", version = NULL, config = <a href='spark_config.html'>spark_config</a>(),
  extensions = sparklyr::<a href='https://rdrr.io/pkg/sparklyr/man/register_extension.html'>registered_extensions</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>master</td>
<td>Spark cluster url to connect to. 
Use <code>"local"</code> to
connect to a local instance of Spark installed via<code> <a href='spark_install.html'>spark_install</a></code>.</td>
    </tr>
<tr>
<td>spark_home</td>
<td>The path to a Spark installation. 
Defaults to the path
provided by the <code>SPARK_HOME</code> environment variable. 
If<code> SPARK_HOME</code> is defined, it will always be used unless the<code> version</code> parameter is specified to force the use of a locally
installed version.</td>
    </tr>
<tr>
<td>method</td>
<td>The method used to connect to Spark. 
Default connection method
is <code>"shell"</code> to connect using spark-submit, use <code>"livy"</code> to
perform remote connections using HTTP, or <code>"databricks"</code> when using a
Databricks clusters.</td>
    </tr>
<tr>
<td>app_name</td>
<td>The application name to be used while running in the Spark
cluster.</td>
    </tr>
<tr>
<td>version</td>
<td>The version of Spark to use. 
Required for <code>"local"</code> Spark
connections, optional otherwise.</td>
    </tr>
<tr>
<td>config</td>
<td>Custom configuration for the generated Spark connection. 
See<code> <a href='spark_config.html'>spark_config</a></code> for details.</td>
    </tr>
<tr>
<td>extensions</td>
<td>Extension R packages to enable for this connection. 
By
default, all packages enabled through the use of<code> <a href='register_extension.html'>sparklyr::register_extension</a></code> will be passed here.</td>
    </tr>
<tr>
<td>packages</td>
<td>A list of Spark packages to load. 
For example, <code>"delta"</code> or<code> "kafka"</code> to enable Delta Lake or Kafka. 
Also supports full versions like<code> "io.delta:delta-core_2.11:0.4.0"</code>. 
This is similar to adding packages into the<code> sparklyr.shell.packages</code> configuration option. 
Notice that the <code>version</code>
parameter is used to choose the correect package, otherwise assumes the latest version
is being used.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>file</td>
<td>Path to R source file to submit for batch execution.</td>
    </tr>
    </table>
<h3>Details</h3>
When using <code>method = "livy"</code>, it is recommended to specify <code>version</code>
parameter to improve performance by using precompiled code rather than uploading
sources. 
By default, jars are downloaded from GitHub but the path to the correct<code> sparklyr</code> JAR can also be specified through the <code>livy.jars</code> setting.
<h3>Examples</h3>
    <code class="sourceCode r">
sc &lt;- spark_connect(master = "spark://HOST:PORT")
<a href='connection_is_open.html'>connection_is_open</a>(sc)</div>#&gt; [1] TRUE
spark_disconnect(sc)</div></code>
<h2>Find a given Spark installation by version. </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Install versions of Spark for use with local Spark connections
  (i.e. <code> spark_connect(master = "local"</code>)
<code class="sourceCode r">spark_install_find(version = NULL, hadoop_version = NULL,
  installed_only = TRUE, latest = FALSE, hint = FALSE)

spark_install(version = NULL, hadoop_version = NULL, reset = TRUE,
  logging = "INFO", verbose = <a href='https://rdrr.io/r/base/interactive.html'>interactive</a>())

spark_uninstall(version, hadoop_version)

spark_install_dir()

spark_install_tar(tarfile)

spark_installed_versions()

spark_available_versions(show_hadoop = FALSE, show_minor = FALSE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>version</td>
<td>Version of Spark to install. 
See <code>spark_available_versions</code> for a list of supported versions</td>
    </tr>
<tr>
<td>hadoop_version</td>
<td>Version of Hadoop to install. 
See <code>spark_available_versions</code> for a list of supported versions</td>
    </tr>
<tr>
<td>installed_only</td>
<td>Search only the locally installed versions?</td>
    </tr>
<tr>
<td>latest</td>
<td>Check for latest version?</td>
    </tr>
<tr>
<td>hint</td>
<td>On failure should the installation code be provided?</td>
    </tr>
<tr>
<td>reset</td>
<td>Attempts to reset settings to defaults.</td>
    </tr>
<tr>
<td>logging</td>
<td>Logging level to configure install. 
Supported options: "WARN", "INFO"</td>
    </tr>
<tr>
<td>verbose</td>
<td>Report information as Spark is downloaded / installed</td>
    </tr>
<tr>
<td>tarfile</td>
<td>Path to TAR file conforming to the pattern spark-###-bin-(hadoop)?### where ###
reference spark and hadoop versions respectively.</td>
    </tr>
<tr>
<td>show_hadoop</td>
<td>Show Hadoop distributions?</td>
    </tr>
<tr>
<td>show_minor</td>
<td>Show minor Spark versions?</td>
    </tr>
    </table>
<h3>Value</h3>
List with information about the installed version.
<h2>View Entries in the Spark Log </h2>
<a href="#arguments">Arguments</a>
View the most recent entries in the Spark log. 
This can be useful when
inspecting output / errors produced by Spark during the invocation of
various commands.
<code class="sourceCode r">spark_log(sc, n = 100, filter = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>n</td>
<td>The max number of log entries to retrieve. 
Use <code>NULL</code> to
retrieve all entries within the log.</td>
    </tr>
<tr>
<td>filter</td>
<td>Character string to filter log entries.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2>Open the Spark web interface </h2>
<a href="#arguments">Arguments</a>
Open the Spark web interface
<code class="sourceCode r">spark_web(sc, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2>Check whether the connection is open </h2>
<a href="#arguments">Arguments</a>
Check whether the connection is open
<code class="sourceCode r">connection_is_open(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td><code>spark_connection</code></td>
    </tr>
    </table>
<h2>A Shiny app that can be used to construct a &lt;code&gt;spark_connect&lt;/code&gt; statement </h2>
A Shiny app that can be used to construct a <code>spark_connect</code> statement
<code class="sourceCode r">connection_spark_shinyapp()</code>
<h2>Runtime configuration interface for the Spark Session </h2>
<a href="#arguments">Arguments</a>
Retrieves or sets runtime configuration entries for the Spark Session
<code class="sourceCode r">spark_session_config(sc, config = TRUE, value = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>config</td>
<td>The configuration entry name(s) (e.g., <code>"spark.sql.shuffle.partitions"</code>).
Defaults to <code>NULL</code> to retrieve all configuration entries.</td>
    </tr>
<tr>
<td>value</td>
<td>The configuration value to be set. 
Defaults to <code>NULL</code> to retrieve
configuration entries.</td>
    </tr>
    </table>
<h2>Set/Get Spark checkpoint directory </h2>
<a href="#arguments">Arguments</a>
Set/Get Spark checkpoint directory
<code class="sourceCode r">spark_set_checkpoint_dir(sc, dir)

spark_get_checkpoint_dir(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>dir</td>
<td>checkpoint directory, must be HDFS path of running on cluster</td>
    </tr>
    </table>
<h2>Generate a Table Name from Expression </h2>
<a href="#arguments">Arguments</a>
Attempts to generate a table name from an expression; otherwise,
assigns an auto-generated generic name with "sparklyr_" prefix.
<code class="sourceCode r">spark_table_name(expr)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>expr</td>
<td>The expression to attempt to use as name</td>
    </tr>
    </table>
<h2>Get the Spark Version Associated with a Spark Installation </h2>
<a href="#arguments">Arguments</a>
Retrieve the version of Spark associated with a Spark installation.
<code class="sourceCode r">spark_version_from_home(spark_home, default = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>spark_home</td>
<td>The path to a Spark installation.</td>
    </tr>
<tr>
<td>default</td>
<td>The default version to be inferred, in case
version lookup failed, e.g. 
no Spark installation was found
at <code>spark_home</code>.</td>
    </tr>
    </table>
<h2>Retrieves a dataframe available Spark versions that van be installed. </h2>
<a href="#arguments">Arguments</a>
Retrieves a dataframe available Spark versions that van be installed.
<code class="sourceCode r">spark_versions(latest = TRUE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>latest</td>
<td>Check for latest version?</td>
    </tr>
    </table>
<h2>Kubernetes Configuration </h2>
<a href="#arguments">Arguments</a>
Convenience function to initialize a Kubernetes configuration instead
of <code><a href='spark_config.html'>spark_config()</a></code>, exposes common properties to set in Kubernetes
clusters.
<code class="sourceCode r">spark_config_kubernetes(master, version = "2.3.2",
  image = "spark:sparklyr", driver = <a href='random_string.html'>random_string</a>("sparklyr-"),
  account = "spark", jars = "local:///opt/sparklyr", forward = TRUE,
  executors = NULL, conf = NULL, timeout = 120, ports = <a href='https://rdrr.io/r/base/c.html'>c</a>(8880,
  8881, 4040), fix_config = <a href='https://rdrr.io/r/base/identical.html'>identical</a>(.Platform$OS.type, "windows"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>master</td>
<td>Kubernetes url to connect to, found by running <code>kubectl cluster-info</code>.</td>
    </tr>
<tr>
<td>version</td>
<td>The version of Spark being used.</td>
    </tr>
<tr>
<td>image</td>
<td>Container image to use to launch Spark and sparklyr. 
Also known
as <code>spark.kubernetes.container.image</code>.</td>
    </tr>
<tr>
<td>driver</td>
<td>Name of the driver pod. 
If not set, the driver pod name is set
to "sparklyr" suffixed by id to avoid name conflicts. 
Also known as<code> spark.kubernetes.driver.pod.name</code>.</td>
    </tr>
<tr>
<td>account</td>
<td>Service account that is used when running the driver pod. 
The driver
pod uses this service account when requesting executor pods from the API
server. 
Also known as <code>spark.kubernetes.authenticate.driver.serviceAccountName</code>.</td>
    </tr>
<tr>
<td>jars</td>
<td>Path to the sparklyr jars; either, a local path inside the container
image with the sparklyr jars copied when the image was created or, a path
accesible by the container where the sparklyr jars were copied. 
You can find
a path to the sparklyr jars by running <code><a href='https://rdrr.io/r/base/system.file.html'>system.file("java/", package = "sparklyr")</a></code>.</td>
    </tr>
<tr>
<td>forward</td>
<td>Should ports used in sparklyr be forwarded automatically through Kubernetes?
Default to <code>TRUE</code> which runs <code>kubectl port-forward</code> and <code>pkill kubectl</code>
on disconnection.</td>
    </tr>
<tr>
<td>executors</td>
<td>Number of executors to request while connecting.</td>
    </tr>
<tr>
<td>conf</td>
<td>A named list of additional entries to add to <code>sparklyr.shell.conf</code>.</td>
    </tr>
<tr>
<td>timeout</td>
<td>Total seconds to wait before giving up on connection.</td>
    </tr>
<tr>
<td>ports</td>
<td>Ports to forward using kubectl.</td>
    </tr>
<tr>
<td>fix_config</td>
<td>Should the spark-defaults.conf get fixed? <code>TRUE</code> for Windows.</td>
    </tr>
<tr>
<td>...</td>
<td>Additional parameters, currently not in use.</td>
    </tr>
    </table>
<h2>Retrieve Available Settings </h2>
Retrieves available sparklyr settings that can be used in configuration files or <code><a href='spark_config.html'>spark_config()</a></code>.
<code class="sourceCode r">spark_config_settings()</code>
<h2>Find Spark Connection </h2>
<a href="#arguments">Arguments</a>
Finds an active spark connection in the environment given the
connection parameters.
<code class="sourceCode r">spark_connection_find(master = NULL, app_name = NULL, method = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>master</td>
<td>The Spark master parameter.</td>
    </tr>
<tr>
<td>app_name</td>
<td>The Spark application name.</td>
    </tr>
<tr>
<td>method</td>
<td>The method used to connect to Spark.</td>
    </tr>
    </table>
<h2>Fallback to Spark Dependency </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Helper function to assist falling back to previous Spark versions.
<code class="sourceCode r">spark_dependency_fallback(spark_version, supported_versions)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>spark_version</td>
<td>The Spark version being requested in <code>spark_dependencies</code>.</td>
    </tr>
<tr>
<td>supported_versions</td>
<td>The Spark versions that are supported by this extension.</td>
    </tr>
    </table>
<h3>Value</h3>
A Spark version to use.
<h2>Create Spark Extension </h2>
<a href="#arguments">Arguments</a>
Creates an R package ready to be used as an Spark extension.
<code class="sourceCode r">spark_extension(path)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>path</td>
<td>Location where the extension will be created.</td>
    </tr>
    </table>
<h2>Reads from a Spark Table into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Reads from a Spark Table into a Spark DataFrame.
<code class="sourceCode r">spark_load_table(sc, name, path, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0,
  memory = TRUE, overwrite = TRUE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read libsvm file into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Read libsvm file into a Spark DataFrame.
<code class="sourceCode r">spark_read_libsvm(sc, name = NULL, path = name, repartition = 0,
  memory = TRUE, overwrite = TRUE, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read a CSV file into a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Read a tabular data file into a Spark DataFrame.
<code class="sourceCode r">spark_read_csv(sc, name = NULL, path = name, header = TRUE,
  columns = NULL, infer_schema = <a href='https://rdrr.io/r/base/NULL.html'>is.null</a>(columns), delimiter = ",",
  quote = "\"", escape = "\\", charset = "UTF-8",
  null_value = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0,
  memory = TRUE, overwrite = TRUE, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>header</td>
<td>Boolean; should the first row of data be used as a header?
Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>infer_schema</td>
<td>Boolean; should column types be automatically inferred?
Requires one extra pass over the data. 
Defaults to <code><a href='https://rdrr.io/r/base/NULL.html'>is.null(columns)</a></code>.</td>
    </tr>
<tr>
<td>delimiter</td>
<td>The character used to delimit each column. 
Defaults to <samp>','</samp>.</td>
    </tr>
<tr>
<td>quote</td>
<td>The character used as a quote. 
Defaults to <samp>'"'</samp>.</td>
    </tr>
<tr>
<td>escape</td>
<td>The character used to escape other characters. 
Defaults to <samp>'\'</samp>.</td>
    </tr>
<tr>
<td>charset</td>
<td>The character set. 
Defaults to <samp>"UTF-8"</samp>.</td>
    </tr>
<tr>
<td>null_value</td>
<td>The character to use for null, or missing, values. 
Defaults to <code>NULL</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>),
  as well as the local file system (<code>file://</code>).

If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf<code> spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href='http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html'>Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href='https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_properties'>s3n authentication properties</a>

When <code>header</code> is <code>FALSE</code>, the column names are generated with a<code> V</code> prefix; e.g. <code> V1, V2, ...</code>.
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read from Delta Lake into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Read from Delta Lake into a Spark DataFrame.
<code class="sourceCode r">spark_read_delta(sc, path, name = NULL, version = NULL,
  timestamp = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0,
  memory = TRUE, overwrite = TRUE, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>version</td>
<td>The version of the delta table to read.</td>
    </tr>
<tr>
<td>timestamp</td>
<td>The timestamp of the delta table to read. 
For example,<code> "2019-01-01"</code> or <code>"2019-01-01'T'00:00:00.000Z"</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read from JDBC connection into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Read from JDBC connection into a Spark DataFrame.
<code class="sourceCode r">spark_read_jdbc(sc, name, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0,
  memory = TRUE, overwrite = TRUE, columns = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read a JSON file into a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Read a table serialized in the <a href='http://www.json.org/'>JavaScript
Object Notation</a> format into a Spark DataFrame.
<code class="sourceCode r">spark_read_json(sc, name = NULL, path = name, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  repartition = 0, memory = TRUE, overwrite = TRUE, columns = NULL,
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
  the local file system (<code>file://</code>).

If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf<code> spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href='http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html'>Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href='https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_properties'>s3n authentication properties</a>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read a ORC file into a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Read a <a href='https://orc.apache.org/'>ORC</a> file into a Spark
DataFrame.
<code class="sourceCode r">spark_read_orc(sc, name = NULL, path = name, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  repartition = 0, memory = TRUE, overwrite = TRUE, columns = NULL,
  schema = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>schema</td>
<td>A (java) read schema. 
Useful for optimizing read operation on nested data.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
  the local file system (<code>file://</code>).
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read a Parquet file into a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Read a <a href='https://parquet.apache.org/'>Parquet</a> file into a Spark
DataFrame.
<code class="sourceCode r">spark_read_parquet(sc, name = NULL, path = name, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  repartition = 0, memory = TRUE, overwrite = TRUE, columns = NULL,
  schema = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>schema</td>
<td>A (java) read schema. 
Useful for optimizing read operation on nested data.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
  the local file system (<code>file://</code>).

If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf<code> spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href='http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html'>Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href='https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_properties'>s3n authentication properties</a>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read from a generic source into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Read from a generic source into a Spark DataFrame.
<code class="sourceCode r">spark_read_source(sc, name = NULL, path = name, source,
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0, memory = TRUE,
  overwrite = TRUE, columns = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>source</td>
<td>A data source capable of reading data.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Reads from a Spark Table into a Spark DataFrame. </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Reads from a Spark Table into a Spark DataFrame.
<code class="sourceCode r">spark_read_table(sc, name, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), repartition = 0,
  memory = TRUE, columns = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Read a Text file into a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Read a text file into a Spark DataFrame.
<code class="sourceCode r">spark_read_text(sc, name = NULL, path = name, repartition = 0,
  memory = TRUE, overwrite = TRUE, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), whole = FALSE,
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions used to distribute the
generated table. 
Use 0 (the default) to avoid partitioning.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the data be loaded eagerly into memory? (That
is, should the table be cached?)</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite the table with the given name if it
already exists?</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>whole</td>
<td>Read the entire text file as a single entry? Defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
You can read data from HDFS (<code>hdfs://</code>), S3 (<code>s3a://</code>), as well as
  the local file system (<code>file://</code>).

If you are reading from a secure S3 bucket be sure to set the following in your spark-defaults.conf<code> spark.hadoop.fs.s3a.access.key</code>, <code>spark.hadoop.fs.s3a.secret.key</code> or any of the methods outlined in the aws-sdk
documentation <a href='http://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.html'>Working with AWS credentials</a>
In order to work with the newer <code>s3a://</code> protocol also set the values for <code>spark.hadoop.fs.s3a.impl</code> and <code>spark.hadoop.fs.s3a.endpoint </code>.
In addition, to support v4 of the S3 api be sure to pass the <code>-Dcom.amazonaws.services.s3.enableV4</code> driver options
for the config key <code>spark.driver.extraJavaOptions </code>
For instructions on how to configure <code>s3n://</code> check the hadoop documentation:
<a href='https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Authentication_properties'>s3n authentication properties</a>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Write a Spark DataFrame to a CSV </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Write a Spark DataFrame to a tabular (typically, comma-separated) file.
<code class="sourceCode r">spark_write_csv(x, path, header = TRUE, delimiter = ",",
  quote = "\"", escape = "\\", charset = "UTF-8",
  null_value = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), mode = NULL,
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>header</td>
<td>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>delimiter</td>
<td>The character used to delimit each column, defaults to <code>,</code>.</td>
    </tr>
<tr>
<td>quote</td>
<td>The character used as a quote. 
Defaults to <samp>'"'</samp>.</td>
    </tr>
<tr>
<td>escape</td>
<td>The character used to escape other characters, defaults to <code>\</code>.</td>
    </tr>
<tr>
<td>charset</td>
<td>The character set, defaults to <code>"UTF-8"</code>.</td>
    </tr>
<tr>
<td>null_value</td>
<td>The character to use for default values, defaults to <code>NULL</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Writes a Spark DataFrame into Delta Lake </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Writes a Spark DataFrame into Delta Lake.
<code class="sourceCode r">spark_write_delta(x, path, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Writes a Spark DataFrame into a JDBC table </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Writes a Spark DataFrame into a JDBC table.
<code class="sourceCode r">spark_write_jdbc(x, name, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Write a Spark DataFrame to a JSON file </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Serialize a Spark DataFrame to the <a href='http://www.json.org/'>JavaScript
Object Notation</a> format.
<code class="sourceCode r">spark_write_json(x, path, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Write a Spark DataFrame to a ORC file </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Serialize a Spark DataFrame to the
<a href='https://orc.apache.org/'>ORC</a> format.
<code class="sourceCode r">spark_write_orc(x, path, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Write a Spark DataFrame to a Parquet file </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Serialize a Spark DataFrame to the
<a href='https://parquet.apache.org/'>Parquet</a> format.
<code class="sourceCode r">spark_write_parquet(x, path, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options. 
See <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration'>http://spark.apache.org/docs/latest/sql-programming-guide.html#configuration</a>.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Writes a Spark DataFrame into a generic source </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Writes a Spark DataFrame into a generic source.
<code class="sourceCode r">spark_write_source(x, source, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>source</td>
<td>A data source capable of reading data.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Writes a Spark DataFrame into a Spark table </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Writes a Spark DataFrame into a Spark table.
<code class="sourceCode r">spark_write_table(x, name, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated table.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_text.html'>spark_write_text</a></code>
<h2>Write a Spark DataFrame to a Text file </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Serialize a Spark DataFrame to the plain text format.
<code class="sourceCode r">spark_write_text(x, path, mode = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  partition_by = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>A <code>character</code> element. 
Specifies the behavior when data or
  table already exists. 
Supported values include: 'error', 'append', 'overwrite' and
  ignore. 
Notice that 'overwrite' will also change the column structure.

For more details see also <a href='http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes'>http://spark.apache.org/docs/latest/sql-programming-guide.html#save-modes</a>
  for your version of Spark.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>partition_by</td>
<td>A <code>character</code> vector. 
Partitions the output by the given columns on the file system.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark serialization routines: <code><a href='spark_load_table.html'>spark_load_table</a></code>,
  <code><a href='spark_read_csv.html'>spark_read_csv</a></code>,
  <code><a href='spark_read_delta.html'>spark_read_delta</a></code>,
  <code><a href='spark_read_jdbc.html'>spark_read_jdbc</a></code>,
  <code><a href='spark_read_json.html'>spark_read_json</a></code>,
  <code><a href='spark_read_libsvm.html'>spark_read_libsvm</a></code>,
  <code><a href='spark_read_orc.html'>spark_read_orc</a></code>,
  <code><a href='spark_read_parquet.html'>spark_read_parquet</a></code>,
  <code><a href='spark_read_source.html'>spark_read_source</a></code>,
  <code><a href='spark_read_table.html'>spark_read_table</a></code>,
  <code><a href='spark_read_text.html'>spark_read_text</a></code>,
  <code><a href='spark_save_table.html'>spark_save_table</a></code>,
  <code><a href='spark_write_csv.html'>spark_write_csv</a></code>,
  <code><a href='spark_write_delta.html'>spark_write_delta</a></code>,
  <code><a href='spark_write_jdbc.html'>spark_write_jdbc</a></code>,
  <code><a href='spark_write_json.html'>spark_write_json</a></code>,
  <code><a href='spark_write_orc.html'>spark_write_orc</a></code>,
  <code><a href='spark_write_parquet.html'>spark_write_parquet</a></code>,
  <code><a href='spark_write_source.html'>spark_write_source</a></code>,
  <code><a href='spark_write_table.html'>spark_write_table</a></code>
<h2>Save / Load a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Routines for saving and loading Spark DataFrames.
<code class="sourceCode r">sdf_save_table(x, name, overwrite = FALSE, append = FALSE)

sdf_load_table(sc, name)

sdf_save_parquet(x, path, overwrite = FALSE, append = FALSE)

sdf_load_parquet(sc, path)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The table name to assign to the saved Spark DataFrame.</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite a pre-existing table of the same name?</td>
    </tr>
<tr>
<td>append</td>
<td>Boolean; append to a pre-existing table of the same name?</td>
    </tr>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code> object.</td>
    </tr>
<tr>
<td>path</td>
<td>The path where the Spark DataFrame should be saved.</td>
    </tr>
    </table>
<h2>Spark ML -- Transform, fit, and predict methods (sdf_ interface) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Deprecated methods for transformation, fit, and prediction. 
These are mirrors of the corresponding <a href='ml-transform-methods.html'>ml-transform-methods</a>.
<code class="sourceCode r">sdf_predict(x, model, ...)

sdf_transform(x, transformer, ...)

sdf_fit(x, estimator, ...)

sdf_fit_and_transform(x, estimator, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>model</td>
<td>A <code>ml_transformer</code> or a <code>ml_model</code> object.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments passed to the corresponding <code>ml_</code> methods.</td>
    </tr>
<tr>
<td>transformer</td>
<td>A <code>ml_transformer</code> object.</td>
    </tr>
<tr>
<td>estimator</td>
<td>A <code>ml_estimator</code> object.</td>
    </tr>
    </table>
<h3>Value</h3><code> sdf_predict()</code>, <code>sdf_transform()</code>, and <code>sdf_fit_and_transform()</code> return a transformed dataframe whereas <code>sdf_fit()</code> returns a <code>ml_transformer</code>.
<h2>Create DataFrame for along Object </h2>
<a href="#arguments">Arguments</a>
Creates a DataFrame along the given object.
<code class="sourceCode r">sdf_along(sc, along, repartition = NULL, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("integer",
  "integer64"))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>The associated Spark connection.</td>
    </tr>
<tr>
<td>along</td>
<td>Takes the length from the length of this argument.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions to use when distributing the
data across the Spark cluster.</td>
    </tr>
<tr>
<td>type</td>
<td>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</td>
    </tr>
    </table>
<h2>Bind multiple Spark DataFrames by row and column </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a><code> sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> are implementation of the common pattern of<code> <a href='https://rdrr.io/r/base/do.call.html'>do.call(rbind, sdfs)</a></code> or <code><a href='https://rdrr.io/r/base/do.call.html'>do.call(cbind, sdfs)</a></code> for binding many
Spark DataFrames into one.
<code class="sourceCode r">sdf_bind_rows(..., id = NULL)

sdf_bind_cols(...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>...</td>
<td>Spark tbls to combine.

Each argument can either be a Spark DataFrame or a list of
  Spark DataFrames

When row-binding, columns are matched by name, and any missing
  columns with be filled with NA.

When column-binding, rows are matched by position, so all data
  frames must have the same number of rows.</td>
    </tr>
<tr>
<td>id</td>
<td>Data frame identifier.

When <code>id</code> is supplied, a new column of identifiers is
  created to link each row to its original Spark DataFrame. 
The labels
  are taken from the named arguments to <code>sdf_bind_rows()</code>. 
When a
  list of Spark DataFrames is supplied, the labels are taken from the
  names of the list. 
If no names are found a numeric sequence is
  used instead.</td>
    </tr>
    </table>
<h3>Value</h3><code> sdf_bind_rows()</code> and <code>sdf_bind_cols()</code> return <code>tbl_spark</code>
<h3>Details</h3>
The output of <code>sdf_bind_rows()</code> will contain a column if that column
appears in any of the inputs.
<h2>Broadcast hint </h2>
<a href="#arguments">Arguments</a>
Used to force broadcast hash joins.
<code class="sourceCode r">sdf_broadcast(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
    </table>
<h2>Checkpoint a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Checkpoint a Spark DataFrame
<code class="sourceCode r">sdf_checkpoint(x, eager = TRUE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>an object coercible to a Spark DataFrame</td>
    </tr>
<tr>
<td>eager</td>
<td>whether to truncate the lineage of the DataFrame</td>
    </tr>
    </table>
<h2>Coalesces a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Coalesces a Spark DataFrame
<code class="sourceCode r">sdf_coalesce(x, partitions)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>partitions</td>
<td>number of partitions</td>
    </tr>
    </table>
<h2>Collect a Spark DataFrame into R. </h2>
<a href="#arguments">Arguments</a>
Collects a Spark dataframe into R.
<code class="sourceCode r">sdf_collect(object, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>object</td>
<td>Spark dataframe to collect</td>
    </tr>
<tr>
<td>...</td>
<td>Additional options.</td>
    </tr>
    </table>
<h2>Copy an Object into Spark </h2>
<a href="#arguments">Arguments</a>
    <a href="#advanced-usage">Advanced Usage</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Copy an object into Spark, and return an R object wrapping the
copied object (typically, a Spark DataFrame).
<code class="sourceCode r">sdf_copy_to(sc, x, name, memory, repartition, overwrite, ...)

sdf_import(x, sc, name, memory, repartition, overwrite, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>The associated Spark connection.</td>
    </tr>
<tr>
<td>x</td>
<td>An R object from which a Spark DataFrame can be generated.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the copied table in Spark.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the table be cached into memory?</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions to use when distributing the
table across the Spark cluster. 
The default (0) can be used to avoid
partitioning.</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Boolean; overwrite a pre-existing table with the name <code>name</code>
if one already exists?</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, passed to implementing methods.</td>
    </tr>
    </table>
<h2 id="advanced-usage">Advanced Usage</h2>
<code> sdf_copy_to</code> is an S3 generic that, by default, dispatches to<code> sdf_import</code>. 
Package authors that would like to implement<code> sdf_copy_to</code> for a custom object type can accomplish this by
implementing the associated method on <code>sdf_import</code>.
<h3>See also</h3>
Other Spark data frames: <code><a href='sdf_random_split.html'>sdf_random_split</a></code>,
  <code><a href='sdf_register.html'>sdf_register</a></code>, <code><a href='sdf_sample.html'>sdf_sample</a></code>,
  <code><a href='sdf_sort.html'>sdf_sort</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "spark://HOST:PORT")
sdf_copy_to(sc, iris)</div>#&gt;     Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
#&gt; 1            5.1         3.5          1.4         0.2     setosa
#&gt; 2            4.9         3.0          1.4         0.2     setosa
#&gt; 3            4.7         3.2          1.3         0.2     setosa
#&gt; 4            4.6         3.1          1.5         0.2     setosa
#&gt; 5            5.0         3.6          1.4         0.2     setosa
#&gt; 6            5.4         3.9          1.7         0.4     setosa
#&gt; 7            4.6         3.4          1.4         0.3     setosa
#&gt; 8            5.0         3.4          1.5         0.2     setosa
#&gt; 9            4.4         2.9          1.4         0.2     setosa
#&gt; 10           4.9         3.1          1.5         0.1     setosa
#&gt; 11           5.4         3.7          1.5         0.2     setosa
#&gt; 12           4.8         3.4          1.6         0.2     setosa
#&gt; 13           4.8         3.0          1.4         0.1     setosa
#&gt; 14           4.3         3.0          1.1         0.1     setosa
#&gt; 15           5.8         4.0          1.2         0.2     setosa
#&gt; 16           5.7         4.4          1.5         0.4     setosa
#&gt; 17           5.4         3.9          1.3         0.4     setosa
#&gt; 18           5.1         3.5          1.4         0.3     setosa
#&gt; 19           5.7         3.8          1.7         0.3     setosa
#&gt; 20           5.1         3.8          1.5         0.3     setosa
#&gt; 21           5.4         3.4          1.7         0.2     setosa
#&gt; 22           5.1         3.7          1.5         0.4     setosa
#&gt; 23           4.6         3.6          1.0         0.2     setosa
#&gt; 24           5.1         3.3          1.7         0.5     setosa
#&gt; 25           4.8         3.4          1.9         0.2     setosa
#&gt; 26           5.0         3.0          1.6         0.2     setosa
#&gt; 27           5.0         3.4          1.6         0.4     setosa
#&gt; 28           5.2         3.5          1.5         0.2     setosa
#&gt; 29           5.2         3.4          1.4         0.2     setosa
#&gt; 30           4.7         3.2          1.6         0.2     setosa
#&gt; 31           4.8         3.1          1.6         0.2     setosa
#&gt; 32           5.4         3.4          1.5         0.4     setosa
#&gt; 33           5.2         4.1          1.5         0.1     setosa
#&gt; 34           5.5         4.2          1.4         0.2     setosa
#&gt; 35           4.9         3.1          1.5         0.2     setosa
#&gt; 36           5.0         3.2          1.2         0.2     setosa
#&gt; 37           5.5         3.5          1.3         0.2     setosa
#&gt; 38           4.9         3.6          1.4         0.1     setosa
#&gt; 39           4.4         3.0          1.3         0.2     setosa
#&gt; 40           5.1         3.4          1.5         0.2     setosa
#&gt; 41           5.0         3.5          1.3         0.3     setosa
#&gt; 42           4.5         2.3          1.3         0.3     setosa
#&gt; 43           4.4         3.2          1.3         0.2     setosa
#&gt; 44           5.0         3.5          1.6         0.6     setosa
#&gt; 45           5.1         3.8          1.9         0.4     setosa
#&gt; 46           4.8         3.0          1.4         0.3     setosa
#&gt; 47           5.1         3.8          1.6         0.2     setosa
#&gt; 48           4.6         3.2          1.4         0.2     setosa
#&gt; 49           5.3         3.7          1.5         0.2     setosa
#&gt; 50           5.0         3.3          1.4         0.2     setosa
#&gt; 51           7.0         3.2          4.7         1.4 versicolor
#&gt; 52           6.4         3.2          4.5         1.5 versicolor
#&gt; 53           6.9         3.1          4.9         1.5 versicolor
#&gt; 54           5.5         2.3          4.0         1.3 versicolor
#&gt; 55           6.5         2.8          4.6         1.5 versicolor
#&gt; 56           5.7         2.8          4.5         1.3 versicolor
#&gt; 57           6.3         3.3          4.7         1.6 versicolor
#&gt; 58           4.9         2.4          3.3         1.0 versicolor
#&gt; 59           6.6         2.9          4.6         1.3 versicolor
#&gt; 60           5.2         2.7          3.9         1.4 versicolor
#&gt; 61           5.0         2.0          3.5         1.0 versicolor
#&gt; 62           5.9         3.0          4.2         1.5 versicolor
#&gt; 63           6.0         2.2          4.0         1.0 versicolor
#&gt; 64           6.1         2.9          4.7         1.4 versicolor
#&gt; 65           5.6         2.9          3.6         1.3 versicolor
#&gt; 66           6.7         3.1          4.4         1.4 versicolor
#&gt; 67           5.6         3.0          4.5         1.5 versicolor
#&gt; 68           5.8         2.7          4.1         1.0 versicolor
#&gt; 69           6.2         2.2          4.5         1.5 versicolor
#&gt; 70           5.6         2.5          3.9         1.1 versicolor
#&gt; 71           5.9         3.2          4.8         1.8 versicolor
#&gt; 72           6.1         2.8          4.0         1.3 versicolor
#&gt; 73           6.3         2.5          4.9         1.5 versicolor
#&gt; 74           6.1         2.8          4.7         1.2 versicolor
#&gt; 75           6.4         2.9          4.3         1.3 versicolor
#&gt; 76           6.6         3.0          4.4         1.4 versicolor
#&gt; 77           6.8         2.8          4.8         1.4 versicolor
#&gt; 78           6.7         3.0          5.0         1.7 versicolor
#&gt; 79           6.0         2.9          4.5         1.5 versicolor
#&gt; 80           5.7         2.6          3.5         1.0 versicolor
#&gt; 81           5.5         2.4          3.8         1.1 versicolor
#&gt; 82           5.5         2.4          3.7         1.0 versicolor
#&gt; 83           5.8         2.7          3.9         1.2 versicolor
#&gt; 84           6.0         2.7          5.1         1.6 versicolor
#&gt; 85           5.4         3.0          4.5         1.5 versicolor
#&gt; 86           6.0         3.4          4.5         1.6 versicolor
#&gt; 87           6.7         3.1          4.7         1.5 versicolor
#&gt; 88           6.3         2.3          4.4         1.3 versicolor
#&gt; 89           5.6         3.0          4.1         1.3 versicolor
#&gt; 90           5.5         2.5          4.0         1.3 versicolor
#&gt; 91           5.5         2.6          4.4         1.2 versicolor
#&gt; 92           6.1         3.0          4.6         1.4 versicolor
#&gt; 93           5.8         2.6          4.0         1.2 versicolor
#&gt; 94           5.0         2.3          3.3         1.0 versicolor
#&gt; 95           5.6         2.7          4.2         1.3 versicolor
#&gt; 96           5.7         3.0          4.2         1.2 versicolor
#&gt; 97           5.7         2.9          4.2         1.3 versicolor
#&gt; 98           6.2         2.9          4.3         1.3 versicolor
#&gt; 99           5.1         2.5          3.0         1.1 versicolor
#&gt; 100          5.7         2.8          4.1         1.3 versicolor
#&gt; 101          6.3         3.3          6.0         2.5  virginica
#&gt; 102          5.8         2.7          5.1         1.9  virginica
#&gt; 103          7.1         3.0          5.9         2.1  virginica
#&gt; 104          6.3         2.9          5.6         1.8  virginica
#&gt; 105          6.5         3.0          5.8         2.2  virginica
#&gt; 106          7.6         3.0          6.6         2.1  virginica
#&gt; 107          4.9         2.5          4.5         1.7  virginica
#&gt; 108          7.3         2.9          6.3         1.8  virginica
#&gt; 109          6.7         2.5          5.8         1.8  virginica
#&gt; 110          7.2         3.6          6.1         2.5  virginica
#&gt; 111          6.5         3.2          5.1         2.0  virginica
#&gt; 112          6.4         2.7          5.3         1.9  virginica
#&gt; 113          6.8         3.0          5.5         2.1  virginica
#&gt; 114          5.7         2.5          5.0         2.0  virginica
#&gt; 115          5.8         2.8          5.1         2.4  virginica
#&gt; 116          6.4         3.2          5.3         2.3  virginica
#&gt; 117          6.5         3.0          5.5         1.8  virginica
#&gt; 118          7.7         3.8          6.7         2.2  virginica
#&gt; 119          7.7         2.6          6.9         2.3  virginica
#&gt; 120          6.0         2.2          5.0         1.5  virginica
#&gt; 121          6.9         3.2          5.7         2.3  virginica
#&gt; 122          5.6         2.8          4.9         2.0  virginica
#&gt; 123          7.7         2.8          6.7         2.0  virginica
#&gt; 124          6.3         2.7          4.9         1.8  virginica
#&gt; 125          6.7         3.3          5.7         2.1  virginica
#&gt; 126          7.2         3.2          6.0         1.8  virginica
#&gt; 127          6.2         2.8          4.8         1.8  virginica
#&gt; 128          6.1         3.0          4.9         1.8  virginica
#&gt; 129          6.4         2.8          5.6         2.1  virginica
#&gt; 130          7.2         3.0          5.8         1.6  virginica
#&gt; 131          7.4         2.8          6.1         1.9  virginica
#&gt; 132          7.9         3.8          6.4         2.0  virginica
#&gt; 133          6.4         2.8          5.6         2.2  virginica
#&gt; 134          6.3         2.8          5.1         1.5  virginica
#&gt; 135          6.1         2.6          5.6         1.4  virginica
#&gt; 136          7.7         3.0          6.1         2.3  virginica
#&gt; 137          6.3         3.4          5.6         2.4  virginica
#&gt; 138          6.4         3.1          5.5         1.8  virginica
#&gt; 139          6.0         3.0          4.8         1.8  virginica
#&gt; 140          6.9         3.1          5.4         2.1  virginica
#&gt; 141          6.7         3.1          5.6         2.4  virginica
#&gt; 142          6.9         3.1          5.1         2.3  virginica
#&gt; 143          5.8         2.7          5.1         1.9  virginica
#&gt; 144          6.8         3.2          5.9         2.3  virginica
#&gt; 145          6.7         3.3          5.7         2.5  virginica
#&gt; 146          6.7         3.0          5.2         2.3  virginica
#&gt; 147          6.3         2.5          5.0         1.9  virginica
#&gt; 148          6.5         3.0          5.2         2.0  virginica
#&gt; 149          6.2         3.4          5.4         2.3  virginica
#&gt; 150          5.9         3.0          5.1         1.8  virginica
</div></code>
<h2>Cross Tabulation </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Builds a contingency table at each combination of factor levels.
<code class="sourceCode r">sdf_crosstab(x, col1, col2)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame</td>
    </tr>
<tr>
<td>col1</td>
<td>The name of the first column. 
Distinct items will make the first item of each row.</td>
    </tr>
<tr>
<td>col2</td>
<td>The name of the second column. 
Distinct items will make the column names of the DataFrame.</td>
    </tr>
    </table>
<h3>Value</h3>
A DataFrame containing the contingency table.
<h2>Debug Info for Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Prints plan of execution to generate <code>x</code>. 
This plan will, among other things, show the
number of partitions in parenthesis at the far left and indicate stages using indentation.
<code class="sourceCode r">sdf_debug_string(x, print = TRUE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An R object wrapping, or containing, a Spark DataFrame.</td>
    </tr>
<tr>
<td>print</td>
<td>Print debug information?</td>
    </tr>
    </table>
<h2>Compute summary statistics for columns of a data frame </h2>
<a href="#arguments">Arguments</a>
Compute summary statistics for columns of a data frame
<code class="sourceCode r">sdf_describe(x, cols = <a href='https://rdrr.io/r/base/colnames.html'>colnames</a>(x))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercible to a Spark DataFrame</td>
    </tr>
<tr>
<td>cols</td>
<td>Columns to compute statistics for, given as a character vector</td>
    </tr>
    </table>
<h2>Support for Dimension Operations </h2>
<a href="#arguments">Arguments</a><code> sdf_dim()</code>,  <code>sdf_nrow()</code> and <code>sdf_ncol()</code> provide similar
functionality to <code><a href='https://rdrr.io/r/base/dim.html'>dim()</a></code>, <code><a href='https://rdrr.io/r/base/nrow.html'>nrow()</a></code> and <code><a href='https://rdrr.io/r/base/nrow.html'>ncol()</a></code>.
<code class="sourceCode r">sdf_dim(x)

sdf_nrow(x)

sdf_ncol(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object (usually a <code>spark_tbl</code>).</td>
    </tr>
    </table>
<h2>Spark DataFrame is Streaming </h2>
<a href="#arguments">Arguments</a>
Is the given Spark DataFrame a streaming data?
<code class="sourceCode r">sdf_is_streaming(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
    </table>
<h2>Returns the last index of a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Returns the last index of a Spark DataFrame. 
The Spark<code> mapPartitionsWithIndex</code> function is used to iterate
through the last nonempty partition of the RDD to find the last record.
<code class="sourceCode r">sdf_last_index(x, id = "id")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>id</td>
<td>The name of the index column.</td>
    </tr>
    </table>
<h2>Create DataFrame for Length </h2>
<a href="#arguments">Arguments</a>
Creates a DataFrame for the given length.
<code class="sourceCode r">sdf_len(sc, length, repartition = NULL, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("integer",
  "integer64"))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>The associated Spark connection.</td>
    </tr>
<tr>
<td>length</td>
<td>The desired length of the sequence.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions to use when distributing the
data across the Spark cluster.</td>
    </tr>
<tr>
<td>type</td>
<td>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</td>
    </tr>
    </table>
<h2>Gets number of partitions of a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Gets number of partitions of a Spark DataFrame
<code class="sourceCode r">sdf_num_partitions(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
    </table>
<h2>Persist a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
Persist a Spark DataFrame, forcing any pending computations and (optionally)
serializing the results to disk.
<code class="sourceCode r">sdf_persist(x, storage.level = "MEMORY_AND_DISK")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>storage.level</td>
<td>The storage level to be used. 
Please view the
<a href='http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence'>Spark Documentation</a>
for information on what storage levels are accepted.</td>
    </tr>
    </table>
<h3>Details</h3>
Spark DataFrames invoke their operations lazily -- pending operations are
deferred until their results are actually needed. 
Persisting a Spark
DataFrame effectively 'forces' any pending computations, and then persists
the generated Spark DataFrame as requested (to memory, to disk, or
otherwise).

Users of Spark should be careful to persist the results of any computations
which are non-deterministic -- otherwise, one might see that the values
within a column seem to 'change' as new operations are performed on that
data set.
<h2>Pivot a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#examples">Examples</a>
Construct a pivot table over a Spark Dataframe, using a syntax similar to
that from <code><a href='https://rdrr.io/pkg/reshape2/man/cast.html'>reshape2::dcast</a></code>.
<code class="sourceCode r">sdf_pivot(x, formula, fun.aggregate = "count")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>A two-sided R formula of the form <code>x_1 + x_2 + ... 
~ y_1</code>.
The left-hand side of the formula indicates which variables are used for grouping,
and the right-hand side indicates which variable is used for pivoting. 
Currently,
only a single pivot column is supported.</td>
    </tr>
<tr>
<td>fun.aggregate</td>
<td>How should the grouped dataset be aggregated? Can be
a length-one character vector, giving the name of a Spark aggregation function
to be called; a named R list mapping column names to an aggregation method,
or an R function that is invoked on the grouped dataset.</td>
    </tr>
    </table>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

# aggregating by mean
iris_tbl %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a>(Petal_Width = <a href='https://rdrr.io/r/base/ifelse.html'>ifelse</a>(Petal_Width &gt; 1.5, "High", "Low" )) %&gt;%
  sdf_pivot(Petal_Width ~ Species,
    fun.aggregate = <a href='https://rdrr.io/r/base/list.html'>list</a>(Petal_Length = "mean"))

# aggregating all observations in a list
iris_tbl %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a>(Petal_Width = <a href='https://rdrr.io/r/base/ifelse.html'>ifelse</a>(Petal_Width &gt; 1.5, "High", "Low" )) %&gt;%
  sdf_pivot(Petal_Width ~ Species,
    fun.aggregate = <a href='https://rdrr.io/r/base/list.html'>list</a>(Petal_Length = "collect_list"))
}</div></code>
<h2>Project features onto principal components </h2>
<a href="#arguments">Arguments</a>
    <a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a>
Project features onto principal components
<code class="sourceCode r">sdf_project(object, newdata, features = <a href='https://rdrr.io/r/base/dimnames.html'>dimnames</a>(object$pc)[[1]],
  feature_prefix = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>object</td>
<td>A Spark PCA model object</td>
    </tr>
<tr>
<td>newdata</td>
<td>An object coercible to a Spark DataFrame</td>
    </tr>
<tr>
<td>features</td>
<td>A vector of names of columns to be projected</td>
    </tr>
<tr>
<td>feature_prefix</td>
<td>The prefix used in naming the output features</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>
The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. 
These functions will 'force' any pending SQL in a<code> dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. 
Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the R level, these operations will only be executed when you explicitly<code> <a href='collect.html'>collect()</a></code> the table.
<h2>Compute (Approximate) Quantiles with a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Given a numeric column within a Spark DataFrame, compute
approximate quantiles (to some relative error).
<code class="sourceCode r">sdf_quantile(x, column, probabilities = <a href='https://rdrr.io/r/base/c.html'>c</a>(0, 0.25, 0.5, 0.75, 1),
  relative.error = 1e-05)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>column</td>
<td>The column for which quantiles should be computed.</td>
    </tr>
<tr>
<td>probabilities</td>
<td>A numeric vector of probabilities, for
which quantiles should be computed.</td>
    </tr>
<tr>
<td>relative.error</td>
<td>The relative error -- lower values imply more
precision in the computed quantiles.</td>
    </tr>
    </table>
<h2>Partition a Spark Dataframe </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Partition a Spark DataFrame into multiple groups. 
This routine is useful
for splitting a DataFrame into, for example, training and test datasets.
<code class="sourceCode r">sdf_random_split(x, ..., weights = NULL,
  seed = <a href='https://rdrr.io/r/base/sample.html'>sample</a>(.Machine$integer.max, 1))

sdf_partition(x, ..., weights = NULL,
  seed = <a href='https://rdrr.io/r/base/sample.html'>sample</a>(.Machine$integer.max, 1))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercable to a Spark DataFrame.</td>
    </tr>
<tr>
<td>...</td>
<td>Named parameters, mapping table names to weights. 
The weights
will be normalized such that they sum to 1.</td>
    </tr>
<tr>
<td>weights</td>
<td>An alternate mechanism for supplying weights -- when
specified, this takes precedence over the <code>...</code> arguments.</td>
    </tr>
<tr>
<td>seed</td>
<td>Random seed to use for randomly partitioning the dataset. 
Set
this if you want your partitioning to be reproducible on repeated runs.</td>
    </tr>
    </table>
<h3>Value</h3>
An R <code>list</code> of <code>tbl_spark</code>s.
<h3>Details</h3>
The sampling weights define the probability that a particular observation
will be assigned to a particular partition, not the resulting size of the
partition. 
This implies that partitioning a DataFrame with, for example,
<code> sdf_random_split(x, training = 0.5, test = 0.5)</code>

is not guaranteed to produce <code>training</code> and <code>test</code> partitions
of equal size.
<h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>
The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. 
These functions will 'force' any pending SQL in a<code> dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. 
Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the R level, these operations will only be executed when you explicitly<code> <a href='collect.html'>collect()</a></code> the table.
<h3>See also</h3>
Other Spark data frames: <code><a href='sdf_copy_to.html'>sdf_copy_to</a></code>,
  <code><a href='sdf_register.html'>sdf_register</a></code>, <code><a href='sdf_sample.html'>sdf_sample</a></code>,
  <code><a href='sdf_sort.html'>sdf_sort</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
# randomly partition data into a 'training' and 'test'
# dataset, with 60% of the observations assigned to the
# 'training' dataset, and 40% assigned to the 'test' dataset
<a href='https://rdrr.io/r/utils/data.html'>data</a>(diamonds, package = "ggplot2")
diamonds_tbl &lt;- <a href='copy_to.html'>copy_to</a>(sc, diamonds, "diamonds")
partitions &lt;- diamonds_tbl %&gt;%
  sdf_random_split(training = 0.6, test = 0.4)
<a href='https://rdrr.io/r/base/print.html'>print</a>(partitions)

# alternate way of specifying weights
weights &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>(training = 0.6, test = 0.4)
diamonds_tbl %&gt;% sdf_random_split(weights = weights)
}</div></code>
<h2>Read a Column from a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
Read a single column from a Spark DataFrame, and return
the contents of that column back to R.
<code class="sourceCode r">sdf_read_column(x, column)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>column</td>
<td>The name of a column within <code>x</code>.</td>
    </tr>
    </table>
<h3>Details</h3>
It is expected for this operation to preserve row order.
<h2>Register a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a>
    <a href="#see-also">See also</a>
Registers a Spark DataFrame (giving it a table name for the
Spark SQL context), and returns a <code>tbl_spark</code>.
<code class="sourceCode r">sdf_register(x, name = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame.</td>
    </tr>
<tr>
<td>name</td>
<td>A name to assign this table.</td>
    </tr>
    </table>
<h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>
The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. 
These functions will 'force' any pending SQL in a<code> dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. 
Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the R level, these operations will only be executed when you explicitly<code> <a href='collect.html'>collect()</a></code> the table.
<h3>See also</h3>
Other Spark data frames: <code><a href='sdf_copy_to.html'>sdf_copy_to</a></code>,
  <code><a href='sdf_random_split.html'>sdf_random_split</a></code>, <code><a href='sdf_sample.html'>sdf_sample</a></code>,
  <code><a href='sdf_sort.html'>sdf_sort</a></code>
<h2>Repartition a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Repartition a Spark DataFrame
<code class="sourceCode r">sdf_repartition(x, partitions = NULL, partition_by = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>partitions</td>
<td>number of partitions</td>
    </tr>
<tr>
<td>partition_by</td>
<td>vector of column names used for partitioning, only supported for Spark 2.0+</td>
    </tr>
    </table>
<h2>Model Residuals </h2>
<a href="#arguments">Arguments</a>
This generic method returns a Spark DataFrame with model
residuals added as a column to the model training data.
<code class="sourceCode r"># S3 method for ml_model_generalized_linear_regression
sdf_residuals(object,
  type = <a href='https://rdrr.io/r/base/c.html'>c</a>("deviance", "pearson", "working", "response"), ...)

# S3 method for ml_model_linear_regression
sdf_residuals(object, ...)

sdf_residuals(object, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>object</td>
<td>Spark ML model object.</td>
    </tr>
<tr>
<td>type</td>
<td>type of residuals which should be returned.</td>
    </tr>
<tr>
<td>...</td>
<td>additional arguments</td>
    </tr>
    </table>
<h2>Randomly Sample Rows from a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a>
    <a href="#see-also">See also</a>
Draw a random sample of rows (with or without replacement)
from a Spark DataFrame.
<code class="sourceCode r">sdf_sample(x, fraction = 1, replacement = TRUE, seed = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercable to a Spark DataFrame.</td>
    </tr>
<tr>
<td>fraction</td>
<td>The fraction to sample.</td>
    </tr>
<tr>
<td>replacement</td>
<td>Boolean; sample with replacement?</td>
    </tr>
<tr>
<td>seed</td>
<td>An (optional) integer seed.</td>
    </tr>
    </table>
<h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>
The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. 
These functions will 'force' any pending SQL in a<code> dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. 
Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the R level, these operations will only be executed when you explicitly<code> <a href='collect.html'>collect()</a></code> the table.
<h3>See also</h3>
Other Spark data frames: <code><a href='sdf_copy_to.html'>sdf_copy_to</a></code>,
  <code><a href='sdf_random_split.html'>sdf_random_split</a></code>,
  <code><a href='sdf_register.html'>sdf_register</a></code>, <code><a href='sdf_sort.html'>sdf_sort</a></code>
<h2>Read the Schema of a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
Read the schema of a Spark DataFrame.
<code class="sourceCode r">sdf_schema(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
An R <code>list</code>, with each <code>list</code> element describing the
  <code>name</code> and <code>type</code> of a column.
<h3>Details</h3>
The <code>type</code> column returned gives the string representation of the
underlying Spark  type for that column; for example, a vector of numeric
values would be returned with the type <code>"DoubleType"</code>. 
Please see the
<a href='http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.types.package'>Spark Scala API Documentation</a>
for information on what types are available and exposed by Spark.
<h2>Separate a Vector Column into Scalar Columns </h2>
<a href="#arguments">Arguments</a>
Given a vector column in a Spark DataFrame, split that
into <code>n</code> separate columns, each column made up of
the different elements in the column <code>column</code>.
<code class="sourceCode r">sdf_separate_column(x, column, into = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>column</td>
<td>The name of a (vector-typed) column.</td>
    </tr>
<tr>
<td>into</td>
<td>A specification of the columns that should be
generated from <code>column</code>. 
This can either be a
vector of column names, or an R list mapping column
names to the (1-based) index at which a particular
vector element should be extracted.</td>
    </tr>
    </table>
<h2>Create DataFrame for Range </h2>
<a href="#arguments">Arguments</a>
Creates a DataFrame for the given range
<code class="sourceCode r">sdf_seq(sc, from = 1L, to = 1L, by = 1L, repartition = type,
  type = <a href='https://rdrr.io/r/base/c.html'>c</a>("integer", "integer64"))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>The associated Spark connection.</td>
    </tr>
<tr>
<td>from, to</td>
<td>The start and end to use as a range</td>
    </tr>
<tr>
<td>by</td>
<td>The increment of the sequence.</td>
    </tr>
<tr>
<td>repartition</td>
<td>The number of partitions to use when distributing the
data across the Spark cluster.</td>
    </tr>
<tr>
<td>type</td>
<td>The data type to use for the index, either <code>"integer"</code> or <code>"integer64"</code>.</td>
    </tr>
    </table>
<h2>Sort a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#transforming-spark-dataframes">Transforming Spark DataFrames</a>
    <a href="#see-also">See also</a>
Sort a Spark DataFrame by one or more columns, with each column
sorted in ascending order.
<code class="sourceCode r">sdf_sort(x, columns)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercable to a Spark DataFrame.</td>
    </tr>
<tr>
<td>columns</td>
<td>The column(s) to sort by.</td>
    </tr>
    </table>
<h2 id="transforming-spark-dataframes">Transforming Spark DataFrames</h2>
The family of functions prefixed with <code>sdf_</code> generally access the Scala
Spark DataFrame API directly, as opposed to the <code>dplyr</code> interface which
uses Spark SQL. 
These functions will 'force' any pending SQL in a<code> dplyr</code> pipeline, such that the resulting <code>tbl_spark</code> object
returned will no longer have the attached 'lazy' SQL operations. 
Note that
the underlying Spark DataFrame <em>does</em> execute its operations lazily, so
that even though the pending set of operations (currently) are not exposed at
the R level, these operations will only be executed when you explicitly<code> <a href='collect.html'>collect()</a></code> the table.
<h3>See also</h3>
Other Spark data frames: <code><a href='sdf_copy_to.html'>sdf_copy_to</a></code>,
  <code><a href='sdf_random_split.html'>sdf_random_split</a></code>,
  <code><a href='sdf_register.html'>sdf_register</a></code>, <code><a href='sdf_sample.html'>sdf_sample</a></code>
<h2>Spark DataFrame from SQL </h2>
<a href="#arguments">Arguments</a>
Defines a Spark DataFrame from a SQL query, useful to create Spark DataFrames
without collecting the results immediately.
<code class="sourceCode r">sdf_sql(sc, sql)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>sql</td>
<td>a 'SQL' query used to generate a Spark DataFrame.</td>
    </tr>
    </table>
<h2>Add a Sequential ID Column to a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Add a sequential ID column to a Spark DataFrame. 
The Spark<code> zipWithIndex</code> function is used to produce these. 
This differs from<code> sdf_with_unique_id</code> in that the IDs generated are independent of
partitioning.
<code class="sourceCode r">sdf_with_sequential_id(x, id = "id", from = 1L)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>id</td>
<td>The name of the column to host the generated IDs.</td>
    </tr>
<tr>
<td>from</td>
<td>The starting value of the id column</td>
    </tr>
    </table>
<h2>Add a Unique ID Column to a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
Add a unique ID column to a Spark DataFrame. 
The Spark<code> monotonicallyIncreasingId</code> function is used to produce these and is
guaranteed to produce unique, monotonically increasing ids; however, there
is no guarantee that these IDs will be sequential. 
The table is persisted
immediately after the column is generated, to ensure that the column is
stable -- otherwise, it can differ across new computations.
<code class="sourceCode r">sdf_with_unique_id(x, id = "id")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>id</td>
<td>The name of the column to host the generated IDs.</td>
    </tr>
    </table>
<h2>Spark ML -- Decision Trees </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform classification and regression using decision trees.
<code class="sourceCode r">ml_decision_tree_classifier(x, formula = NULL, max_depth = 5,
  max_bins = 32, min_instances_per_node = 1, min_info_gain = 0,
  impurity = "gini", seed = NULL, thresholds = NULL,
  cache_node_ids = FALSE, checkpoint_interval = 10,
  max_memory_in_mb = 256, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("decision_tree_classifier_"), ...)

ml_decision_tree(x, formula = NULL, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("auto", "regression",
  "classification"), features_col = "features", label_col = "label",
  prediction_col = "prediction", variance_col = NULL,
  probability_col = "probability",
  raw_prediction_col = "rawPrediction", checkpoint_interval = 10L,
  impurity = "auto", max_bins = 32L, max_depth = 5L,
  min_info_gain = 0, min_instances_per_node = 1L, seed = NULL,
  thresholds = NULL, cache_node_ids = FALSE, max_memory_in_mb = 256L,
  uid = <a href='random_string.html'>random_string</a>("decision_tree_"), response = NULL,
  features = NULL, ...)

ml_decision_tree_regressor(x, formula = NULL, max_depth = 5,
  max_bins = 32, min_instances_per_node = 1, min_info_gain = 0,
  impurity = "variance", seed = NULL, cache_node_ids = FALSE,
  checkpoint_interval = 10, max_memory_in_mb = 256,
  variance_col = NULL, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("decision_tree_regressor_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>max_depth</td>
<td>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</td>
    </tr>
<tr>
<td>max_bins</td>
<td>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. 
More bins give higher granularity.</td>
    </tr>
<tr>
<td>min_instances_per_node</td>
<td>Minimum number of instances each child must
have after split.</td>
    </tr>
<tr>
<td>min_info_gain</td>
<td>Minimum information gain for a split to be considered
at a tree node. 
Should be &gt;= 0, defaults to 0.</td>
    </tr>
<tr>
<td>impurity</td>
<td>Criterion used for information gain calculation. 
Supported: "entropy"
and "gini" (default) for classification and "variance" (default) for regression. 
For<code> ml_decision_tree</code>, setting <code>"auto"</code> will default to the appropriate
criterion based on model type.</td>
    </tr>
<tr>
<td>seed</td>
<td>Seed for random numbers.</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>cache_node_ids</td>
<td>If <code>FALSE</code>, the algorithm will pass trees to executors to match instances with nodes.
If <code>TRUE</code>, the algorithm will cache node IDs for each instance. 
Caching can speed up training of deeper trees.
Defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>checkpoint_interval</td>
<td>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 
10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</td>
    </tr>
<tr>
<td>max_memory_in_mb</td>
<td>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration,
and its aggregates may exceed this size. 
Defaults to 256.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
<tr>
<td>type</td>
<td>The type of model to fit. <code> "regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. 
When <code>"auto"</code> is used, the model type is
inferred based on the response variable type -- if it is a numeric type,
then regression is used; classification otherwise.</td>
    </tr>
<tr>
<td>variance_col</td>
<td>(Optional) Column name for the biased sample variance of prediction.</td>
    </tr>
<tr>
<td>response</td>
<td>(Deprecated) The name of the response column (as a length-one character vector.)</td>
    </tr>
<tr>
<td>features</td>
<td>(Deprecated) The name of features (terms) to use for the model fit.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<code> ml_decision_tree</code> is a wrapper around <code>ml_decision_tree_regressor.tbl_spark</code> and <code>ml_decision_tree_classifier.tbl_spark</code> and calls the appropriate method based on model type.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

dt_model &lt;- iris_training %&gt;%
  ml_decision_tree(Species ~ .)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(dt_model, iris_test)

<a href='ml_evaluator.html'>ml_multiclass_classification_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- Generalized Linear Regression </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform regression using Generalized Linear Model (GLM).
<code class="sourceCode r">ml_generalized_linear_regression(x, formula = NULL,
  family = "gaussian", link = NULL, fit_intercept = TRUE,
  offset_col = NULL, link_power = NULL, link_prediction_col = NULL,
  reg_param = 0, max_iter = 25, weight_col = NULL, solver = "irls",
  tol = 1e-06, variance_power = 0, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("generalized_linear_regression_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>family</td>
<td>Name of family which is a description of the error distribution to be used in the model. 
Supported options: "gaussian", "binomial", "poisson", "gamma" and "tweedie". 
Default is "gaussian".</td>
    </tr>
<tr>
<td>link</td>
<td>Name of link function which provides the relationship between the linear predictor and the mean of the distribution function. 
See for supported link functions.</td>
    </tr>
<tr>
<td>fit_intercept</td>
<td>Boolean; should the model be fit with an intercept term?</td>
    </tr>
<tr>
<td>offset_col</td>
<td>Offset column name. 
If this is not set, we treat all instance offsets as 0.0. 
The feature specified as offset has a constant coefficient of 1.0.</td>
    </tr>
<tr>
<td>link_power</td>
<td>Index in the power link function. 
Only applicable to the Tweedie family. 
Note that link power 0, 1, -1 or 0.5 corresponds to the Log, Identity, Inverse or Sqrt link, respectively. 
When not set, this value defaults to 1 - variancePower, which matches the R "statmod" package.</td>
    </tr>
<tr>
<td>link_prediction_col</td>
<td>Link prediction (linear predictor) column name. 
Default is not set, which means we do not output link prediction.</td>
    </tr>
<tr>
<td>reg_param</td>
<td>Regularization parameter (aka lambda)</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>weight_col</td>
<td>The name of the column to use as weights for the model fit.</td>
    </tr>
<tr>
<td>solver</td>
<td>Solver algorithm for optimization.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>variance_power</td>
<td>Power in the variance function of the Tweedie distribution which provides the relationship between the variance and mean of the distribution. 
Only applicable to the Tweedie family. 
(see <a href='https://en.wikipedia.org/wiki/Tweedie_distribution'>Tweedie Distribution (Wikipedia)</a>) Supported values: 0 and [1, Inf). 
Note that variance power 0, 1, or 2 corresponds to the Gaussian, Poisson or Gamma family, respectively.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.

Valid link functions for each family is listed below. 
The first link function of each family is the default one.

gaussian: "identity", "log", "inverse"

binomial: "logit", "probit", "loglog"

poisson: "log", "identity", "sqrt"

gamma: "inverse", "identity", "log"

tweedie: power link function specified through <code>link_power</code>. 
The default link power in the tweedie family is <code>1 - variance_power</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
mtcars_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

# Specify the grid
family &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("gaussian", "gamma", "poisson")
link &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("identity", "log")
family_link &lt;- <a href='https://rdrr.io/r/base/expand.grid.html'>expand.grid</a>(family = family, link = link, stringsAsFactors = FALSE)
family_link &lt;- <a href='https://rdrr.io/r/base/data.frame.html'>data.frame</a>(family_link, rmse = 0)

# Train the models
for (i in 1:<a href='https://rdrr.io/r/base/nrow.html'>nrow</a>(family_link)) {
  glm_model &lt;- mtcars_training %&gt;%
    ml_generalized_linear_regression(mpg ~ .,
family = family_link[i, 1],
link = family_link[i, 2]
    )

  pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(glm_model, mtcars_test)
  family_link[i, 3] &lt;- <a href='ml_evaluator.html'>ml_regression_evaluator</a>(pred, label_col = "mpg")
}

family_link
}</div></code>
<h2>Spark ML -- Gradient Boosted Trees </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform binary classification and regression using gradient boosted trees. 
Multiclass classification is not supported yet.
<code class="sourceCode r">ml_gbt_classifier(x, formula = NULL, max_iter = 20, max_depth = 5,
  step_size = 0.1, subsampling_rate = 1,
  feature_subset_strategy = "auto", min_instances_per_node = 1L,
  max_bins = 32, min_info_gain = 0, loss_type = "logistic",
  seed = NULL, thresholds = NULL, checkpoint_interval = 10,
  cache_node_ids = FALSE, max_memory_in_mb = 256,
  features_col = "features", label_col = "label",
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("gbt_classifier_"), ...)

ml_gradient_boosted_trees(x, formula = NULL, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("auto",
  "regression", "classification"), features_col = "features",
  label_col = "label", prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction", checkpoint_interval = 10,
  loss_type = <a href='https://rdrr.io/r/base/c.html'>c</a>("auto", "logistic", "squared", "absolute"),
  max_bins = 32, max_depth = 5, max_iter = 20L, min_info_gain = 0,
  min_instances_per_node = 1, step_size = 0.1, subsampling_rate = 1,
  feature_subset_strategy = "auto", seed = NULL, thresholds = NULL,
  cache_node_ids = FALSE, max_memory_in_mb = 256,
  uid = <a href='random_string.html'>random_string</a>("gradient_boosted_trees_"), response = NULL,
  features = NULL, ...)

ml_gbt_regressor(x, formula = NULL, max_iter = 20, max_depth = 5,
  step_size = 0.1, subsampling_rate = 1,
  feature_subset_strategy = "auto", min_instances_per_node = 1,
  max_bins = 32, min_info_gain = 0, loss_type = "squared",
  seed = NULL, checkpoint_interval = 10, cache_node_ids = FALSE,
  max_memory_in_mb = 256, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("gbt_regressor_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>max_iter</td>
<td>Maxmimum number of iterations.</td>
    </tr>
<tr>
<td>max_depth</td>
<td>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</td>
    </tr>
<tr>
<td>step_size</td>
<td>Step size (a.k.a. 
learning rate) in interval (0, 1] for shrinking the contribution of each estimator. 
(default = 0.1)</td>
    </tr>
<tr>
<td>subsampling_rate</td>
<td>Fraction of the training data used for learning each decision tree, in range (0, 1]. 
(default = 1.0)</td>
    </tr>
<tr>
<td>feature_subset_strategy</td>
<td>The number of features to consider for splits at each tree node. 
See details for options.</td>
    </tr>
<tr>
<td>min_instances_per_node</td>
<td>Minimum number of instances each child must
have after split.</td>
    </tr>
<tr>
<td>max_bins</td>
<td>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. 
More bins give higher granularity.</td>
    </tr>
<tr>
<td>min_info_gain</td>
<td>Minimum information gain for a split to be considered
at a tree node. 
Should be &gt;= 0, defaults to 0.</td>
    </tr>
<tr>
<td>loss_type</td>
<td>Loss function which GBT tries to minimize. 
Supported: <code>"squared"</code> (L2) and <code>"absolute"</code> (L1) (default = squared) for regression and <code>"logistic"</code> (default) for classification. 
For <code>ml_gradient_boosted_trees</code>, setting <code>"auto"</code>
will default to the appropriate loss type based on model type.</td>
    </tr>
<tr>
<td>seed</td>
<td>Seed for random numbers.</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>checkpoint_interval</td>
<td>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 
10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</td>
    </tr>
<tr>
<td>cache_node_ids</td>
<td>If <code>FALSE</code>, the algorithm will pass trees to executors to match instances with nodes.
If <code>TRUE</code>, the algorithm will cache node IDs for each instance. 
Caching can speed up training of deeper trees.
Defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>max_memory_in_mb</td>
<td>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration,
and its aggregates may exceed this size. 
Defaults to 256.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
<tr>
<td>type</td>
<td>The type of model to fit. <code> "regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. 
When <code>"auto"</code> is used, the model type is
inferred based on the response variable type -- if it is a numeric type,
then regression is used; classification otherwise.</td>
    </tr>
<tr>
<td>response</td>
<td>(Deprecated) The name of the response column (as a length-one character vector.)</td>
    </tr>
<tr>
<td>features</td>
<td>(Deprecated) The name of features (terms) to use for the model fit.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.

The supported options for <code>feature_subset_strategy</code> are
<code> "auto"</code>: Choose automatically for task: If <code>num_trees == 1</code>, set to <code>"all"</code>. 
If <code>num_trees &gt; 1</code> (forest), set to <code>"sqrt"</code> for classification and to <code>"onethird"</code> for regression.
<code> "all"</code>: use all features
<code> "onethird"</code>: use 1/3 of the features
<code> "sqrt"</code>: use use sqrt(number of features)
<code> "log2"</code>: use log2(number of features)
<code> "n"</code>: when <code>n</code> is in the range (0, 1.0], use n * number of features. 
When <code>n</code> is in the range (1, number of features), use <code>n</code> features. 
(default = <code>"auto"</code>)
<code> ml_gradient_boosted_trees</code> is a wrapper around <code>ml_gbt_regressor.tbl_spark</code> and <code>ml_gbt_classifier.tbl_spark</code> and calls the appropriate method based on model type.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

gbt_model &lt;- iris_training %&gt;%
  ml_gradient_boosted_trees(Sepal_Length ~ Petal_Length + Petal_Width)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(gbt_model, iris_test)

<a href='ml_evaluator.html'>ml_regression_evaluator</a>(pred, label_col = "Sepal_Length")
}</div></code>
<h2>Spark ML -- K-Means Clustering </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
K-means clustering with support for k-means|| initialization proposed by Bahmani et al.
  Using `ml_kmeans()` with the formula interface requires Spark 2.0+.
<code class="sourceCode r">ml_kmeans(x, formula = NULL, k = 2, max_iter = 20, tol = 1e-04,
  init_steps = 2, init_mode = "k-means||", seed = NULL,
  features_col = "features", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("kmeans_"), ...)

ml_compute_cost(model, dataset)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>k</td>
<td>The number of clusters to create</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>init_steps</td>
<td>Number of steps for the k-means|| initialization mode. 
This is an advanced setting -- the default of 2 is almost always enough. 
Must be &gt; 0. 
Default: 2.</td>
    </tr>
<tr>
<td>init_mode</td>
<td>Initialization algorithm. 
This can be either "random" to choose random points as initial cluster centers, or "k-means||" to use a parallel variant of k-means++ (Bahmani et al., Scalable K-Means++, VLDB 2012). 
Default: k-means||.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, see Details.</td>
    </tr>
<tr>
<td>model</td>
<td>A fitted K-means model returned by <code>ml_kmeans()</code></td>
    </tr>
<tr>
<td>dataset</td>
<td>Dataset on which to calculate K-means cost</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the clustering estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a clustering model.
<code> tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the estimator. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>. 
This signature does not apply to <code><a href='ml_lda.html'>ml_lda()</a></code>.
<code> ml_compute_cost()</code> returns the K-means cost (sum of
  squared distances of points to their nearest center) for the model
  on the given data.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-clustering.html'>http://spark.apache.org/docs/latest/ml-clustering.html</a> for
  more information on the set of clustering algorithms.

Other ml clustering algorithms: <code><a href='ml_bisecting_kmeans.html'>ml_bisecting_kmeans</a></code>,
  <code><a href='ml_gaussian_mixture.html'>ml_gaussian_mixture</a></code>, <code><a href='ml_lda.html'>ml_lda</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)
ml_kmeans(iris_tbl, Species ~ .)
}</div></code>
<h2>Spark ML -- Latent Dirichlet Allocation </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#parameter-details">Parameter details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Latent Dirichlet Allocation (LDA), a topic model designed for text documents.
<code class="sourceCode r">ml_lda(x, formula = NULL, k = 10, max_iter = 20,
  doc_concentration = NULL, topic_concentration = NULL,
  subsampling_rate = 0.05, optimizer = "online",
  checkpoint_interval = 10, keep_last_checkpoint = TRUE,
  learning_decay = 0.51, learning_offset = 1024,
  optimize_doc_concentration = TRUE, seed = NULL,
  features_col = "features",
  topic_distribution_col = "topicDistribution",
  uid = <a href='random_string.html'>random_string</a>("lda_"), ...)

ml_describe_topics(model, max_terms_per_topic = 10)

ml_log_likelihood(model, dataset)

ml_log_perplexity(model, dataset)

ml_topics_matrix(model)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>k</td>
<td>The number of clusters to create</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>doc_concentration</td>
<td>Concentration parameter (commonly named "alpha") for the prior placed on documents' distributions over topics ("theta"). 
See details.</td>
    </tr>
<tr>
<td>topic_concentration</td>
<td>Concentration parameter (commonly named "beta" or "eta") for the prior placed on topics' distributions over terms.</td>
    </tr>
<tr>
<td>subsampling_rate</td>
<td>(For Online optimizer only) Fraction of the corpus to be sampled and used in each iteration of mini-batch gradient descent, in range (0, 1]. 
Note that this should be adjusted in synch with <code>max_iter</code> so the entire corpus is used. 
Specifically, set both so that maxIterations * miniBatchFraction greater than or equal to 1.</td>
    </tr>
<tr>
<td>optimizer</td>
<td>Optimizer or inference algorithm used to estimate the LDA model. 
Supported: "online" for Online Variational Bayes (default) and "em" for Expectation-Maximization.</td>
    </tr>
<tr>
<td>checkpoint_interval</td>
<td>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 
10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</td>
    </tr>
<tr>
<td>keep_last_checkpoint</td>
<td>(Spark 2.0.0+) (For EM optimizer only) If using checkpointing, this indicates whether to keep the last checkpoint. 
If <code>FALSE</code>, then the checkpoint will be deleted. 
Deleting the checkpoint can cause failures if a data partition is lost, so set this bit with care. 
Note that checkpoints will be cleaned up via reference counting, regardless.</td>
    </tr>
<tr>
<td>learning_decay</td>
<td>(For Online optimizer only) Learning rate, set as an exponential decay rate. 
This should be between (0.5, 1.0] to guarantee asymptotic convergence. 
This is called "kappa" in the Online LDA paper (Hoffman et al., 2010). 
Default: 0.51, based on Hoffman et al.</td>
    </tr>
<tr>
<td>learning_offset</td>
<td>(For Online optimizer only) A (positive) learning parameter that downweights early iterations. 
Larger values make early iterations count less. 
This is called "tau0" in the Online LDA paper (Hoffman et al., 2010) Default: 1024, following Hoffman et al.</td>
    </tr>
<tr>
<td>optimize_doc_concentration</td>
<td>(For Online optimizer only) Indicates whether the <code>doc_concentration</code> (Dirichlet parameter for document-topic distribution) will be optimized during training. 
Setting this to true will make the model more expressive and fit the training data better. 
Default: <code>FALSE</code></td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>topic_distribution_col</td>
<td>Output column with estimates of the topic mixture distribution for each document (often called "theta" in the literature). 
Returns a vector of zeros for an empty document.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, see Details.</td>
    </tr>
<tr>
<td>model</td>
<td>A fitted LDA model returned by <code>ml_lda()</code>.</td>
    </tr>
<tr>
<td>max_terms_per_topic</td>
<td>Maximum number of terms to collect for each topic. 
Default value of 10.</td>
    </tr>
<tr>
<td>dataset</td>
<td>test corpus to use for calculating log likelihood or log perplexity</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the clustering estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a clustering model.
<code> tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the estimator. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>. 
This signature does not apply to <code>ml_lda()</code>.
<code> ml_describe_topics</code> returns a DataFrame with topics and their top-weighted terms.
<code> ml_log_likelihood</code> calculates a lower bound on the log likelihood of
  the entire corpus
<h3>Details</h3>
For `ml_lda.tbl_spark` with the formula interface, you can specify named arguments in `...` that will
  be passed `ft_regex_tokenizer()`, `ft_stop_words_remover()`, and `ft_count_vectorizer()`. 
For example, to increase the
  default `min_token_length`, you can use `ml_lda(dataset, ~ text, min_token_length = 4)`.

Terminology for LDA:

"term" = "word": an element of the vocabulary

"token": instance of a term appearing in a document

"topic": multinomial distribution over terms representing some concept

"document": one piece of text, corresponding to one row in the input data

Original LDA paper (journal version): Blei, Ng, and Jordan. 
"Latent Dirichlet Allocation." JMLR, 2003.

Input data (<code>features_col</code>): LDA is given a collection of documents as input data, via the <code>features_col</code> parameter. 
Each document is specified as a Vector of length <code>vocab_size</code>, where each entry is the count for the corresponding term (word) in the document. 
Feature transformers such as <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code> and <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code> can be useful for converting text to word count vectors
<h2 id="parameter-details">Parameter details</h2>
<h3><code>doc_concentration</code></h3>
This is the parameter to a Dirichlet distribution, where larger values mean more smoothing (more regularization). 
If not set by the user, then <code>doc_concentration</code> is set automatically. 
If set to singleton vector [alpha], then alpha is replicated to a vector of length k in fitting. 
Otherwise, the <code>doc_concentration</code> vector must be length k. 
(default = automatic)

Optimizer-specific parameter settings:

EM

Currently only supports symmetric distributions, so all values in the vector should be the same.

Values should be greater than 1.0

default = uniformly (50 / k) + 1, where 50/k is common in LDA libraries and +1 follows from Asuncion et al. 
(2009), who recommend a +1 adjustment for EM.

Online

Values should be greater than or equal to 0

default = uniformly (1.0 / k), following the implementation from <a href='https://github.com/Blei-Lab/onlineldavb'>here</a>

<h3><code>topic_concentration</code></h3>

This is the parameter to a symmetric Dirichlet distribution.

Note: The topics' distributions over terms are called "beta" in the original LDA paper by Blei et al., but are called "phi" in many later papers such as Asuncion et al., 2009.

If not set by the user, then <code>topic_concentration</code> is set automatically. 
(default = automatic)

Optimizer-specific parameter settings:

EM

Value should be greater than 1.0

default = 0.1 + 1, where 0.1 gives a small amount of smoothing and +1 follows Asuncion et al. 
(2009), who recommend a +1 adjustment for EM.

Online

Value should be greater than or equal to 0

default = (1.0 / k), following the implementation from <a href='https://github.com/Blei-Lab/onlineldavb'>here</a>.

<h3><code>topic_distribution_col</code></h3>
This uses a variational approximation following Hoffman et al. 
(2010), where the approximate distribution is called "gamma." Technically, this method returns this approximation "gamma" for each document.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-clustering.html'>http://spark.apache.org/docs/latest/ml-clustering.html</a> for
  more information on the set of clustering algorithms.

Other ml clustering algorithms: <code><a href='ml_bisecting_kmeans.html'>ml_bisecting_kmeans</a></code>,
  <code><a href='ml_gaussian_mixture.html'>ml_gaussian_mixture</a></code>,
  <code><a href='ml_kmeans.html'>ml_kmeans</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(janeaustenr)
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

lines_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc,
  <a href='https://rdrr.io/pkg/janeaustenr/man/austen_books.html'>austen_books</a>()[<a href='https://rdrr.io/r/base/c.html'>c</a>(1:30), ],
  name = "lines_tbl",
  overwrite = TRUE
)

# transform the data in a tidy form
lines_tbl_tidy &lt;- lines_tbl %&gt;%
  <a href='ft_tokenizer.html'>ft_tokenizer</a>(
    input_col = "text",
    output_col = "word_list"
  ) %&gt;%
  <a href='ft_stop_words_remover.html'>ft_stop_words_remover</a>(
    input_col = "word_list",
    output_col = "wo_stop_words"
  ) %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/mutate.html'>mutate</a>(text = explode(wo_stop_words)) %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/filter.html'>filter</a>(text != "") %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/select.html'>select</a>(text, book)

lda_model &lt;- lines_tbl_tidy %&gt;%
  ml_lda(~text, k = 4)

# vocabulary and topics
<a href='https://rdrr.io/pkg/generics/man/tidy.html'>tidy</a>(lda_model)
}</div></code>
<h2>Spark ML -- Linear Regression </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform regression using linear regression.
<code class="sourceCode r">ml_linear_regression(x, formula = NULL, fit_intercept = TRUE,
  elastic_net_param = 0, reg_param = 0, max_iter = 100,
  weight_col = NULL, loss = "squaredError", solver = "auto",
  standardization = TRUE, tol = 1e-06, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("linear_regression_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>fit_intercept</td>
<td>Boolean; should the model be fit with an intercept term?</td>
    </tr>
<tr>
<td>elastic_net_param</td>
<td>ElasticNet mixing parameter, in range [0, 1]. 
For alpha = 0, the penalty is an L2 penalty. 
For alpha = 1, it is an L1 penalty.</td>
    </tr>
<tr>
<td>reg_param</td>
<td>Regularization parameter (aka lambda)</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>weight_col</td>
<td>The name of the column to use as weights for the model fit.</td>
    </tr>
<tr>
<td>loss</td>
<td>The loss function to be optimized. 
Supported options: "squaredError"
and "huber". 
Default: "squaredError"</td>
    </tr>
<tr>
<td>solver</td>
<td>Solver algorithm for optimization.</td>
    </tr>
<tr>
<td>standardization</td>
<td>Whether to standardize the training features before fitting the model.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
mtcars_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

lm_model &lt;- mtcars_training %&gt;%
  ml_linear_regression(mpg ~ .)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(lm_model, mtcars_test)

<a href='ml_evaluator.html'>ml_regression_evaluator</a>(pred, label_col = "mpg")
}</div></code>
<h2>Spark ML -- Logistic Regression </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform classification using logistic regression.
<code class="sourceCode r">ml_logistic_regression(x, formula = NULL, fit_intercept = TRUE,
  elastic_net_param = 0, reg_param = 0, max_iter = 100,
  threshold = 0.5, thresholds = NULL, tol = 1e-06,
  weight_col = NULL, aggregation_depth = 2,
  lower_bounds_on_coefficients = NULL,
  lower_bounds_on_intercepts = NULL,
  upper_bounds_on_coefficients = NULL,
  upper_bounds_on_intercepts = NULL, features_col = "features",
  label_col = "label", family = "auto",
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("logistic_regression_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>fit_intercept</td>
<td>Boolean; should the model be fit with an intercept term?</td>
    </tr>
<tr>
<td>elastic_net_param</td>
<td>ElasticNet mixing parameter, in range [0, 1]. 
For alpha = 0, the penalty is an L2 penalty. 
For alpha = 1, it is an L1 penalty.</td>
    </tr>
<tr>
<td>reg_param</td>
<td>Regularization parameter (aka lambda)</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>threshold</td>
<td>in binary classification prediction, in range [0, 1].</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>weight_col</td>
<td>The name of the column to use as weights for the model fit.</td>
    </tr>
<tr>
<td>aggregation_depth</td>
<td>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</td>
    </tr>
<tr>
<td>lower_bounds_on_coefficients</td>
<td>(Spark 2.2.0+) Lower bounds on coefficients if fitting under bound constrained optimization.
The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.</td>
    </tr>
<tr>
<td>lower_bounds_on_intercepts</td>
<td>(Spark 2.2.0+) Lower bounds on intercepts if fitting under bound constrained optimization.
The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.</td>
    </tr>
<tr>
<td>upper_bounds_on_coefficients</td>
<td>(Spark 2.2.0+) Upper bounds on coefficients if fitting under bound constrained optimization.
The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression.</td>
    </tr>
<tr>
<td>upper_bounds_on_intercepts</td>
<td>(Spark 2.2.0+) Upper bounds on intercepts if fitting under bound constrained optimization.
The bounds vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>family</td>
<td>(Spark 2.1.0+) Param for the name of family which is a description of the label distribution to be used in the model. 
Supported options: "auto", "binomial", and "multinomial."</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
mtcars_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

lr_model &lt;- mtcars_training %&gt;%
  ml_logistic_regression(am ~ gear + carb)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(lr_model, mtcars_test)

<a href='ml_evaluator.html'>ml_binary_classification_evaluator</a>(pred)
}</div></code>
<h2>Extracts data associated with a Spark ML model </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Extracts data associated with a Spark ML model
<code class="sourceCode r">ml_model_data(object)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>object</td>
<td>a Spark ML model</td>
    </tr>
    </table>
<h3>Value</h3>
A tbl_spark
<h2>Spark ML -- Multilayer Perceptron </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Classification model based on the Multilayer Perceptron. 
Each layer has sigmoid activation function, output layer has softmax.
<code class="sourceCode r">ml_multilayer_perceptron_classifier(x, formula = NULL, layers = NULL,
  max_iter = 100, step_size = 0.03, tol = 1e-06, block_size = 128,
  solver = "l-bfgs", seed = NULL, initial_weights = NULL,
  thresholds = NULL, features_col = "features", label_col = "label",
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("multilayer_perceptron_classifier_"), ...)

ml_multilayer_perceptron(x, formula = NULL, layers, max_iter = 100,
  step_size = 0.03, tol = 1e-06, block_size = 128,
  solver = "l-bfgs", seed = NULL, initial_weights = NULL,
  features_col = "features", label_col = "label", thresholds = NULL,
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("multilayer_perceptron_classifier_"),
  response = NULL, features = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>layers</td>
<td>A numeric vector describing the layers -- each element in the vector gives the size of a layer. 
For example, <code><a href='https://rdrr.io/r/base/c.html'>c(4, 5, 2)</a></code> would imply three layers, with an input (feature) layer of size 4, an intermediate layer of size 5, and an output (class) layer of size 2.</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>step_size</td>
<td>Step size to be used for each iteration of optimization (&gt; 0).</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>block_size</td>
<td>Block size for stacking input data in matrices to speed up the computation. 
Data is stacked within partitions. 
If block size is more than remaining data in a partition then it is adjusted to the size of this data. 
Recommended size is between 10 and 1000. 
Default: 128</td>
    </tr>
<tr>
<td>solver</td>
<td>The solver algorithm for optimization. 
Supported options: "gd" (minibatch gradient descent) or "l-bfgs". 
Default: "l-bfgs"</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>initial_weights</td>
<td>The initial weights of the model.</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
<tr>
<td>response</td>
<td>(Deprecated) The name of the response column (as a length-one character vector.)</td>
    </tr>
<tr>
<td>features</td>
<td>(Deprecated) The name of features (terms) to use for the model fit.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<code> ml_multilayer_perceptron()</code> is an alias for <code>ml_multilayer_perceptron_classifier()</code> for backwards compatibility.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;-  <a href='spark-connections.html'>spark_connect</a>(master = "local")

iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)
partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

mlp_model &lt;- iris_training %&gt;%
  ml_multilayer_perceptron_classifier(Species ~ ., layers = <a href='https://rdrr.io/r/base/c.html'>c</a>(4,3,3))

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(mlp_model, iris_test)

<a href='ml_evaluator.html'>ml_multiclass_classification_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- Naive-Bayes </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Naive Bayes Classifiers. 
It supports Multinomial NB (see <a href='http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html'>here</a>) which can handle finitely supported discrete data. 
For example, by converting documents into TF-IDF vectors, it can be used for document classification. 
By making every vector a binary (0/1) data, it can also be used as Bernoulli NB (see <a href='http://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html'>here</a>). 
The input feature values must be nonnegative.
<code class="sourceCode r">ml_naive_bayes(x, formula = NULL, model_type = "multinomial",
  smoothing = 1, thresholds = NULL, weight_col = NULL,
  features_col = "features", label_col = "label",
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("naive_bayes_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>model_type</td>
<td>The model type. 
Supported options: <code>"multinomial"</code>
and <code>"bernoulli"</code>. 
(default = <code>multinomial</code>)</td>
    </tr>
<tr>
<td>smoothing</td>
<td>The (Laplace) smoothing parameter. 
Defaults to 1.</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>weight_col</td>
<td>(Spark 2.1.0+) Weight column name. 
If this is not set or empty, we treat all instance weights as 1.0.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

nb_model &lt;- iris_training %&gt;%
  ml_naive_bayes(Species ~ .)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(nb_model, iris_test)

<a href='ml_evaluator.html'>ml_multiclass_classification_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- OneVsRest </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Reduction of Multiclass Classification to Binary Classification. 
Performs reduction using one against all strategy. 
For a multiclass classification with k classes, train k models (one per class). 
Each example is scored against all k models and the model with highest score is picked to label the example.
<code class="sourceCode r">ml_one_vs_rest(x, formula = NULL, classifier = NULL,
  features_col = "features", label_col = "label",
  prediction_col = "prediction", uid = <a href='random_string.html'>random_string</a>("one_vs_rest_"),
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>classifier</td>
<td>Object of class <code>ml_estimator</code>. 
Base binary classifier that we reduce multiclass classification into.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h2>Feature Transformation -- PCA (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
PCA trains a model to project vectors to a lower dimensional space of the top k principal components.
<code class="sourceCode r">ft_pca(x, input_col = NULL, output_col = NULL, k = NULL,
  uid = <a href='random_string.html'>random_string</a>("pca_"), ...)

ml_pca(x, features = tbl_vars(x), k = <a href='https://rdrr.io/r/base/length.html'>length</a>(features),
  pc_prefix = "PC", ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>k</td>
<td>The number of principal components</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>features</td>
<td>The columns to use in the principal components
analysis. 
Defaults to all columns in <code>x</code>.</td>
    </tr>
<tr>
<td>pc_prefix</td>
<td>Length-one character vector used to prepend names of components.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<code> ml_pca()</code> is a wrapper around <code>ft_pca()</code> that returns a
  <code>ml_model</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/select.html'>select</a>(-Species) %&gt;%
  ml_pca(k = 2)
}</div></code>
<h2>Spark ML -- Random Forest </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform classification and regression using random forests.
<code class="sourceCode r">ml_random_forest_classifier(x, formula = NULL, num_trees = 20,
  subsampling_rate = 1, max_depth = 5, min_instances_per_node = 1,
  feature_subset_strategy = "auto", impurity = "gini",
  min_info_gain = 0, max_bins = 32, seed = NULL, thresholds = NULL,
  checkpoint_interval = 10, cache_node_ids = FALSE,
  max_memory_in_mb = 256, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("random_forest_classifier_"), ...)

ml_random_forest(x, formula = NULL, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("auto", "regression",
  "classification"), features_col = "features", label_col = "label",
  prediction_col = "prediction", probability_col = "probability",
  raw_prediction_col = "rawPrediction",
  feature_subset_strategy = "auto", impurity = "auto",
  checkpoint_interval = 10, max_bins = 32, max_depth = 5,
  num_trees = 20, min_info_gain = 0, min_instances_per_node = 1,
  subsampling_rate = 1, seed = NULL, thresholds = NULL,
  cache_node_ids = FALSE, max_memory_in_mb = 256,
  uid = <a href='random_string.html'>random_string</a>("random_forest_"), response = NULL,
  features = NULL, ...)

ml_random_forest_regressor(x, formula = NULL, num_trees = 20,
  subsampling_rate = 1, max_depth = 5, min_instances_per_node = 1,
  feature_subset_strategy = "auto", impurity = "variance",
  min_info_gain = 0, max_bins = 32, seed = NULL,
  checkpoint_interval = 10, cache_node_ids = FALSE,
  max_memory_in_mb = 256, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("random_forest_regressor_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>num_trees</td>
<td>Number of trees to train (&gt;= 1). 
If 1, then no bootstrapping is used. 
If &gt; 1, then bootstrapping is done.</td>
    </tr>
<tr>
<td>subsampling_rate</td>
<td>Fraction of the training data used for learning each decision tree, in range (0, 1]. 
(default = 1.0)</td>
    </tr>
<tr>
<td>max_depth</td>
<td>Maximum depth of the tree (&gt;= 0); that is, the maximum
number of nodes separating any leaves from the root of the tree.</td>
    </tr>
<tr>
<td>min_instances_per_node</td>
<td>Minimum number of instances each child must
have after split.</td>
    </tr>
<tr>
<td>feature_subset_strategy</td>
<td>The number of features to consider for splits at each tree node. 
See details for options.</td>
    </tr>
<tr>
<td>impurity</td>
<td>Criterion used for information gain calculation. 
Supported: "entropy"
and "gini" (default) for classification and "variance" (default) for regression. 
For<code> ml_decision_tree</code>, setting <code>"auto"</code> will default to the appropriate
criterion based on model type.</td>
    </tr>
<tr>
<td>min_info_gain</td>
<td>Minimum information gain for a split to be considered
at a tree node. 
Should be &gt;= 0, defaults to 0.</td>
    </tr>
<tr>
<td>max_bins</td>
<td>The maximum number of bins used for discretizing
continuous features and for choosing how to split on features at
each node. 
More bins give higher granularity.</td>
    </tr>
<tr>
<td>seed</td>
<td>Seed for random numbers.</td>
    </tr>
<tr>
<td>thresholds</td>
<td>Thresholds in multi-class classification to adjust the probability of predicting each class. 
Array must have length equal to the number of classes, with values &gt; 0 excepting that at most one value may be 0. 
The class with largest value <code>p/t</code> is predicted, where <code>p</code> is the original probability of that class and <code>t</code> is the class's threshold.</td>
    </tr>
<tr>
<td>checkpoint_interval</td>
<td>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 
10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</td>
    </tr>
<tr>
<td>cache_node_ids</td>
<td>If <code>FALSE</code>, the algorithm will pass trees to executors to match instances with nodes.
If <code>TRUE</code>, the algorithm will cache node IDs for each instance. 
Caching can speed up training of deeper trees.
Defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>max_memory_in_mb</td>
<td>Maximum memory in MB allocated to histogram aggregation.
If too small, then 1 node will be split per iteration,
and its aggregates may exceed this size. 
Defaults to 256.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
<tr>
<td>type</td>
<td>The type of model to fit. <code> "regression"</code> treats the response
as a continuous variable, while <code>"classification"</code> treats the response
as a categorical variable. 
When <code>"auto"</code> is used, the model type is
inferred based on the response variable type -- if it is a numeric type,
then regression is used; classification otherwise.</td>
    </tr>
<tr>
<td>response</td>
<td>(Deprecated) The name of the response column (as a length-one character vector.)</td>
    </tr>
<tr>
<td>features</td>
<td>(Deprecated) The name of features (terms) to use for the model fit.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.

The supported options for <code>feature_subset_strategy</code> are
<code> "auto"</code>: Choose automatically for task: If <code>num_trees == 1</code>, set to <code>"all"</code>. 
If <code>num_trees &gt; 1</code> (forest), set to <code>"sqrt"</code> for classification and to <code>"onethird"</code> for regression.
<code> "all"</code>: use all features
<code> "onethird"</code>: use 1/3 of the features
<code> "sqrt"</code>: use use sqrt(number of features)
<code> "log2"</code>: use log2(number of features)
<code> "n"</code>: when <code>n</code> is in the range (0, 1.0], use n * number of features. 
When <code>n</code> is in the range (1, number of features), use <code>n</code> features. 
(default = <code>"auto"</code>)
<code> ml_random_forest</code> is a wrapper around <code>ml_random_forest_regressor.tbl_spark</code> and <code>ml_random_forest_classifier.tbl_spark</code> and calls the appropriate method based on model type.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

rf_model &lt;- iris_training %&gt;%
  ml_random_forest(Species ~ ., type = "classification")

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(rf_model, iris_test)

<a href='ml_evaluator.html'>ml_multiclass_classification_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- Survival Regression </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Fit a parametric survival regression model named accelerated failure time (AFT) model (see <a href='https://en.wikipedia.org/wiki/Accelerated_failure_time_model'>Accelerated failure time model (Wikipedia)</a>) based on the Weibull distribution of the survival time.
<code class="sourceCode r">ml_aft_survival_regression(x, formula = NULL, censor_col = "censor",
  quantile_probabilities = <a href='https://rdrr.io/r/base/c.html'>c</a>(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,
  0.99), fit_intercept = TRUE, max_iter = 100L, tol = 1e-06,
  aggregation_depth = 2, quantiles_col = NULL,
  features_col = "features", label_col = "label",
  prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("aft_survival_regression_"), ...)

ml_survival_regression(x, formula = NULL, censor_col = "censor",
  quantile_probabilities = <a href='https://rdrr.io/r/base/c.html'>c</a>(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95,
  0.99), fit_intercept = TRUE, max_iter = 100L, tol = 1e-06,
  aggregation_depth = 2, quantiles_col = NULL,
  features_col = "features", label_col = "label",
  prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("aft_survival_regression_"), response = NULL,
  features = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>censor_col</td>
<td>Censor column name. 
The value of this column could be 0 or 1. 
If the value is 1, it means the event has occurred i.e. 
uncensored; otherwise censored.</td>
    </tr>
<tr>
<td>quantile_probabilities</td>
<td>Quantile probabilities array. 
Values of the quantile probabilities array should be in the range (0, 1) and the array should be non-empty.</td>
    </tr>
<tr>
<td>fit_intercept</td>
<td>Boolean; should the model be fit with an intercept term?</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>aggregation_depth</td>
<td>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</td>
    </tr>
<tr>
<td>quantiles_col</td>
<td>Quantiles column name. 
This column will output quantiles of corresponding quantileProbabilities if it is set.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
<tr>
<td>response</td>
<td>(Deprecated) The name of the response column (as a length-one character vector.)</td>
    </tr>
<tr>
<td>features</td>
<td>(Deprecated) The name of features (terms) to use for the model fit.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<code> ml_survival_regression()</code> is an alias for <code>ml_aft_survival_regression()</code> for backwards compatibility.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

<a href='https://rdrr.io/r/base/library.html'>library</a>(survival)
<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
ovarian_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, ovarian, name = "ovarian_tbl", overwrite = TRUE)

partitions &lt;- ovarian_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

ovarian_training &lt;- partitions$training
ovarian_test &lt;- partitions$test

sur_reg &lt;- ovarian_training %&gt;%
  ml_aft_survival_regression(futime ~ ecog_ps + rx + age + resid_ds, censor_col = "fustat")

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(sur_reg, ovarian_test)
pred
}</div></code>
<h2>Add a Stage to a Pipeline </h2>
<a href="#arguments">Arguments</a>
Adds a stage to a pipeline.
<code class="sourceCode r">ml_add_stage(x, stage)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A pipeline or a pipeline stage.</td>
    </tr>
<tr>
<td>stage</td>
<td>A pipeline stage.</td>
    </tr>
    </table>
<h2>Spark ML -- ALS </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#examples">Examples</a>
Perform recommendation using Alternating Least Squares (ALS) matrix factorization.
<code class="sourceCode r">ml_als(x, formula = NULL, rating_col = "rating", user_col = "user",
  item_col = "item", rank = 10, reg_param = 0.1,
  implicit_prefs = FALSE, alpha = 1, nonnegative = FALSE,
  max_iter = 10, num_user_blocks = 10, num_item_blocks = 10,
  checkpoint_interval = 10, cold_start_strategy = "nan",
  intermediate_storage_level = "MEMORY_AND_DISK",
  final_storage_level = "MEMORY_AND_DISK", uid = <a href='random_string.html'>random_string</a>("als_"),
  ...)

ml_recommend(model, type = <a href='https://rdrr.io/r/base/c.html'>c</a>("items", "users"), n = 1)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula.
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.
The ALS model requires a specific formula format, please use <code>rating_col ~ user_col + item_col</code>.</td>
    </tr>
<tr>
<td>rating_col</td>
<td>Column name for ratings. 
Default: "rating"</td>
    </tr>
<tr>
<td>user_col</td>
<td>Column name for user ids. 
Ids must be integers. 
Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. 
Default: "user"</td>
    </tr>
<tr>
<td>item_col</td>
<td>Column name for item ids. 
Ids must be integers. 
Other numeric types are supported for this column, but will be cast to integers as long as they fall within the integer value range. 
Default: "item"</td>
    </tr>
<tr>
<td>rank</td>
<td>Rank of the matrix factorization (positive). 
Default: 10</td>
    </tr>
<tr>
<td>reg_param</td>
<td>Regularization parameter.</td>
    </tr>
<tr>
<td>implicit_prefs</td>
<td>Whether to use implicit preference. 
Default: FALSE.</td>
    </tr>
<tr>
<td>alpha</td>
<td>Alpha parameter in the implicit preference formulation (nonnegative).</td>
    </tr>
<tr>
<td>nonnegative</td>
<td>Whether to apply nonnegativity constraints. 
Default: FALSE.</td>
    </tr>
<tr>
<td>max_iter</td>
<td>Maximum number of iterations.</td>
    </tr>
<tr>
<td>num_user_blocks</td>
<td>Number of user blocks (positive). 
Default: 10</td>
    </tr>
<tr>
<td>num_item_blocks</td>
<td>Number of item blocks (positive). 
Default: 10</td>
    </tr>
<tr>
<td>checkpoint_interval</td>
<td>Set checkpoint interval (&gt;= 1) or disable checkpoint (-1).
E.g. 
10 means that the cache will get checkpointed every 10 iterations, defaults to 10.</td>
    </tr>
<tr>
<td>cold_start_strategy</td>
<td>(Spark 2.2.0+) Strategy for dealing with unknown or new users/items at prediction time. 
This may be useful in cross-validation or production scenarios, for handling user/item ids the model has not seen in the training data. 
Supported values: - "nan": predicted value for unknown ids will be NaN. 
- "drop": rows in the input DataFrame containing unknown ids will be dropped from the output DataFrame containing predictions. 
Default: "nan".</td>
    </tr>
<tr>
<td>intermediate_storage_level</td>
<td>(Spark 2.0.0+) StorageLevel for intermediate datasets. 
Pass in a string representation of <code>StorageLevel</code>. 
Cannot be "NONE". 
Default: "MEMORY_AND_DISK".</td>
    </tr>
<tr>
<td>final_storage_level</td>
<td>(Spark 2.0.0+) StorageLevel for ALS model factors. 
Pass in a string representation of <code>StorageLevel</code>. 
Default: "MEMORY_AND_DISK".</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>model</td>
<td>An ALS model object</td>
    </tr>
<tr>
<td>type</td>
<td>What to recommend, one of <code>items</code> or <code>users</code></td>
    </tr>
<tr>
<td>n</td>
<td>Maximum number of recommendations to return</td>
    </tr>
    </table>
<h3>Value</h3>
ALS attempts to estimate the ratings matrix R as the product of two lower-rank matrices, X and Y, i.e. 
X * Yt = R. 
Typically these approximations are called 'factor' matrices. 
The general approach is iterative. 
During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. 
The newly-solved factor matrix is then held constant while solving for the other factor matrix.

This is a blocked implementation of the ALS factorization algorithm that groups the two sets of factors (referred to as "users" and "products") into blocks and reduces communication by only sending one copy of each user vector to each product block on each iteration, and only for the product blocks that need that user's feature vector. 
This is achieved by pre-computing some information about the ratings matrix to determine the "out-links" of each user (which blocks of products it will contribute to) and "in-link" information for each product (which of the feature vectors it receives from each user block it will depend on). 
This allows us to send only an array of feature vectors between each user block and product block, and have the product block find the users' ratings and update the products based on these messages.

For implicit preference data, the algorithm used is based on "Collaborative Filtering for Implicit Feedback Datasets", available at <a href='https://doi.org/10.1109/ICDM.2008.22'>https://doi.org/10.1109/ICDM.2008.22</a>, adapted for the blocked approach used here.

Essentially instead of finding the low-rank approximations to the rating matrix R, this finds the approximations for a preference matrix P where the elements of P are 1 if r is greater than 0 and 0 if r is less than or equal to 0. 
The ratings then act as 'confidence' values related to strength of indicated user preferences rather than explicit ratings given to items.

The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_als</code> recommender object, which is an Estimator.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the recommender appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a recommender
  estimator is constructed then immediately fit with the input
  <code>tbl_spark</code>, returning a recommendation model, i.e. <code> ml_als_model</code>.
<h3>Details</h3><code> ml_recommend()</code> returns the top <code>n</code> users/items recommended for each item/user, for all items/users. 
The output has been transformed (exploded and separated) from the default Spark outputs to be more user friendly.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

movies &lt;- <a href='https://rdrr.io/r/base/data.frame.html'>data.frame</a>(
  user   = <a href='https://rdrr.io/r/base/c.html'>c</a>(1, 2, 0, 1, 2, 0),
  item   = <a href='https://rdrr.io/r/base/c.html'>c</a>(1, 1, 1, 2, 2, 0),
  rating = <a href='https://rdrr.io/r/base/c.html'>c</a>(3, 1, 2, 4, 5, 4)
)
movies_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, movies)

model &lt;- ml_als(movies_tbl, rating ~ user + item)

<a href='ml-transform-methods.html'>ml_predict</a>(model, movies_tbl)

ml_recommend(model, type = "item", 1)
}</div></code>
<h2>Utility functions for LSH models </h2>
<a href="#arguments">Arguments</a>
Utility functions for LSH models
<code class="sourceCode r">ml_approx_nearest_neighbors(model, dataset, key, num_nearest_neighbors,
  dist_col = "distCol")

ml_approx_similarity_join(model, dataset_a, dataset_b, threshold,
  dist_col = "distCol")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>model</td>
<td>A fitted LSH model, returned by either <code><a href='ft_lsh.html'>ft_minhash_lsh()</a></code>
or <code><a href='ft_lsh.html'>ft_bucketed_random_projection_lsh()</a></code>.</td>
    </tr>
<tr>
<td>dataset</td>
<td>The dataset to search for nearest neighbors of the key.</td>
    </tr>
<tr>
<td>key</td>
<td>Feature vector representing the item to search for.</td>
    </tr>
<tr>
<td>num_nearest_neighbors</td>
<td>The maximum number of nearest neighbors.</td>
    </tr>
<tr>
<td>dist_col</td>
<td>Output column for storing the distance between each result row and the key.</td>
    </tr>
<tr>
<td>dataset_a</td>
<td>One of the datasets to join.</td>
    </tr>
<tr>
<td>dataset_b</td>
<td>Another dataset to join.</td>
    </tr>
<tr>
<td>threshold</td>
<td>The threshold for the distance of row pairs.</td>
    </tr>
    </table>
<h2>Frequent Pattern Mining -- FPGrowth </h2>
<a href="#arguments">Arguments</a>
A parallel FP-growth algorithm to mine frequent itemsets.
<code class="sourceCode r">ml_fpgrowth(x, items_col = "items", min_confidence = 0.8,
  min_support = 0.3, prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("fpgrowth_"), ...)

ml_association_rules(model)

ml_freq_itemsets(model)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>items_col</td>
<td>Items column name. 
Default: "items"</td>
    </tr>
<tr>
<td>min_confidence</td>
<td>Minimal confidence for generating Association Rule.<code> min_confidence</code> will not affect the mining for frequent itemsets, but
will affect the association rules generation. 
Default: 0.8</td>
    </tr>
<tr>
<td>min_support</td>
<td>Minimal support level of the frequent pattern. 
[0.0, 1.0].
Any pattern that appears more than (min_support * size-of-the-dataset) times
 will be output in the frequent itemsets. 
Default: 0.3</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>model</td>
<td>A fitted FPGrowth model returned by <code>ml_fpgrowth()</code></td>
    </tr>
    </table>
<h2>Spark ML - Evaluators </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#examples">Examples</a>
A set of functions to calculate performance metrics for prediction models. 
Also see the Spark ML Documentation <a href='https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package'>https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.evaluation.package</a>
<code class="sourceCode r">ml_binary_classification_evaluator(x, label_col = "label",
  raw_prediction_col = "rawPrediction", metric_name = "areaUnderROC",
  uid = <a href='random_string.html'>random_string</a>("binary_classification_evaluator_"), ...)

ml_binary_classification_eval(x, label_col = "label",
  prediction_col = "prediction", metric_name = "areaUnderROC")

ml_multiclass_classification_evaluator(x, label_col = "label",
  prediction_col = "prediction", metric_name = "f1",
  uid = <a href='random_string.html'>random_string</a>("multiclass_classification_evaluator_"), ...)

ml_classification_eval(x, label_col = "label",
  prediction_col = "prediction", metric_name = "f1")

ml_regression_evaluator(x, label_col = "label",
  prediction_col = "prediction", metric_name = "rmse",
  uid = <a href='random_string.html'>random_string</a>("regression_evaluator_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code> object or a <code>tbl_spark</code> containing label and prediction columns. 
The latter should be the output of <code><a href='sdf-transform-methods.html'>sdf_predict</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Name of column string specifying which column contains the true labels or values.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>metric_name</td>
<td>The performance metric. 
See details.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Name of the column that contains the predicted
label or value NOT the scored probability. 
Column should be of type<code> Double</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
The calculated performance metric
<h3>Details</h3>
The following metrics are supported

Binary Classification: <code>areaUnderROC</code> (default) or <code>areaUnderPR</code> (not available in Spark 2.X.)

Multiclass Classification: <code>f1</code> (default), <code>precision</code>, <code>recall</code>, <code>weightedPrecision</code>, <code>weightedRecall</code> or <code>accuracy</code>; for Spark 2.X: <code>f1</code> (default), <code>weightedPrecision</code>, <code>weightedRecall</code> or <code>accuracy</code>.

Regression: <code>rmse</code> (root mean squared error, default),
   <code>mse</code> (mean squared error), <code>r2</code>, or <code>mae</code> (mean absolute error.)
<code> ml_binary_classification_eval()</code> is an alias for <code>ml_binary_classification_evaluator()</code> for backwards compatibility.
<code> ml_classification_eval()</code> is an alias for <code>ml_multiclass_classification_evaluator()</code> for backwards compatibility.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
mtcars_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, mtcars, name = "mtcars_tbl", overwrite = TRUE)

partitions &lt;- mtcars_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

mtcars_training &lt;- partitions$training
mtcars_test &lt;- partitions$test

# for multiclass classification
rf_model &lt;- mtcars_training %&gt;%
  <a href='ml_random_forest.html'>ml_random_forest</a>(cyl ~ ., type = "classification")

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(rf_model, mtcars_test)

ml_multiclass_classification_evaluator(pred)

# for regression
rf_model &lt;- mtcars_training %&gt;%
  <a href='ml_random_forest.html'>ml_random_forest</a>(cyl ~ ., type = "regression")

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(rf_model, mtcars_test)

ml_regression_evaluator(pred, label_col = "cyl")

# for binary classification
rf_model &lt;- mtcars_training %&gt;%
  <a href='ml_random_forest.html'>ml_random_forest</a>(am ~ gear + carb, type = "classification")

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(rf_model, mtcars_test)

ml_binary_classification_evaluator(pred)
}</div></code>
<h2>Spark ML -- Bisecting K-Means Clustering </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
A bisecting k-means algorithm based on the paper "A comparison of document clustering techniques" by Steinbach, Karypis, and Kumar, with modification to fit Spark. 
The algorithm starts from a single cluster that contains all points. 
Iteratively it finds divisible clusters on the bottom level and bisects each of them using k-means, until there are k leaf clusters in total or no leaf clusters are divisible. 
The bisecting steps of clusters on the same level are grouped together to increase parallelism. 
If bisecting all divisible clusters on the bottom level would result more than k leaf clusters, larger clusters get higher priority.
<code class="sourceCode r">ml_bisecting_kmeans(x, formula = NULL, k = 4, max_iter = 20,
  seed = NULL, min_divisible_cluster_size = 1,
  features_col = "features", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("bisecting_bisecting_kmeans_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>k</td>
<td>The number of clusters to create</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>min_divisible_cluster_size</td>
<td>The minimum number of points (if greater than or equal to 1.0) or the minimum proportion of points (if less than 1.0) of a divisible cluster (default: 1.0).</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the clustering estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a clustering model.
<code> tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the estimator. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>. 
This signature does not apply to <code><a href='ml_lda.html'>ml_lda()</a></code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-clustering.html'>http://spark.apache.org/docs/latest/ml-clustering.html</a> for
  more information on the set of clustering algorithms.

Other ml clustering algorithms: <code><a href='ml_gaussian_mixture.html'>ml_gaussian_mixture</a></code>,
  <code><a href='ml_kmeans.html'>ml_kmeans</a></code>, <code><a href='ml_lda.html'>ml_lda</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/select.html'>select</a>(-Species) %&gt;%
  ml_bisecting_kmeans(k = 4 , Species ~ .)
}</div></code>
<h2>Wrap a Spark ML JVM object </h2>
<a href="#arguments">Arguments</a>
Identifies the associated sparklyr ML constructor for the JVM object by inspecting its
  class and performing a lookup. 
The lookup table is specified by the
  `sparkml/class_mapping.json` files of sparklyr and the loaded extensions.
<code class="sourceCode r">ml_call_constructor(jobj)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>jobj</td>
<td>The jobj for the pipeline stage.</td>
    </tr>
    </table>
<h2>Chi-square hypothesis testing for categorical data. </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#examples">Examples</a>
Conduct Pearson's independence test for every feature against the
  label. 
For each feature, the (feature, label) pairs are converted
  into a contingency matrix for which the Chi-squared statistic is
  computed. 
All label and feature values must be categorical.
<code class="sourceCode r">ml_chisquare_test(x, features, label)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>features</td>
<td>The name(s) of the feature columns. 
This can also be the name
of a single vector column created using <code><a href='ft_vector_assembler.html'>ft_vector_assembler()</a></code>.</td>
    </tr>
<tr>
<td>label</td>
<td>The name of the label column.</td>
    </tr>
    </table>
<h3>Value</h3>
A data frame with one row for each (feature, label) pair with p-values,
  degrees of freedom, and test statistics.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_chisquare_test(iris_tbl, features = features, label = "Species")
}</div></code>
<h2>Spark ML - Clustering Evaluator </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#examples">Examples</a>
Evaluator for clustering results. 
The metric computes the Silhouette measure using the squared
  Euclidean distance. 
The Silhouette is a measure for the validation of the consistency
   within clusters. 
It ranges between 1 and -1, where a value close to 1 means that the
    points in a cluster are close to the other points in the same cluster and far from the
    points of the other clusters.
<code class="sourceCode r">ml_clustering_evaluator(x, features_col = "features",
  prediction_col = "prediction", metric_name = "silhouette",
  uid = <a href='random_string.html'>random_string</a>("clustering_evaluator_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code> object or a <code>tbl_spark</code> containing label and prediction columns. 
The latter should be the output of <code><a href='sdf-transform-methods.html'>sdf_predict</a></code>.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Name of features column.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Name of the prediction column.</td>
    </tr>
<tr>
<td>metric_name</td>
<td>The performance metric. 
Currently supports "silhouette".</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The calculated performance metric
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

formula &lt;- Species ~ .

# Train the models
kmeans_model &lt;- <a href='ml_kmeans.html'>ml_kmeans</a>(iris_training, formula = formula)
b_kmeans_model &lt;- <a href='ml_bisecting_kmeans.html'>ml_bisecting_kmeans</a>(iris_training, formula = formula)
gmm_model &lt;- <a href='ml_gaussian_mixture.html'>ml_gaussian_mixture</a>(iris_training, formula = formula)

# Predict
pred_kmeans &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(kmeans_model, iris_test)
pred_b_kmeans &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(b_kmeans_model, iris_test)
pred_gmm &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(gmm_model, iris_test)

# Evaluate
ml_clustering_evaluator(pred_kmeans)
ml_clustering_evaluator(pred_b_kmeans)
ml_clustering_evaluator(pred_gmm)
}</div></code>
<h2>Constructors for `ml_model` Objects </h2>
<a href="#arguments">Arguments</a>
Functions for developers writing extensions for Spark ML. 
These functions are constructors
  for `ml_model` objects that are returned when using the formula interface.
<code class="sourceCode r">new_ml_model_prediction(pipeline_model, formula, dataset, label_col,
  features_col, ..., class = <a href='https://rdrr.io/r/base/character.html'>character</a>())

new_ml_model(pipeline_model, formula, dataset, ..., class = <a href='https://rdrr.io/r/base/character.html'>character</a>())

new_ml_model_classification(pipeline_model, formula, dataset, label_col,
  features_col, predicted_label_col, ..., class = <a href='https://rdrr.io/r/base/character.html'>character</a>())

new_ml_model_regression(pipeline_model, formula, dataset, label_col,
  features_col, ..., class = <a href='https://rdrr.io/r/base/character.html'>character</a>())

new_ml_model_clustering(pipeline_model, formula, dataset, features_col,
  ..., class = <a href='https://rdrr.io/r/base/character.html'>character</a>())

ml_supervised_pipeline(predictor, dataset, formula, features_col,
  label_col)

ml_clustering_pipeline(predictor, dataset, formula, features_col)

ml_construct_model_supervised(constructor, predictor, formula, dataset,
  features_col, label_col, ...)

ml_construct_model_clustering(constructor, predictor, formula, dataset,
  features_col, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>pipeline_model</td>
<td>The pipeline model object returned by `ml_supervised_pipeline()`.</td>
    </tr>
<tr>
<td>formula</td>
<td>The formula used for data preprocessing</td>
    </tr>
<tr>
<td>dataset</td>
<td>The training dataset.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>class</td>
<td>Name of the subclass.</td>
    </tr>
<tr>
<td>predictor</td>
<td>The pipeline stage corresponding to the ML algorithm.</td>
    </tr>
<tr>
<td>constructor</td>
<td>The constructor function for the `ml_model`.</td>
    </tr>
    </table>
<h2>Compute correlation matrix </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#examples">Examples</a>
Compute correlation matrix
<code class="sourceCode r">ml_corr(x, columns = NULL, method = <a href='https://rdrr.io/r/base/c.html'>c</a>("pearson", "spearman"))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>columns</td>
<td>The names of the columns to calculate correlations of. 
If only one
column is specified, it must be a vector column (for example, assembled using<code> ft_vector_assember()</code>).</td>
    </tr>
<tr>
<td>method</td>
<td>The method to use, either <code>"pearson"</code> or <code>"spearman"</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
A correlation matrix organized as a data frame.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("Petal_Width", "Petal_Length", "Sepal_Length", "Sepal_Width")

ml_corr(iris_tbl, columns = features , method = "pearson")
}</div></code>
<h2>Spark ML -- Tuning </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#examples">Examples</a>
Perform hyper-parameter tuning using either K-fold cross validation or train-validation split.
<code class="sourceCode r">ml_sub_models(model)

ml_validation_metrics(model)

ml_cross_validator(x, estimator = NULL, estimator_param_maps = NULL,
  evaluator = NULL, num_folds = 3, collect_sub_models = FALSE,
  parallelism = 1, seed = NULL,
  uid = <a href='random_string.html'>random_string</a>("cross_validator_"), ...)

ml_train_validation_split(x, estimator = NULL,
  estimator_param_maps = NULL, evaluator = NULL, train_ratio = 0.75,
  collect_sub_models = FALSE, parallelism = 1, seed = NULL,
  uid = <a href='random_string.html'>random_string</a>("train_validation_split_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>model</td>
<td>A cross validation or train-validation-split model.</td>
    </tr>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>estimator</td>
<td>A <code>ml_estimator</code> object.</td>
    </tr>
<tr>
<td>estimator_param_maps</td>
<td>A named list of stages and hyper-parameter sets to tune. 
See details.</td>
    </tr>
<tr>
<td>evaluator</td>
<td>A <code>ml_evaluator</code> object, see <a href='ml_evaluator.html'>ml_evaluator</a>.</td>
    </tr>
<tr>
<td>num_folds</td>
<td>Number of folds for cross validation. 
Must be &gt;= 2. 
Default: 3</td>
    </tr>
<tr>
<td>collect_sub_models</td>
<td>Whether to collect a list of sub-models trained during tuning.
If set to <code>FALSE</code>, then only the single best sub-model will be available after fitting.
If set to true, then all sub-models will be available. 
Warning: For large models, collecting
all sub-models can cause OOMs on the Spark driver.</td>
    </tr>
<tr>
<td>parallelism</td>
<td>The number of threads to use when running parallel algorithms. 
Default is 1 for serial execution.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>train_ratio</td>
<td>Ratio between train and validation data. 
Must be between 0 and 1. 
Default: 0.75</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_cross_validator</code> or <code>ml_traing_validation_split</code> object.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the tuning estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a tuning estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a <code>ml_cross_validation_model</code> or a
  <code>ml_train_validation_split_model</code> object.

For cross validation, <code>ml_sub_models()</code> returns a nested
  list of models, where the first layer represents fold indices and the
  second layer represents param maps. 
For train-validation split,
  <code>ml_sub_models()</code> returns a list of models, corresponding to the
  order of the estimator param maps.
<code> ml_validation_metrics()</code> returns a data frame of performance
  metrics and hyperparameter combinations.
<h3>Details</h3><code> ml_cross_validator()</code> performs k-fold cross validation while <code>ml_train_validation_split()</code> performs tuning on one pair of train and validation datasets.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

# Create a pipeline
pipeline &lt;- <a href='ml_pipeline.html'>ml_pipeline</a>(sc) %&gt;%
  <a href='ft_r_formula.html'>ft_r_formula</a>(Species ~ . ) %&gt;%
  <a href='ml_random_forest.html'>ml_random_forest_classifier</a>()

# Specify hyperparameter grid
grid &lt;- <a href='https://rdrr.io/r/base/list.html'>list</a>(
  random_forest = <a href='https://rdrr.io/r/base/list.html'>list</a>(
    num_trees = <a href='https://rdrr.io/r/base/c.html'>c</a>(5,10),
    max_depth = <a href='https://rdrr.io/r/base/c.html'>c</a>(5,10),
    impurity = <a href='https://rdrr.io/r/base/c.html'>c</a>("entropy", "gini")
  )
)

# Create the cross validator object
cv &lt;- ml_cross_validator(
  sc, estimator = pipeline, estimator_param_maps = grid,
  evaluator = <a href='ml_evaluator.html'>ml_multiclass_classification_evaluator</a>(sc),
  num_folds = 3,
  parallelism = 4
)

# Train the models
cv_model &lt;- <a href='ml-transform-methods.html'>ml_fit</a>(cv, iris_tbl)

# Print the metrics
ml_validation_metrics(cv_model)

}</div></code>
<h2>Default stop words </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Loads the default stop words for the given language.
<code class="sourceCode r">ml_default_stop_words(sc, language = <a href='https://rdrr.io/r/base/c.html'>c</a>("english", "danish", "dutch",
  "finnish", "french", "german", "hungarian", "italian", "norwegian",
  "portuguese", "russian", "spanish", "swedish", "turkish"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code></td>
    </tr>
<tr>
<td>language</td>
<td>A character string.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
A list of stop words.
<h3>Details</h3>
Supported languages: danish, dutch, english, finnish, french,
  german, hungarian, italian, norwegian, portuguese, russian, spanish,
  swedish, turkish. 
Defaults to English. 
See <a href='http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/'>http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/</a>
  for more details
<h3>See also</h3><code> <a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>
<h2>Evaluate the Model on a Validation Set </h2>
<a href="#arguments">Arguments</a>
Compute performance metrics.
<code class="sourceCode r">ml_evaluate(x, dataset)

# S3 method for ml_model_logistic_regression
ml_evaluate(x, dataset)

# S3 method for ml_logistic_regression_model
ml_evaluate(x, dataset)

# S3 method for ml_model_linear_regression
ml_evaluate(x, dataset)

# S3 method for ml_linear_regression_model
ml_evaluate(x, dataset)

# S3 method for ml_model_generalized_linear_regression
ml_evaluate(x, dataset)

# S3 method for ml_generalized_linear_regression_model
ml_evaluate(x, dataset)

# S3 method for ml_evaluator
ml_evaluate(x, dataset)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An ML model object or an evaluator object.</td>
    </tr>
<tr>
<td>dataset</td>
<td>The dataset to be validate the model on.</td>
    </tr>
    </table>
<h2>Spark ML - Feature Importance for Tree Models </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Spark ML - Feature Importance for Tree Models
<code class="sourceCode r">ml_feature_importances(model, ...)

ml_tree_feature_importance(model, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>model</td>
<td>A decision tree-based model.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
For <code>ml_model</code>, a sorted data frame with feature labels and their relative importance.
  For <code>ml_prediction_model</code>, a vector of relative importances.
<h2>Feature Transformation -- Word2Vec (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Word2Vec transforms a word into a code for further natural language processing or machine learning process.
<code class="sourceCode r">ft_word2vec(x, input_col = NULL, output_col = NULL,
  vector_size = 100, min_count = 5, max_sentence_length = 1000,
  num_partitions = 1, step_size = 0.025, max_iter = 1, seed = NULL,
  uid = <a href='random_string.html'>random_string</a>("word2vec_"), ...)

ml_find_synonyms(model, word, num)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>vector_size</td>
<td>The dimension of the code that you want to transform from words. 
Default: 100</td>
    </tr>
<tr>
<td>min_count</td>
<td>The minimum number of times a token must appear to be included in
the word2vec model's vocabulary. 
Default: 5</td>
    </tr>
<tr>
<td>max_sentence_length</td>
<td>(Spark 2.0.0+) Sets the maximum length (in words) of each sentence
in the input data. 
Any sentence longer than this threshold will be divided into
chunks of up to <code>max_sentence_length</code> size. 
Default: 1000</td>
    </tr>
<tr>
<td>num_partitions</td>
<td>Number of partitions for sentences of words. 
Default: 1</td>
    </tr>
<tr>
<td>step_size</td>
<td>Param for Step size to be used for each iteration of optimization (&gt; 0).</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>model</td>
<td>A fitted <code>Word2Vec</code> model, returned by <code>ft_word2vec()</code>.</td>
    </tr>
<tr>
<td>word</td>
<td>A word, as a length-one character vector.</td>
    </tr>
<tr>
<td>num</td>
<td>Number of words closest in similarity to the given word to find.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<code> ml_find_synonyms()</code> returns a DataFrame of synonyms and cosine similarities
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>
<h2>Spark ML -- Transform, fit, and predict methods (ml_ interface) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
Methods for transformation, fit, and prediction. 
These are mirrors of the corresponding <a href='sdf-transform-methods.html'>sdf-transform-methods</a>.
<code class="sourceCode r">is_ml_transformer(x)

is_ml_estimator(x)

ml_fit(x, dataset, ...)

ml_transform(x, dataset, ...)

ml_fit_and_transform(x, dataset, ...)

ml_predict(x, dataset, ...)

# S3 method for ml_model_classification
ml_predict(x, dataset,
  probability_prefix = "probability_", ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>ml_estimator</code>, <code>ml_transformer</code> (or a list thereof), or <code>ml_model</code> object.</td>
    </tr>
<tr>
<td>dataset</td>
<td>A <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>probability_prefix</td>
<td>String used to prepend the class probability output columns.</td>
    </tr>
    </table>
<h3>Value</h3>
When <code>x</code> is an estimator, <code>ml_fit()</code> returns a transformer whereas <code>ml_fit_and_transform()</code> returns a transformed dataset. 
When <code>x</code> is a transformer, <code>ml_transform()</code> and <code>ml_predict()</code> return a transformed dataset. 
When <code>ml_predict()</code> is called on a <code>ml_model</code> object, additional columns (e.g. 
probabilities in case of classification models) are appended to the transformed output for the user's convenience.
<h3>Details</h3>
These methods are
<h2>Spark ML -- Gaussian Mixture clustering. </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
This class performs expectation maximization for multivariate Gaussian Mixture Models (GMMs). 
A GMM represents a composite distribution of independent Gaussian distributions with associated "mixing" weights specifying each's contribution to the composite. 
Given a set of sample points, this class will maximize the log-likelihood for a mixture of k Gaussians, iterating until the log-likelihood changes by less than <code>tol</code>, or until it has reached the max number of iterations. 
While this process is generally guaranteed to converge, it is not guaranteed to find a global optimum.
<code class="sourceCode r">ml_gaussian_mixture(x, formula = NULL, k = 2, max_iter = 100,
  tol = 0.01, seed = NULL, features_col = "features",
  prediction_col = "prediction", probability_col = "probability",
  uid = <a href='random_string.html'>random_string</a>("gaussian_mixture_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>k</td>
<td>The number of clusters to create</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>probability_col</td>
<td>Column name for predicted class conditional probabilities. 
Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the clustering estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, an estimator is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a clustering model.
<code> tbl_spark</code>, with <code>formula</code> or <code>features</code> specified: When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the estimator. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>. 
This signature does not apply to <code><a href='ml_lda.html'>ml_lda()</a></code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-clustering.html'>http://spark.apache.org/docs/latest/ml-clustering.html</a> for
  more information on the set of clustering algorithms.

Other ml clustering algorithms: <code><a href='ml_bisecting_kmeans.html'>ml_bisecting_kmeans</a></code>,
  <code><a href='ml_kmeans.html'>ml_kmeans</a></code>, <code><a href='ml_lda.html'>ml_lda</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

gmm_model &lt;- ml_gaussian_mixture(iris_tbl, Species ~ .)
pred &lt;- <a href='sdf-transform-methods.html'>sdf_predict</a>(iris_tbl, gmm_model)
<a href='ml_clustering_evaluator.html'>ml_clustering_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- ML Params </h2>
<a href="#arguments">Arguments</a>
Helper methods for working with parameters for ML objects.
<code class="sourceCode r">ml_is_set(x, param, ...)

ml_param_map(x, ...)

ml_param(x, param, allow_null = FALSE, ...)

ml_params(x, params = NULL, allow_null = FALSE, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark ML object, either a pipeline stage or an evaluator.</td>
    </tr>
<tr>
<td>param</td>
<td>The parameter to extract or set.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>allow_null</td>
<td>Whether to allow <code>NULL</code> results when extracting parameters. 
If <code>FALSE</code>, an error will be thrown if the specified parameter is not found. 
Defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>params</td>
<td>A vector of parameters to extract.</td>
    </tr>
    </table>
<h2>Spark ML -- Isotonic Regression </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Currently implemented using parallelized pool adjacent violators algorithm. 
Only univariate (single feature) algorithm supported.
<code class="sourceCode r">ml_isotonic_regression(x, formula = NULL, feature_index = 0,
  isotonic = TRUE, weight_col = NULL, features_col = "features",
  label_col = "label", prediction_col = "prediction",
  uid = <a href='random_string.html'>random_string</a>("isotonic_regression_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>feature_index</td>
<td>Index of the feature if <code>features_col</code> is a vector column (default: 0), no effect otherwise.</td>
    </tr>
<tr>
<td>isotonic</td>
<td>Whether the output sequence should be isotonic/increasing (true) or antitonic/decreasing (false). 
Default: true</td>
    </tr>
<tr>
<td>weight_col</td>
<td>The name of the column to use as weights for the model fit.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_linear_svc.html'>ml_linear_svc</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

iso_res &lt;- iris_tbl %&gt;%
  ml_isotonic_regression(Petal_Length ~ Petal_Width)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(iso_res, iris_test)

pred
}</div></code>
<h2>Feature Transformation -- StringIndexer (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
A label indexer that maps a string column of labels to an ML column of
  label indices. 
If the input column is numeric, we cast it to string and
  index the string values. 
The indices are in <code>[0, numLabels)</code>, ordered by
  label frequencies. 
So the most frequent label gets index 0. 
This function
  is the inverse of <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>.
<code class="sourceCode r">ft_string_indexer(x, input_col = NULL, output_col = NULL,
  handle_invalid = "error", string_order_type = "frequencyDesc",
  uid = <a href='random_string.html'>random_string</a>("string_indexer_"), ...)

ml_labels(model)

ft_string_indexer_model(x, input_col = NULL, output_col = NULL, labels,
  handle_invalid = "error",
  uid = <a href='random_string.html'>random_string</a>("string_indexer_model_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>handle_invalid</td>
<td>(Spark 2.1.0+) Param for how to handle invalid entries. 
Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). 
Default: "error"</td>
    </tr>
<tr>
<td>string_order_type</td>
<td>(Spark 2.3+)How to order labels of string column.
The first label after ordering is assigned an index of 0. 
Options are<code> "frequencyDesc"</code>, <code>"frequencyAsc"</code>, <code>"alphabetDesc"</code>, and <code>"alphabetAsc"</code>.
Defaults to <code>"frequencyDesc"</code>.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>model</td>
<td>A fitted StringIndexer model returned by <code>ft_string_indexer()</code></td>
    </tr>
<tr>
<td>labels</td>
<td>Vector of labels, corresponding to indices to be assigned.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<code> ml_labels()</code> returns a vector of labels, corresponding to indices to be assigned.
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.
<code> <a href='ft_index_to_string.html'>ft_index_to_string</a></code>

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Spark ML -- LinearSVC </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Perform classification using linear support vector machines (SVM). 
This binary classifier optimizes the Hinge Loss using the OWLQN optimizer. 
Only supports L2 regularization currently.
<code class="sourceCode r">ml_linear_svc(x, formula = NULL, fit_intercept = TRUE, reg_param = 0,
  max_iter = 100, standardization = TRUE, weight_col = NULL,
  tol = 1e-06, threshold = 0, aggregation_depth = 2,
  features_col = "features", label_col = "label",
  prediction_col = "prediction", raw_prediction_col = "rawPrediction",
  uid = <a href='random_string.html'>random_string</a>("linear_svc_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>Used when <code>x</code> is a <code>tbl_spark</code>. 
R formula as a character string or a formula. 
This is used to transform the input dataframe before fitting, see <a href='ft_r_formula.html'>ft_r_formula</a> for details.</td>
    </tr>
<tr>
<td>fit_intercept</td>
<td>Boolean; should the model be fit with an intercept term?</td>
    </tr>
<tr>
<td>reg_param</td>
<td>Regularization parameter (aka lambda)</td>
    </tr>
<tr>
<td>max_iter</td>
<td>The maximum number of iterations to use.</td>
    </tr>
<tr>
<td>standardization</td>
<td>Whether to standardize the training features before fitting the model.</td>
    </tr>
<tr>
<td>weight_col</td>
<td>The name of the column to use as weights for the model fit.</td>
    </tr>
<tr>
<td>tol</td>
<td>Param for the convergence tolerance for iterative algorithms.</td>
    </tr>
<tr>
<td>threshold</td>
<td>in binary classification prediction, in range [0, 1].</td>
    </tr>
<tr>
<td>aggregation_depth</td>
<td>(Spark 2.1.0+) Suggested depth for treeAggregate (&gt;= 2).</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>prediction_col</td>
<td>Prediction column name.</td>
    </tr>
<tr>
<td>raw_prediction_col</td>
<td>Raw prediction (a.k.a. 
confidence) column name.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; see Details.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns an instance of a <code>ml_estimator</code> object. 
The object contains a pointer to
  a Spark <code>Predictor</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the predictor appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a predictor is constructed then
  immediately fit with the input <code>tbl_spark</code>, returning a prediction model.
<code> tbl_spark</code>, with <code>formula</code>: specified When <code>formula</code>
    is specified, the input <code>tbl_spark</code> is first transformed using a
    <code>RFormula</code> transformer before being fit by
    the predictor. 
The object returned in this case is a <code>ml_model</code> which is a
    wrapper of a <code>ml_pipeline_model</code>.
<h3>Details</h3>
When <code>x</code> is a <code>tbl_spark</code> and <code>formula</code> (alternatively, <code>response</code> and <code>features</code>) is specified, the function returns a <code>ml_model</code> object wrapping a <code>ml_pipeline_model</code> which contains data pre-processing transformers, the ML predictor, and, for classification models, a post-processing transformer that converts predictions into class labels. 
For classification, an optional argument <code>predicted_label_col</code> (defaults to <code>"predicted_label"</code>) can be used to specify the name of the predicted label column. 
In addition to the fitted <code>ml_pipeline_model</code>, <code>ml_model</code> objects also contain a <code>ml_pipeline</code> object where the ML predictor stage is an estimator ready to be fit against data. 
This is utilized by <code><a href='ml-persistence.html'>ml_save</a></code> with <code>type = "pipeline"</code> to faciliate model refresh workflows.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-classification-regression.html'>http://spark.apache.org/docs/latest/ml-classification-regression.html</a> for
  more information on the set of supervised learning algorithms.

Other ml algorithms: <code><a href='ml_aft_survival_regression.html'>ml_aft_survival_regression</a></code>,
  <code><a href='ml_decision_tree.html'>ml_decision_tree_classifier</a></code>,
  <code><a href='ml_gradient_boosted_trees.html'>ml_gbt_classifier</a></code>,
  <code><a href='ml_generalized_linear_regression.html'>ml_generalized_linear_regression</a></code>,
  <code><a href='ml_isotonic_regression.html'>ml_isotonic_regression</a></code>,
  <code><a href='ml_linear_regression.html'>ml_linear_regression</a></code>,
  <code><a href='ml_logistic_regression.html'>ml_logistic_regression</a></code>,
  <code><a href='ml_multilayer_perceptron_classifier.html'>ml_multilayer_perceptron_classifier</a></code>,
  <code><a href='ml_naive_bayes.html'>ml_naive_bayes</a></code>,
  <code><a href='ml_one_vs_rest.html'>ml_one_vs_rest</a></code>,
  <code><a href='ml_random_forest.html'>ml_random_forest_classifier</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

partitions &lt;- iris_tbl %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/filter.html'>filter</a>(Species != "setosa") %&gt;%
  <a href='sdf_random_split.html'>sdf_random_split</a>(training = 0.7, test = 0.3, seed = 1111)

iris_training &lt;- partitions$training
iris_test &lt;- partitions$test

svc_model &lt;- iris_training %&gt;%
  ml_linear_svc(Species ~ .)

pred &lt;- <a href='ml-transform-methods.html'>ml_predict</a>(svc_model, iris_test)

<a href='ml_evaluator.html'>ml_binary_classification_evaluator</a>(pred)
}</div></code>
<h2>Spark ML -- Model Persistence </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Save/load Spark ML objects
<code class="sourceCode r">ml_save(x, path, overwrite = FALSE, ...)

# S3 method for ml_model
ml_save(x, path, overwrite = FALSE,
  type = <a href='https://rdrr.io/r/base/c.html'>c</a>("pipeline_model", "pipeline"), ...)

ml_load(sc, path)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A ML object, which could be a <code>ml_pipeline_stage</code> or a <code>ml_model</code></td>
    </tr>
<tr>
<td>path</td>
<td>The path where the object is to be serialized/deserialized.</td>
    </tr>
<tr>
<td>overwrite</td>
<td>Whether to overwrite the existing path, defaults to <code>FALSE</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>type</td>
<td>Whether to save the pipeline model or the pipeline.</td>
    </tr>
<tr>
<td>sc</td>
<td>A Spark connection.</td>
    </tr>
    </table>
<h3>Value</h3><code> ml_save()</code> serializes a Spark object into a format that can be read back into <code>sparklyr</code> or by the Scala or PySpark APIs. 
When called on <code>ml_model</code> objects, i.e. 
those that were created via the <code>tbl_spark - formula</code> signature, the associated pipeline model is serialized. 
In other words, the saved model contains both the data processing (<code>RFormulaModel</code>) stage and the machine learning stage.
<code> ml_load()</code> reads a saved Spark object into <code>sparklyr</code>. 
It calls the correct Scala <code>load</code> method based on parsing the saved metadata. 
Note that a <code>PipelineModel</code> object saved from a sparklyr <code>ml_model</code> via <code>ml_save()</code> will be read back in as an <code>ml_pipeline_model</code>, rather than the <code>ml_model</code> object.
<h2>Spark ML -- Pipelines </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Create Spark ML Pipelines
<code class="sourceCode r">ml_pipeline(x, ..., uid = <a href='random_string.html'>random_string</a>("pipeline_"))</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>Either a <code>spark_connection</code> or <code>ml_pipeline_stage</code> objects</td>
    </tr>
<tr>
<td>...</td>
<td><code>ml_pipeline_stage</code> objects.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the ML estimator.</td>
    </tr>
    </table>
<h3>Value</h3>
When <code>x</code> is a <code>spark_connection</code>, <code>ml_pipeline()</code> returns an empty pipeline object. 
When <code>x</code> is a <code>ml_pipeline_stage</code>, <code>ml_pipeline()</code> returns an <code>ml_pipeline</code> with the stages set to <code>x</code> and any transformers or estimators given in <code>...</code>.
<h2>Spark ML -- Pipeline stage extraction </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Extraction of stages from a Pipeline or PipelineModel object.
<code class="sourceCode r">ml_stage(x, stage)

ml_stages(x, stages = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>ml_pipeline</code> or a <code>ml_pipeline_model</code> object</td>
    </tr>
<tr>
<td>stage</td>
<td>The UID of a stage in the pipeline.</td>
    </tr>
<tr>
<td>stages</td>
<td>The UIDs of stages in the pipeline as a character vector.</td>
    </tr>
    </table>
<h3>Value</h3>
For <code>ml_stage()</code>: The stage specified.

For <code>ml_stages()</code>: A list of stages. 
If <code>stages</code> is not set, the function returns all stages of the pipeline in a list.
<h2>Standardize Formula Input for `ml_model` </h2>
<a href="#arguments">Arguments</a>
Generates a formula string from user inputs, to be used in `ml_model` constructor.
<code class="sourceCode r">ml_standardize_formula(formula = NULL, response = NULL,
  features = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>formula</td>
<td>The `formula` argument.</td>
    </tr>
<tr>
<td>response</td>
<td>The `response` argument.</td>
    </tr>
<tr>
<td>features</td>
<td>The `features` argument.</td>
    </tr>
    </table>
<h2>Spark ML -- Extraction of summary metrics </h2>
<a href="#arguments">Arguments</a>
Extracts a metric from the summary object of a Spark ML model.
<code class="sourceCode r">ml_summary(x, metric = NULL, allow_null = FALSE)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark ML model that has a summary.</td>
    </tr>
<tr>
<td>metric</td>
<td>The name of the metric to extract. 
If not set, returns the summary object.</td>
    </tr>
<tr>
<td>allow_null</td>
<td>Whether null results are allowed when the metric is not found in the summary.</td>
    </tr>
    </table>
<h2>Spark ML -- UID </h2>
<a href="#arguments">Arguments</a>
Extracts the UID of an ML object.
<code class="sourceCode r">ml_uid(x)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark ML object</td>
    </tr>
    </table>
<h2>Feature Transformation -- CountVectorizer (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Extracts a vocabulary from document collections.
<code class="sourceCode r">ft_count_vectorizer(x, input_col = NULL, output_col = NULL,
  binary = FALSE, min_df = 1, min_tf = 1, vocab_size = 2^18,
  uid = <a href='random_string.html'>random_string</a>("count_vectorizer_"), ...)

ml_vocabulary(model)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>binary</td>
<td>Binary toggle to control the output vector values.
If <code>TRUE</code>, all nonzero counts (after <code>min_tf</code> filter applied)
are set to 1. 
This is useful for discrete probabilistic models that
 model binary events rather than integer counts. 
Default: <code>FALSE</code></td>
    </tr>
<tr>
<td>min_df</td>
<td>Specifies the minimum number of different documents a
term must appear in to be included in the vocabulary. 
If this is an
integer greater than or equal to 1, this specifies the number of
documents the term must appear in; if this is a double in [0,1), then
this specifies the fraction of documents. 
Default: 1.</td>
    </tr>
<tr>
<td>min_tf</td>
<td>Filter to ignore rare words in a document. 
For each
document, terms with frequency/count less than the given threshold
are ignored. 
If this is an integer greater than or equal to 1, then
this specifies a count (of times the term must appear in the document);
if this is a double in [0,1), then this specifies a fraction (out of
the document's token count). 
Default: 1.</td>
    </tr>
<tr>
<td>vocab_size</td>
<td>Build a vocabulary that only considers the top<code> vocab_size</code> terms ordered by term frequency across the corpus.
Default: <code>2^18</code>.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>model</td>
<td>A <code>ml_count_vectorizer_model</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<code> ml_vocabulary()</code> returns a vector of vocabulary built.
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Binarizer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Apply thresholding to a column, such that values less than or equal to the<code> threshold</code> are assigned the value 0.0, and values greater than the
threshold are assigned the value 1.0. 
Column output is numeric for
compatibility with other modeling functions.
<code class="sourceCode r">ft_binarizer(x, input_col, output_col, threshold = 0,
  uid = <a href='random_string.html'>random_string</a>("binarizer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>threshold</td>
<td>Threshold used to binarize continuous features.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  ft_binarizer(input_col  = "Sepal_Length",
 output_col = "Sepal_Length_bin",
 threshold  = 5) %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/select.html'>select</a>(Sepal_Length, Sepal_Length_bin, Species)
}</div></code>
<h2>Feature Transformation -- Bucketizer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Similar to R's <code><a href='https://rdrr.io/r/base/cut.html'>cut</a></code> function, this transforms a numeric column
into a discretized column, with breaks specified through the <code>splits</code>
parameter.
<code class="sourceCode r">ft_bucketizer(x, input_col = NULL, output_col = NULL, splits = NULL,
  input_cols = NULL, output_cols = NULL, splits_array = NULL,
  handle_invalid = "error", uid = <a href='random_string.html'>random_string</a>("bucketizer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>splits</td>
<td>A numeric vector of cutpoints, indicating the bucket boundaries.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>Names of input columns.</td>
    </tr>
<tr>
<td>output_cols</td>
<td>Names of output columns.</td>
    </tr>
<tr>
<td>splits_array</td>
<td>Parameter for specifying multiple splits parameters. 
Each
element in this array can be used to map continuous features into buckets.</td>
    </tr>
<tr>
<td>handle_invalid</td>
<td>(Spark 2.1.0+) Param for how to handle invalid entries. 
Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). 
Default: "error"</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(dplyr)

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

iris_tbl %&gt;%
  ft_bucketizer(input_col  = "Sepal_Length",
  output_col = "Sepal_Length_bucket",
  splits     = <a href='https://rdrr.io/r/base/c.html'>c</a>(0, 4.5, 5, 8)) %&gt;%
  <a href='https://dplyr.tidyverse.org/reference/select.html'>select</a>(Sepal_Length, Sepal_Length_bucket, Species)
}</div></code>
<h2>Feature Transformation -- ChiSqSelector (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Chi-Squared feature selection, which selects categorical features to use for predicting a categorical label
<code class="sourceCode r">ft_chisq_selector(x, features_col = "features", output_col = NULL,
  label_col = "label", selector_type = "numTopFeatures", fdr = 0.05,
  fpr = 0.05, fwe = 0.05, num_top_features = 50, percentile = 0.1,
  uid = <a href='random_string.html'>random_string</a>("chisq_selector_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code><a href='ft_r_formula.html'>ft_r_formula</a></code>.</td>
    </tr>
<tr>
<td>selector_type</td>
<td>(Spark 2.1.0+) The selector type of the ChisqSelector. 
Supported options: "numTopFeatures" (default), "percentile", "fpr", "fdr", "fwe".</td>
    </tr>
<tr>
<td>fdr</td>
<td>(Spark 2.2.0+) The upper bound of the expected false discovery rate. 
Only applicable when selector_type = "fdr". 
Default value is 0.05.</td>
    </tr>
<tr>
<td>fpr</td>
<td>(Spark 2.1.0+) The highest p-value for features to be kept. 
Only applicable when selector_type= "fpr". 
Default value is 0.05.</td>
    </tr>
<tr>
<td>fwe</td>
<td>(Spark 2.2.0+) The upper bound of the expected family-wise error rate. 
Only applicable when selector_type = "fwe". 
Default value is 0.05.</td>
    </tr>
<tr>
<td>num_top_features</td>
<td>Number of features that selector will select, ordered by ascending p-value. 
If the number of features is less than <code>num_top_features</code>, then this will select all features. 
Only applicable when selector_type = "numTopFeatures". 
The default value of <code>num_top_features</code> is 50.</td>
    </tr>
<tr>
<td>percentile</td>
<td>(Spark 2.1.0+) Percentile of features that selector will select, ordered by statistics value descending. 
Only applicable when selector_type = "percentile". 
Default value is 0.1.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Discrete Cosine Transform (DCT) (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
A feature transformer that takes the 1D discrete cosine transform of a real
  vector. 
No zero padding is performed on the input vector. 
It returns a real
  vector of the same length representing the DCT. 
The return vector is scaled
  such that the transform matrix is unitary (aka scaled DCT-II).
<code class="sourceCode r">ft_dct(x, input_col = NULL, output_col = NULL, inverse = FALSE,
  uid = <a href='random_string.html'>random_string</a>("dct_"), ...)

ft_discrete_cosine_transform(x, input_col, output_col, inverse = FALSE,
  uid = <a href='random_string.html'>random_string</a>("dct_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>inverse</td>
<td>Indicates whether to perform the inverse DCT (TRUE) or forward DCT (FALSE).</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3><code> ft_discrete_cosine_transform()</code> is an alias for <code>ft_dct</code> for backwards compatibility.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- ElementwiseProduct (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Outputs the Hadamard product (i.e., the element-wise product) of each input vector
  with a provided "weight" vector. 
In other words, it scales each column of the
  dataset by a scalar multiplier.
<code class="sourceCode r">ft_elementwise_product(x, input_col = NULL, output_col = NULL,
  scaling_vec = NULL, uid = <a href='random_string.html'>random_string</a>("elementwise_product_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>scaling_vec</td>
<td>the vector to multiply with input vectors</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- FeatureHasher (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Feature Transformation -- FeatureHasher (Transformer)
<code class="sourceCode r">ft_feature_hasher(x, input_cols = NULL, output_col = NULL,
  num_features = 2^18, categorical_cols = NULL,
  uid = <a href='random_string.html'>random_string</a>("feature_hasher_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>Names of input columns.</td>
    </tr>
<tr>
<td>output_col</td>
<td>Name of output column.</td>
    </tr>
<tr>
<td>num_features</td>
<td>Number of features. 
Defaults to \(2^18\).</td>
    </tr>
<tr>
<td>categorical_cols</td>
<td>Numeric columns to treat as categorical features.
By default only string and boolean columns are treated as categorical,
so this param can be used to explicitly specify the numerical columns to
treat as categorical.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
Feature hashing projects a set of categorical or numerical features into a
  feature vector of specified dimension (typically substantially smaller than
  that of the original feature space). 
This is done using the hashing trick
  <a href='https://en.wikipedia.org/wiki/Feature_hashing'>https://en.wikipedia.org/wiki/Feature_hashing</a> to map features to indices
  in the feature vector.

The FeatureHasher transformer operates on multiple columns. 
Each column may
    contain either numeric or categorical features. 
Behavior and handling of
    column data types is as follows: -Numeric columns: For numeric features,
    the hash value of the column name is used to map the feature value to its
    index in the feature vector. 
By default, numeric features are not treated
    as categorical (even when they are integers). 
To treat them as categorical,
    specify the relevant columns in categoricalCols. 
-String columns: For
     categorical features, the hash value of the string "column_name=value"
     is used to map to the vector index, with an indicator value of 1.0.
     Thus, categorical features are "one-hot" encoded (similarly to using
     OneHotEncoder with drop_last=FALSE). 
-Boolean columns: Boolean values
     are treated in the same way as string columns. 
That is, boolean features
     are represented as "column_name=true" or "column_name=false", with an
     indicator value of 1.0.

Null (missing) values are ignored (implicitly zero in the resulting feature vector).

The hash function used here is also the MurmurHash 3 used in HashingTF. 
Since a
 simple modulo on the hashed value is used to determine the vector index, it is
 advisable to use a power of two as the num_features parameter; otherwise the
 features will not be mapped evenly to the vector indices.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- HashingTF (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Maps a sequence of terms to their term frequencies using the hashing trick.
<code class="sourceCode r">ft_hashing_tf(x, input_col = NULL, output_col = NULL, binary = FALSE,
  num_features = 2^18, uid = <a href='random_string.html'>random_string</a>("hashing_tf_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>binary</td>
<td>Binary toggle to control term frequency counts.
If true, all non-zero counts are set to 1. 
This is useful for discrete
probabilistic models that model binary events rather than integer
counts. 
(default = <code>FALSE</code>)</td>
    </tr>
<tr>
<td>num_features</td>
<td>Number of features. 
Should be greater than 0. 
(default = <code>2^18</code>)</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- IDF (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Compute the Inverse Document Frequency (IDF) given a collection of documents.
<code class="sourceCode r">ft_idf(x, input_col = NULL, output_col = NULL, min_doc_freq = 0,
  uid = <a href='random_string.html'>random_string</a>("idf_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>min_doc_freq</td>
<td>The minimum number of documents in which a term should appear. 
Default: 0</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Imputer (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Imputation estimator for completing missing values, either using the mean or
  the median of the columns in which the missing values are located. 
The input
  columns should be of numeric type. 
This function requires Spark 2.2.0+.
<code class="sourceCode r">ft_imputer(x, input_cols = NULL, output_cols = NULL,
  missing_value = NULL, strategy = "mean",
  uid = <a href='random_string.html'>random_string</a>("imputer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>The names of the input columns</td>
    </tr>
<tr>
<td>output_cols</td>
<td>The names of the output columns.</td>
    </tr>
<tr>
<td>missing_value</td>
<td>The placeholder for the missing values. 
All occurrences of<code> missing_value</code> will be imputed. 
Note that null values are always treated
as missing.</td>
    </tr>
<tr>
<td>strategy</td>
<td>The imputation strategy. 
Currently only "mean" and "median" are
supported. 
If "mean", then replace missing values using the mean value of the
feature. 
If "median", then replace missing values using the approximate median
value of the feature. 
Default: mean</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- IndexToString (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
A Transformer that maps a column of indices back to a new column of
  corresponding string values. 
The index-string mapping is either from
  the ML attributes of the input column, or from user-supplied labels
   (which take precedence over ML attributes). 
This function is the inverse
   of <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>.
<code class="sourceCode r">ft_index_to_string(x, input_col = NULL, output_col = NULL,
  labels = NULL, uid = <a href='random_string.html'>random_string</a>("index_to_string_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>labels</td>
<td>Optional param for array of labels specifying index-string mapping.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.
<code> <a href='ft_string_indexer.html'>ft_string_indexer</a></code>

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>, <code><a href='ft_interaction.html'>ft_interaction</a></code>,
  <code><a href='ft_lsh.html'>ft_lsh</a></code>, <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Interaction (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Implements the feature interaction transform. 
This transformer takes in Double and
  Vector type columns and outputs a flattened vector of their feature interactions.
  To handle interaction, we first one-hot encode any nominal features. 
Then, a
  vector of the feature cross-products is produced.
<code class="sourceCode r">ft_interaction(x, input_cols = NULL, output_col = NULL,
  uid = <a href='random_string.html'>random_string</a>("interaction_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>The names of the input columns</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- LSH (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Locality Sensitive Hashing functions for Euclidean distance
  (Bucketed Random Projection) and Jaccard distance (MinHash).
<code class="sourceCode r">ft_bucketed_random_projection_lsh(x, input_col = NULL,
  output_col = NULL, bucket_length = NULL, num_hash_tables = 1,
  seed = NULL, uid = <a href='random_string.html'>random_string</a>("bucketed_random_projection_lsh_"),
  ...)

ft_minhash_lsh(x, input_col = NULL, output_col = NULL,
  num_hash_tables = 1L, seed = NULL,
  uid = <a href='random_string.html'>random_string</a>("minhash_lsh_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>bucket_length</td>
<td>The length of each hash bucket, a larger bucket lowers the
false negative rate. 
The number of buckets will be (max L2 norm of input vectors) /
bucketLength.</td>
    </tr>
<tr>
<td>num_hash_tables</td>
<td>Number of hash tables used in LSH OR-amplification. 
LSH
OR-amplification can be used to reduce the false negative rate. 
Higher values
for this param lead to a reduced false negative rate, at the expense of added
 computational complexity.</td>
    </tr>
<tr>
<td>seed</td>
<td>A random seed. 
Set this value if you need your results to be
reproducible across repeated calls.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

ft_lsh_utils

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- MaxAbsScaler (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Rescale each feature individually to range [-1, 1] by dividing through the
  largest maximum absolute value in each feature. 
It does not shift/center the
  data, and thus does not destroy any sparsity.
<code class="sourceCode r">ft_max_abs_scaler(x, input_col = NULL, output_col = NULL,
  uid = <a href='random_string.html'>random_string</a>("max_abs_scaler_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  <a href='ft_vector_assembler.html'>ft_vector_assembler</a>(input_col = features,
        output_col = "features_temp") %&gt;%
  ft_max_abs_scaler(input_col = "features_temp",
       output_col = "features")
}</div></code>
<h2>Feature Transformation -- MinMaxScaler (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Rescale each feature individually to a common range [min, max] linearly using
  column summary statistics, which is also known as min-max normalization or
  Rescaling
<code class="sourceCode r">ft_min_max_scaler(x, input_col = NULL, output_col = NULL, min = 0,
  max = 1, uid = <a href='random_string.html'>random_string</a>("min_max_scaler_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>min</td>
<td>Lower bound after transformation, shared by all features Default: 0.0</td>
    </tr>
<tr>
<td>max</td>
<td>Upper bound after transformation, shared by all features Default: 1.0</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  <a href='ft_vector_assembler.html'>ft_vector_assembler</a>(input_col = features,
        output_col = "features_temp") %&gt;%
  ft_min_max_scaler(input_col = "features_temp",
       output_col = "features")
}</div></code>
<h2>Feature Transformation -- NGram (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
A feature transformer that converts the input array of strings into an array of n-grams. 
Null values in the input array are ignored. 
It returns an array of n-grams where each n-gram is represented by a space-separated string of words.
<code class="sourceCode r">ft_ngram(x, input_col = NULL, output_col = NULL, n = 2,
  uid = <a href='random_string.html'>random_string</a>("ngram_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>n</td>
<td>Minimum n-gram length, greater than or equal to 1. 
Default: 2, bigram features</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
When the input is empty, an empty array is returned. 
When the input array length is less than n (number of elements per n-gram), no n-grams are returned.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Normalizer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Normalize a vector to have unit norm using the given p-norm.
<code class="sourceCode r">ft_normalizer(x, input_col = NULL, output_col = NULL, p = 2,
  uid = <a href='random_string.html'>random_string</a>("normalizer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>p</td>
<td>Normalization in L^p space. 
Must be &gt;= 1. 
Defaults to 2.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- OneHotEncoder (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
One-hot encoding maps a column of label indices to a column of binary
vectors, with at most a single one-value. 
This encoding allows algorithms
which expect continuous features, such as Logistic Regression, to use
categorical features. 
Typically, used with  <code><a href='ft_string_indexer.html'>ft_string_indexer()</a></code> to
index a column first.
<code class="sourceCode r">ft_one_hot_encoder(x, input_col = NULL, output_col = NULL,
  drop_last = TRUE, uid = <a href='random_string.html'>random_string</a>("one_hot_encoder_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>drop_last</td>
<td>Whether to drop the last category. 
Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- OneHotEncoderEstimator (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
A one-hot encoder that maps a column of category indices
  to a column of binary vectors, with at most a single one-value
  per row that indicates the input category index. 
For example
  with 5 categories, an input value of 2.0 would map to an output
  vector of [0.0, 0.0, 1.0, 0.0]. 
The last category is not included
  by default (configurable via dropLast), because it makes the
  vector entries sum up to one, and hence linearly dependent. 
So
  an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0].
<code class="sourceCode r">ft_one_hot_encoder_estimator(x, input_cols = NULL, output_cols = NULL,
  handle_invalid = "error", drop_last = TRUE,
  uid = <a href='random_string.html'>random_string</a>("one_hot_encoder_estimator_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>Names of input columns.</td>
    </tr>
<tr>
<td>output_cols</td>
<td>Names of output columns.</td>
    </tr>
<tr>
<td>handle_invalid</td>
<td>(Spark 2.1.0+) Param for how to handle invalid entries. 
Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). 
Default: "error"</td>
    </tr>
<tr>
<td>drop_last</td>
<td>Whether to drop the last category. 
Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- PolynomialExpansion (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Perform feature expansion in a polynomial space. 
E.g. 
take a 2-variable feature
  vector as an example: (x, y), if we want to expand it with degree 2, then
  we get (x, x * x, y, x * y, y * y).
<code class="sourceCode r">ft_polynomial_expansion(x, input_col = NULL, output_col = NULL,
  degree = 2, uid = <a href='random_string.html'>random_string</a>("polynomial_expansion_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>degree</td>
<td>The polynomial degree to expand, which should be greater
than equal to 1. 
A value of 1 means no expansion. 
Default: 2</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- QuantileDiscretizer (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a><code> ft_quantile_discretizer</code> takes a column with continuous features and outputs
  a column with binned categorical features. 
The number of bins can be
  set using the <code>num_buckets</code> parameter. 
It is possible that the number
  of buckets used will be smaller than this value, for example, if there
  are too few distinct values of the input to create enough distinct
  quantiles.
<code class="sourceCode r">ft_quantile_discretizer(x, input_col = NULL, output_col = NULL,
  num_buckets = 2, input_cols = NULL, output_cols = NULL,
  num_buckets_array = NULL, handle_invalid = "error",
  relative_error = 0.001, uid = <a href='random_string.html'>random_string</a>("quantile_discretizer_"),
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>num_buckets</td>
<td>Number of buckets (quantiles, or categories) into which data
points are grouped. 
Must be greater than or equal to 2.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>Names of input columns.</td>
    </tr>
<tr>
<td>output_cols</td>
<td>Names of output columns.</td>
    </tr>
<tr>
<td>num_buckets_array</td>
<td>Array of number of buckets (quantiles, or categories)
into which data points are grouped. 
Each value must be greater than or equal to 2.</td>
    </tr>
<tr>
<td>handle_invalid</td>
<td>(Spark 2.1.0+) Param for how to handle invalid entries. 
Options are
'skip' (filter out rows with invalid values), 'error' (throw an error), or
'keep' (keep invalid values in a special additional bucket). 
Default: "error"</td>
    </tr>
<tr>
<td>relative_error</td>
<td>(Spark 2.0.0+) Relative error (see documentation for
org.apache.spark.sql.DataFrameStatFunctions.approxQuantile
<a href='https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions'>here</a>
for description). 
Must be in the range [0, 1]. 
default: 0.001</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
NaN handling: null and NaN values will be ignored from the column
  during <code>QuantileDiscretizer</code> fitting. 
This will produce a <code>Bucketizer</code>
  model for making predictions. 
During the transformation, <code>Bucketizer</code>
  will raise an error when it finds NaN values in the dataset, but the
  user can also choose to either keep or remove NaN values within the
  dataset by setting <code>handle_invalid</code> If the user chooses to keep NaN values,
  they will be handled specially and placed into their own bucket,
  for example, if 4 buckets are used, then non-NaN data will be put
  into buckets[0-3], but NaNs will be counted in a special bucket[4].

Algorithm: The bin ranges are chosen using an approximate algorithm (see
  the documentation for org.apache.spark.sql.DataFrameStatFunctions.approxQuantile
  <a href='https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions'>here</a> for a detailed description). 
The precision of the approximation can be
  controlled with the <code>relative_error</code> parameter. 
The lower and upper bin
  bounds will be -Infinity and +Infinity, covering all real values.

Note that the result may be different every time you run it, since the sample
  strategy behind it is non-deterministic.

In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.
<code> <a href='ft_bucketizer.html'>ft_bucketizer</a></code>

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- RFormula (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Implements the transforms required for fitting a dataset against an R model
  formula. 
Currently we support a limited subset of the R operators,
  including <code>~</code>, <code>.</code>, <code>:</code>, <code>+</code>, and <code>-</code>. 
Also see the R formula docs here:
  <a href='http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html'>http://stat.ethz.ch/R-manual/R-patched/library/stats/html/formula.html</a>
<code class="sourceCode r">ft_r_formula(x, formula = NULL, features_col = "features",
  label_col = "label", force_index_label = FALSE,
  uid = <a href='random_string.html'>random_string</a>("r_formula_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>formula</td>
<td>R formula as a character string or a formula. 
Formula objects are
converted to character strings directly and the environment is not captured.</td>
    </tr>
<tr>
<td>features_col</td>
<td>Features column name, as a length-one character vector. 
The column should be single vector column of numeric values. 
Usually this column is output by <code>ft_r_formula</code>.</td>
    </tr>
<tr>
<td>label_col</td>
<td>Label column name. 
The column should be a numeric column. 
Usually this column is output by <code>ft_r_formula</code>.</td>
    </tr>
<tr>
<td>force_index_label</td>
<td>(Spark 2.1.0+) Force to index label whether it is numeric or
string type. 
Usually we index label only when it is string type. 
If
the formula was used by classification algorithms, we can force to index
label even it is numeric type by setting this param with true.
Default: <code>FALSE</code>.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
The basic operators in the formula are:

~ separate target and terms

+ concat terms, "+ 0" means removing intercept

- remove a term, "- 1" means removing intercept

: interaction (multiplication for numeric values, or binarized categorical values)

. 
all columns except target

Suppose a and b are double columns, we use the following simple examples to illustrate the
  effect of RFormula:
<code> y ~ a + b</code> means model <code>y ~ w0 + w1 * a + w2 * b</code>
where <code>w0</code> is the intercept and <code>w1, w2</code> are coefficients.
<code> y ~ a + b + a:b - 1</code> means model <code>y ~ w1 * a + w2 * b + w3 * a * b</code>
where <code>w1, w2, w3</code> are coefficients.

RFormula produces a vector column of features and a double or string column
 of label. 
Like when formulas are used in R for linear regression, string
 input columns will be one-hot encoded, and numeric columns will be cast to
 doubles. 
If the label column is of type string, it will be first transformed
 to double with StringIndexer. 
If the label column does not exist in the
 DataFrame, the output label column will be created from the specified
 response variable in the formula.

In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- RegexTokenizer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
A regex based tokenizer that extracts tokens either by using the provided
regex pattern to split the text (default) or repeatedly matching the regex
(if <code>gaps</code> is false). 
Optional parameters also allow filtering tokens using a
minimal length. 
It returns an array of strings that can be empty.
<code class="sourceCode r">ft_regex_tokenizer(x, input_col = NULL, output_col = NULL,
  gaps = TRUE, min_token_length = 1, pattern = "\\s+",
  to_lower_case = TRUE, uid = <a href='random_string.html'>random_string</a>("regex_tokenizer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>gaps</td>
<td>Indicates whether regex splits on gaps (TRUE) or matches tokens (FALSE).</td>
    </tr>
<tr>
<td>min_token_length</td>
<td>Minimum token length, greater than or equal to 0.</td>
    </tr>
<tr>
<td>pattern</td>
<td>The regular expression pattern to be used.</td>
    </tr>
<tr>
<td>to_lower_case</td>
<td>Indicates whether to convert all characters to lowercase before tokenizing.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- StandardScaler (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Standardizes features by removing the mean and scaling to unit variance using
  column summary statistics on the samples in the training set. 
The "unit std"
   is computed using the corrected sample standard deviation, which is computed
   as the square root of the unbiased sample variance.
<code class="sourceCode r">ft_standard_scaler(x, input_col = NULL, output_col = NULL,
  with_mean = FALSE, with_std = TRUE,
  uid = <a href='random_string.html'>random_string</a>("standard_scaler_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>with_mean</td>
<td>Whether to center the data with mean before scaling. 
It will
build a dense output, so take care when applying to sparse input. 
Default: FALSE</td>
    </tr>
<tr>
<td>with_std</td>
<td>Whether to scale the data to unit standard deviation. 
Default: TRUE</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
iris_tbl &lt;- <a href='sdf_copy_to.html'>sdf_copy_to</a>(sc, iris, name = "iris_tbl", overwrite = TRUE)

features &lt;- <a href='https://rdrr.io/r/base/c.html'>c</a>("Sepal_Length", "Sepal_Width", "Petal_Length", "Petal_Width")

iris_tbl %&gt;%
  <a href='ft_vector_assembler.html'>ft_vector_assembler</a>(input_col = features,
        output_col = "features_temp") %&gt;%
  ft_standard_scaler(input_col = "features_temp",
       output_col = "features",
       with_mean = TRUE)
}</div></code>
<h2>Feature Transformation -- StopWordsRemover (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
A feature transformer that filters out stop words from input.
<code class="sourceCode r">ft_stop_words_remover(x, input_col = NULL, output_col = NULL,
  case_sensitive = FALSE,
  stop_words = <a href='ml_default_stop_words.html'>ml_default_stop_words</a>(<a href='spark_connection.html'>spark_connection</a>(x), "english"),
  uid = <a href='random_string.html'>random_string</a>("stop_words_remover_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>case_sensitive</td>
<td>Whether to do a case sensitive comparison over the stop words.</td>
    </tr>
<tr>
<td>stop_words</td>
<td>The words to be filtered out.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.
<code> <a href='ml_default_stop_words.html'>ml_default_stop_words</a></code>

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- Tokenizer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
A tokenizer that converts the input string to lowercase and then splits it
by white spaces.
<code class="sourceCode r">ft_tokenizer(x, input_col = NULL, output_col = NULL,
  uid = <a href='random_string.html'>random_string</a>("tokenizer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- VectorAssembler (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Combine multiple vectors into a single row-vector; that is,
where each row element of the newly generated column is a
vector formed by concatenating each row element from the
specified input columns.
<code class="sourceCode r">ft_vector_assembler(x, input_cols = NULL, output_col = NULL,
  uid = <a href='random_string.html'>random_string</a>("vector_assembler_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_cols</td>
<td>The names of the input columns</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- VectorIndexer (Estimator) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Indexing categorical feature columns in a dataset of Vector.
<code class="sourceCode r">ft_vector_indexer(x, input_col = NULL, output_col = NULL,
  max_categories = 20, uid = <a href='random_string.html'>random_string</a>("vector_indexer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>max_categories</td>
<td>Threshold for the number of values a categorical feature can take. 
If a feature is found to have &gt; <code>max_categories</code> values, then it is declared continuous. 
Must be greater than or equal to 2. 
Defaults to 20.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3>
In the case where <code>x</code> is a <code>tbl_spark</code>, the estimator fits against <code>x</code>
  to obtain a transformer, which is then immediately used to transform <code>x</code>, returning a <code>tbl_spark</code>.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- VectorSlicer (Transformer) </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#see-also">See also</a>
Takes a feature vector and outputs a new feature vector with a subarray of the original features.
<code class="sourceCode r">ft_vector_slicer(x, input_col = NULL, output_col = NULL,
  indices = NULL, uid = <a href='random_string.html'>random_string</a>("vector_slicer_"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>input_col</td>
<td>The name of the input column.</td>
    </tr>
<tr>
<td>output_col</td>
<td>The name of the output column.</td>
    </tr>
<tr>
<td>indices</td>
<td>An vector of indices to select features from a vector column.
Note that the indices are 0-based.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='sql-transformer.html'>ft_sql_transformer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Feature Transformation -- SQLTransformer </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
Implements the transformations which are defined by SQL statement. 
Currently we
  only support SQL syntax like 'SELECT ... 
FROM __THIS__ ...' where '__THIS__' represents
  the underlying table of the input dataset. 
The select clause specifies the
  fields, constants, and expressions to display in the output, it can be any
  select clause that Spark SQL supports. 
Users can also use Spark SQL built-in
  function and UDFs to operate on these selected columns.
<code class="sourceCode r">ft_sql_transformer(x, statement = NULL,
  uid = <a href='random_string.html'>random_string</a>("sql_transformer_"), ...)

ft_dplyr_transformer(x, tbl, uid = <a href='random_string.html'>random_string</a>("dplyr_transformer_"),
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A <code>spark_connection</code>, <code>ml_pipeline</code>, or a <code>tbl_spark</code>.</td>
    </tr>
<tr>
<td>statement</td>
<td>A SQL statement.</td>
    </tr>
<tr>
<td>uid</td>
<td>A character string used to uniquely identify the feature transformer.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
<tr>
<td>tbl</td>
<td>A <code>tbl_spark</code> generated using <code>dplyr</code> transformations.</td>
    </tr>
    </table>
<h3>Value</h3>
The object returned depends on the class of <code>x</code>.
<code> spark_connection</code>: When <code>x</code> is a <code>spark_connection</code>, the function returns a <code>ml_transformer</code>,
  a <code>ml_estimator</code>, or one of their subclasses. 
The object contains a pointer to
  a Spark <code>Transformer</code> or <code>Estimator</code> object and can be used to compose
  <code>Pipeline</code> objects.
<code> ml_pipeline</code>: When <code>x</code> is a <code>ml_pipeline</code>, the function returns a <code>ml_pipeline</code> with
  the transformer or estimator appended to the pipeline.
<code> tbl_spark</code>: When <code>x</code> is a <code>tbl_spark</code>, a transformer is constructed then
  immediately applied to the input <code>tbl_spark</code>, returning a <code>tbl_spark</code>
<h3>Details</h3><code> ft_dplyr_transformer()</code> is a wrapper around <code>ft_sql_transformer()</code> that
  takes a <code>tbl_spark</code> instead of a SQL statement. 
Internally, the <code>ft_dplyr_transformer()</code>
  extracts the <code>dplyr</code> transformations used to generate <code>tbl</code> as a SQL statement
  then passes it on to <code>ft_sql_transformer()</code>. 
Note that only single-table <code>dplyr</code> verbs
  are supported and that the <code>sdf_</code> family of functions are not.
<h3>See also</h3>
See <a href='http://spark.apache.org/docs/latest/ml-features.html'>http://spark.apache.org/docs/latest/ml-features.html</a> for
  more information on the set of transformations available for DataFrame
  columns in Spark.

Other feature transformers: <code><a href='ft_binarizer.html'>ft_binarizer</a></code>,
  <code><a href='ft_bucketizer.html'>ft_bucketizer</a></code>,
  <code><a href='ft_chisq_selector.html'>ft_chisq_selector</a></code>,
  <code><a href='ft_count_vectorizer.html'>ft_count_vectorizer</a></code>, <code><a href='ft_dct.html'>ft_dct</a></code>,
  <code><a href='ft_elementwise_product.html'>ft_elementwise_product</a></code>,
  <code><a href='ft_feature_hasher.html'>ft_feature_hasher</a></code>,
  <code><a href='ft_hashing_tf.html'>ft_hashing_tf</a></code>, <code><a href='ft_idf.html'>ft_idf</a></code>,
  <code><a href='ft_imputer.html'>ft_imputer</a></code>,
  <code><a href='ft_index_to_string.html'>ft_index_to_string</a></code>,
  <code><a href='ft_interaction.html'>ft_interaction</a></code>, <code><a href='ft_lsh.html'>ft_lsh</a></code>,
  <code><a href='ft_max_abs_scaler.html'>ft_max_abs_scaler</a></code>,
  <code><a href='ft_min_max_scaler.html'>ft_min_max_scaler</a></code>, <code><a href='ft_ngram.html'>ft_ngram</a></code>,
  <code><a href='ft_normalizer.html'>ft_normalizer</a></code>,
  <code><a href='ft_one_hot_encoder_estimator.html'>ft_one_hot_encoder_estimator</a></code>,
  <code><a href='ft_one_hot_encoder.html'>ft_one_hot_encoder</a></code>, <code><a href='ft_pca.html'>ft_pca</a></code>,
  <code><a href='ft_polynomial_expansion.html'>ft_polynomial_expansion</a></code>,
  <code><a href='ft_quantile_discretizer.html'>ft_quantile_discretizer</a></code>,
  <code><a href='ft_r_formula.html'>ft_r_formula</a></code>,
  <code><a href='ft_regex_tokenizer.html'>ft_regex_tokenizer</a></code>,
  <code><a href='ft_standard_scaler.html'>ft_standard_scaler</a></code>,
  <code><a href='ft_stop_words_remover.html'>ft_stop_words_remover</a></code>,
  <code><a href='ft_string_indexer.html'>ft_string_indexer</a></code>,
  <code><a href='ft_tokenizer.html'>ft_tokenizer</a></code>,
  <code><a href='ft_vector_assembler.html'>ft_vector_assembler</a></code>,
  <code><a href='ft_vector_indexer.html'>ft_vector_indexer</a></code>,
  <code><a href='ft_vector_slicer.html'>ft_vector_slicer</a></code>, <code><a href='ft_word2vec.html'>ft_word2vec</a></code>
<h2>Compile Scala sources into a Java Archive (jar) </h2>
<a href="#arguments">Arguments</a>
Compile the <code>scala</code> source files contained within an R package
into a Java Archive (<code>jar</code>) file that can be loaded and used within
a Spark environment.
<code class="sourceCode r">compile_package_jars(..., spec = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>...</td>
<td>Optional compilation specifications, as generated by<code> spark_compilation_spec</code>. 
When no arguments are passed,<code> spark_default_compilation_spec</code> is used instead.</td>
    </tr>
<tr>
<td>spec</td>
<td>An optional list of compilation specifications. 
When
set, this option takes precedence over arguments passed to<code> ...</code>.</td>
    </tr>
    </table>
<h2>Read configuration values for a connection </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Read configuration values for a connection
<code class="sourceCode r">connection_config(sc, prefix, not_prefix = <a href='https://rdrr.io/r/base/list.html'>list</a>())</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td><code>spark_connection</code></td>
    </tr>
<tr>
<td>prefix</td>
<td>Prefix to read parameters for
(e.g. <code> spark.context.</code>, <code>spark.sql.</code>, etc.)</td>
    </tr>
<tr>
<td>not_prefix</td>
<td>Prefix to not include.</td>
    </tr>
    </table>
<h3>Value</h3>
Named list of config parameters (note that if a prefix was
 specified then the names will not include the prefix)
<h2>Downloads default Scala Compilers </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a><code> compile_package_jars</code> requires several versions of the
scala compiler to work, this is to match Spark scala versions.
To help setup your environment, this function will download the
required compilers under the default search path.
<code class="sourceCode r">download_scalac(dest_path = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>dest_path</td>
<td>The destination path where scalac will be
downloaded to.</td>
    </tr>
    </table>
<h3>Details</h3>
See <code>find_scalac</code> for a list of paths searched and used by
this function to install the required compilers.
<h2>Discover the Scala Compiler </h2>
<a href="#arguments">Arguments</a>
Find the <code>scalac</code> compiler for a particular version of<code> scala</code>, by scanning some common directories containing<code> scala</code> installations.
<code class="sourceCode r">find_scalac(version, locations = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>version</td>
<td>The <code>scala</code> version to search for. 
Versions
of the form <code>major.minor</code> will be matched against the<code> scalac</code> installation with version <code>major.minor.patch</code>;
if multiple compilers are discovered the most recent one will be
used.</td>
    </tr>
<tr>
<td>locations</td>
<td>Additional locations to scan. 
By default, the
directories <code>/opt/scala</code> and <code>/usr/local/scala</code> will
be scanned.</td>
    </tr>
    </table>
<h2>Access the Spark API </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#spark-context">Spark Context</a>
    <a href="#java-spark-context">Java Spark Context</a>
    <a href="#hive-context">Hive Context</a>
    <a href="#spark-session">Spark Session</a>
Access the commonly-used Spark objects associated with a Spark instance.
These objects provide access to different facets of the Spark API.
<code class="sourceCode r">spark_context(sc)

java_context(sc)

hive_context(sc)

spark_session(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
    </table>
<h3>Details</h3>
The <a href='http://spark.apache.org/docs/latest/api/scala/#package'>Scala API documentation</a>
is useful for discovering what methods are available for each of these
objects. 
Use <code><a href='invoke.html'>invoke</a></code> to call methods on these objects.
<h2 id="spark-context">Spark Context</h2>
The main entry point for Spark functionality. 
The <strong>Spark Context</strong>
represents the connection to a Spark cluster, and can be used to create<code> RDD</code>s, accumulators and broadcast variables on that cluster.
<h2 id="java-spark-context">Java Spark Context</h2>
A Java-friendly version of the aforementioned <strong>Spark Context</strong>.
<h2 id="hive-context">Hive Context</h2>
An instance of the Spark SQL execution engine that integrates with data
stored in Hive. 
Configuration for Hive is read from <code>hive-site.xml</code> on
the classpath.

Starting with Spark &gt;= 2.0.0, the <strong>Hive Context</strong> class has been
deprecated -- it is superceded by the <strong>Spark Session</strong> class, and<code> hive_context</code> will return a <strong>Spark Session</strong> object instead.
Note that both classes share a SQL interface, and therefore one can invoke
SQL through these objects.
<h2 id="spark-session">Spark Session</h2>
Available since Spark 2.0.0, the <strong>Spark Session</strong> unifies the
<strong>Spark Context</strong> and <strong>Hive Context</strong> classes into a single
interface. 
Its use is recommended over the older APIs for code
targeting Spark 2.0.0 and above.
<h2>Runtime configuration interface for Hive </h2>
<a href="#arguments">Arguments</a>
Retrieves the runtime configuration interface for Hive.
<code class="sourceCode r">hive_context_config(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
    </table>
<h2>Invoke a Method on a JVM Object </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#examples">Examples</a>
Invoke methods on Java object references. 
These functions provide a
mechanism for invoking various Java object methods directly from R.
<code class="sourceCode r">invoke(jobj, method, ...)

invoke_static(sc, class, method, ...)

invoke_new(sc, class, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>jobj</td>
<td>An R object acting as a Java object reference (typically, a <code>spark_jobj</code>).</td>
    </tr>
<tr>
<td>method</td>
<td>The name of the method to be invoked.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments, currently unused.</td>
    </tr>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>class</td>
<td>The name of the Java class whose methods should be invoked.</td>
    </tr>
    </table>
<h3>Details</h3>
Use each of these functions in the following scenarios:

<table class='table'>
<tr><td><code>invoke</code></td><td>Execute a method on a Java object reference (typically, a <code>spark_jobj</code>).</td><td><code>invoke_static</code></td></tr>
<tr><td>Execute a static method associated with a Java class.</td><td><code>invoke_new</code></td><td>Invoke a constructor associated with a Java class.</td></tr>
</table>
<h3>Examples</h3>
    <code class="sourceCode r">
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "spark://HOST:PORT")
<a href='spark-api.html'>spark_context</a>(sc) %&gt;%
  invoke("textFile", "file.csv", 1L) %&gt;%
    invoke("count")
</div></code>
<h2>Register a Package that Implements a Spark Extension </h2>
<a href="#arguments">Arguments</a>
    <a href="#note">Note</a>
Registering an extension package will result in the package being
automatically scanned for spark dependencies when a connection to Spark is
created.
<code class="sourceCode r">register_extension(package)

registered_extensions()</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>package</td>
<td>The package(s) to register.</td>
    </tr>
    </table>
<h2 id="note">Note</h2>
Packages should typically register their extensions in their
  <code>.onLoad</code> hook -- this ensures that their extensions are registered
  when their namespaces are loaded.
<h2>Define a Spark Compilation Specification </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
For use with <code><a href='compile_package_jars.html'>compile_package_jars</a></code>. 
The Spark compilation
specification is used when compiling Spark extension Java Archives, and
defines which versions of Spark, as well as which versions of Scala, should
be used for compilation.
<code class="sourceCode r">spark_compilation_spec(spark_version = NULL, spark_home = NULL,
  scalac_path = NULL, scala_filter = NULL, jar_name = NULL,
  jar_path = NULL, jar_dep = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>spark_version</td>
<td>The Spark version to build against. 
This can
be left unset if the path to a suitable Spark home is supplied.</td>
    </tr>
<tr>
<td>spark_home</td>
<td>The path to a Spark home installation. 
This can
be left unset if <code>spark_version</code> is supplied; in such a case,<code> sparklyr</code> will attempt to discover the associated Spark
installation using <code><a href='spark_home_dir.html'>spark_home_dir</a></code>.</td>
    </tr>
<tr>
<td>scalac_path</td>
<td>The path to the <code>scalac</code> compiler to be used
during compilation of your Spark extension. 
Note that you should
ensure the version of <code>scalac</code> selected matches the version of<code> scalac</code> used with the version of Spark you are compiling against.</td>
    </tr>
<tr>
<td>scala_filter</td>
<td>An optional R function that can be used to filter
which <code>scala</code> files are used during compilation. 
This can be
useful if you have auxiliary files that should only be included with
certain versions of Spark.</td>
    </tr>
<tr>
<td>jar_name</td>
<td>The name to be assigned to the generated <code>jar</code>.</td>
    </tr>
<tr>
<td>jar_path</td>
<td>The path to the <code>jar</code> tool to be used
during compilation of your Spark extension.</td>
    </tr>
<tr>
<td>jar_dep</td>
<td>An optional list of additional <code>jar</code> dependencies.</td>
    </tr>
    </table>
<h3>Details</h3>
Most Spark extensions won't need to define their own compilation specification,
and can instead rely on the default behavior of <code>compile_package_jars</code>.
<h2>Default Compilation Specification for Spark Extensions </h2>
<a href="#arguments">Arguments</a>
This is the default compilation specification used for
Spark extensions, when used with <code><a href='compile_package_jars.html'>compile_package_jars</a></code>.
<code class="sourceCode r">spark_default_compilation_spec(pkg = infer_active_package_name(),
  locations = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>pkg</td>
<td>The package containing Spark extensions to be compiled.</td>
    </tr>
<tr>
<td>locations</td>
<td>Additional locations to scan. 
By default, the
directories <code>/opt/scala</code> and <code>/usr/local/scala</code> will
be scanned.</td>
    </tr>
    </table>
<h2>Retrieve the Spark Connection Associated with an R Object </h2>
<a href="#arguments">Arguments</a>
Retrieve the <code>spark_connection</code> associated with an R object.
<code class="sourceCode r">spark_connection(x, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An R object from which a <code>spark_connection</code> can be obtained.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2>Runtime configuration interface for the Spark Context. </h2>
<a href="#arguments">Arguments</a>
Retrieves the runtime configuration interface for the Spark Context.
<code class="sourceCode r">spark_context_config(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
    </table>
<h2>Retrieve a Spark DataFrame </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
This S3 generic is used to access a Spark DataFrame object (as a Java
object reference) from an R object.
<code class="sourceCode r">spark_dataframe(x, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An R object wrapping, or containing, a Spark DataFrame.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Value</h3>
A <code><a href='spark_jobj.html'>spark_jobj</a></code> representing a Java object reference
  to a Spark DataFrame.
<h2>Define a Spark dependency </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
Define a Spark dependency consisting of a set of custom JARs and Spark packages.
<code class="sourceCode r">spark_dependency(jars = NULL, packages = NULL, initializer = NULL,
  catalog = NULL, repositories = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>jars</td>
<td>Character vector of full paths to JAR files.</td>
    </tr>
<tr>
<td>packages</td>
<td>Character vector of Spark packages names.</td>
    </tr>
<tr>
<td>initializer</td>
<td>Optional callback function called when initializing a connection.</td>
    </tr>
<tr>
<td>catalog</td>
<td>Optional location where extension JAR files can be downloaded for Livy.</td>
    </tr>
<tr>
<td>repositories</td>
<td>Character vector of Spark package repositories.</td>
    </tr>
<tr>
<td>...</td>
<td>Additional optional arguments.</td>
    </tr>
    </table>
<h3>Value</h3>
An object of type `spark_dependency`
<h2>Set the SPARK_HOME environment variable </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#examples">Examples</a>
Set the <code>SPARK_HOME</code> environment variable. 
This slightly speeds up some
operations, including the connection time.
<code class="sourceCode r">spark_home_set(path = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>path</td>
<td>A string containing the path to the installation location of
Spark. 
If <code>NULL</code>, the path to the most latest Spark/Hadoop versions is
used.</td>
    </tr>
<tr>
<td>...</td>
<td>Additional parameters not currently used.</td>
    </tr>
    </table>
<h3>Value</h3>
The function is mostly invoked for the side-effect of setting the<code> SPARK_HOME</code> environment variable. 
It also returns <code>TRUE</code> if the
environment was successfully set, and <code>FALSE</code> otherwise.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
# Not run due to side-effects
spark_home_set()
}</div></code>
<h2>Retrieve a Spark JVM Object Reference </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
This S3 generic is used for accessing the underlying Java Virtual Machine
(JVM) Spark objects associated with R objects. 
These objects act as
references to Spark objects living in the JVM. 
Methods on these objects
can be called with the <code><a href='invoke.html'>invoke</a></code> family of functions.
<code class="sourceCode r">spark_jobj(x, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An R object containing, or wrapping, a <code>spark_jobj</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3><code> <a href='invoke.html'>invoke</a></code>, for calling methods on Java object references.
<h2>Get the Spark Version Associated with a Spark Connection </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
Retrieve the version of Spark associated with a Spark connection.
<code class="sourceCode r">spark_version(sc)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
The Spark version as a <code><a href='https://rdrr.io/r/base/numeric_version.html'>numeric_version</a></code>.
<h3>Details</h3>
Suffixes for e.g. 
preview versions, or snapshotted versions,
are trimmed -- if you require the full Spark version, you can
retrieve it with <code><a href='invoke.html'>invoke(spark_context(sc), "version")</a></code>.
<h2>Apply an R Function in Spark </h2>
<a href="#arguments">Arguments</a>
    <a href="#configuration">Configuration</a>
    <a href="#examples">Examples</a>
Applies an R function to a Spark object (typically, a Spark DataFrame).
<code class="sourceCode r">spark_apply(x, f, columns = NULL, memory = !<a href='https://rdrr.io/r/base/NULL.html'>is.null</a>(name),
  group_by = NULL, packages = NULL, context = NULL, name = NULL,
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object (usually a <code>spark_tbl</code>) coercable to a Spark DataFrame.</td>
    </tr>
<tr>
<td>f</td>
<td>A function that transforms a data frame partition into a data frame.
  The function <code>f</code> has signature <code>f(df, context, group1, group2, ...)</code> where
  <code>df</code> is a data frame with the data to be processed, <code>context</code>
  is an optional object passed as the <code>context</code> parameter and <code>group1</code> to
  <code>groupN</code> contain the values of the <code>group_by</code> values. 
When
  <code>group_by</code> is not specified, <code>f</code> takes only one argument.

Can also be an <code>rlang</code> anonymous function. 
For example, as <code>~ .x + 1</code>
  to define an expression that adds one to the given <code>.x</code> data frame.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types for
the transformed object. 
When not specified, a sample of 10 rows is taken to
infer out the output columns automatically, to avoid this performance penalty,
specify the column types. 
The sample size is configurable using the<code> sparklyr.apply.schema.infer</code> configuration option.</td>
    </tr>
<tr>
<td>memory</td>
<td>Boolean; should the table be cached into memory?</td>
    </tr>
<tr>
<td>group_by</td>
<td>Column name used to group by data frame partitions.</td>
    </tr>
<tr>
<td>packages</td>
<td>Boolean to distribute <code><a href='https://rdrr.io/r/base/libPaths.html'>.libPaths()</a></code> packages to each node,
  a list of packages to distribute, or a package bundle created with
  <code><a href='spark_apply_bundle.html'>spark_apply_bundle()</a></code>.

Defaults to <code>TRUE</code> or the <code>sparklyr.apply.packages</code> value set in
  <code><a href='spark_config.html'>spark_config()</a></code>.

For clusters using Yarn cluster mode, <code>packages</code> can point to a package
  bundle created using <code><a href='spark_apply_bundle.html'>spark_apply_bundle()</a></code> and made available as a Spark
  file using <code>config$sparklyr.shell.files</code>. 
For clusters using Livy, packages
  can be manually installed on the driver node.

For offline clusters where <code><a href='https://rdrr.io/r/utils/available.packages.html'>available.packages()</a></code> is not available,
  manually download the packages database from
 https://cran.r-project.org/web/packages/packages.rds and set
  <code><a href='https://rdrr.io/r/base/Sys.setenv.html'>Sys.setenv(sparklyr.apply.packagesdb = "&lt;pathl-to-rds&gt;")</a></code>. 
Otherwise,
  all packages will be used by default.

For clusters where R packages already installed in every worker node,
  the <code>spark.r.libpaths</code> config entry can be set in <code><a href='spark_config.html'>spark_config()</a></code>
  to the local packages library. 
To specify multiple paths collapse them
  (without spaces) with a comma delimiter (e.g., <code>"/lib/path/one,/lib/path/two"</code>).</td>
    </tr>
<tr>
<td>context</td>
<td>Optional object to be serialized and passed back to <code>f()</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>Optional table name while registering the resulting data frame.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2 id="configuration">Configuration</h2>
<code> <a href='spark_config.html'>spark_config()</a></code> settings can be specified to change the workers
environment.

For instance, to set additional environment variables to each
worker node use the <code>sparklyr.apply.env.*</code> config, to launch workers
without <code>--vanilla</code> use <code>sparklyr.apply.options.vanilla</code> set to<code> FALSE</code>, to run a custom script before launching Rscript use<code> sparklyr.apply.options.rscript.before</code>.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

# creates an Spark data frame with 10 elements then multiply times 10 in R
<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% spark_apply(function(df) df * 10)

}</div></code>
<h2>Create Bundle for Spark Apply </h2>
<a href="#arguments">Arguments</a>
Creates a bundle of packages for <code><a href='spark_apply.html'>spark_apply()</a></code>.
<code class="sourceCode r">spark_apply_bundle(packages = TRUE, base_path = <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>())</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>packages</td>
<td>List of packages to pack or <code>TRUE</code> to pack all.</td>
    </tr>
<tr>
<td>base_path</td>
<td>Base path used to store the resulting bundle.</td>
    </tr>
    </table>
<h2>Log Writer for Spark Apply </h2>
<a href="#arguments">Arguments</a>
Writes data to log under <code><a href='spark_apply.html'>spark_apply()</a></code>.
<code class="sourceCode r">spark_apply_log(..., level = "INFO")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>...</td>
<td>Arguments to write to log.</td>
    </tr>
<tr>
<td>level</td>
<td>Severity level for this entry; recommended values: <code>INFO</code>,<code> ERROR</code> or <code>WARN</code>.</td>
    </tr>
    </table>
<h2>Create a Spark Configuration for Livy </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#details">Details</a>
Create a Spark Configuration for Livy
<code class="sourceCode r">livy_config(config = <a href='spark_config.html'>spark_config</a>(), username = NULL,
  password = NULL, negotiate = FALSE,
  custom_headers = <a href='https://rdrr.io/r/base/list.html'>list</a>(`X-Requested-By` = "sparklyr"), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>config</td>
<td>Optional base configuration</td>
    </tr>
<tr>
<td>username</td>
<td>The username to use in the Authorization header</td>
    </tr>
<tr>
<td>password</td>
<td>The password to use in the Authorization header</td>
    </tr>
<tr>
<td>negotiate</td>
<td>Whether to use gssnegotiate method or not</td>
    </tr>
<tr>
<td>custom_headers</td>
<td>List of custom headers to append to http requests. 
Defaults to <code><a href='https://rdrr.io/r/base/list.html'>list("X-Requested-By" = "sparklyr")</a></code>.</td>
    </tr>
<tr>
<td>...</td>
<td>additional Livy session parameters</td>
    </tr>
    </table>
<h3>Value</h3>
Named list with configuration data
<h3>Details</h3>
Extends a Spark <code><a href='spark_config.html'>spark_config()</a></code> configuration with settings
for Livy. 
For instance, <code>username</code> and <code>password</code>
define the basic authentication settings for a Livy session.

The default value of <code>"custom_headers"</code> is set to <code><a href='https://rdrr.io/r/base/list.html'>list("X-Requested-By" = "sparklyr")</a></code>
in order to facilitate connection to Livy servers with CSRF protection enabled.

Additional parameters for Livy sessions are:
<dl class='dl-horizontal'>
  <dt><code>proxy_user</code></dt><dd>User to impersonate when starting the session
</dd>
  <dt><code>jars</code></dt><dd>jars to be used in this session
</dd>
  <dt><code>py_files</code></dt><dd>Python files to be used in this session
</dd>
  <dt><code>files</code></dt><dd>files to be used in this session
</dd>
  <dt><code>driver_memory</code></dt><dd>Amount of memory to use for the driver process
</dd>
  <dt><code>driver_cores</code></dt><dd>Number of cores to use for the driver process
</dd>
  <dt><code>executor_memory</code></dt><dd>Amount of memory to use per executor process
</dd>
  <dt><code>executor_cores</code></dt><dd>Number of cores to use for each executor
</dd>
  <dt><code>num_executors</code></dt><dd>Number of executors to launch for this session
</dd>
  <dt><code>archives</code></dt><dd>Archives to be used in this session
</dd>
  <dt><code>queue</code></dt><dd>The name of the YARN queue to which submitted
</dd>
  <dt><code>name</code></dt><dd>The name of this session
</dd>
  <dt><code>heartbeat_timeout</code></dt><dd>Timeout in seconds to which session be orphaned
</dd>

</dl>

Note that <code>queue</code> is supported only by version 0.4.0 of Livy or newer.
If you are using the older one, specify queue via <code>config</code> (e.g.<code> config = spark_config(spark.yarn.queue = "my_queue")</code>).
<h2>Start Livy </h2>
<a href="#arguments">Arguments</a>
Starts the livy service.

Stops the running instances of the livy service.
<code class="sourceCode r">livy_service_start(version = NULL, spark_version = NULL, stdout = "",
  stderr = "", ...)

livy_service_stop()</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>version</td>
<td>The version of <samp>livy</samp> to use.</td>
    </tr>
<tr>
<td>spark_version</td>
<td>The version of <samp>spark</samp> to connect to.</td>
    </tr>
<tr>
<td>stdout, stderr</td>
<td>where output to 'stdout' or 'stderr' should
be sent. 
Same options as <code>system2</code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h2>Find Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#examples">Examples</a>
Finds and returns a stream based on the stream's identifier.
<code class="sourceCode r">stream_find(sc, id)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>The associated Spark connection.</td>
    </tr>
<tr>
<td>id</td>
<td>The stream identifier to find.</td>
    </tr>
    </table>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;%
  <a href='spark_write_parquet.html'>spark_write_parquet</a>(path = "parquet-in")

stream &lt;- <a href='stream_read_parquet.html'>stream_read_parquet</a>(sc, "parquet-in") %&gt;%
 <a href='stream_write_parquet.html'>stream_write_parquet</a>("parquet-out")

stream_id &lt;- <a href='stream_id.html'>stream_id</a>(stream)
stream_find(sc, stream_id)
}</div></code>
<h2>Generate Test Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
Generates a local test stream, useful when testing streams locally.
<code class="sourceCode r">stream_generate_test(df = <a href='https://rdrr.io/r/base/rep.html'>rep</a>(1:1000), path = "source",
  distribution = <a href='https://rdrr.io/r/base/Round.html'>floor</a>(10 + 1e+05 * stats::<a href='https://rdrr.io/r/stats/Binomial.html'>dbinom</a>(1:20, 20, 0.5)),
  iterations = 50, interval = 1)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>df</td>
<td>The data frame used as a source of rows to the stream, will
be cast to data frame if needed. 
Defaults to a sequence of one thousand
entries.</td>
    </tr>
<tr>
<td>path</td>
<td>Path to save stream of files to, defaults to <code>"source"</code>.</td>
    </tr>
<tr>
<td>distribution</td>
<td>The distribution of rows to use over each iteration,
defaults to a binomial distribution. 
The stream will cycle through the
distribution if needed.</td>
    </tr>
<tr>
<td>iterations</td>
<td>Number of iterations to execute before stopping, defaults
to fifty.</td>
    </tr>
<tr>
<td>interval</td>
<td>The inverval in seconds use to write the stream, defaults
to one second.</td>
    </tr>
    </table>
<h3>Details</h3>
This function requires the <code>callr</code> package to be installed.
<h2>Spark Stream&#39;s Identifier </h2>
<a href="#arguments">Arguments</a>
Retrieves the identifier of the Spark stream.
<code class="sourceCode r">stream_id(stream)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The spark stream object.</td>
    </tr>
    </table>
<h2>Spark Stream&#39;s Name </h2>
<a href="#arguments">Arguments</a>
Retrieves the name of the Spark stream if available.
<code class="sourceCode r">stream_name(stream)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The spark stream object.</td>
    </tr>
    </table>
<h2>Read CSV Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a CSV stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_csv(sc, path, name = NULL, header = TRUE, columns = NULL,
  delimiter = ",", quote = "\"", escape = "\\",
  charset = "UTF-8", null_value = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>header</td>
<td>Boolean; should the first row of data be used as a header?
Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>delimiter</td>
<td>The character used to delimit each column. 
Defaults to <samp>','</samp>.</td>
    </tr>
<tr>
<td>quote</td>
<td>The character used as a quote. 
Defaults to <samp>'"'</samp>.</td>
    </tr>
<tr>
<td>escape</td>
<td>The character used to escape other characters. 
Defaults to <samp>'\'</samp>.</td>
    </tr>
<tr>
<td>charset</td>
<td>The character set. 
Defaults to <samp>"UTF-8"</samp>.</td>
    </tr>
<tr>
<td>null_value</td>
<td>The character to use for null, or missing, values. 
Defaults to <code>NULL</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("csv-in")
<a href='https://rdrr.io/r/utils/write.table.html'>write.csv</a>(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "csv-in")

stream &lt;- stream_read_csv(sc, csv_path) %&gt;% <a href='stream_write_csv.html'>stream_write_csv</a>("csv-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Read JSON Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a JSON stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_json(sc, path, name = NULL, columns = NULL,
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("json-in")
jsonlite::<a href='https://rdrr.io/pkg/jsonlite/man/read_json.html'>write_json</a>(<a href='https://rdrr.io/r/base/list.html'>list</a>(a = <a href='https://rdrr.io/r/base/c.html'>c</a>(1,2), b = <a href='https://rdrr.io/r/base/c.html'>c</a>(10,20)), "json-in/data.json")

json_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "json-in")

stream &lt;- stream_read_json(sc, json_path) %&gt;% <a href='stream_write_json.html'>stream_write_json</a>("json-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Read Kafka Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a Kafka stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_kafka(sc, name = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
Please note that Kafka requires installing the appropriate
 package by conneting with a config setting where <code>sparklyr.shell.packages</code>
 is set to, for Spark 2.3.2, <code>"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2"</code>.
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

config &lt;- <a href='spark_config.html'>spark_config</a>()

# The following package is dependent to Spark version, for Spark 2.3.2:
config$sparklyr.shell.packages &lt;- "org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2"

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local", config = config)

read_options &lt;- <a href='https://rdrr.io/r/base/list.html'>list</a>(kafka.bootstrap.servers = "localhost:9092", subscribe = "topic1")
write_options &lt;- <a href='https://rdrr.io/r/base/list.html'>list</a>(kafka.bootstrap.servers = "localhost:9092", topic = "topic2")

stream &lt;- stream_read_kafka(sc, options = read_options) %&gt;%
  <a href='stream_write_kafka.html'>stream_write_kafka</a>(options = write_options)

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Read ORC Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads an <a href='https://orc.apache.org/'>ORC</a> stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_orc(sc, path, name = NULL, columns = NULL,
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% <a href='spark_write_orc.html'>spark_write_orc</a>("orc-in")

stream &lt;- stream_read_orc(sc, "orc-in") %&gt;% <a href='stream_write_orc.html'>stream_write_orc</a>("orc-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Read Parquet Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a parquet stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_parquet(sc, path, name = NULL, columns = NULL,
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% <a href='spark_write_parquet.html'>spark_write_parquet</a>("parquet-in")

stream &lt;- stream_read_parquet(sc, "parquet-in") %&gt;% <a href='stream_write_parquet.html'>stream_write_parquet</a>("parquet-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Read Socket Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a Socket stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_scoket(sc, name = NULL, columns = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(),
  ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>columns</td>
<td>A vector of column names or a named vector of column types.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

# Start socket server from terminal, example: nc -lk 9999
stream &lt;- stream_read_scoket(sc, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(host = "localhost", port = 9999))
stream

}</div></code>
<h2>Read Text Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Reads a text stream as a Spark dataframe stream.
<code class="sourceCode r">stream_read_text(sc, path, name = NULL, options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>sc</td>
<td>A <code>spark_connection</code>.</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("text-in")
<a href='https://rdrr.io/r/base/writeLines.html'>writeLines</a>("A text entry", "text-in/text.txt")

text_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "text-in")

stream &lt;- stream_read_text(sc, text_path) %&gt;% <a href='stream_write_text.html'>stream_write_text</a>("text-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Render Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#examples">Examples</a>
Collects streaming statistics to render the stream as an 'htmlwidget'.
<code class="sourceCode r">stream_render(stream = NULL, collect = 10, stats = NULL, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The stream to render</td>
    </tr>
<tr>
<td>collect</td>
<td>The interval in seconds to collect data before rendering the
'htmlwidget'.</td>
    </tr>
<tr>
<td>stats</td>
<td>Optional stream statistics collected using <code><a href='stream_stats.html'>stream_stats()</a></code>,
when specified, <code>stream</code> should be omitted.</td>
    </tr>
<tr>
<td>...</td>
<td>Additional optional arguments.</td>
    </tr>
    </table>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("iris-in")
<a href='https://rdrr.io/r/utils/write.table.html'>write.csv</a>(iris, "iris-in/iris.csv", row.names = FALSE)

stream &lt;- <a href='stream_read_csv.html'>stream_read_csv</a>(sc, "iris-in/") %&gt;%
  <a href='stream_write_csv.html'>stream_write_csv</a>("iris-out/")

stream_render(stream)
<a href='stream_stop.html'>stream_stop</a>(stream)
}</div></code>
<h2>Stream Statistics </h2>
<a href="#arguments">Arguments</a>
    <a href="#value">Value</a>
    <a href="#examples">Examples</a>
Collects streaming statistics, usually, to be used with <code><a href='stream_render.html'>stream_render()</a></code>
to render streaming statistics.
<code class="sourceCode r">stream_stats(stream, stats = <a href='https://rdrr.io/r/base/list.html'>list</a>())</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The stream to collect statistics from.</td>
    </tr>
<tr>
<td>stats</td>
<td>An optional stats object generated using <code>stream_stats()</code>.</td>
    </tr>
    </table>
<h3>Value</h3>
A stats object containing streaming statistics that can be passed
  back to the <code>stats</code> parameter to continue aggregating streaming stats.
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")
<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;%
  <a href='spark_write_parquet.html'>spark_write_parquet</a>(path = "parquet-in")

stream &lt;- <a href='stream_read_parquet.html'>stream_read_parquet</a>(sc, "parquet-in") %&gt;%
 <a href='stream_write_parquet.html'>stream_write_parquet</a>("parquet-out")

stream_stats(stream)
}</div></code>
<h2>Stops a Spark Stream </h2>
<a href="#arguments">Arguments</a>
Stops processing data from a Spark stream.
<code class="sourceCode r">stream_stop(stream)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The spark stream object to be stopped.</td>
    </tr>
    </table>
<h2>Spark Stream Continuous Trigger </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Creates a Spark structured streaming trigger to execute
continuously. 
This mode is the most performant but not all operations
are supported.
<code class="sourceCode r">stream_trigger_continuous(checkpoint = 5000)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>checkpoint</td>
<td>The checkpoint interval specified in milliseconds.</td>
    </tr>
    </table>
<h3>See also</h3><code> <a href='stream_trigger_interval.html'>stream_trigger_interval</a></code>
<h2>Spark Stream Interval Trigger </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
Creates a Spark structured streaming trigger to execute
over the specified interval.
<code class="sourceCode r">stream_trigger_interval(interval = 1000)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>interval</td>
<td>The execution interval specified in milliseconds.</td>
    </tr>
    </table>
<h3>See also</h3><code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>
<h2>View Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#examples">Examples</a>
Opens a Shiny gadget to visualize the given stream.
<code class="sourceCode r">stream_view(stream, ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>stream</td>
<td>The stream to visualize.</td>
    </tr>
<tr>
<td>...</td>
<td>Additional optional arguments.</td>
    </tr>
    </table>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {
<a href='https://rdrr.io/r/base/library.html'>library</a>(sparklyr)
sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("iris-in")
<a href='https://rdrr.io/r/utils/write.table.html'>write.csv</a>(iris, "iris-in/iris.csv", row.names = FALSE)

<a href='stream_read_csv.html'>stream_read_csv</a>(sc, "iris-in/") %&gt;%
  <a href='stream_write_csv.html'>stream_write_csv</a>("iris-out/") %&gt;%
  stream_view() %&gt;%
  <a href='stream_stop.html'>stream_stop</a>()
}</div></code>
<h2>Watermark Stream </h2>
<a href="#arguments">Arguments</a>
Ensures a stream has a watermark defined, which is required for some
operations over streams.
<code class="sourceCode r">stream_watermark(x, column = "timestamp", threshold = "10 minutes")</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercable to a Spark Streaming DataFrame.</td>
    </tr>
<tr>
<td>column</td>
<td>The name of the column that contains the event time of the row,
if the column is missing, a column with the current time will be added.</td>
    </tr>
<tr>
<td>threshold</td>
<td>The minimum delay to wait to data to arrive late, defaults
to ten minutes.</td>
    </tr>
    </table>
<h2>Write Console Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into console logs.
<code class="sourceCode r">stream_write_console(x, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% dplyr::<a href='https://dplyr.tidyverse.org/reference/mutate.html'>transmute</a>(text = <a href='https://rdrr.io/r/base/character.html'>as.character</a>(id)) %&gt;% <a href='spark_write_text.html'>spark_write_text</a>("text-in")

stream &lt;- <a href='stream_read_text.html'>stream_read_text</a>(sc, "text-in") %&gt;% stream_write_console()

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write CSV Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into a tabular (typically, comma-separated) stream.
<code class="sourceCode r">stream_write_csv(x, path, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>(path,
  "checkpoint"), header = TRUE, delimiter = ",", quote = "\"",
  escape = "\\", charset = "UTF-8", null_value = NULL,
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The path to the file. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>header</td>
<td>Should the first row of data be used as a header? Defaults to <code>TRUE</code>.</td>
    </tr>
<tr>
<td>delimiter</td>
<td>The character used to delimit each column, defaults to <code>,</code>.</td>
    </tr>
<tr>
<td>quote</td>
<td>The character used as a quote. 
Defaults to <samp>'"'</samp>.</td>
    </tr>
<tr>
<td>escape</td>
<td>The character used to escape other characters, defaults to <code>\</code>.</td>
    </tr>
<tr>
<td>charset</td>
<td>The character set, defaults to <code>"UTF-8"</code>.</td>
    </tr>
<tr>
<td>null_value</td>
<td>The character to use for default values, defaults to <code>NULL</code>.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("csv-in")
<a href='https://rdrr.io/r/utils/write.table.html'>write.csv</a>(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "csv-in")

stream &lt;- <a href='stream_read_csv.html'>stream_read_csv</a>(sc, csv_path) %&gt;% stream_write_csv("csv-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write JSON Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into a JSON stream.
<code class="sourceCode r">stream_write_json(x, path, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>(path,
  "checkpoints", <a href='random_string.html'>random_string</a>("")), options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The destination path. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("json-in")
jsonlite::<a href='https://rdrr.io/pkg/jsonlite/man/read_json.html'>write_json</a>(<a href='https://rdrr.io/r/base/list.html'>list</a>(a = <a href='https://rdrr.io/r/base/c.html'>c</a>(1,2), b = <a href='https://rdrr.io/r/base/c.html'>c</a>(10,20)), "json-in/data.json")

json_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "json-in")

stream &lt;- <a href='stream_read_json.html'>stream_read_json</a>(sc, json_path) %&gt;% stream_write_json("json-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write Kafka Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#details">Details</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into an kafka stream.
<code class="sourceCode r">stream_write_kafka(x, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(),
  checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("checkpoints", <a href='random_string.html'>random_string</a>("")),
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>Details</h3>
Please note that Kafka requires installing the appropriate
 package by conneting with a config setting where <code>sparklyr.shell.packages</code>
 is set to, for Spark 2.3.2, <code>"org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2"</code>.
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

config &lt;- <a href='spark_config.html'>spark_config</a>()

# The following package is dependent to Spark version, for Spark 2.3.2:
config$sparklyr.shell.packages &lt;- "org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2"

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local", config = config)

read_options &lt;- <a href='https://rdrr.io/r/base/list.html'>list</a>(kafka.bootstrap.servers = "localhost:9092", subscribe = "topic1")
write_options &lt;- <a href='https://rdrr.io/r/base/list.html'>list</a>(kafka.bootstrap.servers = "localhost:9092", topic = "topic2")

stream &lt;- <a href='stream_read_kafka.html'>stream_read_kafka</a>(sc, options = read_options) %&gt;%
  stream_write_kafka(options = write_options)

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write Memory Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into a memory stream.
<code class="sourceCode r">stream_write_memory(x, name = <a href='random_string.html'>random_string</a>("sparklyr_tmp_"),
  mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(),
  checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("checkpoints", name, <a href='random_string.html'>random_string</a>("")),
  options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>name</td>
<td>The name to assign to the newly generated stream.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("csv-in")
<a href='https://rdrr.io/r/utils/write.table.html'>write.csv</a>(iris, "csv-in/data.csv", row.names = FALSE)

csv_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "csv-in")

stream &lt;- <a href='stream_read_csv.html'>stream_read_csv</a>(sc, csv_path) %&gt;% stream_write_memory("csv-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write a ORC Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into an <a href='https://orc.apache.org/'>ORC</a> stream.
<code class="sourceCode r">stream_write_orc(x, path, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>(path,
  "checkpoints", <a href='random_string.html'>random_string</a>("")), options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The destination path. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% <a href='spark_write_orc.html'>spark_write_orc</a>("orc-in")

stream &lt;- <a href='stream_read_orc.html'>stream_read_orc</a>(sc, "orc-in") %&gt;% stream_write_orc("orc-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write Parquet Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into a parquet stream.
<code class="sourceCode r">stream_write_parquet(x, path, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>(path,
  "checkpoints", <a href='random_string.html'>random_string</a>("")), options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The destination path. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_text.html'>stream_write_text</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='sdf_len.html'>sdf_len</a>(sc, 10) %&gt;% <a href='spark_write_parquet.html'>spark_write_parquet</a>("parquet-in")

stream &lt;- <a href='stream_read_parquet.html'>stream_read_parquet</a>(sc, "parquet-in") %&gt;% stream_write_parquet("parquet-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Write Text Stream </h2>
<a href="#arguments">Arguments</a>
    <a href="#see-also">See also</a>
    <a href="#examples">Examples</a>
Writes a Spark dataframe stream into a text stream.
<code class="sourceCode r">stream_write_text(x, path, mode = <a href='https://rdrr.io/r/base/c.html'>c</a>("append", "complete", "update"),
  trigger = <a href='stream_trigger_interval.html'>stream_trigger_interval</a>(), checkpoint = <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>(path,
  "checkpoints", <a href='random_string.html'>random_string</a>("")), options = <a href='https://rdrr.io/r/base/list.html'>list</a>(), ...)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>A Spark DataFrame or dplyr operation</td>
    </tr>
<tr>
<td>path</td>
<td>The destination path. 
Needs to be accessible from the cluster.
Supports the <samp>"hdfs://"</samp>, <samp>"s3a://"</samp> and <samp>"file://"</samp> protocols.</td>
    </tr>
<tr>
<td>mode</td>
<td>Specifies how data is written to a streaming sink. 
Valid values are<code> "append"</code>, <code>"complete"</code> or <code>"update"</code>.</td>
    </tr>
<tr>
<td>trigger</td>
<td>The trigger for the stream query, defaults to micro-batches runnnig
every 5 seconds. 
See <code><a href='stream_trigger_interval.html'>stream_trigger_interval</a></code> and<code> <a href='stream_trigger_continuous.html'>stream_trigger_continuous</a></code>.</td>
    </tr>
<tr>
<td>checkpoint</td>
<td>The location where the system will write all the checkpoint
information to guarantee end-to-end fault-tolerance.</td>
    </tr>
<tr>
<td>options</td>
<td>A list of strings with additional options.</td>
    </tr>
<tr>
<td>...</td>
<td>Optional arguments; currently unused.</td>
    </tr>
    </table>
<h3>See also</h3>
Other Spark stream serialization: <code><a href='stream_read_csv.html'>stream_read_csv</a></code>,
  <code><a href='stream_read_json.html'>stream_read_json</a></code>,
  <code><a href='stream_read_kafka.html'>stream_read_kafka</a></code>,
  <code><a href='stream_read_orc.html'>stream_read_orc</a></code>,
  <code><a href='stream_read_parquet.html'>stream_read_parquet</a></code>,
  <code><a href='stream_read_scoket.html'>stream_read_scoket</a></code>,
  <code><a href='stream_read_text.html'>stream_read_text</a></code>,
  <code><a href='stream_write_console.html'>stream_write_console</a></code>,
  <code><a href='stream_write_csv.html'>stream_write_csv</a></code>,
  <code><a href='stream_write_json.html'>stream_write_json</a></code>,
  <code><a href='stream_write_kafka.html'>stream_write_kafka</a></code>,
  <code><a href='stream_write_memory.html'>stream_write_memory</a></code>,
  <code><a href='stream_write_orc.html'>stream_write_orc</a></code>,
  <code><a href='stream_write_parquet.html'>stream_write_parquet</a></code>
<h3>Examples</h3>
    <code class="sourceCode r">if (FALSE) {

sc &lt;- <a href='spark-connections.html'>spark_connect</a>(master = "local")

<a href='https://rdrr.io/r/base/files2.html'>dir.create</a>("text-in")
<a href='https://rdrr.io/r/base/writeLines.html'>writeLines</a>("A text entry", "text-in/text.txt")

text_path &lt;- <a href='https://rdrr.io/r/base/file.path.html'>file.path</a>("file://", <a href='https://rdrr.io/r/base/getwd.html'>getwd</a>(), "text-in")

stream &lt;- <a href='stream_read_text.html'>stream_read_text</a>(sc, text_path) %&gt;% stream_write_text("text-out")

<a href='stream_stop.html'>stream_stop</a>(stream)

}</div></code>
<h2>Reactive spark reader </h2>
<a href="#arguments">Arguments</a>
Given a spark object, returns a reactive data source for the contents
of the spark object. 
This function is most useful to read Spark streams.
<code class="sourceCode r">reactiveSpark(x, intervalMillis = 1000, session = NULL)</code>
<h3>Arguments</h3>
    <table class="ref-arguments">
<colgroup>
<col class="name" />
<col class="desc" />
    </colgroup>
<tr>
<td>x</td>
<td>An object coercable to a Spark DataFrame.</td>
    </tr>
<tr>
<td>intervalMillis</td>
<td>Approximate number of milliseconds to wait to retrieve
updated data frame. 
This can be a numeric value, or a function that returns
a numeric value.</td>
    </tr>
<tr>
<td>session</td>
<td>The user session to associate this file reader with, or NULL if
none. 
If non-null, the reader will automatically stop when the session ends.</td>
    </tr>
    </table>
<script type='text/javascript' src='readbook.js'></script>
