<base target="_blank"><html><head><title>statologyContents 3</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 3"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 3</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">Confidence Interval for a Standard Deviation</span></h2>
A <b>confidence interval for a standard deviation </b>is a range of values that is likely to contain a population standard deviation with a certain level of confidence.
This tutorial explains the following:
The motivation for creating this confidence interval.
The formula to create this confidence interval.
An example of how to calculate this confidence interval.
How to interpret this confidence interval.
<h3>Confidence Interval for a Standard Deviation: Motivation</h3>
The reason to create a confidence interval for a standard deviation is because we want to capture our uncertainty when estimating a population standard deviation.
For example, suppose we want to estimate the standard deviation of weight of a certain species of turtle in Florida. Since there are thousands of turtles in Florida, it would be extremely time-consuming and costly to go around and weigh each individual turtle.
Instead, we might take a  simple random sample  of 50 turtles and use the standard deviation of weight of the turtles in this sample to estimate the true population standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CImean1.png">
The problem is that the standard deviation in the sample is not guaranteed to exactly match the standard deviation in the whole population. So, to capture this uncertainty we can create a confidence interval that contains a range of values that are likely to contain the true standard deviation in the population.
<h3>Confidence Interval for a Standard Deviation: Formula</h3>
We use the following formula to calculate a confidence interval for a mean:
<b>Confidence Interval = [√(n-1)s<sup>2</sup>/X<sup>2</sup><sub>α/2</sub>, √(n-1)s<sup>2</sup>/X<sup>2</sup><sub>1-α/2</sub>]</b>
where:
<b>n:</b> sample size
<b>s:</b> sample standard deviation
<b>X<sup>2</sup>:</b> Chi-square critical value with n-1 degrees of freedom.
<h3>Confidence Interval for a Standard Deviation: Example</h3>
Suppose we collect a random sample of turtles with the following information:
Sample size <b>n = 27</b>
Sample standard deviation <b>s = 6.43</b>
Here is how to find various confidence intervals for the true population standard deviation:
<b>90% Confidence Interval: </b>[<b>√</b>(27-1)*6.43<sup>2</sup>/38.885,   <b>√</b>(27-1)*6.43<sup>2</sup>/15.379) = <b>[5.258, 8.361]</b>
<b>95% Confidence Interval: </b>[<b>√</b>(27-1)*6.43<sup>2</sup>/41.923,   <b>√</b>(27-1)*6.43<sup>2</sup>/13.844) = <b>[5.064, 8.812]</b>
<b>99% Confidence Interval: </b>[<b>√</b>(27-1)*6.43<sup>2</sup>/48.289,   <b>√</b>(27-1)*6.43<sup>2</sup>/11.160) = <b>[4.718, 9.814]</b>
<em><b>Note: </b>You can also find these confidence intervals by using the  Confidence Interval for a Standard Deviation Calculator .</em>
<h3>Confidence Interval for a Standard Deviation: Interpretation</h3>
The way we would interpret a confidence interval is as follows:
There is a 95% chance that the confidence interval of [5.064, 8.812] contains the true population standard deviation.
Another way of saying the same thing is that there is only a 5% chance that the true population standard deviation lies outside of the 95% confidence interval. That is, there’s only a 5% chance that the true population standard deviation is greater than 8.812 or less than 5.064.
<h2><span class="orange">Confidence Interval vs. Prediction Interval: What’s the Difference?</span></h2>
Two types of intervals that are often used in regression analysis are <b>confidence intervals</b> and <b>prediction intervals</b>.
Here’s the difference between the two intervals:
<b>Confidence intervals</b> represent a range of values that are likely to contain the true mean value of some response variable based on specific values of one or more predictor variables.
<b>Prediction intervals</b> represent a range of values that are likely to contain the true value of some response variable for a <em>single new observation</em> based on specific values of one or more predictor variables.
For example, suppose we fit a  simple linear regression model  that uses the number of bedrooms to predict the selling price of a house:
Price = β<sub>0</sub> + β<sub>1</sub>(number of bedrooms)
If we’d like to estimate the mean selling price of houses with three bedrooms, we would use a confidence interval.
However, if we’d like to estimate the selling price of a specific new home that just came on the market with three bedrooms, we would use a prediction interval.
<b>Note</b>: Since prediction intervals attempt to create an interval for a specific new observation, there’s more uncertainty in our estimate and thus prediction intervals are always wider than confidence intervals.
<h3>Confidence Interval vs. Prediction Interval: Difference in Formulas</h3>
We use the following formula to calculate a <b>confidence interval</b>:
<U+0177><sub>0</sub>  +/-  t<sub>α/2,n-2</sub> * S<sub>yx</sub>√((x<sub>0</sub> – x<U+0304>)<sup>2</sup>/SS<sub>x</sub> + 1/n)
We use the following formula to calculate a <b>prediction interval</b>:
<U+0177><sub>0</sub>  +/-  t<sub>α/2,n-2</sub> * S<sub>yx</sub>√((x<sub>0</sub> – x<U+0304>)<sup>2</sup>/SS<sub>x</sub> + 1/n + 1)
where:
<b><U+0177><sub>0</sub></b>: Estimated mean value of response variable
<b>t<sub>α/2,n-2</sub></b>: t-critical value with n-2 degrees of freedom
<b>S<sub>yx</sub></b>: Standard error of response variable
<b>x<sub>0</sub></b>: specific value of predictor variable 
<b>x<U+0304></b>: mean value of predictor variable
<b>SS<sub>x</sub></b>: Sum of squares for predictor variable
<b>n</b>: Total sample size
Notice that the formula for a prediction interval contains an extra one in the square root portion, which means the standard error will always be larger than a confidence interval.
Thus, <b>a prediction interval will always be wider than a confidence interval</b>.
<h3>Example: Interpreting Confidence Intervals vs. Prediction Intervals</h3>
Suppose we have the following dataset that shows the number of bedrooms and the selling price for 20 houses in a particular neighborhood:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/conf_vs_pred1.png">
Now suppose we fit a simple linear regression model to this dataset in R:
<b>#define data
df &lt;- data.frame(beds=c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3,        3, 3, 3, 3, 4, 4, 4, 5, 5, 6), price=c(120, 133, 139, 185, 148, 160, 192, 205, 244, 213,         236, 280, 275, 273, 312, 311, 304, 415, 396, 488))
#fit simple linear regression model
model &lt;- lm(price~beds, data=df)
#view model fit
summary(model)
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   39.450     13.248   2.978  0.00807 ** 
beds          70.667      4.031  17.529 9.26e-13 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 24.19 on 18 degrees of freedom
Multiple R-squared:  0.9447,Adjusted R-squared:  0.9416 
F-statistic: 307.3 on 1 and 18 DF,  p-value: 9.257e-13</b>
The fitted regression model turns out to be:
Selling price (thousands) = 39.450 + 70.667(number of bedrooms)
We can use the following code to calculate a confidence interval for the mean selling price of houses that have three bedrooms:
<b>#define new house
new &lt;- data.frame(beds=c(3))
#confidence interval for mean selling price of house with 3 bedrooms
predict(model, newdata = new, interval = "confidence")
     fit     lwr     upr
1 251.45 240.087 262.813</b>
The 95% confidence interval for the mean selling price of a house with three bedrooms is [$240k, $262k].
We can then use the following code to calculate a prediction interval for the selling price of a new house that just came on the market that has three bedrooms:
<b>#define new house
new &lt;- data.frame(beds=c(3))
#confidence interval for mean selling price of house with 3 bedrooms
predict(model, newdata = new, interval = "prediction")
     fit      lwr      upr
1 251.45 199.3783 303.5217</b>
The 95% prediction interval for the selling price of a new house with three bedrooms is [$199k, $303k].
Notice that the prediction interval is much wider than the confidence interval because there is more uncertainty around the selling price of a single new house as opposed to the mean selling price of all houses with three bedrooms.
 An Introduction to Confidence Intervals 
 4 Examples of Confidence Intervals in Real Life 
 How to Calculate Confidence Intervals in Excel 
The following tutorials offer additional information about prediction intervals:
 Prediction Interval Calculator 
 How to Calculate a Prediction Interval in R 
 How to Calculate a Prediction Interval in Excel 
<h2><span class="orange">How to Calculate Confidence Intervals in Excel</span></h2>
A  confidence interval  is a range of values that is likely to contain a population parameter with a certain level of confidence. It is calculated using the following general formula:
<b>Confidence Interval</b> = (point estimate)  +/-  (critical value)*(standard error)
This formula creates an interval with a lower bound and an upper bound, which likely contains a population parameter with a certain level of confidence:
<b>Confidence Interval </b> = [lower bound, upper bound]
This tutorial explains how to calculate the following confidence intervals in Excel:
<b>1.</b> Confidence Interval for a Mean
<b>2.</b> Confidence Interval for a Difference in Means
<b>3.</b> Confidence Interval for a Proportion
<b>4.</b> Confidence Interval for a Difference in Proportions
Let’s jump in!
<h3>Example 1: Confidence Interval for a Mean</h3>
We use the following formula to calculate a  confidence interval for a mean :
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the chosen z-value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
<b>Example: </b>Suppose we collect a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
The following screenshot shows how to calculate a 95% confidence interval for the true population mean weight of turtles:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/confIntExcel1.png">
The 95% confidence interval for the true population mean weight of turtles is <b>[292.75, 307.25]</b>.
<h3>Example 2: Confidence Interval for a Difference in Means</h3>
We use the following formula to calculate a confidence interval for a  difference in population means :
<b>Confidence interval</b> = (x<sub>1</sub>–x<sub>2</sub>) +/- t*√((s<sub>p</sub><sup>2</sup>/n<sub>1</sub>) + (s<sub>p</sub><sup>2</sup>/n<sub>2</sub>))
where:
x<sub>1</sub>, x<sub>2</sub>: sample 1 mean, sample 2 mean
t: the t-critical value based on the confidence level and (n<sub>1</sub>+n<sub>2</sub>-2) degrees of freedom
s<sub>p</sub><sup>2</sup>: pooled variance, calculated as ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup>) / (n<sub>1</sub>+n<sub>2</sub>-2)
t: the t-critical value
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
<b>Example: </b>Suppose we want to estimate the difference in mean weight between two different species of turtles, so we go out and gather a random sample of 15 turtles from each population. Here is the summary data for each sample:
<b>Sample 1:</b>
x<sub>1</sub> = 310
s<sub>1</sub> = 18.5
n<sub>1</sub> = 15
<b>Sample 2:</b>
x<sub>2</sub> = 300
s<sub>2</sub> = 16.4
n<sub>2</sub> = 15
The following screenshot shows how to calculate a 95% confidence interval for the true difference in population means:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/confIntExcel2.png">
The 95% confidence interval for the true difference in population means is <b>[-3.08, 23.08]</b>.
<h3>Example 3: Confidence Interval for a Proportion</h3>
We use the following formula to calculate a  confidence interval for a proportion :
<b>Confidence Interval = p  +/-  z*√p(1-p) / n</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
<b>Example: </b>Suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p = 0.56</b>
The following screenshot shows how to calculate a 95% confidence interval for the true proportion of residents in the entire county who are in favor of the law:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/confIntExcel3.png">
The 95% confidence interval for the true proportion of residents in the entire county who are in favor of the law is <b>[.463, .657]</b>.
<h3>Example 4: Confidence Interval for a Difference in Proportions</h3>
We use the following formula to calculate a  confidence interval for a difference in proportions :
<b>Confidence interval = (p<sub>1</sub>–p<sub>2</sub>)  +/-  z*√(p<sub>1</sub>(1-p<sub>1</sub>)/n<sub>1 </sub>+ p<sub>2</sub>(1-p<sub>2</sub>)/n<sub>2</sub>)</b>
where:
p<sub>1</sub>, p<sub>2</sub>: sample 1 proportion, sample 2 proportion
z: the z-critical value based on the confidence level
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
<b>Example: </b>Suppose we want to estimate the difference in the proportion of residents who support a certain law in county A compared to the proportion who support the law in county B. Here is the summary data for each sample:
<b>Sample 1:</b>
n<sub>1</sub> = 100
p<sub>1</sub> = 0.62 (i.e. 62 out of 100 residents support the law)
<b>Sample 2:</b>
n<sub>2</sub> = 100
p<sub>2</sub> = 0.46 (i.e. 46 our of 100 residents support the law)
The following screenshot shows how to calculate a 95% confidence interval for the true difference in proportion of residents who support the law between the counties:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/confIntExcel4.png">
The 95% confidence interval for the true difference in proportion of residents who support the law between the counties is <b>[.024, .296]</b>.
<em>You can find more Excel tutorials  here .</em>
<h2><span class="orange">How to Calculate Confidence Intervals in Python</span></h2>
A <b>confidence interval for a mean </b>is a range of values that is likely to contain a population mean with a certain level of confidence.
It is calculated as:
<b>Confidence Interval = x  +/-  t*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t: </b>t-value that corresponds to the confidence level
<b>s: </b>sample standard deviation
<b>n: </b>sample size
This tutorial explains how to calculate confidence intervals in Python.
<h3>Confidence Intervals Using the t Distribution</h3>
If we’re working with a small sample (n &lt;30), we can use the  t.interval() function  from the scipy.stats library to calculate a confidence interval for a population mean.
The following example shows how to calculate a confidence interval for the true population mean height (in inches) of a certain species of plant, using a sample of 15 plants:
<b>import numpy as np
import scipy.stats as st
#define sample data
data = [12, 12, 13, 13, 15, 16, 17, 22, 23, 25, 26, 27, 28, 28, 29]
#create 95% confidence interval for population mean weight
st.t.interval(alpha=0.95, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) 
(16.758, 24.042)</b>
The 95% confidence interval for the true population mean height is <b>(16.758, 24.042)</b>.
You’ll notice that the larger the confidence level, the wider the confidence interval. For example, here’s how to calculate a 99% C.I. for the exact same data:
<b>#create 99% confidence interval for same sample
st.t.interval(alpha=0.99, df=len(data)-1, loc=np.mean(data), scale=st.sem(data)) 
(15.348, 25.455)</b>
The 99% confidence interval for the true population mean height is <b>(15.348, 25.455)</b>. Notice that this interval is wider than the previous 95% confidence interval.
<h3>Confidence Intervals Using the Normal Distribution</h3>
If we’re working with larger samples (n≥30), we can assume that the sampling distribution of the sample mean is normally distributed (thanks to the  Central Limit Theorem ) and can instead use the  norm.interval() function  from the scipy.stats library.
The following example shows how to calculate a confidence interval for the true population mean height (in inches) of a certain species of plant, using a sample of 50 plants:
<b>import numpy as np
import scipy.stats as st
#define sample data
np.random.seed(0)
data = np.random.randint(10, 30, 50)
#create 95% confidence interval for population mean weight
st.norm.interval(alpha=0.95, loc=np.mean(data), scale=st.sem(data))
(17.40, 21.08)</b>
The 95% confidence interval for the true population mean height is <b>(17.40, 21.08)</b>.
And similar to the t distribution, larger confidence levels lead to wider confidence intervals. For example, here’s how to calculate a 99% C.I. for the exact same data:
<b>#create 99% confidence interval for same sample
st.norm.interval(alpha=0.99, loc=np.mean(data), scale=st.sem(data))
(16.82, 21.66)</b>
The 95% confidence interval for the true population mean height is <b>(17.82, 21.66)</b>.
<h3>How to Interpret Confidence Intervals</h3>
Suppose our 95% confidence interval for the true population mean height of a species of plant is:
<b>95% confidence interval = (16.758, 24.042)</b>
The way to interpret this confidence interval is as follows:
There is a 95% chance that the confidence interval of [16.758, 24.042] contains the true population mean height of plants.
Another way of saying the same thing is that there is only a 5% chance that the true population mean lies outside of the 95% confidence interval. That is, there’s only a 5% chance that the true population mean height of plants is less than 16.758 inches or greater than 24.042 inches.
<h2><span class="orange">How to Calculate Confidence Intervals on a TI-84 Calculator</span></h2>
A <b>confidence interval (C.I.) </b>is a range of values that is likely to include a  population parameter  with a certain degree of confidence.
This tutorial explains how to calculate the following confidence intervals on a TI-84 calculator:
<b>Confidence interval for a population mean; σ known</b>
<b>Confidence interval for a population mean; σ unknown</b>
<b>Confidence interval for a population proportion</b>
<h3>Example 1: C.I. for a population mean; σ known</h3>
Find a 95% confidence interval for a population mean, given the following information:
sample mean x = 14
sample size n = 35
population standard deviation = 4
<b>Step 1: Choose Z Interval.</b>
Press Stat and then scroll over to <b>TESTS</b>. Highlight <b>7:ZInterval </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI1.png">
<b>Step 2: Fill in the necessary information.</b>
The calculator will ask for the following information:
<b>Inpt: </b>Choose whether you are working with raw data (Data) or summary statistics (Stats). In this case, we will highlight Stats and press ENTER.
<b>σ:</b> The population standard deviation. We will type 4 and press  ENTER.
<b>x:</b> The sample mean. We will type 14 and press  ENTER.
<b>n</b>: The sample size. We will type 35 and press ENTER.
<b>C-level</b>:The confidence level We will type 0.95 and press ENTER.
Lastly, highlight Calculate and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI2.png">
<b>Step 3: Interpret the results.</b>
Once you press ENTER, the 95% confidence interval for the population mean will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI3.png">
The 95% confidence interval for the population mean is <b>(12.675, 15.325)</b>.
<h3>Example 2: C.I. for a population mean; σ unknown</h3>
Find a 95% confidence interval for a population mean, given the following information:
sample mean x = 12
sample size n = 19
sample standard deviation = 6.3
<b>Step 1: Choose T Interval.</b>
Press Stat and then scroll over to <b>TESTS</b>. Highlight<b> 8:TInterval </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI4.png">
<b>Step 2: Fill in the necessary information.</b>
The calculator will ask for the following information:
<b>Inpt: </b>Choose whether you are working with raw data (Data) or summary statistics (Stats). In this case, we will highlight Stats and press ENTER.
<b>x:</b> The sample mean. We will type 12 and press  ENTER.
<b>Sx:</b> The sample standard deviation. We will type 6.3 and press  ENTER.
<b>n</b>: The sample size. We will type 19 and press ENTER.
<b>C-level</b>:The confidence level We will type 0.95 and press ENTER.
Lastly, highlight Calculate and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI5.png">
<b>Step 3: Interpret the results.</b>
Once you press ENTER, the 95% confidence interval for the population mean will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI6.png">
The 95% confidence interval for the population mean is <b>(8.9635, 15.037)</b>.
<h3>Example 3: C.I. for a population proportion</h3>
Find a 95% confidence interval for a population proportion, given the following information:
number of “successes” (x) = 12
number of trials (n) = 19
<b>Step 1: Choose 1 Proportion Z Interval.</b>
Press Stat and then scroll over to <b>TESTS</b>. Highlight<b> 1-PropZInt </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI7.png">
<b>Step 2: Fill in the necessary information.</b>
The calculator will ask for the following information:
<b>x: </b>The number of successes. We will type 12 and press  ENTER.
<b>n</b>: The number of trials. We will type 19 and press ENTER.
<b>C-level</b>:The confidence level We will type 0.95 and press ENTER.
Lastly, highlight Calculate and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI8.png">
<b>Step 3: Interpret the results.</b>
Once you press ENTER, the 95% confidence interval for the population proportion will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CI9.png">
The 95% confidence interval for the population proportion is <b>(0.41468, 0.84848)</b>.
<h2><span class="orange">What are Confidence Intervals?</span></h2>
Often in statistics we’re interested in measuring  population parameters  – numbers that describe some characteristic of an entire population.
Two of the most common population parameters are:
<b>1. Population mean:</b> the mean value of some variable in a population (e.g. the mean height of males in the U.S.)
<b>2. Population proportion:</b> the proportion of some variable in a population (e.g. the proportion of residents in a county who support a certain law)
Although we’re interested in measuring these parameters, it’s usually too costly and time-consuming to actually go around and collect data on every individual in a population in order to calculate the population parameter.
Instead, we typically take a random sample from the overall population and use data from the sample to estimate the population parameter.
For example, suppose we want to estimate the mean weight of a certain species of turtle in Florida. Since there are thousands of turtles in Florida, it would be extremely time-consuming and costly to go around and weigh each individual turtle.
Instead, we might take a  simple random sample  of 50 turtles and use the mean weight of the turtles in this sample to estimate the true population mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CImean1.png"(max-width: 405px) 100vw, 405px">
The problem is that the mean weight of turtles in the sample is not guaranteed to exactly match the mean weight of turtles in the whole population. For example, we might just happen to pick a sample full of low-weight turtles or perhaps a sample full of heavy turtles.
In order to capture this uncertainty we can create a confidence interval. A <b>confidence interval </b>is a range of values that is likely to contain a population parameter with a certain level of confidence. It is calculated using the following general formula:
<b>Confidence Interval</b> = (point estimate)  +/-  (critical value)*(standard error)
This formula creates an interval with a lower bound and an upper bound, which likely contains a population parameter with a certain level of confidence.
<b>Confidence Interval </b> = [lower bound, upper bound]
For example, the formula to calculate a confidence interval for a population mean is as follows:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the chosen z-value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
For example, suppose we collect a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
Here is how to find calculate the 90% confidence interval for the true population mean weight:
<b>90% Confidence Interval: </b>300 +/-  1.645*(18.5/√25) = <b>[293.91, 306.09]</b>
We interpret this confidence interval as follows:
There is a 90% chance that the confidence interval of [293.91, 306.09] contains the true population mean weight of turtles.
Another way of saying the same thing is that there is only a 10% chance that the true population mean lies outside of the 90% confidence interval. That is, there’s only a 10% chance that the true population mean weight of turtles is greater than 306.09 pounds or less than 293.91 pounds.
It’s worth nothing that there are two numbers that can affect the size of a confidence interval:
<b>1. The sample size: </b>The larger the sample size, the more narrow the confidence interval. 
<b>2. The confidence level: </b>The larger the confidence level, the wider the confidence interval.
<h2>Types of Confidence Intervals</h2>
There are many types of confidence intervals. Here are the most commonly used ones:
<h3>Confidence Interval for a Mean</h3>
A <b>confidence interval for a mean</b> is a range of values that is likely to contain a population mean with a certain level of confidence. The formula to calculate this interval is:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the chosen z-value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
<b>Resources:
</b> How to Calculate a Confidence Interval for a Mean 
 Confidence Interval for a Mean Calculator 
<h3>Confidence Interval for the Difference Between Means</h3>
A <b>confidence interval (C.I.) for a difference between means </b>is a range of values that is likely to contain the true difference between two population means with a certain level of confidence. The formula to calculate this interval is:
<b>Confidence interval</b> = (x<sub>1</sub>–x<sub>2</sub>) +/- t*√((s<sub>p</sub><sup>2</sup>/n<sub>1</sub>) + (s<sub>p</sub><sup>2</sup>/n<sub>2</sub>))
where:
x<sub>1</sub>, x<sub>2</sub>: sample 1 mean, sample 2 mean
t: the t-critical value based on the confidence level and (n<sub>1</sub>+n<sub>2</sub>-2) degrees of freedom
s<sub>p</sub><sup>2</sup>: pooled variance
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
where:
The pooled variance is calculated as: <b>s<sub>p</sub><sup>2</sup></b> = ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup>) / (n<sub>1</sub>+n<sub>2</sub>-2)
The t-critical value <b>t </b>can be found using the  Inverse t Distribution calculator 
<b>Resources:
</b> How to Calculate a Confidence Interval for the Difference Between Means 
 Confidence Interval for the Difference Between Means Calculator 
<h3>Confidence Interval for a Proportion</h3>
A <b>confidence interval for a proportion</b> is a range of values that is likely to contain a population proportion with a certain level of confidence. The formula to calculate this interval is:
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
<b>Resources:
</b> How to Calculate a Confidence Interval for a Proportion 
 Confidence Interval for a Proportion Calculator 
<h3>Confidence Interval for the Difference in Proportions</h3>
A <b>confidence interval for the difference in proportions</b> is a range of values that is likely to contain the true difference between two population proportions with a certain level of confidence.. The formula to calculate this interval is:
<b>Confidence interval = (p<sub>1</sub>–p<sub>2</sub>)  +/-  z*√(p<sub>1</sub>(1-p<sub>1</sub>)/n<sub>1 </sub>+ p<sub>2</sub>(1-p<sub>2</sub>)/n<sub>2</sub>)</b>
where:
p<sub>1</sub>, p<sub>2</sub>: sample 1 proportion, sample 2 proportion
z: the z-critical value based on the confidence level
n<sub>1</sub>, n<sub>2</sub>: sample 1 size, sample 2 size
<b>Resources:
</b> How to Calculate a Confidence Interval for the Difference in Proportions 
 Confidence Interval for the Difference in Proportions Calculator 
<h2><span class="orange">Confidence Level vs. Confidence Interval: What’s the Difference?</span></h2>
Often in statistics we’re interested in measuring  population parameters  – numbers that describe some characteristic of an entire  population .
For example, we might be interested in measuring the mean height of males in a certain country.
Since it’s too costly and time-consuming to collect data on the height of every male in the country, we would instead collect data on a  simple random sample  of males. We would then use the mean height of males in this sample to estimate the mean height of all males in the country.
Unfortunately, the mean height of males in the sample is not guaranteed to exactly match the mean height of males in the whole population. For example, we might just happen to pick a sample full of shorter men or perhaps a sample full of taller men.
In order to capture our uncertainty around our estimate of the true population mean, we can create a confidence interval.
<b>Confidence Interval:</b> A range of values that is likely to contain a population parameter with a certain level of confidence.
A confidence interval is calculated using the following general formula:
<b>Confidence Interval</b> = (point estimate)  +/-  (critical value)*(standard error)
For example, the formula to calculate a confidence interval for a  population mean  is as follows:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the z critical value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The z critical value that you will use in the formula is dependent on the <b>confidence level</b> that you choose.
<b>Confidence Level:</b> The percentage of all possible samples that are expected to include the true population parameter.
The most common choices for confidence levels include 90%, 95%, and 99%.
The following table shows the z critical value that corresponds to these popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z critical value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
For example, suppose we measure the heights of 25 men and find the following:
Sample size <b>n = 25</b>
Sample mean height <b>x = 70 inches</b>
Sample standard deviation <b>s = 1.2 inches</b>
Here is how to find calculate a confidence interval for the true population mean height using a <b>90% confidence level</b>:
90% Confidence Interval: 70 +/-  1.645*(1.2/√25) = <b>[69.6052, 70.3948]</b>
This means that if we used the same sampling method to select different samples and calculated a confidence interval for each sample, we would expected the true population mean height to fall within the interval 90% of the time.
Now suppose we instead calculate a confidence interval using a <b>95% confidence level:</b>
95% Confidence Interval: 70 +/-  1.96*(1.2/√25) = <b>[69.5296, 70.4704]</b>
Notice that this confidence interval is wider than the previous one. This is because the higher the confidence level, the wider the confidence interval.
<b>The higher the confidence level, the wider the confidence interval.</b>
This should make sense intuitively: A wider confidence level has a higher probability of containing a true population parameter.
<h3>Summary</h3>
In summary:
A <b>confidence interval</b> is a range of values that is likely to contain a population parameter with a certain level of confidence. It uses the following basic formula:
 Confidence Interval = (point estimate)  +/-  (critical value)*(standard error)
The <b>confidence level</b> determines the critical value to use in that formula. The higher the confidence level, the larger the critical value and thus the wider the confidence interval. 
<h2><span class="orange">Confidentiality vs Anonymity: What’s the Difference?</span></h2>
When researchers use surveys to collect data from individuals, they often say that the survey will be conducted <b>confidentially </b>or <b>anonymously</b>. These two terms are often confused by individuals, but the distinction between them is important.
<h2>Collecting Data Confidentially</h2>
When data is collected <b>confidentially</b>, researchers are able to identify individual subjects and their specific responses. Typically researchers will assign a number or some code to each individual so that they’re able to be identified.
Once survey data is collected, there are several ways to ensure that it is protected and remains confidential including:
Using physical safeguards to protect the data such as locked cabinets, secluded interview rooms, private offices, password-protected data centers, etc.
Allowing as few individuals as possible to be able to access the data to prevent the possibility that anyone leaks the information on accident.
Using computer passwords, anti-virus software, firewalls, and encryption to ensure that any digitally-stored data cannot be accessed by anyone without permission.
When the findings of a survey are reported, the total data should be aggregated together so that the responses of any particular individual cannot be known. For example, a study may say that “40% of individuals said they felt confident in their negotiation skills” rather than saying something like “individuals with the last names of Smith, Anderson, Miller, and Hovak said they felt confident in their negotiation skills.”
All statistics and figures shared in the findings of a study should be stated at the group level, not the individual level.
<h2>Collecting Data Anonymously</h2>
When data is collected <b>anonymously</b>, researchers are not able to identify individual subjects and their specific responses. That is, only the individuals themselves know they participated in the study and only they know their specific responses.
When data is collected in this manner, individuals are de-identified and there are no codes assigned to individuals so it’s impossible to link any specific responses to certain individuals.
This means that no information about specific individuals is collected such as address, name, phone number, social security number, or any other information that would make it possible to tie an individual to their survey responses.
<h2>
<b>Confidentiality vs. Anonymity</b>
</h2>
It’s important to note that a research study cannot collect data <em>both </em>confidentially and anonymously.
For example, if researchers invite individuals to answer survey questions in a private room in person, then obviously the data won’t be anonymous since the researchers know which individuals provided which responses. In this case, they must ensure that the survey data is confidentially collected and stored.
On the other hand, if individuals complete a survey online anonymously, there’s no need to store the data confidentially because there are no unique identifying characteristics that could tie survey responses to specific individuals. In this case, researchers simply need to ensure that when they share the data that it’s aggregated and reported at the group level.
For data that is collected through online surveys, researchers also need to make sure that it’s not possible to identify the specific IP Address that survey responses came from, otherwise it will be possible to identify which individuals at specific IP addresses provided which responses. This would violate the anonymity of the individuals.
<h2>The Importance of Informing Individuals</h2>
Whether data is collected confidentially or anonymously, it’s important that researchers inform the individuals in the study on how the data will be collected, stored, and shared before they let the individuals provide survey responses. In either case, it’s important that the individuals know that their responses will be private.
<h2><span class="orange">How to Use the confint() Function in R</span></h2>
You can use the <b>confint()</b> function in R to calculate a confidence interval for one or more parameters in a fitted regression model.
This function uses the following basic syntax:
<b>confint(object, parm, level=0.95)</b>
where:
<b>object</b>: Name of the fitted regression model
<b>parm</b>: Parameters to calculate confidence interval for (default is all)
<b>level</b>: Confidence level to use (default is 0.95)
The following example shows how to use this function in practice.
<h2>Example: How to Use confint() Function in R</h2>
Suppose we have the following data frame in R that shows the number of hours spent studying, number of practice exams taken, and final exam score for 10 students in some class:
<b>#create data frame
df &lt;- data.frame(score=c(77, 79, 84, 85, 88, 99, 95, 90, 92, 94), hours=c(1, 1, 2, 3, 2, 4, 4, 2, 3, 3), prac_exams=c(2, 3, 3, 2, 4, 5, 4, 3, 5, 4))
#view data frame
df
   score hours prac_exams
1     77     1          2
2     79     1          3
3     84     2          3
4     85     3          2
5     88     2          4
6     99     4          5
7     95     4          4
8     90     2          3
9     92     3          5
10    94     3          4
</b>
Now suppose we would like to fit the following multiple linear regression model in R:
Exam score = β<sub>0</sub> + β<sub>1</sub>(hours) + β<sub>2</sub>(practice exams)
We can use the  lm()  function to fit this model:
<b>#fit multiple linear regression model
fit &lt;- lm(score ~ hours + prac_exams, data=df)
#view summary of model
summary(fit)
Call:
lm(formula = score ~ hours + prac_exams, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-2.4324 -1.2632 -0.8956  0.4316  5.1412 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  68.4029     2.8723  23.815 5.85e-08 ***
hours         4.1912     0.9961   4.207   0.0040 ** 
prac_exams    2.6912     0.9961   2.702   0.0306 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 2.535 on 7 degrees of freedom
Multiple R-squared:  0.9005,Adjusted R-squared:  0.8721 
F-statistic: 31.68 on 2 and 7 DF,  p-value: 0.0003107
</b>
Notice that the model summary displays the fitted regression coefficients:
Intercept = 68.4029
hours = 4.1912
prac_exams = 2.6912
To obtain a 95% confidence interval for each of these coefficients, we can use the <b>confint()</b> function:
<b>#calculate 95% confidence interval for each coefficient in model
confint(fit)
 2.5 %    97.5 %
(Intercept) 61.6111102 75.194772
hours        1.8357237  6.546629
prac_exams   0.3357237  5.046629
</b>
The 95% confidence interval for each parameter is shown:
95% C.I. for Intercept = [61.61, 75.19]
95% C.I. for hours = [1.84, 6.55]
95% C.I. for prac_exams = [0.34, 5.05]
To instead calculate a 99% confidence interval, simply change the value for the <b>level</b> argument:
<b>#calculate 99% confidence interval for each coefficient in model
confint(fit, level=0.99)
 0.5 %    99.5 %
(Intercept) 58.3514926 78.454390
hours        0.7052664  7.677087
prac_exams  -0.7947336  6.177087
</b>
And to only calculate a confidence interval for a specific parameter, simply specify the coefficient using the <b>parm</b> argument:
<b>#calculate 99% confidence interval for hours
confint(fit, parm='hours', level=0.99)
          0.5 %   99.5 %
hours 0.7052664 7.677087
</b>
Notice that the 99% confidence interval is shown for the hours variable only.
<h2>Additional Resources</h2>
The following tutorials provide additional information about linear regression in R:
 How to Interpret Regression Output in R 
 How to Perform Simple Linear Regression in R 
 How to Perform Multiple Linear Regression in R 
 How to Perform Logistic Regression in R 
<h2><span class="orange">What is a Confounding Variable? (Definition & Example)</span></h2>
In any experiment, there are two main variables:
<b>The independent variable: </b>the variable that an experimenter changes or controls so that they can observe the effects on the dependent variable.
<b>The dependent variable: </b>the variable being measured in an experiment that is “dependent” on the independent variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/confound1.png">
Researchers are often interested in understanding how changes in the independent variable affect the dependent variable.
However, sometimes there is a third variable that is not accounted for that can affect the relationship between the two variables under study.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/confound2.png">
This type of variable is known as a <b>confounding variable</b> and it can <em>confound</em> the results of a study and make it appear that there exists some type of cause-and-effect relationship between two variables that doesn’t actually exist.
<b>Confounding variable:</b> A variable that is not included in an experiment, yet affects the relationship between the two variables in an experiment.
 
This type of variable can <em>confound</em> the results of an experiment and lead to unreliable findings.
For example, suppose a researcher collects data on ice cream sales and shark attacks and finds that the two variables are highly correlated. Does this mean that increased ice cream sales cause more shark attacks?
That’s unlikely. The more likely cause is the confounding variable <b>temperature</b>. When it is warmer outside, more people buy ice cream and more people go in the ocean.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/confound3.png">
<h3>Requirements for Confounding Variables</h3>
In order for a variable to be a confounding variable, it must meet the following requirements:
<b>1. It must be correlated with the independent variable.</b>
In the previous example, temperature was correlated with the independent variable of ice cream sales. In particular, warmer temperatures are associated with higher ice cream sales and cooler temperatures are associated with lower sales.
<b>2. It must have a causal relationship with the dependent variable.</b>
In the previous example, temperature had a direct causal effect on the number of shark attacks. In particular, warmer temperatures cause more people to go into the ocean which directly increases the probability of shark attacks occurring.
<h3>Why Are Confounding Variables Problematic?</h3>
Confounding variables are problematic for two reasons:
<b>1. Confounding variables can make it seem that cause-and-effect relationships exist when they don’t.</b>
In our previous example, the confounding variable of temperature made it seem like there existed a cause-and-effect relationship between ice cream sales and shark attacks.
However, we know that ice cream sales don’t cause shark attacks. The confounding variable of temperature just made it seem this way.
<b>2. Confounding variables can mask the true cause-and-effect relationship between variables.</b>
Suppose we’re studying the ability of exercise to reduce blood pressure. One potential confounding variable is starting weight, which is correlated with exercise and has a direct causal effect on blood pressure.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/confound4.png">
While increased exercise may lead to reduced blood pressure, an individual’s starting weight also has a big impact on the relationship between these two variables.
<h3>Confounding Variables & Internal Validity</h3>
In technical terms, confounding variables affect the <b>internal validity</b> of a study, which refers to how valid it is to attribute any changes in the dependent variable to changes in the independent variable.
When confounding variables are present, we can’t always say with complete confidence that the changes we observe in the dependent variable are a direct result of changes in the independent variable.
<h3>How to Reduce the Effect of Confounding Variables</h3>
There are several ways to reduce the effect of confounding variables, including the following methods:
<b>1. Random Assignment</b>
 Random assignment  refers to the process of randomly assigning individuals in a study to either a treatment group or a control group.
For example, suppose we want to study the effect of a new pill on blood pressure. If we recruit 100 individuals to participate in the study then we might use a random number generator to randomly assign 50 individuals to a control group (no pill) and 50 individuals to a treatment group (new pill).
By using random assignment, we increase the chances that the two groups will have roughly similar characteristics, which means that any difference we observe between the two groups can be attributed to the treatment.
This means the study should have <b>internal validity </b>– it’s valid to attribute any differences in blood pressure between the groups to the pill itself as opposed to differences between the individuals in the groups.
<b>2. Blocking</b>
 Blocking  refers to the practice of dividing individuals in a study into “blocks” based on some value of a confounding variable to eliminate the effect of the confounding variable.
For example, suppose researchers want to understand the effect that a new diet has on weight less. The independent variable is the new diet and the dependent variable is the amount of weight loss.
However, a confounding variable that will likely cause variation in weight loss is <b>gender</b>. It’s likely that the gender of an individual will effect the amount of weight they’ll lose, regardless of whether the new diet works or not.
One way to handle this problem is to place individuals into one of two blocks: 
Male
Female
Then, within each block we would randomly assign individuals to one of two treatments:
A new diet
A standard diet
By doing this, the variation within each block would be much lower compared to the variation among all individuals and we would be able to gain a better understanding of how the new diet affects weight loss while controlling for gender.
<b>3. Matching</b>
A  matched pairs design  is a type of experimental design in which we “match” individuals based on values of potential confounding variables.
For example, suppose researchers want to know how a new diet affects weight loss compared to a standard diet. Two potential confounding variables in this situation are <b>age</b> and <b>gender</b>.
To account for this, researchers recruit 100 subjects, then group the subjects into 50 pairs based on their age and gender. For example:
A 25-year-old male will be paired with another 25-year-old male, since they “match” in terms of age and gender.
A 30-year-old female will be paired with another 30-year-old female since they also match on age and gender, and so on.
Then, within each pair, one subject will randomly be assigned to follow the new diet for 30 days and the other subject will be assigned to follow the standard diet for 30 days.
At the end of the 30 days, researchers will measure the total weight loss for each subject.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/matchedpairs00.png">
By using this type of design, researchers can be confident that any differences in weight loss can be attributed to the type of diet used rather than the confounding variables <b>age</b> and <b>gender</b>.
This type of design does have a few drawbacks, including:
<b>1. Losing two subjects if one drops out.</b> If one subject decides to drop out of the study, you actually lose two subjects since you no longer have a complete pair.
<b>2. Time-consuming to find matches</b>. It can be quite time-consuming to find subjects who match on certain variables, such as gender and age.
<b>3. Impossible to match subjects perfectly</b>. No matter how hard you try, there will always be some variation within the subjects in each pair.
However, if a study has the resources available to implement this design it can be highly effective at eliminating the effects of confounding variables.
<h2><span class="orange">How to Create a Confusion Matrix in Excel</span></h2>
 Logistic regression  is a type of regression we can use when the response variable is binary.
One common way to evaluate the quality of a logistic regression model is to create a <b>confusion matrix</b>, which is a 2×2 table that shows the predicted values from the model vs. the actual values from the test dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/confusionR1.png">
The following step-by-step example shows how to create a confusion matrix in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter a column of actual values for a response variable along with the predicted values by a logistic regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/confExcel1.png">
<h3>Step 2: Create the Confusion Matrix</h3>
Next, we’ll use the <b>COUNTIFS()</b> formula to count the number of values that are “0” in the Actual column and also “0” in the Predicted column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/confExcel2.png">
We’ll use a similar formula to fill in every other cell in the confusion matrix:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/confExcel3.png">
<h3>Step 3: Calculate Accuracy, Precision and Recall</h3>
Once we’ve created the confusion matrix, we can calculate the following metrics:
<b>Accuracy</b>: Percentage of correct predictions
<b>Precision</b>: Correct positive predictions relative to total positive predictions
<b>Recall</b>: Correct positive predictions relative to total actual positives
The following formulas show how to calculate each of these metrics in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/confExcel4.png">
The higher the accuracy, the better a model is able to correctly classify observations.
In this example, our model has an accuracy of <b>0.7</b> which tells us that it correctly classified 70% of observations.
If we’d like, we can compare this accuracy to that of other logistic regression models to determine which model is best at classifying observations into categories of 0 or 1.
<h2><span class="orange">How to Create a Confusion Matrix in R (Step-by-Step)</span></h2>
 Logistic regression  is a type of regression we can use when the response variable is binary.
One common way to evaluate the quality of a logistic regression model is to create a <b>confusion matrix</b>, which is a 2×2 table that shows the predicted values from the model vs. the actual values from the test dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/confusionR1.png">
The following step-by-step example shows how to create a confusion matrix in R.
<h3>Step 1: Fit the Logistic Regression Model</h3>
For this example we’ll use the <b>Default</b> dataset from the <b>ISLR</b> package. We’ll use student status, bank balance, and annual income to predict the probability that a given individual defaults on their loan.
The following code shows how to fit a logistic regression model to this dataset:
<b>#load necessary packages
library(caret)
library(InformationValue)
library(ISLR)
#load dataset
data &lt;- Default
#split dataset into training and testing set
set.seed(1)
sample &lt;- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train &lt;- data[sample, ]
test &lt;- data[!sample, ]
#fit logistic regression model
model &lt;- glm(default~student+balance+income, family="binomial", data=train)
</b>
<h3>Step 2: Create the Confusion Matrix</h3>
Next, we’ll use the <b>confusionMatrix()</b> function from the <b>caret</b> package to create a confusion matrix:
<b>#use model to predict probability of default
predicted &lt;- predict(model, test, type="response")
#convert defaults from "Yes" and "No" to 1's and 0's
test$default &lt;- ifelse(test$default=="Yes", 1, 0)
#find optimal cutoff probability to use to maximize accuracy
optimal &lt;- optimalCutoff(test$default, predicted)[1]
#create confusion matrix
confusionMatrix(test$default, predicted)
     0  1
0 2912 64
1   21 39
</b>
<h3>Step 3: Evaluate the Confusion Matrix</h3>
We can also calculate the following metrics using the confusion matrix:
<b>Sensitivity:</b> The “true positive rate” – the percentage of individuals the model correctly predicted would default.
<b>Specificity:</b> The “true negative rate” – the percentage of individuals the model correctly predicted would <em>not</em> default.
<b>Total misclassification rate:</b> The percentage of total incorrect classifications made by the model.
The following code shows how to calculate these metrics:
<b>#calculate sensitivity
sensitivity(test$default, predicted)
[1] 0.3786408
#calculate specificity
specificity(test$default, predicted)
[1] 0.9928401
#calculate total misclassification error rate
misClassError(test$default, predicted, threshold=optimal)
[1] 0.027</b>
The total misclassification error rate is <b>2.7%</b> for this model.
In general, the lower this rate the better the model is able to predict outcomes, so this particular model turns out to be very good at predicting whether an individual will default or not. 
<footer>
<imghttps://secure.gravatar.com/avatar/41b43d5c604e6853226dfae01a8b4767?s=68&d=mm&r=g"><b>Kingsley</b> <span>says:
<!-- .comment-author -->
 <time datetime="2022-09-29T15:31:11-04:00">September 29, 2022 at 3:31 pm</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
You’ve been really helpful, Zac! Thanks for your short yet powerful tutorials. A real life saver you are!
<!-- .comment-content -->
 Reply </article>
<h2><span class="orange">How to Create a Confusion Matrix in Python</span></h2>
 Logistic regression  is a type of regression we can use when the response variable is binary.
One common way to evaluate the quality of a logistic regression model is to create a <b>confusion matrix</b>, which is a 2×2 table that shows the predicted values from the model vs. the actual values from the test dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/confusionR1.png">
To create a confusion matrix for a logistic regression model in Python, we can use the <b>confusion_matrix()</b> function from the  sklearn  package:
<b>from sklearn import metrics
metrics.confusion_matrix(y_actual, y_predicted)
</b>
The following example shows how to use this function to create a confusion matrix for a logistic regression model in Python.
<h3>Example: Creating a Confusion Matrix in Python</h3>
Suppose we have the following two arrays that contain the actual values for a response variable along with the predicted values by a logistic regression model:
<b>#define array of actual values
y_actual = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
#define array of predicted values
y_predicted = [0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]
</b>
We can use the <b>confusion_matrix()</b> function from sklearn to create a confusion matrix for this data:
<b>from sklearn import metrics
#create confusion matrix
c_matrix = metrics.confusion_matrix(y_actual, y_predicted)
#print confusion matrix
print(c_matrix)
[[6 4]
 [2 8]]</b>
If we’d like, we can use the <b>crosstab()</b> function from pandas to make a more visually appealing confusion matrix:
<b>import pandas as pd
y_actual = pd.Series(y_actual, name='Actual')
y_predicted = pd.Series(y_predicted, name='Predicted')
#create confusion matrix
print(pd.crosstab(y_actual, y_predicted))
Predicted  0  1
Actual         
0          6  4
1          2  8</b>
The columns show the predicted values for the response variable and the rows show the actual values.
We can also calculate the accuracy, precision, and recall using functions from the sklearn package:
<b>#print accuracy of model
print(metrics.accuracy_score(y_actual, y_predicted))
0.7
#print precision value of model
print(metrics.precision_score(y_actual, y_predicted))
0.667
#print recall value of model
print(metrics.recall_score(y_actual, y_predicted))
0.8
</b>
Here is a quick refresher on accuracy, precision, and recall:
<b>Accuracy</b>: Percentage of correct predictions
<b>Precision</b>: Correct positive predictions relative to total positive predictions
<b>Recall</b>: Correct positive predictions relative to total actual positives
And here is how each of these metrics was actually calculated in our example:
<b>Accuracy</b>: (6+8) / (6+4+2+8) = <b>0.7</b>
<b>Precision</b>: 8 / (8+4) = <b>0.667</b>
<b>Recall</b>: 8 / (2+8) = <b>0.8</b>
<h2><span class="orange">The Constant Variance Assumption: Definition & Example</span></h2>
<b>Linear regression</b> is a technique we use to quantify the relationship between one or more predictor variables and a  response variable .
One of the key assumptions of linear regression is that the residuals have constant variance at every level of the predictor variable(s).
If this assumption is not met, the residuals are said to suffer from  heteroscedasticity . When this occurs, the estimates for the model coefficients become unreliable.
<h3>How to Assess Constant Variance</h3>
The most common way to determine if the residuals of a regression model have constant variance is to create a <b>fitted values vs. residuals plot</b>.
This is a type of plot that displays the fitted values of the regression model along the x-axis and the residuals of those fitted values along the y-axis.
If the spread of the residuals is roughly equal at each level of the fitted values, we say that the constant variance assumption is met.
Otherwise, if the spread of the residuals systematically increases or decreases, this assumption is likely violated. 
<b>Note</b>: This type of plot can only be created <em>after</em> fitting a regression model to the dataset.
The following plot shows an example of a fitted values vs. residual plot that displays <b>constant variance</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/constantVar2.png">
Notice how the residuals are scattered randomly about zero in no particular pattern with roughly constant variance at every level of the fitted values.
The following plot shows an example of a fitted values vs. residual plot that displays <b>non-constant variance</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/constantVar1.png">
Notice that the spread of the residuals grows larger and larger as the fitted values increase. This is a typical sign of non-constant variance.
This tells us that our regression model suffers from non-constant variance of residuals and thus the estimates for the model coefficients aren’t reliable.
<h3>How to Fix a Violation of Constant Variance</h3>
If the assumption of constant variance is violated, the most common way to deal with it is to transform the response variable using one of the three transformations:
<b>1. Log Transformation: </b>Transform the response variable from y to <b>log(y)</b>
<b>2. Square Root Transformation: </b>Transform the response variable from y to <b>√y</b>
<b>3. Cube Root Transformation:</b> Transform the response variable from y to <b>y<sup>1/3</sup></b>
By performing these transformations, the problem of non-constant variance typically goes away.
<h2><span class="orange">What is Content Validity? (Definition & Example)</span></h2>
The term <b>content validity</b> refers to how well a survey or test measures the construct that it sets out to measure.
For example, suppose a professor wants to test the overall knowledge of his students in the subject of elementary statistics. His test would have content validity if:
The test covers every topic of elementary statistics that he taught in the class.
The test does not cover unrelated topics such as history, economics, biology, etc.
A test lacks content validity if it doesn’t cover all aspects of a construct it sets out to measure or if it covers topics that are unrelated to the construct in any way.
<h3>When is Content Validity Used?</h3>
In practice, content validity is often used to assess the validity of tests that assess content knowledge. Examples include:
<b>Example 1: Statistics Final Exam</b>
A final exam at the end of a semester for a statistics course would have content validity if it covers every topic discussed in the course and excludes all other irrelevant  topics.
<b>Example 2: Pilot’s License</b>
An exam that tests whether or not individuals have enough knowledge to acquire their pilot’s license would have content validity if it includes questions that cover every possible topic discussed in a pilot’s course and exclude all other questions that aren’t relevant for the license.
<b>Example 3: Real Estate License</b>
An exam that tests whether or not individuals possess enough knowledge to get a real estate license would have content validity if it covers every topic that needs to be understood by a real estate agent and excludes all other questions that aren’t relevant.
In each situation, content validity can help determine if a test covers all aspects of the construct that it sets out to measure.
<h3>How to Measure Content Validity</h3>
In a  1975 paper , C.H. Lawshe developed the following technique to assess content validity:
<b>Step 1: Collect data from subject matter experts.</b>
Lawshe proposed that each subject matter expert (SME) on a judging panel should respond to the question:
<em>“Is the skill or knowledge measured by this item ‘essential,’ ‘useful, but not essential,’ or ‘not necessary’ to the performance of the job?”</em>
Each SME should provide this response to each question on a test.
<b>Step 2: Calculate the content validity ratio.</b>
Next, Lawshe proposed the following formula to quantify the content validity ratio of each question on the test:
Content Validity Ratio = (n<sub>e</sub> – N/2) / (N/2)
where:
<b>n<sub>e</sub>:</b> The number of subject matter experts indicating “essential”
<b>N:</b> The total number of SME panelists
If the content validity ratio for a given question falls below a certain critical value, it’s likely that the question is not measuring the construct of interest as well as it should.
The following table shows the critical values based on the number of SME panelists:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/content_validity1.png">
The content validity index, denoted as CVI, is the mean content validity ratio of all questions on a test. The closer the CVI is to 1, the higher the overall content validity of a test.
The following example shows how to calculate content validity for a certain test.
<h3>Example: Measuring Content Validity</h3>
Suppose we ask a panel of 10 judges to rate 6 items on a test. The green boxes in the following table shows which judges rated each item as an “essential” item:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/content_validity2.png">
The content validity ratio for the first item would be calculated as:
Content Validity Ratio = (n<sub>e</sub> – N/2) / (N/2) = (9 – 10/2) / (10/2) = <b>0.8</b>
We could calculate the content validity ratio for each item in a similar manner:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/content_validity4.png">
From the critical values table, we can see that an item is considered to have content validity for a panel of 10 judges only if it has a CVR value above 0.62.
For this particular test, only three of the items pass this threshold.
Lastly, we can also calculate the content validity index (CVI) of the entire test as the average of all the CVR values:
CVI = (0.8 -0.2 + 1 + 0.8 + 0.6 + 0) / 6 = <b>0.5</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/content_validity5.png">
This CVI value is quite low, which indicates that the test likely doesn’t measure the construct of interest as well as it could.
It would be recommended to remove or modify the items that have low CVR values to improve the overall content validity of the test.
<h3>Content Validity vs. Face Validity</h3>
Content validity is different from<em> face validity</em>, which is when a survey or test appears valid at face value to both the individuals who take it and the individuals who administer it.
 Face validity  is a less technical way of assessing the validity of a test and it’s often used just used as a quick way to detect whether or not a test should be modified in some way before being used.
<h2><span class="orange">How to Create a Contingency Table in Excel</span></h2>
A <b>contingency table </b>(sometimes called “crosstabs”) is a type of table that summarizes the relationship between two categorical variables.
Fortunately it’s easy to create a contingency table for variables in Excel by using the pivot table function. This tutorial shows an example of how to do so.
<h3>Example: Contingency Table in Excel</h3>
Suppose we have the following dataset that shows information for 20 different product orders, including the type of product purchased (TV, computer, or radio) along with the country (A, B, or C) that the product was purchased in:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel1.png">
Use the following steps to create a contingency table for this dataset:
<b>Step 1: Select the PivotTable option.</b>
Click the <b>Insert </b>tab, then click <b>PivotTable</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel2.png">
In the window that pops up, choose <b>A1:C21 </b>for the range of values. Then choose a location to place the pivot table in. We’ll choose cell <b>E2 </b>within the existing worksheet:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel3.png">
Once you click <b>OK</b>, an empty contingency table will appear in cell E2.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel4.png">
<b>Step 2: Populate the contingency table</b>.
In the window that appears on the right, drag Country into the box titled <b>Rows</b>, drag Product into the box titled <b>Columns</b>, and drag Order Number into the box titled <b>Values</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel5-1.png">
<em><b>Note: </b>If Values first appears as “Sum of Order Number” simply click the dropdown arrow and select <b>Value Field Settings</b>. Then choose <b>Count </b>and click <b>OK</b>.</em>
Once you do so, the frequency values will automatically be populated in the contingency table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/contingencytableexcel6-1.png">
<b>Step  3: Interpret the contingency table</b>.
The way to interpret the values in the table is as follows:
<b>Row Totals:</b>
A total of <b>4 </b>orders were made from country A.
A total of <b>8 </b>orders were made from country B.
A total of <b>8 </b>orders were made from country C.
<b>Column Totals:</b>
A total of <b>6 </b>computers were purchased.
A total of <b>5 </b>radios were purchased.
A total of <b>9 </b>TV’s were purchased.
<b>Individual Cells:</b>
A total of <b>1 </b>computer was purchased from country A.
A total of <b>3 </b>computers were purchased from country B.
A total of <b>2 </b>computers were purchased from country C.
A total of <b>0 </b>radios were purchased from country A.
A total of <b>2 </b>radios were purchased from country B.
A total of <b>3 </b>radios were purchased from country C.
A total of <b>3 </b>TV’s were purchased from country A.
A total of <b>3 </b>TV’s were purchased from country B.
A total of <b>3 </b>TV’s were purchased from country C.
<h2><span class="orange">How to Create a Contingency Table in R</span></h2>
A <b>contingency table </b>(sometimes called “crosstabs”) is a type of table that summarizes the relationship between two categorical variables.
Fortunately it’s easy to create a contingency table for variables in R by using the pivot table function. This tutorial shows an example of how to do so.
<h3>Example: Contingency Table in R</h3>
Suppose we have the following dataset that shows information for 20 different product orders, including the type of product purchased along with the country that the product was purchased in:
<b>#create data
df &lt;- data.frame(order_num = 1:20, product=rep(c('TV', 'Radio', 'Computer'), times=c(9, 6, 5)), country=rep(c('A', 'B', 'C', 'D'), times=5))
#view data
df
   order_num  product country
1          1       TV       A
2          2       TV       B
3          3       TV       C
4          4       TV       D
5          5       TV       A
6          6       TV       B
7          7       TV       C
8          8       TV       D
9          9       TV       A
10        10    Radio       B
11        11    Radio       C
12        12    Radio       D
13        13    Radio       A
14        14    Radio       B
15        15    Radio       C
16        16 Computer       D
17        17 Computer       A
18        18 Computer       B
19        19 Computer       C
20        20 Computer       D</b>
To create a contingency table, we can simply use the <b>table()</b> function and provide the variables product and country as the arguments:
<b>#create contingency table
table &lt;- table(df$product, df$country)
#view contingency table
table
           A B C D
  Computer 1 1 1 2
  Radio    1 2 2 1
  TV       3 2 2 2
</b>
We can also use the <b>addmargins()</b> function to add margins to the table:
<b>#add margins to contingency table
table_w_margins &lt;- addmargins(table)
#view contingency table
table_w_margins
            A  B  C  D Sum
  Computer  1  1  1  2   5
  Radio     1  2  2  1   6
  TV        3  2  2  2   9
  Sum       5  5  5  5  20
</b>
Here is how to interpret the table:
The value in the bottom right corner shows the total number of products ordered: 20.
The values along the right side show the row sums: A total of 5 computers were ordered, 6 radios were ordered, and 9 TV’s were ordered.
The values along the bottom of the table show the column sums: A total of 5 products were ordered from country A, 5 from country B, 5 from country C, and 5 from country D.
The values inside the table show the number of specific products ordered from each country: 1 computer from country A, 1 radio from country A, 3 TV’s from country A, etc.
<h2><span class="orange">How to Create a Contingency Table in Python</span></h2>
A <b>contingency table</b> is a type of table that summarizes the relationship between two categorical variables.
To create a contingency table in Python, we can use the  pandas.crosstab()  function, which uses the following sytax:
<b>pandas.crosstab(index, columns)</b>
where:
<b>index:</b> name of variable to display in the rows of the contingency table
<b>columns:</b> name of variable to display in the columns of the contingency table
 The following step-by-step example shows how to use this function to create a contingency table in Python.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that shows information for 20 different product orders, including the type of product purchased (TV, computer, or radio) along with the country (A, B, or C) that the product was purchased in:
<b>import pandas as pd
#create data
df = pd.DataFrame({'Order': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10,            11, 12, 13, 14, 15, 16, 17, 18, 19, 20],   'Product': ['TV', 'TV', 'Comp', 'TV', 'TV', 'Comp',               'Comp', 'Comp', 'TV', 'Radio', 'TV', 'Radio', 'Radio',               'Radio', 'Comp', 'Comp', 'TV', 'TV', 'Radio', 'TV'],   'Country': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B',               'B', 'C', 'C', 'C', 'C', 'C', 'C', 'C', 'C']})
#view data
df
        OrderProductCountry
01TVA
12TVA
23CompA
34TVA
45TVB
56CompB
67CompB
78CompB
89TVB
910RadioB
1011TVB
1112RadioB
1213RadioC
1314RadioC
1415CompC
1516CompC
1617TVC
1718TVC
1819RadioC
1920TVC</b>
<h3>Step 2: Create the Contingency Table</h3>
The following code shows how to create a contingency table to count the number of each product ordered by each country:
<b>#create contingency table
pd.crosstab(index=df['Country'], columns=df['Product'])
ProductCompRadioTV
Country
A103
B323
C233</b>
Here’s how to interpret the table:
A total of <b>1 </b>computer was purchased from country A.
A total of <b>3 </b>computers were purchased from country B.
A total of <b>2 </b>computers were purchased from country C.
A total of <b>0 </b>radios were purchased from country A.
A total of <b>2 </b>radios were purchased from country B.
A total of <b>3 </b>radios were purchased from country C.
A total of <b>3 </b>TV’s were purchased from country A.
A total of <b>3 </b>TV’s were purchased from country B.
A total of <b>3 </b>TV’s were purchased from country C.
<h3>Step 3: Add Margin Totals to the Contingency Table</h3>
We can use the argument <b>margins=True</b> to add the margin totals to the contingency table:
<b>#add margins to contingency table
pd.crosstab(index=df['Country'], columns=df['Product'], margins=True)
ProductCompRadioTVAll
Country
A1034
B3238
C2338
All65920 </b>
The way to interpret the values in the table is as follows:
<b>Row Totals:</b>
A total of <b>4 </b>orders were made from country A.
A total of <b>8 </b>orders were made from country B.
A total of <b>8 </b>orders were made from country C.
<b>Column Totals:</b>
A total of <b>6 </b>computers were purchased.
A total of <b>5 </b>radios were purchased.
A total of <b>9 </b>TV’s were purchased.
The value in the bottom right corner of the table shows that a total of <b>20</b> products were ordered from all countries.
<h2><span class="orange">Continuity Correction Calculator</span></h2>
A <b> continuity correction </b> is used when you want to use a normal distribution to approximate a binomial distribution.
This calculator allows you to apply a continuity correction to a normal distribution to find approximate probabilities for a binomial distribution.
Simply enter the appropriate values for a given binomial distribution below and then click the “Calculate” button.
<label for="n"><b>n</b> (number of trials)</label>
<input type="number" id="n" min="0" value="5">
<label for="X"><b>X</b> (number of successes)</label>
<input type="number" id="X" min="0" value="3">
<label for="p"><b>p</b> (probability of success in a given trial)</label>
<input type="number" id="p" min="0" value="0.4" step=".01">
<input type="button" id="button" onclick="binomialCalc()" value="Calculate">
<b>Exact binomial probabilities:</b>
<div>
P(X = <span>3): 0.23040
<div>
P(X ≤ <span>3): 0.92196
<div>
P(X &lt; <span>3): 0.68256
<div>
P(X ≥ <span>3): 0.31744
<div>
P(X > <span>3): 0.08704
<b>Approximate probabilities using continuity correction:</b>
<div>
P(2.5 &lt; X &lt; 3.5): 0.23040
<div>
P(X &lt; 3.5): 0.92196
<div>
P(X &lt; 2.5): 0.68256
<div>
P(X > 2.5): 0.31744
<div>
P(X > 3.5): 0.08704
<script>
function binomialCalc() {
//get input values
var X = document.getElementById('X').value;
var p = document.getElementById('p').value;
var n = document.getElementById('n').value;
//calculate cumulative probabilities
var probArray = [];
for (var i = 0; i <= X; i++) {
probArray[i] = (math.factorial(n) / (math.factorial(n-i) * math.factorial(i))) * Math.pow(p, i) * Math.pow((1-p), (n-i));
}
//assign probabilities to variable names
var exactProb = (math.factorial(n) / (math.factorial(n-X) * math.factorial(X))) * Math.pow(p, X) * Math.pow((1-p), (n-X));
var lessEqualProb = math.sum(probArray);
var lessProb = lessEqualProb - exactProb;
var greaterEqualProb = 1 - lessProb;
var greaterProb = 1 - lessEqualProb;
//output probabilities
document.getElementById('exactProb').innerHTML = exactProb.toFixed(5);
document.getElementById('lessEqualProb').innerHTML = lessEqualProb.toFixed(5);
document.getElementById('lessProb').innerHTML = lessProb.toFixed(5);
document.getElementById('greaterEqualProb').innerHTML = greaterEqualProb.toFixed(5);
document.getElementById('greaterProb').innerHTML = greaterProb.toFixed(5);
//change X to reflect value that user entered
document.getElementsByClassName('user_X')[0].innerHTML = X;
document.getElementsByClassName('user_X')[1].innerHTML = X;
document.getElementsByClassName('user_X')[2].innerHTML = X;
document.getElementsByClassName('user_X')[3].innerHTML = X;
document.getElementsByClassName('user_X')[4].innerHTML = X;
//find correct continuity correction values
var user_X1 = X-.5;
var user_X2 = X-(-.5);
var user_X3 = X-(-.5);
var user_X4 = X-.5;
var user_X5 = X-.5;
var user_X6 = X-(-.5);
//apply continuity correction values
document.getElementById('user_X1').innerHTML = (user_X1).toFixed(1);
document.getElementById('user_X2').innerHTML = (user_X2).toFixed(1);
document.getElementById('user_X3').innerHTML = (user_X3).toFixed(1);
document.getElementById('user_X4').innerHTML = (user_X4).toFixed(1);
document.getElementById('user_X5').innerHTML = (user_X5).toFixed(1);
document.getElementById('user_X6').innerHTML = (user_X6).toFixed(1);
//find z scores
var mean_z = n*p;
var stdev_z = Math.sqrt(n*p*(1-p));
//find z-scores
var approxProb1 = jStat.normal.cdf(user_X2, mean_z, stdev_z) - jStat.normal.cdf(user_X1, mean_z, stdev_z);
var approxProb2 = jStat.normal.cdf(user_X3, mean_z, stdev_z);
var approxProb3 = jStat.normal.cdf(user_X4, mean_z, stdev_z);
var approxProb4 = 1-jStat.normal.cdf(user_X5, mean_z, stdev_z);
var approxProb5 = 1-jStat.normal.cdf(user_X6, mean_z, stdev_z);
//output probabilities
document.getElementById('approxProb1').innerHTML = approxProb1.toFixed(5);
document.getElementById('approxProb2').innerHTML = approxProb2.toFixed(5);
document.getElementById('approxProb3').innerHTML = approxProb3.toFixed(5);
document.getElementById('approxProb4').innerHTML = approxProb4.toFixed(5);
document.getElementById('approxProb5').innerHTML = approxProb5.toFixed(5);
}
</script>
<h2><span class="orange">A Simple Explanation of Continuity Correction in Statistics</span></h2>
A <b>continuity correction </b>is applied when you want to use a continuous distribution to approximate a discrete distribution. Typically it is used when you want to use a  normal distribution  to approximate a  binomial distribution .
Recall that the binomial distribution tells us the probability of obtaining <em>x </em>successes in <em>n </em>trials, given the probability of success in a single trial is <em>p</em>. To answer questions about probability with a binomial distribution we could simply use a  Binomial Distribution Calculator , but we could also <em>approximate </em>the probability using a normal distribution with a continuity correction.
A continuity correction is the name given to <b>adding or subtracting 0.5 to a discrete x-value</b>.
For example, suppose we would like to find the probability that a coin lands on heads less than or equal to 45 times during 100 flips. That is, we want to find P(X ≤ 45). To use the normal distribution to approximate the binomial distribution, we would instead find P(X ≤ 45.5).
The following table shows when you should add or subtract 0.5, based on the type of probability you’re trying to find:
<table><tbody>
<tr>
<th><b>Using Binomial Distribution</b></th>
<th><b>Using Normal Distribution with Continuity Correction</b></th>
</tr>
<tr>
<td style="text-align: center;">X = 45</td>
<td style="text-align: center;">44.5 &lt; X &lt; 45.5</td>
</tr>
<tr>
<td style="text-align: center;">X ≤ 45</td>
<td style="text-align: center;">X &lt; 45.5</td>
</tr>
<tr>
<td style="text-align: center;">X &lt; 45</td>
<td style="text-align: center;">X &lt; 44.5</td>
</tr>
<tr>
<td style="text-align: center;">X ≥ 45</td>
<td style="text-align: center;">X > 44.5</td>
</tr>
<tr>
<td style="text-align: center;">X > 45</td>
<td style="text-align: center;">X > 45.5</td>
</tr>
</tbody></table>
<b>Note: </b>
 
It’s only appropriate to apply a continuity correction to the normal distribution to approximate the binomial distribution when n*p and n*(1-p) are both at least 5.
 
For example, suppose n = 15 and p = 0.6. In this case:
 
n*p = 15 * 0.6 = 9
 
n*(1-p) = 15 * (1 – 0.6) = 15 * (0.4) = 6
 
Since both of these numbers are greater than or equal to 5, it would be okay to apply a continuity correction in this scenario.
The following example illustrates how to apply a continuity correction to the normal distribution to approximate the binomial distribution.
<h2>Example of Applying a Continuity Correction</h2>
Suppose we want to know the probability that a coin lands on heads less than or equal to 43 times during 100 flips. In this case:
n = number of trials = 100
X = number of successes = 43
p = probability of success in a given trial = 0.50
We can plug these numbers into the  Binomial Distribution Calculator  to see that the probability of the coin landing on heads less than or equal to 43 times is <b>0.09667</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/contCorrection2.png">
To approximate the binomial distribution by applying a continuity correction to the normal distribution, we can use the following steps:
<b>Step 1: Verify that n*p and n*(1-p) are both at least 5</b>.
n*p = 100*0.5 = 50
n*(1-p) = 100*(1 – 0.5) = 100*0.5 = 50
Both numbers are greater than or equal to 5, so we’re good to proceed.
<b>Step 2: Determine if you should add or subtract 0.5</b>
Referring to the table above, we see that we’re supposed to <b>add 0.5 </b>when we’re working with a probability in the form of X ≤ 43. Thus, we will be finding P(X&lt; 43.5).
<b>Step 3: Find the mean (μ) and standard deviation (σ) of the binomial distribution.</b>
<b>μ</b> = n*p = 100*0.5 = 50
<b>σ </b>= √n*p*(1-p) = √100*.5*(1-.5) = √25 = 5
<b>Step 4: Find the z-score using the mean and standard deviation found in the previous step.</b>
<b>z </b>= (x – μ) / σ = (43.5 – 50) / 5 = -6.5 / 5 = -1.3.
<b>Step 5: Use the Z table to find the probability associated with the z-score.</b>
According to the Z table, the probability associated with z = -1.3 is <b>0.0968</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/contCorrection3.png">
Thus, the exact probability we found using the binomial distribution was <b>0.09667 </b>while the approximate probability we found using the continuity correction with the normal distribution was <b>0.0968</b>. These two values are pretty close.
<h2>When to Use a Continuity Correction</h2>
Before modern statistical software existed and calculations had to be done manually, continuity corrections were often used to find probabilities involving discrete distributions. Today, continuity corrections play less of a role in computing probabilities since we can typically rely on software or calculators to calculate probabilities for us.
Instead, it’s simply a topic discussed in statistics classes to illustrate the relationship between a binomial distribution and a normal distribution and to show that it’s possible for a normal distribution to approximate a binomial distribution by applying a continuity correction.
<h2>Continuity Correction Calculator</h2>
Use the  Continuity Correction Calculator  to automatically apply a continuity correction to a normal distribution to approximate binomial probabilities.
<h2><span class="orange">How to Fix: contrasts can be applied only to factors with 2 or more levels</span></h2>
One common error you may encounter in R is:
<b>Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
  contrasts can be applied only to factors with 2 or more levels</b>
This error occurs when you attempt to fit a regression model using a predictor variable that is either a factor or character and only has one unique value.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Example: How to Fix ‘contrasts can be applied only to factors with 2 or more levels’</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=as.factor(4), var3=c(7, 7, 8, 3, 2), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    4    7    1
2    3    4    7    1
3    3    4    8    2
4    4    4    3    8
5    5    4    2    9
</b>
Notice that the predictor variable <b>var2</b> is a factor and only has one unique value.
If we attempt to fit a multiple linear regression model using <b>var2</b> as one of the predictor variables, we’ll get the following error:
<b>#attempt to fit regression model
model &lt;- lm(var4 ~ var1 + var2 + var3, data=df)
Error in `contrasts&lt;-`(`*tmp*`, value = contr.funs[1 + isOF[nn]]) : 
  contrasts can be applied only to factors with 2 or more levels
</b>
We get this error because <b>var2</b> only has one unique value: 4. Since there isn’t any variation at all in this predictor variable, R is unable to effectively fit a regression model.
We can actually use the following syntax to count the number of unique values for each variable in our data frame:
<b>#count unique values for each variable
sapply(lapply(df, unique), length)
var1 var2 var3 var4 
   4    1    4    4 
</b>
And we can use the  lapply()  function to display each of the unique values for each variable:
<b>#display unique values for each variable
lapply(df[c('var1', 'var2', 'var3')], unique)
$var1
[1] 1 3 4 5
$var2
[1] 4
Levels: 4
$var3
[1] 7 8 3 2</b>
We can see that <b>var2</b> is the only variable that has one unique value. Thus, we can fix this error by simply dropping var2 from the regression model:
<b>#fit regression model without using <em>var2</em> as a predictor variable
model &lt;- lm(var4 ~ var1 + var3, data=df)
#view model summary
summary(model)
Call:
lm(formula = var4 ~ var1 + var3, data = df)
Residuals:
       1        2        3        4        5 
 0.02326 -1.23256  0.91860  0.53488 -0.24419 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)   8.4070     3.6317   2.315   0.1466  
var1          0.6279     0.6191   1.014   0.4172  
var3         -1.1512     0.3399  -3.387   0.0772 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 1.164 on 2 degrees of freedom
Multiple R-squared:  0.9569,Adjusted R-squared:  0.9137 
F-statistic: 22.18 on 2 and 2 DF,  p-value: 0.04314</b>
By dropping <b>var2</b> from the regression model, we no longer encounter the error from earlier.
<h2><span class="orange">How to Convert Categorical Variable to Numeric in Pandas</span></h2>
You can use the following basic syntax to convert a categorical variable to a numeric variable in a pandas DataFrame:
<b>df['column_name'] = pd.factorize(df['column_name'])[0]
</b>
You can also use the following syntax to convert every categorical variable in a DataFrame to a numeric variable:
<b>#identify all categorical variables
cat_columns = df.select_dtypes(['object']).columns
#convert all categorical variables to numeric
df[cat_columns] = df[cat_columns].apply(lambda x: pd.factorize(x)[0])
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert One Categorical Variable to Numeric</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'position': ['G', 'G', 'F', 'G', 'F', 'C', 'G', 'F', 'C'],   'points': [5, 7, 7, 9, 12, 9, 9, 4, 13],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 10]})
#view DataFrame
df
        teamposition pointsrebounds
0AG 511
1AG 78
2AF 710
3BG 96
4BF 126
5BC 95
6CG 99
7CF 412
8CC 1310
</b>
We can use the following syntax to convert the ‘team’ column to numeric:
<b>#convert 'team' column to numeric
df['team'] = pd.factorize(df['team'])[0]
#view updated DataFrame
df
teamposition pointsrebounds
00G 511
10G 78
20F 710
31G 96
41F 126
51C 95
62G 99
72F 412
82C 1310
</b>
Here is how the conversion worked:
Each team that had a value of ‘<b>A</b>‘ was converted to <b>0</b>.
Each team that had a value of ‘<b>B</b>‘ was converted to <b>1</b>.
Each team that had a value of ‘<b>C</b>‘ was converted to <b>2</b>.
<h3>Example 2: Convert Multiple Categorical Variables to Numeric</h3>
Once again suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],   'position': ['G', 'G', 'F', 'G', 'F', 'C', 'G', 'F', 'C'],   'points': [5, 7, 7, 9, 12, 9, 9, 4, 13],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 10]})
#view DataFrame
df
        teamposition pointsrebounds
0AG 511
1AG 78
2AF 710
3BG 96
4BF 126
5BC 95
6CG 99
7CF 412
8CC 1310
</b>
We can use the following syntax to convert every categorical variable in the DataFrame to a numeric variable:
<b>#get all categorical columns
cat_columns = df.select_dtypes(['object']).columns
#convert all categorical columns to numeric
df[cat_columns] = df[cat_columns].apply(lambda x: pd.factorize(x)[0])
#view updated DataFrame
df
teamposition pointsrebounds
000 511
100 78
201 710
310 96
411 126
512 95
620 99
721 412
822 1310
</b>
Notice that the two categorical columns (team and position) both got converted to numeric while the points and rebounds columns remained the same.
<b>Note</b>: You can find the complete documentation for the pandas <b>factorize()</b> function  here .
<h2><span class="orange">How to Convert Categorical Variables to Numeric in R</span></h2>
You can use one of the following methods to convert a categorical variable to a numeric variable in R:
<b>Method 1: Convert One Categorical Variable to Numeric</b>
<b>df$var1 &lt;- unclass(df$var1)
</b>
<b>Method 2: Convert Multiple Categorical Variables to Numeric</b>
<b>df[, c('var1', 'var2')] &lt;- sapply(df[, c('var1', 'var2')], unclass)</b>
<b>Method 3: Convert All Categorical Variables to Numeric</b>
<b>df[sapply(df, is.factor)] &lt;- data.matrix(df[sapply(df, is.factor)])</b>
The following examples show how to use each method with the following data frame:
<b>#create data frame with some categorical variables
df &lt;- data.frame(team=as.factor(c('A', 'B', 'C', 'D')), conf=as.factor(c('AL', 'AL', 'NL', 'NL')), win=as.factor(c('Yes', 'No', 'No', 'Yes')), points=c(122, 98, 106, 115))
#view data frame
df
  team conf win points
1    A   AL Yes    122
2    B   AL  No     98
3    C   NL  No    106
4    D   NL Yes    115
</b>
<h3>Method 1: Convert One Categorical Variable to Numeric</h3>
The following code shows how to convert one categorical variable in a data frame to a numeric variable:
<b>#convert 'team' variable to numeric
df$team &lt;- unclass(df$team)
#view updated data frame
df
  team conf win points
1    1   AL Yes    122
2    2   AL  No     98
3    3   NL  No    106
4    4   NL Yes    115
</b>
Notice that the values for the ‘team’ variable have been converted to numeric values.
<h3>Method 2: Convert Multiple Categorical Variables to Numeric</h3>
The following code shows how to convert multiple categorical variables in a data frame to numeric variables:
<b>#convert 'team' and 'win' variables to numeric
df[, c('team', 'win')] &lt;- sapply(df[, c('team', 'win')], unclass)
#view updated data frame
df
  team conf win points
1    1   AL   2    122
2    2   AL   1     98
3    3   NL   1    106
4    4   NL   2    115
</b>
Notice that the values for the ‘team’  and ‘win’ variables have been converted to numeric values.
<h3>Method 3: Convert All Categorical Variables to Numeric</h3>
The following code shows how to convert all categorical variables in a data frame to numeric variables:
<b>#convert all categorical variables to numeric
df[sapply(df, is.factor)] &lt;- data.matrix(df[sapply(df, is.factor)])
#view updated data frame
df
  team conf win points
1    1    1   2    122
2    2    1   1     98
3    3    2   1    106
4    4    2   2    115
</b>
Notice that the values for each of the categorical variables in the data frame have been converted to numeric values.
<h2><span class="orange">How to Convert a Character to a Timestamp in R</span></h2>
You can use the <b>strptime()</b> function to convert a character to a timestamp in R. This function uses the following basic syntax:
<b>strptime(character, format = “%Y-%m-%d %H:%M:%S”)</b>
where:
<b>character:</b> The name of the character to be converted
<b>format:</b> The timestamp format to convert the character to
This tutorial provides several examples of how to use this syntax in practice.
<h3>Example 1: Convert Character to Year-Month-Day Format</h3>
The following code shows how to convert a character to a timestamp with a year-month-date format:
<b>#create character variable
char &lt;- "2021-10-15"
#display class of character variable
class(char)
[1] "character"
#convert character to timestamp
time &lt;- strptime(char, "%Y-%m-%d")
#display timestamp variable
time
[1] "2021-10-15 UTC"
#display class of timestamp variable
class(time)
[1] "POSIXlt" "POSIXt"
</b>
<h3>Example 2: Convert Character to Hours-Minutes-Seconds Format</h3>
The following code shows how to convert a character to a timestamp with hours, minutes, and seconds included:
<b>#create character variable
char &lt;- "2021-10-15 4:30:00"
#convert character to timestamp
time &lt;- strptime(char, "%Y-%m-%d %H:%M:%S")
#display timestamp variable
time
[1] "2021-10-15 04:30:00 UTC"
</b>
<h3>Example 3: Convert Character to Timestamp and Specify Time Zone</h3>
The following code shows how to convert a character to a timestamp and specify the time zone as Eastern Standard Time using the <b>tz</b> argument:
<b>#create character variable
char &lt;- "2021-10-15"
#convert character to timestamp with specific time zone
time &lt;- strptime(char, "%Y-%m-%d", tz="EST")
#display timestamp variable
time
[1] "2021-10-15 EST"
</b>
<h3>Example 4: Convert a Data Frame Column to Timestamp </h3>
The following code shows how to convert a column in a data frame from a character to a timestamp:
<b>#create data frame
df &lt;- data.frame(date=c("2021-10-15", "2021-10-19", "2021-10-20"), sales=c(4, 13, 19))
#display data frame
class(df$date)
[1] "character"
#convert date column to timestamp
df$date &lt;- strptime(df$date, "%Y-%m-%d")
#display class of date column
class(df$date)
[1] "POSIXlt" "POSIXt" 
</b>
You can find more R tutorials on  this page .
<h2><span class="orange">How to Convert Columns to DateTime in Pandas</span></h2>
Often you may be interested in converting one or more columns in a pandas DataFrame to a DateTime format. Fortunately this is easy to do using the  to_datetime()  function.
This tutorial shows several examples of how to use this function on the following DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame
df = pd.DataFrame({'event': ['A', 'B', 'C'],   'start_date': ['20150601', '20160201', '20170401'],   'end_date': ['20150608', '20160209', '20170416'] })
#view DataFrame
df
eventstart_dateend_date
0A2015060120150608
1B2016020120160209
2C20170401201704161
#view column data types
df.dtypes
event         object
start_date    object
end_date      object
dtype: object</b>
<h3>Example 1: Convert a Single Column to DateTime</h3>
The following code shows how to convert the “start_date” column from a string to a DateTime format:
<b>#convert start_date to DateTime format
df['start_date'] = pd.to_datetime(df['start_date'])
#view DataFrame
df
        eventstart_dateend_date
0A2015-06-0120150608
1B2016-02-0120160209
2C2017-04-0120170416
#view column date types
df.dtypes
event                 object
start_date    datetime64[ns]
end_date              object
dtype: object</b>
Note that the to_datetime() function is smart and can typically infer the correct date format to use, but you can also specify the format to use with the <b>format </b>argument:
<b>#convert start_date to DateTime format
df['start_date'] = pd.to_datetime(df['start_date'], format='%Y%m%d')
#view DataFrame
df
        eventstart_dateend_date
0A2015-06-0120150608
1B2016-02-0120160209
2C2017-04-0120170416
#view column date types
df.dtypes
event                 object
start_date    datetime64[ns]
end_date              object
dtype: object</b>
<h3>Example 2: Convert Multiple Columns to DateTime</h3>
The following code shows how to convert both the “start_date” and “end_date” columns from strings to DateTime formats:
<b>#convert start_date and end_date to DateTime formats
df[['start_date', 'end_date']] = df[['start_date', 'end_date']].apply(pd.to_datetime)
#view DataFrame
df
eventstart_dateend_date
0A2015-06-012015-06-08
1B2016-02-012016-02-09
2C2017-04-012017-04-16
#view column date types
df.dtypes
event                 object
start_date    datetime64[ns]
end_date      datetime64[ns]
dtype: object</b>
<h3>Example 3: Convert Columns to DateTime Format with Seconds</h3>
In some cases you may also have columns that include a date along with the hours, minutes and seconds, such as the following DataFrame:
<b>#create DataFrame
df = pd.DataFrame({'event': ['A', 'B', 'C'],   'start_date': ['20150601043000', '20160201054500', '20170401021215'],   'end_date': ['20150608', '20160209', '20170416'] })
#view DataFrame
df
        eventstart_dateend_date
0A2015060104300020150608
1B2016020105450020160209
2C2017040102121520170416
</b>
Once again, the to_datetime() function is smart and can usually infer the correct format to use without us specifying it:
<b>#convert start_date to DateTime format
df['start_date'] = pd.to_datetime(df['start_date'])
#view DataFrame
df
        eventstart_date        end_date
0A2015-06-01 04:30:0020150608
1B2016-02-01 05:45:0020160209
2C2017-04-01 02:12:1520170416
#view column date types
df.dtypes
event                 object
start_date    datetime64[ns]
end_date              object
dtype: object</b>
Of course, in the wild you’re likely to come across a variety of weird DateTime formats so you may have to actually use the <b>format </b>argument to tell Python exactly what DateTime format to use.
In those cases, refer to  this page  for a complete list of % DateTime operators you can use to specify formats.
<h2><span class="orange">How to Convert Data Frame Column to Vector in R</span></h2>
You can use one of the following three methods to convert a data frame column to a vector in R:
<b>#use $ operator
new_vector &lt;- df$column_name
#use indexing
new_vector &lt;- df[['column_name']]
#use 'pull' from <em>dplyr</em> package
new_vector &lt;- dplyr::pull(df, column_name)</b>
Each of these methods returns identical results.
The following examples show how to use each of these methods in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(a=c(1, 2, 5, 6, 12, 14), b=c(8, 8, 9, 14, 22, 19), c=c(3, 3, 2, 1, 2, 10))
#display data frame
df
   a  b  c
1  1  8  3
2  2  8  3
3  5  9  2
4  6 14  1
5 12 22  2
6 14 19 10</b>
<h3>Example 1: Use $ Operator</h3>
The following code shows how to use the $ operator to convert a data frame column to a vector:
<b>#convert column 'a' to vector
new_vector &lt;- df$a
#view vector
new_vector
[1]  1  2  5  6 12 14
#view class of vector
class(new_vector)
[1] "numeric"
</b>
<h3>Example 2: Use Indexing</h3>
The following code shows how to use indexing to convert a data frame column to a vector:
<b>#convert column 'a' to vector
new_vector &lt;- df[['a']]
#view vector
new_vector
[1]  1  2  5  6 12 14
#view class of vector
class(new_vector)
[1] "numeric"</b>
<h3>Example 3: Use ‘pull’ from dplyr</h3>
The following code shows how to use the ‘pull’ function from the  dplyr  package to convert a data frame column to a vector:
<b>library(dplyr)
#convert column 'a' to vector
new_vector &lt;- pull(df, a)
#view vector
new_vector
[1]  1  2  5  6 12 14
#view class of vector
class(new_vector)
[1] "numeric"</b>
Notice that all three methods return identical results.
<b>Note</b>: If you happen to be working with an extremely large dataset, the ‘pull’ function from the dplyr package will perform the fastest out of the three functions shared in this tutorial.
<h2><span class="orange">How to Convert Datetime to Date in Pandas</span></h2>
Often you may want to convert a datetime to a date in pandas. Fortunately this is easy to do using the <b>.dt.date</b> function, which takes on the following syntax:
<b>df['date_column'] = pd.to_datetime(df['datetime_column']).dt.date
</b>
<h3>Example: Datetime to Date in Pandas</h3>
For example, suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create pandas DataFrame with two columns
df = pd.DataFrame({'sales': [4, 11],   'time': ['2020-01-15 20:02:58', '2020-01-18 14:43:24']})
#view DataFrame 
print(df)
salestime
042020-01-15 20:02:58
1112020-01-18 14:43:24
</b>
To convert the ‘time’ column to just a date, we can use the following syntax:
<b>#convert datetime column to just date
df['time'] = pd.to_datetime(df['time']).dt.date
#view DataFrame
print(df)
salestime
042020-01-15
1112020-01-18
</b>
Now the ‘time’ column just displays the date without the time.
<h3>Using Normalize() for datetime64 Dtypes</h3>
You should note that the code above will return an <b>object</b> dtype:
<b>#find dtype of each column in DataFrame
df.dtypes
sales     int64
time     object
dtype:   object</b>
If you instead want <b>datetime64 </b>then you can <b>normalize() </b>the time component, which will keep the dtype as <b>datetime64 </b>but it will only display the date:
<b>#convert datetime column to just date
df['time'] = pd.to_datetime(df['time']).dt.normalize()
#view DataFrame
print(df)
salestime
042020-01-15
1112020-01-18
#find dtype of each column in DataFrame
df.dtypes
sales             int64
time     datetime64[ns]
dtype: object
</b>
Once again only the date is displayed, but the ‘time’ column is a <b>datetime64 </b>dtype.
<h2><span class="orange">How to Convert Excel Date Format to Proper Date in R</span></h2>
You can use the following methods to convert Excel dates that are formatted as numbers into proper dates in R:
<b>Method 1: Convert Excel Number to Proper Date in R</b>
<b>df$date &lt;- as.Date(df$date, origin = "1899-12-30")
</b>
<b>Method 2: Convert Excel Number to Proper Datetime in R</b>
<b>library(openxlsx)
df$datetime &lt;- convertToDateTime(df$datetime)</b>
The following examples show how to use each method in practice with an Excel file called <b>sales_data.xlsx</b> that contains the following data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/dateexcel1.jpg"532">
<h2>Example 1: Convert Excel Number to Proper Date in R</h2>
The following code shows how to use the <b>as.Date()</b> function in base R to convert the numeric values in the <b>date</b> column of the Excel file into proper dates in R:
<b>library(readxl)
#import Excel file into R as data frame
df &lt;- read_excel("C:\\Users\\bob\\Documents\\sales_data.xlsx")
#view data frame
df
# A tibble: 10 x 3
    date datetime sales
        
 1 44563   44563.    14
 2 44566   44567.    19
 3 44635   44636.    22
 4 44670   44670.    29
 5 44706   44706.    24
 6 44716   44716.    25
 7 44761   44761.    25
 8 44782   44782.    30
 9 44864   44864.    35
10 44919   44920.    28
#convert Excel number format to proper R date
df$date &lt;- as.Date(df$date, origin = "1899-12-30")
#view updated data frame
df
# A tibble: 10 x 3
   date       datetime sales
            
 1 2022-01-02   44563.    14
 2 2022-01-05   44567.    19
 3 2022-03-15   44636.    22
 4 2022-04-19   44670.    29
 5 2022-05-25   44706.    24
 6 2022-06-04   44716.    25
 7 2022-07-19   44761.    25
 8 2022-08-09   44782.    30
 9 2022-10-30   44864.    35
10 2022-12-24   44920.    28</b>
Notice that the values in the <b>date</b> column are now formatted as proper dates.
<h2>Example 2: Convert Excel Number to Proper Datetime in R</h2>
The following code shows how to use the <b>convertToDateTime()</b> function from the <b>openxlsx</b> package in R to convert the numeric values in the <b>datetime</b> column of the Excel file into proper datetimes in R:
<b>library(readxl)
library(openxlsx)
#import Excel file into R as data frame
df &lt;- read_excel("C:\\Users\\bob\\Documents\\sales_data.xlsx")
#view data frame
df
# A tibble: 10 x 3
    date datetime sales
        
 1 44563   44563.    14
 2 44566   44567.    19
 3 44635   44636.    22
 4 44670   44670.    29
 5 44706   44706.    24
 6 44716   44716.    25
 7 44761   44761.    25
 8 44782   44782.    30
 9 44864   44864.    35
10 44919   44920.    28
#convert Excel datetime to proper datetime in R
df$datetime &lt;- convertToDateTime(df$datetime)
#view updated data frame
df
# A tibble: 10 x 3
    date datetime            sales  
 1 44563 2022-01-02 04:14:00    14
 2 44566 2022-01-05 12:15:00    19
 3 44635 2022-03-15 15:34:00    22
 4 44670 2022-04-19 09:45:00    29
 5 44706 2022-05-25 10:30:00    24
 6 44716 2022-06-04 10:15:00    25
 7 44761 2022-07-19 01:13:00    25
 8 44782 2022-08-09 02:15:00    30
 9 44864 2022-10-30 04:34:00    35
10 44919 2022-12-24 21:23:00    28
</b>
Notice that the values in the <b>datetime</b> column are now formatted as proper dates.
<b>Note</b>: You can also use the <b>convertToDate()</b> function from the <b>openxlsx</b> package to convert a numeric date to a proper date in R.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Import Excel Files into R 
 How to Export Data Frame to an Excel File in R 
 How to Export Data Frames to Multiple Excel Sheets in R 
<h2><span class="orange">How to Convert Factor to Character in R (With Examples)</span></h2>
You can use the following syntax to convert a factor to a character in R:
<b>x &lt;- as.character(x)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert Vector Factor to Character</h3>
The following code shows how to convert a factor vector to a character vector:
<b>#create factor vector
x &lt;- factor(c('A', 'B', 'C', 'D'))
#view class
class(x)
[1] "factor"
#convert factor vector to character
x &lt;- as.character(x)
#view class
class(x)
[1] "character"
</b>
<h3>Example 2: Convert Data Frame Column to Character</h3>
The following code shows how to convert a column from a factor to a character in a data frame:
<b>#create data frame
df &lt;- data.frame(name=factor(c('A', 'B', 'C', 'D')), status=factor(c('Y', 'Y', 'N', 'N')), income=c(45, 89, 93, 96))
#view class of each column
sapply(df, class)
     name    status    income 
 "factor"  "factor" "numeric" 
#convert name column to character
df$name &lt;- as.character(df$name)
#view class of each column
sapply(df, class) 
       name      status      income 
"character"    "factor"   "numeric" </b>
<h3>Example 3: Convert All Factor Columns to Character</h3>
The following code shows how to convert all factor columns to character in a data frame:
<b>#create data frame
df &lt;- data.frame(name=factor(c('A', 'B', 'C', 'D')), status=factor(c('Y', 'Y', 'N', 'N')), income=c(45, 89, 93, 96))
#view class of each column
sapply(df, class)
     name    status    income 
 "factor"  "factor" "numeric" 
#convert name column to character
x &lt;- sapply(df, is.factor)
df[x] &lt;- lapply(df[x], as.character)
#view class of each column
sapply(df, class) 
       name       status      income 
"character"  "character"   "numeric" </b>
<h3>Example 4: Convert All Data Frame Columns to Character</h3>
The following code shows how to convert every column to character in a data frame:
<b>#create data frame
df &lt;- data.frame(name=factor(c('A', 'B', 'C', 'D')), status=factor(c('Y', 'Y', 'N', 'N')), income=c(45, 89, 93, 96))
#view class of each column
sapply(df, class)
     name    status    income 
 "factor"  "factor" "numeric" 
#convert all columns to character
df &lt;- lapply(df, as.character)
#view class of each column
sapply(df, class) 
       name       status      income 
"character"  "character"  "characer" </b>
<h2><span class="orange">How to Convert Factor to Date in R (With Examples)</span></h2>
You can use one of the following two methods to quickly convert a factor to a date in R:
<b>Method 1: Use Base R</b>
<b>as.Date(factor_variable, format = '%m/%d/%Y')
</b>
<b>Method 2: Use Lubridate</b>
<b>library(lubridate)
mdy(factor_variable)
</b>
The following examples show how to use each method with the following data frame:
<b>#create data frame
df &lt;- data.frame(day=factor(c('1/1/2020', '1/13/2020', '1/15/2020')), sales=c(145, 190, 223))
#view data frame
df
        day sales
1  1/1/2020   145
2 1/13/2020   190
3 1/15/2020   223
#view class of 'day' variable
class(df$day)
[1] "factor"
</b>
<h3>Example 1: Convert Factor to Date Using Base R</h3>
The following code shows how to convert the ‘day’ variable in the data frame from a factor to a date using the <b>as.Date()</b> function from base R:
<b>#convert 'day' column to date format
df$day &lt;- as.Date(df$day, format = '%m/%d/%Y')
#view updated data frame
df
         day sales
1 2020-01-01   145
2 2020-01-13   190
3 2020-01-15   223
#view class of 'day' variable
class(df$day)
[1] "Date"
</b>
Notice that the ‘day’ variable has been converted to a date format.
<h3>Example 2: Convert Factor to Date Using Lubridate</h3>
The following code shows how to convert the ‘day’ variable from a factor to a date using the <b>mdy()</b> function from the lubridate package:
<b>library(lubridate)
#convert 'day' column to date format
df$day &lt;- mdy(df$day)
#view updated data frame
df
         day sales
1 2020-01-01   145
2 2020-01-13   190
3 2020-01-15   223
#view class of 'day' variable
class(df$day)
[1] "Date"
</b>
The ‘day’ variable has been converted to a date format.
Note that <b>mdy()</b> indicates a month-day-year format. 
<b>Note</b>: You can find the complete documentation for the lubridate package  here .
<h2><span class="orange">How to Convert a List to a Data Frame in R</span></h2>
There are many cases in which you might want to convert a list to a data frame in R. This tutorial explains three different ways to do so.
<h3>Method 1: Base R</h3>
The following code snippet shows how to convert a list to a data frame using only base R:
<b>#create list
my_list &lt;- list(letters[1:5], letters[6:10])
my_list
[[1]]
[1] "a" "b" "c" "d" "e"
[[2]]
[1] "f" "g" "h" "i" "j"
#convert list to data frame
data.frame(t(sapply(my_list,c)))
  X1 X2 X3 X4 X5
1  a  b  c  d  e
2  f  g  h  i  j
</b>
In this example, <b>sapply </b>converts the list to a matrix, then <b>data.frame </b>converts the matrix to a data frame. The end result is a data frame of two rows and five columns.
<h3>Method 2: Data Table</h3>
The following code snippet shows how to convert a list of two nested lists into a data frame with two rows and three columns using the  rbindlist  function from the <b>data.table </b>library:
<b>#load data.table library
library(data.table)
#create list
my_list &lt;- list(a = list(var1 = 1, var2 = 2, var3 = 3),
                b = list(var1 = 4, var2 = 5, var3 = 6))
my_list 
$a
$a$var1
[1] 1
$a$var2
[1] 2
$a$var3
[1] 3
$b
$b$var1
[1] 4
$b$var2
[1] 5
$b$var3
[1] 6
#convert list to data frame
rbindlist(my_list)
   var1 var2 var3
1:    1    2    3
2:    4    5    6</b>
This results in a data table with two rows and three columns. If you’d like to convert this data table to a data frame, you can simply use <b>as.data.frame(DT)</b>.
This method converts a list to a data frame faster than the previous method if you’re working with a very large dataset.
<h3>Method 3: Dplyr</h3>
The following code snippet shows how to convert a list of two nested lists into a data frame with two rows and three columns using the  bind_rows  function from the <b>dplyr </b>library:
<b>#load library
library(dplyr)
#create list
my_list &lt;- list(a = list(var1 = 1, var2 = 2, var3 = 3),
                b = list(var1 = 4, var2 = 5, var3 = 6))
my_list
$a
$a$var1
[1] 1
$a$var2
[1] 2
$a$var3
[1] 3
$b
$b$var1
[1] 4
$b$var2
[1] 5
$b$var3
[1] 6
#convert list to data frame
bind_rows(my_list)
# A tibble: 2 x 3
   var1  var2  var3
    
1     1     2     3
2     4     5     6
</b>
This results in a data frame with two rows and three columns.
This method also tends to work faster than base R when you’re working with large datasets.
<h2><span class="orange">How to Convert List to Matrix in R (With Examples)</span></h2>
You can use the following syntax to convert a list to a matrix in R:
<b>#convert list to matrix (by row)
matrix(unlist(my_list), ncol=3, byrow=TRUE)
#convert list to matrix (by column)
matrix(unlist(my_list), ncol=3)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert List to Matrix (By Rows)</h3>
The following code shows how convert a list to a matrix (by rows) in R:
<b>#create list
my_list &lt;- list(1:3, 4:6, 7:9, 10:12, 13:15)
#view list
my_list
[[1]]
[1] 1 2 3
[[2]]
[1] 4 5 6
[[3]]
[1] 7 8 9
[[4]]
[1] 10 11 12
[[5]]
[1] 13 14 15
#convert list to matrix
matrix(unlist(my_list), ncol=3, byrow=TRUE)
     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
[4,]   10   11   12
[5,]   13   14   15
</b>
The result is a matrix with 5 rows and 3 columns.
<h3>Example 2: Convert List to Matrix (By Columns)</h3>
The following code shows how to convert a list to a matrix (by columns) in R:
<b>#create list
my_list &lt;- list(1:5, 6:10, 11:15)
#view list
my_list
[[1]]
[1] 1 2 3 4 5
[[2]]
[1]  6  7  8  9 10
[[3]]
[1] 11 12 13 14 15
#convert list to matrix
matrix(unlist(my_list), ncol=3)
     [,1] [,2] [,3]
[1,]    1    6   11
[2,]    2    7   12
[3,]    3    8   13
[4,]    4    9   14
[5,]    5   10   15</b>
The result is a matrix with 5 rows and 3 columns.
<h3>Cautions on Converting a List to Matrix</h3>
Note that R will throw an error if you attempt to convert a list to a matrix in which each position of the list doesn’t have the same number of elements.
The following example illustrates this point:
<b>#create list
my_list &lt;- list(1:5, 6:10, 11:13)
#view list
my_list
[[1]]
[1] 1 2 3 4 5
[[2]]
[1]  6  7  8  9 10
[[3]]
[1] 11 12 13
#attempt to convert list to matrix
matrix(unlist(my_list), ncol=3)
Warning message:
In matrix(unlist(my_list), ncol = 3) :
  data length [13] is not a sub-multiple or multiple of the number of rows [5]</b>
<h2><span class="orange">How to Convert List to NumPy Array (With Examples)</span></h2>
You can use the following basic syntax to convert a list in Python to a NumPy array:
<b>import numpy as np
my_list = [1, 2, 3, 4, 5]
my_array = np.asarray(my_list)
</b>
The following examples shows how to use this syntax in practice.
<h3>Example 1: Convert List to NumPy Array</h3>
The following code shows how to convert a list in Python to a NumPy array:
<b>import numpy as np
#create list of values
my_list = [3, 4, 4, 5, 7, 8, 12, 14, 14, 16, 19]
#convert list to NumPy array
my_array = np.asarray(my_list)
#view NumPy array
print(my_array)
[ 3  4  4  5  7  8 12 14 14 16 19]
#view object type
type(my_array)
numpy.ndarray</b>
Note that you can also use the <b>dtype</b> argument to specify a certain data type for the new NumPy array when performing the conversion:
<b>import numpy as np
#create list of values
my_list = [3, 4, 4, 5, 7, 8, 12, 14, 14, 16, 19]
#convert list to NumPy array
my_array = np.asarray(my_list, dtype=np.float64)
#view data type of NumPy array
print(my_array.dtype)
float64
</b>
<h3>Example 2: Convert List of Lists to NumPy Array of Arrays</h3>
The following code shows how to convert a list of lists to a NumPy array of arrays:
<b>import numpy as np
#create list of lists
my_list_of_lists = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
#convert list to NumPy array
my_array = np.asarray(my_list_of_lists)
#view NumPy array
print(my_array)
[[1 2 3]
 [4 5 6]
 [7 8 9]]
</b>
We can then use the <b>shape</b> function to quickly get the dimensions of the new array of arrays:
<b>print(my_array.shape)
(3, 3)
</b>
This tells us that the NumPy array of arrays has three rows and three columns.
<h2><span class="orange">How to Convert List to Vector in R (With Examples)</span></h2>
You can use one of the following methods to convert a list to a vector in R:
<b>#use unlist() function
new_vector &lt;- unlist(my_list, use.names = FALSE)
#use flatten_*() function from purrr library
new_vector &lt;- purrr::flatten(my_list)
</b>
The following examples show how to use each of these methods in practice with the following list:
<b>#create list
my_list &lt;- list(A = c(1, 2, 3),
                B = c(4, 5),
                C = 6)
#display list
my_list
$A
[1] 1 2 3
$B
[1] 4 5
$C
[1] 6
</b>
<h3>Example 1: Convert List to Vector Using unlist() Function</h3>
The following code shows how to convert a list to a vector using the <b>unlist()</b> function:
<b>#convert list to vector
new_vector &lt;- unlist(my_list)
#display vector
new_vector
A1 A2 A3 B1 B2  C 
 1  2  3  4  5  6 
</b>
Note that you can specify <b>use.names = FALSE</b> to remove the names from the vector:
<b>#convert list to vector
new_vector &lt;- unlist(my_list, use.names = FALSE)
#display vector
new_vector
[1] 1 2 3 4 5 6
</b>
<h3>Example 2: Convert List to Vector Using flatten_* Function</h3>
The following code shows how to convert a list to a vector using the family of flatten_* functions from the  purrr  package:
<b>library(purrr) 
#convert list to vector
new_vector &lt;- flatten_dbl(my_list)
#display vector
new_vector
[1] 1 2 3 4 5 6
</b>
The <b>flatten_dbl()</b> function specifically converts the list to a vector of type double.
Note that we could use <b>flatten_chr()</b> to convert a character list to a vector of type character:
<b>library(purrr) 
#define character list
my_char_list &lt;- list(A = c('a', 'b', 'c'),     B = c('d', 'e'),     C = 'f')
#convert character list to character vector
new_char_vector &lt;- flatten_chr(my_char_list)
#display vector
new_char_vector
[1] "a" "b" "c" "d" "e" "f"
</b>
Check out  this page  for a complete list of the family of flatten_* functions.
<b>Note:</b> If you’re working with an extremely large list, the flatten_* functions will perform quicker than the unlist() function from base R.
<h2><span class="orange">How to Convert Matrix to Vector in R (With Examples)</span></h2>
You can use the following syntax to convert a matrix to a vector in R:
<b>#convert matrix to vector (sorted by columns) using c()
new_vector &lt;- c(my_matrix)
#convert matrix to vector (sorted by rows) using c()
new_vector &lt;- c(t(my_matrix))
#convert matrix to vector (sorted by columns) using as.vector()
new_vector &lt;- as.vector(my_matrix)
#convert matrix to vector (sorted by rows) using as.vector()
new_vector &lt;- as.vector(t(my_matrix))
</b>
Note that the <b>c()</b> and <b>as.vector()</b> functions will return identical results.
The following examples show how to use each of these functions in practice with the following matrix:
<b>#create matrix
my_matrix &lt;- matrix(1:20, nrow = 5)
#display matrix
my_matrix
     [,1] [,2] [,3] [,4]
[1,]    1    6   11   16
[2,]    2    7   12   17
[3,]    3    8   13   18
[4,]    4    9   14   19
[5,]    5   10   15   20
</b>
<h3>Example 1: Convert Matrix to Vector (sorted by columns) Using c() function</h3>
The following code shows how to convert a matrix to a vector (sorted by columns) using the <b>c()</b> function:
<b>#convert matrix to vector (sorted by columns)
new_vector &lt;- c(my_matrix)
#display vector
new_vector
[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
</b>
<h3>Example 2: Convert Matrix to Vector (sorted by rows) Using c() function</h3>
The following code shows how to convert a matrix to a vector (sorted by rows) using the <b>c()</b> function:
<b>#convert matrix to vector (sorted by rows)
new_vector &lt;- c(t(my_matrix))
#display vector
new_vector
[1]  1  6 11 16  2  7 12 17  3  8 13 18  4  9 14 19  5 10 15 20
</b>
<h3>Example 3: Convert Matrix to Vector (sorted by columns) Using as.vector() function</h3>
The following code shows how to convert a matrix to a vector (sorted by columns) using the <b>as.vector()</b> function:
<b>#convert matrix to vector (sorted by columns)
new_vector &lt;- as.vector(my_matrix)
#display vector
new_vector
[1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
</b>
<h3>Example 4: Convert Matrix to Vector (sorted by rows) Using as.vector() function</h3>
The following code shows how to convert a matrix to a vector (sorted by rows) using the <b>as.vector()</b> function:
<b>#convert matrix to vector (sorted by rows)
new_vector &lt;- as.vector(t(my_matrix))
#display vector
new_vector
[1]  1  6 11 16  2  7 12 17  3  8 13 18  4  9 14 19  5 10 15 20</b>
<h2><span class="orange">Convert Between Month Name & Number in Google Sheets</span></h2>
You can use the following formulas to convert between month names and numbers in Google Sheets:
<b>Method 1: Convert Month Number to Month Name</b>
<b>#convert month number in cell A1 to full month name
=TEXT(A1*29, "mmmm")
#convert month number in cell A1 to abbreviated month name
=TEXT(A1*29, "mmm")
</b>
<b>Method 2: Convert Month Name to Month Number</b>
<b>#convert month name in cell A1 to month number
=MONTH(A1&1)</b>
The following examples show how to use these formulas in practice.
<h3>Example 1: Convert Month Number to Month Name in Google Sheets</h3>
The following screenshot shows how to use the <b>TEXT()</b> function to convert month numbers (from 1 to 12) to month names (from January to December):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/convert1.png">
Note that we can also use the <b>“mmm”</b> format to convert the month number to the abbreviated month name:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/convert2.png">
<h3>Example 2: Convert Month Name to Month Number in Google Sheets</h3>
The following screenshot shows how to use the <b>MONTH()</b> function to convert month names (from January to December) to month numbers (from 1 to 12):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/convert3.png">
Note that this function will work on abbreviated month names as well:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/convert4.png">
<h2><span class="orange">How to Convert Multiple Columns to Numeric Using dplyr</span></h2>
You can use the following methods to convert multiple columns to numeric using the  dplyr  package:
<b>Method 1: Convert Specific Columns to Numeric</b>
<b>library(dplyr) 
df %>% mutate_at(c('col1', 'col2'), as.numeric)
</b>
<b>Method 2: Convert All Character Columns to Numeric</b>
<b>library(dplyr)
df %>% mutate_if(is.character, as.numeric)</b>
The following examples show how to use each method in practice.<b> </b>
<h3>Example 1: Convert Specific Columns to Numeric</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), position=c('G', 'G', 'G', 'F', 'F'), assists=c('33', '28', '31', '39', '34'), rebounds=c('30', '28', '24', '24', '28'))
#view structure of data frame
str(df)
'data.frame':5 obs. of  4 variables:
 $ team    : chr  "A" "B" "C" "D" ...
 $ position: chr  "G" "G" "G" "F" ...
 $ assists : chr  "33" "28" "31" "39" ...
 $ rebounds: chr  "30" "28" "24" "24" ...</b>
We can see that every column in the data frame is currently a character.
To convert just the <b>assists</b> and <b>rebounds</b> columns to numeric, we can use the following syntax:
<b>library(dplyr) 
#convert assists and rebounds columns to numeric
df &lt;- df %>% mutate_at(c('assists', 'rebounds'), as.numeric)
#view structure of updated data frame
str(df)
'data.frame':5 obs. of  4 variables:
 $ team    : chr  "A" "B" "C" "D" ...
 $ position: chr  "G" "G" "G" "F" ...
 $ assists : num  33 28 31 39 34
 $ rebounds: num  30 28 24 24 28
</b>
We can see that the <b>assists</b> and <b>rebounds</b> columns are now both numeric.
<h3>Example 2: Convert All Character Columns to Numeric</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(ranking=factor(c(1, 4, 3, 2, 5)), assists=c('12', '10', '8', '11', '15'), points=c('33', '28', '31', '39', '34'), rebounds=c('30', '28', '24', '24', '28'))
#view structure of data frame
str(df)
'data.frame':5 obs. of  4 variables:
 $ ranking : Factor w/ 5 levels "1","2","3","4",..: 1 4 3 2 5
 $ assists : chr  "12" "10" "8" "11" ...
 $ points  : chr  "33" "28" "31" "39" ...
 $ rebounds: chr  "30" "28" "24" "24" ...</b>
We can see that three of the columns in the data frame are character columns.
To convert all of the character columns to numeric, we can use the following syntax:
<b>library(dplyr) 
#convert all character columns to numeric
df &lt;- df %>% mutate_if(is.character, as.numeric)
#view structure of updated data frame
str(df)
'data.frame':5 obs. of  4 variables:
 $ ranking : Factor w/ 5 levels "1","2","3","4",..: 1 4 3 2 5
 $ assists : num  12 10 8 11 15
 $ points  : num  33 28 31 39 34
 $ rebounds: num  30 28 24 24 28
</b>
We can see that all of the character columns are now numeric.
<b>Note</b>: Refer to the  dplyr documentation page  for a complete explanation of the <b>mutate_at</b> and <b>mutate_if</b> functions.
<h2><span class="orange">How to Convert Numbers to Dates in R</span></h2>
Often you may need to convert numbers to date formats in R. The easiest way to do this is by using the  lubridate  package, which has several helpful functions for dealing with dates in R.
This tutorial provides several examples of how to use these functions in practice.
<h3>Example 1: Convert Integers to Dates</h3>
The following code shows how to convert a column of integer values in a data frame to a date format by using the <b>ymd()</b> function:
<b>library(lubridate)
#create data frame
df &lt;- data.frame(date = c(20201022, 20201023, 20201026, 20201027, 20201028), sales = c(4, 7, 8, 9, 12))
#convert date column from numeric to year-month-date format
df$date &lt;- ymd(df$date)
#view data frame
df
        date sales
1 2020-10-22     4
2 2020-10-23     7
3 2020-10-26     8
4 2020-10-27     9
5 2020-10-28    12
#view class of date column
class(df$date)
[1] "Date"
</b>
Note that the lubridate package has several functions to handle different date formats.
For example, the following shows how to convert a column of integer values in a data frame to a date format by using the <b>ydm()</b> function:
<b>library(lubridate)
#create data frame
df &lt;- data.frame(date = c(20202210, 20202310, 20202610, 20202710, 20202810), sales = c(4, 7, 8, 9, 12))
#convert date column from numeric to year-month-date format
df$date &lt;- ydm(df$date)
#view data frame
df
        date sales
1 2020-10-22     4
2 2020-10-23     7
3 2020-10-26     8
4 2020-10-27     9
5 2020-10-28    12
#view class of date column
class(df$date)
[1] "Date"</b>
<h3>Example 2: Convert Months & Years to Dates</h3>
The following code shows how to convert a column of numeric values that represent the number of months from January 1st, 2010 to a date format by using the <b>months()</b> function:
<b>library(lubridate)
#create data frame
df &lt;- data.frame(date = c(11, 15, 18, 22, 24), sales = c(4, 7, 8, 9, 12))
#convert date column from numeric to year-month-date format
df$date &lt;- as.Date('2010-01-01') + months(df$date)
#view data frame
df
        date  sales
1 2010-12-01      4
2 2011-04-01      7
3 2011-07-01      8
4 2011-11-01      9
5 2012-01-01     12
#view class of date column
class(df$date)
[1] "Date"</b>
And the following code shows how to convert a column of numeric values that represent the number of years from January 1st, 2010 to a date format by using the <b>years()</b> function:
<b>library(lubridate)
#create data frame
df &lt;- data.frame(date = c(11, 15, 18, 22, 24), sales = c(4, 7, 8, 9, 12))
#convert date column from numeric to year-month-date format
df$date &lt;- as.Date('2010-01-01') + years(df$date)
#view data frame
df
        date  sales
1 2021-01-01      4
2 2025-01-01      7
3 2028-01-01      8
4 2032-01-01      9
5 2034-01-01     12
#view class of date column
class(df$date)
[1] "Date"</b>
<b>Bonus:</b> Refer to  this cheat sheet  to gain a better understanding of the functions available in the lubridate package.
<h2><span class="orange">How to Convert Pandas Series to DataFrame (With Examples)</span></h2>
You can use the following basic syntax to convert a pandas Series to a pandas DataFrame:
<b>my_df = my_series.to_frame(name='column_name')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Convert One Series to Pandas DataFrame</h3>
Suppose we have the following pandas Series:
<b>import pandas as pd
#create pandas Series
my_series = pd.Series([3, 4, 4, 8, 14, 17, 20])
#view pandas Series
print(my_series)
0     3
1     4
2     4
3     8
4    14
5    17
6    20
dtype: int64
#view object type
print(type(my_series))
&lt;class 'pandas.core.series.Series'>
</b>
We can use the <b>to_frame()</b> function to quickly convert this pandas Series to a pandas DataFrame:
<b>#convert Series to DataFrame and specify column name to be 'values'
my_df = my_series.to_frame(name='values')
#view pandas DataFrame 
print(my_df)
   values
0       3
1       4
2       4
3       8
4      14
5      17
6      20
#view object type 
print(type(my_df))
&lt;class 'pandas.core.frame.DataFrame'>
</b>
<h3>Example 2: Convert Multiple Series to Pandas DataFrame</h3>
Suppose we have three different pandas Series:
<b>import pandas as pd
#define three Series
name = pd.Series(['A', 'B', 'C', 'D', 'E'])
points = pd.Series([34, 20, 21, 57, 68])
assists = pd.Series([8, 12, 14, 9, 11])
</b>
We can use the following syntax to convert each Series into a DataFrame and concatenate the three DataFrames into one final DataFrame:
<b>#convert each Series to a DataFrame
name_df = name.to_frame(name='name')
points_df = points.to_frame(name='points')
assists_df = assists.to_frame(name='assists')
#concatenate three Series into one DataFrame
df = pd.concat([name_df, points_df, assists_df], axis=1)
#view final DataFrame
print(df)
  name  points  assists
0    A      34        8
1    B      20       12
2    C      21       14
3    D      57        9
4    E      68       11
</b>
The final result is a pandas DataFrame where each Series represents a column.
<h2><span class="orange">How to Convert a String to Datetime in R</span></h2>
You can use the following syntax to convert a string to a datetime in R:
<b>as.POSIXct(string_name, format="%Y-%m-%d %H:%M:%S", tz="UTC")
</b>
The following examples show how to use this syntax in practice:
<h3>Example 1: Convert One String to Datetime</h3>
The following code shows how to convert a single string in R to a datetime format:
<b>#define string variable
string_x &lt;- "2020-01-01 14:45:18"
#convert string variable to datetime variable
datetime_x &lt;- as.POSIXct(string_x, format="%Y-%m-%d %H:%M:%S", tz="UTC")
#view new datetime variable
datetime_x
[1] "2020-01-01 14:45:18 UTC"
#view class of datetime variable 
class(datetime_x)
[1] "POSIXct" "POSIXt" 
</b>
<h3>Example 2: Convert Column of Strings to Datetime</h3>
Suppose we have the following data frame with a column that contains a string of datetimes:
<b>#define data frame
df &lt;- data.frame(day=c("2020-01-01 14:45:18", "2020-02-01 14:00:11",            "2020-03-01 12:40:10", "2020-04-01 11:00:00"), sales=c(13, 18, 22, 19))
#view data frame
df
  day sales
1 2020-01-01 14:45:18    13
2 2020-02-01 14:00:11    18
3 2020-03-01 12:40:10    22
4 2020-04-01 11:00:00    19
</b>
We can convert this column from strings to datetimes using the following syntax:
<b>#convert column of strings to datetime
df$day &lt;- as.POSIXct(df$day, format="%Y-%m-%d %H:%M:%S", tz="UTC")
#view class of 'day' column
class(df$day)
[1] "POSIXct" "POSIXt" 
</b>
Note that in these examples we used a specific datetime format. Refer to  this page  for a complete documentation of the potential datetime formats you can use.
<h2><span class="orange">How to Convert Strings to Float in Pandas</span></h2>
You can use the following methods to convert a string to a float in pandas:
<b>Method 1: Convert a Single Column to Float</b>
<b>#convert "assists" column from string to float
df['assists'] = df['assists'].astype(float)
</b>
<b>Method 2: Convert Multiple Columns to Float</b>
<b>#convert both "assists" and "rebounds" from strings to floats
df[['assists', 'rebounds']] = df[['assists', 'rebounds']].astype(float)</b>
<b>Method 3: Convert All Columns to Float</b>
<b>#convert all columns to float
df = df.astype(float)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [np.nan, 12, 15, 14, 19],   'assists': ['5', np.nan, '7', '9', '12'],   'rebounds': ['11', '8', '10', '6', '6']})  
#view DataFrame
df
 pointsassistsrebounds
0NaN5.011
112.0NaN8
215.07.010
314.09.06
419.012.06
#view column data types
df.dtypes
points      float64
assists      object
rebounds     object
dtype: object</b>
<h2>Example 1: Convert a Single Column to Float</h2>
The following syntax shows how to convert the <b>assists</b> column from a string to a float:
<b>#convert "assists" from string to float
df['assists'] = df['assists'].astype(float)
#view column data types
df.dtypes
points      float64
assists     float64
rebounds     object
dtype: object
</b>
<h2>Example 2: Convert Multiple Columns to Float</h2>
The following syntax shows how to convert both the <b>assists</b> and <b>rebounds</b> columns from strings to floats:
<b>#convert both "assists" and "rebounds" from strings to floats
df[['assists', 'rebounds']] = df[['assists', 'rebounds']].astype(float)
#view column data types
df.dtypes
points      float64
assists     float64
rebounds    float64
dtype: object
</b>
<h2>Example 3: Convert All Columns to Float</h2>
The following syntax shows how to convert all of the columns in the DataFrame to floats:
<b>#convert all columns to float
df = df.astype(float)
#view column data types
df.dtypes
points      float64
assists     float64
rebounds    float64
dtype: object</b>
<h2>Bonus: Convert String to Float and Fill in NaN Values</h2>
The following syntax shows how to convert the <b>assists</b> column from string to float and simultaneously fill in the NaN values with zeros:
<b>#convert "assists" from string to float and fill in NaN values with zeros
df['assists'] = df['assists'].astype(float).fillna(0)
#view DataFrame
df
        pointsassistsrebounds
0NaN5.011
112.00.08
215.07.010
314.09.06
419.012.06</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Convert object to int 
 Pandas: How to Convert Floats to Integers 
 Pandas: How to Convert Specific Columns to NumPy Array 
<h2><span class="orange">How to Convert Strings to Dates in R (With Examples)</span></h2>
Often when you import date and time data into R, values will be imported as strings.
The easiest way to convert strings to dates in R is with the <b>as.Date()</b> function, which uses the following syntax:
<b>as.Date(x, format)</b>
where:
<b>x:</b> A single string value or a vector of string values.
<b>format:</b> The format to use for the date. The default is YYYY-MM-DD.
You can use the <b>?strftime</b> command in R to view a complete list of arguments available to use for the date format, but the most common ones include:
<b>%d:</b> Day of the month as decimal number (01-31)
<b>%m:</b> Month as decimal number (01-12)
<b>%y:</b> Year without century (e.g. 04)
<b>%Y:</b> Year with century (e.g. 2004)
This tutorial shows several examples of how to use the <b>as.Date()</b> function in practice.
<h3>Example 1: Convert a Single String to a Date</h3>
The following code shows how to convert a single string value to a date:
<b>#create string value
x &lt;- c("2021-07-24")
#convert string to date
new &lt;- as.Date(x, format="%Y-%m-%d")
new
[1] "2021-07-24"
#check class of new variable
class(new)
[1] "Date"
</b>
<h3>Example 2: Convert a Vector of Strings to Dates</h3>
The following code shows how to convert a vector of strings to dates:
<b>#create vector of strings
x &lt;- c("2021-07-24", "2021-07-26", "2021-07-30")
#convert string to date
new &lt;- as.Date(x, format="%Y-%m-%d")
new
[1] "2021-07-24" "2021-07-26" "2021-07-30"
#check class of new variable
class(new)
[1] "Date"</b>
<h3>Example 3: Convert a Data Frame Column to Dates</h3>
The following code shows how to convert a data frame column of strings to dates:
<b>#create data frame
df &lt;- data.frame(day = c("2021-07-24", "2021-07-26", "2021-07-30"), sales=c(22, 25, 28), products=c(3, 6, 7))
#view structure of data frame
str(df)
'data.frame':3 obs. of  3 variables:
 $ day     : Factor w/ 3 levels "2021-07-24","2021-07-26",..: 1 2 3
 $ sales   : num  22 25 28
 $ products: num  3 6 7
#convert <i>day</i> variable to date
df$day &lt;- as.Date(df$day, format="%Y-%m-%d")
#view structure of new data frame
str(df)
'data.frame':3 obs. of  3 variables:
 $ day     : Date, format: "2021-07-24" "2021-07-26" ...
 $ sales   : num  22 25 28
 $ products: num  3 6 7
</b>
<h3>Example 4: Convert Multiple Date Frame Columns to Dates</h3>
The following code shows how to convert multiple data frame column of strings to dates:
<b>#create data frame
df &lt;- data.frame(start = c("2021-07-24", "2021-07-26", "2021-07-30"), end = c("2021-07-25", "2021-07-28", "2021-08-02"), products=c(3, 6, 7))
#view structure of data frame
str(df)
'data.frame':3 obs. of  3 variables:
 $ start   : Factor w/ 3 levels "2021-07-24","2021-07-26",..: 1 2 3
 $ end     : Factor w/ 3 levels "2021-07-25","2021-07-28",..: 1 2 3
 $ products: num  3 6 7
#convert <i>start </i>and <em>end</em> variables to date
df[,c('start', 'end')] = lapply(df[,c('start', 'end')],                function(x) as.Date(x, format="%Y-%m-%d"))
#view structure of new data frame
str(df)
'data.frame':3 obs. of  3 variables:
 $ start   : Date, format: "2021-07-24" "2021-07-26" ...
 $ end     : Date, format: "2021-07-25" "2021-07-28" ...
 $ products: num  3 6 7</b>
You can learn more about the<b> lapply()</b> function used in this example  here .
<h2><span class="orange">How to Convert Table to Data Frame in R (With Examples)</span></h2>
You can use the following basic syntax to convert a table to a data frame in R:
<b>df &lt;- data.frame(rbind(table_name))
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Convert Table to Data Frame in R</h3>
First, let’s  create a table  in R:
<b>#create matrix with 4 columns
tab &lt;- matrix(1:8, ncol=4, byrow=TRUE)
#define column names and row names of matrix
colnames(tab) &lt;- c('A', 'B', 'C', 'D')
rownames(tab) &lt;- c('F', 'G')
#convert matrix to table 
tab &lt;- as.table(tab)
#view table 
tab
  A B C D
F 1 2 3 4
G 5 6 7 8
#view class
class(tab)
[1] "table"
</b>
Next, let’s convert the table to a data frame:
<b>#convert table to data frame
df &lt;- data.frame(rbind(tab))
#view data frame
df
  A B C D
F 1 2 3 4
G 5 6 7 8
#view class
class(df)
[1] "data.frame"
</b>
We can see that the table has been converted to a data frame.
Note that we can also use the <b>row.names</b> function to quickly  change the row names  of the data frame as well:
<b>#change row names to list of numbers
row.names(df) &lt;- 1:nrow(df)
#view updated data frame
df
  A B C D
1 1 2 3 4
2 5 6 7 8
</b>
Notice that the row names have been changed from “F” and “G” to 1 and 2.
<h2><span class="orange">How to Convert Z-Scores to Raw Scores (Step-by-Step)</span></h2>
A <b>z-score</b> tells us how many standard deviations away a value is from the mean. We use the following formula to calculate a z-score:
<b>Z-Score = (x – μ) / σ</b>
where:
<b>x:</b> A raw data value
<b>μ:</b> The mean of the dataset
<b>σ:</b> The standard deviation of the dataset
To convert a z-score into a raw score (or “raw data value”), we can use the following formula:
<b>Raw Score = μ + zσ</b>
The following examples show how to convert z-scores to raw scores in practice.
<h3>Example 1: Annual Incomes</h3>
In a certain city, the mean household annual income is $45,000 with a standard deviation of $6,000. 
Suppose a certain household has an annual income with a z-score of 1.5. What is their annual income?
To solve this, we can use the raw score formula:
Raw score = μ + zσ
Raw score = $45,000 + 1.5*$6,000
Raw score = $54,000
A household with a z-score of 1.5 has an annual income of <b>$54,000</b>.
<h3>Example 2: Exam Scores</h3>
For a certain math exam, the mean score is 81 with a standard deviation of 5.
Suppose a certain student has an exam score with a z-score of -2. What is their exam score?
To solve this, we can use the raw score formula:
Raw score = μ + zσ
Raw score = 81+ (-2)*5
Raw score = 71
A student with a z-score of -2 received an exam score of <b>71</b>.
<h3>Example 3: Plant Heights</h3>
The mean height of a certain species of plant is 8 inches with a standard deviation of 1.2 inches.
Suppose a certain plant has a height with a z-score of 0. What is the height of this plant?
To solve this, we can use the raw score formula:
Raw score = μ + zσ
Raw score = 8+ 0*5
Raw score = 8
A plant with a z-score of 0 is <b>8</b> inches tall.
<h2><span class="orange">How to Convert Between Z-Scores and Percentiles in Excel</span></h2>
A <b>z-score </b>tells us how many standard deviations away a certain value is from the mean of a dataset.
A <b>percentile </b>tells us what percentage of observations fall below a certain value in a dataset.
Often we wish to convert between z-scores and percentiles, depending on the type of question we’re trying to answer. Fortunately, Excel has two built-in functions that make it easy to convert between the two.
<h3>How to Convert Z-Scores to Percentiles in Excel</h3>
We can use the following built-in Excel function to convert a z-score to a percentile:
<b>=NORM.S.DIST(z, cumulative)</b>
where:
<b>z</b> = z-score of a certain data value
<b>cumulative</b> = TRUE returns the cumulative distribution function; FALSE returns the probability distribution function. We will use TRUE to calculate percentiles.
For example, here is how to convert a z-score of 1.78 to a percentile:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/zScorePercentileExcel1-1.png">
It turns out that a z-score of 1.78 corresponds to a percentile of roughly <b>96.2</b>. In plain English, this means a data value that has a z-score of 1.78 is larger than roughly 96.2% of all other data values in the dataset.
<h3>How to Convert Percentiles to Z-Scores in Excel</h3>
We can use the following built-in Excel function to convert a percentile to a z-score:
<b>=NORM.S.INV(probability)</b>
where:
<b>probability </b>= the percentile you’re interested in converting.
For example, here is how to convert a percentile of 0.85 to a z-score:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/zScorePercentileExcel2.png">
It turns out that a percentile of 0.85 corresponds to a z-score of roughly <b>1.036</b>. In plain English, this means a data value located at the 85th percentile in a dataset has a z-score of 1.036.
<h3>The Relationship Between Percentiles and Z-Scores</h3>
Z-scores can take on any value between negative infinity and infinity. Percentiles, however, can only take on values between 0 and 100.
A z-score of 0 corresponds to a percentile of exactly 0.50. Thus, any z-score greater than 0 corresponds to a percentile greater than 0.50 and any z-score less than 0 corresponds to a percentile less than 0.50.
Depending on the type of question you’re trying to answer, it might be more helpful to know either the z-score or the percentile.
For example, suppose Jessica gets a 90% on a certain exam. This might seem like a high score, but what if the exam was really easy and her whole class did well on it? To determine how good her score is relative to all of the other scores in the class, we could calculate both the z-score and the percentile of her exam score.
If her exam score corresponds to a <b>z-score </b>of 1.23, this means her exam score was 1.23 standard deviations above the mean exam score. This z-score also correspond to a <b>percentile</b> of about 0.89, which means she scored higher than 89% of her classmates. Notice how both the z-score and the percentile provide us with useful information about her exam score.
<h2><span class="orange">How to Convert Between Z-Scores and Percentiles in R</span></h2>
A <b>z-score </b>tells us how many standard deviations away a certain value is from the mean of a dataset.
A <b>percentile </b>tells us what percentage of  observations  fall below a certain value in a dataset.
Often you may want to convert between z-scores and percentiles.
You can use the following methods to do so in R:
<b>Method 1: Convert Z-Scores to Percentiles</b>
<b>percentile &lt;- pnorm(z)
</b>
<b>Method 2: Convert Percentiles to Z-Scores</b>
<b>z &lt;- qnorm(percentile)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Convert Z-Scores to Percentiles in R</h3>
We can use the built-in <b>pnorm</b> function in R to convert a z-score to a percentile.
For example, here is how to convert a z-score of 1.78 to a percentile:
<b>#convert z-score of 1.78 to percentile
percentile &lt;- pnorm(1.78)
#display percentile
percentile
[1] 0.962462
</b>
It turns out that a z-score of 1.78 corresponds to a percentile of <b>96.2</b>.
We interpret this to mean that a z-score of 1.78 is larger than about <b>96.2%</b> of all other values in the dataset.
<h3>Example 2: Convert Percentiles to Z-Scores in R</h3>
We can use the built-in <b>qnorm</b> function in R to convert a percentile to a z-score.
For example, here is how to convert a percentile of 0.85 to a z-score:
<b>#convert percentile of 0.85 to z-score
z &lt;- qnorm(0.85)
#display z-score
z
[1] 1.036433
</b>
It turns out that a percentile of 0.85 corresponds to a z-score of <b>1.036</b>.
We interpret this to mean that a data value located at the 85th percentile in a dataset has a z-score of <b>1.036</b>.
Also note that we can use the <b>qnorm</b> function to convert an entire vector of percentiles to z-scores:
<b>#define vector of percentiles
p_vector &lt;- c(0.1, 0.35, 0.5, 0.55, 0.7, 0.9, 0.92)
#convert all percentiles in vector to z-scores
qnorm(p_vector)
[1] -1.2815516 -0.3853205  0.0000000  0.1256613  0.5244005  1.2815516  1.4050716
</b>
Here’s how to interpret the output:
A percentile of 0.1 corresponds to a z-score of <b>-1.28</b>.
A percentile of 0.35 correspond to a z-score of <b>-0.38</b>.
A percentile of 0.5 corresponds to a z-score of <b>0</b>.
And so on.
<h2><span class="orange">How to Calculate Cook’s Distance in Python</span></h2>
<b>Cook’s distance</b> is used to identify influential  observations  in a regression model.
The formula for Cook’s distance is:
<b>D<sub>i</sub></b> = (r<sub>i</sub><sup>2</sup> / p*MSE) * (h<sub>ii</sub> / (1-h<sub>ii</sub>)<sup>2</sup>)
where:
<b>r</b><sub><b>i</b> </sub>is the i<sup>th</sup> residual
<b>p </b>is the number of coefficients in the regression model
<b>MSE</b> is the mean squared error
<b>h</b><sub>ii</sub> is the i<sup>th</sup> leverage value
Essentially Cook’s distance measures how much all of the fitted values in the model change when the i<sup>th</sup> observation is deleted.
The larger the value for Cook’s distance, the more influential a given observation.
A general rule of thumb is that any observation with a Cook’s distance greater than 4/n (where <em>n</em> = total observations) is considered to be highly influential.
This tutorial provides a step-by-step example of how to calculate Cook’s distance for a given regression model in Python.
<h3>Step 1: Enter the Data</h3>
First, we’ll create a small dataset to work with in Python:
<b>import pandas as pd
#create dataset
df = pd.DataFrame({'x': [8, 12, 12, 13, 14, 16, 17, 22, 24, 26, 29, 30],   'y': [41, 42, 39, 37, 35, 39, 45, 46, 39, 49, 55, 57]})
</b>
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll fit a  simple linear regression model :
<b>import statsmodels.api as sm
#define response variable
y = df['y']
#define explanatory variable
x = df['x']
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit() </b>
<h3>Step 3: Calculate Cook’s Distance</h3>
Next, we’ll calculate Cook’s distance for each observation in the model:
<b>#suppress scientific notation
import numpy as np
np.set_printoptions(suppress=True)
#create instance of influence
influence = model.get_influence()
#obtain Cook's distance for each observation
cooks = influence.cooks_distance
#display Cook's distances
print(cooks)
(array([0.368, 0.061, 0.001, 0.028, 0.105, 0.022, 0.017, 0.   , 0.343,
        0.   , 0.15 , 0.349]),
 array([0.701, 0.941, 0.999, 0.973, 0.901, 0.979, 0.983, 1.   , 0.718,
        1.   , 0.863, 0.713]))
</b>
By default, the <b>cooks_distance()</b> function displays an array of values for Cook’s distance for each observation followed by an array of corresponding p-values.
For example:
Cook’s distance for observation #1: <b>.368</b> (p-value: .701)
Cook’s distance for observation #2: <b>.061</b> (p-value: .941)
Cook’s distance for observation #3: <b>.001</b> (p-value: .999)
And so on.
<h3>Step 4: Visualize Cook’s Distances</h3>
Lastly, we can create a scatterplot to visualize the values for the predictor variable vs. Cook’s distance for each observation:
<b>import matplotlib.pyplot as plt
plt.scatter(df.x, cooks[0])
plt.xlabel('x')
plt.ylabel('Cooks Distance')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cooksPython1.png">
<h3>Closing Thoughts</h3>
It’s important to note that Cook’s Distance should be used as a way to <em>identify</em> potentially influential observations. Just because an observation is influential doesn’t necessarily mean that it should be deleted from the dataset.
First, you should verify that the observation isn’t a result of a data entry error or some other odd occurrence. If it turns out to be a legit value, you can then decide if it’s appropriate to delete it, leave it be, or simply replace it with an alternative value like the median.
<h2><span class="orange">How to Calculate Correlation Between Categorical Variables</span></h2>
Often we use the  Pearson Correlation Coefficient  to calculate the correlation between continuous numerical variables.
However, we must use a different metric to calculate the correlation between categorical variables – that is, variables that take on names or labels such as:
Marital status (single, married, divorced)
Smoking status (smoker, non-smoker)
Eye color (blue, brown, green)
There are three metrics that are commonly used to calculate the correlation between categorical variables:
<b>1. Tetrachoric Correlation:</b> Used to calculate the correlation between binary categorical variables.
<b>2. Polychoric Correlation:</b> Used to calculate the correlation between ordinal categorical variables.
<b>3. Cramer’s V:</b> Used to calculate the correlation between nominal categorical variables.
The following sections provide an example of how to calculate each of these three metrics.
<h3>Metric 1: Tetrachoric Correlation</h3>
<b>Tetrachoric correlation</b> is used to calculate the correlation between binary categorical variables. Recall that binary variables are variables that can only take on one of two possible values.
The value for tetrachoric correlation ranges from -1 to 1 where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation.
For example, suppose want to know whether or not gender is associated with political party preference so we take a simple random sample of 100 voters and survey them on their political party preference.
The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/tetra2.png">
We would use tetrachoric correlation in this scenario because each categorical variable is binary – that is, each variable can only take on two possible values.
We can use the following code in R to calculate the tetrachoric correlation between the two variables:
<b>library(psych)
#create 2x2 table
data = matrix(c(19, 12, 30, 39), nrow=2)
#view table
data
#calculate tetrachoric correlation
tetrachoric(data)
tetrachoric correlation 
[1] 0.27</b>
The tetrachoric correlation turns out to be <b>0.27</b>. This value is fairly low, which indicates that there is a weak association (if any) between gender and political party preference.
<h3>Metric 2: Polychoric Correlation</h3>
<b>Polychoric correlation</b> is used to calculate the correlation between ordinal categorical variables. Recall that  ordinal variables  are variables whose possible values have a natural order.
The value for polychoric correlation ranges from -1 to 1 where -1 indicates a strong negative correlation, 0 indicates no correlation, and 1 indicates a strong positive correlation.
For example, suppose want to know whether or not two different movie ratings agencies have a high correlation between their movie ratings.
We ask each agency to rate 20 different movies on a scale of 1 to 3 with 1 indicating “bad”, 2 indicating “mediocre”, and 3 indicating “good.”
The following table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/poly11.png">
We can use the following code in R to calculate the polychoric correlation between the ratings of the two agencies:
<b>library(polycor)
#define movie ratings
x &lt;- c(1, 1, 2, 2, 3, 2, 2, 3, 2, 3, 3, 2, 1, 2, 2, 1, 1, 1, 2, 2)
y &lt;- c(1, 1, 2, 1, 3, 3, 3, 2, 2, 3, 3, 3, 2, 2, 2, 1, 2, 1, 3, 3)
#calculate polychoric correlation between ratings
polychor(x, y)
[1] 0.7828328
</b>
The polychoric correlation turns out to be <b>0.78</b>. This value is quite high, which indicates that there is a strong positive association between the ratings from each agency.
<h3>Metric 3: Cramer’s V</h3>
<b>Cramer’s V</b> is used to calculate the correlation between nominal categorical variables. Recall that nominal variables are ones that take on category labels but have no natural ordering.
The value for Cramer’s V ranges from 0 to 1, with 0 indicating no association between the variables and 1 indicating a strong association between the variables.
For example, suppose we want to know if there is a correlation between eye color and gender so we survey 50 individuals and obtain the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/poly12.png">
We can use the following code in R to calculate Cramer’s V for these two variables:
<b>library(rcompanion)
#create table
data = matrix(c(6, 9, 8, 5, 12, 10), nrow=2)
#view table
data
     [,1] [,2] [,3]
[1,]    6    8   12
[2,]    9    5   10
#calculate Cramer's V
cramerV(data)
Cramer V 
  0.1671</b>
Cramer’s V turns out to be <b>0.1671</b>. This value is quite low, which indicates that there is a weak association between gender and eye color.
<h2><span class="orange">How to Calculate Correlation Between Continuous & Categorical Variables</span></h2>
When we would like to calculate the correlation between two continuous variables, we typically use the  Pearson correlation coefficient .
However, when we would like to calculate the correlation between a continuous variable and a  categorical variable , we can use something known as <b>point biserial correlation</b>.
Point biserial correlation is used to calculate the correlation between a binary categorical variable (a variable that can only take on two values) and a continuous variable and has the following properties:
Point biserial correlation can range between -1 and 1.
For each group created by the binary variable, it is assumed that the continuous variable is normally distributed with equal variances.
For each group created by the binary variable, it is assumed that there are no extreme outliers.
The following example shows how to calculate a point biserial correlation in practice.
<h2>Example: Calculating a Point Biserial Correlation</h2>
Suppose a college professor would like to determine if there is a correlation between gender and score on particular aptitude exam.
He collects the following data on 12 males and 12 females in his class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/correlation1.jpg"154">
Since <b>gender</b> is a categorical variable and <b>score</b> is a continuous variable, it makes sense to calculate a point-biserial correlation between the two variables.
The professor can use any statistical software (including Excel, R, Python, SPSS, Stata) to calculate the point-biserial correlation between the two variables.
The following code shows how to calculate the point-biserial correlation in R, using the value 0 to represent females and 1 to represent males for the gender variable:
<b>#define values for gender
gender &lt;- c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1)
#define values for score
score &lt;- c(77, 78, 79, 79, 82, 84, 85, 88, 89, 91, 91, 94,
           84, 84, 84, 85, 85, 86, 86, 86, 89, 91, 94, 98)
#calculate point-biserial correlation
cor.test(gender, score)
Pearson's product-moment correlation
data:  gender and score
t = 1.3739, df = 22, p-value = 0.1833
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.1379386  0.6147832
sample estimates:
      cor 
0.2810996 </b>
From the output we can see that the point biserial correlation coefficient is <b>0.281</b> and the corresponding p-value is <b>0.1833</b>.
Since the correlation coefficient is positive, it tells us that there is a positive correlation between gender and score.
Since we coded the males as 1 and females as 0, this indicates that scores tend to be higher for males (i.e. scores tend to increase as gender “increases” from 0 to 1)
However, since the p-value is not less than .05, this correlation coefficient is not statistically significant.
<h2>Additional Resources</h2>
The following tutorials explain how to calculate point biserial correlation using different statistical software:
 How to Calculate Point-Biserial Correlation in Excel 
 How to Calculate Point-Biserial Correlation in R 
 How to Calculate Point-Biserial Correlation in Python 
<h2><span class="orange">How to Calculate Correlation Between Multiple Variables in R</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
This tutorial explains how to calculate the correlation between multiple variables in R, using the following data frame as an example:
<b>#create data frame
df &lt;- data.frame(a &lt;- c(2, 3, 3, 5, 6, 9, 14, 15, 19, 21, 22, 23), b &lt;- c(23, 24, 24, 23, 17, 28, 38, 34, 35, 39, 41, 43), c &lt;- c(13, 14, 14, 14, 15, 17, 18, 19, 22, 20, 24, 26), d &lt;- c(6, 6, 7, 8, 8, 8, 7, 6, 5, 3, 3, 2))
</b>
<h3>Example 1: Correlation Between Two Variables</h3>
The following code shows how to calculate the correlation between two variables in the data frame:
<b>cor(df$a, df$b)
[1] 0.9279869
</b>
<h3>Example 2: Correlation Between Multiple Variables</h3>
The following code shows how to calculate the correlation between three variables in the data frame:
<b>cor(df[, c('a', 'b', 'c')])
          a         b         c
a 1.0000000 0.9279869 0.9604329
b 0.9279869 1.0000000 0.8942139
c 0.9604329 0.8942139 1.0000000</b>
The way to interpret the output is as follows:
The correlation between <em>a</em> and <em>b</em> is 0.9279869.
The correlation between <em>a</em> and <em>c</em> is 0.9604329.
The correlation between <em>b</em> and <em>c</em> is 0.8942139.
<h3>Example 3: Correlation Between All Variables</h3>
The following code shows how to calculate the correlation between all variables in a data frame:
<b>cor(df)
           a          b          c          d
a  1.0000000  0.9279869  0.9604329 -0.7915488
b  0.9279869  1.0000000  0.8942139 -0.7917973
c  0.9604329  0.8942139  1.0000000 -0.8063549
d -0.7915488 -0.7917973 -0.8063549  1.0000000</b>
<h3>Example 4: Correlation Between Only Numerical Variables</h3>
The following code shows how to calculate the correlation between only the numerical variables in a data frame:
<b>cor(df[,unlist(lapply(df, is.numeric))])
           a          b          c          d
a  1.0000000  0.9279869  0.9604329 -0.7915488
b  0.9279869  1.0000000  0.8942139 -0.7917973
c  0.9604329  0.8942139  1.0000000 -0.8063549
d -0.7915488 -0.7917973 -0.8063549  1.0000000
</b>
<h3>Example 5: Visualize Correlations</h3>
The following code shows how to create a pairs plot – a type of plot that lets you visualize the relationship between each pairwise combination of variables:
<b>#load psych package
library(psych)
#create pairs plot
pairs.panels(df)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/pairs1.png">
<h2><span class="orange">How to Calculate Correlation By Group in R</span></h2>
You can use the following basic syntax to calculate the correlation between two variables by group in R:
<b>library(dplyr)
df %>%
  group_by(group_var) %>%
  summarize(cor=cor(var1, var2))
</b>
This particular syntax calculates the  correlation  between <b>var1</b> and <b>var2</b>, grouped by <b>group_var</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Calculate Correlation By Group in R</h2>
Suppose we have the following data frame that contains information about basketball players on various teams:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'), points=c(18, 22, 19, 14, 14, 11, 20, 28), assists=c(2, 7, 9, 3, 12, 10, 14, 21))
#view data frame
df
  team points assists
1    A     18       2
2    A     22       7
3    A     19       9
4    A     14       3
5    B     14      12
6    B     11      10
7    B     20      14
8    B     28      21
</b>
We can use the following syntax from the  dplyr  package to calculate the correlation between <b>points</b> and <b>assists</b>, grouped by <b>team</b>:
<b>library(dplyr)
df %>%
  group_by(team) %>%
  summarize(cor=cor(points, assists))
# A tibble: 2 x 2
  team    cor
   
1 A     0.603
2 B     0.982
</b>
From the output we can see:
The correlation coefficient between points and assists for team A is <b>.603</b>.
The correlation coefficient between points and assists for team B is <b>.982</b>.
Since both correlation coefficients are positive, this tells us that the relationship between points and assists for both teams is positive.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
<h2><span class="orange">How to Calculate a Pearson Correlation Coefficient by Hand</span></h2>
A  Pearson Correlation Coefficient  measures the linear association between two variables.
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The formula to calculate a Pearson Correlation Coefficient, denoted <em>r</em>, is:
<figure style="width: 323px"><img class="lazy" data-src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b9c2079a3ffc1aacd36201ea0a3fb2460dc226f"323"><figcaption>Source:  Wikipedia </figcaption></figure>This tutorial provides a step-by-step example of how to calculate a Pearson Correlation Coefficient by hand for the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand1.png">
<h3>Step 1: Calculate the Mean of X and Y</h3>
First, we’ll calculate the mean of both the X and Y values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand2.png">
<h3>Step 2: Calculate the Difference Between Means</h3>
Next, we’ll calculate the difference between each of the individual X and Y values and their respective means:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand3-1.png">
<h3>Step 3: Calculate the Remaining Values</h3>
Next, we’ll calculate the remaining values needed to complete the Pearson Correlation Coefficient formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand4.png">
<h3>Step 4: Calculate the Sums</h3>
Next, we’ll calculate the sums of the the last three columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand5.png">
<h3>Step 5: Calculate the Pearson Correlation Coefficient</h3>
Now we’ll simply plug in the sums from the previous step into the formula for the Pearson Correlation Coefficient:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pearsonHand6.png">
The Pearson Correlation Coefficient turns out to be <b>0.947</b>.
Since this value is close to 1, this is an indication that X and Y are strongly positively correlated.
In other words, as the value for X increases the value for Y also increases in a highly predictable fashion.
<h2><span class="orange">How to Calculate a Correlation Coefficient on a TI-84 Calculator</span></h2>
A <b>correlation coefficient</b> is a measure of the linear association between two variables. It can take on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
You can use the following steps to calculate the correlation coefficient between two variables on a TI-84 calculator:
<b>Step 1: Turn on diagnostics.</b>
First, we need to turn on diagnostics. To do so, press 2nd and then press the number 0. This will take us to the CATALOG screen.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI1.png">
Scroll down to DiagnosticOn and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI2.png">
Then press ENTER once more.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI3.png">
The diagnostics are now turned on so that we can calculate the correlation coefficient between two variables.
<b>Step 2: Input the data.</b>
Next, we need to enter the data values for our two variables. Press Stat and then press EDIT. Enter the values for the first variable in column L1 and the values for the second variable in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI4.png">
<b>Step 3: Find the correlation coefficient.</b>
Next, we will calculate the correlation coefficient between the two variables. Press Stat and then scroll over to <b>CALC</b>. Then scroll down to <b>8: Linreg(a+bx)</b> and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI5.png">
For Xlist and Ylist, make sure L1 and L2 are selected since these are the columns we used to input our data. Leave <b>FreqList </b>blank. Scroll down to <b>Calculate </b>and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI6.png">
On the new screen we can see that the correlation coefficient (r) between the two variables is <b>0.9145</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI7.png">
<h3>How to Interpret a Correlation Coefficient</h3>
The following table shows the rule of thumb for interpreting the strength of the relationship between two variables based on the value of <em>r</em>:
<table><tbody>
<tr>
<th><b>Absolute value of <em>r</em></b></th>
<th><b>Strength of relationship</b></th>
</tr>
<tr>
<td>r &lt; 0.25</td>
<td>No relationship</td>
</tr>
<tr>
<td>0.25 &lt; r &lt; 0.5</td>
<td>Weak relationship</td>
</tr>
<tr>
<td>0.5 &lt; r &lt; 0.75</td>
<td>Moderate relationship</td>
</tr>
<tr>
<td>r > 0.75</td>
<td>Strong relationship</td>
</tr>
</tbody></table>
In our example, a correlation coefficient of <b>0.9145 </b>indicates a strong positive relationship between the two variables.
<h2><span class="orange">Correlation Does Not Imply Causation: 5 Real-World Examples</span></h2>
The phrase “<b>correlation does not imply causation</b>” is often used in statistics to point out that correlation between two variables does not necessarily mean that one variable causes the other to occur.
To better understand this phrase, consider the following real-world examples.
<h3>Example 1: Ice Cream Sales & Shark Attacks</h3>
If we collect data for monthly ice cream sales and monthly shark attacks around the United States each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause1.png">
Does this mean that consuming ice cream causes shark attacks?
Not quite. The more likely explanation is that more people consume ice cream and get in the ocean when it’s warmer outside, which explains why these two variables are so highly correlated.
Although ice cream sales and shark attacks are highly correlated, one does not cause the other.
<h3>Example 2: Master’s Degrees vs. Box Office Revenue</h3>
If we collect data for the total number of Master’s degrees issued by universities each year and the total box office revenue generated by year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause2.png">
Does this mean that issuing more Master’s degrees is causing the box office revenue to increase each year?
Not quite. The more likely explanation is that the global population has been increasing each year, which means more Master’s degrees are issued each year and the sheer number of people attending movies each year are both increasing in roughly equal amounts. 
Although these two variables are correlated, one does not cause the other.
<h3>Example 3: Pool Drownings vs. Nuclear Energy Production</h3>
If we collect data for the total number of pool drownings each year and the total amount of energy produced by nuclear power plants each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause3.png">
Does this mean that increased pool drownings are somehow causing more nuclear energy to be produced?
Not exactly. The more likely explanation is that global population has been increasing, which means more people are drowning in pools and nuclear energy production is becoming more viable each year which explains why it has increased.
Although these two variables are highly correlated, one does not cause the other.
<h3>Example 4: Measles Cases vs. Marriage Rate</h3>
If we collect data for the total number of measles cases in the U.S. each year and the marriage rate each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause4.png">
Does this mean that reduced measles cases is causing lower marriage rates?
Not exactly. Instead, the two variables are independent – modern medicine is causing measles cases to drop and fewer people are getting married due to various reasons each year.
Although these two variables are highly correlated, one does not cause the other.
<h3>Example 5: High School Graduates vs. Pizza Consumption</h3>
If we collect data for the total number of high school graduates and total pizza consumption in the U.S. each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause5.png">
Does this mean that an increased number of high school graduates is leading to more pizza consumption in the United States?
Not quite. The more likely explanation is that U.S. population has been increasing over time, which means that the number of people receiving a high school degree and the total pizza being consumed are both increasing as population increases.
Although these two variables are correlated, one does not cause the other.
<h2><span class="orange">6 Examples of Correlation in Real Life</span></h2>
In statistics, <b>correlation</b> is a measure of the linear relationship between two variables.
The value for a correlation coefficient is always between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The following examples illustrate real-life scenarios of negative, positive, and no correlation between variables.
<h3>Negative Correlation Examples</h3>
<b>Example 1: Time Spent Running vs. Body Fat</b>
The more time an individual spends running, the lower their body fat tends to be. In other words, the variable running time and the variable body fat have a negative correlation. As time spent running increases, body fat decreases.
If we created a scatterplot of time spent running vs. body fat, it may look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/corReal1.png">
<b>Example 2: Time Spent Watching TV vs. Exam Scores</b>
The more time a student spends watching TV, the lower their exam scores tend to be. In other words, the variable time spent watching TV and the variable exam score have a negative correlation. As time spent watching TV increases, exam scores decrease.
If we created a scatterplot of time spent watching TV vs. exam scores, it may look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/corReal2.png">
<h3>Positive Correlation Examples</h3>
<b>Example 1: Height vs. Weight</b>
The correlation between the height of an individual and their weight tends to be positive. In other words, individuals who are taller also tend to weigh more.
If we created a scatterplot of height vs. weight, it may look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/corReal3.png">
<b>Example 2: Temperature vs. Ice Cream Sales</b>
The correlation between the temperature and total ice cream sales is positive. In other words, when it’s hotter outside the total ice cream sales of companies tends to be higher since more people buy ice cream when it’s hot out.
If we created a scatterplot of temperature vs. ice cream sales, it may look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/corReal4.png">
<h3>No Correlation Examples</h3>
<b>Example 1: Coffee Consumption vs. Intelligence</b>
The amount of coffee that individuals consume and their IQ level has a correlation of zero. In other words, knowing how much coffee an individual drinks doesn’t give us an idea of what their IQ level might be.
If we created a scatterplot of daily coffee consumption vs. IQ level, it may look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation2.png">
<b>Example 2: Shoe Size vs. Movies Watched</b>
The shoe size of individuals and the number of movies they watch per year has a correlation of zero. In other words, knowing the shoe size of an individual doesn’t give us an idea of how many movies they watch per year.
If we created a scatterplot of shoe size vs. number of movies watched, it may look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation4.png">
<h2><span class="orange">How to Create a Correlation Graph in Excel (With Example)</span></h2>
Often you may want to create a graph in Excel that allows you to visualize the  correlation  between two variables.
This tutorial provides a step-by-step example of how to create this type of correlation graph in Excel.
<h2>Step 1: Create the Data</h2>
First, let’s create a dataset with two variables in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/corrgraph1.jpg"441">
<h2>Step 2: Create a Scatterplot</h2>
Next, highlight the cell range <b>A2:B21</b>.
On the top ribbon, click the <b>Insert </b>tab, then click <b>Insert Scatter (X, Y) </b>in the <b>Charts</b> group and click the first option to create a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine2.png">
The following scatterplot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/corrgraph2.jpg"659">
<h2>Step 3: Add Correlation Coefficient</h2>
To calculate the correlation coefficient between the two variables, type the following formula into cell <b>A23</b>:
<b>=CORREL(A2:A21, B2:B21)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/corrgraph4.jpg"411">
The correlation coefficient between these two variables is <b>0.9835</b>.
Feel free to add this value in the title of the scatterplot if you’d like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/corrgraph5.jpg">
Note that the correlation between two variables can range between -1 and 1 where:
<b>-1</b> indicates a perfect negative linear correlation
<b>0</b> indicates no linear correlation
<b>1</b> indicates a perfect positive linear correlation
In our example, a correlation of <b>0.9835</b> represents a strong positive correlation between the two variables.
This matches the pattern that we see in the scatterplot: As the value for x increases, the value for y also increases in a highly predictable manner.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Create a Scatterplot Matrix in Excel 
 How to Add Labels to Scatterplot Points in Excel 
 How to Add a Horizontal Line to a Scatterplot in Excel 
 How to Create a Scatterplot with Multiple Series in Excel 
<h2><span class="orange">How to Create a Correlation Heatmap in R (With Example)</span></h2>
You can use the following basic syntax to create a correlation heatmap in R:
<b>#calculate correlation between each pairwise combination of variables
cor_df &lt;- round(cor(df), 2)
#melt the data frame
melted_cormat &lt;- melt(cor_df)
#create correlation heatmap
ggplot(data = melted_cormat, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), size = 5) +
  scale_fill_gradient2(low = "blue", high = "red",       limit = c(-1,1), name="Correlation") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.background = element_blank())
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Create Correlation Heatmap in R</h2>
Suppose we have the following data frame in R that shows various statistics for eight different basketball players:
<b>#create data frame
df &lt;- data.frame(points=c(22, 25, 30, 16, 14, 18, 29, 22), assists=c(4, 4, 5, 7, 8, 6, 7, 12), rebounds=c(10, 7, 7, 6, 8, 5, 4, 3), blocks=c(12, 4, 4, 6, 5, 3, 8, 5))
#view data frame
df
  points assists rebounds blocks
1     22       4       10     12
2     25       4        7      4
3     30       5        7      4
4     16       7        6      6
5     14       8        8      5
6     18       6        5      3
7     29       7        4      8
8     22      12        3      5</b>
Suppose we would like to create a correlation heatmap to visualize the  correlation coefficient  between each pairwise combination of variables in the data frame.
Before we create the correlation heatmap, we must first calculate the correlation coefficient between each variable using <b>cor()</b> and then transform the results into a usable format using the <b>melt()</b> function from the <b>reshape2</b> package:
<b>library(reshape2)
#calculate correlation coefficients, rounded to 2 decimal places
cor_df &lt;- round(cor(df), 2)
#melt the data frame
melted_cor &lt;- melt(cor_df)
#view head of melted data frame
head(melted_cor)
      Var1    Var2 value
1   points  points  1.00
2  assists  points -0.27
3 rebounds  points -0.16
4   blocks  points  0.10
5   points assists -0.27
6  assists assists  1.00</b>
Next, we can use the <b>geom_tile()</b> function from the <b>ggplot2</b> package to create correlation heatmap:
<b>library(ggplot2)
#create correlation heatmap
ggplot(data = melted_cor, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), size = 5) +
  scale_fill_gradient2(low = "blue", high = "red",       limit = c(-1,1), name="Correlation") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.background = element_blank())</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/correlationheatmap1.jpg">
The result is a correlation heatmap that allows us to visualize the correlation coefficient between each pairwise combination of variables.
In this particular heatmap, the correlation coefficients take on the following colors:
<b>Blue</b> if they are close to <b>-1</b>
<b>White</b> if they are close to <b>0</b>
<b>Red</b> if they are close to <b>1</b>
Feel free to use whatever colors you’d like for the <b>low</b> and <b>high</b> arguments within the <b>scale_fill_gradient2()</b> function. 
For example, you could instead use “red” for the low value and “green” for the high value:
<b>library(ggplot2)
#create correlation heatmap
ggplot(data = melted_cor, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() +
  geom_text(aes(Var2, Var1, label = value), size = 5) +
  scale_fill_gradient2(low = "red", high = "green",       limit = c(-1,1), name="Correlation") +
  theme(axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        panel.background = element_blank())</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/correlationheatmap2.jpg">
<b>Note</b>: You can also specify hex color codes to use if you’d like even more control over the exact colors in the correlation heatmap.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Rotate Axis Labels in ggplot2 
 How to Set Axis Breaks in ggplot2 
 How to Set Axis Limits in ggplot2 
 How to Change Legend Labels in ggplot2 
<h2><span class="orange">How to Calculate Correlation in Python</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
This tutorial explains how to calculate the correlation between variables in Python.
<h3>How to Calculate Correlation in Python</h3>
To calculate the correlation between two variables in Python, we can use the Numpy <b>corrcoef()</b> function.
<b>import numpy as np
np.random.seed(100)
#create array of 50 random integers between 0 and 10
var1 = np.random.randint(0, 10, 50)
#create a positively correlated array with some random noise
var2 = var1 + np.random.normal(0, 10, 50)
#calculate the correlation between the two arrays
np.corrcoef(var1, var2)
[[ 1. 0.335]
[ 0.335 1. ]]
</b>
We can see that the correlation coefficient between these two variables is <b>0.335</b>, which is a positive correlation.
By default, this function produces a matrix of correlation coefficients. If we only wanted to return the correlation coefficient between the two variables, we could use the following syntax:
<b>np.corrcoef(var1, var2)[0,1]
0.335
</b>
To test if this correlation is statistically significant, we can calculate the p-value associated with the Pearson correlation coefficient by using the Scipy <b>pearsonr()</b> function, which returns the Pearson correlation coefficient along with the two-tailed p-value.
<b>from scipy.stats.stats import pearsonr
pearsonr(var1, var2)
(0.335, 0.017398)
</b>
The correlation coefficient is <b>0.335 </b>and the two-tailed  p-value is <b>.017</b>. Since this p-value is less than .05, we would conclude that there is a statistically significant correlation between the two variables.
If you’re interested in calculating the correlation between several variables in a Pandas DataFrame, you can simpy use the <b>.corr() </b>function.
<b>import pandas as pd
data = pd.DataFrame(np.random.randint(0, 10, size=(5, 3)), columns=['A', 'B', 'C'])
data
  A B C
0 8 0 9
1 4 0 7
2 9 6 8
3 1 8 1
4 8 0 8
#calculate correlation coefficients for all pairwise combinations
data.corr()
          A         B         C
A  1.000000 -0.775567 -0.493769
B -0.775567  1.000000  0.000000
C -0.493769  0.000000  1.000000
</b>
And if you’re only interested in calculating the correlation between two specific variables in the DataFrame, you can specify the variables:
<b>data['A'].corr(data['B'])
-0.775567
</b>
<h2><span class="orange">How to Calculate Correlation in SAS (With Examples)</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which measures the linear association between two variables<em>.</em>
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
The following examples show how to use <b>proc corr</b> in SAS to calculate the correlation coefficient between variables in the SAS built-in dataset called  Fish , which contains various measurements for 159 different fish caught in a lake in Finland.
We can use <b>proc print</b> to view the first 10 observations from this dataset:
<b>/*view first 10 observations from <em>Fish</em> dataset*/
proc print data=sashelp.Fish (obs=10);
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/fish1.jpg"492">
<h3>Example 1: Correlation Between Two Variables</h3>
We can use the following code to calculate the Pearson correlation coefficient between the variables Height and Width:
<b>/*calculate correlation coefficient between Height and Width*/
proc corr data=sashelp.fish;
var Height Width;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/corrSAS1.jpg"434">
The first table displays summary statistics for both Height and Width.
The second table displays the Pearson correlation coefficient between the two variables, including a  p-value  that tells us if the correlation is statistically significant.
From the output we can see:
Pearson correlation coefficient: <b>0.79288</b>
P-value: <b>&lt;.0001</b>
This tells us that there is a strong positive correlation between Height and Width and that the correlation is statistically significant since the p-value is less than α = .05.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
<h3>Example 2: Correlation Between All Variables</h3>
We can use the following code to calculate the Pearson correlation coefficient between all pairwise combinations of variables in the dataset:
<b>/*calculate correlation coefficient between all pairwise combinations of variables*/
proc corr data=sashelp.fish;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/corrSAS2.jpg">
The output shows a  correlation matrix , which contains the Pearson correlation coefficient and corresponding p-values for each pairwise combination of numeric variables in the dataset.
For example:
The Pearson correlation coefficient between Weight and Length1 is <b>0.91644</b>
The Pearson correlation coefficient between Weight and Length2 is <b>0.91937</b>
The Pearson correlation coefficient between Weight and Length3 is <b>0.92447</b>
And so on.
<h3>Example 3: Visualize Correlation with a Scatterplot</h3>
We can also use the <b>plots</b> function to create a scatterplot to visualize the correlation between two variables:
<b>/*visualize correlation between Height and Width*/
proc corr data=sashelp.fish plots=scatter(nvar=all);;
var Height Width;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/corrSAS3.jpg"569">
From the plot we can see the strong positive correlation between Height and Width. As Height increases, Width tends to increase as well.
In the top left corner of the plot we can also see the total observations used, the correlation coefficient, and the p-value for the correlation coefficient.
<h2><span class="orange">Correlation Matrix Calculator</span></h2>
This calculator creates a correlation matrix for up to five different variables.
Simply enter the data values for up to five variables into the boxes below, then press the “Calculate” button.
<b>Variable 1</b>
<textarea id="a" rows="3" cols="40">11, 12, 12, 14, 16, 19, 19, 20, 21, 21</textarea>
<b>Variable 2</b>
<textarea id="b" rows="3" cols="40">16, 16, 16, 17, 18, 19, 22, 24, 25, 32</textarea>
<b>Variable 3</b>
<textarea id="c" rows="3" cols="40">18, 18, 19, 21, 21, 22, 22, 23, 24, 26</textarea>
<b>Variable  4</b>
<textarea id="d" rows="3" cols="40"></textarea>
<b>Variable  5</b>
<textarea id="e" rows="3" cols="40"></textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Correlation Matrix</b>
<table><tbody>
<tr>
<th><b></b></th>
            <th style="background-color: #bee3ff"><b>Var1</b></th>
            <th style="background-color: #bee3ff"><b>Var2</b></th>
            <th style="background-color: #bee3ff"><b>Var3</b></th>
            <th style="background-color: #bee3ff"><b>Var4</b></th>
            <th style="background-color: #bee3ff"><b>Var5</b></th>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var1</b></td>
            <td>1.0000</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var2</b></td>
            <td>0.8476</td>
            <td>1.0000</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var3</b></td>
            <td>0.9378</td>
            <td>0.9305</td>
            <td>1.0000</td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var4</b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var5</b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
</tbody></table>
<script>
var div_table = document.getElementById('words_table');
function calc() {
//get raw data for each group
var v1  = document.getElementById('a').value.split(',').map(Number);
var v2  = document.getElementById('b').value.split(',').map(Number);
var v3  = document.getElementById('c').value.split(',').map(Number);
var v4  = document.getElementById('d').value.split(',').map(Number);
var v5 = document.getElementById('e').value.split(',').map(Number);
if (v1.length >1){
var cor11 = jStat.corrcoeff(v1, v1);
document.getElementById('cor11').innerHTML = cor11.toFixed(4);
};
if (v2.length >1){
var cor21 = jStat.corrcoeff(v2, v1);
var cor22 = jStat.corrcoeff(v2, v2);
document.getElementById('cor21').innerHTML = cor21.toFixed(4);
document.getElementById('cor22').innerHTML = cor22.toFixed(4);
};
if (v3.length >1){
var cor31 = jStat.corrcoeff(v3, v1);
var cor32 = jStat.corrcoeff(v3, v2);
var cor33 = jStat.corrcoeff(v3, v3);
document.getElementById('cor31').innerHTML = cor31.toFixed(4);
document.getElementById('cor32').innerHTML = cor32.toFixed(4);
document.getElementById('cor33').innerHTML = cor33.toFixed(4);
};
if (v4.length >1){
var cor41 = jStat.corrcoeff(v4, v1);
var cor42 = jStat.corrcoeff(v4, v2);
var cor43 = jStat.corrcoeff(v4, v3);
var cor44 = jStat.corrcoeff(v4, v4);
document.getElementById('cor41').innerHTML = cor41.toFixed(4);
document.getElementById('cor42').innerHTML = cor42.toFixed(4);
document.getElementById('cor43').innerHTML = cor43.toFixed(4);
document.getElementById('cor44').innerHTML = cor44.toFixed(4);
};
if (v5.length >1){
var cor51 = jStat.corrcoeff(v5, v1);
var cor52 = jStat.corrcoeff(v5, v2);
var cor53 = jStat.corrcoeff(v5, v3);
var cor54 = jStat.corrcoeff(v5, v4);
var cor55 = jStat.corrcoeff(v5, v5);
document.getElementById('cor51').innerHTML = cor51.toFixed(4);
document.getElementById('cor52').innerHTML = cor52.toFixed(4);
document.getElementById('cor53').innerHTML = cor53.toFixed(4);
document.getElementById('cor54').innerHTML = cor54.toFixed(4);
document.getElementById('cor55').innerHTML = cor55.toFixed(4);
};
} //end massive calculator script
</script>
<h2><span class="orange">How to Create and Interpret a Correlation Matrix in Excel</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>.</em>
It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
But in some cases we want to understand the correlation between more than just one pair of variables. 
In these cases, we can create a  correlation matrix , which is a square table that shows the the correlation coefficients between several pairwise combination of variables. 
This tutorial explains how to create and interpret a correlation matrix in Excel.
<h3>How to Create a Correlation Matrix in Excel</h3>
Suppose we have the following dataset that shows the average numbers of points, rebounds, and assists for 10 basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrMatrixExcel1.png">
To create a correlation matrix for this dataset, go to the <b>Data</b> tab along the top ribbon of Excel and click <b>Data Analysis</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
<em>If you don’t see this option, then you need to first  load the free Data Analysis Toolpak in Excel </em>.
In the new window that pops up, select <b>Correlation </b>and click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrMatrixExcel2.png">
For <b>Input Range</b>, select the cells where the data is located (including the first row with the labels). Check the box next to <b>Labels in first row</b>. For <b>Output Range</b>, select a cell where you’d like the correlation matrix to appear. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrMatrixExcel3.png">
This will automatically produce the following correlation matrix:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrMatrixExcel4.png">
<h3>How to Interpret a Correlation Matrix in Excel</h3>
The values in the individual cells of the correlation matrix tell us the Pearson Correlation Coefficient between each pairwise combination of variables. For example:
<b>Correlation between Points and Rebounds: </b>-0.04639. Points and rebounds are slightly negatively correlated, but this value is so close to zero that there isn’t strong evidence for a significant association between these two variables.
<b>Correlation between Points and Assists: </b>0.121871. Points and assists are slightly positively correlated, but this value also is fairly close to zero so there isn’t strong evidence for a significant association between these two variables.
<b>Correlation between Rebounds and Assists: </b>0.713713. Rebounds and assists are strongly positively correlated. That is, players who have more rebounds also tend to have more assists.
Notice that the diagonal values in the correlation matrix are all equal to 1 because the correlation between a variable and itself is always 1. In practice, this number isn’t useful to interpret.
<h3>Bonus: Visualizing Correlation Coefficients</h3>
One easy way to visualize the value of the correlation coefficients in the table is to apply <b>Conditional Formatting </b>to the table.
Along the top ribbon in Excel, go to the <b>Home </b>tab, then the <b>Styles </b>group.
Click <b>Conditional Formatting Chart</b>, then click <b>Color Scales</b>, then click the <b>Green-Yellow-Red Color Scale</b>.
This automatically applies the following color scale to the correlation matrix:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrMatrixExcel5.png">
This helps us easily visualize the strength of the correlations between the variables.
This is a particularly helpful trick if we’re working with a correlation matrix that has a lot of variables because it helps us quickly identify the variables that have the strongest correlations.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
<h2><span class="orange">How to Create a Correlation Matrix in Google Sheets</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
But in some cases we want to understand the correlation between more than just one pair of variables. In these cases, we can create a  correlation matrix , which is a square table that shows the the correlation coefficients between several pairwise combination of variables. 
This tutorial explains how to create and interpret a correlation matrix in Google Sheets.
<h3>How to Create a Correlation Matrix in Google Sheets</h3>
Suppose we have the following dataset that shows the average numbers of points, rebounds, and assists for 10 basketball players:
To create a correlation matrix for this dataset, we can use the <b>CORREL() </b>function with the following syntax:
 <b>COVAR(data_y, data_x) </b>
The covariance matrix for this dataset is shown in cells <b>B15:D17 </b>while the formulas used to create the covariance matrix are shown in cells <b>B21:D23 </b>below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/corMatrixSheets1.png">
<h3>How to Interpret a Correlation Matrix</h3>
The values in the individual cells of the correlation matrix tell us the Pearson Correlation Coefficient between each pairwise combination of variables. For example:
<b>Correlation between Points and Rebounds: </b>-0.0464. Points and rebounds are slightly negatively correlated, but this value is so close to zero that there isn’t strong evidence for a significant association between these two variables.
<b>Correlation between Points and Assists: </b>0.1219. Points and assists are slightly positively correlated, but this value also is fairly close to zero so there isn’t strong evidence for a significant association between these two variables.
<b>Correlation between Rebounds and Assists: </b>0.7137. Rebounds and assists are strongly positively correlated. That is, players who have more rebounds also tend to have more assists.
Notice that the diagonal values in the correlation matrix are all equal to 1 because the correlation between a variable and itself is always 1. In practice, this number isn’t useful to interpret.
<h2><span class="orange">How to Create a Correlation Matrix in R (4 Examples)</span></h2>
A  correlation matrix  is a square table that shows the  correlation coefficients  between variables in a dataset.
It offers a quick way to understand the strength of the linear relationships that exist between variables in a dataset.
There are four common ways to create a correlation matrix in R:
<b>Method 1: The cor Function (For getting simple matrix of correlation coefficients)</b>
<b>cor(df)</b>
<b>Method 2: The rcorr Function (For getting p-values of correlation coefficients)</b>
<b>library(Hmisc)
rcorr(as.matrix(df))</b>
<b>Method 3: The corrplot Function (For visualizing correlation matrix)</b>
<b>library(corrplot)
corrplot(cor(df))
</b>
<b>Method 4: The ggcorrplot Function (For visualizing correlation matrix)</b>
<b>library(ggcorrplot)
ggcorrplot(cor(df))</b>
The following examples show how to use each method with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(assists=c(4, 5, 5, 6, 7, 8, 8, 10), rebounds=c(12, 14, 13, 7, 8, 8, 9, 13), points=c(22, 24, 26, 26, 29, 32, 20, 14))
#view data frame
df
  assists rebounds points
1       4       12     22
2       5       14     24
3       5       13     26
4       6        7     26
5       7        8     29
6       8        8     32
7       8        9     20
8      10       13     14
</b>
<h3>Example 1: The cor Function</h3>
We can use the <b>cor()</b> function from base R to create a correlation matrix that shows the correlation coefficients between each variable in our data frame:
<b>#create correlation matrix
cor(df)
            assists   rebounds     points
assists   1.0000000 -0.2448608 -0.3295730
rebounds -0.2448608  1.0000000 -0.5220917
points   -0.3295730 -0.5220917  1.0000000
</b>
The correlation coefficients along the diagonal of the table are all equal to 1 because each variable is perfectly correlated with itself.
All of the other correlation coefficients indicate the correlation between different pairwise combinations of variables. For example:
The correlation coefficient between assists and rebounds is <b>-0.245</b>.
The correlation coefficient between assists and points  is <b>-0.330</b>.
The correlation coefficient between rebounds and points  is <b>-0.522</b>.
<h3>Example 2: The rcorr Function</h3>
We can use the <b>rcorr()</b> function from the <b>Hmisc</b> package in R to create a correlation matrix that shows the correlation coefficients between each variable in our data frame:
<b>library(Hmisc)
#create matrix of correlation coefficients and p-values
rcorr(as.matrix(df))
         assists rebounds points
assists     1.00    -0.24  -0.33
rebounds   -0.24     1.00  -0.52
points     -0.33    -0.52   1.00
n= 8 
P
         assists rebounds points
assists          0.5589   0.4253
rebounds 0.5589           0.1844
points   0.4253  0.1844 </b>
The first matrix shows the correlation coefficients between the variables and the second matrix shows the corresponding p-values.
For example, the correlation coefficient between assists and rebounds is <b>-0.24</b> and the p-value for this correlation coefficient is <b>0.5589</b>.
This tells us that the correlation between the two variables is negative but it’s not a statistically significant correlation since the p-value is not less than .05.
<h3>Example 3: The corrplot Function</h3>
We can use the <b>corrplot()</b> function from the <b>corrplot</b> package in R to visual the correlation matrix:
<b>library(corrplot)
#visualize correlation matrix
corrplot(cor(df))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/corrplot1.jpg"363">
The color and size of the circles in the correlation matrix help us visualization the correlations between each variable.
For example, the circle where the assists and rebounds variables intersect is small and light red, which tells us that the correlation is low and negative.
<h3>Example 4: The corrplot Function</h3>
We can use the <b>ggcorrplot()</b> function from the <b>ggcorrplot</b> package in R to visualize the correlation matrix:
<b>library(ggcorrplot)
#visualize correlation matrix
ggcorrplot(cor(df))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/corrplot2.jpg"429">
The color of the squares in the correlation matrix help us visualization the correlations between each variable.
<h2><span class="orange">How to Create a Correlation Matrix in MATLAB</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>.</em>
It takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation.
0 indicates no linear correlation.
1 indicates a perfectly positive linear correlation.
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
But in some cases we want to understand the correlation between more than just one pair of variables. 
In these cases, we can create a  correlation matrix , which is a square table that shows the the correlation coefficients between several pairwise combination of variables.
This tutorial explains how to create and interpret a correlation matrix in Matlab.
<h3>How to Create a Correlation Matrix in Matlab</h3>
Use the following steps to create a correlation matrix in Matlab.
<b>Step 1: Create the dataset.</b>
<b>rng(0);
A = randn(10,1);
B = randn(10,1);
C = randn(10,1);
all = [A B C];
</b>
<b>Step 2: Create the correlation matrix.</b>
<b>R = corrcoef(all)
R =
   1.0000      0.4518    -0.5003
   0.4518      1.0000    -0.8017
  -0.5003     -0.8017     1.0000</b>
<b>Step 3: Interpret the correlation matrix.</b>
The correlation coefficients along the diagonal of the table are all equal to 1 because each variable is perfectly correlated with itself.
All of the other correlation coefficients indicate the correlation between different pairwise combinations of variables. For example:
The correlation coefficient between ‘a’ and ‘b’ is <b>0.4518</b>.
The correlation coefficient between ‘a’ and ‘c’ is <b>-0.5003</b>.
The correlation coefficient between ‘b’ and ‘c’  is <b>-0.8017.</b>
<b>Step 4: Find the p-values of the correlation coefficients.</b>
<b>[R,P] = corrcoef(all)
R =
   1.0000      0.4518    -0.5003
   0.4518      1.0000    -0.8017
  -0.5003     -0.8017     1.0000
P =
   1.0000      0.1899     0.1408
   0.1899      1.0000     0.0053
   0.1408      0.0053     1.0000
</b>
The way to interpret the p-values is as follows:
The p-value for the correlation coefficient between ‘a’ and ‘b’ is <b>0.1899</b>.
The p-value for the correlation coefficient between ‘a’ and ‘c’ is <b> 0.1408</b>.
The p-value for the correlation coefficient between ‘b’ and ‘c’  is <b>0.0053.</b>
If the  p-value  is less than some significance level (e.g. 0.05) then we can say that the correlation between the two variables is statistically significant.
In this case, the correlation between variables ‘b’ and ‘c’ is the only statistically significant correlation.
<h2><span class="orange">How to Create a Correlation Matrix in Python</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>.</em>
It takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation.
0 indicates no linear correlation.
1 indicates a perfectly positive linear correlation.
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
But in some cases we want to understand the correlation between more than just one pair of variables. In these cases, we can create a  correlation matrix , which is a square table that shows the the correlation coefficients between several pairwise combination of variables. 
This tutorial explains how to create and interpret a correlation matrix in Python.
<h3>How to Create a Correlation Matrix in Python</h3>
Use the following steps to create a correlation matrix in Python.
<b>Step 1: Create the dataset.</b>
<b>import pandas as pd
data = {'assists': [4, 5, 5, 6, 7, 8, 8, 10],
        'rebounds': [12, 14, 13, 7, 8, 8, 9, 13],
        'points': [22, 24, 26, 26, 29, 32, 20, 14]
        }
df = pd.DataFrame(data, columns=['assists','rebounds','points'])
df
   assist  rebounds  points
041222
151424
251326
36726
47829
58832
68920
7101314
</b>
<b>Step 2: Create the correlation matrix.</b>
<b>#create correlation matrix
df.corr()
                assists   rebounds     points
assists        1.000000  -0.244861  -0.329573
rebounds      -0.244861   1.000000  -0.522092
points        -0.329573  -0.522092   1.000000
#create same correlation matrix with coefficients rounded to 3 decimals 
df.corr().round(3)
       assistsrebounds  points
assists         1.000  -0.245  -0.330
rebounds-0.245   1.000  -0.522
points        -0.330  -0.522   1.000
</b>
<b>Step 3: Interpret the correlation matrix.</b>
The correlation coefficients along the diagonal of the table are all equal to 1 because each variable is perfectly correlated with itself.
All of the other correlation coefficients indicate the correlation between different pairwise combinations of variables. For example:
The correlation coefficient between assists and rebounds is <b>-0.245</b>.
The correlation coefficient between assists and points  is <b>-0.330</b>.
The correlation coefficient between rebounds and points  is <b>-0.522</b>.
<b>Step 4: Visualize the correlation matrix (optional).</b>
You can visualize the correlation matrix by using the  styling options  available in pandas:
<b>corr = df.corr()
corr.style.background_gradient(cmap='coolwarm')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/correlationMatrixpython1.png">
You can also change the argument of <b>cmap </b>to produce a correlation matrix with different colors.
<b>corr = df.corr()
corr.style.background_gradient(cmap='RdYlGn')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/correlationMatrixPython2.png">
<b>corr = df.corr()
corr.style.background_gradient(cmap='bwr')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/correlationMatrixPython3.png">
<b>corr = df.corr()
corr.style.background_gradient(cmap='PuOr')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/correlationMatrixPython4.png">
<b>Note</b>: For a complete list of <b>cmap</b> arguments, refer to the  matplotlib documentation .
<h2><span class="orange">How to Create a Correlation Matrix in SPSS</span></h2>
A  correlation matrix  is a square table that shows the Pearson correlation coefficients between different variables in a dataset.
As a quick refresher, the  Pearson correlation coefficient  is a measure of the linear association between two variables<em>. </em>It takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
This tutorial explains how to create and interpret a correlation matrix in SPSS.
<h3>Example: How to Create a Correlation Matrix in SPSS</h3>
Use the following steps to create a correlation matrix for this dataset that shows the average assists, rebounds, and points for eight basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS1.png">
<b>Step 1: Select bivariate correlation.</b>
Click the <b>Analyze </b>tab.
Click <b>Correlate</b>.
Click <b>Bivariate</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS2.png">
<b>Step 2: Create the correlation matrix.</b>
Each variable in the dataset will initially be shown in the box on the left:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS3.png">
Select each variable you’d like to include in the correlation matrix and click the arrow to transfer them into the <b>Variables </b>box. We’ll use all three variables in this example.
Under <b>Correlation Coefficients</b>, choose whether you’d like to use Pearson, Kendall’s tau, or Spearman correlation. We’ll leave it as Pearson for this example.
Under <b>Test of Significance</b>, choose whether to use a two-tailed test or one-tailed test to determine if two variables have a statistically significant association. We’ll leave it as Two-tailed.
Check the box next to <b>Flag significant correlations </b>if you’d like SPSS to flag variables that are significantly correlated.
Lastly, click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS4.png">
Once you click <b>OK</b>, the following correlation matrix will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS5.png">
<b>Step 3: Interpret the correlation matrix.</b>
The correlation matrix displays the following three metrics for each variable:
<b>Pearson Correlation: </b>A measure of the linear association between two variables, ranging from -1 to 1.
<b>Sig. (2-tailed): </b>The two-tailed p-value associated with the correlation coefficient. This tells you if two variables have a statistically significant association (e.g. if p &lt; 0.05)
<b>N: </b>The number of pairs used to calculate the Pearson Correlation coefficient.
For example, here’s how to interpret the output for the variable Assists:
The Pearson correlation coefficient between Assists and Rebounds is <b>-.245</b>. Since this number is negative, it means these two variables have a negative association.
The p-value associated with the Pearson correlation coefficient for Assists and Rebounds is <b>.559</b>. Since this value is not less than 0.05, the two variables don’t have a statistically significant association.
The number of pairs used to calculate the Pearson correlation coefficient was<b> 8</b> (e.g. 8 pairs of players were used in this calculation).
<b>Step 4: Visualize the correlation matrix.</b>
You can also create a scatterplot matrix to visualize the linear relationship between each of the variables.
Click the <b>Graphs</b> tab.
Click <b>Chart Builder</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS6.png">
For chart type, click <b>Scatter/Dot</b>.
Click the image that says <b>Scatterplot matrix</b>.
In the <b>Variables</b> box in the top left, hold Ctrl and click on all three variable names. Drag them to the box along the bottom of the chart that says <b>Scattermatrix</b>.
Lastly, click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS7.png">
The following scatterplot matrix will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/corrSPSS8.png">
Each individual scatterplot shows the pairwise combinations between two variables. For example, the scatterplot in the bottom left corner shows the pairwise combinations for Points and Assists for each of the 8 players in the dataset.
A scatterplot matrix is optional, but it does offer a nice way to visualize the relationship between each pairwise combination of variables in a dataset.
<h2><span class="orange">How to Create a Correlation Matrix in Stata</span></h2>
In statistics, we’re often interested in understanding the relationship between two variables. For example, we might want to understand the relationship between the number of hours a student studies and the exam score they receive.
One way to quantify this relationship is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
But in some cases we want to understand the correlation between more than just one pair of variables. In these cases, we can create a  <b>correlation matrix</b> , which is a square table that shows the the correlation coefficients between several pairwise combination of variables. 
In this tutorial we explain how to create a correlation matrix in Stata.
<h2>How to Create a Correlation Matrix in Stata</h2>
The command <b>corr </b>can be used to produce a correlation matrix for a particular dataset in Stata.
To illustrate this, let’s load the 1980 census data into Stata by typing the following into the command box:
<b>use http://www.stata-press.com/data/r13/census13</b>
We can then get a quick summary of the dataset by typing the following into the command box:
<b>summarize</b> 
This produces the following table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/correlationStata1.png">
We see that the dataset contains nine different variables. To create a correlation matrix for every pairwise combination of variables in the dataset, we can type the following into the command box:
<b>corr</b>
This produces the following correlation matrix:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/correlationStata2.png">
The numbers shown in the table represent the Pearson Correlation Coefficients for each pairwise combination of variables. For example, the correlation between <em>pop</em> and <em>state</em> is <b>-0.0540</b>. This indicates that these two variables are slightly negatively correlated.
Notice that the correlation along the diagonals of the table are each 1.0000, since each variable is perfectly correlated with itself.
You can also create a correlation matrix for only a certain subset of variables in a dataset by specifying the variables after the <b>corr </b>command. For example, here is how to create a correlation matrix for just the variables <em>pop</em>, <em>medage</em>, and <em>region</em>:
<b>corr pop medage region</b>
This produces the following correlation matrix for just these three variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/correlationStata3.png">
It’s also possible to put a star next to the correlation coefficients that are statistically significant at a certain significance level by using the <b>pwcorr </b>command (which produces the same result as <b>corr</b>) along with the <b>star() </b>command.
For example, the following code produces a correlation matrix for every variable in the census dataset and places a star next to the correlation coefficients that are statistically significant at α = 0.05:
<b>pwcorr, star(.05)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/correlationStata4.png">
Notice how several of the correlation coefficients in the table are statistically significant at  α = 0.05. We could set α to be any number we’d like, but common choices are .01, .05, and .10.
In general, the lower we set the value of  α, the fewer correlation coefficients will be statistically significant. For example, suppose we set  α = 0.01. 
<b>pwcorr, star(.01)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/correlationStata5.png">
Notice how fewer correlation coefficients have a star next to them.
<h2><span class="orange">How to Perform a Correlation Test in Excel (Step-by-Step)</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient  which is a measure of the linear association between two variables.
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
To determine if a correlation coefficient is statistically significant you can perform a correlation test, which involves calculating a t-score and a corresponding p-value.
The formula to calculate the t-score is:
<b>t = r√(n-2) / (1-r<sup>2</sup>)</b>
where:
<b>r:</b> Correlation coefficient
<b>n:</b> The sample size
The p-value is calculated as the corresponding two-sided p-value for the t-distribution with n-2 degrees of freedom.
The following step-by-step example shows how to perform a correlation test in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter some data values for two variables in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/correlationTestExcel1-1.png">
<h3>Step 2: Calculate the Correlation Coefficient</h3>
Next, we can use the <b>CORREL()</b> function to calculate the correlation coefficient between the two variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/correlationTestExcel2.png">
The correlation coefficient between the two variables turns out to be <b>0.803702</b>.
This is a highly positive correlation coefficient, but to determine if it’s statistically significant we need to calculate the corresponding t-score and p-value.
<h3>Step 3: Calculate the Test Statistic and P-Value</h3>
Next, we can use the following formulas to calculate the test statistic and the corresponding p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/correlationTestExcel3.png">
 
The test statistic turns out to be <b>4.27124 </b>and the corresponding p-value is <b>0.001634</b>.
Since this p-value is less than .05, we have sufficient evidence to say that the correlation between the two variables is statistically significant. 
<h2><span class="orange">How to Perform a Correlation Test in Python (With Example)</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which measures the linear association between two variables<em>.</em>
It always takes on a value between -1 and 1 where:
<b>-1</b> indicates a perfectly negative linear correlation
<b>0</b> indicates no linear correlation
<b>1</b> indicates a perfectly positive linear correlation
To determine if a correlation coefficient is statistically significant, you can calculate the corresponding t-score and p-value.
The formula to calculate the t-score of a correlation coefficient (r) is:
<b>t</b> = r * √n-2 / √1-r<sup>2</sup>
The p-value is then calculated as the corresponding two-sided p-value for the t-distribution with n-2 degrees of freedom.
<h3>Example: Correlation Test in Python</h3>
To determine if the correlation coefficient between two variables is statistically significant, you can perform a correlation test in Python using the <b>pearsonr</b> function from the <b>SciPy</b> library.
This function returns the correlation coefficient between two variables along with the two-tailed p-value.
For example, suppose we have the following two arrays in Python:
<b>#create two arrays
x = [3, 4, 4, 5, 7, 8, 10, 12, 13, 15]
y = [2, 4, 4, 5, 4, 7, 8, 19, 14, 10]
</b>
We can import the <b>pearsonr</b> function and calculate the Pearson correlation coefficient between the two arrays:
<b>from scipy.stats.stats import pearsonr
#calculation correlation coefficient and p-value between x and y
pearsonr(x, y)
(0.8076177030748631, 0.004717255828132089)
</b>
Here’s how to interpret the output:
Pearson correlation coefficient (r): <b>0.8076</b>
Two-tailed p-value: <b>0.0047</b>
Since the correlation coefficient is close to 1, this tells us that there is a strong positive association between the two variables.
And since the corresponding p-value is less than .05, we conclude that there is a statistically significant association between the two variables.
Note that we can also extract the individual correlation coefficient and p-value from the <b>pearsonr</b> function as well:
<b>#extract correlation coefficient (rounded to 4 decimal places)
r = round(pearsonr(x, y)[0], 4)
print(r)
0.8076
#extract p-value (rounded to 4 decimal places) 
p = round(pearsonr(x, y)[1], 4)
print(p) 
0.0047
</b>
These values are a bit easier to read compared to the output from the original <b>pearsonr</b> function.
<h2><span class="orange">How to Perform a Correlation Test in R (With Examples)</span></h2>
One way to quantify the relationship between two variables is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>.</em>
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
To determine if a correlation coefficient is statistically significant, you can calculate the corresponding t-score and p-value.
The formula to calculate the t-score of a correlation coefficient (r) is:
<b>t</b> = r * √n-2 / √1-r<sup>2</sup>
The p-value is calculated as the corresponding two-sided p-value for the t-distribution with n-2 degrees of freedom.
<h3>Example: Correlation Test in R</h3>
To determine if the correlation coefficient between two variables is statistically significant, you can perform a correlation test in R using the following syntax:
<b>cor.test(x, y, method=c(“pearson”, “kendall”, “spearman”))</b>
where:
<b>x, y:</b> Numeric vectors of data.
<b>method: </b>Method used to calculate correlation between two vectors. Default is “pearson.”
For example, suppose we have the following two vectors in R:
<b>x &lt;- c(2, 3, 3, 5, 6, 9, 14, 15, 19, 21, 22, 23)
y &lt;- c(23, 24, 24, 23, 17, 28, 38, 34, 35, 39, 41, 43)
</b>
Before we perform a correlation test between the two variables, we can create a quick scatterplot to view their relationship:
<b>#create scatterplot 
plot(x, y, pch=16)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/scatterR1.png">
There appears to be a positive correlation between the two variables. That is, as one increases the other tends to increase as well.
To see if this correlation is statistically significant, we can perform a correlation test:
<b>#perform correlation test between the two vectors
cor.test(x, y)
Pearson's product-moment correlation
data:  x and y
t = 7.8756, df = 10, p-value = 1.35e-05
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 0.7575203 0.9799783
sample estimates:
      cor 
0.9279869 </b>
The correlation coefficient between the two vectors turns out to be <b>0.9279869</b>.
The test statistic turns out to be <b>7.8756 </b>and the corresponding p-value is <b>1.35e-05</b>.
Since this value is less than .05, we have sufficient evidence to say that the correlation between the two variables is statistically significant.
<h2><span class="orange">Correlation vs. Association: What’s the Difference?</span></h2>
Two terms that are sometimes used interchangeably are <b>correlation</b> and <b>association</b>. However, in the field of statistics these two terms have slightly different meanings.
In particular, when we use the word <b>correlation</b> we’re typically talking about the  Pearson Correlation Coefficient . This is a measure of the linear association between two  random variables  <em>X </em>and <em>Y. </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
Conversely, when statisticians use the word <b>association</b> they can be talking about <em>any</em> relationship between two variables, whether it’s linear <em>or</em> non-linear.
To illustrate this idea, consider the following examples.
<h3>Visualizing Correlation vs. Association with Scatterplots</h3>
We use two words to describe the correlation between two random variables:
<b>1. Direction</b>
<b>Positive:</b> Two random variables have a positive correlation if <em>Y</em> tends to increase as <em>X</em> increases.
<b>Negative:</b> Two random variables have a negative correlation if <em>Y</em> tends to decrease as <em>X</em> increases.
<b>2. Strength</b>
<b>Weak:</b> Two random variables have a weak correlation if the points in a scatterplot are loosely scattered.
<b>Strong:</b> Two random variables have a strong correlation if the points in a scatterplot are tightly packed together.
The following  scatterplots  illustrate examples of each type of correlation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/correlation_vs_association1.png">
Compared to correlation, the word <b>association</b> can tell us whether or not there is <em>any</em> relationship between two random variables: linear <em>or</em> non-linear.
The following scatterplots illustrate some examples:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/correlation_vs_association2.png">
The scatterplot in the top left corner illustrates a quadratic relationship between two random variables, which means there <em>is</em> an association between the two variables but it’s not a linear one.
If we calculated the correlation between the two variables, it would likely be close to zero because there is no linear relationship between them.
However, just knowing that the correlation between the two variables is zero can be misleading because it hides the fact there there exists a non-linear relationship instead.
<h3>Correlation vs. Association: A Summary</h3>
The terms correlation and association have the following similarities and differences:
<b>Similarities:</b>
Both terms are used to describe whether or not there is a relationship between two random variables.
Both terms can use scatterplots to analyze the relationship bewteen two random variables.
<b>Differences:</b>
Correlation can only tell us if two random variables have a linear relationship while association can tell us if two random variables have a linear <em>or</em> non-linear relationship.
Correlation quantifies the relationship between two random variables by using a number between -1 and 1, but association does not use a specific number to quantify a relationship.
<h2><span class="orange">Correlation vs. Regression: What’s the Difference?</span></h2>
<b>Correlation</b> and <b>regression</b> are two terms in statistics that are related, but not quite the same.
In this tutorial, we’ll provide a brief explanation of both terms and explain how they’re similar and different.
<h3>What is Correlation?</h3>
<b>Correlation</b> measures the linear association between two variables, <em>x</em> and <em>y</em>. It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
For example, suppose we have the following dataset that contains two variables: (1) Hours studied and (2) Exam Score received for 20 different students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/cor_vs_reg1.png">
If we created a scatterplot of hours studied vs. exam score, here’s what it would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/cor_vs_reg2.png">
Just from looking at the plot, we can tell that students who study more tend to earn higher exam scores. In other words, we can visually see that there is a <b>positive correlation</b> between the two variables.
Using a calculator, we can find that the correlation between these two variables is r = <b>0.915</b>. Since this value is close to 1, it confirms that there is a strong positive correlation between the two variables.
<h3>What is Regression?</h3>
<b>Regression </b>is a method we can use to understand how changing the values of the <em>x</em> variable affect the values of the <em>y</em> variable.
A regression model uses one variable, <em>x</em>, as the predictor variable, and the other variable, <em>y</em>, as the  response variable . It then finds an equation with the following form that best describes the relationship between the two variables:
<b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x</b>
where:
<b> <U+0177>:</b> The predicted value of the response variable
<b>b<sub>0</sub>:</b> The y-intercept (the value of y when x is equal to zero)
<b>b<sub>1</sub>:</b> The regression coefficient (the average increase in y for a one unit increase in x)
<b>x:</b> The value of the predictor variable
For example, consider our dataset from earlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/cor_vs_reg1.png">
Using a  linear regression calculator , we find that the following equation best describes the relationship between these two variables:
Predicted exam score = 65.47 + 2.58*(hours studied)
The way to interpret this equation is as follows:
The predicted exam score for a student who studies zero hours is <b>65.47</b>.
The average increase in exam score associated with one additional hour studied is <b>2.58</b>.
We can also use this equation to predict the score that a student will receive based on the number of hours studied.
For example, a student who studies 6 hours is expected to receive a score of <b>80.95</b>:
Predicted exam score = 65.47 + 2.58*(6) = <b>80.95</b>.
We can also plot this equation as a line on a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/cor_vs_reg3.png">
We can see that the regression line “fits” the data quite well.
Recall earlier that the correlation between these two variables was r = <b>0.915</b>. It turns out that we can square this value and get a number called “r-squared” that describes the total proportion of  variance  in the response variable that can be explained by the predictor variable.
In this example, r<sup>2</sup> = 0.915<sup>2 </sup>= <b>0.837</b>. This means that 83.7% of the variation in exam scores can be explained by the number of hours studied.
<h3>Correlation vs. Regression: Similarities & Differences</h3>
Here is a summary of the similarities and differences between correlation and regression:
<b>Similarities:</b>
Both quantify the direction of a relationship between two variables.
Both quantify the strength of a relationship between two variables.
<b>Differences:</b>
Regression is able to show a cause-and-effect relationship between two variables. Correlation does not do this.
Regression is able to use an equation to predict the value of one variable, based on the value of another variable. Correlation does not does this.
Regression uses an equation to quantify the relationship between two variables. Correlation uses a single number.
<h2><span class="orange">Correlations in Stata: Pearson, Spearman, and Kendall</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with -1 indicating a perfect negative relationship, 0 indicating no relationship, and 1 indicating a perfect positive relationship.
There are three common ways to measure correlation:
<b>Pearson Correlation: </b>Used to measure the correlation between two continuous variables. (e.g. height and weight)
<b>Spearman Correlation: </b>Used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class)
<b>Kendall’s Correlation: </b>Used when you wish to use Spearman Correlation but the sample size is small and there are many tied ranks.
This tutorial explains how to find all three types of correlations in Stata.
<h3>Loading the Data</h3>
For each of the following examples we will use a dataset called <em>auto</em>. You can load this dataset by typing the following into the Command box:
<b>use http://www.stata-press.com/data/r13/auto</b>
We can get a quick look at the dataset by typing the following into the Command box:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/scatterStata1.png"(max-width: 467px) 100vw, 467px">
We can see that there are 12 total variables in the dataset.
<h3>How to Find Pearson Correlation in Stata</h3>
We can find the  Pearson Correlation Coefficient  between the variables <em>weight </em>and <em>length</em> by using the <b>pwcorr </b>command:
<b>pwcorr weight length</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata1.png">
The Pearson Correlation coefficient between these two variables is <b>0.9460</b>. To determine if this correlation coefficient is significant, we can find the p-value by using the <b>sig </b>command:
<b>pwcorr weight length, sig</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata2.png">
The p-value is <b>0.000</b>. Since this is less than 0.05, the correlation between these two variables is statistically significant.
To find the Pearson Correlation Coefficient for multiple variables, simply type in a list of variables after the <b>pwcorr </b>command:
<b>pwcorr weight length displacement, sig</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata3.png">
Here is how to interpret the output:
Pearson Correlation between weight and length = 0.9460 | p-value = 0.000
Pearson Correlation between weight and displacement = 0.8949 | p-value = 0.000
Pearson Correlation between displacement and length = 0.8351 | p-value = 0.000
<h3>How to Find Spearman Correlation in Stata</h3>
We can find the Spearman Correlation Coefficient between the variables <em>trunk </em>and <i>rep78 </i>by using the <b>spearman </b>command:
<b>spearman trunk rep78</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata4.png">
Here is how to interpret the output:
<b>Number of obs: </b>This is the number of pairwise observations used to calculate the Spearman Correlation Coefficient. Because there were some missing values for the variable <em>rep78</em>, Stata used only 69 (rather than the full 74) pairwise observations.
<b>Spearman’s rho: </b>This is the Spearman correlation coefficient. In this case, it’s -0.2235, indicating there is a negative correlation between the two variables. As one increases, the other tends to decrease.
<b>Prob > |t|: </b>This is the p-value associated with the hypothesis test. In this case, the p-value is 0.0649, which indicates there is not a statistically significant correlation between the two variables at α = 0.05.
We can find the Spearman Correlation Coefficient for multiple variables by simply typing more variables after the <b>spearman </b>command. We can find the correlation coefficient and the corresponding p-value for each pairwise correlation by using the <b>stats(rho p) </b>command:
<b>spearman trunk rep78 gear_ratio, stats(rho p)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata5.png">
Here is how to interpret the output:
Spearman Correlation between trunk and rep78 = -0.2235 | p-value = 0.0649
Spearman Correlation between trunk and gear_ratio = -0.5187 | p-value = 0.0000
Spearman Correlation between gear_ratio and rep78 = 0.4275 | p-value = 0.0002
<h3>How to Find Kendall’s Correlation in Stata</h3>
We can find  Kendall’s Correlation Coefficient  between the variables <em>trunk </em>and <i>rep78 </i>by using the <b>ktau </b>command:
<b>ktau trunk rep78</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata6.png">
Here is how to interpret the output:
<b>Number of obs: </b>This is the number of pairwise observations used to calculate Kendall’s Correlation Coefficient. Because there were some missing values for the variable <em>rep78</em>, Stata used only 69 (rather than the full 74) pairwise observations.
<b>Kendall’s tau-b: </b>This is Kendall’s correlation coefficient between the two variables. We typically use this value instead of tau-a because tau-b makes adjustments for ties. In this case, tau-b = -0.1752, indicating a negative correlation between the two variables.
<b>Prob > |z|: </b>This is the p-value associated with the hypothesis test. In this case, the p-value is 0.0662, which indicates there is not a statistically significant correlation between the two variables at α = 0.05.
We can find Kendall’s Correlation Coefficient for multiple variables by simply typing more variables after the <b>ktau </b>command. We can find the correlation coefficient and the corresponding p-value for each pairwise correlation by using the <b>stats(taub p) </b>command:
<b>ktau trunk rep78 gear_ratio, stats(taub p)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/corrStata7.png">
Kendall’s Correlation between trunk and rep78 = -0.1752 | p-value = 0.0662
Kendall’s Correlation between trunk and gear_ratio = -0.3753 | p-value = 0.0000
Kendall’s Correlation between gear_ratio and rep78 = 0.3206 | p-value = 0.0006
<h2><span class="orange">How to Calculate Cosine Similarity in Excel</span></h2>
<b>Cosine Similarity </b>is a measure of the similarity between two vectors of an inner product space.
For two vectors, A and B, the Cosine Similarity is calculated as:
<b>Cosine Similarity</b> = ΣA<sub>i</sub>B<sub>i</sub> / (√ΣA<sub>i</sub><sup>2</sup>√ΣB<sub>i</sub><sup>2</sup>)
This tutorial explains how to calculate the Cosine Similarity between vectors in Excel.
<h3>Cosine Similarity Between Two Vectors in Excel</h3>
Suppose we have the following two vectors in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cosine1.png">
We can use the following formula to calculate the Cosine Similarity between the two vectors in Excel:
<b>=SUMPRODUCT(A$2:A$9,B2:B9)/(SQRT(SUMSQ(B2:B9))*SQRT(SUMSQ($A$2:$A$9)))
</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cosine2.png">
The Cosine Similarity between the two vectors turns out to be <b>0.965195</b>.
Note that we interpret the value for Cosine Similarity as follows:
A value of -1 indicates maximum dissimilarity
A value of 0 indicates that the two vectors are orthogonal
A value of 1 indicates maximum similarity
A value of <b>0.965195</b> indicates a high level of similarity between the two vectors in our example.
 How to Calculate Cosine Similarity in R 
 How to Calculate Cosine Similarity in Python 
For an in-depth explanation of Cosine Similarity, refer to the following Wikipedia article:
 Wikipedia: In-depth explanation of Cosine Similarity 
<h2><span class="orange">How to Calculate Cosine Similarity in Python</span></h2>
<b>Cosine Similarity </b>is a measure of the similarity between two vectors of an inner product space.
For two vectors, A and B, the Cosine Similarity is calculated as:
<b>Cosine Similarity</b> = ΣA<sub>i</sub>B<sub>i</sub> / (√ΣA<sub>i</sub><sup>2</sup>√ΣB<sub>i</sub><sup>2</sup>)
This tutorial explains how to calculate the Cosine Similarity between vectors in Python using functions from the  NumPy  library.
<h3>Cosine Similarity Between Two Vectors in Python</h3>
The following code shows how to calculate the Cosine Similarity between two arrays in Python:
<b>from numpy import dot
from numpy.linalg import norm
#define arrays
a = [23, 34, 44, 45, 42, 27, 33, 34]
b = [17, 18, 22, 26, 26, 29, 31, 30]
#calculate Cosine Similarity
cos_sim = dot(a, b)/(norm(a)*norm(b))
cos_sim
0.965195008357566
</b>
The Cosine Similarity between the two arrays turns out to be <b>0.965195</b>.
Note that this method will work on two arrays of any length:
<b>import numpy as np
from numpy import dot
from numpy.linalg import norm
#define arrays
a = np.random.randint(10, size=100)
b = np.random.randint(10, size=100)
#calculate Cosine Similarity
cos_sim = dot(a, b)/(norm(a)*norm(b))
cos_sim
0.7340201613960431</b>
However, it only works if the two arrays are of equal length:
<b>import numpy as np
from numpy import dot
from numpy.linalg import norm
#define arrays
a = np.random.randint(10, size=90) #length=90
b = np.random.randint(10, size=100) #length=100
#calculate Cosine Similarity
cos_sim = dot(a, b)/(norm(a)*norm(b))
cos_sim
ValueError: shapes (90,) and (100,) not aligned: 90 (dim 0) != 100 (dim 0)
</b>
<h3>Notes</h3>
<b>1.</b> There are multiple ways to calculate the Cosine Similarity using Python, but as  this Stack Overflow thread  explains, the method explained in this post turns out to be the fastest.
<b>2. </b>Refer to  this Wikipedia page  to learn more details about Cosine Similarity.
<h2><span class="orange">How to Calculate Cosine Similarity in R</span></h2>
<b>Cosine Similarity </b>is a measure of the similarity between two vectors of an inner product space.
For two vectors, A and B, the Cosine Similarity is calculated as:
<b>Cosine Similarity</b> = ΣA<sub>i</sub>B<sub>i</sub> / (√ΣA<sub>i</sub><sup>2</sup>√ΣB<sub>i</sub><sup>2</sup>)
This tutorial explains how to calculate the Cosine Similarity between vectors in R using the <b>cosine()</b> function from the <b>lsa</b> library.
<h3>Cosine Similarity Between Two Vectors in R</h3>
The following code shows how to calculate the Cosine Similarity between two vectors in R:
<b>library(lsa)
#define vectors
a &lt;- c(23, 34, 44, 45, 42, 27, 33, 34)
b &lt;- c(17, 18, 22, 26, 26, 29, 31, 30)
#calculate Cosine Similarity
cosine(a, b)
         [,1]
[1,] 0.965195
</b>
The Cosine Similarity between the two vectors turns out to be <b>0.965195</b>.
<h3>Cosine Similarity of a Matrix in R</h3>
The following code shows how to calculate the Cosine Similarity between a matrix of vectors:
<b>library(lsa)
#define matrix
a &lt;- c(23, 34, 44, 45, 42, 27, 33, 34)
b &lt;- c(17, 18, 22, 26, 26, 29, 31, 30)
c &lt;- c(34, 35, 35, 36, 51, 29, 30, 31)
data &lt;- cbind(a, b, c)
#calculate Cosine Similarity
cosine(data)
          a         b         c
a 1.0000000 0.9651950 0.9812406
b 0.9651950 1.0000000 0.9573478
c 0.9812406 0.9573478 1.0000000
</b>
Here is how to interpret the output:
The Cosine Similarity between vectors <em>a </em>and <em>b </em>is <b>0.9651950</b>.
The Cosine Similarity between vectors <em>a </em>and <em>c </em>is <b>0.9812406</b>.
The Cosine Similarity between vectors <em>b </em>and <em>c </em>is <b>0.9573478</b>.
<h3>Notes</h3>
<b>1.</b> The <b>cosine() </b>function will work with a square matrix of any size.
<b>2.</b> The <b>cosine() </b>function will work on a matrix, but <em>not </em>on a data frame. However, you can easily convert a data frame to a matrix in R by using the <b>as.matrix() </b>function.
<b>3. </b>Refer to  this Wikipedia page  to learn more details about Cosine Similarity.
<h2><span class="orange">How to Count by Group in Excel</span></h2>
You can use the following formula to count the number of occurrences by group in an Excel spreadsheet:
<b>=COUNTIF(group_range, criteria)
</b>
The following example shows how to use this formula in practice.
<h2>Example: Count by Group in Excel</h2>
Suppose we have the following dataset that shows the total points scored by 15 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel00.png">
Now suppose we’d like to count the number of players, grouped by team.
To do so, we can use the <b>=UNIQUE()</b> function to first create a list of the unique teams. We’ll type the following formula into cell F2:
<b>=UNIQUE(B2:B16)</b>
Once we press enter, a list of unique team names will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel0.png">
Next, we can use the <b>=COUNTIF()</b> function to find the count of players on each team.
We’ll type in the following formula into cell G2:
<b>=COUNTIF(B2:B16, F2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/countgroup1.png">
We’ll then copy and paste this formula into the remaining cells in column G:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/countgroup2.png">
That’s it!
Column F displays each of the unique teams and column G displays the count of players on each team.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Calculate a Five Number Summary in Excel 
 How to Calculate the Mean and Standard Deviation in Excel 
 How to Calculate the Interquartile Range (IQR) in Excel 
<h2><span class="orange">How to Count Observations by Group in R</span></h2>
Often you may be interested in counting the number of  observations  by group in R. 
Fortunately this is easy to do using the  count()  function from the  dplyr  library.
This tutorial explains several examples of how to use this function in practice using the following data frame:
<b>#create data frame
df &lt;- data.frame(team = c('A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'C', 'C', 'C', 'C'), position = c('G', 'G', 'F', 'G', 'F', 'F', 'F', 'G', 'G', 'F', 'F', 'F'), points = c(4, 13, 7, 8, 15, 15, 17, 9, 21, 22, 25, 31))
#view data frame
df
   team position points
1     A        G      4
2     A        G     13
3     A        F      7
4     B        G      8
5     B        F     15
6     B        F     15
7     B        F     17
8     B        G      9
9     C        G     21
10    C        F     22
11    C        F     25
12    C        F     31
</b>
<h3>Example 1: Count by One Variable</h3>
The following code shows how to count the total number of players by team:
<b>library(dplyr)
#count total observations by variable 'team'
df %>% count(team)
# A tibble: 3 x 2
  team      n
   
1 A         3
2 B         5
3 C         4
</b>
From the output we can see that:
Team A has 3 players
Team B has 5 players
Team C has 4 players
This single count() function gives us a nice idea of the distribution of players by team.
Note that we can also <b>sort </b>the counts if we’d like:
<b>#count total observations by variable 'team'
df %>% count(team, sort=TRUE)
# A tibble: 3 x 2
  team      n
   
1 B         5
2 C         4
3 A         3</b>
<h3>Example 2: Count by Multiple Variables</h3>
We can also sort by more than one variable:
<b>#count total observations by 'team' and 'position'
df %>% count(team, position)
# A tibble: 6 x 3
  team  position     n
       
1 A     F            1
2 A     G            2
3 B     F            3
4 B     G            2
5 C     F            3
6 C     G            1
</b>
From the output we can see that:
Team A has 1 player at the ‘F’ (forward) position and 2 players at the ‘G’ (guard) position.
Team B has 3 players at the ‘F’ (forward) position and 2 players at the ‘G’ (guard) position.
Team C has 3 players at the ‘F’ (forward) position and 1 player at the ‘G’ (guard) position.
<h3>Example 3: Weighted Count</h3>
We can also “weight” the counts of one variable by another variable. For example, the following code shows how to count the total observations per team, using the variable ‘points’ as the weight:
<b>df %>% count(team, wt=points)
# A tibble: 3 x 2
  team      n
   
1 A        24
2 B        64
3 C        99
</b>
<em>You can find the complete documentation for the <b>count() </b>function  here .</em>
<h2><span class="orange">How to Count Characters in Google Sheets (3 Examples)</span></h2>
You can use the following three methods to count characters in Google Sheets:
<b>Method 1: Count Total Characters in Cell</b>
<b>=LEN(A2)
</b>
<b>Method 2: Count Total Characters in Column</b>
<b>=SUMPRODUCT(LEN(A2:A11))</b>
<b>Method 3: Count Specific Characters in Cell</b>
<b>=LEN(A2)- LEN(SUBSTITUTE(UPPER(A2),"R",""))</b>
The following examples show how to use each method with the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countchar1.jpg"472">
<h3>Example 1: Count Total Characters in Cell</h3>
We can use the following formula to count the total number of characters in cell <b>A2</b>:
<b>=LEN(A2)</b>
We can then copy and paste this formula down to each remaining cell in column A to count the total number of characters in each cell:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countchar2.jpg">
From the output we can see:
The team “Mavericks” contains <b>9</b> total characters
The team “Warriors” contains <b>8</b> total characters
The team “Heat” contains <b>4</b> total characters
And so on.
<b>Note</b>: The <b>LEN()</b> function will also count any blank spaces in the cell.
<h3>Example 2: Count Total Characters in Column</h3>
We can use the following formula to count the total number of characters in the cell range <b>A2:A11</b>:
<b>=SUMPRODUCT(LEN(A2:A11))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countchar3.jpg"516">
From the output we can see that there are <b>63</b> total characters in column A.
<h3>Example 3: Count Specific Characters in Cell</h3>
We can use the following formula to count the total number of characters equal to “R” in cell <b>A2</b>:
<b>=LEN(A2)- LEN(SUBSTITUTE(UPPER(A2),"R",""))</b>
We can then copy and paste this formula down to each remaining cell in column A to count the total number of characters equal to “R” in each cell:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countchar4.jpg">
From the output we can see:
There is <b>1</b> “R” character in cell A2.
There are <b>3</b> “R” characters in cell A3.
There are <b>0</b> “R” characters in cell A4.
And so on.
<h2><span class="orange">How to Count Frequency of Text in Excel</span></h2>
You can use the <b>COUNTIF(range, criteria)</b> function to count how often specific text occurs in an Excel column.
The following examples show how to use this function in practice.
<h3>Example 1: Count Frequency of One Specific Text</h3>
Suppose we have the following column in Excel that shows various NBA team names:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/count_text1.png">
If we’d like to count how often the team name “Hornets” appears in the column, we can use the following formula:
<b>=COUNTIF(A2:A21, "Hornets")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/count_text2.png">
We can see that “Hornets” appears <b>3</b> times.
<h3>Example 2: Count Frequency of Multiple Text</h3>
If we’d like to count the frequency of multiple different text, we can use the <b>UNIQUE()</b> function to get an array of every unique text that appears in a column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/count_text3.png">
We can then use the <b>COUNTIF()</b> function to count how frequently each team name occurs:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/count_text4.png">
We can see that:
The team name “Mavericks” occurs <b>3</b> times.
The team name “Spurs” occurs <b>2</b> times.
The team name “Lakers” occurs <b>3</b> times.
And so on.
<h2><span class="orange">How to COUNT IF True in Google Sheets</span></h2>
You can use the following formula in Google Sheets to count the number of TRUE values in a given column:
<b>=COUNTIF(A2:A11, TRUE)
</b>
This particular formula counts the number of cells in <b>A2:A11</b> that contain the value TRUE in the cell.
The following example shows how to use this syntax in practice.
<h2>Example: COUNT IF True in Google Sheets</h2>
Suppose we have the following column of TRUE and FALSE values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/true1.jpg"474">
We can use the following formula to count the number of TRUE values in the column:
<b>=COUNTIF(A2:A11, TRUE)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/true2.jpg"472">
From the output we can see that there are 6 total TRUE values in the column.
We can manually verify this is correct by counting the TRUE values in the column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/true3.jpg"494">
There are indeed 6 total TRUE values.
<b>Note</b>: To count the number of FALSE values in the column instead, simply replace <b>TRUE</b> with <b>FALSE</b> in the formula.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 Google Sheets: COUNTIF Greater Than Zero 
 Google Sheets: How to Use COUNTIF with OR 
 Google Sheets: How to Use COUNTIF From Another Sheet 
<h2><span class="orange">How to Count Names in Excel (3 Examples)</span></h2>
You can use the following methods to count names in Excel:
<b>Method 1: Count Cells with Exact Name</b>
<b>=COUNTIF(A2:A11, "Bob Johnson")</b>
<b>Method 2: Count Cells with Partial Name</b>
<b>=COUNTIF(A2:A11, "*Johnson*")</b>
<b>Method 3: Count Cells with One of Several Names</b>
<b>=COUNTIF(A2:A11, "*Johnson*") + COUNTIF(A2:A11, "*Smith*")</b>
The following examples show how to use each method with the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/countnames1.jpg"472">
<h2>Example 1: Count Cells with Exact Name</h2>
We can use the following formula to count the number of cells in column A that contain the exact name “Bob Johnson”:
<b>=COUNTIF(A2:A11, "Bob Johnson")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/countnames2.jpg">
We can see that there are <b>2</b> cells that contain “Bob Johnson” as the exact name.
<h2>Example 2: Count Cells with Partial Name</h2>
We can use the following formula to count the number of cells in column A that contain “Johnson” anywhere in the name:
<b>=COUNTIF(A2:A11, "*Johnson*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/countnames3.jpg"509">
We can see that there are <b>4</b> cells that contain “Johnson” somewhere in the name.
<h2>Example 3: Count Cells with One of Several Names</h2>
We can use the following formula to count the number of cells in column A that contain “Johnson” or “Smith” somewhere in the name:
<b>=COUNTIF(A2:A11, "*Johnson*") + COUNTIF(A2:A11, "*Smith*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/countnames4.jpg"669">
We can see that there are <b>6</b> cells that contain either “Johnson” or “Smith” somewhere in the name.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 How to Count Specific Words in Excel 
 How to Count Unique Values by Group in Excel 
 How to Use COUNTIF with Multiple Ranges in Excel 
<h2><span class="orange">How to Count Specific Words in Google Sheets</span></h2>
You can use the following formula to count the occurrence of a specific word in Google Sheets:
<b>=COUNTIF(B2:B15, "*Guard*")
</b>
This particular formula will count the number of cells in the range <b>B2:B15</b> that contain the word “Guard” somewhere in the cell.
The following example shows how to use this formula in practice.
<h2>Example: Count Specific Words in Google Sheets</h2>
Suppose we have the following dataset that contains information about various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/contain1.jpg"510">
We can use the following formula to count the number of cells in column B that contain “Guard” somewhere in the cell:
<b>=COUNTIF(B2:B15, "*Guard*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/contain2.jpg"670">
From the output we can see that 6 cells in column B contain “Guard” somewhere in the cell.
We can confirm this is correct by manually identifying each of these cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/contain3.jpg"670">
To find the count of cells that contain a different word, simply replace Guard with another word in the formula.
For example, use the following formula to count the number of cells that contain “Forward” somewhere in the cell:
<b>=COUNTIF(B2:B15, "*Forward*")</b>
<b>Note</b>: You can find the complete documentation for the <b>COUNTIF</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 How to Extract Substring in Google Sheets 
 How to Reverse a Text String in Google Sheets 
 How to Extract Numbers from String in Google Sheets 
<h2><span class="orange">COUNT vs. COUNTA in Excel: What’s the Difference?</span></h2>
In Excel, the <b>COUNT</b> and <b>COUNTA</b> functions both count the number of cells in a range, but they use slightly different behaviors:
The <b>COUNT</b> function counts the number of cells in a range that contain numbers.
The <b>COUNTA</b> function counts the number of cells in a range that are not empty.
The <b>COUNT</b> function is useful for counting the number of cells in a range that contain <em>numeric</em> values.
The <b>COUNTA</b> function is useful for counting the number of cells in a range that contain <em>any</em> value.
The following examples show how to use each function in practice.
<h3>Example 1: All Values in Range are Numeric</h3>
If we use the <b>COUNT</b> and <b>COUTNA</b> functions to count the number of cells in a range in which every cell is numeric (or blank), the two functions will return the same value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/counta1.jpg"576">
In this case, there are 9 cells in the <b>Sales</b> column that contain numeric values and one cell that is blank.
The <b>COUNT</b> function tells us that there are 9 cells with <em>numeric</em> values in the Sales column.
The <b>COUNTA</b> function tells us that there are 9 cells with <em>any</em> value in the Sales column.
<h3>Example 2: Some Values in Range are Not Numeric</h3>
If we use the <b>COUNT</b> and <b>COUTNA</b> functions to count the number of cells in a range in which some cells are not numeric, the two functions will return different values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/counta2.jpg"524">
In this case, there are seven cells in the <b>Sales</b> column that contain numeric values, two cells that contain character values, and one cell that is blank.
The <b>COUNT</b> function tells us that there are 7 cells with <em>numeric</em> values in the Sales column.
The <b>COUNTA</b> function tells us that there are 9 cells with <em>any</em> value in the Sales column.
<h2><span class="orange">How to Use COUNTIF Contains in Google Sheets</span></h2>
You can use the following formulas in Google Sheets to count the number of cells that contain certain strings:
<b>Method 1: COUNTIF Contains (One Criteria)</b>
<b>=COUNTIF(A2:A11, "*string*")
</b>
This formula counts the number of cells in <b>A2:A11</b> that contain “string” anywhere in the cell.
<b>Method 2: COUNTIF Contains (Multiple Criteria)</b>
<b>=COUNTIFS(A2:A11, "*string1*", B2:B11, "*string2*")
</b>
This formula counts the number of rows where the cell in the range <b>A2:A11</b> contains “string1” and the cell in the range <b>B2:B11</b> contains “string2.”
<b>Note</b>: The <b>*</b> operator is a wildcard character in Google Sheets.
The following examples show how to use each method in practice.
<h2>Example 1: COUNTIF Contains (One Criteria)</h2>
We can use the following formula to count the number of cells in the <b>Team</b> column that contain the string “Mav”:
<b>=COUNTIF(A2:A11,"*Mav*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/countif1.jpg"588">
From the output we can see that 5 rows in the Team column contain “Mav” somewhere in their cell.
<h2>Example 2: COUNTIF Contains (Multiple Criteria)</h2>
We can use the following formula to count the number of rows where the <b>Team</b> column contains “Mav” and the <b>Position</b> column contains “Guar”:
<b>=COUNTIFS(A2:A11, "*Mav*", B2:B11, "*Guar*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/countif2.jpg"590">
From the output we can see that 2 rows contain “Mav” in the <b>Team</b> column and “Guar” in the <b>Position</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 Google Sheets: COUNTIF Greater Than Zero 
 Google Sheets: How to Use COUNTIF with OR 
 Google Sheets: How to Use COUNTIF From Another Sheet 
<h2><span class="orange">Google Sheets: How to Use COUNTIF From Another Sheet</span></h2>
You can use the following basic syntax to use a <b>COUNTIF</b> from another sheet in Google Sheets:
<b>=COUNTIF(Sheet1!A1:B20, ">10")
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: COUNTIF From Another Sheet</h3>
Suppose we have the following sheet named <b>Sheet1</b> in Google Sheets that contains some data about basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/countifAnother1.png">
Now suppose we’d like to switch to <b>Sheet2</b> and count the total players who have more than 30 points.
We can use the following syntax to do so:
<b>=COUNTIF(Sheet1!B2:B9, ">30")</b>
Here’s how to apply this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/countifAnother2.png">
We can see that <b>2</b> players have more than 30 points.
<h3>Example 2: COUNTIFS From Another Sheet</h3>
Suppose we have the following sheet that contains some data about basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/countifAnother3.png">
Now suppose we’d like to switch to Sheet2 and count the total players who are on team A <em>and</em> have more than 30 points.
We can use a <b>COUNTIFS</b> function to do so since we’re using multiple criteria when counting:
<b>=COUNTIFS(Sheet1!A2:A9, "A", Sheet1!B2:B9, ">30")</b>
Here’s how to apply this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/countifAnother4.png">
We can see that <b>3</b> players belong to team A <em>and</em> have more than 30 points.
<h2><span class="orange">How to Use COUNTIF with OR in Excel</span></h2>
Often you may want to count the number of cells in a range in Excel that meet one of several criteria.
You can use the following basic syntax to do so:
<b>=SUM(COUNTIF(A:A,{"Value1", "Value2", "Value3"})) </b>
This particular formula counts the number of cells in column A that are equal to “Value1”, “Value2”, or “Value3.”
The following example shows how to use this syntax in practice.
<h3>Example: Use COUNTIF with OR in Excel</h3>
Suppose we have the following dataset in Excel that shows information about various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countifor1.jpg"467">
We can use the following formula to count the number of cells in column A that have a value of “East” or “South”:
<b>=SUM(COUNTIF(A:A,{"East", "South"}))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countifor2.jpg">
We can see that a total of <b>5</b> cells have a value of “East” or “South” in column <b>A</b>.
Note that we can also use similar syntax to count the number of cells that have specific numeric values.
For example, we can use the following formula to count the number of cells in column <b>C</b> that have a value of 95, 99, or 103:
<b>=SUM(COUNTIF(C:C,{95, 99, 103}))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countifor3.jpg"545">
We can see that a total of <b>6</b> cells have a value of 95, 99, or 103 in column C.
We could also use similar syntax to count the number of cells that have values greater than or less than certain numbers.
For example, we can use the following formula to count the number of cells in column <b>C</b> that have a value greater than 100 <em>or</em> less than 90:
<b>=SUM(COUNTIF(C:C,{95, 99, 103}))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/countifor4.jpg"540">
We can see that a total of <b>7</b> cells have a value greater than 100 <em>or</em> less than 90 in column C.
<h2><span class="orange">How to Use COUNTIF with OR in Google Sheets</span></h2>
Often you may want to count the number of cells in a range in Google Sheets that meet one of several criteria.
You can use the following basic syntax to do so:
<b>=ArrayFormula(SUM(COUNTIF(A:A,{"Value1", "Value2", "Value3"})))
</b>
This particular formula counts the number of cells in column A that are equal to “Value1”, “Value2”, or “Value3.”
The following example shows how to use this syntax in practice.
<h3>Example: Use COUNTIF with OR in Google Sheets</h3>
Suppose we have the following data in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/or1.png">
We can use the following formula to count the number of cells in column A that have a value of “East” or “South”:
<b>=ArrayFormula(SUM(COUNTIF(A:A,{"East", "South"})))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/or2.png">
We can see that a total of <b>5</b> cells have a value of “East” or “South” in column A. 
We can also use the following formula to count the number of cells in column C that have a value of 95, 99, or 103:
<b>=ArrayFormula(SUM(COUNTIF(C:C,{95, 99, 103})))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/or3.png">
We can see that a total of <b>6</b> cells have a value of 95, 99, or 103 in column C.
<h2><span class="orange">How to Perform a COUNTIF Function in Python</span></h2>
Often you may be interested in only counting the number of rows in a pandas DataFrame that meet some criteria.
Fortunately this is easy to do using the following basic syntax:
<b>sum(df.column_name == some_value)
</b>
The following examples show how to use this syntax in practice on the following data frame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [3, 4, 5, 6, 7, 8, 9, 10, 10, 12, 13],   'y': [3, 4, 5, 7, 9, 13, 15, 19, 23, 24, 29]})
#view head of DataFrame
df.head()
xy
033
144
255
367
479
</b>
<h3>Example 1: Count Rows Equal to Some Value</h3>
The following code shows how to count the number of rows where the x variable is equal to 10:
<b>sum(df.x == 10)
2
</b>
The following code shows how to count the number of rows where the x variable is equal to 10 <em>or</em> the y variable is equal to 5:
<b>sum((df.x == 10) | (df.y == 5))
3</b>
The following code shows how to count the number of rows where the x variable is <em>not</em> equal to 10:
<b>sum(df.x != 10)
9</b>
<h3>Example 2: Count Rows Greater or Equal to Some Value</h3>
The following code shows how to count the number of rows where x is greater than 10:
<b>sum(df.x > 10) 
2</b>
The following code shows how to count the number of rows where x is less than or equal to 7:
<b>sum(df.x &lt;= 7)
 
5</b>
<h3>Example 3: Count Rows Between Two Values</h3>
The following code shows how to count the number of rows where x is between 10 and 20:
<b>sum((df.x >= 5) & (df.x &lt;= 10))
7</b>
<h2><span class="orange">How to Perform a COUNTIF Function in R</span></h2>
Often you may be interested in <em>only </em>counting the number of rows in an R data frame that meet some criteria. Fortunately this is easy to do using the following basic syntax:
<b>sum(df$column == value, na.rm=TRUE)
</b>
The following examples show how to use this syntax in practice on the following data frame:
<b>#create data frame
data &lt;- data.frame(team=c('Mavs', 'Mavs', 'Spurs', 'Spurs', 'Lakers'),   points=c(14, NA, 8, 17, 22),   rebounds=c(8, 5, 5, 9, 12))
#view data frame
data
    team points rebounds
1   Mavs     14        8
2   Mavs     NA        5
3  Spurs      8        5
4  Spurs     17        9
5 Lakers     22       12
</b>
<h3>Example 1: Count Rows Equal to Some Value</h3>
The following code shows how to count the number of rows where the team name is equal to “Mavs”:
<b>sum(data$team == 'Mavs')
[1] 2
</b>
The following code shows how to count the number of rows where the team name is equal to “Mavs” or “Lakers”:
<b>sum(data$team == 'Mavs' | data$team == 'Lakers')
[1] 3</b>
The following code shows how to count the number of rows where the team name is <em>not </em>equal to “Lakers”:
<b>sum(data$team != 'Lakers')
[1] 4</b>
<h3>Example 2: Count Rows Greater or Equal to Some Value</h3>
The following code shows how to count the number of rows where points is greater than 10:
<b>sum(data$points > 10, na.rm=TRUE)
[1] 3</b>
The following code shows how to count the number of rows where rebounds is less than or equal to 9:
<b>sum(data$rebounds &lt;= 9, na.rm=TRUE)
[1] 4</b>
<h3>Example 3: Count Rows Between Two Values</h3>
The following code shows how to count the number of rows where points is between 10 and 20:
<b>sum(data$points > 10 & data$points &lt; 20, na.rm=TRUE)
[1] 2</b>
The following code shows how to count the number of rows where rebounds is between 8 and 10:
<b>sum(data$rebounds > 8 & data$rebounds &lt; 10, na.rm=TRUE)
[1] 1</b>
<h2><span class="orange">How to Use COUNTIF with Wildcards in Google Sheets</span></h2>
You can use the following formulas in Google Sheets to use wildcard characters with the COUNTIF function:
<b>Formula 1: COUNTIF with One Wildcard</b>
<b>=COUNTIF(A2:A11, "*string*")
</b>
This particular formula counts the number of cells in the range <b>A2:A11</b> that contain “string” anywhere in the cell.
<b>Formula 2: COUNTIF with Multiple Wildcards</b>
<b>=COUNTIFS(A2:A11, "*string1*", B2:B11, "*string2*")
</b>
This particular formula counts the number of cells where <b>A2:A11</b> contains “string1” and where <b>B2:B11</b> contains “string2.”
The following examples show how to use each formula in practice.
<h3>Example 1: COUNTIF with One Wildcard</h3>
We can use the following formula to count the number of cells in column A that contain “avs” anywhere in the cell:
<b>=COUNTIF(A2:A11, "*avs*")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/wildcard11.jpg"652">
From the output we can see that there are <b>5</b> cells in column A that contain “avs” in the name.
<h3>Example 2: COUNTIF with Multiple Wildcards</h3>
We can use the following formula to count the number of rows where <b>A2:A11</b> contains “avs” and where <b>B2:B11</b> contains “Gua”:
<b>=COUNTIFS(A2:A11, "*avs*", B2:B11, "*Gua*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/wildcard12.jpg"677">
From the output we can see that there are <b>2</b> rows where the team contains “avs” and the position contains “Gua” somewhere in the cell.
<h2><span class="orange">Covariance Matrix Calculator</span></h2>
This calculator creates a covariance matrix for up to five different variables.
Simply enter the data values for up to five variables into the boxes below, then press the “Calculate” button.
<b>Variable 1</b>
<textarea id="a" rows="3" cols="40">11, 12, 12, 14, 16, 19, 19, 20, 21, 21</textarea>
<b>Variable 2</b>
<textarea id="b" rows="3" cols="40">16, 16, 16, 17, 18, 19, 22, 24, 25, 32</textarea>
<b>Variable 3</b>
<textarea id="c" rows="3" cols="40">18, 18, 19, 21, 21, 22, 22, 23, 24, 26</textarea>
<b>Variable  4</b>
<textarea id="d" rows="3" cols="40"></textarea>
<b>Variable  5</b>
<textarea id="e" rows="3" cols="40"></textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Covariance Matrix</b>
<table><tbody>
<tr>
<th><b></b></th>
            <th style="background-color: #bee3ff"><b>Var1</b></th>
            <th style="background-color: #bee3ff"><b>Var2</b></th>
            <th style="background-color: #bee3ff"><b>Var3</b></th>
            <th style="background-color: #bee3ff"><b>Var4</b></th>
            <th style="background-color: #bee3ff"><b>Var5</b></th>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var1</b></td>
            <td>15.8333</td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var2</b></td>
            <td>17.7222</td>
            <td>27.6111</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var3</b></td>
            <td>9.6667</td>
            <td>12.6667</td>
            <td>6.7111</td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var4</b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td style="background-color: #bee3ff"><b>Var5</b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
</tbody></table>
<script>
var div_table = document.getElementById('words_table');
function calc() {
//get raw data for each group
var v1  = document.getElementById('a').value.split(',').map(Number);
var v2  = document.getElementById('b').value.split(',').map(Number);
var v3  = document.getElementById('c').value.split(',').map(Number);
var v4  = document.getElementById('d').value.split(',').map(Number);
var v5 = document.getElementById('e').value.split(',').map(Number);
if (v1.length >1){
var cor11 = jStat.covariance(v1, v1);
document.getElementById('cor11').innerHTML = cor11.toFixed(4);
};
if (v2.length >1){
var cor21 = jStat.covariance(v2, v1);
var cor22 = jStat.covariance(v2, v2);
document.getElementById('cor21').innerHTML = cor21.toFixed(4);
document.getElementById('cor22').innerHTML = cor22.toFixed(4);
};
if (v3.length >1){
var cor31 = jStat.covariance(v3, v1);
var cor32 = jStat.covariance(v3, v2);
var cor33 = jStat.covariance(v3, v3);
document.getElementById('cor31').innerHTML = cor31.toFixed(4);
document.getElementById('cor32').innerHTML = cor32.toFixed(4);
document.getElementById('cor33').innerHTML = cor33.toFixed(4);
};
if (v4.length >1){
var cor41 = jStat.covariance(v4, v1);
var cor42 = jStat.covariance(v4, v2);
var cor43 = jStat.covariance(v4, v3);
var cor44 = jStat.covariance(v4, v4);
document.getElementById('cor41').innerHTML = cor41.toFixed(4);
document.getElementById('cor42').innerHTML = cor42.toFixed(4);
document.getElementById('cor43').innerHTML = cor43.toFixed(4);
document.getElementById('cor44').innerHTML = cor44.toFixed(4);
};
if (v5.length >1){
var cor51 = jStat.covariance(v5, v1);
var cor52 = jStat.covariance(v5, v2);
var cor53 = jStat.covariance(v5, v3);
var cor54 = jStat.covariance(v5, v4);
var cor55 = jStat.covariance(v5, v5);
document.getElementById('cor51').innerHTML = cor51.toFixed(4);
document.getElementById('cor52').innerHTML = cor52.toFixed(4);
document.getElementById('cor53').innerHTML = cor53.toFixed(4);
document.getElementById('cor54').innerHTML = cor54.toFixed(4);
document.getElementById('cor55').innerHTML = cor55.toFixed(4);
};
} //end massive calculator script
</script>
<h2><span class="orange">How to Create a Covariance Matrix in Excel</span></h2>
Covariance </b>is a measure of how changes in one variable are associated with changes in a second variable. Specifically, it’s a measure of the degree to which two variables are linearly associated.
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
The formula to calculate the covariance between two variables, <em>X</em> and <em>Y</em> is:
<!-- /wp:paragraph -->
<!-- wp:paragraph {"align":"center","customTextColor":"#000000"} -->
<b>COV(</b><em><b>X</b></em><b>, </b><em><b>Y</b></em><b>)</b> = Σ(x-x)(y-y) / n
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
A <b>covariance matrix</b> is a square matrix that shows the covariance between many different variables. This can be an easy, useful way to understand how different variables are related in a dataset.
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
The following example shows how to create a covariance matrix in Excel using a simple dataset.
<!-- /wp:paragraph -->
<!-- wp:heading {"level":3} -->
<h3>How to Create a Covariance Matrix in Excel</h3>
<!-- /wp:heading -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
Suppose we have the following dataset that shows the test scores of 10 different students for three subjects: math, science, and history.
<!-- /wp:paragraph -->
<!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance1.jpg"><figcaption></figcaption></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
To create a covariance matrix for this dataset, click on the <b>Data Analysis</b> option in the top right of Excel under the <em>Data</em> tab.
<!-- /wp:paragraph -->
<!-- wp:image {"id":1698,"align":"center"} -->
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/toolpak3.jpg"></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
<em><b>Note: </b>If you don’t see the Data Analysis option, you need to first  load the Data Analysis Toolpak .</em>
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
Once you click this option, a new window will appear. Click on <b>Covariance</b>.
<!-- /wp:paragraph -->
<!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance2.jpg"></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
In the <em>Input Range</em> box, type “$A$1:$C$11”, since this is the range of cells where our dataset is located. Check the box that says <em>Labels in first row</em> to tell Excel that the labels for our variables are located in the first row. Then, in the <em>Output Range</em> box, type any cell where you would like the covariance matrix to appear. I chose cell $E$2. Then click <em>OK</em>.
<!-- /wp:paragraph -->
<!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance3-1.jpg" alt=""></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
The covariance matrix is automatically generated and appears in cell $E$2:
<!-- /wp:paragraph -->
<!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance4.jpg"></figure>
<!-- /wp:image -->
<!-- wp:heading {"level":3} -->
<h3>How to Interpret a Covariance Matrix</h3>
<!-- /wp:heading -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
Once we have a covariance matrix, it’s rather simple to interpret the values in the matrix.
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
The values along the diagonals of the matrix are simply the variances of each subject. For example:
<!-- /wp:paragraph -->
<!-- wp:list {"className":"has-text-color"} -->
The variance of the math scores is 64.96
The variance of the science scores is 56.4
The variance of the history scores is 75.56
<!-- /wp:list --><!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance5.jpg"></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
The other values in the matrix represent the covariances between the various subjects. For example:
<!-- /wp:paragraph -->
<!-- wp:list {"className":"has-text-color"} -->
The covariance between the math and science scores is 33.2
The covariance between the math and history scores is -24.44
The covariance between the science and history scores is -24.1
<!-- /wp:list --><!-- wp:image {"align":"center"} -->
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/statology_covariance6.jpg"></figure>
<!-- /wp:image -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
A <b>positive number</b> for covariance indicates that two variables tend to increase or decrease in tandem. For example, math and science have a positive covariance (33.2), which indicates that students who score high on math also tend to score high on science. Likewise, students who score low on math also tend to score low on science.
<!-- /wp:paragraph -->
<!-- wp:paragraph {"customTextColor":"#000000"} -->
A <b>negative number</b> for covariance indicates that as one variable increases, a second variable tends to decrease. For example, math and history have a negative covariance (-24.44), which indicates that students who score high on math tend to score low on history. Likewise, students who score low on math tend to score high on history.
<!-- /wp:paragraph -->
<h2><span class="orange">How to Create a Covariance Matrix in Google Sheets</span></h2>
Covariance </b>is a measure of how changes in one variable are associated with changes in a second variable. Specifically, it’s a measure of the degree to which two variables are linearly associated.
The formula to calculate the covariance between two variables, <em>X</em> and <em>Y</em> is:
<b>COV(</b><em><b>X</b></em><b>, </b><em><b>Y</b></em><b>)</b> = Σ(x-x)(y-y) / n
A <b>covariance matrix</b> is a square matrix that shows the covariance between many different variables. This can be a useful way to understand how different variables are related in a dataset.
The following example shows how to create a covariance matrix in Google Sheets for a given dataset.
<h3>How to Create a Covariance Matrix in Google Sheets</h3>
Suppose we have the following dataset that shows the test scores of 10 different students for three subjects: math, science, and history.
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/covMatrixSheets1.png"><figcaption></figcaption></figure>
To create a covariance matrix for this dataset, we can use the <b>COVAR() </b>function with the following syntax:
 <b>COVAR(data_y, data_x) </b>
The covariance matrix for this dataset is shown in cells <b>B15:D17 </b>while the formulas used to create the covariance matrix are shown in cells <b>B21:D23 </b>below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/covMatrixSheets2.png">
<h3>How to Interpret a Covariance Matrix</h3>
Once we have a covariance matrix, it’s simple to interpret the values in the matrix.
The values along the diagonals of the matrix are simply the variances of each subject. For example:
The variance of the math scores is 64.96
The variance of the science scores is 56.4
The variance of the history scores is 75.56
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/covMatrixSheets3.png"></figure>
The other values in the matrix represent the covariances between the various subjects. For example:
The covariance between the math and science scores is 33.2
The covariance between the math and history scores is -24.44
The covariance between the science and history scores is -24.1
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/covMatrixSheets4.png"></figure>
A <b>positive number</b> for covariance indicates that two variables tend to increase or decrease in tandem. For example, math and science have a positive covariance (33.2), which indicates that students who score high on math also tend to score high on science. Likewise, students who score low on math also tend to score low on science.
A <b>negative number</b> for covariance indicates that as one variable increases, a second variable tends to decrease. For example, math and history have a negative covariance (-24.44), which indicates that students who score high on math tend to score low on history. Likewise, students who score low on math tend to score high on history.
<h2><span class="orange">How to Create a Covariance Matrix in Python</span></h2>
<b>Covariance </b>is a measure of how changes in one variable are associated with changes in a second variable. Specifically, it’s a measure of the degree to which two variables are linearly associated.
A <b>covariance matrix</b> is a square matrix that shows the covariance between many different variables. This can be a useful way to understand how different variables are related in a dataset.
The following example shows how to create a covariance matrix in Python.
<h3>How to Create a Covariance Matrix in Python</h3>
Use the following steps to create a covariance matrix in Python.
<b>Step 1: Create the dataset.</b>
First, we’ll create a dataset that contains the test scores of 10 different students for three subjects: math, science, and history.
<b>import numpy as np
math = [84, 82, 81, 89, 73, 94, 92, 70, 88, 95]
science = [85, 82, 72, 77, 75, 89, 95, 84, 77, 94]
history = [97, 94, 93, 95, 88, 82, 78, 84, 69, 78]
data = np.array([math, science, history])
</b>
<b>Step 2: Create the covariance matrix.</b>
Next, we’ll create the covariance matrix for this dataset using the numpy function <b>cov()</b>, specifying that <b>bias = True </b>so that we are able to calculate the population covariance matrix. 
<b>np.cov(data, bias=True)
array([[ 64.96,  33.2 , -24.44],
       [ 33.2 ,  56.4 , -24.1 ],
       [-24.44, -24.1 ,  75.56]])
</b>
<b>Step 3: Interpret the covariance matrix.</b>
The values along the diagonals of the matrix are simply the variances of each subject. For example:
The variance of the math scores is 64.96
The variance of the science scores is 56.4
The variance of the history scores is 75.56
The other values in the matrix represent the covariances between the various subjects. For example:
The covariance between the math and science scores is 33.2
The covariance between the math and history scores is -24.44
The covariance between the science and history scores is -24.1
A <b>positive number</b> for covariance indicates that two variables tend to increase or decrease in tandem. For example, math and science have a positive covariance (33.2), which indicates that students who score high on math also tend to score high on science. Conversely, students who score low on math also tend to score low on science.
A <b>negative number</b> for covariance indicates that as one variable increases, a second variable tends to decrease. For example, math and history have a negative covariance (-24.44), which indicates that students who score high on math tend to score low on history. Conversely, students who score low on math tend to score high on history.
<b>Step 4: Visualize the covariance matrix (optional).</b>
You can visualize the covariance matrix by using the <b>heatmap() </b>function from the seaborn package:
<b>import seaborn as sns
import matplotlib.pyplot as plt
cov = np.cov(data, bias=True)
labs = ['math', 'science', 'history']
sns.heatmap(cov, annot=True, fmt='g', xticklabels=labs, yticklabels=labs)
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/covarianceMatrixPython2.png">
You can also change the colormap by specifying the <b>cmap </b>argument:
<b>sns.heatmap(cov, annot=True, fmt='g', xticklabels=labs, yticklabels=labs, cmap='YlGnBu')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/covarianceMatrixPython3.png">
<em>For more details on how to style this heatmap, refer to the  seaborn documentation .</em>
<h2><span class="orange">How to Create a Covariance Matrix in R</span></h2>
<b>Covariance </b>is a measure of how changes in one variable are associated with changes in a second variable. Specifically, it’s a measure of the degree to which two variables are linearly associated.
A <b>covariance matrix</b> is a square matrix that shows the covariance between many different variables. This can be a useful way to understand how different variables are related in a dataset.
The following example shows how to create a covariance matrix in R.
<h3>How to Create a Covariance Matrix in R</h3>
Use the following steps to create a covariance matrix in R.
<b>Step 1: Create the data frame.</b>
First, we’ll create a data frame that contains the test scores of 10 different students for three subjects: math, science, and history.
<b>#create data frame
data &lt;- data.frame(math = c(84, 82, 81, 89, 73, 94, 92, 70, 88, 95),   science = c(85, 82, 72, 77, 75, 89, 95, 84, 77, 94),   history = c(97, 94, 93, 95, 88, 82, 78, 84, 69, 78))
#view data frame
data
   math science history
1    84      85      97
2    82      82      94
3    81      72      93
4    89      77      95
5    73      75      88
6    94      89      82
7    92      95      78
8    70      84      84
9    88      77      69
10   95      94      78
</b>
<b>Step 2: Create the covariance matrix.</b>
Next, we’ll create the covariance matrix for this dataset using the <b>cov() </b>function:
<b>#create covariance matrix
cov(data)
             math   science   history
math     72.17778  36.88889 -27.15556
science  36.88889  62.66667 -26.77778
history -27.15556 -26.77778  83.95556
</b>
<b>Step 3: Interpret the covariance matrix.</b>
The values along the diagonals of the matrix are simply the variances of each subject. For example:
The variance of the math scores is 72.18
The variance of the science scores is 62.67
The variance of the history scores is 83.96
The other values in the matrix represent the covariances between the various subjects. For example:
The covariance between the math and science scores is 36.89
The covariance between the math and history scores is -27.16
The covariance between the science and history scores is -26.78
A <b>positive number</b> for covariance indicates that two variables tend to increase or decrease in tandem. For example, math and science have a positive covariance (36.89), which indicates that students who score high on math also tend to score high on science. Conversely, students who score low on math also tend to score low on science.
A <b>negative number</b> for covariance indicates that as one variable increases, a second variable tends to decrease. For example, math and history have a negative covariance (-27.16), which indicates that students who score high on math tend to score low on history. Conversely, students who score low on math tend to score high on history.
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">How to Create a Covariance Matrix in SPSS</span></h2>
<b>Covariance </b>is a measure of how changes in one variable are associated with changes in a second variable. Specifically, it’s a measure of the degree to which two variables are linearly associated.
The formula to calculate the covariance between two variables, <em>X</em> and <em>Y</em> is:
<b>COV(</b><em><b>X</b></em><b>, </b><em><b>Y</b></em><b>)</b> = Σ(x-x)(y-y) / n
A <b>covariance matrix</b> is a square matrix that shows the covariance between different variables in a dataset.
This tutorial explains how to create a covariance matrix for a given dataset in SPSS.
<h3>Example: Covariance Matrix in SPSS</h3>
Suppose we have the following dataset that shows the test scores of 10 different students for three subjects: math, science, and history:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS1.png">
To create a covariance matrix for this dataset, click the <b>Analyze </b>tab, then <b>Correlate</b>, then <b>Bivariate</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS2.png">
In the new window that pops up, drag each of the three variables into the box labelled <b>Variables</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS3.png">
Next, click <b>Options</b>. Check the box next to <b>Cross-product deviations and covariances</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS4.png">
Then click <b>OK</b>. The output will appear in a new window:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS5.png">
To obtain the covariance for each pairwise combination of variables, you must divide the <b>Sum of Squares and Cross-products </b>by <b>N</b>.
For example, the covariance between math and science can be calculated as:
COV(math, science) = 332.000 / 10 = <b>33.2</b>.
Similarly, the covariance between math and history can be calculated as:
COV(math, history) = -244.400 / 10 = <b>-24.44</b>.
You can also obtain the variance for each variable by dividing the <b>Sum of Squares and Cross-products </b>by <b>N</b>.
For example, the variance for math can be calculated as:
VAR(math) = 649.600 / 10 = <b>64.96</b>.
You can obtain the entire covariance matrix for this dataset by performing similar calculations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/covarianceSPSS6.png">
<h3>How to Interpret a Covariance Matrix</h3>
The values along the diagonals of the covariance matrix are simply the variances of each subject. For example:
The variance of the math scores is <b>64.96</b>.
The variance of the science scores is <b>56.4</b>.
The variance of the history scores is <b>75.56</b>.
The other values in the matrix represent the covariances between the various subjects. For example:
The covariance between the math and science scores is <b>33.2</b>.
The covariance between the math and history scores is <b>-24.44</b>.
The covariance between the science and history scores is <b>-24.1</b>.
A <b>positive number</b> for covariance indicates that two variables tend to increase or decrease in tandem. For example, math and science have a positive covariance (33.2), which indicates that students who score high on math also tend to score high on science. Likewise, students who score low on math also tend to score low on science.
A <b>negative number</b> for covariance indicates that as one variable increases, a second variable tends to decrease. For example, science and history have a negative covariance (-24.1), which indicates that students who score high on science tend to score low on history. Likewise, students who score low on science tend to score high on history.
<h2><span class="orange">COVARIANCE.P vs. COVARIANCE.S in Excel: What’s the Difference?</span></h2>
In statistics, <b>covariance</b> is a way to measure how changes in one variable are associated with changes in another variable.
A positive value for covariance indicates that an increase in one variable is associated with an increase in another variable.
A negative value indicates that an increase in one variable is associated with a decrease in another variable.
There are two different functions you can use to calculate covariance in Excel:
<b>1. COVARIANCE.P:</b> This function calculates the population covariance. Use this function when the range of values represents the entire population.
This function uses the following formula:
Population covariance = Σ(x<sub>i</sub>–x)(y<sub>i</sub>–y) / n
where:
<b>Σ:</b> A greek symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value for variable x
<b>x</b>: The mean value for variable x
<b>y<sub>i</sub>:</b> The i<sup>th</sup> value for variable y
<b>y</b>: The mean value for variable y
<b>n:</b> The total number of observations
<b>2. COVARIANCE.S:</b> This function calculates the sample covariance. Use this function when the range of values represents a sample of values, rather than an entire population.
This function uses the following formula:
Sample covariance = Σ(x<sub>i</sub>–x)(y<sub>i</sub>–y) / (n-1)
where:
<b>Σ:</b> A greek symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value for variable x
<b>x</b>: The mean value for variable x
<b>y<sub>i</sub>:</b> The i<sup>th</sup> value for variable y
<b>y</b>: The mean value for variable y
<b>n:</b> The total number of observations
Notice the subtle difference between the two formulas: <b>COVARIANCE.P</b> divides by <b>n</b> while <b>COVARIANCE.S</b> divides by <b>n-1</b>.
Because of this, the <b>COVARIANCE.S</b> formula will always produce a larger value because it divides by a smaller value.
The following example shows how to use each formula in practice.
<h2>Example: COVARIANCE.P vs. COVARIANCE.S in Excel</h2>
Suppose we have the following dataset in Excel that shows the points and assists for 15 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/covar1.jpg"455">
The following screenshot shows how to calculate the covariance between Points and Assists using the two different covariance formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/covar2.jpg"627">
The sample covariance turns out to be <b>15.69 </b>and the population covariance turns out to be <b>14.64</b>.
As mentioned earlier, the sample covariance will always be larger than the population covariance.
<h2>When to Use COVARIANCE.P vs. COVARIANCE.S</h2>
In most cases, we’re unable to collect data for an entire population so we instead collect data for just a  sample  of the population.
Thus, we almost always use <b>COVARIANCE.S</b> to calculate the covariance of a dataset because our dataset typically represents a sample.
In the rare case where your data represents an entire population, you may use the <b>COVARIANCE.P</b> function instead.
<h2>Additional Resources</h2>
The following tutorials explain the difference between other commonly used Excel functions:
 STDEV.P vs. STDEV.S in Excel: What’s the Difference? 
 PERCENTILE.EXC vs. PERCENTILE.INC in Excel: What’s the Difference? 
 QUARTILE.EXC vs. QUARTILE.INC in Excel: What’s the Difference? 
<h2><span class="orange">Covariance vs. Variance: What’s the Difference?</span></h2>
<b>Variance</b> and <b>covariance</b> are two terms used often in statistics. Although they sound similar, they’re quite different.
<b>Variance</b> measures how spread out values are in a given dataset.
<b>Covariance</b> measures how changes in one variable are associated with changes in a second variable.
This tutorial provides a brief explanation of each term along with examples of how to calculate each.
<h3>Variance: Formula, Example, and When to Use</h3>
Variance measures how spread out values are in a given dataset.
<b>Formula:</b>
The formula to find the variance of a sample (denoted as <b>s<sup>2</sup></b>) is:
<b>s<sup>2</sup></b> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>x</b>: The sample mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> observation in the sample
<b>N</b>: The sample size
<b> Σ</b>: A Greek symbol that means “sum”
<b>Example:</b>
Suppose we have the following dataset with 10 values:
Dataset: 6, 7, 10, 13, 14, 14, 18, 19, 22, 24
Using a calculator, we can find that the sample variance is <b>36.678</b>.
Now suppose we had another dataset with 10 values:
Dataset: 6, 13, 19, 24, 25, 30, 36, 43, 49, 55
The sample variance of this dataset turns out to be <b>248.667</b>.
The variance of the second dataset is much larger than the first, which indicates that the values in the second dataset are much more spread out compared to the values in the first dataset.
<b>When to Use:</b>
We use variance when we want to quantify how spread out values are in a dataset. The higher the variance, the more spread out values the values are. The value for variance can range from zero (no spread at all) to any number greater than zero.
<h3>Covariance: Definition, Example, and When to Use</h3>
Covariance measures how changes in one variable are associated with changes in a second variable.
<b>Formula:</b>
The formula to find the covariance between two variables, <em>X</em> and <em>Y</em> is:
<b>COV(</b><em><b>X</b></em><b>, </b><em><b>Y</b></em><b>)</b> = Σ(x<sub>i</sub>–x)(y<sub>i</sub>–y) / n
where:
<b>x</b>: The sample mean of variable <em>X</em>
<b>x<sub>i</sub></b>: The i<sup>th</sup> observation of variable <em>X</em>
<b>y</b>: The sample mean of variable <em>Y</em>
<b>y<sub>i</sub></b>: The i<sup>th</sup> observation of variable <em>Y</em>
<b>n</b>: The total number of pairwise observations
<b> Σ</b>: A Greek symbol that means “sum”
<b>Example:</b>
Suppose we have the following dataset with 10 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cov_vs_var1.png">
Using a calculator, we can find that the covariance between <em>X</em> and <em>Y</em> is <b>31.8</b>.
Since this value is positive, it tells us that as the values for <em>X</em> increase, the values for <em>Y</em> tend to increase as well.
Now suppose we had another dataset with 10 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cov_vs_var2.png">
Using a calculator, we can find that the covariance between <em>X</em> and <em>Y</em> is <b>-38.55</b>.
Since this value is negative, it tells us that as the values for <em>X</em> increase, the values for <em>Y</em> tend to decrease.
<b>When to Use:</b>
We use covariance when we want to quantify the relationship between two variables. A positive value for covariance indicates a positive relationship between two variables while a negative value indicates a negative relationship between two variables.
<h2><span class="orange">What is a Covariate in Statistics?</span></h2>
In statistics, researchers are often interested in understanding the relationship between one or more  explanatory variables  and a  response variable .
However, occasionally there may be other variables that can affect the response variable that are not of interest to researchers. These variables are known as <b>covariates</b>.
<b>Covariates:</b> Variables that affect a response variable, but are not of interest in a study. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/covariate1.png">
For example, suppose researchers want to know if three different studying techniques lead to different average exam scores at a certain school. The studying technique is the explanatory variable and the exam score is the response variable.
However, there’s bound to exist some variation in the student’s studying abilities within the three groups. If this isn’t accounted for, it will be unexplained variation within the study and will make it harder to actually see the true relationship between studying technique and exam score.
One way to account for this could be to use the student’s current grade in the class as a <b>covariate</b>. It’s well known that the student’s current grade is likely correlated with their future exam scores.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/covariate2.png">
Thus, although current grade is not a variable of interest in this study, it can be included as a covariate so that researchers can see if studying technique affects exam scores even after accounting for the student’s current grade in the class.
Covariates appear most often in two types of settings: ANOVA (analysis of variance) and Regression.
<h3>Covariates in ANOVA</h3>
When we perform an ANOVA (whether it’s a  one-way ANOVA ,  two-way ANOVA , or something more complex), we’re interested in finding out whether or not there is a difference between the means of three or more independent groups.
In our previous example, we were interested in understanding whether or not there was a difference in mean exam scores between three different studying techniques. To understand this, we could have conducted a one-way ANOVA.
However, since we knew that a student’s current grade was also likely to affect exam scores we could include it as a covariate and instead perform an  ANCOVA  (analysis of covariance).
This is similar to an ANOVA, except that we include a continuous variable (student’s current grade) as a <b>covariate </b>so that we can understand whether or not there is a difference in mean exam scores between the three studying techniques, <em>even after accounting for the student’s current grade</em>.
<h3>Covariates in Regression</h3>
When we perform a linear regression, we’re interested in quantifying the relationship between one or more explanatory variables and a response variable.
For example, we could run a  simple linear regression  to quantify the relationship between square footage and house prices in a certain city. However, it may be known that the age of a house is also a variable that affects house price.
In particular, older houses may be correlated with lower house prices. In this case, the age of the house would be a <b>covariate </b>since we’re not actually interested in studying it, but we know that it has an effect on house price.
Thus, we could include house age as an explanatory variable and run a multiple linear regression with square footage and house age as explanatory variables and house price as the response variable.
Thus, the  regression coefficient  for square footage would then tell us the average change in house price associated with a one unit increase in square footage <em>after accounting for house age</em>.
<h2><span class="orange">How to Perform a Cramer-Von Mises Test in R (With Examples)</span></h2>
The <b>Cramer-Von Mises test </b>is used to determine whether or not a sample comes from a  normal distribution .
This type of test is useful for determining whether or not a given dataset comes from a normal distribution, which is a  common assumption  used in many statistical tests including  regression ,  ANOVA ,  t-tests , and many others.
We can easily perform a Cramer-Von Mises test using the<b> cvm.test()</b> function from the <b>goftest</b> package in R.
The following example shows how to use this function in practice.
<h2>Example 1: Cramer-Von Mises Test on Normal Data</h2>
The following code shows how to perform a Cramer-Von Mises test on a dataset with a sample size n=100:
<b>library(goftest)
#make this example reproducible
set.seed(0)
#create dataset of 100 random values generated from a normal distribution
data &lt;- rnorm(100)
#perform Cramer-Von Mises test for normality
cvm.test(data, 'pnorm')
Cramer-von Mises test of goodness-of-fit
Null hypothesis: Normal distribution
Parameters assumed to be fixed
data:  data
omega2 = 0.078666, p-value = 0.7007
</b>
The p-value of the test turns out to be <b>0.7007</b>.
Since this value is not less than .05, we can assume the sample data comes from a population that is normally distributed.
This result shouldn’t be surprising since we generated the sample data using the <b>rnorm()</b> function, which generates random values from a  standard normal distribution .
<b>Related:</b>  A Guide to dnorm, pnorm, qnorm, and rnorm in R 
We can also produce a histogram to visually verify that the sample data is normally distributed:
<b>hist(data, col='steelblue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/shapiroR1.png">
We can see that the distribution is fairly bell-shaped with one peak in the center of the distribution, which is typical of data that is normally distributed.
<h2>Example 2: Cramer-Von Mises Test on Non-Normal Data</h2>
The following code shows how to perform a Cramer-Von Mises test on a dataset with a sample size of 100 in which the values are randomly generated from a  Poisson distribution :
<b>library(goftest)
#make this example reproducible
set.seed(0)
#create dataset of 100 random values generated from a Poisson distribution
data &lt;- rpois(n=100, lambda=3)
#perform Cramer-Von Mises test for normality
cvm.test(data, 'pnorm')
Cramer-von Mises test of goodness-of-fit
Null hypothesis: Normal distribution
Parameters assumed to be fixed
data:  data
omega2 = 27.96, p-value &lt; 2.2e-16
</b>
The p-value of the test turns out to be extremely small.
Since this value is less than .05, we have sufficient evidence to say that the sample data does <em>not </em>come from a population that is normally distributed.
This result shouldn’t be surprising since we generated the sample data using the<b> rpois()</b> function, which generates random values from a Poisson distribution.
<b>Related:</b>  A Guide to dpois, ppois, qpois, and rpois in R 
We can also produce a histogram to visually see that the sample data is not normally distributed:
<b>hist(data, col='coral2')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/shapiroR2.png">
We can see that the distribution is  right-skewed  and doesn’t have the typical “bell-shape” associated with a normal distribution.
Thus, our histogram matches the results of the Cramer-Von Mises test and confirms that our sample data does not come from a normal distribution.
<h2>What to Do with Non-Normal Data</h2>
If a given dataset is <em>not</em> normally distributed, we can often perform one of the following transformations to make it more normal:
<b>1. Log Transformation: </b>Transform the response variable from y to <b>log(y)</b>.
<b>2. Square Root Transformation: </b>Transform the response variable from y to <b>√y</b>.
<b>3. Cube Root Transformation: </b>Transform the response variable from y to <b>y<sup>1/3</sup></b>.
By performing these transformations, the response variable typically becomes closer to normally distributed.
Refer to  this tutorial  to see how to perform these transformations in practice.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other normality tests in R:
 How to Perform a Shapiro-Wilk Test in R 
 How to Conduct an Anderson-Darling Test in R 
 How to Conduct a Kolmogorov-Smirnov Test in R 
<h2><span class="orange">How to Calculate Cramer’s V in Excel</span></h2>
<b>Cramer’s V</b> is a measure of the strength of association between two  nominal variables .
It ranges from 0 to 1 where:
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a strong association between the two variables.
It is calculated as:
<b>Cramer’s V = √(X<sup>2</sup>/n) / min(c-1, r-1)</b>
where:
<b>X<sup>2</sup>:</b> The Chi-square statistic
<b>n:</b> Total sample size
<b>r:</b> Number of rows
<b>c:</b> Number of columns
This tutorial provides two examples of how to calculate Cramer’s V for a contingency table in Excel.
<h3>Example: Calculating Cramer’s V in Excel</h3>
Suppose we would like to understand if there is an association between two exam prep methods and the passing rate of students.
The following table shows the number of students who passed and failed the exam, based on the exam prep method they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cramerExcel2.png">
The following screenshot shows the exact formulas we can use to calculate Cramer’s V for a 2×2 table that contains data for 36 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cramerExcel3.png">
Cramer’s V turns out to be <b>0.1617</b>.
We can use the following table to determine whether a Cramer’s V should be considered a small, medium, or large effect size based on the degrees of freedom used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cramerExcel1.png">
In this example, the degrees of freedom is equal to 1. Thus, a Cramer’s V of 0.1617 would be considered a small effect size.
In other words, there’s a fairly weak association between the exam prep method used and the passing rate of students.
<h2><span class="orange">How to Calculate Cramer’s V in Python</span></h2>
<b>Cramer’s V</b> is a measure of the strength of association between two  nominal variables .
It ranges from 0 to 1 where:
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a strong association between the two variables.
It is calculated as:
<b>Cramer’s V = √(X<sup>2</sup>/n) / min(c-1, r-1)</b>
where:
<b>X<sup>2</sup>:</b> The Chi-square statistic
<b>n:</b> Total sample size
<b>r:</b> Number of rows
<b>c:</b> Number of columns
This tutorial provides a couple examples of how to calculate Cramer’s V for a contingency table in Python.
<h3>Example 1: Cramer’s V for a 2×2 Table</h3>
The following code shows how to calculate Cramer’s V for a 2×2 table:
<b>#load necessary packages and functions
import scipy.stats as stats
import numpy as np
#create 2x2 table
data = np.array([[7,12], [9,8]])
#Chi-squared test statistic, sample size, and minimum of rows and columns
X2 = stats.chi2_contingency(data, correction=False)[0]
n = np.sum(data)
minDim = min(data.shape)-1
#calculate Cramer's V 
V = np.sqrt((X2/n) / minDim)
#display Cramer's V
print(V)
0.1617</b>
Cramer’s V turns out to be <b>0.1617</b>, which indicates a fairly weak association between the two variables in the table.
<h3>Example 2: Cramer’s V for Larger Tables</h3>
Note that we can use the <b>CramerV</b> function to calculate Cramer’s V for a table of any size.
The following code shows how to calculate Cramer’s V for a table with 2 rows and 3 columns:
<b>#load necessary packages and functions
import scipy.stats as stats
import numpy as np
#create 2x2 table
data = np.array([[6,9], [8, 5], [12, 9]])
#Chi-squared test statistic, sample size, and minimum of rows and columns
X2 = stats.chi2_contingency(data, correction=False)[0]
n = np.sum(data)
minDim = min(data.shape)-1
#calculate Cramer's V 
V = np.sqrt((X2/n) / minDim)
#display Cramer's V
print(V)
0.1775</b>
Cramer’s V turns out to be <b>0.1775</b>.
Note that this example used a table with 2 rows and 3 columns, but this exact same code works for a table of any dimensions.
<h2><span class="orange">How to Calculate Cramer’s V in R</span></h2>
<b>Cramer’s V</b> is a measure of the strength of association between two  nominal variables .
It ranges from 0 to 1 where:
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a strong association between the two variables.
It is calculated as:
<b>Cramer’s V = √(X<sup>2</sup>/n) / min(c-1, r-1)</b>
where:
<b>X<sup>2</sup>:</b> The Chi-square statistic
<b>n:</b> Total sample size
<b>r:</b> Number of rows
<b>c:</b> Number of columns
This tutorial provides a couple examples of how to calculate Cramer’s V for a contingency table in R.
<h3>Example 1: Cramer’s V for a 2×2 Table</h3>
The following code shows how to use the <b>CramerV</b> function from the <b>rcompanion</b> package to calculate Cramer’s V for a 2×2 table:
<b>#create 2x2 table
data = matrix(c(7,9,12,8), nrow = 2)
#view dataset
data
     [,1] [,2]
[1,]    7   12
[2,]    9    8
#load rcompanion library
library(rcompanion)
#calculate Cramer's V
cramerV(data)
Cramer V 
  0.1617</b>
Cramer’s V turns out to be <b>0.1617</b>, which indicates a fairly weak association between the two variables in the table.
Note that we can also produce a confidence interval for Cramer’s V by indicating <b>ci = TRUE</b>:
<b>cramerV(data, ci = TRUE)
  Cramer.V lower.ci upper.ci
1   0.1617 0.003487   0.4914
</b>
We can see that Cramer’s V remains unchanged at <b>0.1617</b>, but we now have a 95% confidence interval that contains a range of values that is likely to contain the true value of Cramer’s V.
This interval turns out to be: [<b>.003487</b>, <b>.4914</b>].
<h3>Example 2: Cramer’s V for Larger Tables</h3>
Note that we can use the <b>CramerV</b> function to calculate Cramer’s V for a table of any size.
The following code shows how to calculate Cramer’s V for a table with 2 rows and 3 columns:
<b>#create 2x3 table
data = matrix(c(6, 9, 8, 5, 12, 9), nrow = 2)
#view dataset
data
     [,1] [,2] [,3]
[1,]    6    8   12
[2,]    9    5    9
#load rcompanion library
library(rcompanion)
#calculate Cramer's V
cramerV(data)
Cramer V 
  0.1775</b>
Cramer’s V turns out to be <b>0.1775</b>.
<em>You can find the complete documentation for the CramerV function  here .</em>
<h2><span class="orange">How to Create Categorical Variables in R (With Examples)</span></h2>
You can use the following syntax to create a  categorical variable  in R:
<b>#create categorical variable from scratch
cat_variable &lt;- factor(c('A', 'B', 'C', 'D'))
#create categorical variable (with two possible values) from existing variable
cat_variable &lt;- as.factor(ifelse(existing_variable &lt; 4, 1, 0))
#create categorical variable (with multiple possible values) from existing variable
cat_variable &lt;- as.factor(ifelse(existing_variable &lt; 3, 'A',          ifelse(existing_variable &lt; 4, 'B',           ifelse(existing_variable &lt; 5, 'C',           ifelse(existing_variable &lt; 6, 'D',0)))))
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Create a Categorical Variable from Scratch</h3>
The following code shows how to create a categorical variable from scratch:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 10, 12), var4=c(14, 16, 22, 19, 18))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3   14
2    3    7    3   16
3    3    8    6   22
4    4    3   10   19
5    5    2   12   18
#add categorical variable named 'type' to data frame
df$type &lt;- factor(c('A', 'B', 'B', 'C', 'D'))
#view updated data frame
df
  var1 var2 var3 var4 type
1    1    7    3   14    A
2    3    7    3   16    B
3    3    8    6   22    B
4    4    3   10   19    C
5    5    2   12   18    D
</b>
<h3>Example 2: Create a Categorical Variable (with Two Values) from Existing Variable</h3>
The following code shows how to create a categorical variable from an existing variable in a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 10, 12), var4=c(14, 16, 22, 19, 18))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3   14
2    3    7    3   16
3    3    8    6   22
4    4    3   10   19
5    5    2   12   18
#add categorical variable named 'type' using values from 'var4' column
df$type &lt;- as.factor(ifelse(df$var1 &lt; 4, 1, 0))
#view updated data frame
df
  var1 var2 var3 var4 type
1    1    7    3   14    1
2    3    7    3   16    1
3    3    8    6   22    1
4    4    3   10   19    0
5    5    2   12   18    0</b>
Using the <b>ifelse()</b> statement, we created a new categorical variable called “type” that takes the following values:
<b>1</b> if the value in the ‘var1’ column is less than 4.
<b>0</b> if the value in the ‘var1’ column is not less than 4.
<h3>
<b>Example 3: Create a Categorical Variable (with Multiple Values) from Existing Variable</b>
</h3>
The following code shows how to create a categorical variable (with multiple values) from an existing variable in a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 10, 12), var4=c(14, 16, 22, 19, 18))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3   14
2    3    7    3   16
3    3    8    6   22
4    4    3   10   19
5    5    2   12   18
#add categorical variable named 'type' using values from 'var4' column
df$type &lt;- as.factor(ifelse(df$var1 &lt; 3, 'A',     ifelse(df$var1 &lt; 4, 'B',      ifelse(df$var1 &lt; 5, 'C',      ifelse(df$var1 &lt; 6, 'D', 'E')))))
#view updated data frame
df
  var1 var2 var3 var4 type
1    1    7    3   14    A
2    3    7    3   16    B
3    3    8    6   22    B
4    4    3   10   19    C
5    5    2   12   18    D</b>
Using the <b>ifelse()</b> statement, we created a new categorical variable called “type” that takes the following values:
‘<b>A</b>‘ if the value in the ‘var1’ column is less than 3.
Else, ‘<b>B</b>‘ if the value in the ‘var1’ column is less than 4.
Else, ‘<b>C</b>‘ if the value in the ‘var1’ column is less than 5.
Else, ‘<b>D</b>‘ if the value in the ‘var1’ column is less than 6.
Else, ‘<b>E</b>‘.
<h2><span class="orange">How to Create a New Column Based on a Condition in Pandas</span></h2>
Often you may want to create a new column in a pandas DataFrame based on some condition.
This tutorial provides several examples of how to do so using the following DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame
df
ratingpointsassistsrebounds
09025511
1852078
28214710
3881686
4942756
5902079
6761266
77515910
88714910
9861957
</b>
<h3>Example 1: Create a New Column with Binary Values</h3>
The following code shows how to create a new column called ‘Good’ where the value is ‘yes’ if the points in a given row is above 20 and ‘no’ if not:
<b>#create new column titled 'Good'
df['Good'] = np.where(df['points']>20, 'yes', 'no')
#view DataFrame 
df
        ratingpointsassistsrebounds  Good
09025511  yes
1852078  no
28214710  no
3881686  no
4942756  yes
5902079  no
6761266  no
77515910  no
88714910  no
9861957  no
</b>
<h3>Example 2: Create a New Column with Multiple Values</h3>
The following code shows how to create a new column called ‘Good’ where the value is:
‘Yes’ if the points ≥ 25
‘Maybe’ if 15 ≤ points &lt; 25
‘No’ if points &lt; 15
<b>#define function for classifying players based on points
def f(row):
    if row['points'] &lt; 15:
        val = 'no'
    elif row['points'] &lt; 25:
        val = 'maybe'
    else:
        val = 'yes'
    return val
#create new column 'Good' using the function above
df['Good'] = df.apply(f, axis=1)
#view DataFrame 
df
        ratingpointsassistsrebounds Good
09025511 yes
1852078 maybe
28214710 no
3881686 maybe
4942756 yes
5902079 maybe
6761266 no
77515910 maybe
88714910 no
9861957 maybe
</b>
<h3>Example 3: Create a New Column Based on Comparison with Existing Column</h3>
The following code shows how to create a new column called ‘assist_more’ where the value is:
‘Yes’ if assists > rebounds.
‘No’ otherwise.
<b>#create new column titled 'assist_more'
df['assist_more'] = np.where(df['assists']>df['rebounds'], 'yes', 'no')
#view DataFrame 
df
        ratingpointsassistsrebounds assist_more
09025511 no
1852078 no
28214710 no
3881686 yes
4942756 no
5902079 no
6761266 no
77515910 no
88714910 no
9861957 no</b>
<em>You can find more Python tutorials  here .</em>
<h2><span class="orange">How to Create an Empty Data Frame in R (With Examples)</span></h2>
There are two basic ways to create an empty data frame in R:
<b>Method 1: Matrix with Column Names</b>
<b>#create data frame with 0 rows and 3 columns
df &lt;- data.frame(matrix(ncol = 3, nrow = 0))
#provide column names
colnames(df) &lt;- c('var1', 'var2', 'var3')
</b>
<b>Method 2: Initialize Empty Vectors</b>
<b>#create data frame with 5 empty vectors
df2 &lt;- data.frame(Doubles=double(), Integers=integer(), Factors=factor(), Logicals=logical(), Characters=character(), stringsAsFactors=FALSE)
</b>
This tutorial shows examples of how to use these two methods in practice.
<h3>Method 1: Matrix with Column Names</h3>
The first way to create an empty data frame is by using the following steps:
Define a matrix with 0 rows and however many columns you’d like.
Then use the <b>data.frame()</b> function to convert it to a data frame and the <b>colnames()</b> function to give it column names.
Then use the <b>str() </b>function to analyze the structure of the resulting data frame.
For example:
<b>#create data frame with 0 rows and 5 columns
df &lt;- data.frame(matrix(ncol = 5, nrow = 0))
#provide column names
colnames(df) &lt;- c('var1', 'var2', 'var3', 'var4', 'var5')
#view structure of the data frame
str(df)
'data.frame':0 obs. of  5 variables:
 $ var1: logi 
 $ var2: logi 
 $ var3: logi 
 $ var4: logi 
 $ var5: logi 
</b>
We can see that the resulting data frame has 0 observations (i.e. rows),  5 variables (i.e. columns), and each of the variables are of the class <em>logical</em>.
Although each variable is of the class <em>logical</em>, you can still add rows to the variables that are of different types.
<h3>Method 2: Initialize Empty Vectors</h3>
The second way to create an empty data frame is by using the following steps:
Define a data frame as a set of empty vectors with specific class types.
Specify <b>stringsAsFactors=False</b> so that any character vectors are treated as strings, not factors.
For example:
<b>#create data frame with 5 empty vectors
df2 &lt;- data.frame(Doubles=double(),  Integers=integer(),  Factors=factor(),  Logicals=logical(),  Characters=character(),  stringsAsFactors=FALSE)
#view structure of the data frame
str(df2)
'data.frame':0 obs. of  5 variables:
 $ Doubles   : num 
 $ Integers  : int 
 $ Factors   : Factor w/ 0 levels: 
 $ Logicals  : logi 
 $ Characters: chr  
</b>
We can see that the resulting data frame has 0 observations (i.e. rows),  5 variables (i.e. columns), and each of the variables are five different classes.
Note that we were also able to provide column names for the data frame in just one step (e.g. the first column name is “Doubles”, the second column name is “Integers” and so on.
<h2><span class="orange">How to Create an Empty List in R (With Examples)</span></h2>
You can use the following syntax to create an empty list in R:
<b>#create empty list with length of zero
empty_list &lt;- list()
#create empty list of length 10
empty_list &lt;- vector(mode='list', length=10)
</b>
The following examples show how to use these functions in practice.
<h3>Example 1: Create Empty List in R with Length of Zero</h3>
The following code shows how to create an empty list with a length of zero in R:
<b>#create empty list
empty_list &lt;- list()
#verify that empty_list is of class 'list'
class(empty_list)
[1] "list"
#view length of list
length(empty_list)
[1] 0
</b>
The result is a list with a length of 0.
<h3>Example 2: Create Empty List in R with Specific Length</h3>
The following code shows how to create an empty list of length 8 in R:
<b>#create empty list of length 8
empty_list &lt;- vector(mode='list', length=8)
#verify that empty_list is of class 'list'
class(empty_list)
[1] "list"
#view list
empty_list
[[1]]
NULL
[[2]]
NULL
[[3]]
NULL
[[4]]
NULL
[[5]]
NULL
[[6]]
NULL
[[7]]
NULL
[[8]]
NULL
</b>
The result is a list with a length of 8 in which every element in the list is NULL.
<h3>Example 3: Append Values to Empty List in R</h3>
One of the most common reasons to create an empty list is to then fill it with values using a loop.
The following code shows how to create an empty list, then fill it with values:
<b>#create empty list of length 8
empty_list &lt;- vector(mode='list', length=8)
#get length of list
len &lt;- length(empty_list)
#define values to append to list
new &lt;- c(3, 5, 12, 14, 17, 18, 18, 20)
#fill values in list
i = 1
while(i &lt;= length(new)) {
    empty_list[[i]] &lt;- new[i]
    i &lt;- i + 1
}
#display updated list
empty_list
[[1]]
[1] 3
[[2]]
[1] 5
[[3]]
[1] 12
[[4]]
[1] 14
[[5]]
[1] 17
[[6]]
[1] 18
[[7]]
[1] 18
[[8]]
[1] 20
</b>
Notice that the empty list is now filled with the new values that we specified.
<h2><span class="orange">How to Create an Empty Matrix in R (With Examples)</span></h2>
You can use the following syntax to create an empty matrix of a specific size in R:
<b>#create empty matrix with 10 rows and 3 columns
empty_matrix &lt;- matrix(, nrow=10, ncol=3)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Create Empty Matrix of Specific Size</h3>
The following code shows how to create an empty matrix of a specific size in R:
<b>#create empty matrix with 10 rows and 3 columns
empty_matrix &lt;- matrix(, nrow=10, ncol=3)
#view empty matrix
empty_matrix
      [,1] [,2] [,3]
 [1,]   NA   NA   NA
 [2,]   NA   NA   NA
 [3,]   NA   NA   NA
 [4,]   NA   NA   NA
 [5,]   NA   NA   NA
 [6,]   NA   NA   NA
 [7,]   NA   NA   NA
 [8,]   NA   NA   NA
 [9,]   NA   NA   NA
[10,]   NA   NA   NA
#view class
class(empty_matrix)
[1] "matrix" "array" 
</b>
The result is a matrix with 10 rows and 3 columns in which every element in the matrix is blank.
<h3>Example 2: Create Matrix of Unknown Size</h3>
If you don’t know what the final size of the matrix will be ahead of time, you can use the following code to generate the data for the columns of the matrix and bind each column together using the  cbind()  function:
<b>#create empty list
my_list &lt;- list()
#add data using for loop
for(i in 1:4) {
    my_list[[i]] &lt;- rnorm(10)
}
#column bind values into a matrix
my_matrix = do.call(cbind, my_list)
#view final matrix
my_matrix
            [,1]        [,2]       [,3]       [,4]
 [1,]  1.3064332  1.18175760  2.1603867  1.2378847
 [2,]  0.8618439  0.66663694  0.1113606  0.2062029
 [3,] -0.4689356 -0.03200797 -1.3872632  1.6531437
 [4,] -0.4664767 -0.79285400  0.3972758  0.1632975
 [5,]  0.5880580  1.05795303 -0.5655543 -0.3557376
 [6,]  0.5412100 -0.32070294 -0.3687303 -1.1778959
 [7,]  0.5073627 -0.24925226  1.0031305  0.6336998
 [8,]  0.8047177  0.10968558  0.3225197  1.6776955
 [9,]  1.5755134  1.40435730  1.8360239  0.5612274
[10,] -0.6430913  0.01173386  0.3181037 -0.8414270
</b>
The result is a matrix with 10 rows and 4 columns.
<h2><span class="orange">How to Create an Empty Vector in R (With Examples)</span></h2>
You can use one of the following methods to create an empty vector in R:
<b>#create empty vector with length zero and no specific class
empty_vec &lt;- vector()
#create empty vector with length zero and a specific class
empty_vec &lt;- character()
#create empty vector with specific length
empty_vec &lt;- rep(NA, times=10)
</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Create Empty Vector with Length Zero</h3>
The following code shows how to create a vector with a length of zero and no specific class:
<b>#create empty vector with length zero and no specific class
empty_vec &lt;- vector()
#display length of vector
length(empty_vec)
[1] 0
</b>
We can then fill the vector with values if we’d like:
<b>#add values 1 through 10 to empty vector
empty_vec &lt;- c(empty_vec, 1:10)
#view updated vector
empty_vec
[1]  1  2  3  4  5  6  7  8  9 10
</b>
<h3>Method 2: Create Empty Vector of Specific Class</h3>
The following code shows how to create empty vectors of specific classes:
<b>#create empty vector of class 'character'
empty_vec &lt;- character()
class(empty_vec)
[1] "character"
#create empty vector of class 'numeric'
empty_vec &lt;- numeric()
class(empty_vec)
numeric(0)
#create empty vector of class 'logical'
empty_vec &lt;- logical()
class(empty_vec)
logical(0)
</b>
<h3>Method 3: Create Empty Vector with Specific Length</h3>
The following code shows how to create a vector with a specific length in R:
<b>#create empty vector with length 10
empty_vec &lt;- rep(NA, times=10)
#display empty vector
empty_vec
[1] NA NA NA NA NA NA NA NA NA NA
</b>
If you know the length of the vector at the outset, this is the most memory-efficient solution in R.
<h2><span class="orange">How to Create a Table in Google Sheets (Step-by-Step)</span></h2>
This tutorial provides a step-by-step example of how to create beautiful tables in Google Sheets.
<h3>Step 1: Enter the Raw Data</h3>
First, let’s enter the raw values for some dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t1.png">
<h3>Step 2: Format the Header</h3>
First, let’s make the header text bold and centered:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t2.png">
<h3>Step 3: Format the Columns</h3>
Next, we’ll format the values in the columns. As a rule of thumb, use the following formats:
<b>Text:</b> Align text values left.
<b>Numbers:</b> Align numbers center.
Our text columns are already left-aligned, so we’ll simply center-align the values in the points column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t3.png">
<h3>Step 4: Use Alternating Colors</h3>
Next, we can format the table to use alternating colors.
Simply highlight all of our data, then click <b>Format</b> along the top ribbon, then click <b>Alternating colors</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t4-1.png">
In the new window that appears, click any style that you’d like. We’ll choose the teal style:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t5.png">
<h3>Step 5: Add a Border</h3>
Lastly, we’ll add a border to every cell in our table.
Simply highlight all of our data, then click the <b>Border</b> button and click <b>All borders</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t6.png">
Our table is complete:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/t7.png">
<h2><span class="orange">How to Create Tables in Excel (With Example)</span></h2>
The following step-by-step example shows how to create and format tables in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following data about various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table11.jpg"460">
<h3>Step 2: Create the Table</h3>
To turn this data into a table, first highlight all of the cells in the range <b>A1:C11</b>.
Then click the <b>Insert</b> tab along the top ribbon and then click the <b>Table</b> icon:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table12.jpg">
In the new window that appears, verify that the range for the table is correct and check the box next to <b>My table has headers</b>, then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table13.jpg"475">
The following table will automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table14.jpg"483">
<h3>Step 3: Format the Table</h3>
To modify the appearance of the table, simply click on any value in the table. Then click on the <b>Table Design</b> tab along the top ribbon.
This will give you a variety of options to change the appearance of the table.
For our example, we’ll uncheck the box next to <b>Filter Button</b> to remove the dropdown arrows in the header of our table and we’ll choose the yellow table style:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table15.jpg"679">
The style of our table will automatically be updated:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/table16.jpg"489">
Feel free to play around with the design options in the <b>Table Design</b> tab to make the table appear however you’d like.
<h2><span class="orange">How to Create Tables in Python (With Examples)</span></h2>
The easiest way to create tables in Python is to use <b>tablulate()</b> function from the  tabulate  library.
To use this function, we must first install the library using pip:
<b>pip install tabulate
</b>
We can then load the library:
<b>from tabulate import tabulate
</b>
We can then use the following basic syntax to create tables:
<b>print(tabulate(data, headers=col_names, tablefmt="grid", showindex="always"))
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Create Table with Headers</h3>
The following code shows how to create a basic table with headers:
<b>#create data
data = [["Mavs", 99], 
        ["Suns", 91], 
        ["Spurs", 94], 
        ["Nets", 88]]
  
#define header names
col_names = ["Team", "Points"]
  
#display table
print(tabulate(data, headers=col_names))
Team      Points
------  --------
Mavs          99
Suns          91
Spurs         94
Nets          88
</b>
<h3>Example 2: Create Table with Fancy Grid</h3>
The following code shows how to create a table with headers and a fancy grid:
<b>#create data
data = [["Mavs", 99], 
        ["Suns", 91], 
        ["Spurs", 94], 
        ["Nets", 88]]
  
#define header names
col_names = ["Team", "Points"]
  
#display table
print(tabulate(data, headers=col_names, tablefmt="fancy_grid"))
╒════════╤══════════╕
│ Team   │   Points │
╞════════╪══════════╡
│ Mavs   │       99 │
├────────┼──────────┤
│ Suns   │       91 │
├────────┼──────────┤
│ Spurs  │       94 │
├────────┼──────────┤
│ Nets   │       88 │
╘════════╧══════════╛</b>
Note that the <b>tablefmt</b> argument accepts several different options including:
grid
fancy_grid
pipe
pretty
simple
Refer to the  tabulate documentation  for a complete list of potential table formats.
<h3>Example 3: Create Table with Index Column</h3>
The following code shows how to create a table with headers, a fancy grid, and an index column:
<b>#create data
data = [["Mavs", 99], 
        ["Suns", 91], 
        ["Spurs", 94], 
        ["Nets", 88]]
  
#define header names
col_names = ["Team", "Points"]
  
#display table
print(tabulate(data, headers=col_names, tablefmt="fancy_grid", showindex="always"))
╒════╤════════╤══════════╕
│    │ Team   │   Points │
╞════╪════════╪══════════╡
│  0 │ Mavs   │       99 │
├────┼────────┼──────────┤
│  1 │ Suns   │       91 │
├────┼────────┼──────────┤
│  2 │ Spurs  │       94 │
├────┼────────┼──────────┤
│  3 │ Nets   │       88 │
╘════╧════════╧══════════╛</b>
<h2><span class="orange">How to Create Tables in R (With Examples)</span></h2>
There are two ways to quickly create tables in R:
<b>Method 1: Create a table from existing data.</b>
<b>tab &lt;- table(df$row_variable, df$column_variable)
</b>
<b>Method 2: Create a table from scratch.</b>
<b>tab &lt;- matrix(c(7, 5, 14, 19, 3, 2, 17, 6, 12), ncol=3, byrow=TRUE)
colnames(tab) &lt;- c('colName1','colName2','colName3')
rownames(tab) &lt;- c('rowName1','rowName2','rowName3')
tab &lt;- as.table(tab)
</b>
This tutorial shows an example of how to create a table using each of these methods.
<h3>Create a Table from Existing Data</h3>
The following code shows how to create a table from existing data:
<b>#make this example reproducible
set.seed(1)
#define data
df &lt;- data.frame(team=rep(c('A', 'B', 'C', 'D'), each=4), pos=rep(c('G', 'F'), times=8), points=round(runif(16, 4, 20),0))
#view head of data 
head(df)
  team pos points
1    A   G      8
2    A   F     10
3    A   G     13
4    A   F     19
5    B   G      7
6    B   F     18
#create table with 'position' as rows and 'team' as columns
tab1 &lt;- table(df$pos, df$team)
tab1
  A B C D
F 2 2 2 2
G 2 2 2 2
</b>
This table displays the frequencies for each combination of team and position. For example:
2 players are on position ‘F’ on team ‘A’
2 players are on position ‘G’ on team ‘A’
2 players are on position ‘F’ on team ‘B’
2 players are on position ‘G’ on team ‘B’
And so on.
<h3>Create a Table from Scratch</h3>
The following code shows how to create a table with 4 columns a 2 rows from scratch:
<b>#create matrix with 4 columns
tab &lt;- matrix(rep(2, times=8), ncol=4, byrow=TRUE)
#define column names and row names of matrix
colnames(tab) &lt;- c('A', 'B', 'C', 'D')
rownames(tab) &lt;- c('F', 'G')
#convert matrix to table 
tab &lt;- as.table(tab)
#view table 
tab
  A B C D
F 2 2 2 2
G 2 2 2 2</b>
Notice that this table is the exact same as the one created in the previous example.
<h2><span class="orange">How to Create Tables in SAS (With Examples)</span></h2>
You can use <b>proc sql</b> to quickly create tables in SAS.
There are two ways to do so:
<b>1.</b> Create a Table from Scratch
<b>2.</b> Create a Table from Existing Data
The following examples show how to do both using <b>proc sql</b>.
<h2>Example 1: Create a Table from Scratch</h2>
The following code shows how to create a table with three columns using <b>proc sql</b> in SAS:
<b>/*create empty table*/
proc sql;
   create table my_table
       (team char(10),
        points num,
        rebounds num);
/*insert values into table*/          
   insert into my_table
      values('Mavs', 99, 22)
      values('Hawks', 104, 20)
      values('Hornets', 88, 25)
      values('Lakers', 113, 19)
      values('Warriors', 109, 32);
/*display table*/
   select * from my_table;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/tablesas1.jpg">
We used <b>create table</b> to create an empty table, then used <b>insert into</b> to add values to the table, then used <b>select * from</b> to display the table.
The result is a table with three columns that show various information for different basketball teams.
<h2>Example 2: Create a Table from Existing Data</h2>
The following code shows how to use <b>proc sql</b> to create a table by using an existing dataset that we created in the previous example:
<b>/*create table from existing datset*/
proc sql;
   create table my_table2 as
      select team as Team_Name,
             points as Points_Scored
         from my_table;
         
/*display table*/
   select * from my_table2;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/tablesas2.jpg"208">
The result is a table that contains two columns with values that come from an existing dataset.
<b>Note</b>: We used the <b>as</b> function to specify the column names to be used in the table, but you don’t have to use the <b>as</b> function if you don’t want to rename the columns.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Create Frequency Tables in SAS 
 How to Count Distinct Values in SAS 
 How to Count Observations by Group in SAS 
<h2><span class="orange">How to Use createDataPartition() Function in R</span></h2>
You can use the <b>createDataPartition()</b> function from the <b>caret</b> package in R to partition a data frame into training and testing sets for model building.
This function uses the following basic syntax:
<b>createDataPartition(y, times = 1, p = 0.5, list = TRUE, …)</b>
where:
<b>y</b>: vector of outcomes
<b>times</b>: number of partitions to create
<b>p</b>: percentage of data to use in training set
<b>list</b>: whether to store results in list or not
The following example shows how to use this function in practice.
<h2>Example: Using createDataPartition() in R</h2>
Suppose we have some data frame in R with 1,000 rows that contains information about <b>hours</b> studied by students and their corresponding <b>score</b> on a final exam:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(hours=runif(1000, min=0, max=10), score=runif(1000, min=40, max=100))
#view head of data frame
head(df)
     hours    score
1 8.966972 55.93220
2 2.655087 71.84853
3 3.721239 81.09165
4 5.728534 62.99700
5 9.082078 97.29928
6 2.016819 47.10139</b>
Suppose we would like to fit a  simple linear regression model  that uses hours studied to predict final exam score.
Suppose we would like to train the model on 80% of the rows in the data frame and test it on the remaining 20% of rows.
The following code shows how to use the <b>createDataPartition()</b> function from the <b>caret</b> package to split the data frame into training and testing sets:
<b>library(caret)
#partition data frame into training and testing sets
train_indices &lt;- createDataPartition(df$score, times=1, p=.8, list=FALSE)
#create training set
df_train &lt;- df[train_indices , ]
#create testing set
df_test  &lt;- df[-train_indices, ]
#view number of rows in each set
nrow(df_train)
[1] 800
nrow(df_test)
[1] 200
</b>
We can see that our training dataset contains 800 rows, which represents 80% of the original dataset.
Similarly, we can see that our test dataset contains 200 rows, which represents 20% of the original dataset.
We can also view the first few rows of each set:
<b>#view head of training set
head(df_train)
     hours    score
1 8.966972 55.93220
2 2.655087 71.84853
3 3.721239 81.09165
4 5.728534 62.99700
5 9.082078 97.29928
7 8.983897 42.34600
#view head of testing set
head(df_test)
      hours    score
6  2.016819 47.10139
12 2.059746 96.67170
18 7.176185 92.61150
23 2.121425 89.17611
24 6.516738 50.47970
25 1.255551 90.58483
</b>
We can then proceed to train the regression model using the training set and assess its performance using the testing set.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Perform K-Fold Cross Validation in R 
 How to Perform Multiple Linear Regression in R 
 How to Perform Logistic Regression in R 
<h2><span class="orange">A Simple Explanation of Criterion Validity</span></h2>
<b>Criterion validity</b> refers to how well the measurement of one variable can predict the response of another variable.
One variable is referred to as the  explanatory variable  while the other variable is referred to as the  criterion variable .
For example, we might want to know how well some college entrance exam is able to predict the first semester grade point average of students.
The entrance exam would be the explanatory variable and the criterion variable would be the first semester GPA.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity1.png">
We want to know if it’s <em>valid</em> to use this particular explanatory variable as a way to predict the criterion variable.
<h3>How to Measure Criterion Validity</h3>
We typically measure criterion validity using a metric like the  Pearson Correlation Coefficient , which takes on value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the association between the two variables.
For example, if we collected data on entrance exam scores and first semester GPA for 1,000 students and found that the correlation between the two variables was <b>0.843</b> then this would mean the two variables are highly correlated.
In other words, students who score high on the entrance exam also tend to earn high GPA’s during their first semester. Conversely, students who score low on the entrance exam tend to earn low GPA’s during their first semester.
<h3>Types of Criterion Validity</h3>
There are two main types of criterion validity:
<b>1.  Predictive Validity </b>
The first type of criterion validity is known as predictive validity, which determines whether or not the measurement of one variable is able to accurately predict the measurement of some variable in the future.
The previous example of measuring a student’s college entrance exam score and their first semester GPA is an example of measuring predictive validity because we measure the two variables at different points in time.
In other words, we’re trying to determine if the entrance exam score can <em>predict</em> first semester GPA well.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity2.png">
<b>2.  Concurrent Validity </b>
The second type of criterion validity is known as concurrent validity, which measures two variables <em>concurrently</em> (i.e. at the same time) to see if one variable is significantly associated with the other.
An example of this would be if a company administers some type of test to see if the scores on the test are correlated with employee productivity.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/criterionValidity3.png">
The benefit of this approach is that we don’t have to wait until some point in the future to take a measurement on the criterion variable we’re interested in.
<h2><span class="orange">What is a Criterion Variable? (Explanation + Examples)</span></h2>
A <b>criterion variable </b>is simply another name for a <em>dependent variable </em>or a  <em>response variable</em> . This is the variable that is being predicted in a statistical analysis.
Just as explanatory variables have different names like <em>predictor variables </em>or <em>independent variables</em>, a response variable also has interchangeable names like <em>dependent variable </em>or <b><em>criterion variable</em></b>.
<h2>What are Some Examples of Criterion Variables?</h2>
The following scenarios illustrate examples of criterion variables in several different settings.
<h3>Example 1: Simple Linear Regression</h3>
 Simple linear regression  is a statistical method we use to understand the relationship between two variables, x and y. One variable, x, is known as the predictor variable. The other variable, y, is known as the <b>criterion variable</b>, or <em>response variable</em>.
In simple linear regression, we find a “line of best fit” that describes the relationship between the predictor variable and the criterion variable.
For example, we may fit a simple linear regression model to a dataset using <em>hours studied </em>as the predictor variable and <em>test score </em>as the criterion variable. In this case, we would use simple linear regression to attempt to predict the value of our criterion variable <em>test score</em>.
Or, as another example, we may fit a simple linear regression model to a dataset using <em>weight </em>to predict the value for <em>height </em>for a group of people. In this case, our criterion variable is <em>height </em>since that’s the value we’re interested in predicting.
If we plotted the values for height and weight on a  scatterplot , the criterion variable <em>height </em>would be on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height.jpg">
In general, the criterion variablewill be along the y-axis when we create a scatterplot and the predictor variable will be along the x-axis.
<h3>Example 2: Multiple Linear Regression</h3>
<b>Multiple linear regression </b>is similar to simple linear regression, except we use several predictor variables to predict the value of one criterion variable. 
For example, we may use the predictor variables <em>hours studied </em>and <em>hours of sleep the night before the test</em> to predict the value of the criterion variable <em>test score</em>. In this case, our criterion variable is the variable being predicted in this analysis.
<h3>Example 3: ANOVA</h3>
An <b>ANOVA </b>(analysis of variance) is a statistical technique we use to find out if there is a statistically significant difference between the means of three or more independent groups.
For example, we may want to determine if three different exercise programs impact weight loss differently. The predictor variable we’re studying is <em>exercise program</em> and it has three <em>levels</em>.
The <b>criterion variable</b> is <em>weight loss, </em>measured in pounds. We can conduct a  one-way ANOVA  to determine if there is a statistically significant difference between the resulting weight loss from the three programs.
In this case, we’re interested in understanding whether the value of the criterion variable <em>weight loss </em>differs among the three exercise programs. 
If we instead analyzed <em>exercise program </em>and  <em>average hours slept per night,</em> we would conduct a two-way ANOVA since we are interested in seeing how two factors impact weight loss.
Once again, though, our <b>criterion variable</b> is still <em>weight loss </em>because we are interested in how the value of this variable differs for different levels of <em>exercise </em>and <em>sleep</em>.
<b>Additional Reading: </b> A Simple Explanation of Criterion Validity 
<h2><span class="orange">How to Calculate Critical Values in Google Sheets</span></h2>
Whenever you perform a  hypothesis test , you will get a test statistic as a result. To determine if the results of the test are statistically significant, you can compare the test statistic to a<b> critical value</b>.
If the absolute value of the test statistic is greater than the critical value, then the results of the test are statistically significant.
To find a <b>t-critical value</b> in Google Sheets, you can use the following syntax:
<b>T.INV</b>(probability, deg_freedom) – Returns t-critical value for a one-tailed t-test.
<b>T.INV.2T</b>(probability, deg_freedom) – Returns t-critical value for a two-tailed t-test.
To find a <b>z-critical value</b> in Google sheets, you can use the following syntax:
<b>NORM.S.INV</b>(probability) – Returns critical value for a one-tailed z-test.
<b>NORM.S.INV</b>(probability/2) – Returns critical value for a two-tailed z-test.
The following examples show how to use each of these functions in practice.
<h3>Example 1: One-Tailed T-Test Critical Value</h3>
To find the t-critical value for a left-tailed test with a significance level of 0.05 and degrees of freedom = 11, we can type the following formula into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/crit1.png">
The critical value is <b>-1.79588</b>.
<h3>Example 2: Two-Tailed T-Test Critical Value</h3>
To find the t-critical value for a two-tailed test with a significance level of 0.05 and degrees of freedom = 11, we can type the following formula into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/crit2.png">
The critical value is <b>2.200985</b>.
<h3>Example 3: One-Tailed Z-Test Critical Value</h3>
To find the z-critical value for a left-tailed test with a significance level of 0.05, we can type the following formula into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/crit3.png">
The critical value is <b>-1.64485</b>.
<h3>Example 4: Two-Tailed Z-Test Critical Value</h3>
To find the z-critical value for a two-tailed test with a significance level of 0.05, we can type the following formula into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/crit4.png">
The critical value is <b>-1.96</b>.
<h3>Cautions on Finding Critical Values in Google Sheets</h3>
Note that the <b>T.INV</b>(), <b>T.INV.2T</b>(), and <b>NORM.S.INV</b>() functions will throw an error if any of the following occur:
If any argument is non-numeric.
If the value for <em>probability </em>is less than zero or greater than 1.
If the value for <em>deg_freedom</em><em> </em>is less than 1.
You can find more Google Sheets tutorials on  this page .
<h2><span class="orange">Critical Z Value Calculator</span></h2>
This calculator finds the <b>z critical value</b> associated with a given significance level.
Simply fill in the significance level below, then click the “Calculate” button.
<label for="sig"><b>Significance level</b></label>
<input type="number" id="sig" min="0" max="1" value=".05">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
z critical value (right-tailed): <b>1.645</b>
z critical value (two-tailed): +/- <b>1.960</b>
<script>
function calc() {
//get input values and calculate WHIP
var sig = document.getElementById('sig').value*1;
var answer = Math.abs(jStat.normal.inv(sig, 0, 1));
var answer2 = Math.abs(jStat.normal.inv(sig/2, 0, 1));
//output
document.getElementById('answer').innerHTML = answer.toFixed(3);
document.getElementById('answer2').innerHTML = answer2.toFixed(3);
}
</script>
<h2><span class="orange">Cronbach’s Alpha Calculator</span></h2>
<b>Cronbach’s Alpha</b> is a way to measure the  internal consistency  of a survey or questionnaire.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
To calculate Cronbach’s Alpha for a given survey or questionnaire, simply enter the responses for up to five questions into the boxes below, then press the “Calculate” button.
<b>Responses to Question 1</b>
<textarea id="a" rows="3" cols="40">1, 2, 2, 3, 2, 2, 3, 3, 2, 3</textarea>
<b>Responses to Question 2</b>
<textarea id="b" rows="3" cols="40">1, 1, 1, 2, 3, 3, 2, 3, 3, 3</textarea>
<b>Responses to Question 3</b>
<textarea id="c" rows="3" cols="40">1, 1, 2, 1, 2, 3, 3, 3, 2, 3</textarea>
<b>Responses to Question 4</b>
<textarea id="d" rows="3" cols="40"></textarea>
<b>Responses to Question 5</b>
<textarea id="e" rows="3" cols="40"></textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Cronbach’s Alpha: 0.77344
<script>
function calc() {
//define addition function
function add(a, b) {return a + b;}
//get raw data for each group
var group_a  = document.getElementById('a').value.split(',').map(Number);
var group_b  = document.getElementById('b').value.split(',').map(Number);
var group_c  = document.getElementById('c').value.split(',').map(Number);
var group_d  = document.getElementById('d').value.split(',').map(Number);
var group_e  = document.getElementById('e').value.split(',').map(Number);
//verify they exist
if (group_a.length < 2){ group_a = null};
if (group_b.length < 2){ group_b = null};
if (group_c.length < 2){ group_c = null};
if (group_d.length < 2){ group_d = null};
if (group_e.length < 2){ group_e = null};
var all_groups_holder = group_a.concat(group_b, group_c, group_d, group_e);
var all_groups = all_groups_holder.filter(function (el) {
  return el != null;
});
//find total number of groups (k)
if (group_a != null) { var flag_group_a = 1; } else { var flag_group_a = 0;};
if (group_b != null) { var flag_group_b = 1; } else { var flag_group_b = 0;};
if (group_c != null) { var flag_group_c = 1; } else { var flag_group_c = 0;};
if (group_d != null) { var flag_group_d = 1; } else { var flag_group_d = 0;};
if (group_e != null) { var flag_group_e = 1; } else { var flag_group_e = 0;};
var k = [flag_group_a, flag_group_b, flag_group_c, flag_group_d, flag_group_e].reduce(add, 0);
//find sum of variances (si2)
if (group_a != null) { var var_group_a = jStat.variance(group_a,true); } else { var var_group_a = 0;};
if (group_b != null) { var var_group_b = jStat.variance(group_b,true); } else { var var_group_b = 0;};
if (group_c != null) { var var_group_c = jStat.variance(group_c,true); } else { var var_group_c = 0;};
if (group_d != null) { var var_group_d = jStat.variance(group_d,true); } else { var var_group_d = 0;};
if (group_e != null) { var var_group_e = jStat.variance(group_e,true); } else { var var_group_e = 0;};
var si2 = [var_group_a, var_group_b, var_group_c, var_group_d, var_group_e].reduce(add, 0);
var jump = all_groups.length / k;
//find sum of row total variances (sy2)
let row_totals = [];
for (let i = 0; i < group_a.length; i++) {
row_totals[i] = [all_groups[i], all_groups[i+jump], all_groups[i+jump*2]].reduce(add, 0);
}
var sy2 = jStat.variance(row_totals,true);
//compute Cronbach's Alpha
varalpha = (k/(k-1))*((sy2-si2)/sy2);
//output Cronbach's Alpha        
document.getElementById('alpha').innerHTML = alpha.toFixed(5);
}
</script>
<h2><span class="orange">How to Calculate Cronbach’s Alpha in Excel</span></h2>
<b>Chronbach’s Alpha</b> is a way to measure the  internal consistency  of a questionnaire or survey.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
The following step-by-step example explains how to calculate Cronbach’s Alpha in Excel.
<h3>Step 1: Enter the Data</h3>
Suppose a restaurant manager wants to measure overall satisfaction among customers. She decides to send out a survey to 10 customers who can rate the restaurant on a scale of 1 to 3 for various categories.
First, let’s enter the data that contains the survey responses for each of the 10 customers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cronbachExcel1.png">
<h3>Step 2: Perform a Two-Factor ANOVA Without Replication</h3>
Next, we’ll perform a two-way ANOVA without replication.
To do so, click the <b>Data</b> tab along the top ribbon and then click the <b>Data Analysis</b> option under the <b>Analysis</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option available, you need to first  load the Analysis ToolPak .
In the dropdown menu that appears, click <b>Anova: Two-Factor Without Replication</b> and then click <b>OK</b>. In the new window that appears, fill in the following information and then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cronbachExcel2.png">
The following results will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cronbachExcel3.png">
<h3>Step 3: Calculate Cronbach’s Alpha</h3>
Next, we’ll use the following formula to calculate Cronbach’s Alpha:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/cronbachExcel4.png">
Cronbach’s Alpha turns out to be <b>0.773</b>.
The following table describes how different values of Cronbach’s Alpha are usually interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
Since we calculated Cronbach’s Alpha to be <b>0.773</b>, we would say that the internal consistency of this survey is “Acceptable.”
<b>Bonus:</b> Feel free to use this  Cronbach’s Alpha Calculator  to automatically find Cronbach’s Alpha for a given dataset.
<h2><span class="orange">How to Calculate Cronbach’s Alpha in Python</span></h2>
<b>Chronbach’s Alpha</b> is a way to measure the  internal consistency  of a questionnaire or survey.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
The following example shows how to calculate Cronbach’s Alpha in Python.
<h3>Example: Calculating Cronbach’s Alpha in Python</h3>
Suppose a restaurant manager wants to measure overall satisfaction among customers, so she sends out a survey to 10 customers who can rate the restaurant on a scale of 1 to 3 for various categories.
The following pandas DataFrame shows the results of the survey:
<b>import pandas as pd
#enter survey responses as a DataFrame
df = pd.DataFrame({'Q1': [1, 2, 2, 3, 2, 2, 3, 3, 2, 3],   'Q2': [1, 1, 1, 2, 3, 3, 2, 3, 3, 3],   'Q3': [1, 1, 2, 1, 2, 3, 3, 3, 2, 3]})
#view DataFrame
df
        Q1Q2Q3
0111
1211
2212
3321
4232
5233
6323
7333
8232
9333</b>
To calculate Cronbach’s Alpha for the survey responses, we can use the <b>cronbach_alpha()</b> function from the <b>pingouin</b> library.
First, we’ll install the pingouin library:
<b>pip install pingouin
</b>
Next, we’ll use the <b>cronbach_alpha()</b> function to calculate Cronbach’s Alpha:
<b>import pingouin as pg
pg.cronbach_alpha(data=df)
(0.7734375, array([0.336, 0.939]))</b>
Cronbach’s Alpha turns out to be <b>0.773</b>.
The 95% confidence interval for Cronbach’s Alpha is also given: <b>[.336, .939]</b>.
<em><b>Note:</b> This confidence interval is extremely wide because our sample size is so small. In practice, it’s recommended to use a sample size of at least 20. We used a sample size of 10 here for simplicity sake.</em>
The default confidence interval is 95%, but we can specify a different confidence level using the <b>ci</b> argument:
<b>import pingouin as pg
#calculate Cronbach's Alpha and corresponding 99% confidence interval
pg.cronbach_alpha(data=df, ci=.99)
(0.7734375, array([0.062, 0.962]))</b>
The value for Cronbach’s Alpha remains the same, but the confidence interval is much wider since we used a higher confidence level.
The following table describes how different values of Cronbach’s Alpha are usually interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
Since we calculated Cronbach’s Alpha to be <b>0.773</b>, we would say that the internal consistency of this survey is “Acceptable.”
<b>Bonus:</b> Feel free to use this  Cronbach’s Alpha Calculator  to find Cronbach’s Alpha for a given dataset.
<h2><span class="orange">How to Calculate Cronbach’s Alpha in R (With Examples)</span></h2>
<b>Chronbach’s Alpha</b> is a way to measure the  internal consistency  of a questionnaire or survey.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
The easiest way to calculate Cronbach’s Alpha is to use the <b>cronbach.alpha()</b> function from the <b>ltm</b> package.
This tutorial provides an example of how to use this function in practice.
<h3>Example: How to Calculate Cronbach’s Alpha in R</h3>
Suppose a restaurant manager wants to measure overall satisfaction among customers, so she sends out a survey to 10 customers who can rate the restaurant on a scale of 1 to 3 for various categories.
We can use the following code to calculate Cronbach’s Alpha for the survey responses:
<b>library(ltm)
#enter survey responses as a data frame
data &lt;- data.frame(Q1=c(1, 2, 2, 3, 2, 2, 3, 3, 2, 3),   Q2=c(1, 1, 1, 2, 3, 3, 2, 3, 3, 3),   Q3=c(1, 1, 2, 1, 2, 3, 3, 3, 2, 3))
#calculate Cronbach's Alpha
cronbach.alpha(data)
Cronbach's alpha for the 'data' data-set
Items: 3
Sample units: 10
alpha: 0.773</b>
Cronbach’s Alpha turns out to be <b>0.773</b>.
Note that we can also specify <b>CI=True</b> to return a 95% confidence interval for Cronbach’s Alpha:
<b>#calculate Cronbach's Alpha with 95% confidence interval
cronbach.alpha(data, CI=TRUE)
Cronbach's alpha for the 'data' data-set
Items: 3
Sample units: 10
alpha: 0.773
Bootstrap 95% CI based on 1000 samples
 2.5% 97.5% 
0.053 0.930 
</b>
We can see that the 95% confidence interval for Cronbach’s Alpha is <b>[.053, .930]</b>.
<em><b>Note:</b> This confidence interval is extremely wide because our sample size is so small. In practice, it’s recommended to use a sample size of at least 20. We used a sample size of 10 here for simplicity sake.</em>
The following table describes how different values of Cronbach’s Alpha are usually interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
Since we calculated Cronbach’s Alpha to be <b>0.773</b>, we would say that the internal consistency of this survey is “Acceptable.”
<b>Bonus:</b> Feel free to use this  Cronbach’s Alpha Calculator  to find Cronbach’s Alpha for a given dataset.
<h2><span class="orange">How to Calculate Cronbach’s Alpha in SAS (With Example)</span></h2>
<b>Chronbach’s Alpha</b> is a way to measure the  internal consistency  of a questionnaire or survey.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
The following example shows how to calculate Cronbach’s Alpha for a dataset in SAS.
<h3>Example: How to Calculate Cronbach’s Alpha in SAS</h3>
Suppose a restaurant manager wants to measure overall satisfaction among customers, so she sends out a survey to 10 customers who can rate the restaurant on a scale of 1 to 3 for various categories.
We can use the following code to create the dataset that holds the survey responses in SAS:
<b>/*create dataset*/
data survey_data;
    input Question1 Question2 Question3;
    datalines;
1 1 1
2 1 1
2 1 2
3 2 1
2 3 2
2 3 3
3 2 3
3 3 3
2 3 2
3 3 3
;
run;
/*view dataset*/
proc print data=survey_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/cron1.jpg"281">
We can use the <b>proc corr</b> function to calculate Cronbach’s Alpha:
<b>/*calculate Cronbach's Alpha*/
proc corr data=survey_data alpha;
    var Question1-Question3;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/cron2.jpg"436">
The output tables provide us with a lot of information, but the main value we’re interested in is the <b>Raw</b> value in the table titled <b>Cronbach Coefficient Alpha</b>.
From this table we can see that Cronbach’s Alpha turns out to be <b>0.773</b>.
The following table describes how different values of Cronbach’s Alpha are usually interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
Since we calculated Cronbach’s Alpha to be <b>0.773</b>, we would say that the internal consistency of this survey is “Acceptable.”
<b>Bonus:</b> Feel free to use this  Cronbach’s Alpha Calculator  to find Cronbach’s Alpha for a given dataset.
<h2><span class="orange">How to Calculate Cross Correlation in Python</span></h2>
<b>Cross correlation</b> is a way to measure the degree of similarity between a time series and a lagged version of another time series.
This type of correlation is useful to calculate because it can tell us if the values of one time series are predictive of the future values of another time series. In other words, it can tell us if one time series is a leading indicator for another time series.
This type of correlation is used in many different fields, including:
<b>Business:</b> Marketing spend is often considered to be a leading indicator for future revenue of businesses. For example, if a business spends an abnormally high amount of money on marketing during one quarter, then total revenue is expected to be high <em>x</em> quarters later.
<b>Economics:</b> The consumer confidence index (CCI) is considered to be a leading indicator for the gross domestic product (GDP) of a country. For example, if CCI is high during a given month, the GDP is likely to be higher <em>x</em> months later.
The following example shows how to calculate the cross correlation between two time series in Python.
<h3>Example: How to Calculate Cross Correlation in Python</h3>
Suppose we have the following time series in Python that show the total marketing spend (in thousands) for a certain company along with the the total revenue (in thousands) during 12 consecutive months:
<b>import numpy as np
#define data 
marketing = np.array([3, 4, 5, 5, 7, 9, 13, 15, 12, 10, 8, 8])
revenue = np.array([21, 19, 22, 24, 25, 29, 30, 34, 37, 40, 35, 30]) </b>
We can calculate the cross correlation for every lag between the two time series by using the <b>ccf()</b> function from the  statsmodels package  as follows:
<b>import statsmodels.api as sm
#calculate cross correlation
sm.tsa.stattools.ccf(marketing, revenue, adjusted=False)
array([ 0.77109358,  0.46238654,  0.19352232, -0.06066296, -0.28159595,
       -0.44531104, -0.49159463, -0.35783655, -0.15697476, -0.03430078,
        0.01587722,  0.0070399 ])</b>
Here’s how to interpret this output:
The cross correlation at lag 0 is <b>0.771</b>.
The cross correlation at lag 1 is <b>0.462</b>.
The cross correlation at lag 2 is <b>0.194</b>.
The cross correlation at lag 3 is <b>-0.061</b>.
And so on.
Notice that the correlation between the two time series becomes less and less positive as the number of lags increases. This tells us that marketing spend during a given month is quite predictive of revenue one or two months later, but not predictive of revenue beyond more than two months.
This intuitively makes sense – we would expect that high marketing spend during a given month is predictive of increased revenue during the next two months, but not necessarily predictive of revenue several months into the future.
<h2><span class="orange">How to Calculate Cross Correlation in R</span></h2>
<b>Cross correlation</b> is a way to measure the degree of similarity between a time series and a lagged version of another time series.
This type of correlation is useful to calculate because it can tell us if the values of one time series are predictive of the future values of another time series. In other words, it can tell us if one time series is a leading indicator for another time series.
This type of correlation is used in many different fields, including:
<b>Economics:</b> The consumer confidence index (CCI) is considered to be a leading indicator for the gross domestic product (GDP) of a country. For example, if CCI is high during a given month, the GDP is likely to be higher <em>x</em> months later.
<b>Business:</b> Marketing spend is often considered to be a leading indicator for future revenue of businesses. For example, if a business spends an abnormally high amount of money on marketing during one quarter, then total revenue is expected to be high <em>x</em> quarters later.
<b>Biology:</b> Total ocean pollution is considered to be a leading indicator of the population of a certain species of turtle. For example, if pollution is higher during a given year then the total population of turtles is expected to be lower <em>x</em> years later.
The following example shows how to calculate the cross correlation between two time series in R.
<h3>Example: How to Calculate Cross Correlation in R</h3>
Suppose we have the following time series in R that show the total marketing spend (in thousands) for a certain company along with the the total revenue (in thousands) during 12 consecutive months:
<b>#define data
marketing &lt;- c(3, 4, 5, 5, 7, 9, 13, 15, 12, 10, 8, 8)
revenue &lt;- c(21, 19, 22, 24, 25, 29, 30, 34, 37, 40, 35, 30) </b>
We can calculate the cross correlation for every lag between the two time series by using the <b>ccf()</b> function as follows:
<b>#calculate cross correlation
ccf(marketing, revenue)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/crossCorrelationR1.png">
This plot displays the correlation between the two time series at various lags.
To display the actual correlation values, we can use the following syntax:
<b>#display cross correlation values
print(ccf(marketing, revenue))
Autocorrelations of series ‘X’, by lag
    -7     -6     -5     -4     -3     -2     -1      0      1      2      3 
-0.430 -0.351 -0.190  0.123  0.489  0.755  0.868  0.771  0.462  0.194 -0.061 
     4      5      6      7 
-0.282 -0.445 -0.492 -0.358
</b>
Here’s how to interpret this output:
The cross correlation at lag 0 is <b>0.771</b>.
The cross correlation at lag 1 is <b>0.462</b>.
The cross correlation at lag 2 is <b>0.194</b>.
The cross correlation at lag 3 is <b>-0.061</b>.
And so on.
Notice that the correlation between the two time series is quite positive within lags -2 to 2, which tells us that marketing spend during a given month is quite predictive of revenue one and two months later.
This intuitively makes sense – we would expect that high marketing spend during a given month is predictive of increased revenue during the next two months.
<h2><span class="orange">How to Do a Cross Join in R (With Example)</span></h2>
The easiest way to perform a cross join in R is to use the <b>crossing()</b> function from the  tidyr  package:
<b>library(tidyr)
#perform cross join on df1 and df2
crossing(df1, df2)
</b>
The following example shows how to use this function in practice.
<h3>Example: Perform Cross Join in R</h3>
Suppose we have the following two data frames in R:
<b>#define first data frame
df1 = data.frame(team1=c('A', 'B', 'C', 'D'), points=c(18, 22, 19, 14))
df1
  team1 points
1     A     18
2     B     22
3     C     19
4     D     14
#define second data frame
df2 = data.frame(team2=c('A', 'B', 'F'), assists=c(4, 9, 8)) 
df2
  team2 assists
1     A       4
2     B       9
3     F       8</b>
We can use the <b>crossing()</b> function from the <b>tidyr</b> package to perform a cross join on these two data frames:
<b>library(tidyr)
#perform cross join 
cross &lt;- crossing(df1, df2)
#view result
cross
# A tibble: 12 x 4
   team1 points team2 assists
         
 1 A         18 A           4
 2 A         18 B           9
 3 A         18 F           8
 4 B         22 A           4
 5 B         22 B           9
 6 B         22 F           8
 7 C         19 A           4
 8 C         19 B           9
 9 C         19 F           8
10 D         14 A           4
11 D         14 B           9
12 D         14 F           8
</b>
The result is a data frame that contains every possible combination of rows from each data frame.
For example, the first row of the first data frame contains team <b>A</b> and <b>18</b> points. This row is matched with every single row in the second data frame.
Next, the second row of the first data frame contains team <b>B</b> and <b>22</b> points. This row is also matched with every single row in the second data frame.
The end result is a data frame with 12 rows.
<h2><span class="orange">What is a Cross-Lagged Panel Design? (Definition & Example)</span></h2>
A <b>cross-lagged panel design</b> is a type of structural equation model that measures two different variables at two points in time.
For example, suppose we measure the total amount of money spent on education and the median household income in a certain country during two different points in time.
We could use the following diagram to visualize this cross-lagged panel design:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cross1-1.png">
<b>Note:</b> r<sub>e2015, h2015 </sub>indicates the correlation between education spend in 2015 and median household income in 2015.
The name “cross” comes from the fact that we’re analyzing the relationship from one variable to another and vice-versa.
The name “lagged” comes from the fact that we’re measuring both variables at two different points in time.
<h3>How to Assess a Cross-Lagged Panel Design</h3>
A cross-lagged panel design estimates a total of six relations:
<b>Two synchronous relations</b>. This design measures the synchronous relations between the two variables at the same time points:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cross2.png">
<b>Two stability relations</b>. This design measures the stability relations between the same variables at different points in time:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cross3.png">
<b>Two cross-lagged relations</b>. This design measures the cross-lagged relations between the two variables at different points in time:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cross4.png">
If either of the cross-lagged correlations are significantly different from zero, it’s assumed that there exists a causal relationship between the two variables at the two different points in time.
For example, if r<sub>e2015, h2020</sub> is significantly different from zero then it’s assumed that education spend <em>causes</em> a change in household income.
<h3>Assumptions of a Cross-Lagged Panel Design</h3>
A cross-lagged panel design is considered a valid way to identify causal relationships between variables, but it makes a couple crucial assumptions:
<b>Synchronicity:</b> This design assumes that the measurements on the two variables at each point in time were made at the exact same time.
<b>Stationarity:</b> This design assumes that the variables and the relationships between the variables remain constant over time.
If these assumptions are violated, it may not be valid to use this type of model to identify causal relationships.
<h2><span class="orange">How to Calculate a Cross Product in Excel</span></h2>
Assuming we have vector A with elements (A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>) and vector B with elements (B<sub>1</sub>, B<sub>2</sub>, B<sub>3</sub>), we can calculate the cross product of these two vectors as:
Cross Product = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
For example, suppose we have the following vectors:
Vector A: (1, 2, 3)
Vector B: (4, 5, 6)
We could calculate the cross product of these vectors as:
Cross Product = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
Cross Product = [(2*6) – (3*5), (3*4) – (1*6), (1*5) – (2*4)]
Cross Product = (-3, 6, -3)
The following example shows how to calculate this exact cross product in Excel.
<h3>Example: Calculating Cross Product in Excel</h3>
To calculate the cross product between two vectors in Excel, we’ll first input the values for each vector:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/crossExcel1.png">
Next, we’ll calculate the first value of the cross product:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/crossExcel2.png">
Then we’ll calculate the second value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/crossExcel3.png">
Lastly, we’ll calculate the third value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/crossExcel4.png">
The cross product turns out to be (-3, 6, -3).
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/crossExcel5.png">
This matches the cross product that we calculated by hand earlier.
<h2><span class="orange">How to Calculate a Cross Product in R</span></h2>
Assuming we have vector A with elements (A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>) and vector B with elements (B<sub>1</sub>, B<sub>2</sub>, B<sub>3</sub>), we can calculate the cross product of these two vectors as:
<b>Cross Product</b> = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
For example, suppose we have the following vectors:
Vector A: (1, 2, 3)
Vector B: (4, 5, 6)
We could calculate the cross product of these vectors as:
Cross Product = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
Cross Product = [(2*6) – (3*5), (3*4) – (1*6), (1*5) – (2*4)]
Cross Product = (-3, 6, -3)
You can use one of the following two methods to calculate the cross product of two vectors in R:
<b>Method 1: Use cross() function from pracma package</b>
<b>library(pracma)
  
#calculate cross product of vectors A and B
cross(A, B)
</b>
<b>Method 2: Define your own function</b>
<b>#define function to calculate cross product 
cross &lt;- function(x, y, i=1:3) {
  create3D &lt;- function(x) head(c(x, rep(0, 3)), 3)
  x &lt;- create3D(x)
  y &lt;- create3D(y)
  j &lt;- function(i) (i-1) %% 3+1
  return (x[j(i+1)]*y[j(i+2)] - x[j(i+2)]*y[j(i+1)])
}
#calculate cross product
cross(A, B)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Use cross() function from pracma package</h3>
The following code shows how to use the <b>cross()</b> function from the  pracma  package to calculate the cross product between two vectors:
<b>library(pracma)
  
#define vectors
A &lt;- c(1, 2, 3)
B &lt;- c(4, 5, 6)
  
#calculate cross product
cross(A, B)
[1] -3  6 -3
</b>
The cross product turns out to be <b>(-3, 6, -3)</b>.
This matches the cross product that we calculated earlier by hand.
<h3>Example 2: Define your own function</h3>
The following code shows how to define your own function to calculate the cross product between two vectors:
<b>#define function to calculate cross product 
cross &lt;- function(x, y, i=1:3) {
  create3D &lt;- function(x) head(c(x, rep(0, 3)), 3)
  x &lt;- create3D(x)
  y &lt;- create3D(y)
  j &lt;- function(i) (i-1) %% 3+1
  return (x[j(i+1)]*y[j(i+2)] - x[j(i+2)]*y[j(i+1)])
}
#define vectors
A &lt;- c(1, 2, 3)
B &lt;- c(4, 5, 6)
#calculate cross product
cross(A, B)
[1] -3 6 -3
</b>
The cross product turns out to be <b>(-3, 6, -3)</b>.
This matches the cross product that we calculated in the previous example.
<h2><span class="orange">How to Calculate a Cross Product in Python</span></h2>
Assuming we have vector A with elements (A<sub>1</sub>, A<sub>2</sub>, A<sub>3</sub>) and vector B with elements (B<sub>1</sub>, B<sub>2</sub>, B<sub>3</sub>), we can calculate the cross product of these two vectors as:
<b>Cross Product</b> = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
For example, suppose we have the following vectors:
Vector A: (1, 2, 3)
Vector B: (4, 5, 6)
We could calculate the cross product of these vectors as:
Cross Product = [(A<sub>2</sub>*B<sub>3</sub>) – (A<sub>3</sub>*B<sub>2</sub>), (A<sub>3</sub>*B<sub>1</sub>) – (A<sub>1</sub>*B<sub>3</sub>), (A<sub>1</sub>*B<sub>2</sub>) – (A<sub>2</sub>*B<sub>1</sub>)]
Cross Product = [(2*6) – (3*5), (3*4) – (1*6), (1*5) – (2*4)]
Cross Product = (-3, 6, -3)
You can use one of the following two methods to calculate the cross product of two vectors in Python:
<b>Method 1: Use cross() function from NumPy</b>
<b>import numpy as np
  
#calculate cross product of vectors A and B
np.cross(A, B)
</b>
<b>Method 2: Define your own function</b>
<b>#define function to calculate cross product 
def cross_prod(a, b):
    result = [a[1]*b[2] - a[2]*b[1],
            a[2]*b[0] - a[0]*b[2],
            a[0]*b[1] - a[1]*b[0]]
    return result
#calculate cross product
cross_prod(A, B)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Use cross() function from NumPy</h3>
The following code shows how to use the  cross()  function from NumPy to calculate the cross product between two vectors:
<b>import numpy as np
#define vectors
A = np.array([1, 2, 3])
B = np.array([4, 5, 6])
  
#calculate cross product of vectors A and B
np.cross(A, B)
[-3, 6, -3]
</b>
The cross product turns out to be <b>(-3, 6, -3)</b>.
This matches the cross product that we calculated earlier by hand.
<h3>Example 2: Define your own function</h3>
The following code shows how to define your own function to calculate the cross product between two vectors:
<b>#define function to calculate cross product 
def cross_prod(a, b):
    result = [a[1]*b[2] - a[2]*b[1],
            a[2]*b[0] - a[0]*b[2],
            a[0]*b[1] - a[1]*b[0]]
    return result
#define vectors
A = np.array([1, 2, 3])
B = np.array([4, 5, 6])
#calculate cross product
cross_prod(A, B)
[-3, 6, -3]
</b>
The cross product turns out to be <b>(-3, 6, -3)</b>.
This matches the cross product that we calculated in the previous example.
<h2><span class="orange">How to Create a Crosstab in Excel (Step-by-Step)</span></h2>
A <b>crosstab</b> is a table that summarizes the relationship between two categorical variables.
The following step-by-step example explains how to create a crosstab in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following dataset into Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/crosstab_excel1.png">
<h3>Step 2: Create the Crosstab</h3>
Next, click the <b>Insert</b> tab along the top ribbon and then click the <b>PivotTable</b> button.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/crosstab_excel2.png">
In the new window that appears, select the range that contains the data as the <b>Table/Range</b> and choose any cell you’d like in the <b>Existing Worksheet</b> to place the crosstab. We’ll choose cell <b>E2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/crosstab_excel3-1.png">
<h3>Step 3: Populate the Crosstab with Values</h3>
Once you click OK, a new window on the right side of the screen will appear.
Drag the <b>Team</b> variable to the <b>Rows</b> area, the <b>Position</b> variable to the <b>Columns</b> area, then the <b>Position</b> variable again to the <b>Values</b> area as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/crosstab_excel4.png">
Once you do so, the following crosstab will appear in the cell that you specified:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/crosstab_excel5.png">
<h3>Step 4: Interpret the Crosstab</h3>
Here’s how to interpret the values in the crosstab:
<b>Row Totals:</b>
A total of <b>6</b> players are on team A
A total of <b>6</b> players are on team B
<b>Column Totals:</b>
A total of <b>3</b> players have a position of Center
A total of <b>4</b> players have a position of Forward
A total of <b>5</b> players have a position of Guard
<b>Individual Cells:</b>
<b>1</b> player has a position of Center on team A
<b>3</b> players have a position of Forward on team A
<b>2</b> players have a position of Guard on team A
<b>2</b> players have a position of Center on team B
<b>1</b> player has a position of Forward on team B
<b>3</b> players have a position of Guard on team B
<h2><span class="orange">How to Create a Crosstab in Google Sheets</span></h2>
A <b>crosstab</b> is a table that summarizes the relationship between two categorical variables.
The following step-by-step example shows how to create a crosstab in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following dataset into Google Sheets that shows information for various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/crosstab1.jpg"580">
<h3>Step 2: Create the Crosstab</h3>
Next, click the <b>Insert</b> tab along the top ribbon and then click <b>Pivot table</b> from the dropdown menu:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/crosstab2.jpg"530">
In the new window that appears, enter <b>Sheet1!A1:C13</b> as the Data range and <b>Sheet1!E1</b> as the Insert to location, then click <b>Create</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/crosstab3.jpg"657">
<h3>Step 3: Populate the Crosstab with Values</h3>
Once you click <b>Create</b>, a new Pivot table editor panel will appear on the right side of the screen.
Choose Team for the <b>Rows</b>, Position for the <b>Columns</b>, and Points for the <b>Values</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/crosstab4.jpg"328">
Once you do so, the following crosstab will appear in the cell that you specified:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/crosstab5.jpg">
<h3>Step 4: Interpret the Crosstab</h3>
Here’s how to interpret the values in the crosstab:
<b>Row Grand Totals:</b>
A total of <b>6</b> players are on team A
A total of <b>6</b> players are on team B
<b>Column Grand Totals:</b>
A total of <b>3</b> players have a position of Center
A total of <b>4</b> players have a position of Forward
A total of <b>5</b> players have a position of Guard
<b>Individual Cells:</b>
<b>1</b> player has a position of Center on team A
<b>3</b> players have a position of Forward on team A
<b>2</b> players have a position of Guard on team A
<b>2</b> players have a position of Center on team B
<b>1</b> player has a position of Forward on team B
<b>3</b> players have a position of Guard on team B
<h2><span class="orange">Cubic Regression in Excel (Step-by-Step)</span></h2>
<b>Cubic regression</b> is a regression technique we can use when the relationship between a predictor variable and a response variable is non-linear.
The following step-by-step example shows how to fit a cubic regression model to a dataset in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel1.png">
<h3>Step 2: Perform Cubic Regression</h3>
Next, we can use the following formula in Excel to fit a cubic regression model in Excel:
<b>=LINEST(B2:B13, A2:A13^{1,2,3})
</b>
The following screenshot shows how to perform cubic regression for our particular example:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel2.png">
Using the coefficients in the output, we can write the following estimated regression model:
<U+0177> = -32.0118 + 9.832x – 0.3214x<sup>2</sup> + 0.0033x<sup>3</sup>
<h3>Step 3: Visualize the Cubic Regression Model</h3>
We can also create a scatterplot with the fitted regression line to visualize the cubic regression model.
First, highlight the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel3.png">
Then click the <b>Insert</b> tab along the top ribbon and click the first option within the <b>Insert Scatter (X, Y)</b> option in the <b>Charts</b> group. This will produce the following scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel4.png">
Next, click the green plus sign in the top right corner of the chart and click the arrow to the right of <b>Trendline</b>. In the dropdown menu that appears, click <b>More Options</b>…
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel5.png">
Next, click the <b>Polynomial</b> trendline option and select <b>3</b> for the order. Then check the box next to “Display Equation on chart”
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel6.png">
The following trendline and equation will appear on the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/cubicExcel.png">
Notice that the equation in the chart matches the equation that we calculated using the <b>LINEST()</b> function.
<h2><span class="orange">How to Perform Cubic Regression in Python</span></h2>
<b>Cubic regression</b> is a type of regression we can use to quantify the relationship between a predictor variable and a response variable when the relationship between the variables is non-linear.
This tutorial explains how to perform cubic regression in Python.
<h2>Example: Cubic Regression in Python</h2>
Suppose we have the following pandas DataFrame that contains two variables (x and y):
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'x': [6, 9, 12, 16, 22, 28, 33, 40, 47, 51, 55, 60],   'y': [14, 28, 50, 64, 67, 57, 55, 57, 68, 74, 88, 110]})
#view DataFrame
print(df)
     x    y
0    6   14
1    9   28
2   12   50
3   16   64
4   22   67
5   28   57
6   33   55
7   40   57
8   47   68
9   51   74
10  55   88
11  60  110
</b>
If we make a simple scatterplot of this data we can see that the relationship between the two variables is non-linear:
<b>import matplotlib.pyplot as plt
#create scatterplot
plt.scatter(df.x, df.y)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/cubic1.jpg"463">
As the value for x increases, y increases up to a certain point, then decreases, then increases once more.
This pattern with two “curves” in the plot is an indication of a cubic relationship between the two variables.
This means a cubic regression model is a good candidate for quantifying the relationship between the two variables.
To perform cubic regression, we can fit a polynomial regression model with a degree of 3 using the  numpy.polyfit()  function:
<b>import numpy as np
#fit cubic regression model
model = np.poly1d(np.polyfit(df.x, df.y, 3))
#add fitted cubic regression line to scatterplot
polyline = np.linspace(1, 60, 50)
plt.scatter(df.x, df.y)
plt.plot(polyline, model(polyline))
#add axis labels
plt.xlabel('x')
plt.ylabel('y')
#display plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/cubic2.jpg">
We can obtain the fitted cubic regression equation by printing the model coefficients:
<b>print(model)
          3          2
0.003302 x - 0.3214 x + 9.832 x - 32.01
</b>
The fitted cubic regression equation is:
<b>y = 0.003302(x)<sup>3</sup> – 0.3214(x)<sup>2</sup> + 9.832x – 30.01</b>
We can use this equation to calculate the expected value for y based on the value for x.
For example, if x is equal to 30 then the expected value for y is 64.844:
y = 0.003302(30)<sup>3</sup> – 0.3214(30)<sup>2</sup> + 9.832(30) – 30.01 = 64.844
We can also write a short function to obtain the R-squared of the model, which is the proportion of the variance in the response variable that can be explained by the predictor variables.
<b>#define function to calculate r-squared
def polyfit(x, y, degree):
    results = {}
    coeffs = np.polyfit(x, y, degree)
    p = np.poly1d(coeffs)
    #calculate r-squared
    yhat = p(x)
    ybar = np.sum(y)/len(y)
    ssreg = np.sum((yhat-ybar)**2)
    sstot = np.sum((y - ybar)**2)
    results['r_squared'] = ssreg / sstot
    return results
#find r-squared of polynomial model with degree = 3
polyfit(df.x, df.y, 3)
{'r_squared': 0.9632469890057967}
</b>
In this example, the R-squared of the model is <b>0.9632</b>.
This means that 96.32% of the variation in the response variable can be explained by the predictor variable.
Since this value is so high, it tells us that the cubic regression model does a good job of quantifying the relationship between the two variables.
<b>Related:</b>  What is a Good R-squared Value? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Simple Linear Regression in Python 
 How to Perform Quadratic Regression in Python 
 How to Perform Polynomial Regression in Python 
<h2><span class="orange">How to Calculate a Cumulative Average in Excel</span></h2>
A <b>cumulative average</b> tells us the average of a series of values up to a certain point.
The following step-by-step example shows how to calculate a cumulative average for a dataset in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for a given dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cumAvg1.png">
<h3>Step 2: Calculate the First Cumulative Average Value</h3>
Next, we can use the following formula to calculate the first cumulative average value:
<b>=AVERAGE($A$2:A2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cumAvg2.png">
<h3>Step 3: Calculate the Remaining Cumulative Average Values</h3>
Next, we can simply copy and paste this formula down to the remaining cells in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cumAvg3.png">
We would interpret the cumulative average values as:
The cumulative average of the first value is <b>3</b>.
The cumulative average of the first two values is <b>4.5</b>.
The cumulative average of the first three values is <b>3</b>.
And so on.
<h3>Step 4: Plot the Cumulative Average Values</h3>
We can plot the cumulative average values by highlighting every value in column B, then clicking the <b>Insert</b> tab along the top ribbon, then clicking <b>Insert Line Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/cumAvg4.png">
<h2><span class="orange">How to Calculate a Cumulative Average in R</span></h2>
A <b>cumulative average</b> tells us the average of a series of values up to a certain point.
You can use the following methods to calculate the cumulative average of values in R:
<b>Method 1: Use Base R</b>
<b>cum_avg &lt;- cumsum(x) / seq_along(x)</b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
cum_avg &lt;- cummean(x)</b>
Both methods return the exact same result, but the <b>dplyr</b> method tends to work faster on large data frames.
The following examples show how to use each method in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(day=seq(1:16), sales=c(3, 6, 0, 2, 4, 1, 0, 1, 4, 7, 3, 3, 8, 3, 5, 5))
#view head of data frame
head(df)
  day sales
1   1     3
2   2     6
3   3     0
4   4     2
5   5     4
6   6     1
</b>
<h3>Example 1: Calculate Cumulative Average Using Base R</h3>
We can use the following code to add a new column to our data frame that shows the cumulative average of sales:
<b>#add new column that contains cumulative avg. of sales
df$cum_avg_sales &lt;- cumsum(df$sales) / seq_along(df$sales) 
#view updated data frame
df
   day sales cum_avg_sales
1    1     3      3.000000
2    2     6      4.500000
3    3     0      3.000000
4    4     2      2.750000
5    5     4      3.000000
6    6     1      2.666667
7    7     0      2.285714
8    8     1      2.125000
9    9     4      2.333333
10  10     7      2.800000
11  11     3      2.818182
12  12     3      2.833333
13  13     8      3.230769
14  14     3      3.214286
15  15     5      3.333333
16  16     5      3.437500
</b>
We would interpret the cumulative average values as:
The cumulative average of the first sales value is <b>3</b>.
The cumulative average of the first two sales values is <b>4.5</b>.
The cumulative average of the first three sales values is <b>3</b>.
The cumulative average of the first four sales values is <b>2.75</b>.
And so on.
<h3>Example 2: Calculate Cumulative Average Using dplyr</h3>
We can also use the <b>cummean</b> function from the  dplyr  package in R to calculate a cumulative average.
The following code shows how to use this function to add a new column to our data frame that shows the cumulative average of sales:
<b>library(dplyr)
#add new column that contains cumulative avg. of sales
df$cum_avg_sales &lt;- cummean(df$sales) 
#view updated data frame
df
   day sales cum_avg_sales
1    1     3      3.000000
2    2     6      4.500000
3    3     0      3.000000
4    4     2      2.750000
5    5     4      3.000000
6    6     1      2.666667
7    7     0      2.285714
8    8     1      2.125000
9    9     4      2.333333
10  10     7      2.800000
11  11     3      2.818182
12  12     3      2.833333
13  13     8      3.230769
14  14     3      3.214286
15  15     5      3.333333
16  16     5      3.437500
</b>
Notice that this method returns the exact same results as the previous method.
<h2><span class="orange">How to Calculate a Cumulative Average in Python</span></h2>
A <b>cumulative average</b> tells us the average of a series of values up to a certain point.
You can use the following syntax to calculate the cumulative average of values in a column of a pandas DataFrame:
<b>df['column_name'].expanding().mean()</b>
The following example shows how to use this syntax in practice.
<h3>Example: Calculate Cumulative Average in Python</h3>
Suppose we have the following pandas DataFrame that shows the total sales made by some store during 16 consecutive days:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'day': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],   'sales': [3, 6, 0, 2, 4, 1, 0, 1, 4, 7, 3, 3, 8, 3, 5, 5]})
#view first five rows of DataFrame
df.head()
daysales
013
126
230
342
454</b>
We can use the following syntax to calculate the cumulative average of the sales column:
<b>#calculate average of 'sales' column
df['sales'].expanding().mean()
0     3.000000
1     4.500000
2     3.000000
3     2.750000
4     3.000000
5     2.666667
6     2.285714
7     2.125000
8     2.333333
9     2.800000
10    2.818182
11    2.833333
12    3.230769
13    3.214286
14    3.333333
15    3.437500
Name: sales, dtype: float64
</b>
We would interpret the cumulative average values as:
The cumulative average of the first sales value is <b>3</b>.
The cumulative average of the first two sales values is <b>4.5</b>.
The cumulative average of the first three sales values is <b>3</b>.
The cumulative average of the first four sales values is <b>2.75</b>.
And so on.
Note that you can also use the following code to add the cumulative average sales values as a new column in the DataFrame:
<b>#add cumulative average sales as new column
df['cum_avg_sales'] = df['sales'].expanding().mean()
#view updated DataFrame
df
daysalescum_avg_sales
0133.000000
1264.500000
2303.000000
3422.750000
4543.000000
5612.666667
6702.285714
7812.125000
8942.333333
91072.800000
101132.818182
111232.833333
121383.230769
131433.214286
141553.333333
151653.437500
</b>
The <b>cum_avg_sales</b> column shows the cumulative average of the values in the “sales” column.
<h2><span class="orange">Cumulative Frequency Calculator</span></h2>
<b>Cumulative frequency</b> is a measure of the total frequencies up to a certain point in a list of data values.
To calculate the cumulative frequency for a list of data values, simply enter the comma-separated values in the box below and then click the “Calculate” button.
<textarea id="input_data" name="x" rows="5" cols="40">4, 14, 16, 22, 24, 25, 37, 38, 38, 40, 41, 41, 43, 44</textarea>
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<script>
function calc() {
//remove current table if one exists
var element = document.getElementsByTagName('table')[0];
    if(element) {element.parentNode.removeChild(element)}
//get input values
var input_data = document.getElementById('input_data').value.split(',').map(Number);
//calculate stuff
var occurrence = function (array) {
    "use strict";
    var result = {};
    if (array instanceof Array) { // Check if input is array.
        array.forEach(function (v, i) {
            if (!result[v]) { // Initial object property creation.
                result[v] = [i]; // Create an array for that property.
            } else { // Same occurrences found.
                result[v].push(i); // Fill the array.
            }
        });
    }
    return result;
};
var size = Object.keys(occurrence(input_data)).length;
//create array of relative and cumulative values
var relFreqArray = [];
for (var i = 0; i < size; i++){
    relFreqArray[i] = (occurrence(input_data)[Object.keys(occurrence(input_data))[i]].length)
}
for (var cumsum = [relFreqArray[0]], i = 0, l = relFreqArray.length-1; i<l; i++) {
    cumsum[i+1] = cumsum[i] + relFreqArray[i+1]; 
}
//generate table of frequencies
var table = document.createElement('table');
    function boldHTML(text) {
  var element = document.createElement("b");
  element.innerHTML = text;
  return element;
}
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    var td4 = document.createElement('td');
    td1.appendChild(boldHTML('Value'));
    td2.appendChild(boldHTML('Frequency'));
    td4.appendChild(boldHTML('Cumulative Frequency'));
    tr.appendChild(td1);
    tr.appendChild(td2);
    tr.appendChild(td4);
    table.appendChild(tr);
for (var i = 0; i < size; i++){
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    var td4 = document.createElement('td');
    var text1 = document.createTextNode(Object.keys(occurrence(input_data))[i]);
    var text2 = document.createTextNode(occurrence(input_data)[Object.keys(occurrence(input_data))[i]].length);
    var text4 = document.createTextNode(cumsum[i]);
    td1.appendChild(text1);
    td2.appendChild(text2);
    td4.appendChild(text4);
    tr.appendChild(td1);
    tr.appendChild(td2);
    tr.appendChild(td4);
    table.appendChild(tr);
}
document.getElementById('table_output').appendChild(table);
//output results
//document.getElementById('ss').innerHTML = ss.toFixed(2);
}
</script>
<h2><span class="orange">How to Calculate Cumulative Frequency in Excel</span></h2>
A <b>frequency table </b>is a table that displays information about frequencies. Frequencies simply tell us how many times a certain event has occurred. 
For example, the following table shows how many items a shop sold in different price ranges in a given week:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
</tr>
</tbody></table>
The first column displays the price class and the second column displays the frequency of that class.
It’s also possible to calculate the <b>cumulative frequency </b>for each class, which is simply the sum of the frequencies up to a certain class.
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Cumulative Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">41</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">54</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">66</td>
</tr>
</tbody></table>
For example, the first cumulative frequency is simply equal to the first frequency of <b>20</b>.
The second cumulative frequency is the sum of the first two frequencies: 20 + 21 = <b>41</b>.
The third cumulative frequency is the sum of the first three frequencies: 20 + 21 + 13 = <b>54</b>.
And so on.
The following example illustrates how to find cumulative frequencies in Excel.
<h3>Example: Cumulative Frequency in Excel</h3>
First, we will enter the class and the frequency in columns A and B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel1.png">
Next, we will calculate the cumulative frequency of each class in column C. 
In the image below, Column D shows the formulas we used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cumFreqExcel1.png">
We can also create an  ogive chart  to visualize the cumulative frequencies.
To create the ogive chart, hold down CTRL and highlight columns A and C.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cumFreqExcel3.png">
Then go to the <b>Charts </b>group in the <b>Insert </b>tab and click the first chart type in <b>Insert Column or Bar Chart</b>:
Along the top ribbon in Excel, go to the <b>Insert</b> tab, then the <b>Charts </b>group. Click <b>Scatter Chart</b>, then click <b>Scatter with Straight Lines and Markers</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/ogive12.jpg"529">
This will automatically produce the following ogive graph:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cumFreqExcel4.png">
Feel free to modify the axes and the title to make the graph more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/cumFreqExcel5.png">
<h2><span class="orange">How to Create a Cumulative Sum Chart in Excel (With Example)</span></h2>
This tutorial provides a step-by-step example of how to create the following cumulative sum chart in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart7.jpg">
Let’s jump in!
<h3>Step 1: Enter the Data</h3>
First, let’s create the following dataset that shows the total sales of some item during each month in a year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart1.jpg"401">
<h3>Step 2: Calculate the Cumulative Sum</h3>
Next, we’ll use the following formula to calculate the cumulative sum of sales:
<b>=SUM($B$2:B2)
</b>
We can type this formula into cell <b>C2</b> and then copy and paste it to every remaining cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart2.jpg"454">
<h3>Step 3: Create Bar Chart with Average Line</h3>
Next, highlight the cell range <b>A1:C13</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Clustered Column</b> within the <b>Charts</b> group.
The following chart will be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart3.jpg"566">
Next, right click anywhere on the chart and then click <b>Change Chart Type</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart4.jpg"558">
In the new window that appears, click <b>Combo</b> and then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart5.jpg"605">
The chart will be converted into a bar chart with a line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart6.jpg"554">
The blue bars represent the sales each month and the orange line represents the cumulative sales.
<h3>Step 4: Customize the Chart (Optional)</h3>
Feel free to add a title, customize the colors, customize the line style, and adjust the width of the bars to make the plot more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/cumchart7.jpg">
<h2><span class="orange">How to Create a Cumulative Sum Chart in Google Sheets</span></h2>
This tutorial provides a step-by-step example of how to create the following cumulative sum chart in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative5.jpg"596">
Let’s jump in!
<h2>Step 1: Enter the Data</h2>
First, let’s create the following dataset that shows the total sales of some item during each month in a year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative1.jpg"488">
<h2>Step 2: Calculate the Cumulative Sum</h2>
Next, we’ll use the following formula to calculate the cumulative sum of sales:
<b>=SUM($B$2:B2)
</b>
We can type this formula into cell <b>C2</b> and then drag and fill it to every remaining cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative2.jpg"493">
<h2>Step 3: Create Cumulative Sum Chart</h2>
Next, highlight the cell range <b>A1:C13</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative3.jpg"535">
In the <b>Chart editor</b> panel, click the <b>Setup</b> tab, then choose the chart titled<b> Combo chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative4.jpg"350">
The following chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/cumulative5.jpg"596">
The blue bars represent the sales each month and the red line represents the cumulative sales.
Feel free to customize the title, customize the colors, customize the line style, and adjust the width of the bars to make the plot look however you’d like.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Add Average Line to Chart in Google Sheets 
 How to Add Trendline to Chart in Google Sheets 
 How to Plot Multiple Lines in Google Sheets 
<h2><span class="orange">How to Calculate Cumulative Sums in R (With Examples)</span></h2>
You can use the <b>cumsum() </b>function from base R to easily calculate the cumulative sum of a vector of numeric values.
This tutorial explains how to use this function to calculate the cumulative sum of a vector along with how to visualize a cumulative sum.
<h3>How to Calculate a Cumulative Sum in R</h3>
The following code shows how to calculate the cumulative sum of sales for a given company over the course of 15 sales quarters:
<b>#create dataset
data &lt;- data.frame(quarter=1:15,   sales=c(1, 2, 2, 5, 4, 7, 5, 7, 6, 8, 5, 9, 11, 12, 4))
#create new column in dataset that contains cumulative sales
data$cum_sales &lt;- cumsum(data$sales)
#view dataset
data
   quarter sales cum_sales
1        1     1         1
2        2     2         3
3        3     2         5
4        4     5        10
5        5     4        14
6        6     7        21
7        7     5        26
8        8     7        33
9        9     6        39
10      10     8        47
11      11     5        52
12      12     9        61
13      13    11        72
14      14    12        84
15      15     4        88
</b>
The values shown in the <b>cum_sales</b> column represent the total sales up to and including that quarter. For example, the cumulative sales in quarter 5 are calculated as: 1+2+2+5+4 = <b>14</b>.
<h3>How to Visualize a Cumulative Sum in R</h3>
Once we’ve calculated the cumulative sales, we can create a simple line chart in base R to visualize the cumulative sales by quarter:
<b>plot(data$cum_sales, type='l', xlab='Quarter', ylab='Cumulative Sales')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/cumSum1.png">
Alternatively, we can use the R visualization library  ggplot2  to create the same line chart:
<b>library(ggplot2)
ggplot(data, aes(x=quarter, y=cum_sales)) +
  geom_line() +
  labs(x='Quarter', y='Cumulative Sales')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/cumSum2.png">
<h2><span class="orange">Curve Fitting in Excel (With Examples)</span></h2>
Often you may want to find the equation that best fits some curve for a dataset in Excel.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel0.png">
Fortunately this is fairly easy to do using the <b>Trendline</b> function in Excel.
This tutorial provides a step-by-step example of how to fit an equation to a curve in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset to work with:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel1.png">
<h3>Step 2: Create a Scatterplot</h3>
Next, let’s create a scatterplot to visualize the dataset.
First, highlight cells <b>A2:B16</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel2.png">
Next, click the <b>Insert</b> tab along the top ribbon, and then click the first plot option under <b>Scatter</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel3.png">
This produces the following scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel4.png">
<h3>Step 3: Add a Trendline</h3>
Next, click anywhere on the scatterplot. Then click the <b>+</b> sign in the top right corner. In the dropdown menu, click the arrow next to <b>Trendline</b> and then click <b>More Options</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel5.png">
In the window that appears to the right, click the button next to <b>Polynomial</b>. Then check the boxes next to <b>Display Equation on chart</b> and <b>Display R-squared value on chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel6.png">
This produces the following curve on the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel7.png">
The equation of the curve is as follows:
y = 0.3302x<sup>2</sup> – 3.6682x + 21.653
The  R-squared  tells us the percentage of the variation in the  response variable  that can be explained by the predictor variables. The R-squared for this particular curve is <b>0.5874</b>.
<h3>Step 4: Choose the Best Trendline</h3>
We can also increase the order of the Polynomial that we use to see if a more flexible curve does a better job of fitting the dataset.
For example, we could choose to set the Polynomial Order to be 4: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel8.png">
This results in the following curve:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel9.png">
The equation of the curve is as follows:
y = -0.0192x<sup>4</sup> + 0.7081x<sup>3</sup> – 8.3649x<sup>2</sup> + 35.823x – 26.516
The R-squared for this particular curve is <b>0.9707</b>.
This R-squared is considerably higher than that of the previous curve, which indicates that it fits the dataset much more closely.
We can also use this equation of the curve to predict the value of the response variable based on the predictor variable. For example if <em>x</em> = 4 then we would predict that <em>y</em> = <b>23.34</b>:
y = -0.0192(4)<sup>4</sup> + 0.7081(4)<sup>3</sup> – 8.3649(4)<sup>2</sup> + 35.823(4) – 26.516 = 23.34
You can find more Excel tutorials on  this page .
<h2><span class="orange">Curve Fitting in Google Sheets (With Examples)</span></h2>
Often you may want to find the equation that best fits some curve for a dataset in Google Sheets.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets1.jpg"564">
Fortunately this is fairly easy to do using the <b>Trendline</b> function in Google Sheets.
This tutorial provides a step-by-step example of how to fit an equation to a curve in Google Sheets.
<h2>Step 1: Create the Data</h2>
First, let’s create a fake dataset to work with:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets2.jpg"476">
<h2>Step 2: Create a Scatterplot</h2>
Next, let’s create a scatterplot to visualize the dataset.
Highlight cells <b>A2:B16</b>, then click the <b>Insert</b> tab, then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets3.jpg"480">
By default, Google Sheets will insert a line chart.
However, we can easily change this to a scatterplot.
In the <b>Chart editor</b> panel that appears on the right side of the screen, click the dropdown arrow next to <b>Chart type</b> and choose <b>Scatter chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets4.jpg"335">
The following scatterplot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets5.jpg"545">
<h2>Step 3: Add a Trendline</h2>
Within the <b>Chart editor</b> panel, click the <b>Customize</b> tab. Then click the <b>Series</b> dropdown option. Then check the box next to <b>Trendline</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets6.jpg"313">
Then check the box below it that says <b>Show R<sup>2</sup></b>.
The following linear trendline will automatically be added to the plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets7-1.jpg"608">
The R-squared tells us the percentage of the variation in the  response variable  that can be explained by the predictor variables.
The R-squared for this particular curve is <b>0.363</b>.
<b>Related:</b>  What is a Good R-squared Value? 
<h2>Step 4: Choose the Best Trendline</h2>
From the plot above, it’s clear that the linear trendline does a poor job of capturing the behavior of the data.
Instead, we can choose to fit a polynomial curve.
To do so, click the dropdown arrow under <b>Type</b> and choose <b>Polynomial</b>.
Then click the dropdown arrow under <b>Polynomial degree</b> and choose <b>4</b>.
Lastly, click the dropdown arrow under <b>Label</b> and click <b>Use Equation</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets8.jpg"355">
This results in the following curve:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/curvesheets9.jpg">
The equation of the curve is as follows:
y = -0.0192x<sup>4</sup> + 0.7081x<sup>3</sup> – 8.3649x<sup>2</sup> + 35.823x – 26.516
The R-squared for this particular curve is <b>0.971</b>.
This R-squared is considerably higher than that of the previous trendline, which indicates that it fits the dataset much more closely.
We can also use this equation of the curve to predict the value of the response variable based on the predictor variable.
For example if <em>x</em> = 4 then we would predict that <em>y</em> = <b>23.34</b>:
y = -0.0192(4)<sup>4</sup> + 0.7081(4)<sup>3</sup> – 8.3649(4)<sup>2</sup> + 35.823(4) – 26.516 = 23.34
<b>Note</b>: You may have to play around with the value for the polynomial degree until you find a curve that appears to fit the data well without  overfitting .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Perform Linear Regression in Google Sheets 
 How to Find A Line of Best Fit in Google Sheets 
 How to Create a Forecast in Google Sheets 
<h2><span class="orange">Curve Fitting in R (With Examples)</span></h2>
Often you may want to find the equation that best fits some curve in R.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curveFittingR1.png">
The following step-by-step example explains how to fit curves to data in R using the <b>poly()</b> function and how to determine which curve fits the data best.
<h3>Step 1: Create & Visualize Data</h3>
First, let’s create a fake dataset and then create a scatterplot to visualize the data:
<b>#create data frame
df &lt;- data.frame(x=1:15, y=c(3, 14, 23, 25, 23, 15, 9, 5, 9, 13, 17, 24, 32, 36, 46))
#create a scatterplot of x vs. y
plot(df$x, df$y, pch=19, xlab='x', ylab='y')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curveFittingR0.png">
<h3>Step 2: Fit Several Curves</h3>
Next, let’s fit several polynomial regression models to the data and visualize the curve of each model in the same plot:
<b>#fit polynomial regression models up to degree 5
fit1 &lt;- lm(y~x, data=df)
fit2 &lt;- lm(y~poly(x,2,raw=TRUE), data=df)
fit3 &lt;- lm(y~poly(x,3,raw=TRUE), data=df)
fit4 &lt;- lm(y~poly(x,4,raw=TRUE), data=df)
fit5 &lt;- lm(y~poly(x,5,raw=TRUE), data=df)
#create a scatterplot of x vs. y
plot(df$x, df$y, pch=19, xlab='x', ylab='y')
#define x-axis values
x_axis &lt;- seq(1, 15, length=15)
#add curve of each model to plot
lines(x_axis, predict(fit1, data.frame(x=x_axis)), col='green')
lines(x_axis, predict(fit2, data.frame(x=x_axis)), col='red')
lines(x_axis, predict(fit3, data.frame(x=x_axis)), col='purple')
lines(x_axis, predict(fit4, data.frame(x=x_axis)), col='blue')
lines(x_axis, predict(fit5, data.frame(x=x_axis)), col='orange')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curveFittingR2.png">
To determine which curve best fits the data, we can look at the  adjusted R-squared  of each model.
This value tells us the percentage of the variation in the response variable that can be explained by the predictor variable(s) in the model, adjusted for the number of predictor variables.
<b>#calculated adjusted R-squared of each model
summary(fit1)$adj.r.squared
summary(fit2)$adj.r.squared
summary(fit3)$adj.r.squared
summary(fit4)$adj.r.squared
summary(fit5)$adj.r.squared
[1] 0.3144819
[1] 0.5186706
[1] 0.7842864
[1] 0.9590276
[1] 0.9549709
</b>
From the output we can see that the model with the highest adjusted R-squared is the fourth-degree polynomial, which has an adjusted R-squared of <b>0.959</b>.
<h3>Step 3: Visualize the Final Curve</h3>
Lastly, we can create a scatterplot with the curve of the fourth-degree polynomial model:
<b>#create a scatterplot of x vs. y
plot(df$x, df$y, pch=19, xlab='x', ylab='y')
#define x-axis values
x_axis &lt;- seq(1, 15, length=15)
#add curve of fourth-degree polynomial model
lines(x_axis, predict(fit4, data.frame(x=x_axis)), col='blue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curveFittingR1.png">
We can also get the equation for this line using the <b>summary()</b> function:
<b>summary(fit4)
Call:
lm(formula = y ~ poly(x, 4, raw = TRUE), data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-3.4490 -1.1732  0.6023  1.4899  3.0351 
Coefficients:         Estimate Std. Error t value Pr(>|t|)    
(Intercept)             -26.51615    4.94555  -5.362 0.000318 ***
poly(x, 4, raw = TRUE)1  35.82311    3.98204   8.996 4.15e-06 ***
poly(x, 4, raw = TRUE)2  -8.36486    0.96791  -8.642 5.95e-06 ***
poly(x, 4, raw = TRUE)3   0.70812    0.08954   7.908 1.30e-05 ***
poly(x, 4, raw = TRUE)4  -0.01924    0.00278  -6.922 4.08e-05 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 2.424 on 10 degrees of freedom
Multiple R-squared:  0.9707,Adjusted R-squared:  0.959 
F-statistic: 82.92 on 4 and 10 DF,  p-value: 1.257e-07
</b>
The equation of the curve is as follows:
y = -0.0192x<sup>4</sup> + 0.7081x<sup>3</sup> – 8.3649x<sup>2</sup> + 35.823x – 26.516
We can use this equation to predict the value of the  response variable  based on the predictor variables in the model. For example if <em>x</em> = 4 then we would predict that <em>y</em> = <b>23.34</b>:
y = -0.0192(4)<sup>4</sup> + 0.7081(4)<sup>3</sup> – 8.3649(4)<sup>2</sup> + 35.823(4) – 26.516 = 23.34
<h2><span class="orange">Curve Fitting in Python (With Examples)</span></h2>
Often you may want to fit a curve to some dataset in Python.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvePython3.png">
The following step-by-step example explains how to fit curves to data in Python using the  numpy.polyfit()  function and how to determine which curve fits the data best.
<h3>Step 1: Create & Visualize Data</h3>
First, let’s create a fake dataset and then create a scatterplot to visualize the data:
<b>import pandas as pd
import matplotlib.pyplot as plt
#create DataFrame
df = pd.DataFrame({'x': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],   'y': [3, 14, 23, 25, 23, 15, 9, 5, 9, 13, 17, 24, 32, 36, 46]})
#create scatterplot of x vs. y
plt.scatter(df.x, df.y)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvePython1.png">
<h3>Step 2: Fit Several Curves</h3>
Next, let’s fit several polynomial regression models to the data and visualize the curve of each model in the same plot:
<b>import numpy as np
#fit polynomial models up to degree 5
model1 = np.poly1d(np.polyfit(df.x, df.y, 1))
model2 = np.poly1d(np.polyfit(df.x, df.y, 2))
model3 = np.poly1d(np.polyfit(df.x, df.y, 3))
model4 = np.poly1d(np.polyfit(df.x, df.y, 4))
model5 = np.poly1d(np.polyfit(df.x, df.y, 5))
#create scatterplot
polyline = np.linspace(1, 15, 50)
plt.scatter(df.x, df.y)
#add fitted polynomial lines to scatterplot 
plt.plot(polyline, model1(polyline), color='green')
plt.plot(polyline, model2(polyline), color='red')
plt.plot(polyline, model3(polyline), color='purple')
plt.plot(polyline, model4(polyline), color='blue')
plt.plot(polyline, model5(polyline), color='orange')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvePython2.png">
To determine which curve best fits the data, we can look at the  adjusted R-squared  of each model.
This value tells us the percentage of the variation in the response variable that can be explained by the predictor variable(s) in the model, adjusted for the number of predictor variables.
<b>#define function to calculate adjusted r-squared
def adjR(x, y, degree):
    results = {}
    coeffs = np.polyfit(x, y, degree)
    p = np.poly1d(coeffs)
    yhat = p(x)
    ybar = np.sum(y)/len(y)
    ssreg = np.sum((yhat-ybar)**2)
    sstot = np.sum((y - ybar)**2)
    results['r_squared'] = 1- (((1-(ssreg/sstot))*(len(y)-1))/(len(y)-degree-1))
    return results
#calculated adjusted R-squared of each model
adjR(df.x, df.y, 1)
adjR(df.x, df.y, 2)
adjR(df.x, df.y, 3)
adjR(df.x, df.y, 4)
adjR(df.x, df.y, 5)
{'r_squared': 0.3144819}
{'r_squared': 0.5186706}
{'r_squared': 0.7842864}
{'r_squared': 0.9590276}
{'r_squared': 0.9549709}
</b>
From the output we can see that the model with the highest adjusted R-squared is the fourth-degree polynomial, which has an adjusted R-squared of <b>0.959</b>.
<h3>Step 3: Visualize the Final Curve</h3>
Lastly, we can create a scatterplot with the curve of the fourth-degree polynomial model:
<b>#fit fourth-degree polynomial
model4 = np.poly1d(np.polyfit(df.x, df.y, 4))
#define scatterplot
polyline = np.linspace(1, 15, 50)
plt.scatter(df.x, df.y)
#add fitted polynomial curve to scatterplot
plt.plot(polyline, model4(polyline), '--', color='red')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvePython3.png">
We can also get the equation for this line using the <b>print()</b> function:
<b>print(model4)
          4          3         2
-0.01924 x + 0.7081 x - 8.365 x + 35.82 x - 26.52
</b>
The equation of the curve is as follows:
y = -0.01924x<sup>4</sup> + 0.7081x<sup>3</sup> – 8.365x<sup>2</sup> + 35.82x – 26.52
We can use this equation to predict the value of the  response variable  based on the predictor variables in the model. For example if <em>x</em> = 4 then we would predict that <em>y</em> = <b>23.32</b>:
y = -0.0192(4)<sup>4</sup> + 0.7081(4)<sup>3</sup> – 8.365(4)<sup>2</sup> + 35.82(4) – 26.52 = 23.32
<h2><span class="orange">How to Interpret a Curved Residual Plot (With Example)</span></h2>
<b>Residual plots</b> are used to assess whether or not the  residuals  in a regression model are normally distributed and whether or not they exhibit  heteroscedasticity .
Ideally, you would like the points in a residual plot to be randomly scattered around a value of zero with no clear pattern.
If you encounter a residual plot where the points in the plot have a curved pattern, it likely means that the regression model you have specified for the data is not correct.
In most cases, it means that you attempted to fit a linear regression model to a dataset that instead follows a quadratic trend.
The following example shows how to interpret (and fix) a curved residual plot in practice.
<h2>Example: Interpreting a Curved Residual Plot</h2>
Suppose we collect the following data on the number of hours worked per week and the reported happiness level (on a scale of 0-100) for 11 different people in some office:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/quadTI1.png">
If we create a simple scatter plot of hours worked vs. happiness level, here’s what it would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/curved1.jpg"571">
Now suppose we would like to fit a regression model using hours worked to predict happiness level.
The following code shows how fit a <b>simple linear regression model</b> to this dataset and produce a residual plot in R:
<b>#create dataframe
df &lt;- data.frame(hours=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60), happiness=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))
#fit linear regression model
linear_model &lt;- lm(happiness ~ hours, data=df)
#get list of residuals 
res &lt;- resid(linear_model)
#produce residual vs. fitted plot
plot(fitted(linear_model), res, xlab='Fitted Values', ylab='Residuals')
#add a horizontal line at 0 
abline(0,0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/curved2.jpg">
The x-axis displays the fitted values and the y-axis displays the residuals.
From the plot we can see that there is a curved pattern in the residuals, which indicates that a linear regression model does not provide an appropriate fit to this dataset.
The following code shows how fit a <b>quadratic regression model</b> to this dataset and produce a residual plot in R:
<b>#create dataframe
df &lt;- data.frame(hours=c(6, 9, 12, 14, 30, 35, 40, 47, 51, 55, 60), happiness=c(14, 28, 50, 70, 89, 94, 90, 75, 59, 44, 27))
#define quadratic term to use in model
df$hours2 &lt;- df$hours^2
#fit quadratic regression model
quadratic_model &lt;- lm(happiness ~ hours + hours2, data=df)
#get list of residuals 
res &lt;- resid(quadratic_model)
#produce residual vs. fitted plot
plot(fitted(quadratic_model), res, xlab='Fitted Values', ylab='Residuals')
#add a horizontal line at 0 
abline(0,0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/curved3.jpg"477">
Once again the x-axis displays the fitted values and the y-axis displays the residuals.
From the plot we can see that the residuals are randomly scattered around zero and there is no clear pattern in the residuals.
This tells us that a quadratic regression model does a much better job of fitting this dataset compared to a linear regression model.
This should make sense considering we saw that the true relationship between hours worked and happiness level appeared to be quadratic instead of linear.
<h2>Additional Resources</h2>
The following tutorials explain how to create residual plots using different statistical software:
 How to Create a Residual Plot by Hand 
 How to Create a Residual Plot in R 
 How to Create a Residual Plot in Excel 
 How to Create a Residual Plot in Python 
<h2><span class="orange">What is Curvilinear Regression? (Definition & Examples)</span></h2>
<b>Curvilinear regression</b> is the name given to any regression model that attempts to fit a <em>curve</em> as opposed to a straight line.
Common examples of curvilinear regression models include:
<b>Quadratic Regression:</b> Used when a quadratic relationship exists between a predictor variable and a  response variable . When graphed, this type of relationship looks like a “U” or an upside-down “U” on a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvilinear1.png">
<b>Cubic Regression:</b> Used when a cubic relationship exists between a predictor variable and a response variable. When graphed, this type of relationship has two distinct curves on a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvilinear2.png">
These are both in contrast to  simple linear regression  in which the relationship between the predictor variable and the response variable is linear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/curvilinear3.png">
<h3>The Formula of Curvilinear Regression Models</h3>
A <b>simple linear regression model</b> attempts to fit a dataset using the following formula:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x
where:
<b><U+0177>:</b> The response variable
<b>β<sub>0</sub>, β<sub>1</sub>:</b> The regression coefficients
<b>x:</b> The predictor variable
In contrast, a <b>quadratic regression model</b> uses the following formula:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x + β<sub>2</sub>x<sup>2</sup>
And a <b>cubic regression model</b> uses the following formula:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x + β<sub>2</sub>x<sup>2</sup> + β<sub>3</sub>x<sup>3</sup>
A more general name given to regression models that include exponents is <b>polynomial regression</b>, which takes on the following formula:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x + β<sub>2</sub>x<sup>2</sup> + … + β<sub>k</sub>x<sup>k</sup>
The value for <em>k</em> indicates the <b>degree</b> of the polynomial. Although the degree can be any positive number, in practice we rarely fit polynomial regression models with a degree higher than 3 or 4.
By using exponents in the regression model formula, polynomial regression models are able to fit <em>curves</em> to datasets instead of straight lines.
<h3>When to Use Curvilinear Regression</h3>
The easiest way to know whether or not you should use curvilinear regression is to create a scatterplot of the predictor variable and response variable.
If the scatterplot displays a linear relationship between the two variables, then simple linear regression is likely appropriate to use.
However, if the scatterplot shows a quadratic, cubic, or some other curvilinear pattern between the predictor and response variable, then curvilinear regression is likely more appropriate to use.
You can also fit a simple linear regression model and a curvilinear regression model and compare the <b>adjusted R-squared values</b> of each model to determine which model offers a better fit to the data.
The adjusted R-squared is useful because it tells you the proportion of the variance in the response variable that can be explained by the predictor variable(s), adjusted for the number of predictor variables in the model.
In general, the model with the higher adjusted R-squared value offers a better fit to the dataset.
<h2><span class="orange">How to Perform Data Binning in Excel (With Example)</span></h2>
Placing numeric data into <b>bins</b> is a useful way to summarize the distribution of values in a dataset.
The following example shows how to perform data binning in Excel.
<h3>Example: Data Binning in Excel</h3>
Suppose we have the following dataset that shows the number of points scored by various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/binning1.jpg"443">
To place each of the values into bins, we can create a new column that defines the largest value for each bin:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/binning2.jpg"454">
In this example, we have specified the following bins:
0-5
6-10
11-15
16-20
21-25
26-30
To calculate how many data values fall into each bin, click the <b>Data</b> tab along the top ribbon, then click <b>Data Analysis</b> within the <b>Analyze</b> group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/toolpak.png">
<b>Note</b>: If you don’t see an option for Data Analysis, you need to first  load the free Analysis Toolpak  in Excel.
In the new window that appears, click <b>Histogram</b>, then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/binning3.jpg"545">
Choose A2:A16 as the <b>Input Range</b>, C2:C7 as the <b>Bin Range</b>, E2 as the <b>Output Range</b>, and check the box next to <b>Chart Output</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/binning4.jpg"573">
The number of values that fall into each bin will automatically be calculated:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/binning5.jpg"578">
From the output we can see:
<b>2</b> values fall into the 0-5 bin.
<b>2</b> values fall into the 6-10 bin.
<b>3</b> values fall into the 11-15 bin.
<b>1</b> value falls into the 16-20 bin.
<b>3</b> values fall into the 21-25 bin.
<b>4</b> values fall into the 26-30 bin.
<b>0</b> values are greater than 30.
The histogram allows us to visualize this distribution of data values as well.
<b>Note</b>: In this example, we chose to make each bin the same width but we can make individual bins different sizes if we’d like.
<h2><span class="orange">How to Perform Data Binning in Python (With Examples)</span></h2>
You can use the following basic syntax to perform data binning on a pandas DataFrame:
<b>import pandas as pd
#perform binning with 3 bins
df['new_bin'] = pd.qcut(df['variable_name'], q=3)
</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [4, 4, 7, 8, 12, 13, 15, 18, 22, 23, 23, 25],   'assists': [2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8],   'rebounds': [7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9]})
#view DataFrame
print(df)
    points  assists  rebounds
0        4        2         7
1        4        5         7
2        7        4         4
3        8        7         6
4       12        7         3
5       13        8         8
6       15        5         9
7       18        4         9
8       22        5        12
9       23       11        11
10      23       13         8
11      25        8         9
</b>
<h3>Example 1: Perform Basic Data Binning</h3>
The following code shows how to perform data binning on the <b>points</b> variable using the  qcut()  function with specific break marks:
<b>#perform data binning on <em>points</em> variable
df['points_bin'] = pd.qcut(df['points'], q=3)
#view updated DataFrame
print(df)
    points  assists  rebounds        points_bin
0        4        2         7   (3.999, 10.667]
1        4        5         7   (3.999, 10.667]
2        7        4         4   (3.999, 10.667]
3        8        7         6   (3.999, 10.667]
4       12        7         3  (10.667, 19.333]
5       13        8         8  (10.667, 19.333]
6       15        5         9  (10.667, 19.333]
7       18        4         9  (10.667, 19.333]
8       22        5        12    (19.333, 25.0]
9       23       11        11    (19.333, 25.0]
10      23       13         8    (19.333, 25.0]
11      25        8         9    (19.333, 25.0]
</b>
Notice that each row of the data frame has been placed in one of three bins based on the value in the points column.
We can use the <b>value_counts()</b> function to find how many rows have been placed in each bin:
<b>#count frequency of each bin
df['points_bin'].value_counts()
(3.999, 10.667]     4
(10.667, 19.333]    4
(19.333, 25.0]      4
Name: points_bin, dtype: int64
</b>
We can see that each bin contains 4 observations.
<h3>Example 2: Perform Data Binning with Specific Quantiles</h3>
We can also perform data binning by using specific quantiles:
<b>#perform data binning on <em>points</em> variable with specific quantiles
df['points_bin'] = pd.qcut(df['points'], q=[0, .2, .4, .6, .8, 1])
#view updated DataFrame
print(df)
    points  assists  rebounds    points_bin
0        4        2         7  (3.999, 7.2]
1        4        5         7  (3.999, 7.2]
2        7        4         4  (3.999, 7.2]
3        8        7         6   (7.2, 12.4]
4       12        7         3   (7.2, 12.4]
5       13        8         8  (12.4, 16.8]
6       15        5         9  (12.4, 16.8]
7       18        4         9  (16.8, 22.8]
8       22        5        12  (16.8, 22.8]
9       23       11        11  (22.8, 25.0]
10      23       13         8  (22.8, 25.0]
11      25        8         9  (22.8, 25.0]
</b>
<h3>
<b>Example 3: Perform Data Binning with Labels</b>
</h3>
We can also perform data binning by using specific quantiles and specific labels:
<b>#perform data binning on <em>points</em> variable with specific quantiles and labels
df['points_bin'] = pd.qcut(df['points'],           q=[0, .2, .4, .6, .8, 1],           labels=['A', 'B', 'C', 'D', 'E'])
#view updated DataFrame
print(df)
    points  assists  rebounds points_bin
0        4        2         7          A
1        4        5         7          A
2        7        4         4          A
3        8        7         6          B
4       12        7         3          B
5       13        8         8          C
6       15        5         9          C
7       18        4         9          D
8       22        5        12          D
9       23       11        11          E
10      23       13         8          E
11      25        8         9          E
</b>
Notice that each row has been assigned a bin based on the value of the <b>points</b> column and the bins have been labeled using letters.
<h2><span class="orange">How to Perform Data Binning in R (With Examples)</span></h2>
You can use one of the following two methods to perform data binning in R:
<b>Method 1: Use cut() Function</b>
<b>library(dplyr)
#perform binning with custom breaks
df %>% mutate(new_bin = cut(variable_name, breaks=c(0, 10, 20, 30)))
#perform binning with specific number of bins
df %>% mutate(new_bin = cut(variable_name, breaks=3))
</b>
<b>Method 2: Use ntile() Function</b>
<b>library(dplyr)
#perform binning with specific number of bins
df %>% mutate(new_bin = ntile(variable_name, n=3))
</b>
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(points=c(4, 4, 7, 8, 12, 13, 15, 18, 22, 23, 23, 25), assists=c(2, 5, 4, 7, 7, 8, 5, 4, 5, 11, 13, 8), rebounds=c(7, 7, 4, 6, 3, 8, 9, 9, 12, 11, 8, 9))
#view head of data frame
head(df)
  points assists rebounds
1      4       2        7
2      4       5        7
3      7       4        4
4      8       7        6
5     12       7        3
6     13       8        8</b>
<h3>Example 1: Perform Data Binning with cut() Function</h3>
The following code shows how to perform data binning on the <b>points</b> variable using the <b>cut()</b> function with specific break marks:
<b>library(dplyr)
#perform data binning on <em>points</em> variable
df %>% mutate(points_bin = cut(points, breaks=c(0, 10, 20, 30)))
   points assists rebounds points_bin
1       4       2        7     (0,10]
2       4       5        7     (0,10]
3       7       4        4     (0,10]
4       8       7        6     (0,10]
5      12       7        3    (10,20]
6      13       8        8    (10,20]
7      15       5        9    (10,20]
8      18       4        9    (10,20]
9      22       5       12    (20,30]
10     23      11       11    (20,30]
11     23      13        8    (20,30]
12     25       8        9    (20,30]
</b>
Notice that each row of the data frame has been placed in one of three bins based on the value in the points column.
We could also specify the number of breaks to use to create bins of equal width that range from the minimum value to the maximum value of the <b>points</b> column:
<b>library(dplyr)
#perform data binning on <em>points</em> variable
df %>% mutate(points_bin = cut(points, breaks=3))
   points assists rebounds points_bin
1       4       2        7  (3.98,11]
2       4       5        7  (3.98,11]
3       7       4        4  (3.98,11]
4       8       7        6  (3.98,11]
5      12       7        3    (11,18]
6      13       8        8    (11,18]
7      15       5        9    (11,18]
8      18       4        9    (11,18]
9      22       5       12    (18,25]
10     23      11       11    (18,25]
11     23      13        8    (18,25]
12     25       8        9    (18,25]
</b>
<h3>
<b>Example 2: Perform Data Binning with ntile() Function</b>
</h3>
The following code shows how to perform data binning on the <b>points</b> variable using the <b>ntile()</b> function with a specific number of resulting bins:
<b>library(dplyr)
#perform data binning on <em>points</em> variable
df %>% mutate(points_bin = ntile(points, n=3))
   points assists rebounds points_bin
1       4       2        7          1
2       4       5        7          1
3       7       4        4          1
4       8       7        6          1
5      12       7        3          2
6      13       8        8          2
7      15       5        9          2
8      18       4        9          2
9      22       5       12          3
10     23      11       11          3
11     23      13        8          3
12     25       8        9          3
</b>
Notice that each row has been assigned a bin from 1 to 3 based on the value of the <b>points</b> column.
It’s best to use the <b>ntile()</b> function when you’d like an integer value to be displayed in each row as opposed to an interval showing the range of the bin.
<h2><span class="orange">data.table vs. data frame in R: Three Key Differences</span></h2>
In the R programming language, a <b>data.frame</b> is part of base R.
Any <b>data.frame</b> can be converted to a <b>data.table</b> by using the <b>setDF</b> function from the <b>data.table</b> package.
A data.table offers the following benefits over a data.frame in R:
<b>1.</b> You can use the  fread  function from the data.table package to read a file into a data.table <em>much</em> faster than base R functions such as  read.csv , which read files into a data.frame.
<b>2.</b> You can perform operations (such as grouping and aggregating) on a data.table <em>much</em> faster than a data.frame.
<b>3.</b> When printing a data.frame to a console, R will attempt to display every single row from the data.frame. However, a data.table will only display the first 100 rows, which can prevent your session from freezing or crashing if you’re working with a massive dataset.
The following examples illustrate these differences between data.frames and data.tables in practice.
<h2>Difference #1: Faster Importing with fread Function</h2>
The following code shows how to import some data frame with 10,000 rows and 100 columns using the <b>fread</b> function from the data.table package and the <b>read.csv</b> function from base R:
<b>library(microbenchmark)
library(data.table)
#make this example reproducible
set.seed(1)
#create data frame with 10,000 rows and 100 columns
df &lt;- as.data.frame(matrix(runif(10^4 * 100), nrow = 10^4))
#export CSV to current working directory
write.csv(df, "test.csv", quote = FALSE)
#import CSV file using fread and read.csv and time how long it takes
results &lt;- microbenchmark(
  read.csv = read.csv("test.csv", header = TRUE, stringsAsFactors = FALSE),
  fread = fread("test.csv", sep = ",", stringsAsFactors = FALSE),
  times = 10)
#view results
results
Unit: milliseconds
     expr      min       lq      mean   median       uq       max neval cld
 read.csv 817.1867 892.8748 1026.7071 899.5755 926.9120 1964.0540    10   b
    fread 113.5889 116.2735  136.4079 124.3816 136.0534  211.7484    10  a </b>
From the results we can see that <b>fread</b> is roughly 10 times faster at importing this CSV file compared to the <b>read.csv</b> function.
Note that this difference will be even greater for larger datasets.
<h2>Difference #2: Faster Data Manipulation with data.table</h2>
In general, <b>data.table</b> can also perform any data manipulation task much faster than a <b>data.frame</b>.
For example, the following code shows how to calculate the mean of one variable, grouped by another variable in both a data.table and data.frame:
<b>library(microbenchmark)
library(data.table)
#make this example reproducible
set.seed(1)
#create data frame with 10,000 rows and 100 columns
d_frame &lt;- data.frame(team=rep(c('A', 'B'), each=5000),      points=c(rnorm(10000, mean=20, sd=3)))
#create data.table from data.frame
d_table &lt;- setDT(d_frame)
#calculate mean of points grouped by team in data.frame and data.table
results &lt;- microbenchmark(
  mean_d_frame = aggregate(d_frame$points, list(d_frame$team), FUN=mean),
  mean_d_table = d_table[ ,list(mean=mean(points)), by=team],
  times = 10)
#view results
results
Unit: milliseconds
         expr    min     lq    mean median     uq    max neval cld
 mean_d_frame 2.9045 3.0077 3.11683 3.1074 3.1654 3.4824    10   b
 mean_d_table 1.0539 1.1140 1.52002 1.2075 1.2786 3.6084    10  a </b>
From the results we can see that <b>data.table</b> is about three times faster than <b>data.frame</b>.
For larger datasets, this difference will be even greater.
<h2>Difference #3: Fewer Printed Lines with data.table</h2>
When printing a<b> data.frame</b> to a console, R will attempt to display every single row from the data.frame.
However, a <b>data.table</b> will only display the first 100 rows, which can prevent your session from freezing or crashing if you’re working with a massive dataset.
For example, in the following code we create both a data frame and a data.table with 200 rows.
When printing the data.frame, R will attempt to print every single row while printing the data.table will only show the first five rows and last five rows:
<b>library(data.table)
#make this example reproducible
set.seed(1)
#create data frame
d_frame &lt;- data.frame(x=rnorm(200),      y=rnorm(200),      z=rnorm(200))
#view data frame
d_frame
               x           y             z
1   -0.055303118  1.54858564 -2.065337e-02
2    0.354143920  0.36706204 -3.743962e-01
3   -0.999823809 -1.57842544  4.392027e-01
4    2.586214840  0.17383147 -2.081125e+00
5   -1.917692199 -2.11487401  4.073522e-01
6    0.039614766  2.21644236  1.869164e+00
7   -1.942259548  0.81566443  4.740712e-01
8   -0.424913746  1.01081030  4.996065e-01
9   -1.753210825 -0.98893038 -6.290307e-01
10   0.232382655 -1.25229873 -1.324883e+00
11   0.027278832  0.44209325 -3.221920e-01
...
#create data table
d_table &lt;- setDT(d_frame)
#view data table
d_table
               x           y           z
  1: -0.05530312  1.54858564 -0.02065337
  2:  0.35414392  0.36706204 -0.37439617
  3: -0.99982381 -1.57842544  0.43920275
  4:  2.58621484  0.17383147 -2.08112491
  5: -1.91769220 -2.11487401  0.40735218
 ---                                    
196: -0.06196178  1.08164065  0.58609090
197:  0.34160667 -0.01886703  1.61296255
198: -0.38361957 -0.03890329  0.71377217
199: -0.80719743 -0.89674205 -0.49615702
200: -0.26502679 -0.15887435 -1.73781026</b>
This is a benefit that <b>data.table</b> offers compared to <b>data.frame</b>, especially when working with massive datasets that you don’t want to accidently print to the console.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Append Rows to a Data Frame in R 
 How to Keep Certain Columns in R 
 How to Select Only Numeric Columns in R 
<h2><span class="orange">How to Use Data Validation in Google Sheets</span></h2>
<b>Data validation</b> can be used in Google Sheets to control what a user can enter into a particular cell.
The following example shows how to use data validation in practice.
<h3>Example 1: Data Validation in Google Sheets</h3>
Suppose we are sending out a survey to users in Google Sheets in which we want them to type in cell <b>B1</b> the number of days per week they exercise.
To ensure that users can only enter numbers between 0 and 7, we can click cell <b>B1</b>, then click the <b>Data</b> tab, then click <b>Data validation</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/valid1.jpg">
In the new window that appears, we’ll fill in the following information:
<b>Cell range</b>: data_validation!B1
<b>Criteria</b>: number (between 0 and 7)
<b>On invalid data</b>: Show warning
<b>Appearance</b>: Show validation help text (with custom text)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/valid2.jpg"634">
Once we click <b>Save</b>, the data validation rule will automatically be applied to cell <b>B2</b>.
Now, if we type a number in cell <b>B2</b> that is outside of the range 0-7 we’ll receive a warning message:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/valid3.jpg"532">
This warning message lets the user know that the input must be a number between 0 and 7.
If we instead enter a number between 0 and 7 then we won’t receive any warning message:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/valid4.jpg"499">
Notice that we don’t receive a warning message because we typed in a valid number.
<b>Note</b>: In this example we chose to use a number range as the criteria for the data validation but we could also specify to use a date, text, or list of items as the data validation instead.
<h2><span class="orange">A Guide to dbinom, pbinom, qbinom, and rbinom in R</span></h2>
This tutorial explains how to work with the  binomial distribution  in R using the functions <b>dbinom</b>, <b>pbinom</b>, <b>qbinom</b>, and <b>rbinom</b>.
<h2>dbinom</h2>
The function <b>dbinom </b>returns the value of the probability density function (pdf) of the binomial distribution given a certain random variable <em>x</em>, number of trials (size) and probability of success on each trial (prob). The syntax for using dbinom is as follows:
<b>dbinom(x, size, prob) </b>
Put simply, <b>dbinom </b>finds the probability of getting a certain number of<em> </em>successes <b>(x)</b> in a certain number of trials <b>(size)</b> where the probability of success on each trial is fixed <b>(prob)</b>.
The following examples illustrates how to solve some probability questions using dbinom.
<b>Example 1:</b> <em>Bob makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?</em>
<b>#find the probability of 10 successes during 12 trials where the probability of
#success on each trial is 0.6
dbinom(x=10, size=12, prob=.6)
# [1] 0.06385228
</b>
The probability that he makes exactly 10 shots is<b> 0.0639</b>.
<b>Example 2:</b> <em>Sasha flips a fair coin 20 times. What is the probability that the coin lands on heads exactly 7 times?</em>
<b>#find the probability of 7 successes during 20 trials where the probability of
#success on each trial is 0.5
dbinom(x=7, size=20, prob=.5)
# [1] 0.07392883
</b>
The probability that the coin lands on heads exactly 7 times is <b>0.0739</b>.
<h2>pbinom</h2>
The function <b>pbinom </b>returns the value of the cumulative density function (cdf) of the binomial distribution given a certain random variable <em>q</em>, number of trials (size) and probability of success on each trial (prob). The syntax for using pbinom is as follows:
<b>pbinom(q, size, prob) </b>
Put simply, <b>pbinom </b>returns the area to the left of a given value <em>q</em><em> </em>in the binomial distribution. If you’re interested in the area to the right of a given value <em>q</em>, you can simply add the argument <b>lower.tail = FALSE</b>
<b>pbinom(q, size, prob, lower.tail = FALSE) </b>
The following examples illustrates how to solve some probability questions using pbinom.
<b>Example 1:</b><em> Ando flips a fair coin 5 times. What is the probability that the coin lands on heads more than 2 times?</em>
<b>#find the probability of more than 2 successes during 5 trials where the
#probability of success on each trial is 0.5
pbinom(2, size=5, prob=.5, lower.tail=FALSE)
# [1] 0.5</b>
The probability that the coin lands on heads more than 2 times is<b> 0.5</b>.
<b>Example 2:</b><em>  Suppose Tyler scores a strike on 30% of his attempts when he bowls. If he bowls 10 times, what is the probability that he scores 4 or fewer strikes?</em>
<b>#find the probability of 4 or fewer successes during 10 trials where the
#probability of success on each trial is 0.3
pbinom(4, size=10, prob=.3)
# [1] 0.8497317</b>
The probability that he scores 4 or fewer strikes is <b>0.8497</b>.
<h2>qbinom</h2>
The function <b>qbinom </b>returns the value of the inverse cumulative density function (cdf) of the binomial distribution given a certain random variable <em>q</em>, number of trials (size) and probability of success on each trial (prob). The syntax for using qbinom is as follows:
<b>qbinom(q, size, prob) </b>
Put simply, you can use <b>qbinom </b>to find out the p<sup>th</sup> quantile of the binomial distribution.
The following code illustrates a few examples of <b>qbinom </b>in action:
<b>#find the 10th quantile of a binomial distribution with 10 trials and prob
#of success on each trial = 0.4
qbinom(.10, size=10, prob=.4)
# [1] 2
#find the 40th quantile of a binomial distribution with 30 trials and prob
#of success on each trial = 0.25
qbinom(.40, size=30, prob=.25)
# [1] 7
</b>
<h2>rbinom</h2>
The function <b>rbinom </b>generates a vector of binomial distributed random variables given a vector length <em>n</em>, number of trials (size) and probability of success on each trial (prob). The syntax for using rbinom is as follows:
<b>rbinom(n, size, prob) </b>
The following code illustrates a few examples of <b>rnorm</b> in action:
<b>#generate a vector that shows the number of successes of 10 binomial experiments with
#100 trials where the probability of success on each trial is 0.3.
results &lt;- rbinom(10, size=100, prob=.3)
results
# [1] 31 29 28 30 35 30 27 39 30 28
#find mean number of successes in the 10 experiments (compared to expected
#mean of 30)
mean(results)
# [1] 32.8
#generate a vector that shows the number of successes of 1000 binomial experiments
#with 100 trials where the probability of success on each trial is 0.3.
results &lt;- rbinom(1000, size=100, prob=.3)
#find mean number of successes in the 100 experiments (compared to expected
#mean of 30)
mean(results)
# [1] 30.105
</b>
Notice how the more random variables we create, the closer the mean number of successes is to the expected number of successes. 
<em>Note: “Expected number of successes” = <b>n</b> * <b>p</b> where <b>n</b> is the number of trials and <b>p</b> is the probability of success on each trial.</em>
<h2><span class="orange">How to Calculate Deciles in Excel (With Examples)</span></h2>
In statistics, <b>deciles</b> are numbers that split a dataset into ten groups of equal frequency.
The first decile is the point where 10% of all data values lie below it. The second decile is the point where 20% of all data values lie below it, and so forth.
We can use the following function to calculate the deciles for a dataset in Excel:
<b>=PERCENTILE(CELL RANGE, PERCENTILE)
</b>
The following example shows how to use this function in practice.
<h3>Example: Calculate Deciles in Excel</h3>
Suppose we have the following dataset with 20 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/decileExcel1.png">
The following image shows how to calculate the deciles for the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/decileExcel2.png">
The way to interpret the deciles is as follows:
20% of all data values lie below <b>67.8</b>.
30% of all data values lie below <b>76.5</b>.
40% of all data values lie below <b>83.6</b>.
And so on.
To place each data value into a decile, we can use the <b>PERCENTRANK.EXC()</b> function, which uses the following syntax:
<b>=PERCENTRANK.EXC(CELL RANGE, DATA VALUE, SIGNIFICANCE)</b>
The following image shows how to use this function for our dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/decileExcel3.png">
Note that this function finds the relative rank of a value in a dataset as a percentage and rounds to one digit, which is equivalent to finding the decile that the value falls in.
The way to interpret the output is as follows:
The data value 58 falls between the percentile 0 and 0.1, thus it falls in the first decile.
The data value 64 falls between the percentile 0.1 and 0.2, thus it falls in the second decile.
The data value 67 falls between the percentile 0.1 and 0.2, thus it falls in the second decile.
The data value 68 falls between the percentile 0.2 and 0.3, thus it falls in the third decile.
And so on.
<h2><span class="orange">How to Calculate Deciles in Google Sheets (With Examples)</span></h2>
In statistics, <b>deciles</b> are numbers that split a dataset into ten groups of equal frequency.
The first decile is the point where 10% of all data values lie below it.
The second decile is the point where 20% of all data values lie below it.
The third decile is the point where 30% of all data values lie below it.
 And so on.
We can use the following function to calculate the deciles for a dataset in Google Sheets:
<b>=PERCENTILE(CELL RANGE, PERCENTILE)
</b>
For example, we would use the following formula to calculate the value of the third decile for a dataset in the range A1:A50:
<b>=PERCENTILE(A1:A50, 0.3)</b>
The following example shows how to use this function in practice.
<h3>Example: Calculate Deciles in Google Sheets</h3>
Suppose we have the following dataset with 20 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/decileSheets1.jpg"429">
The following image shows how to calculate the deciles for the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/decileSheets2.jpg">
Here is how to interpret each decile value:
10% of all data values lie below <b>63.4</b>.
20% of all data values lie below <b>67.8</b>.
30% of all data values lie below <b>76.5</b>.
And so on.
To place each data value into a decile, we can use the <b>PERCENTRANK.EXC()</b> function, which uses the following syntax:
<b>=PERCENTRANK.EXC(CELL RANGE, DATA VALUE, SIGNIFICANCE)</b>
The following image shows how to use this function for our dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/decileSheets3.jpg"492">
Note that this function finds the relative rank of a value in a dataset as a percentage and rounds to one digit, which is equivalent to finding the decile that the value falls in.
The way to interpret the output is as follows:
The data value 58 falls between the percentile 0 and 0.1, thus it falls in the first decile.
The data value 64 falls between the percentile 0.1 and 0.2, thus it falls in the second decile.
The data value 67 falls between the percentile 0.1 and 0.2, thus it falls in the second decile.
The data value 68 falls between the percentile 0.2 and 0.3, thus it falls in the third decile.
And so on.
<h2><span class="orange">How to Calculate Deciles in Python (With Examples)</span></h2>
In statistics, <b>deciles</b> are numbers that split a dataset into ten groups of equal frequency.
The first decile is the point where 10% of all data values lie below it. The second decile is the point where 20% of all data values lie below it, and so on.
We can use the following syntax to calculate the deciles for a dataset in Python:
<b>import numpy as np
np.percentile(var, np.arange(0, 100, 10))
</b>
The following example shows how to use this function in practice.
<h3>Example: Calculate Deciles in Python</h3>
The following code shows how to create a fake dataset with 20 values and then calculate the values for the deciles of the dataset:
<b>import numpy as np
#create data
data = np.array([56, 58, 64, 67, 68, 73, 78, 83, 84, 88, 89, 90, 91, 92, 93, 93, 94, 95, 97, 99])
#calculate deciles of data
np.percentile(data, np.arange(0, 100, 10))
array([56. , 63.4, 67.8, 76.5, 83.6, 88.5, 90.4, 92.3, 93.2, 95.2])</b>
The way to interpret the deciles is as follows:
10% of all data values lie below <b>63.4</b>
20% of all data values lie below <b>67.8</b>.
30% of all data values lie below <b>76.5</b>.
40% of all data values lie below <b>83.6</b>.
50% of all data values lie below <b>88.5</b>.
60% of all data values lie below <b>90.4</b>.
70% of all data values lie below <b>92.3</b>.
80% of all data values lie below <b>93.2</b>.
90% of all data values lie below <b>95.2</b>.
Note that the first value in the output (56) simply denotes the minimum value in the dataset.
<h3>Example: Place Values into Deciles in Python</h3>
To place each data value into a decile, we can use the <b>qcut</b> pandas function.
Here’s how to use this function for the dataset we created in the previous example:
<b>import pandas as pd
#create data frame
df = pd.DataFrame({'values': [56, 58, 64, 67, 68, 73, 78, 83, 84, 88,              89, 90, 91, 92, 93, 93, 94, 95, 97, 99]})
#calculate decile of each value in data frame
df['Decile'] = pd.qcut(df['values'], 10, labels=False)
#display data frame
df
valuesDecile
0560
1580
2641
3671
4682
5732
6783
7833
8844
9884
10895
11905
12916
13926
14937
15937
16948
17958
18979
19999
</b>
The way to interpret the output is as follows:
The data value 56 falls between the percentile 0% and 10%, thus it falls in decile <b>0</b>.
The data value 58 falls between the percentile 0% and 10%, thus it falls in decile <b>0</b>.
The data value 64 falls between the percentile 10% and 20%, thus it falls in decile <b>1</b>..
The data value 67 falls between the percentile 10% and 20%, thus it falls decile <b>1</b>.
The data value 68 falls between the percentile 20% and 30%, thus it falls decile <b>2</b>.
And so on.
<h2><span class="orange">How to Calculate Deciles in R (With Examples)</span></h2>
In statistics, <b>deciles</b> are numbers that split a dataset into ten groups of equal frequency.
The first decile is the point where 10% of all data values lie below it. The second decile is the point where 20% of all data values lie below it, and so on.
We can use the following syntax to calculate the deciles for a dataset in R:
<b>quantile(data, probs = seq(.1, .9, by = .1))
</b>
The following example shows how to use this function in practice.
<h3>Example: Calculate Deciles in R</h3>
The following code shows how to create a fake dataset with 20 values and then calculate the values for the deciles of the dataset:
<b>#create dataset
data &lt;- c(56, 58, 64, 67, 68, 73, 78, 83, 84, 88,
          89, 90, 91, 92, 93, 93, 94, 95, 97, 99)
#calculate deciles of dataset
quantile(data, probs = seq(.1, .9, by = .1))
 10%  20%  30%  40%  50%  60%  70%  80%  90% 
63.4 67.8 76.5 83.6 88.5 90.4 92.3 93.2 95.2 
</b>
The way to interpret the deciles is as follows:
10% of all data values lie below <b>63.4</b>
20% of all data values lie below <b>67.8</b>.
30% of all data values lie below <b>76.5</b>.
40% of all data values lie below <b>83.6</b>.
50% of all data values lie below <b>88.5</b>.
60% of all data values lie below <b>90.4</b>.
70% of all data values lie below <b>92.3</b>.
80% of all data values lie below <b>93.2</b>.
90% of all data values lie below <b>95.2</b>.
It’s worth noting that the value at the 50th percentile is equal to the median value of the dataset.
<h3>Example: Place Values into Deciles in R</h3>
To place each data value into a decile, we can use the <b>ntile(x, ngroups)</b> function from the  dplyr  package in R.
Here’s how to use this function for the dataset we created in the previous example:
<b>library(dplyr) 
#create dataset
data &lt;- data.frame(values=c(56, 58, 64, 67, 68, 73, 78, 83, 84, 88,            89, 90, 91, 92, 93, 93, 94, 95, 97, 99))
#place each value into a decile
data$decile &lt;- ntile(data, 10)
#view data
data
   values decile
1      56      1
2      58      1
3      64      2
4      67      2
5      68      3
6      73      3
7      78      4
8      83      4
9      84      5
10     88      5
11     89      6
12     90      6
13     91      7
14     92      7
15     93      8
16     93      8
17     94      9
18     95      9
19     97     10
20     99     10
</b>
The way to interpret the output is as follows:
The data value 56 falls between the percentile 0% and 10%, thus it falls in the first decile.
The data value 58 falls between the percentile 0% and 10%, thus it falls in the first decile.
The data value 64 falls between the percentile 10% and 20%, thus it falls in the second decile.
The data value 67 falls between the percentile 10% and 20%, thus it falls in the second decile.
The data value 68 falls between the percentile 20% and 30%, thus it falls in the third decile.
And so on.
<h2><span class="orange">Decision Rule Calculator</span></h2>
In  hypothesis testing , we want to know whether we should reject or fail to reject some statistical hypothesis.
To make this decision, we compare the p-value of the test statistic to a significance level we have chosen to use for the test. If the p-value is less than the significance level, we reject the null hypothesis. Otherwise, we fail to reject the null hypothesis.
This calculator tells you whether you should reject or fail to reject a null hypothesis based on the value of the test statistic, the format of the test (one-tailed or two-tailed), and the significance level you have chosen to use.
<b>One-tailed or two-tailed hypothesis?</b>
<label for="one_tailed">One-tailed</label>
<input type="radio" id="one_tailed" name="tails"><label for="two_tailed">Two-tailed</label>
<input type="radio" id="two_tailed" name="tails" checked>
<b>Significance level</b>
<label for="one">0.01</label>
<input type="radio" id="one" name="sig" value="0.01"><label for="five">0.05</label>
<input type="radio" id="five" name="sig" value="0.05" checked><label for="ten">0.10</label>
<input type="radio" id="ten" name="sig" value="0.10">
<b>Z-statistic or t-statistic?</b>
<label for="z">Z-statistic</label>
<input type="radio" id="z" name="test_stats" onclick="dataOption()" checked><label for="t">t-statistic</label>
<input type="radio" id="t" name="test_stats" onclick="dataOption()">
<label for="dof"><b>Degrees of freedom</b></label>
<input type="number" id="dof" value="17">
<label for="stat"><b>Value of the test statistic</b></label>
<input type="number" id="stat" value="1.34">
<input type="button" id="button_calc" onclick="calc()" value="Find Decision Rule">
Decision Rule: <b>fail to reject the null hypothesis</b>
<b>Explanation:</b>
The p-value for a Z-statistic of 1.34 for a two-tailed test is <b>0.18025</b>. Since this p-value is greater than 0.05, we <b>fail to reject the null hypothesis</b>.
<script>
//create function that shows or hides elements based on button click
var div_table = document.getElementById('words_t');
div_table.style.display = 'none';
function dataOption() {
    if (document.getElementById('t').checked) {
div_table.style.display = 'block';
       } else {
div_table.style.display = 'none';
      }
}
function calc() {
//get test statistic
var input_test_stat = document.getElementById('stat').value*1;
var dof = document.getElementById('dof').value*1;
//get tails input
var input_tails = 1;
var output_tail = 'one-tailed';
if (document.getElementById('two_tailed').checked) {
  input_tails = 2;
  output_tail = 'two-tailed';
}
//get significance level input
var input_sig = 0;
if (document.getElementById('one').checked) {
  sig_level = 0.01;
} else if (document.getElementById('five').checked) {
  sig_level = 0.05;
} else {
  sig_level = 0.10;
}
//compute p-value
var p_output = jStat.normal.cdf(Math.abs(input_test_stat), 0, 1)*input_tails;
var input_test_name = 'Z-statistic';
if (document.getElementById('t').checked) {
  p_output = jStat.studentt.cdf(Math.abs(input_test_stat), dof)*input_tails;
  var input_test_name = 't-statistic';
}
if (p_output >= 1) {
  p_output = 2 - p_output;
}
//last correction for one-tailed
if (input_tails == 1) {
  p_output = 1 - p_output;
}
//compute decision rule
var test_output_sign = 'greater than';
var test_output_decision = 'fail to reject the null hypothesis';
if (p_output < sig_level) {
var test_output_sign = 'less than';
var test_output_decision = 'reject the null hypothesis';}
//output
document.getElementById('p_output').innerHTML = p_output.toFixed(5);
document.getElementById('test_output_name').innerHTML = input_test_name;
document.getElementById('test_output').innerHTML = input_test_stat;
document.getElementById('test_output_tail').innerHTML = output_tail;
document.getElementById('test_output_sign').innerHTML = test_output_sign;
document.getElementById('test_output_sig').innerHTML = sig_level;
document.getElementById('test_output_decision').innerHTML = test_output_decision;
document.getElementById('decision').innerHTML = test_output_decision;
}
</script>
<h2><span class="orange">Decision Tree vs. Random Forests: What’s the Difference?</span></h2>
A <b>decision tree</b> is a type of machine learning model that is used when the relationship between a set of predictor variables and a response variable is non-linear.
The basic idea behind a decision tree is to build a “tree” using a set of predictor variables that predicts the value of some response variable using decision rules.
For example, we might use the predictor variables “years played” and “average home runs” to predict the annual salary of professional baseball players.
Using this dataset , here’s what the decision tree model might look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/decision2.png">
Here’s how we would interpret this decision tree:
Players with less than 4.5 years played have a predicted salary of <b>$225.8k</b>.
Players with greater than or equal to 4.5 years played and less than 16.5 average home runs have a predicted salary of <b>$577.6k</b>.
Players with greater than or equal to 4.5 years played and greater than or equal to 16.5 average home runs have a predicted salary of <b>$975.6k</b>.
The main advantage of a decision tree is that it can be fit to a dataset quickly and the final model can be neatly visualized and interpreted using a “tree” diagram like the one above.
The main disadvantage is that a decision tree is prone to  overfitting  a training dataset, which means it’s likely to perform poorly on unseen data. It can also be heavily influenced by outliers in the dataset.
An extension of the decision tree is a model known as a <b>random forest</b>, which is essentially a collection of decision trees.
Here are the steps we use to build a random forest model:
<b>1.</b> Take bootstrapped samples from the original dataset.
<b>2.</b> For each bootstrapped sample, build a decision tree using a random subset of the predictor variables.
<b>3.</b> Average the predictions of each tree to come up with a final model.
The benefit of random forests is that they tend to perform much better than decision trees on unseen data and they’re less prone to outliers.
The downside of random forests is that there’s no way to visualize the final model and they can take a long time to build if you don’t have enough computational power or if the dataset you’re working with is extremely large.
<h3>Pros & Cons: Decision Trees vs. Random Forests</h3>
The following table summarizes the pros and cons of decision trees vs. random forests:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/decision1.png">
Here’s a brief explanation of each row in the table:
<b>1. Interpretability</b>
Decision trees are easy to interpret because we can create a tree diagram to visualize and understand the final model.
Conversely, we can’t visualize a random forest and it can often be difficulty to understand how the final random forest model makes decisions.
<b>2. Accuracy</b>
Since decision trees are likely to overfit a training dataset, they tend to perform less than stellar on unseen datasets.
Conversely, random forests tend to be highly accurate on unseen datasets because they avoid overfitting training datasets.
<b>3. Overfitting</b>
As mentioned earlier, decision trees often overfit training data – this means they’re likely to fit the “noise” in a dataset as opposed to the true underlaying pattern.
Conversely, because random forests only use some predictor variables to build each individual decision tree, the final trees tend to be decorrelated which means random forest models are unlikely to overfit datasets.
<b>4. Outliers</b>
Decision trees are highly prone to being affected by outliers.
Conversely, since a random forest model builds many individual decision trees and then takes the average of those trees predictions, it’s much less likely to be affected by outliers.
<b>5. Computation</b>
Decision trees can be fit to datasets quickly. 
Conversely, random forests are much more computationally intensive and can take a long time to build depending on the size of the dataset.
<h3>When to Use Decision Trees vs. Random Forests</h3>
As a rule of thumb:
You should use a <b>decision tree</b> if you want to build a non-linear model quickly and you want to be able to easily interpret how the model is making decisions.
However, you should a <b>random forest</b> if you have plenty of computational ability and you want to build a model that is likely to be highly accurate without worrying about how to interpret the model.
In the real-world, machine learning engineers and data scientists often use random forests because they’re highly accurate and modern-day computers and systems can often handle large datasets that couldn’t previously be handled in the past.
 An Introduction to Decision Trees 
 An Introduction to Random Forests 
The following tutorials explain how to fit decision trees and random forests in R:
 How to Fit Decision Trees in R 
 How to Build Random Forests in R 
<h2><span class="orange">How to Delete Data Frames in R (With Examples)</span></h2>
The R programming language offers two helpful functions for viewing and removing objects within an R workspace:
<b>ls(): </b>List all objects in current workspace
<b>rm(): </b>Remove one or more objects from current workspace
This tutorial explains how to use the <b>rm()</b> function to delete data frames in R and the <b>ls()</b> function to confirm that a data frame has been deleted.
<h3>Delete a Single Data Frame</h3>
The following code shows how to delete a single data frame from your current R workspace:
<b>#list all objects in current R workspace
ls()
[1] "df1" "df2" "df3" "x"
#remove df1
rm(df1)
#list all objects in workspace
ls()
[1] "df2" "df3" "x"  
</b>
<h3>Delete Multiple Data Frames</h3>
The following code shows how to delete multiple data frames from your current R workspace:
<b>#list all objects in current R workspace
ls()
[1] "df1" "df2" "df3" "x"
#remove df1 and df2
rm("df1", "df2")
#list all objects in workspace
ls()
[1] "df3" "x"  </b>
<h3>Delete All Data Frames</h3>
The following code shows how to delete all objects that are of type “data.frame” in your current R workspace:
<b>#list all objects in current R workspace
ls()
[1] "df1" "df2" "df3" "x"
#remove all objects of type "data.frame"
rm(list=ls(all=TRUE)[sapply(mget(ls(all=TRUE)), class) == "data.frame"])
#list all objects in workspace
ls()
[1] "x" </b>
You can also use the  grepl() function  to delete all objects in the workspace that contain the phrase “df”:
<b>#list all objects in current R workspace
ls()
[1] "df1" "df2" "df3" "x"
#remove all objects that contain "df"
rm(list = ls()[grepl("df", ls())])
#list all objects in workspace
ls()
[1] "x" </b>
<h2><span class="orange">How to Delete Multiple Columns in R (With Examples)</span></h2>
Often you may want to delete multiple columns at once from a data frame in R.
The easiest way to do this is with the following syntax:
<b>df[ , c('column_name1', 'column_name2')] &lt;- list(NULL)
</b>
For example, the following syntax shows how to delete columns 2 and 3 from a given data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 2, 9, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 7))
#delete columns 2 and 3 from data frame
df[ , c('var2', 'var3')] &lt;- list(NULL)
#view data frame
df
  var1 var4
1    1    1
2    3    1
3    2    2
4    9    8
5    5    7
</b>
We can also delete columns according to their index:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 2, 9, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 7))
#delete columns in position 2 and 3
df[ , c(2, 3)] &lt;- list(NULL)
#view data frame
df
  var1 var4
1    1    1
2    3    1
3    2    2
4    9    8
5    5    7</b>
And we can use the following syntax to delete all columns in a range:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 2, 9, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 7))
#delete columns in range 1 through 3
df[ , 1:3] &lt;- list(NULL)
#view data frame
df
  var4
1    1
2    1
3    2
4    8
5    7</b>
In general it’s recommended to delete columns by their name rather than their position simply because if you add or reorder columns then the positions could change.
By using column names, you ensure that you delete the correct columns regardless of their position.
<h2><span class="orange">What are Density Curves? (Explanation & Examples)</span></h2>
A <b>density curve </b>is a curve on a graph that represents the distribution of values in a dataset. It’s useful for three reasons:
<b>1.</b> A density curve gives us a good idea of the “shape” of a distribution, including whether or not a distribution has one or more “peaks” of frequently occurring values and whether or not the distribution is skewed to the left or the right.
<b>2.</b> A density curve lets us visually see where  the mean and the median  of a distribution are located.
<b>3.</b> A density curve lets us visually see what percentage of observations in a dataset fall between different values.
The most famous density curve is the bell-shaped curve that represents the  normal distribution .
To gain a better understanding of density curves, consider the following example.
<h3>Example: Creating & Interpreting a Density Curve</h3>
Suppose we have the following dataset that shows the height of 20 different plants (in inches) in a certain field:
<b>4, 5, 5, 6, 6, 6, 6, 7, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 2, 2</b>
If we created a simple histogram to display the relative frequencies of each value, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/density1.png">
The x-axis shows the data value and the y-axis shows the relative frequency (e.g. the value “7” occurs 5 times out of 20 total values in the dataset, thus it has a relative frequency of 25% or <b>0.25</b>.
And if we created a <b>density curve </b>to capture the “shape” of this distribution, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/density2.png">
The curve is highest near the center of the distribution because that’s where the most values are located. It’s also lowest near the ends of the distribution because fewer plants take on those values (e.g. a height of 4 inches or 10 inches).
<h3>How to Interpret Density Curves</h3>
Density curves come in all shapes and sizes and they allow us to gain a quick visual understanding of the distribution of values in a given dataset. In particular, they’re useful for helping us visualize:
<b>1. Skewness</b>
Skewness is a way to describe the symmetry of a distribution. Density curves allow us to quickly see whether or not a graph is left skewed, right skewed, or has no skew:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/density3.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/density4.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/density5.png">
<b>2. The location of the mean & median</b>
Depending on the skewness of a density curve, we can quickly know whether the mean or median is larger in a given distribution. In particular:
If a density curve is <b>left skewed</b>, then the mean is <em>less </em>than the median.
If a density curve is <b>right skewed</b>, then the mean is <em>greater </em>than the median.
If a density curve has <b>no skew</b>, then the mean is <em>equal </em>to the median.
<b>3. Number of Peaks</b>
Density curves also allow us to quickly see how many “peaks” there are in a given distribution. In each of the examples above, the distributions only had one peak, so we would describe those distributions as <b>unimodal</b>. 
However, some distributions can have two peaks which we call  bimodal distributions . And in rare cases we can also have <b>multimodal distributions </b>that have two or more peaks.
By simply creating a density curve for a given dataset, we can quickly see how many peaks are in the distribution.
<h3>Properties of Density Curves</h3>
Density curves have the following properties:
The area under the curve always adds up to 100%.
The curve will never dip below the x-axis.
Keep these two facts in mind when you create or interpret density curves for different distributions.
<h2><span class="orange">How to Describe the Shape of Histograms (With Examples)</span></h2>
A <b>histogram</b> is a type of chart that allows us to visualize the distribution of values in a dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape1.png">
The x-axis displays the values in the dataset and the y-axis shows the frequency of each value.
Depending on the values in the dataset, a histogram can take on many different shapes.
The following examples show how to describe a variety of different histograms.
<h3>1. Bell-Shaped</h3>
A histogram is bell-shaped if it resembles a “bell” curve and has one single peak in the middle of the distribution. The most common real-life example of this type of distribution is the  normal distribution .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape2.png">
<h3>2. Uniform</h3>
A histogram is described as “uniform” if every value in a dataset occurs roughly the same number of times. This type of histogram often looks like a rectangle with no clear peaks.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape3.png">
<h3>3. Bimodal</h3>
A histogram is described as “bimodal” if it has two distinct peaks. We often say that this type of distribution has multiple modes – that is, multiple values occur most frequently in the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape4.png">
<b>Related:</b>  What is a Bimodal Distribution? 
<h3>4. Multimodal</h3>
A histogram is described as “multimodal” if it has more than two distinct peaks.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape5.png">
<b>Related:</b>  What is a Multimodal Distribution? 
<h3>5. Left Skewed</h3>
A histogram is left skewed if it has a “tail” on the left side of the distribution. Sometimes this type of distribution is also called “negatively” skewed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape6.png">
<b>Related:</b>  5 Examples of Negatively Skewed Distributions 
<h3>6. Right Skewed</h3>
A histogram is right skewed if it has a “tail” on the right side of the distribution. Sometimes this type of distribution is also called “positively” skewed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape7.png">
<b>Related:</b>  5 Examples of Positively Skewed Distributions 
<h3>7. Random</h3>
The shape of a distribution can be described as “random” if there is no clear pattern in the data at all.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/hist_shape8.png">
<h2><span class="orange">Descriptive vs. Inferential Statistics: What’s the Difference?</span></h2>
There are two main branches in the field of statistics:
<b>Descriptive Statistics</b>
<b>Inferential Statistics</b>
This tutorial explains the difference between the two branches and why each one is useful in certain situations.
<h2>Descriptive Statistics</h2>
In a nutshell, <b>descriptive statistics </b>aims to <em>describe </em>a chunk of  raw data  using summary statistics, graphs, and tables.
Descriptive statistics are useful because they allow you to understand a group of data much more quickly and easily compared to just staring at rows and rows of raw data values.
For example, suppose we have a set of raw data that shows the test scores of 1,000 students at a particular school. We might be interested in the average test score along with the distribution of test scores.
Using descriptive statistics, we could find the average score and create a graph that helps us visualize the distribution of scores.
This allows us to understand the test scores of the students much more easily compared to just staring at the raw data.
<h3>Common Forms of Descriptive Statistics</h3>
There are three common forms of descriptive statistics:
<b>1. Summary statistics. </b>These are statistics that <em>summarize </em>the data using a single number. There are two popular types of summary statistics:
<b> Measures of central tendency :</b> these numbers describe where the center of a dataset is located. Examples include the <em>mean </em>and the <em>median</em>.
<b> Measures of dispersion :</b> these numbers describe how spread out the values are in the dataset. Examples include the <em>range</em>, <em>interquartile range</em>, <em>standard deviation</em>, and <em>variance</em>.
<b>2. Graphs</b>. Graphs help us visualize data. Common types of graphs used to visualize data include  boxplots ,  histograms ,  stem-and-leaf plots , and  scatterplots .
<b>3. Tables</b>. Tables can help us understand how data is distributed. One common type of table is a <em>frequency table</em>, which tells us how many data values fall within certain ranges. 
<h3>Example of Using Descriptive Statistics</h3>
The following example illustrates how we might use descriptive statistics in the real world.
Suppose 1,000 students at a certain school all take the same test. We are interested in understanding the distribution of test scores, so we use the following descriptive statistics:
<b>1. Summary Statistics</b>
<b>Mean: 82.13</b>. This tells us that the average test score among all 1,000 students is 82.13.
<b>Median: 84. </b>This tells us that half of all students scored higher than 84 and half scored lower than 84.
<b>Max: 100. Min: 45. </b>This tells us the maximum score that any student obtained was 100 and the minimum score was 45. The <em>range</em> – which tells us the difference between the max and the min – is 55.
<b>2. Graphs</b>
To visualize the distribution of test scores, we can create a histogram – a type of chart that uses rectangular bars to represent frequencies.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/descriptive_vs_inferential1-2.jpg"546">
Based on this histogram, we can see that the distribution of test scores is roughly bell-shaped. Most of the students scored between 70 and 90, while very few scored above 95 and fewer still scored below 50.
<b>3. Tables</b>
Another easy way to gain an understanding of the distribution of scores is to create a frequency table. For example, the following frequency table shows what percentage of students scored between various ranges:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/descriptive_vs_inferential1-3.jpg"175">
We can see that just 4% of the total students scored above a 95. We can also see that (12% + 9% + 4% = ) 25% of all students scored an 85 or higher.
A frequency table is particularly helpful if we want to know what percentage of the data values fall above or below a certain value. For example, suppose the school considers an “acceptable” test score to be any score above a 75.
By looking at the frequency table, we can easily see that (20% + 22% + 12% + 9% + 4% = ) 67% of the students received an acceptable test score.
<h2>Inferential Statistics</h2>
In a nutshell, <b>inferential statistics </b>uses a small sample of data to draw <em>inferences </em>about the larger population that the sample came from.
For example, we might be interested in understanding the political preferences of millions of people in a country.
However, it would take too long and be too expensive to actually survey every individual in the country. Thus, we would instead take a smaller survey of say, 1,000 Americans, and use the results of the survey to draw inferences about the population as a whole.
This is the whole premise behind inferential statistics – we want to answer some question about a population, so we obtain data for a small sample of that population and use the data from the sample to draw inferences about the population.
<h3>The Importance of a Representative Sample</h3>
In order to be confident in our ability to use a sample to draw inferences about a population, we need to make sure that we have a <b> representative sample  </b>– that is, a sample in which the characteristics of the individuals in the sample closely match the characteristics of the overall population.
Ideally, we want our sample to be like a “mini version” of our population. So, if we want to draw inferences on a population of students composed of 50% girls and 50% boys, our sample would not be representative if it included 90% boys and only 10% girls.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/rep_sample1.jpg"422">
If our sample is not similar to the overall population, then we cannot generalize the findings from the sample to the overall population with any confidence.
<h3>How to Obtain a Representative Sample</h3>
To maximize the chances that you obtain a representative sample, you need to focus on two things:
<b>1. Make sure you use a random sampling method.</b>
There are several different random  sampling methods  that you can use that are likely to produce a representative sample, including:
A simple random sample
A systematic random sample
A cluster random sample
A stratified random sample
Random sampling methods tend to produce representative samples because every member of the population has an equal chance of being included in the sample.
<b>2. Make sure your sample size is large enough</b>. 
Along with using an appropriate sampling method, it’s important to ensure that the sample is large enough so that you have enough data to generalize to the larger population.
To determine how large your sample should be, you have to consider the population size you’re studying, the confidence level you’d like to use, and the margin of error you consider to be acceptable. 
Fortunately, you can use online calculators to plug in these values and see how large your sample needs to be.
<h3>Common Forms of Inferential Statistics</h3>
There are three common forms of inferential statistics:
<b>1. Hypothesis Tests.</b>
Often we’re interested in answering questions about a population such as:
Is the percentage of people in Ohio in support of candidate A higher than 50%?
Is the mean height of a certain plant equal to 14 inches?
Is there a difference between the mean height of students at School A compared to School B?
To answer these questions we can perform a  hypothesis test , which allows us to use data from a sample to draw conclusions about populations.
<b>2. Confidence Intervals</b>. 
Sometimes we’re interested in estimating some value for a population. For example, we might be interested in the mean height of a certain plant species in Australia.
Instead of going around and measuring every single plant in the country, we might collect a small sample of plants and measure each one. Then, we can use the mean height of the plants in the sample to estimate the mean height for the population.
However, our sample is unlikely to provide a perfect estimate for the population. Fortunately, we can account for this uncertainty by creating a  confidence interval , which provides a range of values that we’re confident the true population parameter falls in.
For example, we might produce a 95% confidence interval of [13.2, 14.8], which says we’re 95% confident that the true mean height of this plant species is between 13.2 inches and 14.8 inches.
<b>3. Regression</b>.
Sometimes we’re interested in understanding the relationship between two variables in a population. 
For example, suppose we want to know if <em>hours spent studying per week </em>is related to <em>test scores</em>. To answer this question, we could perform a technique known as  regression analysis .
So, we may observe the number of hours studied along with the test scores for 100 students and perform a regression analysis to see if there is a significant relationship between the two variables.
If  the p-value of the regression turns out to be significant , then we can conclude that there is a significant relationship between these two variables in the overall population of students.
<h3>The Difference Between Descriptive and Inferential Statistics</h3>
In summary, the difference between descriptive and inferential statistics can be described as follows:
<b>Descriptive statistics</b> use summary statistics, graphs, and tables to <em>describe </em>a data set.
This is useful for helping us gain a quick and easy understanding of a data set without pouring over all of the individual data values.
<b>Inferential statistics </b>use samples to draw <em>inferences </em>about larger populations.
Depending on the question you want to answer about a population, you may decide to use one or more of the following methods: hypothesis tests, confidence intervals, and regression analysis.
If you do choose to use one of these methods, keep in mind that  your sample needs to be representative of your population , or the conclusions you draw will be unreliable.
<h2><span class="orange">How to Calculate Descriptive Statistics in Google Sheets</span></h2>
<b>Descriptive statistics</b> are values that describe a dataset. They help us gain an understanding of where the center of the dataset is located along with how spread out the values are in the dataset.
The following example shows how to calculate the following descriptive statistics for a dataset in Google Sheets:
<b>Mean</b> (the average value)
<b>Median</b> (the middle value)
<b>Mode</b> (the most frequently occurring value)
<b>Range</b> (the difference between minimum and maximum value)
<b>Standard deviation</b> (the spread of the values)
<b>Sample size</b> (total number of observations)
<h3>Example: Calculating Descriptive Statistics in Google Sheets</h3>
Suppose we have the following dataset with 20 values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/descript1.png">
The following screenshot shows how to calculate various descriptive statistics for this dataset, including the formulas used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/descript2.png">
We can use the following descriptive statistics to get an idea of where the <b>center</b> of the dataset is located:
Mean: 16.3
Median: 14.5
Mode: 13
We can use the following descriptive statistics to get an idea of <b>how spread out</b> the values are in the dataset:
Range: 31
Standard Deviation: 9.0618
Lastly, we can use the following descriptive statistic to understand how many <b>total observations</b> are in the dataset:
 Sample Size: 20
Using these six descriptive statistics, we can gain a pretty good understanding of the distribution of values in this dataset.
<h2><span class="orange">How to Calculate Descriptive Statistics in R (With Example)</span></h2>
<b>Descriptive statistics</b> are values that describe a dataset.
They help us gain an understanding of where  the center  of the dataset is located along with how  spread out  the values are in the dataset.
There are two functions we can use to calculate descriptive statistics in R:
<b>Method 1: Use summary() Function</b>
<b>summary(my_data)</b>
The <b>summary()</b> function calculates the following values for each variable in a data frame in R:
Minimum
1st Quartile
Median
Mean
3rd Quartile
Maximum
<b>Method 2: Use sapply() Function</b>
<b>sapply(my_data, sd, na.rm=TRUE)
</b>
The <b>sapply()</b> function can be used to calculate descriptive statistics other than the ones calculated by the <b>summary()</b> function for each variable in a data frame.
For example, the<b> sapply()</b> function above calculates the standard deviation of each variable in a data frame.
The following example shows how to use both of these functions to calculate descriptive statistics for variables in a data frame in R.
<h2>Example: Calculating Descriptive Statistics in R</h2>
Suppose we have the following data frame in R that contains three variables:
<b>#create data frame
df &lt;- data.frame(x=c(1, 4, 4, 5, 6, 7, 10, 12), y=c(2, 2, 3, 3, 4, 5, 11, 11), z=c(8, 9, 9, 9, 10, 13, 15, 17))
#view data frame
df
   x  y  z
1  1  2  8
2  4  2  9
3  4  3  9
4  5  3  9
5  6  4 10
6  7  5 13
7 10 11 15
8 12 11 17</b>
We can use the<b> summary()</b> function to calculate a variety of descriptive statistics for each variable:
<b>#calculate descriptive statistics for each variable
summary(df)
       x                y                z        
 Min.   : 1.000   Min.   : 2.000   Min.   : 8.00  
 1st Qu.: 4.000   1st Qu.: 2.750   1st Qu.: 9.00  
 Median : 5.500   Median : 3.500   Median : 9.50  
 Mean   : 6.125   Mean   : 5.125   Mean   :11.25  
 3rd Qu.: 7.750   3rd Qu.: 6.500   3rd Qu.:13.50  
 Max.   :12.000   Max.   :11.000   Max.   :17.00 </b>
We can also use brackets to only calculate descriptive statistics for specific variables in the data frame:
<b>#calculate descriptive statistics for 'x' and 'z' only
summary(df[ , c('x', 'z')])
       x                z        
 Min.   : 1.000   Min.   : 8.00  
 1st Qu.: 4.000   1st Qu.: 9.00  
 Median : 5.500   Median : 9.50  
 Mean   : 6.125   Mean   :11.25  
 3rd Qu.: 7.750   3rd Qu.:13.50  
 Max.   :12.000   Max.   :17.00 
</b>
We can also use the <b>sapply()</b> function to calculate specific descriptive statistics for each variable.
For example, the following code shows how to calculate the standard deviation of each variable:
<b>#calculate standard deviation for each variable
sapply(df, sd, na.rm=TRUE)
       x        y        z 
3.522884 3.758324 3.327376 </b>
We can also use a <b>function()</b> within <b>sapply()</b> to calculate descriptive statistics.
For example, the following code shows how to calculate  the range  for each variable:
<b>#calculate range for each variable
sapply(df, function(df) max(df, na.rm=TRUE)-min(df, na.rm=TRUE))
 x  y  z 
11  9  9
</b>
Lastly, we can create a complex function that calculates some descriptive statistic and then use this function with the <b>sapply()</b> function.
For example, the following code shows how to calculate  the mode  of each variable in the data frame:
<b>#define function that calculates mode
find_mode &lt;- function(x) {
  u &lt;- unique(x)
  tab &lt;- tabulate(match(x, u))
  u[tab == max(tab)]
}
#calculate mode for each variable
sapply(df, find_mode)
$x
[1] 4
$y
[1]  2  3 11
$z
[1] 9
</b>
From the output we can see:
The mode of variable x is <b>4</b>.
The mode of variable y is <b>2</b>, <b>3</b>, and <b>11</b> (since each of these values occurred most frequently)
The mode of variable z is <b>9</b>.
By using the <b>summary()</b> and <b>sapply()</b> functions, we can calculate any descriptive statistics that we’d like for each variable in a data frame.
<h2><span class="orange">How to Calculate Descriptive Statistics for Variables in SPSS</span></h2>
The best way to understand a dataset is to calculate  descriptive statistics  for the variables within the dataset. There are three common forms of descriptive statistics:
<b>1. Summary statistics </b>– Numbers that summarize a variable using a single number. Examples include the mean, median, standard deviation, and range.
<b>2. Tables</b> – Tables can help us understand how data is distributed. One example is a frequency table, which tells us how many data values fall within certain ranges. 
<b>3. Graphs </b>– These help us visualize data. An example would be a  histogram .
This tutorial explains how to calculate descriptive statistics for variables in SPSS.
<h3>Example: Descriptive Statistics in SPSS</h3>
Suppose we have the following dataset that contains four variables for 20 students in a certain class:
Exam score
Hours spent studying
Prep exams taken
Current grade in the class
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal1.png">
Here is how to calculate descriptive statistics for each of these four variables:
<h3>Summary Statistics</h3>
To calculate summary statistics for each variable, click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Descriptives</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc1.png">
In the new window that pops up, drag each of the four variables into the box labelled Variable(s). If you’d like, you can click the <b>Options </b>button and select the specific descriptive statistics you’d like SPSS to calculate. Then click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc2.png">
Once you click <b>OK</b>, a table will appear that displays the following descriptive statistics for each variable:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc3.png">
Here is how to interpret the numbers in this table for the variable <b>score</b>:
<b>N: </b>The total number of observations. In this case there are 20.
<b>Minimum: </b>The minimum value for exam score. In this case it’s 68.
<b>Maximum: </b>The maximum value for exam score. In this case it’s 99.
<b>Mean: </b>The mean exam score. In this case it’s 82.75.
<b>Std. Deviation: </b>The standard deviation in exam scores. In this case it’s 8.985.
This table allows us to quickly understand the range of each variable (using the minimum and maximum), the central location of each variable (using the mean), and how spread out the values are for each variable (using the standard deviation).
<h3>Tables</h3>
To produce a frequency table for each variable, click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Frequencies</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc4.png">
In the new window that pops up, drag each variable into the box labelled Variable(s). Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc5.png">
A frequency table for each variable will appear. For example, here’s the one for the variable <b>hours</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc6.png">
The way to interpret the table is as follows:
The first column displays each unique value for the variable <b>hours</b>. In this case, the unique values are 1, 2, 3, 4, 5, 6, and 16.
The second column displays the frequency of each value. For example, the value 1 appears 1 time, the value 2 appear 4 times, and so on.
The third column displays the percent for each value. For example, the value 1 makes up 5% of all values in the dataset. The value 2 makes up 20% of all values in the dataset, and so on.
The last column displays the cumulative percent. For example the values 1 and 2 make up a cumulative 25% of the total dataset. The values 1, 2, and 3 make up a cumulative 60% of the dataset, and so on.
This table gives us a nice idea about the distribution of the data values for each variable.
<h3>Graphs</h3>
Graphs also help us understand the distribution of data values for each variable in a dataset. One of the most popular graphs for doing so is a histogram.
To create a histogram for a given variable in a dataset, click the <b>Graphs </b>tab, then <b>Chart Builder</b>.
In the new window that pops up, choose <b>Histogram </b>from the “Choose from” panel. Then drag the first histogram option into the main editing window. Then drag your variable of interest onto the x-axis. We’ll use <b>score </b>for this example. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc7.png">
Once you click <b>OK</b>, a histogram will appear that displays the distribution of values for the variable <b>score</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/desc8.png">
From the histogram we can see that the range of exam scores varies between 65 and 100, with most of the scores being between 70 and 90.
We can repeat this process to create a histogram for each of the other variables in the dataset as well.
<h2><span class="orange">How to Determine Equal or Unequal Variance in t-tests</span></h2>
When we want to compare the means of two independent groups, we can choose between two different tests:
<b> Student’s t-test:  </b>Assumes that both groups of data are sampled from populations that follow a normal distribution and that both populations have the same variance.
<b> Welch’s t-test:  </b>Assumes that both groups of data are sampled from populations that follow a normal distribution, but <b>it does not assume that those two populations have the same variance</b>.
So, if the two samples do not have equal variance then it’s best to use the Welch’s t-test.
<b>But how do we determine if the two samples have equal variance?</b>
There are two ways to do so:
<b>1. Use the Variance Rule of Thumb.</b>
As a rule of thumb, if the ratio of the larger variance to the smaller variance is less than 4 then we can assume the variances are approximately equal and use the Student’s t-test.
For example, suppose we have the following two samples:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/equalVar1.png">
Sample 1 has a variance of 24.86 and sample 2 has a variance of 15.76.
The ratio of the larger sample variance to the smaller sample variance would be calculated as:
Ratio = 24.86 / 15.76 = 1.577
Since this ratio is less than 4, we could assume that the variances between the two groups are approximately equal.
Thus, we could proceed to perform Student’s t-test to determine if the two groups have the same mean.
<b>2. Perform an F-test.</b>
An <b>F-test</b> is a formal statistical test that uses the following null and alternative hypotheses:
H<sub>0</sub>: The samples have equal variances.
H<sub>A</sub>: The samples do not have equal variances.
The test statistic is calculated as follows:
F = s<sub>1</sub><sup>2</sup> / s<sub>2</sub><sup>2</sup>
where s<sub>1</sub><sup>2 </sup>and s<sub>2</sub><sup>2 </sup>are the sample variances. 
If the p-value that corresponds to the test statistic is less than some significance level (like 0.05), then we have sufficient evidence to say that the samples do not have equal variances.
Once again suppose we have the following two samples:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/equalVar1.png">
To perform an F-test on these two samples, we can calculate the F test statistic as:
F = s<sub>1</sub><sup>2</sup> / s<sub>2</sub><sup>2</sup>
F = 24.86 / 15.76
F = 1.577
According to the  F-Distribution Calculator , an F-value of 1.577 with numerator df = n<sub>1</sub>-1 = 12 and denominator df = n<sub>2</sub>-1 = 12 has a corresponding p-value of 0.22079.
Since this p-value is not less than .05, we fail to reject the null hypothesis. In other words, we can assume the sample variances are equal.
Thus, we could proceed to perform Student’s t-test to determine if the two groups have the same mean.
 Two Sample t-test in Excel 
 Two Sample t-test on a TI-84 Calculator 
 Two Sample t-test in SPSS 
 Two Sample t-Test in Python 
 Two Sample t-test Calculator 
And if you decide to perform the Welch’s t-test, you can use the following tutorials as references:
 Welch’s t-test in Excel 
 Welch’s t-test in R 
 Welch’s t-test in Python 
 Welch’s t-test Calculator 
<h2><span class="orange">How to Detrend Data (With Examples)</span></h2>
To “detrend” time series data means to remove an underlying trend in the data. The main reason we would want to do this is to more easily see subtrends in the data that are seasonal or cyclical.
For example, consider the following time series data that represents the total sales for some company during 20 consecutive periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend1.png">
Clearly the sales is trending upwards over time, but there also appears to be a cyclical or seasonal trend in the data, which can be seen by the tiny “hills” that occur over time.
To gain a better view of this cyclical trend, we can detrend the data. In this case, this would involve removing the overall upward trend over time so that the resulting data represents just the cyclical trend.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend2.png">
There are two common methods used to detrend time series data:
<b>1. Detrend by Differencing</b>
<b>2. Detrend by Model Fitting</b>
This tutorial provides a brief explanation of each method.
<h3>Method 1: Detrend by Differencing</h3>
One way to detrend time series data is to simply create a new dataset where each observation is the difference between itself and the previous observation.
For example, the following image shows how to use differencing to detrend a data series.
To obtain the first value of the detrended time series data, we calculate 13 – 8 = 5. Then to obtain the next value we calculate 18-13 = 5, and so on.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend3.png">
The following plot shows the original time series data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend1.png">
And this plot shows the detrended data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend2.png">
Notice how it’s much easier to see the seasonal trend in the time series data in this plot because the overall upward trend has been removed.
<h3>Method 2: Detrend by Model Fitting</h3>
Another way to detrend time series data is to fit a regression model to the data and then calculate the difference between the observed values and the predicted values from the model.
For example, suppose we have the same dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend4.png">
If we fit a  simple linear regression model  to the data, we can obtain a predicted value for each  observation  in the dataset.
We can then find the difference between the actual value and the predicted value for each observation. These differences represent the detrended data.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend5.png">
If we create a plot of the detrended data, we can visualize the seasonal or cyclical trend in the data much more easily:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/detrend6.png">
Note that we used linear regression in this example, but it’s possible to use a more complex method like exponential regression if there is more of an exponentially increasing or decreasing trend in the data.
<h2><span class="orange">How to Use DEVSQ in Excel (With Example)</span></h2>
You can use the <b>DEVSQ</b> function in Excel to calculate the sum of squares of deviations for a given sample.
This function uses the following basic syntax:
<b>=DEVSQ(value1, value2, value3, ...)</b>
Here’s the formula that <b>DEVSQ</b> actually uses:
Sum of squares of deviations = Σ(x<sub>i</sub> – x)<sup>2</sup>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> data value
x: The sample mean
The following example shows how to use this function in practice.
<h2>Example: How to Use DEVSQ in Excel</h2>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq3.jpg"458">
We can use the following formula to calculated the sum of squares of deviations for this dataset:
<b>=DEVSQ(A2:A13)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq4.jpg"487">
The sum of squares of deviations turns out to be <b>319</b>.
We can confirm this is correct by manually calculating the sum of squares of deviations for this dataset.
<b>Note</b>: The average value of this dataset is 9.5.
Knowing this, we can simply plug in the values from the dataset into the formula for sum of squares of deviations:
Sum of squares of deviations = Σ(x<sub>i</sub> – x)<sup>2</sup>
Sum of squares of deviations = (2-9.5)<sup>2</sup> + (3-9.5)<sup>2</sup> + (5-9.5)<sup>2</sup> + (5-9.5)<sup>2</sup> + (7-9.5)<sup>2</sup> + (8-9.5)<sup>2</sup> + (9-9.5)<sup>2</sup> + (12-9.5)<sup>2</sup> + (14-9.5)<sup>2</sup> + (15-9.5)<sup>2</sup> + (16-9.5)<sup>2</sup> + (18-9.5)<sup>2</sup>
Sum of squares of deviations = 56.25 + 42.25 + 20.25 + 20.25 + 6.25 + 2.25 + 0.25 + 6.25 + 20.25 + 30.25 + 42.25 + 72.25
Sum of squares of deviations = <b>319</b>
The sum of squares of deviations turns out to be <b>319</b>.
This matches the value that we calculated using the <b>DEVSQ</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Excel:
 How to Perform a Median IF Function in Excel 
 How to Perform a Percentile IF Function in Excel 
 How to Use LARGE IF Function in Excel 
<h2><span class="orange">How to Use DEVSQ in Google Sheets (With Example)</span></h2>
You can use the <b>DEVSQ</b> function in Google Sheets to calculate the sum of squares of deviations for a given sample.
This function uses the following basic syntax:
<b>=DEVSQ(value1, value2, value3, ...)</b>
Here’s the formula that <b>DEVSQ</b> actually uses:
Sum of squares of deviations = Σ(x<sub>i</sub> – x)<sup>2</sup>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> data value
x: The sample mean
The following example shows how to use this function in practice.
<h2>Example: How to Use DEVSQ in Google Sheets</h2>
Suppose we have the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq1.jpg"479">
We can use the following formula to calculated the sum of squares of deviations for this dataset:
<b>=DEVSQ(A2:A13)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq2.jpg">
The sum of squares of deviations turns out to be <b>319</b>.
We can confirm this is correct by manually calculating the sum of squares of deviations for this dataset.
<b>Note</b>: The average value of this dataset is 9.5.
Knowing this, we can simply plug in the values from the dataset into the formula for sum of squares of deviations:
Sum of squares of deviations = Σ(x<sub>i</sub> – x)<sup>2</sup>
Sum of squares of deviations = (2-9.5)<sup>2</sup> + (3-9.5)<sup>2</sup> + (5-9.5)<sup>2</sup> + (5-9.5)<sup>2</sup> + (7-9.5)<sup>2</sup> + (8-9.5)<sup>2</sup> + (9-9.5)<sup>2</sup> + (12-9.5)<sup>2</sup> + (14-9.5)<sup>2</sup> + (15-9.5)<sup>2</sup> + (16-9.5)<sup>2</sup> + (18-9.5)<sup>2</sup>
Sum of squares of deviations = 56.25 + 42.25 + 20.25 + 20.25 + 6.25 + 2.25 + 0.25 + 6.25 + 20.25 + 30.25 + 42.25 + 72.25
Sum of squares of deviations = <b>319</b>
The sum of squares of deviations turns out to be <b>319</b>.
This matches the value that we calculated using the <b>DEVSQ</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 How to Use COUNTIF with Multiple Ranges in Google Sheets 
 How to Use SUMIF with Multiple Columns in Google Sheets 
 How to Sum Across Multiple Sheets in Google Sheets 
<h2><span class="orange">How to Calculate DFBETAS in R</span></h2>
In statistics, we often want to know how influential different  observations  are in regression models.
One way to calculate the influence of observations is by using a metric known as <b>DFBETAS</b>, which tells us the standardized effect on each coefficient of deleting each individual observation.
This metric gives us an idea of how influential each observation is on each coefficient estimate in a given regression model.
This tutorial shows a step-by-step example of how to calculate and visualize DFBETAS for each observation in a model in R.
<h3>Step 1: Build a Regression Model</h3>
First, we’ll build a  multiple linear regression model  using the built-in <b>mtcars</b> dataset in R:
<b>#fit a regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09
</b>
<h3>Step 2: Calculate DFBETAS for each Observation</h3>
Next, we’ll use the built-in <b>dfbetas()</b> function to calculate the DFBETAS values for each observation in the model:
<b>#calculate DFBETAS for each observation in the model
dfbetas &lt;- as.data.frame(dfbetas(model))
#display DFBETAS for each observation
dfbetas
      (Intercept)         disp            hp
Mazda RX4           -0.1174171253  0.030760632  1.748143e-02
Mazda RX4 Wag       -0.1174171253  0.030760632  1.748143e-02
Datsun 710          -0.1694989349  0.086630144 -3.332781e-05
Hornet 4 Drive       0.0577309674  0.078971334 -8.705488e-02
Hornet Sportabout   -0.0204333878  0.237526523 -1.366155e-01
Valiant             -0.1711908285 -0.139135639  1.829038e-01
Duster 360          -0.0312338677 -0.005356209  3.581378e-02
Merc 240D           -0.0312259577 -0.010409922  2.433256e-02
Merc 230            -0.0865872595  0.016428917  2.287867e-02
Merc 280            -0.1560683502  0.078667906 -1.911180e-02
Merc 280C           -0.2254489597  0.113639937 -2.760800e-02
Merc 450SE           0.0022844093  0.002966155 -2.855985e-02
Merc 450SL           0.0009062022  0.001176644 -1.132941e-02
Merc 450SLC          0.0041566755  0.005397169 -5.196706e-02
Cadillac Fleetwood   0.0388832216 -0.134511133  7.277283e-02
Lincoln Continental  0.0483781688 -0.121146607  5.326220e-02
Chrysler Imperial   -0.1645266331  0.236634429 -3.917771e-02
Fiat 128             0.5720358325 -0.181104179 -1.265475e-01
Honda Civic          0.3490872162 -0.053660545 -1.326422e-01
Toyota Corolla       0.7367058819 -0.268512348 -1.342384e-01
Toyota Corona       -0.2181110386  0.101336902  5.945352e-03
Dodge Challenger    -0.0270169005 -0.123610713  9.441241e-02
AMC Javelin         -0.0406785103 -0.141711468  1.074514e-01
Camaro Z28           0.0390139262  0.012846225 -5.031588e-02
Pontiac Firebird    -0.0549059340  0.574544346 -3.689584e-01
Fiat X1-9            0.0565157245 -0.017751582 -1.262221e-02
Porsche 914-2        0.0839169111 -0.028670987 -1.240452e-02
Lotus Europa         0.3444562478 -0.402678927  2.135224e-01
Ford Pantera L      -0.1598854695 -0.094184733  2.320845e-01
Ferrari Dino        -0.0343997122  0.248642444 -2.344154e-01
Maserati Bora       -0.3436265545 -0.511285637  7.319066e-01
Volvo 142E          -0.1784974091  0.132692956 -4.433915e-02
</b>
For each observation, we can see the difference in the coefficient estimate for the intercept, the variable <em>disp</em>, and the variable <em>hp</em> that occurs when we delete that particular observation.
Typically we consider an observation to be highly influential on the estimate of a given coefficient if it has a DBETAS value greater than a threshold of  2/√n where <em>n</em> is the number of observations.
In this example, the threshold would be <b>0.3535534</b>:
<b>#find number of observations
n &lt;- nrow(mtcars)
#calculate DFBETAS threshold value
thresh &lt;- 2/sqrt(n)
thresh
[1] 0.3535534
</b>
Step 3: Visualize the DFBETAS</b>
Lastly, we can create plots to visualize the DFBETAS value for each observation and for each predictor in the model:
<b>#specify 2 rows and 1 column in plotting region
par(mfrow=c(2,1))
#plot DFBETAS for <em>disp</em> with threshold lines
plot(dfbetas$disp, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
#plot DFBETAS for <em>hp</em> with threshold lines 
plot(dfbetas$hp, type='h')
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dfbetas1.png">
In each plot, the x-axis displays the index of each observation in the dataset and the y-value displays the corresponding DFBETAS for each observation and each predictor.
From the first plot we can see that three observations exceed the absolute value of the threshold of <b>0.3535534 </b>and in the second plot we can see that two observations exceed the absolute value of the threshold.
We may choose to investigate these observations more closely to determine if they’re overly influential in estimating the coefficients in the model.
<h2><span class="orange">How to Calculate DFFITS in R</span></h2>
In statistics, we often want to know how influential different  observations  are in regression models.
One way to calculate the influence of observations is by using a metric known as <b>DFFITS</b>, which stands for “difference in fits.”
This metric tells us how much the predictions made by a regression model change when we leave out an individual observation.
This tutorial shows a step-by-step example of how to calculate and visualize DFFITS for each observation in a model in R.
<h3>Step 1: Build a Regression Model</h3>
First, we’ll build a  multiple linear regression model  using the built-in <b>mtcars</b> dataset in R:
<b>#load the dataset
data(mtcars)
#fit a regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09
</b>
<h3>Step 2: Calculate DFFITS for each Observation</h3>
Next, we’ll use the built-in <b>dffits()</b> function to calculate the DFFITS value for each observation in the model:
<b>#calculate DFFITS for each observation in the model
dffits &lt;- as.data.frame(dffits(model))
#display DFFITS for each observation
dffits
    dffits(model)
Mazda RX4             -0.14633456
Mazda RX4 Wag         -0.14633456
Datsun 710            -0.19956440
Hornet 4 Drive         0.11540062
Hornet Sportabout      0.32140303
Valiant               -0.26586716
Duster 360             0.06282342
Merc 240D             -0.03521572
Merc 230              -0.09780612
Merc 280              -0.22680622
Merc 280C             -0.32763355
Merc 450SE            -0.09682952
Merc 450SL            -0.03841129
Merc 450SLC           -0.17618948
Cadillac Fleetwood    -0.15860270
Lincoln Continental   -0.15567627
Chrysler Imperial      0.39098449
Fiat 128               0.60265798
Honda Civic            0.35544919
Toyota Corolla         0.78230167
Toyota Corona         -0.25804885
Dodge Challenger      -0.16674639
AMC Javelin           -0.20965432
Camaro Z28            -0.08062828
Pontiac Firebird       0.67858692
Fiat X1-9              0.05951528
Porsche 914-2          0.09453310
Lotus Europa           0.55650363
Ford Pantera L         0.31169050
Ferrari Dino          -0.29539098
Maserati Bora          0.76464932
Volvo 142E            -0.24266054
</b>
Typically we take a closer look at observations that have DFFITS values greater than a threshold of  2√p/n where:
<b>p:</b> Number of predictor variables used in the model
<b>n:</b> Number of observations used in the model
In this example, the threshold would be <b>0.5</b>:
<b>#find number of predictors in model
p &lt;- length(model$coefficients)-1
#find number of observations
n &lt;- nrow(mtcars)
#calculate DFFITS threshold value
thresh &lt;- 2*sqrt(p/n)
thresh
[1] 0.5
</b>
We can sort the observations based on their DFFITS values to see if any of them exceed the threshold:
<b>#sort observations by DFFITS, descending
dffits[order(-dffits['dffits(model)']), ]
 [1]  0.78230167  0.76464932  0.67858692  0.60265798  0.55650363  0.39098449
 [7]  0.35544919  0.32140303  0.31169050  0.11540062  0.09453310  0.06282342
[13]  0.05951528 -0.03521572 -0.03841129 -0.08062828 -0.09682952 -0.09780612
[19] -0.14633456 -0.14633456 -0.15567627 -0.15860270 -0.16674639 -0.17618948
[25] -0.19956440 -0.20965432 -0.22680622 -0.24266054 -0.25804885 -0.26586716
[31] -0.29539098 -0.32763355
</b>
We can see that the first five observations have a DFFITS value greater than 0.5, which means we may want to investigate these observations closer to determine if they’re highly influential in the model.
<h3>Step 3: Visualize the DFFITS for each Observation</h3>
Lastly, we can create a quick plot to visualize the DFFITS for each observation:
<b>#plot DFFITS values for each observation
plot(dffits(model), type = 'h')
#add horizontal lines at absolute values for threshold
abline(h = thresh, lty = 2)
abline(h = -thresh, lty = 2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dffits1.png">
The x-axis displays the index of each observation in the dataset and the y-value displays the corresponding DFFITS value for each observation.
<h2><span class="orange">A Guide to dgeom, pgeom, qgeom, and rgeom in R</span></h2>
This tutorial explains how to work with the  geometric distribution  in R using the following functions
<b>dgeom</b>: returns the value of the geometric probability density function.
<b>pgeom</b>: returns the value of the geometric cumulative density function.
<b>qgeom</b>: returns the value of the inverse geometric cumulative density function.
<b>rgeom</b>: generates a vector of geometric distributed random variables.
Here are some examples of cases where you might use each of these functions.
<h2>dgeom</h2>
The <b>dgeom </b>function finds the probability of experiencing a certain amount of failures before experiencing the first success in a series of Bernoulli trials, using the following syntax:
<b>dgeom(x, prob) </b>
where:
<b>x: </b>number of failures before first success
<b>prob: </b>probability of success on a given trial
Here’s an example of when you might use this function in practice:
<b>A researcher is waiting outside of a library to ask people if they support a certain law. The probability that a given person supports the law is p = 0.2. What is the probability that the fourth person the researcher talks to is the first person to support the law?</b>
<b>dgeom(x=3, prob=.2)
#0.1024
</b>
The probability that the researchers experiences 3 “failures” before the first success is <b>0.1024</b>.
<h2>pgeom</h2>
The <b>pgeom </b>function finds the probability of experiencing a certain amount of failures or less before experiencing the first success in a series of Bernoulli trials, using the following syntax:
<b>pgeom(q, prob) </b>
where:
<b>q: </b>number of failures before first success
<b>prob: </b>probability of success on a given trial
Here’s are a couple examples of when you might use this function in practice:
<b>A researcher is waiting outside of a library to ask people if they support a certain law. The probability that a given person supports the law is p = 0.2. What is the probability that the researcher will have to talk to 3 or less people to find someone who supports the law?</b>
<b>pgeom(q=3, prob=.2)
#0.5904</b>
The probability that the researcher will have to talk to 3 or less people to find someone who supports the law is <b>0.5904</b>.
<b>A researcher is waiting outside of a library to ask people if they support a certain law. The probability that a given person supports the law is p = 0.2. What is the probability that the researcher will have to talk to more than 5 people to find someone who supports the law?</b>
<b>1 - pgeom(q=5, prob=.2)
#0.262144</b>
The probability that the researcher will have to talk to more than 5 people to find someone who supports the law is <b>0.262144</b>.
<h2>qgeom</h2>
The <b>qgeom </b>function finds the number of failures that corresponds to a certain percentile, using the following syntax:
<b>qgeom(p, prob) </b>
where:
<b>p: </b>percentile
<b>prob: </b>probability of success on a given trial
Here’s an example of when you might use this function in practice:
<b>A researcher is waiting outside of a library to ask people if they support a certain law. The probability that a given person supports the law is p = 0.2. We will consider a “failure” to mean that a person does not support the law. How many “failures” would the researcher need to experience to be at the 90th percentile for number of failures before the first success?</b>
<b>qgeom(p=.90, prob=0.2)
#10
</b>
The researcher would need to experience <b>10</b> “failures” to be at the 90th percentile for number of failures before the first success.
<h2>rgeom</h2>
The <b>rgeom </b>function generates a list of random values that represent the number of failures before the first success, using the following syntax:
<b>rgeom(n, prob) </b>
where:
<b>n: </b>number of values to generate
<b>prob: </b>probability of success on a given trial
Here’s an example of when you might use this function in practice:
<b>A researcher is waiting outside of a library to ask people if they support a certain law. The probability that a given person supports the law is p = 0.2. We will consider a “failure” to mean that a person does not support the law. Simulate 10 scenarios for how many “failures” the researcher will experience until she finds someone who supports the law.</b>
<b>set.seed(0) #make this example reproducible
rgeom(n=10, prob=.2)
# 1 2 1 10 7 4 1 7 4 1
</b>
The way to interpret this is as follows:
During the first simulation, the researcher experienced 1 failure before finding someone who supported the law.
During the second simulation, the researcher experienced 2 failures before finding someone who supported the law.
During the third simulation, the researcher experienced 1 failure before finding someone who supported the law.
During the fourth simulation, the researcher experienced 10 failures before finding someone who supported the law.
And so on.
<h2><span class="orange">How to Interpret Diagnostic Plots in R</span></h2>
Linear regression models are used to describe the relationship between one or more predictor variables and a response variable.
However, once we’ve fit a regression model it’s a good idea to also produce <b>diagnostic plots</b> to analyze the residuals of the model and make sure that a linear model is appropriate to use for the particular data we’re working with.
This tutorial explains how to create and interpret diagnostic plots for a given regression model in R.
<h3>Example: Create & Interpret Diagnostic Plots in R</h3>
Suppose we fit a simple linear regression model using ‘hours studied’ to predict ‘exam score’ for students in a certain class:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 6), score=c(67, 65, 68, 77, 73, 79, 81, 88, 80, 67, 84, 93, 90, 91)) 
#fit linear regression model
model = lm(score ~ hours, data=df)
</b>
We can use the <b>plot()</b> command to produce four diagnostic plots for this regression model:
<b>#produce diagnostic plots for regression model
plot(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic1.png">
<h3>Diagnostic Plot #1: Residuals vs. Leverage Plot</h3>
This plot is used to identify influential observations. If any points in this plot fall outside of Cook’s distance (the dashed lines) then it is an influential observation.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic2.png">
In our example we can see that observation #10 lies closest to the border of Cook’s distance, but it doesn’t fall outside of the dashed line. This means there aren’t any overly influential points in our dataset.
<h3>Diagnostic Plot #2: Scale-Location Plot</h3>
This plot is used to check the assumption of equal variance (also called “homoscedasticity”) among the residuals in our regression model. If the red line is roughly horizontal across the plot, then the assumption of equal variance is likely met.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic3.png">
In our example we can see that the red line isn’t exactly horizontal across the plot, but it doesn’t deviate too wildly at any point. We would likely declare that the assumption of equal variance is not violated in this case.
<b>Related:</b>  Understanding Heteroscedasticity in Regression Analysis 
<h3>Diagnostic Plot #3: Normal Q-Q Plot</h3>
This plot is used to determine if the residuals of the regression model are normally distributed. If the points in this plot fall roughly along a straight diagonal line, then we can assume the residuals are normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic4.png">
In our example we can see that the points fall roughly along the straight diagonal line. The observations #10 and #8 deviate a bit from the line at the tail ends, but not enough to declare that the residuals are non-normally distributed.
<h3>Diagnostic Plot #4: Residuals vs. Fitted Plot</h3>
This plot is used to determine if the residuals exhibit non-linear patterns. If the red line across the center of the plot is roughly horizontal then we can assume that the residuals follow a linear pattern.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic5.png">
In our example we can see that the red line deviates from a perfect horizontal line but not severely. We would likely declare that the residuals follow a roughly linear pattern and that a linear regression model is appropriate for this dataset.
<h2><span class="orange">A Complete Guide to the diamonds Dataset in R</span></h2>
The <b>diamonds </b>dataset is a dataset that comes built-in with the  ggplot2  package in R.
It contains measurements on 10 different variables (like price, color, clarity, etc.) for 53,940 different diamonds.
This tutorial explains how to explore, summarize, and visualize the <b>diamonds</b> dataset in R.
<h3>Load the diamonds Dataset</h3>
Since the <b>diamonds </b>dataset is a built-in dataset in ggplot2, we must first install (if we haven’t already) and load the ggplot2 package:
<b>#install ggplot2 if not already installed
install.packages('ggplot2')
#load ggplot2
library(ggplot2)</b>
Once we’ve loaded ggplot2, we can use the <b>data()</b> function to load the <b>diamonds</b> dataset:
<b>data(diamonds)</b>
We can take a look at the first six rows of the dataset by using the <b>head()</b> function:
<b>#view first six rows of diamonds dataset
head(diamonds)
  carat cut       color clarity depth table price     x     y     z
1 0.23  Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43
2 0.21  Premium   E     SI1      59.8    61   326  3.89  3.84  2.31
3 0.23  Good      E     VS1      56.9    65   327  4.05  4.07  2.31
4 0.290 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63
5 0.31  Good      J     SI2      63.3    58   335  4.34  4.35  2.75
6 0.24  Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48
</b>
<h3>Summarize the diamonds Dataset</h3>
We can use the <b>summary()</b> function to quickly summarize each variable in the dataset:
<b>#summarize diamonds dataset
summary(diamonds)
     carat               cut        color        clarity          depth      
 Min.   :0.2000   Fair     : 1610   D: 6775   SI1    :13065   Min.   :43.00  
 1st Qu.:0.4000   Good     : 4906   E: 9797   VS2    :12258   1st Qu.:61.00  
 Median :0.7000   Very Good:12082   F: 9542   SI2    : 9194   Median :61.80  
 Mean   :0.7979   Premium  :13791   G:11292   VS1    : 8171   Mean   :61.75  
 3rd Qu.:1.0400   Ideal    :21551   H: 8304   VVS2   : 5066   3rd Qu.:62.50  
 Max.   :5.0100                     I: 5422   VVS1   : 3655   Max.   :79.00                      J: 2808   (Other): 2531                  
     table           price             x                y                z         
 Min.   :43.00   Min.   :  326   Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  
 1st Qu.:56.00   1st Qu.:  950   1st Qu.: 4.710   1st Qu.: 4.720   1st Qu.: 2.910  
 Median :57.00   Median : 2401   Median : 5.700   Median : 5.710   Median : 3.530  
 Mean   :57.46   Mean   : 3933   Mean   : 5.731   Mean   : 5.735   Mean   : 3.539  
 3rd Qu.:59.00   3rd Qu.: 5324   3rd Qu.: 6.540   3rd Qu.: 6.540   3rd Qu.: 4.040  
 Max.   :95.00   Max.   :18823   Max.   :10.740   Max.   :58.900   Max.   :31.800   </b>
For each of the numeric variables we can see the following information:
<b>Min</b>: The minimum value.
<b>1st Qu</b>: The value of the first quartile (25th percentile).
<b>Median</b>: The median value.
<b>Mean</b>: The mean value.
<b>3rd Qu</b>: The value of the third quartile (75th percentile).
<b>Max</b>: The maximum value.
For the categorical variables in the dataset (cut, color, and clarity) we see a frequency count of each value.
 For example, for the <b>cut</b> variable:
<b>Fair</b>: This value occurs 1,610 times.
<b>Good</b>: This value occurs 4,906 times.
<b>Very Good</b>: This value occurs 12,082 times.
<b>Premium</b>: This value occurs 13,791 times.
<b>Ideal</b>: This value occurs 21,551 times.
We can use the <b>dim()</b> function to get the dimensions of the dataset in terms of number of rows and number of columns:
<b>#display rows and columns
dim(diamonds)
[1] 53940 10
</b>
We can see that the dataset has <b>53,940 </b>rows and <b>10</b> columns.
We can also use the <b>names()</b> function to display the column names of the data frame:
<b>#display column names
names(diamonds)
[1] "carat"   "cut"     "color"   "clarity" "depth"   "table"   "price"   "x"      
[9] "y"       "z"     
</b>
<h3>Visualize the diamonds Dataset</h3>
We can also create some plots to visualize the values in the dataset.
For example, we can use the <b>geom_histogram()</b> function to create a histogram of the values for a certain variable:
<b>#create histogram of values for price
ggplot(data=diamonds, aes(x=price)) +
  geom_histogram(fill="steelblue", color="black") +
  ggtitle("Histogram of Price Values")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/diamonds2.jpg"566">
We can also use the <b>geom_point()</b> function to create a scatterplot of any pairwise combination of variables:
<b>#create scatterplot of carat vs. price, using cut as color variable
ggplot(data=diamonds, aes(x=carat, y=price, color=cut)) + 
  geom_point()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/diamonds3.jpg"593">
We can also use the <b>geom_boxplot()</b> function to create a boxplot of one variable grouped by another variable:
<b>#create scatterplot of price, grouped by cut
ggplot(data=diamonds, aes(x=cut, y=price)) + 
  geom_boxplot(fill="steelblue")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/diamonds4.jpg"556">
By using these functions from ggplot2, we can learn a great deal about the variables in the <b>diamonds</b> dataset.
 A Complete Guide to the Iris Dataset in R 
 A Complete Guide to the mtcars Dataset in R 
<h2><span class="orange">What Are Dichotomous Variables? (Definition & Example)</span></h2>
A <b>dichotomous variable</b> is a type of variable that only takes on two possible values.
Some examples of dichotomous variables include:
Gender: Male or Female
Coin Flip: Heads or Tails
Property Type: Residential or Commercial
Athlete Status: Professional or Amateur
Exam Results: Pass or Fail
These types of variables occur all the time in practice. For example, consider the following dataset that contains 10  observations  and 4 variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dichotomous1.png">
The variables <b>gender</b> and <b>Won Championship</b> are dichotomous because they can each only take on two possible values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dichotomous2.png">
However, the variables <b>Division </b>and <b>Average Points</b> are not dichotomous because they can take on multiple values.
<b>Bonus Tip:</b>
 
You can remember that dichotomous variables can only take on two values by remembering that the prefix “di” is a Greek word that means “two”, “twice”, or “double.”
<h3>How to Create Dichotomous Variables</h3>
It’s worth noting that we can create a dichotomous variable from a continuous variable by simply separating values based on some threshold.
For example, in the previous dataset we could turn the variable <b>Average Points</b> into a dichotomous variable by classifying players with an average above 15 as “high scorers” and those with an average below 15 as “low scorers”:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dichotomous3.png">
<h3>How to Visualize Dichotomous Variables</h3>
We typically visualize dichotomous variables by using a simple bar chart to represent the frequencies of each value it can take on.
For example, the following bar chart shows the frequencies of each gender in the previous dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dichotomous4.png">
We could also display the frequencies as percentages on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/dichotomous5.png">
This allows us to easily see that 70% of the total athletes in the dataset are male and 30% are female.
<h3>How to Analyze Dichotomous Variables</h3>
There are several ways to analyze dichotomous variables. Two of the most common ways include:
<b>1. One proportion z-test</b>
A  one proportion z-test  determines whether or not some observed proportion is equal to a theoretical one.
For example, we might use this test to determine if the true proportion of athletes who are male in some population is equal to 50%.
<b>2. Point-biserial correlation</b>
 Point-biserial correlation  is used to measure the relationship between a dichotomous variable and a continuous variable.
This type of correlation takes on a value between -1 and 1 where:
-1 indicates a perfectly negative correlation between two variables
0 indicates no correlation between two variables
1 indicates a perfectly positive correlation between two variables
For example, we might calculate the point-biserial correlation between gender and average points per game to understand how strongly these two variables are related.
<h2><span class="orange">Augmented Dickey-Fuller Test in R (With Example)</span></h2>
A time series is said to be “stationary” if it has no trend, exhibits constant variance over time, and has a constant autocorrelation structure over time.
One way to test whether a time series is stationary is to perform an <b>augmented Dickey-Fuller test</b>, which uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.
<b>H<sub>A</sub>:</b> The time series is stationary.
If the  p-value  from the test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that the time series is stationary.
The following step-by-step example shows how to perform an augmented Dickey-Fuller test in R for a given time series.
<h3>Example: Augmented Dickey-Fuller Test in R</h3>
Suppose we have the following time series data in R:
<b>data &lt;- c(3, 4, 4, 5, 6, 7, 6, 6, 7, 8, 9, 12, 10)</b>
Before we perform an augmented Dickey-Fuller test on the data, we can create a quick plot to visualize the data:
<b>plot(data, type='l')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/adf2.png">
To perform an augmented Dickey-Fuller test, we can use the  adf.test()  function from the <b>tseries </b>library.  
The following code shows how to use this function:
<b>library(tseries)
#perform augmented Dickey-Fuller test 
adf.test(data)
Augmented Dickey-Fuller Test
data:  data
Dickey-Fuller = -2.2048, Lag order = 2, p-value = 0.4943
alternative hypothesis: stationary
</b>
Here’s how to interpret the most important values in the output:
Test statistic: <b>-2.2048</b>
P-value: <b>0.4943</b>
Since the p-value is not less than .05, we fail to reject the null hypothesis.
This means the time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.
<h2><span class="orange">Augmented Dickey-Fuller Test in Python (With Example)</span></h2>
A time series is said to be “stationary” if it has no trend, exhibits constant variance over time, and has a constant autocorrelation structure over time.
One way to test whether a time series is stationary is to perform an <b>augmented Dickey-Fuller test</b>, which uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.
<b>H<sub>A</sub>:</b> The time series is stationary.
If the  p-value  from the test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that the time series is stationary.
The following step-by-step example shows how to perform an augmented Dickey-Fuller test in Python for a given time series.
<h3>Example: Augmented Dickey-Fuller Test in Python</h3>
Suppose we have the following time series data in Python:
<b>data = [3, 4, 4, 5, 6, 7, 6, 6, 7, 8, 9, 12, 10]</b>
Before we perform an augmented Dickey-Fuller test on the data, we can create a quick plot to visualize the data:
<b>import matplotlib.pyplot as plt
plt.plot(data)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/adf1.png">
To perform an augmented Dickey-Fuller test, we can use the  adfuller()  function from the <b>statsmodels</b> library. First, we need to install statsmodels:
<b>pip install statsmodels
</b>
Next, we can use the following code to perform the augmented Dickey-Fuller test:
<b>from statsmodels.tsa.stattools import adfuller
#perform augmented Dickey-Fuller test
adfuller(data)
(-0.9753836234744063,
 0.7621363564361013,
 0,
 12,
 {'1%': -4.137829282407408,
  '5%': -3.1549724074074077,
  '10%': -2.7144769444444443},
 31.2466098872313)
</b>
Here’s how to interpret the most important values in the output:
Test statistic: <b>-0.97538</b>
P-value: <b>0.7621</b>
Since the p-value is not less than .05, we fail to reject the null hypothesis.
This means the time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.
<h2><span class="orange">How to Use diff Function in R (With Examples)</span></h2>
You can use the <b>diff()</b> function in R to calculate lagged differences between consecutive elements in vectors.
<b>diff(x)
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Find Lagged Differences Between Consecutive Elements</h3>
The following code shows how to find the lagged differences between elements in a vector:
<b>#define vector
x &lt;- c(4, 6, 9, 8, 13)
#find lagged differences between consecutive elements
diff(x)
[1]  2  3 -1  5
</b>
Here is how the lagged differences were calculated:
6 – 4 = <b>2</b>
9 – 6 = <b>3</b>
8 – 9 = <b>-1</b>
13 – 8 = <b>5</b>
<h3>Example 2: Find Lagged Differences Between Non-Consecutive Elements</h3>
The following code shows how to use the <b>lag</b> argument to find the lagged differences between elements that are <b>2</b> positions apart in a vector:
<b>#define vector
x &lt;- c(4, 6, 9, 8, 13)
#find lagged differences between elements 2 positions apart
diff(x, lag=2)
[1] 5 2 4
</b>
Here is how the lagged differences were calculated:
9 – 4 = <b>5</b>
8 – 6 = <b>2</b>
13 – 9 = <b>4</b>
<h3>Example 3: Find Lagged Differences in Column of Data Frame</h3>
The following code shows how to find the lagged differences between a specific column in a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    7    3    1
3    3    8    6    2
4    4    3    6    8
5    5    2    8    9
#find lagged differences between elements in 'var1' column
diff(df$var1)
[1] 2 0 1 1
</b>
<h3>Example 4: Find Lagged Differences in Several Columns of Data Frame</h3>
The following code shows how to use the <b>sapply()</b> function to find the lagged differences between several columns in a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    7    3    1
3    3    8    6    2
4    4    3    6    8
5    5    2    8    9
#find lagged differences between elements in each column
sapply(df, diff)
     var1 var2 var3 var4
[1,]    2    0    0    0
[2,]    0    1    3    1
[3,]    1   -5    0    6
[4,]    1   -1    2    1
</b>
<h2><span class="orange">How to Calculate the Difference Between Two Dates in Excel</span></h2>
We can use the <b>DATEDIF()</b> function to calculate the difference between two dates in Excel.
This function uses the following syntax:
<b>=DATEDIF(Start_Date, End_Date, Metric)</b>
where:
<b>Start_Date:</b> The start date
<b>End_Date:</b> The end date
<b>Metric:</b> The metric to calculate. Options include:
“d”: Days
“m”: Months
“y”: Years
It’s important to note that this function won’t automatically appear in Excel until you completely type <b>=DATEDIF(</b> into one of the cells. 
The following examples show how to use this function to calculate the difference between two dates in Excel.
<h3>Example 1: Difference in Days</h3>
The following image shows how to calculate the difference (in days) between two dates:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/datedif1.png">
This tells us that there are <b>1,127</b> full days between 1/1/2018 and 2/1/2021.
<h3>Example 2: Difference in Months</h3>
The following image shows how to calculate the difference (in months) between two dates:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/datedif2.png">
<h3>Example 3: Difference in Years</h3>
The following image shows how to calculate the difference (in years) between two dates:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/datedif3.png">
<h3>Example 4: Difference in Years & Months</h3>
The following image shows how to calculate the difference (in years and months) between two dates:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/datedif4.png">
You can find more Excel tutorials on  this page .
<h2><span class="orange">The Differences Between ANOVA, ANCOVA, MANOVA, and MANCOVA</span></h2>
This tutorial explains the differences between the statistical methods <b>ANOVA, ANCOVA, MANOVA, and MANCOVA</b>.
<h2>ANOVA</h2>
An  <b>ANOVA</b>  (“Analysis of Variance”) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. The two most common types of ANOVAs are the one-way ANOVA and two-way ANOVA.
<b>One-Way ANOVA: </b>Used to determine how one factor impacts a response variable.
<b>Example: </b>You randomly split up a class of 90 students into three groups of 30. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam. You want to know whether or not the studying technique has an impact on exam scores so you conduct a one-way ANOVA to determine if there is a statistically significant difference between the mean scores of the three groups.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA1.jpg">
<b>Two-Way ANOVA:</b> Used to determine how two factors impact a response variable, and to determine whether or not there is an interaction between the two factors on the response variable.
<b>Example:</b> You want to determine if level of exercise (no exercise, light exercise, intense exercise) and gender (male, female) impact weight loss. In this case, the two factors you’re studying are exercise and gender and your response variable is weight loss (measured in pounds). You can conduct a two-way ANOVA to determine if exercise and gender impact weight loss and to determine if there is an interaction between exercise and gender on weight loss.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA2.jpg">
<h2>ANCOVA</h2>
An  <b>ANCOVA</b>  (“Analysis of Covariance”) is also used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. Unlike an ANOVA, however, an ANCOVA includes one or more <b>covariates</b>, which can help us better understand how a factor impacts a response variable <em>after accounting for some covariate(s)</em>.
<b>Example: </b>Consider the same example we used in the One-Way ANOVA. We split up a class of 90 students into three groups of 30. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam.
We want to know whether or not the studying technique has an impact on exam scores, <em>but we want to account for the grade that the student already has in the class </em>so we use their current grade as a covariate and conduct an ANCOVA to determine if there is a statistically significant difference between the mean scores of the three groups.
This allows us to test whether or not studying technique has an impact on exam scores after the influence of the covariate has been removed. Thus, if we find that there is a statistically significant difference in exam scores between the three studying techniques, we can be sure that this difference exists <em>even after accounting for the students current grade in the class (i.e. if they’re already doing well or not in the class)</em>.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA3.jpg">
<h2>MANOVA</h2>
A <b>MANOVA</b> (“Multivariate Analysis of Variance”) is identical to an ANOVA, except it uses two or more response variables. Similar to the ANOVA, it can also be one-way or two-way. 
<em>Note: An ANOVA can also be three-way, four-way, etc. but they are less common.</em>
<b>One-Way MANOVA Example: </b>We want to know how level of education (i.e. high school, associates degree, bachelors degrees, masters degree, etc.) impacts both annual income and amount of student loan debt. In this case, we have one factor (level of education) and two response variables (annual income and student loan debt), so we need to conduct a one-way MANOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA4.jpg">
<b>Two-Way MANOVA Example: </b>We want to know how level of education and gender impacts both annual income and amount of student loan debt. In this case, we have two factors (level of education and gender) and two response variables (annual income and student loan debt), so we need to conduct a two-way MANOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA5.jpg">
<h2>MANCOVA</h2>
A <b>MANCOVA</b> (“Multivariate Analysis of Covariance”) is identical to a MANOVA, except it also includes one or more covariates. Similar to a MANOVA, a MANCOVA can also be one-way or two-way. 
<b>One-Way MANCOVA Example: </b>We want to know how a students level of education impacts both their annual income and amount of student loan debt. However, we want to account for the annual income of the students parents as well. In this case, we have one factor (level of education), one covariate (annual income of the students parents) and two response variables (annual income of student and student loan debt), so we need to conduct a one-way MANCOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA6.jpg">
<b>Two-Way MANCOVA Example: </b>We want to know how a students level of education and their gender impacts both their annual income and amount of student loan debt. However, we want to account for the annual income of the students parents as well. In this case, we have two factors (level of education and gender), one covariate (annual income of the students parents) and two response variables (annual income of student and student loan debt), so we need to conduct a two-way MANCOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA7.jpg">
<h2><span class="orange">What is a Directional Hypothesis? (Definition & Examples)</span></h2>
A <b>statistical hypothesis</b> is an assumption about a  <b>population parameter</b> . For example, we may assume that the mean height of a male in the U.S. is 70 inches.
The assumption about the height is the<em> statistical hypothesis</em> and the true mean height of a male in the U.S. is the<em> population parameter</em>.
To test whether a statistical hypothesis about a population parameter is true, we obtain a random sample from the population and perform a <b>hypothesis test</b> on the sample data.
Whenever we perform a hypothesis test, we always write down a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
A hypothesis test can either contain a directional hypothesis or a non-directional hypothesis:
<b>Directional hypothesis:</b> The alternative hypothesis contains the less than (“&lt;“) or greater than (“>”) sign. This indicates that we’re testing whether or not there is a positive or negative effect.
<b>Non-directional hypothesis:</b> The alternative hypothesis contains the not equal (“≠”) sign. This indicates that we’re testing whether or not there is some effect, without specifying the direction of the effect.
Note that directional hypothesis tests are also called “one-tailed” tests and non-directional hypothesis tests are also called “two-tailed” tests.
Check out the following examples to gain a better understanding of directional vs. non-directional hypothesis tests.
<h2>Example 1: Baseball Programs</h2>
A baseball coach believes a certain 4-week program will increase the mean hitting percentage of his players, which is currently 0.285.
To test this, he measures the hitting percentage of each of his players before and after participating in the program.
He then performs a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ = .285 (the program will have no effect on the mean hitting percentage)
<b>H<sub>A</sub>:</b> μ > .285 (the program will cause mean hitting percentage to increase)
This is an example of a <b>directional hypothesis</b> because the alternative hypothesis contains the greater than “>” sign. The coach believes that the program will influence the mean hitting percentage of his players in a <em>positive</em> direction.
<h2>Example 2: Plant Growth</h2>
A biologist believes that a certain pesticide will cause plants to grow less during a one-month period than they normally do, which is currently 10 inches.
To test this, she applies the pesticide to each of the plants in her laboratory for one month.
She then performs a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ = 10 inches (the pesticide will have no effect on the mean plant growth)
<b>H<sub>A</sub>:</b> μ &lt; 10 inches (the pesticide will cause mean plant growth to decrease)
This is also an example of a <b>directional hypothesis</b> because the alternative hypothesis contains the less than “&lt;” sign. The biologist believes that the pesticide will influence the mean plant growth in a <em>negative</em> direction.
<h2>Example 3: Studying Technique</h2>
A professor believes that a certain studying technique will influence the mean score that her students receive on a certain exam, but she’s unsure if it will increase or decrease the mean score, which is currently 82.
To test this, she lets each student use the studying technique for one month leading up to the exam and then administers the same exam to each of the students.
She then performs a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ = 82 (the studying technique will have no effect on the mean exam score)
<b>H<sub>A</sub>:</b> μ ≠ 82 (the studying technique will cause the mean exam score to be different than 82)
This is an example of a <b>non-directional hypothesis</b> because the alternative hypothesis contains the not equal “≠” sign. The professor believes that the studying technique will influence the mean exam score, but doesn’t specify whether it will cause the mean score to increase or decrease.
<h2><span class="orange">What Are Disjoint Events? (Definition & Examples)</span></h2>
<b>Disjoint events</b> are events that cannot occur at the same time.
Written in probability notation, events <em>A</em> and <em>B</em> are disjoint if their  intersection  is zero. This can be written as:
P(A and B) = 0
P(A∩B) = 0
For example, suppose we select a random card from a deck. Let event A be the event that the card is a Spade or a Club and let event B be the event that the card is a Heart or a Diamond.
We would define the  sample space  for the events as follows:
A = {Spade, Club}
B = {Heart, Diamond}
Notice that there is no overlap between the two sample spaces. Thus, events A and B are disjoint events because they both cannot occur at the same time.
<b>Note:</b> Disjoint events are also said to be  mutually exclusive .
<h3>Examples of Disjoint Events</h3>
Here are a few more examples of disjoint events.
<b>Example 1: Coin Toss</b>
Suppose you flip a coin. Let event A be the event that the coin lands on heads and let event B be the event that the coin lands on tails.
Event A and event B would be disjoint because they both cannot occur at the same time. The coin cannot land on heads <em>and</em> tails.
<b>Example 2: Dice Roll</b>
Suppose you roll a dice. Let event A be the event that the dice lands on an odd number and let event B be the event that the dice lands on an even number.
Event A and event B would be disjoint because they both cannot occur at the same time. The dice cannot land on an even number <em>and</em> an odd number.
<b>Example 3: Pro Bowl Location</b>
Suppose the NFL wants to choose a location to host the Pro Bowl. They have narrowed down the options to Miami and San Diego. They place both names in a hat and randomly select one. Let event A be the event that they select Miami and let event B be the event that they select San Diego.
Event A and event B would be disjoint because they both cannot occur at the same time. Miami and San Diego cannot both be selected.
<h3>Visualizing Disjoint Events</h3>
One useful way to visualize disjoint events is by creating a Venn diagram.
If two events are <b>disjoint</b> then they would not overlap at all in a Venn diagram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/disjoint1.png">
Conversely, if two events are <b>non-disjoint</b> then there would be at least some overlap in the Venn diagram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/disjoint2.png">
<h3>The Probability of Disjoint Events</h3>
As mentioned earlier, if two events are disjoint then the probability that they both occur at once is zero.
P(A∩B) = 0
Similarly, the probability that <em>either</em> event occurs can be calculated by adding up their individual probabilities.
P(A∪B) = P(A) + P(B)
For example, let event A be the event that a dice lands on a 1 or a 2 and let event B be the event that a dice lands on a 5 or a 6.
We would define the sample space for the events as follows:
A = {1, 2}
B = {5, 6}
We would calculate the probability the event A or event B occurs as:
P(A∪B) = P(A) + P(B)
P(A∪B) = 2/6 + 2/6
P(A∪B) = 4/6 = 2/3
The probability that event A <em>or</em> event B occurs is <b>2/3</b>.
<h2><span class="orange">Disjoint vs. Independent Events: What’s the Difference?</span></h2>
Two terms that students often confuse are <b>disjoint</b> and <b>independent</b>.
Here’s the difference in a nutshell:
We say that two events are <b>disjoint</b> if they cannot occur at the same time.
We say that two events are <b>independent</b> if the occurrence of one event has no effect on the probability of the other event occurring.
The following examples illustrate the difference between these two terms in various scenarios.
<h3>Example 1: Flipping a Coin</h3>
<b>Scenario 1:</b> Suppose we flip a coin once. If we define event A as the coin landing on heads and we define event B as the coin landing on tails, then event A and event B are <b>disjoint</b> because the coin can’t possibly land on heads <em>and</em> tails.
<b>Scenario 2</b>: Suppose we flip a coin twice. If we define event A as the coin landing on heads on the first flip and we define event B as the coin landing on heads on the second flip, then event A and event B are <b>independent</b> because the outcome of one coin flip doesn’t affect the outcome of the other.
<h3>Example 2: Rolling a Dice</h3>
<b>Scenario 1:</b> Suppose we roll a dice once. If we let event A be the event that the dice lands on an even number and we let event B be the event that the dice lands on an odd number, then event A and event B are <b>disjoint</b> because the dice can’t possibly land on an even number <em>and</em> an odd number at the same time.
<b>Scenario 2</b>: Suppose we roll a dice twice. If we define event A as the dice landing on a “5” on the first roll and we define event B as the dice landing on a “5” on the second roll, then event A and event B are <b>independent</b> because the outcome of one dice roll doesn’t affect the outcome of the other.
<h3>Example 3: Selecting a Card</h3>
<b>Scenario 1:</b> Suppose we select a card from a standard 52-card deck. If we let event A be the event that the card is a Spade and we let event B be the event that the card is a Diamond, then event A and event B are <b>disjoint</b> because the card can’t possibly be a Spade <em>and</em> a Diamond at the same time.
<b>Scenario 2</b>: Suppose we select a card from a standard 52-card deck twice in a row with replacement. If we define event A as the card being a Spade on the first draw and we define event B as the card being a Spade on the second draw, then event A and event B are <b>independent</b> because the outcome of one draw doesn’t affect the outcome of the other.
<h3>Probability Notation: Disjoint vs. Independent Events</h3>
Written in probability notation, we say that events A and B are <b>disjoint</b> if their  intersection  is zero. This can be written as:
P(A∩B) = 0
For example, suppose we roll a dice once. Let event A be the event that the dice lands on an even number and let event B be the event that the dice lands on an odd number.
We would define the  sample space  for the events as follows:
A = {2, 4, 6}
B = {1, 3, 5}
Notice that there is no overlap between the two sample spaces. Thus, events A and B are <b>disjoint</b> events because they both cannot occur at the same time.
Thus, we could write:
P(A∩B) = 0
Similarly, written in probability notation, we say that events A and B are <b>independent </b>if the following is true:
P(A∩B) = P(A) * P(B)
For example, suppose we roll a dice twice. Let event A be the event that the dice lands on a “5” on the first roll and let event B be the event that the dice lands on a “5” on the second roll.
If we write out all of the 36 possible ways for the dice to land, we would find that in only 1 out of the 36 scenarios the dice lands on a “5” both times. Thus, we would say P(A∩B) = 1/36. 
We also know that the probability of the dice landing on a “5” during the first roll is P(A) = 1/6. 
We also know that the probability of the dice landing on a “5” during the second roll is P(B) = 1/6. 
Thus, we could write:
P(A∩B) = P(A) * P(B)
1/36 = 1/6 * 1/6
1/36 = 1/36
Since this equation holds true, we could indeed say that event A and event B are <b>independent</b> in this scenario.
<h2><span class="orange">How to Use the dist Function in R (With Examples)</span></h2>
The <b>dist()</b> function in R can be used to calculate a distance matrix, which displays the distances between the rows of a matrix or data frame.
This function uses the following basic syntax:
<b>dist(x, method=”euclidean”)</b>
where:
<b>x:</b> The name of the matrix or data frame.
<b>method:</b> The distance measure to use. Default is “euclidean” but options include “maximum”, “manhattan”, “canberra”, “binary” or “minkowski”.
The following examples show how to use this function in practice with the following data frame:
<b>#define four vectors
a &lt;- c(2, 4, 4, 6)
b &lt;- c(5, 5, 7, 8)
c &lt;- c(9, 9, 9, 8)
d &lt;- c(1, 2, 3, 3)
#row bind four vectors into matrix
mat &lt;- rbind(a, b, c, d)
#view matrix
mat
  [,1] [,2] [,3] [,4]
a    2    4    4    6
b    5    5    7    8
c    9    9    9    8
d    1    2    3    3</b>
<h3>Example 1: Use dist() to Calculate Euclidean Distance</h3>
The <b>Euclidean distance</b> between two vectors, A and B, is calculated as:
<b>Euclidean distance = √Σ(A<sub>i</sub>-B<sub>i</sub>)<sup>2</sup></b>
The following code shows how to compute a distance matrix that shows the Euclidean distance between each row of a matrix in R:
<b>#calculate Euclidean distance between each row in matrix
dist(mat)
          a         b         c
b  4.795832                    
c 10.148892  6.000000          
d  3.872983  8.124038 13.190906
</b>
Here’s how to interpret the output:
The Euclidean distance between row a and row b is <b>4.795832</b>.
The Euclidean distance between row a and row c is <b>10.148892</b>.
The Euclidean distance between row a and row d is <b>3.872983</b>.
The Euclidean distance between row b and row c is <b>6.000000</b>.
The Euclidean distance between row b and row d is <b>8.124038</b>.
The Euclidean distance between row c and row d is <b>13.190906</b>.
<h3>Example 2: Use dist() to Calculate Maximum Distance</h3>
The <b>Maximum distance</b> between two vectors, A and B, is calculated as the maximum difference between any pairwise elements.
The following code shows how to compute a distance matrix that shows the Maximum distance between each row of a matrix in R:
<b>#calculate Maximum distance between each row in matrix
dist(mat, method="maximum")
  a b c
b 3    
c 7 4  
d 3 5 8</b>
<h3>Example 3: Use dist() to Calculate Canberra Distance</h3>
The <b>Canberra distance</b> between two vectors, A and B, is calculated as:
<b>Canberra distance = Σ |A<sub>i</sub>-B<sub>i</sub>| / |A<sub>i</sub>| + |B<sub>i</sub>|</b>
The following code shows how to compute a distance matrix that shows the Canberra distance between each row of a matrix in R:
<b>#calculate Canberra distance between each row in matrix
dist(mat, method="canberra")
          a         b         c
b 0.9552670                    
c 1.5484515 0.6964286          
d 1.1428571 1.9497835 2.3909091</b>
<h3>Example 4: Use dist() to Calculate Binary Distance</h3>
The <b>Binary distance</b> between two vectors, A and B, is calculated as the proportion of elements that the two vectors share.
The following code shows how to compute a distance matrix that shows the Binary distance between each row of a matrix in R:
<b>#calculate Binary distance between each row in matrix
dist(mat, method="binary")
  a b c
b 0    
c 0 0  
d 0 0 0</b>
<h3>Example 5: Use dist() to Calculate Minkowski Distance</h3>
The <b>Minkowski distance</b> between two vectors, A and B, is calculated as:
<b>Minkowski distance = (Σ|a<sub>i</sub> – b<sub>i</sub>|<sup>p</sup>)<sup>1/p</sup></b>
where <em>i</em> is the i<sup>th</sup> element in each vector and <em>p</em> is an integer.
The following code shows how to compute a distance matrix that shows the Minkowski distance (using p=3) between each row of a matrix in R:
<b>#calculate Minkowski distance between each row in matrix
dist(mat, method="minkowski", p=3)
          a         b         c
b  3.979057                    
c  8.439010  5.142563          
d  3.332222  6.542133 10.614765</b>
<h2><span class="orange">Dixon’s Q Test: Definition + Example</span></h2>
<b>Dixon’s Q Test</b>, often referred to simply as the <b>Q Test</b>, is a statistical test that is used for detecting outliers in a dataset.
The test statistic for the Q test is as follows:
<b>Q</b> = |x<sub>a</sub> – x<sub>b</sub>| / R
where <b>x<sub>a</sub></b> is the suspected outlier, <b>x<sub>b</sub></b> is the data point closest to x<sub>a</sub>, and <b>R</b> is the range of the dataset. In most cases, x<sub>a</sub> is the maximum value in the dataset but it can also be the minimum value.
It’s important to note that the Q test is typically performed on small datasets and the test assumes that the data is normally distributed. It’s also important to note that the Q test should only be conducted one time for a given dataset.
<h2>How to Conduct Dixon’s Q Test By Hand</h2>
Suppose we have the following dataset: 
<b>1, 3, 5, 7, 8, 9, 13, 25</b>
We can follow the  standard five-step procedure for hypothesis testing  to conduct Dixon’s Q Test by hand to determine if the maximum value in this dataset is an outlier:
<b>Step 1. State the hypotheses. </b>
The null hypothesis (H0): The max is not an outlier.
The alternative hypothesis: (Ha): The max <em>is </em>an outlier.
<b>Step 2. Determine a significance level to use.</b>
Common choices are 0.1, 0.05, and 0.01. We will use a .05 level of significance for this example.
<b>Step 3. Find the test statistic.</b>
<b>Q</b> = |x<sub>a</sub> – x<sub>b</sub>| / R
In this case, our max value is x<sub>a </sub>= 25, our next closest value is x<sub>b </sub>= 13, and our range is R = 25 – 1 = 24.
Thus, <b>Q </b> = |25 – 13| / 24 = <b>0.5</b>.
Next, we can compare this test statistic to the Q test critical values, which are shown below for various sample sizes (n) and confidence levels:
<b>n       90%       95%       99%</b>
<b>3 </b>   0.941    0.970    0.994
<b>4</b>    0.765    0.829    0.926
<b>5</b>    0.642    0.710    0.821
<b>6</b>    0.560    0.625    0.740
<b>7</b>    0.507    0.568    0.680
<b>8</b>    0.468    0.526    0.634
<b>9</b>    0.437    0.493    0.598
<b>10</b> 0.412    0.466    0.568
<b>11</b> 0.392    0.444    0.542
<b>12</b> 0.376    0.426    0.522
<b>13</b> 0.361    0.410    0.503
<b>14</b> 0.349    0.396    0.488
<b>15</b> 0.338    0.384    0.475
<b>16</b> 0.329    0.374    0.463
<b>17</b> 0.320    0.365    0.452
<b>18</b> 0.313    0.356    0.442
<b>19</b> 0.306    0.349    0.433
<b>20</b> 0.300    0.342    0.425
<b>21</b> 0.295    0.337    0.418
<b>22</b> 0.290    0.331    0.411
<b>23</b> 0.285    0.326    0.404
<b>24</b> 0.281    0.321    0.399
<b>25</b> 0.277    0.317    0.393
<b>26</b> 0.273    0.312    0.388
<b>27</b> 0.269    0.308    0.384
<b>28</b> 0.266    0.305    0.380
<b>29</b> 0.263    0.301    0.376
<b>30</b> 0.260    0.290    0.372
The critical value for a sample size of 8 and a confidence level of 95% is <b>0.526</b>.
<b>Step 4. Reject or fail to reject the null hypothesis.</b>
Since our test statistic Q (0.5) is less than the critical value (0.526), we fail to reject the null hypothesis.
<b>Step 5. Interpret the results. </b>
Since we failed to reject the null hypothesis, we conclude that the max value <em>25 </em>is not an outlier in this dataset.
<h2>How to Conduct Dixon’s Q Test in R</h2>
To conduct Dixon’s Q Test on the same dataset in R, we can use the <b>dixon.test() </b>function from the <b>outliers </b>library, which uses the following syntax:
dixon.test(data, , type = 10, opposite = FALSE)
<b>data: </b>a numeric vector of data values
<b>type: </b>the type of formula to use to conduct the test statistic Q. Set to 10 to use the formula outlined earlier.
<b>opposite: </b>If FALSE, the test determines if the maximum value is an outlier. If TRUE, the test determines if the minimum value is an outlier. This is FALSE by default. 
<em>Note</em>: <em>Find the complete documentation for dixon.test()  here .</em>
The following code illustrates how to conduct Dixon’s Q Test to determine if the maximum value in the dataset is an outlier.
<b>#load the <em>outliers </em>library
library(outliers)
#create data
data &lt;- c(1, 3, 5, 7, 8, 9, 13, 25)
#conduct Dixon's Q Test
dixon.test(data, type = 10)
#Dixon test for outliers
#
#data:  data
#Q = 0.5, p-value = 0.06913
#alternative hypothesis: highest value 25 is an outlier
</b>
From the output we can see that the test statistic is Q = <b>0.5</b> and the corresponding p-value is <b>0.06913</b>. Thus, we fail to reject the null hypothesis at a 0.05 significance level and conclude that <em>25 </em>is not an outlier. This matches the result we got by hand.

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
