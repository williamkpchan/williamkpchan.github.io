<base target="_blank"><html><head><title>statologyContents 8</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 8"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 8</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">Comparing grep() vs. grepl() in R: What’s the Difference?</span></h2>
Two functions that people often get mixed up in R are <b>grep() </b>and <b>grepl()</b>. Both functions allow you to see whether a certain pattern exists in a character string, but they return different results:
<b>grepl() </b>returns TRUE when a pattern exists in a character string.
<b>grep() </b>returns a vector of indices of the character strings that contain the pattern.
The following example illustrates this difference:
<b>#create a vector of data
data &lt;- c('P Guard', 'S Guard', 'S Forward', 'P Forward', 'Center')
grep('Guard', data)
[1] 1 2
grepl('Guard', data) 
[1]  TRUE  TRUE FALSE FALSE FALSE</b>
The following examples show when you might want to use one of these functions over the other.
<h3>When to Use grepl()</h3>
<b>1. Filter Rows that Contain a Certain String</b>
One of the most common uses of <b>grepl() </b>is for filtering rows in a data frame that contain a certain string:
<b>library(dplyr)
#create data frame
df &lt;- data.frame(player = c('P Guard', 'S Guard', 'S Forward', 'P Forward', 'Center'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#filter rows that contain the string 'Guard' in the player column
df %>% filter(grepl('Guard', player))
   player points rebounds
1 P Guard     12        5
2 S Guard     15        7</b>
<b>Related: </b> How to Filter Rows that Contain a Certain String Using dplyr 
<h3>When to Use grep()</h3>
<b>1. Select Columns that Contain a Certain String</b>
You can use <b>grep() </b>to select columns in a data frame that contain a certain string:
<b>library(dplyr)
#create data frame
df &lt;- data.frame(player = c('P Guard', 'S Guard', 'S Forward', 'P Forward', 'Center'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#select columns that contain the string 'p' in their name
df %>% select(grep('p', colnames(df)))
     player points
1   P Guard     12
2   S Guard     15
3 S Forward     19
4 P Forward     22
5    Center     32</b>
<b>2. Count the Number of Rows that Contain a Certain String</b>
You can use <b>grep() </b>to count the number of rows in a data frame that contain a certain string:
<b>#create data frame
df &lt;- data.frame(player = c('P Guard', 'S Guard', 'S Forward', 'P Forward', 'Center'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#count how many rows contain the string 'Guard' in the player column
length(grep('Guard', df$player))
[1] 2</b>
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">The Complete Guide: How to Group & Summarize Data in R</span></h2>
Two of the most common tasks that you’ll perform in data analysis are grouping and summarizing data. 
Fortunately the  dplyr  package in R allows you to quickly group and summarize data.
This tutorial provides a quick guide to getting started with dplyr.
<h2>Install & Load the dplyr Package</h2>
Before you can use the functions in the dplyr package, you must first load the package:
<b>#install dplyr (if not already installed)
install.packages('dplyr')
#load dplyr 
library(dplyr)</b>
Next, we’ll illustrate several examples of how to use the functions in dplyr to group and summarize data using the built-in R dataset called <b>mtcars</b>:
<b>#obtain rows and columns of <em>mtcars</em>
dim(mtcars)
[1] 32 11
#view first six rows of <em>mtcars</em>
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</b>
The basic syntax that we’ll use to group and summarize data is as follows:
<b>data %>%
  group_by(col_name) %>%
  summarize(summary_name = summary_function)
</b>
<em><b>Note: </b>The functions summarize() and summarise() are equivalent.</em>
<h2>Example 1: Find Mean & Median by Group</h2>
The following code shows how to calculate  measures of central tendency  by group including the mean and the median:
<b>#find mean mpg by cylinder
mtcars %>%
  group_by(cyl) %>%
  summarize(mean_mpg = mean(mpg, na.rm = TRUE))
# A tibble: 3 x 2
    cyl mean_mpg
      
1     4     26.7
2     6     19.7
3     8     15.1
#find median mpg by cylinder
mtcars %>%
  group_by(cyl) %>%
  summarize(median_mpg = median(mpg, na.rm = TRUE))
# A tibble: 3 x 2
    cyl median_mpg
        
1     4       26  
2     6       19.7
3     8       15.2</b>
<h2>Example 2: Find Measures of Spread by Group</h2>
The following code shows how to calculate  measures of dispersion  by group including the standard deviation, interquartile range, and median absolute deviation:
<b>#find sd, IQR, and mad by cylinder
mtcars %>%
  group_by(cyl) %>%
  summarize(sd_mpg = sd(mpg, na.rm = TRUE),
            iqr_mpg = IQR(mpg, na.rm = TRUE),
            mad_mpg = mad(mpg, na.rm = TRUE))
# A tibble: 3 x 4
    cyl sd_mpg iqr_mpg mad_mpg
          
1     4   4.51    7.60    6.52
2     6   1.45    2.35    1.93
3     8   2.56    1.85    1.56</b>
<h2>Example 3: Find Count by Group</h2>
The following code shows how to find the count and the unique count by group in R:
<b>#find row count and unique row count by cylinder
mtcars %>%
  group_by(cyl) %>%
  summarize(count_mpg = n(),
            u_count_mpg = n_distinct(mpg))
# A tibble: 3 x 3
    cyl count_mpg u_count_mpg
              
1     4        11           9
2     6         7           6
3     8        14          12
</b>
<h2>Example 4: Find Percentile by Group</h2>
The following code shows how to find the 90th percentile of values for mpg by cylinder group:
<b>#find 90th percentile of mpg for each cylinder group
mtcars %>%
  group_by(cyl) %>%
  summarize(quant90 = quantile(mpg, probs = .9))
# A tibble: 3 x 2
    cyl quant90
     
1     4    32.4
2     6    21.2
3     8    18.3</b>
<h2>Additional Resources</h2>
You can find the complete documentation for the dplyr package along with helpful visualize cheat sheets  here .
Other useful functions that you can use along with <b>group_by()</b> and <b>summarize()</b> include functions for  filtering data frame rows  and  arranging rows in certain orders .
<h2><span class="orange">How to Create a Grouped Barplot in R (With Examples)</span></h2>
A <b>grouped barplot</b> is a type of chart that displays quantities for different variables, <i>grouped</i> by another variable.
This tutorial explains how to create grouped barplots in R using the data visualization library  ggplot2 .
<h3>Grouped Barplot in ggplot2</h3>
Suppose we have the following data frame that displays the average points scored per game for nine basketball players:
<b>#create data frame
df &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=3), position=rep(c('Guard', 'Forward', 'Center'), times=3), points=c(14, 8, 8, 16, 3, 7, 17, 22, 26))
#view data frame
df
  team position points
1    A    Guard     14
2    A  Forward      8
3    A   Center      8
4    B    Guard     16
5    B  Forward      3
6    B   Center      7
7    C    Guard     17
8    C  Forward     22
9    C   Center     26</b>
We can use the following code to create a grouped barplot that displays the points scored by each player, grouped by team and position:
<b>library(ggplot2)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='dodge', stat='identity')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/stackedBarR1.png">
<h3>Customizing a Grouped Barplot</h3>
We can also customize the title, axes labels, theme, and colors of the grouped barplot to make it look however we’d like:
<b>library(ggplot2)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='dodge', stat='identity') +
  theme_minimal() + 
  labs(x='Team', y='Points', title='Avg. Points Scored by Position & Team') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold')) +
  scale_fill_manual('Position', values=c('coral2', 'steelblue', 'pink'))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/stackedBarR2.png">
We can customize the appearance even more by using one of the themes in the <b>ggthemes</b> library. For example, we could use the Wall Street Journal Theme from this library:
<b>install.packages('ggthemes')
library(ggplot2)
library(ggthemes)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='dodge', stat='identity') +
  theme_wsj()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/stackedBarR3.png">
<em>Refer to our  Complete Guide to the Best ggplot2 Themes  for even more themes.</em>
<h2><span class="orange">How to Create a Grouped Boxplot in R Using ggplot2</span></h2>
<b>Boxplots</b> are useful for visualizing the five-number summary of a dataset, which includes:
The minimum
The first quartile
The median
The third quartile
The maximum
<b>Related: </b> A Gentle Introduction to Boxplots 
Fortunately it’s easy to create boxplots in R using the visualization library  ggplot2 .
It’s also to create boxplots <em>grouped </em>by a particular variable in a dataset. For example, suppose we have the following dataset that displays the increase in efficiency for 150 basketball players on three different teams based on two different training programs:
<b>#define variables
team=rep(c('A', 'B', 'C'), each=50)
program=rep(c('low', 'high'), each=25)
increase=seq(1:150)+sample(1:100, 100, replace=TRUE)
#create dataset using variables
data=data.frame(team, program, increase)
#view first six rows of dataset 
head(data)
  team program increase
1    A     low       62
2    A     low       37
3    A     low       49
4    A     low       60
5    A     low       64
6    A     low      105
</b>
We can use the following code to create boxplots that display the increase in efficiency for players, grouped by team and filled in based on the training program:
<b>library(ggplot2)
ggplot(data, aes(x=team, y=increase, fill=program)) + 
  geom_boxplot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/groupedBoxplotR1.png">
We can use similar syntax to create boxplots that display the increase in efficiency for players, grouped by training program and filled in based on the team:
<b>library(ggplot2)
ggplot(data, aes(x=program, y=increase, fill=team)) + 
  geom_boxplot()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/groupedBoxplotR2.png">
A similar alternative is to use <b>faceting</b>, in which each subgroup is shown in its own panel:
<b>library(ggplot2)
ggplot(data, aes(x=team, y=increase, fill=program)) + 
  geom_boxplot() +
  facet_wrap(~program)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/groupedBoxplotR3.png">
Depending on the data you’re working with, faceting may or may not make sense for your visualization needs.
<h2><span class="orange">How to Conduct Grubbs’ Test in Excel</span></h2>
<b>Grubbs’ Test</b> is a statistical test that can be used to identify the presence of one outlier in a dataset. To use this test, the dataset you’re analyzing should be approximately normally distributed and, ideally, should have at least 7 observations.
<b>Note: </b><em>If you suspect that there is more than one outlier in the dataset, then you should instead use  the generalized extreme studentized deviate test for outliers .</em>
If you suspect that the maximum value in the dataset is an outlier, the test statistic is calculated as:
<b>G</b> = (x<sub>max</sub> – x) / s
If you suspect that the minimum value in the dataset is an outlier, the test statistic is calculated as:
<b>G</b> = (x – x<sub>min</sub>) / s
And if you’re not sure if the maximum value or minimum value in the dataset is an outlier and you want to perform a two-sided test, then the test statistic is calculated as:
<b>G</b> = max|x<sub>i</sub> – x| / s
where  <em>x</em> is the sample mean and <em>s </em>is the sample standard deviation.
The critical value for the test is calculated as:
<b>G<sub>critical</sub></b>= (n-1)t<sub>critical</sub>  /  √[n(n-2 + t<sup>2</sup><sub>critical</sub>)]
where t<sub>critical</sub> is the critical value of the t distribution with n-2 degrees of freedom and the significance level is α/n for a single-tail test and α/(2n) for a two-tailed test.
<h2>Example: Grubbs’ Test in Excel</h2>
Determine whether or not the value 60 is an outlier in the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/grubbs1.jpg"282">
<b>Step 1: </b>First, we need to make sure that the data is approximately normally distributed. To do so, we can create a histogram to verify that the distribution roughly has a bell-shape. The following screenshot shows how to create a histogram in Excel using  the Data Analysis ToolPak :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/grubbs2.jpg"619">
From the histogram, we can see that the data is roughly normally distributed. This means that we can go ahead and conduct Grubbs’ Test.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/grubbs3.jpg">
<b>Step 2: </b>Next, we’ll conduct Grubbs’ Test to determine if the value 60 is actually an outlier in the dataset. The screenshot below shows the formulas to use to conduct Grubbs’ Test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/grubbs4.jpg"581">
The test statistic, <b>G</b>, in cell D4 is <b>3.603219</b>.
The critical value, <b>G<sub>critical</sub></b>, in cell D11 is <b>2.556581</b>. Since the test statistic is greater than the critical value, this means that the value 60 is indeed an outlier in this dataset. 
<h2>What to Do if an Outlier is Identified</h2>
If Grubbs’ Test does identify an outlier in your dataset, you have a few options:
<b>Double check to make sure that the value is not a typo or a data entry error.</b> Occasionally, values that show up as outliers in datasets are simply typos made by an individual when entering the data. Go back and verify that the value was entered correctly before you make any further decisions.
<b>Assign a new value to the outlier</b>. If the outlier turns out to be a result of a typo or data entry error, you may decide to assign a new value to it, such as  the mean or the median  of the dataset.
<b>Remove the outlier. </b>If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis.
No matter what you decide to do with the outlier, be sure to make a note of it when you present the final conclusions of your analysis. 
<h2><span class="orange">How to Perform Grubbs’ Test in Python</span></h2>
<b>Grubbs’ Test</b> is used to identify the presence of outliers in a dataset. To use this test, a dataset should be approximately normally distributed and have at least 7 observations.
This tutorial explains how to perform Grubbs’ Test in Python.
<h3>Grubbs’ Test in Python</h3>
To perform Grubbs’ Test in Python, we can use the smirnov_grubbs() function from the  outlier_utils  package, which uses the following syntax:
<b>smirnov_grubbs.test(data, alpha=.05)</b>
where:
<b>data: </b>A numeric vector of data values
<b>alpha: </b>The significance level to use for the test. Default is .05
To use this function, you need to first install the  outlier_utils  package:
<b>pip install outlier_utils
</b>
Once this package is installed, you can perform Grubbs’ Test. The following examples illustrate how to do so.
<h3>Example 1: Two-Sided Grubbs’ Test</h3>
The following code illustrates how to perform a two-sided Grubbs’ test, which will detect outliers on both ends of the dataset.
<b>import numpy as np
from outliers import smirnov_grubbs as grubbs
#define data
data = np.array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40])
#perform Grubbs' test
grubbs.test(data, alpha=.05)
array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22,  8, 21, 28, 11,  9, 29])
</b>
This function simply returns an array with the outliers removed. In this case, the max value of 40 was an outlier, so it was removed.
<h3>Example 2: One-Sided Grubbs’ Test</h3>
The following code illustrates how to perform a one-sided Grubbs’ test for both the minimum value and the maximum value in a dataset:
<b>import numpy as np
from outliers import smirnov_grubbs as grubbs
#define data
data = np.array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40])
#perform Grubbs' test to see if minimum value is an outlier
grubbs.min_test(data, alpha=.05)
array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22,  8, 21, 28, 11,  9, 29, 40])
#perform Grubbs' test to see if minimum value is an outlier
grubbs.max_test(data, alpha=.05)
array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29])
</b>
The minimum outlier test did not detect the minimum value as an outlier. However, the maximum outlier test did determine that the max value of 40 was an outlier, so it was removed.
<h3>Example 3: Extract the Index of the Outlier</h3>
The following code illustrates how to extract the index of the outlier value:
<b>import numpy as np
from outliers import smirnov_grubbs as grubbs
#define data
data = np.array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40])
#perform Grubbs' test and identify index (if any) of the outlier
grubbs.max_test_indices(data, alpha=.05)
[16]
</b>
This tells us that there is an outlier in index position 16 of the array.
<h3>Example 4: Extract the Value of the Outlier</h3>
The following code illustrates how to extract the value of the outlier:
<b>import numpy as np
from outliers import smirnov_grubbs as grubbs
#define data
data = np.array([5, 14, 15, 15, 14, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40])
#perform Grubbs' test and identify the actual value (if any) of the outlier
grubbs.max_test_outliers(data, alpha=.05)
[40]
</b>
This tells us that there is one outlier with a value of 40.
<h3>How to Handle Outliers</h3>
If Grubbs’ Test identifies an outlier in your dataset, you have a few options:
<b>1. Double check to make sure that the value is not a typo or a data entry error.</b> Sometimes values that show up as outliers in datasets are simply typos made by an individual when entering the data. First, verify that the value was entered correctly before you make any further decisions.
<b>2. Assign a new value to the outlier</b>. If the outlier turns out to be a result of a typo or data entry error, you may decide to assign a new value to it, such as  the mean or the median  of the dataset.
<b>3.Remove the outlier. </b>If the value is a true outlier, you may choose to remove it if it will have a significant impact on your analysis.
<h2><span class="orange">How to Perform Grubbs’ Test in R</span></h2>
<b>Grubbs’ Test</b> is a statistical test that can be used to identify the presence of outliers in a dataset. To use this test, a dataset should be approximately normally distributed and have at least 7 observations.
This tutorial explains how to perform Grubbs’ Test in R to detect outliers in a dataset.
<h3>Example: Grubbs’ Test in R</h3>
To perform Grubbs’ Test in R, we can use the <b>grubbs.test()</b> function from the <b>Outliers</b> package, which uses the following syntax:
<b>grubbs.test(x, type = 10, opposite = FALSE, two.sided = FALSE)</b>
where:
<b>x: </b>a numeric vector of data values
<b>type: </b>10 = test if max value is outlier, 11 = test if both min and max value are outliers, 20  = test if there are two outliers on one tail
<b>opposite: </b>logical indicating whether you want to check not the value with largest difference from the mean, but opposite (lowest, if most suspicious is highest etc.)
<b>two-sided: </b>logical value indicating whether or not you should treat the test as two-sided
This test uses the following two hypotheses:
<b>H<sub>0</sub> (null hypothesis): </b>There is no outlier in the data.
<b>H<sub>A</sub> (alternative hypothesis): </b>There is an outlier in the data.
The following example illustrates how to perform Grubbs’ Test to determine if the max value in a dataset is an outlier:
<b>#load Outliers package
library(Outliers)
#create data
data &lt;- c(5, 14, 15, 15, 14, 13, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40)
#perform Grubbs' Test to see if '40' is an outlier
grubbs.test(data)
#Grubbs test for one outlier
#
#data:  data
#G = 2.65990, U = 0.55935, p-value = 0.02398
#alternative hypothesis: highest value 40 is an outlier
</b>
The test statistic of the test is <b>G = 2.65990 </b>and the corresponding p-value is <b>p = 0.02398</b>. Since this value is less than 0.05, we will reject the null hypothesis and conclude that the max value of 40 is an outlier.
If we instead wanted to test whether the lowest value of ‘5’ was an outlier, we could use the <b>opposite=TRUE</b> command:
<b>#perform Grubbs' Test to see if '5' is an outlier
grubbs.test(data, opposite=TRUE)
#Grubbs test for one outlier
#
#data:  data
#G = 1.4879, U = 0.8621, p-value = 1
#alternative hypothesis: lowest value 5 is an outlier
</b>
The test statistic is <b>G = 1.4879 </b>and the corresponding p-value is <b>p = 1</b>. Since this value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the minimum value of ‘5’ is an outlier.
Lastly, suppose we had two large values at one end of the dataset: 40 and 42. To test if <em>both </em>of these values are outliers, we could perform Grubbs’ Test and specify that <b>type=20</b>:
<b>#create dataset with two large values at one end: 40 and 42
data &lt;- c(5, 14, 15, 15, 14, 13, 19, 17, 16, 20, 22, 8, 21, 28, 11, 9, 29, 40, 42) 
#perform Grubbs' Test to see if both 40 and 42 are outliers
grubbs.test(data, type=20)
#Grubbs test for two outliers
#
#data:  data
#U = 0.38111, p-value = 0.01195
#alternative hypothesis: highest values 40 , 42 are outliers
</b>
The p-value of the test is <b>0.01195</b>. Since this is less than 0.05, we can reject the null hypothesis and conclude that we have sufficient evidence to say the values 40 and 42 are both outliers.
<h3>What to Do if an Outlier is Identified</h3>
If Grubbs’ Test does identify an outlier in your dataset, you have a few options:
<b>1. Double check to make sure that the value is not a typo or a data entry error.</b> Occasionally, values that show up as outliers in datasets are simply typos made by an individual when entering the data. Go back and verify that the value was entered correctly before you make any further decisions.
<b>2. Assign a new value to the outlier</b>. If the outlier turns out to be a result of a typo or data entry error, you may decide to assign a new value to it, such as  the mean or the median  of the dataset.
<b>3.Remove the outlier. </b>If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis.
<h2><span class="orange">How to Use the gsub() Function in R (With Examples)</span></h2>
The <b>gsub()</b> function in R can be used to replace all occurrences of certain text within a string in R.
This function uses the following basic syntax:
<b>gsub(pattern, replacement, x) </b>
where:
<b>pattern</b>: The pattern to look for
<b>replacement</b>: The replacement for the pattern
<b>x</b>: The string to search
The following examples show how to use this function in practice.
<h3>Example 1: Replace Text in String</h3>
The following code shows how to replace a specific piece of text in a string:
<b>#define string
x &lt;- "This is a fun sentence"
#replace 'fun' with 'great'
x &lt;- gsub('fun', 'great', x)
#view updated string
x
[1] "This is a great sentence"
</b>
<h3>Example 2: Replace Single Text String in Vector</h3>
The following code shows how to replace multiple occurrences of a text in a vector:
<b>#define vector
x &lt;- c('Mavs', 'Mavs', 'Spurs', 'Nets', 'Spurs', 'Mavs')
#replace 'Mavs' with 'M'
x &lt;- gsub('Mavs', 'M', x)
#view updated vector
x
[1] "M"     "M"     "Spurs" "Nets"  "Spurs" "M"
</b>
<h3>Example 3: Replace Multiple Text Strings in Vector</h3>
The following code shows how to replace multiple occurrences of two different text strings in a vector:
<b>#define vector
x &lt;- c('A', 'A', 'B', 'C', 'D', 'D')
#replace 'A' or 'B' or 'C' with 'X'
x &lt;- gsub('A|B|C', 'X', x)
#view updated string
x
[1] "X" "X" "X" "X" "D" "D"
</b>
<h3>Example 4: Replace Text in Data Frame</h3>
The following code shows how to replace text in a data frame:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D'), conf=c('West', 'West', 'East', 'East'), points=c(99, 98, 92, 87), rebounds=c(18, 22, 26, 19))
#view data frame
df
  team conf points rebounds
1    A West     99       18
2    B West     98       22
3    C East     92       26
4    D East     87       19
#replace 'West' and 'East' with 'W' and 'E'
df$conf &lt;- gsub('West', 'W', df$conf)
df$conf &lt;- gsub('East', 'E', df$conf)
#view updated data frame
df
  team conf points rebounds
1    A    W     99       18
2    B    W     98       22
3    C    E     92       26
4    D    E     87       19</b>
<h2><span class="orange">How to Calculate Hamming Distance in Excel</span></h2>
The <b>Hamming distance</b> between two vectors is simply the sum of corresponding elements that differ between the vectors.
For example, suppose we have the following two vectors:
<b>x = [1, 2, 3, 4]
y = [1, 2, 5, 7]
</b>
The Hamming distance between the two vectors would be <b>2</b>, since this is the total number of corresponding elements that have different values.
To calculate the Hamming distance between two columns in Excel, we can use the following syntax:
<b>=COUNT(RANGE1)-SUMPRODUCT(--(RANGE1 = RANGE2))</b>
Here’s what the formula does in a nutshell:
<b>COUNT</b> finds the total number of observations in the first column.
<b>RANGE1 = RANGE2</b> compares each pairwise observations between the columns and returns a TRUE or FALSE.
<b>– –</b> converts TRUE and FALSE values to 0 and 1.
<b>SUMPRODUCT</b> finds the sum of all 1’s.
This tutorial provides several examples of how to use this calculation in practice.
<h3>Example 1: Hamming Distance Between Binary Vectors</h3>
The following code shows how to calculate the Hamming distance between two columns in Excel that each contain only two possible values: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/hammingExcel1.png">
The Hamming distance between the two columns is <b>3</b>.
<h3>Example 2: Hamming Distance Between Numerical Vectors</h3>
The following code shows how to calculate the Hamming distance between two columns in Excel that each contain several numerical values:
<b> <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/hammingExcel2.png"></b>
The Hamming distance between the two vectors is <b>7</b>.
<h2><span class="orange">How to Calculate Hamming Distance in R (With Examples)</span></h2>
The <b>Hamming distance</b> between two vectors is simply the sum of corresponding elements that differ between the vectors.
For example, suppose we have the following two vectors:
<b>x = [1, 2, 3, 4]
y = [1, 2, 5, 7]
</b>
The Hamming distance between the two vectors would be <b>2</b>, since this is the total number of corresponding elements that have different values.
To calculate the Hamming distance between two vectors in R, we can use the following syntax:
<b>sum(x != y)</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Hamming Distance Between Binary Vectors</h3>
The following code shows how to calculate the Hamming distance between two vectors that each contain only two possible values:
<b>#create vectors
x &lt;- c(0, 0, 1, 1, 1)
y &lt;- c(0, 1, 1, 1, 0)
#find Hamming distance between vectors
sum(x != y)
[1] 2
</b>
The Hamming distance between the two vectors is <b>2</b>.
<h3>Example 2: Hamming Distance Between Numerical Vectors</h3>
The following code shows how to calculate the Hamming distance between two vectors that each contain several numerical values:
<b>#create vectors
x &lt;- c(7, 12, 14, 19, 22)
y &lt;- c(7, 12, 16, 26, 27)
#find Hamming distance between vectors
sum(x != y)
[1] 3
</b>
The Hamming distance between the two vectors is <b>3</b>.
<h3>Example 3: Hamming Distance Between String Vectors</h3>
The following code shows how to calculate the Hamming distance between two vectors that each contain several character values:
<b>#create vectors
x &lt;- c('a', 'b', 'c', 'd')
y &lt;- c('a', 'b', 'c', 'r')
#find Hamming distance between vectors
sum(x != y)
[1] 3
</b>
The Hamming distance between the two vectors is <b>1</b>.
<h2><span class="orange">How to Calculate Hamming Distance in Python (With Examples)</span></h2>
The <b>Hamming distance</b> between two vectors is simply the sum of corresponding elements that differ between the vectors.
For example, suppose we have the following two vectors:
<b>x = [1, 2, 3, 4]
y = [1, 2, 5, 7]
</b>
The Hamming distance between the two vectors would be <b>2</b>, since this is the total number of corresponding elements that have different values.
To calculate the Hamming distance between two arrays in Python we can use the  hamming()  function from the scipy.spatial.distance library, which uses the following syntax:
<b>scipy.spatial.distance.hamming(array1, array2)</b>
Note that this function returns the <b>percentage</b> of corresponding elements that differ between the two arrays.
Thus, to obtain the Hamming distance we can simply multiply by the length of one of the arrays:
<b>scipy.spatial.distance.hamming(array1, array2) * len(array1)</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Hamming Distance Between Binary Arrays</h3>
The following code shows how to calculate the Hamming distance between two arrays that each contain only two possible values:
<b>from scipy.spatial.distance import hamming
#define arrays
x = [0, 1, 1, 1, 0, 1]
y = [0, 0, 1, 1, 0, 0]
#calculate Hamming distance between the two arrays
hamming(x, y) * len(x)
2.0
</b>
The Hamming distance between the two arrays is <b>2</b>.
<h3>Example 2: Hamming Distance Between Numerical Arrays</h3>
The following code shows how to calculate the Hamming distance between two arrays that each contain several numerical values:
<b>from scipy.spatial.distance import hamming
#define arrays
x = [7, 12, 14, 19, 22]
y = [7, 12, 16, 26, 27]
#calculate Hamming distance between the two arrays
hamming(x, y) * len(x)
3.0</b>
The Hamming distance between the two arrays is <b>3</b>.
<h3>Example 3: Hamming Distance Between String Arrays</h3>
The following code shows how to calculate the Hamming distance between two arrays that each contain several character values:
<b>from scipy.spatial.distance import hamming 
#define arrays
x = ['a', 'b', 'c', 'd']
y = ['a', 'b', 'c', 'r']
#calculate Hamming distance between the two arrays
hamming(x, y) * len(x)
1.0
</b>
The Hamming distance between the two arrays is <b>1</b>.
<h2><span class="orange">How to Easily Create Heatmaps in Python</span></h2>
Suppose we have the following dataset in Python that displays the number of sales a certain shop makes during each weekday for five weeks:
<b>import numpy as np
import pandas as pd 
import seaborn as sns
#create a dataset
np.random.seed(0)
data = {'day': np.tile(['Mon', 'Tue', 'Wed', 'Thur', 'Fri'], 5),
        'week': np.repeat([1, 2, 3, 4, 5], 5),
        'sales': np.random.randint(0, 50, size=25)
        }
df = pd.DataFrame(data,columns=['day','week','sales'])
df = df.pivot('day', 'week', 'sales')
view first ten rows of dataset
df[:10]
week 1 2 3 4 5
day
Fri 336124613
Mon443923 124
Thur 321242325
Tue47 9 63817
Wed 019243937
</b>
<h3>Create Basic Heatmap</h3>
We can create a basic heatmap using the s<b>ns.heatmap()</b> function:
<b>sns.heatmap(df)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python1.png">
The colorbar on the righthand side displays a legend for what values the various colors represent. 
<h3>Add Lines to Heatmap</h3>
You can add lines between the squares in the heatmap using the <b>linewidths </b>argument:
<b>sns.heatmap(df, linewidths=.5)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python2.png">
<h3>Add Annotations to Heatmap</h3>
You can also add annotations to the heatmap using the <b>annot=True </b>argument:
<b>sns.heatmap(df, linewidths=.5, annot=True)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python3.png">
<h3>Hide Colorbar from Heatmap</h3>
You can also hide the colorbar entirely using the <b>cbar=False </b>option:
<b>sns.heatmap(df, linewidths=.5, annot=True, cbar=False)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python4.png">
<h3>Change Color Theme of Heatmap</h3>
You can also change the color theme using the <b>cmap </b>argument. For example, you could set the colors to range from yellow to green to blue:
<b>sns.heatmap(df, cmap='YlGnBu')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python5.png">
Or you could have the colors range from red to blue:
<b>sns.heatmap(df, cmap='RdBu')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/heatmap_python6.png">
<em>For a complete list of colormaps, refer to the  matplotlib documentation .</em>
<h2><span class="orange">How to Create a Heatmap in R Using ggplot2</span></h2>
This tutorial explains how to create a heatmap in R using ggplot2.
<h3>Example: Creating a Heatmap in R</h3>
To create a heatmap, we’ll use the built-in R dataset <b>mtcars</b>.
<b>#view first six rows of <em>mtcars
</em>head(mtcars)
#                   mpg cyl disp  hp drat    wt  qsec vs am gear carb
#Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
Currently <b>mtcars</b> is in a wide format, but we need to melt it into a long format in order to create the heatmap.
<b>#load <em>reshape2 </em>package to use melt() function
library(reshape2)
#melt mtcars into long format
melt_mtcars &lt;- melt(mtcars)
#add column for car name
melt_mtcars$car &lt;- rep(row.names(mtcars), 11)
#view first six rows of <em>melt_mtcars</em>
head(melt_mtcars)
#  variable value               car
#1      mpg  21.0         Mazda RX4
#2      mpg  21.0     Mazda RX4 Wag
#3      mpg  22.8        Datsun 710
#4      mpg  21.4    Hornet 4 Drive
#5      mpg  18.7 Hornet Sportabout
#6      mpg  18.1           Valiant</b>
We can use the following code to create the heatmap in ggplot2:
<b>library(ggplot2)
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = value), colour = "white") +
  scale_fill_gradient(low = "white", high = "red")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap1.jpg">
Unfortunately, since the values for <em>disp </em>are much larger than the values for all the other variables in the data frame, it’s hard to see the color variation for the other variables.
One way to solve this problem is to rescale the values for each variable from 0 to 1 using the <b>rescale()</b> function in the scales() package and the <b>ddply()</b> function in the plyr() package:
<b>#load libraries
library(plyr)
library(scales)
#rescale values for all variables in melted data frame
melt_mtcars &lt;- ddply(melt_mtcars, .(variable), transform, rescale = rescale(value))
#create heatmap using rescaled values
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = rescale), colour = "white") +
  scale_fill_gradient(low = "white", high = "red")
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap2.jpg">
We can also change up the colors of the heatmap by changing the colors used in the scale_fill_gradient() argument:
<b>#create heatmap using blue color scale
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = rescale), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap3.jpg">
Note that the heatmap is currently ordered by car name. We could instead order the heatmap according to the values of one of the variables like <em>mpg</em> using the following code:
<b>#define car name as a new column, then order by <em>mpg</em> descending
mtcars$car &lt;- row.names(mtcars)
mtcars$car &lt;- with(mtcars, reorder(car, mpg))
#melt mtcars into long format
melt_mtcars &lt;- melt(mtcars)
#rescale values for all variables in melted data frame
melt_mtcars &lt;- ddply(melt_mtcars, .(variable), transform, rescale = rescale(value))
#create heatmap using rescaled values
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = rescale), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap4.jpg">
To order the heatmap by <em>mpg </em>ascending, we simply need to use <b>-mpg </b>in the reorder() argument:
<b>#define car name as a new column, then order by mpg descending
mtcars$car &lt;- row.names(mtcars)
mtcars$car &lt;- with(mtcars, reorder(car, -mpg))
#melt mtcars into long format
melt_mtcars &lt;- melt(mtcars)
#rescale values for all variables in melted data frame
melt_mtcars &lt;- ddply(melt_mtcars, .(variable), transform, rescale = rescale(value))
#create heatmap using rescaled values
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = rescale), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap5.jpg">
Lastly, we can remove the x-axis and y-axis labels along with the legend if we don’t like how it looks using the labs() and theme() arguments:
<b>#create heatmap with no axis labels or legend
ggplot(melt_mtcars, aes(variable, car)) +
  geom_tile(aes(fill = rescale), colour = "white") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(x = "", y = "") +
  theme(legend.position = "none")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/heatmap6.jpg">
<h2><span class="orange">Hedges’ g Calculator</span></h2>
<b> Hedges’ g </b> is a way to measure  effect size , which gives us an idea of how much two groups differ. It is used as an alternative to Cohen’s D when the sample sizes between two groups is not equal.
To calculate Hedges’ g, simply fill in the information below and then click the “Calculate” button.
<label><b>x<sub>1</sub></b> (sample 1 mean)</label>
<input type="number" id="x1" value="15.2">
<label><b>s<sub>1</sub></b> (sample 1 standard deviation)</label>
<input type="number" id="s1" value="4.4">
<label><b>n<sub>1</sub></b> (sample 1 size)</label>
<input type="number" id="n1" value="39">
<label><b>x<sub>2</sub></b> (sample 2 mean)</label>
<input type="number" id="x2" value="14">
<label><b>s<sub>2</sub></b> (sample 2 standard deviation)</label>
<input type="number" id="s2" value="3.6">
<label><b>n<sub>2</sub></b> (sample 2 size)</label>
<input type="number" id="n2" value="34">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Hedges’ g: 0.296451
<script>
function calc() {
//get input values
var x1 = +document.getElementById('x1').value;
var s1 = +document.getElementById('s1').value;
var n1 = +document.getElementById('n1').value;
var x2 = +document.getElementById('x2').value;
var s2 = +document.getElementById('s2').value;
var n2 = +document.getElementById('n2').value;
//calculate stuff
var diff = Math.abs(x1-x2);
var pool = Math.sqrt(((n1-1)*s1*s1 - (-1*((n2-1)*s2*s2)))/(n1-(-1*n2)-2));
var g = diff/pool;
//output probabilities
document.getElementById('g').innerHTML = g.toFixed(6);
}
</script>
<h2><span class="orange">What is Hedges’ g? (Definition & Example)</span></h2>
In  hypothesis testing , we often use  p-values  to determine if there is a statistically significant difference between two groups.
However, while a p-value can tell us whether or not there is a statistically significant difference between two groups, an  effect size  can tell us <em>the size</em> of this difference.
One of the most common ways to measure effect size is to use <b>Hedges’ g</b>, which is calculated as follows:
<b>g = (x<sub>1</sub> – x<sub>2</sub>) / √((n<sub>1</sub>-1)*s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)*s<sub>2</sub><sup>2</sup>) / (n<sub>1</sub>+n<sub>2</sub>-2)</b>
where:
x<sub>1</sub>, x<sub>2</sub>: The sample 1 mean and sample 2 mean, respectively
n<sub>1</sub>, n<sub>2</sub>: The sample 1 size and sample 2 size, respectively
s<sub>1</sub><sup>2</sup>, s<sub>2</sub><sup>2</sup>: The sample 1 variance and sample 2 variance, respectively
The following example shows how to calculate Hedges’ g for two samples.
<h3>Example: Calculating Hedge’s g</h3>
Suppose we have the following two samples:
<b>Sample 1:</b>
x<sub>1</sub>: 15.2
s<sub>1</sub>: 4.4
n<sub>1</sub>: 39
<b>Sample 2:</b>
x<sub>2</sub>: 14
s<sub>2</sub>: 3.6
n<sub>2</sub>: 34
Here is how to calculate Hedges’ g for these two samples:
g = (x<sub>1</sub> – x<sub>2</sub>) / √((n<sub>1</sub>-1)*s<sub>1</sub><sup>2</sup> + (n<sub>2</sub>-1)*s<sub>2</sub><sup>2</sup>) / (n<sub>1</sub>+n<sub>2</sub>-2)
g = (15.2 – 14) / √((39-1)*4.4<sup>2</sup> + (34-1)*3.6<sup>2</sup>) / (39+34-2)
g = 1.2 / 4.04788
g = 0.29851
Hedges’ g turns out to be <b>0.29851</b>.
<b>Bonus:</b> Use this  online calculator  to automatically calculate Hedges’ g for any two samples.
<h3>How to Interpret Hedges’ g</h3>
As a rule of thumb, here is how to interpret Hedge’s g:
<b>0.2 </b>= Small effect size
<b>0.5</b> = Medium effect size
<b>0.8 </b>= Large effect size
In our example, an effect size of <b>0.29851</b> would likely be considered a small effect size. This means that even if the difference between the two group means is statistically significant, the actual difference between the group means is trivial.
<h3>Hedges’ g vs. Cohen’s d</h3>
Another common way to measure effect size is known as  Cohen’s d , which uses the following formula:
<b>d = (x<sub>1</sub> – x<sub>2</sub>) / √(s<sub>1</sub><sup>2</sup> + s<sub>2</sub><sup>2</sup>) / 2</b>
The only difference between Cohen’s d and Hedges’ g is that Hedges’ g takes each sample size into consideration when calculating the overall effect size.
Thus, it’s recommended to use Hedge’s g to calculate effect size when the two sample sizes are not equal.
If the two sample sizes are equal then Hedges’ g and Cohen’s d will be the exact same value.
<h2><span class="orange">Here is How to Find the P-Value from the F-Distribution Table</span></h2>
The <b>F distribution table </b>is a table that shows the critical values of the F distribution. To use the F distribution table, you only need three values:
The numerator degrees of freedom
The denominator degrees of freedom
The alpha level
The F distribution is used most commonly in an Analysis of Variance, or <em>ANOVA</em> for short. For example, here is what the output table for an ANOVA might look like:
<table>
<thead><tr>
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
<th>P</th>
</tr></thead>
<tbody>
<tr>
<td>Treatment</td>
<td>58.8</td>
<td>2</td>
<td>29.4</td>
<td>1.74</td>
<td>0.217</td>
</tr>
<tr>
<td>Error</td>
<td>202.8</td>
<td>12</td>
<td>16.9</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Total</td>
<td>261.6</td>
<td>14</td>
</tr>
</tbody>
</table>
In this example, the numerator degrees of freedom for the F statistic is <b>2</b>, the denominator degrees of freedom for the F statistic is <b>12</b>, and the F statistic itself is <b>1.74</b>. Suppose the alpha level we are using is 0.10. In the table above, we see that the p-value for this F statistic is 0.217. Since 0.217 is greater than the alpha level, we would conclude that this F statistic is not statistically significant.
If we instead wanted to use the  F Distribution Table , we would use the F Distribution Table for alpha = 0.10. We would locate the critical value in the table that corresponds to a numerator degrees of freedom of 2 (DF1 = 2 in the table) and a denominator degrees of freedom of 12 (DF2 = 12 in the table) and find that this value is <b>2.8068</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/f1.png">
Since our F statistic of <b>1.74</b> from the ANOVA table is not greater than the F critical value of <b>2.8068</b> from the F Distribution table, we would conclude that the F statistic is not significant at the alpha level of 0.10.
<h2>The F Distribution Table Provides Critical Values, Not P-Values</h2>
Notice in the example above that the F Distribution Table simply gives us an F critical value to compare our F statistic to. The F Distribution Table does not directly give us a p-value.
<b>If you have an F statistic with a numerator degrees of freedom and denominator degrees of freedom and you would like to find the p-value for it, then you would need to use an  F Distribution Calculator .</b>
For example, suppose we knew that our F statistic was 1.74, the numerator degrees of freedom was 2, and the denominator degrees of freedom was 12 and we wanted to find the p-value for this F statistic. In this case, we would enter the following numbers into the F Distribution Calculator:
<em>Note: Leave the last box blank. The calculator will automatically find this value for you.</em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/F_to_p.jpg">
This tells us that the cumulative probability is 0.78300. This is the area to the left of the F statistic in the F distribution. Typically we’re interested in the area to the right of the F statistic, so in this case the p-value would be 1 – 0.78300 = 0.217.
<h2>When to Use the F Distribution Table</h2>
If you are interested in finding the F critical value for a given numerator degrees of freedom, denominator degrees of freedom, and alpha level, then you should use the F distribution table.
Instead, if you have a given F statistic (from an ANOVA or some other statistical test) with a given numerator degrees of freedom and denominator degrees of freedom and you simply want to know the p-value of that F statistic, then you would need to use an  F Distribution Calculator  to do so.
<h2><span class="orange">Here is How to Find the P-Value from the t-Distribution Table</span></h2>
The <b>t distribution table </b>is a table that shows the critical values of the t distribution. To use the t distribution table, you only need three values:
A significance level (common choices are 0.01, 0.05, and 0.10)
The degrees of freedom
The type of test (one-tailed or two-tailed)
<figure style="width: 940px"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/t_dist.png"><figcaption>t distribution table</figcaption></figure>The t distribution table is commonly used in the following hypothesis tests:
A hypothesis test for a mean
A hypothesis test  for a difference in means
A hypothesis test for a difference in paired means
When you conduct each of these tests, you’ll end up with a test statistic <em>t</em>. To find out if this test statistic is statistically significant at some alpha level, you have two options:
Compare the test statistic <em>t </em>to a critical value from the t distribution table.
Compare the p-value of the test statistic <em>t </em>to a chosen alpha level.
Let’s walk through an example of how to use each of these approaches.
<h2>Examples</h2>
Suppose we conduct a two-sided hypothesis test at alpha level <b>0.05</b> to find out if mean weight loss differs between two diets. Suppose our test statistic <em>t </em>is <b>1.34</b> and our degrees of freedom is <b>22</b>. We would like to know if these results are statistically significant.
<h3>Compare the test statistic <em>t </em>to a critical value</h3>
The first approach we can use to determine if our results are statistically significant is to compare the test statistic <em>t </em>of <b>1.34 </b>to the critical value in the t distribution table. The critical value is the value in the table that aligns with a two-tailed value of <b>0.05 </b>and a degrees of freedom of <b>22</b>. This number turns out to be <b>2.074</b>:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/tTable1.jpg">
Since out test statistic <em>t </em>(<b>1.34</b>) is smaller than the critical value (<b>2.074</b>), we fail to reject the null hypothesis of our test. We do not have sufficient evidence to say that the mean weight loss between the two diets is statistically significant at alpha level 0.05.
<h3>Compare the p-value to a chosen alpha level</h3>
The second approach we can use to determine if our results are statistically significant is to find the p-value for the test statistic <em>t </em>of <b>1.34</b>. In order to find this p-value, <b>we can’t use the t distribution table because it only provides us with critical values, not p-values</b>.
So, in order to find this p-value we need to use a  T Score to P Value Calculator  with the following inputs:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/tTable2.jpg"439">
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/tTable3.jpg"338">
The p-value for a test statistic <em>t </em>of <b>1.34 </b>for a two-tailed test with <b>22 </b>degrees of freedom is <b>0.19392</b>. Since this number is greater than our alpha level of <b>0.05</b>, we fail to reject the null hypothesis of our test. We do not have sufficient evidence to say that the mean weight loss between the two diets is statistically significant at alpha level 0.05.
<h2>When to Use the t Distribution Table</h2>
If you are interested in finding the t critical value for a given significance level, degrees of freedom, and type of test (one-tailed or two-tailed), then you should use the  t distribution table .
Instead, if you have a given test statistic <em>t</em> and you simply want to know the p-value of that test statistic, then you would need to use a  T Score to P Value Calculator  to do so.
<h2><span class="orange">Here is How to Interpret a P-Value of 0.000</span></h2>
When you run a statistical test, whether it’s a chi-square test, a test for a population mean, a test for a population proportion, a linear regression, or any other test, you’re often interested in the resulting p-value from that test.
A p-value simply tells you the strength of evidence in support of a null hypothesis.
If the p-value is less than the significance level, we reject the null hypothesis.
So, when you get a p-value of 0.000, you should compare it to the significance level. Common significance levels include 0.1, 0.05, and 0.01.
Since 0.000 is lower than all of these significance levels, we would reject the null hypothesis in each case.
Let’s walk through an example to clear things up.
<h2>Example: Getting a P-Value of 0.000</h2>
A factory claims that they produce tires that each weigh 200 pounds.
An auditor comes in and tests the null hypothesis that the mean weight of a tire is 200 pounds against the alternative hypothesis that the mean weight of a tire is not 200 pounds, using a 0.05 level of significance.
<b>The null hypothesis (H0):</b> μ = 200
<b>The alternative hypothesis: (Ha):</b> μ ≠ 200
Upon conducting a hypothesis test for a mean, the auditor gets a p-value of 0.000.
Since the p-value of 0.000 is less than the significance level of 0.05, the auditor rejects the null hypothesis.
Thus, he concludes that there is sufficient evidence to say that the true average weight of a tire is not 200 pounds.
<h2>What a P-Value of 0.000 Means</h2>
Whether you use Microsoft Excel, a TI-84 calculator, SPSS, or some other software to compute the p-value of a statistical test, often times the p-value is not exactly 0.000, but rather something extremely small like 0.000000000023.
Most software only display three decimal places, though, which is why the p-value shows up as 0.000.
<h2>Conclusion</h2>
If you conduct a statistical test using a significance level of 0.1, 0.05, or 0.01 (or any significance level greater than 0.000) and get a p-value of 0.000, then reject the null hypothesis.
<b>Related</b>  An Explanation of P-Values and Statistical Significance 
<h2><span class="orange">Understanding Heteroscedasticity in Regression Analysis</span></h2>
In regression analysis, <b>heteroscedasticity </b>(sometimes spelled heteroskedasticity) refers to the unequal scatter of residuals or error terms. Specfically, it refers to the case where there is a systematic change in the spread of the residuals over the range of measured values.
Heteroscedasticity is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has <em>homoscedasticity</em>, which means constant variance.
When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this.
This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.
This tutorial explains how to detect heteroscedasticity, what causes heteroscedasticity, and potential ways to fix the problem of heteroscedasticity.
<h2>How to Detect Heteroscedasticity</h2>
The simplest way to detect heteroscedasticity is with a <em>fitted value vs. residual plot</em>. 
Once you fit a regression line to a set of data, you can then create a scatterplot that shows the fitted values of the model vs. the residuals of those fitted values.
The scatterplot below shows a typical <em>fitted value vs. residual plot</em> in which heteroscedasticity is present.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/het1.jpg" alt="">
Notice how the residuals become much more spread out as the fitted values get larger. This “cone” shape is a telltale sign of heteroscedasticity.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/het2.jpg" alt="">
<h2>What Causes Heteroscedasticity?</h2>
Heteroscedasticity occurs naturally in datasets where there is a large range of observed data values. For example:
Consider a dataset that includes the annual income and expenses of 100,000 people across the United States. For individuals with lower incomes, there will be lower variability in the corresponding expenses since these individuals likely only have enough money to pay for the necessities. For individuals with higher incomes, there will be higher variability in the corresponding expenses since these individuals have more money to spend if they choose to. Some higher-income individuals will choose to spend most of their income, while some may choose to be frugal and only spend a portion of their income, which is why the variability in expenses among these higher-income individuals will inherently be higher.
Consider a dataset that includes the populations and the count of flower shops in 1,000 different cities across the United States. For cities with small populations, it may be common for only one or two flower shops to be present. But in cities with larger populations, there will be a much greater variability in the number of flower shops. These cities may have anywhere between 10 to 100 shops. This means when we create a regression analysis and use population to predict number of flower shops, there will inherently be greater variability in the residuals for the cities with higher populations.
Some datasets are simply more prone to heteroscedasticity than others.
<h2>How to Fix Heteroscedasticity</h2>
There are three common ways to fix heteroscedasticity:
<h3>1. Transform the dependent variable</h3>
One way to fix heteroscedasticity is to transform the dependent variable in some way. One common transformation is to simply take the log of the dependent variable.
For example, if we are using population size (independent variable) to predict the number of flower shops in a city (dependent variable), we may instead try to use population size to predict the log of the number of flower shops in a city.
Using the log of the dependent variable, rather than the original dependent variable, often causes heteroskedasticity to go away.
<h3>2. Redefine the dependent variable</h3>
Another way to fix heteroscedasticity is to redefine the dependent variable. One common way to do so is to use a <em>rate</em> for the dependent variable, rather than the raw value.
For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita.
In most cases, this reduces the variability that naturally occurs among larger populations since we’re measuring the number of flower shops per person, rather than the sheer amount of flower shops.
<h3>3. Use weighted regression</h3>
Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value.
Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
<h2>Conclusion</h2>
Heteroscedasticity is a fairly common problem when it comes to regression analysis because so many datasets are inherently prone to non-constant variance.
However, by using a <em>fitted value vs. residual plot</em>, it can be fairly easy to spot heteroscedasticity.
And through transforming the dependent variable, redefining the dependent variable, or using weighted regression, the problem of heteroscedasticity can often be eliminated.
<h2><span class="orange">Hierarchical Clustering in R: Step-by-Step Example</span></h2>
Clustering is a technique in machine learning that attempts to find groups or <em>clusters</em> of  observations  within a dataset such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.
Clustering is a form of  unsupervised learning  because we’re simply attempting to find structure within a dataset rather than predicting the value of some  response variable .
Clustering is often used in marketing when companies have access to information like:
Household income
Household size
Head of household Occupation
Distance from nearest urban area
When this information is available, clustering can be used to identify households that are similar and may be more likely to purchase certain products or respond better to a certain type of advertising.
One of the most common forms of clustering is known as  k-means clustering . Unfortunately this method requires us to pre-specify the number of clusters <em>K</em>.
An alternative to this method is known as <b>hierarchical clustering</b>, which does not require us to pre-specify the number of clusters to be used and is also able to produce a tree-based representation of the observations known as a <em>dendrogram</em>.
<h3>What is Hierarchical Clustering?</h3>
Similar to k-means clustering, the goal of hierarchical clustering is to produce clusters of observations that are quite similar to each other while the observations in different clusters are quite different from each other.
In practice, we use the following steps to perform hierarchical clustering:
<b>1. Calculate the pairwise dissimilarity between each observation in the dataset.</b>
First, we must choose some distance metric – like the  Euclidean distance  – and use this metric to compute the dissimilarity between each observation in the dataset.
For a dataset with <em>n</em> observations, there will be a total of <em>n</em>(<em>n</em>-1)/2 pairwise dissimilarities.
<b>2. Fuse observations into clusters.</b>
At each step in the algorithm, fuse together the two observations that are most similar into a single cluster.
Repeat this procedure until all observations are members of one large cluster. The end result is a tree, which can be plotted as a <em>dendrogram</em>.
To determine how close together two clusters are, we can use a few different methods including:
<b>Complete linkage clustering:</b> Find the max distance between points belonging to two different clusters.
<b>Single linkage clustering:</b> Find the minimum distance between points belonging to two different clusters.
<b>Mean linkage clustering:</b> Find all pairwise distances between points belonging to two different clusters and then calculate the average.
<b>Centroid linkage clustering:</b> Find the centroid of each cluster and calculate the distance between the centroids of two different clusters.
<b>Ward’s minimum variance method:</b> Minimize the total 
Depending on the structure of the dataset, one of these methods may tend to produce better (i.e. more compact) clusters than the other methods.
<h3>Hierarchical Clustering in R</h3>
The following tutorial provides a step-by-step example of how to perform hierarchical clustering in R.
<h3>Step 1: Load the Necessary Packages</h3>
First, we’ll load two packages that contain several useful functions for hierarchical clustering in R.
<b>library(factoextra)
library(cluster)</b>
<h3>Step 2: Load and Prep the Data</h3>
For this example we’ll use the <em>USArrests</em> dataset built into R, which contains the number of arrests per 100,000 residents in each U.S. state in 1973 for <em>Murder</em>, <em>Assault</em>, and <em>Rape </em>along with the percentage of the population in each state living in urban areas, <em>UrbanPop</em>.
The following code shows how to do the following:
Load the <em>USArrests</em> dataset
Remove any rows with missing values
Scale each variable in the dataset to have a mean of 0 and a standard deviation of 1
<b>#load data
df &lt;- USArrests
#remove rows with missing values</b>
<b>df &lt;- na.omit(df)
#scale each variable to have a mean of 0 and sd of 1</b>
<b>df &lt;- scale(df)
#view first six rows of dataset
head(df)
               Murder   Assault   UrbanPop         Rape
Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
Arizona    0.07163341 1.4788032  0.9989801  1.042878388
Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
California 0.27826823 1.2628144  1.7589234  2.067820292
Colorado   0.02571456 0.3988593  0.8608085  1.864967207
</b>
<h3>Step 3: Find the Linkage Method to Use</h3>
To perform hierarchical clustering in R we can use the <b>agnes()</b> function from the <em>cluster</em> package, which uses the following syntax:
<b>agnes(data, method)</b>
where:
<b>data:</b> Name of the dataset.
<b>method:</b> The method to use to calculate dissimilarity between clusters.
Since we don’t know beforehand which method will produce the best clusters, we can write a short function to perform hierarchical clustering using several different methods.
Note that this function calculates the agglomerative coefficient of each method, which is metric that measures the strength of the clusters. The closer this value is to 1, the stronger the clusters.
<b>#define linkage methods
m &lt;- c( "average", "single", "complete", "ward")
names(m) &lt;- c( "average", "single", "complete", "ward")
#function to compute agglomerative coefficient
ac &lt;- function(x) {
  agnes(df, method = x)$ac
}
#calculate agglomerative coefficient for each clustering linkage method
sapply(m, ac)
  average    single  complete      ward 
0.7379371 0.6276128 0.8531583 0.9346210 
</b>
We can see that Ward’s minimum variance method produces the highest agglomerative coefficient, thus we’ll use that as the method for our final hierarchical clustering:
<b>#perform hierarchical clustering using Ward's minimum variance
clust &lt;- agnes(df, method = "ward")
#produce dendrogram
pltree(clust, cex = 0.6, hang = -1, main = "Dendrogram") </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/hclust1.png">
Each leaf at the bottom of the dendrogram represents an observation in the original dataset. As we move up the dendrogram from the bottom, observations that are similar to each other are fused together into a branch.
<h3>Step 4: Determine the Optimal Number of Clusters</h3>
To determine how many clusters the observations should be grouped in, we can use a metric known as the  gap statistic , which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.
We can calculate the gap statistic for each number of clusters using the <b>clusGap()</b> function from the <em>cluster</em> package along with a plot of clusters vs. gap statistic using the <b>fviz_gap_stat()</b> function:
<b>#calculate gap statistic for each number of clusters (up to 10 clusters)
gap_stat &lt;- clusGap(df, FUN = hcut, nstart = 25, K.max = 10, B = 50)
#produce plot of clusters vs. gap statistic
fviz_gap_stat(gap_stat)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/hclust2.png">
From the plot we can see that the gap statistic is highest at k = 4 clusters. Thus, we’ll choose to group our observations into 4 distinct clusters.
<h3>Step 5: Apply Cluster Labels to Original Dataset</h3>
To actually add cluster labels to each observation in our dataset, we can use the <b>cutree()</b> method to cut the dendrogram into 4 clusters:
<b>#compute distance matrix
d &lt;- dist(df, method = "euclidean")
#perform hierarchical clustering using Ward's method
final_clust &lt;- hclust(d, method = "ward.D2" )
#cut the dendrogram into 4 clusters
groups &lt;- cutree(final_clust, k=4)
#find number of observations in each cluster
table(groups)
 1  2  3  4 
 7 12 19 12</b>
We can then append the cluster labels  of each state back to the original dataset:
<b>#append cluster labels to original data
final_data &lt;- cbind(USArrests, cluster = groups)
#display first six rows of final data
head(final_data)
               Murder Assault UrbanPop Rape cluster
Alabama          13.2     236       58 21.2       1
Alaska           10.0     263       48 44.5       2
Arizona           8.1     294       80 31.0       2
Arkansas          8.8     190       50 19.5       3
California        9.0     276       91 40.6       2
Colorado          7.9     204       78 38.7       2
</b>
Lastly, we can use the <b>aggregate()</b> function to find the mean of the variables in each cluster:
<b>#find mean values for each cluster
aggregate(final_data, by=list(cluster=final_data$cluster), mean)
  cluster    Murder  Assault UrbanPop     Rape cluster
1       1 14.671429 251.2857 54.28571 21.68571       1
2       2 10.966667 264.0000 76.50000 33.60833       2
3       3  6.210526 142.0526 71.26316 19.18421       3
4       4  3.091667  76.0000 52.08333 11.83333       4
</b>
We interpret this output is as follows:
The mean number of murders per 100,000 citizens among the states in cluster 1 is <b>14.67</b>.
The mean number of assaults per 100,000 citizens among the states in cluster 1 is <b>251.28</b>.
The mean percentage of residents living in an urban area among the states in cluster 1 is <b>54.28%</b>.
The mean number of rapes per 100,000 citizens among the states in cluster 1 is <b>21.68.</b>
You can find the complete R code used in this example  here .
<h2><span class="orange">How to Perform Hierarchical Regression in Stata</span></h2>
<b>Hierarchical regression </b>is a technique we can use to compare several different linear models.
The basic idea is that we first fit a linear regression model with just one explanatory variable. Then we fit another regression model using an additional explanatory variable. If the R-squared (the proportion of variance in the response variable that can be explained by the explanatory variables) in the second model is significantly higher than the R-squared in the previous model, this means the second model is better.
We then repeat the process of fitting additional regression models with more explanatory variables and seeing if the newer models offer any improvement over the previous models.
This tutorial provides an example of how to perform hierarchical regression in Stata.
<h3>Example: Hierarchical Regression in Stata</h3>
We’ll use a built-in dataset called <em>auto </em>to illustrate how to perform hierarchical regression in Stata. First, load the dataset by typing the following into the Command box:
<b>sysuse auto</b>
We can get a quick summary of the data by using the following command:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata1.png">
We can see that the dataset contains information about 12 different variables for 74 total cars.
We will fit the following three linear regression models and use hierarchical regression to see if each subsequent model provides a significant improvement to the previous model or not:
<b>Model 1:</b> price = intercept + mpg
<b>Model 2:</b> price = intercept + mpg + weight
<b>Model 3:</b> price = intercept + mpg + weight + gear ratio
In order to perform hierarchical regression in Stata, we will first need to install the <b>hireg </b>package. To do so, type the following into the Command box:
<b>findit hireg</b>
In the window that pops up, click <b>hireg from http://fmwww.bc.edu/RePEc/bocode/h</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata2.png">
In the next window, click the link that says <b>click here to install</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata3.png">
The package will install in a matter of seconds. Next, to perform hierarchical regression we will use the following command:
<b>hireg price (mpg) (weight) (gear_ratio)</b>
Here is what this tells Stata to do:
Perform hierarchical regression using <b>price </b>as the response variable in each model.
For the first model, use <b>mpg </b>as the explanatory variable.
For the second model, add in <b>weight </b>as an additional explanatory variable.
For the third model, add in <b>gear_ratio </b>as another explanatory variable.
Here is the output of the first model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata4.png">
We see that the R-squared of the model is <b>0.2196 </b>and the overall p-value (Prob > F) for the model is <b>0.0000</b>, which is statistically significant at α = 0.05.
Next, we see the output of the second model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata5.png">
The R-squared of this model is <b>0.2934</b>, which is larger than the first model. To determine if this difference is statistically significant, Stata performed an F-test which resulted in the following numbers at the bottom of the output:
R-squared difference between the two models = <b>0.074</b>
F-statistic for the difference = <b>7.416</b>
Corresponding p-value of the F-statistic = <b>0.008</b>
Because the p-value is less than 0.05, we conclude that there is a statistically significant improvement in the second model compared to the first model.
Lastly, we can see the output of the third model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata6.png">
The R-squared of this model is <b>0.3150</b>, which is larger than the second model. To determine if this difference is statistically significant, Stata performed an F-test which resulted in the following numbers at the bottom of the output:
R-squared difference between the two models = <b>0.022</b>
F-statistic for the difference = <b>2.206</b>
Corresponding p-value of the F-statistic = <b>0.142</b>
Because the p-value is not less than 0.05, we don’t have sufficient evidence to say that the third model offers any improvement over the second model.
At the very end of the output we can see that Stata provides a summary of the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/hierArchStata7.png">
In this particular example, we would conclude that model 2 offered a significant improvement over model 1, but model 3 did not offer a significant improvement over model 2.
<h2><span class="orange">What is High Dimensional Data? (Definition & Examples)</span></h2>
<b>High dimensional data</b> refers to a dataset in which the number of features <em>p</em> is larger than the number of  observations  <em>N</em>, often written as <em>p</em> >> <em>N</em>.
For example, a dataset that has <em>p</em> = 6 features and only <em>N</em> = 3 observations would be considered high dimensional data because the number of features is larger than the number of observations.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/highDim1.png">
One common mistake people make is assuming that “high dimensional data” simply means a dataset that has a lot of features. However, that’s incorrect. A dataset could have 10,000 features, but if it has 100,000 observations then it’s not high dimensional.
<em><b>Note:</b> Refer to Chapter 18 in  The Elements of Statistical Learning  for a deep dive into the math underlying high dimensional data.</em>
<h3>Why is High Dimensional Data a Problem?</h3>
When the number of features in a dataset exceeds the number of observations, we will never have a deterministic answer.
In other words, it becomes impossible to find a model that can describe the relationship between the predictor variables and the  response variable  because we don’t have enough observations to train the model on.
<h3>Examples of High Dimensional Data</h3>
The following examples illustrate high dimensional datasets in different fields.
<b>Example 1: Healthcare Data</b>
High dimensional data is common in healthcare datasets where the number of features for a given individual can be massive (i.e. blood pressure, resting heart rate, immune system status, surgery history, height, weight, existing conditions, etc.).
In these datasets, it’s common for the number of features to be larger than the number of observations.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/highDim2.png">
<b>Example 2: Financial Data</b>
High dimensional data is also common in financial datasets where the number of features for a given stock can be quite large (i.e. PE Ratio, Market Cap, Trading Volume, Dividend Rate, etc.)
In these types of dataset, it’s common for the number of features to be much greater than the number of individual stocks.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/highDim3.png">
<b>Example 3: Genomics</b>
High dimensional data also occurs often in the field of genomics where the number of gene features for a given individual can be massive.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/highDim4.png">
<h3>How to Handle High Dimensional Data</h3>
There are two common ways to deal with high dimensional data:
<b>1. Choose to include fewer features.</b>
The most obvious way to avoid dealing with high dimensional data is to simply include fewer features in the dataset.
There are several ways to decide which features to drop from a dataset, including:
<b>Drop features with many missing values:</b> If a given column in a dataset has a lot of missing values, you may be able to drop it completely without losing much information.
<b>Drop features with low variance:</b> If a given column in a dataset has values that change very little, you may be able to drop it since it’s unlikely to offer as much useful information about a response variable compared to other features.
<b>Drop features with low correlation with the response variable: </b>If a certain feature is not highly correlated with the response variable of interest, you can likely drop it from the dataset since it’s unlikely to be a useful feature in a model.
<b>2. Use a regularization method.</b>
Another way to handle high dimensional data without dropping features from the dataset is to use a regularization technique such as:
 Principal Components Analysis 
 Principal Components Regression 
 Ridge Regression 
 Lasso Regression 
Each of these techniques can be used to effectively deal with high dimensional data.
<em>You can find a complete list of all machine learning tutorials on Statology on  this page .</em>
<h2><span class="orange">How to Estimate the Mean and Median of Any Histogram</span></h2>
A <b>histogram</b> is a chart that helps us visualize the distribution of values in a dataset.
The x-axis of a histogram displays bins of data values and the y-axis tells us how many observations in a dataset fall in each bin.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histogram_mean3.png">
Although histograms are useful for visualizing distributions, it’s not always obvious what the <b>mean</b> and <b>median</b> values are just from looking at the histograms.
And while it’s not possible to find the exact mean and median values of a distribution just from looking at a histogram, it’s possible to estimate both values. This tutorial explains how to do so.
<h3>How to Estimate the Mean of a Histogram</h3>
We can use the following formula to find the best estimate of the mean of any histogram:
<b>Best Estimate of Mean:</b> Σm<sub>i</sub>n<sub>i</sub> / N
where:
<b>m<sub>i</sub>:</b> The  midpoint  of the i<sup>th</sup> bin
<b>n<sub>i</sub>:</b> The frequency of the i<sup>th</sup> bin
<b>N:</b> The total sample size
For example, consider the following histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histogram_mean3.png">
Our best estimate of the mean would be:
Mean = (5.5*2 + 15.5*7 + 25.5*10 + 35.5*3 + 45.5*1) / 23 = <b>22.89</b>.
By looking at the histogram, this seems like a reasonable estimate of the mean.
<h3>How to Estimate the Median of a Histogram</h3>
We can use the following formula to find the best estimate of the median of any histogram:
<b>Best Estimate of Median:</b> L + ( (n/2 – F) / f ) * w
where:
<b>L:</b> The lower limit of the median group
<b>n:</b> The total number of observations
<b>F:</b> The cumulative frequency up to the median group
<b>f:</b> The frequency of the median group
<b>w:</b> The width of the median group
Once again, consider the following histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histogram_mean3.png">
Our best estimate of the median would be:
Median = 21 + ( (25/2 – 9) / 10) * 9 = <b>24.15</b>.
From looking at the histogram, this also seems to be a reasonable estimate of the median.
<b>Related:</b>  How to Estimate the Standard Deviation of Any Histogram 
<h2><span class="orange">How to Find the Mode of a Histogram (With Example)</span></h2>
The <b>mode</b> of a dataset represents the value that occurs most often.
To find the mode in a histogram, we can use the following steps:
<b>1.</b> Identify the tallest bar.
<b>2.</b> Draw a line from the left corner of the tallest bar to the left corner of the bar immediately after it.
<b>3.</b> Draw a line from the right corner of the tallest bar to the right corner of the bar immediately before it.
<b>4.</b> Identify the point where the two lines intersect. Then draw a line straight down to the x-axis. The point where the line hits the x-axis is our best estimate for the mode.
The following step-by-step example shows how to find the mode of the following histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/modehist1.jpg"511">
<h2>Step 1: Identify the Tallest Bar</h2>
First, we need to identify the tallest bar in the histogram.
This is the bar with the bin range of 16 to 20:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/modehist2.jpg"531">
<h2>Step 2: Draw the First Line</h2>
Next, we need to draw a line from the left corner of the tallest bar to the left corner of the bar immediately after it:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/modehist3.jpg"527">
<h2>
<b>Step 3: Draw the Second Line</b>
</h2>
Next, we need to draw a line from the right corner of the tallest bar to the right corner of the bar immediately before it:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/modehist4.jpg"528">
<h2>Step 4: Identify the Point of Intersection</h2>
Next, we need to identify the point where the two lines intersect. Then draw a line straight down to the x-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/modehist5.jpg"527">
The point where the line hits the x-axis is our best estimate for the mode.
In this example, our best estimate for the mode is roughly <b>17</b>.
<b>Note</b>: Since the data in a histogram is grouped into bins, it’s not possible to know the exact value of the mode but the method that we used here allows us to make our best estimate.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks involving histograms:
 How to Estimate the Mean and Median of Any Histogram 
 How to Estimate the Standard Deviation of Any Histogram 
 How to Compare Histograms 
<h2><span class="orange">How to Create a Histogram of Residuals in R</span></h2>
One of the main  assumptions of linear regression  is that the  residuals  are normally distributed.
One way to visually check this assumption is to create a histogram of the residuals and observe whether or not the distribution follows a “bell-shape” reminiscent of the  normal distribution .
This tutorial provides a step-by-step example of how to create a histogram of residuals for a regression model in R.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data to work with:
<b>#make this example reproducible
set.seed(0)
#create data
x1 &lt;- rnorm(n=100, 2, 1)
x2 &lt;- rnorm(100, 4, 3)
y  &lt;- rnorm(100, 2, 3)
data &lt;- data.frame(x1, x2, y)
#view first six rows of data
head(data)
        x1        x2          y
1 3.262954 6.3455776 -1.1371530
2 1.673767 1.6696701 -0.6886338
3 3.329799 2.1520303  5.8081615
4 3.272429 4.1397409  3.7815228
5 2.414641 0.6088427  4.3269030
6 0.460050 5.7301563  6.6721111</b>
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll fit a  multiple linear regression model  to the data:
<b>#fit multiple linear regression model
model &lt;- lm(y ~ x1 + x2, data=data)</b>
<h3>Step 3: Create a Histogram of Residuals</h3>
Lastly, we’ll use the <b>ggplot</b> visualization package to create a histogram of the residuals from the model:
<b>#load ggplot2
library(ggplot2)
#create histogram of residuals
ggplot(data = data, aes(x = model$residuals)) +
    geom_histogram(fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histResid1.png">
Note that we can also specify the number of bins to place the residuals in by using the <b>bin</b> argument.
The fewer the bins, the wider the bars will be in the histogram. For example, we could specify <b>20 bins</b>:
<b>#create histogram of residuals
ggplot(data = data, aes(x = model$residuals)) +
    geom_histogram(bins = 20, fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histResid2.png">
Or we could specify <b>10 bins</b>:
<b>#create histogram of residuals
ggplot(data = data, aes(x = model$residuals)) +
    geom_histogram(bins = 10, fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histResid3.png">
No matter how many bins we specify, we can see that the residuals are roughly normally distributed.
We could also perform a formal statistical test like the Shapiro-Wilk, Kolmogorov-Smirnov, or Jarque-Bera to test for normality.
However, keep in mind that these tests are sensitive to large sample sizes – that is, they often conclude that the residuals are not normal when the sample size is large.
For this reason, it’s often easier to assess normality by creating a histogram of the residuals.
<h2><span class="orange">How to Estimate the Standard Deviation of Any Histogram</span></h2>
A <b>histogram</b> offers a useful way to visualize the distribution of values in a dataset.
The x-axis of a histogram displays bins of data values and the y-axis tells us how many observations in a dataset fall in each bin.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histogram_mean3.png">
Since a histogram places observations in bins, it’s not possible to calculate the exact standard deviation of the dataset represented by the histogram but it’s possible to estimate the standard deviation.
The following example shows how to do so.
<b>Related:</b>  How to Estimate the Mean and Median of Any Histogram 
<h3>How to Estimate the Standard Deviation of a Histogram</h3>
In order to estimate the standard deviation of a histogram, we must first estimate the mean.
We can use the following formula to estimate the mean:
<b>Mean:</b> Σm<sub>i</sub>n<sub>i</sub> / N
where:
<b>m<sub>i</sub>:</b> The midpoint of the i<sup>th</sup> bin
<b>n<sub>i</sub>:</b> The frequency of the i<sup>th</sup> bin
<b>N:</b> The total sample size
For example, suppose we have the following histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/histogram_mean3.png">
Here’s how we would estimate the mean value of this histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/mean_sd_grouped2.png">
We estimate the mean to be <b>22.89</b>. 
<b>Note:</b> The  midpoint  for each group can be found by taking the average of the lower and upper value in the range. For example, the midpoint for the first group is calculated as: (1+10) / 2 = 5.5.
Now that we have an estimate for the mean, we can use the following formula to estimate the standard deviation:
<b>Standard Deviation:</b> √Σn<sub>i</sub>(m<sub>i</sub>-μ)<sup>2</sup> / (N-1)
where:
<b>n<sub>i</sub>:</b> The frequency of the i<sup>th</sup> bin
<b>m<sub>i</sub>:</b> The midpoint of the i<sup>th</sup> bin
<b>μ</b>: The mean
<b>N:</b> The total sample size
Here’s how we would apply this formula to our dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/mean_sd_grouped3-1.png">We estimate that the standard deviation of the dataset is <b>9.6377</b>.
Although this isn’t guaranteed to match the exact standard deviation of the dataset (since we don’t know the  raw data values  of the dataset), it represents our best estimate of the standard deviation.
<h2><span class="orange">How to Create a Histogram of Two Variables in R</span></h2>
A histogram is a useful way to visualize the distribution of values for a given variable.
To create a histogram for one variable in R, you can use the <b>hist()</b> function. And to create a histogram for two variables in R, you can use the following syntax:
<b>hist(variable1, col='red')
hist(variable2, col='blue', add=TRUE)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Create a Histogram of Two Variables in R</h3>
The following code shows how to create a histogram of two variables in R:
<b>#make this example reproducible
set.seed(1)
#define data
x1 = rnorm(1000, mean=0.6, sd=0.1)
x2 = rnorm(1000, mean=0.4, sd=0.1)
#plot two histograms in same graph
hist(x1, col='red')
hist(x2, col='blue', add=TRUE)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multHistR1.png">
Since the values of the histograms overlap, it’s a good idea to use <b>rgb()</b> colors with increased transparency:
<b>#make this example reproducible
set.seed(1)
#define data
x1 = rnorm(1000, mean=0.6, sd=0.1)
x2 = rnorm(1000, mean=0.4, sd=0.1)
#plot two histograms in same graph
hist(x1, col=rgb(0,0,1,0.2), xlim=c(0, 1),
     xlab='Values', ylab='Frequency', main='Histogram for two variables')
hist(x2, col=rgb(1,0,0,0.2), add=TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multHistR2.png">
You can also add a legend to make the histograms easier to interpret:
<b>#make this example reproducible
set.seed(1)
#define data
x1 = rnorm(1000, mean=0.6, sd=0.1)
x2 = rnorm(1000, mean=0.4, sd=0.1)
#plot two histograms in same graph
hist(x1, col=rgb(0,0,1,0.2), xlim=c(0, 1),
     xlab='Values', ylab='Frequency', main='Histogram for two variables')
hist(x2, col=rgb(1,0,0,0.2), add=TRUE)
#add legend
legend('topright', c('Variable 1', 'Variable 2'),
       fill=c(rgb(0,0,1,0.2), rgb(1,0,0,0.2)))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multHistR3.png">
You can find more R tutorials on  this page .
<h2><span class="orange">How to Create Histograms in SPSS</span></h2>
A <b>histogram </b>is a type of chart that uses rectangular bars to represent frequencies. It’s a helpful chart for visualizing the distribution of values in a dataset.
This tutorial explains how to create and modify histograms in SPSS.
<h3>Example: Histograms in SPSS</h3>
Suppose we have the following dataset that shows the average number of points scored per game by 20 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS1.png">
To create a histogram for this dataset, we can click on the <b>Graphs </b>tab, then <b>Chart Builder</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS2.png">
In the window that pops up, select <b>Histogram </b>in the <b>Choose from</b> list and drag it into the editing window. Then drag the variable <b>points</b> into the x-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS3.png">
Once you click <b>OK</b>, the following histogram will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS4.png">
By default, SPSS chooses an interval width to use for the bars in the graph. However, you can change this width by right clicking on any of the bars in the graph and then clicking <b>Edit Content > In Separate Window</b>.
In the new window that pops up, double click on any of the bars to bring up a <b>Properties </b>window. You can then choose the exact interval width you’d like to use. For example, we could use a width of <b>2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS5.png">
Once we click <b>Apply</b>, the histogram will be updated with a new interval width of 2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS6.png">
Note that the smaller the interval width, the more bars will appear in the histogram. The larger the interval width, the fewer bars will appear in the histogram.
<h2><span class="orange">How to Create and Modify Histograms in Stata</span></h2>
A <b>histogram </b>is a type of chart that uses rectangular bars to represent frequencies. It’s a helpful way to visualize the distribution of data values.
This tutorial explains how to create and modify histograms in Stata.
<h2>How to Create Histograms in Stata</h2>
We’ll use a dataset called <em>auto </em>to illustrate how to create and modify histograms in Stata.
First, load the data by typing the following into the Command box:
<b>use http://www.stata-press.com/data/r13/auto</b>
We can get a quick look at the dataset by typing the following into the Command box:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/scatterStata1.png">
We can see that there are 12 total variables in the dataset.
<h3>Basic Histogram</h3>
We can create a histogram for the variable <em>length </em>by using the <b>hist </b>command:
<b>hist length</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata1.png">
<h3>Histogram with Frequencies</h3>
By default, Stata displays the density on the y-axis. You can change the y-axis to display the actual frequencies by using the <b>freq </b>command:
<b>hist length, freq</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata2.png">
<h3>Histogram with Percentages</h3>
You can also change the y-axis to display percentages instead of frequencies by using the <b>percent </b>command:
<b>hist length, percent</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata3.png">
<h3>Changing the Number of Bins</h3>
When you use the <b>hist </b>function in Stata, it automatically tells you how many “bins” it used. For example, in the previous examples it always used 8 bins:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata4.png">
However, we can specify the exact number of bins by using the <b>bin() </b>command. For example, the following code tells Stata to use 16 bins instead of 8:
<b>hist length, percent bin(16)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata5.png">
We can also tell Stata to use fewer bins:
<b>hist length, percent bin(4)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata6.png">
Notice that the more bins you use, the more granularity you can see in the data.
<h3>Adding a Normal Density to a Histogram</h3>
You can add a normal  density curve  to a histogram by using the <b>normal </b>command:
<b>hist length, normal</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata10.png">
<h2>How to Modify Histograms in Stata</h2>
We can use several different commands to modify the appearance of the histograms.
<h3>Adding a Title</h3>
We can add a title to the plot using the <b>title() </b>command:
<b>hist length, title(“Distribution of Length”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata7.png">
<h3>Adding a Subtitle</h3>
We can also add a subtitle underneath the title using the <b>subtitle() </b>command:
<b>hist length, title(“Distribution of Length”) subtitle(“n = 74 cars”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata8.png">
<h3>Adding a Comment</h3>
We can also add a note or comment at the bottom of the graph by using the <b>note() </b>command:
<b>hist length, note(“Source: 1978 Automobile Data”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/histStata9.png">
<h2><span class="orange">How to Use hjust & vjust to Move Elements in ggplot2</span></h2>
You can use the <b>hjust</b> and <b>vjust</b> arguments to move elements horizontally and vertically, respectively, in ggplot2.
The following examples show how to use <b>hjust</b> and <b>vjust</b> in different scenarios.
<h2>Example 1: Move Title Position in ggplot2</h2>
The following code shows how to create a scatter plot in ggplot2 with a title in the default position (left-aligned):
<b>library(ggplot2)
#create scatter plot with title in default position
ggplot(data=mtcars, aes(x=mpg, y=wt)) +
  geom_point() +
  ggtitle("Plot Title") </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just1.jpg"460">
And the following code shows how to center-align the title by using <b>hjust=0.5</b>:
<b>library(ggplot2)
#create scatter plot with title center-aligned
ggplot(data=mtcars, aes(x=mpg, y=wt)) +
  geom_point() +
  ggtitle("Plot Title") +
  theme(plot.title = element_text(hjust=.5))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just2.jpg"477">
<b>Note</b>: You can also use <b>hjust=1</b> to right-align the title.
<h2>Example 2: Move Axis Label Position in ggplot2</h2>
The following code shows how to create a bar chart in ggplot2 in which the x-axis labels are rotated 90 degrees to make them easier to read:
<b>library(ggplot2)
#create data frame
df = data.frame(team=c('The Amazing Amazon Anteaters',       'The Rowdy Racing Raccoons',       'The Crazy Camping Cobras'),
                points=c(14, 22, 11))
#create bar plot to visualize points scored by each team
ggplot(data=df, aes(x=team, y=points)) +
  geom_bar(stat='identity') +
  theme(axis.text.x = element_text(angle=90)) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just3.jpg"478">
We can use the <b>hjust</b> and <b>vjust</b> arguments to adjust the x-axis labels so that they line up more closely with the tick marks on the x-axis:
<b>library(ggplot2)
#create data frame
df = data.frame(team=c('The Amazing Amazon Anteaters',       'The Rowdy Racing Raccoons',       'The Crazy Camping Cobras'),
                points=c(14, 22, 11))
#create bar plot to visualize points scored by each team
ggplot(data=df, aes(x=team, y=points)) +
  geom_bar(stat='identity') +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1) </b>
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just4.jpg"483">
<h2>Example 3: Move Text Position in ggplot2</h2>
The following code shows how to create a scatter plot in ggplot2 with annotated text for each point in the plot:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(player=c('Brad', 'Ty', 'Spencer', 'Luke', 'Max'), points=c(17, 5, 12, 20, 22), assists=c(4, 3, 7, 7, 5))
#create scatter plot with annotated labels
ggplot(df) +
  geom_point(aes(x=points, y=assists)) + 
  geom_text(aes(x=points, y=assists, label=player)) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just5.jpg"479">
We can use the <b>vjust</b> argument to move the text elements up vertically so that they’re easier to read:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(player=c('Brad', 'Ty', 'Spencer', 'Luke', 'Max'), points=c(17, 5, 12, 20, 22), assists=c(4, 3, 7, 7, 5))
#create scatter plot with annotated labels
ggplot(df) +
  geom_point(aes(x=points, y=assists)) + 
  geom_text(aes(x=points, y=assists, label=player), vjust=-.6)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just56JPG.jpg"479">
We could also use a positive value for <b>vjust</b> to move the text elements down vertically:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(player=c('Brad', 'Ty', 'Spencer', 'Luke', 'Max'), points=c(17, 5, 12, 20, 22), assists=c(4, 3, 7, 7, 5))
#create scatter plot with annotated labels
ggplot(df) +
  geom_point(aes(x=points, y=assists)) + 
  geom_text(aes(x=points, y=assists, label=player), vjust=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/just7.jpg"471">
The annotated text is now located below each point in the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Change the Legend Title in ggplot2 
 How to Rotate Axis Labels in ggplot2 
 How to Fix in R: could not find function “ggplot” 
<h2><span class="orange">How to Create Horizontal Boxplots in R</span></h2>
A  boxplot  (sometimes called a box-and-whisker plot) is a plot that shows the five-number summary of a dataset, which includes the following values:
Minimum
First Quartile
Median
Third Quartile
Maximum
To create a horizontal boxplot in base R, we can use the following code:
<b>#create one horizontal boxplot
boxplot(df$values, horizontal=TRUE)
#create several horizontal boxplots by group
boxplot(values~group, data=df, horizontal=TRUE)
</b>
And to create a horizontal boxplot in  ggplot2 , we can use the following code:
<b>#create one horizontal boxplot
ggplot(df, aes(y=values)) + 
  geom_boxplot() +
  coord_flip()
#create several horizontal boxplots by group
ggplot(df, aes(x=group, y=values)) + 
  geom_boxplot() +
  coord_flip()
</b>
The following examples show how to create horizontal boxplots in both base R and ggplot2.
<h3>Example 1: Horizontal Boxplots in Base R</h3>
The following code shows how to create a horizontal boxplot for one variable in a data frame in R:
<b>#create data
df &lt;- data.frame(points=c(7, 8, 9, 12, 12, 5, 6, 6, 8, 11, 6, 8, 9, 13, 17), team=rep(c('A', 'B', 'C'), each=5))
#create horizontal boxplot for points
boxplot(df$points, horizontal=TRUE, col='steelblue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/horizontalBox1.png">
The following code shows how to create several horizontal boxplots based on groups:
<b>#create data
df &lt;- data.frame(points=c(7, 8, 9, 12, 12, 5, 6, 6, 8, 11, 6, 8, 9, 13, 17), team=rep(c('A', 'B', 'C'), each=5))
#create horizontal boxplots grouped by team
boxplot(points~team, data=df, horizontal=TRUE, col='steelblue', las=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/h2.png">
Note that the argument <b>las=2</b> tells R to make the y-axis labels perpendicular to the axis.
<h3>Example 2: Horizontal Boxplots in ggplot2</h3>
The following code shows how to create a horizontal boxplot for one variable in ggplot2:
<b>library(ggplot2)
#create data
df &lt;- data.frame(points=c(7, 8, 9, 12, 12, 5, 6, 6, 8, 11, 6, 8, 9, 13, 17), team=rep(c('A', 'B', 'C'), each=5))
#create horizontal boxplot for points
ggplot(df, aes(y=points)) + 
  geom_boxplot(fill='steelblue') +
  coord_flip()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/h3.png">
The following code shows how to create several horizontal boxplots in ggplot2 based on groups:
<b>library(ggplot2)
#create data
df &lt;- data.frame(points=c(7, 8, 9, 12, 12, 5, 6, 6, 8, 11, 6, 8, 9, 13, 17), team=rep(c('A', 'B', 'C'), each=5))
#create horizontal boxplot for points
ggplot(df, aes(x=team, y=points)) + 
  geom_boxplot(fill='steelblue') +
  coord_flip()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/h4.png">
<h2><span class="orange">How to Add a Horizontal Line to a Scatterplot in Excel</span></h2>
Occasionally you may want to add a horizontal line to a scatterplot in Excel to represent some threshold or limit.
This tutorial provides a step-by-step example of how to quickly add a horizontal line to any scatterplot in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create the following fake dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel1.png">
<h3>Step 2: Create the Scatterplot</h3>
Next, highlight the data in the cell range <b>A2:B17</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel2.png">
Along the top ribbon, click <b>Insert</b> and then click the first chart in the <b>Insert Scatter (X, Y) or Bubble Chart</b> group within the <b>Charts</b> group. The following scatterplot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel3.png">
<h3>Step 3: Add a Horizontal Line</h3>
Now suppose we would like to add a horizontal line at <b>y = 20</b>.
To do this, we can create a fake data series that shows the minimum and maximum value along the x-axis (0 and 20) as well as two y-values that are both equal to 20:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel4.png">
Next, right click anywhere on the chart and click <b>Select Data</b>. In the window that appears, click <b>Add</b> under the <b>Legend Entries (Series)</b> section.
In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel5.png">
Once you click <b>OK</b>, two orange dots will appear on the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel6.png">
Right click on one of the orange dots and click <b>Format Data Series…</b>
In the window that appears on the right side of the screen, click <b>Solid Line</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel10.png">
 This will turn the two orange points into a solid orange line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/horizontalExcel7.png">
Feel free to modify the color, thickness, and style of the line to make it more aesthetically pleasing.
If you would like to add multiple horizontal lines to a single chart, simply repeat this process using different y-values.
<b>Related:</b>  How to Add Average Line to Bar Chart in Excel 
<h2><span class="orange">How Do Outliers Affect the Mean?</span></h2>
In statistics, the <b>mean </b>of a dataset is the average value. It’s useful to know because it gives us an idea of where the “center” of the dataset is located. It is calculated using the simple formula:
<b>mean</b> = (sum of observations) / (number of observations)
For example, suppose we have the following dataset:
[1, 4, 5, 6, 7]
The mean of the dataset is (1+4+5+6+7) / (5) = <b>4.6</b>
But while the mean is a useful and easy to calculate, it does have one drawback: <b>It can be affected by outliers</b>. In particular, the smaller the dataset, the more that an outlier could affect the mean.
To illustrate this, consider the following classic example:
Ten men are sitting in a bar. The average income of the ten men is $50,000. Suddenly one man walks out and Bill Gates walks in. Now the average income of the ten men in the bar is $40 million.
This example shows how one outlier (Bill Gates) could drastically affect the mean.
<h2>Small & Large Outliers</h2>
An outlier can affect the mean by being unusually small or unusually large. In the previous example, Bill Gates had an unusually large income, which caused the mean to be misleading.
However, an unusually small value can also affect the mean. To illustrate this, consider the following example:
Ten students take an exam and receive the following scores:
 
[0, 88, 90, 92, 94, 95, 95, 96, 97, 99]
 
The mean score is <b>84.6</b>.
 
However, if we remove the “0” score from the dataset, then the mean score becomes <b>94</b>.
The one unusually low score of one student drags the mean down for the entire dataset.
<h2>Sample Size & Outliers</h2>
The smaller the sample size of the dataset, the more an outlier has the potential to affect the mean.
For example, suppose we have a dataset of 100 exam scores where all of the students scored at least a 90 or higher except for one student who scored a zero:
[<b>0</b>, 90, 90, 92, 94, 95, 95, 96, 97, 99, 94, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99, 93, 90, 90, 92, 94, 95, 95, 96, 97, 99]
The mean turns out to be <b>93.18</b>. If we removed the “0” from the dataset, the mean would be <b>94.12</b>. This is a relatively small difference. This shows that even an extreme outlier only has a small effect if the dataset is large enough.
<h2>How to Handle Outliers</h2>
If you’re worried that an outlier is present in your dataset, you have a few options:
<b>Make sure the outlier is not the result of a data entry error.</b> Sometimes an individual simply enters the wrong data value when recording data. If an outlier is present, first verify that the value was entered correctly and that it wasn’t an error.
<b>Assign a new value to the outlier</b>. If the outlier turns out to be a result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
<b>Remove the outlier. </b>If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<h2>Use the Median</h2>
Another way to find the “center” of a dataset is to use <b>the median</b>, which is found by arranging all of the individual values in a dataset from smallest to largest and finding the middle value.
Because of the way it is calculated, the median is less affected by outliers and it does a better job of capturing the central location of a distribution when there are outliers present.
For example, consider the following chart that shows the square footage of houses in a particular neighborhood:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/mean_dist3.jpg">
The mean is heavily influenced by a couple extremely large houses, while the median is not. Thus, the median does a better job of capturing the “typical” square footage of a house in this neighborhood compared to the mean.
<b>Further Reading:</b>
<b> Measures of Central Tendency – Mean, Median, and Mode 
 Dixon’s Q Test for Detecting Outliers 
 Outlier Calculator </b>
<h2><span class="orange">How Hard is Statistics? (And Why It Doesn’t Need to Be!)</span></h2>
The field of <b>statistics</b> revolves around collecting, analyzing, interpreting, and presenting data.
Because the world is becoming a more digital place, more data is being generated now compared to any time in human history.
This means that it’s more important than ever to have a basic understanding of statistics so that you know how to work with data!
For many students in college, it is becoming a requirement to take at least one statistics course even if you aren’t majoring in statistics.
For example, most students who major in healthcare, business, computer science, finance, and education will all be required to take at least one statistics course.
And each of these students tend to have the same question:
<em><b>How hard is statistics?</b></em>
The answer: On the surface level, statistics <em>looks hard</em> only because of all the intimidating symbols.
However, almost all of statistics can be boiled down to <b>knowing which formula to use in different situations</b>.
Once you’re able to identify which formula to use for a specific problem, it just becomes a matter of plugging numbers into the formula to get an answer.
<h2>Example: The Importance of Knowing Formulas in Statistics</h2>
Here is an example of a typical problem you might encounter in an introductory statistics course:
<b>Question:</b> A manufacturer claims that a particular automobile model will get 50 miles per gallon on the highway. The researchers at a consumer-oriented magazine believe that this claim is high and plan a test with a simple random sample of 30 cars. Assuming the standard deviation between individual cars is 2.3 miles per gallon, what should the researchers conclude if the sample mean is 49 miles per gallon?
<b>This question looks pretty intimidating.</b>
However, the hardest part is simply identifying the formula that you need to use to solve this problem.
In this case, this problem requires you to use a  one sample z-test  with the following formula:
<b>z = (x – μ<sub>0</sub>) / (σ/√n)</b>
where:
<b>x: </b>sample mean
<b>μ<sub>0</sub>:</b> hypothesized population mean
<b>σ: </b>population standard deviation
<b>n: </b>sample size
Once you know that this is the formula to use, you can simply plug in the numbers given to you in the question:
z = (x – μ<sub>0</sub>) / (σ/√n)
z = (49 – 50) / (2.3/√30)
z = -2.3814
We can then use the  Z Score to P Value Calculator  to find that the p-value for a one-tailed test with z = -2.3814 is .0087. Since this p-value is less than .05, there is sufficient evidence to reject the manufacturer’s claim.
The question looks intimidating, but the hardest part is actually just knowing what formula to use.
The good news is that the more questions you encounter like this, the better you’ll be able to identify which formula to use.
Notice that we didn’t have to perform any difficult calculus or complex math. We simply needed to plug values into a formula.
<h2>Why Statistics Doesn’t Have to Be Hard</h2>
Here at Statology, I’ve made it my goal to make statistics as simple as possible to learn.
I’ve even created the following products that teach all of the core concepts taught in any introductory statistics course:
 Introduction to Statistics Course : An online course that includes 19 videos with 2 hours of total content that teaches you the core concepts taught in introductory statistics.
 Statology Study : An online study guide with over 100 practice problems and solutions that helps you understand all of the core concepts taught in any introductory statistics course.
 Elementary Statistics Formula Sheet : A printable formula sheet that contains the formulas for the most common confidence intervals and hypothesis tests in Elementary Statistics, all neatly arranged on one page.
 Statistics in Excel Made Easy : A collection of 16 Excel spreadsheets that contain built-in formulas to perform the most commonly used statistical tests.
Each of these resources, along with the free  tutorials  and  statistics calculators , are designed to make your life easier as a student and make statistics a bit more bearable.
<h2>Additional Resources</h2>
The following tutorials explain the importance of statistics in various fields:
 The Importance of Statistics in Healthcare 
 The Importance of Statistics in Education 
 The Importance of Statistics in Psychology 
 The Importance of Statistics in Finance 
<h2><span class="orange">How to Apply a Function to Selected Columns in Pandas</span></h2>
You can use the following syntax to apply a function to one or more columns of a pandas DataFame:
<b>#divide values in <em>column1</em> by 2
df['column1'] = df['column1'] / 2
#divide values in <em>column1</em> and <em>column2</em> by 2
df[['column1', 'column2']] = df[['column1', 'column2']] / 2
#divide values in every column of DataFrame by 2
df = df / 2
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Apply Function to One Column</h3>
The following code shows how to apply a function to just one column of a DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [10, 12, 12, 14, 13, 18],   'rebounds': [7, 7, 8, 13, 7, 4],   'assists': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
#divide values in <em>points</em> column by 2
df['points'] = df['points'] / 2
#view updated DataFrame
df
        pointsrebounds assists
05.07 11
16.07 8
26.08 10
37.013 6
46.57 6
59.04 5
</b>
<h3>Example 2: Apply Function to Specific Columns</h3>
The following code shows how to apply a function to specific columns of a DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [10, 12, 12, 14, 13, 18],   'rebounds': [7, 7, 8, 13, 7, 4],   'assists': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
#divide values in <em>points</em> and <em>rebounds</em> column by 2
df[['points', 'rebounds']] = df[['points', 'rebounds']] / 2
#view updated DataFrame
df
pointsrebounds assists
05.03.5 11
16.03.5 8
26.04.0 10
37.06.5 6
46.53.5 6
59.02.0 5</b>
<h3>Example 3: Apply Function to All Columns</h3>
The following code shows how to apply a function to every column of a DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [10, 12, 12, 14, 13, 18],   'rebounds': [7, 7, 8, 13, 7, 4],   'assists': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
pointsrebounds assists
0107 11
1127 8
2128 10
31413 6
4137 6
5184 5
#divide values in every column by 2
df = df / 2
#view updated DataFrame
df
        pointsrebounds assists
05.03.5 5.5
16.03.5 4.0
26.04.0 5.0
37.06.5 3.0
46.53.5 3.0
59.02.0 2.5</b>
<h2><span class="orange">How to Calculate a P-Value from a T-Test By Hand</span></h2>
One of the most common tests used in statistics is the <b>t-test</b>, which is often used to determine if a population mean is equal to some value.
For example, suppose we want to know if the mean height of a certain species of plant is equal to 15 inches. To test this, we could  collect a random sample  of 20 plants, find the sample mean and sample standard deviation, and perform a t-test to determine if the mean height is actually equal to 15 inches.
The null and alternative hypothesis for the test are as follows:
<b>H<sub>0</sub>:</b> μ = 15
<b>H<sub>a</sub>:</b> μ ≠ 15
The formula for the test statistic is:
<b><em>t</em></b> = (x-μ) / (s/√n) 
where <em>x</em> is the sample mean, <em>μ</em> is the hypothesized mean (in our example it would be 15), <em>s</em> is the sample standard deviation, and <em>n</em> is the sample size.
Once we know the value of <em>t</em>, we can use statistical software or  an online calculator  to find the corresponding p-value. If the p-value is less than some alpha level (common choices are .01, .05, and .10) then we can reject the null hypothesis and conclude that the mean height of the plants is <em>not </em>equal to 15 inches.
However,<b> it’s also possible to estimate the p-value of the test by hand using a t-Distribution table</b>. In this post, we’ll explain how to do so.
<h2>Example: Calculating the p-value from a t-test by hand</h2>
<b>Problem</b>: Bob wants to know if the  mean height of a certain species of plant is equal to 15 inches. To test this, he collects a random sample of 20 plants and finds that the sample mean is 14 inches and the sample standard deviation is 3 inches. Conduct a t-test using a .05 alpha level to determine if the true mean height for the population is actually 15 inches.
<b>Solution:</b>
<b>Step 1: State the null and alternative hypotheses.</b>
<b>H<sub>0</sub>:</b> μ = 15
<b>H<sub>a</sub>:</b> μ ≠ 15
<b>Step 2: Find the test statistic.</b>
<b><em>t</em></b>  =  (x-μ) / (s/√n)   =  (14-15) / (3/√20)   = <b>-1.49</b>
<b>Step 3: Find the p-value for the test statistic.</b>
To find the p-value by hand, we need to use the t-Distribution table with n-1 degrees of freedom. In our example, our sample size is n = 20, so n-1 = 19.
In the t-Distribution table below, we need to look at the row that corresponds to “19” on the left-hand side and attempt to look for the absolute value of our test statistic <b>1.49</b>.
Notice that 1.49 does not show up in the table, but it does fall between the two values <b>1.328 </b>and <b>1.729</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/t_table3.jpg"435">
Next, we can look at the two alpha levels at the top of the table that correspond to these two numbers. We see that they are <b>0.1 </b>and <b>0.5</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/t_table4.jpg"434">
This means that the p-value for a one-sided test is between 0.1 and 0.05. Let’s call it .075. Since our t-test is two-sided, we need to multiply this value by 2. So, our estimated p-value is .075 * 2 = <b>0.15</b>.
<b>Step 4: Draw a conclusion.</b>
Since this p-value is <em>not </em>less than our chosen alpha level of .05, we can’t reject the null hypothesis. Thus, we don’t have sufficient evidence to say that the true mean height of this species of plant is different than 15 inches.
<h3>Verifying the Results with a Calculator</h3>
We can plug our test statistic <em>t </em>and our degrees of freedom into  an online p-value calculator  to see how close our estimated p-value was to the true p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/t_calc.jpg"294">
The true p-value is <b>0.15264</b>, which is pretty close to our estimated p-value of <b>0.15</b>.
<h2>Conclusion</h2>
We saw in this post that it’s possible to estimate the p-value of a t-test by hand using the t-Distribution table. However, in most scenarios you will never have to calculate the p-value by hand and instead you can use either statistical software like R and Excel, or an online calculator to find the exact p-value of the test.
In most cases, especially in rigorous statistical studies and experiments, you will want to use a calculator to find the exact p-value from a t-test so that you can be as accurate as possible, but it’s good to know that you can still estimate the p-value from a t-test by hand if you absolutely need to.
<h2><span class="orange">How to Calculate Mallows’ Cp in R</span></h2>
In regression analysis,  Mallows’ Cp  is a metric that is used to pick the best regression model among several potential models.
We can identify the “best” regression model by identifying the model with the lowest Cp value that is close to <em>p</em>+1, where <em>p</em> is the number of predictor variables in the model.
The easiest way to calculate Mallows’ Cp in R is to use the  ols_mallows_cp()  function from the <b>olsrr</b> package.
The following example shows how to use this function to calculate Mallows’ Cp to pick the best regression model among several potential models in R.
<h3>Example: Calculating Mallows’ Cp in R</h3>
Suppose we would like to fit three different multiple linear regression models using variables from the <b>mtcars</b> dataset.
The following code shows how to fit the following regression models:
Predictor variables in Full Model: All 10 variables
Predictor variables in Model 1: disp, hp, wt, qsec
Predictor variables in Model 2: disp, qsec
Predictor variables in Model 3: disp, wt
The following code shows how to fit each of these regression models and use the  ols_mallows_cp()  function to calculate the Mallows’ Cp of each model:
<b>library(olsrr)
#fit full model
full_model &lt;- lm(mpg ~ ., data = mtcars)
#fit three smaller models
model1 &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
model2 &lt;- lm(mpg ~ disp + qsec, data = mtcars)
model3 &lt;- lm(mpg ~ disp + wt, data = mtcars)
#calculate Mallows' Cp for each model
ols_mallows_cp(model1, full_model)
[1] 4.430434
ols_mallows_cp(model2, full_model)
[1] 18.64082
ols_mallows_cp(model3, full_model)
[1] 9.122225
</b>
Here’s how to interpret the output:
Model 1: <em>p</em> + 1 = 5, Mallows’ Cp = 4.43
Model 2: <em>p</em> + 1 = 3, Mallows’ Cp = 18.64
Model 3: <em>p</em> + 1 = 30, Mallows’ Cp = 9.12
We can see that model 1 has a value for Mallows’ Cp that is closest to <em>p</em> + 1, which indicates that it’s the best model that leads to the least amount of bias among the three potential models.
<h3>Notes on Mallows’ Cp</h3>
Here are few things to keep in mind with regards to Mallows’ Cp:
If every potential model has a high value for Mallows’ Cp, this is an indication that some important predictor variables are likely missing from each model.
If several potential models have low values for Mallow’s Cp, choose the model with the lowest value as the best model to use.
Keep in mind that Mallows’ Cp is only one way to identify the “best” regression model among several potential models.
Another commonly used metric is adjusted R-squared, which tells us the proportion of variance in the  response variable  that can be explained by the predictor variables in the model, adjusted for the number of predictor variables used.
When deciding which regression model is best among a list of several different models, it’s recommended to look at both Mallows’ Cp and adjusted R-squared.
<h2><span class="orange">How to Calculate Margin of Error in Excel</span></h2>
Often in statistics, we’re interested in estimating a  population parameter  using a sample. 
For example, we might want to know the mean height of students at a particular school. If the school has 1,000 total students, it might take too long to measure every student so instead we could take  a simple random sample  of 50 students and calculate the mean height of students in this sample. 
And while the mean height of students in the sample might be a good estimate of the true population mean, there is no guarantee that the sample mean is exactly equal to the population mean. In other words, there exists some uncertainty. 
One way to account for uncertainty is create a  confidence interval , which is a range of values that we believe contains the true population parameter.
For example, if the mean height of students in the sample is 67 inches, our confidence interval for the true mean height of all students in the population might be [65 inches, 69 inches], which means we’re confident that the true mean height of students in the population is between 65 and 69 inches.
A confidence interval is composed of two parts:
<b>Point estimate</b> – often this is a sample mean or sample proportion.
<b>Margin of error</b> – a number that represents the uncertainty of the point estimate.
The formula to create a confidence interval is:
<b>Confidence Interval</b> = point estimate +/-  margin of error
<h2>Margin of Error Formula</h2>
If you’re creating a confidence interval for a <b>population mean</b>, then the formula for the margin of error is:
<b>Margin of error:</b> Z * σ / √n
Where:
<b>Z</b>: Z-score
<b>σ</b>: Population standard deviation
<b>n</b>: Sample size
<em><b>Note: </b>If the population standard deviation is unknown, then you can replace Z with t<sub>n-1</sub>, which is the t critical-value that comes from the  t distribution table  with n-1 degrees of freedom.</em>
And if you’re creating a confidence interval for a <b>population proportion</b>, then the formula for the margin of error is:
<b>Margin of error:</b> Z * √(p*(1-p)) / n)
Where:
<b>Z</b>: Z-score
<b>p</b>: Sample proportion
<b>n</b>: Sample size
Note that the Z-score you’ll use for this calculation is dependent on you chosen confidence level. The following table shows the Z-scores associated with common confidence levels:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>Z-score</b></th>
</tr>
<tr>
<td style="text-align: center;">80%</td>
<td style="text-align: center;">1.282</td>
</tr>
<tr>
<td style="text-align: center;">85%</td>
<td style="text-align: center;">1.44</td>
</tr>
<tr>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">95%</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">99%</td>
<td style="text-align: center;">2.576</td>
</tr>
</tbody></table>
Next, we’ll walk through two examples of how to calculate the margin of error in Excel.
<h2>Example 1: Margin of Error for a Population Mean</h2>
Suppose we want to find the mean height of a certain plant. It is known that the population standard deviation, σ, is 2 inches. We collect a random sample of 100 plants and find that the sample mean is 14 inches. Find a 95% confidence interval for the true mean height of this certain plant.
Since we’re finding a confidence interval for the mean height, the formula we will use for the margin of error is: <b>Z * σ / √n</b>
The following image shows how to calculate the margin of error for this confidence interval:
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/marginOfError1.jpg"294">
The margin of error turns out to be <b>0.392</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/marginOfError2.jpg"230">
Thus, the confidence interval for the true mean height of plants would be 14 +/ 0.392 = [13.608, 14.392].
<h2>Example 2: Margin of Error for a Population Proportion</h2>
Suppose we want to know what percentage of individuals in a certain city support a candidate named Bob. In a simple random sample of 200 individuals, 120 said they supported Bob (i.e. 60% support him). Find a 99% confidence interval for the true percentage of people in the entire city who support Bob. 
Since we’re finding a confidence interval for the mean height, the formula we will use for the margin of error is: <b>Z * √(p*(1-p)) / n)</b>
The following image shows how to calculate the margin of error for this confidence interval:
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/marginOfError3.jpg"459">
The margin of error turns out to be <b>0.089</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/marginOfError4.jpg"336">
Thus, the confidence interval for the true percentage of individuals in this city that support Bob is 0.6 +/- 0.089 = [ 0.511, 0.689].
<h2>Notes on Finding the Appropriate Z-Score or t-Score</h2>
If you’re finding the confidence interval for a population mean and you’re unsure of whether or not to use a Z-Score or a t-Score for the margin of error calculation, refer to this helpful diagram to help you decide:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/z_table_vs_t_table.jpg"445">
Also, if you don’t have a helpful table that shows you which Z-Score or t-Score to use based on your confidence interval, you can always use the following commands in Excel to find the correct Z-Score or t-Score to use:
To find Z-Score: <b>=NORM.INV(probability, 0, 1)</b>
For example, to find the Z-Score associated with a 95% confidence level, you’d type <b>=NORM.INV(.975, 0, 1)</b>, which turns out to be 1.96.
To find t-Score: <b>=T.INV(probability, degrees of freedom)</b>
For example, to find the t-Score associated with a 90% confidence level and 12 degrees of freedom, you’d type <b>=T.INV(.95, 12)</b>, which turns out to be 1.78.
<h2><span class="orange">How to Calculate Mean Squared Error (MSE) in Excel</span></h2>
One of the most common metrics used to measure the forecast accuracy of a model is <b>MSE</b>, which stands for <b>mean squared error</b>. It is calculated as:
<b>MSE </b>= (1/n) * Σ(actual – forecast)<sup>2</sup>
where:
<b>Σ</b> – a fancy symbol that means “sum”
<b>n</b> – sample size
<b>actual</b> – the actual data value
<b>forecast</b> – the forecasted data value
The lower the value for MSE, the better a model is able to forecast values accurately.
<h2>How to Calculate MSE in Excel</h2>
To calculate MSE in Excel, we can perform the following steps:
<b>Step 1: Enter the actual values and forecasted values in two separate columns.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/mse1.png">
<b>Step 2: Calculate the squared error for each row.</b>
Recall that the squared error is calculated as: (actual – forecast)<sup>2</sup>. We will use this formula to calculate the squared error for each row.
Column D displays the squared error and Column E shows the formula we used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/mse2.png">
Repeat this formula for each row:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/mse3.png">
<b>Step 3: Calculate the mean squared error.</b>
Calculate MSE by simply finding the average of the values in column D:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/mse4.png">
The MSE of this model turns out to be <b>5.917</b>.
<h2><span class="orange">How to Calculate MSE in R</span></h2>
One of the most common metrics used to measure the prediction accuracy of a model is <b>MSE</b>, which stands for <b>mean squared error</b>. It is calculated as:
<b>MSE </b>= (1/n) * Σ(actual – prediction)<sup>2</sup>
where:
<b>Σ</b> – a fancy symbol that means “sum”
<b>n</b> – sample size
<b>actual</b> – the actual data value
<b>prediction</b> – the predicted data value
The lower the value for MSE, the more accurately a model is able to predict values.
<h2>How to Calculate MSE in R</h2>
Depending on what format your data is in, there are two easy methods you can use to calculate the MSE of a regression model in R.
<h3>Method 1: Calculate MSE from Regression Model</h3>
In one scenario, you may have a fitted regression model and would simply like to calculate the MSE of the model. For example, you may have the following regression model:
<b>#load mtcars dataset
data(mtcars)
#fit regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#get model summary
model_summ &lt;-summary(model)
</b>
To calculate the MSE for this model, you can use the following formula:
<b>#calculate MSE
mean(model_summ$residuals^2)
[1] 8.85917</b>
This tells us that the MSE is <b>8.85917</b>.
<h3>Method 2: Calculate MSE from a list of Predicted and Actual Values</h3>
In another scenario, you may simply have a list of predicted and actual values. For example:
<b>#create data frame with a column of actual values and a column of predicted values
data &lt;- data.frame(pred = predict(model), actual = mtcars$mpg)
#view first six lines of data
head(data)
      pred actual
Mazda RX4         23.14809   21.0
Mazda RX4 Wag     23.14809   21.0
Datsun 710        25.14838   22.8
Hornet 4 Drive    20.17416   21.4
Hornet Sportabout 15.46423   18.7
Valiant           21.29978   18.1
</b>
In this case, you can use the following formula to calculate the MSE:
<b>#calculate MSE
mean((data$actual - data$pred)^2)
[1] 8.85917</b>
This tells us that the MSE is <b>8.85917</b>, which matches the MSE that we calculated using the previous method.
<h2><span class="orange">How to Calculate Residuals in Regression Analysis</span></h2>
 Simple linear regression  is a statistical method you can use to understand the relationship between two variables, x and y.
One variable, <b>x</b>, is known as the predictor variable. The other variable, <b>y</b>, is known as the  response variable .
For example, suppose we have the following dataset with the weight and height of seven individuals:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height1.jpg"> 
Let <em>weight </em>be the predictor variable and let <em>height </em>be the response variable.
If we graph these two variables using a  scatterplot , with weight on the x-axis and height on the y-axis, here’s what it would look like:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/07/statology_residuals1.jpg">
From the scatterplot we can clearly see that as weight increases, height tends to increase as well, but to actually <em>quantify </em>this relationship between weight and height, we need to use linear regression.
Using linear regression, we can find the line that best “fits” our data:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/07/statology_residuals2.jpg">
The formula for this line of best fit is written as:
<U+0177> = b<sub>0</sub> + b<sub>1</sub>x
where <U+0177> is the predicted value of the response variable, b<sub>0</sub> is the y-intercept, b<sub>1</sub> is the regression coefficient, and x is the value of the predictor variable.
In this example, the line of best fit is:
height = 32.783 + 0.2001*(weight)
<h2>How to Calculate Residuals</h2>
Notice that the data points in our scatterplot don’t always fall exactly on the line of best fit:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/07/statology_residuals2.jpg">
This difference between the data point and the line is called the <b>residual</b>. For each data point, we can calculate that point’s residual by taking the difference between it’s actual value and the predicted value from the line of best fit.
<h3>Example 1: Calculating a Residual</h3>
For example, recall the weight and height of the seven individuals in our dataset:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height1.jpg"> 
The first individual has a weight of <b>140 </b>lbs. and a height of <b>60 </b>inches.
To find out the predicted height for this individual, we can plug their weight into the line of best fit equation:
height = 32.783 + 0.2001*(weight)
Thus, the predicted height of this individual is:
height = 32.783 + 0.2001*(140)
height = 60.797 inches
Thus, the residual for this data point is 60 – 60.797 = <b>-0.797</b>.
<h3>Example 2: Calculating a Residual</h3>
We can use the exact same process we used above to calculate the residual for each data point. For example, let’s calculate the residual for the second individual in our dataset:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height1.jpg"> 
The second individual has a weight of <b>155 </b>lbs. and a height of <b>62 </b>inches.
To find out the predicted height for this individual, we can plug their weight into the line of best fit equation:
height = 32.783 + 0.2001*(weight)
Thus, the predicted height of this individual is:
height = 32.783 + 0.2001*(155)
height = 63.7985 inches
Thus, the residual for this data point is 62 – 63.7985 = <b>-1.7985</b>.
<h3>Calculating All Residuals</h3>
Using the same method as the previous two examples, we can calculate the residuals for every data point:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/07/statology_residuals3.jpg" alt="">
Notice that some of the residuals are positive and some are negative. <b>If we add up all of the residuals, they will add up to zero.</b>
This is because linear regression finds the line that minimizes the total squared residuals, which is why the line perfectly goes through the data, with some of the data points lying above the line and some lying below the line.
<h2>Visualizing Residuals</h2>
Recall that a <b>residual </b>is simply the distance between the actual data value and the value predicted by the regression line of best fit. Here’s what those distances look like visually on a scatterplot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/07/statology_residuals4.jpg" alt="">
Notice that some of the residuals are larger than others. Also, some of the residuals are positive and some are negative as we mentioned earlier.
<h2>Creating a Residual Plot</h2>
The whole point of calculating residuals is to see how well the regression line fits the data.
Larger residuals indicate that the regression line is a poor fit for the data, i.e. the actual data points do not fall close to the regression line.
Smaller residuals indicate that the regression line fits the data better, i.e. the actual data points fall close to the regression line.
One useful type of plot to visualize all of the residuals at once is a residual plot. A <b>residual plot</b> is a type of plot that displays the predicted values against the residual values for a regression model.
This type of plot is often used to assess whether or not a linear regression model is appropriate for a given dataset and to check for  heteroscedasticity  of residuals.
Check out  this tutorial  to find out how to create a residual plot for a simple linear regression model in Excel.
<h2><span class="orange">How to Calculate RMSE in R</span></h2>
The <b>root mean square error (RMSE) </b>is a metric that tells us how far apart our predicted values are from our observed values in a regression analysis, on average. It is calculated as:
<b>RMSE </b>= √[ Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n ]
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation in the dataset
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation in the dataset
n is the sample size
This tutorial explains two methods you can use to calculate RMSE in R.
<h3>Method 1: Write Your Own Function</h3>
Suppose we have a dataset with one column that contains the actual data values and one column that contains the predicted data values:
<b>#create dataset
data &lt;- data.frame(actual=c(34, 37, 44, 47, 48, 48, 46, 43, 32, 27, 26, 24),   predicted=c(37, 40, 46, 44, 46, 50, 45, 44, 34, 30, 22, 23))
#view dataset
data
   actual predicted
1      34        37
2      37        40
3      44        46
4      47        44
5      48        46
6      48        50
7      46        45
8      43        44
9      32        34
10     27        30
11     26        22
12     24        23
</b>
To compute the RMSE, we can use the following function:
<b>#calculate RMSE
sqrt(mean((data$actual - data$predicted)^2))
[1] 2.43242
</b>
The root mean square error is <b>2.43242</b>.
<h3>Method 2: Use a Package</h3>
We could also calculate RMSE for the same dataset using the <b>rmse()</b> function from the <b>Metrics</b> package, which uses the following syntax:
<b>rmse(actual, predicted)</b>
where:
<b>actual: </b>actual values
<b>prediced: </b>predicted values
Here is the syntax we would use in our example:
<b>#load Metrics package
library(Metrics)
calculate RMSE
rmse(data$actual, data$predicted)
[1] 2.43242</b>
The root mean square error is <b>2.43242</b>, which matches what we calculated earlier using our own function.
<h2>How to Interpret RMSE</h2>
RMSE is a useful way to see how well a regression model is able to fit a dataset.
The larger the RMSE, the larger the difference between the predicted and observed values, which means the worse a regression model fits the data. Conversely, the smaller the RMSE, the better a model is able to fit the data.
It can be particularly useful to compare the RMSE of two different models with each other to see which model fits the data better.
<h2><span class="orange">How to Calculate Sxx in Statistics (With Example)</span></h2>
In statistics, <b>Sxx</b> represents the sum of squared deviations from the mean value of x.
This value is often calculated when fitting a  simple linear regression model  by hand.
We use the following formula to calculate Sxx:
<b>Sxx = Σ(x<sub>i</sub> – x)<sup>2</sup></b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value of x
<b>x</b>: The mean value of x
The following example shows how to use this formula in practice.
<h2>Example: Calculating Sxx by Hand</h2>
Suppose we would like to fit a simple linear regression model to the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/sxx1-1.jpg"163">
Suppose we would like to calculate Sxx, which represents the sum of squared deviations from the mean value of x.
First, we must calculate the mean value of x:
x = (1 + 2 + 2 + 3 + 5 + 8) / 6 = 3.5
Next, we can use the following formula to calculate the value for Sxx:
Sxx = Σ(x<sub>i</sub> – x)<sup>2</sup>
Sxx = (1-3.5)<sup>2</sup>+(2-3.5)<sup>2</sup>+(2-3.5)<sup>2</sup>+(3-3.5)<sup>2</sup>+(5-3.5)<sup>2</sup>+(8-3.5)<sup>2</sup>
Sxx = 6.25 + 2.25 + 2.25 + .25 + 2.25 + 20.25
Sxx = 33.5
The value for Sxx turns out to be <b>33.5</b>.
This tells us that the sum of squared deviations between the individual x values and the mean x value is 33.5.
Note that we could also use the  Sxx Calculator  to automatically calculate the value of Sxx for this model as well:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/sxx2.jpg">
The calculator returns a value of <b>33.5</b>, which matches the value that we calculated by hand.
Note that we use the following formulas to perform simple linear regression by hand:
y = a + bx
where:
a = y – bx
b = Sxy / Sxx
The calculation for Sxx is just one calculation that we must perform in order to fit a simple linear regression model.
<b>Related:</b>  How to Calculate Sxy in Statistics 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in statistics:
 How to Perform Simple Linear Regression by Hand 
 How to Perform Multiple Linear Regression by Hand 
<h2><span class="orange">How to Calculate Sxy in Statistics (With Example)</span></h2>
In statistics, <b>Sxy</b> represents the sum of the product of the differences between x values and the mean of x and the differences between y values and the mean of y.
This value is often calculated when fitting a  simple linear regression model  by hand.
We use the following formula to calculate Sxy:
<b>Sxy = Σ(x<sub>i</sub> – x)(y<sub>i</sub> – y)</b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value of x
<b>x</b>: The mean value of x
<b>y<sub>i</sub></b>: The i<sup>th</sup> value of y
<b>y</b>: The mean value of y
The following example shows how to use this formula in practice.
<h2>Example: Calculating Sxy by Hand</h2>
Suppose we would like to fit a simple linear regression model to the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/sxy1.jpg"165">
Suppose we would like to calculate Sxy for this dataset.
First, we must calculate the mean value of x:
x = (1 + 2 + 2 + 3 + 5 + 8) / 6 = 3.5
Then, we must calculate the mean value of y:
y = (8 + 12 + 14 + 19 + 22 + 21) / 6 = 16
Using these values, the following screenshot shows how to calculate the value for Sxy:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/sxy2.jpg">
The value for Sxy turns out to be <b>59</b>.
Note that we could also use the  Sxy Calculator  to automatically calculate the value of Sxy for this model as well:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/sxy3.jpg"344">
The calculator returns a value of <b>59</b>, which matches the value that we calculated by hand.
Note that we use the following formulas to perform simple linear regression by hand:
y = a + bx
where:
a = y – bx
b = Sxy / Sxx
The calculation for Sxy is just one calculation that we must perform in order to fit a simple linear regression model.
<b>Related:</b>  How to Calculate Sxx in Statistics 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in statistics:
 How to Perform Simple Linear Regression by Hand 
 How to Perform Multiple Linear Regression by Hand 
<h2><span class="orange">How to Calculate the P-Value of an F-Statistic in R</span></h2>
An F-test produces an <b>F-statistic</b>. To find the <b>p-value</b> associated with an F-statistic in R, you can use the following command:
<b>pf(fstat, df1, df2, lower.tail = FALSE)</b>
<b>fstat</b> – the value of the f-statistic
<b>df1</b> – degrees of freedom 1
<b>df2</b> – degrees of freedom 2
<b>lower.tail</b> – whether or not to return the probability associated with the lower tail of the F distribution. This is TRUE by default.
For example, here is how to find the p-value associated with an F-statistic of 5, with degrees of freedom 1 = 3 and degrees of freedom 2 = 14:
<b>pf(5, 3, 14, lower.tail = FALSE)
#[1] 0.01457807</b>
One of the most common uses of an F-test is for  testing the overall significance of a regression model . In the following example, we show how to calculate the p-value of the F-statistic for a regression model.
<h2>Example: Calculating p-value from F-statistic</h2>
Suppose we have a dataset that shows the total number of hours studied, total prep exams taken, and final exam score received for 12 different students:
<b>#create dataset
data &lt;- data.frame(study_hours = c(3, 7, 16, 14, 12, 7, 4, 19, 4, 8, 8, 3),</b>
<b>                   prep_exams = c(2, 6, 5, 2, 7, 4, 4, 2, 8, 4, 1, 3),</b>
<b>                   final_score = c(76, 88, 96, 90, 98, 80, 86, 89, 68, 75, 72, 76))</b>
<b>
#view first six rows of dataset
head(data)
#  study_hours prep_exams final_score
#1           3          2          76
#2           7          6          88
#3          16          5          96
#4          14          2          90
#5          12          7          98
#6           7          4          80</b>
Next, we can fit a linear regression model to this data using <em>study hours </em>and <em>prep exams </em>as the predictor variables and <em>final score </em>as the response variable. Then, we can view the output of the model:
<b>#fit regression model
model &lt;- lm(final_score ~ study_hours + prep_exams, data = data)
#view output of the model
summary(model)
#Call:
#lm(formula = final_score ~ study_hours + prep_exams, data = data)
#
#Residuals:
#    Min      1Q  Median      3Q     Max 
#-13.128  -5.319   2.168   3.458   9.341 
#
#Coefficients:
#            Estimate Std. Error t value Pr(>|t|)    
#(Intercept)   66.990      6.211  10.785  1.9e-06 ***
#study_hours    1.300      0.417   3.117   0.0124 *  
#prep_exams     1.117      1.025   1.090   0.3041    
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#Residual standard error: 7.327 on 9 degrees of freedom
#Multiple R-squared:  0.5308,Adjusted R-squared:  0.4265 
#F-statistic: 5.091 on 2 and 9 DF,  p-value: 0.0332
</b>
On the very last line of the output we can see that the F-statistic for the overall regression model is <b>5.091</b>. This F-statistic has 2 degrees of freedom for the numerator and 9 degrees of freedom for the denominator. R automatically calculates that the p-value for this F-statistic is<b> 0.0332</b>.
In order to calculate this equivalent p-value ourselves, we could use the following code:
<b>pf(5.091, 2, 9, lower.tail = FALSE)
#[1] 0.0331947</b>
Notice that we get the same answer (but with more decimals displayed) as the linear regression output above.
<h2><span class="orange">How to Calculate VIF in Excel</span></h2>
 Multicollinearity  in regression analysis occurs when two or more explanatory variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model. 
Fortunately, it’s possible to detect multicollinearity using a metric known as the<b> variance inflation factor (VIF)</b>, which measures the correlation and strength of correlation between the explanatory variables in a regression model.
This tutorial explains how to calculate VIF in Excel.
<h2>Example: Calculating VIF in Excel</h2>
For this example we will perform a multiple linear regression using the following dataset that describes the attributes of 10 basketball players. We will fit a regression model using rating as the response variable and points, assists, and rebounds as the explanatory variables. Then, we’ll identify the VIF values for each explanatory variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel1.png">
<b>Step 1: Perform a multiple linear regression.</b>
Along the top ribbon, go to the Data tab and click on Data Analysis. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
Once you click on Data Analysis, a new window will pop up. Select <em>Regression </em>and click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel2.png">
Fill in the necessary arrays for the response variables and the explanatory variables, then click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel3.png">
This produces the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel4.png">
<b>Step 2: Calculate the VIF for each explanatory variable.</b>
Next, we can calculate the VIF for each of the three explanatory variables by performing individual regressions using one explanatory variable as the response variable and the other two as the explanatory variables.
For example, we can calculate the VIF for the variable <em>points </em>by performing a multiple linear regression using <em>points </em>as the response variable and <em>assists </em>and <em>rebounds </em>as the explanatory variables.
This produces the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel5.png">
The VIF for <em>points </em>is calculated as 1 / (1 – R Square) = 1 / (1 – .433099) = <b>1.76</b>.
We can then repeat this process for the other two variables <em>assists </em>and <em>rebounds</em>.
It turns out that the VIF for the three explanatory variables are as follows:
points: <b>1.76</b>
assists: <b>1.96</b>
rebounds: <b>1.18</b>
<h2>How to Interpret VIF Values</h2>
The value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:
A value of 1 indicates there is no correlation between a given explanatory variable and any other explanatory variables in the model.
A value between 1 and 5 indicates moderate correlation between a given explanatory variable and other explanatory variables in the model, but this is often not severe enough to require attention.
A value greater than 5 indicates potentially severe correlation between a given explanatory variable and other explanatory variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.
Given that each of the VIF values for the explanatory variables in our regression model are close to 1, multicollinearity is not a problem in our example.
<h2><span class="orange">How to Calculate VIF in Python</span></h2>
 Multicollinearity  in regression analysis occurs when two or more explanatory variables are highly correlated with each other, such that they do not provide unique or independent information in the regression model.
If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model. 
One way to detect multicollinearity is by using a metric known as the<b> variance inflation factor (VIF)</b>, which measures the correlation and strength of correlation between the explanatory variables in a  regression model .
This tutorial explains how to calculate VIF in Python.
<h2>Example: Calculating VIF in Python</h2>
For this example we’ll use a dataset that describes the attributes of 10 basketball players:
<b>import numpy as np
import pandas as pd
#create dataset
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view dataset
df
ratingpointsassistsrebounds
09025511
1852078
28214710
3881686
4942756
5902079
6761266
77515910
88714910
9861957</b>
Suppose we would like to fit a multiple linear regression model using rating as the response variable and points, assists, and rebounds as the explanatory variables.
To calculate the VIF for each explanatory variable in the model, we can use the  variance_inflation_factor() function  from the statsmodels library:
<b>from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor
#find design matrix for linear regression model using 'rating' as response variable 
y, X = dmatrices('rating ~ points+assists+rebounds', data=df, return_type='dataframe')
#calculate VIF for each explanatory variable
vif = pd.DataFrame()
vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif['variable'] = X.columns
#view VIF for each explanatory variable 
vif
       VIF variable
0101.258171Intercept
1  1.763977   points
2  1.959104  assists
3  1.175030 rebounds</b>
We can observe the VIF values for each of the explanatory variables:
<b>points: </b>1.76
<b>assists: </b>1.96
<b>rebounds: </b>1.18
<em><b>Note: </b>Ignore the VIF for the “Intercept” in the model since this value is irrelevant.</em>
<h2>How to Interpret VIF Values</h2>
The value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:
A value of 1 indicates there is no correlation between a given explanatory variable and any other explanatory variables in the model.
A value between 1 and 5 indicates moderate correlation between a given explanatory variable and other explanatory variables in the model, but this is often not severe enough to require attention.
A value greater than 5 indicates potentially severe correlation between a given explanatory variable and other explanatory variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.
Given that each of the VIF values for the explanatory variables in our regression model are close to 1, multicollinearity is not a problem in our example.
<h2><span class="orange">How to Conduct a Jarque-Bera Test in R</span></h2>
The <b>Jarque-Bera test</b> is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a  normal distribution .
The test statistic of the Jarque-Bera test is always a positive number and if it’s far from zero, it indicates that the sample data do not have a normal distribution.
The test statistic <em><b>JB</b> </em>is defined as:
<em><b>JB</b> </em> =[(n-k+1) / 6] * [S<sup>2</sup> + (0.25*(C-3)<sup>2</sup>)]
where <em>n </em>is the number of observations in the sample, <em>k </em>is the number of regressors (k=1 if not used in the context of regression), <em>S </em>is the sample skewness, and <em>C </em>is the sample kurtosis.
Under the null hypothesis of normality, <em>JB ~ </em>X<sup>2</sup>(2)
This tutorial explains how to conduct a Jarque-Bera test in R.
<h2>Jarque-Bera test in R</h2>
To conduct a Jarque-Bera test for a sample dataset, we can use the <b>tseries</b> package:
<b>#install (if not already installed) and load <em>tseries </em>package
if(!require(tseries)){install.packages('tseries')}
#generate a list of 100 normally distributed random variables
dataset &lt;- rnorm(100)
#conduct Jarque-Bera test
jarque.bera.test(dataset)
</b>
This generates the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/jarqueBera.jpg" alt="">
This tells us that the test statistic is 0.67446 and the p-value of the test is 0.7137. In this case, we would fail to reject the null hypothesis that the data is normally distributed.
This result shouldn’t be surprising since the dataset we generated is composed of 100 random variables that follow a normal distribution.
Consider instead if we generated a dataset that was comprised of a list of 100 uniformly distributed random variables:
<b>#install (if not already installed) and load <em>tseries </em>package
if(!require(tseries)){install.packages('tseries')}
#generate a list of 100 uniformly distributed random variables
dataset &lt;- runif(100)
#conduct Jarque-Bera test
jarque.bera.test(dataset)
</b>
This generates the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/jarqueBera2.jpg" alt="">
This tells us that the test statistic is 8.0807 and the p-value of the test is 0.01759. In this case, we would reject the null hypothesis that the data is normally distributed. We have sufficient evidence to say that the data in this example is not normally distributed.
This result shouldn’t be surprising since the dataset we generated is composed of 100 random variables that follow a uniform distribution. After all, the data is expected to be uniformly distributed, not normally distributed.
<h2><span class="orange">How to Create a Residual Plot in Excel</span></h2>
A <b>residual plot</b> is a type of plot that displays the fitted values against the residual values for a regression model.
This type of plot is often used to assess whether or not a linear regression model is appropriate for a given dataset and to check for  heteroscedasticity  of residuals.
This tutorial explains how to create a residual plot for a simple linear regression model in Excel.
<h2>How to Create a Residual Plot in Excel</h2>
Use the following steps to create a residual plot in Excel:
<b>Step 1: Enter the data values in the first two columns. </b>For example, enter the values for the predictor variable in A2:A13 and the values for the response variable in B2:B13.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot1.jpg">
<b>Step 2: Create a scatterplot. </b>Highlight the values in cells A2:B13. Then, navigate to the <em>INSERT </em>tab along the top ribbon. Click on the first option for <em>Scatter </em>within the <em>Charts </em>area.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot2.jpg">
The following chart will appear:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot3.jpg">
<b>Step 3: Display trend line equation on the scatterplot. </b>Click “Add Chart Elements” from the <em>DESIGN </em>tab, then “Trendline”, and then “More Trendline Option. Leave “Linear” selected and check “Display Equation on Chart.” Close the “Format Trendline” panel.
The trend line equation will now be displayed on the scatterplot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot4.jpg">
<b>Step 4: Calculate the predicted values. </b>Enter the trendline equation in cell C2, replacing “x” with “A1” like so:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot5.jpg">
Then, click cell C2 and double-click the small “Fill Handle” at the bottom right of the cell. This will copy the formula in cell C2 to the rest of the cells in the column:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot6.jpg">
<b>Step 5: Calculate the residuals.</b> Enter <em>B2-C2</em> in cell D2. Then, click cell D2 and double-click the small “Fill Handle” at the bottom right of the cell. This will copy the formula in cell D2 to the rest of the cells in the column:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot7.jpg">
<b>Step 6: Create the residual plot. </b>Highlight cells A2:A13. Hold the “Ctrl” key and highlight cells D2:D13. Then, navigate to the <em>INSERT </em>tab along the top ribbon. Click on the first option for <em>Scatter </em>within the <em>Charts </em>area. The following chart will appear:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot8.jpg">
This is the residual plot. The x-axis displays the fitted values and the y-axis displays the residuals.
Feel free to modify the title, axes, and gridlines to make the plot look more visually appealing:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/residPlot9.jpg">
<h2><span class="orange">How to Create a Survival Curve in Excel</span></h2>
A <b>survival curve</b> is a chart that shows the proportion of a population that is still alive after a given age, or at a given time after contracting some type of disease.
This tutorial shows how to create a survival curve in Excel.
<h2>Creating a Survival Curve in Excel</h2>
Suppose we have the following dataset that shows how long a patient was in a medical trial (<em>column A</em>) and whether or not the patient was still alive at the end of the trial (<em>column B</em>).
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/survivalCurve.jpg"><figcaption></figcaption></figure>
In order to create a survival curve for this data, we need to first get the data in the correct format, then use the built-in Excel charts to create the curve.
<h3>Formatting the Data</h3>
Use the following steps to get the data in the correct format.
<b>Step 1: List all of the unique “Years in trial” values in column A in column D:</b>
<em>Note: Always include “0” as the first value.</em>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve2-1.jpg"><figcaption>
</figcaption></figure>
<b>Step 2: Create the values in columns E through H using the formulas shown below.</b>
<em>Note: we removed the value “18” in column D since there were no death values associated with that time.</em>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve3.jpg"></figure>Here are the formulas used in the following cells:
<b>E3:</b> =COUNTIFS($A$2:$A$16,D3,$B$2:$B$16,1)
<b>F2: </b>=COUNTIF($A$2:$A$16, “>”&D2-1)
<b>G3:</b> =1-(E3/F3)
<b>H2:</b> =1
<b>H3:</b> =H2*G3
In order to fill in all of the other values in column E, simply highlight the range E4:E13 and press Ctrl-D. Fill in all of the other values in columns F through H using the same trick.
Now we’re ready to create the survival curve.
<h3>Creating the Survival Curve</h3>
Use the following steps to create the survival curve.
<b>Step 1: Copy the values in columns D and H into the columns J and K.</b>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve4.jpg"></figure>
<b>Step 2: Copy the values in the range J3:J13 to J14:J24. Then copy the values in the range K2:K12 to K14:K24.</b>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve5.jpg"></figure>
<b>Step 3: Create a list of values in column L as shown below, then sort from smallest to largest values in column L:</b>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve6.jpg" alt=""><figcaption>Create list of values in column L</figcaption></figure>
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve7.jpg" alt=""><figcaption>Sort columns J through L smallest to largest based on column L</figcaption></figure>
<b>Step 4: Highlight cells J2:K24, then select “Insert” > “Charts|Scatter” > “Scatter with Straight Lines and Markers” option.</b>
Feel free to modify the title, axes names, and chart colors. The resulting chart will look something like this:
<figure><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/surivivalCurve8.jpg"></figure>
<h2><span class="orange">How to Easily Calculate the Dot Product in Excel</span></h2>
This tutorial explains how to calculate the dot product in Excel.
<h3>What is the Dot Product?</h3>
Given vector <em>a</em> = [a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>] and vector <em>b</em> = [b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>], the <b>dot product</b> of vector a and vector b, denoted as <b>a · b</b>, is given by:
<b>a · b</b> = a<sub>1</sub> * b<sub>1</sub> + a<sub>2</sub> * b<sub>2</sub> + a<sub>3</sub> * b<sub>3</sub>
For example, if <em>a</em> = [2, 5, 6] and <em>b</em> = [4, 3, 2], then the dot product of <em>a</em> and <em>b</em> would be equal to:
<b>a · b = </b>2*4 + 5*3 + 6*2
<b>a · b = </b>8 + 15 + 12
<b>a · b = </b>35
In essence, the <b>dot product </b>is the sum of the products of the corresponding entries in two vectors.
<h3>How to Find the Dot Product in Excel</h3>
To find the dot product of two vectors in Excel, we can use the followings steps:
<b>1. Enter the data</b>. Enter the  data values for each vector in their own columns. For example, enter the data values for vector <em>a</em> = [2, 5, 6] into column A and the data values for vector <em>b</em> = [4, 3, 2] into column B:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/dotProduct1.jpg" alt="">
<b>2. Calculate the dot product.</b> To calculate the dot product, we can use the Excel function <b>SUMPRODUCT()</b>, which uses the following syntax:
<b>SUMPRODUCT(array1, [array2], …)</b>
<b>array </b>– the first array or range to multiply, then add.
<b>array2 </b>– the second array or range to multiply, then add.
In this example, we can type the following into cell <b>D1</b> to calculate the dot product between vector <em>a</em> and vector<em> b</em>:
<b>=SUMPRODUCT(A1:A3, B1:B3)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/dotProduct2.jpg" alt="">
This produces the value <b>35</b>, which matches the answer we got by hand.
Note that we can use <b>SUMPRODUCT() </b>to find the dot product for any length of vectors. For example, suppose vector <em>a </em>and <em>b </em>were both of length 20. Then we could enter the following formula in cell <b>D1 </b>to calculate their dot product:
<b>=SUMPRODUCT(A1:A20, B1:B20)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/dotProduct3.jpg" alt="">
<h3>Potential Errors in Calculating the Dot Product</h3>
The function <b>SUMPRODUCT() </b>will return a <b>#VALUE!</b> error if the vectors do not have equal length.
For example, if vector <em>a</em> has length 20 and vector <em>b</em> has length 19, then the formula <b>=SUMPRODUCT(A1:A20, B1:B19)</b> will return an error.
The two vectors need to have the same length in order for the dot product to be calculated.
<h2><span class="orange">How to Easily Calculate the Mean Absolute Deviation in Excel</span></h2>
The <b>mean absolute deviation </b>is a way to measure the dispersion for a set of data values.
A low value for the mean absolute deviation is an indication that the data values are concentrated closely together. A high value for the mean absolute deviation is an indication that the data values are more spread out.
The formula to calculate the mean absolute deviation is as follows:
<b>Mean absolute deviation </b>= (Σ |x<sub>i</sub> – x|) / n
<b>Σ</b> – just a fancy symbol that means “sum”
<b>x<sub>i</sub></b> – the i<sup>th</sup> data value
<b>x </b>– the mean value
<b>n </b>– sample size
<h2>How to Calculate the Mean Absolute Deviation in Excel</h2>
To calculate the <b>mean absolute deviation </b>in Excel, we can perform the following steps:
<b>Step 1: Enter the data</b>. For this example, we’ll enter 15 data values in cells A2:A16.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/mad1.jpg" alt="">
<b>Step 2: Find the mean value</b>. In cell D1, type the following formula: <b>=AVERAGE(A2:A16)</b>. This calculates the mean value for the data values, which turns out to be <b>15.8</b>.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/mad2.jpg" alt="">
<b>Step 3: Calculate the absolute deviations.</b> In cell B2, type the following formula: <b>=ABS(A2-$D$1)</b>. This calculates the absolute deviation of the value in cell A2 from the mean value in the dataset.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/mad3.jpg" alt="">
Next, click cell B2. Then, hover over the bottom right corner of the cell until a black <b>+ </b>sign appears. Double click the <b>+ </b>sign to fill in the remaining values in column B.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/mad4.jpg" alt="">
<b>Step 4: Calculate the mean absolute deviation</b>. In cell B17, type the following formula: <b>=AVERAGE(B2:B16)</b>. This calculates the mean absolute deviation for the data values, which turns out to be <b>6.1866</b>.
Note that you can use these four steps to calculate the mean absolute deviation for any number of data values. In this example, we used 15 data values but you could use these exact steps to calculate the mean absolute deviation for 5 data values or 5,000 data values.
<em>Another common way to measure the forecasting accuracy of a model is MAPE – mean absolute percentage error. Read about how to calculate MAPE in Excel  here .</em>
<h2><span class="orange">How to Find a P-Value from a Z-Score in Excel</span></h2>
Many  hypothesis tests  in statistics result in a z-test statistic. Once we find this z-test statistic, we typically find the p-value associated with it. If this p-value is less than a certain alpha level (e.g. 0.10, 0.05, 0.01), then we reject the null hypothesis of the test and conclude that our findings are significant.
This tutorial illustrates several examples of how to find the p-value from a z-score in Excel using the function <b>NORM.DIST</b>, which takes the following arguments:
<b>NORM.DIST</b>(x, mean, standard_dev, cumulative)
where:
<em>x </em>is the z-score we’re interested in.
<em>mean </em>is the mean of the distribution – we’ll use “0” for the standard normal distribution.
<em>standard_dev </em>is the standard deviation of the distribution – we’ll use “1” for the standard normal distribution.
<em>cumulative </em>takes a value of “TRUE” (returns the CDF) or “FALSE” (returns the PDF) – we’ll use “TRUE” to get the value of the cumulative distribution function.
Let’s check out a couple examples.
<h3>Example 1: Finding a P-value from a Z-score (Two-Tailed Test)</h3>
A company wants to know whether or not a new type of battery has a different average life than the current standard battery, which has an average life of 18 hours. In  a random sample  of 100 of the new batteries, they find that the average life is 19 hours with a standard deviation of 4 hours.
Conduct a two-tailed hypothesis test using an alpha level of .05 to determine if the average life of the new battery is different than the average life of the current standard battery.
<b>Step 1: State the hypotheses. </b>
The null hypothesis (H<sub>0</sub>): μ = 18
The alternative hypothesis: (Ha): μ ≠ 18
<b>Step 2: Find the z-test statistic.</b>
Test statistic z  =  (x-μ) / (s/√n)  = (19-18) / (4/√100)  =<b> 2.5</b>
<b>Step 3: Find the p-value of the z-test statistic using Excel.</b>
To find the p-value for z = 2.5, we will use the following formula in Excel: <b>=1 – NORM.DIST(2.5, 0, 1, TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/pvalue_score_excel1.jpg">
This tells us that the one-sided p-value is <b>.00621</b>, but since we’re conducting a two-tailed test we need to multiply this value by 2, so the p-value will be .00612 * 2 = <b>.01224</b>.
<b>Step 4: Reject or fail to reject the null hypothesis.</b>
Since the p-value of <b>.01224 </b>is less than our chosen alpha level of <b>.05</b>, we reject the null hypothesis. We have sufficient evidence to say that the average life of the new battery is significantly different than the average life of the current standard battery.
<h3>Example 2: Finding a P-value from a Z-score (One-Tailed Test)</h3>
A botanist believes that the mean height of a certain plant is less than 14 inches. She randomly selects 30 plants and measures them. She finds that the mean height is 13.5 inches with a standard deviation of 2 inches.
Conduct a one-tailed hypothesis test using an alpha level of .01 to determine if the mean height of this plant is actually less than 14 inches.
<b>Step 1: State the hypotheses. </b>
The null hypothesis (H0): μ≥ 14
The alternative hypothesis: (Ha): μ &lt; 14
<b>Step 2: Find the z-test statistic.</b>
Test statistic z  =  (x-μ) / (s/√n)  = (13.5-14) / (2/√30)  = <b>-1.369</b>
<b>Step 3: Find the p-value of the z-test statistic using Excel.</b>
To find the p-value for z = -1.369, we will use the following formula in Excel: <b>=NORM.DIST(-1.369, 0, 1, TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/pvalue_score_excel2.jpg"426">
This tells us that the one-sided p-value is <b>.08550</b>.
<b>Step 4: Reject or fail to reject the null hypothesis.</b>
Since the p-value of <b>.08550 </b>is greater than our chosen alpha level of <b>.01</b>, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the average height of this certain species of plant is less than 14 inches.
<em><b>For more statistics tutorials in Excel, be sure to check out our complete list of  Excel Guides .</b></em>
<h2><span class="orange">How to Find Class Boundaries (With Examples)</span></h2>
In a frequency distribution, <b>class boundaries</b> are the values that separate the classes.
We use the following steps to calculate the class boundaries in a frequency distribution:
<b>1. </b>Subtract the upper class limit for the first class from the lower class limit for the second class.
<b>2.</b> Divide the result by two.
<b>3. </b>Subtract the result from the lower class limit and add the result to the the upper class limit for each class.
The following examples show how to use these steps in practice to calculate class boundaries in a frequency distribution.
<h3>Example 1: Calculating Class Boundaries</h3>
Suppose we have the following frequency distribution that represents the number of wins by various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries1.png">
Use the following steps to calculate the class boundaries:
<b>1. </b>Subtract the upper class limit for the first class from the lower class limit for the second class.
The upper class limit for the first class is 30 and the lower class limit for the second class is 31. Thus, we get: 31 – 30 = <b>1</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries2.png">
<b>2.</b> Divide the result by two.
Next, we divide the result by 2. So, we get 1/2 = <b>0.5</b>.
<b>3. </b>Subtract the result from the lower class limit and add the result to the the upper class limit for each class.
Lastly, we subtract 0.5 from the lower class limit and add 0.5 to the upper class limit for each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries3.png">
We interpret the results as follows:
The first class has a lower class boundary of 25.5 and an upper class boundary of 30.5.
The second class has a lower class boundary of 30.5 and an upper class boundary of 35.5.
The third class has a lower class boundary of 35.5 and an upper class boundary of 40.5.
And so on.
<h3>Example 2: Calculating Class Boundaries</h3>
Suppose we have the following frequency distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries4.png">
Use the following steps to calculate the class boundaries:
<b>1. </b>Subtract the upper class limit for the first class from the lower class limit for the second class.
The upper class limit for the first class is 60.9 and the lower class limit for the second class is 61. Thus, we get: 61 – 60.9 = <b>0.1</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries5.png">
<b>2.</b> Divide the result by two.
Next, we divide the result by 2. So, we get 0.1/2 = <b>0.05</b>.
<b>3. </b>Subtract the result from the lower class limit and add the result to the the upper class limit for each class.
Lastly, we subtract 0.05 from the lower class limit and add 0.05 to the upper class limit for each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries6-1.png">
We interpret the results as follows:
The first class has a lower class boundary of 55.95 and an upper class boundary of 60.95.
The second class has a lower class boundary of 60.95 and an upper class boundary of 65.95.
The third class has a lower class boundary of 65.95 and an upper class boundary of 70.95.
And so on.
<h2><span class="orange">How to Find Class Intervals (With Examples)</span></h2>
In a frequency distribution, a <b>class interval</b> represents the difference between the upper class limit and the lower class limit.
In other words, a class interval represents the <b>width</b> of each class in a frequency distribution.
The following examples show how to calculate class intervals for different frequency distributions.
<h3>Example 1: Calculating Class Intervals</h3>
Suppose we have the following frequency distribution that represents the number of wins by different basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries1.png">
The lower class limit and upper class limit are simply the smallest and largest possible values in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classLimit2.png">
The <b>class interval</b> is the difference between the upper class limit and the lower class limit.
For example, the size of the class interval for the first class is <b>30 – 26 = 4</b>.
Similarly, the size of the class interval for the second class is <b>31 – 35 = 4</b>.
If we calculate the size of the class interval for each class in the frequency distribution, we’ll find that each class interval has a size of <b>4</b>.
<h3>Example 2: Calculating Class Intervals</h3>
Suppose we have the following frequency distribution that represents the exam grades received by students in a certain class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classInt1.png">
The lower class limit and upper class limit are simply the smallest and largest possible values in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classInt2.png">
The <b>class interval</b> is the difference between the upper class limit and the lower class limit.
For example, the size of the class interval for the first class is <b>30 – 21 = 9</b>.
Similarly, the size of the class interval for the second class is <b>40 – 31 = 9</b>.
If we calculate the size of the class interval for each class in the frequency distribution, we’ll find that each class interval has a size of <b>9</b>.
<h2><span class="orange">How to Find Class Limits (With Examples)</span></h2>
In a frequency distribution, <b>class limits</b> represent the smallest and largest data values that can belong to each class.
Each class in a frequency distribution has a lower class limit and an upper class limit:
<b>Lower class limit:</b> The smallest data value that can belong to a class.
<b>Upper class limit:</b> The largest data value that can belong to a class.
The following examples show how to find class limits for different frequency distributions.
<h3>Example 1: Finding Class Limits in a Frequency Distribution</h3>
Suppose we have the following frequency distribution that represents the number of wins by different basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries1.png">
The <b>lower class limit</b> is simply the smallest possible value in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classLimit1.png">
Conversely, the <b>upper class limit</b> is the largest possible value in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classLimit2.png">
<h3>Example 2: Finding Class Limits in a Frequency Distribution</h3>
Suppose we have the following frequency distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/classBoundaries4.png">
The <b>lower class limit</b> is the smallest possible value in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classLimit3.png">
And the <b>upper class limit</b> is the largest possible value in each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/classLimit4.png">
<h2><span class="orange">How to Find Class Size (With Examples)</span></h2>
In statistics, <b>class size</b> refers to the difference between the upper and lower boundaries of a class in a frequency distribution.
The following examples shows how to find the class size for various frequency distributions.
<h3>Example 1: Finding Class Size for Basketball Data</h3>
Suppose we have the following frequency distribution that describes the number of points scored by various basketball players in a league:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/class_size1.png">
The first class has a lower limit of 1 and an upper limit of 5. Thus, the class size would be calculated as:
Class size: 5 – 1 = <b>4</b>
The second class has a lower limit of 6 and an upper limit of 10. Thus, the class size would be calculated as:
Class size: 10 – 6 = <b>4</b>
No matter which class we analyze in the frequency distribution, we’ll find that the class size is <b>4</b>. 
<h3>Example 2: Finding Class Size for Sales Data</h3>
Suppose we have the following frequency distribution that describes the number of widgets sold by a certain company on different days:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/class_size2.png">
The first class has a lower limit of 1 and an upper limit of 10. Thus, the class size would be calculated as:
Class size: 10 – 1 = <b>9</b>
The second class has a lower limit of 11 and an upper limit of 20. Thus, the class size would be calculated as:
Class size: 20 – 11 = <b>9</b>
No matter which class we analyze in the frequency distribution, we’ll find that the class size is <b>9</b>. 
<h2><span class="orange">How to Find the Interquartile Range (IQR) of a Box Plot</span></h2>
A <b>box plot</b> is a type of plot that displays the five number summary of a dataset, which includes:
The minimum value
The first quartile (the 25th percentile)
The median value
The third quartile (the 75th percentile)
The maximum value
To make a box plot, we draw a box from the first to the third quartile. Then we draw a vertical line at the median. Lastly, we draw “whiskers” from the quartiles to the minimum and maximum value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/iqr_box3.png">
The <b>interquartile range</b>, often abbreviated IQR, is the difference between the third quartile and the first quartile.
IQR  = Q3 – Q1
This tells us how spread out the middle 50% of values are in a given dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/iqr_box4.png">
The following examples show how to find the interquartile range (IQR) of a box plot in practice.
<h2>Example 1: Exam Scores</h2>
The following box plot shows the distribution of scores on a certain college exam. What is the interquartile range of the exam scores?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/iqr_box1.png">
We can find the following values on the box plot to answer this:
Q3 (Upper Quartile) = 90
Q1 (Lower Quartile) = 70
Interquartile Range (IQR) = 90 – 70 = 20
The interquartile range of the exam scores is <b>20</b>.
<h2>Example 2: Points Scored</h2>
The following box plot shows the distribution of points scored by basketball players in a certain league. What is the interquartile range of the distribution?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/iqr_box2.png"> 
We can find the following values on the box plot to answer this:
Q3 (Upper Quartile) = 27
Q1 (Lower Quartile) = 15
Interquartile Range (IQR) = 27 – 15 = 12
The interquartile range of the distribution is <b>12</b>.
<h2>Example 3: Comparing Plant Heights</h2>
The following box plots show the distribution of heights for two different plant species: Red and Blue. Which distribution has a larger interquartile range?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/iqr_box5.png">
First, let’s find the interquartile range of the red box plot:
Q3 (Upper Quartile) = 30
Q1 (Lower Quartile) = 20
Interquartile Range (IQR) = 30 – 20 = <b>10</b>
Next, let’s find the interquartile range of the blue box plot:
Q3 (Upper Quartile) = 27
Q1 (Lower Quartile) = 15
Interquartile Range (IQR) = 27 – 15 = <b>12</b>
The interquartile range for the Blue species is larger.
<h2>Additional Resources</h2>
The following tutorials provide additional information about box plots:
 Box Plot Generator 
 How to Compare Box Plots 
 How to Identify Skewness in Box Plots 
 How to Interpret the Interquartile Range 
<h2><span class="orange">How to Find the Max Value of Columns in Pandas</span></h2>
Often you may be interested in finding the max value of one or more columns in a pandas DataFrame. Fortunately you can do this easily in pandas using the  max()  function.
This tutorial shows several examples of how to use this function.
<h3>Example 1: Find the Max Value of a Single Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [np.nan, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame 
df
        playerpointsassistsrebounds
0A255NaN
1B2078.0
2C14710.0
3D1686.0
4E2756.0
5F2079.0
6G1266.0
7H15910.0
8I14910.0
9J1957.0
</b>
We can find the max value of the column titled “points” by using the following syntax:
<b>df['points'].max()
27</b>
The max() function will also exclude NA’s by default. For example, if we find the max of the “rebounds” column, the first value of “NaN” will simply be excluded from the calculation:
<b>df['rebounds'].max()
10.0
</b>
The max of a string column is defined as the highest letter in the alphabet:
<b>df['player'].max()
'J'
</b>
<h3>Example 2: Find the Max of Multiple Columns</h3>
We can find the max of multiple columns by using the following syntax:
<b>#find max of points and rebounds columns
df[['rebounds', 'points']].max()
rebounds    10.0
points      27.0
dtype: float64
</b>
<h3>Example 3: Find the Max of All Columns</h3>
We can find also find the max of all numeric columns by using the following syntax:
<b>#find max of all numeric columns in DataFrame
df.max()
player       J
points      27
assists      9
rebounds    10
dtype: object
</b>
<h3>Example 4: Find Row that Corresponds to Max</h3>
We can find also return the entire row that corresponds to the max value in a certain column. For example, the following syntax returns the entire row that corresponds to the player with the max points:
<b>#return entire row of player with the max points
df[df['points']==df['points'].max()]
playerpointsassistsrebounds
4E2756.0
</b>
If multiple rows have the same max value, each row will be returned. For example, suppose player D also scored 27 points:
<b>#return entire row of players with the max points
df[df['points']==df['points'].max()]
        playerpointsassistsrebounds
3D2786.0
4E2756.0
</b>
<em>You can find the complete documentation for the max() function  here .</em>
<h2><span class="orange">How to Find the P-Value from the Chi-Square Distribution Table</span></h2>
The  Chi-square distribution table <b> </b>is a table that shows the critical values of the Chi-square distribution. To use the Chi-square distribution table, you only need two values:
A significance level (common choices are 0.01, 0.05, and 0.10)
Degrees of freedom
The Chi-square distribution table is commonly used in the following statistical tests:
 Chi-Square Test of Independence 
 Chi-Square Goodness of Fit Test 
When you conduct each of these tests, you’ll end up with a test statistic <em>X<sup>2</sup></em>. To find out if this test statistic is statistically significant at some alpha level, you have two options:
Compare the test statistic <em>X<sup>2 </sup></em>to a critical value from the Chi-square distribution table.
Compare the p-value of the test statistic <em>X<sup>2</sup></em><em> </em>to a chosen alpha level.
Let’s walk through an example of how to use each of these approaches.
<h2>Examples</h2>
Suppose we conduct some type of Chi-Square test and end up with a test statistic <em>X<sup>2  </sup></em>of <b>27.42 </b>and our degrees of freedom is <b>14</b>. We would like to know if these results are statistically significant.
<h3>Compare the test statistic <em>X<sup>2</sup></em><em> </em>to a critical value from the Chi-square distribution table</h3>
The first approach we can use to determine if our results are statistically significant is to compare the test statistic <em>X<sup>2 </sup></em><em> </em>of <b>27.42 </b>to the critical value in the Chi-square distribution table. The critical value is the value in the table that aligns with a significance value of <b>0.05 </b>and a degrees of freedom of <b>14</b>. This number turns out to be <b>23.685</b>:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/chiTable1.jpg" alt="">
Since out test statistic <em>X<sup>2</sup></em><em> </em>(<b>27.42</b>) is larger than the critical value (<b>23.685</b>), we reject the null hypothesis of our test. We have sufficient evidence to say that our results are statistically significant at alpha level 0.05.
<h3>Compare the p-value of the test statistic <em>X<sup>2</sup></em><em> </em>to a chosen alpha level</h3>
The second approach we can use to determine if our results are statistically significant is to find the p-value for the test statistic <em>X<sup>2</sup></em><em> </em>of <b>27.42</b>. In order to find this p-value, <b>we can’t use the Chi-square distribution table because it only provides us with critical values, not p-values</b>.
So, in order to find this p-value we need to use a  Chi-Square Distribution Calculator  with the following inputs:
<b><em>Note</em></b>:<em> Fill in the values for “Degrees of Freedom” and “Chi-square critical value”, but leave “cumulative probability” blank and click the “Calculate P-value” button.</em>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/chiTable2.jpg" alt="">
The calculator returns the cumulative probability, so to find the p-value we can simply use 1 – 0.98303 = <b>0.01697</b>.
Since the p-value <b>(0.01697) </b>is less than our alpha level of <b>0.05</b>, we reject the null hypothesis of our test. We have sufficient evidence to say that our results are statistically significant at alpha level 0.05.
<h2>When to Use the Chi-square Distribution Table</h2>
If you are interested in finding the Chi-square critical value for a given significance level and degrees of freedom, then you should use the  Chi-square Distribution Table .
Instead, if you have a given test statistic <em>X<sup>2</sup></em> and you simply want to know the p-value of that test statistic, then you would need to use a  Chi-Square Distribution Calculator  to do so.
<h2><span class="orange">How to Find the T Critical Value in Excel</span></h2>
Whenever you conduct a t-test, you will get a test statistic as a result. To determine if the results of the t-test are statistically significant, you can compare the test statistic to a<b> T critical value</b>. If the absolute value of the test statistic is greater than the T critical value, then the results of the test are statistically significant.
The T critical value can be found by using a  t distribution table  or by using statistical software.
To find the T critical value, you need to specify:
A significance level (common choices are 0.01, 0.05, and 0.10)
The degrees of freedom
The type of test (one-tailed or two-tailed)
Using these three values, you can determine the T critical value to be compared with the test statistic.
<b>Related:  How to Find the Z Critical Value in Excel </b>
<h2>How to Find the T Critical Value in Excel</h2>
Excel offers two functions to find the T critical value.
<h3>T.INV</h3>
To find the T critical value in Excel for a <em>one-tailed test</em>, you can use the<b> T.INV.()</b> function, which uses the following syntax:
<b>T.INV</b>(probability, deg_freedom)
<b>probability: </b>The significance level to use
<b>deg_freedom</b>: The degrees of freedom
This function returns the critical value from the t distribution for a one-tailed test based on the significance level and the degrees of freedom provided.
<h3>T.INV.2T</h3>
To find the T critical value in Excel for a <em>two-tailed test</em>, you can use the<b> T.INV.2T()</b> function, which uses the following syntax:
<b>T.INV.2T</b>(probability, deg_freedom)
<b>probability: </b>The significance level to use
<b>deg_freedom</b>: The degrees of freedom
This function returns the critical value from the t distribution for a two-tailed test based on the significance level and the degrees of freedom provided.
<h2>Examples of Finding the T Critical Value in Excel</h2>
The following examples illustrate how to find the T critical value for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test</h3>
To find the T critical value for a left-tailed test with a significance level of 0.05 and degrees of freedom = 11, we can type the following formula into Excel: <b>T.INV(0.05, 11)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/t_left.jpg">
This returns the value <b>-1.79588</b>. This is the critical value for a left-tailed test with significance level of 0.05 and degrees of freedom = 11.
<h3>Right-tailed test</h3>
To find the T critical value for a right-tailed test with a significance level of 0.05 and degrees of freedom = 11, we can type the following formula into Excel: <b>ABS(T.INV(0.05, 11))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/t_right.jpg">
This returns the value <b>1.79588</b>. This is the critical value for a two-tailed test with significance level of 0.05 and degrees of freedom = 11.
<h3>Two-tailed test</h3>
To find the T critical value for a two-tailed test with a significance level of 0.05 and degrees of freedom = 11, we can type the following formula into Excel: <b>T.INV.2T(0.05, 11)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/t_two.jpg">
This returns the value <b>2.200985</b>. This is the critical value for a two-tailed test with significance level of 0.05 and degrees of freedom = 11.
Note that this also matches the number we would find in the  t distribution table  with α = 0.05 for two tails and DF <em>(degrees of freedom)</em> = 11.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/ttable.jpg">
<h2>Cautions on Finding the T Critical Value in Excel</h2>
Note that both the <b>T.INV()</b> and <b>T.INV.2T() </b>functions in Excel will throw an error if any of the following occur:
If any argument is non-numeric.
If the value for <em>probability </em>is less than zero or greater than 1.
If the value for <em>deg_freedom</em><em> </em>is less than 1.
 
<h2><span class="orange">How to Find the T Critical Value in Python</span></h2>
Whenever you conduct a t-test, you will get a test statistic as a result. To determine if the results of the t-test are statistically significant, you can compare the test statistic to a<b> T critical value</b>. If the absolute value of the test statistic is greater than the T critical value, then the results of the test are statistically significant.
The T critical value can be found by using a  t distribution table  or by using statistical software.
To find the T critical value, you need to specify:
A significance level (common choices are 0.01, 0.05, and 0.10)
The degrees of freedom
Using these two values, you can determine the T critical value to be compared with the test statistic.
<h3>How to Find the T Critical Value in Python</h3>
To find the T critical value in Python, you can use the  scipy.stats.t.ppf() function , which uses the following syntax:
<b>scipy.stats.t.ppf(q, df)</b>
where:
<b>q: </b>The significance level to use
<b>df</b>: The degrees of freedom
The following examples illustrate how to find the T critical value for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test </h3>
Suppose we want to find the T critical value for a left-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>import scipy.stats
#find T critical value
scipy.stats.t.ppf(q=.05,df=22)
-1.7171
</b>
The T critical value is <b>-1.7171</b>. Thus, if the test statistic is less than this value, the results of the test are statistically significant.
<h3>Right-tailed test </h3>
Suppose we want to find the T critical value for a right-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>import scipy.stats
#find T critical value
scipy.stats.t.ppf(q=1-.05,df=22)
1.7171
</b>
The T critical value is <b>1.7171</b>. Thus, if the test statistic is greater than this value, the results of the test are statistically significant.
<h3>Two-tailed test </h3>
Suppose we want to find the T critical value for a two-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>import scipy.stats
#find T critical value
scipy.stats.t.ppf(q=1-.05/2,df=22)
2.0739
</b>
Whenever you perform a two-tailed test, there will be two critical values. In this case, the T critical values are <b>2.0739 </b>and <b>-2.0739</b>. Thus, if the test statistic is less than -2.0739 or greater than 2.0739, the results of the test are statistically significant.
<em>Refer to the  SciPy documentation  for the exact details of the t.ppf() function.</em>
<h2><span class="orange">How to Find the T Critical Value on a TI-84 Calculator</span></h2>
When you conduct a t-test, you will get a test statistic as a result. To determine if the results of the t-test are statistically significant, you can compare the test statistic to a<b> T critical value</b>. If the absolute value of the test statistic is greater than the T critical value, then the results of the test are statistically significant.
To find the T critical value on a TI-84 calculator, we can use the following function:
<b>invT(probability, v)</b>
where:
<b>probability:</b> the significance level
<b>v:</b> the degrees of freedom
You can access this function on a TI-84 calculator by pressing 2nd and then pressing vars. This will take you to a <b>DISTR </b>screen where you can then use <b>invT()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tcritTI1.png">
This tutorial shares several examples of how to use the invT() function to find T critical values on a TI-84 calculator.
<h3>Example 1: T Critical Value for a Left-Tailed Test</h3>
<b>Question: </b>Find the T critical value for a left-tailed test with a significance level of 0.05 and degrees of freedom = 11.
<b>Answer: </b>invT(.05, 11) = <b>-1.7959</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tcritTI2.png">
<b>Interpretation: </b>If the test statistic of the t-test is less than <b>-1.7959</b>, then the results of the test are statistically significant at α = 0.05.
<h3>Example 2: T Critical Value for a Right-Tailed Test</h3>
<b>Question: </b>Find the T critical value for a right-tailed test with a significance level of 0.05 and degrees of freedom = 24.
<b>Answer: </b>invT(1-.05, 24) = <b>1.71088</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tcritTI3.png">
<b>Interpretation: </b>If the test statistic of the t-test is greater than <b>1.71088</b>, then the results of the test are statistically significant at α = 0.05.
<h3>Example 3: T Critical Value for a Two-Tailed Test</h3>
<b>Question: </b>Find the T critical value for a two-tailed test with a significance level of 0.05 and degrees of freedom = 13.
<b>Answer: </b>invT(.05/2, 13) = <b>-2.1604, 2.1604</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/tcritTI4.png">
<b>Interpretation: </b>Since this is a two=tailed test, we actually have two critical values: <b>-2.1604</b> and <b>2.1604</b>. If the test statistic of the t-test is less than <b>-2.1604 </b>or greater than <b>2.1604</b>, then the results of the test are statistically significant at α = 0.05.
<h2><span class="orange">How to Find Z Alpha/2 (za/2)</span></h2>
Whenever you come across the term <b>z<sub>α/2</sub></b> in statistics, it is simply referring to the <b>z critical value </b>from the  z table  that corresponds to α/2.
This tutorial explains the following:
How to find z<sub>α/2</sub> using a z table.
How to find z<sub>α/2</sub> using a calculator.
The most common values for z<sub>α/2</sub>.
Let’s jump in!
<h3>How to find z<sub>α/2</sub> using a z table</h3>
Suppose we want to find z<sub>α/2</sub> for some test that is using a 90% confidence level.
In this case, α would be 1 – 0.9 = <b>0.1</b>. Thus, α/2 = 0.1/2 = <b>0.05</b>.
To find the corresponding z critical value, we would simply look for <b>0.05</b> in a z table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/za2.png">
Notice that the exact value of <b>0.05 </b>doesn’t appear in the table, but it would be directly between the values <b>.0505 </b>and <b>.0495</b>. The corresponding z critical values on the outside of the table are <b>-1.64</b> and <b>-1.65</b>.
By splitting the difference, we see that the z critical value would be <b>-1.645</b>. And typically when we use z<sub>α/2</sub> we take the absolute value. Thus, z<sub>.01/2</sub> = <b>1.645</b>.
<h3>How to find z<sub>α/2</sub> using a calculator</h3>
We can also use a  Critical Z Value Calculator  to find z<sub>α/2</sub> for some test.
For example, for some test that is using a 90% confidence level we can simply enter <b>0.1 </b>as the significance level and the calculator will automatically return the value of <b>1.645 </b>as the corresponding critical z value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/za2_2.png">
<h3>Common Values for z<sub>α/2</sub></h3>
The following table displays the most common critical values for different values of α:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/za2_3.png">
The way to interpret this table is as follows:
For a test using a 90% confidence level (e.g. α = 0.1), the z critical value is <b>1.645</b>.
For a test using a 95% confidence level (e.g. α = 0.05), the z critical value is <b>1.96</b>.
For a test using a 99% confidence level (e.g. α = 0.01), the z critical value is <b>5.576</b>.
And so on.
<h2><span class="orange">How to Fix in R: error in sort.int(x, na.last, decreasing, …) : ‘x’ must be atomic</span></h2>
One error message you may encounter when using R is:
<b>Error in sort.int(x, na.last = na.last, decreasing = decreasing, ...) : 
  'x' must be atomic
</b>
This error occurs when you attempt to sort a list.
By default, R is only capable of sorting atomic objects like vectors. Thus, to use <b>sort()</b> with a list you must first use the <b>unlist()</b> function.
The following example shows how to resolve this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following list in R:
<b>#create list
some_list &lt;- list(c(4, 3, 7), 2, c(5, 12, 19)) 
#view list
some_list
[[1]]
[1] 4 3 7
[[2]]
[1] 2
[[3]]
[1]  5 12 19
#view class
class(some_list)
[1] "list"
</b>
Now suppose we attempt to sort the values in the list:
<b>#attempt to sort the values in the list
sort(some_list)
Error in sort.int(x, na.last = na.last, decreasing = decreasing, ...) : 
  'x' must be atomic
</b>
We receive an error because R is not capable of sorting lists directly.
<h3>How to Avoid the Error</h3>
To avoid the error, we must first use the <b>unlist()</b> function as follows:
<b>#sort values in list
sort(unlist(some_list))
[1]  2  3  4  5  7 12 19
</b>
Notice that we’re able to successfully sort the list of values without any error because we first used <b>unlist()</b>, which converted the list to a numeric vector.
By default, R sorts the values in ascending order.
However, we can use <b>decreasing=TRUE</b> to sort the values in descending order instead:
<b>#sort values in list in descending order
sort(unlist(some_list), decreasing=TRUE)
[1] 19 12  7  5  4  3  2
</b>
Notice that the values are now sorted in descending order.
<h2><span class="orange">How to Fix in R: non-numeric argument to binary operator</span></h2>
One error you may encounter in R is:
<b>Error in df$var1- df$var2: non-numeric argument to binary operator 
</b>
This error occurs when you attempt to perform some  binary operation  on two vectors and one of the vectors is non-numeric.
Examples of binary operations include:
Subtraction (<b>–</b>)
Addition (<b>+</b>)
Multiplication (<b>*</b>)
Division (<b>/</b>)
This error occurs most often when one of the vectors you provide is a character vector.
This tutorial shares exactly how to fix this error.
<h3>How to Reproduce the Error</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(period = c(1, 2, 3, 4, 5, 6, 7, 8), sales = c(14, 13, 10, 11, 19, 9, 8, 7), returns = c('1', '0', '2', '1', '1', '2', '2', '3'))
#view data frame
df
  period sales returns
1      1    14       1
2      2    13       0
3      3    10       2
4      4    11       1
5      5    19       1
6      6     9       2
7      7     8       2
8      8     7       3</b>
Now suppose we attempt to create a new column called ‘net’ by subtracting the ‘returns’ column from the ‘sales’ column:
<b>#attempt to create new column called 'net'
df$net &lt;- df$sales - df$returns
Error in df$sales * df$returns : non-numeric argument to binary operator
</b>
An error occurs because the ‘returns’ column is of the class ‘character’ and it’s not possible to subtract a character column from a numeric column.
<b>#display class of 'sales' column
class(df$sales)
[1] "numeric"
#display class of 'returns' column
class(df$returns)
[1] "character"
</b>
<h3>How to Fix the Error</h3>
The way to fix this error is to use <b>as.numeric()</b> to convert the ‘returns’ column to numeric before performing the subtraction:
<b>#create new column called 'net'
df$net &lt;- df$sales - as.numeric(df$returns)
#view updated data frame
df
  period sales returns net
1      1    14       1  13
2      2    13       0  13
3      3    10       2   8
4      4    11       1  10
5      5    19       1  18
6      6     9       2   7
7      7     8       2   6
8      8     7       3   4</b>
We’re able to perform the subtraction without any errors because both the ‘sales’ and the ‘returns’ columns were numeric.
<h2><span class="orange">How to Generate a Sample Using the Sample Function in R</span></h2>
The <b>sample()</b> function in R allows you to take a random sample of elements from a dataset or a vector, either with or without replacement.
The basic syntax for the sample() function is as follows:
<b>sample(x, size, replace = <span>FALSE, prob = <span>NULL)</b>
<b>x</b>: a dataset or vector from which to choose the sample
<b>size</b>: size of the sample
<b>replace</b>: should sampling be with replacement? (this is FALSE by default)
<b>prob</b>: a vector of probability weights for obtaining the elements of the vector being sampled
<em>The complete documentation for sample() can be found  here .</em>
The following examples illustrate practical examples of using sample().
<h2>Generating a Sample from a Vector</h2>
Suppose we have vector <em>a </em>with 10 elements in it:
<b>#define vector <em>a</em> with 10 elements in it
a &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)</b>
To generate a random sample of 5 elements from vector <em>a</em> without replacement, we can use the following syntax:
<b>#generate random sample of 5 elements from vector <em>a
</em>sample(a, 5)
#[1] 3 1 4 7 5</b>
It’s important to note that each time we generate a random sample, it’s likely that we will get a different set of elements each time. 
<b>#generate another random sample of 5 elements from vector <em>a
</em>sample(a, 5)
#[1] 1 8 7 4 2</b>
If we would like to be able to replicate our results and work with the same sample each time, we can use <b>set.seed()</b>.
<b>#set.seed(some random number) to ensure that we get the same sample each time
set.seed(122)
#define vector <em>a</em> with 10 elements in it
a &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
#generate random sample of 5 elements from vector <em>a
</em>sample(a, 5)
#[1] 10 9 2 1 4
#generate another random sample of 5 elements from vector <em>a
</em>sample(a, 5)
#[1] 10 9 2 1 4
</b>
We can also use the argument <b>replace = TRUE</b> so that we are sampling with replacement. This means that each element in the vector can be chosen to be in the sample more than once.
<b>#generate random sample of 5 elements from vector <em>a </em>using sampling with replacement
sample(a, 5, replace = TRUE)
# 10 10 2 1 6</b>
<h2>Generating a Sample from a Dataset</h2>
Another common use of the sample() function is to generate a random sample of rows from a dataset. For the following example, we will generate a random sample of 10 rows from the built-in R dataset <b>iris</b>, which has 150 total rows.
<b>#view first 6 rows of iris dataset
head(iris)
# Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1         5.1         3.5          1.4         0.2  setosa
#2         4.9         3.0          1.4         0.2  setosa
#3         4.7         3.2          1.3         0.2  setosa
#4         4.6         3.1          1.5         0.2  setosa
#5         5.0         3.6          1.4         0.2  setosa
#6         5.4         3.9          1.7         0.4  setosa</b>
<b>#set<b> seed to ensure that this example is replicable</b>
set.seed(100)
#choose a random vector of 10 elements from all 150 rows in iris dataset
sample_rows &lt;- sample(1:nrow(iris), 10)
sample_rows
#[1] 47 39 82 9 69 71 117 53 78 25
#choose the 10 rows of the iris dataset that match the row numbers above
sample &lt;- iris[sample_rows, ]
sample
#   Sepal.Length Sepal.Width Petal.Length Petal.Width    Species
#47          5.1         3.8          1.6         0.2     setosa
#39          4.4         3.0          1.3         0.2     setosa
#82          5.5         2.4          3.7         1.0 versicolor
#9           4.4         2.9          1.4         0.2     setosa
#69          6.2         2.2          4.5         1.5 versicolor
#71          5.9         3.2          4.8         1.8 versicolor
#117         6.5         3.0          5.5         1.8  virginica
#53          6.9         3.1          4.9         1.5 versicolor
#78          6.7         3.0          5.0         1.7 versicolor
#25          4.8         3.4          1.9         0.2     setosa</b>
Note that if you copy and paste the above code in your own R console, you should get the exact same sample since we used<b> set.seed(100)</b> to ensure that we get the same sample each time.
<h2><span class="orange">How to Identify Influential Data Points Using Cook’s Distance</span></h2>
<b>Cook’s distance</b>, often denoted D<sub>i</sub>, is used in  regression analysis  to identify influential data points that may negatively affect your regression model.
The formula for Cook’s distance is:
<b>D<sub>i</sub></b> = (r<sub>i</sub><sup>2</sup> / p*MSE) * (h<sub>ii</sub> / (1-h<sub>ii</sub>)<sup>2</sup>)
where:
<b>r</b><sub><b>i</b> </sub>is the i<sup>th</sup> residual
<b>p </b>is the number of coefficients in the regression model
<b>MSE</b> is the mean squared error
<b>h</b><sub>ii</sub> is the i<sup>th</sup> leverage value
Although the formula looks a bit complicated, the good news is that most statistical softwares can easily compute this for you.
Essentially, Cook’s Distance does one thing: <b>it measures how much all of the fitted values in the model change when the i<sup>th</sup> data point is deleted.</b>
A data point that has a large value for Cook’s Distance indicates that it strongly influences the fitted values. A general rule of thumb is that any point with a Cook’s Distance over 4/n (<em>where n is the total number of data points</em>) is considered to be an outlier.
It’s important to note that Cook’s Distance is often used as a way to <em>identify </em>influential data points. Just because a data point is influential doesn’t mean it should necessarily be deleted – first you should check to see if the data point has simply been incorrectly recorded or if there is something strange about the data point that may point to an interesting finding.
<h3>How to Calculate Cook’s Distance in R</h3>
The following example illustrates how to calculate Cook’s Distance in R. 
First, we’ll load two libraries that we’ll need for this example:
<b>library(ggplot2)
library(gridExtra)</b>
Next, we’ll define two data frames: one with two outliers and one with no outliers.
<b>#create data frame with no outliers
no_outliers &lt;- data.frame(x = c(1, 2, 2, 3, 4, 5, 7, 3, 2, 12, 11, 15, 14, 17, 22),          y = c(22, 23, 24, 23, 19, 34, 35, 36, 36, 34, 32, 38, 41,                42, 44))
#create data frame with two outliers
outliers &lt;- data.frame(x = c(1, 2, 2, 3, 4, 5, 7, 3, 2, 12, 11, 15, 14, 17, 22),       y = c(190, 23, 24, 23, 19, 34, 35, 36, 36, 34, 32, 38, 41,             42, 180))</b>
Next, we’ll create a  scatterplot  to display the two data frames side by side:
<b>#create scatterplot for data frame with no outliers
no_outliers_plot &lt;- ggplot(data = no_outliers, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm) +
  ylim(0, 200) +
  ggtitle("No Outliers")
#create scatterplot for data frame with outliers
outliers_plot &lt;- ggplot(data = outliers, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm) +
  ylim(0, 200) +
  ggtitle("With Outliers")
#plot the two scatterplots side by side
gridExtra::grid.arrange(no_outliers_plot, outliers_plot, ncol=2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/cooksD.jpg">
We can see how outliers negatively influence the fit of the regression line in the second plot.
To identify influential points in the second dataset, we can can calculate <b>Cook’s Distance</b> for each observation in the dataset and then plot these distances to see which observations are larger than the traditional threshold of 4/n:
<b>#fit the linear regression model to the dataset with outliers
model &lt;- lm(y ~ x, data = outliers)
#find Cook's distance for each observation in the dataset
cooksD &lt;- cooks.distance(model)
# Plot Cook's Distance with a horizontal line at 4/n to see which observations
#exceed this thresdhold
n &lt;- nrow(outliers)
plot(cooksD, main = "Cooks Distance for Influential Obs")
abline(h = 4/n, lty = 2, col = "steelblue") # add cutoff line</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/cooksD2.jpg">
We can clearly see that the first and last observation in the dataset exceed the 4/n threshold. Thus, we would identify these two observations as influential data points that have a negative impact on the regression model.
If we would like to remove any observations that exceed the 4/n threshold, we can do so using the following code:
<b>#identify influential points
influential_obs &lt;- as.numeric(names(cooksD)[(cooksD > (4/n))])
#define new data frame with influential points removed
outliers_removed &lt;- outliers[-influential_obs, ]</b>
Next, we can compare two scatterplots: one shows the regression line with the influential points present and the other shows the regression line with the influential points removed:
<b>#create scatterplot with outliers present
outliers_present &lt;- ggplot(data = outliers, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm) +
  ylim(0, 200) +
  ggtitle("Outliers Present")
#create scatterplot with outliers removed
outliers_removed &lt;- ggplot(data = outliers_removed, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = lm) +
  ylim(0, 200) +
  ggtitle("Outliers Removed")
#plot both scatterplots side by side
gridExtra::grid.arrange(outliers_present, outliers_removed, ncol = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/cooksD3.jpg">
We can clearly see how much better the regression line fits the data with the two influential data points removed.
<h3>Technical Notes</h3>
Most statistical softwares have the ability to easily compute Cook’s Distance for each observation in a dataset.
Keep in mind that Cook’s Distance is simply a way to <em>identify </em>influential points.
There are many ways to <em>deal with </em>influential points including: removing these points, replacing these points with some value like the mean or median, or simply keeping the points in the model but making a careful note about this when reporting the regression results.
<h2><span class="orange">How to Interpret Interquartile Range (With Examples)</span></h2>
The <b>interquartile range</b> of a dataset, often abbreviated IQR, is the difference between the first quartile (the 25th percentile) and the third quartile (the 75th percentile) of the dataset.
In simple terms, it measures the spread of the middle 50% of values.
<b>IQR = Q3 – Q1</b>
For example, suppose we have the following dataset that shows the height of 17 different plants (in inches) in a lab:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
According to the  Interquartile Range Calculator , the interquartile range (IQR) for this dataset is calculated as:
<b>Q1:</b> 12
<b>Q3:</b> 26.5
<b>IQR</b> = Q3 – Q1 = 14.5
This tells us that the middle 50% of values in the dataset have a spread of <b>14.5</b> inches.
<h3>Why the Interquartile Range is Useful</h3>
The interquartile range is one way to measure the spread of values in a dataset, but there are other  measures of spread  such as:
<b>Range:</b> Measures the difference between the minimum and maximum value in a dataset.
<b>Standard Deviation:</b> Measures the typical deviation of individual values from the mean value in a dataset.
The benefit of using the interquartile range (IQR) to measure the spread of values in a dataset is that it is not affected by extreme outliers.
For example, an extremely small or extremely large value in a dataset will not affect the calculation of the IQR because the IQR only uses the values at the 25th percentile and 75th percentile of the dataset.
To illustrate this, consider the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
This dataset has the following measures of spread
<b>IQR:</b> 14.5
<b>Standard Deviation:</b> 9.25 
<b>Range:</b> 31
However, consider if the dataset had one extreme outlier:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32, <b>378</b>
We could use a calculator to find the following measures of spread for this dataset:
<b>IQR:</b> 15
<b>Standard Deviation:</b> 85.02
<b>Range:</b> 377
Notice that the interquartile range barely changes when an outlier is present, while the standard deviation and range both dramatically change.
<h3>Comparing Interquartile Ranges Between Datasets</h3>
The interquartile range can also be used to compare the spread of values between different datasets.
For example, suppose we have three datasets with the following IQR values:
IQR of dataset 1: <b>13.5</b>
IQR of dataset 2: <b>24.4</b>
IQR of dataset 3: <b>8.7</b>
This tells us that the spread of the middle 50% of values is largest for dataset 2 and smallest for dataset 3.
<h2><span class="orange">How to Interpret MAPE Values</span></h2>
One of the most common metrics used to measure the forecasting accuracy of a model is the <b>mean absolute percentage error</b>, often abbreviated as <b>MAPE</b>.
It is calculated as:
<b>MAPE</b> = (1/n) * Σ(|actual – forecast| / |actual|) * 100
where:
<b>Σ</b> – A symbol that means “sum”
<b>n</b> – Sample size
<b>actual</b> – The actual data value
<b>forecast</b> – The forecasted data value
MAPE is commonly used because it’s easy to interpret. For example, a MAPE value of 14% means that the average difference between the forecasted value and the actual value is 14%.
The following example shows how to calculate and interpret a MAPE value for a given model.
<h3>Example: Interpret the MAPE Value for a Given Model</h3>
Suppose a grocery chain builds a model to forecast future sales. The following chart shows the actual sales and the forecasted sales from the model for 12 consecutive sales periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/mape_interpret1.png">
We can use the following formula to calculate the absolute percent error of each forecast:
Absolute percent error = |actual-forecast| / |actual| * 100
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/mape_interpret2.png">
We can then calculate the mean of the absolute percent errors:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/mape_interpret3.png">
The MAPE for this model turns out to be <b>5.12%</b>.
This tells us that the mean absolute percent error between the sales predicted by the model and the actual sales is <b>5.12%</b>.
To determine whether this is a  good value for MAPE  depends on the industry standards.
If the standard model in the grocery industry produces a MAPE value of 2%, then this value of 5.12% might be considered high.
Conversely, if most forecasting models in the grocery industry produce MAPE values between 10% and 15%, then a MAPE value of 5.12% may be considered low and this model may be considered excellent at forecasting future sales.
<h3>Comparing MAPE Values from Different Models</h3>
The MAPE is particularly useful for comparing the fit of different models.
For example, suppose a grocery chain want to build a model to forecast future sales and they want to find the best possible model among several potential models.
Suppose they fit three different models and find their corresponding MAPE values:
MAPE of Model 1: <b>14.5%</b>
MAPE of Model 2: <b>16.7%</b>
MAPE of Model 3: <b>9.8%</b>
Model 3 has the lowest MAPE value, which tells us that it’s able to forecast future sales most accurately among the three potential models.
<h2><span class="orange">How to Interpret Regression Coefficients</span></h2>
In statistics,  regression analysis  is a technique that can be used to analyze the relationship between predictor variables and a response variable.
When you use software (like  R ,  Stata ,  SPSS , etc.) to perform a regression analysis, you will receive a regression table as output that summarize the results of the regression.
Arguably the most important numbers in the output of the regression table are the <b>regression coefficients</b>. Yet, despite their importance, many people have a hard time correctly interpreting these numbers.
This tutorial walks through an example of a regression analysis and provides an in-depth explanation of how to interpret the regression coefficients that result from the regression.
<b>Related: </b> How to Read and Interpret an Entire Regression Table 
<h3>A Regression Analysis Example</h3>
Suppose we are interested in running a regression analysis using the following variables:
<b>Predictor Variables</b>
Total number of hours studied (<em>continuous variable – between 0 and 20</em>)
Whether or not a student used a tutor (<em>categorical variable – “yes” or “no”</em>)
<b>Response Variable</b>
Exam score ( <em>c</em><em>ontinuous variable – between 1 and 100</em>)
We are interested in examining the relationship between the predictor variables and the response variable to find out if hours studied and whether or not a student used a tutor actually have a meaningful impact on their exam score.
Suppose we run a regression analysis and get the following output:
<table><tbody>
<tr>
<th style="text-align: left;">Term</th>
<th style="text-align: left;">Coefficient</th>
<th style="text-align: left;">Standard Error</th>
<th style="text-align: left;">t Stat</th>
<th style="text-align: left;">P-value</th>
</tr>
<tr>
<td><b>Intercept</b></td>
<td>48.56</td>
<td>14.32</td>
<td>3.39</td>
<td>0.002</td>
</tr>
<tr>
<td><b>Hours studied</b></td>
<td>2.03</td>
<td>0.67</td>
<td>3.03</td>
<td>0.009</td>
</tr>
<tr>
<td><b>Tutor</b></td>
<td>8.34</td>
<td>5.68</td>
<td>1.47</td>
<td>0.138</td>
</tr>
</tbody></table>
Let’s take a look at how to interpret each regression coefficient.
<h3>Interpreting the Intercept</h3>
The <b>intercept </b>term in a regression table tells us the average expected value for the response variable when all of the predictor variables are equal to zero.
In this example, the regression coefficient for the intercept is equal to <b>48.56</b>. This means that for a student who studied for zero hours (<em>Hours studied = 0) </em>and did not use a tutor (<em>Tutor = 0), </em>the average expected exam score is 48.56.
It’s important to note that the regression coefficient for the intercept is only meaningful if it’s reasonable that all of the predictor variables in the model can actually be equal to zero. In this example, it’s certainly possible for a student to have studied for zero hours (<em>Hours studied = 0) </em>and to have also not used a tutor (<em>Tutor = 0). </em>Thus, the interpretation for the regression coefficient of the intercept is meaningful in this example.
In some cases, though, the regression coefficient for the intercept is not meaningful. For example, suppose we ran a regression analysis using <em>square footage </em>as a predictor variable and <em>house value </em>as a response variable. In the output regression table, the regression coefficient for the intercept term would not have a meaningful interpretation since <em>square footage </em>of a house can never actually be equal to zero. In that case, the regression coefficient for the intercept term simply anchors the regression line in the right place.
<h3>Interpreting the Coefficient of a Continuous Predictor Variable</h3>
For a continuous predictor variable, the regression coefficient represents the difference in the predicted value of the response variable for each one-unit change in the predictor variable, assuming all other predictor variables are held constant.
In this example, <em>Hours studied </em>is a continuous predictor variable that ranges from 0 to 20 hours. In some cases, a student studied as few as zero hours and in other cases a student studied as much as 20 hours.
From the regression output, we can see that the regression coefficient for <em>Hours studied </em>is <b>2.03</b>. This means that, on average, each additional hour studied is associated with an increase of 2.03 points on the final exam, assuming the predictor variable <em>Tutor </em>is held constant.
For example, consider student A who studies for 10 hours and uses a tutor. Also consider student B who studies for 11 hours and also uses a tutor. According to our regression output, student B is expected to receive an exam score that is 2.03 points higher than student A.
The p-value from the regression table tells us whether or not this regression coefficient is actually statistically significant. We can see that the p-value for <em>Hours studied </em>is <b>0.009</b>, which is statistically significant at an alpha level of 0.05.
<b>Note: </b>The alpha level should be chosen before the regression analysis is conducted – common choices for the alpha level are 0.01, 0.05, and 0.10.
<b>Related post: </b> An Explanation of P-Values and Statistical Significance 
<h3>Interpreting the Coefficient of a Categorical Predictor Variable</h3>
For a categorical predictor variable, the regression coefficient represents the difference in the predicted value of the response variable between the category for which the predictor variable = 0 and the category for which the predictor variable = 1.
In this example, <em>Tutor </em>is a categorical predictor variable that can take on two different values:
1 = the student used a tutor to prepare for the exam
0 = the student did not used a tutor to prepare for the exam
From the regression output, we can see that the regression coefficient for <em>Tutor </em>is <b>8.34</b>. This means that, on average, a student who used a tutor scored 8.34 points higher on the exam compared to a student who did not used a tutor, assuming the predictor variable <em>Hours studied </em>is held constant.
For example, consider student A who studies for 10 hours and uses a tutor. Also consider student B who studies for 10 hours and does not use a tutor. According to our regression output, student A is expected to receive an exam score that is 8.34 points higher than student B.
The p-value from the regression table tells us whether or not this regression coefficient is actually statistically significant. We can see that the p-value for <em>Tutor </em>is <b>0.138</b>, which is not statistically significant at an alpha level of 0.05. This indicates that although students who used a tutor scored higher on the exam, this difference could have been due to random chance.
<h3>Interpreting All of the Coefficients At Once</h3>
We can use all of the coefficients in the regression table to create the following estimated regression equation:
Expected exam score = 48.56 + 2.03*(Hours studied) + 8.34*(Tutor)
<b>Note</b><em><b>: </b>Keep in mind that the predictor variable “Tutor” was not statistically significant at alpha level 0.05, so you may choose to remove this predictor from the model and not use it in the final estimated regression equation.</em>
Using this estimated regression equation, we can predict the final exam score of a student based on their total hours studied and whether or not they used a tutor.
For example, a student who studied for 10 hours and used a tutor is expected to receive an exam score of:
Expected exam score = 48.56 + 2.03*(10) + 8.34*(1) = <b>77.2</b>
<h3>Considering Correlation When Interpreting Regression Coefficients </h3>
It’s important to keep in mind that predictor variables can influence each other in a regression model. For example, most predictor variables will be at least somewhat related to one another (e.g. perhaps a student who studies more is also more likely to use a tutor).
This means that regression coefficients will change when different predict variables are added or removed from the model.
One good way to see whether or not the correlation between predictor variables is severe enough to influence the regression model in a serious way is to  check the VIF between the predictor variables . This will tell you whether or not the correlation between predictor variables is a problem that should be addressed before you decide to interpret the regression coefficients.
If you are running a  simple linear regression model  with only one predictor, then correlated predictor variables will not be a problem.
<h2><span class="orange">How to Interpret Residual Standard Error</span></h2>
The <b>residual standard error</b> is used to measure how well a  regression model  fits a dataset.
In simple terms, it measures the standard deviation of the residuals in a regression model.
It is calculated as:
<b>Residual standard error = √Σ(y – <U+0177>)<sup>2</sup>/df</b>
where:
<b>y:</b> The observed value
<b><U+0177>:</b> The predicted value
<b>df:</b> The degrees of freedom, calculated as the total number of observations – total number of model parameters.
The smaller the residual standard error, the better a regression model fits a dataset. Conversely, the higher the residual standard error, the worse a regression model fits a dataset.
A regression model that has a small residual standard error will have data points that are closely packed around the fitted regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate1.png">
The  residuals  of this model (the difference between the observed values and the predicted values) will be small, which means the residual standard error will also be small.
Conversely, a regression model that has a large residual standard error will have data points that are more loosely scattered around the fitted regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate2.png">
The  residuals  of this model will be larger, which means the residual standard error will also be larger.
The following example shows how to calculate and interpret the residual standard error of a regression model in R.
<h3>Example: Interpreting Residual Standard Error</h3>
Suppose we would like to fit the following multiple linear regression model:
mpg = β<sub>0</sub> + β<sub>1</sub>(displacement) + β<sub>2</sub>(horsepower)
This model uses the predictor variables “displacement” and “horsepower” to predict the miles per gallon that a given car gets.
The following code shows how to fit this regression model in R:
<b>#load built-in <em>mtcars </em>dataset
data(mtcars)
#fit regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Call:
lm(formula = mpg ~ disp + hp, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-4.7945 -2.3036 -0.8246  1.8582  6.9363 
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09</b>
Near the bottom of the output we can see that the residual standard error of this model is <b>3.127</b>.
This tells us that the regression model predicts the mpg of cars with an average error of about 3.127.
<h3>Using Residual Standard Error to Compare Models</h3>
The residual standard error is particularly useful for comparing the fit of different regression models.
For example, suppose we fit two different regression models to predict the mpg of cars. The residual standard error of each model is as follows:
Residual standard error of model 1: <b>3.127</b>
Residual standard error of model 2: <b>5.657</b>
Since model 1 has a lower residual standard error, it fits the data better than model 2. Thus, we would prefer to use model 1 to predict the mpg of cars because the predictions it makes are closer to the observed mpg values of the cars.
<h2><span class="orange">How to Interpret Root Mean Square Error (RMSE)</span></h2>
 Regression analysis  is a technique we can use to understand the relationship between one or more predictor variables and a  response variable . 
One way to assess how well a regression model fits a dataset is to calculate the <b>root mean square error</b>, which is a metric that tells us the average distance between the predicted values from the model and the actual values in the dataset.
The lower the RMSE, the better a given model is able to “fit” a dataset.
The formula to find the root mean square error, often abbreviated <b>RMSE</b>, is as follows:
<b>RMSE = </b>√Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation in the dataset
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation in the dataset
n is the sample size
The following example shows how to interpret RMSE for a given regression model.
<h3>Example: How to Interpret RMSE for a Regression Model</h3>
Suppose we would like to build a regression model that uses “hours studied” to predictor “exam score” of students on a particular college entrance exam.
We collect the following data for 15 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rmse_interpret1.png">
We then use statistical software (like Excel, SPSS, R, Python) etc. to find the following fitted regression model:
Exam Score = 75.95 + 3.08*(Hours Studied)
We can then use this equation to predict the exam score of each student, based on how many hours they studied:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rmse_interpret2.png">
We can then calculate the squared difference between each predicted exam score and the actual exam score. Then we can take the square root of the mean of these differences:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/rmse_interpret3.png">
The RMSE for this regression model turns out to be <b>5.681</b>.
Recall that the  residuals  of a regression model are the differences between the observed data values and the predicted values from the model.
<b>Residual</b> = (P<sub>i</sub> – O<sub>i</sub>)
where
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation in the dataset
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation in the dataset
And recall that the RMSE of a regression model is calculated as:
<b>RMSE = </b>√Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n
This means that <b>the RMSE represents the square root of the variance of the residuals.</b>
This is a useful value to know because it gives us an idea of the average distance between the observed data values and the predicted data values.
This is in contrast to the  R-squared  of the model, which tells us the proportion of the variance in the response variable that can be explained by the predictor variable(s) in the model.
<h3>Comparing RMSE Values from Different Models</h3>
The RMSE is particularly useful for comparing the fit of different regression models.
For example, suppose we want to build a regression model to predict the exam score of students and we want to find the best possible model among several potential models.
Suppose we fit three different regression models and find their corresponding RMSE values:
RMSE of Model 1: <b>14.5</b>
RMSE of Model 2: <b>16.7</b>
RMSE of Model 3: <b>9.8</b>
Model 3 has the lowest RMSE, which tells us that it’s able to fit the dataset the best out of the three potential models.
<h2><span class="orange">How to Interpret the C-Statistic of a Logistic Regression Model</span></h2>
This tutorial provides a simple explanation of how to interpret the <b>c-statistic</b> of a logistic regression model.
<h3>What is Logistic Regression?</h3>
<b>Logistic Regression</b> is a statistical method that we use to fit a regression model when the <em>response </em><em>variable </em>is binary. Here are some examples of when we may use logistic regression:
We want to know how exercise, diet, and weight impact the probability of having a heart attack. The response variable is <em>heart attack</em> and it has two potential outcomes: a heart attack occurs or does not occur.
We want to know how GPA, ACT score, and number of AP classes taken impact the probability of getting accepted into a particular university. The response variable is <em>acceptance </em>and it has two potential outcomes: accepted or not accepted.
We want to know whether word count and email title impact the probability that an email is spam. The response variable is <em>spam </em>and it has two potential outcomes: spam or not spam.
Note that the predictor variables can be numerical or categorical; what’s important is that the response variable is binary. When this is the case, logistic regression is an appropriate model to use to explain the relationship between the predictor variables and the response variable.
<h2>How to Assess the Goodness of Fit of a Logistic Regression Model</h2>
Once we have fit a logistic regression model to a dataset, we are often interested in <em>how well </em>the model fits the data. Specifically, we are interested in how well the model is able to accurately predict positive outcomes and negative outcomes.
<b>Sensitivity </b>refers to the probability that the model predicts a positive outcome for an observation when indeed the outcome is positive.
<b>Specificity </b>refers to the probability that the model predicts a negative outcome for an observation when indeed the outcome is negative.
A logistic regression model is perfect at classifying observations if it has 100% sensitivity and 100% specificity, but in practice this almost never occurs.
Once we fit the logistic regression model, it can be used to calculate the probability that a given observation has a positive outcome, based on the values of the predictor variables.
To determine if an observation should be classified as positive, we can choose a cut-point such that observations with a fitted probability above the cut-point are classified as positive and any observations with a fitted probability below the cut-point are classified as negative.
For example, suppose we choose the cut-point to be 0.5. This means that any observation with a fitted probability greater than 0.5 will be predicted to have a positive outcome, while any observation with a fitted probability less than or equal to 0.5 will be predicted to have a negative outcome.
<h2>Plotting the ROC Curve</h2>
One of the most common ways to visualize the sensitivity vs. specificity of a model is by plotting a <b>ROC</b> <b>(Receiver Operating Characteristic) curve</b>, which is a plot of the values of sensitivity vs. 1-specificity as the value of the cut-off point moves from 0 to 1:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/c_statistic.jpg">
A model with high sensitivity and high specificity will have a ROC curve that hugs the top left corner of the plot. A model with low sensitivity and low specificity will have a curve that is close to the 45-degree diagonal line.
The <b>AUC</b> <b>(area under curve)</b> gives us an idea of how well the model is able to distinguish between positive and negative outcomes. The AUC can range from 0 to 1. The higher the AUC, the better the model is at correctly classifying outcomes.
This means that a model with a ROC curve that hugs the top left corner of the plot would have a high area under the curve, and thus be a model that does a good job of correctly classifying outcomes. Conversely, a model with a ROC curve that hugs the 45-degree diagonal line would have a low area under the curve, and thus be a model that does a poor job of classifying outcomes.
<h2>Understanding the C-Statistic</h2>
The <b>c-statistic</b>, also known as the <em>concordance statistic</em>, is equal to to the AUC (area under curve) and has the following interpretations:
A value below 0.5 indicates a poor model.
A value of 0.5 indicates that the model is no better out classifying outcomes than random chance.
The closer the value is to 1, the better the model is at correctly classifying outcomes.
A value of 1 means that the model is perfect at classifying outcomes.
Thus, a c-statistic gives us an idea about how good a model is at correctly classifying outcomes.
In a clinical setting, it’s possible to calculate the c-statistic by taking all possible pairs of individuals consisting of one individual who experienced a positive outcome and one individual who experienced a negative outcome. Then, the c-statistic can be calculated as the proportion of such pairs in which the individual who experienced a positive outcome had a higher predicted probability of experiencing the outcome than the individual who did not experience the positive outcome.
For example, suppose we fit a logistic regression model using predictor variables <em>age </em>and <em>blood pressure </em>to predict the likelihood of a heart attack.
To find the c-statistic of the model, we could identify all possible pairs of individuals consisting of one individual who experienced a heart attack and one individual who did not experience a heart attack. Then, the c-statistic can be calculated as the proportion of such pairs in which the individual who experienced the heart attack did indeed have a higher predicted probability of experiencing a heart attack compared to the individual who did not experience the heart attack.
<h2>Conclusion</h2>
In this article, we learned the following:
<b>Logistic Regression</b> is a statistical method that we use to fit a regression model when the <em>response </em><em>variable </em>is binary.
To assess the goodness of fit of a logistic regression model, we can look at the <b>sensitivity </b>and <b>specificity</b>, which tell us how well the model is able to classify outcomes correctly.
To visualize the sensitivity and specificity, we can create a <b>ROC curve</b>.
The <b>AUC (area under the curve)</b> indicates how well the model is able to classify outcomes correctly. When a ROC curve hugs the top left corner of the plot, this is an indication that the model is good at classifying outcomes correctly.
The<b> c-statistic</b> is equal to the AUC (area under the curve), and can also be calculated by taking all possible pairs of individuals consisting of one individual who experienced a positive outcome and one individual who experienced a negative outcome. Then, the c-statistic is the proportion of such pairs in which the individual who experienced a positive outcome had a higher predicted probability of experiencing the outcome than the individual who did not experience the positive outcome.
The closer a <b>c-statistic</b> is to 1, the better a model is able to classify outcomes correctly.
<h2><span class="orange">How to Load the Analysis ToolPak in Excel</span></h2>
The <b>Analysis ToolPak</b> is a free Microsoft Excel add-in program that provides tools needed to perform complex statistical, financial, or engineering analyses. 
To load the Analysis ToolPak, simply follow the steps below:
<b>1.</b> Click the <b>File</b> tab in the top left corner, then click <b>Options</b>.
2. Under <b>Add-Ins</b>, click the Analysis <b>ToolPak, </b>then click <b>Go</b>.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/toolpak1.jpg"> 
<b>3.</b> Check the <b>Analysis ToolPak</b> check box, and then click <b>OK</b>.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/toolpak2.jpg"> 
4. On the <b>Data</b> tab, in the <b>Analysis</b> group, you now have the option to click on <b>Data Analysis</b>, which will give you the ability to perform many complex analyses.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/toolpak3.jpg"> 
<h2><span class="orange">How to Normalize Data in R</span></h2>
In most cases, when people talk about “normalizing” variables in a dataset, it means they’d like to scale the values such that the variable has a mean of 0 and a standard deviation of 1. 
The most common reason to normalize variables is when you’re conducting some type of multivariate analysis (i.e. you want to understand the relationship between several predictor variables and a response variable) and you want each variable to contribute equally to the analysis.
When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis.
This is common when one variable measures something like salary ($0 to $100,000) and another variable measures something like age (0 to 100 years).
By normalizing the variables, we can be sure that each variable contributes equally to the analysis. Two common ways to normalize (or “scale”) variables include:
<b>Min-Max Normalization:</b> (X – min(X)) / (max(X) – min(X))
<b>Z-Score Standardization:</b> (X – μ) / σ
Next, we’ll show how to implement both of these techniques in R.
<h2>How to Normalize (or “Scale”) Variables in R</h2>
For each of the following examples, we’ll use the built-in R dataset <b>iris </b>to illustrate how to normalize or scale variables in R:
<b>#view first six rows of <em>iris </em>dataset
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
</b>
<h3>Min-Max Normalization</h3>
The formula for a min-max normalization is:
(X – min(X))/(max(X) – min(X))
For each value of a variable, we simply find how far that value is from the minimum value, then divide by the range. 
To implement this in R, we can define a simple function and then use  lapply  to apply that function to whichever columns in the <b>iris </b>dataset we would like:
<b>#define Min-Max normalization function
min_max_norm &lt;- function(x) {
    (x - min(x)) / (max(x) - min(x))
  }
#apply Min-Max normalization to first four columns in <em>iris </em>dataset
iris_norm &lt;- as.data.frame(lapply(iris[1:4], min_max_norm))
#view first six rows of normalized <em>iris </em>dataset
head(iris_norm)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width
#1   0.22222222   0.6250000   0.06779661  0.04166667
#2   0.16666667   0.4166667   0.06779661  0.04166667
#3   0.11111111   0.5000000   0.05084746  0.04166667
#4   0.08333333   0.4583333   0.08474576  0.04166667
#5   0.19444444   0.6666667   0.06779661  0.04166667
#6   0.30555556   0.7916667   0.11864407  0.12500000
</b>
Notice that each of the columns now have values that range from 0 to 1. Also notice that the fifth column “Species” was dropped from this data frame. We can easily add it back by using the following code:
<b>#add back <em>Species </em>column
iris_norm$Species &lt;- iris$Species
#view first six rows of <em>iris_norm
</em>head(iris_norm)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1   0.22222222   0.6250000   0.06779661  0.04166667  setosa
#2   0.16666667   0.4166667   0.06779661  0.04166667  setosa
#3   0.11111111   0.5000000   0.05084746  0.04166667  setosa
#4   0.08333333   0.4583333   0.08474576  0.04166667  setosa
#5   0.19444444   0.6666667   0.06779661  0.04166667  setosa
#6   0.30555556   0.7916667   0.11864407  0.12500000  setosa
</b>
<h3>Z-Score Standardization</h3>
The drawback of the min-max normalization technique is that it brings the data values towards the mean. If we want to make sure that outliers get weighted more than other values, a z-score standardization is a better technique to implement.
The formula for a z-score standardization is:
(X – μ) / σ
For each value of a variable, we simply subtract the mean value of the variable, then divide by the standard deviation of the variable. 
To implement this in R, we have a few different options:
<b>1. Standardize one variable</b>
If we simply want to standardize one variable in a dataset, such as Sepal.Width in the <b>iris </b>dataset, we can use the following code:
<b>#standardize <em>Sepal.Width
</em>iris$Sepal.Width &lt;- (iris$Sepal.Width - mean(iris$Sepal.Width)) / sd(iris$Sepal.Width)
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1  1.01560199          1.4         0.2  setosa
#2          4.9 -0.13153881          1.4         0.2  setosa
#3          4.7  0.32731751          1.3         0.2  setosa
#4          4.6  0.09788935          1.5         0.2  setosa
#5          5.0  1.24503015          1.4         0.2  setosa
#6          5.4  1.93331463          1.7         0.4  setosa
</b>
The values of <em>Sepal.Width </em>are now scaled such that the mean is 0 and the standard deviation is 1. We can even verify this if we’d like:
<b>#find mean of <em>Sepal.Width
</em>mean(iris$Sepal.Width)
#[1] 2.034094e-16 #basically zero
#find standard deviation of <em>Sepal.Width
</em>sd(iris$Sepal.Width)
#[1] 1</b>
<b>2. Standardize several variables using the scale function</b>
To standardize several variables, we can simply use the <em>scale </em>function. For example, the following code shows how to scale the first four columns of the <b>iris </b>dataset:
<b>#standardize first four columns of <em>iris </em>dataset
iris_standardize &lt;- as.data.frame(scale(iris[1:4]))
#view first six rows of standardized dataset
head(iris_standardize)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width
#1   -0.8976739  1.01560199    -1.335752   -1.311052
#2   -1.1392005 -0.13153881    -1.335752   -1.311052
#3   -1.3807271  0.32731751    -1.392399   -1.311052
#4   -1.5014904  0.09788935    -1.279104   -1.311052
#5   -1.0184372  1.24503015    -1.335752   -1.311052
#6   -0.5353840  1.93331463    -1.165809   -1.048667
</b>
Note that the <em>scale </em>function, by default, attempts to standardize every column in a data frame. Thus, we would get an error if we attempted to use <b>scale(iris)</b> because the <em>Species</em> column is not numeric and cannot be standardized:
<b>scale(iris)
#Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
</b>
However, it is possible to standardize only certain variables in a data frame while also keeping all other variables the same by using the <b>dplyr </b>package. For example, the following code standardizes the variables <em>Sepal.Width </em>and <em>Sepal.Length </em>while keeping all other variables the same:
<b>#load <em>dplyr </em>package
library(dplyr)
#standardize <em>Sepal.Width </em>and <em>Sepal.Length </em>
iris_new &lt;- iris %>% mutate_each_(list(~scale(.) %>% as.vector),                  vars = c("Sepal.Width","Sepal.Length"))
#view first six rows of new data frame
head(iris_new)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1   -0.8976739  1.01560199          1.4         0.2  setosa
#2   -1.1392005 -0.13153881          1.4         0.2  setosa
#3   -1.3807271  0.32731751          1.3         0.2  setosa
#4   -1.5014904  0.09788935          1.5         0.2  setosa
#5   -1.0184372  1.24503015          1.4         0.2  setosa
#6   -0.5353840  1.93331463          1.7         0.4  setosa
</b>
Notice that <em>Sepal.Length </em>and <em>Sepal.Width </em>are standardized such that both variables have a mean of 0 and a standard deviation of 1, while the other three variables in the data frame remain unchanged.
<h2><span class="orange">How to Perform a Mann-Whitney U Test in Excel</span></h2>
A <b>Mann-Whitney U test</b> (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30).
It is considered to be the nonparametric equivalent to the  two sample t-test .
This tutorial explains how to perform a Mann-Whitney U test in Excel.
<h2>Example: Mann-Whitney U Test in Excel</h2>
Researchers want to know if a fuel treatment leads to a change in the average mpg of a car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with the fuel treatment and 12 cars without it.
Because the sample sizes are small and they suspect that the sample distributions are not normally distributed, they decided to perform a Mann-Whitney U test to determine if there is a statistically significant difference in mpg between the two groups.
Perform the following steps to conduct a Mann-Whitney U test in Excel.
<b>Step 1: Enter the data.</b>
Enter the data as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyExcel1.png">
<b>Step 2: Calculate the ranks for both groups.</b>
Next, we’ll calculate the ranks for each group. The following image shows the formula to use to calculate the rank of the first value in the Treated group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyExcel2.png">
Although this formula is fairly complicated, you only have to enter it one time. Then, you can simply drag the formula to all of the other cells to fill in the ranks:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyExcel3.png">
<b>Step 3: Calculate the necessary values for the test statistic.</b>
Next, we’ll use the following formulas to calculate the sum of the ranks for each group, the sample size for each group, the U test statistic for each group, and the overall U test statistic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyExcel4.png">
<b>Step 4: Calculate the z test statistic and the corresponding p-value.</b>
Lastly, we’ll use the following formulas to calculate the z test statistic and the corresponding  p-value  to determine if we should reject or fail to reject the null hypothesis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyExcel5.png">
The null hypothesis of the test states that the two groups have the same mean mpg. Since the p-value of the test is (<b>0.20402387</b>) is not smaller than our significance level of 0.05, we fail to reject the null hypothesis.
We do not have sufficient evidence to say that the true mean mpg is different between the two groups.
<h2><span class="orange">How to Perform Cross Validation for Model Performance in R</span></h2>
In statistics, we often build models for two reasons:
To gain an understanding of the relationship between one or more predictor variables and a response variable.
To use a model to predict future observations.
<b>Cross validation </b>is useful for estimating how well a model is able to predict future observations.
For example, we may build a  mulitple linear regression model  that uses <em>age</em> and <em>income</em> as predictor variables and <em>loan default status</em> as the response variable. In this case, we may wish to fit the model to a dataset and then use that model to predict, based on a new applicant’s income and age, the probability that they will default on a loan.
To determine if the model has strong predictive ability, we need to use the model to make predictions on data that it has not seen before. This will allow us to estimate the <b>prediction error</b> of the model.
<h2>Using Cross Validation to Estimate Prediction Error</h2>
<b>Cross validation</b> refers to different ways we can estimate the prediction error. The general approach of cross-validation is as follows:
<b>1.</b> Set aside a certain number of observations in the dataset – typically 15-25% of all observations.
<b>2.</b> Fit (or “train”) the model on the observations that we keep in the dataset.
<b>3.</b> Test how well the model can make predictions on the observations that we did not use to train the model.
<h2>Measuring the Quality of a Model</h2>
When we use the fitted model to make predictions on new observations, we can use several different metrics to measure the quality of the model, including:
<b>Multiple R-squared:</b> This measures the strength of the linear relationship between the predictor variables and the response variable. A multiple R-squared of 1 indicates a perfect linear relationship while a multiple R-squared of 0 indicates no linear relationship whatsoever. The higher the multiple R-squared, the better the predictor variables are able to predict the response variable.
<b>Root Mean Squared Error (RMSE):</b> This measures the average prediction error made by the model in predicting the value for a new observation. This is the average distance between the true value of an observation and the value predicted by the model. Lower values for RMSE indicate a better model fit.
<b>Mean Absolute Error (MAE):</b> This is the average absolute difference between the true value of an observation and the value predicted by the model. This metric is generally less sensitive to outliers compared to RMSE. Lower values for MAE indicate a better model fit.
<h2>Implementing Four Different Cross-Validation Techniques in R</h2>
Next, we will explain how to implement the following cross validation techniques in R:
<b>1.</b> Validation Set Approach
<b>2.</b> k-fold Cross Validation
<b>3.</b> Leave One Out Cross Validation
<b>4.</b> Repeated k-fold Cross Validation
To illustrate how to use these different techniques, we will use a subset of the built-in R dataset <em>mtcars</em>:
<b>#define dataset
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#view first six rows of new data
head(data)
#                   mpg disp  hp drat
#Mazda RX4         21.0  160 110 3.90
#Mazda RX4 Wag     21.0  160 110 3.90
#Datsun 710        22.8  108  93 3.85
#Hornet 4 Drive    21.4  258 110 3.08
#Hornet Sportabout 18.7  360 175 3.15
#Valiant           18.1  225 105 2.76
</b>
We will build a multiple linear regression model using <em>disp</em>, <em>hp</em>, and <em>drat</em> as predictor variables and <em>mpg </em>as the response variable.
<h2>Validation Set Approach</h2>
The <b>validation set approach</b> works as follows:
<b>1.</b> Split the data into two sets: one set is used to train the model (i.e. estimate the parameters of the model) and the other set is used to test the model. Typically the training set is generated by randomly selecting 70-80% of the data, and the other remaining 20 – 30% of the data is used as the test set.
<b>2.</b> Build the model using the training data set.
<b>3.</b> Use the model to make predictions on the data in the test set.
<b>4.</b> Measure the quality of the model using metrics like R-squared, RMSE, and MAE.
<h3>Example:</h3>
The following example uses the dataset we defined above. First, we split the data into
a training set and test set, using 80% of the data as the training set and the remaining 20% of the data as the test set. Next, we build the model using the training set. Then, we use the model to make predictions on the test set. Lastly, we measure the quality of the model using R-squared, RMSE, and MAE.
<b>#load <em>dplyr</em> library used for data manipulation
library(dplyr)
#load <em>caret</em> library used for partitioning data into training and test set
library(caret)
#make this example reproducible
set.seed(0)
#define the dataset
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#split the dataset into a training set (80%) and test set (20%).
training_obs &lt;- data$mpg %>% createDataPartition(p = 0.8, list = FALSE)
train &lt;- data[training_obs, ]
test &lt;- data[-training_obs, ]
# Build the linear regression model on the training set
model &lt;- lm(mpg ~ ., data = train)
# Use the model to make predictions on the test set
predictions &lt;- model %>% predict(test)
#Examine R-squared, RMSE, and MAE of predictions
data.frame(R_squared = R2(predictions, test$mpg),
           RMSE = RMSE(predictions, test$mpg),
           MAE = MAE(predictions, test$mpg))
#  R_squared     RMSE     MAE
#1 0.9213066 1.876038 1.66614
</b>
When comparing different models, the one that produces the lowest RMSE on the test set is the preferred model.
<h3>Pros & Cons of this Approach</h3>
The advantage of the validation set approach is that it’s straightforward and computationally efficient. The disadvantage is that the model is built only using a portion of the total data. If the data that we happen to leave out of the training set contains interesting or valuable information, the model will not take this into account.
<h2>k-fold Cross Validation Approach</h2>
The <b>k-fold cross validation approach</b> works as follows:
<b>1.</b> Randomly split the data into k “folds” or subsets (e.g. 5 or 10 subsets).
<b>2.</b> Train the model on all of the data, leaving out only one subset.
<b>3.</b> Use the model to make predictions on the data in the subset that was left out.
<b>4.</b> Repeat this process until each of the k subsets has been used as the test set.
<b>5</b>. Measure the quality of the model by calculating the average of the k test errors. This is known
as the cross-validation error.
<h3>Example</h3>
In this example, we first split the data into 5 subsets. Then, we fit the model using all of the data except one subset. Next, we use the model to make predictions on the subset that was left out and record the test error (using R-squared, RMSE, and MAE). We repeat this process until each subset has been used as the test set. Then we simply compute the average of the 5 test errors.
<b>#load <em>dplyr</em> library used for data manipulation
library(dplyr)
#load <em>caret</em> library used for partitioning data into training and test set
library(caret)
#make this example reproducible
set.seed(0)
#define the dataset
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#define the number of subsets (or "folds") to use
train_control &lt;- trainControl(method = "cv", number = 5)
#train the model
model &lt;- train(mpg ~ ., data = data, method = "lm", trControl = train_control)
#Summarize the results
print(model)
#Linear Regression 
#
#32 samples
# 3 predictor
#
#No pre-processing
#Resampling: Cross-Validated (5 fold) 
#Summary of sample sizes: 26, 25, 26, 25, 26 
#Resampling results:
#
#  RMSE      Rsquared   MAE     
#  3.095501  0.7661981  2.467427
#
#Tuning parameter 'intercept' was held constant at a value of TRUE
</b>
<h3>Pros & Cons of this Approach</h3>
The advantage of the k-fold cross validation approach over the validation set approach is that it builds the model several different times using different chunks of data each time, so we have no chance of leaving out important data when building the model.
The subjective part of this approach is in choosing what value to use for k, i.e. how many subsets to split the data into. In general, lower values of k lead to higher bias but lower variability, while higher values of k lead to lower bias but higher variability.
In practice, k is typically chosen to be 5 or 10, as this number of subsets tends to avoid too much bias and too much variability simultaneously.
<h2>Leave One Out Cross Validation (LOOCV) Approach</h2>
The <b>LOOCV approach</b> works as follows:
<b>1.</b> Build the model using all observations in the dataset except for one.
<b>2.</b> Use the model to predict the value of the missing observation. Record the test error of this prediction.
<b>3.</b> Repeat this process for every observation in the dataset.
<b>4.</b> Measure the quality of the model by calculating the average of the all of the prediction errors.
<h3>Example</h3>
The following example illustrates how to use perform LOOCV for the same dataset that we used in the previous examples:
<b>#load <em>dplyr</em> library used for data manipulation
library(dplyr)
#load <em>caret</em> library used for partitioning data into training and test set
library(caret)
#make this example reproducible
set.seed(0)
#define the dataset
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#specify that we want to use LOOCV
train_control &lt;- trainControl(method = "LOOCV")
#train the model
model &lt;- train(mpg ~ ., data = data, method = "lm", trControl = train_control)
#summarize the results
print(model)
#Linear Regression 
#
#32 samples
# 3 predictor
#
#No pre-processing
#Resampling: Leave-One-Out Cross-Validation 
#Summary of sample sizes: 31, 31, 31, 31, 31, 31, ... 
#Resampling results:
#
#  RMSE      Rsquared   MAE     
#  3.168763  0.7170704  2.503544
#
#Tuning parameter 'intercept' was held constant at a value of TRUE
</b>
<h3>Pros & Cons of this Approach</h3>
The benefit of LOOCV is that we use all data points, which generally reduces potential bias. However, since we use the model to predict the value for each observation, this could lead to higher variability in prediction error.
Another drawback of this approach is that it has to fit so many models that it can become computationally inefficient and cumbersome.
<h2>Repeated k-fold Cross Validation Approach</h2>
We can perform <b>repeated k-fold cross validation </b>by simply performing k-fold cross validation several times. The final error is the mean error from the number of repeats.
The following example performs 5-fold cross validation, repeated 4 different times:
<b>#load <em>dplyr</em> library used for data manipulation
library(dplyr)
#load <em>caret</em> library used for partitioning data into training and test set
library(caret)
#make this example reproducible
set.seed(0)
#define the dataset
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#define the number of subsets to use and number of times to repeat k-fold CV
train_control &lt;- trainControl(method = "repeatedcv", number = 5, repeats = 4)
#train the model
model &lt;- train(mpg ~ ., data = data, method = "lm", trControl = train_control)
#summarize the results
print(model)
#Linear Regression 
#
#32 samples
# 3 predictor
#
#No pre-processing
#Resampling: Cross-Validated (5 fold, repeated 4 times) 
#Summary of sample sizes: 26, 25, 26, 25, 26, 25, ... 
#Resampling results:
#
#  RMSE      Rsquared   MAE     
#  3.176339  0.7909337  2.559131
#
#Tuning parameter 'intercept' was held constant at a value of TRUE
</b>
<h3>Pros & Cons of this Approach</h3>
The benefit of the repeated k-fold cross validation approach is that for each repeat, the data will be split up into slightly different subsets, which should give an even more unbiased estimate of the prediction error of the model. The drawback of this approach is that it can be computationally intensive since we have to repeat the model-fitting process several times.
<h2>How to Choose the Number of Folds in Cross Validation</h2>
The most subjective part of performing cross validation is n deciding how many folds (i.e. subsets) to use. In general, the smaller number of folds, the more biased the error estimates, but the less variable they will be. Conversely, the higher number of folds, the less biased the error estimates, but the higher variable they will be.
It’s also important to keep in mind computational time. For each fold, you will have to train a new model, and if this is a slow process then it could take a long time if you choose a high number of folds. 
In practice, cross validation is typically performed with 5 or 10 folds because this allows for a nice balance between variability and bias, while also being computationally efficient.
<h2>How to Choose a Model After Performing Cross Validation</h2>
Cross validation is used as a way to assess the prediction error of a model. It can help us choose between two or more different models by highlighting which model has the lowest prediction error (based on RMSE, R-squared, etc.).
Once we have used cross validation to pick the best model, we then use <em>all </em>of the data available to fit the chosen model. We don’t use the actual model instances we trained during cross validation for our final model.
For example, we may use 5-fold cross validation to determine which model is best to use between two different regression models. However, once we identify which model is best to use, we then use <em>all </em>of the data to fit the final model. In other words, we don’t leave out one of the folds when building the final model.
<h2><span class="orange">How to Perform the Friedman Test in Excel</span></h2>
The <b>Friedman Test </b>is a non-parametric alternative to the  Repeated Measures ANOVA . It is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to perform the Friedman Test in Excel.
<h3>Example: The Friedman Test in Excel</h3>
Use the following steps to perform the Friedman Test in Excel.
<b>Step 1: Enter the data.</b>
Enter the following data, which shows the reaction time (in seconds) of 10 patients on three different drugs. Since each patient is measured on each of the three drugs, we will use the Friedman Test to determine if the mean reaction time differs between drugs.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/friedman1.png">
<b>Step 2: Rank the data.</b>
Next, rank the data values in each row in ascending order using the <b>=RANK.AVG() </b>function. The following formula shows how to calculate the rank for the response time of patient 1 on drug 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/friedman2.png">
Copy this formula to the rest of the cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/friedman3.png">
Then, calculate the sum of the ranks for each column along with the squared sum of ranks:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/friedman4.png">
<b>Step 3: Calculate the test statistic and the corresponding p-value.</b>
The test statistic is defined as:
Q = 12/nk(k+1) * ΣR<sub>j</sub><sup>2</sup> – 3n(k+1)
where:
n = number of patients
k = number of treatment groups
R<sub>j</sub><sup>2</sup> =sum of ranks for the j<sup>th</sup> group
Under the null hypothesis, Q follows a chi-square distribution with k-1 degrees of freedom.
The following screenshot shows the formulas used to calculate the test statistic, Q, and the corresponding p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/friedman5.png">
The test statistic is Q = <b>12.35 </b>and the corresponding p-value is p = <b>0.00208</b>. Since this value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all three drugs. We have sufficient evidence to conclude that the type of drug used leads to statistically significant differences in response time.
<b>Step 4: Report the results.</b>
Lastly, we want to report the results of the test. Here is an example of how to do so:
A Friedman Test was conducted on 10 patients to examine the effect that three different drugs had on response time. Each patient used each drug once.
 
Results showed that the type of drug used lead to statistically significant differences in response time (Q = 12.35, p = 0.00208).
<h2><span class="orange">How to Plot Multiple Lines (data series) in One Chart in R</span></h2>
This tutorial explains how to plot multiple lines (i.e. data series) in one chart in R.
To plot multiple lines in one chart, we can either use base R or install a fancier package like ggplot2.
<h2>Using Base R</h2>
Here are two examples of how to plot multiple lines in one chart using Base R.
<h3>Example 1: Using Matplot</h3>
If you have a dataset that is in a  wide format , one simple way to plot multiple lines in one chart is by using matplot:
<b>#Create a fake dataset with 3 columns (ncol=3) composed of randomly generated
#numbers from a uniform distribution with minimum = 1 and maximum = 10
data &lt;- matrix(runif(30,1,10), ncol=3)
data
        [,1]     [,2]     [,3]
#[1,] 5.371653 3.490919 3.953603
#[2,] 9.551883 2.681054 9.506765
#[3,] 3.525686 1.027758 8.059011
#[4,] 9.923080 1.337935 1.112361
#[5,] 7.273972 7.627546 1.174340
#[6,] 8.859109 3.778144 9.384526
#[7,] 9.614542 3.866029 7.301729
#[8,] 9.288085 5.804041 8.347907
#[9,] 1.696849 4.650687 7.220209
#[10,] 5.820941 4.799682 5.243663
#plot the three columns of the dataset as three lines and add a legend in
#the top right corner of the chart
matplot(data, type = "b",pch=1,col = 1:3)
legend("topright", legend = 1:3, col=1:3, pch=1)
</b>
This code generates the following chart:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/multipleLines.jpg" alt="">
 
<h3>Example 2: Using Points & Lines</h3>
Another way to plot multiple lines is to plot them one by one, using the built-in R functions points() and lines(). The code below demonstrates an example of this approach:
<b>#generate an x-axis along with three data series
x  &lt;- c(1,2,3,4,5,6)
y1 &lt;- c(2,4,7,9,12,19)
y2 &lt;- c(1,5,9,8,9,13)
y3 &lt;- c(3,6,12,14,17,15)
</b>
<b>#plot the first data series using plot()
plot(x, y1, type="o", col="blue", pch="o", ylab="y", lty=1)
#add second data series to the same chart using points() and lines()
points(x, y2, col="red", pch="*")
lines(x, y2, col="red",lty=2)
#add third data series to the same chart using points() and lines()
points(x, y3, col="dark red",pch="+")
lines(x, y3, col="dark red", lty=3)
#add a legend in top left corner of chart at (x, y) coordinates = (1, 19)
legend(1,19,legend=c("y1","y2","y3"), col=c("blue","red","black"),                   pch=c("o","*","+"),lty=c(1,2,3), ncol=1)</b>
This code generates the following chart:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/multipleLines2.jpg" alt="">
<h2>Using ggplot2</h2>
Here is an example of how to plot multiple lines in one chart using ggplot2.
<b>#install (if not already installed) and load ggplot2 package
if(!require(ggplot2)){install.packages('ggplot2')}
#generate fake dataset with three columns 'x', 'value', and 'variable'
data &lt;- data.frame(x=rep(1:5, 3),   value=sample(1:100, 15),    variable=rep(paste0('series', 1:3), each=5))
#view dataset
head(data)
  x value variable
1 1 93    series1
2 2 64    series1
3 3 36    series1
4 4 17    series1
5 5 95    series1
6 1 80    series2
#plot all three series on the same chart using geom_line()
ggplot(data = data, aes(x=x, y=value)) + geom_line(aes(colour=variable))
</b>
This generates the following chart:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/multipleLines3.jpg" alt="">
<h2><span class="orange">How to Read a Correlation Matrix</span></h2>
In statistics, we’re often interested in understanding the relationship between two variables.
For example, we might want to understand the relationship between the number of hours a student studies and the exam score they receive.
One way to quantify this relationship is to use the  Pearson correlation coefficient , which is a measure of the linear association between two variables<em>. </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
The further away the correlation coefficient is from zero, the stronger the relationship between the two variables.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
But in some cases we want to understand the correlation between more than just one pair of variables. In these cases, we can create a <b>correlation matrix</b>, which is a square table that shows the the correlation coefficients between several variables. 
<h2>Example of a Correlation Matrix</h2>
The correlation matrix below shows the correlation coefficients between several variables related to education:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix0.jpg">
Each cell in the table shows the correlation between two specific variables. For example, the highlighted cell below shows that the correlation between “hours spent studying” and “exam score” is <b>0.82</b>, which indicates that they’re strongly positively correlated. More hours spent studying is strongly related to higher exam scores.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix2.jpg">
And the highlighted cell below shows that the correlation between “hours spent studying” and “hours spent sleeping” is <b>-0.22</b>, which indicates that they’re weakly negatively correlated. More hours spent studying is associated with less hours spent sleeping.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix3.jpg">
And the highlighted cell below shows that the correlation between “hours spent sleeping” and “IQ score” is <b>0.06</b>, which indicates that they’re basically not correlated. There is very little association between the number of hours a student sleeps and their IQ score.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix9.jpg">
Also notice that the correlation coefficients along the diagonal of the table are all equal to 1 because each variable is perfectly correlated with itself. These cells aren’t useful for interpretation.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix4.jpg">
<h2>Variations of the Correlation Matrix</h2>
Notice that a correlation matrix is perfectly symmetrical. For example, the top right cell shows the exact same value as the bottom left cell:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix5.jpg">
This is because both cells are measuring the correlation between “hours spent studying” and “school rating.” 
Because a correlation matrix is symmetrical, half of the correlation coefficients shown in the matrix are redundant and unnecessary. Thus, sometimes only half of the correlation matrix will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix6.jpg">
And sometimes a correlation matrix will be colored in like a heat map to make the correlation coefficients even easier to read:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix7.jpg">
<h2>When to Use a Correlation Matrix</h2>
In practice, a correlation matrix is commonly used for three reasons:
<b>1. A correlation matrix conveniently summarizes a dataset.</b>
A correlation matrix is a simple way to summarize the correlations between all variables in a dataset. For example, suppose we have the following dataset that has the following information for 1,000 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/correlationMatrix8.jpg">
It would be very difficult to understand the relationship between each variable by simply staring at the raw data. Fortunately, a correlation matrix can help us quickly understand the correlations between each pair of variables.
<b>2. A correlation matrix serves as a diagnostic for regression.</b>
One key assumption of  multiple linear regression  is that no independent variable in the model is highly correlated with another variable in the model.
When two independent variables are highly correlated, this results in a problem known as  multicollinearity  and it can make it hard to interpret the results of the regression.
One of the easiest ways to detect a potential multicollinearity problem is to look at a correlation matrix and visually check whether any of the variables are highly correlated with each other.
<b>3. A correlation matrix can be used as an input in other analyses.</b>
A correlation matrix is used as an input for other complex analyses such as exploratory factor analysis and structural equation models.
<h2><span class="orange">How to Read the Binomial Distribution Table</span></h2>
The binomial distribution table<b> </b>is a table that shows probabilities associated with the  binomial distribution . To use the binomial distribution table, you only need three values:
<b>n: </b>the number of trials
<b>r: </b>the number of “successes” during n trials
<b>p: </b>the probability of success on a given trial
Using these three numbers, you can use the binomial distribution table to find the probability of obtaining exactly <b>r </b>successes during <b>n </b>trials when the probability of success on each trial is <b>p</b>.
The following examples illustrate how to read the binomial distribution table.
<h3>Example 1</h3>
<b>Question: </b>Jessica makes 60% of her free-throw attempts. If she shoots 6 free throws, what is the probability that she makes exactly 4?
To answer this question, we can look up the value in the binomial distribution table that corresponds to <b>n </b>= 6, <b>r </b>= 4, and <b>p </b>= 0.60:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomTable2.png">
The probability that Jessica makes exactly 4 out of 6 free throws is <b>0.311</b>.
<h3>Example 2</h3>
<b>Question: </b>Jessica makes 60% of her free-throw attempts. If she shoots 6 free throws, what is the probability that she makes less than 4?
To find this probability, we actually have to add up the following probabilities:
P(makes less than 4) = P(makes 0) + P(makes 1) + P(makes 2) + P(makes 3)
So, we can look up each of these four probabilities in the binomial distribution table and add them up:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomTable3.png">
According to the table, P(makes less than 4) = .004 + .037 + .138 + .276 = <b>0.455</b>.
The probability that Jessica makes less than 4 free throws is <b>0.455</b>.
<h3>Example 3</h3>
<b>Question: </b>Jessica makes 60% of her free-throw attempts. If she shoots 6 free throws, what is the probability that she makes 4 or more?
To find this probability, we have to add up the following probabilities:
P(makes 4 or more) = P(makes 4) + P(makes 5) + P(makes 6)
So, we can look up each of these three probabilities in the binomial distribution table and add them up:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomTable4.png">
According to the table, P(makes 4 or more) = .311 + .187 + .047 = <b>0.545</b>.
The probability that Jessica makes 4 or more free throws is <b>0.545</b>.
<h2><span class="orange">How to Read a Box Plot with Outliers (With Example)</span></h2>
A <b>box plot</b> is a type of plot that displays the five number summary of a dataset, which includes:
The minimum value
The first quartile (the 25th percentile)
The median value
The third quartile (the 75th percentile)
The maximum value
To make a box plot, we first draw a box from the first to the third quartile.
Then we draw a vertical line at the median.
Lastly, we draw “whiskers” from the quartiles to the minimum and maximum value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/outlierbox1.jpg"424">
In most statistical software, an observation is defined as an outlier if it meets one of the following two requirements:
The observation is 1.5 times the interquartile range less than the first quartile (Q1)
The observation is 1.5 times the interquartile range greater than the third quartile (Q3).
If an outlier does exist in a dataset, it is usually labeled with a tiny dot outside of the range of the whiskers in the box plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/outlierbox2.jpg">
When this occurs, the “minimum” and “maximum” values in the box plot are simply assigned the values of Q1 – 1.5*IQR and Q3 + 1.5*IQR, respectively.
The following example shows how to interpret box plots with and without outliers.
<h2>Example: Interpreting a Box Plot With Outliers</h2>
Suppose we create the following two box plots to visualize the distribution of points scored by basketball players on two different teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/outlierbox3.jpg"504">
The box plot on the left for team A has no outliers since there are no tiny dots located outside of the minimum or maximum whisker.
However, the box plot on the right for team B has one outlier located above the “maximum” and one outlier located below the “minimum” value.
Here is the actual five number summary for the distribution of the “Points” variable for Team B:
Minimum value: 1.1
First Quartile: 10.5
Median: 12.7
Third Quartile: 15.6
Maximum value: 23.5
Here is how to calculate the boundaries for potential outliers:
<b>Interquartile Range</b>: Third Quartile – First Quartile = 15.6 – 10.5 = 5.1
<b>Lower Boundary</b>: Q1 – 1.5*IQR = 10.5 – 1.5*5.1 = 2.85
<b>Upper Boundary</b>: Q3 + 1.5*IQR = 15.6 + 1.5*5.1 = 23.25
The whiskers for the minimum and maximum values in the box plot are placed at <b>2.85</b> and <b>23.25</b>.
Thus, the observations with values of <b>1.1</b> and <b>23.5</b> are both labeled as outliers in the box plot since they lie outside of the lower and upper boundaries.
<b>Bonus</b>: Here is the exact code that we used to create these two box plots in the R programming language:
<b>library(ggplot2)
#make this example reproducible 
set.seed(2)
#create data frame
df &lt;- data.frame(Team = factor(rep(c("A", "B"), each = 200)),  Points = c(rnorm(200, mean = 15, sd = 3),            rnorm(200, mean = 12, sd = 4))) 
#create box plots
ggplot(df, aes(x = Team, y = Points)) +
  stat_boxplot(geom = "errorbar", width = 0.5) +  
  geom_boxplot() 
#calculate summary statistics for each team
tapply(df$Points, df$Team, summary)
</b>
<h2>Additional Resources</h2>
The following tutorials provide additional information about box plots:
 How to Compare Box Plots 
 How to Identify Skewness in Box Plots 
 How to Find the Interquartile Range of a Box Plot 
<h2><span class="orange">How to Read the Chi-Square Distribution Table</span></h2>
This tutorial explains how to read and interpret  the Chi-Square distribution table .
<h2>What is the Chi-Square Distribution Table?</h2>
The <b>Chi-Square distribution table </b>is a table that shows the critical values of the Chi-Square distribution. To use the Chi-Square distribution table, you only need to know two values:
The degrees of freedom for the Chi-Square test
The alpha level for the test (common choices are 0.01, 0.05, and 0.10)
The following image shows the first 20 rows of the Chi-Square distribution table, with the degrees of freedom along the left side of the table and the alpha levels along the top of the table:
<em><b>Note: </b>You can find a full Chi-Square distribution table with more degrees of freedom  here .</em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table_small.jpg"706">
The critical values within the table are often compared to the test statistic of a Chi-Square test. If the test statistic is greater than the critical value found in the table, then you can reject the null hypothesis of the Chi-Square test and conclude that the results of the test are statistically significant.
<h2>Examples of How to Use the Chi-Square Distribution Table</h2>
We will demonstrate how to use the Chi-Square distribution table with the following three types of Chi-Square tests:
Chi-Square Test for Independence
Chi-Square Test for Goodness of Fit
Chi-Square Test for Homogeneity
<h3>Chi-Square Test for Independence</h3>
We use a <b>Chi-Square test for independence </b>when we want to test whether or not there is a significant association between two categorical variables. 
<b>Example: </b>Suppose we want to know whether or not gender is associated with political party preference. We take a simple random sample of 500 voters and survey them on their political party preference. Using a 0.05 level of significance, we conduct a chi-square test for independence to determine if gender is associated with political party preference. The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table1.jpg"491">
It turns out that the test statistic for this Chi-Square test is 0.864.
Next, we can find the critical value for the test in the Chi-Square distribution table. The degrees of freedom is equal to (#rows-1) * (#columns-1) = (2-1) * (3-1) = 2 and the problem told us that we are to use a 0.05 alpha level. Thus, according to the Chi-Square distribution table, the critical value of the test is <b>5.991</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table2.jpg"704">
Since our test statistic is smaller than our critical value, we fail to reject the null hypothesis. This means we do not have sufficient evidence to state that there is an association between gender and political party preference.
<h2>Chi-Square Test for Goodness of Fit</h2>
We use a <b>chi-square goodness of fit test</b> when we want to test whether or not a categorical variable follows a hypothesized distribution.
<b>Example: </b>An owner of a shop claims that 30% of all his weekend customers visit on Friday, 50% on Saturday, and 20% on Sunday. An independent researcher visits the shop on a random weekend and finds that 91 customers visit on Friday, 104 visit on Saturday, and 65 visit on Sunday. Using a 0.10 level of significance, we conduct a chi-square test for goodness of fit to determine if the data is consistent with the shop owner’s claim.
In this case, the test statistic turns out to be 10.616.
Next, we can find the critical value for the test in the Chi-Square distribution table. The degrees of freedom is equal to (#outcomes-1) = 3-1 = 2 and the problem told us that we are to use a 0.10 alpha level. Thus, according to the Chi-Square distribution table, the critical value of the test is <b>4.605</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table3.jpg"706">
Since our test statistic is greater than our critical value, we reject the null hypothesis. This means we have sufficient evidence to say the true distribution of customers who come in to this shop on weekends is not equal to 30% on Friday, 50% on Saturday, and 20% on Sunday.
<h2>Chi-Square Test for Homogeneity</h2>
We use a <b>chi-square test for homogeneity</b> when we want to formally test whether or not there is a difference in proportions between several groups.
<b>Example:</b>A basketball training facility wants to see if two new training programs improve the proportion of their players who pass a difficult shooting test. 172 players are randomly assigned to program 1, 173 to program 2, and 215 to the current program. After using the training programs for one month, the players then take a shooting test. The table below shows the number of players who pass the shooting test, based on which program they used.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table4.jpg"556">
Using a 0.05 level of significance, we conduct a chi-square test for homogeneity to determine if the pass rate is the same or each training program. 
It turns out that the test statistic for this Chi-Square test is 4.208.
Next, we can find the critical value for the test in the Chi-Square distribution table. The degrees of freedom is equal to (#rows-1) * (#columns-1) = (2-1) * (3-1) = 2 and the problem told us that we are to use a 0.05 alpha level. Thus, according to the Chi-Square distribution table, the critical value of the test is <b>5.991</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/chi_square_table2.jpg"704">
Since our test statistic is smaller than our critical value, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the three training programs produce different results.
<h2><span class="orange">How to Read a Semi-Log Graph (With Examples)</span></h2>
A <b>semi-log graph</b> is a type of graph that uses a linear scale on the x-axis and a logarithmic scale on the y-axis.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/semi1.png">
We often use this type of graph when the values for the y variable have much greater variability compared to the values for the x variable.
This type of graph is particularly useful for visualizing <b>percentage change</b> of some variable over time.
The following examples show how to read semi-log graphs in practice.
<h3>Example 1: Semi-Log Graph for Plant Growth</h3>
Suppose a biologist wants to create a line chart to visualize the growth of a certain plant during a 20-week period.
She first creates the following line chart using a linear scale on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/semi2.png">
This chart is useful for visualizing the raw change in plant height from one week to the next.
However, she can use a <b>semi-log graph</b> to more easily visualize the percentage change in plant height from one week to the next:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/semi3.png">
Notice that the y-axis is measured on a logarithmic scale.
Using this graph, we can see that the percentage growth of the plant is quickest in the early weeks and then slows down dramatically in the later weeks.
<h3>Example 2: Semi-Log Graph for Investment Growth</h3>
Suppose an investor wants to create a line chart to visualize the growth of a certain investment during a 30-year period.
He first creates the following line chart using a linear scale on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/semi4.png">
This chart is useful for visualizing the raw change in the investment value from one year to the next.
However, he can use a <b>semi-log graph</b> to more easily visualize the percentage change in the investment value from one year to the next:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/semi5.png">
Notice that the y-axis is measured on a logarithmic scale.
Using this graph, we can see that the percentage growth of the investment is actually consistent from one year to the next.
<h2><span class="orange">How to Read the t-Distribution Table</span></h2>
This tutorial explains how to read and interpret  the t-Distribution table .
<h2>What is the t-Distribution Table?</h2>
The <b>t-distribution table </b>is a table that shows the critical values of the t distribution. To use the t-distribution table, you only need to know three values:
The degrees of freedom of the t-test
The number of tails of the t-test (one-tailed or two-tailed)
The alpha level of the t-test (common choices are 0.01, 0.05, and 0.10)
Here is an example of the t-Distribution table, with the degrees of freedom listed along the left side of the table and the alpha levels listed along the top of the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/t_dist.png">
When you conduct a t-test, you can compare the test statistic from the t-test to the critical value from the t-Distribution table. If the test statistic is greater than the critical value found in the table, then you can reject the null hypothesis of the t-test and conclude that the results of the test are statistically significant.
Let’s walk through some examples of how to use the t-Distribution table.
<h2>Examples of How to Use the t-Distribution Table</h2>
The following examples explain how to use the t-Distribution table in several different scenarios.
<h3>Example #1: One-tailed t-test for a mean</h3>
A researcher recruits 20 subjects for a study and conducts a one-tailed t-test for a mean using an alpha level of 0.05.
<b>Question:</b> Once she conducts her one-tailed t-test and obtains a test statistic <em>t</em>, what critical value should she compare <em>t </em>to?
<b>Answer: </b>For a t-test with one sample, the degrees of freedom is equal to <em>n-1</em>, which is 20-1 = 19 in this case. The problem also tells us that she is conducting a one-tailed test and that she is using an alpha level of 0.05, so the corresponding critical value in the t-distribution table is <b>1.729</b>.
<h3>Example #2: Two-tailed t-test for a mean</h3>
A researcher recruits 18 subjects for a study and conducts a two-tailed t-test for a mean using an alpha level of 0.10.
<b>Question:</b> Once she conducts her two-tailed t-test and obtains a test statistic <em>t</em>, what critical value should she compare <em>t </em>to?
<b>Answer: </b>For a t-test with one sample, the degrees of freedom is equal to <em>n-1</em>, which is 18-1 = 17 in this case. The problem also tells us that she is conducting a two-tailed test and that she is using an alpha level of 0.10, so the corresponding critical value in the t-distribution table is <b>1.74</b>.
<h3>Example #3: Determining the critical value</h3>
A researcher conducts a two-tailed t-test for a mean using a sample size of 14 and an alpha level of 0.05. 
<b>Question:</b> What would the absolute value of her test statistic <em>t </em>need to be in order for her to reject the null hypothesis?
<b>Answer: </b>For a t-test with one sample, the degrees of freedom is equal to <em>n-1</em>, which is 14-1 = 13 in this case. The problem also tells us that she is conducting a two-tailed test and that she is using an alpha level of 0.05, so the corresponding critical value in the t-distribution table is <b>2.16</b>. This means that she can reject the null hypothesis if the test statistic <em>t </em>is less than -2.16 or greater than 2.16.
<h3>Example #4: Comparing a critical value to a test statistic</h3>
A researcher conducts a right-tailed t-test for a mean using a sample size of 19 and an alpha level of 0.10.
<b>Question:</b> The test statistic <em>t </em>turns out to be 1.48. Can she reject the null hypothesis?
<b>Answer: </b>For a t-test with one sample, the degrees of freedom is equal to <em>n-1</em>, which is 19-1 = 18 in this case. The problem also tells us that she is conducting a right-tailed test (which is a one-tailed test) and that she is using an alpha level of 0.10, so the corresponding critical value in the t-distribution table is <b>1.33</b>. Since her test statistic <em>t </em>is greater than 1.33, she can reject the null hypothesis.
<h2>Should You Use the t Table or the z Table?</h2>
One problem that students frequently encounter is determining if they should use the t-distribution table or the z table to find the critical values for a particular problem. If you’re stuck on this decision, you can use the following flow chart to determine which table you should use:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/z_table_vs_t_table.jpg">
<h2><span class="orange">How to Read the F-Distribution Table</span></h2>
This tutorial explains how to read and interpret  the F-distribution table .
<h3>What is the F-Distribution Table?</h3>
The <b>F-distribution table </b>is a table that shows the critical values of the F distribution. To use the F distribution table, you only need three values:
The numerator degrees of freedom
The denominator degrees of freedom
The alpha level (common choices are 0.01, 0.05, and 0.10)
The following table shows the F-distribution table for alpha = 0.10. The numbers along the top of the table represent the numerator degrees of freedom (labeled as <em>DF1 </em>in the table) and the numbers along the left hand side of the table represent the denominator degrees of freedom (labeled as <em>DF2 </em>in the table). 
<em>Feel free to click on the table to zoom in.</em>
  
The critical values within the table are often compared to the F statistic of an F test. If the F statistic is greater than the critical value found in the table, then you can reject the null hypothesis of the F test and conclude that the results of the test are statistically significant.
<h2>Examples of How to Use the F-Distribution Table</h2>
The F-distribution table is used to find the critical value for an F test. The three most common scenarios in which you’ll conduct an F test are as follows:
F test in regression analysis to test for the overall significance of a regression model.
F test in ANOVA (analysis of variance) to test for an overall difference between group means.
F test to find out if two populations have equal variances.
Let’s walk through an example of how to use the F-distribution table in each of these scenarios.
<h3>F Test in Regression Analysis</h3>
Suppose we conduct a multiple linear regression analysis using <em>hours studied </em>and <em>prep </em><em>exams taken </em>as predictor variables and <em>final exam score </em>as the response variable. When we run the regression analysis, we receive the following output:
<table>
<thead><tr>
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
<th>P</th>
</tr></thead>
<tbody>
<tr>
<td>Regression</td>
<td>546.53</td>
<td>2</td>
<td>273.26</td>
<td>5.09</td>
<td>0.033</td>
</tr>
<tr>
<td>Residual</td>
<td>483.13</td>
<td>9</td>
<td>53.68</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Total</td>
<td>1029.66</td>
<td>11</td>
</tr>
</tbody>
</table>
In regression analysis, the f statistic is calculated as regression MS / residual MS. This statistic indicates whether the regression model provides a better fit to the data than a model that contains no independent variables. In essence, it tests if the regression model as a whole is useful.
In this example, <b>the F statistic is 273.26 / 53.68 = 5.09</b>.
Suppose we want to know if this F statistic is significant at level alpha = 0.05. Using the F-distribution table for alpha = 0.05, with numerator of degrees of freedom <b>2 </b>(<em>df for Regression) </em>and denominator degrees of freedom <b>9 </b>(<em>df for Residual)</em>, we find that the F critical value is <b>4.2565</b>.
  
Since our f statistic (<b>5.09</b>) is greater than the F critical value (<b>4.2565)</b>, we can conclude that the regression model as a whole is statistically significant.
<h3>F test in ANOVA</h3>
Suppose we want to know whether or not three different studying techniques lead to different exam scores. To test this, we recruit 60 students. We randomly assign 20 students each to use one of the three studying techniques for one month in preparation for an exam. Once all of the students take the exam, we then conduct a  one-way ANOVA  to find out whether or not studying technique has an impact on exam scores. The following table shows the results of the one-way ANOVA:
<table>
<thead><tr>
<th>Source</th>
<th>SS</th>
<th>df</th>
<th>MS</th>
<th>F</th>
<th>P</th>
</tr></thead>
<tbody>
<tr>
<td>Treatment</td>
<td>58.8</td>
<td>2</td>
<td>29.4</td>
<td>1.74</td>
<td>0.217</td>
</tr>
<tr>
<td>Error</td>
<td>202.8</td>
<td>12</td>
<td>16.9</td>
<td></td>
<td></td>
</tr>
<tr>
<td>Total</td>
<td>261.6</td>
<td>14</td>
</tr>
</tbody>
</table>
In an ANOVA, the f statistic is calculated as Treatment MS / Error MS. This statistic indicates whether or not the mean score for all three groups is equal.
In this example, <b>the F statistic is 29.4 / 16.9 = 1.74</b>.
Suppose we want to know if this F statistic is significant at level alpha = 0.05. Using the F-distribution table for alpha = 0.05, with numerator of degrees of freedom <b>2 </b>(<em>df for Treatment) </em>and denominator degrees of freedom <b>12 </b>(<em>df for Error)</em>, we find that the F critical value is <b>3.8853</b>.
  
Since our f statistic (<b>1.74</b>) is not greater than the F critical value (<b>3.8853)</b>, we conclude that there is not a statistically significant difference between the mean scores of the three groups.
<h3>F test for Equal Variances of Two Populations</h3>
Suppose we want to know whether or not the variances for two populations are equal. To test this, we can conduct an F-test for equal variances in which we take a random sample of 25 observations from each population and find the sample variance for each sample.
The test statistic for this F-Test is defined as follows:
<b>F-statistic</b> = s<sub>1</sub><sup>2</sup> / s<sub>2</sub><sup>2</sup>
where s<sub>1</sub><sup>2</sup>  and s<sub>2</sub><sup>2</sup> are the sample variances. The further this ratio is from one, the stronger the evidence for unequal population variances. 
The critical value for the F-Test is defined as follows:
<b>F Critical Value</b> = the value found in the F-distribution table with n<sub>1</sub>-1 and n<sub>2</sub>-1 degrees of freedom and a significance level of α.
Suppose the sample variance for sample 1 is 30.5 and the sample variance for sample 2 is 20.5. This means that our test statistic is 30.5 / 20.5 = <b>1.487</b>. To find out if this test statistic is significant at alpha = 0.10, we can find the critical value in the F-distribution table associated with alpha = 0.10, numerator df = 24, and denominator df = 24. This number turns out to be <b>1.7019</b>.
  
Since our f statistic (<b>1.487</b>) is not greater than the F critical value (<b>1.7019)</b>, we conclude that there is not a statistically significant difference between the variances of these two populations.
<h2><span class="orange">How to Rename Data Frame Columns in R</span></h2>
This tutorial explains how to rename data frame columns in R using a variety of different approaches.
For each of these examples, we’ll be working with the built-in dataset <b>mtcars</b> in R.
<h2>Renaming the First <em>n </em>Columns Using Base R</h2>
There are a total of 11 column names in<b> mtcars:</b>
<b>#view column names of mtcars
names(mtcars)
# [1] "mpg" "cyl" "disp" "hp" "drat" "wt" "qsec" "vs" "am" "gear"
# [11] "carb"</b>
To rename the first 4 columns, we can use the following syntax:
<b>#rename first 4 columns
names(mtcars) &lt;- c("miles_gallon", "cylinders", "display", "horsepower")
names(mtcars)
#[1] "miles_gallon" "cylinders" "display" "horsepower" NA 
#[6]  NA             NA          NA        NA          NA 
#[11] NA </b>
Notice that R starts with the first column name, and simply renames as many columns as you provide it with. In this example, since there are 11 column names and we only provided 4 column names, only the first 4 columns were renamed. To rename all 11 columns, we would need to provide a vector of 11 column names.
<h2>Renaming<em> </em>Columns by Name Using Base R</h2>
If we want to rename a specific column in the <b>mtcars </b>dataset, such as the column “wt”, we can do so by name:
<b>#rename just the "wt" column in mtcars
names(mtcars)[names(mtcars)=="wt"] &lt;- "weight"
names(mtcars)
#[1] "mpg" "cyl" "disp" "hp" "drat" "weight" "qsec" "vs" 
#[9] "am" "gear" "carb" </b>
Notice how only the “wt” column is renamed to “weight” and all of the other columns keep their original names.
<h2>Renaming<em> </em>Columns by Index Using Base R</h2>
We can also rename a specific column in the <b>mtcars </b>dataset by index. For example, here is how to rename the second column name “cyl” by index:
<b>#rename the second column name in mtcars
names(mtcars)[2] &lt;- "cylinders"
names(mtcars)
# [1] "mpg" "cylinders" "disp" "hp" "drat" "wt" 
# [7] "qsec" "vs" "am" "gear" "carb"</b>
Notice how only the “cyl” column is renamed to “cylinders” and all of the other columns keep their original names.
<h2>Renaming<em> </em>Columns Using dplyr</h2>
Another way to rename columns in R is by using the <b>rename()</b> function in the <b>dplyr</b> package. The basic syntax for doing so is as follows:
<b>data %>% rename(new_name1 = old_name1, new_name2 = old_name2, ....)</b>
For example, here is how to rename the “mpg” and “cyl” column names in the <b>mtcars</b> dataset:
<b>#install (if not already installed) and load dplyr package
if(!require(dplyr)){install.packages('dplyr')}
#rename the "mpg" and "cyl" columns
new_mtcars &lt;- mtcars %>% 
                rename(  miles_g = mpg,  cylinder = cyl  )
#view new column names
names(new_mtcars)
# [1] "miles_g" "cylinder" "disp" "hp" "drat" "wt" 
# [7] "qsec" "vs" "am" "gear" "carb" </b>
Using this approach,  you can rename as many columns at once as you’d like by name.
<h2>Renaming<em> </em>Columns Using data.table</h2>
Yet another way to rename columns in R is by using the <b>setnames()</b> function in the <b>data.table </b>package. The basic syntax for doing so is as follows:
<b>setnames(data, old=c("old_name1","old_name2"), new=c("new_name1", "new_name2"))</b>
For example, here is how to rename the “mpg” and “cyl” column names in the <b>mtcars</b> dataset:
<b>#install (if not already installed) and load data.table package
if(!require(data.table)){install.packages('data.table')}
#rename "mpg" and "cyl" column names in mtcars
setnames(mtcars, old=c("mpg","cyl"), new=c("miles_g", "cylinder"))
#view new column names
names(mtcars)
#[1] "miles_g" "cylinder" "disp" "hp" "drat" "wt" 
#[7] "qsec" "vs" "am" "gear" "carb"  </b>
Using this approach,  you can rename as many columns at once as you’d like by name.
<h2><span class="orange">The Complete Guide: How to Report ANOVA Results</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
When reporting the results of a one-way ANOVA, we always use the following general structure:
A brief description of the independent and dependent variable.
The overall F-value of the ANOVA and the corresponding p-value.
The results of the post-hoc comparisons (if the p-value was statistically significant).
Here’s the exact wording we can use:
A one-way ANOVA was performed to compare the effect of [independent variable] on [dependent variable].
 
A one-way ANOVA revealed that there [was or was not] a statistically significant difference in [dependent variable] between at least two groups (F(between groups df, within groups df) = [F-value], p = [p-value]).
 
Tukey’s HSD Test for multiple comparisons found that the mean value of [dependent variable] was significantly different between [group name] and [group name] (p = [p-value], 95% C.I. = [lower, upper]).
 
There was no statistically significant difference between [group name] and [group name] (p=[p-value]).
The following example shows how to report the results of a one-way ANOVA in practice.
<h3>Example: Reporting the Results of a One-Way ANOVA</h3>
Suppose a researcher recruits 30 students to participate in a study. The students are randomly assigned to use one of three studying techniques for the next month to prepare for an exam. At the end of the month, all of the students take the same test.
The researcher then performs a one-way ANOVA to determine if there is a difference in mean exam scores between the three groups.
The following table shows the results of the one-way ANOVA along with the Tukey post-hoc multiple comparisons table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss10.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss11.png">
Here is how to report the results of the one-way ANOVA:
A one-way ANOVA was performed to compare the effect of three different studying techniques on exam scores.
 
A one-way ANOVA revealed that there was a statistically significant difference in mean exam score between at least two groups (F(2, 27) = [4.545], p = 0.02).
 
Tukey’s HSD Test for multiple comparisons found that the mean value of exam score was significantly different between technique 1 and technique 2 (p = 0.024, 95% C.I. = [-14.48, -0.92]).
 
There was no statistically significant difference in mean exam scores between technique 1 and technique 3 (p=0.883) or between technique 2 and technique 3 (p=0.067).
<h3>Things to Keep in Mind</h3>
Here are a few things to keep in mind when reporting the results of a one-way ANOVA:
<b>Use a descriptive statistics table.</b>
It can be helpful to present a descriptive statistics table that shows the mean and standard deviation of values in each treatment group as well to give the reader a more complete picture of the data.
For example, SPSS produces the following descriptive statistics table that shows the mean and standard deviation of exam scores for students in each of the three study technique groups:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/anovaspss9.png">
<b>Only report post-hoc results if necessary.</b>
If the overall p-value of the ANOVA is not statistically significant, then you will not conduct post-hoc multiple comparisons between groups. This means you obviously don’t have to report any post-hoc results in the final report.
If you do have to conduct post-hoc tests, the Tukey HSD test is the most commonly used one but occasionally you may use the  Scheffe or Bonferroni test  instead.
<b>Round p-values when necessary.</b>
As a general rule of thumb, the overall F value and any p-values in ANOVA results are rounded to either two or three decimal places for brevity.
No matter how many decimal places you choose to use, be sure to be consistent throughout the report.
<h2><span class="orange">How to Report Chi-Square Results in APA Format</span></h2>
There are two types of Chi-Square tests that are commonly used:
<b> Chi-Square Goodness of Fit Test </b>: Used to determine whether or not a categorical variable follows a hypothesized distribution.
 <b>Chi-Square Test of Independence</b> : Used to determine whether or not there is a significant association between two categorical variables.
We use the following general structure to report the results of a <b>Chi-Square Goodness of Fit Test</b> in APA format:
A Chi-Square Goodness of Fit Test was performed to determine whether the proportion of <b>[variable name]</b> was equal between <b>[number of groups]</b>.
 
The proportions <b>[did or did not]</b> differ by <b>[variable name]</b>, X<sup>2</sup>(<b>df, N</b>) = <b>[X<sup>2</sup> value]</b>, p = <b>[p-value]</b>.
And we use the following general structure to report the results of a <b>Chi-Square Test of Independence</b> in APA format:
A Chi-Square Test of Independence was performed to assess the relationship between <b>[variable 1]</b> and <b>[variable 2]</b>.
 
There <b>[was or was not]</b> a significant relationship between the two variables, <em>X<sup>2</sup></em>(<b>df, N</b>) = <b>[X<sup>2</sup> value]</b>, p = <b>[p-value]</b>.
Keep the following in mind when reporting the results of a Chi-Square test in APA format:
Round the p-value to three decimal places.
Round the value for the Chi-Square test statistic <em>X<sup>2</sup></em> to two decimal places.
Drop the leading 0 for the p-value and <em>X<sup>2</sup></em> (e.g. use .72, not 0.72)
The following examples show how to report the results of both types of Chi-Square tests in practice.
<h3>Example 1: Reporting Results of Chi-Square Goodness of Fit Test</h3>
Suppose an economist collected data on the proportion of residents in three different cities who supported a certain law. He performed a Chi-Square Goodness of Fit test to determine if the proportion of residents who supported the law differed between the three cities.
Here is how to report the results in APA format:
A Chi-Square Goodness of Fit Test was performed to determine whether the proportion of residents who supported a certain law was equal between three different cities.
 
The proportions did not differ by city, <em>X<sup>2</sup></em>(2, N = 60) = 4.36, p = .113.
<h3>Example 2: Reporting Results of Chi-Square Test of Independence</h3>
A professor collects data on sports preference and gender among his students. He performs a Chi-Square Test of Independence to determine if there is a significant relationship between the two variables.
Here is how to report the results in APA format:
A Chi-Square Test of Independence was performed to assess the relationship between sports preference and gender.
 
There was a significant relationship between the two variables, <em>X<sup>2</sup></em>(2, N=50) = 7.34, p = .025. Women were less likely to prefer football compared to men.
<h2><span class="orange">The Complete Guide: How to Report Confidence Intervals</span></h2>
A  confidence interval  is a range of values that is likely to contain some population parameter with a certain level of confidence.
When reporting confidence intervals, we always use the following format:
<b>95% CI [LL, UL]</b>
 
where
 
<b>LL</b>: Lower limit of confidence interval
<b>UL</b>: Upper limit of confidence interval
The following examples show how to report confidence intervals for different statistical tests in practice.
<h3>Example 1: Confidence Interval for a Mean</h3>
Suppose a biologist wants to know the mean weight of a certain species of turtles.
She measures the weight of a random sample of 25 turtles and finds the sample mean weight to be 300 pounds with a 95% confidence interval of [292.75 pounds, 307.25 pounds].
Here’s how she might report the results:
A formal study has revealed that the average weight of turtles in this population is 300 pounds, 95% CI [292.75, 307.25].
<h3>Example 2: Confidence Interval for the Difference in Means</h3>
Suppose a biologist wants to estimate the difference in mean weight between two different populations of turtles.
She collects data for both populations of turtles and finds the mean difference to be 10 pounds with a 90% confidence interval of [-3.07 pounds, 23.07 pounds].
Here’s how she might report the results:
A formal study has revealed that the difference in average weights between the two populations of turtles is 10 pounds, 90% CI [-3.07, 23.07].
<h3>Example 3: Confidence Interval for a Proportion</h3>
Suppose a biologist wants to estimate the proportion of a certain species of turtles that have spots on their backs.
She collects data for a random sample of turtles and finds that 18% (.18) of them have spots with a 99% confidence interval of [0.15, 0.21].
Here’s how she might report the results:
A formal study has revealed that 18% of turtles in this population have spots on their back, 99% CI [0.15, 0.21].
<h3>Example 4: Confidence Interval for the Difference in Proportions</h3>
Suppose a biologist wants to estimate the difference in proportions of two species of turtles that have spots on their backs.
She collects data for both populations and finds that the mean difference in the proportions is 7% (.07) with a 95% confidence interval of [0.02, 0.12].
Here’s how she might report the results:
A formal study has revealed that the difference in proportion of turtles who have spots on their backs between the two populations is 7%, 95% CI [0.02, 0.12].
<h2><span class="orange">How to Report Cronbach’s Alpha (With Examples)</span></h2>
<b>Chronbach’s Alpha</b> is a way to measure the  internal consistency  of a questionnaire or survey.
Cronbach’s Alpha ranges between 0 and 1, with higher values indicating that the survey or questionnaire is more reliable.
When reporting the value of Cronbach’s Alpha in a final report, you need to include the following two values:
The number of items used on the subscale.
The value of Cronbach’s Alpha.
The following examples show how to report Cronbach’s Alpha in different situations.
<h3>Example 1: Reporting Cronbach’s Alpha for One Subscale</h3>
Suppose a restaurant manager wants to measure overall satisfaction among customers. She decides to send out a survey to 200 customers who can rate the restaurant on a scale of 1 to 5 for 12 different categories.
When she receives the results of the survey, she finds that the value for Cronbach’s Alpha is 0.84.
Here is how she would report Cronbach’s Alpha in a formal write-up:
A satisfaction survey was sent to 200 customers. The survey consisted of 12 items and the value for Cronbach’s Alpha for the survey was α = .84.
<h3>Example 2: Reporting Cronbach’s Alpha for Multiple Subscales</h3>
Suppose a Human Resources manager at a company sends out a three-part questionnaire to all 500 employees at the company.
When she receives the results of the survey, she calculates the value for Cronbach’s Alpha for all three of the subscales.
Here is how she would report the value of Cronbach’s Alpha in a formal write-up:
A three-part questionnaire was sent to 500 employees. The agreeableness subscale consisted of 10 items (α = .65), the leadership subscale consisted of 12 items (α = .82), and the overall satisfaction subscale consisted of 14 items (α = .88).
<h3>Notes</h3>
The following table describes how different values of Cronbach’s Alpha are usually interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
When reporting the value of Cronbach’s Alpha for a given scale or survey, you can reference this table to determine if the value is at least considered “acceptable” or not.
<h2><span class="orange">How to Report Fisher’s Exact Test Results</span></h2>
<b>Fisher’s exact test</b> is used to determine whether or not there is a significant association between two categorical variables.
It is typically used as an alternative to the  Chi-Square Test of Independence  when one or more of the cell counts in a 2×2 table is less than 5.
When reporting the results of Fisher’s exact test, we usually use the following general structure:
A brief mention of the two variables.
The p-value of the test (and whether it represents a one-tailed or two-tailed p-value).
Here’s the exact wording we can use:
Fisher’s exact test was used to determine if there was a significant association between [variable #1] and [#variable 2].
 
There [was or was not] a statistically significant association between [variable #1] and [variable #2] (p = [p-value]).
The following example shows how to report the results of Fisher’s exact test in practice.
<h3>Example: Reporting Results of Fisher’s Exact Test</h3>
Suppose we want to know whether or not gender is associated with political party preference at a particular college.
To explore this, we randomly survey 25 students on campus. The following table shows the results of the survey:
<table><tbody>
<tr>
<th> </th>
<th><b>Democrat</b></th>
<th><b>Republican</b></th>
</tr>
<tr>
<td><b>Female</b></td>
<td>8</td>
<td>4</td>
</tr>
<tr>
<td><b>Male</b></td>
<td>4</td>
<td>9</td>
</tr>
</tbody></table>
Since one or more of the cell counts in the table is less than 5, we can use Fisher’s exact test to determine if there is a statistically significant association between gender and political party preference.
Suppose we carry out the test  using SPSS  and get the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/fisherSPSS4.png">
Here is how to report the results of the test:
Fisher’s exact test was used to determine if there was a significant association between gender and political party preference.
 
There was not a statistically significant association between the two variables (two-tailed p = .115).
<h3>Things to Keep in Mind</h3>
Here are a few things to keep in mind when reporting the results of Fisher’s exact test:
<b>1. Use a descriptive statistics table.</b>
It can be helpful to present a descriptive statistics table that shows the total number of individuals used in the survey or study along with the total proportion of individuals that belonged to each variable in order to give the reader a more complete picture of the data.
<b>2. There is no test statistic to report.</b>
Unlike a Chi-Square test of independence, Fisher’s exact test has no test statistic to report.
Instead, we simply report the p-value of the test and note that we used Fisher’s exact test. This is a commonly used test, so it’s well known that there will be no test statistic included in the final report.
<h2><span class="orange">The Complete Guide: How to Report Logistic Regression Results</span></h2>
<b>Logistic regression</b> is a type of regression analysis we use when the  response variable  is binary.
We can use the following general format to report the results of a logistic regression model:
Logistic regression was used to analyze the relationship between [predictor variable 1], [predictor variable 2], … [predictor variable <em>n</em>] and [response variable].
 
It was found that, holding all other predictor variables constant, the odds of [response variable] occurring [increased or decreased] by [some percent] (95% CI [Lower Limit, Upper Limit]) for a one -unit increase in [predictor variable 1].
 
It was found that, holding all other predictor variables constant, the odds of [response variable] occurring [increased or decreased] by [some percent] (95% CI [Lower Limit, Upper Limit]) for a one -unit increase in [predictor variable 2].
 
…
We can use this basic syntax to report the odds ratios and corresponding 95% confidence interval for the odds ratios of each predictor variable in the model.
The following example shows how to report the results of a logistic regression model in practice.
<h3>Example: Reporting Logistic Regression Results</h3>
Suppose a professor wants to understand whether or not two different studying programs (program A vs. program B) and number of hours studied affect the probability that a student passes the final exam in his class.
He fits a logistic regression model using hours studied and studying program as the predictor variables and exam result (pass or fail) as the response variable.
The following output shows the results of the logistic regression model:
<b>Coefficients:
            Estimate    Std. Error z value     Pr(>|z|)    
(Intercept)   -2.415         0.623  -3.876       &lt;0.000
program_A      0.344         0.156   2.205        0.027
hours          0.006         0.002   3.000        0.003
</b>
Before we report the results of the logistic regression model, we should first calculate the odds ratio for each predictor variable by using the formula e<sup>β</sup>.
For example, here’s how to calculate the odds ratio for each predictor variable:
Odds ratio of Program: e<sup>.344</sup> = 1.41
Odds ratio of Hours: e<sup>.006</sup> = 1.006
We should also calculate the 95% confidence interval for the odds ratio of each predictor variable using the formula e<sup>(β +/- 1.96*std error)</sup>.
For example, here’s how to calculate the odds ratio for each predictor variable:
95% C.I. for odds ratio of Program: e<sup>.344 +/- 1.96*.156</sup> = [1.04 , 1.92]
95% C.I. for odds ratio of Hours: e<sup>.006 +/- 1.96*.002</sup> = [1.002 , 1.009]
Now that we’ve calculated the odds ratio and corresponding confidence interval for each predictor variable, we can report the results of the model as follows:
Logistic regression was used to analyze the relationship between studying program and hours studied on the probability of passing a final exam.
 
It was found that, holding hours studied constant, the odds of passing the final exam increased by 41% (95% CI [.04, .92]) for students who used studying program A compared to studying program B.
 
It was also found that, holding studying program constant, the odds of passing the final exam increased by .6% (95% CI [.002, .009]) for each additional hour studied.
Note that we reported the odds ratios for the predictor variables as opposed to the beta values from the model because the odds ratios are easier to interpret and understand.
<h2><span class="orange">The Complete Guide: How to Report Odds Ratios</span></h2>
In statistics, an <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group compared to the odds of an event occurring in a control group.
When reporting an odds ratio, we typically include the following:
The value of the odds ratio
The confidence interval for the odds ratio
How to interpret the odds ratio in the context of the problem
For example, we might report something like this:
There was no significant difference in the odds of contracting a disease between the smoking and non-smoking groups (OR = 1.44, 95% CI [0.91, 1.97] ).
<b>Note</b>: If a confidence interval for an odds ratio includes the number “1” then there is not a statistically difference in the odds of an event happening between the two groups. Read a full explanation  here .
The following examples show how to report an odds ratio in different scenarios.
<h3>Example 1: Odds Ratio Between Training Programs</h3>
Suppose a basketball coach uses a new training program to see if it increases the number of players who are able to pass a certain skills test, compared to an old training program.
The coach recruits 50 players to use each program and records the number of players who pass using each program.
He finds that the odds ratio between the two programs is 0.599 and the 95% confidence interval for the odds ratio is [0.245, 1.467].
Here is how he may report the results:
There was no significant difference in the odds of passing the skills test between players who used the new program compared to players who use the old program (OR = 0.599, 95% CI [0.245, 1.467]).
<h3>Example 2: Odds Ratio Between Drugs</h3>
Suppose a doctor recruits 20 patients to try drug A and 20 patients to try drug B to determine if there is a difference in the odds of a patient being able to pass a breath-holding test.
He finds that the odds ratio between program A and program B is 1.78 and the 95% confidence interval for the odds ratio is [1.57, 1.99].
Here is how she may report the results:
There was a significant difference in the odds of passing the breath-holding test between patients who took drug A compared to patients who took drug B (OR = 1.78, 95% CI [1.57, 1.99]).
<h3>Example 3: Odds Ratio Between Studying Programs</h3>
Suppose a teacher recruits 30 students to use a weekly studying program and 30 students to use a daily studying program to determine if there is a difference in the odds of a student being able to pass a specific exam.
She finds that the odds ratio between the weekly program and the daily program is 1.22 and the 95% confidence interval for the odds ratio is [0.91, 1.53].
Here is how she may report the results:
There was not a significant difference in the odds of passing the exam between the two studying programs (OR = 1.22, 95% CI [0.91, 1.53]).
<h2><span class="orange">How to Report P-Values in APA Format (With Examples)</span></h2>
In statistics, <b>p-values</b> are used in  hypothesis testing  with t-tests, Chi-square tests, regression models, ANOVA models, and a variety of other statistical methods.
When reporting p-values in a formal report, you should adhere to the following guidelines:
A p-value larger than .01 should be reported to two decimal places, p-values between .01 and .001 to three decimal places, and p-values less than .001 simply as <em>p</em> &lt; .001.
Do not write a zero in front of the p-value. 
Never write <em>p</em> = .000 (although some statistical software report this) because it’s not possible. Instead, write <em>p</em> &lt; .001.
Report the test statistic along with the p-value to give the reader complete information.
It’s important to note that there is no standard way to write p-values in reports. Different journals and institutions have different standard formats, but the most common ones you’ll encounter include:
<em>p</em>
<em>p</em> value
p-value
P value
P
Before writing your results, you should check the standard format used by the journal or institution where your report will be published.
The following examples show how to report p-values from different statistical tests.
<h2>Example 1: How to Report P-Values from a t-Test</h2>
Suppose researchers want to know if a new fuel treatment leads to a change in the average miles per gallon of a certain car.
To test this, they conduct an experiment in which 12 cars receive the new fuel treatment and 12 cars do not.
The following screenshot shows the results of the independent samples t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/twoSampSPSS5.png">
Here’s how to report the results of the test:
A two sample t-test was performed to compare miles per gallon between fuel treatment and no fuel treatment.
 
There was not a significant difference in miles per gallon between fuel treatment (M = 22.75, SD = 3.25) and no fuel treatment (M = 21, SD = 2.73); <em>t</em>(22) = -1.428, <em>p</em> = .17.
In this example, since the p-value was greater than .01 we only reported the value to two decimal places.
<h2>Example 2: How to Report P-Values from a Chi-Square Test</h2>
Suppose a professor collects data on political party preference and gender among his students.
He performs a  Chi-Square Test of Independence  to determine if there is a significant relationship between the two variables.
The test returns the following results:
<em>X<sup>2</sup></em> test statistic: 15.33
p-value = .004
Here is how to report the results in APA format:
A Chi-Square Test of Independence was performed to assess the relationship between political party preference and gender.
 
There was a significant relationship between the two variables, <em>X<sup>2</sup></em>(2, N=500) = 15.33, <em>p</em> = .004.
In this example, since the p-value was between .01  and .001 we reported the value to three decimal places.
<h2>Example 3: How to Report P-Values from a Two Proportion Z-Test</h2>
Suppose researchers want to know if there is a difference in the proportion of residents who support a certain law in county A compared to the proportion who support the law in county B.
They survey a  simple random sample  of 50 residents from each county and then perform a  two proportion z-test  with the following results
The test returns the following results:
z test statistic: 4.77
p-value = .000
Here is how to report the results in APA format:
A two proportion z-test was performed to determine if there was a difference in the proportion of residents who supported a certain law between county A and county B.
 
There was a significant difference in the proportion of residents who supported the law between the two counties, <em>z</em> = 4.77, <em>p</em> &lt; .001.
In this example, since the p-value was reported as .000 by the software, we reported the value as <em>p</em> &lt; .001 since it’s not possible to have a p-value equal to exactly zero.
<b>Related:</b>  How to Interpret a P-Value of 0.000 
<h2>Additional Resources</h2>
The following tutorials explain how to report the results of other statistical methods:
 How to Report Confidence Intervals 
 How to Report T-Test Results 
 How to Report Regression Results 
 How to Report ANOVA Results 
<h2><span class="orange">How to Report Pearson’s r in APA Format (With Examples)</span></h2>
A  Pearson Correlation Coefficient , often denoted <em>r</em>, measures the linear association between two variables.
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
We use the following general structure to report a Pearson’s <em>r</em> in APA format:
A Pearson correlation coefficient was computed to assess the linear relationship between <b>[variable 1]</b> and <b>[variable 2]</b>.
 
There was a <b>[negative or positive]</b> correlation between the two variables, r(<b>df</b>) = <b>[r value]</b>, p = <b>[p-value]</b>.
Keep in mind the following when reporting Pearson’s <em>r</em> in APA format:
Round the p-value to three decimal places.
Round the value for <em>r</em> to two decimal places.
Drop the leading 0 for the p-value and <em>r</em> (e.g. use .77, not 0.77)
The degrees of freedom (df) is calculated as N – 2.
The following examples show how to report Pearson’s <em>r</em> in APA format in various scenarios.
<h3>Example 1: Hours Studied vs. Exam Score Received</h3>
A professor collected data for the number of hours studied and the exam score received for 40 students in his class. He found the Pearson correlation coefficient between the two variables to be 0.48 with a corresponding p-value of 0.002.
Here is how to report Pearson’s <em>r</em> in APA format:
A Pearson correlation coefficient was computed to assess the linear relationship between hours studied and exam score.
 
There was a positive correlation between the two variables, r(38) = .48, p = .002.
<h3>Example 2: Time Spent Running vs. Body Fat</h3>
A doctor collected data for the number of hours spent running per week and body fat percentage for 35 patients. He found the Pearson correlation coefficient between the two variables to be -0.37 with a corresponding p-value of 0.029.
Here is how to report Pearson’s <em>r</em> in APA format:
A Pearson correlation coefficient was computed to assess the linear relationship between hours spent running and body fat percentage.
 
There was a negative correlation between the two variables, r(33) = -.37, p = .029.
<h3>Example 3: Ad Spend vs. Revenue Generated</h3>
A company collected data for the amount of money spent on advertising and the total revenue generated during 15 consecutive sales periods. They found the Pearson correlation coefficient between the two variables to be 0.71 with a corresponding p-value of 0.003.
Here is how to report Pearson’s <em>r</em> in APA format:
A Pearson correlation coefficient was computed to assess the linear relationship between advertising spend and total revenue.
 
There was a positive correlation between the two variables, r(13) = .71, p = .003.
<h2><span class="orange">The Complete Guide: How to Report Regression Results</span></h2>
In statistics, linear regression models are used to quantify the relationship between one or more predictor variables and a  response variable .
We can use the following general format to report the results of a  simple linear regression model :
Simple linear regression was used to test if [predictor variable] significantly predicted [response variable].
 
The fitted regression model was: [fitted regression equation]
 
The overall regression was statistically significant (R<sup>2</sup> = [R<sup>2</sup> value], F(df regression, df residual) = [F-value], p = [p-value]).
 
It was found that [predictor variable] significantly predicted [response variable] (β = [β-value], p = [p-value]).
And we can use the following format to report the results of a  multiple linear regression model :
Multiple linear regression was used to test if [predictor variable 1], [predictor variable 2], … significantly predicted [response variable].
 
The fitted regression model was: [fitted regression equation]
 
The overall regression was statistically significant (R<sup>2</sup> = [R<sup>2</sup> value], F(df regression, df residual) = [F-value], p = [p-value]).
 
It was found that [predictor variable 1] significantly predicted [response variable] (β = [β-value], p = [p-value]).
 
It was found that [predictor variable 2] did not significantly predict [response variable] (β = [β-value], p = [p-value]).
The following examples show how to report regression results for both a simple linear regression model and a multiple linear regression model.
<h3>Example: Reporting Results of Simple Linear Regression</h3>
Suppose a professor would like to use the number of hours studied to predict the exam score that students will receive on a certain exam. He collects data for 20 students and fits a simple linear regression model.
The following screenshot shows the output of the regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/simpleRegressionExcel5.png">
Here is how to report the results of the model:
Simple linear regression was used to test if hours studied significantly predicted exam score.
 
The fitted regression model was: Exam score = 67.1617 + 5.2503*(hours studied).
 
The overall regression was statistically significant (R<sup>2</sup> = .73, F(1, 18) = 47.99, p &lt; .000).
 
It was found that hours studied significantly predicted exam score (β = 5.2503, p &lt; .000).
<h3>Example: Reporting Results of Multiple Linear Regression</h3>
Suppose a professor would like to use the number of hours studied and the number of prep exams taken to predict the exam score that students will receive on a certain exam. He collects data for 20 students and fits a multiple linear regression model.
The following screenshot shows the output of the regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
Here is how to report the results of the model:
Multiple linear regression was used to test if hours studied and prep exams taken significantly predicted exam score.
 
The fitted regression model was: Exam Score = 67.67 + 5.56*(hours studied) – 0.60*(prep exams taken)
 
The overall regression was statistically significant (R<sup>2</sup> = 0.73, F(2, 17) = 23.46, p = &lt; .000).
 
It was found that hours studied significantly predicted exam score (β = 5.56, p = &lt; .000).
 
It was found that prep exams taken did not significantly predict exam score (β = -0.60, p = 0.52).
<h2><span class="orange">How to Report the Results of a Repeated Measures ANOVA</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group..
When reporting the results of a repeated measures ANOVA, we always use the following general structure:
A brief description of the independent and dependent variable.
The overall F-value of the ANOVA and the corresponding p-value.
Here’s the exact wording we can use:
A repeated measures ANOVA was performed to compare the effect of [independent variable] on [dependent variable].
 
There [was or was not] a statistically significant difference in [dependent variable] between at least two groups (F(between groups df, within groups df) = [F-value], p = [p-value]).
The following example shows how to report the results of a repeated measures ANOVA in practice.
<h3>Example: Reporting Results of a Repeated Measures ANOVA</h3>
Researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs.
Since each patient is measured on each of the four drugs, they use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
The following table shows the results of the repeated measures ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/repeatedMeasuresResults1.png">
Here is how to report the results:
A repeated measures ANOVA was performed to compare the effect of a certain drug on reaction time.
 
There was a statistically significant difference in reaction time between at least two groups (F(4, 3) = 18.106, p &lt; .000).
<h3>Things to Keep in Mind</h3>
Here are a few things to keep in mind when reporting the results of a repeated measures ANOVA:
<b>Use a descriptive statistics table.</b>
It can be helpful to present a descriptive statistics table that shows the mean and standard deviation of values in each treatment group as well to give the reader a more complete picture of the data.
<b>Round p-values when necessary.</b>
As a general rule of thumb, you should round the values for the overall F value and any p-values to either two or three decimal places for brevity.
No matter how many decimal places you use, be sure to be consistent throughout the report.
<h2><span class="orange">The Complete Guide: How to Report Skewness & Kurtosis</span></h2>
In statistics, <b>skewness </b>and <b>kurtosis </b>are two ways to measure the shape of a distribution.
<b>Skewness </b>is a measure of the asymmetry of a distribution. This value can be positive or negative.
Negative skew indicates that the tail is on the left side of the distribution, which extends towards more negative values.
Positive skew indicates that the tail is on the right side of the distribution, which extends towards more positive values.
A value of zero indicates that there is no skewness in the distribution at all, meaning the distribution is perfectly  symmetrical .
<b>Kurtosis </b>is a measure of whether or not a distribution is heavy-tailed or light-tailed relative to a  normal distribution .
The kurtosis of a normal distribution is 3.
If a given distribution has a kurtosis less than 3, it is said to be <em>playkurtic</em>, which means it tends to produce fewer and less extreme outliers than the normal distribution.
If a given distribution has a kurtosis greater than 3, it is said to be <em>leptokurtic</em>, which means it tends to produce more outliers than the normal distribution.
<b>Note: </b>Some formulas (Fisher’s definition) subtract 3 from the kurtosis to make it easier to compare with the normal distribution. Using this definition, a distribution would have kurtosis greater than a normal distribution if it had a kurtosis value greater than 0.
When reporting the skewness and kurtosis of a given distribution in a formal write-up, we generally use the following format:
The skewness of [variable name] was found to be -.89, indicating that the distribution was left-skewed.
 
The kurtosis of [variable name] was found to be 4.26, indicating that the distribution was more heavy-tailed compared to the normal distribution.
Keep in mind the following when reporting the results:
Round the values for skewness and kurtosis to two decimal places.
Drop the leading 0 when reporting the values (e.g. use .79, not 0.79)
The following example shows how to use this format in practice.
<h3>Example: Reporting Skewness & Kurtosis</h3>
Suppose we’re analyzing the distribution of exam scores among students at a certain university.
Using statistical software, we calculate the values for the skewness and kurtosis of the distribution to be:
Skewness: <b>-1.391777</b>
Kurtosis: <b>4.170865</b>
We would report these values as follows:
The skewness of the exam scores was found to be -1.39, indicating that the distribution was left-skewed.
 
The kurtosis of the exam scores was found to be 4.17, indicating that the distribution was more heavy-tailed compared to the normal distribution.
Along with reporting these values for skewness and kurtosis, we generally include some chart to visualize the distribution of values such as a histogram or boxplot so the reader can get a visual understanding of the distribution as well.
<h2><span class="orange">How to Report Spearman’s Correlation in APA Format</span></h2>
Spearman’s rank correlation is used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class).
We use the following general structure to report Spearman’s correlation in APA format:
Spearman’s rank correlation was computed to assess the relationship between <b>[variable 1]</b> and <b>[variable 2]</b>.
 
There was a <b>[negative or positive]</b> correlation between the two variables, r(<b>df</b>) = <b>[r value]</b>, p = <b>[p-value]</b>.
Keep the following in mind when reporting Spearman’s rank correlation in APA format:
Round the p-value to three decimal places.
Round the value for <em>r</em> to two decimal places.
Drop the leading 0 for the p-value and <em>r</em> (e.g. use .77, not 0.77)
The degrees of freedom (df) is calculated as N – 2.
The following examples show how to report Spearman’s rank correlation in APA format in various scenarios.
<b>Related:</b>  When to Use Spearman’s Rank Correlation (2 Scenarios) 
<h3>Example 1: Math Score vs. Science Score</h3>
A teacher collected data for the rank of math scores and the rank of science scores for 30 students in her class. She found Spearman’s rank correlation between the two variables to be 0.48 with a corresponding p-value of 0.043.
Here is how to report Spearman’s rank correlation in APA format:
Spearman’s rank correlation was computed to assess the relationship between math scores and science scores.
 
There was a positive correlation between the two variables, r(28) = .48, p = .043.
<h3>Example 2: Points vs. Rebounds</h3>
A sports scientist collected data for the rank of points scored vs. rebounds collected by 50 professional basketball players. He found Spearman’s rank correlation between the two variables to be -0.27 with a corresponding p-value of 0.026.
Here is how to report Spearman’s rank correlation in APA format:
Spearman’s rank correlation was computed to assess the relationship between points scored and rebounds collected.
 
There was a negative correlation between the two variables, r(48) = -.27, p = .026.
<h3>Example 3: Hours Worked vs. Productivity</h3>
A company collected data for the total hours worked vs. overall productivity of 25 employees. They found Spearman’s rank correlation between the two variables to be 0.57 with a corresponding p-value of 0.039.
Here is how to report Spearman’s rank correlation in APA format:
Spearman’s rank correlation was computed to assess the relationship between hours worked and overall productivity.
 
There was a positive correlation between the two variables, r(23) = .57, p = .039.
<h2><span class="orange">How to Report T-Test Results (With Examples)</span></h2>
We can use the following general format to report the results of a  one sample t-test :
A one sample t-test was performed to compare [variable of interest] against the population mean.
 
The mean value of [variable of interest] (M = [Mean], SD = [standard deviation]) was significantly [higher, lower, or different] than the population mean; t(df) = [t-value], p = [p-value].
We can use the following format to report the results of an  independent two samples t-test :
A two sample t-test was performed to compare [response variable of interest] in [group 1] and [group 2].
 
There [was or was not] a significant difference in [response variable of interest] between [group1] (M = [Mean], SD = [standard deviation]) and [group2] (M = [Mean], SD = [standard deviation]); t(df) = [t-value], p = [p-value].
We can use the following format to report the results of a  paired samples t-test :
A paired samples t-test was performed to compare [response variable of interest] in [group 1] and [group 2].
 
There [was or was not] a significant difference in [response variable of interest] between [group1] (M = [Mean], SD = [standard deviation]) and [group2] (M = [Mean], SD = [standard deviation]); t(df) = [t-value], p = [p-value].
<b>Note:</b> The “M” in the results stands for sample mean, the “SD” stands for sample standard deviation, and “df” stands for degrees of freedom associated with the t-test statistic.
The following examples show how to report the results of each type of t-test in practice.
<h2>Example: Reporting Results of a One Sample T-Test</h2>
A botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a random sample of 12 plants and performs a one sample-test.
The following screenshot shows the results of the test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS5.png">
Here’s how to report the results of the test:
A one sample t-test was performed to compare the mean height of a certain species of plant against the population mean.
 
The mean value of height (M = 14.33, SD = 1.37) was not significantly different than the population mean; t(11) = -1.685, p = .120.
<h2>Example: Reporting Results of an Independent Samples T-Test</h2>
Researchers want to know if a new fuel treatment leads to a change in the average miles per gallon of a certain car. To test this, they conduct an experiment in which 12 cars receive the new fuel treatment and 12 cars do not.
The following screenshot shows the results of the independent samples t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/twoSampSPSS5.png">
Here’s how to report the results of the test:
A two sample t-test was performed to compare miles per gallon between fuel treatment and no fuel treatment.
 
There was not a significant difference in miles per gallon between fuel treatment (M = 22.75, SD = 3.25) and no fuel treatment (M = 21, SD = 2.73); t(22) = -1.428, p = .167.
<h2>Example: Reporting Results of a Paired Samples T-Test</h2>
Researchers want to know if a new fuel treatment leads to a change in the average mpg of a certain car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with and without the fuel treatment.
The following screenshot shows the results of the paired samples t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/pairedSPSS4.png">
Here’s how to report the results of the test:
A paired samples t-test was performed to compare miles per gallon between fuel treatment and no fuel treatment.
 
There was a significant difference in miles per gallon between fuel treatment (M = 22.75, SD = 3.25) and no fuel treatment (M = 21, SD = 2.73); t(11) = -2.244, p = .046.
<h2>Additional Resources</h2>
Use the following calculators to automatically perform various t-tests:
 One Sample t-test Calculator 
 Two Sample t-test Calculator 
 Paired Samples t-test Calculator 
<h2><span class="orange">The Complete Guide: How to Report Two-Way ANOVA Results</span></h2>
A  two-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups that have been split on two variables.
When reporting the results of a two-way ANOVA, we always use the following general structure:
A brief description of the independent and dependent variables.
Whether or not there was a significant interaction effect between the two independent variables.
Whether or not the two independent variables had a statistically significant effect on the dependent variable.
Here’s the exact wording we can use:
A two-way ANOVA was performed to analyze the effect of [independent variable 1] and [independent variable 2] on [dependent variable].
 
A two-way ANOVA revealed that there [was or was not] a statistically significant interaction between the effects of [independent variable 1] and [independent variable 2] (F(df interaction, df within) = [F-value], p = [p-value]).
 
Simple main effects analysis showed that [independent variable 1] [did or did not] have a statistically significant effect on [dependent variable] (p = [p-value]).
 
Simple main effects analysis showed that [independent variable 2] [did or did not] have a statistically significant effect on [dependent variable] (p = [p-value]).
The following example shows how to report the results of a two-way ANOVA in practice.
<h3>Example: Reporting the Results of a Two-Way ANOVA</h3>
A botanist wants to know whether different levels of sunlight exposure and watering frequency effect plant growth. She plants 40 seeds and lets them grow for one month under different conditions for sunlight exposure and watering frequency.
She then performs a two-way ANOVA to determine if sunlight exposure and watering frequency effect plant growth.
The following table shows the results of the two-way ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/reportTwoWay1.png">
Here is how to report the results of the two-way ANOVA:
A two-way ANOVA was performed to analyze the effect of watering frequency and sunlight exposure on plant growth.
 
A two-way ANOVA revealed that there was not a statistically significant interaction between the effects of watering frequency and sunlight exposure (F(3, 32) = 1.242, p = .311).
 
Simple main effects analysis showed that watering frequency did not have a statistically significant effect on plant growth (p = .975).
 
Simple main effects analysis showed that sunlight exposure did have a statistically significant effect on plant growth (p &lt; .000).
<h3>Things to Keep in Mind</h3>
Here are a few things to keep in mind when reporting the results of a two-way ANOVA:
<b>1. Use a descriptive statistics table if necessary.</b>
It can be helpful to present a descriptive statistics table that shows the mean and standard deviation of values in each treatment group as well to give the reader a more complete picture of the data.
<b>2. Round p-values when necessary.</b>
As a rule of thumb, the overall F-value and any p-values in ANOVA results are rounded to either two or three decimal places for brevity.
No matter how many decimal places you choose to use, simply be consistent throughout the report.
<h2><span class="orange">How to Use Mutate to Create New Variables in R</span></h2>
This tutorial explains how to use the <b>mutate()</b> function in R to add new variables to a data frame.
<h2>Adding New Variables in R</h2>
The following functions from the <b>dplyr </b>library can be used to add new variables to a data frame:
<b>mutate()</b> – adds new variables to a data frame while preserving existing variables
<b>transmute() </b>– adds new variables to a data frame and drops existing variables
<b>mutate_all()</b> –  modifies all of the variables in a data frame at once
<b>mutate_at()</b> –  modifies specific variables by name
<b>mutate_if()</b> – modifies all variables that meet a certain condition
<h2>mutate()</h2>
The <b>mutate()</b> function adds new variables to a data frame while preserving any existing variables. The basic synax for mutate() is as follows:
<b>data &lt;- mutate(new_variable = existing_variable/3)</b>
<b>data:</b> the new data frame to assign the new variables to
<b>new_variable:</b> the name of the new variable
<b>existing_variable:</b> the existing variable in the data frame that you wish to perform some operation on to create the new variable
For example, the following code illustrates how to add a new variable <em>root_sepal_width </em>to the built-in <em>iris </em>dataset:
<b>#define data frame as the first six lines of the <em>iris </em>dataset
data &lt;- head(iris)
#view data
data
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
#load <em>dplyr </em>library
library(dplyr)
#define new column <em>root_sepal_width </em>as the square root of the <em>Sepal.Width </em>variable
data %>% mutate(root_sepal_width = sqrt(Sepal.Width))
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species root_sepal_width
#1          5.1         3.5          1.4         0.2  setosa         1.870829
#2          4.9         3.0          1.4         0.2  setosa         1.732051
#3          4.7         3.2          1.3         0.2  setosa         1.788854
#4          4.6         3.1          1.5         0.2  setosa         1.760682
#5          5.0         3.6          1.4         0.2  setosa         1.897367
#6          5.4         3.9          1.7         0.4  setosa         1.974842
</b>
<h2>transmute()</h2>
The <b>transmute()</b> function adds new variables to a data frame and drops existing variables. The following code illustrates how to add two new variables to a dataset and remove all existing variables:
<b>#define data frame as the first six lines of the <em>iris </em>dataset
data &lt;- head(iris)
#view data
data
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
#define two new variables and remove all existing variables
data %>% transmute(root_sepal_width = sqrt(Sepal.Width),   root_petal_width = sqrt(Petal.Width))
#  root_sepal_width root_petal_width
#1         1.870829        0.4472136
#2         1.732051        0.4472136
#3         1.788854        0.4472136
#4         1.760682        0.4472136
#5         1.897367        0.4472136
#6         1.974842        0.6324555
</b>
<h2>mutate_all()</h2>
The <b>mutate_all()</b> function modifies all of the variables in a data frame at once, allowing you to perform a specific function on all of the variables by using the <b>funs()</b>function. The following code illustrates how to divide all of the columns in a data frame by 10 using <b>mutate_all()</b>:
<b>#define new data frame as the first six rows of <em>iris </em>without the <em>Species </em>variable
data2 &lt;- head(iris) %>% select(-Species)
#view the new data frame
data2
#  Sepal.Length Sepal.Width Petal.Length Petal.Width
#1          5.1         3.5          1.4         0.2
#2          4.9         3.0          1.4         0.2
#3          4.7         3.2          1.3         0.2
#4          4.6         3.1          1.5         0.2
#5          5.0         3.6          1.4         0.2
#6          5.4         3.9          1.7         0.4
#divide all variables in the data frame by 10
data2 %>% mutate_all(funs(./10))
#  Sepal.Length Sepal.Width Petal.Length Petal.Width
#1         0.51        0.35         0.14        0.02
#2         0.49        0.30         0.14        0.02
#3         0.47        0.32         0.13        0.02
#4         0.46        0.31         0.15        0.02
#5         0.50        0.36         0.14        0.02
#6         0.54        0.39         0.17        0.04
</b>
Note that additional variables can be added to the data frame by specifying a new name to be appended to the old variable name:
<b>data2 %>% mutate_all(funs(mod = ./10))
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mod
#1          5.1         3.5          1.4         0.2             0.51
#2          4.9         3.0          1.4         0.2             0.49
#3          4.7         3.2          1.3         0.2             0.47
#4          4.6         3.1          1.5         0.2             0.46
#5          5.0         3.6          1.4         0.2             0.50
#6          5.4         3.9          1.7         0.4             0.54
#  Sepal.Width_mod Petal.Length_mod Petal.Width_mod
#1            0.35             0.14            0.02
#2            0.30             0.14            0.02
#3            0.32             0.13            0.02
#4            0.31             0.15            0.02
#5            0.36             0.14            0.02
#6            0.39             0.17            0.04
</b>
<h2>mutate_at()</h2>
The <b>mutate_at()</b> function modifies specific variables by name. The following code illustrates how to divide two specific variables by 10 using <b>mutate_at()</b>:
<b>data2 %>% mutate_at(c("Sepal.Length", "Sepal.Width"), funs(mod = ./10))
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length_mod
#1          5.1         3.5          1.4         0.2             0.51
#2          4.9         3.0          1.4         0.2             0.49
#3          4.7         3.2          1.3         0.2             0.47
#4          4.6         3.1          1.5         0.2             0.46
#5          5.0         3.6          1.4         0.2             0.50
#6          5.4         3.9          1.7         0.4             0.54
#  Sepal.Width_mod
#1            0.35
#2            0.30
#3            0.32
#4            0.31
#5            0.36
#6            0.39</b>
<h2>mutate_if()</h2>
The <b>mutate_if()</b> function modifies all variables that meet a certain condition. The following code illustrates how to use the <b>mutate_if() </b>function to convert any variables of type <em>factor </em>to type <em>character</em>:
<b>#find variable type of each variable in a data frame
data &lt;- head(iris)
sapply(data, class)
#Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species 
#   "numeric"    "numeric"    "numeric"    "numeric"     "factor" 
#convert any variable of type <em>factor </em>to type <em>character</em>
new_data &lt;- data %>% mutate_if(is.factor, as.character)
sapply(new_data, class)
#Sepal.Length  Sepal.Width Petal.Length  Petal.Width      Species 
#   "numeric"    "numeric"    "numeric"    "numeric"  "character"</b>
 The following code illustrates how to use the <b>mutate_if() </b>function to round any variables of type <em>numeric </em>to one decimal place:
<b>#define data as first six rows of <em>iris </em>dataset
data &lt;- head(iris)
#view data
data
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
#round any variables of type <em>numeric </em>to one decimal place
data %>% mutate_if(is.numeric, round, digits = 0)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1            5           4            1           0  setosa
#2            5           3            1           0  setosa
#3            5           3            1           0  setosa
#4            5           3            2           0  setosa
#5            5           4            1           0  setosa
#6            5           4            2           0  setosa
</b>
<b>Further reading:
 A Guide to apply(), lapply(), sapply(), and tapply() in R 
 How to Arrange Rows in R 
 How to Filter Rows in R 
</b>
<h2><span class="orange">How to use the Z Table (With Examples)</span></h2>
A  z-table  is a table that tells you what percentage of values fall below a certain z-score in a standard normal distribution.
A z-score simply tells you how many standard deviations away an individual data value falls from the mean. It is calculated as:
<b>z-score = (x – μ) / σ</b>
where:
<b>x: </b>individual data value
<b>μ: </b>population mean
<b>σ: </b>population standard deviation
This tutorial shows several examples of how to use the z table.
<h3>Example 1</h3>
The scores on a certain college entrance exam are normally distributed with mean μ = 82 and standard deviation σ = 8. Approximately what percentage of students score less than 84 on the exam?
<b>Step 1: Find the z-score.</b>
First, we will find the z-score associated with an exam score of 84:
z-score = (x – μ) /  σ = (84 – 82) / 8 = 2 / 8 = <b>0.25</b>
<b>Step 2: Use the z-table to find the percentage that corresponds to the z-score.</b>
Next, we will look up the value <b>0.25 </b>in the  z-table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/ztable1.png">
Approximately <b>59.87% </b>of students score less than 84 on this exam.
<h3>Example 2</h3>
The height of plants in a certain garden are normally distributed with a mean of  μ = 26.5 inches and a standard deviation of σ = 2.5 inches. Approximately what percentage of plants are greater than 26 inches tall?
<b>Step 1: Find the z-score.</b>
First, we will find the z-score associated with a height of 26 inches.
z-score = (x – μ) /  σ = (26 – 26.5) / 2.5 = -0.5 / 2.5 = <b>-0.2</b>
<b>Step 2: Use the z-table to find the percentage that corresponds to the z-score.</b>
Next, we will look up the value <b>-0.2 </b>in the  z-table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/ztable2.png">
We see that 42.07% of values fall below a z-score of -0.2. However, in this example we want to know what percentage of values are <em>greater </em>than -0.2, which we can find by using the formula 100% – 42.07% = 57.93%.
Thus, aproximately <b>59.87% </b>of the plants in this garden are greater than 26 inches tall.
<h3>Example 3</h3>
The weight of a certain species of dolphin is normally distributed with a mean of μ = 400 pounds and a standard deviation of σ = 25 pounds. Approximately what percentage of dolphins weigh between 410 and 425 pounds?
<b>Step 1: Find the z-scores.</b>
First, we will find the z-scores associated with 410 pounds and 425 pounds
z-score of 410 = (x – μ) /  σ = (410 – 400) / 25 = 10 / 25 = <b>0.4</b>
z-score of 425 = (x – μ) /  σ = (425 – 400) / 25 = 25 / 25 = <b>1</b>
<b>Step 2: Use the z-table to find the percentages that corresponds to each z-score.</b>
First, we will look up the value <b>0.4 </b>in the  z-table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/ztable3.png">
Then, we will look up the value <b>1 </b>in the  z-table :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/ztable4.png">
Lastly, we will subtract the smaller value from the larger value: <b>0.8413 – 0.6554 = 0.1859</b>.
Thus, approximately <b>18.59% </b>of dolphins weigh between 410 and 425 pounds.
<h2><span class="orange">How to Write a Null Hypothesis (5 Examples)</span></h2>
A hypothesis test uses sample data to determine whether or not some claim about a  population parameter  is true.
Whenever we perform a hypothesis test, we always write a null hypothesis and an alternative hypothesis, which take the following forms:
<b>H<sub>0</sub></b> (Null Hypothesis): Population parameter =,  ≤, ≥ some value
<b>H<sub>A</sub></b> (Alternative Hypothesis): Population parameter &lt;, >, ≠ some value
<em>Note that the <b>null hypothesis always contains the equal sign</b>.</em>
We interpret the hypotheses as follows:
<b>Null hypothesis:</b> The sample data provides no evidence to support some claim being made by an individual.
<b>Alternative hypothesis:</b> The sample data <em>does</em> provide sufficient evidence to support the claim being made by an individual.
For example, suppose it’s assumed that the average height of a certain species of plant is 20 inches tall. However, one botanist claims the true average height is greater than 20 inches.
To test this claim, she may go out and collect a  random sample  of plants. She can then use this sample data to perform a hypothesis test using the following two hypotheses:
<b>H<sub>0</sub></b>: μ ≤ 20 (the true mean height of plants is equal to or even less than 20 inches)
<b>H<sub>A</sub></b>: μ > 20 (the true mean height of plants is greater than 20 inches)
If the sample data gathered by the botanist shows that the mean height of this species of plants is significantly greater than 20 inches, she can reject the null hypothesis and conclude that the mean height is greater than 20 inches.
Read through the following examples to gain a better understanding of how to write a null hypothesis in different situations.
<h3>Example 1: Weight of Turtles</h3>
A biologist wants to test whether or not the true mean weight of a certain species of turtles is 300 pounds. To test this, he goes out and measures the weight of a random sample of 40 turtles.
Here is how to write the null and alternative hypotheses for this scenario:
<b>H<sub>0</sub></b>: μ = 300 (the true mean weight is equal to 300 pounds)
<b>H<sub>A</sub></b>: μ ≠ 300 (the true mean weight is not equal to 300 pounds)
<h3>Example 2: Height of Males</h3>
It’s assumed that the mean height of males in a certain city is 68 inches. However, an independent researcher believes the true mean height is greater than 68 inches. To test this, he goes out and collects the height of 50 males in the city.
Here is how to write the null and alternative hypotheses for this scenario:
<b>H<sub>0</sub></b>: μ ≤ 68 (the true mean height is equal to or even less than 68 inches)
<b>H<sub>A</sub></b>: μ > 68 (the true mean height is greater than 68 inches)
<h3>Example 3: Graduation Rates</h3>
A university states that 80% of all students graduate on time. However, an independent researcher believes that less than 80% of all students graduate on time. To test this, she collects data on the proportion of students who graduated on time last year at the university.
Here is how to write the null and alternative hypotheses for this scenario:
<b>H<sub>0</sub></b>: p ≥ 0.80 (the true proportion of students who graduate on time is 80% or higher)
<b>H<sub>A</sub></b>: μ &lt; 0.80 (the true proportion of students who graduate on time is less than 80%)
<h3>Example 4: Burger Weights</h3>
A food researcher wants to test whether or not the true mean weight of a burger at a certain restaurant is 7 ounces. To test this, he goes out and measures the weight of a random sample of 20 burgers from this restaurant.
Here is how to write the null and alternative hypotheses for this scenario:
<b>H<sub>0</sub></b>: μ = 7 (the true mean weight is equal to 7 ounces)
<b>H<sub>A</sub></b>: μ ≠ 7 (the true mean weight is not equal to 7 ounces)
<h3>Example 5: Citizen Support</h3>
A politician claims that less than 30% of citizens in a certain town support a certain law. To test this, he goes out and surveys 200 citizens on whether or not they support the law.
Here is how to write the null and alternative hypotheses for this scenario:
<b>H<sub>0</sub></b>: p ≥ .30 (the true proportion of citizens who support the law is greater than or equal to 30%)
<b>H<sub>A</sub></b>: μ &lt; 0.30 (the true proportion of citizens who support the law is less than 30%)
<h2><span class="orange">How to Write a Confidence Interval Conclusion (Step-by-Step)</span></h2>
A  confidence interval  is a range of values that is likely to contain a  population parameter  with a certain level of confidence. It is written as:
<b>Confidence Interval </b> = [lower bound, upper bound]
We can use the following sentence structure to write a conclusion about a confidence interval:
We are <b>[% level of confidence]</b> confident that <b>[population parameter]</b> is between <b>[lower bound, upper bound]</b>.
The following examples show how to write confidence interval conclusions for different statistical tests.
<h3>Example 1: Confidence Interval Conclusion for a Mean</h3>
Suppose a biologist wants to estimate the mean weight of dolphins in a population. She collects data for a  simple random sample  of 50 different dolphins and constructs the following 95% confidence interval:
95% confidence interval = [480.5, 502.5]
Here’s how to write a conclusion for this confidence interval:
The biologist is 95% confident that the mean weight of dolphins in this population is between 480.5 pounds and 502.5 pounds.
<h3>Example 2: Confidence Interval Conclusion for a Difference in Means</h3>
Suppose a zoologist wants to estimate the difference in mean weights between two different species of turtles. He collects data for a simple random sample of 25 of each species and constructs the following 90% confidence interval:
90% confidence interval = [3.44, 12.33]
Here’s how to write a conclusion for this confidence interval:
The zoologist is 90% confident that the difference in mean weight between these two species of turtles is between 3.44 pounds and 12.33 pounds.
<h3>Example 3: Confidence Interval Conclusion for a Proportion</h3>
Suppose a politician wants to estimate the proportion of citizens in his city who support a certain law. He sends out a survey to 200 citizens and constructs the following 99% confidence interval for the proportion of citizens who support the law:
99% confidence interval = [0.25, 0.35]
Here’s how to write a conclusion for this confidence interval:
The politician is 99% confident that the proportion of citizens in the entire city who support a certain law is between 0.25 and 0.35.
<h3>Example 4: Confidence Interval Conclusion for a Difference in Proportions</h3>
Suppose a researcher wants to estimate the difference in the proportion of citizens between city A and city B who support a certain law. He sends out a survey to 500 citizens in each city and constructs the following 95% confidence interval for the difference in proportions of citizens who support the law:
95% confidence interval = [0.02, 0.08]
Here’s how to write a conclusion for this confidence interval:
The researcher is 95% confident that the difference in the proportion of citizens who support a certain law between city A and city B is between 0.02 and 0.08.
<h2><span class="orange">Hypergeometric Distribution Calculator</span></h2>
This calculator finds probabilities associated with the hypergeometric distribution based on user provided input.
<label>Population size</label>
<input type="number" id="pop" value="25">
<label># Successes in population</label>
<input type="number" id="popx" value="13">
<label>Sample size</label>
<input type="number" id="sample" value="12">
<label># Successes in sample (x)</label>
<input type="number" id="samplex" value="4">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
P(X = 4): 0.06806
P(X &lt; 4): 0.01312
P(X ≤ 4): 0.08118
P(X > 4): 0.91882
P(X ≥ 4): 0.98688
<script>
function calc() {
//get input values
var pop = +document.getElementById('pop').value;
var popx = +document.getElementById('popx').value;
var sample = +document.getElementById('sample').value;
var x = +document.getElementById('samplex').value;
//calculate SE
var exactP = jStat.hypgeom.pdf(x, pop, popx, sample);
var lessP = jStat.hypgeom.cdf(x-1, pop, popx, sample);
var lessEP = jStat.hypgeom.cdf(x, pop, popx, sample);
var greatP = 1-jStat.hypgeom.cdf(x, pop, popx, sample);
var greatEP = 1-jStat.hypgeom.cdf(x-1, pop, popx, sample);
//output probabilities
document.getElementById('exactP').innerHTML = exactP.toFixed(5);
document.getElementById('lessP').innerHTML = lessP.toFixed(5);
document.getElementById('lessEP').innerHTML = lessEP.toFixed(5);
document.getElementById('greatP').innerHTML = greatP.toFixed(5);
document.getElementById('greatEP').innerHTML = greatEP.toFixed(5);
document.getElementById('x1').innerHTML = x;
document.getElementById('x2').innerHTML = x;
document.getElementById('x3').innerHTML = x;
document.getElementById('x4').innerHTML = x;
document.getElementById('x5').innerHTML = x;
}
</script>
<h2><span class="orange">How to Use the Hypergeometric Distribution in Excel</span></h2>
The  hypergeometric distribution  describes the probability of choosing <em>k </em>objects with a certain feature in <em>n </em>draws without replacement, from a finite population of size <em>N </em>that contains <em>K </em>objects with that feature.
If a  random variable  <em>X</em> follows a hypergeometric distribution, then the probability of choosing <em>k </em>objects with a certain feature can be found by the following formula:
<b>P(X=k) = <sub>K</sub>C<sub>k</sub> (<sub>N-K</sub>C<sub>n-k</sub>) / <sub>N</sub>C<sub>n</sub></b>
where:
<b>N: </b>population size
<b>K: </b>number of objects in population with a certain feature
<b>n: </b>sample size
<b>k: </b>number of objects in sample with a certain feature
<b><sub>K</sub>C<sub>k</sub>: </b>number of combinations of K things taken k at a time
To calculate probabilities related to the hypergeometric distribution in Excel, we can use the following formula:
<b>=HYPGEOM.DIST(sample_s, number_sample, population_s, number_pop, cumulative)
</b>
where:
<b>sample_s</b>: number of successes in sample
<b>number_sample</b>: size of sample
<b>population_s</b>: number of successes in population
<b>number_pop</b>: population size
<b>cumulative</b>: Whether to calculate the cumulative distribution function
The following examples show how to use this formula in practice.
<h3>Example 1: Picking Cards from a Deck</h3>
There are 4 Queens in a standard deck of 52 cards. Suppose we randomly pick a card from a deck, then, without replacement, randomly pick another card from the deck. What is the probability that both cards are Queens?
We can use the following formula in Excel to calculate the probability that both cards are Queens:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/hypgeomExcel1.png">
The probability that both cards are Queens is .<b>00452</b>.
<h3>Example 2: Picking Balls from an Urn</h3>
An urn contains 3 red balls and 5 green balls. You randomly choose 4 balls. What is the probability that you choose exactly 2 red balls?
We can use the following formula in Excel to find this probability:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/hypgeomExcel2.png">
The probability that you choose exactly 2 red balls is .<b>428571</b>.
<h3>Example 3: Choosing Marbles from a Basket</h3>
A basket contains 7 purple marbles and 3 pink marbles. You randomly choose 6 marbles. What is the probability that you choose exactly 3 pink marbles?
We can use the following formula in Excel to find this probability:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/hypgeomExcel3.png">
The probability that you choose exactly 3 pink marbles is .<b>16667</b>.
<h2><span class="orange">An Introduction to the Hypergeometric Distribution</span></h2>
The <b>hypergeometric distribution</b> describes the probability of choosing <em>k </em>objects with a certain feature in <em>n </em>draws without replacement, from a finite population of size <em>N </em>that contains <em>K </em>objects with that feature.
If a  random variable  <em>X</em> follows a hypergeometric distribution, then the probability of choosing <em>k </em>objects with a certain feature can be found by the following formula:
<b>P(X=k) = <sub>K</sub>C<sub>k</sub> (<sub>N-K</sub>C<sub>n-k</sub>) / <sub>N</sub>C<sub>n</sub></b>
where:
<b>N: </b>population size
<b>K: </b>number of objects in population with a certain feature
<b>n: </b>sample size
<b>k: </b>number of objects in sample with a certain feature
<b><sub>K</sub>C<sub>k</sub>: </b>number of combinations of K things taken k at a time
For example, there are 4 Queens in a standard deck of 52 cards. Suppose we randomly pick a card from a deck, then, without replacement, randomly pick another card from the deck. What is the probability that both cards are Queens? 
To answer this, we can use the hypergeometric distribution with the following parameters:
<b>N: </b>population size = 52 cards
<b>K: </b>number of objects in population with a certain feature = 4 queens
<b>n: </b>sample size = 2 draws
<b>k: </b>number of objects in sample with a certain feature = 2 queens
Plugging these numbers in the formula, we find the probability to be:
<b>P(X=2) </b>= <sub>K</sub>C<sub>k</sub> (<sub>N-K</sub>C<sub>n-k</sub>) / <sub>N</sub>C<sub>n</sub> = <sub>4</sub>C<sub>2</sub> (<sub>52-4</sub>C<sub>2-2</sub>) / <sub>52</sub>C<sub>2</sub> = 6*1/ 1326 = <b>0.00452</b>.
This should make sense intuitively. If you imagine yourself pulling two cards out of a deck, one after the other, the probability that <em>both </em>cards are Queens should be very low.
<h3>Properties of the Hypergeometric Distribution</h3>
The hypergeometric distribution has the following properties:
The mean of the distribution is<b> (nK) / N</b>
The variance of the distribution is <b>(nK)(N-K)(N-n) / (N<sup>2</sup>(n-1))</b>
<h3>Hypergeometric Distribution Practice Problems</h3>
Use the following practice problems to test your knowledge of the hypergeometric distribution.
<em><b>Note: </b>We will use the  Hypergeometric Distribution Calculator  to calculate the answers to these questions.</em>
<b>Problem 1</b>
<b>Question: </b>Suppose we randomly pick four cards from a deck without replacement. What is the probability that two of the cards are Queens? 
To answer this, we can use the hypergeometric distribution with the following parameters:
<b>N: </b>population size = 52 cards
<b>K: </b>number of objects in population with a certain feature = 4 queens
<b>n: </b>sample size = 4 draws
<b>k: </b>number of objects in sample with a certain feature = 2 queens
Plugging these numbers into the Hypergeometric Distribution Calculator, we find the probability to be <b>0.025</b>.
<b>Problem 2</b>
<b>Question: </b>An urn contains 3 red balls and 5 green balls. You randomly choose 4 balls. What is the probability that you choose exactly 2 red balls?
To answer this, we can use the hypergeometric distribution with the following parameters:
<b>N: </b>population size = 8 balls
<b>K: </b>number of objects in population with a certain feature = 3 red balls
<b>n: </b>sample size = 4 draws
<b>k: </b>number of objects in sample with a certain feature = 2 red balls
Plugging these numbers into the Hypergeometric Distribution Calculator, we find the probability to be <b>0.42857</b>.
<b>Problem 3</b>
<b>Question: </b>A basket contains 7 purple marbles and 3 pink marbles. You randomly choose 6 marbles. What is the probability that you choose exactly 3 pink marbles?
To answer this, we can use the hypergeometric distribution with the following parameters:
<b>N: </b>population size = 10 marbles
<b>K: </b>number of objects in population with a certain feature = 3 pink balls
<b>n: </b>sample size = 6 draws
<b>k: </b>number of objects in sample with a certain feature = 3 pink balls
Plugging these numbers into the Hypergeometric Distribution Calculator, we find the probability to be <b>0.16667</b>.
<h2><span class="orange">How to Write Hypothesis Test Conclusions (With Examples)</span></h2>
A  hypothesis test  is used to test whether or not some hypothesis about a  population parameter  is true.
To perform a hypothesis test in the real world, researchers obtain a  random sample  from the population and perform a hypothesis test on the sample data, using a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the  p-value  of the hypothesis test is less than some significance level (e.g. α = .05), then <b>we reject the null hypothesis</b>.
Otherwise, if the p-value is not less than some significance level then <b>we fail to reject the null hypothesis</b>.
When writing the conclusion of a hypothesis test, we typically include:
Whether we reject or fail to reject the null hypothesis.
The significance level.
A short explanation in the context of the hypothesis test.
For example, we would write:
We <b>reject the null hypothesis</b> at the 5% significance level.
 
There is sufficient evidence to support the claim that…
Or, we would write:
We <b>fail to reject the null hypothesis</b> at the 5% significance level.
 
There is not sufficient evidence to support the claim that…
The following examples show how to write a hypothesis test conclusion in both scenarios.
<h3>Example 1: Reject the Null Hypothesis Conclusion</h3>
Suppose a biologist believes that a certain fertilizer will cause plants to grow more during a one-month period than they normally do, which is currently 20 inches. To test this, she applies the fertilizer to each of the plants in her laboratory for one month.
She then performs a hypothesis test at a 5% significance level using the following hypotheses:
<b>H<sub>0</sub>:</b> μ = 20 inches (the fertilizer will have no effect on the mean plant growth)
<b>H<sub>A</sub>:</b> μ > 20 inches (the fertilizer will cause mean plant growth to increase)
Suppose the p-value of the test turns out to be 0.002.
Here is how she would report the results of the hypothesis test:
We <b>reject the null hypothesis</b> at the 5% significance level.
 
There is sufficient evidence to support the claim that this particular fertilizer causes plants to grow more during a one-month period than they normally do.
<h3>Example 2: Fail to Reject the Null Hypothesis Conclusion</h3>
Suppose the manager of a manufacturing plant wants to test whether or not some new method changes the number of defective widgets produced per month, which is currently 250. To test this, he measures the mean number of defective widgets produced before and after using the new method for one month.
He performs a hypothesis test at a 10% significance level using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean number of defective widgets is the same before and after using the new method)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> ≠ μ<sub>before</sub> (the mean number of defective widgets produced is different before and after using the new method)
Suppose the p-value of the test turns out to be 0.27.
Here is how he would report the results of the hypothesis test:
We <b>fail to reject the null hypothesis</b> at the 10% significance level.
 
There is not sufficient evidence to support the claim that the new method leads to a change in the number of defective widgets produced per month.
<h2><span class="orange">How to Perform Hypothesis Testing in Python (With Examples)</span></h2>
A  hypothesis test  is a formal statistical test we use to reject or fail to reject some statistical hypothesis.
This tutorial explains how to perform the following hypothesis tests in Python:
One sample t-test
Two sample t-test
Paired samples t-test
Let’s jump in!
<h2>Example 1: One Sample t-test in Python</h2>
A  one sample t-test  is used to test whether or not the mean of a population is equal to some value.
For example, suppose we want to know whether or not the mean weight of a certain species of some turtle is equal to 310 pounds.
To test this, we go out and collect a simple random sample of turtles with the following weights:
<b>Weights</b>: 300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303
The following code shows how to use the  ttest_1samp()  function from the<b> scipy.stats</b> library to perform a one sample t-test:
<b>import scipy.stats as stats
#define data
data = [300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303]
#perform one sample t-test
stats.ttest_1samp(a=data, popmean=310)
Ttest_1sampResult(statistic=-1.5848116313861254, pvalue=0.1389944275158753)
</b>
The t test statistic is <b>-1.5848 </b>and the corresponding two-sided p-value is <b>0.1389</b>.
The two hypotheses for this particular one sample t-test are as follows:
<b>H<sub>0</sub>: </b>μ = 310 (the mean weight for this species of turtle is 310 pounds)
H<sub>A</sub>: </b>μ ≠310 (the mean weight is <em style="color: #000000;">not </em>310 pounds)
Because the p-value of our test<b> (0.1389) </b>is greater than alpha = 0.05, we fail to reject the null hypothesis of the test.
We do not have sufficient evidence to say that the mean weight for this particular species of turtle is different from 310 pounds.
<h2>Example 2: Two Sample t-test in Python</h2>
A  two sample t-test  is used to test whether or not the means of two populations are equal.
For example, suppose we want to know whether or not the mean weight between two different species of turtles is equal.
To test this, we collect a simple random sample of turtles from each species with the following weights:
<b>Sample 1</b>: 300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303
<b>Sample 2</b>: 335, 329, 322, 321, 324, 319, 304, 308, 305, 311, 307, 300, 305
The following code shows how to use the  ttest_ind()  function from the<b> scipy.stats</b> library to perform this two sample t-test:
<b>import scipy.stats as stats 
#define array of turtle weights for each sample
sample1 = [300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303]
sample2 = [335, 329, 322, 321, 324, 319, 304, 308, 305, 311, 307, 300, 305]
#perform two sample t-test
stats.ttest_ind(a=sample1, b=sample2) 
Ttest_indResult(statistic=-2.1009029257555696, pvalue=0.04633501389516516) </b>
The t test statistic is –<b>2.1009</b> and the corresponding two-sided p-value is <b>0.0463</b>.
The two hypotheses for this particular two sample t-test are as follows:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2 </sub>(the mean weight between the two species is equal)
H<sub>A</sub>: </b>μ<sub style="color: #000000;">1</sub> ≠ μ<sub style="color: #000000;">2 </sub>(the mean weight between the two species is not equal)
Since the p-value of the test (0.0463) is less than .05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean weight between the two species is not equal.
<h2>Example 3: Paired Samples t-test in Python</h2>
A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
For example, suppose we want to know whether or not a certain training program is able to increase the max vertical jump (in inches) of basketball players.
To test this, we may recruit a simple random sample of 12 college basketball players and measure each of their max vertical jumps. Then, we may have each player use the training program for one month and then measure their max vertical jump again at the end of the month.
The following data shows the max jump height (in inches) before and after using the training program for each player:
<b>Before</b>: 22, 24, 20, 19, 19, 20, 22, 25, 24, 23, 22, 21
<b>After</b>: 23, 25, 20, 24, 18, 22, 23, 28, 24, 25, 24, 20
The following code shows how to use the  ttest_rel() function  from the <b>scipy.stats</b> library to perform this paired samples t-test:
<b>import scipy.stats as stats  
#define before and after max jump heights
before = [22, 24, 20, 19, 19, 20, 22, 25, 24, 23, 22, 21]
after = [23, 25, 20, 24, 18, 22, 23, 28, 24, 25, 24, 20]
#perform paired samples t-test
stats.ttest_rel(a=before, b=after)
Ttest_relResult(statistic=-2.5289026942943655, pvalue=0.02802807458682508)
</b>
The t test statistic is –<b>2.5289</b> and the corresponding two-sided p-value is <b>0.0280</b>.
The two hypotheses for this particular paired samples t-test are as follows:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2 </sub>(the mean jump height before and after using the program is equal)
H<sub>A</sub>: </b>μ<sub style="color: #000000;">1</sub> ≠ μ<sub style="color: #000000;">2 </sub>(the mean jump height before and after using the program is not equal)
Since the p-value of the test (0.0280) is less than .05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean jump height before and after using the training program is not equal.
<h2><span class="orange">Hypothesis Test vs. Confidence Interval: What’s the Difference?</span></h2>
Two of the most commonly used procedures in statistics are  hypothesis tests  and  confidence intervals . 
Here’s the difference between the two:
A <b>hypothesis test</b> is a formal statistical test that is used to determine if some hypothesis about a population parameter is true.
A <b>confidence interval </b>is a range of values that is likely to contain a population parameter with a certain level of confidence.
This tutorial shares a brief overview of each method along with their similarities and differences.
<h2>The Basics of Hypothesis Tests</h2>
A hypothesis test is used to test whether or not some hypothesis about a  population parameter  is true.
To perform a hypothesis test in the real world, researchers will obtain a  random sample  from the population and perform a hypothesis test on the sample data, using a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the  p-value  of the hypothesis test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that the alternative hypothesis is true.
<h3>Hypothesis Test Example</h3>
 Suppose a manufacturing facility wants to test whether or not some new method changes the number of defective widgets produced per month, which is currently 250.
To test this, they may measure the mean number of defective widgets produced before and after using the new method for one month.
They can perform a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean number of defective widgets is the same before and after using the new method)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> ≠ μ<sub>before</sub> (the mean number of defective widgets produced is different before and after using the new method)
Suppose they perform a  one sample t-test  and end up with a p-value of .0032.
Since this p-value is less than α = .05, the facility can reject the null hypothesis and conclude that the new method leads to a change in the number of defective widgets produced per month.
<h2>The Basics of Confidence Intervals</h2>
A confidence interval is a range of values that is likely to contain a population parameter with a certain level of confidence.
To calculate a confidence interval in the real world, researchers will obtain a  random sample  from the population and use the following formula to calculate a confidence interval for the population mean:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the chosen z-value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
<h3>Confidence Interval Example</h3>
Suppose a biologist wants to estimate the mean weight of turtles in a certain population so she collects a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
Here is how to find calculate the 90% confidence interval for the true population mean weight:
<b>90% Confidence Interval: </b>300 +/-  1.645*(18.5/√25) = <b>[293.91, 306.09]</b>
The biologist can be 90% confident that the true mean weight of a turtle in this population is between 293.1 pounds and 306.09 pounds.
<h2>Hypothesis Test vs. Confidence Interval: When to Use Each</h2>
The decision to use a hypothesis test or a confidence interval depends on the question you’re attempting to answer.
You should use a<b> confidence interval</b> when you want to estimate the value of a population parameter.
You should use a <b>hypothesis test</b> when you want to determine if some hypothesis about a population parameter is likely true or not.
To test your knowledge of when to use each procedure, consider the following scenarios.
<h3>Scenario 1: Hours Spent Studying</h3>
Suppose an academic researcher wants to measure the mean number of hours that college students spend studying per week.
Which procedure should she use to answer this question?
She should use a <b>confidence interval </b>because she’s interested in estimating the value of a population parameter.
<h3>Scenario 2: New Medication</h3>
Suppose a doctor wants to test whether or not a new medication is able to reduce blood pressure more than the current standard medication.
Which procedure should he use to answer this question?
He should use a <b>hypothesis test</b> because he’s interested in understanding whether or not a specific assumption about a population parameter is true.
<h2><span class="orange">The Complete Guide: Hypothesis Testing in Excel</span></h2>
In statistics, a  hypothesis test  is used to test some assumption about a  population parameter .
There are many different types of hypothesis tests you can perform depending on the type of data you’re working with and the goal of your analysis.
This tutorial explains how to perform the following types of hypothesis tests in Excel:
One sample t-test
Two sample t-test
Paired samples t-test
One proportion z-test
Two proportion z-test
Let’s jump in!
<h3>Example 1: One Sample t-test in Excel</h3>
A <b>one sample t-test</b> is used to test whether or not the mean of a population is equal to some value.
For example, suppose a botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. 
To test this, she collects a  random sample  of 12 plants and records each of their heights in inches.
She would write the hypotheses for this particular one sample t-test as follows:
<b>H<sub>0</sub>: </b>μ = 15
H<sub>A</sub>: </b>μ ≠15
Refer to  this tutorial  for a step-by-step explanation of how to perform this hypothesis test in Excel.
<h3>Example 2: Two Sample t-test in Excel</h3>
A <b>two sample t-test</b> is used to test whether or not the means of two populations are equal.
For example, suppose researchers want to know whether or not two different species of plants have the same mean height.
To test this, they collect a random sample of 20 plants from each species and measure their heights.
The researchers would write the hypotheses for this particular two sample t-test as follows:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2</sub>
H<sub>A</sub>: </b>μ<sub>1</sub> ≠ μ<sub>2</sub>
Refer to  this tutorial  for a step-by-step explanation of how to perform this hypothesis test in Excel.
<h3>Example 3: Paired Samples t-test in Excel</h3>
A <b>paired samples t-test</b> is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
For example, suppose we want to know whether a certain study program significantly impacts student performance on a particular exam.
To test this, we have 20 students in a class take a pre-test. Then, we have each of the students participate in the study program for two weeks. Then, the students retake a post-test of similar difficulty.
We would write the hypotheses for this particular two sample t-test as follows:
<b>H<sub>0</sub>: </b>μ<sub>pre</sub> = μ<sub>post</sub>
H<sub>A</sub>: </b>μ<sub>pre</sub> ≠ μ<sub>post</sub>
Refer to  this tutorial  for a step-by-step explanation of how to perform this hypothesis test in Excel.
<h3>Example 4: One Proportion z-test in Excel</h3>
A <b>one proportion z-test </b>is used to compare an observed proportion to a theoretical one.
For example, suppose a phone company claims that 90% of its customers are satisfied with their service.
To test this claim, an independent researcher gathered a simple random sample of 200 customers and asked them if they are satisfied with their service.
We would write the hypotheses for this particular two sample t-test as follows:
<b>H<sub>0</sub>: </b>p = 0.90
H<sub>A</sub>: </b>p ≠ 0.90
Refer to  this tutorial  for a step-by-step explanation of how to perform this hypothesis test in Excel.
<h3>Example 5: Two Proportion z-test in Excel</h3>
A <b>two proportion z-test </b>is used to test for a difference between two population proportions.
For example, suppose a superintendent of a school district claims that the percentage of students who prefer chocolate milk over regular milk in school cafeterias is the same for school 1 and school 2.
To test this claim, an independent researcher obtains a simple random sample of 100 students from each school and surveys them about their preferences. 
We would write the hypotheses for this particular two sample t-test as follows:
<b>H<sub>0</sub>: </b>p<sub>1</sub>= p<sub>2</sub>
H<sub>A</sub>:</b> p<sub>1</sub> ≠ p<sub>2</sub>
Refer to  this tutorial  for a step-by-step explanation of how to perform this hypothesis test in Excel.
<h2><span class="orange">The Complete Guide: Hypothesis Testing in R</span></h2>
A  hypothesis test  is a formal statistical test we use to reject or fail to reject some statistical hypothesis.
This tutorial explains how to perform the following hypothesis tests in R:
One sample t-test
Two sample t-test
Paired samples t-test
We can use the <b>t.test()</b> function in R to perform each type of test:
<b>#one sample t-test
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, …)</b>
where:
<b>x, y:</b> The two samples of data.
<b>alternative:</b> The alternative hypothesis of the test.
<b>mu:</b> The true value of the mean.
<b>paired:</b> Whether to perform a paired t-test or not.
<b>var.equal:</b> Whether to assume the  variances are equal  between the samples.
<b>conf.level:</b> The  confidence level  to use.
The following examples show how to use this function in practice.
<h3>Example 1: One Sample t-test in R</h3>
A  one sample t-test  is used to test whether or not the mean of a population is equal to some value.
For example, suppose we want to know whether or not the mean weight of a certain species of some turtle is equal to 310 pounds. We go out and collect a simple random sample of turtles with the following weights:
<b>Weights</b>: 300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303
The following code shows how to perform this one sample t-test in R:
<b>#define vector of turtle weights
turtle_weights &lt;- c(300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303)
#perform one sample t-test
t.test(x = turtle_weights, mu = 310)
One Sample t-test
data:  turtle_weights
t = -1.5848, df = 12, p-value = 0.139
alternative hypothesis: true mean is not equal to 310
95 percent confidence interval:
 303.4236 311.0379
sample estimates:
mean of x 
 307.2308 </b>
From the output we can see:
t-test statistic: <b>-1.5848</b>
degrees of freedom: <b>12</b>
p-value: <b>0.139</b>
95% confidence interval for true mean: <b>[303.4236, 311.0379]</b>
mean of turtle weights: <b>307.230</b>
Since the p-value of the test (0.139) is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the mean weight of this species of turtle is different from 310 pounds.
<h3>Example 2: Two Sample t-test in R</h3>
A  two sample t-test  is used to test whether or not the means of two populations are equal.
For example, suppose we want to know whether or not the mean weight between two different species of turtles is equal. To test this, we collect a simple random sample of turtles from each species with the following weights:
<b>Sample 1</b>: 300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303
<b>Sample 2</b>: 335, 329, 322, 321, 324, 319, 304, 308, 305, 311, 307, 300, 305
The following code shows how to perform this two sample t-test in R:
<b>#define vector of turtle weights for each sample
sample1 &lt;- c(300, 315, 320, 311, 314, 309, 300, 308, 305, 303, 305, 301, 303)
sample2 &lt;- c(335, 329, 322, 321, 324, 319, 304, 308, 305, 311, 307, 300, 305)
#perform two sample t-test
t.test(x = sample1, y = sample2)
Welch Two Sample t-test
data:  sample1 and sample2
t = -2.1009, df = 19.112, p-value = 0.04914
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -14.73862953  -0.03060124
sample estimates:
mean of x mean of y 
 307.2308  314.6154 </b>
From the output we can see:
t-test statistic: <b>-2.1009</b>
degrees of freedom: <b>19.112</b>
p-value: <b>0.04914</b>
95% confidence interval for true mean difference: <b>[-14.74, -0.03]</b>
mean of sample 1 weights: <b>307.2308</b>
mean of sample 2 weights: <b>314.6154</b>
Since the p-value of the test (0.04914) is less than .05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean weight between the two species is not equal.
<h3>Example 3: Paired Samples t-test in R</h3>
A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
For example, suppose we want to know whether or not a certain training program is able to increase the max vertical jump (in inches) of basketball players.
To test this, we may recruit a simple random sample of 12 college basketball players and measure each of their max vertical jumps. Then, we may have each player use the training program for one month and then measure their max vertical jump again at the end of the month.
The following data shows the max jump height (in inches) before and after using the training program for each player:
<b>Before</b>: 22, 24, 20, 19, 19, 20, 22, 25, 24, 23, 22, 21
<b>After</b>: 23, 25, 20, 24, 18, 22, 23, 28, 24, 25, 24, 20
The following code shows how to perform this paired samples t-test in R:
<b>#define before and after max jump heights
before &lt;- c(22, 24, 20, 19, 19, 20, 22, 25, 24, 23, 22, 21)
after &lt;- c(23, 25, 20, 24, 18, 22, 23, 28, 24, 25, 24, 20)
#perform paired samples t-test
t.test(x = before, y = after, paired = TRUE)
Paired t-test
data:  before and after
t = -2.5289, df = 11, p-value = 0.02803
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -2.3379151 -0.1620849
sample estimates:
mean of the differences   -1.25</b>
From the output we can see:
t-test statistic: <b>-2.5289</b>
degrees of freedom: <b>11</b>
p-value: <b>0.02803</b>
95% confidence interval for true mean difference: <b>[-2.34, -0.16]</b>
mean difference between before and after: <b>-1.25</b>
Since the p-value of the test (0.02803) is less than .05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean jump height before and after using the training program is not equal.
<h2><span class="orange">4 Examples of Hypothesis Testing in Real Life</span></h2>
In statistics,  hypothesis tests  are used to test whether or not some hypothesis about a  population parameter  is true.
To perform a hypothesis test in the real world, researchers will obtain a  random sample  from the population and perform a hypothesis test on the sample data, using a null and alternative hypothesis:
<b>Null Hypothesis (H<sub>0</sub>):</b> The sample data occurs purely from chance.
<b>Alternative Hypothesis (H<sub>A</sub>):</b> The sample data is influenced by some non-random cause.
If the  p-value  of the hypothesis test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that we have sufficient evidence to say that the alternative hypothesis is true.
The following examples provide several situations where hypothesis tests are used in the real world.
<h3>Example 1: Biology</h3>
Hypothesis tests are often used in biology to determine whether some new treatment, fertilizer, pesticide, chemical, etc. causes increased growth, stamina, immunity, etc. in plants or animals.
For example, suppose a biologist believes that a certain fertilizer will cause plants to grow more during a one-month period than they normally do, which is currently 20 inches. To test this, she applies the fertilizer to each of the plants in her laboratory for one month.
She then performs a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ = 20 inches (the fertilizer will have no effect on the mean plant growth)
<b>H<sub>A</sub>:</b> μ > 20 inches (the fertilizer will cause mean plant growth to increase)
If the p-value of the test is less than some significance level (e.g. α = .05), then she can reject the null hypothesis and conclude that the fertilizer leads to increased plant growth.
<h3>Example 2: Clinical Trials</h3>
Hypothesis tests are often used in clinical trials to determine whether some new treatment, drug, procedure, etc. causes improved outcomes in patients.
For example, suppose a doctor believes that a new drug is able to reduce blood pressure in obese patients. To test this, he may measure the blood pressure of 40 patients before and after using the new drug for one month.
He then performs a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean blood pressure is the same before and after using the drug)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> &lt; μ<sub>before</sub> (the mean blood pressure is less after using the drug)
If the p-value of the test is less than some significance level (e.g. α = .05), then he can reject the null hypothesis and conclude that the new drug leads to reduced blood pressure.
<h3>Example 3: Advertising Spend</h3>
Hypothesis tests are often used in business to determine whether or not some new advertising campaign, marketing technique, etc. causes increased sales.
For example, suppose a company believes that spending more money on digital advertising leads to increased sales. To test this, the company may increase money spent on digital advertising during a two-month period and collect data to see if overall sales have increased.
They may perform a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean sales is the same before and after spending more on advertising)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> > μ<sub>before</sub> (the mean sales increased after spending more on advertising)
If the p-value of the test is less than some significance level (e.g. α = .05), then the company can reject the null hypothesis and conclude that increased digital advertising leads to increased sales.
<h3>Example 4: Manufacturing</h3>
Hypothesis tests are also used often in manufacturing plants to determine if some new process, technique, method, etc. causes a change in the number of defective products produced.
For example, suppose a certain manufacturing plant wants to test whether or not some new method changes the number of defective widgets produced per month, which is currently 250. To test this, they may measure the mean number of defective widgets produced before and after using the new method for one month.
They can then perform a hypothesis test using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean number of defective widgets is the same before and after using the new method)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> ≠ μ<sub>before</sub> (the mean number of defective widgets produced is different before and after using the new method)
If the p-value of the test is less than some significance level (e.g. α = .05), then the plant can reject the null hypothesis and conclude that the new method leads to a change in the number of defective widgets produced per month.
<h2><span class="orange">Introduction to Hypothesis Testing</span></h2>
A <b>statistical hypothesis</b> is an assumption about a  population parameter .
For example, we may assume that the mean height of a male in the U.S. is 70 inches.
The assumption about the height is the <b>statistical hypothesis</b> and the true mean height of a male in the U.S. is the <b>population parameter</b>.
A <b>hypothesis test</b> is a formal statistical test we use to reject or fail to reject a statistical hypothesis.
<h2>The Two Types of Statistical Hypotheses</h2>
To test whether a statistical hypothesis about a population parameter is true, we obtain a  random sample  from the population and perform a hypothesis test on the sample data.
There are two types of statistical hypotheses:
The <b>null hypothesis</b>, denoted as H<sub>0</sub>, is the hypothesis that the sample data occurs purely from chance.
The <b>alternative hypothesis</b>, denoted as H<sub>1</sub> or H<sub>a</sub>, is the hypothesis that the sample data is influenced by some non-random cause.
<h2>Hypothesis Tests</h2>
A <b>hypothesis test</b> consists of five steps:
<b>1. State the hypotheses. </b>
State the null and alternative hypotheses. These two hypotheses need to be mutually exclusive, so if one is true then the other must be false.
<b>2. Determine a significance level to use for the hypothesis.</b>
Decide on a significance level. Common choices are .01, .05, and .1. 
<b>3. Find the test statistic.</b>
Find the test statistic and the corresponding p-value. Often we are analyzing a population mean or proportion and the general formula to find the test statistic is: (sample statistic – population parameter) / (standard deviation of statistic)
<b>4. Reject or fail to reject the null hypothesis.</b>
Using the test statistic or the p-value, determine if you can reject or fail to reject the null hypothesis based on the significance level.
The <em>p-value</em> tells us the strength of evidence in support of a null hypothesis. If the p-value is less than the significance level, we reject the null hypothesis.
<b>5. Interpret the results. </b>
Interpret the results of the hypothesis test in the context of the question being asked. 
<h2>The Two Types of Decision Errors</h2>
There are two types of decision errors that one can make when doing a hypothesis test:
<b>Type I error:</b> You reject the null hypothesis when it is actually true. The probability of committing a Type I error is equal to the significance level, often called <em>alpha</em>, and denoted as α.
<b>Type II error:</b> You fail to reject the null hypothesis when it is actually false. The probability of committing a Type II error is called the Power of the test or  <em>Beta</em> , denoted as β.
<h2>One-Tailed and Two-Tailed Tests</h2>
A statistical hypothesis can be one-tailed or two-tailed.
A <b>one-tailed hypothesis</b> involves making a “greater than” or “less than ” statement.
For example, suppose we assume the mean height of a male in the U.S. is greater than or equal to 70 inches. The null hypothesis would be H0: μ ≥ 70 inches and the alternative hypothesis would be Ha: μ &lt; 70 inches.
A <b>two-tailed hypothesis</b> involves making an “equal to” or “not equal to” statement.
For example, suppose we assume the mean height of a male in the U.S. is equal to 70 inches. The null hypothesis would be H0: μ = 70 inches and the alternative hypothesis would be Ha: μ ≠ 70 inches.
<em>Note:</em> The “equal” sign is always included in the null hypothesis, whether it is =, ≥, or ≤.
<b>Related:</b>  What is a Directional Hypothesis? 
<h2>Types of Hypothesis Tests</h2>
There are many different types of hypothesis tests you can perform depending on the type of data you’re working with and the goal of your analysis.
The following tutorials provide an explanation of the most common types of hypothesis tests:
 Introduction to the One Sample t-test 
 Introduction to the Two Sample t-test 
 Introduction to the Paired Samples t-test 
 Introduction to the One Proportion Z-Test 
 Introduction to the Two Proportion Z-Test 
<h2><span class="orange">What Are i.i.d. Random Variables? (Definition & Examples)</span></h2>
In statistics,  random variables  are said to be i.i.d. – <b>independently and identically distributed</b> – if the following two conditions are met:
<b>(1) Independent</b> – The outcome of one event does not affect the outcome of another.
<b>(2) Identically Distributed</b> – The probability distribution of each event is identical.
The following scenarios illustrate examples of i.i.d. random variables in practice.
<h3>Example 1: Tossing a Coin</h3>
Suppose we toss a coin 10 times and keep track of the number of times the coin lands on heads.
This is an example of a random variable that is independently and identically distributed because the following two conditions are met:
<b>(1) Independent</b> – The outcome of one coin toss does not affect the outcome of another coin toss. Each toss is independent.
<b>(2) Identically Distributed</b> – The probability that a coin lands on heads on any given toss is 0.5. This probability does not change from one toss to the next.
<h3>Example 2: Rolling a Dice</h3>
Suppose we roll a dice 50 times and keep track of the number of times the dice lands on the number 4.
This is an example of a random variable that is independently and identically distributed because the following two conditions are met:
<b>(1) Independent</b> – The outcome of one dice roll does not affect the outcome of another dice roll. Each roll is independent.
<b>(2) Identically Distributed</b> – The probability that a dice lands on “4” on any given roll is 1/6. This probability does not change from one roll to the next.
<h3>Example 3: Spinning a Spinner</h3>
Suppose we spin a spinner 100 times that is equally split into four colors (red, blue, green, and purple) and keep track of the number of times that it lands on purple.
This is an example of a random variable that is independently and identically distributed because the following two conditions are met:
<b>(1) Independent</b> – The outcome of one spin does not affect the outcome of another spin. Each spin is independent.
<b>(2) Identically Distributed</b> – The probability that the spinner lands on purple on any given spin is 0.25. This probability does not change from one spin to the next.
<h3>Example 4: Choosing a Card</h3>
A standard deck of cards contains 52 cards, 4 of which are Queens. Suppose we randomly draw a card from a standard deck, then place the card back in the deck. Suppose we repeat this 100 times and keep track of the number of times we draw a Queen.
This is an example of a random variable that is independently and identically distributed because the following two conditions are met:
<b>(1) Independent</b> – The outcome of one draw does not affect the outcome of another draw. Each draw is independent.
<b>(2) Identically Distributed</b> – The probability that we choose a Queen on any given draw is 4/52. This probability does not change from one draw to the next.
<h2><span class="orange">How to Use the identical() Function in R (With Examples)</span></h2>
The <b>identical()</b> function in R can be used to test whether or not two objects in R are exactly equal.
This function uses the following basic syntax:
<b>identical(x, y, …)</b>
where:
<b>x</b>: The name of an object in R
<b>y</b>: The name of another object in R
This function returns <b>TRUE</b> if the two objects are exactly equal or <b>FALSE</b> if they are not.
The following examples show how to use this function to test if two strings, two vectors, and two data frames are exactly equal.
<h2>Example 1: Use identical() to Test if Two Strings are Equal</h2>
The following code shows how to use the <b>identical()</b> function to test if two strings are equal:
<b>#define two strings
string1 &lt;- 'This is some string'
string2 &lt;- 'This is some string'
#check if two strings are identical
identical(string1, string2)
[1] TRUE
</b>
The function returns <b>TRUE</b> since the two strings are indeed exactly identical.
The following code shows how to use the <b>identical()</b> function to test if another two strings are exactly equal:
<b>#define two strings
string1 &lt;- 'This is some string'
string2 &lt;- 'This is some cool string'
#check if two strings are identical
identical(string1, string2)
[1] FALSE</b>
The function returns <b>FALSE </b>since the two strings are not exactly identical.
<h2>Example 2: Use identical() to Test if Two Vectors are Equal</h2>
The following code shows how to use the <b>identical()</b> function to test if two vectors are equal:
<b>#define two vectors
vector1 &lt;- c('A', 'B', 'C', 'D', 'E', 'F')
vector2 &lt;- c('A', 'B', 'C', 'D', 'E', 'F')
#check if two vectors are identical
identical(vector1, vector2)
[1] TRUE
</b>
The function returns <b>TRUE</b> since the two vectors are indeed exactly identical.
The following code shows how to use the <b>identical()</b> function to test if another two vectors are exactly equal:
<b>#define two vectors
vector1 &lt;- c('A', 'B', 'C', 'D', 'E', 'F')
vector2 &lt;- c('A', 'B', 'C', 'D')
#check if two vectors are identical
identical(vector1, vector2)
[1] FALSE</b>
The function returns <b>FALSE </b>since the two vectors are not exactly identical.
<h2>Example 3: Use identical() to Test if Two Data Frames are Equal</h2>
The following code shows how to use the <b>identical()</b> function to test if two data frames are equal:
<b>#define two data frames
df1 &lt;- data.frame(team=c('A', 'B', 'C', 'D'),  points=c(14, 20, 22, 29))
df2 &lt;- data.frame(team=c('A', 'B', 'C', 'D'),  points=c(14, 20, 22, 29))
#check if two data frames are equal
identical(df1, df2)
[1] TRUE
</b>
The function returns <b>TRUE</b> since the two data frames are indeed exactly identical.
The following code shows how to use the <b>identical()</b> function to test if another two data frames are exactly equal:
<b>#define two data frames
df1 &lt;- data.frame(team=c('A', 'B', 'C', 'D'),  points=c(14, 20, 22, 29))
df2 &lt;- data.frame(team=c('A', 'B', 'C', 'D'),  points=c(99, 20, 22, 29))
#check if two data frames are equal
identical(df1, df2)
[1] FALSE</b>
The function returns <b>FALSE </b>since the two data frames are not exactly identical.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use the dim() Function in R 
 How to Use the transform() Function in R 
 How to Use the intersect() Function in R 
<h2><span class="orange">How to Create the Identity Matrix in R (With Examples)</span></h2>
In linear algebra, the <b>identity matrix</b> is a square matrix with ones on the main diagonal and zeros everywhere else.
You can create the identity matrix in R by using one of the following three methods:
<b>#create identity matrix using diag()
diag(5)
#create identity matrix using diag() with explicit nrow argument
diag(nrow=5)
#create identity matrix by creating matrix of zeros, then filling diagonal with ones
mat &lt;- matrix(0, 5, 5)
diag(mat) &lt;- 1
</b>
Each of these methods lead to the same result.
The following examples show how to use each of these methods in practice.
<h3>Example 1: Create Identity Matrix Using diag()</h3>
The following code shows how to use the <b>diag()</b> function to create an identity matrix with 5 rows and 5 columns:
<b>#create 5x5 identity matrix
ident &lt;- diag(5)
#view matrix
ident
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]    0    1    0    0    0
[3,]    0    0    1    0    0
[4,]    0    0    0    1    0
[5,]    0    0    0    0    1
</b>
The result is a 5×5 square matrix with ones on the main diagonal and zeros everywhere else.
<h3>
<b>Example 2: Create Identity Matrix Using diag(nrow)</b>
</h3>
The following code shows how to use the diag(nrow) function to create a 5×5 identity matrix:
<b>#create 5x5 identity matrix
ident &lt;- diag(nrow=5)
#view matrix
ident
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]    0    1    0    0    0
[3,]    0    0    1    0    0
[4,]    0    0    0    1    0
[5,]    0    0    0    0    1</b>
<h3>Example 3: Create Identity Matrix in Two Steps</h3>
The following code shows how create a 5×5 identity matrix by first creating a 5×5 matrix with all zeros, then converting the main diagonal values to be ones:
<b>#create 5x5 matrix with zeros in all positions
ident &lt;- matrix(0, 5, 5)
#make diagonal values 1
diag(ident) &lt;- 1
#view matrix
ident
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    0    0    0    0
[2,]    0    1    0    0    0
[3,]    0    0    1    0    0
[4,]    0    0    0    1    0
[5,]    0    0    0    0    1
</b>
Notice that each of the three methods produce the exact same identity matrix.
<h2><span class="orange">How to Use idxmax() Function in Pandas (With Examples)</span></h2>
You can use the <b>pandas.DataFrame.idxmax()</b> function to return the index of the maximum value across a specified axis in a pandas DataFrame.
This function uses the following syntax:
<b>DataFrame.idxmax(axis=0, skipna=True)</b>
where:
<b>axis</b>: The axis to use (0 = rows, 1 = columns). Default is 0.
<b>skipna</b>: Whether or not to exclude NA or null values. Default is True.
The following examples show how to use this function in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 8, 9, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 11, 6, 6, 5]},   index=['Andy','Bob', 'Chad', 'Dan', 'Eric', 'Frank'])
#view DataFrame
df
        pointsassistsrebounds
Andy25511
Bob1278
Chad15711
Dan896
Eric9126
Frank2395</b>
<h3>Example 1: Find Index that has Max Value for Each Column</h3>
The following code shows how to find the index that has the maximum value for each column:
<b>#find index that has max value for each column
df.idxmax(axis=0)
points      Andy
assists     Eric
rebounds    Andy
dtype: object
</b>
From the output we can see:
The player with the highest value in the <b>points</b> column is Andy.
The player with the highest value in the <b>assists</b> column is Eric.
The player with the highest value in the <b>rebounds</b> column is Andy.
It’s important to note that the idxmax() function will return the <b>first occurrence</b> of the maximum value. 
For example, notice that Andy and Chad both had 11 rebounds. Since Andy appears first in the DataFrame his name is returned.
<h3>Example 2: Find Column that has Max Value for Each Row</h3>
The following code shows how to find the column that has the maximum value for each row:
<b>#find column that has max value for each row
df.idxmax(axis=1)
Andy      points
Bob       points
Chad      points
Dan      assists
Eric     assists
Frank     points
dtype: object
</b>
From the output we can see:
The highest value in the row labelled “Andy” can be found in the <b>points</b> column.
The highest value in the row labelled “Bob” can be found in the <b>points</b> column.
The highest value in the row labelled “Chad” can be found in the <b>points</b> column.
The highest value in the row labelled “Dan” can be found in the <b>assists</b> column.
The highest value in the row labelled “Eric” can be found in the <b>assists</b> column.
The highest value in the row labelled “Andy” can be found in the <b>points</b> column.
Refer to the  pandas documentation  for a complete explanation of the idxmax() function.
<h2><span class="orange">How to Combine the IF and AND Functions in Google Sheets</span></h2>
You can use the following basic syntax to use the <b>IF</b> and <b>AND </b>functions together in Google Sheets to determine if some cell meets several criteria:
<b>=IF(AND(A1="String", B1>10), "value1", "value2")
</b>
If the value in cell A1 is equal to “String” <i>and </i>if the value in cell B1 is greater than 10, then we return value1, otherwise we return value2.
Note that we can use as many logical comparisons as we’d like within the <b>AND</b> function.
The following examples show how to use this syntax in practice.
<b>Related:</b>  How to Combine the IF and OR Functions in Google Sheets 
<h3>Example 1: Combine IF and AND Functions with String Comparisons</h3>
Suppose we have two columns in Google Sheets that contain the conference and number of wins for various NBA teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/and1.png">
Suppose we classify a team as “Good” if they have more than 40 wins.
We can use the following formula with the <b>IF</b> and <b>AND </b>functions to determine if each team is in the West <em>and</em> Good:
<b>=IF(AND(A2="West", B2>40), "Yes", "No")
</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/and2.png">
If a given team is in the West <em>and</em> they have more than 40 wins, we return a value of “Yes”, otherwise we return “No.”
<h3>Example 2: Combine IF and AND Functions with Numeric Comparisons</h3>
Suppose we have columns that contain the number of points and assists for various basketball players and we’d like to classify each player as “Good” or “Bad.”
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/if3.png">
Let’s say that if a player has more than 20 points <i>and </i>more than 5 assists, we will classify them as “Good”, otherwise we’ll classify them as “Bad.”
We can use the following formula with the <b>IF</b> and <b>AND </b>functions to determine if each player should be classified as “Good” or Bad”:
<b>=IF(AND(A2>20, B2>5), "Good", "Bad")
</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/and3.png">
If a given player has more than 20 points <i>and </i>more than 5 assists, we classify them as “Good.”
Otherwise we classify them as “Bad.”
<h2><span class="orange">How to Combine the IF and OR Functions in Google Sheets</span></h2>
You can use the following basic syntax to use the <b>IF</b> and <b>OR</b> functions together in Google Sheets to determine if some cell meets one of multiple criteria:
<b>=IF(OR(A2="String", B2>10), "value1", "value2")
</b>
If the value in cell A2 is equal to “String” <em>or</em> if the value in cell B2 is greater than 10, then we return value1, otherwise we return value2.
Note that we can use as many logical comparisons as we’d like within the <b>OR</b> function.
The following examples show how to use this syntax in practice.
<h3>Example 1: Combine IF and OR Functions with String Comparisons</h3>
Suppose we have a column that contains the names of NBA teams and we’d like to determine if each team is based in Texas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/if1.png">
Note that the only teams that are based in Texas are the Mavs, Rockets, and Spurs.
We can use the following formula with the <b>IF</b> and <b>OR</b> functions to determine if each team is based in Texas:
<b>=IF(OR(A2="Mavs", A2="Rockets", A2="Spurs"), "Yes", "No")
</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/if2.png">
If a given team is from Texas, we return a value of “Yes”, otherwise we return “No.”
<h3>Example 2: Combine IF and OR Functions with Numeric Comparisons</h3>
Suppose we have columns that contain the number of points and assists for various basketball players and we’d like to classify each player as “Good” or “Bad.”
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/if3.png">
Let’s say that if a player has more than 20 points <em>or</em> more than 10 assists, we will classify them as “Good”, otherwise we’ll classify them as “Bad.”
We can use the following formula with the <b>IF</b> and <b>OR</b> functions to determine if each player should be classified as “Good” or Bad”:
<b>=IF(OR(A2>20, B2>10), "Good", "Bad")
</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/if4.png">
If a given player has more than 20 points <em>or</em> more than 10 assists, we classify them as “Good.”
Otherwise we classify them as “Bad.”
<h2><span class="orange">How to Use IF-THEN-DO in SAS (With Examples)</span></h2>
You can use an <b>IF-THEN-DO</b> statement in SAS to <em>do</em> a block of statements <em>if</em> some condition is true.
This statement uses the following basic syntax:
<b>if var1 = "value" then do;
    new_var2 = 10;
    new_var3 = 5;
    end;
</b>
<b>Note</b>: An IF-THEN statement is used when you only want to do <em>one</em> statement. An IF-THEN-DO statement is used when you want to do several statements.
The following example shows how to use an <b>IF-THEN-DO</b> statement in practice.
<h3>Example: <b>IF-THEN-DO in SAS</b></h3>
Suppose we have the following dataset in SAS that shows the total sales made by two stores during consecutive days:
<b>/*create dataset*/
data original_data;
    input store $ sales;
    datalines;
A 14
A 19
A 22
A 20
A 16
A 26
B 40
B 43
B 29
B 30
B 35
B 33
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lagsas3.jpg"146">
We can use the following IF-THEN-DO statement to create two new variables that take on certain values if the store is equal to “A” in the original dataset:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if store = "A" then do;
    region="East";
    country="Canada";
    end;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/ifthendo1.jpg">
Here’s how this chunk of code worked:
If the store was equal to “A” then a new variable called <b>region</b> was created with a value of “East” <em>and</em> a new variable called <b>country</b> was created with a value of “Canada.”
Note that we can use multiple IF-THEN-DO statements as well:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if store = "A" then do;
    region="East";
    country="Canada";
    end;
    if store = "B" then do;
    region="West";
    country="USA";
    end; 
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/ifthendo2.jpg"286">
Here’s how this chunk of code worked:
If the store was equal to “A” then a new variable called <b>region</b> was created with a value of “East” and a new variable called <b>country</b> was created with a value of “Canada.”
If the store was equal to “B” then the value for <b>region</b> was “West” and the value for <b>country</b> was “USA.”
<h2><span class="orange">How to Use IFERROR with VLOOKUP in Google Sheets</span></h2>
You can use the following formula with <b>IFERROR</b> and <b>VLOOKUP</b> in Google Sheets to return a value other than #N/A when the VLOOKUP function does not find a particular value in a range:
<b>=IFERROR(VLOOKUP("string",A2:B11,2,FALSE),"Does Not Exist")
</b>
This particular formula looks for “string” in the range A2:B11 and attempts to return the corresponding value in the second column of this range.
If it does not find “string” then it simply returns “Does Not Exist” instead of a #N/A value.
The following example shows how to use this formula in practice.
<h3>Example: Use IFERROR with VLOOKUP in Google Sheets</h3>
Suppose we have the following dataset that shows the number of points scored by various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/iferror1.jpg"556">
Suppose we use the following <b>VLOOKUP</b> formula to find the number of points associated with the “Raptors” team:
<b>=VLOOKUP("Raptors",A2:B11,2,FALSE)
</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/iferror2.jpg"600">
This formula returns a value of <b>#N/A</b> because the “Raptors” do not exist in the Team column.
However, we can use the following <b>IFERROR</b> function with the <b>VLOOKUP</b> function to return a value of “Does Not Exist” instead of<b> #N/A</b>:
<b>=IFERROR(VLOOKUP("Raptors",A2:B11,2,FALSE),"Does Not Exist")
</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/iferror3.jpg">
Since the “Raptors” do not exist in the Team column, the formula returns a value of “Does Not Exist” instead of a <b>#N/A</b> value.
<h2><span class="orange">How to Import CSV Files into R (Step-by-Step)</span></h2>
Suppose I have a CSV file called <b>data.csv</b> saved in the following location:
<b>C:\Users\Bob\Desktop\data.csv</b>
And suppose the CSV file contains the following data:
<b>team, points, assists
'A', 78, 12
'B', 85, 20
'C', 93, 23
'D', 90, 8
'E', 91, 14
</b>
There are three common ways to import this CSV file into R:
<b>1. Use read.csv from base R </b>(Slowest method, but works fine for smaller datasets)
<b>data1 &lt;- read.csv("C:\\Users\\Bob\\Desktop\\data.csv", header=TRUE, stringsAsFactors=FALSE)
</b>
<b>2. Use read_csv from readr package </b>(2-3x faster than read.csv)
<b>library(readr)
data2 &lt;- read_csv("C:\\Users\\Bob\\Desktop\\data.csv")</b>
<b>3. Use fread from data.table package </b>(2-3x faster than read_csv)
<b>library(data.table)
data3 &lt;- fread("C:\\Users\\Bob\\Desktop\\data.csv")</b>
This tutorial shows an example of how to use each of these methods to import the CSV file into R.
<h3>Method 1: Using read.csv</h3>
If your CSV file is reasonably small, you can just use the <b>read.csv</b> function from Base R to import it.
When using this method, be sure to specify <b>stringsAsFactors=FALSE</b> so that R doesn’t convert character or categorical variables into factors.
The following code shows how to use <b>read.csv</b> to import this CSV file into R:
<b>#import data
data1 &lt;- read.csv("C:\\Users\\Bob\\Desktop\\data.csv", header=TRUE, stringsAsFactors=FALSE)
#view structure of data
str(data1)
'data.frame':   5 obs. of  3 variables:
 $ team   : chr  "'A'" "'B'" "'C'" "'D'" ...
 $ points : int  78 85 93 90 91
 $ assists: int  12 20 23 8 14
</b>
<h3>Method 2: Using read_csv</h3>
If you’re working with larger files, you can use the <b>read_csv</b> function from the readr package:
<b>library(readr)
#import data
data2 &lt;- read_csv("C:\\Users\\Bob\\Desktop\\data.csv")
#view structure of data
str(data2)
'data.frame': 5 obs. of 3 variables:
 $ team : chr "'A'" "'B'" "'C'" "'D'" ...
 $ points : int 78 85 93 90 91
 $ assists: int 12 20 23 8 14
</b>
<h3>Method 3: Using fread</h3>
If your CSV is extremely large, the fastest way to import it into R is with the <b>fread</b> function from the data.table package:
<b>library(data.table)
#import data
data3 &lt;- fread("C:\\Users\\Bob\\Desktop\\data.csv")
#view structure of data
str(data3)
Classes 'data.table' and 'data.frame':  5 obs. of  3 variables:
 $ team   : chr  "'A'" "'B'" "'C'" "'D'" ...
 $ points : int  78 85 93 90 91
 $ assists: int  12 20 23 8 14</b>
Note that in each example we used double backslashes (\\) in the file path to avoid the following common error:
<b>Error: '\U' used without hex digits in character string starting ""C:\U"
</b>
<h2><span class="orange">How to Import .dta Files into R (Step-by-Step)</span></h2>
The easiest way to import .dta files into R is to use the <b>read_dta()</b> function from the  haven  library.
This function uses the following basic syntax:
<b>data &lt;- read_dta('C:/Users/User_Name/file_name.dta')
</b>
The following step-by-step example shows how to import a .dta file into R in practice.
<h3>Step 1: Download a .dta Data File</h3>
For this example, we’ll download the .dta file called <b>cola.dta</b> from  this page .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/statafile2.png">
<h3>Step 2: Install haven Package</h3>
Next, we’ll install the haven package in R:
<b>install.packages('haven')</b>
We’ll then load the package:
<b>library(haven)</b>
<h3>Step 3: Import the .dta File</h3>
Next, we’ll use the <b>read_dta()</b> function to import the .dta file:
<b>data &lt;- read_dta('C:/Users/bob/Downloads/cola.dta')</b>
Once we’ve imported the .dta file, we can get a quick summary of the data:
<b>#view class of data
class(data)
[1] "tbl_df"     "tbl"        "data.frame"
#display dimensions of data frame
dim(data)
[1] 5466    5
#view first six rows of data
head(data)
     ID CHOICE PRICE FEATURE DISPLAY
1     1      0 1.79        0       0
2     1      0 1.79        0       0
3     1      1 1.79        0       0
4     2      0 1.79        0       0
5     2      0 1.79        0       0
6     2      1 0.890       1       1
</b>
We can see that the file imported successfully as a data frame and that it has <b>5</b> columns and <b>5,466</b> rows.
<h2><span class="orange">How to Import Excel Files into R (Step-by-Step)</span></h2>
The easiest way to import an Excel file into R is by using the <b>read_excel()</b> function from the  readxl package .
This function uses the following syntax:
<b>read_excel(path, sheet = NULL)</b>
where:
<b>path:</b> Path to the xls/xlsx file
<b>sheet:</b> The sheet to read. This can be the name of the sheet or the position of the sheet. If this is not specified, the first sheet is read.
This tutorial provides an example of how to use this function to import an Excel file into R.
<h3>Example: Import an Excel File into R</h3>
Suppose I have an Excel file saved in the following location:
<b>C:\Users\Bob\Desktop\data.xlsx</b>
The file contains the following data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/importExcel1.png">
The following code shows how to import this Excel file into R:
<b>#install and load readxl package
install.packages('readxl')
library(readxl)
#import Excel file into R
data &lt;- read_excel('C:\\Users\\Bob\\Desktop\\data.xlsx')
</b>
Note that we used double backslashes (\\) in the file path to avoid the following common error:
<b>Error: '\U' used without hex digits in character string starting ""C:\U"
</b>
We can use the following code to quickly view the data:
<b>#view entire dataset
data
#A tibble: 5 x 3
 team  points  assists
 &lt;chr>   &lt;dbl>   &lt;dbl>
1 A         78      12
2 B         85      20
3 C         93      23
4 D         90       8
5 E         91      14
</b>
We can see that R imported the Excel file and automatically determined that <em>team</em> was a string variable while <em>points</em> and <em>assists</em> were numerical variables.
<h2><span class="orange">The Easiest Way to Use NumPy: import numpy as np</span></h2>
<b>NumPy</b>, which stands for Numerical Python, is a scientific computing library built on top of the Python programming language.
The most common way to import NumPy into your Python environment is to use the following syntax:
<b>import numpy as np
</b>
The <b>import numpy</b> portion of the code tells Python to bring the NumPy library into your current environment.
The <b>as np</b> portion of the code then tells Python to give NumPy the alias of <b>np</b>. This allows you to use NumPy functions by simply typing np.function_name rather than numpy.function_name.
Once you’ve imported NumPy, you can then use the functions built in it to quickly create and analyze data.
<h3>How to Create a Basic NumPy Array</h3>
The most common data type you’ll work with in NumPy is the <b>array</b>, which can be created by using the <b>np.array()</b> function.
The following code shows how to create a basic one-dimensional NumPy array:
<b>import numpy as np
#define array
x = np.array([1, 12, 14, 9, 5])
#display array
print(x)
[ 1 12 14  9  5]
#display number of elements in array
x.size
5</b>
You can also create multiple arrays and perform operations on them such as addition, subtraction, multiplication, etc.
<b>import numpy as np 
#define arrays 
x = np.array([1, 12, 14, 9, 5])
y = np.array([2, 3, 3, 4, 2])
#add the two arrays
x+y
array([ 3, 15, 17, 13,  7])
#subtract the two arrays
x-y
array([-1,  9, 11,  5,  3])
#multiply the two arrays
x*y
array([ 2, 36, 42, 36, 10])
</b>
Check out  the absolute beginner’s guide to NumPy  for a detailed introduction to all of the basic NumPy functions.
<h3>Potential Errors when Importing NumPy</h3>
One potential error you may encounter when importing NumPy is:
<b>NameError: name 'np' is not defined
</b>
This occurs when you fail to give NumPy an alias when importing it. Read  this tutorial  to find out how to quickly fix this error.
<h2><span class="orange">The Easiest Way to Use Pandas in Python: import pandas as pd</span></h2>
<b>pandas</b> is an open source data analysis library built on top of the Python programming language.
The most common way to import pandas into your Python environment is to use the following syntax:
<b>import pandas as pd
</b>
The <b>import pandas</b> portion of the code tells Python to bring the pandas data analysis library into your current environment.
The <b>as pd</b> portion of the code then tells Python to give pandas the alias of <b>pd</b>. This allows you to use pandas functions by simply typing pd.function_name rather than pandas.function_name.
Once you’ve imported pandas, you can then use the functions built in it to create and analyze data.
<h3>How to Create Series & DataFrames</h3>
The most common data types you’ll work with in pandas are <b>series</b> and <b>DataFrames</b>.
<b>1. Series</b>
A Series is a 1-dimensional array. The following code shows how to quickly create a Series using pandas:
<b>import pandas as pd 
#define Series
x = pd.Series([25, 12, 15, 14, 19, 23, 25, 29])
#display Series
print(x)
0    25
1    12
2    15
3    14
4    19
5    23
6    25
7    29
dtype: int64
</b>
<b>2. DataFrame</b>
A DataFrame is a 2-dimensional array. The following code shows how to quickly create a DataFrame using pandas:
<b>import pandas as pd 
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#display DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12</b>
<h3>Potential Errors when Importing Pandas</h3>
There are two potential errors you may encounter when import pandas:
<b>1. NameError: name ‘pd’ is not defined</b>
One error you may encounter is:
<b>NameError: name 'pd' is not defined
</b>
This occurs when you fail to give pandas an alias when importing it. Read  this tutorial  to find out how to quickly fix this error.
<b>2. No module named pandas</b>
Another error you may encounter is:
<b>no module name 'pandas'
</b>
This occurs when Python does not detect the pandas library in your current environment. Read  this tutorial  to find out how to fix this error.
<h2><span class="orange">How to Import SAS Files into R (Step-by-Step)</span></h2>
The easiest way to import SAS files into R is to use the <b>read_sas()</b> function from the  haven  library.
This function uses the following basic syntax:
<b>data &lt;- read_sas('C:/Users/User_Name/file_name.sas7bdat')
</b>
The following step-by-step example shows how to import a SAS file into R in practice.
<h3>Step 1: Download a SAS Data File</h3>
For this example, we’ll download the SAS file called <b>cola.sas7bdat</b> from  this page .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/sasimport1.png">
<h3>Step 2: Install haven Package</h3>
Next, we’ll install the haven package in R:
<b>install.packages('haven')</b>
We’ll then load the package:
<b>library(haven)</b>
<h3>Step 3: Import the SAS File</h3>
Next, we’ll use the <b>read_sas()</b> function to import the SAS file:
<b>data &lt;- read_sas('C:/Users/bob/Downloads/cola.sas7bdat')</b>
Once we’ve imported the SAS file, we can get a quick summary of the data:
<b>#view class of data
class(data)
[1] "tbl_df"     "tbl"        "data.frame"
#display dimensions of data frame
dim(data)
[1] 5466    5
#view first six rows of data
head(data)
     ID CHOICE PRICE FEATURE DISPLAY
1     1      0 1.79        0       0
2     1      0 1.79        0       0
3     1      1 1.79        0       0
4     2      0 1.79        0       0
5     2      0 1.79        0       0
6     2      1 0.890       1       1
</b>
We can see that the file imported successfully as a data frame and that it has <b>5</b> columns and <b>5,466</b> rows.
<h2><span class="orange">The Easiest Way to Use Seaborn: import seaborn as sns</span></h2>
<b>Seaborn</b> is a Python data visualization library built on top of  Matplotlib .
The most common way to import Seaborn into your Python environment is to use the following syntax:
<b>import seaborn as sns
</b>
The <b>import seaborn </b>portion of the code tells Python to bring the Seaborn library into your current environment.
The <b>as sns</b> portion of the code then tells Python to give Seaborn the alias of <b>sns</b>. This allows you to use Seaborn functions by simply typing sns.function_name rather than seaborn.function_name.
Once you’ve imported Seaborn, you can then use the functions built in it to quickly visualize data.
<h3>Set the Seaborn Theme</h3>
Once you’ve imported Seaborn, you can set the default theme for plots by using the following function:
<b>sns.set_theme(style='darkgrid')</b>
This function takes the following potential styles as arguments:
<b>darkgrid</b> (dark background with white gridlines)
<b>whitegrid</b> (white background with grey gridlines)
<b>dark</b> (dark background with no gridlines)
<b>white</b> (white background with no gridlines)
<b>ticks</b> (white background with axis ticks and no gridlines)
It’s recommended to set the theme after importing the Seaborn library.
<h3>Create Your First Plot</h3>
Once you’ve imported Seaborn and set the theme, you’re ready to create your first plot.
Seaborn has several built-in plots you can create, including:
scatterplot
lineplot
histplot
kdeplot
ecdfplot
rugplot
stripplot
swarmplot
boxplot
violinplot
pointplot
barplot
For example, here’s how to create a simple scatterplot using the built-in Seaborn <b>tips</b> dataset:
<b>import seaborn as sns
#set theme
sns.set_theme(style='darkgrid')
#load tips dataset
tips = sns.load_dataset('tips')
#create scatterplot
sns.scatterplot(data=tips, x='total_bill', y='tip')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/sns1.png">
And here’s how to create a violin plot using the same dataset:
<b>import seaborn as sns
#set theme
sns.set_theme(style='dark')
#load tips dataset
tips = sns.load_dataset('tips')
#create scatterplot
sns.violinplot(data=tips, x='total_bill', color='purple')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/sns2.png">
For a comprehensive overview of Seaborn plotting functions, refer to  this documentation page .
<h2><span class="orange">How to Import SPSS Files into R (Step-by-Step)</span></h2>
The easiest way to import SPSS files into R is to use the <b>read_sav()</b> function from the  haven  library.
This function uses the following basic syntax:
<b>data &lt;- read_sav('C:/Users/User_Name/file_name.sav')
</b>
The following step-by-step example shows how to import a SPSS file into R in practice.
<h3>Step 1: Download a SPSS File</h3>
For this example, we’ll download the SPSS file called <b>healthdata.sav</b> from  this page .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/spssfile1.png">
<h3>Step 2: Install haven Package</h3>
Next, we’ll install the haven package in R:
<b>install.packages('haven')</b>
We’ll then load the package:
<b>library(haven)</b>
<h3>Step 3: Import the SPSS File</h3>
Next, we’ll use the <b>read_sav()</b> function to import the SPSS file:
<b>data &lt;- read_sav('C:/Users/bob/Downloads/healthdata.sav')</b>
Once we’ve imported the SPSS file, we can get a quick summary of the data:
<b>#view class of data
class(data)
[1] "tbl_df"     "tbl"        "data.frame"
#display dimensions of data frame
dim(data)
[1] 185   3
#view first six rows of data
head(data)
 CD EXERC HEALTH
1 1 [ordered]           3      6
2 2 [did not order]     3      7
3 2 [did not order]     5      6
4 2 [did not order]     5      3
5 1 [ordered]           5      6
6 2 [did not order]     2      3
</b>
We can see that the file imported successfully as a data frame and that it has <b>185</b> rows and <b>3</b> columns.
<h2><span class="orange">How to Import TSV Files into R (Including Example)</span></h2>
You can use the following basic syntax to import a TSV file into R:
<b>library(readr)
#import TSV file into data frame
df &lt;- read_tsv('C:/Users/bob/Downloads/data.tsv')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Import TSV File into R (With Column Names)</h3>
Suppose I have the following TSV file called <b>data.tsv</b> saved somewhere on my computer:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tsv1.png">
I can use the following syntax to import this TSV file into a data frame in R:
<b>library(readr)
#import TSV file into data frame
df &lt;- read_tsv('C:/Users/bob/Downloads/data.tsv')
#view data frame
df
# A tibble: 5 x 3
  team  points rebounds
      
1 A         33       12
2 B         25        6
3 C         31        6
4 D         22       11
5 E         20        7</b>
We can see that the TSV file was successfully imported into R.
<h3>Example 2: Import TSV File into R (No Column Names)</h3>
Suppose I have the following TSV file called <b>data.tsv</b> with no column names:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tsv2.png">
I can use the <b>col_names</b> argument to specify that there are no column names when importing this TSV file into R:
<b>library(readr)
#import TSV file into data frame
df &lt;- read_tsv('C:/Users/bob/Downloads/data.tsv', col_names=FALSE)
#view data frame
df
  X1       X2    X3
    
1 A        33    12
2 B        25     6
3 C        31     6
4 D        22    11
5 E        20     7</b>
By default, R provides the column names X1, X2, and X3.
I can use the following syntax to easily  rename the columns :
<b>#rename columns
names(df) &lt;- c('team', 'points', 'rebounds')
#view updated data frame
df
  team  points rebounds
        
1 A         33       12
2 B         25        6
3 C         31        6
4 D         22       11
5 E         20        7</b>
<b>Note</b>: You can find the complete documentation for the <b>read_tsv()</b> function  here .
<h2><span class="orange">Why is the Mean Important in Statistics?</span></h2>
The <b>mean</b> of a dataset represents the average value of the dataset. It is calculated as:
Mean = Σx<sub>i</sub> / n
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> observation in a dataset
<b>n:</b> The total number of observations in the dataset 
For example, suppose we have the following dataset with 11 observations:
<b>Dataset:</b> 3, 4, 4, 6, 7, 8, 12, 13, 15, 16, 17
The mean of the dataset is calculated as:
Mean = (3+4+4+6+7+8+12+13+15+16+17) / 11 = <b>9.54</b>
In statistics, the mean is important for the following reasons:
<b>1.</b> The mean gives us an idea of where the “center” of a dataset is located.
<b>2.</b> Because of how it’s calculated, the mean carries a piece of information from <em>every</em>  observation  in a dataset.
The following example illustrates both of these reasons.
<h3>Example: Calculating the Mean of a Dataset</h3>
Suppose we have a dataset that contains the selling price of 10,000 different homes in a certain city. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/medianImportance1.png">
Instead of staring at thousands of rows of  raw data , we can calculate the mean value to quickly understand the average selling price of homes in this city.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/meanimportance1.jpg"477">
By knowing that the mean selling price is $297,000, we get an idea of what the “typical” house sells for in this city.
This single value for the mean is much easier to interpret compared to staring at all of the rows of raw data.
And since every single house selling price was used to calculate the mean, we could multiply the average selling price by the total number of houses to find the total selling price of all houses in this city:
Total selling price of all houses = Average selling price * Number of houses
Total selling price of all houses = $297,000 * 10,000
Total selling price of all houses = $2,970,000,000
We can see that the total selling price of all houses in this city is $2.97 billion.
<h3>When to Use the Mean</h3>
When analyzing datasets, we’re often interested in understanding where the center value is located.
In statistics, there are two common metrics that we use to measure the center of a dataset:
<b>Mean</b>: The average value in a dataset
<b>Median</b>: The middle value in a dataset
The mean is the most common way to measure the center of a dataset, but it can actually be misleading in the following situations:
When the distribution is  skewed .
When the distribution contains  outliers .
To illustrate this, consider the following two examples.
<b>Example 1: Calculating the Mean of a Skewed Distribution</b>
Consider the following distribution of salaries for residents in a certain city:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/median_dist.jpg"473">
The large salaries on the right side of the distribution pull the mean away from the center of the distribution.
Thus, the median does a better job of capturing the “typical” salary of a resident than the mean because the distribution is right-skewed.
In this particular example, the mean salary is $47,000 while the median salary is $32,000.
Thus, the median is much more representative of the typical salary in this city.
<b>Example 2: Calculating the Mean When Outliers Are Present</b>
Consider the following chart that shows the square footage of houses on a certain street:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/mean_dist3.jpg">
The mean is heavily influenced by a couple extremely large houses, while the median is not.
We can see that the median does a better job of capturing the “typical” square footage of a house on this street compared to the mean because it isn’t influenced by the extreme outlier values.
<h3>Summary</h3>
Here’s a quick summary of the main takeaways from this article:
The mean represents the average value in a dataset.
The mean is important because it gives us an idea of where the center value is located in a dataset.
The mean is also important because it carries a piece of information from <em>every</em> observation in a dataset.
The mean can be misleading when a dataset is skewed or contains outlies. In these scenarios, the median provides a more accurate idea of where the “center” of a dataset is located.
<h2><span class="orange">Why is the Median Important in Statistics?</span></h2>
The <b>median</b> represents the middle value of a dataset, when all of the values are arranged from smallest to largest.
For example, the median in the following dataset is 19:
Dataset: 3, 4, 11, 15, <b>19</b>, 22, 23, 23, 26
The median also represents the 50<sup>th</sup> percentile of a dataset. That is, exactly half of the values in the dataset are larger than the median and half of the values are lower.
The median is an important metric to calculate because it gives us an idea of where the “center” of a dataset is located. It also gives us an idea of the “typical” value in a given dataset.
For example, suppose we have a dataset that contains the selling price of 10,000 different homes in a certain city. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/medianImportance1.png">
Instead of staring at rows and rows of  raw data , we can calculate the median value to quickly understand the middle selling price of homes in this city.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/medianImportance2.png">
By knowing that the median selling price is $271,000, we know that exactly half of the 10,000 homes sold for more than this amount and half sold for less.
This also gives us an idea of the “typical” selling price of homes in this city.
<h3>When to Use the Median</h3>
When analyzing datasets, we’re often interested in understanding where the center value is located.
In statistics, there are two common metrics that we use to measure the center of a dataset:
<b>Mean</b>: The average value in a dataset
<b>Median</b>: The middle value in a dataset
It turns out that the median is a more useful metric in the following circumstances:
When the distribution is  skewed .
When the distribution contains outliers.
To illustrate this, consider the following two examples.
<b>Example 1: Calculating the Median of a Skewed Distribution</b>
Consider the following distribution of salaries for residents in a certain city:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/median_dist.jpg"473">
The median does a better job of capturing the “typical” salary of a resident than the mean because the distribution is right-skewed.
This means the large salaries on the right side of the distribution pull the mean away from the center of the distribution.
In this particular example, the mean salary is $47,000 while the median salary is $32,000. The median is much more representative of the typical salary in this city.
<b>Example 2: Calculating the Median When Outliers Are Present</b>
Consider the following chart that shows the square footage of houses on a certain street:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/mean_dist3.jpg">
The mean is heavily influenced by a couple extremely large houses, while the median is not.
We can see that the median does a better job of capturing the “typical” square footage of a house on this street compared to the mean because it isn’t influenced by the extreme outlier values.
<h3>Summary</h3>
Here’s a quick summary of the main points made in this article:
The median represents the middle value in a dataset.
The median is important because it gives us an idea of where the center value is located in a dataset.
The median tends to be more useful to calculate than the mean when a distribution is skewed and/or has outliers.
<h2><span class="orange">Why is the Mode Important in Statistics?</span></h2>
The <b>mode</b> represents the value that occurs most often in a dataset.
A dataset can have no mode (if no value repeats), one mode, or multiple modes.
For example, the mode in the following dataset is 19:
Dataset: 3, 4, 11, 15, <b>19</b>, <b>19</b>, <b>19</b>, 22, 22, 23, 23, 26
This is the value that occurs most often.
In statistics, the mode is important for the following reasons:
<b>Reason 1</b>: It lets us know which value(s) in a dataset is the most common.
<b>Reason 2</b>: It’s useful for finding the most frequently occurring value in categorical data when the mean and median can’t be calculated.
<b>Reason 3</b>: It gives us an idea of where the “center” of a dataset is located, although the median and mean are more commonly used (as we’ll see later in this article).
The following examples illustrate each of these reasons in practice.
<h3>Reason 1: The Mode Tells Us Which Value is Most Common</h3>
Suppose we have a dataset with 100,000 rows that contain the selling price of houses throughout the United States:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant1.jpg"158">
Suppose we use some statistical software (like  Excel ,  R ,  Python , etc.) to calculate the mode of this dataset and find that there are three modes:
$280,000
$300,000
$305,000
This immediately gives us an idea of which house prices occur most frequently in the dataset.
Calculating the mode is also much quicker than staring at thousands of rows of data and attempting to identify which house prices occur most often.
<h3>Reason 2: The Mode Finds the Most Common Value in Categorical Data</h3>
Suppose we have a dataset with 1,000 rows that tells us the color of the car owned by individuals in a certain neighborhood:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant2.jpg"164">
The variable “color” is a  categorical variable , which means the values fall into categories (“red”, “yellow”, “black”, etc.) so we can’t calculate a quantitative value like the mean or median.
However, we can calculate the mode because this simply represents the most commonly occurring value in the dataset.
For example, we might use some statistical software to find that the mode of this dataset is “black” – which tells us that the most frequently occurring car color in this dataset is black.
<h3>Reason 3: The Mode Gives Us an Idea of Where the Center of a Dataset is Located</h3>
The mode is also considered a  measure of central tendency , which means it can give us an idea of where the “center” of the dataset is located.
For example, suppose we have the following dataset that shows the exam scores of 20 different students in a class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant3.jpg"147">
The mode turns out to be <b>82</b> – this is the most common exam score. This also turns out to be a good indication of the where the “center” exam score value is located in this dataset.
However, suppose we instead had the following dataset of exam scores:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant4.jpg"150">
In this dataset, <b>72</b> is the mode exam score. However, this turns out to be a poor indication of where the “center” exam score is located.
The mean exam score is <b>82.9</b> and the median exam score is <b>82.5</b>, which both give us a better idea of where the “center” value is located compared to the mode.
<h3>Summary</h3>
Here’s a quick summary of the main points made in this article:
The mode represents the value(s) that occurs most often in a dataset.
The mode tells us the most common value in categorical data when the mean and median can’t be used.
The mode gives us an idea of where the “center” of a dataset is located, but it can be misleading compared to the mean or median.
<h2><span class="orange">Why is the Range Important in Statistics?</span></h2>
In statistics, the <b>range</b> represents the difference between the smallest and largest value in a dataset.
For example, suppose we have the following dataset:
Dataset: 3, 4, 11, 15, 19, 19, 19, 22, 22, 23, 23, 26
We can use the following formula to calculate the range:
Range = Maximum value – Minimum value
Range = 26 – 3
Range = 23
The range is <b>23</b>. This represents the difference between the smallest and largest values in the dataset.
In statistics, the range is important for the following reasons:
<b>Reason 1</b>: It tell us the spread of the entire dataset.
<b>Reason 2</b>: It tells us what extreme values are possible in a given dataset.
The following examples illustrate each of these reasons in practice.
<h3>Reason 1: The Range Tells Us the Spread of an Entire Dataset</h3>
The range tells us the spread of an entire dataset.
For example, suppose we have the following dataset that shows the exam scores of 20 different students in a class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant3.jpg"147">
The range for the exam scores would be calculated as:
Range = Maximum value – Minimum value
Range = 98 – 68
Range = 30
The range turns out to be <b>30</b>. This represents the difference between the highest exam score and the lowest exam score in the class.
Knowing just this metric, the teacher of the class can gain a quick understanding of the spread of values in exam scores among all of the students.
<h3>Reason 2: The Range tells us what extreme values are possible in a given dataset</h3>
The range tells us what extreme values are possible in a given dataset.
For example, suppose a realtor has access to a database that contains the selling price of 100,000 houses in a certain city in the United States:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/modeimportant1.jpg"158">
Suppose we use some statistical software (like  Excel ,  R ,  Python , etc.) to calculate the range of this dataset and find the following:
Range = Maximum Value – Minimum Value
Range = 854,000 – 194,000
Range = 660,000
If the realtor has a client who has a purchasing budget of less than $194,000 or greater than $854,000, the realtor can immediately know that there will be no houses in this particular city that meet the purchasing criteria.
<h2>The Drawback of Using the Range</h2>
The range suffers from one drawback: <b>It is influenced by outliers</b>.
To illustrate this, consider the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
The range of this dataset is 32 – 1 = <b>31</b>.
However, consider if the dataset had one extreme outlier:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32, <b>378</b>
The range of this dataset would now be 378 – 1 = <b>377</b>.
Notice how the range changes dramatically as a result of one outlier.
Before calculating the range of any dataset, it’s a good idea to first check if there are any outliers that could cause the range to be misleading.
<h2>Additional Resources</h2>
The following tutorials explain the importance of other metrics in statistics:
 Why is the Mean Important in Statistics? 
 Why is the Median Important in Statistics? 
 Why is the Mode Important in Statistics? 
 Why is Standard Deviation Important in Statistics? 
<h2><span class="orange">The Importance of Statistics in Accounting (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of accounting, statistics is important for the following reasons:
<b>Reason 1</b>: Accountants use descriptive statistics to summarize data related to revenue, expenses, and profit for companies.
<b>Reason 2</b>: Accountants use line charts, box plots, scatter plots, and other charts to visualize trends related to revenue and expenses for companies.
<b>Reason 3</b>: Accountants use time series forecasting models to predict future revenue, expenses, and profits for companies.
In the rest of this article, we elaborate on each of these reasons.
<h2>Reason 1: Using Descriptive Statistics to Summarize Data</h2>
 Descriptive statistics  are used to <em>describe</em> data.
Accountants often use descriptive statistics to summarize data related to the finances of companies.
For example, an accountant who works for a retail company may calculate the following descriptive statistics during one business quarter:
Total revenue
Total expenses
Percentage change in new customers
Percentage of products returned by customers
Using these metrics, the accountant can gain a clear understanding of the financial state of the company and also compare these metrics to previous quarters to understand how the metrics are trending over time.
<h2>Reason 2: Using Charts to Visualize Trends</h2>
Another way that statistics is used in accounting is in the form of plots and charts such as:
Line charts
Scatter plots
Box Plots
Each of these charts allow accountants to visualize how different metrics are trending in a company over time.
For example, line charts are commonly used to visualize how revenue, expenses, and profit are trending year over year.
If an accountant creates a line chart to visualize yearly profits over the past five years, they will quickly see whether profits are trending up or down over time and can then share these findings with company management.
<h2>Reason 3: Using Time Series Forecasting to Predict Future Metrics</h2>
Another way that statistics is used in accounting is in the form of time series forecasting.
For example, an accountant may use historical data on revenue, expenses, and profit to forecast future values for each of these metrics for some company.
By forecasting these values, the accountant can inform the company on how much revenue, expenses, and profit they can expect in the coming months, quarters, and years.
Time series forecasting also offers accountants a way to predict a <em>range</em> of potential values for each of these metrics, which can inform a company on the best-case and worst-case scenarios for revenue and expenses in the future.
<b>Related:</b>  How to Plot a Time Series in Excel 
<h2>Additional Resources</h2>
The following articles explain the importance of statistics in other fields:
 The Importance of Statistics in Research 
 The Importance of Statistics in Healthcare 
 The Importance of Statistics in Business 
 The Importance of Statistics in Economics 
 The Importance of Statistics in Education 
<h2><span class="orange">The Importance of Statistics in Business (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In a business setting, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows a business to understand consumer behavior better using descriptive statistics.
<b>Reason 2</b>: Statistics allows a business to spot trends using data visualization.
<b>Reason 3</b>: Statistics allows a business to understand the relationship between different variables using regression models.
<b>Reason 4</b>: Statistics allows a business to segment consumers into groups using cluster analysis. 
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Understand Consumer Behavior Using Descriptive Statistics</h3>
 Descriptive statistics  are used to <em>describe</em> datasets.
Businesses in almost every field use descriptive statistics to gain a better understanding of how their consumers behave.
For example, a grocery store might calculate the following descriptive statistics:
The mean number of customers who come in each day.
The median sales order per customer.
The standard deviation of the age of the customers who come in the store.
The sum of the sales made each month.
Using these metrics, the store can gain a strong understanding of who their customers are and how they behave.
On the other hand, a bank might calculate the following descriptive statistics:
The percentage of customers who default on their loan.
The mean number of new customers who join the bank each day.
The sum of the total deposits made by all customers each month.
Using these metrics, the bank can get an idea of how their customers behave and how they handle their money.
Not all businesses build statistical models or perform complex calculations, but just about every business uses descriptive statistics to gain a better understanding of their customers.
<h3>Reason 2: Spot Trends Using Data Visualization</h3>
Another common way that statistics is used in business is through data visualizations such as line charts, histograms, boxplots, pie charts and other charts.
These types of charts are often used to help a business spot trends.
For example, a small business might create the following  combo chart  to visualize the number o f new clients and total sales they make each month:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/combo6.png">
Using this simple chart, the business can quickly see that both their sales and number of new clients tends to increase the most in the final quarter of the year.
This can allow the business to be prepared with more staff, later hours, more inventory, etc. during this time of year.
<h3>Reason 3: Understand the Relationship Between Variables Using Regression Models</h3>
Another way that statistics is used in business settings is in the form of  linear regression models .
These are models that allow a business to understand the relationship between one or more predictor variables and a  response variable .
For example, a grocery store might track their total amount spent on print advertising, their total amount spent on online advertising, and their total revenue.
They might then build the following multiple linear regression model:
Sales = 840.35 + 2.55(TV advertising) + 4.87(online advertising)
Here’s how to interpret the  regression coefficients  in this model:
For each additional dollar spent on TV advertising, the total revenue increases by <b>$2.55</b> (assuming online advertising is held constant).
For each additional dollar spent on online advertising, the total revenue increases by <b>$4.87</b> (assuming TV advertising is held constant).
Using this model, the grocery store can quickly see that their money is better spent on online advertising as opposed to TV advertising.
<b>Note</b>: In this example, we only used two predictor variables (TV advertising and online advertising), but in practice businesses often build regression models with far more predictor variables.
<h3>Reason 4: Segment Consumers into Groups Using Cluster Analysis</h3>
Another way that statistics is used in business settings is in the form of  cluster analysis .
This is a  machine learning technique  that allows a business to group together similar people based on different attributes.
Retail companies often use clustering to identify groups of households that are similar to each other.
For example, a retail company may collect the following information on households:
Household income
Household size
Head of household Occupation
Distance from nearest urban area
They can then feed these variables into a clustering algorithm to perhaps identify the following clusters:
Cluster 1: Small family, high spenders
Cluster 2: Larger family, high spenders
Cluster 3: Small family, low spenders
Cluster 4: Large family, low spenders
The company can then send personalized advertisements or sales letters to each household based on how likely they are to respond to specific types of advertisements.
<h2><span class="orange">The Importance of Statistics in Economics (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of economics, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows economists to understand the state of the economy using descriptive statistics.
<b>Reason 2</b>: Statistics allows economists to spot trends in the economy using data visualizations.
<b>Reason 3</b>: Statistics allows economists to quantify the relationship between variables using regression models.
<b>Reason 4</b>: Statistics allows economists to forecast trends in the economy.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Understand the Economy Using Descriptive Statistics</h3>
 Descriptive statistics  are used to <em>describe</em> data.
Economists often calculate the following descriptive statistics for a given region:
The mean household income.
The standard deviation of household incomes.
The sum of gross domestic product.
The percentage change in total new jobs.
Using these metrics, economists can gain a better understanding of the state of the economy in a particular region.
They can then use these metrics to inform politicians or law makers on the best methods to use to ensure that the economy remains healthy and grows.
<b>Note</b>: These types of descriptive statistics are used at every level of economics. For example, economists at the national level, state level, city level, county level, etc. all use descriptive statistics to gain a better understanding of the state of the economy in their area.
<h3>Reason 2: Spot Trends Using Data Visualization</h3>
Another common way that statistics is used in economics is through data visualizations such as line charts, histograms, boxplots, pie charts and other charts.
These types of charts are often used to help economists spot trends that can help them see if the economy is improving or declining.
For example, suppose an economist creates the following line chart that shows the overall unemployment rate in a certain city by year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/economics1.jpg"567">
Just from looking at this bar chart, the economist will be able to tell that the unemployment rate is decreasing over time, which is a sign that the economy is strong and more citizens are becoming employed.
<h3>Reason 3: Quantify Relationship Between Variables Using Regression Models</h3>
Another way that statistics is used in economics is in the form of  regression models .
These are models that allow economists to quantify the relationship between one or more predictor variables and a  response variable .
For example, an economist may have access to data on total years of education, hours spent working per week, and household income for households in a certain city.
They might then build the following multiple linear regression model:
Income = 35,870.22 + 1,500.24(years of education) + 400.34(hours spent working per week)
Here’s how to interpret the regression coefficients in this model:
For each additional year of education, total household income increases by an average of <b>$1,500.24</b> (assuming hours spent working is held constant).
For each additional hour spent working per week, total household income increases by an average of <b>$400.34</b> (assuming years of education is held constant).
Using this model, the economist can quickly understand that increasing education and total hours worked is associated with a higher household income.
They can also quantify exactly how much additional education and additional working hours affects household income.
<h3>Reason 4: Forecast Trends in the Economy</h3>
Another way that statistics is used in economics is in the form of forecasting trends.
For example, an economist may collect data on total sales (in millions) of goods in a certain country and then create a forecast for future sales:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast5.jpg"528">
Using this forecast, the economist can predict (with a certain  level of confidence ) how the economy is likely to perform in the coming months and years.
<h2><span class="orange">The Importance of Statistics in Education (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In an education setting, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows educators to understand student performance using descriptive statistics.
<b>Reason 2</b>: Statistics allows educators to spot trends in student performance using data visualizations.
<b>Reason 3</b>: Statistics allows educators to compare different teaching methods using hypothesis tests.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Understand Student Performance Using Descriptive Statistics</h3>
 Descriptive statistics  are used to <em>describe</em> data.
In an education setting, a teacher might calculate the following descriptive statistics for students in her class:
The mean score on a certain exam.
The standard deviation of scores on a certain exam.
The range of scores on a certain exam.
The percentage of students who passed a certain exam.
The 90th percentile of scores on a certain exam.
Using these metrics, the teacher can gain a better understanding of how the students in her class performed on a certain exam.
She can then decide if she needs to change her teaching method if too few students pass the exam or perhaps offer additional tutoring opportunities for students who score low on the  exam, etc.
<b>Note</b>: These types of descriptive statistics are used at every level of education. For example, a principal can use descriptive statistics to monitor exam scores of students in an entire school. Or a state department of education can use descriptive statistics to monitor exam scores for students in an entire state.
<h3>Reason 2: Spot Trends Using Data Visualization</h3>
Another common way that statistics is used in education is through data visualizations such as line charts, histograms, boxplots, pie charts and other charts.
These types of charts are often used to help educators spot trends in both class performance and individual student performance.
For example, suppose a teacher creates the following bar chart to visualize the average score on exams throughout the year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/education1.jpg"532">
Just from looking at this bar chart, the teacher will be able to tell that the average exam score of students in her class is slowly decreasing as the year goes on.
This can allow the teacher to make an improvement in her teaching methods or perhaps administer a survey to her students to ask for their feedback on her teaching methods, etc.
<h3>Reason 3: Compare Teaching Methods Using Hypothesis Tests</h3>
Another way that statistics is used in education is in the form of  hypothesis tests .
These are tests that educators can use to determine if there is a statistical significance between different teaching methods.
For example, suppose a teacher wants to determine if a certain study program affects test scores. To test this, he  randomly selects  15 students to take a pre-test. Then, he has each student use the study program for one month and then a post-test of similar difficulty.
The test scores for each of the 15 students are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/paired1.jpg"304">
To compare the difference between the mean scores on the pre-test and post-test, the teacher can use a  paired samples t-test  because for each student their pre-test score can be paired with their post-test score.
Suppose the professor uses statistical software to perform this paired samples t-test and receives the following results:
t test statistic: <b>-2.97</b>
p-value: <b>.0101</b>
In this example, the paired samples t-test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>The mean pre-test and post-test scores are equal
<b>H<sub>A</sub>: </b>The mean pre-test and post-test scores are <em>not</em> equal
Since the p-value (<b>0.0101</b>) is less than 0.05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean test score is different for students before and after participating in the study program.
<b>Note</b>: This is just one example of a hypothesis test that is used in education. Other common tests include a  one sample t-test ,  two sample t-test ,  one-way ANOVA , and  two-way ANOVA .
<h2><span class="orange">The Importance of Statistics in Finance (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of finance, statistics is important for the following reasons:
<b>Reason 1</b>: Descriptive statistics allow financial analysts to summarize data related to revenue, expenses, and profit for companies.
<b>Reason 2</b>: Regression models allow financial analysts to quantify the relationship between variables related to promotions, advertising, sales, and other variables.
<b>Reason 3</b>: Time series forecasting allows financial analysts to predict future revenue, expenses, new customers, sales, etc. for a variety of companies.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Using Descriptive Statistics to Summarize Data</h3>
 Descriptive statistics  are used to <em>describe</em> data.
Financial analysts often use descriptive statistics to summarize data related to the finances of companies.
For example, a financial analyst who works for a retail company may calculate the following descriptive statistics during one business quarter:
Mean number of daily sales
Median number of daily sales
Standard deviation of daily sales
Total revenue
Total expenses
Percentage change in new customers
Percentage of products returned by customers
Using these metrics, the analyst can gain a strong understanding of the current financial state of the company and also compare these metrics to previous quarters to understand how the metrics are trending over time.
They can then use these metrics to inform the organization on areas that could use improvement to help the company increases revenue or reduce expenses.
<h3>Reason 2: Using Regression Models to Quantify the Relationship Between Variables</h3>
Another way that statistics is used in finance is in the form of  regression models .
These are models that allow financial analysts to quantify the relationship between one or more predictor variables and a  response variable .
For example, an analyst may have access to data on total money spent on TV advertising, online advertising, and total revenue generated.
They might then build the following multiple linear regression model:
Revenue = 76.4 + 4.2(online advertising) + 0.8(TV advertising)
Here’s how to interpret the  regression coefficients  in this model:
For each additional dollar spent on online advertising, revenue increases by an average of $4.20 (assuming dollars spent on TV advertising is held constant).
For each additional dollar spent on TV advertising, revenue increases by an average of $0.80 (assuming dollars spent on online advertising is held constant).
Using this model, a financial analyst can quickly understand that money spent on online advertising results in much higher average revenue compared to money spent on TV advertising.
<h3>Reason 3: Using Time Series Forecasting to Predict Future Values</h3>
Another way that statistics is used in finance is in the form of time series forecasting.
For example, a financial analyst may use historical data to forecast the total revenue, expenses, new customers, product sales, etc. for a company.
By forecasting these values, the analyst can inform the company on how many new customers to expect, how many new employees to hire based on increase revenue, and a variety of other metrics.
<h2><span class="orange">The Importance of Statistics in Healthcare (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of healthcare, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows healthcare professionals to monitor the health of individuals using descriptive statistics.
<b>Reason 2</b>: Statistics allows healthcare professionals to quantify the relationship between variables using regression models.
<b>Reason 3</b>: Statistics allows healthcare professionals to compare the effectiveness of different medical procedures using hypothesis tests.
<b>Reason 4</b>: Statistics allows healthcare professionals to understand the effect of lifestyle choices on health using incidence rate ratio.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Monitor the Health of Individuals Using Descriptive Statistics</h3>
 Descriptive statistics  are used to <em>describe</em> data.
Healthcare professionals often calculate the following descriptive statistics for a given individual:
Mean resting heart rate.
Mean blood pressure.
Fluctuation in weight during a certain time period.
Using these metrics, healthcare professionals can gain a better understanding of the overall health of individuals.
They can then use these metrics to inform individuals on ways they can improve their health or even prescribe specific medications based on the health of the individual.
<h3>Reason 2: Quantify Relationship Between Variables Using Regression Models</h3>
Another way that statistics is used in healthcare is in the form of  regression models .
These are models that allow healthcare professionals to quantify the relationship between one or more predictor variables and a  response variable .
For example, a healthcare professional may have access to data on total hours spent exercising per day, total time spent sitting per day, and overall  weight of individuals.
They might then build the following multiple linear regression model:
Weight = 124.33 – 15.33(hours spent exercising per day) + 1.04(hours spent sitting per day)
Here’s how to interpret the  regression coefficients  in this model:
For each additional hour spent exercising per day, total weight decreases by an average of 15.33 pounds (assuming hours spent sitting is held constant).
For each additional hour spent sitting per day, total weight increases by an average of 1.04 pounds (assuming hours spent exercising is held constant).
Using this model, a healthcare professional can quickly understand that more time spent exercising is associated with lower weight and more time spent sitting is associated with higher weight.
They can also quantify exactly how much exercise and sitting affect weight.
<h3>Reason 3: Compare Medical Procedures Using Hypothesis Tests</h3>
Another way that statistics is used in healthcare is in the form of  hypothesis tests .
These are tests that healthcare professionals can use to determine if there is a statistical significance between different medical procedures or treatments.
For example, suppose a doctor believes that a new drug is able to reduce blood pressure in obese patients. To test this, he may measure the blood pressure of 40 patients before and after using the new drug for one month.
He then performs a  paired samples t- test  using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean blood pressure is the same before and after using the drug)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> &lt; μ<sub>before</sub> (the mean blood pressure is less after using the drug)
If the  p-value  of the test is less than some significance level (e.g. α = .05), then he can reject the null hypothesis and conclude that the new drug leads to reduced blood pressure.
<b>Note</b>: This is just one example of a hypothesis test that is used in healthcare. Other common tests include a  one sample t-test ,  two sample t-test ,  one-way ANOVA , and  two-way ANOVA .
<h3>Reason 4: Understand Effects of Lifestyle Choices on Health Using Incidence Rate Ratio</h3>
An <b>incidence rate ratio</b> allows healthcare professionals to compare the incident rate between two different groups.
For example, suppose it’s known that people who smoke develop lung cancer at a rate of 7 per 100 person-years.
Conversely, suppose it’s known that people who do not smoke develop lung cancer at a rate of 1.5 per 100 person-years.
We would calculate the incidence rate ratio (often abbreviated IRR) as:
IRR = Incidence rate among smokers / Incidence rate among non-smokers
IRR = (7/100) / (1.5/100)
IRR = <b>4.67</b>
Here’s how a healthcare professional would interpret this value: The lung cancer rate among smokers is 4.67 times as high as the rate among non-smokers.
Using this simple calculation, healthcare professionals can gain a good understanding of how different lifestyle choices (like smoking) affect health in individuals.
<h2><span class="orange">The Importance of Statistics in Nursing (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of nursing, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows nurses to understand how to interpret descriptive statistics like mean, median, standard deviation, range, and percentiles.
<b>Reason 2</b>: Statistics allows nurses to understand how to interpret the findings from recent clinical trials and how to communicate these findings to patients.
<b>Reason 3</b>: Statistics allows nurses to understand how to interpret odds ratios, which can give patients an idea of the risk factors of different medications or lifestyle choices.
In the rest of this article, we elaborate on each of these reasons.
<h2>Reason 1: Understand How to Interpret Descriptive Statistics</h2>
 Descriptive statistics  are used to <em>describe</em> data.
In a medical setting, a nurse might have access to the following descriptive statistics for a patient:
The mean weight of the patient during a given time interval.
The standard deviation of weight of the patient during a given time interval.
The percentile of height, weight, blood pressure, and heart rate for an patient.
Using these metrics, the nurse can gain a better understanding of the overall health of a given patient and give recommendations to the patient for improving their health.
For example, suppose a nurse can see that a patient is in the 93rd percentile of weight for their age group.
By taking a statistics course, a nurse would know that this means the individual has a weight that is greater than 93% of all individuals in their same age group.
This is a clear indication that the individual is not at a healthy weight and the nurse can proceed to recommend a certain medication or lifestyle change that could positively affect the individual.
<h2>Reason 2: Understand How to Interpret Findings from Clinical Trials</h2>
Another important reason for nurses to understand statistics is so that they know how to interpret the findings from clinical trials.
For example, suppose researchers conduct a new clinical trial to determine if a new medication affects weight loss.
Suppose the following results are reported from the trial in a medical journal:
There was a significant difference in average weight loss between the new medication (M = 5.75, SD = 1.25) and the placebo (M = 0.23, SD = 0.97); p = .021.
A nurse who has taken a statistics course will known that the<em> p</em> reported in the results represents the  p-value  from a two sample t-test.
And since this p-value is less than .05, they’ll know that the findings from the study are statistically significant, which indicates that there is a statistically significant difference in weight loss among patients who took the new medication compared to those who took a placebo.
By understanding how to interpret these findings, they can relay this information to patients who are considering taking the new medication for weight loss.
<b>Note</b>: This is just one example of a statistical test that may be performed in clinical trials. Other common tests include a  one sample t-test ,  paired samples t-test ,  one-way ANOVA , and  two-way ANOVA .
<h2>
<b>Reason 3: Understand How to Interpret Odds Ratios</b>
</h2>
Another important reason for nurses to understand statistics is so they know how to interpret odds ratios.
An <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group compared to the odds of an event occurring in a control group.
For example, suppose researchers want to understand the relationship between a mother’s age and the probability of having a baby with a healthy birthweight.
To explore this, they perform logistic regression using age as a predictor variable and healthy birthweight (no = 0, yes =1) as a  response variable .
Suppose they collect data for 200 mothers and fit a logistic regression model. Here are the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/oddsLess1.png">
The odds ratio for the predictor variable <em>age</em> is less than 1. This means that each additional increase of one year in age is associated with a decrease in the odds of a mother having a healthy baby.
In particular, we can use the following formula to quantify the change in the odds:
Change in Odds %: (OR-1) * 100
For example, the odds ratio (OR) for age is 0.92. Thus, we could calculate:
Change in Odds %: (0.92 – 1) * 100 = <b>-8%</b>
This means that each additional increase of one year in age is associated with an <b>8% decrease</b> in the odds of a mother having a healthy baby.
By understanding how to interpret this odds ratio, a nurse can clearly communicating this finding to a potential mother.
<h2>Additional Resources</h2>
The following articles explain the importance of statistics in other fields:
 The Importance of Statistics in Business 
 The Importance of Statistics in Education 
 The Importance of Statistics in Economics 
 The Importance of Statistics in Research 
 The Importance of Statistics in Healthcare 
<h2><span class="orange">The Importance of Statistics in Psychology (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of psychology, statistics is important for the following reasons:
<b>Reason 1</b>: Descriptive statistics allow psychologists to summarize data related to human performance, happiness, and other metrics.
<b>Reason 2</b>: Regression models allow psychologists to quantify the relationship between variables related to human performance, happiness, and other metrics.
<b>Reason 3</b>: Hypothesis tests allow psychologists to compare the effectiveness of different methods, techniques, and procedures on human performance, happiness, and other metrics.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Using Descriptive Statistics to Summarize Data</h3>
 Descriptive statistics  are used to <em>describe</em> data.
Psychologists often use descriptive statistics to summarize data related to individuals.
For example, an  industrial-organizational psychologist  might calculate the following descriptive statistics for individuals who work at a certain company:
Overall satisfaction with salary (e.g. scale of 1-7)
Overall satisfaction with workplace culture
Overall satisfaction with working hours
Using these metrics, an I/O psychologist can gain a better understanding of how satisfied employees are at the company.
They can then use these metrics to inform the organization on areas that could use improvement to make the workplace a more enjoyable environment for the employees.
<h3>Reason 2: Using Regression Models to Quantify the Relationship Between Variables</h3>
Another way that statistics is used in psychology is in the form of  regression models .
These are models that allow psychologists to quantify the relationship between one or more predictor variables and a  response variable .
For example, a psychologist may have access to data on total hours spent exercising per day, total hours spent working per day, and overall  happiness (e.g. scale of 0-100) of individuals.
They might then build the following multiple linear regression model:
Happiness = 76.4 + 9.3(hours spent exercising per day) – 0.4(hours spent working per day)
Here’s how to interpret the  regression coefficients  in this model:
For each additional hour spent exercising per day, overall happiness increases by an average of 9.3 points (assuming hours spent working is held constant).
For each additional hour spent working per day, overall happiness decreases by an average of 0.4 points (assuming hours spent exercising is held constant).
Using this model, a psychologist can quickly understand that more time spent exercising is associated with increased overall happiness and more time spent working is associated with lower overall happiness.
They can also quantify exactly how much exercise and working affect overall happiness.
<h3>Reason 3: Using Hypothesis Tests to Compare Methods</h3>
Another way that statistics is used in psychology is in the form of  hypothesis tests .
These are tests that psychologists can use to determine if there is a statistical significance between different methods, techniques, or procedures.
For example, suppose a sports psychologist believes that a new workout method is able to increase the mental well-being of college basketball players. To test this, he may measure the well-being (e.g. scale of 1-7) of 40 players before and after implementing the new workout method for one month.
He can then perform a  paired samples t- test  using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean well-being is the same before and after using the method)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> > μ<sub>before</sub> (the mean well-being is greater after using the method)
If the  p-value  of the test is less than some significance level (e.g. α = .05), then he can reject the null hypothesis and conclude that the new method leads to increased well-being among players
<b>Note</b>: This is just one example of a hypothesis test that is used in psychology. Other common tests include a  one sample t-test ,  two sample t-test ,  one-way ANOVA , and  two-way ANOVA .
<h2><span class="orange">The Importance of Statistics in Research (With Examples)</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
In the field of research, statistics is important for the following reasons:
<b>Reason 1</b>: Statistics allows researchers to design studies such that the findings from the studies can be extrapolated to a larger population.
<b>Reason 2</b>: Statistics allows researchers to perform hypothesis tests to determine if some claim about a new drug, new procedure, new manufacturing method, etc. is true.
<b>Reason 3</b>: Statistics allows researchers to create confidence intervals to capture uncertainty around population estimates.
In the rest of this article, we elaborate on each of these reasons.
<h3>Reason 1: Statistics Allows Researchers to Design Studies</h3>
Researchers are often interested in answering questions about  populations  like:
What is the average weight of a certain species of bird?
What is the average height of a certain species of plant?
What percentage of citizens in a certain city support a certain law?
One way to answer these questions is to go around and collect data on every single individual in the population of interest.
However, this is typically too costly and time-consuming which is why researchers instead take a <b>sample </b>of the population and use the data from the sample to draw conclusions about the population as a whole.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CIprop1.png">
There are many different methods researchers can potentially use to obtain individuals to be in a sample. These are known as <b>sampling methods</b>.
There are two classes of sampling methods:
<b>Probability sampling methods</b>: Every member in a population has an equal probability of being selected to be in the sample.
<b>Non-probability sampling methods</b>: Not every member in a population has an equal probability of being selected to be in the sample.
By using probability sampling methods, researchers can maximize the chances that they obtain a sample that is  representative  of the overall population.
This allows researchers to extrapolate the findings from the sample to the overall population.
Read more about the two classes of sampling methods  here .
<h3>Reason 2: Statistics Allows Researchers to Perform Hypothesis Tests</h3>
Another way that statistics is used in research is in the form of  hypothesis tests .
These are tests that researchers can use to determine if there is a statistical significance between different medical procedures or treatments.
For example, suppose a scientist believes that a new drug is able to reduce blood pressure in obese patients. To test this, he measures the blood pressure of 30 patients before and after using the new drug for one month.
He then performs a  paired samples t- test  using the following hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>after</sub> = μ<sub>before</sub> (the mean blood pressure is the same before and after using the drug)
<b>H<sub>A</sub>:</b> μ<sub>after</sub> &lt; μ<sub>before</sub> (the mean blood pressure is less after using the drug)
If the  p-value  of the test is less than some significance level (e.g. α = .05), then he can reject the null hypothesis and conclude that the new drug leads to reduced blood pressure.
<b>Note</b>: This is just one example of a hypothesis test that is used in research. Other common tests include a  one sample t-test ,  two sample t-test ,  one-way ANOVA , and  two-way ANOVA .
<h3>Reason 3: Statistics Allows Researchers to Create Confidence Intervals</h3>
Another way that statistics is used in research is in the form of  confidence intervals .
A confidence interval is a range of values that is likely to contain a population parameter with a certain level of confidence.
For example, suppose researchers are interested in estimating the mean weight of a certain species of turtle.
Instead of going around and weighing every single turtle in the population, researchers may instead take a simple random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
Using the  confidence interval for a mean formula , researchers may then construct the following 95% confidence interval:
<b>95% Confidence Interval: </b>300 +/-  1.96*(18.5/√25) = <b>[292.75, 307.25]</b>
The researchers would then claim that they’re 95% confident that the true mean weight for this population of turtles is between 292.75 pounds and 307.25 pounds.
<h2>Additional Resources</h2>
The following articles explain the importance of statistics in other fields:
 The Importance of Statistics in Healthcare 
 The Importance of Statistics in Nursing 
 The Importance of Statistics in Business 
 The Importance of Statistics in Economics 
 The Importance of Statistics in Education 
<h2><span class="orange">How to Impute Missing Values in R (With Examples)</span></h2>
Often you may want to replace missing values in the columns of a data frame in R with the mean or the median of that particular column.
To replace the missing values in a single column, you can use the following syntax:
<b>df$col[is.na(df$col)] &lt;- mean(df$col, na.rm=TRUE)
</b>
And to replace the missing values in multiple columns, you can use the following syntax:
<b>for(i in 1:ncol(df)) {
  df[ , i][is.na(df[ , i])] &lt;- mean(df[ , i], na.rm=TRUE)
}
</b>
This tutorial explains exactly how to use these functions in practice.
<h3>Example 1: Replace Missing Values with Column Means</h3>
The following code shows how to replace the missing values in the first column of a data frame with the mean value of the first column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, NA, NA, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#replace missing values in first column with mean of first column
df$var1[is.na(df$var1)] &lt;- mean(df$var1, na.rm=TRUE)
#view data frame with missing values replaced
df
      var1 var2 var3 var4
1 1.000000    7    3    1
2 3.333333    7    3    1
3 3.333333    8    6    2
4 4.000000    3    6    8
5 5.000000    2    8    9
</b>
The mean value in the first column was <b>3.333</b>, so the missing values in the first column were replaced with <b>3.333</b>.
The following code shows how to replace the missing values in each column with the mean of its own column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, NA, NA, 4, 5), var2=c(7, 7, 8, NA, 2), var3=c(NA, 3, 6, NA, 8), var4=c(1, 1, 2, 8, 9))
#replace missing values in each column with column means
for(i in 1:ncol(df)) {
  df[ , i][is.na(df[ , i])] &lt;- mean(df[ , i], na.rm=TRUE)
}
#view data frame with missing values replaced
df
      var1 var2     var3 var4
1 1.000000    7 5.666667    1
2 3.333333    7 3.000000    1
3 3.333333    8 6.000000    2
4 4.000000    6 5.666667    8
5 5.000000    2 8.000000    9</b>
<h3>Example 2: Replace Missing Values with Column Medians</h3>
The following code shows how to replace the missing values in the first column of a data frame with the median value of the first column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, NA, NA, 4, 5), var2=c(7, 7, 8, NA, 2), var3=c(NA, 3, 6, NA, 8), var4=c(1, 1, 2, 8, 9))
#replace missing values in first column with median of first column
df$var1[is.na(df$var1)] &lt;- median(df$var1, na.rm=TRUE)
#view data frame with missing values replaced
df
  var1 var2 var3 var4
1    1    7   NA    1
2    4    7    3    1
3    4    8    6    2
4    4   NA   NA    8
5    5    2    8    9
</b>
The median value in the first column was <b>4</b>, so the missing values in the first column were replaced with <b>4</b>.
The following code shows how to replace the missing values in each column with the median of its own column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, NA, NA, 4, 5), var2=c(7, 7, 8, NA, 2), var3=c(NA, 3, 6, NA, 8), var4=c(1, 1, 2, 8, 9))
#replace missing values in each column with column medians
for(i in 1:ncol(df)) {
  df[ , i][is.na(df[ , i])] &lt;- median(df[ , i], na.rm=TRUE)
}
#view data frame with missing values replaced
df
  var1 var2 var3 var4
1    1    7    6    1
2    4    7    3    1
3    4    8    6    2
4    4    7    6    8
5    5    2    8    9</b>
<h2><span class="orange">How to Use %in% Operator in R (With Examples)</span></h2>
The <b>%in% </b>operator in R allows you to determine whether or not an element belongs to a vector or data frame.
This tutorial provides three examples of how to use this function in different scenarios.
<h3>Example 1: Use %in% with Vectors</h3>
We can use the <b>%in%</b> operator to determine how many elements of one vector belong to another vector:
<b>#define two vectors of data
data1 &lt;- c(3, 5, 7, 7, 14, 19, 22, 25)
data2 &lt;- c(1, 2, 3, 4, 5)
#produce new vector that contains elements of data1 that are in data2
data1[data1 %in% data2]
[1] 3 5
</b>
We can see that the values <b>3 </b>and <b>5 </b>are the only elements from the vector titled <em>data2 </em>that are in the vector titled <em>data1</em>.
<h3>Example 2: Use %in% to filter Data Frames</h3>
We can also use the <b>%in%</b> operator to filter for rows in a data frame that contain certain values:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'C'), points=c(67, 72, 77, 89, 84, 97), assists=c(14, 16, 12, 22, 25, 20))
#view data frame
df
  team points assists
1    A     67      14
2    A     72      16
3    B     77      12
4    B     89      22
5    B     84      25
6    C     97      20
#produce new data frame that only contains rows where team is 'B'
df_new &lt;- df[df$team %in% c('B'), ]
df_new
  team points assists
3    B     77      12
4    B     89      22
5    B     84      25
#produce new data frame that only contains rows where team is 'B' or 'C'
df_new2 &lt;- df[df$team %in% c('B', 'C'), ]
df_new2
  team points assists
3    B     77      12
4    B     89      22
5    B     84      25
6    C     97      20
</b>
<h3>Example 3: Use %in% to Create Data Frame Columns</h3>
We can also use the <b>%in%</b> operator to create new data frame columns.
For example, the following code shows how to create a new column titled <em>division </em>that places teams ‘A’ and ‘C’ in the ‘East’ and teams ‘B’ in the ‘West’:
<b>library(dplyr)
#define data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'C'), points=c(67, 72, 77, 89, 84, 97), assists=c(14, 16, 12, 22, 25, 20))
#view data frame
df
  team points assists
1    A     67      14
2    A     72      16
3    B     77      12
4    B     89      22
5    B     84      25
6    C     97      20
#create new column called <em>division</em>
df$division = if_else(df$team %in% c('A', 'C'), 'East', 'West')
df
  team points assists division
1    A     67      14     East
2    A     72      16     East
3    B     77      12     West
4    B     89      22     West
5    B     84      25     West
6    C     97      20     East</b>
<h2><span class="orange">What is Incidence Rate Ratio? (Definition & Example)</span></h2>
An <b>incidence rate ratio</b> allows us to compare the incident rate between two different groups.
For example, suppose it’s known that people who smoke develop lung cancer at a rate of 7 per 100 person-years.
Conversely, suppose it’s known that people who do not smoke develop lung cancer at a rate of 1.5 per 100 person-years.
We would calculate the incidence rate ratio (often abbreviated IRR) as:
IRR = Incidence rate among smokers / Incidence rate among non-smokers
IRR = (7/100) / (1.5/100)
IRR = <b>4.67</b>
Here’s how we would interpret this value: The lung cancer rate among smokers is 4.67 times as high as the rate among non-smokers.
<h3>How to Interpret Incidence Rate Ratios</h3>
Here is how to interpret an incidence rate ratio (IRR):
<b>IRR Less than 1:</b> This indicates that the incident rate is lower in an exposed group compared to an unexposed group. 
For example, if smokers developed lung cancer at a rate of 7 per 100 person-years and non-smokers developed lung cancer at a rate of 10 per 100 person-years, then the IRR would be 7/10 = <b>0.7</b>.
This would mean that smokers experience the incidence (lung cancer) less often than non-smokers.
<b>IRR Equal to 1:</b> This indicates that the incident rate is equal among those in an exposed group and those in an unexposed group. 
For example, if smokers developed lung cancer at a rate of 7 per 100 person-years and non-smokers developed lung cancer at a rate of 7 per 100 person-years, then the IRR would be 7/7 = <b>1</b>.
This would mean that smokers experience the incidence (lung cancer) just as often as non-smokers.
<b>IRR Greater than 1:</b> This indicates that the incident rate is greater in an exposed group compared to an unexposed group. 
For example, if smokers developed lung cancer at a rate of 7 per 100 person-years and non-smokers developed lung cancer at a rate of 1.5 per 100 person-years, then the IRR would be 7/1.5 = <b>4.67</b>.
This would mean that smokers experience the incidence (lung cancer) more often than non-smokers.
<h3>Why is Incidence Rate Ratio Useful?</h3>
Incidence rate ratio is a useful metric because it’s so easy to interpret and it allows us to immediately understand if exposure to something increases or decreases the rate of some incidence.
For example, just knowing that the IRR of smoking is <b>4.67</b> tells us that lung cancer occurs far more often in smokers compared to non-smokers.
We also know that the larger the value for IRR, the greater the ratio of some incident in an exposed group compared to an unexposed group.
Conversely, the closer IRR is to 1 the smaller the difference in the incident rate between an exposed group and an unexposed group.
<h3>Example: Calculating Incidence Rate Ratio</h3>
Suppose a doctor collects data on how often individuals develop a disease, based on their BMI (body mass index). 
The following table summarizes the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/IRR1.png">
Using this table, we can calculate the following metrics:
Incidence Rate Ratio of (BMI > 30) vs. (BMI 25-30) = 1.48 / 1.12 = <b>1.32</b>
Interpretation: The disease rate among individuals with BMI > 30 is 1.32 times as high as the rate among individuals with BMI between 25 and 30.
Incidence Rate Ratio of (BMI > 30) vs. (BMI &lt; 25) = 1.48 / 0.54 = <b>2.74</b>
Interpretation: The disease rate among individuals with BMI > 30 is 2.74 times as high as the rate among individuals with BMI less than 25.
Incidence Rate Ratio of (BMI 25-30) vs. (BMI &lt; 25) = 1.12 / 0.54 = <b>2.07</b>
Interpretation: The disease rate among individuals with BMI between 25 and 30 is 2.07 times as high as the rate among individuals with BMI less than 25.
<h2><span class="orange">Independent vs. Dependent Variables: What’s the Difference?</span></h2>
In an experiment, there are two main variables:
<b>The independent variable: </b>the variable that an experimenter changes or controls so that they can observe the effects on the dependent variable.
<b>The dependent variable: </b>the variable being measured in an experiment that is “dependent” on the independent variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/ind_dep1.png">
In an experiment, an experimenter is interested in seeing how the dependent variable changes as a result of the independent being changed or manipulated in some way.
<h2>Example of an Independent and Dependent Variable</h2>
For example, a researcher might change the amount of water they provide to a certain plant to observe how it affects the growth rate of the plant.
In this example, the amount of water given to the plant is controlled by the researcher and, thus, is <b>the independent variable</b>. The growth rate is <b>the dependent variable</b> because it is directly dependent on the amount of water that the plant receives and it’s the variable we’re interested in measuring.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/ind_dep2.png">
<h2>How to Remember the Difference Between Independent and Dependent Variables</h2>
An easy way to remember the difference between independent and dependent variables is to insert the two variables into the following sentence in such a way that it makes sense:
Changing <b>(independent variable)</b> affects the value of<b> (dependent variable)</b>.
For example, it would make sense to say:
Changing <b>the amount of water</b> affects the value of<b> the plant growth rate</b>.
This is how we know that amount of water is the independent variable and plant growth rate is the dependent variable.
If we tried reversing the positions of these two variables, the sentence wouldn’t make sense:
Changing <b>the plant growth rate</b> affects the value of<b> the amount of water</b>.
Thus, we know that we must have the independent and dependent variables switched around.
<h2>More Examples </h2>
Here are a few more examples of independent and dependent variables.
<h3>Example 1:</h3>
A marketer changes the amount of money they spend on advertisements to see how it affects total sales.
<b>Independent variable: </b>amount spent on advertisements
<b>Dependent variable: </b>total sales
<h3>Example 2:</h3>
A doctor changes the dose of a particular medicine to see how it affects the blood pressure of a patient.
<b>Independent variable: </b>dosage level of medicine
<b>Dependent variable: </b>blood pressure
<h3>Example 3:</h3>
A researcher changes the version of a study guide given to students to see how it affects exam scores.
<b>Independent variable: </b>the version of the study guide
<b>Dependent variable: </b>exam scores
<h2>Independent vs. Dependent Variables on a Graph</h2>
When we create a graph, the independent variable will go on the x-axis and the dependent variable will go on the y-axis.
For example, suppose a researcher provides different amounts of water for 20 different plants and measures the growth rate of each plant. The following scatterplot shows the amount of water and the growth rate for each plant:
The independent variable (amount of water) is shown on the x-axis while the dependent variable (growth rate) is shown on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/ind_dep3.png">
<h2><span class="orange">Excel: Use INDEX and MATCH to Return Multiple Values Vertically</span></h2>
You can use the following basic formula with INDEX and MATCH to return multiple values vertically in Excel:
<b>=IFERROR(INDEX($B$2:$B$11,SMALL(IF($D$2=$A$2:$A$11,ROW($A$2:$A$11)-ROW($A$2)+1),ROW(1:1))),"")
</b>
This particular formula returns all of the values in the range <b>B2:B11</b> where the corresponding value in the range <b>A2:A11</b> is equal to the value in cell <b>D2</b>.
The following example shows how to use this formula in practice.
<h2>Example: Use INDEX and MATCH to Return Multiple Values Vertically</h2>
Suppose we have the following dataset in Excel that shows the name and team of various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/indexmatch1.jpg"487">
Now suppose we would like to return the names of each player who is on the Mavs team.
To do so, we can type the following formula into cell <b>E2</b>:
<b>=IFERROR(INDEX($B$2:$B$11,SMALL(IF($D$2=$A$2:$A$11,ROW($A$2:$A$11)-ROW($A$2)+1),ROW(1:1))),"")</b>
Once we press <b>Enter</b>, the name of the first player on the Mavs team will be returned:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/indexmatch2.jpg"547">
We can then drag and fill this formula down to the remaining cells in column E to display the names of each player on the Mavs team:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/indexmatch3.jpg">
Notice that the names of each of the four players on the Mavs team are now shown.
Note that if you change the name of the team in cell <b>D2</b>, the names of the players shown in column E will change accordingly:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/indexmatch4.jpg"504">
The names of each of the three players on the Nets team are now shown.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 Excel: How to Perform a VLOOKUP with Two Lookup Values 
 Excel: How to Use VLOOKUP to Return Multiple Columns 
 Excel: How to Use VLOOKUP to Return All Matches 
<h2><span class="orange">Python: How to Find Index of Max Value in List</span></h2>
You can use the following syntax to find the index of the max value of a list in Python:
<b>#find max value in list
max_value = max(list_name)
#find index of max value in list 
max_index = list_name.index(max_value)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Find Index of Max Value in List</h3>
The following code shows how to find the max value in a list along with the index of the max value:
<b>#define list of numbers
x = [9, 3, 22, 7, 15, 16, 8, 8, 5, 2]
#find max value in list
max_value = max(x)
#find index of max value in list
max_index = x.index(max_value)
#display max value
print(max_value)
22
#display index of max value
print(max_index)
2</b>
The maximum value in the list is <b>22</b> and we can see that it’s located at index value <b>2</b> in the list.
<b>Note:</b> Index values start at 0 in Python.
<h3>
<b>Example 2: Find Index of Max Value in List with Ties</b>
</h3>
The following code shows how to find the max value in a list along with the index of the max value when there are <em>multiple</em> max values.
<b>#define list of numbers with multiple max values
x = [9, 3, 22, 7, 15, 16, 8, 8, 5, 22]
#find max value in list
max_value = max(x)
#find indices of max values in list
indices = [index for index, val in enumerate(x) if val == max_value]
#display max value
print(max_value)
22
#display indices of max value
print(indices)
[2, 9]</b>
The maximum value in the list is <b>22</b> and we can see that it occurs at index values <b>2</b> and <b>9</b> in the list.
<h2><span class="orange">How to Find the Indicated Area Under the Standard Normal Curve</span></h2>
One of the most common questions in elementary statistics is:
<b>“Find the indicated area under the standard normal curve.”</b>
These types of questions can be answered by using values found in the  z table . This tutorial explains how to use the z table to answer the following four types of these questions:
Find the area under the curve less than some value.
Find the area under the curve greater than some value.
Find the area under the curve between two values.
Find the area under the curve outside of two values.
<h3>Example 1: Find the Indicated Area Less Than Some Value</h3>
<b>Question:</b> Find the area under the standard normal curve to the left of z = 1.26.
<b>Solution: </b>To answer this question, we simply need to look up the value in the  z table  that corresponds to 1.26:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/indicated1.png">
The area under the standard normal curve to the left of z = 1.26 is <b>0.8962</b>.
<h3>Example 2: Find the Indicated Area Greater Than Some Value</h3>
<b>Question:</b> Find the area under the standard normal curve to the right of z = -1.81.
<b>Solution: </b>To answer this question, we simply need to look up the value in the  z table  that corresponds to -1.81 and subtract it from 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/indicated2.png">
The area under the standard normal curve to the right of z = -1.81 is 1 – .0351 –  <b>0.9649</b>.
<h3>Example 3: Find the Indicated Area Between Two Values</h3>
<b>Question:</b> Find the area under the standard normal curve between z = -1.81 and z = 1.26
<b>Solution: </b>To answer this question, we simply need to subtract the area to the left of z = -1.81 from the area to the left of 1.26.
In the previous examples, we found that the area to the left of z = -1.81 was .0351 and the area to the left of z = 1.26 was .8962. 
Thus, the area under the standard normal curve between z = -1.81 and z = 1.26 is .8962 – .0351 = <b>0.8611</b>.
<h3>Example 4: Find the Indicated Area Outside of Two Values</h3>
<b>Question:</b> Find the area under the standard normal curve outside of z = -1.81 and z = 1.26
<b>Solution: </b>To answer this question, we need to add up the area to the left of z = -1.81 and the area to the right of z = 1.26.
The area to the left of z = -1.81 is <b>.0351 </b>and the area to the right of z = 1.26 is 1-.8962 = <b>.1038</b>.
Thus, the area between z = -1.81 and z = 1.26 is .0351 + .1038 = <b>.1389</b>.
<h3>Bonus: The Standard Normal Curve Area Calculator</h3>
You can use  this calculator  to automatically find the area under the standard normal curve between two values.
<h2><span class="orange">Inference vs. Prediction: What’s the Difference?</span></h2>
Often in the field of statistics we’re interested in using data for one of two reasons:
<b>(1) Inference:</b> We want to understand the nature of the relationship between the predictor variables and the  response variable  in an existing dataset.
<b>(2) Prediction:</b> We want to use an existing dataset to build a model that predicts the value of the response variable of a new observation.
For example, suppose we have the following dataset that contains information about houses:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/inf1-1.png">
<b>An example of inference:</b>
Suppose we build a  multiple linear regression model  using square feet, number of bedrooms, and number of bathrooms as predictor variables and price as the response variable.
We could then use the regression coefficients to understand the average change in price associated with a one unit change in each of the predictor variables.
For example, we could understand how much price changes (on average) with each additional bedroom, each additional bathroom, and each additional square foot.
<b>An example of prediction:</b>
We could build the same multiple linear regression model and use it to predict how much a new home will be worth based on its square footage, number of bedrooms, and number of bathrooms.
For example, we could use the model to predict the price of a new home that has 3 bedrooms, 3 bathrooms, and 2,000 square feet.
We could then compare our prediction with the actual listing price and assess whether or not the home appears to be under- or over-valued.
The following examples illustrate the difference between inference and prediction in different scenarios:
<h3>Example 1: Inference vs. Prediction in Sports</h3>
Suppose we have the following dataset that contains information about professional basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/inf2.png">
<b>An example of inference:</b>
Suppose we build a multiple linear regression model using points, rebounds, and assists as predictor variables and wins as the response variable.
We could then use the model to understand how much the number of wins changes (on average) with each additional point, rebound, and assist.
<b>An example of prediction:</b>
We could build the same multiple linear regression model and use it to predict how many wins a team will have based on their number of points, rebounds, and assists.
For example, we could use the model to predict the number of wins that a team with 90 points, 40 rebounds, and 30 assists will have.
<h3>Example 2: Inference vs. Prediction in Business</h3>
Suppose we have the following dataset that contains information about annual revenue (in millions) for various businesses:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/inf3.png">
<b>An example of inference:</b>
Suppose we build a multiple linear regression model using advertising spend, number of employees, and total acquisitions as predictor variables and annual revenue as the response variable.
We could then use the model to understand how much the total annual revenue changes (on average) with each additional dollar spent on advertising, each additional employee, and each additional acquisition.
<b>An example of prediction:</b>
We could build the same multiple linear regression model and use it to predict the annual revenue of a business based on their total marketing spend, number of employees, and total acquisitions.
For example, we could use the model to predict the annual revenue of a business that spends $25 million on advertising, has 40 employees, and has had 2 acquisitions.
<h2><span class="orange">What is an Influential Observation in Statistics?</span></h2>
In statistics, an <b>influential observation</b> is an observation in a dataset that, when removed, dramatically changes the  coefficient estimates  of a regression model.
The most common way to measure the influence of observations is to use <b>Cook’s distance</b>, which quantifies how much all of the fitted values in a regression model change when the i<sup>th</sup> observation is deleted.
As a rule of thumb, any observation with a Cook’s distance greater than 1 is considered to be an observation with high leverage.
The following example shows how to calculate and interpret Cook’s distance for a given dataset to detect potential influential observations.
<h3>Example: Detecting Influential Observations</h3>
Suppose we have the following dataset with 14 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/influence1.png">
Now suppose we fit a  simple linear regression model . The regression output is shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/influence2.png">
Using statistical software, we can calculate the following values for Cook’s distance for each observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/influence3.png">
Notice that the last observation has a value significantly greater than 1 for Cook’s distance, which tells us that it’s an influential observation.
Suppose we remove this value from the dataset and fit a new simple linear regression model. The output for this model is shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/influence4.png">
Notice that the regression coefficients for the intercept and x both changed dramatically. This tells us that removing the influential observation from the dataset completely changed the fitted regression model.
The following plots show the difference between these two fitted regression equations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/influence5.png">
Notice how much the one influential observation changes the regression line. By removing this observation, we were able to find a regression line that fits the data much more closely.
<h3>Notes</h3>
It’s important to note that Cook’s distance should be used as a way to <em>identify</em> potentially influential observations. However, just because an observation is influential doesn’t necessarily mean that it should be deleted from the dataset.
First, you should verify that the observation isn’t a result of a data entry error or some other odd occurrence. If it turns out to be a legit value, you can then decide to deal with it in one of the following ways:
Delete it from the dataset.
Leave it in the dataset.
Replace it with an alternative value like the mean or median.
Depending on your specific scenario, one of these options may make more sense than the others.
<h3>How to Calculate Cook’s Distance in Practice</h3>
The following tutorials explain how to calculate Cook’s distance for a given dataset in Python and R:
 How to Calculate Cook’s Distance in Python 
 How to Calculate Cook’s Distance in R 
<h2><span class="orange">How to Do an Inner Join in R (With Examples)</span></h2>
There are two common ways to perform an inner join in R:
<b>Method 1: Use Base R</b>
<b>merge(df1, df2, by='column_to_join_on')
</b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
inner_join(df1, df2, by='column_to_join_on')</b>
Both methods will produce the same result, but the dplyr method will tend to work faster on extremely large datasets.
The following examples show how to use each of these functions in practice with the following data frames:
<b>#define first data frame
df1 = data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'), points=c(18, 22, 19, 14, 14, 11, 20, 28))
df1
  team points
1    A     18
2    B     22
3    C     19
4    D     14
5    E     14
6    F     11
7    G     20
8    H     28
#define second data frame
df2 = data.frame(team=c('A', 'B', 'C', 'D', 'G', 'H'), assists=c(4, 9, 14, 13, 10, 8))
df2
  team assists
1    A       4
2    B       9
3    C      14
4    D      13
5    G      10
6    H       8</b>
<h3>Example 1: Inner Join Using Base R</h3>
We can use the <b>merge()</b> function in base R to perform an inner join, using the ‘team’ column as the column to join on:
<b>#perform inner join using base R
df3 &lt;- merge(df1, df2, by='team')
#view result
df3
  team points assists
1    A     18       4
2    B     22       9
3    C     19      14
4    D     14      13
5    G     20      10
6    H     28       8
</b>
Notice that only the teams that were in both datasets are kept in the final dataset.
<h3>Example 2: Inner Join Using dplyr</h3>
We can use the <b>inner_join()</b> function from the  dplyr  package to perform an inner join, using the ‘team’ column as the column to join on:
<b>library(dplyr)
#perform inner join using dplyr 
df3 &lt;- inner_join(df1, df2, by='team')
#view result
df3
  team points assists
1    A     18       4
2    B     22       9
3    C     19      14
4    D     14      13
5    G     20      10
6    H     28       8</b>
Notice that this matches the result we obtained from using the <b>merge()</b> function in base R.
<h2><span class="orange">How to Do an Inner Join in Pandas (With Example)</span></h2>
You can use the following basic syntax to perform an inner join in pandas:
<b>import pandas as pd
df1.merge(df2, on='column_name', how='inner')
</b>
The following example shows how to use this syntax in practice.
<h2>Example: How to Do Inner Join in Pandas</h2>
Suppose we have the following two pandas DataFrames that contains information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [18, 22, 19, 14, 14, 11, 20, 28]})
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'G', 'H'],    'assists': [4, 9, 14, 13, 10, 8]})
#view DataFrames
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
4    E      14
5    F      11
6    G      20
7    H      28
print(df2)
  team  assists
0    A        4
1    B        9
2    C       14
3    D       13
4    G       10
5    H        8</b>
We can use the following code to perform an inner join, which only keeps the rows where the <b>team</b> name appears in both DataFrames:
<b>#perform left join
df1.merge(df2, on='team', how='inner')
teampointsassists
0A184
1B229
2C1914
3D1413
4G2010
5H288
</b>
The only rows contained in the merged DataFrame are the ones where the team name appears in both DataFrames.
Notice that two teams were dropped (teams E and F) because they didn’t appear in both DataFrames.
Note that you can also use <b>pd.merge()</b> with the following syntax to return the exact same result:
<b>#perform left join
pd.merge(df1, df2, on='team', how='inner')
teampointsassists
0A184
1B229
2C1914
3D1413
4G2010
5H288</b>
Notice that this merged DataFrame matches the one from the previous example.
<b>Note</b>: You can find the complete documentation for the <b>merge</b> function  here .
<h2><span class="orange">How to Fix: Input contains NaN, infinity or a value too large for dtype(‘float64’)</span></h2>
One common error you may encounter when using Python is:
<b>ValueError: Input contains infinity or a value too large for dtype('float64').
</b>
This error usually occurs when you attempt to use some function from the scikit-learn module, but the DataFrame or matrix you’re using as input has NaN values or infinite values.
The following example shows how to resolve this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'x1': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4],   'x2': [1, 3, 3, 5, 2, 2, 1, np.inf, 0, 3, 4],   'y': [np.nan, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90]})
#view DataFrame
print(df)
    x1   x2     y
0    1  1.0   NaN
1    2  3.0  78.0
2    2  3.0  85.0
3    4  5.0  88.0
4    2  2.0  72.0
5    1  2.0  69.0
6    5  1.0  94.0
7    4  inf  94.0
8    2  0.0  88.0
9    4  3.0  92.0
10   4  4.0  90.0</b>
Now suppose we attempt to fit a  multiple linear regression model  using functions from  scikit-learn :
<b>from sklearn.linear_model import LinearRegression
#initiate linear regression model
model = LinearRegression()
#define predictor and response variables
X, y = df[['x1', 'x2']], df.y
#fit regression model
model.fit(X, y)
#print model intercept and coefficients
print(model.intercept_, model.coef_)
ValueError: Input contains infinity or a value too large for dtype('float64').
</b>
We receive an error since the DataFrame we’re using has both infinite and NaN values.
<h2>How to Fix the Error</h2>
The way to resolve this error is to first remove any rows from the DataFrame that contain infinite or NaN values:
<b>#remove rows with any values that are not finite
df_new = df[np.isfinite(df).all(1)]
#view updated DataFrame
print(df_new)
    x1   x2     y
1    2  3.0  78.0
2    2  3.0  85.0
3    4  5.0  88.0
4    2  2.0  72.0
5    1  2.0  69.0
6    5  1.0  94.0
8    2  0.0  88.0
9    4  3.0  92.0
10   4  4.0  90.0
</b>
The two rows that had infinite or NaN values have been removed.
We can now proceed to fit our linear regression model:
<b>from sklearn.linear_model import LinearRegression
#initiate linear regression model
model = LinearRegression()
#define predictor and response variables
X, y = df_new[['x1', 'x2']], df_new.y
#fit regression model
model.fit(X, y)
#print model intercept and coefficients
print(model.intercept_, model.coef_)
69.85144124168515 [ 5.72727273 -0.93791574]
</b>
Notice that we don’t receive any error this time because we first removed the rows with infinite or NaN values from the DataFrame.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in Python:
 How to Fix in Python: ‘numpy.ndarray’ object is not callable 
 How to Fix: TypeError: ‘numpy.float64’ object is not callable 
 How to Fix: Typeerror: expected string or bytes-like object 
<h2><span class="orange">How to Insert a Column Into a Pandas DataFrame</span></h2>
Often you may want to insert a new column into a pandas DataFrame. Fortunately this is easy to do using the pandas  insert()  function, which uses the following syntax:
<b>insert(loc, column, value, allow_duplicates=False)</b>
where:
<b>loc: </b>Index to insert column in. First column is 0.
<b>column: </b>Name to give to new column.
<b>value:</b> Array of values for the new column.
<b>allow_duplicates: </b>Whether or not to allow new column name to match existing column name. Default is False.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Insert New Column as First Column</h3>
The following code shows how to insert a new column as the first column of an existing DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
        pointsassistsrebounds
025511
11278
215710
31496
419126
#insert new column 'player' as first column
player_vals = ['A', 'B', 'C', 'D', 'E']
df.insert(loc=0, column='player', value=player_vals)
df
        playerpointsassistsrebounds
0A25511
1B1278
2C15710
3D1496
4E19126
</b>
<h3>Example 2: Insert New Column as a Middle Column</h3>
The following code shows how to insert a new column as the third column of an existing DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#insert new column 'player' as third column
player_vals = ['A', 'B', 'C', 'D', 'E']
df.insert(loc=2, column='player', value=player_vals)
df
        pointsassistsplayerrebounds
0255A11
1127B8
2157C10
3149D6
41912E6</b>
<h3>Example 3: Insert New Column as Last Column</h3>
The following code shows how to insert a new column as the last column of an existing DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#insert new column 'player' as last column
player_vals = ['A', 'B', 'C', 'D', 'E']
df.insert(loc=len(df.columns), column='player', value=player_vals)
df
        pointsassistsplayerrebounds
0255A11
1127B8
2157C10
3149D6
41912E6</b>
Note that using <b>len(df.columns) </b>allows you to insert a new column as the last column in any dataFrame, no matter how many columns it may have.
<em>You can find the complete documentation for the insert() function  here .</em>
<h2><span class="orange">Instrumental Variables: Definition & Examples</span></h2>
Often in statistics we’re interested in estimating the effect that one variable has on another. For example, perhaps we want to know:
How does amount of time spent studying affect exam scores?
How does a certain drug affect blood pressure?
How does stress affect heart rate?
In each scenario, we want to understand whether or not some predictor variable affects a  response variable . However, often there will be other variables that affect the relationship between the two variables.
For example, suppose we use a certain drug as our predictor variable and blood pressure as our response variable. We are only interested in the effect that the drug has on blood pressure:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/instrumentalvariable1.png">
However, other variables like time spent exercising, overall diet, and stress levels also affect blood pressure:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/instrumentalvariable2.png">
Thus, if we run a  simple linear regression  using the drug as our predictor variable and blood pressure as our response variable, we can’t be sure that the  regression coefficients  will accurately capture the effect that the drug has on blood pressure because outside factors (exercise, diet, stress, etc.) could also be playing a role.
One potential way to get around this problem is to use an <b>instrumental variable</b>.
<h3>What is an Instrumental Variable?</h3>
An <b>instrumental variable</b> is a third variable introduced into regression analysis that is correlated with the predictor variable, but uncorrelated with the response variable. By using this variable, it becomes possible to estimate the true causal effect that some predictor variable has on a response variable.
For example, suppose we want to estimate the effect that a certain drug has on blood pressure:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/instrumentalvariable1.png">
An example of an instrumental variable that we may use in this regression analysis is an individual’s proximity to a pharmacy.
This variable “proximity” would likely be highly correlated with whether or not the individual takes the certain drug because an individual wouldn’t be able to obtain it in the first place if they don’t live near a pharmacy.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/instrumentalvariable3.png">
However, the variable “proximity” is not expected to have any correlation with blood pressure. The only association it would have with blood pressure is through the predictor variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/instrumentalvariable4.png">
The way that we actually use an instrumental variable is through instrumental variables regression, sometimes called  two-stage least squares regression .
<h3>Instrumental Variables Regression</h3>
Instrumental variables regression (or two-stage least squares regression) uses the following approach to estimate the effect that a predictor variable has on a response variable:
<b>Stage 1: Fit a regression model using the instrumental variable as the predictor variable.</b>
In our specific example, we would first fit the following regression model:
Certain drug = B<sub>0</sub> + B<sub>1</sub>(proximity)
We would then be left with predicted values for certain drug (cd), which we’ll call cd<sub>hat</sub>.
<b>Stage 2: Fit a second regression model using the predicted values for cd<sub>hat</sub>.</b>
Next, we’ll fit the following regression model:
Blood pressure = B<sub>0</sub> + B<sub>1</sub>(cd<sub>hat</sub>)
If the regression coefficient for cd<sub>hat</sub> turns out to be statistically significant, then we can say that there is a causal effect of the drug on blood pressure. 
The reason we can say this is because we solely used “proximity” to come up with cd<sub>hat</sub> and since we know that proximity should not be correlated with blood pressure, any significant correlation in the second stage regression can be attributed to the certain drug.
<h3>Cautions on Using Instrumental Variables</h3>
An instrumental variable should only be used if it meets the following criteria:
It is highly correlated with the predictor variable.
It is not correlated with the response variable.
It is not correlated with the other variables that are left out of the model (e.g. proximity is not correlated with exercise, diet, or stress).
If an instrumental variable does not meet this criteria, then it should not be used in the regression model because it will likely produce unreliable and biased results.
<h3>Bonus: A Video Explanation of Instrumental Variables</h3>
The following video by Ashley Hodgson provides an excellent visual explanation of instrumental variables:
<iframe title="Identification, Part 3: Instrumental Variables" src="https://www.youtube.com/embed/J2BMFBMO14o?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2><span class="orange">How to Catch integer(0) in R (With Examples)</span></h2>
Sometimes when you use the  which()  function in R, you may end up with <b>integer(0)</b> as a result, which indicates that none of the elements in a vector evaluated to TRUE.
For example, suppose we use the following code to check which elements in a vector are equal to the value 10:
<b>#define vector of values
data &lt;- c(1, 2, 4, 4, 5, 7, 8, 9)
#find elements in vector equal to 10
x &lt;- which(data == 10)
#view results
x
integer(0)
</b>
Since none of the elements in the vector are equal to 10, the result is an integer of length 0, written as <b>integer(0)</b> in R.
It’s important to note that an <b>integer(0)</b> is not an error, but sometimes you may just want to be aware of when it occurs.
The following examples show how to catch an <b>integer(0)</b> in R.
<h3>Example 1: Catch integer(0) in R Using identical() Function</h3>
The easiest way to catch an <b>integer(0)</b> in R is to use the <b>identical()</b> function in the following manner:
<b>#define vector of values
data &lt;- c(1, 2, 4, 4, 5, 7, 8, 9)
#find elements in vector equal to 10
x &lt;- which(data == 10)
#test if x is identical to integer(0)
identical(x, integer(0))
[1] TRUE
</b>
Since our result is equal to <b>integer(0)</b>, R returns <b>TRUE</b>.
This lets us know that the result of the which() function is an integer of length 0.
<h3>Example 2: Catch integer(0) in R Using if else Function</h3>
Another way to catch an<b> integer(0)</b> is to define an if else function that returns something specific if an <b>integer(0)</b> occurs.
For example, we could define the following function to return the phrase “It is an integer(0)” if an <b>integer(0)</b> occurs:
<b>#define function to catch integer(0)
integer0_test &lt;- function(data) {
 
  if(identical(data, integer(0))) {
    return('It is an integer(0)')
  }
  else {
    return(data)
  }
}
</b>
We can then use this function:
<b>#define vector of values
data &lt;- c(1, 2, 4, 4, 5, 7, 8, 9)
#find elements in vector equal to 10
x &lt;- which(data == 10)
#use function to test if x is integer(0)
integer0_test(x)
[1] "It is an integer(0)"
</b>
Since x is indeed an <b>integer(0)</b>, our function returns the phrase that we specified.
And if x is not an <b>integer(0)</b>, our function will simply return the result of the which() function:
<b>#define vector of values
data &lt;- c(1, 2, 4, 4, 5, 7, 8, 9)
#find elements in vector equal to 4
x &lt;- which(data == 4)
#use function to test if x is integer(0)
integer0_test(x)
[1] 3 4
</b>
Our function returns <b>3</b> and <b>4</b> because these are the positions of the elements in the vector that are equal to the value 4.
<h2><span class="orange">What is Inter-rater Reliability? (Definition & Example)</span></h2>
In statistics, <b>inter-rater reliability</b> is a way to measure the level of agreement between multiple raters or judges.
It is used as a way to assess the reliability of answers produced by different items on a test. If a test has lower inter-rater reliability, this could be an indication that the items on the test are confusing, unclear, or even unnecessary.
There are two common ways to measure inter-rater reliability:
<b>1. Percent Agreement</b>
The simple way to measure inter-rater reliability is to calculate the percentage of items that the judges agree on.
This is known as <b>percent agreement</b>, which always ranges between 0 and 1 with 0 indicating no agreement between raters and 1 indicating perfect agreement between raters.
For example, suppose two judges are asked to rate the difficulty of 10 items on a test from a scale of 1 to 3. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/interrater1.png">
For each question, we can write “1” if the two judges agree and “0” if they don’t agree.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/interrater2.png">
The percentage of questions the judges agreed on was 7/10 = <b>70%</b>.
<b>2. Cohen’s Kappa</b>
The more difficult (and more rigorous) way to measure inter-rater reliability is to use use  Cohen’s Kappa , which calculates the percentage of items that the raters agree on, while accounting for the fact that the raters may happen to agree on some items purely by chance.
The formula for Cohen’s kappa is calculated as:
<b>k = (p<sub>o</sub> – p<sub>e</sub>) / (1 – p<sub>e</sub>)</b>
where:
<b>p<sub>o</sub>:</b> Relative observed agreement among raters
<b>p<sub>e</sub>:</b> Hypothetical probability of chance agreement
Cohen’s Kappa always ranges between 0 and 1, with 0 indicating no agreement between raters and 1 indicating perfect agreement between raters.
For a step-by-step example of how to calculate Cohen’s Kappa, refer to  this tutorial .
<h3>How to Interpret Inter-Rater Reliability</h3>
The higher the inter-rater reliability, the more consistently multiple judges rate items or questions on a test with similar scores.
In general, an inter-rater agreement of at least 75% is required in most fields for a test to be considered reliable. However, higher inter-rater reliabilities may be needed in specific fields.
For example, an inter-rater reliability of 75% may be acceptable for a test that seeks to determine how well a TV show will be received.
On the other hand, an inter-rater reliability of 95% may be required in medical settings in which multiple doctors are judging whether or not a certain treatment should be used on a given patient.
Note that in most academic settings and rigorous fields of research, Cohen’s Kappa is used to calculate inter-rater reliability.
<h2><span class="orange">How to Create an Interaction Plot in Excel</span></h2>
A  2×2 factorial design  is used to understand how two independent variables (each with two  levels ) affect one  dependent variable .
A useful way to visualize the effects that the two independent variables have on the dependent variable is to create an interaction plot, which displays the mean value of the dependent variable at each level of the independent variables.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel1.png">
The following step-by-step example shows how to create and interpret an interaction plot in Excel.
<h3>Step 1: Enter the Data</h3>
Suppose we want to understand the effects of sunlight exposure (low vs. high) and watering frequency (daily vs. weekly) on the growth of a certain species of plant.
Suppose we collect data on 60 plants and calculate the mean plant growth at each level of sunlight exposure and watering frequency. Here’s how to enter these mean values into Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel2.png">
This tell us that:
The mean growth for plants that received high sunlight and daily watering was about <b>8.2</b> inches.
The mean growth for plants that received high sunlight and weekly watering was about <b>9.6</b> inches.
The mean growth for plants that received low sunlight and daily watering was about <b>5.3</b> inches.
The mean growth for plants that received low sunlight and weekly watering was about <b>5.8</b> inches.
<h3>Step 2: Create the Interaction Plot</h3>
Next, we need to highlight the values in the cell range C4:E6 as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel3.png">
Then click the <b>Insert</b> tab along the top ribbon in Excel, then click the first chart option within the <b>Line Chart</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel4.png">
This will automatically insert the following line chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel5.png">
Feel free to modify the title, axis labels and colors on the plot to make it more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/interactionExcel1.png">
<h3>Step 3: Interpret the Interaction Plot</h3>
Here’s how to interpret the lines in the plot:
<b>Main Effect of Watering Frequency:</b>
Since the two lines are mostly flat, this tells us that there is very little difference in plant growth between daily and weekly watering. This tells us that watering frequency probably doesn’t have a statistically significant effect on plant growth.
<b>Main Effect of Sunlight:</b>
Since the two lines are fairly far apart, this tells us that there is a noticeable difference between low sunlight and high sunlight.
For example, we can see that the average growth for plants that received low sunlight was around 5-6 inches. We can also see that the average growth for plants that received high sunlight was around 8-9 inches.
This tells us that sunlight probably has a statistically significant effect on plant growth.
<b>Interaction Effect Between Watering Frequency & Sunlight</b>
Since the two lines in the plot are mostly parallel, this tells us that there is no interaction effect between watering frequency and sunlight.
For example, the effect that sunlight exposure has on plant growth isn’t dependent on watering frequency. Similarly, the effect that watering frequency has on plant growth isn’t dependent on sunlight exposure.
If the two lines in the plot were not parallel (or if they crossed each other) then this would be an indication that there is an interaction effect between watering frequency and sunlight on plant growth.
<h2><span class="orange">How to Create an Interaction Plot in R</span></h2>
A  two-way ANOVA  is used to determine if there is a difference between the means of three or more independent groups that have been split on two factors.
We use a two-way ANOVA when we’d like to know if two specific factors affect a certain response variable. However, sometimes there is an <b>interaction effect </b>present between the two factors, which can impact the way we interpret the relationship between the factors and the response variable.
For example, we might want to know if the factors (1) <em>exercise</em> and (2) <em>gender</em> affect the response variable <em>weight loss</em>. While it’s possible that both factors affect weight loss, it’s also possible that the two factors interact with each other.
For example, it’s possible that exercise leads to weight loss at different rates for males and females. In this case, there is an interact effect between exercise and gender.
The easiest way to detect and understand interaction effects between two factors is with an <b>interaction plot</b>.
This is a type of plot that displays the fitted values of a response variable on the y-axis and the values of the first factor on the x-axis. Meanwhile, the lines in the plot represent the values of the second factor of interest.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/interactionPlot1.png">
This tutorial explains how to create and interpret an interaction plot in R.
<h3>Example: Interaction Plot in R</h3>
Suppose researchers want to determine if exercise intensity and gender impact weight loss. To test this, they recruit 30 men and 30 women to participate in an experiment in which they randomly assign 10 of each to follow a program of either no exercise, light exercise, or intense exercise for one month.
Use the following steps to create a data frame in R, perform a two-way ANOVA, and create an interaction plot to visualize the interaction effect between exercise and gender.
<b>Step 1: Create the data.</b>
The following code shows how to create a data frame in R:
<b>#make this example reproducible
set.seed(10)
#create data frame
data &lt;- data.frame(gender = rep(c("Male", "Female"), each = 30),   exercise = rep(c("None", "Light", "Intense"), each = 10, times = 2),   weight_loss = c(runif(10, -3, 3), runif(10, 0, 5), runif(10, 5, 9),                   runif(10, -4, 2), runif(10, 0, 3), runif(10, 3, 8)))
#view first six rows of data frame
head(data)
  gender exercise weight_loss
1   Male     None  0.04486922
2   Male     None -1.15938896
3   Male     None -0.43855400
4   Male     None  1.15861249
5   Male     None -2.48918419
6   Male     None -1.64738030</b>
<b>Step 2: Fit the two-way ANOVA model.</b>
The following code shows how to fit a two-way ANOVA to the data:
<b>#fit the two-way ANOVA model
model &lt;- aov(weight_loss ~ gender * exercise, data = data)
#view the model output
summary(model)
# Df Sum Sq Mean Sq F value Pr(>F)    
#gender           1   15.8   15.80  11.197 0.0015 ** 
#exercise         2  505.6  252.78 179.087 &lt;2e-16 ***
#gender:exercise  2   13.0    6.51   4.615 0.0141 *  
#Residuals       54   76.2    1.41                   
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
Note that the p-value (<b>0.0141</b>) for the interaction term between exercise and gender is statistically significant, which indicates that there is a significant interaction effect between the two factors.
<b>Step 3: Create the interaction plot.</b>
The following code shows how to create an interaction plot for exercise and gender:
<b>interaction.plot(x.factor = data$exercise, #x-axis variable trace.factor = data$gender, #variable for lines response = data$weight_loss, #y-axis variable fun = median, #metric to plot ylab = "Weight Loss", xlab = "Exercise Intensity", col = c("pink", "blue"), lty = 1, #line type lwd = 2, #line width trace.label = "Gender")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/interactionPlot1.png">
In general, if the two lines on the interaction plot are parallel then there is no interaction effect. However, if the lines intersect then there is likely an interaction effect.
We can see in this plot that the lines for males and females do intersect, which indicates that there is likely an interaction effect between the variables of exercise intensity and gender.
This matches the fact that the p-value in the output of the ANOVA table was statistically significant for the interaction term in the ANOVA model.
<h2><span class="orange">How to Interpret the Intercept in a Regression Model (With Examples)</span></h2>
The <b>intercept</b> (sometimes called the “constant”) in a regression model represents the mean value of the response variable when all of the predictor variables in the model are equal to zero.
This tutorial explains how to interpret the intercept value in both simple linear regression and multiple linear regression models.
<h3>Interpreting the Intercept in Simple Linear Regression</h3>
A simple linear regression model takes the following form:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>(x)
where:
<U+0177>: The predicted value for the response variable
β<sub>0</sub>: The mean value of the response variable when x = 0
β<sub>1</sub>: The average change in the response variable for a one unit increase in x
x: The value for the predictor variable
In some cases, it makes sense to interpret the value for the intercept in a simple linear regression model but not always. The following examples illustrate this.
<b>Example 1: Intercept Makes Sense to Interpret</b>
Suppose we’d like to fit a simple linear regression model using <em>hours studied</em> as a predictor variable and <em>exam score</em> as the response variable.
We collect this data for 50 students in a certain college course and fit the following regression model:
Exam score = 65.4 + 2.67(hours)
The value for the intercept term in this model is <b>65.4</b>. This means the average exam score is <b>65.4</b> when the number of hours studied is equal to zero.
This makes sense to interpret since it’s plausible for a student to study for zero hours in preparation for an exam.
<b>Example 2: Intercept Does Not Make Sense to Interpret</b>
Suppose we’d like to fit a simple linear regression model using <em>weight</em> (in pounds) as a predictor variable and <em>height</em> (in inches) as the response variable.
We collect this data for 50 individuals and fit the following regression model:
Height = 22.3 + 0.28(pounds)
The value for the intercept term in this model is <b>22.3</b>. This would mean the average height of a person is <b>22.3</b> inches when their weight is equal to zero.
This does not make sense to interpret since it’s not possible for a person to weigh zero pounds. 
However, we still need to keep the intercept term in the model in order to use the model to make predictions. The intercept just doesn’t have any meaningful interpretation for this model.
<h3>Interpreting the Intercept in Multiple Linear Regression</h3>
A multiple linear regression model takes the following form:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>(x<sub>1</sub>) + β<sub>2</sub>(x<sub>2</sub>) + β<sub>3</sub>(x<sub>3</sub>) + … + β<sub>k</sub>(x<sub>k</sub>)
where:
<U+0177>: The predicted value for the response variable
β<sub>0</sub>: The mean value of the response variable when all predictor variables are zero
β<sub>j</sub>: The average change in the response variable for a one unit increase in the j<sup>th</sup> predictor variable, assuming all other predictor variables are held constant
x<sub>j</sub>: The value for the j<sup>th</sup> predictor variable
Similar to simple linear regression, it makes sense to interpret the value for the intercept in a multiple linear regression model sometimes but not always. The following examples illustrate this.
<b>Example 1: Intercept Makes Sense to Interpret</b>
Suppose we’d like to fit a multiple linear regression model using <em>hours studied</em> and <em>prep exams taken</em> as the predictor variables and <em>exam score</em> as the response variable.
We collect this data for 50 students in a certain college course and fit the following regression model:
Exam score = 58.4 + 2.23(hours) + 1.34(# prep exams)
The value for the intercept term in this model is <b>58.4</b>. This means the average exam score is <b>58.4</b> when the number of hours studied and the number of prep exams taken are both equal to zero.
This makes sense to interpret since it’s plausible for a student to study for zero hours and take zero prep exams before the actual exam.
<b>Example 2: Intercept Does Not Make Sense to Interpret</b>
Suppose we’d like to fit a multiple linear regression model using <em>square footage</em> and <em>number of bedrooms</em> as predictor variables and <em>selling price</em> as the response variable.
We collect this data for 100 houses in a certain city and fit the following regression model:
Price = 87,244 + 3.44(square footage) + 843.45(# bedrooms)
The value for the intercept term in this model is <b>87,244</b>. This would mean the average selling price of a house is <b>$87,244</b> when the square footage and number of bedrooms in a house are both equal to zero.
This does not make sense to interpret since it’s not possible for a house to have zero square footage and zero bedrooms.
However, we still need to keep the intercept term in the model in order to use it to make predictions. The intercept just doesn’t have any meaningful interpretation for this model.
<h2><span class="orange">A Simple Explanation of Internal Consistency</span></h2>
<b>Internal consistency </b>refers to how well a survey, questionnaire, or test actually measures what you want it to measure. The higher the internal consistency, the more confident you can be that your survey is reliable.
The most common way to measure internal consistency is by using a statistic known as <em>Cronbach’s Alpha</em>, which calculates the pairwise correlations between items in a survey. The value for Cronbach’s Alpha can range between negative infinity and one.
The following table describes how various values of Cronbach’s Alpha are typically interpreted:
<table><tbody>
<tr>
<th><b>Cronbach’s Alpha</b></th>
<th><b>Internal consistency</b></th>
</tr>
<tr>
<td>0.9 ≤ α</td>
<td>Excellent</td>
</tr>
<tr>
<td>0.8 ≤ α &lt; 0.9</td>
<td>Good</td>
</tr>
<tr>
<td>0.7 ≤ α &lt; 0.8</td>
<td>Acceptable</td>
</tr>
<tr>
<td>0.6 ≤ α &lt; 0.7</td>
<td>Questionable</td>
</tr>
<tr>
<td>0.5 ≤ α &lt; 0.6</td>
<td>Poor</td>
</tr>
<tr>
<td>α &lt; 0.5</td>
<td>Unacceptable</td>
</tr>
</tbody></table>
Next, we’ll walk through an example to provide an intuitive understanding of internal consistency.
<h2>An Example</h2>
Suppose a restaurant manager wants to measure overall satisfaction among customers, so she sends out a survey with the following questions to which customers can respond s<em>trongly disagree, disagree, neutral, agree</em>, or <em>strongly agree</em>.
<b>1.</b> I was satisfied with my experience.
<b>2.</b> I would recommend your restaurant to family and friends.
<b>3. </b>I would visit this restaurant again at some point in the future.
Each of these questions measures customer satisfaction in slightly different ways, but a given customer should response to each survey question with roughly the same response.
For example, a customer that was highly satisfied with their experience should also be highly likely to recommend the restaurant to family and friends and also be highly likely to visit the restaurant again at some point in the future.
For this survey, the internal consistency (as measured by Cronbach’s Alpha) should be fairly high, which would indicate that the items in the survey actually measure what we want them to measure.
But consider if the following question was added to the survey:
<b>4. </b>I am a fan of baseball.
Since this question is completely unrelated to overall customer satisfaction, it would likely lower the internal consistency of the survey.
Or consider instead if question 3 was re-worded as follows:
<b>3. </b>I would probably (not definitely, but maybe most likely) visit this restaurant at some point in the near future, given the right circumstances if I was in the right mood.
Since the wording of this question is so confusing and obscure, it’s possible that different customers will interpret it differently, and thus provide differing responses. This would likely result in a lower internal consistency.
<h2>What to Do if Internal Consistency is Low</h2>
If the internal consistency (as measured by Cronbach’s Alpha) is low for a given survey, there are two ways that you can potentially increase it:
<b>1.</b> Remove items from the survey that have a low correlation with other items on the survey (e.g. removing the item that says “I am a fan of baseball.”)
2. Add items to the survey that are likely to correlate with other items on the survey (e.g. adding an item that says “I often feel that money spent at this restaurant is money well spent”). If you choose this option, be careful not to add items that are redundant to the items already on the survey.
<h2><span class="orange">How to Interpolate Missing Values in Excel</span></h2>
Often you may have one or more missing values in a series in Excel that you’d like to fill in.
The simplest way to fill in missing values is to use the <b>Fill Series</b> function within the <b>Editing</b> section on the <b>Home</b> tab.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill1.png">
This tutorial provides two examples of how to use this function in practice.
<h3>Example 1: Fill in Missing Values for a Linear Trend</h3>
Suppose we have the following dataset with a few missing values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill2.png">
If we create a quick line chart of this data, we’ll see that the data appears to follow a linear trend:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill3.png">
To fill in the missing values, we can highlight the range starting before and after the missing values, then click <b>Home > Editing > Fill > Series</b>. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill4.png">
If we leave the Type as <b>Linear</b>, Excel will use the following formula to determine what step value to use to fill in the missing data:
Step = (End – Start) / (#Missing obs + 1)
For this example, it determines the step value to be: (35-20) / (4+1) = <b>3</b>.
Once we click <b>OK</b>, Excel automatically fills in the missing values by adding 3 to the each subsequent value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill5.png">
<h3>Example 2: Fill in Missing Values for a Growth Trend</h3>
Suppose we have the following dataset with a few missing values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill6.png">
If we create a quick line chart of this data, we’ll see that the data appears to follow an exponential (or “growth”) trend:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill7.png">
To fill in the missing values, we can highlight the range starting before and after the missing values, then click <b>Home > Editing > Fill > Series</b>. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill8.png">
If we select the Type as <b>Growth</b> and click the box next to <b>Trend</b>, Excel automatically identifies the growth trend in the data and fills in the missing values.
Once we click <b>OK</b>, Excel fills in the missing values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/seriesFill9.png">
From the plot we can see that the filled-in values match the general trend of the data quite well.
<em>You can find more Excel tutorials  here .</em>
<h2><span class="orange">How to Interpolate Missing Values in Google Sheets</span></h2>
Often you may have one or more missing values in a series in Google Sheets that you’d like to fill in.
The following step-by-step example shows how to interpolate missing values in practice.
<h2>Step 1: Create the Data</h2>
First, let’s create a dataset in Google Sheets that contains some missing values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/interpolatesheets1.jpg"521">
<h2>Step 2: Calculate the Step Value </h2>
Next, we will use the following formula to determine what step value to use to fill in the missing data:
<b>Step</b> = (End – Start) / (#Missing observations + 1)
For this example, we would calculate the step value as:
Step = (35 – 20) / (4 – 1) = <b>3</b>
We can type the following formula into cell <b>D1</b> to automatically calculate this value:
<b>=(A13-A8)/(COUNTBLANK(A9:A12)+1)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/interpolatesheets2.jpg"483">
<h2>Step 3: Interpolate the Missing Values</h2>
Next, we need to add the step value to the missing values, starting with the first missing value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/interpolatesheets3.jpg"390">
We can then drag and fill this formula down to each remaining blank cell:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/interpolatesheets4.jpg"376">
<h2>Step 4: Visualize the Interpolated Values</h2>
Lastly, we can create a line chart to see if the interpolated values seem to fit the dataset well.
To do so, highlight cells <b>A2:A21</b>, then click the <b>Insert</b> tab, then click <b>Chart</b>.
In the <b>Chart editor</b> panel that appears on the right side of the screen, choose <b>Line chart</b> as the <b>Chart type</b>.
The following line chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/interpolatesheets5.jpg"579">
The interpolated values seem to fit the trend of the dataset well.
<h2>Additional Resource</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Add Multiple Trendlines to Chart in Google Sheets 
 How to Add a Vertical Line to a Chart in Google Sheets 
 How to Add Average Line to Chart in Google Sheets 
<h2><span class="orange">Interpolation vs. Extrapolation: What’s the Difference?</span></h2>
Two terms that students often confuse in statistics are <b>interpolation</b> and <b>extrapolation</b>.
Here’s the difference:
<b>Interpolation</b> refers to predicting values that are <em>inside</em> of a range of data points.
<b>Extrapolation</b> refers to predicting values that are <em>outside</em> of a range of data points.
The following example illustrates the difference between the two terms.
<h3>Example: Interpolation vs. Extrapolation</h3>
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp1.png">
We may decide to fit a  simple linear regression model  to these points:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp2.png">
We could then use the fitted regression model to predict the values of points both <em>inside</em> and <em>outside</em> of the range of data points.
When we use the fitted regression model to predict the values of points inside the existing range of data points it is known as <b>interpolation.</b>
Conversely, when we use the fitted regression model to predict the values of points outside the existing range it is known as <b>extrapolation</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp3.png">
<h3>The Potential Danger of Extrapolation</h3>
When we perform extrapolation, we assume that the same pattern that exists inside the current range of data points also exists outside of the range as well.
However, this can be a dangerous assumption because it’s possible that the pattern that exists outside the current range of data points is quite different:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp4.png">
For this reason, it can be dangerous to use extrapolation to predict the values of data points that fall outside of the range of values that was used to build the regression model.
In practice, it’s often fine to use extrapolation to predict the values of points that fall just slightly outside of the range of existing values but the further outside the range the higher the likelihood that the difference between the predicted value and the actual value will be large.
<h3>When to Use Extrapolation</h3>
Often it requires domain-specific expertise to determine if extrapolation is a reasonable idea or not.
For example, suppose a marketing department at a business fits a simple linear regression model using advertising spend as the predictor variable and total revenue as the response variable.
In this scenario, it may be reasonable to assume that a steady increase in advertising spend will lead to a predictable increase in total revenue:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp5.png">
In this scenario, we may be quite confident in our ability to extrapolate values.
However, consider a scenario where a biologist wants to use total fertilizer to predict plant growth.
She may decide to fit a simple linear regression model to the data points, but since there is an upper limit on how tall plants can grow, it probably doesn’t make sense to use extrapolation to predict the values of points outside of the range of values used to fit the model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/interp6.png">
In this scenario, we may be considerably less confident in our ability to extrapolate values.
<b>The Takeaway</b>: Extrapolation can make sense in some fields more than others, but there is always a potential danger that the pattern that exists within the range of values used to fit the model does not exist outside of the range.
<h2><span class="orange">Complete Guide: How to Interpret ANOVA Results in Excel</span></h2>
An  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
The following example provides a complete guide on how to interpret the results of a one-way ANOVA in Excel.
<h3>Example: How to Interpret ANOVA Results in Excel</h3>
Suppose a teacher  randomly assigns  30 students in her class to use one of three studying methods to prepare for an exam.
The following screenshot shows the scores of the students based on the method they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/interpretANOVA0.png">
Suppose the teacher wants to perform a one-way ANOVA to determine if the mean scores are the same across all three groups.
To perform a one-way ANOVA in Excel, click the <b>Data</b> tab along the top ribbon, then click <b>Data Analysis</b> within the <b>Analyze</b> group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/toolpak.png">
If you don’t see the<b> Data Analysis</b> option, then you need to first load the free  Analysis ToolPak .
Once you click this, a new window will appear. Select <b>Anova: Single Factor</b>, then click <b>OK</b>.
In the new window that appears, enter the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/interpretANOVA1.png">
Once you click <b>OK</b>, the results of the one-way ANOVA will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/interpretANOVA2.png">
There are two tables that are shown in the output: <b>SUMMARY</b> and <b>ANOVA</b>.
Here’s how to interpret the values in each table:
<b>SUMMARY Table</b>:
<b>Groups</b>: The names of the groups
<b>Count</b>: The number of  observations  in each group
<b>Sum</b>: The sum of the values in each group
<b>Average</b>: The average value in each group
<b>Variance</b>: The variance of the values in each group
This table provides us with several useful summary statistics for each group used in the ANOVA.
From this table we can see that the students who used Method 3 had the highest average exam score (86.7) but they also had the highest  variance  in exam scores (13.56667).
To determine if the differences in the group means are statistically significant we must refer to the ANOVA table.
<b>ANOVA Table</b>:
<b>Source of Variation</b>: The variation being measured (either between groups or within groups)
<b>SS</b>: The sum of squares for each source of variation
<b>df</b>: The degrees of freedom, calculated as #groups-1 for df Between and #observations – #groups for df Within
<b>MS</b>: The mean sum of squares, calculated as SS / df
<b>F</b>: The overall F-value, calculated as MS Between / MS Within
<b>P-value</b>: The p-value corresponding to the overall F-value
<b>F crit</b>: The F critical value that corresponds to α = .05
The most important value in this table is the  p-value , which turns out to be <b>0.002266</b>.
Recall that a one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: All group means are equal.
<b>H<sub>A</sub></b>: All group means are not equal.
Since the p-value is less than α = .05, we reject the null hypothesis of the one-way ANOVA and conclude that we have sufficient evidence to say that not all of the group means are equal.
This means that the three studying methods do not all lead to the same average exam scores.
<b>Note</b>: You could also compare the overall F value to the F critical value to determine whether you should reject or fail to reject the null hypothesis. In this case, since the overall F value is greater than the F critical value we would reject the null hypothesis. Note that the p-value approach and the F critical value approach will always lead to the same conclusion.
<h2><span class="orange">Complete Guide: How to Interpret ANOVA Results in R</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial provides a complete guide on how to interpret the results of a one-way ANOVA in R.
<h3>Step 1: Create the Data</h3>
Suppose we want to determine if three different workout programs lead to different average weight loss in individuals.
To test this, we recruit 90 people to participate in an experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month.
The following code creates the data frame we’ll be working with:
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = rep(c('A', 'B', 'C'), each = 30),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#view first six rows of data frame
head(data)
  program weight_loss
1       A   2.6900916
2       A   0.7965260
3       A   1.1163717
4       A   1.7185601
5       A   2.7246234
6       A   0.6050458
</b>
<h3>Step 2: Perform the ANOVA</h3>
Next, we’ll use the <b>aov()</b> command to perform a one-way ANOVA:
<b>#fit one-way ANOVA model
model &lt;- aov(weight_loss ~ program, data = data)
</b>
<h3>Step 3: Interpret the ANOVA Results</h3>
Next, we’ll use the <b>summary()</b> command to view the results of the one-way ANOVA:
<b>#view summary of one-way ANOVA model
summary(model)
            Df Sum Sq Mean Sq F value   Pr(>F)    
program      2  98.93   49.46   30.83 7.55e-11 ***
Residuals   87 139.57    1.60                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</b>
Here’s how to interpret every value in the output:
<b>Df program:</b> The degrees of freedom for the variable <em>program</em>. This is calculated as #groups -1. In this case, there were 3 different workout programs, so this value is: 3-1 = <b>2</b>.
<b>Df Residuals:</b> The degrees of freedom for the residuals. This is calculated as #total observations – # groups. In this case, there were 90 observations and 3 groups, so this value is: 90 -3 = <b>87</b>.
<b>Sum Sq program:</b> The sum of squares associated with the variable <em>program</em>. This value is <b>98.93</b>.
<b>Sum Sq Residuals:</b> The sum of squares associated with the residuals or “errors.” This value is <b>139.57</b>.
<b>Mean Sq. Program:</b> The mean sum of squares associated with program. This is calculated as Sum Sq. program / Df program. In this case, this is calculated as: 98.93 / 2 = <b>49.46</b>.
<b>Mean Sq. Residuals:</b> The mean sum of squares associated with the residuals. This is calculated as Sum Sq. residuals / Df residuals. In this case, this is calculated as: 139.57 / 87 = <b>1.60</b>.
<b>F Value:</b> The overall F-statistic of the ANOVA model. This is calculated as Mean Sq. program / Mean sq. Residuals. In this case, it is calculated as: 49.46 / 1.60 = <b>30.83</b>.
<b>Pr(>F):</b> The p-value associated with the F-statistic with numerator df = 2 and denominator df = 87. In this case, the p-value is <b>7.552e-11</b>, which is an extremely tiny number.
The most important value in the entire output is the p-value because this tells us whether there is a significant difference in the mean values between the three groups.
Recall that a one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null hypothesis):</b> All group means are equal.
<b>H<sub>A </sub>(alternative hypothesis):</b> At least one group mean is different from the rest.
Since the p-value in our ANOVA table (.7552e-11) is less than .05, we have sufficient evidence to reject the null hypothesis.
This means we have sufficient evidence to say that the mean weight loss experienced by the individuals is not equal between the three workout programs.
<h3>Step 4: Perform Post-Hoc Tests (If Necessary)</h3>
If the p-value in the ANOVA output is less than .05, we reject the null hypothesis. This tells us that the mean value between each group is not equal. However, it doesn’t tell us <em>which</em> groups differ from each other.
In order to find this out, we must perform a  post hoc test . In R, we can use the <b>TukeyHSD()</b> function to do so:
<b>#perform Tukey post-hoc test
TukeyHSD(model)
$program
         diff       lwr      upr     p adj
B-A 0.9777414 0.1979466 1.757536 0.0100545
C-A 2.5454024 1.7656076 3.325197 0.0000000
C-B 1.5676610 0.7878662 2.347456 0.0000199
</b>
Here’s how to interpret the results:
The adjusted p-value for the mean difference between group A and B is <b>.0100545</b>.
The adjusted p-value for the mean difference between group A and C is <b>.0000000</b>.
The adjusted p-value for the mean difference between group B and C is <b>.0000199</b>.
Since each of the adjusted p-values is less than .05, we can conclude that there is a significant difference in mean weight loss between <em>each</em> group.
<h2><span class="orange">How to Interpret Cohen’s d (With Examples)</span></h2>
In statistics, we often use  p-values  to determine if there is a statistically significant difference between the mean of two groups.
However, while a p-value can tell us whether or not there is a statistically significant difference between two groups, an  effect size  can tell us how large this difference actually is.
One of the most common measurements of effect size is <b>Cohen’s d</b>, which is calculated as:
Cohen’s d = (x<sub>1</sub> – x<sub>2</sub>) / √(s<sub>1</sub><sup>2 </sup>+ s<sub>2</sub><sup>2</sup>) / 2
where:
x<sub>1</sub> , x<sub>2</sub>: mean of sample 1 and sample 2, respectively
s<sub>1</sub><sup>2</sup>, s<sub>2</sub><sup>2</sup>: variance of sample 1 and sample 2, respectively
Using this formula, here is how we interpret Cohen’s d:
A <em>d </em>of <b>0.5</b> indicates that the two group means differ by 0.5 standard deviations.
A <em>d </em>of <b>1</b> indicates that the group means differ by 1 standard deviation.
A <em>d</em> of <b>2</b> indicates that the group means differ by 2 standard deviations.
And so on.
Here’s another way to interpret cohen’s d: An effect size of 0.5 means the value of the average person in group 1 is 0.5 standard deviations above the average person in group 2.
The following table shows the percentage of individuals in group 2 that would be below the average score of a person in group 1, based on cohen’s d.
<table><tbody>
<tr>
<th><b>Cohen’s d</b></th>
<th><b>Percentage of Group <em>2</em> who would be below average person in Group <em>1</em></b></th>
</tr>
<tr>
<td>0.0</td>
<td>50%</td>
</tr>
<tr>
<td>0.2</td>
<td>58%</td>
</tr>
<tr>
<td>0.4</td>
<td>66%</td>
</tr>
<tr>
<td>0.6</td>
<td>73%</td>
</tr>
<tr>
<td>0.8</td>
<td>79%</td>
</tr>
<tr>
<td>1.0</td>
<td>84%</td>
</tr>
<tr>
<td>1.2</td>
<td>88%</td>
</tr>
<tr>
<td>1.4</td>
<td>92%</td>
</tr>
<tr>
<td>1.6</td>
<td>95%</td>
</tr>
<tr>
<td>1.8</td>
<td>96%</td>
</tr>
<tr>
<td>2.0</td>
<td>98%</td>
</tr>
<tr>
<td>2.5</td>
<td>99%</td>
</tr>
<tr>
<td>3.0</td>
<td>99.9%</td>
</tr>
</tbody></table>
We often use the following rule of thumb when interpreting Cohen’s d:
A value of <b>0.2</b> represents a small effect size.
A value of <b>0.5</b> represents a medium effect size.
A value of <b>0.8</b> represents a large effect size.
The following example shows how to interpret Cohen’s d in practice.
<h3>Example: Interpreting Cohen’s d</h3>
Suppose a botanist applies two different fertilizers to plants to determine if there is a significant difference in average plant growth (in inches) after one month.
Here is a summary of the plant growth for each group:
<b>Fertilizer #1:</b>
x<sub>1</sub>: 15.2
s<sub>1</sub>: 4.4
<b>Fertilizer #2:</b>
x<sub>2</sub>: 14
s<sub>2</sub>: 3.6
Here is how we would calculate Cohen’s d to quantify the difference between the two group means:
Cohen’s d = (x<sub>1</sub> – x<sub>2</sub>) / √(s<sub>1</sub><sup>2 </sup>+ s<sub>2</sub><sup>2</sup>) / 2
Cohen’s d = (15.2 – 14) / √(4.4<sup>2 </sup>+ 3.6<sup>2</sup>) / 2
Cohen’s d = 0.2985
Cohen’s d is <b>0.2985</b>.
Here’s how to interpret this value for Cohen’s d: The average height of plants that received fertilizer #1 is <b>0.2985</b> standard deviations greater than the average height of plants that received fertilizer #2.
Using the rule of thumb mentioned earlier, we would interpret this to be a small effect size.
In other words, whether or not there is a statistically significant difference in the mean plant growth between the two fertilizers, the actual difference between the group means is trivial.
<h2><span class="orange">How to Interpret Cramer’s V (With Examples)</span></h2>
<b>Cramer’s V</b> is a measure of the strength of association between two  nominal variables .
It ranges from 0 to 1 where:
<b>0</b> indicates no association between the two variables.
<b>1</b> indicates a perfect association between the two variables.
It is calculated as:
<b>Cramer’s V = √(X<sup>2</sup>/n) / min(c-1, r-1)</b>
where:
<b>X<sup>2</sup>:</b> The Chi-square statistic
<b>n:</b> Total sample size
<b>r:</b> Number of rows
<b>c:</b> Number of columns
<h3>How to Interpret Cramer’s V</h3>
The following table shows how to interpret Cramer’s V based on the degrees of freedom:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Degrees of freedom</b></th>
<th style="text-align: center;"><b>Small</b></th>
<th style="text-align: center;"><b>Medium</b></th>
<th style="text-align: center;"><b>Large</b></th>
</tr>
<tr>
<td style="text-align: center;"><b>1</b></td>
<td style="text-align: center;">0.10</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">0.50</td>
</tr>
<tr>
<td style="text-align: center;"><b>2</b></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">0.35</td>
</tr>
<tr>
<td style="text-align: center;"><b>3</b></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.17</td>
<td style="text-align: center;">0.29</td>
</tr>
<tr>
<td style="text-align: center;"><b>4</b></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.25</td>
</tr>
<tr>
<td style="text-align: center;"><b>5</b></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">0.13</td>
<td style="text-align: center;">0.22</td>
</tr>
</tbody></table>
The following examples show how to interpret Cramer’s V in different situations.
<h3>Example 1: Interpreting Cramer’s V for 2×3 Table</h3>
Suppose we want to know if there is an association between eye color and gender so we survey 50 individuals and obtain the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/poly12.png">
We can use the following code in R to calculate Cramer’s V for these two variables:
<b>library(rcompanion)
#create table
data = matrix(c(6, 9, 8, 5, 12, 10), nrow=2)
#view table
data
     [,1] [,2] [,3]
[1,]    6    8   12
[2,]    9    5   10
#calculate Cramer's V
cramerV(data)
Cramer V 
  0.1671</b>
Cramer’s V turns out to be <b>0.1671</b>.
The degrees of freedom would be calculated as:
df = min(#rows-1, #columns-1)
df = min(1, 2)
df = <b>1</b>
Referring to the table above, we can see that a Cramer’s V of <b>0.1671</b> and degrees of freedom = <b>1</b> indicates a small (or “weak”) association between eye color and gender.
<h3>Example 2: Interpreting Cramer’s V for 3×3 Table</h3>
Suppose we want to know if there is an association between eye color and political party preference so we survey 50 individuals and obtain the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/cramer1.png">
We can use the following code in R to calculate Cramer’s V for these two variables:
<b>library(rcompanion)
#create table
data = matrix(c(8, 2, 4, 5, 8, 6, 6, 3, 8), nrow=3)
#view table
data
     [,1] [,2] [,3]
[1,]    8    5    6
[2,]    2    8    3
[3,]    4    6    8
#calculate Cramer's V
cramerV(data)
Cramer V 
  0.246</b>
Cramer’s V turns out to be <b>0.246</b>.
The degrees of freedom would be calculated as:
df = min(#rows-1, #columns-1)
df = min(2, 2)
df = <b>2</b>
Referring to the table above, we can see that a Cramer’s V of <b>0.246</b> and degrees of freedom = <b>2</b> indicates a medium (or “moderate”) association between eye color and political party preference.
<h2><span class="orange">How to Interpret glm Output in R (With Example)</span></h2>
The <b>glm()</b> function in R can be used to fit generalized linear models.
This function uses the following syntax:
<b>glm(formula, family=gaussian, data, …)</b>
where:
<b>formula:</b> The formula for the linear model (e.g. y ~ x1 + x2)
<b>family:</b> The statistical family to use to fit the model. Default is gaussian but other options include binomial, Gamma, and poisson among others.
<b>data:</b> The name of the data frame that contains the data
In practice, this function is used most often to fit  logistic regression models  by specifying the ‘binomial’ family.
The following example shows how to interpret the glm output in R for a logistic regression model.
<h2>Example: How to Interpret glm Output in R</h2>
For this example, we’ll use the built-in  mtcars  dataset in R:
<b>#view first six rows of <em>mtcars</em> dataset
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
We will use the variables <b>disp</b> and <b>hp</b> to predict the probability that a given car takes on a value of 1 for the <b>am</b> variable.
The following code shows how to use the <b>glm()</b> function to fit this logistic regression model:
<b>#fit logistic regression model
model &lt;- glm(am ~ disp + hp, data=mtcars, family=binomial)
#view model summary
summary(model)
Call:
glm(formula = am ~ disp + hp, family = binomial, data = mtcars)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9665  -0.3090  -0.0017   0.3934   1.3682  
Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)  1.40342    1.36757   1.026   0.3048  
disp        -0.09518    0.04800  -1.983   0.0474 *
hp           0.12170    0.06777   1.796   0.0725 .
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 43.230  on 31  degrees of freedom
Residual deviance: 16.713  on 29  degrees of freedom
AIC: 22.713
Number of Fisher Scoring iterations: 8
</b>
Here’s how to interpret each piece of the output:
<h3>Coefficients & P-Values</h3>
The <b>coefficient estimate</b> in the output indicate the average change in the log odds of the response variable associated with a one unit increase in each predictor variable.
For example, a one unit increase in the predictor variable disp is associated with an average change of -0.09518 in the log odds of the response variable am taking on a value of 1. This means that higher values of disp are associated with a lower likelihood of the am variable taking on a value of 1.
The <b>standard error</b> gives us an idea of the variability associated with the coefficient estimate. We then divide the coefficient estimate by the standard error to obtain a z value.
For example, the <b>z value</b> for the predictor variable disp is calculated as -.09518 / .048 = -1.983.
The <b>p-value</b> Pr(>|z|) tells us the probability associated with a particular z value. This essentially tells us how well each predictor variable is able to predict the value of the response variable in the model.
For example, the p-value associated with the z value for the disp variable is .0474. Since this value is less than .05, we would say that disp is a statistically significant predictor variable in the model.
Depending on your preferences, you may decide to use a significance level of .01, .05, or 0.10 to determine whether or not each predictor variable is statistically significant.
<h3>Null & Residual Deviance</h3>
The <b>null deviance</b> in the output tells us how well the response variable can be predicted by a model with only an intercept term.
The <b>residual deviance</b> tells us how well the response variable can be predicted by the specific model that we fit with <em>p</em> predictor variables. The lower the value, the better the model is able to predict the value of the response variable.
To determine if a model is “useful” we can compute the Chi-Square statistic as:
<b>X<sup>2</sup></b> = Null deviance – Residual deviance
with <em>p</em> degrees of freedom.
We can then find the p-value associated with this Chi-Square statistic. The lower the p-value, the better the model is able to fit the dataset compared to a model with just an intercept term.
For example, in our regression model we can observe the following values in the output for the null and residual deviance:
<b>Null deviance</b>: 43.23 with df = 31
<b>Residual deviance</b>: 16.713 with df = 29
We can use these values to calculate the X<sup>2</sup> statistic of the model:
X<sup>2</sup> = Null deviance – Residual deviance
X<sup>2</sup> = 43.23 – 16.713
X<sup>2</sup> = 26.517
There are <em>p</em> = 2 predictor variables degrees of freedom.
We can use the  Chi-Square to P-Value Calculator  to find that a X<sup>2</sup> value of 26.517 with 2 degrees of freedom has a p-value of 0.000002.
Since this p-value is much less than .05, we would conclude that the model is highly useful.
<h3>AIC</h3>
The Akaike information criterion (<b>AIC</b>) is a metric that is used to compare the fit of different regression models. The lower the value, the better the regression model is able to fit the data.
It is calculated as:
AIC = 2K – 2<em>ln</em>(L)
where:
<b>K:</b> The number of model parameters.
<b><em>ln</em>(L)</b>: The log-likelihood of the model. This tells us how likely the model is, given the data.
The actual value for the AIC is meaningless.
However, if you fit several regression models, you can compare the AIC value of each model. The model with the lowest AIC offers the best fit.
<b>Related:</b>  What is Considered a Good AIC Value? 
<h2><span class="orange">How to Interpret Log-Likelihood Values (With Examples)</span></h2>
The <b>log-likelihood value</b> of a regression model is a way to measure the goodness of fit for a model. The higher the value of the log-likelihood, the better a model fits a dataset.
The log-likelihood value for a given model can range from negative infinity to positive infinity. The actual log-likelihood value for a given model is mostly meaningless, but <b>it’s useful for comparing two or more models</b>.
In practice, we often fit several regression models to a dataset and choose the model with the highest log-likelihood value as the model that fits the data best.
The following example shows how to interpret log-likelihood values for different regression models in practice.
<h3>Example: Interpreting Log-Likelihood Values</h3>
Suppose we have the following dataset that shows the number of bedrooms, number of bathrooms, and selling price of 20 different houses in a particular neighborhood:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/logLik1.png">
Suppose we’d like to fit the following two regression models and determine which one offers a better fit to the data:
<b>Model 1</b>: Price = β<sub>0</sub> + β<sub>1</sub>(number of bedrooms)
<b>Model 2</b>: Price = β<sub>0</sub> + β<sub>1</sub>(number of bathrooms)
The following code shows how to fit each regression model and calculate the log-likelihood value of each model in R:
<b>#define data
df &lt;- data.frame(beds=c(1, 1, 1, 2, 2, 2, 2, 3, 3, 3,        3, 3, 3, 3, 4, 4, 4, 5, 5, 6), baths=c(2, 1, 4, 3, 2, 2, 3, 5, 4, 3,         4, 4, 3, 4, 2, 4, 3, 5, 6, 7), price=c(120, 133, 139, 185, 148, 160, 192, 205, 244, 213,         236, 280, 275, 273, 312, 311, 304, 415, 396, 488))
#fit models
model1 &lt;- lm(price~beds, data=df)
model2 &lt;- lm(price~baths, data=df)
#calculate log-likelihood value of each model
logLik(model1)
'log Lik.' -91.04219 (df=3)
logLik(model2)
'log Lik.' -111.7511 (df=3)
</b>
The first model has a higher log-likelihood value (<b>-91.04</b>) than the second model (<b>-111.75</b>), which means the first model offers a better fit to the data.
<h3>Cautions on Using Log-Likelihood Values</h3>
When calculating log-likelihood values, it’s important to note that adding more predictor variables to a model will almost always increase the log-likelihood value even if the additional predictor variables aren’t statistically significant.
This means you should only compare the log-likelihood values between two regression models if each model has the same number of predictor variables.
To compare models with different numbers of predictor variables, you can perform a  likelihood-ratio test  to compare the goodness of fit of two nested regression models.
<h2><span class="orange">How to Interpret Margin of Error (With Examples)</span></h2>
In statistics, <b>margin of error</b> is used to assess how precise some estimate is of a population proportion or a population mean.
We use typically use margin of error when calculating confidence intervals for  population parameters .
The following examples show how to calculate and interpret margin of error for a population proportion and a population mean.
<h3>Example 1: Interpret Margin of Error for Population Proportion</h3>
We use the following formula to calculate a confidence interval for a population proportion:
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
The portion of the equation that comes after the +/- sign represents the margin of error:
<b>Margin of Error = z*(√p(1-p) / n)</b>
For example, suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. 
Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p = 0.56</b>
Suppose we would like to calculate a 95% confidence interval for the true proportion of residents in the county that are in favor of the law.
Using the formula above, we calculate the margin of error to be:
<b>Margin of Error = z*(√p(1-p) / n)</b>
<b>Margin of Error = 1.96*(√.56(1-.56) / 100)</b>
<b>Margin of Error = .0973</b>
We can then calculate the 95% confidence interval to be:
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
<b>Confidence Interval = .56  +/-  .0973</b>
<b>Confidence Interval = [.4627, .6573]</b>
The 95% confidence interval for the proportion of residents in the county who are in favor of the law turns out to be <b> [.4627, .6573]</b>.
This means we’re 95% confident that the true proportion of residents who support the law is between 46.27% and 65.73%.
The proportion of residents in the sample who were in favor of the law was 56%, but by subtracting and adding the margin of error to this sample proportion we’re able to construct a confidence interval.
This confidence interval represents a range of values that are highly likely to contain the true proportion of residents in the county who are in favor of the law.
<h3>Example 2: Interpret Margin of Error for Population Mean</h3>
We use the following formula to calculate a confidence interval for a population mean:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>z: </b>the z-critical value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The portion of the equation that comes after the +/- sign represents the margin of error:
<b>Margin of Error = z*(s/√n)</b>
For example, suppose we want to estimate the mean weight of a population of dolphins. We collect a  random sample  of dolphins with the following information:
Sample size <b>n = 40</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
Using the formula above, we calculate the margin of error to be:
<b>Margin of Error = z*(s/√n)</b>
<b>Margin of Error = 1.96*(18.5/√40)</b>
<b>Margin of Error = 5.733</b>
We can then calculate the 95% confidence interval to be:
<b>Confidence Interval = x  +/-  z*(s/√n)</b>
<b>Confidence Interval = 300 +/- 5.733</b>
<b>Confidence Interval =[294.267, 305.733]</b>
The 95% confidence interval for the mean weight of dolphins in this population turns out to be <b>[294.267, 305.733]</b>.
This means we’re 95% confident that the true mean weight of dolphins in this population is between 294.267 pounds and 305.733 pounds.
The mean weight of dolphins in the sample was 300 pounds, but by subtracting and adding the margin of error to this sample mean we’re able to construct a confidence interval.
This confidence interval represents a range of values that are highly likely to contain the true mean weight of dolphins in this population.
<h2><span class="orange">How to Interpret an Odds Ratio Less Than 1</span></h2>
In statistics, an <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group compared to the odds of an event occurring in a control group.
Odds ratios appear most often in  logistic regression , which is a method we use to fit a regression model that has one or more predictor variables and a binary response variable.
One question students often have regarding odds ratios in logistic regression models is: <em><b>How do I interpret an odds ratio less than 1?</b></em>
Here’s how:
<b>If a predictor variable in a logistic regression model has an odds ratio less than 1, it means that a one unit increase in that variable is associated with a <em>decrease</em> in the odds of the response variable occurring.</b>
The following two examples show how to interpret an odds ratio less than 1 for both a continuous variable and a categorical variable.
<h3>Example 1: Interpreting Odds Ratios for Continuous Variables</h3>
Suppose we want to understand the relationship between a mother’s age and the probability of having a baby with a healthy birthweight.
To explore this, we can perform logistic regression using age as a predictor variable and healthy birthweight (no = 0, yes =1) as a  response variable .
Suppose we collect data for 200 mothers and fit a logistic regression model. Here are the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/oddsLess1.png">
The odds ratio for the predictor variable <em>age</em> is less than 1. This means that each additional increase of one year in age is associated with a decrease in the odds of a mother having a healthy baby.
In particular, we can use the following formula to quantify the change in the odds:
Change in Odds %: (OR-1) * 100
For example, the odds ratio (OR) for age is 0.92. Thus, we could calculate:
Change in Odds %: (0.92 – 1) * 100 = <b>-8%</b>
This means that each additional increase of one year in age is associated with an <b>8% decrease</b> in the odds of a mother having a healthy baby.
<h3>Example 2: Interpreting Odds Ratios for Categorical Variables</h3>
Suppose we want to understand the relationship between a mother’s smoking habits and the probability of having a baby with a healthy birthweight.
To explore this, we can perform logistic regression using smoking as a predictor variable (no = 0, yes = 1) and healthy birthweight (no = 0, yes =1) as a response variable.
Suppose we collect data for 200 mothers and fit a logistic regression model. Here are the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/oddsLess2.png">
The odds ratio for the predictor variable <em>smoking</em> is less than 1. This means that increasing from 0 to 1 for smoking (i.e. going from a non-smoker to a smoker) is associated with a decrease in the odds of a mother having a healthy baby.
Once again, we can use the following formula to quantify the change in the odds:
Change in Odds %: (OR-1) * 100
For example, the odds ratio (OR) for smoking is 0.85. Thus, we could calculate:
Change in Odds %: (0.85 – 1) * 100 = <b>-15%</b>
This means that a mother who smokes experiences a reduction of <b>15%</b> in the odds of having a healthy baby compared to a mother that does not smoke.
<h2><span class="orange">How to Interpret Odds Ratios</span></h2>
In statistics, <b>probability </b>refers to the chances of some event happening. It is calculated as:
<b>PROBABILITY:</b>
 
P(event) = (# desirable outcomes) / (# possible outcomes)
For example, suppose we have four red balls and one green ball in a bag. If you close your eyes and randomly select a ball, the probability that you choose a green ball is calculated as:
P(green) = 1 / 5 =<b> 0.2</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oddsRatio1-1.png">
The <b>odds </b>of some event happening can be calculated as:
<b>ODDS:</b>
 
Odds(event) = P(event happens) / 1-P(event happens)
For example, the odds of picking a green ball are (0.2) / 1-(0.2) = 0.2 / 0.8 = <b>0.25</b>.
The <b>odds ratio </b>is the ratio of two odds.
<b>ODDS RATIO:</b>
 
Odds Ratio = Odds of Event A / Odds of Event B
For example, we could calculate the odds ratio between picking a red ball and a green ball.
The probability of picking a red ball is 4/5 = <b>0.8</b>.
The odds of picking a red ball are (0.8) / 1-(0.8) = 0.8 / 0.2 = <b>4</b>.
The <b>odds ratio </b>for picking a red ball compared to a green ball is calculated as:
Odds(red) / Odds(green) = 4 / 0.25 = <b>16</b>.
Thus, the odds of picking a red ball are 16 times larger than the odds of picking a green ball.
<h2>When Are Odds Ratios Used in the Real World?</h2>
In the real world, odds ratios are used in a variety of settings in which researchers want to compare the odds of two events occurring. Here are a couple examples.
<h3>Example #1: Interpreting Odds Ratios</h3>
Researchers want to know if a new treatment improves the odds of a patient experiencing a positive health outcome compared to an existing treatment. The following table shows the number of patients who experienced a positive or negative health outcome, based on treatment.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oddsRatio2.png">
The odds of a patient experiencing a positive outcome under the new treatment can be calculated as:
<b>Odds </b>= P(positive) / 1 – P(positive) = (50/90) / 1-(50/90)  = (50/90) / (40/90) = <b>1.25</b>
The odds of a patient experiencing a positive outcome under the existing treatment can be calculated as:
<b>Odds </b>= P(positive) / 1 – P(positive) = (42/90) / 1-(42/90)  = (42/90) / (48/90) = <b>0.875</b>
Thus, the odds ratio for experiencing a positive outcome under the new treatment compared to the existing treatment can be calculated as:
<b>Odds Ratio </b> = 1.25 / 0.875 = <b>1.428</b>.
We would interpret this to mean that the odds that a patient experiences a positive outcome using the new treatment are <b>1.428</b> <b>times</b> <b>the</b> <b>odds</b> that a patient experiences a positive outcome using the existing treatment.
In other words, the odds of experiencing a positive outcome are increased by <b>42.8%</b> under the new treatment.
<h3>Example #2: Interpreting Odds Ratios</h3>
Marketers want to know if one advertisement causes customers to buy a certain item more often than another advertisement so they show each advertisement to 100 individuals. The following table shows the number of people who bought the item, based on which advertisement they saw:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oddsRatio3.png">
The odds of an individual buying the item after seeing the first advertisement can be calculated as:
<b>Odds </b>= P(bought) / 1 – P(bought) = (73/100) / 1-(73/100)  = (73/100) / (27/100) = <b>2.704</b>
The odds of an individual buying the item after seeing the second advertisement can be calculated as:
<b>Odds </b>= P(bought) / 1 – P(bought) = (65/100) / 1-(65/10)  = (65/100) / (35/100) = <b>1.857</b>
Thus, the odds ratio for a customer buying the item after seeing the first advertisement compared to buying after seeing the second advertisement can be calculated as:
<b>Odds Ratio </b> = 2.704 / 1.857 = <b>1.456</b>.
We would interpret this to mean that the odds that an individual buys the item after seeing the first advertisement are <b>1.456</b> <b>times</b> <b>the</b> <b>odds</b> that an individual buys the item after seeing the second advertisement.
In other words, the odds of buying the item are increased by <b>45.6%</b> using the first advertisement.
<h2><span class="orange">How to Interpret Pr(>|t|) in Regression Model Output in R</span></h2>
Whenever you perform linear regression in R, the output of your regression model will be displayed in the following format:
<b>Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  10.0035     5.9091   1.693   0.1513  
x1            1.4758     0.5029   2.935   0.0325 *
x2           -0.7834     0.8014  -0.978   0.3732 
</b>
The <b>Pr(>|t|)</b> column represents the p-value associated with the value in the <b>t value</b> column.
If the p-value is less than a certain significance level (e.g. α = .05) then the predictor variable is said to have a statistically significant relationship with the response variable in the model.
The following example shows how to interpret values in the Pr(>|t|) column for a given regression model.
<h3>Example: How to Interpret Pr(>|t|) Values</h3>
Suppose we would like to fit a  multiple linear regression model  using predictor variables <b>x1</b> and <b>x2</b> and a single response variable <b>y</b>.
The following code shows how to create a data frame and fit a regression model to the data:
<b>#create data frame
df &lt;- data.frame(x1=c(1, 3, 3, 4, 4, 5, 6, 6), x2=c(7, 7, 5, 6, 5, 4, 5, 6), y=c(8, 8, 9, 9, 13, 14, 17, 14))
#fit multiple linear regression model
model &lt;- lm(y ~ x1 + x2, data=df)
#view model summary
summary(model)
Call:
lm(formula = y ~ x1 + x2, data = df)
Residuals:
      1       2       3       4       5       6       7       8 
 2.0046 -0.9470 -1.5138 -2.2062  1.0104 -0.2488  2.0588 -0.1578 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  10.0035     5.9091   1.693   0.1513  
x1            1.4758     0.5029   2.935   0.0325 *
x2           -0.7834     0.8014  -0.978   0.3732  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 1.867 on 5 degrees of freedom
Multiple R-squared:  0.7876,Adjusted R-squared:  0.7026 
F-statistic: 9.268 on 2 and 5 DF,  p-value: 0.0208</b>
Here’s how to interpret the values in the Pr(>|t|) column:
The p-value for the predictor variable x1 is <b>.0325</b>. Since this value is less than .05, it has a statistically significant relationship with the response variable in the model.
The p-value for the predictor variable x2 is <b>.3732</b>. Since this value is not less than .05, it does not have a statistically significant relationship with the response variable in the model.
The  significance codes  under the coefficient table tell us that a single asterik (*) next to the p-value of .0325 means the p-value is statistically significant at α = .05.
<h3>How is Pr(>|t|) Actually Calculated?</h3>
Here’s how the value for Pr(>|t|) is actually calculated:
<b>Step 1: Calculate the t value</b>
First, we calculate the <b>t value</b> using the following formula:
<b>t value</b> = Estimate / Std. Error
For example, here’s how to calculate the t value for the predictor variable x1:
<b>#calculate t-value
1.4758 / .5029
[1] 2.934579
</b>
<b>Step 2: Calculate the p-value</b>
Next, we calculate the p-value. This represents the probability that the absolute value of the t-distribution is greater than 2.935.
We can use the following formula in R to calculate this value:
<b>p-value</b> = 2 * pt(abs(t value), residual df, lower.tail = FALSE)
For example, here’s how to calculate the p-value for a t-value of 2.935 with 5 residual degrees of freedom:
<b>#calculate p-value
2 * pt(abs(2.935), 5, lower.tail = FALSE)
[1] 0.0324441
</b>
Notice that this p-value matches the p-value in the regression output from above.
<b>Note:</b> The value for the residual degrees of freedom can be found near the bottom of the regression output. In our example, it turned out to be 5:
<b>Residual standard error: 1.867 on 5 degrees of freedom
</b>
<h2><span class="orange">How to Interpret Pr(>|z|) in Logistic Regression Output in R</span></h2>
Whenever you perform logistic regression in R, the output of your regression model will be displayed in the following format:
<b>Coefficients:
              Estimate Std. Error z value Pr(>|z|)  
(Intercept) -17.638452   9.165482  -1.924   0.0543 .
disp         -0.004153   0.006621  -0.627   0.5305  
drat          4.879396   2.268115   2.151   0.0315 * 
</b>
The <b>Pr(>|z|)</b> column represents the p-value associated with the value in the <b>z value</b> column.
If the p-value is less than a certain significance level (e.g. α = .05) then this indicates that the predictor variable has a statistically significant relationship with the  response variable  in the model.
The following example shows how to interpret values in the Pr(>|z|) column for a logistic regression model in practice.
<h3>Example: How to Interpret Pr(>|z|) Values</h3>
The following code shows how to fit a  logistic regression model  in R using the built-in <b>mtcars</b> dataset:
<b>#fit logistic regression model
model &lt;- glm(am ~ disp + drat, data=mtcars, family=binomial)
#view model summary
summary(model)
Call:
glm(formula = am ~ disp + drat, family = binomial, data = mtcars)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5773  -0.2273  -0.1155   0.5196   1.8957  
Coefficients:
              Estimate Std. Error z value Pr(>|z|)  
(Intercept) -17.638452   9.165482  -1.924   0.0543 .
disp         -0.004153   0.006621  -0.627   0.5305  
drat          4.879396   2.268115   2.151   0.0315 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 43.230  on 31  degrees of freedom
Residual deviance: 21.268  on 29  degrees of freedom
AIC: 27.268
Number of Fisher Scoring iterations: 6</b>
Here’s how to interpret the values in the Pr(>|z|) column:
The p-value for the predictor variable “disp” is <b>.5305</b>. Since this value is not less than .05, it does not have a statistically significant relationship with the response variable in the model.
The p-value for the predictor variable “drat” is <b>.0315</b>. Since this value is less than .05, it has a statistically significant relationship with the response variable in the model.
The  significance codes  under the coefficient table tell us that a single asterisk (*) next to the p-value of .0315 means the p-value is statistically significant at α = .05.
<h3>How is Pr(>|z|) Calculated?</h3>
Here’s how the value for Pr(>|z|) is actually calculated:
<b>Step 1: Calculate the z value</b>
First, we calculate the <b>z value</b> using the following formula:
<b>z value</b> = Estimate / Std. Error
For example, here’s how to calculate the z value for the predictor variable “drat”:
<b>#calculate z-value
4.879396 / 2.268115
[1] 2.151
</b>
<b>Step 2: Calculate the p-value</b>
Next, we calculate the two-tailed p-value. This represents the probability that the absolute value of the normal distribution is greater than 2.151 or less than -2.151.
We can use the following formula in R to calculate this value:
<b>p-value</b> = 2 * (1-pnorm(z value))
For example, here’s how to calculate the two-tailed p-value for a z-value of 2.151:
<b>#calculate p-value
2*(1-pnorm(2.151))
[1] 0.0314762
</b>
Notice that this p-value matches the p-value in the regression output from above.
<h2><span class="orange">How to Interpret Regression Output in Excel</span></h2>
Multiple linear regression is one of the most commonly used techniques in all of statistics.
This tutorial explains how to interpret every value in the output of a multiple linear regression model in Excel.
<h3>Example: Interpreting Regression Output in Excel</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain college entrance exam.
To explore this relationship, we can perform multiple linear regression using <b>hours studied</b> and <b>prep exams taken </b>as predictor variables and <b>exam score </b>as a response variable.
The following screenshot shows the regression output of this model in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
Here is how to interpret the most important values in the output: 
<b>Multiple R: 0.857</b>. This represents the multiple correlation between the response variable and the two predictor variables.
<b>R Square: 0.734</b>. This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, 73.4% of the variation in the exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>Adjusted R Square: 0.703</b>. This represents the R Square value, <em>adjusted for the number of predictor variables in the model</em>. This value will also be less than the value for R Square and penalizes models that use too many predictor variables in the model.
<b>Standard error:</b> <b>5.366</b>. This is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of 5.366 units from the regression line.
<b>Observations: 20</b>. The total sample size of the dataset used to produce the regression model.
<b>F: 23.46</b>. This is the overall F statistic for the regression model, calculated as regression MS / residual MS.
<b>Significance F: 0.0000</b>. This is the p-value associated with the overall F statistic. It tells us whether or not the regression model as a whole is statistically significant.
In this case the p-value is less than 0.05, which indicates that the explanatory variables <b>hours studied</b> and <b>prep exams taken </b>combined have a statistically significant association with <b>exam score</b>.
<b>Coefficients: </b>The coefficients for each explanatory variable tell us the average expected change in the response variable, assuming the other explanatory variable remains constant.
For example, for each additional hour spent studying, the average exam score is expected to increase by <b>5.56</b>, assuming that <b>prep exams taken </b>remains constant.
We interpret the coefficient for the intercept to mean that the expected exam score for a student who studies zero hours and takes zero prep exams is <b>67.67</b>.
<b>P-values. </b>The individual p-values tell us whether or not each explanatory variable is statistically significant. We can see that <b>hours studied </b>is statistically significant (p = 0.00) while <b>prep exams taken </b>(p = 0.52) is not statistically significant at α = 0.05.
<h3>How to Write the Estimated Regression Equation</h3>
We can use the coefficients from the output of the model to create the following estimated regression equation:
<b>Exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams)</b>
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study and the number of prep exams they take.
For example, a student who studies for three hours and takes one prep exam is expected to receive a score of <b>83.75</b>:
Exam score = 67.67 + 5.56*(3) – 0.60*(1) = 83.75
Keep in mind that because <b>prep exams taken </b>was not statistically significant (p = 0.52), we may decide to remove it because it doesn’t add any improvement to the overall model.
In this case, we could perform simple linear regression using only <b>hours studied </b>as the explanatory variable.
<h2><span class="orange">How to Interpret Regression Output in R</span></h2>
To fit a  linear regression model  in R, we can use the <b>lm()</b> command.
To view the output of the regression model, we can then use the <b>summary()</b> command.
This tutorial explains how to interpret every value in the regression output in R.
<h3>Example: Interpreting Regression Output in R</h3>
The following code shows how to fit a multiple linear regression model with the built-in <b>mtcars</b> dataset using <em>hp</em>, <em>drat</em>, and <em>wt</em> as predictor variables and <em>mpg</em> as the response variable:
<b>#fit regression model using hp, drat, and wt as predictors
model &lt;- lm(mpg ~ hp + drat + wt, data = mtcars)
#view model summary
summary(model)
Call:
lm(formula = mpg ~ hp + drat + wt, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-3.3598 -1.8374 -0.5099  0.9681  5.7078 
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 29.394934   6.156303   4.775 5.13e-05 ***
hp          -0.032230   0.008925  -3.611 0.001178 ** 
drat         1.615049   1.226983   1.316 0.198755    
wt          -3.227954   0.796398  -4.053 0.000364 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 2.561 on 28 degrees of freedom
Multiple R-squared:  0.8369,Adjusted R-squared:  0.8194 
F-statistic: 47.88 on 3 and 28 DF,  p-value: 3.768e-11
</b>
Here is how to interpret every value in the output:
<h3>Call</h3>
<b>Call:
lm(formula = mpg ~ hp + drat + wt, data = mtcars)</b>
This section reminds us of the formula that we used in our regression model. We can see that we used<b> mpg </b>as the response variable and <b>hp</b>, <b>drat</b>, and <b>wt</b> as our predictor variables. Each variable came from the dataset called <b>mtcars</b>.
<h3>Residuals</h3>
<b>Residuals:
    Min      1Q  Median      3Q     Max 
-3.3598 -1.8374 -0.5099  0.9681  5.7078 
</b>
This section displays a summary of the distribution of residuals from the regression model. Recall that a residual is the difference between the observed value and the predicted value from the regression model.
The minimum residual was <b>-3.3598</b>, the median residual was <b>-0.5099</b> and the max residual was <b>5.7078</b>.
<h3>Coefficients</h3>
<b>Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 29.394934   6.156303   4.775 5.13e-05 ***
hp          -0.032230   0.008925  -3.611 0.001178 ** 
drat         1.615049   1.226983   1.316 0.198755    
wt          -3.227954   0.796398  -4.053 0.000364 ***
---
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</b>
This section displays the estimated coefficients of the regression model. We can use these coefficients to form the following estimated regression equation:
mpg = 29.39 – .03*hp + 1.62*drat – 3.23*wt
For each predictor variable, we’re given the following values:
<b>Estimate:</b> The estimated coefficient. This tells us the average increase in the response variable associated with a one unit increase in the predictor variable, assuming all other predictor variables are held constant.
<b>Std.</b> <b>Error</b>: This is the standard error of the coefficient. This is a measure of the uncertainty in our estimate of the coefficient.
<b>t value:</b> This is the t-statistic for the predictor variable, calculated as (Estimate) / (Std. Error).
<b>Pr(>|t|):</b> This is the p-value that corresponds to the t-statistic. If this value is less than some alpha level (e.g. 0.05) than the predictor variable is said to be statistically significant.
If we used an alpha level of α = .05 to determine which predictors were significant in this regression model, we’d say that <b>hp</b> and <b>wt</b> are statistically significant predictors while <b>drat</b> is not.
<h3>Assessing Model Fit</h3>
<b>Residual standard error: 2.561 on 28 degrees of freedom
Multiple R-squared:  0.8369,Adjusted R-squared:  0.8194 
F-statistic: 47.88 on 3 and 28 DF,  p-value: 3.768e-11
</b>
This last section displays various numbers that help us assess how well the regression model fits our dataset.
<b>Residual standard error:</b> This tells us  the average distance that the observed values fall from the regression line. The smaller the value, the better the regression model is able to fit the data.
The degrees of freedom is calculated as n-k-1 where n = total observations and k = number of predictors. In this example, mtcars has 32 observations and we used 3 predictors in the regression model, thus the degrees of freedom is 32 – 3 – 1 = 28.
<b>Multiple R-Squared:</b> This is known as the coefficient of determination. It tells us the proportion of the variance in the  response variable  that can be explained by the predictor variables.
This value ranges from 0 to 1. The closer it is to 1, the better the predictor variables are able to predict the value of the response variable.
<b>Adjusted R-squared:</b> Ths is a modified version of R-squared that has been adjusted for the number of predictors in the model. It is always lower than the R-squared.
The adjusted R-squared can be useful for comparing the fit of different regression models that use different numbers of predictor variables.
<b>F-statistic:</b> This indicates whether the regression model provides a better fit to the data than a model that contains no independent variables. In essence, it tests if the regression model as a whole is useful.
<b>p-value:</b> This is the p-value that corresponds to the F-statistic. If this value is less than some significance level (e.g. 0.05), then the regression model fits the data better than a model with no predictors.
When building regression models, we hope that this p-value is less than some significance level because it indicates that the predictor variables are actually useful for predicting the value of the response variable.
<h2><span class="orange">How to Interpret Relative Risk (With Examples)</span></h2>
In statistics, <b>relative risk</b> refers to the probability of an event occurring in a treatment group compared to the probability of an event occurring in a control group.
It is calculated as:
Relative Risk = (Prob. of event in treatment group) / (Prob. of event in control group)
As a rule of thumb, here’s how to interpret the values for relative risk:
<b>Relative Risk &lt; 1</b>: The event is less likely to occur in the treatment group
<b>Relative Risk = 1</b>: The event is equally likely to occur in each group
<b>Relative Risk > 1</b>: The event is more likely to occur in the treatment group
The following examples show how to interpret relative risk values in practice.
<h3>Example 1: Relative Risk &lt; 1</h3>
Suppose we want to know if exercise affects the risk of developing some disease.
We collect data and find that 28% of people who exercise regularly develop this disease while 50% of people who do not exercise regularly develop this disease.
In this scenario, we would calculate the relative risk as:
Relative Risk = P(event in treatment group) / P(event in control group)
Relative Risk = P(disease with exercise) / P(disease with no exercise)
Relative Risk = 0.28 / 0.50
Relative Risk = <b>0.56</b>
Since the relative risk is less than 1, this tells us that this disease is less likely to develop in individuals who exercise.
Specifically, we could say that an individual is 44% less likely (1 – .56 = .44) to develop this disease if they exercise regularly.
<h3>Example 2: Relative Risk = 1</h3>
Suppose we want to know if some new studying program affects the ability of students to pass a particular exam.
We collect data and find that 40% of students who use the new studying program pass the exam while 40% of students who do not use the studying program also pass the exam.
In this scenario, we would calculate the relative risk as:
Relative Risk = P(event in treatment group) / P(event in control group)
Relative Risk = P(pass with new program) / P(pass without new program)
Relative Risk = 0.40 / 0.40
Relative Risk = <b>1</b>
Since the relative risk is equal to 1, this tells us that an individual is equally likely to pass the exam whether they use the new studying program or not.
<h3>Example 3: Relative Risk > 1</h3>
Suppose we want to know if smoking affects the risk of developing lung cancer
We collect data and find that 70% of people who smoke develop lung cancer while 5% of people who do not smoke develop lung cancer.
In this scenario, we would calculate the relative risk as:
Relative Risk = P(event in treatment group) / P(event in control group)
Relative Risk = P(lung cancer with smoking) / P(lung cancer without smoking)
Relative Risk = 0.70 / 0.05
Relative Risk = <b>14</b>
Since the relative risk is greater than 1, this tells us that an individual is more likely to develop lung cancer if they smoke.
Specifically, we could say that an individual is 14 times more likely to develop lung cancer if they smoke.
<h3>Interpreting Relative Risk in a Contingency Table</h3>
Often you may have to calculate and interpret relative risk using a 2×2 table, which takes the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
We can use the following formula to calculate relative risk in a 2×2 table:
<b>Relative risk</b> = [A/(A+B)] / [C/(C+D)]
For example, suppose 50 basketball players use a new training program and 50 players use an old training program. At the end of the program we test each player to see if they pass a certain skills test. 
The following 2×2 table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/odds1.png">
We would calculate the relative risk as:
Relative Risk = [A/(A+B)] / [C/(C+D)]
Relative Risk = [34/(34+16)] / [39/(39+11)]
Relative Risk = 0.68 / 0.78
Relative Risk = <b>0.872</b>
Since the relative risk is less than 1, it indicates that the probability of passing is lower under the new program compared to the old program.
Specifically, we could say that an individual is 12.8% less likely (1 – .872 = .128) to pass the skills test if they use the new program.
<h2><span class="orange">How to Interpret a ROC Curve (With Examples)</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when the outcome is indeed positive.
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when the outcome is indeed negative.
An easy way to visualize these two metrics is by creating a <b>ROC curve</b>, which is a plot that displays the sensitivity and specificity of a logistic regression model.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/read_roc1.png">
This tutorial explains how to create and interpret a ROC curve.
<h3>How to Create a ROC Curve</h3>
Once we’ve fit a logistic regression model, we can use the model to classify  observations  into one of two categories.
For example, we might classify observations as either “positive” or “negative.”
The <b>true positive rate</b> represents the proportion of observations that are predicted to be positive when indeed they are positive.
Conversely, the <b>false positive rate</b> represents the proportion of observations that are predicted to be positive when they’re actually negative.
When we create a ROC curve, we plot pairs of the true positive rate vs. the false positive rate for every possible decision threshold of a logistic regression model.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/read_roc1.png">
<h3>How to Interpret a ROC Curve</h3>
The more that the ROC curve hugs the top left corner of the plot, the better the model does at classifying the data into categories.
To quantify this, we can calculate the <b>AUC</b> (area under the curve) which tells us how much of the plot is located under the curve.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/read_roc2.png">
The closer AUC is to 1, the better the model.
A model with an AUC equal to 0.5 would be a perfectly diagonal line and it would represent a model that is no better than a model that makes random classifications.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/read_roc4.png">
It’s particularly useful to calculate the AUC for multiple logistic regression models because it allows us to see which model is best at making predictions.
For example, suppose we fit three different logistic regression models and plot the following ROC curves for each model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/read_roc3.png">
Suppose we calculate the AUC for each model as follows:
<b>Model A:</b> AUC = 0.923
<b>Model B:</b> AUC = 0.794
<b>Model C:</b> AUC = 0.588
Model A has the highest AUC, which indicates that it has the highest area under the curve and is the best model at correctly classifying observations into categories.
 How to Plot a ROC Curve in R 
 How to Plot a ROC Curve in Python 
 How to Create a ROC Curve in Excel 
 How to Create and Interpret a ROC Curve in SPSS 
 How to Create and Interpret a ROC Curve in Stata 
<h2><span class="orange">Complete Guide: How to Interpret t-test Results in Excel</span></h2>
A  two sample t-test  is used to test whether or not the means of two populations are equal.
This tutorial provides a complete guide on how to interpret the results of a two sample t-test in Excel.
<h3>Step 1: Create the Data</h3>
Suppose a biologist want to know whether or not two different species of plants have the same mean height.
To test this, she collects a  simple random sample  of 20 plants from each species:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel1.png">
<h3>Step 2: Perform the Two Sample t-test</h3>
To perform a two sample t-test in Excel, click the <b>Data</b> tab along the top ribbon and then click <b>Data Analysis</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option to click on, you need to first  download the Analysis ToolPak .
In the window that appears, click the option titled <b>t-Test: Two-Sample Assuming Equal Variances</b> and then click <b>OK</b>. Then enter the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel5.png">
Once you click <b>OK</b>, the results of the t-test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel6.png">
<h3>Step 3: Interpret the Results</h3>
Here is how to interpret each line in the results:
<b>Mean: </b>The mean of each sample.
Sample 1 Mean: 15.15
Sample 2 Mean: 15.8
<b>Variance: </b>The variance of each sample.
Sample 1 Variance: 8.13
Sample 2 Variance: 12.9
<b>Observations: </b>The number of observations in each sample.
Sample 1 Observations: 20
Sample 2 Observations: 20
<b>Pooled Variance: </b>The average variance of the samples, calculated by “pooling” the variances of each sample together using the following formula:
 s<sup>2</sup><sub>p</sub> = ((n<sub>1</sub>-1)s<sup>2</sup><sub>1</sub> + (n<sub>2</sub>-1)s<sup>2</sup><sub>2</sub>) / (n<sub>1</sub>+n<sub>2</sub>-2)
s<sup>2</sup><sub>p</sub> = ((20-1)8.13 + (20-1)12.9) / (20+20-2)
s<sup>2</sup><sub>p</sub> = 10.51974
<b>Hypothesized mean difference: </b>The number that we “hypothesize” is the difference between the two population means. In this case, we chose <b>0</b> because we want to test whether or not the difference between the two populations means is 0.
<b>df: </b>The degrees of freedom for the t-test, calculated as:
df = n<sub>1</sub> + n<sub>2</sub> – 2
df = 20 + 20 – 2
df = 38
<b>t Stat: </b>The test statistic <em>t</em>, calculated as:
 <em>t </em> = (x<sub>1</sub> – x<sub>2</sub>) / √s<sup>2</sup><sub>p</sub>(1/n<sub>1</sub> + 1/n<sub>2</sub>)
<em>t </em> = (15.15-15.8) / √10.51974(1/20+1/20)
<em>t</em> = -0.63374
<b>P(T&lt;=t) two-tail: </b>The p-value for a two-tailed t-test. This value can be found by using any  T Score to P Value Calculator  using t = -0.63374 with 38 degrees of freedom.
In this case, p = <b>0.530047</b>. This is larger than 0.05, so we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the two population means are different.
<b>t Critical two-tail: </b>This is the critical value of the test. This value can be found by using a  t Critical value Calculator  with 38 degrees of freedom and a 95% confidence level.
In this case, the critical value turns out to be <b>2.024394</b>. Since our test statistic <em>t </em>is less than this value, we fail to reject the null hypothesis. Once again, this means we do not have sufficient evidence to say that the two population means are different.
<b>Note #1</b>: You will arrive at the same conclusion whether you use the p-value method or the critical value method.
<b>Note #2</b>: If you are performing a  one-tailed hypothesis test , you will instead use the values for P(T&lt;=t) one-tail and t Critical one-tail.
<h2><span class="orange">Complete Guide: How to Interpret t-test Results in R</span></h2>
A  two sample t-test  is used to test whether or not the means of two populations are equal.
This tutorial provides a complete guide on how to interpret the results of a two sample t-test in R.
<h3>Step 1: Create the Data</h3>
Suppose we want to know if two different species of plants have the same mean height. To test this, we collect a  simple random sample  of 12 plants from each species.
<b>#create vector of plant heights from group 1
group1 &lt;- c(8, 8, 9, 9, 9, 11, 12, 13, 13, 14, 15, 19)
#create vector of plant heights from group 2
group2 &lt;- c(11, 12, 13, 13, 14, 14, 14, 15, 16, 18, 18, 19) </b>
<h3>Step 2: Perform & Interpret the Two Sample t-test</h3>
Next, we will use the <b>t.test()</b> command to perform a two sample t-test:
<b>#perform two sample t-test
t.test(group1, group2)
Welch Two Sample t-test
data:  group1 and group2
t = -2.5505, df = 20.488, p-value = 0.01884
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -5.6012568 -0.5654098
sample estimates:
mean of x mean of y 
 11.66667  14.75000 
</b>
Here’s how to interpret the results of the test:
<b>data:</b> This tells us the data that was used in the two sample t-test. In this case, we used the vectors called group1 and group2.
<b>t:</b> This is the t test-statistic. In this case, it is <b>-2.5505</b>.
<b>df</b>: This is the degrees of freedom associated with the t test-statistic. In this case, it’s <b>20.488</b>. Refer to the  Satterthwaire approximation  for an explanation of how this degrees of freedom value is calculated.
<b>p-value:</b> This is the p-value that corresponds to a t test-statistic of -2.5505 and df = 20.488. The p-value turns out to be <b>.01884</b>. We can confirm this value by using the  T Score to P Value calculator .
<b>alternative hypothesis:</b> This tells us the alternative hypothesis used for this particular t-test. In this case, the alternative hypothesis is that the true difference in means between the two groups is not equal to zero.
<b>95 percent confidence interval:</b> This tells us the 95% confidence interval for the true difference in means between the two groups. It turns out to be <b>[-5.601, -.5654]</b>.
<b>sample estimates:</b> This tells us the  sample mean  of each group. In this case, the sample mean of group 1 was <b>11.667</b> and the sample mean of group 2 was <b>14.75</b>.
The two hypotheses for this particular two sample t-test are as follows:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>A</sub>: </b>μ<sub>1</sub> ≠μ<sub>2</sub> (the two population means are <em>not</em> equal)
Because the p-value of our test<b> (.01884) </b>is less than alpha = 0.05, we reject the null hypothesis of the test. This means we have sufficient evidence to say that the mean height of plants between the two populations is different.
<h3>Notes</h3>
The <b>t.test()</b> function in R uses the following syntax:
t.test(x, y, alternative = “two.sided”, mu = 0, paired = FALSE, var.equal = FALSE, conf.level = 0.95)
where:
<b>x, y:</b> The names of the two vectors that contain the data.
<b>alternative:</b> The alternative hypothesis. Options include “two.sided”, “less”, or “greater.”
<b>mu:</b> The value assumed to be the true difference in means.
<b>paired:</b> Whether or not to use a paired t-test.
<b>var.equal:</b> Whether or not the variances are equal between the two groups.
<b>conf.level:</b> The confidence level to use for the test.
In our example above, we used the following assumptions:
We used a two-sided alternative hypothesis.
We tested whether or not the true difference in means was equal to zero.
We used a two sample t-test, not a paired t-test.
We didn’t make the assumption that the  variances were equal  between the groups.
We used a 95%  confidence level .
Feel free to change any of these arguments when you conduct your own t-test, depending on the particular test you want to perform.
<h2>Additional Resources</h2>
 An Introduction to the Two Sample t-test 
 Two Sample t-test Calculator 
<h2><span class="orange">How to Interpret Z-Scores (With Examples)</span></h2>
In statistics, a <b>z-score</b> tells us how many standard deviations away a given value lies from  the mean . We use the following formula to calculate a z-score:
<b>z</b> = (X – μ) / σ
where:
X is a single raw data value
μ is the mean
σ is the standard deviation
A z-score for an individual value can be interpreted as follows:
<b>Positive z-score: </b>The individual value is greater than the mean.
<b>Negative z-score: </b>The individual value is less than the mean.
<b>A z-score of 0:</b> The individual value is equal to the mean.
The larger the absolute value of the z-score, the further away an individual value lies from the mean.
The following example shows how to calculate and interpret z-scores.
<h3>Example: Calculate and Interpret Z-Scores</h3>
Suppose the scores for a certain exam are normally distributed with a mean of 80 and a standard deviation of 4.
<b>Question 1: </b>Find the z-score for an exam score of 87.
We can use the following steps to calculate the z-score:
The mean is μ = 80
The standard deviation is σ = 4
The individual value we’re interested in is X = 87
Thus, z = (X – μ) / σ  =  (87 – 80) /4 = <b>1.75</b>.
This tells us that an exam score of 87 lies <b>1.75 standard deviations <em>above</em> the mean</b>.
<b>Question 2: </b>Find the z-score for an exam score of 75.
We can use the following steps to calculate the z-score:
The mean is μ = 80
The standard deviation is σ = 4
The individual value we’re interested in is X = 75
Thus, z = (X – μ) / σ  =  (75 – 80) /4 = –<b>1.25</b>.
This tells us that an exam score of 75 lies <b>1.25 standard deviations <em>below</em> the mean</b>.
<b>Question 3: </b>Find the z-score for an exam score of 80.
We can use the following steps to calculate the z-score:
The mean is μ = 80
The standard deviation is σ = 4
The individual value we’re interested in is X = 80
Thus, z = (X – μ) / σ  =  (80 – 80) /4 = <b>0</b>.
This tells us that an exam score of 80 is <b>exactly equal to the mean</b>.
<h3>Why Are Z-Scores Useful?</h3>
Z-scores are useful because they give us an idea of how an individual value compares to the rest of a distribution.
For example, is an exam score of 87 good? Well, that depends on the mean and standard deviation of all exam scores.
If the exam scores for the whole population are normally distributed with a mean of 90 and a standard deviation of 4, we would calculate the z-score for 87 to be:
z = (X – μ) / σ  =  (87 – 90) /4 = <b>-0.75</b>.
Since this value is negative, it tells us that an exam score of 87 is actually <em>below</em> the average exam score for the population. Specifically, an exam score of 87 is <b>0.75 standard deviations below the mean</b>.
In a nutshell, z-scores give us an idea of how individual values compare to the mean.
<h3>How to Calculate Z-Scores in Practice</h3>
The following tutorials show step-by-step examples of how to calculate z-scores in different statistical software:
 How to Calculate Z-Scores in Excel 
 How to Calculate Z-Scores in R 
 How to Calculate Z-Scores in Python 
 How to Calculate Z-Scores in SPSS 
<h2><span class="orange">Interquartile Range Calculator</span></h2>
This calculator automatically finds the interquartile range for a given dataset.
Simply enter the comma-separated values of the dataset in the box below and then click the “Calculate” button:
<textarea id="text_area" name="dataset">1, 4, 8, 11, 13, 17</textarea>
<input type="button" id="button" onclick="calcIQR()" value="Calculate Interquartile Range">
<div>
Lower Quartile: <b>4</b>
<div>
Upper Quartile: <b>13</b>
<div>
Interquartile Range: 13 – 4 = <b>9</b>
<script>
function calcIQR(){
    
//get input sample data
    var _sample = document.getElementById('text_area').value.split(',').map(Number);
    // find the median of the sample
    var _median = math.median(_sample)
    // split the data by the median
    var _firstHalf = _sample.filter(function(f){ return f < _median })
    var _secondHalf = _sample.filter(function(f){ return f > _median })
    // find the medians for each split, calculate IQR
    var _25percent = math.median(_firstHalf);
    var _75percent = math.median(_secondHalf);
    var IQR = _75percent - _25percent;
    
//output results
    document.getElementById('lowerQ').innerHTML = _25percent;
document.getElementById('upperQ').innerHTML = _75percent;
document.getElementById('lowerQ2').innerHTML = _25percent;
document.getElementById('upperQ2').innerHTML = _75percent;
document.getElementById('IQR').innerHTML = IQR;
}
</script>
<h2><span class="orange">How to Calculate the Interquartile Range (IQR) in Excel</span></h2>
This tutorial explains how to calculate the <b>interquartile range </b>of a dataset in Excel.
<h2>What is the Interquartile Range?</h2>
The <b>interquartile</b> <b>range</b>, often denoted IQR, is a way to measure the spread of the middle 50% of a dataset. It is calculated as the difference between the first quartile* (Q1) and the third quartile (Q3) of a dataset. 
<em>*Quartiles are simply values that split up a dataset into four equal parts.</em>
For example, suppose we have the following dataset:
[58, 66, 71, 73, 74, 77, 78, 82, 84, 85, 88, 88, 88, 90, 90, 92, 92, 94, 96, 98]
The third quartile turns out to be <b>91</b> and the first quartile is <b>75.5</b>. Thus, the interquartile range (IQR) for this dataset is 91 – 75.5 = <b>15</b>. This tells us how spread out the middle 50% of the values are in this dataset.
<h2>How to Calculate the Interquartile Range in Excel</h2>
Microsoft Excel doesn’t have a built-in function to calculate the IQR of a dataset, but we can easily find it by using the <b>QUARTILE()</b> function, which takes the following arguments:
QUARTILE(array, quart)
<b>array: </b>the array of data you’re interested in.
<b>quart: </b>the quartile you  would like to calculate.
<h3>Example: Finding IQR in Excel</h3>
Suppose we would like to find the IQR for the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/IQR_Excel1.jpg"417">
To find the IQR, we can perform the following steps:
<b>Step 1: Find Q1</b>.
To find the first quartile, we simply type <b>=QUARTILE(A2:A17, 1) </b>into any cell we choose:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/IQR_Excel2-1.jpg">
<b>Step 2: Find Q3</b>.
To find the third quartile, we type <b>=QUARTILE(A2:A17, 3) </b>into any cell we choose:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/IQR_Excel3.jpg">
<b>Step 3: Find IQR</b>.
To find the interquartile range (IQR), we simply subtract Q1 from Q3:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/IQR_Excel4.jpg">
The IQR turns out to be 39.5 – 23.5 = <b>16</b>. This tells us how spread out the middle 50% of the values are in this particular dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/IQR_Excel5.jpg">
<h2>A Shorter Approach</h2>
Note that we could also have found the interquartile range of the dataset in the previous example by using one formula:
<b>=QUARTILE(A2:A17, 3) – QUARTILE(A2:A17, 1)</b>
This would also result in the value <b>16</b>.
<h2>Conclusion</h2>
The interquartile range only represents one way of measuring the “spread” of a dataset. Some other ways to measure spread are  the range, the standard deviation, and the variance . 
The nice part about using the IQR to measure spread is that  it’s resistant to outliers . Since it only tells us the spread of the middle 50% of the dataset, it isn’t affect by unusually small or unusually large outliers.
This makes it a preferable way to measure dispersion compared to a metric like the range, which simply tells us the difference between the largest and the smallest values in a dataset.
<b>Related: </b> How to Calculate the Midrange in Excel 

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
