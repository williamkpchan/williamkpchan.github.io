<base target="_blank"><html><head><title>train Tesseract</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "train Tesseract"
  var markerName = "h2"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>train Tesseract</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="http://christopher5106.github.io/optical/character/recognition/2015/09/01/training-optical-character-recognition-technology-tesseract.html" class="whitebut ">Training technology</a>
<a href="https://pretius.com/how-to-prepare-training-files-for-tesseract-ocr-and-improve-characters-recognition/" class="whitebut ">prepare training files for Tesseract</a>

<a href="https://www.cnblogs.com/wj-1314/p/9454656.html" class="whitebut ">Ê∑±ÂÖ•Â≠¶‰π†Tesseract-ocrËØÜÂà´‰∏≠ÊñáÂπ∂ËÆ≠ÁªÉÂ≠óÂ∫ìÁöÑÊñπÊ≥ï</a>
<a href="https://blog.csdn.net/qq_31112205/article/details/100159963" class="whitebut ">‰ΩøÁî®tesseractËÆ≠ÁªÉËá™Â∑±ÁöÑÂ≠óÂ∫ìÊèêÈ´òËØÜÂà´Áéá</a>
<a href="https://zhuanlan.zhihu.com/p/58366201" class="whitebut ">Tesseract 4.0 LSTMËÆ≠ÁªÉË∂ÖËØ¶ÁªÜÊïôÁ®ã</a>
<a href="https://zhuanlan.zhihu.com/p/40178190" class="whitebut ">python+tesseract ËÆ≠ÁªÉÂíåÁ†¥Ëß£È™åËØÅÁ†Å</a>

</div>
<pre>
<br>
<br>
<h2><span class="orange">How to prepare training files</span></h2>
Over the last few years, optical character recognition (<a href="https://en.wikipedia.org/wiki/Optical_character_recognition">OCR</a>) has become very popular. 
You can find various OCR engines which help you with the OCR process but you should consider <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a> to build your own OCR application. 
It is a very powerful tool and it&#8217;s completely free (licensed under the Apache License, Version 2.0). 
The main advantage of tesseract-ocr is its high accuracy of character recognition. 
Unfortunately, it is poorly documented so you need to put quite an effort to make use of its all features.
Tesseract is very good at recognizing multiple languages and fonts. 
It can be used as a command-line program or an embedded library in a custom application. 
We used it to develop an application that automatically reads data from ID cards. 
It worked well and we did not spent much time on development. 
But we had some problems with specific letters recognition (mixing W and H, O and 0 (zero)). 
So we had to train Tesseract how to read these fonts properly.
Looking for a solution on how to do this, I came across a couple of articles suggesting to use some third-party GUI applications, but I encountered many problems with customizing them and still didn&#8217;t meet my goals. 
Luckily, I found this <a href="http://blog.cedric.ws/how-to-train-tesseract-301">great article by C√©dric Verstraeten </a>which helped me to make it an old-fashioned command-line way. 
Unfortunately, it&#8217;s a little bit outdated and doesn&#8217;t include some details. 
In this article I will try to explain the process step by step.
<h2>What do we need before we begin?</h2>
First, you need to install tesseract-ocr (this tutorial is based on version 3.02). 
Do not forget to add the installation directory to your system path (the installer may not do it). 
You also need these applications:
<ul>
<li><a title="Cygwin" href="https://www.cygwin.com/">Cygwin</a> &#8211; if you are using Windows (or you can rewrite the scripts from this article to Windows Batch)</li>
<li>Qt-box-editor &#8211; this is the only GUI program, you&#8217;re going to need &#8211; to fix the boxes generated by Tesseract, and ensure we feed the right data into it.
<ul>
<li><a href="https://github.com/zdenop/qt-box-editor/downloads">for Windows</a> (I used version 1.08, the newer ones are for some reason not packaged with all needed libraries, what makes the installation more difficult)</li>
<li><a href="https://zdenop.github.io/qt-box-editor/">or for Unix</a> (sources)</li>
</ul>
</li>
</ul>
<h2>Let&#8217;s move on</h2>
First, you must prepare the data which you want to feed into Tesseract. 
You need one or multiple files that together contain at least 1 (but preferably more) occurrence of each glyph of your font. 
I decided that to achieve the best accuracy I should train Tesseract with images preprocessed in exactly the same way as they would be in the final application. 
In my case the font was <a href="https://en.wikipedia.org/wiki/OCR-B">OCR-B</a> &#8211; a font that is used on ID cards in Poland. 
So one of my files looked like this:

<img src="https://www.pretius.com/wp-content/uploads/2016/06/pol.ocrb_.exp0_.png">

The input files must be named accordingly to the Tesseract convention:

For example, if you had 3 .png files with English text in Arial font, their names would be:

eng.arial.exp0.png
eng.arial.exp1.png
eng.arial.exp2.png

Or in my case (14 .tif files, Polish, OCR-B):

pol.ocrb.exp0.tif
pol.ocrb.exp1.tif
...
pol.ocrb.exp13.tif

Once they are all gathered in one place and named correctly, we need to generate the box files for them. 
These files tell Tesseract where each glyph is located. 
Just open the bash console (on Windows it would be cygwin) and launch the script:

Shell

N=13 # set accordingly to the number of files that you have
for i in `seq 0 $N`; do
    tesseract pol.ocrb.exp$i.tif pol.ocrb.exp$i batch.nochop makebox
done

The first two parameters of the command are input and output file names (remember to change them accordingly), then there follow config files ('batch.nochop' and 'makebox') which tell Tesseract what to do. 
You can find them all in $TESSERACT_INSTALATION_DIR/tessdata/configs/ and $TESSERACT_INSTALATION_DIR/tessdata/tessconfigs/ (<a href="http://www.sk-spell.sk.cx/tesseract-ocr-parameters-in-302-version">here you can find the list of parameters you can use in the config files</a>). 
In this case, we are using two of them:

<li>makebox &#8211; tells Tesseract to (only) generate box files</li>
<li>batch.nochop &#8211; tells Tesseract not to use its fancy algorithms for segmenting the picture. 
If your files contain letters in a grid, you should use it, but otherwise you may want to remove it from the command.</li>

<h2>Now it&#8217;s time for some manual work</h2>
Open each file (image file, not *.box file that you generated) with qt-box-editor and correct Tesseract if it made any mistakes (if it did not, you probably don&#8217;t have to train it üôÇ ).
<img src="https://www.pretius.com/wp-content/uploads/2016/06/Przechwytywanie1.png">

<h2>Time to train Tesseract to recognize letters properly</h2>
Now we are going to generate *.traineddata file which can later be loaded to Tesseract, so it can recognize characters the way we want it.
There is yet one important thing to remember before you go further: <strong>If you are using windows make sure all of your files that you are using have the UNIX style end-of-line! </strong>If you are editing them manually you can do it with <a href="https://notepad-plus-plus.org/download/v6.9.2.html">notepad++</a> in Edit -&gt; EOL Conversion.
This is the script I used. 
Do not run it now, read it carefully. 
You will need to customize it to meet your needs.

Shell

function wrap {
    for i in `seq 0 $1`; do
        echo "$2$i$3"
    done
}

N=13 # Change this accordingly to number of files, that you want to feed to tesseract or export it as a script parameter.

# Uncomment this line if, you're rerunning the script
#rm pol.pffmtable  pol.shapetable pol.traineddata pol.unicharset unicharset font_properties pol.inttemp pol.normproto *.tr *.txt

for i in `seq 0 $N`; do
    tesseract pol.ocrb.exp$i.tif pol.ocrb.exp$i nobatch box.train
done
unicharset_extractor `wrap $N "pol.ocrb.exp" ".box"`
echo "ocrb 0 0 1 0 0" &gt; font_properties # tell Tesseract informations about the font
mftraining -F font_properties -U unicharset -O pol.unicharset `wrap $N "pol.ocrb.exp" ".tr"`
cntraining `wrap $N "pol.ocrb.exp" ".tr"`
# rename all files created by mftraing en cntraining, add the prefix pol.:
    mv inttemp pol.inttemp
    mv normproto pol.normproto
    mv pffmtable pol.pffmtable
    mv shapetable pol.shapetable
combine_tessdata pol.

The 'wrap function' is nothing special. 
Just a handy method that repeats a string a given number of times with a different number inside it (run <strong><em>wrap 10 'prefix' 'suffix' </em></strong>if you are not sure what it does). 
The most important part of the script begins after that.
<h3>Remove the old output</h3>
We need to remove all the files generated last time if we run the script again. 
It&#8217;s important because Tesseract sometimes works oddly when the output files are already there (is it a bug or a feature?). 
Remember to change the 'pol' part to 'eng' or any other language you are using (here and in every other occurrence that you will find. 
The same applies to 'ocrb').

Shell

rm pol.pffmtable pol.shapetable pol.traineddata pol.unicharset unicharset font_properties pol.inttemp pol.normproto *.tr *.txt

<h3>Training files</h3>
Now it&#8217;s time to take the box and image files and compound them into training (*.tr) files.

Shell

for i in `seq 0 $N`; do
    tesseract pol.ocrb.exp$i.tif pol.ocrb.exp$i box.train
done

This time we&#8217;re only using 'box.train' config to tell Tesseract to generate *.tr files.
<h3 id="unicharset">Unicharset</h3>
Then we are going to extract the charset from the box files (the command creates a 'unicharset' file).

Shell

unicharset_extractor `wrap $N "pol.ocrb.exp" ".box"`

We use our 'wrap' function to do it for all the files at once, no matter how many of them we have (just set the $N variable to the right value).
<h3>Font properties</h3>
Next we need to create a font_properties file.

Shell

echo "ocrb 0 0 1 0 0" &gt; font_properties # tell Tesseract informations about the font

The syntax is as follows:

fontname italic bold monospace serif fraktur

Do not forget to set the values accordingly to the properties of your font.
<h3>The final training</h3>
It is the time for what everyone has been waiting for:

mftraining -F font_properties -U unicharset -O pol.unicharset `wrap $N "pol.ocrb.exp" ".tr"`
cntraining `wrap $N "pol.ocrb.exp" ".tr"`

<h3>Rename the files</h3>
Now we have to add the language prefix to the generated files, so that they can be nicely consumed in the last step. 
This part of the script is not very sophisticated:

Shell

# rename all files created by mftraing and cntraining, add the prefix pol.:
    mv inttemp pol.inttemp
    mv normproto pol.normproto
    mv pffmtable pol.pffmtable
    mv shapetable pol.shapetable

<h3>Combine it all into a traineddata file</h3>
And the last step. 
Take all the files with pol.* (or other) prefix and combine them into pol.traineddata:

Shell

combine_tessdata pol.

Once the file is ready, you can copy it to $TESSERACT_INSTALATION_DIR/tessdata/ so you can use it from command-line or wherever else you need it (for example in a new application that uses Tesseract as a library).


<h2><span class="orange">Training your tesseract on windows 10</h2>
<a href="https://github.com/UB-Mannheim/tesseract/wiki">Tesseract</a>, an open-source OCR (Optical Character Recognition) engine developed by Google Labs and maintained by Google. 
Compared with Microsoft Office Document Imaging (MODI), we can continuously train a library to convert images into text. 
The ability is constantly enhanced; if the team needs depth, it can also use it as a template to develop an OCR engine that meets its own needs.

<h2>Installation</h2>
<a href="https://github.com/UB-Mannheim/tesseract/wiki">Installation</a>
By following the step after double click the installed package. 
After successful installation, there will be a Tesseract-OCR folder under the corresponding disk. 
Then add this path into the environment variable path.

Open the command line, enter <strong>tesseract</strong>, and press Enter to check its current state.

<h2>How to use</h2>
First prepare an image file, such as test.png.

<img src="https://miro.medium.com/max/552/1*ZMep3uHzLPKHWlWeVu94Yw.png">
Switch the command line to the target image file directory, then enter in the command line.
tesseract test.png output_1 ‚Äìl eng
tesseract imagename outputbase [-l lang] [-psm pagesegmode] [configfile‚Ä¶]
<blockquote>
imagename is the target image file name, which needs to be format suffix; outputbase is the conversion result file name; lang is the language name (you can see the language file eng.traineddata beginning with eng in the tessdata folder in Tesseract-OCR), if not marked- l eng defaults to eng.
</blockquote>
<h2>Training</h2>
Tesseract is still very strong! But it‚Äôs still not accurate enough, so is there any way we can improve the accuracy of tesseract recognition characters? Next, we will use the companion training tool <a href="https://sourceforge.net/projects/vietocr/files/jTessBoxEditor/">jTessBoxEditor</a> to train samples to improve our accuracy!
<ol><li>Merge sample file
Open jTessBoxEditor, Tools-&gt; Merge TIFF, select all the sample files, and save the merged file as num.font.exp0.tif</li></ol>
lang is the language, fontname is the font, and num is the custom number.

2. 
Generate BOX file
Open a command line and change to the directory where num.font.exp0.tif is located, enter, and generate a file named num.font.exp0.box
tesseract --psm 7 num.font.exp0.tif num.font.exp0 batch.nochop makebox
<blockquote>
Grammar:
</blockquote>tesseract  [lang].[fontname].exp[num].tif  [lang].[fontname].exp[num] batch.nochop makebox
<blockquote>
lang is the language name, fontname is the font name, and num is the serial number; in tesseract, you must pay attention to the format.
</blockquote>
3. 
Character correction
Open jTessBoxEditor, BOX Editor-&gt; Open, open num.font.exp0.tif, then edit all the bounding boxes if needed.

4. 
Define character profile
 Generate a text file named font_properties in the target folder with the content.
font 0 0 0 0 0

You can also create new file in the and line by enter:
echo test 0 0 0 0 0 &gt;font_properties

The font_properties file must be formatted as utf-8 to run the following steps.

This command can convert .txt file to utf8 format in PowerShell.
Get-Content .\test.txt | Set-Content -Encoding utf8 test-utf8.txt

5. The final step.

Run this bash file in your path. 
this will generate a trainedata file.
echo Run Tesseract for Training.. 

tesseract.exe num.font.exp0.tif num.font.exp0 nobatch box.train 
 
echo Compute the Character Set.. 

unicharset_extractor.exe num.font.exp0.box 
shapeclustering -F font_properties -U unicharset -O num.unicharset num.font.exp0.tr
mftraining -F font_properties -U unicharset -O num.unicharset num.font.exp0.trecho Clustering.. 

cntraining.exe num.font.exp0.trecho Rename Files.. 

rename normproto num.normproto 
rename inttemp num.inttemp 
rename pffmtable num.pffmtable 
rename shapetable num.shapetableecho Create Tessdata.. 

combine_tessdata.exe num.

<h2><span class="orange">Installation and Data Preparation</span></h2>
To train your font, first, you need to :

Install Tesseract <em>(you don‚Äôt say)</em></li>
Install jTessBoxEditor
This tool is used for creating and editing the ground truth to train the Tesseract. 
Note that you need Java Runtime to be able to open it which you can download<a href="https://www.java.com/en/download/"> <em>https://www.java.com/en/download/</em></a>. 
After you install Java then install jTessBoxEditor (not the FX ones) on<a href="https://sourceforge.net/projects/vietocr/files/jTessBoxEditor/"> <em>https://sourceforge.net/projects/vietocr/files/jTessBoxEditor</em></a><em>/
</em> you can open jTessBoxEditor by extracting the zip files, and run <em>train.bat </em>if you use Windows, or
 <em>jTessBoxEditor.jar </em>if you use Ubuntu</li>
[optional] A working Word Office (Windows) or LibreOffice (Ubuntu) and the .tiff file of your font. 
For example in the case above, I was using <em>OCR-A Extended </em>font type. 
You can easily download your font from google (just search font_name .tiff download). 
Install your font (just double click the .tiff file) Or, it‚Äôs better that you have a collection of images that you want to predict later as training data.
After you have prepared all the installation steps above, you are ready to train your Tesseract. 
Tesseract use ‚Äú<em>language</em>‚Äù as its model for OCR. 
There are many default languages, like <em>eng </em>(English), <em>ind </em>(Indonesian), and so on. 
We try to create a new language for Tesseract to be able to predict our Font, by creating some training data consisting of random numbers using our Font. 
There are 2 ways to do just that. 
First, if you have a collection of images consisting of just your fonts, then you can use that or, the second way, that is to type any number (or character) you want on word using your font, and use snipping tools (windows) or <em>shift key + PrintScreen </em>(Ubuntu) to capture and save it on a folder.

<img src="https://miro.medium.com/max/474/1*lq5NJvTilXjrNJUdNK4jJA.png" width="237" height="19"/>
Training data example

<img src="https://miro.medium.com/max/1132/1*kVrFsAfLrrrY44Qm_MaCog.png">

Training data example

<img src="https://miro.medium.com/max/834/1*cTpusMY6phjgx5X_5BTh4A.png">
Training data example

<img src="https://miro.medium.com/max/1066/1*jaysU3zfbcinMheP6HybgQ.png">

Training data example for multiple lines.
In my experience, 10‚Äì15 data was enough to produce an accurate (<em>subjectively) </em>model which is sufficiently accurate for both clean and some noisy images. 
Note that you should try to create as balanced data as possible, and as close as real case as possible. 
If you want to predict some images with a blue background, red font, then you should create training data with a blue background and red font.
<h2>Training the Tesseract</h2>In general, the training step of Tesseract is :

Merge training data to .tiff file using jTessBoxEditor</li>
Create a training label, by creating a .box files containing predictions of the Tesseract from .tiff file and fix each inaccurate predictions</li>
Train the tesseract
<h2>Step 1. Merge training data</h2>After you are done creating some data, open the <em>jTessBoxEditor. 
</em>At the top bar, go to ‚ÄúTools‚Äù ‚Üí ‚ÄúMerge Tiff‚Äù (or you can just use shortcut Ctrl<em> + M </em>). 
Go to the folder where you have saved your training images. 
Change the filter to PNG (or any extension your images have), select all images, and click ‚ÄúOk‚Äù. 
Then in the selection panel, type in <em>font_name.font.exp0 </em>where font_name is any name you want (this will be the name for your own new <em>Tesseract‚Äôs language).</em>
<h2>Step 2. 
Create a Training Label</h2>Open terminal, navigate to the folder where you saved your training images and .tiff file. 
As we now have the training data, how do we get the training label? Afraid not, you should not label each image manually, as we can use Tesseract and jTessBoxEditor to aid us. 
In the terminal, run below command :
tesseract --psm 6 --oem 3 <em>font_name.font.exp0.tif font_name.font.exp0 makebox</em>

Wait, why suddenly there are psm and oem? What will happen when I type the command above? If you run :
tesseract --help-psm
#or
tesseract --help-oem

You will see that psm means Page Segmentation Modes, meaning how the tesseract treats the image. 
If you want the tesseract to treat each image it sees as a single word, you can choose psm 8. 
In our case, as our images in .tiff file are a collection of single-line text, we choose psm 6. 
As for OEM, it means Ocr Engine Modes, as for tesseract there are legacy engine that works by recognizing character patterns, or using Neural Nets and LTSM engines (if you want to use LTSM, install tesseract version&gt; 4.0.0 ).
Using the above command, we want the tesseract to produce the bounding boxes and the prediction of each image in the .tiff file and save it to font_name.font.exp0.box text file. 
In case you didn‚Äôt know, the.tiff file that we produced earlier contains your training images segmented by ‚Äúpage‚Äù. 
By using the above command, it will produce a .box file containing a prediction, and bounding box of each word in the .tiff file. 
with the name font_name.font.exp0.box
Now open jTessBoxEditor, navigate to the box editor tab, and click open and select the .tiff file. 
You should see that each image on each page has its bounding boxes and prediction. 
Your job now is to fix each bounding box and its char prediction in the .box file. 
(<em>yes, this is the dullest part</em>)
<h2>Step 3. Training the tesseract</h2>After you have created the already fixed .box file and .tiff file. 
Create a new text document contains
font 0 0 0 0 0

Save it as <em>font_properties </em>into the same folder as the .tiff file and .box file. 
Now you are ready to begin the training process! (<em>finally</em>). 
Inside the folder, you should have all these files:

<img src="https://miro.medium.com/max/1368/1*VXYMNKo3oLUcu_ndWBzFew.png">

Now run this command on the terminal :
# Create a .tr file (training file)
tesseract num.font.exp0.tif font_name.font.exp0 nobatch box.train# Create a unicharset file
unicharset_extractor font_name.font.exp0.box# Create a shapetable file
shapeclustering -F font_properties -U unicharset -O font_name.unicharset font_name.font.exp0.tr# Create a pffmtable, intemp file
mftraining -F font_properties -U unicharset -O font_name.unicharset font_name.font.exp0.trecho Clustering..# Create a normproto file
cntraining font_name.font.exp0.tr

If you meet an error, you might need to use tesseract.exe, unicharset_extractor.exe, and cntraining.exe (for windows users). 
You will see some outputs in your terminal, and most importantly in the shapeclustering part. 
If your training images contain all the necessary characters, you will see that the Number of Shapes = {Number of class that you want}. 
For example, if I want to train the tesseract to be able to read the digits number correctly, then the Number of shapes equals to 10 (which is 0,1,2,3 ,‚Ä¶ , 9).
Master shape_table:Number of shapes = 10 max unichars = 1 number with multiple unichars = 0

If your number of shapes does not equal to the number of class that you want, you should go back to create training data, and try to create cleaner data
If you have done everything correctly, you will see 4 major files in your folder. 
shapetable, normproto, intemp, and pffmtable. 
Rename those files into <em>font_name.shapetable, font_name.normproto, font_name.intemp, font_name.pffmtable.</em>

<img src="https://miro.medium.com/max/898/1*i65dFFB7cF8VNo8-Mv_aMQ.png">

Then run:
combine_tessdata font_name.

After you run all the command above, you will see these files in your folder

<img src="https://miro.medium.com/max/666/1*_h56nxQo0w8HlVv72kFK-g.png">

Now copy <em>font_name.traineddata </em>to :
C:/Program Files x86/tesseract/4.0/tessdata  # Windowssudo cp /usr/shared/tesseract/4.0/tessdata # ubuntu

And you are done! Yep, because we use a small amount of data, the training itself doesn‚Äôt take hours, just seconds or maybe minutes. 
Compared to if you have to train a deep learning model (probably using an object detection model) from scratch, it‚Äôs much much faster. 
Now, the next time you run the Tesseract, you could specify your new trained language by using
tesseract -l font_name file0.png

Remember that using the default language before, the result of the above picture using the default Tesseract engine was 40293847 S565647386e2e91L0. 
Using our new trained language, the result was
Warning. 
Invalid resolution 0 dpi. 
Using 70 instead.
Estimating resolution as 147
10293847 565647382910

As you can see the result was much more accurate. 
Yay! With just a few training data and a relatively short amount of time, you have created an OCR model capable to read unique and strange font!
To further check the model‚Äôs result, you can create another .tiff file by using another image or by using the previous .tiff file. 
Open terminal and again, run :
tesseract -l font_name --psm 6 --oem 3 <em>font_name.font.exp0.tif font_name.font.exp0 makebox</em>

Now open jTessBoxEditor ‚Üí box editor ‚Üí open and select your .tiff file. 
Check if your model give more accurate prediction than the previous one. 
You should see that the prediction improved a lot.
<h2>Final Thoughts</h2>‚ÄúBut Is Tesseract the only way to go if you want an out of the box and fast OCR engine?‚Äù you may ask. 
Well of course not, there are a ton of OCR API providers out there if you are willing to take out some cash. 
In my honest opinion, Tesseract is good if your images are really-really clean (for example, a word document, a cashier bill, and so on. 
If your images data contains many noises, you can use thresholding to differentiate the background and the noise from the font itself. 
In my experience, using as little as 10‚Äì20 data, Tesseract was able to compete even with the state of the art object detection model like Faster R-CNN which was trained using a lot more data (with a lot of augmentation as well). 
BUT if your images data have some noises (random dots, dirty mark) with the same color of your font, Tesseract will not be able to predict your images correctly. 
I say you should use Tesseract if you want to build OCR model as fast as possible, or you have a limited amount of training data.

<h2>Tesseract font training</h2>
tools: Anyline training font

put lang = "Font" as second parameter in image_to_string function.

train tesseract for a new font manually with this guide: http://pretius.com/how-to-prepare-training-files-for-tesseract-ocr-and-improve-characters-recognition/

Tesseract (The LSTM model) tutorial
https://www.youtube.com/watch?v=TpD76k2HYms
Training/Fine Tuning Tesseract OCR LSTM for New Fonts

https://github.com/astutejoe/tesseract-tutorial
Training/Fine Tuning Tesseract OCR LSTM for New Fonts

to train tesseract with the new font, then generate .traineddata file with your desired font.
For generating .traineddata, first you will need .tiff file and .box file.
You can create these files using jTessBoxEditor.
<a href="https://sourceforge.net/projects/vietocr/files/jTessBoxEditor/" class="whitebut ">jTessBoxEditor</a>

Tutorial for jBossTextEditor is here.
http://vietocr.sourceforge.net/training.html

While making .tiff file you can set the font in which you have train tesseract.
Either you can jTessBoxEditor for generating .traineddata or serak-tesseract-trainer is also there.
https://code.google.com/archive/p/serak-tesseract-trainer/downloads
serak-tesseract-trainer

I have used both and I would say that for generating tiff and box files jTessBoxEditor is great and for training tesseract use serak.

https://towardsdatascience.com/simple-ocr-with-tesseract-a4341e4564b6
train Tesseract to read unique font












<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... 
more custom settings?
});
</script>
</pre></body></html>