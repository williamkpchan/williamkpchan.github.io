<base target="_blank"><html><head><title>statologyContents 11</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 11"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 11</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Test for Multicollinearity in SPSS</span></h2>
 <b>Multicollinearity</b>  in regression analysis occurs when two or more predictor variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model. 
One way to detect multicollinearity is by using a metric known as the<b> variance inflation factor (VIF)</b>, which measures the correlation and strength of correlation between the predictor variables in a regression model.
This tutorial explains how to use VIF to detect multicollinearity in a regression analysis in SPSS.
<h3>Example: Multicollinearity in SPSS</h3>
Suppose we have the following dataset that shows the exam score of 10 students along with the number of hours they spent studying, the number of prep exams they took, and their current grade in the course:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/vifspss1.png">
We would like to perform a linear regression using <b>score</b> as the response variable and <b>hours</b>, <b>prep_exams</b>, and <b>current_grade</b> as the predictor variables, but we want to make sure that the three predictor variables aren’t highly correlated.
To determine if multicollinearity is a problem, we can produce VIF values for each of the predictor variables.
To do so, click on the <b>Analyze </b>tab, then <b>Regression</b>, then <b>Linear</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/vifspss2.png">
In the new window that pops up, drag <b>score </b>into the box labelled Dependent and drag the three predictor variables into the box labelled Independent(s). Then click <b>Statistics </b>and make sure the box is checked next to <b>Collinearity diagnostics</b>. Then click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/vifspss3.png">
Once you click <b>OK</b>, the following table will be displayed that shows the VIF value for each predictor variable:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/vifspss4.png">
The VIF values for each of the predictor variables are as follows:
hours: <b>1.169</b>
prep_exams: <b>1.403</b>
current_grade: <b>1.522</b>
The value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:
A value of 1 indicates there is no correlation between a given predictor variable and any other predictor variables in the model.
A value between 1 and 5 indicates moderate correlation between a given predictor variable and other predictor variables in the model, but this is often not severe enough to require attention.
A value greater than 5 indicates potentially severe correlation between a given predictor variable and other predictor variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.
We can see that none of the VIF values for the predictor variables in this example are greater than 5, which indicates that multicollinearity will not be a problem in the regression model.
<h2><span class="orange">How to Test for Multicollinearity in Stata</span></h2>
 <b>Multicollinearity</b>  in regression analysis occurs when two or more explanatory variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the regression model. 
For example, suppose you run a multiple linear regression with the following variables:
<b>Response variable: </b>max vertical jump
<b>Explanatory variables: </b>shoe size, height, time spent practicing
In this case, the explanatory variables shoe size and height are likely to be highly correlated since taller people tend to have larger shoe sizes. This means that multicollinearity is likely to be a problem in this regression.
Fortunately, it’s possible to detect multicollinearity using a metric known as the<b> variance inflation factor (VIF)</b>, which measures the correlation and strength of correlation between the explanatory variables in a regression model.
This tutorial explains how to use VIF to detect multicollinearity in a regression analysis in Stata.
<h2>Example: Multicollinearity in Stata</h2>
For this example we will use the Stata built-in dataset called <em>auto</em>. Use the following command to load the dataset:
<b>sysuse auto</b>
We’ll use the <b>regress </b>command to fit a  multiple linear regression model  using price as the response variable and weight, length, and mpg as the explanatory variables:
<b>regress price weight length mpg</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIF1.png">
Next, we’ll use the <b>vif </b>command to test for multicollinearity:
<b>vif</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIF2.png">
This produces a VIF value for each of the explanatory variables in the model. The value for VIF starts at 1 and has no upper limit. A general rule of thumb for interpreting VIFs is as follows:
A value of 1 indicates there is no correlation between a given explanatory variable and any other explanatory variables in the model.
A value between 1 and 5 indicates moderate correlation between a given explanatory variable and other explanatory variables in the model, but this is often not severe enough to require attention.
A value greater than 5 indicates potentially severe correlation between a given explanatory variable and other explanatory variables in the model. In this case, the coefficient estimates and p-values in the regression output are likely unreliable.
We can see that the VIF values for both weight and length are greater than 5, which indicates that multicollinearity is likely a problem in the regression model.
<h3>How to Deal with Multicollinearity</h3>
Often the easiest way to deal with multicollinearity is to simply remove one of the problematic variables since the variable you’re removing is likely redundant anyway and adds little unique or independent information the model.
To determine which variable to remove, we can use the <b>corr </b>command to create a  correlation matrix  to view the correlation coefficients between each of the variables in the model, which can help us identify which variables might be highly correlated with each other and could be causing the problem of multicollinearity:
<b>corr price weight length mpg</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIF3.png">
We can see that length is highly correlated with both weight and mpg, and it has the lowest correlation with the response variable price. Thus, removing length from the model could solve the problem of multicollinearity without reducing the overall quality of the regression model.
To test this, we can perform the regression analysis again using just weight and mpg as explanatory variables:
<b>regress price weight mpg</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIF4.png">
We can see that the adjusted R-squared of this model is <b>0.2735 </b>compared to <b>0.3298 </b>in the previous model. This indicates that the overall usefulness of the model decreased only slightly. Next, we can find the VIF values again using the <b>VIF </b>command:
<b>VIF</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIF5.png">
Both VIF values are below 5, which indicates that multicollinearity is no longer a problem in the model.
<h2><span class="orange">How to Perform Multidimensional Scaling in Python</span></h2>
In statistics, <b>multidimensional scaling</b> is a way to visualize the similarity of observations in a dataset in an abstract cartesian space (usually a 2-D space).
The easiest way to perform multidimensional scaling in Python is by using the <b>MDS()</b> function from the <b>sklearn.manifold</b> sub-module.
The following example shows how to use this function in practice.
<h2>Example: Multidimensional Scaling in Python</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrane
df = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K'],   'points': [4, 4, 6, 7, 8, 14, 16, 19, 25, 25, 28],   'assists': [3, 2, 2, 5, 4, 8, 7, 6, 8, 10, 11],   'blocks': [7, 3, 6, 7, 5, 8, 8, 4, 2, 2, 1],   'rebounds': [4, 5, 5, 6, 5, 8, 10, 4, 3, 2, 2]})
#set player column as index column
df = df.set_index('player')
#view Dataframe
print(df)
        points  assists  blocks  rebounds
player                                   
A            4        3       7         4
B            4        2       3         5
C            6        2       6         5
D            7        5       7         6
E            8        4       5         5
F           14        8       8         8
G           16        7       8        10
H           19        6       4         4
I           25        8       2         3
J           25       10       2         2
K           28       11       1         2
</b>
We can use the following code to perform multidimensional scaling with the <b>MDS()</b> function from the <b>sklearn.manifold</b> module:
<b>from sklearn.manifold import MDS
#perform multi-dimensional scaling
mds = MDS(random_state=0)
scaled_df = mds.fit_transform(df)
#view results of multi-dimensional scaling
print(scaled_df)
[[  7.43654469   8.10247222]
 [  4.13193821  10.27360901]
 [  5.20534681   7.46919526]
 [  6.22323046   4.45148627]
 [  3.74110999   5.25591459]
 [  3.69073384  -2.88017811]
 [  3.89092087  -5.19100988]
 [ -3.68593169  -3.0821144 ]
 [ -9.13631889  -6.81016012]
 [ -8.97898385  -8.50414387]
 [-12.51859044  -9.08507097]]</b>
Each row from the original DataFrame has been reduced to an (x, y) coordinate.
We can use the following code to visualize these coordinates in a 2-D space:
<b>import matplotlib.pyplot as plt
#create scatterplot
plt.scatter(scaled_df[:,0], scaled_df[:,1])
#add axis labels
plt.xlabel('Coordinate 1')
plt.ylabel('Coordinate 2')
#add lables to each point
for i, txt in enumerate(df.index):
    plt.annotate(txt, (scaled_df[:,0][i]+.3, scaled_df[:,1][i]))
#display scatterplot
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/mds2.jpg">
Players from the original DataFrame who have similar values across the original four columns (points, assists, blocks, and rebounds) are located close to each other in the plot.
For example, players <b>F</b> and <b>G</b> are located close to each other. Here are their values from the original DataFrame:
<b>#select rows with index labels 'F' and 'G'
df.loc[['F', 'G']]
        pointsassistsblocksrebounds
player
F14888
G167810
</b>
Their values for points, assists, blocks, and rebounds are all fairly similar, which explains why they’re located so close together in the 2-D plot.
By contrast, consider players <b>B</b> and <b>K</b> who are located far apart in the plot.
If we refer to their values in the original DataFrame, we can see that they’re quite different:
<b>#select rows with index labels 'B' and 'K'
df.loc[['B', 'K']]
        pointsassistsblocksrebounds
player
B4235
K281112</b>
Thus, the 2-D plot is a nice way to visualize how similar each players are across all of the variables in the DataFframe.
Players who have similar stats are grouped close together while players who have very different stats are located far apart from each other in the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Normalize Data in Python 
 How to Remove Outliers in Python 
 How to Test for Normality in Python 
<h2><span class="orange">How to Perform Multidimensional Scaling in R (With Example)</span></h2>
In statistics, <b>multidimensional scaling</b> is a way to visualize the similarity of observations in a dataset in an abstract cartesian space (usually a 2-D space).
The easiest way to perform multidimensional scaling in R is by using the built-in <b>cmdscale()</b> function, which uses the following basic syntax:
<b>cmdscale(d, eig = FALSE, k = 2, …)</b>
where:
<b>d</b>: A distance matrix usually calculated by the <b>dist()</b> function.
<b>eig</b>: Whether or not to return eigenvalues.
<b>k</b>: The number of dimensions to visualize the data in. Default is <b>2</b>.
The following example shows how to use this function in practice.
<h2>Example: Multidimensional Scaling in R</h2>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(points=c(4, 4, 6, 7, 8, 14, 16, 19, 25, 25, 28), assists=c(3, 2, 2, 5, 4, 8, 7, 6, 8, 10, 11), blocks=c(7, 3, 6, 7, 5, 8, 8, 4, 2, 2, 1), rebounds=c(4, 5, 5, 6, 5, 8, 10, 4, 3, 2, 2))
#add row names
row.names(df) &lt;- LETTERS[1:11]
#view data frame
df
  points assists blocks rebounds
A      4       3      7        4
B      4       2      3        5
C      6       2      6        5
D      7       5      7        6
E      8       4      5        5
F     14       8      8        8
G     16       7      8       10
H     19       6      4        4
I     25       8      2        3
J     25      10      2        2
K     28      11      1        2</b>
We can use the following code to perform multidimensional scaling with the <b>cmdscale()</b> function and visualize the results in a 2-D space:
<b>#calculate distance matrix
d &lt;- dist(df)
#perform multidimensional scaling
fit &lt;- cmdscale(d, eig=TRUE, k=2)
#extract (x, y) coordinates of multidimensional scaleing
x &lt;- fit$points[,1]
y &lt;- fit$points[,2]
#create scatter plot
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2",
     main="Multidimensional Scaling Results", type="n")
#add row names of data frame as labels
text(x, y, labels=row.names(df))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/mds1.jpg">
Players from the original data frame who have similar values across the original four columns (points, assists, blocks, and rebounds) are located close to each other in the plot.
For example, players <b>A</b> and <b>C</b> are located close to each other. Here are their values from the original data frame:
<b>#view data frame values for players A and C
df[rownames(df) %in% c('A', 'C'), ]
  points assists blocks rebounds
A      4       3      7        4
C      6       2      6        5</b>
Their values for points, assists, blocks, and rebounds are all quite similar, which explains why they’re located so close together in the 2-D plot.
By contrast, consider players <b>B</b> and <b>K</b> who are located far apart in the plot.
If we refer to their values in the original data fame, we can see that they’re quite different:
<b>#view data frame values for players B and K
df[rownames(df) %in% c('B', 'K'), ]
  points assists blocks rebounds
B      4       2      3        5
K     28      11      1        2</b>
Thus, the 2-D plot is a nice way to visualize how similar each players are across all of the variables in the data frame.
Players who have similar stats are grouped close together while players who have very different stats are located far apart from each other in the plot.
Note that you can also extract the exact (x, y) coordinates for each player in the plot by typing <b>fit</b>, which is the name of the variable that we stored the results of <b>cmdscale()</b> function in:
<b>#view (x, y) coordinates of points in the plot
fit
         [,1]       [,2]
A -10.6617577 -1.2511291
B -10.3858237 -3.3450473
C  -9.0330408 -1.1968116
D  -7.4905743  1.0578445
E  -6.4021114 -1.0743669
F  -0.4618426  4.7392534
G   0.8850934  6.1460850
H   4.7352436 -0.6004609
I  11.3793381 -1.3563398
J  12.0844168 -1.5494108
K  15.3510585 -1.5696166
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Normalize Data in R 
 How to Center Data in R 
 How to Remove Outliers in R 
<h2><span class="orange">What is a Multimodal Distribution?</span></h2>
A <b>multimodal distribution</b> is a probability distribution with two or more modes.
If you create a histogram to visualize a multimodal distribution, you’ll notice that it has more than one peak:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/mulitmodal1.png">
If a distribution has exactly two peaks then it’s considered a  bimodal distribution , which is a specific type of multimodal distribution.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/multimodal2.png">
This is in contrast to a unimodal distribution, which only has one peak:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/multimodal3.png">
Although unimodal distributions like  the normal distribution  are used most often to explain topics in statistics, multimodal distributions actually appear fairly often in practice so it’s useful to know how to recognize and analyze them.
<h3>Examples of Multimodal Distributions</h3>
Here are a few examples of multimodal distributions.
<b>Example 1: Distribution of Exam Scores</b>
Suppose a professor gives an exam to his class. Some of the students studied, while others did not. When the professor creates a histogram of the exam scores, it follows a multimodal distribution with one peak around low scores for students who didn’t study and another peak around high scores for students who did study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal5.png">
<b>Example 2: Height of Different Plant Species</b>
Suppose a scientist goes around a field and measures the height of different plants. Without realizing it, she measures the height of three different species – one that is quite tall, another that is of medium height, and another that is quite short.
When she creates a histogram to visualize the distribution of heights, she finds that it is multimodal – each peak represents the most common height of the three different species.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/multimodal4.png">
<b>Example 3: Distribution of Customers</b>
A restaurant owner tracks how many customers visit each hour. When he goes to create a histogram to visualize the distribution of customers, he finds that the distribution is multimodal – there is a peak during lunch hours and another peak during dinner hours.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal3.png">
<h3>What Causes Multimodal Distributions?</h3>
There are typically one of two underlying causes of multimodal distributions:
<b>1. Multiple groups are lumped together.</b>
Multimodal distributions can occur when you collect data for multiple groups without realizing it.
For example, if a scientist unknowingly measures the height of three different plant species located in the same field, the distribution of all the plants will appear multimodal when placed on the same histogram.
<b>2. There exists an underlying phenomena.</b>
Multimodal distributions can also occur because of some underlying phenomena.
For example, the number of customers who visit a restaurant each hour follows a multimodal distribution since people tend to eat out during two distinct times: lunch and dinner. This underlying human behavior causes the multimodal distribution.
<h3>How to Analyze Multimodal Distributions</h3>
We often describe distributions using  the mean or median  since this gives us an idea of where the “center” of the distribution is located.
Unfortunately, the mean and median aren’t useful to know for a bimodal distribution. For example, the mean exam score for students in the example above is 81:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal6.png">
However, very few students actually scored close to 81. In this case, the mean is misleading. Most students actually scored around 74 or around 88.
A better way to analyze and interpret bimodal distributions is to simply break the data into two separate groups, then analyze the location of the center and the spread for each group individually.
For example, we may break up the exam scores into “low scores” and “high scores” and then find the mean and standard deviation for each group.
When calculating summary statistics for a given distribution like the mean, median, or standard deviation, be sure to visualize the distribution to determine if it is unimodal or multimodal.
If a distribution is multimodal, it can be misleading to describe it using a single mean, median, or standard deviation.
<h2><span class="orange">Multinomial Coefficient Calculator</span></h2>
A <b> multinomial coefficient </b> describes the number of possible partitions of <i>n</i> objects into <i>k</i> groups of size n<sub>1</sub>, n<sub>2</sub>, …, n<sub>k</sub>.
The formula to calculate a multinomial coefficient is:
Multinomial Coefficient = n! / (n<sub>1</sub>! * n<sub>2</sub>! * … * n<sub>k</sub>!)
To calculate a multinomial coefficient, simply fill in the values below and then click the “Calculate” button.
<label><b>n</b> (total objects)</label>
<input type="number" id="n" value="10">
<label><b>k</b> (group sizes)</label>
<input id="k" value="3, 5, 2">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Multinomial coefficient: 2,520
<script>
function calc() {
//get input values
var n = +document.getElementById('n').value;
var k = document.getElementById('k').value.match(/\d+/g).map(Number);
//calculate multinomial coefficient
for (var i = 0; i < k.length; i+= 1) 
   {k[i] = math.factorial(k[i])};
var ksub = k.reduce( (a, b) => a * b );
var p = math.factorial(n)/ksub;
//output probabilities
document.getElementById('p').innerHTML = p.toLocaleString();
}
</script>
<h2><span class="orange">Multinomial Coefficient: Definition & Examples</span></h2>
A <b>multinomial coefficient</b> describes the number of possible partitions of <em>n</em> objects into <em>k</em> groups of size <em>n<sub>1</sub></em>, <em>n<sub>2</sub></em>, …, <em>n<sub>k</sub></em>.
The formula to calculate a multinomial coefficient is:
Multinomial Coefficient = n! / (n<sub>1</sub>! * n<sub>2</sub>! * … * n<sub>k</sub>!)
The following examples illustrate how to calculate the multinomial coefficient in practice.
<h3>Example 1: Letters in a Word</h3>
How many unique partitions of the word ARKANSAS are there?
<b>Solution: </b>We can simply plug in the following values into the formula for the multinomial coefficient:
<b>n</b> (total letters): 8
<b>n<sub>1</sub></b> (letter “A”): 3
<b>n<sub>2</sub></b> (letter “R”): 1
<b>n<sub>3</sub></b> (letter “K”): 1
<b>n<sub>4</sub></b> (letter “N”): 1
<b>n<sub>5</sub></b> (letter “S”): 2
Multinomial Coefficient = 8! / (3! * 1! * 1! * 1! * 2!) = <b>3,360</b>
There are <b>3,360 </b>unique partitions of the word ARKANSAS.
<h3>Example 2: Students by Grade</h3>
A group of six students consists of 3 seniors, 2 juniors, and 1 sophomore. How many unique partitions of this group of students are there by grade?
<b>Solution: </b>We can simply plug in the following values into the formula for the multinomial coefficient:
<b>n</b> (total students): 6
<b>n<sub>1</sub></b> (total seniors): 3
<b>n<sub>2</sub></b> (total juniors): 2
<b>n<sub>3</sub></b> (total sophomores): 1
Multinomial Coefficient = 6! / (3! * 2! * 1!) = <b>60</b>
There are <b>60 </b>unique partitions of these students by grade.
<h3>Example 3: Political Party Preference</h3>
Out of a group of ten residents in a certain county, 3 are Republicans, 5 are Democrats, and 2 are Independents. How many unique partitions of this group of residents are there by political party?
<b>Solution: </b>We can simply plug in the following values into the formula for the multinomial coefficient:
<b>n</b> (total residents): 10
<b>n<sub>1</sub></b> (total Republicans): 3
<b>n<sub>2</sub></b> (total Democrats): 5
<b>n<sub>3</sub></b> (total Independents): 2
Multinomial Coefficient = 10! / (3! * 5! * 2!) = <b>2,520</b>
There are <b>2,520 </b>unique partitions of these residents by political party.
<h2><span class="orange">Multinomial Distribution Calculator</span></h2>
</style>
The <b> multinomial distribution </b> describes the probability of obtaining a specific number of counts for <i>k</i> different outcomes, when each outcome has a fixed probability of occurring.
To calculate this probability, simply fill in the values below for up to 10 outcomes, then click the “Calculate” button:
<i><b>Note:</b> The Probability column must add up to 1.</i>
<table><tbody>
<tr>
<th><b>Outcome</b></th>
<th><b><span>Probability</b></th>
<th><b><span>Frequency</b></th>
</tr>
<tr>
<td>Outcome 1</td>
<td><input type="text" id="p1" value="0.3"></td>
<td><input type="text" id="f1" value="2"></td>
</tr>
<tr>
<td>Outcome 2</td>
<td><input type="text" id="p2" value="0.5"></td>
<td><input type="text" id="f2" value="4"></td>
</tr>
<tr>
<td>Outcome 3</td>
<td><input type="text" id="p3" value="0.2"></td>
<td><input type="text" id="f3" value="1"></td>
</tr>
<tr>
<td>Outcome 4</td>
<td><input type="text" id="p4" value=""></td>
<td><input type="text" id="f4" value=""></td>
</tr>
<tr>
<td>Outcome 5</td>
<td><input type="text" id="p5" value=""></td>
<td><input type="text" id="f5" value=""></td>
</tr>
<tr>
<td>Outcome 6</td>
<td><input type="text" id="p6"></td>
<td><input type="text" id="f6"></td>
</tr>
<tr>
<td>Outcome 7</td>
<td><input type="text" id="p7"></td>
<td><input type="text" id="f7"></td>
</tr>
<tr>
<td>Outcome 8</td>
<td><input type="text" id="p8"></td>
<td><input type="text" id="f8"></td>
</tr>
<tr>
<td>Outcome 9</td>
<td><input type="text" id="p9"></td>
<td><input type="text" id="f9"></td>
</tr>
<tr>
<td>Outcome 10</td>
<td><input type="text" id="p10"></td>
<td><input type="text" id="f10"></td>
</tr>
</tbody></table>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Multinomial Probability: <b>0.118125</b>
<b>Probabilities must add up to 1. They currently add up to 0.359</b>
<script>
//show answer to start
var answer_display = document.getElementById("answer");
//hide error message to start
var error_msg_display = document.getElementById("error_msg");
error_msg_display.style.display = "none";
//define factorial function
function factorial(num)
{
    var rval=1;
    for (var i = 2; i <= num; i++)
        rval = rval * i;
    return rval;
}
function calc() {
//get input data
var p1 = document.getElementById('p1').value;
var p2 = document.getElementById('p2').value;
var p3 = document.getElementById('p3').value;
var p4 = document.getElementById('p4').value;
var p5 = document.getElementById('p5').value;
var p6 = document.getElementById('p6').value;
var p7 = document.getElementById('p7').value;
var p8 = document.getElementById('p8').value;
var p9 = document.getElementById('p9').value;
var p10 = document.getElementById('p10').value;
var f1 = document.getElementById('f1').value;
var f2 = document.getElementById('f2').value;
var f3 = document.getElementById('f3').value;
var f4 = document.getElementById('f4').value;
var f5 = document.getElementById('f5').value;
var f6 = document.getElementById('f6').value;
var f7 = document.getElementById('f7').value;
var f8 = document.getElementById('f8').value;
var f9 = document.getElementById('f9').value;
var f10 = document.getElementById('f10').value;
var p_group = [p1, p2, p3, p4, p5, p6, p7, p8, p9, p10];
var f_group = [f1, f2, f3, f4, f5, f6, f7, f8, f9, f10];
var p_sum = parseFloat(math.sum(p_group)).toFixed(5);
var n = math.sum(f_group);
//do calculations
if (p_sum == 1) {
  answer_display.style.display = "block";
  error_msg_display.style.display = "none";
  var nPORTION = factorial(n);
  
  var xFACT = [];
  for (var i=0; i<f_group.length; i++) {
    xFACT[i] = factorial(f_group[i]);
  }
  var xPORTION = xFACT.reduce((product, n) => product*n, 1);
  var pxFACT = [];
  for (var j=0; j<f_group.length; j++) {
    pxFACT[j] = Math.pow(p_group[j], f_group[j]);
  }
  var pxPORTION = pxFACT.reduce((product, n) => product*n, 1);
  
  var p = nPORTION*pxPORTION/xPORTION;
  document.getElementById('p').innerHTML = p.toFixed(6);
}
else {
  answer_display.style.display = "none";
  error_msg_display.style.display = "block";
  document.getElementById('p_sum').innerHTML = p_sum;
}
  
} //end massive calc function
</script>
<h2><span class="orange">How to Use the Multinomial Distribution in Excel</span></h2>
The  multinomial distribution  describes the probability of obtaining a specific number of counts for <em>k</em> different outcomes, when each outcome has a fixed probability of occurring.
If a  random variable  <em>X</em> follows a multinomial distribution, then the probability that outcome 1 occurs exactly x<sub>1</sub> times, outcome 2 occurs exactly x<sub>2</sub> times, etc. can be found by the following formula:
<b>Probability = n! * (p<sub>1</sub><sup>x<sub>1</sub></sup> * p<sub>2</sub><sup>x<sub>2</sub></sup> * … * p<sub>k</sub><sup>x<sub>k</sub></sup>) /  (x<sub>1</sub>! * x<sub>2</sub>! … * x<sub>k</sub>!)</b>
where:
<b>n: </b>total number of events
<b>x<sub>1</sub>: </b>number of times outcome 1 occurs
<b>p<sub>1</sub>: </b>probability that outcome 1 occurs in a given trial
The following examples show how to calculate multinomial probabilities in Excel.
<h3>Example 1</h3>
In a three-way election for mayor, candidate A receives 10% of the votes, candidate B receives 40% of the votes, and candidate C receives 50% of the votes.
If we select a random sample of 10 voters, what is the probability that 2 voted for candidate A, 4 voted for candidate B, and 4 voted for candidate C?
The following screenshot shows how to calculate this probability in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/multinom1.png">
The probability that exactly 2 people voted for A, 4 voted for B, and 4 voted for C is <b>0.0504</b>.
<h3>Example 2</h3>
Suppose an urn contains 6 yellow marbles, 2 red marbles, and 2 pink marbles.
If we randomly select 4 balls from the urn, with replacement, what is the probability that all 4 balls are yellow?
The following screenshot shows how to calculate this probability in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/multinom2.png">
The probability that all 4 balls are yellow is <b>0.1296</b>.
<h3>Example 3</h3>
Suppose two students play chess against each other. The probability that student A wins a given game is 0.5, the probability that student B wins a given game is 0.3, and the probability that they tie in a given game is 0.2.
If they play 10 games, what is the probability that player A wins 4 times, player B wins 5 times, and they tie 1 time?
The following screenshot shows how to calculate this probability in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/multinom3.png">
The probability that player A wins 4 times, player B wins 5 times, and they tie 1 time is about <b>0.038</b>.
<h2><span class="orange">How to Use the Multinomial Distribution in Python</span></h2>
The  multinomial distribution  describes the probability of obtaining a specific number of counts for <em>k</em> different outcomes, when each outcome has a fixed probability of occurring.
If a  random variable  <em>X</em> follows a multinomial distribution, then the probability that outcome 1 occurs exactly x<sub>1</sub> times, outcome 2 occurs exactly x<sub>2</sub> times, etc. can be found by the following formula:
<b>Probability = n! * (p<sub>1</sub><sup>x<sub>1</sub></sup> * p<sub>2</sub><sup>x<sub>2</sub></sup> * … * p<sub>k</sub><sup>x<sub>k</sub></sup>) /  (x<sub>1</sub>! * x<sub>2</sub>! … * x<sub>k</sub>!)</b>
where:
<b>n: </b>total number of events
<b>x<sub>1</sub>: </b>number of times outcome 1 occurs
<b>p<sub>1</sub>: </b>probability that outcome 1 occurs in a given trial
The following examples show how to use the  scipy.stats.multinomial()  function in Python to answer different probability questions regarding the multinomial distribution.
<h3>Example 1</h3>
In a three-way election for mayor, candidate A receives 10% of the votes, candidate B receives 40% of the votes, and candidate C receives 50% of the votes.
If we select a random sample of 10 voters, what is the probability that 2 voted for candidate A, 4 voted for candidate B, and 4 voted for candidate C?
We can use the following code in Python to answer this question:
<b>from scipy.stats import multinomial
#calculate multinomial probability
multinomial.pmf(x=[2, 4, 4], n=10, p=[.1, .4, .5])
0.05040000000000001</b>
The probability that exactly 2 people voted for A, 4 voted for B, and 4 voted for C is <b>0.0504</b>.
<h3>Example 2</h3>
Suppose an urn contains 6 yellow marbles, 2 red marbles, and 2 pink marbles.
If we randomly select 4 balls from the urn, with replacement, what is the probability that all 4 balls are yellow?
We can use the following code in Python to answer this question:
<b>from scipy.stats import multinomial
#calculate multinomial probability
multinomial.pmf(x=[4, 0, 0], n=4, p=[.6, .2, .2])
0.1295999999999999</b>
The probability that all 4 balls are yellow is about <b>0.1296</b>.
<h3>Example 3</h3>
Suppose two students play chess against each other. The probability that student A wins a given game is 0.5, the probability that student B wins a given game is 0.3, and the probability that they tie in a given game is 0.2.
If they play 10 games, what is the probability that player A wins 4 times, player B wins 5 times, and they tie 1 time?
We can use the following code in Python to answer this question:
<b>from scipy.stats import multinomial
#calculate multinomial probability
multinomial.pmf(x=[4, 5, 1], n=10, p=[.5, .3, .2])
0.03827249999999997
</b>
The probability that player A wins 4 times, player B wins 5 times, and they tie 1 time is about <b>0.038</b>.
<h2><span class="orange">How to Use the Multinomial Distribution in R</span></h2>
The  multinomial distribution  describes the probability of obtaining a specific number of counts for <em>k</em> different outcomes, when each outcome has a fixed probability of occurring.
If a  random variable  <em>X</em> follows a multinomial distribution, then the probability that outcome 1 occurs exactly x<sub>1</sub> times, outcome 2 occurs exactly x<sub>2</sub> times, etc. can be found by the following formula:
<b>Probability = n! * (p<sub>1</sub><sup>x<sub>1</sub></sup> * p<sub>2</sub><sup>x<sub>2</sub></sup> * … * p<sub>k</sub><sup>x<sub>k</sub></sup>) /  (x<sub>1</sub>! * x<sub>2</sub>! … * x<sub>k</sub>!)</b>
where:
<b>n: </b>total number of events
<b>x<sub>1</sub>: </b>number of times outcome 1 occurs
<b>p<sub>1</sub>: </b>probability that outcome 1 occurs in a given trial
To calculate a multinomial probability in R we can use the <b>dmultinom()</b> function, which uses the following syntax:
<b>dmultinom(x=c(1, 6, 8), prob=c(.4, .5, .1))</b>
where:
<b>x</b>: A vector that represents the frequency of each outcome
<b>prob</b>: A vector that represents the probability of each outcome (the sum must be 1)
The following examples show how to use this function in practice.
<h3>Example 1</h3>
In a three-way election for mayor, candidate A receives 10% of the votes, candidate B receives 40% of the votes, and candidate C receives 50% of the votes.
If we select a random sample of 10 voters, what is the probability that 2 voted for candidate A, 4 voted for candidate B, and 4 voted for candidate C?
We can use the following code in R to answer this question:
<b>#calculate multinomial probability
dmultinom(x=c(2, 4, 4), prob=c(.1, .4, .5))
[1] 0.0504
</b>
The probability that exactly 2 people voted for A, 4 voted for B, and 4 voted for C is <b>0.0504</b>.
<h3>Example 2</h3>
Suppose an urn contains 6 yellow marbles, 2 red marbles, and 2 pink marbles.
If we randomly select 4 balls from the urn, with replacement, what is the probability that all 4 balls are yellow?
We can use the following code in R to answer this question:
<b>#calculate multinomial probability
dmultinom(x=c(4, 0, 0), prob=c(.6, .2, .2))
[1] 0.1296
</b>
The probability that all 4 balls are yellow is <b>0.1296</b>.
<h3>Example 3</h3>
Suppose two students play chess against each other. The probability that student A wins a given game is 0.5, the probability that student B wins a given game is 0.3, and the probability that they tie in a given game is 0.2.
If they play 10 games, what is the probability that player A wins 4 times, player B wins 5 times, and they tie 1 time?
We can use the following code in R to answer this question:
<b>#calculate multinomial probability
dmultinom(x=c(4, 5, 1), prob=c(.5, .3, .2))
[1] 0.0382725
</b>
The probability that player A wins 4 times, player B wins 5 times, and they tie 1 time is about <b>0.038</b>.
<h2><span class="orange">An Introduction to the Multinomial Distribution</span></h2>
The <b>multinomial distribution</b> describes the probability of obtaining a specific number of counts for <em>k</em> different outcomes, when each outcome has a fixed probability of occurring.
If a  random variable  <em>X</em> follows a multinomial distribution, then the probability that outcome 1 occurs exactly x<sub>1</sub> times, outcome 2 occurs exactly x<sub>2</sub> times, outcome 3 occurs exactly x<sub>3</sub> times etc. can be found by the following formula:
<b>Probability = n! * (p<sub>1</sub><sup>x<sub>1</sub></sup> * p<sub>2</sub><sup>x<sub>2</sub></sup> * … * p<sub>k</sub><sup>x<sub>k</sub></sup>) /  (x<sub>1</sub>! * x<sub>2</sub>! … * x<sub>k</sub>!)</b>
where:
<b>n: </b>total number of events
<b>x<sub>1</sub>: </b>number of times outcome 1 occurs
<b>p<sub>1</sub>: </b>probability that outcome 1 occurs in a given trial
For example, suppose there are 5 red marbles, 3 green marbles, and 2 blue marbles in an urn. If we randomly select 5 marbles from the urn, with replacement, what is the probability of obtaining exactly 2 red marbles, 2 green marbles, and 1 blue marble?
To answer this, we can use the multinomial distribution with the following parameters:
<b>n</b>: 5
<b>x<sub>1 </sub></b>(# red marbles) = 2, <b>x<sub>2 </sub></b>(# green marbles) = 2, <b>x<sub>3 </sub></b>(# blue marbles) = 1
<b>p<sub>1 </sub></b>(prob. red) = 0.5, <b>p<sub>2 </sub></b>(prob. green) = 0.3, <b>p<sub>3 </sub></b>(prob. blue) = 0.2
Plugging these numbers in the formula, we find the probability to be:
<b>Probability </b>= 5! * (.5<sup>2</sup> * .3<sup>2</sup> * .2<sup>1</sup>) /  (2! * 2! * 1!) = <b>0.135</b>.
<h2>Multinomial Distribution Practice Problems</h2>
Use the following practice problems to test your knowledge of the multinomial distribution.
<em><b>Note: </b>We will use the  Multinomial Distribution Calculator  to calculate the answers to these questions.</em>
<h3>Problem 1</h3>
<b>Question: </b>In a three-way election for mayor, candidate A receives 10% of the votes, candidate B receives 40% of the votes, and candidate C receives 50% of the votes. If we select a random sample of 10 voters, what is the probability that 2 voted for candidate A, 4 voted for candidate B, and 4 voted for candidate C?
<b>Answer:</b> Using the Multinomial Distribution Calculator with the following inputs, we find that the probability is <b>0.0504:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/multinomialCalc1.png">
<h3>Problem 2</h3>
<b>Question: </b>Suppose an urn contains 6 yellow marbles, 2 red marbles, and 2 pink marbles. If we randomly select 4 balls from the urn, with replacement, what is the probability that all 4 balls are yellow?
<b>Answer:</b> Using the Multinomial Distribution Calculator with the following inputs, we find that the probability is <b>0.1296:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/multinomialCalc2.png">
<h3>Problem 3</h3>
<b>Question: </b>Suppose two students play chess against each other. The probability that student A wins a given game is 0.5, the probability that student B wins a given game is 0.3, and the probability that they tie in a given game is 0.2. If they play 10 games, what is the probability that player A wins 4 times, player B wins 5 times, and they tie 1 time?
<b>Answer:</b> Using the Multinomial Distribution Calculator with the following inputs, we find that the probability is <b>0.038272:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/multinomialCalc3.png">
<h3>Additional Resources</h3>
The following tutorials provide an introduction to other common distributions in statistics:
 An Introduction to the Normal Distribution 
 An Introduction to the Binomial Distribution 
 An Introduction to the Poisson Distribution 
 An Introduction to the Geometric Distribution 
<h2><span class="orange">What is a Multinomial Test? (Definition & Example)</span></h2>
A <b>multinomial test</b> is used to determine if a categorical variable follows a hypothesized distribution.
This test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>:</b> A categorical variable follows a hypothesized distribution.
<b>H<sub>A</sub>:</b> A categorical variable <em>does not</em> follow the hypothesized distribution.
If the  p-value  of the test is less than some significance level (e.g. α = .05) then we can reject the null hypothesis and conclude that the variable does not follow the hypothesized distribution.
This test is used when some variable can take on <em>k</em> different outcomes. A classic example of a multinomial test is when we’d like to determine if some dice is fair. When we roll a dice, the probability that it lands on each number (1 through 6) is 1/6.
To test if a dice is fair, we could roll it a certain number of times and see if the number of times it lands on various numbers is significantly different from what we would expect.
The following examples show how to perform a multinomial test using the statistical programming language R.
<h3>Example 1: Fair Dice</h3>
Suppose we would like to determine if a dice is fair. To test this, we roll it 30 times and record the frequency of each outcome. The following table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multinomialTest1.png">
The following code in R can be used to perform a multinomial test:
<b>library(EMT)
#specify probability of each outcome
prob &lt;- c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6)
#specify frequency of each outcome from experiment
actual &lt;- c(4, 5, 2, 9, 5, 5)
#perform multinomial test
multinomial.test(actual, prob)  
 Exact Multinomial Test, distance measure: p
    Events    pObs    p.value
    324632       0     0.4306</b>
The p-value of the test is <b>0.4306</b>. Since this p-value is not less than .05, we will fail reject the null hypothesis. Thus, we don’t have sufficient evidence to say the dice is unfair.
<h3>Example 2: Sales of Products</h3>
Suppose a shop owner hypothesizes that an equal number of customers will buy each of four different products. To test this, he records the number of customers that buy each product during a given week. The following table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multinomialTest2.png">
The following code in R can be used to perform a multinomial test on this dataset:
<b>library(EMT)
#specify probability of each outcome
prob &lt;- c(1/4, 1/4, 1/4, 1/4)
#specify frequency of each outcome from experiment
actual &lt;- c(40, 20, 30, 50)
#perform multinomial test
multinomial.test(actual, prob)  
 Exact Multinomial Test, distance measure: p
    Events    pObs    p.value
    477191       0     0.00226</b>
The p-value of the test is <b>0.00226</b>. Since this p-value is less than .05, we will reject the null hypothesis. Thus, we have sufficient evidence to say that the sales are not equal for each product.
<h3>Example 3: Marbles in a Bag</h3>
Tom claims that the probability of choosing a red, green, or purple marbles from a bag is 0.2, 0.5, and 0.3, respectively. To test this, his friend Mike reaches in the bag and pulls out a marble (with replacement) 100 different times. The following table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/multinomialTest3.png">
The following code in R can be used to perform a multinomial test on this dataset:
<b>library(EMT)
#specify probability of each outcome
prob &lt;- c(.2, .5, .3)
#specify frequency of each outcome from experiment
actual &lt;- c(40, 20, 30, 50)
#perform multinomial test
multinomial.test(actual, prob)  
 Exact Multinomial Test, distance measure: p
    Events    pObs    p.value
      5151  0.0037     0.3999</b>
The p-value of the test is <b>0.3999</b>. Since this p-value is not less than .05, we will fail to reject the null hypothesis. Thus, we have don’t have sufficient evidence to say that the distribution of marbles in the bag is different from the one specified by Tom.
<h2><span class="orange">How to Plot Multiple Boxplots in One Chart in R</span></h2>
A  boxplot  (sometimes called a box-and-whisker plot) is a plot that shows the five-number summary of a dataset.
The five-number summary includes:
The minimum value
The first quartile
The median value
The third quartile
The maximum value
This tutorial explains how to plot multiple boxplots in one plot in R, using base R and ggplot2.
<h3>Boxplots in Base R</h3>
To illustrate how to create boxplots in base R, we’ll work with the built-in <b>airquality</b> dataset in R:
<b>#view first 6 rows of "airquality" dataset 
head(airquality)
# Ozone Solar.R Wind Temp Month Day
#1 41   190     7.4   67   5     1
#2 36   118     8.0   72   5     2
#3 12   149     12.6  74   5     3
#4 18   313     11.5  62   5     4
#5 NA   NA      14.3  56   5     5
#6 28   NA      14.9  66   5     6
</b>
To create a single boxplot for the variable “Ozone”, we can use the following syntax:
<b>#create boxplot for the variable "Ozone" 
boxplot(airquality$Ozone)
</b>
This generates the following boxplot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/boxplot1.jpg" alt="">
Suppose we instead want to generate one boxplot for each month in the dataset. The following syntax shows how to do so:
<b>#create boxplot that displays temperature distribution for each month in the dataset
boxplot(Temp~Month,
data=airquality,
main="Temperature Distribution by Month",
xlab="Month",
ylab="Degrees (F)",
col="steelblue",
border="black"
)
</b>
This generates the following chart that displays one boxplot for each month:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/boxplot2.jpg" alt="">
<h3>Boxplots in ggplot2</h3>
Another way to create boxplots in R is by using the package <em>ggplot2</em>. We’ll use the built-in dataset <b>airquality</b> again for the following examples.
To create a single boxplot for the variable “Ozone” in the airquality dataset, we can use the following syntax:
<b>#create boxplot for the variable "Ozone" 
library(ggplot2)
ggplot(data = airquality, aes(y=Ozone)) + geom_boxplot()
</b>
This generates the following boxplot:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/boxplot3.jpg">
If instead we want to generate one boxplot for each month in the dataset, we can use thee following syntax to do so:
<b>#create boxplot that displays temperature distribution for each month in the dataset
library(ggplot2)
ggplot(data = airquality, aes(x=as.character(Month), y=Temp)) +
    geom_boxplot(fill="steelblue") +
    labs(title="Temperature Distribution by Month", x="Month", y="Degrees (F)")
</b>
This generates the following chart that displays one boxplot for each month:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/boxplot4.jpg">
<h2><span class="orange">How to Plot Multiple Histograms in R (With Examples)</span></h2>
You can use the following syntax to plot multiple histograms on the same chart in <b>base R</b>:
<b>hist(data1, col='red')
hist(data2, col='blue', add=TRUE)
</b>
And you can use the following syntax to plot multiple histograms in <b>ggplot2</b>:
<b>ggplot(df, aes(x = x_var, fill = grouping_var)) +
  geom_histogram(position = 'identity', alpha = 0.4)</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Plot Multiple Histograms in Base R</h3>
The following code shows how to plot multiple histograms in one plot in base R:
<b>#make this example reproducible
set.seed(1)
#define data
x1 = rnorm(1000, mean=0.8, sd=0.2)
x2 = rnorm(1000, mean=0.4, sd=0.1)
#plot two histograms in same graph
hist(x1, col='red', xlim=c(0, 1.5), main='Multiple Histograms', xlab='x')
hist(x2, col='green', add=TRUE)
#add legend
legend('topright', c('x1 variable', 'x2 variable'), fill=c('red', 'green'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/multHist1.png">
<h3>Method 2: Plot Multiple Histograms in ggplot2</h3>
The following code shows how to plot multiple histograms in one plot in R using  ggplot2 :
<b>library(ggplot2)
#make this example reproducible
set.seed(1)
#create data frame
df &lt;- data.frame(var = c(rep('x1', 1000), rep('x2', 1000) ),   value = c(rnorm(1000, mean=0.8, sd=0.1),             rnorm(1000, mean=0.4, sd=0.1)))
#view first six rows of data frame
head(df)
  var     value
1  x1 0.7373546
2  x1 0.8183643
3  x1 0.7164371
4  x1 0.9595281
5  x1 0.8329508
6  x1 0.7179532
#plot multiple histograms
ggplot(df, aes(x=value, fill=var)) +
  geom_histogram( color='#e9ecef', alpha=0.6, position='identity')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/multHist2.png">
You can quickly change the colors of the histograms by using the <b>scale_fill_manual()</b> function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/multHist3.png">
<h2><span class="orange">How to Use Multiple IF Statements in Google Sheets</span></h2>
You can use the following basic syntax to write multiple IF statements in one cell in Google Sheets:
<b>=IF(A2&lt;10, "Bad", IF(A2&lt;20, "Okay", IF(A2&lt;30, "Good", "Great")))
</b>
Here’s what this syntax does:
If the value in cell A2 is less than 10, return the value “Bad”
Otherwise, if the value in cell A2 is less than 20, return the value “Okay”
Otherwise, if the value in cell A2 is less than 30, return the value “Good”
Otherwise, return the value “Great”
The following examples show how to use this syntax in practice.
<h3>Example 1: Use Multiple IF Statements in Google Sheets</h3>
Suppose we have the following column in Google Sheets that shows the points scored by various basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/ifs1.png">
We can use the following syntax to write multiple IF statements to classify the players as “Bad”, “Okay”, “Good”, or “Great”:
<b>=IF(A2&lt;10, "Bad", IF(A2&lt;20, "Okay", IF(A2&lt;30, "Good", "Great")))</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/ifs2.png">
Each player receives a classification based on their number of points scored.
<h3>Example 2: Use IFS Function in Google Sheets</h3>
An easier way to write multiple IF statements is to simply use the <b>IFS</b> function.
We can use the following syntax to write an <b>IFS</b> statement to classify the players as “Bad”, “Okay”, “Good”, or “Great”:
<b>=IFS(A2&lt;10, "Bad", A2&lt;20, "Okay", A2&lt;30, "Good", A2>=30, "Great")
</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/ifs3.png">
This produces the same results as the previous example.
Notice that this syntax is much easier to write because we don’t have to write several nested IF statements.
<h2><span class="orange">The Five Assumptions of Multiple Linear Regression</span></h2>
 Multiple linear regression  is a statistical method we can use to understand the relationship between multiple predictor variables and a  response variable .
However, before we perform multiple linear regression, we must first make sure that five assumptions are met:
<b>1. Linear relationship:</b> There exists a linear relationship between each predictor variable and the response variable.
<b>2. No Multicollinearity:</b> None of the predictor variables are highly correlated with each other.
<b>3. Independence: </b>The observations are independent.
<b>4. Homoscedasticity: </b>The residuals have constant variance at every point in the linear model.
<b>5. Multivariate Normality: </b>The residuals of the model are normally distributed.
If one or more of these assumptions are violated, then the results of the multiple linear regression may be unreliable.
In this article, we provide an explanation for each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2>Assumption 1: Linear Relationship</h2>
Multiple linear regression assumes that there is a linear relationship between each predictor variable and the response variable.
<h3>How to Determine if this Assumption is Met</h3>
The easiest way to determine if this assumption is met is to create a scatter plot of each predictor variable and the response variable.
This allows you to visually see if there is a linear relationship between the two variables.
If the points in the scatter plot roughly fall along a straight diagonal line, then there likely exists a linear relationship between the variables.
For example, the points in the plot below look like they fall on roughly a straight line, which indicates that there is a linear relationship between this particular predictor variable (x) and the response variable (y):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/assumptionsLinReg1.jpg"451">
<h3>What to Do if this Assumption is Violated</h3>
If there is not a linear relationship between one or more of the predictor variables and the response variable, then we have a couple options:
<b>1.</b> Apply a nonlinear transformation to the predictor variable such as taking the log or the square root. This can often transform the relationship to be more linear.
<b>2. </b>Add another predictor variable to the model. For example, if the plot of x vs. y has a parabolic shape then it might make sense to add X<sup>2</sup> as an additional predictor variable in the model.
<b>3.</b> Drop the predictor variable from the model. In the most extreme case, if there exists no linear relationship between a certain predictor variable and the response variable then the predictor variable may not be useful to include in the model.
<h2>Assumption 2: No Multicollinearity</h2>
Multiple linear regression assumes that none of the predictor variables are highly correlated with each other.
When one or more predictor variables are highly correlated, the regression model suffers from  multicollinearity , which causes the coefficient estimates in the model to become unreliable.
<h3>How to Determine if this Assumption is Met</h3>
The easiest way to determine if this assumption is met is to calculate the VIF value for each predictor variable.
VIF values start at 1 and have no upper limit. As a general rule of thumb, VIF values greater than 5* indicate potential multicollinearity.
The following tutorials show how to calculate VIF in various statistical software:
 How to Calculate VIF in R 
 How to Calculate VIF in Python 
 How to Calculate VIF in Excel 
* Sometimes researchers use a VIF value of 10 instead, depending on the field of study.
<h3>What to Do if this Assumption is Violated</h3>
If one or more of the predictor variables has a VIF value greater than 5, the easiest way to resolve this issue is to simply remove the predictor variable(s) with the high VIF values.
Alternatively, if you want to keep each predictor variable in the model then you can use a different statistical method such as  ridge regression ,  lasso regression , or  partial least squares regression  that is designed to handle predictor variables that are highly correlated.
<h2>Assumption 3: Independence</h2>
Multiple linear regression assumes that each observation in the dataset is independent.
<h3>How to Determine if this Assumption is Met</h3>
The simplest way to determine if this assumption is met is to perform a  Durbin-Watson test , which is a formal statistical test that tells us whether or not the residuals (and thus the observations) exhibit autocorrelation.
<h3>What to Do if this Assumption is Violated</h3>
Depending on the nature of the way this assumption is violated, you have a few options:
For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
For negative serial correlation, check to make sure that none of your variables are <em>overdifferenced</em>.
For seasonal correlation, consider adding seasonal  dummy variables  to the model.
<h2>Assumption 4: Homoscedasticity</h2>
Multiple linear regression assumes that the residuals have constant variance at every point in the linear model. When this is not the case, the residuals are said to suffer from  heteroscedasticity .
When heteroscedasticity is present in a regression analysis, the results of the regression model become unreliable.
Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.
<h3>How to Determine if this Assumption is Met</h3>
The simplest way to determine if this assumption is met is to create a plot of standardized residuals versus predicted values.
Once you fit a regression model to a dataset, you can then create a scatter plot that shows the predicted values for the response variable on the x-axis and the standardized residuals of the model on the y-axis.
If the points in the scatter plot exhibit a pattern, then heteroscedasticity is present.
The following plot shows an example of a regression model where heteroscedasticity is not a problem:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/standard1.png">
Notice that the standardized residuals are scattered about zero with no clear pattern.
The following plot shows an example of a regression model where heteroscedasticity <em>is</em> a problem:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/standard2.png">
Notice how the standardized residuals become much more spread out as the predicted values get larger. This “cone” shape is a classic sign of heteroscedasticity:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/standard3.png">
<h3>What to Do if this Assumption is Violated</h3>
There are three common ways to fix heteroscedasticity:
<b>1. Transform the response variable. </b>The most common way to deal with heteroscedasticity is to transform the response variable by taking the log, square root, or cube root of all of the values of the response variable. This often causes heteroscedasticity to go away.
<b>2. Redefine the response variable. </b>One way to redefine the response variable is to use a <em>rate</em>, rather than the raw value. For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita.
In most cases, this reduces the variability that naturally occurs among larger populations since we’re measuring the number of flower shops per person, rather than the sheer amount of flower shops.
<b>3. Use weighted regression. </b>Another way to fix heteroscedasticity is to use weighted regression, which assigns a weight to each data point based on the variance of its fitted value.
Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
<b>Related</b>:  How to Perform Weighted Regression in R 
<h2>Assumption 4: Multivariate Normality</h2>
Multiple linear regression assumes that the residuals of the model are normally distributed.
<h3>How to Determine if this Assumption is Met</h3>
There are two common ways to check if this assumption is met:
<b>1.</b> Check the assumption visually using  Q-Q plots .
A Q-Q plot, short for quantile-quantile plot, is a type of plot that we can use to determine whether or not the residuals of a model follow a normal distribution. If the points on the plot roughly form a straight diagonal line, then the normality assumption is met.
The following Q-Q plot shows an example of residuals that roughly follow a normal distribution:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/qqplot3.jpg">
However, the Q-Q plot below shows an example of when the residuals clearly depart from a straight diagonal line, which indicates that they do not follow  normal distribution:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/qqplot4.jpg">
<b>2. </b>Check the assumption using a formal statistical test like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson.
Keep in mind that these tests are sensitive to large sample sizes – that is, they often conclude that the residuals are not normal when your sample size is extremely large. This is why it’s often easier to use graphical methods like a Q-Q plot to check this assumption.
<h3>What to Do if this Assumption is Violated</h3>
If the normality assumption is violated, you have a couple options:
<b>1.</b> First, verify that there are no extreme outliers present in the data that cause the normality assumption to be violated.
<b>2.</b> Next, you can apply a nonlinear transformation to the response variable such as taking the square root, the log, or the cube root of all of the values of the response variable. This often causes the residuals of the model to become more normally distributed.
<h2><span class="orange">Multiple Linear Regression by Hand (Step-by-Step)</span></h2>
 Multiple linear regression  is a method we can use to quantify the relationship between two or more predictor variables and a  response variable .
This tutorial explains how to perform multiple linear regression by hand.
<h3>Example: Multiple Linear Regression by Hand</h3>
Suppose we have the following dataset with one response variable <em>y</em> and two predictor variables X<sub>1</sub> and X<sub>2</sub>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/multbyHand1.png">
Use the following steps to fit a multiple linear regression model to this dataset.
<b>Step 1: Calculate X<sub>1</sub><sup>2</sup>, X<sub>2</sub><sup>2</sup>, X<sub>1</sub>y, X<sub>2</sub>y and X<sub>1</sub>X<sub>2</sub>.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/multbyHand2.png">
<b>Step 2: Calculate Regression Sums.</b>
Next, make the following regression sum calculations:
Σx<sub>1</sub><sup>2 </sup>= ΣX<sub>1</sub><sup>2 </sup>– (ΣX<sub>1</sub>)<sup>2</sup> / n = 38,767 – (555)<sup>2</sup> / 8 = <b>263.875</b>
Σx<sub>2</sub><sup>2 </sup>= ΣX<sub>2</sub><sup>2 </sup>– (ΣX<sub>2</sub>)<sup>2</sup> / n = 2,823 – (145)<sup>2</sup> / 8 = <b>194.875</b>
Σx<sub>1</sub>y = ΣX<sub>1</sub>y – (ΣX<sub>1</sub>Σy) / n = 101,895 – (555*1,452) / 8 = <b>1,162.5</b>
Σx<sub>2</sub>y = ΣX<sub>2</sub>y – (ΣX<sub>2</sub>Σy) / n = 25,364 – (145*1,452) / 8 = <b>-953.5</b>
Σx<sub>1</sub>x<sub>2</sub> = ΣX<sub>1</sub>X<sub>2</sub> – (ΣX<sub>1</sub>ΣX<sub>2</sub>) / n = 9,859 – (555*145) / 8 = <b>-200.375</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/multbyHand3.png">
<b>Step 3: Calculate b<sub>0</sub>, b<sub>1</sub>, and b<sub>2</sub>.</b>
The formula to calculate b<sub>1 </sub>is: [(Σx<sub>2</sub><sup>2</sup>)(Σx<sub>1</sub>y)  – (Σx<sub>1</sub>x<sub>2</sub>)(Σx<sub>2</sub>y)]  / [(Σx<sub>1</sub><sup>2</sup>) (Σx<sub>2</sub><sup>2</sup>) – (Σx<sub>1</sub>x<sub>2</sub>)<sup>2</sup>]
Thus, <b>b<sub>1 </sub></b>= [(194.875)(1162.5)  – (-200.375)(-953.5)]  / [(263.875) (194.875) – (-200.375)<sup>2</sup>] = <b>3.148</b>
The formula to calculate b<sub>2 </sub>is: [(Σx<sub>1</sub><sup>2</sup>)(Σx<sub>2</sub>y)  – (Σx<sub>1</sub>x<sub>2</sub>)(Σx<sub>1</sub>y)]  / [(Σx<sub>1</sub><sup>2</sup>) (Σx<sub>2</sub><sup>2</sup>) – (Σx<sub>1</sub>x<sub>2</sub>)<sup>2</sup>]
Thus, <b>b<sub>2 </sub></b>= [(263.875)(-953.5)  – (-200.375)(1152.5)]  / [(263.875) (194.875) – (-200.375)<sup>2</sup>] = <b>-1.656</b>
The formula to calculate b<sub>0 </sub>is: y – b<sub>1</sub>X<sub>1</sub> – b<sub>2</sub>X<sub>2</sub>
Thus, <b>b<sub>0 </sub></b>= 181.5 – 3.148(69.375) – (-1.656)(18.125) = <b>-6.867</b>
<b>Step 5: Place b<sub>0</sub>, b<sub>1</sub>, and b<sub>2</sub> in the estimated linear regression equation.</b>
The estimated linear regression equation is: <U+0177> = b<sub>0</sub> + b<sub>1</sub>*x<sub>1</sub> + b<sub>2</sub>*x<sub>2</sub>
In our example, it is <b><U+0177> = -6.867 + 3.148x<sub>1</sub> – 1.656x<sub>2</sub></b>
<h3>How to Interpret a Multiple Linear Regression Equation</h3>
Here is how to interpret this estimated linear regression equation: <U+0177> = -6.867 + 3.148x<sub>1</sub> – 1.656x<sub>2</sub>
<b>b<sub>0</sub> = -6.867</b>. When both predictor variables are equal to zero, the mean value for y is -6.867.
<b>b<sub>1 </sub>= 3.148</b>. A one unit increase in x<sub>1 </sub>is associated with a 3.148 unit increase in y, on average, assuming x<sub>2 </sub>is held constant.
<b>b<sub>2 </sub>= -1.656</b>. A one unit increase in x<sub>2 </sub>is associated with a 1.656 unit decrease in y, on average, assuming x<sub>1 </sub>is held constant.
<h2><span class="orange">How to Perform Multiple Linear Regression in Excel</span></h2>
<b>Multiple linear regression </b>is a method we can use to understand the relationship between two or more explanatory variables and a  response variable .
This tutorial explains how to perform multiple linear regression in Excel.
<em><b>Note: </b>If you only have one explanatory variable, you should instead perform  simple linear regression .</em>
<h3>Example: Multiple Linear Regression in Excel</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain college entrance exam.
To explore this relationship, we can perform multiple linear regression using <b>hours studied</b> and <b>prep exams taken </b>as explanatory variables and <b>exam score </b>as a response variable.
Perform the following steps in Excel to conduct a multiple linear regression.
<b>Step 1: Enter the data.</b>
Enter the following data for the number of hours studied, prep exams taken, and exam score received for 20 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel1.png">
<b>Step 2: Perform multiple linear regression.</b>
Along the top ribbon in Excel, go to the <b>Data</b> tab and click on <b>Data Analysis</b>. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
Once you click on <b>Data Analysis,</b> a new window will pop up. Select <b>Regression </b>and click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel2.png">
For <b>Input Y Range</b>, fill in the array of values for the response variable. For <b>Input X Range</b>, fill in the array of values for the two explanatory variables. Check the box next to <b>Labels </b>so Excel knows that we included the variable names in the input ranges. For <b>Output Range</b>, select a cell where you would like the output of the regression to appear. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel3.png">
The following output will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
<b>Step 3: Interpret the output.</b>
Here is how to interpret the most relevant numbers in the output:
<b>R Square: 0.734</b>. This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, 73.4% of the variation in the exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>Standard error:</b> <b>5.366</b>. This is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of 5.366 units from the regression line.
<b>F: 23.46</b>. This is the overall F statistic for the regression model, calculated as regression MS / residual MS.
<b>Significance F: 0.0000</b>. This is the p-value associated with the overall F statistic. It tells us whether or not the regression model as a whole is statistically significant. In other words, it tells us if the two explanatory variables combined have a statistically significant association with the response variable. In this case the p-value is less than 0.05, which indicates that the explanatory variables <b>hours studied</b> and <b>prep exams taken </b>combined have a statistically significant association with <b>exam score</b>.
<b>P-values. </b>The individual p-values tell us whether or not each explanatory variable is statistically significant. We can see that <b>hours studied </b>is statistically significant (p = 0.00) while <b>prep exams taken </b>(p = 0.52) is not statistically signifciant at α = 0.05. Since <b>prep exams taken </b>is not statistically significant, we may end up deciding to remove it from the model.
<b>Coefficients: </b>The coefficients for each explanatory variable tell us the average expected change in the response variable, assuming the other explanatory variable remains constant. For example, for each additional hour spent studying, the average exam score is expected to increase by <b>5.56</b>, assuming that <b>prep exams taken </b>remains constant.
Here’s another way to think about this: If student A and student B both take the same amount of prep exams but student A studies for one hour more, then student A is expected to earn a score that is <b>5.56</b> points higher than student B.
We interpret the coefficient for the intercept to mean that the expected exam score for a student who studies zero hours and takes zero prep exams is <b>67.67</b>.
<b>Estimated regression equation: </b>We can use the coefficients from the output of the model to create the following estimated regression equation:
<b>exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams)</b>
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study and the number of prep exams they take. For example, a student who studies for three hours and takes one prep exam is expected to receive a score of <b>83.75</b>:
exam score = 67.67 + 5.56*(3) – 0.60*(1) = 83.75
Keep in mind that because <b>prep exams taken </b>was not statistically significant (p = 0.52), we may decide to remove it because it doesn’t add any improvement to the overall model. In this case, we could perform simple linear regression using only <b>hours studied </b>as the explanatory variable.
The results of this simple linear regression analysis can be found  here .
<h2><span class="orange">How to Perform Multiple Linear Regression in SAS</span></h2>
 Multiple linear regression  is a method we can use to understand the relationship between two or more predictor variables and a  response variable .
This tutorial explains how to perform multiple linear regression in SAS.
<h3>Step 1: Create the Data</h3>
Suppose we want to fit a multiple linear regression model that uses number of hours spent studying and number of prep exams taken to predict the final exam score of students:
Exam Score = β<sub>0</sub> + β<sub>1</sub>(hours) +β<sub>2</sub>(prep exams)
First, we’ll use the following code to create a dataset that contains this information for 20 students:
<b>/*create dataset*/
data exam_data;
    input hours prep_exams score;
    datalines;
1 1 76
2 3 78
2 3 85
4 5 88
2 2 72
1 2 69
5 1 94
4 1 94
2 0 88
4 3 92
4 4 90
3 3 75
6 2 96
5 4 90
3 4 82
4 4 85
6 5 99
2 1 83
1 0 62
2 1 76
;
run;
</b>
<h3>Step 2: Perform Multiple Linear Regression</h3>
Next, we’ll use <b>proc reg</b> to fit a multiple linear regression model to the data:
<b>/*fit multiple linear regression model*/
proc reg data=exam_data;
    model score = hours prep_exams;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/multSAS1.jpg"445">
Here is how to interpret the most relevant numbers in each table:
<b>Analysis of Variance Table:</b>
The overall  F-value  of the regression model is <b>23.46 </b>and the corresponding p-value is <b>&lt;.0001</b>.
Since this p-value is less than .05, we conclude that the regression model as a whole is statistically significant.
<b>Model Fit Table:</b>
The <b>R-Square</b> value tells us the percentage of variation in the exam scores that can be explained by the number of hours studied and the number of prep exams taken.
In general, the larger the  R-squared value  of a regression model the better the predictor variables are able to predict the value of the response variable.
In this case, <b>73.4%</b> of the variation in exam scores can be explained by the number of hours studied and number of prep exams taken.
The <b>Root MSE</b> value is also useful to know. This represents the average distance that the observed values fall from the regression line.
In this regression model, the observed values fall an average of <b>5.3657</b> units from the regression line.
<b>Parameter Estimates Table:</b>
We can use the parameter estimate values in this table to write the fitted regression equation:
Exam score = 67.674 + 5.556*(hours) – .602*(prep_exams)
We can use this equation to find the estimated exam score for a student, based on the number of hours they studied and the number of prep exams they took.
For example, a student that studies for 3 hours and takes 2 prep exams is expected to receive an exam score of <b>83.1</b>:
Estimated exam score = 67.674 + 5.556*(3) – .602*(2) = <b>83.1</b>
The p-value for hours (&lt;.0001) is less than .05, which means that it has a statistically significant association with exam score.
However, the p-value for prep exams (.5193) is not less than .05, which means it does not have a statistically significant association with exam score.
We may decide to remove prep exams from the model since it isn’t statistically significant and instead perform  simple linear regression  using hours studied as the only predictor variable.
<h2><span class="orange">How to Perform Multiple Linear Regression in R</span></h2>
This guide walks through an example of how to conduct  multiple linear regression  in R, including:
Examining the data before fitting the model
Fitting the model
Checking the assumptions of the model
Interpreting the output of the model
Assessing the goodness of fit of the model
Using the model to make predictions
Let’s jump in!
<h3>Setup</h3>
For this example we will use the built-in R dataset  <em>mtcars</em> , which contains information about various attributes for 32 different cars:
<b>#view first six lines of <em>mtcars</em>
head(mtcars)
#                   mpg cyl disp  hp drat    wt  qsec vs am gear carb
#Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
#Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
#Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
#Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
#Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
#Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
In this example we will build a multiple linear regression model that uses <em>mpg </em>as the response variable and <em>disp</em>, <em>hp</em>, and <em>drat </em>as the predictor variables.
<b>#create new data frame that contains only the variables we would like to use to
data &lt;- mtcars[ , c("mpg", "disp", "hp", "drat")]
#view first six rows of new data frame
head(data)
#                   mpg disp  hp drat
#Mazda RX4         21.0  160 110 3.90
#Mazda RX4 Wag     21.0  160 110 3.90
#Datsun 710        22.8  108  93 3.85
#Hornet 4 Drive    21.4  258 110 3.08
#Hornet Sportabout 18.7  360 175 3.15
#Valiant           18.1  225 105 2.76</b>
<h3>Examining the Data</h3>
Before we fit the model, we can examine the data to gain a better understanding of it and also visually assess whether or not multiple linear regression could be a good model to fit to this data.
In particular, we need to check if the predictor variables have a <em>linear </em>association with the response variable, which would indicate that a multiple linear regression model may be suitable.
To do so, we can use the <b>pairs() </b>function to create a  scatterplot  of every possible pair of variables:
<b>pairs(data, pch = 18, col = "steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/mlr1.jpg">
From this pairs plot we can see the following:
<em>mpg </em>and <em>disp </em>appear to have a strong negative linear correlation
<em>mpg </em>and <em>hp </em>appear to have a strong positive linear correlation
<em>mpg </em>and <em>drat </em>appear to have a modest negative linear correlation
Note that we could also use the <b>ggpairs() </b>function from the <b>GGally </b>library to create a similar plot that contains the actual  linear correlation coefficients  for each pair of variables:
<b>#install and load the <em>GGally </em>library
install.packages("GGally")
library(GGally)
#generate the pairs plot
ggpairs(data)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/mlr2.jpg">
Each of the predictor variables appears to have a noticeable linear correlation with the response variable <em>mpg</em>, so we’ll proceed to fit the linear regression model to the data.
<h3>Fitting the Model</h3>
The basic syntax to fit a multiple linear regression model in R is as follows:
<b>lm(response_variable ~ predictor_variable1 + predictor_variable2 + ..., data = data)</b>
Using our data, we can fit the model using the following code:
<b>model &lt;- lm(mpg ~ disp + hp + drat, data = data)</b>
<h3>Checking Assumptions of the Model</h3>
Before we proceed to check the output of the model, we need to first check that the model assumptions are met. Namely, we need to verify the following:
<b>1. The distribution of model residuals should be approximately normal.</b>
We can check if this assumption is met by creating a simple histogram of residuals:
<b>hist(residuals(model), col = "steelblue")
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/mlr3.jpg">
Although the distribution is slightly  right skewed , it isn’t abnormal enough to cause any major concerns.
<b>2. The variance of the residuals should be consistent for all observations.</b>
This preferred condition is known as homoskedasticity. Violation of this assumption is known as  heteroskedasticity .
To check if this assumption is met we can create a <em>fitted value vs. residual plot:</em>
<b>#create fitted value vs residual plot
plot(fitted(model), residuals(model))
#add horizontal line at 0
abline(h = 0, lty = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/mlr4.jpg">
Ideally we would like the residuals to be equally scattered at every fitted value. We can see from the plot that the scatter tends to become a bit larger for larger fitted values, but this pattern isn’t extreme enough to cause too much concern.
<h3>Interpreting the Output of the Model</h3>
Once we’ve verified that the model assumptions are sufficiently met, we can look at the output of the model using the <b>summary() </b>function:
<b>summary(model)
#Call:
#lm(formula = mpg ~ disp + hp + drat, data = data)
#
#Residuals:
#    Min      1Q  Median      3Q     Max 
#-5.1225 -1.8454 -0.4456  1.1342  6.4958 
#
#Coefficients:
#             Estimate Std. Error t value Pr(>|t|)   
#(Intercept) 19.344293   6.370882   3.036  0.00513 **
#disp        -0.019232   0.009371  -2.052  0.04960 * 
#hp          -0.031229   0.013345  -2.340  0.02663 * 
#drat         2.714975   1.487366   1.825  0.07863 . 
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#
#Residual standard error: 3.008 on 28 degrees of freedom
#Multiple R-squared:  0.775,Adjusted R-squared:  0.7509 
#F-statistic: 32.15 on 3 and 28 DF,  p-value: 3.28e-09
</b>
From the output we can see the following:
The overall F-statistic of the model is <b>32.15 </b>and the corresponding p-value is <b>3.28e-09</b>. This indicates that the overall model is statistically significant. In other words, the regression model as a whole is useful.
<em>disp </em>is statistically significant at the 0.10 significance level. In particular, the coefficient from the model output tells is that a one unit increase in <em>disp</em> is associated with a -0.019 unit decrease, on average, in <em>mpg</em>, assuming <em>hp</em> and <em>drat</em> are held constant.
<em>hp </em>is statistically significant at the 0.10 significance level. In particular, the coefficient from the model output tells is that a one unit increase in <i>hp </i>is associated with a -0.031 unit decrease, on average, in <em>mpg</em>, assuming <i>disp </i>and <em>drat</em> are held constant.
<em>drat </em>is statistically significant at the 0.10 significance level. In particular, the coefficient from the model output tells is that a one unit increase in <i>drat </i>is associated with a 2.715 unit increase, on average, in <em>mpg</em>, assuming <i>disp </i>and <i>hp </i>are held constant.
<h3>Assessing the Goodness of Fit of the Model</h3>
To assess how “good” the regression model fits the data, we can look at a couple different metrics:
<b>1. Multiple R-Squared</b>
This  measures the strength of the linear relationship between the predictor variables and the response variable. A multiple R-squared of 1 indicates a perfect linear relationship while a multiple R-squared of 0 indicates no linear relationship whatsoever.
Multiple R is also the square root of R-squared, which is the proportion of the variance in the response variable that can be explained by the predictor variables. In this example, the multiple R-squared is <b>0.775</b>. Thus, the R-squared is 0.775<sup>2</sup> = <b>0.601</b>. This indicates that <b>60.1%</b> of the variance in <i>mpg</i> can be explained by the predictors in the model.
<b>Related: </b> What is a Good R-squared Value? 
<b>2. Residual Standard Error</b>
This measures the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of<b> 3.008 units </b>from the regression line<b>.</b>
<b>Related:</b>  Understanding the Standard Error of the Regression 
<h3>Using the Model to Make Predictions</h3>
From the output of the model we know that the fitted multiple linear regression equation is as follows:
mpg<sub>hat</sub> = -19.343 – 0.019*disp – 0.031*hp + 2.715*drat
We can use this equation to make predictions about what <em>mpg </em>will be for new  observations . For example, we can find the predicted value of <em>mpg </em>for a car that has the following attributes:
<em>disp</em> = 220
<em>hp</em> = 150
<em>drat </em> = 3
<b>#define the coefficients from the model output
intercept &lt;- coef(summary(model))["(Intercept)", "Estimate"]
disp &lt;- coef(summary(model))["disp", "Estimate"]
hp &lt;- coef(summary(model))["hp", "Estimate"]
drat &lt;- coef(summary(model))["drat", "Estimate"]
#use the model coefficients to predict the value for <em>mpg</em>
intercept + disp*220 + hp*150 + drat*3
#[1] 18.57373</b>
For a car with <em>disp</em> = 220, <em> hp</em> = 150, and <em>drat</em> = 3, the model predicts that the car would have a <em>mpg </em>of <b>18.57373</b>.
<em>You can find the complete R code used in this tutorial  here .</em>
<h2><span class="orange">How to Perform Multiple Linear Regression in SPSS</span></h2>
<b>Multiple linear regression </b>is a method we can use to understand the relationship between two or more explanatory variables and a response variable.
This tutorial explains how to perform multiple linear regression in SPSS.
<h3>Example: Multiple Linear Regression in SPSS</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain exam. To explore this, we can perform multiple linear regression using the following variables:
<b>Explanatory variables:</b>
Hours studied
Prep exams taken
<b>Response variable:</b>
Exam score
Use the following steps to perform this multiple linear regression in SPSS.
<b>Step 1: Enter the data.</b>
Enter the following data for the number of hours studied, prep exams taken, and exam score received for 20 students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS1.png">
<b>Step 2: Perform multiple linear regression.</b>
Click the <b>Analyze </b>tab, then <b>Regression</b>, then <b>Linear</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS2.png">
Drag the variable <b>score </b>into the box labelled Dependent. Drag the variables <b>hours</b> and <b>prep_exams</b> into the box labelled Independent(s). Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS3.png">
<b>Step 3: Interpret the output.</b>
Once you click <b>OK</b>, the results of the multiple linear regression will appear in a new window.
The first table we’re interested in is titled <b>Model Summary</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS4.png">
Here is how to interpret the most relevant numbers in this table:
<b>R Square: </b>This is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, <b>73.4%</b> of the variation in exam scores can be explained by hours studied and number of prep exams taken.
<b>Std. Error of the Estimate: </b>The  standard error  is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of <b>5.3657</b> units from the regression line.
The next table we’re interested in is titled <b>ANOVA</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS6.png">
Here is how to interpret the most relevant numbers in this table:
<b>F: </b>This is the overall F statistic for the regression model, calculated as Mean Square Regression / Mean Square Residual.
<b>Sig: </b>This is the p-value associated with the overall F statistic. It tells us whether or not the regression model as a whole is statistically significant. In other words, it tells us if the two explanatory variables combined have a statistically significant association with the response variable. In this case the p-value is equal to 0.000, which indicates that the explanatory variables hours studied and prep exams taken have a statistically significant association with exam score.
The next table we’re interested in is titled <b>Coefficients</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/multRegSPSS5.png">
Here is how to interpret the most relevant numbers in this table:
<b>Unstandardized B (Constant): </b>This tells us the average value of the response variable when both predictor variables are zero. In this example, the average exam score is <b>67.674 </b>when hours studied and prep exams taken are both equal to zero.
<b>Unstandardized B (hours): </b>This tells us the average change in exam score associated with a one unit increase in hours studied, assuming number of prep exams taken is held constant. In this case, each additional hour spent studying is associated with an increase of <b>5.556</b> points in exam score, assuming the number of prep exams taken is held constant.
<b>Unstandardized B (prep_exams): </b>This tells us the average change in exam score associated with a one unit increase in prep exams taken, assuming number of hours studied is held constant. In this case, each additional prep exam taken is associated with a decrease of <b>.602</b> points in exam score, assuming the number of hours studied is held constant.
<b>Sig. (hours): </b>This is the p-value for the explanatory variable <b>hours</b>. Since this value (.000) is less than .05, we can conclude that hours studied has a statistically significant association with exam score.
<b>Sig. (prep_exams): </b>This is the p-value for the explanatory variable <b>prep_exams</b>. Since this value (.519) is not less than .05, we cannot conclude that number of prep exams taken has a statistically significant association with exam score.
Lastly, we can form a regression equation using the values shown in the table for <b>constant</b>, <b>hours</b>, and <b>prep_exams</b>. In this case, the equation would be:
Estimated exam score = 67.674 + 5.556*(hours) – .602*(prep_exams)
We can use this equation to find the estimated exam score for a student, based on the number of hours they studied and the number of prep exams they took. For example, a student that studies for 3 hours and takes 2 prep exams is expected to receive an exam score of 83.1:
Estimated exam score = 67.674 + 5.556*(3) – .602*(2) = 83.1
<em><b>Note: </b>Since the explanatory variable <b>prep exams </b>was not found to be statistically significant, we may decide to remove it from the model and instead perform  simple linear regression  using <b>hours studied </b>as the only explanatory variable.</em>
<h2><span class="orange">How to Perform Multiple Linear Regression in Stata</span></h2>
<b>Multiple linear regression </b>is a method you can use to understand the relationship between several explanatory variables and a response variable.
This tutorial explains how to perform multiple linear regression in Stata.
<h2>Example: Multiple Linear Regression in Stata</h2>
Suppose we want to know if miles per gallon and weight impact the price of a car. To test this, we can perform a multiple linear regression using miles per gallon and weight as the two explanatory variables and price as the response variable.
Perform the following steps in Stata to conduct a multiple linear regression using the dataset called <em>auto</em>, which contains data on 74 different cars.
<b>Step 1: Load the data.</b>
Load the data by typing the following into the Command box:
<b> use http://www.stata-press.com/data/r13/auto</b>
<b>Step 2: Get a summary of the data.</b>
Gain a quick understanding of the data you’re working with by typing the following into the Command box:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/simpleRegressionStata1.png">
We can see that there are 12 different variables in the dataset, but the only ones we care about are <em>mpg</em>, <em>weight</em>, and <em>price</em>.
We can see the following basic summary statistics about these three variables:
<b>price | </b>mean = $6,165, min = $3,291, max $15,906
<b>mpg | </b>mean = 21.29, min = 12, max = 41
<b>weight | </b>mean = 3,019 pounds, min = 1,760 pounds, max = 4,840 pounds
<b>Step 3: Perform multiple linear regression.</b>
Type the following into the Command box to perform a multiple linear regression using mpg and weight as explanatory variables and price as a response variable.
<b>regress price mpg weight</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegStata1.png">
Here is how to interpret the most interesting numbers in the output:
<b>Prob > F: </b>0.000. This is the p-value for the overall regression. Since this value is less than 0.05, this indicates that the combined explanatory variables of <em>mpg </em>and <em>weight </em>have a statistically significant relationship with the response variable <em>price</em>.
<b>R-squared:</b> 0.2934. This is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, 29.34% of the variation in price can be explained by mpg and weight.
<b>Coef (mpg): </b>-49.512. This tells us the average change in price that is associated with a one unit increase in the mpg, <em>assuming weight is held constant</em>. In this example, each one unit increase in mpg is associated with an average decrease of about $49.51 in price, assuming weight is held constant.
For example, suppose cars A and B both weigh 2,000 pounds. If car A gets 20 mpg and car B only gets 19 mpg, we would expect the price of car A to be $49.51 less than the price of car B.
<b>P>|t| (mpg): </b>0.567. This is the p-value associated with the test statistic for mpg. Since this value is not less than 0.05, we don’t have evidence to say that mpg has a statistically significant relationship with price.
<b>Coef (weight): </b>1.746. This tells us the average change in price that is associated with a one unit increase in weight, <em>assuming mpg is held constant</em>. In this example, each one unit increase in weight is associated with an average increase of about $1.74 in price, assuming mpg is held constant.
For example, suppose cars A and B both get 20 mpg. If car A weighs one pound more than car B, then car A is expected to cost $1.74 more.
<b>P>|t| (weight): </b>0.008. This is the p-value associated with the test statistic for weight. Since this value is less than 0.05, we have sufficient evidence to say that weight has a statistically significant relationship with price.
<b>Coef (_cons): </b>1946.069. This tells us the average price of a car when both mpg and weight are zero. In this example, the average price is $1,946 when both weight and mpg are zero. This doesn’t actually make much sense to interpret since the weight and mpg of a car can’t be zero, but the number 1946.069 is needed to form a regression equation.
<b>Step 4: Report the results.</b>
Lastly, we want to report the results of our multiple linear regression. Here is an example of how to do so:
Multiple linear regression was performed to quantify the relationship between the weight and mpg of a car and its price. A sample of 74 cars was used in the analysis.
 
Results showed that there was a statistically significant relationship between weight and price (t = 2.72, p = .008), but there was not a statistically significant relationship between mpg and price (and mpg (t = -.57, p = 0.567).
<h2><span class="orange">Introduction to Multiple Linear Regression</span></h2>
When we want to understand the relationship between a single predictor variable and a response variable, we often use  simple linear regression .
However, if we’d like to understand the relationship between <em>multiple</em> predictor variables and a response variable then we can instead use <b>multiple linear regression</b>.
If we have <em>p</em> predictor variables, then a multiple linear regression model takes the form:
<b>Y = β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub> + ε</b>
where:
<b>Y</b>: The response variable
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The average effect on Y of a one unit increase in X<sub>j</sub>, holding all other predictors fixed
<b> ε</b>: The error term
The values for β<sub>0</sub>, β<sub>1</sub>, B<sub>2</sub>, … , β<sub>p</sub> are chosen using <b>the least square method</b>, which minimizes the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)<sup>2</sup></b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
The method used to find these coefficient estimates relies on matrix algebra and we will not cover the details here. Fortunately, any statistical software can calculate these coefficients for you.
<h3>How to Interpret Multiple Linear Regression Output</h3>
Suppose we fit a multiple linear regression model using the predictor variables <em>hours studied</em> and <em>prep exams taken</em> and a response variable <em>exam score</em>.
The following screenshot shows what the multiple linear regression output might look like for this model:
<em><b>Note:</b> The screenshot below shows  multiple linear regression output for Excel , but the numbers shown in the output are typical of the regression output you’ll see using any statistical software.</em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
From the model output, the coefficients allow us to form an estimated multiple linear regression model:
<b>Exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams)</b>
The way to interpret the coefficients are as follows: 
Each additional one unit increase in hours studied is associated with an average increase of <b>5.56</b> points in exam score, <em>assuming prep exams is held constant.</em>
Each additional one unit increase in prep exams taken is associated with an average decrease of <b>0.60</b> points in exam score, <em>assuming hours studied is held constant.</em>
We can also use this model to find the expected exam score a student will receive based on their total hours studied and prep exams taken. For example, a student who studies for 4 hours and takes 1 prep exam is expected to score a <b>89.31</b> on the exam:
Exam score = 67.67 + 5.56*(4) -0.60*(1) = <b>89.31</b>
Here is how to interpret the rest of the model output:
<b>R-Square:</b> This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, 73.4% of the variation in the exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>Standard error:</b> This is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of 5.366 units from the regression line.
<b>F: </b>This is the overall F statistic for the regression model, calculated as regression MS / residual MS.
<b>Significance F:</b> This is the p-value associated with the overall F statistic. It tells us whether or not the regression model as a whole is statistically significant. In other words, it tells us if the two explanatory variables combined have a statistically significant association with the response variable. In this case the p-value is less than 0.05, which indicates that the explanatory variables hours studied and prep exams taken combined have a statistically significant association with exam score.
<b>Coefficient P-values. </b>The individual p-values tell us whether or not each explanatory variable is statistically significant. We can see that hours studied is statistically significant (p = 0.00) while prep exams taken (p = 0.52) is not statistically significant at α = 0.05. Since prep exams taken is not statistically significant, we may end up deciding to remove it from the model.
<h3>How to Assess the Fit of a Multiple Linear Regression Model</h3>
There are two numbers that are commonly used to assess how well a multiple linear regression model “fits” a dataset:
<b>1.</b> <b>R-Squared:</b> This is the proportion of the variance in the  response variable  that can be explained by the predictor variables.
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all. A value of 1 indicates that the response variable can be perfectly explained without error by the predictor variable.
The higher the R-squared of a model, the better the model is able to fit the data.
<b>2. Standard Error:</b> This is the average distance that the observed values fall from the regression line. The smaller the standard error, the better a model is able to fit the data.
If we’re interested in making predictions using a regression model, the standard error of the regression can be a more useful metric to know than R-squared because it gives us an idea of how precise our predictions will be in terms of units.
For a complete explanation of the pros and cons of using R-squared vs. Standard Error for assessing model fit, check out the following articles:
 What is a Good R-squared Value? 
 Understanding the Standard Error of a Regression Model 
<h3>Assumptions of Multiple Linear Regression</h3>
There are four key assumptions that multiple linear regression makes about the data:
<b>1. Linear relationship:</b> There exists a linear relationship between the independent variable, x, and the dependent variable, y.
<b>2. Independence: </b>The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.
<b>3. Homoscedasticity: </b>The residuals have constant variance at every level of x.
<b>4. Normality: </b>The residuals of the model are normally distributed.
For a complete explanation of how to test these assumptions, check out  this article .
<h3>Multiple Linear Regression Using Software</h3>
The following tutorials provide step-by-step examples of how to perform multiple linear regression using different statistical software:
 How to Perform Multiple Linear Regression in R 
 How to Perform Multiple Linear Regression in Python 
 How to Perform Multiple Linear Regression in Excel 
 How to Perform Multiple Linear Regression in SPSS 
 How to Perform Multiple Linear Regression in Stata 
 How to Perform Linear Regression in Google Sheets 
<h2><span class="orange">Multiple R vs. R-Squared: What’s the Difference?</span></h2>
When you fit a  regression model  using most statistical software, you’ll often notice the following two values in the output:
<b>Multiple R:</b> The multiple correlation coefficient between three or more variables.
<b>R-Squared:</b> This is calculated as (Multiple R)<sup>2</sup> and it represents the proportion of the variance in the  response variable  of a regression model that can be explained by the predictor variables. This value ranges from 0 to 1.
In practice, we’re often interested in the R-squared value because it tells us how useful the predictor variables are at predicting the value of the response variable.
However, each time we add a new predictor variable to the model the R-squared is guaranteed to increase even if the predictor variable isn’t useful. 
The <b>adjusted R-squared</b> is a modified version of R-squared that adjusts for the number of predictors in a regression model. It is calculated as:
<b>Adjusted R<sup>2</sup> = 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)]</b>
where:
<b>R<sup>2</sup></b>: The R<sup>2</sup> of the model
<b>n</b>: The number of observations
<b>k</b>: The number of predictor variables
Since R-squared always increases as you add more predictors to a model, adjusted R-squared can serve as a metric that tells you how useful a model is, <em>adjusted for the number of predictors in a model</em>.
To gain a better understanding of each of these terms, consider the following example.
<h3>Example: Multiple R, R-Squared, & Adjusted R-Squared</h3>
Suppose we have the following dataset that contains the following three variables for 12 different students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/multipleR_vs_Rsquared1.png">
Suppose we fit a multiple linear regression model using <em>Study Hours</em> and <em>Current Grade</em> as the predictor variables and <em>Exam Score</em> as the response variable and get the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/multipleR_vs_Rsquared2.png">
We can observe the values for the following three metrics:
<b>Multiple R: 0.978</b>. This represents the multiple correlation between the response variable and the two predictor variables.
<b>R Square: 0.956</b>. This is calculated as (Multiple R)<sup>2</sup> = (0.978)<sup>2</sup> = 0.956. This tells us that 95.6% of the variation in exam scores can be explained by the number of hours spent studying by the student and their current grade in the course.
<b>Adjusted R-Square: 0.946</b>. This is calculated as:
<b>Adjusted R<sup>2</sup></b>= 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)] = 1 – [(1-.956)*(12-1)/(12-2-1)] = 0.946.
This represents the R-squared value, <em>adjusted for the number of predictor variables in the model</em>.
This metric would be useful if we, say, fit another regression model with 10 predictors and found that the Adjusted R-squared of that model was <b>0.88</b>. This would indicate that the regression model with just two predictors is better because it has a higher adjusted R-squared value.
<h2><span class="orange">Excel: How to Use Multiple Linear Regression for Predictive Analysis</span></h2>
Often you may want to use a  multiple linear regression model  you’ve built in Excel to predict the response value of a new observation or data point.
Fortunately this is fairly easy to do and the following step-by-step example shows how to do so.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset to work with in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/multRegressionForecastExcel1.png">
<h3>Step 2: Fit a Multiple Linear Regression Model</h3>
Next, let’s fit a multiple linear regression model using x1 and x2 as predictor variables and y as the response variable.
To do so, we can use the <b>LINEST(y_values, x_values)</b> function as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/multRegressionForecastExcel2.png">
Once we click enter, the regression coefficients appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/multRegressionForecastExcel3.png">
The fitted multiple linear regression model is:
y = 17.1159 + 1.0183(x1) + 0.3963(x2)
<h3>Step 3: Use the Model to Predict a New Value</h3>
Now suppose that we’d like to use this regression model to predict the value of a new observation that has the following values for the predictor variables:
x1: 8
x2: 10
To do so, we can use the following formula in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/multRegressionForecastExcel4.png">
Using these values for the predictor variables, the multiple linear regression model predicts that the value for y will be <b>29.22561</b>.
<h3>Step 4: Use the Model to Predict Several New Values</h3>
If we’d like to use the multiple linear regression model to predict the response value for several new observations, we can simply make absolute cell references to the regression coefficients:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/multRegressionForecastExcel5.png">
<h2><span class="orange">How to Use a MULTIPLY IF Function in Excel</span></h2>
You can use the following basic formula to create a MULTIPLY IF function in Excel:
<b>=PRODUCT(IF(A2:A11="string",B2:B11,""))
</b>
This formula multiplies all of the values together in <b>B2:B11</b> where the corresponding cell in the range <b>A2:A11</b> is equal to “string”.
The following example shows how to use this formula in practice.
<h3>Example: MULTIPLY IF Function in Excel</h3>
Suppose we have the following dataset that shows the total points scored by basketball players on various teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/multiplyif1.jpg"469">
We can use the following formula to multiply together each value in the <b>Points</b> column if the corresponding value in the <b>Team</b> column is equal to “Mavs”:
<b>=PRODUCT(IF(A2:A11="Mavs",B2:B11,""))
</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/multiplyif2.jpg"521">
The product of the values in the <b>Points</b> column for the rows where <b>Team</b> is equal to “Mavs” is <b>700</b>.
We can verify that this is correct by manually multiplying each of the points values together for the Mavs:
Product of points: 7 * 20 * 5 = <b>700</b>
This matches the value that we calculated using the formula.
<b>Note</b>: You can find the complete documentation for the <b>PRODUCT</b> function in Excel  here .
<h2><span class="orange">How to Use a MULTIPLY IF Function in Google Sheets</span></h2>
You can use the following basic formula to create a <b>MULTIPLY IF</b> function in Google Sheets:
<b>=ARRAYFORMULA(PRODUCT(IF(A2:A11="string",B2:B11,"")))
</b>
This formula multiplies all of the values together in <b>B2:B11</b> where the corresponding cell in the range <b>A2:A11</b> is equal to “string”.
The following example shows how to use this formula in practice.
<h2>Example: MULTIPLY IF Function in Google Sheets</h2>
Suppose we have the following dataset that shows the total points scored by basketball players on various teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/multiplyif1.jpg"505">
We can use the following formula to multiply together each value in the <b>points</b> column if the corresponding value in the <b>team</b> column is equal to “Mavs”:
<b>=ARRAYFORMULA(PRODUCT(IF(A2:A11="Mavs",B2:B11,""))) 
</b>
The following screenshot shows how to use this formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/multiplyif2.jpg">
The product of the values in the <b>points</b> column for the rows where <b>team</b> is equal to “Mavs” is <b>700</b>.
We can verify that this is correct by manually multiplying each of the points values together for the Mavs:
Product of points: 7 * 20 * 5 = <b>700</b>
This matches the value that we calculated using the formula.
<b>Note</b>: You can find the complete documentation for the <b>PRODUCT</b> function in Google Sheets  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Use a RANK IF Formula in Google Sheets 
 How to Use AVERAGEIFS in Google Sheets 
 How to Calculate Standard Deviation IF in Google Sheets 
<h2><span class="orange">What is Multistage Sampling? (Definition & Example)</span></h2>
<b>Multistage sampling</b> is a method of obtaining a  sample  from a population by splitting a population into smaller and smaller groups and taking samples of individuals from the smallest resulting groups.
For example, suppose we’re interested in estimating the average household income in the U.S. For simplicity, let’s assume there are 100 million households. This represents the entire population we’re interested in.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/multistage1.png">
However, it would be too expensive and time-consuming to collect income data on each household, so instead we may take a simple random sample of 15 states.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/multistage2.png">
Then, within each state we may take a simple random sample of 10 counties.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/multistage3.png">
Then, within each county we may take a simple random sample of 100 households.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/multistage4.png">
Our resulting sample would contain <b>15,000</b> total households:
Sample = 15 states * 10 counties * 100 households = 15,000 households.
This method of obtaining this sample is known as <b>multistage sampling</b>.
In this example there were 3 different stages, but in practice any sampling method that uses two or more stages can be considered multistage sampling.
The important thing is that we use a <b>probability sampling method</b> at each stage – that is, we use a method in which each member of a group is equally likely to be included in the sample.
Examples of probability sampling methods include:
Simple random sample
Stratified random sample
Cluster random sample
Systematic random sample
You can read details about each of these sampling methods  here .
<h3>Examples of Multistage Sampling</h3>
Multistage sampling is used in a variety of fields, including:
<b>The Census Bureau:</b> The U.S. Census Bureau uses multistage sampling by first taking a simple random sample of counties in each state, then taking another simple random sample of households in each county and collecting data on those households.
<b>Quality Control:</b> Many warehouses use multistage sampling for assessing quality control. For example, a widget manufacturer may take a simple random sample of packages from a certain production run and then take another simple random sample of widgets from each package to get an estimate of the percentage of defective widgets.
<b>Gallup Polls:</b> Most Gallup polls select a simple random sample of districts in each state and then select a simple random sample of households within each district and collect data on those households.
<h3>Benefits of Multistage Sampling</h3>
Multistage sampling offers the following benefits:
It’s convenient.
It’s generally cost-effective.
It’s particularly useful when individuals of interest are geographically dispersed.
It does not require a complete list of all individuals in a population.
Because of these benefits, it is commonly used in a variety of settings.
<h2><span class="orange">Multivariate Adaptive Regression Splines in Python</span></h2>
 Multivariate adaptive regression splines  (MARS) can be used to model nonlinear relationships between a set of predictor variables and a  response variable .
This method works as follows:
<b>1. </b>Divide a dataset into <em>k</em> pieces.
<b>2. </b>Fit a regression model to each piece.
<b>3.</b> Use k-fold cross-validation to choose a value for <em>k</em>.
This tutorial provides a step-by-step example of how to fit a MARS model to a dataset in Python.
<h3>Step 1: Import Necessary Packages</h3>
To fit a MARS model in Python, we’ll use the <b>Earth()</b> function from  sklearn-contrib-py-earth . We’ll start by installing this package:
<b>pip install sklearn-contrib-py-earth
</b>
Next, we’ll install a few other necessary packages:
<b>import pandas as pd
from numpy import mean
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.datasets import make_regression
from pyearth import Earth
</b>
<h3>Step 2: Create a Dataset</h3>
For this example we’ll use the <b>make_regression()</b> function to create a fake dataset with 5,000 observations and 15 predictor variables:
<b>#create fake regression data
X, y = make_regression(n_samples=5000, n_features=15, n_informative=10,       noise=0.5, random_state=5)
</b>
<h3>Step 3: Build & Optimize the MARS Model</h3>
Next, we’ll use the <b>Earth()</b> function to build a MARS model and the <b>RepeatedKFold()</b> function to perform  k-fold cross-validation  to evaluate the model performance.
For this example we’ll perform 10-fold cross-validation, repeated 3 times.
<b>#define the model
model = Earth()
#specify cross-validation method to use to evaluate model
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
#evaluate model performance
scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error',         cv=cv, n_jobs=-1)
#print results
mean(scores)
-1.745345918289
</b>
From the output we can see that the mean absolute error (ignore the negative sign) for this type of model is <b>1.7453</b>.
In practice we can fit a variety of different models to a given dataset (like Ridge, Lasso, Multiple Linear Regression, Partial Least Squares, Polynomial Regression, etc.) and compare the mean absolute error among all models to determine the one that produces the lowest MAE.
Note that we could also use other metrics to measure error such as adjusted R-squared or mean squared error.
You can find the complete Python code used in this example  here .
<h2><span class="orange">Multivariate Adaptive Regression Splines in R</span></h2>
 Multivariate adaptive regression splines  (MARS) can be used to model nonlinear relationships between a set of predictor variables and a  response variable .
This method works as follows:
<b>1. </b>Divide a dataset into <em>k</em> pieces.
<b>2. </b>Fit a regression model to each piece.
<b>3.</b> Use k-fold cross-validation to choose a value for <em>k</em>.
This tutorial provides a step-by-step example of how to fit a MARS model to a dataset in R.
<h3>Step 1: Load Necessary Packages</h3>
For this example we’ll use the <b>Wage</b> dataset from the <b>ISLR<em> </em></b>package, which contains the annual wages for 3,000 individuals along with a variety of predictor variables like age, education, race, and more.
Before we fit a MARS model to the data, we’ll load the necessary packages:
<b>library(ISLR)      #contains Wage dataset
library(dplyr)     #data wrangling
library(ggplot2)   #plotting
library(earth)     #fitting MARS models
library(caret)     #tuning model parameters
</b>
<h3>Step 2: View Data</h3>
Next, we’ll view the first six rows of the dataset we’re working with:
<b>#view first six rows of data
head(Wage)
       year age           maritl     race       education             region
231655 2006  18 1. Never Married 1. White    1. &lt; HS Grad 2. Middle Atlantic
86582  2004  24 1. Never Married 1. White 4. College Grad 2. Middle Atlantic
161300 2003  45       2. Married 1. White 3. Some College 2. Middle Atlantic
155159 2003  43       2. Married 3. Asian 4. College Grad 2. Middle Atlantic
11443  2005  50      4. Divorced 1. White      2. HS Grad 2. Middle Atlantic
376662 2008  54       2. Married 1. White 4. College Grad 2. Middle Atlantic
             jobclass         health      health_ins  logwage      wage
231655  1. Industrial      1. &lt;=Good      2. No       4.318063     75.04315
86582   2. Information     2. >=Very Good 2. No       4.255273     70.47602
161300  1. Industrial      1. &lt;=Good      1. Yes      4.875061     130.98218
155159  2. Information     2. >=Very Good 1. Yes      5.041393     154.68529
11443   2. Information     1. &lt;=Good      1. Yes      4.318063     75.04315
376662  2. Information     2. >=Very Good 1. Yes      4.845098     127.11574
</b>
<h3>Step 3: Build & Optimize the MARS Model</h3>
Next, we’ll build the MARS model for this dataset and perform  k-fold cross-validation  to determine which model produces the lowest test RMSE (root mean squared error).
<b>#create a tuning grid
hyper_grid &lt;- expand.grid(degree = 1:3,          nprune = seq(2, 50, length.out = 10) %>%          floor())
#make this example reproducible
set.seed(1)
#fit MARS model using k-fold cross-validation
cv_mars &lt;- train(
  x = subset(Wage, select = -c(wage, logwage)),
  y = Wage$wage,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid)
#display model with lowest test RMSE
cv_mars$results %>%
  filter(nprune==cv_mars$bestTune$nprune, degree =cv_mars$bestTune$degree)    
degreenpruneRMSE Rsquared   MAE      RMSESDRsquaredSD   MAESD
11233.8164  0.3431804  22.97108  2.2403940.03064269   1.4554
</b>
From the output we can see that the model that produced the lowest test MSE was one with only first-order effects (i.e. no interaction terms) and 12 terms. This model produced a root mean squared error (RMSE) of <b>33.8164</b>.
<em><b>Note:</b> We used method=”earth” to specify a MARS model. You can find the documentation for this method  here .</em>
We can also create a plot to visualize the test RMSE based on the degree and the number of terms:
<b>#display test RMSE by terms and degree
ggplot(cv_mars)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/mars1.png">
In practice we would fit a MARS model along with several other types of models like:
 Multiple Linear Regression 
 Polynomial Regression 
 Ridge Regression 
 Lasso Regression 
 Principal Components Regression 
 Partial Least Squares 
We would then compare each model to determine which one lead to the lowest test error and choose that model as the optimal one to use.
The complete R code used in this example can be found  here .
<h2><span class="orange">An Introduction to Multivariate Adaptive Regression Splines</span></h2>
When the relationship between a set of predictor variables and a  response variable  is linear, we can often use  linear regression , which assumes that the relationship between a given predictor variable and a response variable takes the form:
Y = β<sub>0</sub> + β<sub>1</sub>X + ε
But in practice the relationship between the variables can actually be nonlinear and attempting to use linear regression can result in a poorly fit model.
One way to account for a nonlinear relationship between the predictor and response variable is to use  polynomial regression , which takes the form:
Y = β<sub>0</sub> + β<sub>1</sub>X + β<sub>2</sub>X<sup>2</sup> + … + β<sub>h</sub>X<sup>h</sup> + ε
In this equation, <em>h</em> is referred to as the “degree” of the polynomial. As we increase the value for <em>h</em>, the model becomes more flexible and is able to fit nonlinear data.
However, polynomial regression has a couple drawbacks:
<b>1.</b> Polynomial regression can easily  overfit  a dataset if the degree<em>, h</em>, is chosen to be too large. In practice, <em>h</em> is rarely larger than 3 or 4 because beyond this point it simply fits the noise of a training set and does not generalize well to unseen data.
<b>2. </b>Polynomial regression imposes a global function on the entire dataset, which is not always accurate.
An alternative to polynomial regression is <b>multivariate adaptive regression splines</b>.
<h3>The Basic Idea</h3>
Multivariate adaptive regression splines work as follows:
<b>1. Divide a dataset into <em>k</em> pieces.</b>
First, we divide a dataset into <em>k</em> different pieces. The points where we divide the dataset are known as <em>knots</em>.
We identify the knots by assessing each point for each predictor as a potential knot and creating a linear regression model using the candidate features. The point that is able to reduce the most error in the model is deemed to be the knot.
Once we’ve identified the first knot, we then repeat the process to find additional knots. You can find as many knots as you think is reasonable to start.
<b>2. Fit a regression function to each piece to form a hinge function.</b>
Once we’ve chosen the knots and fit a regression model to each piece of the dataset, we’re left with something known as a <em>hinge function</em>, denoted as <em>h(x-a)</em>, where <em>a</em> is the cutpoint value(s).
For example, the hinge function for a model with one knot may be as follows:
y = β<sub>0</sub> + β<sub>1</sub>(4.3 – x)  if x &lt; 4.3
y = β<sub>0</sub> + β<sub>1</sub>(x – 4.3)  if x > 4.3
In this case, it was determined that choosing <b>4.3</b> to be the cutpoint value was able to reduce the error the most out of all possible cutpoints values. We then fit a different regression model to the values less than 4.3 compared to values greater than 4.3.
A hinge function with two knots may be as follows:
y = β<sub>0</sub> + β<sub>1</sub>(4.3 – x)  if x &lt; 4.3
y = β<sub>0</sub> + β<sub>1</sub>(x – 4.3)  if x > 4.3 & x &lt; 6.7
y = β<sub>0</sub> + β<sub>1</sub>(6.7 – x)  if x > 6.7
In this case, it was determined that choosing <b>4.3</b> and <b>6.7</b> as the cutpoint values was able to reduce the error the most out of all possible cutpoint values. We then fit one regression model to the values less than 4.3, another regression model to values between 4.3 and 6.7, and another regression model to the values greater than 4.3.
<b>3. Choose <em>k</em> based on k-fold cross-validation.</b>
Lastly, once we’ve fit several different models using a different number of knots for each model, we can perform  k-fold cross-validation  to identify the model that produces the lowest test mean squared error (MSE).
The model with the lowest test MSE is chosen to be the model that generalizes best to new data.
<h3>Pros & Cons</h3>
Multivariate adaptive regression splines come with the following pros and cons:
<b>Pros</b>:
It can be used for both  regression and classification problems .
It works well on large datasets.
It offers quick computation.
It does not require you to standardize the predictor variables.
<b>Cons:</b>
It tends to not perform as well as non-linear methods like random forests and gradient boosting machines.
<h3>How to Fit MARS Models in R & Python</h3>
The following tutorials provide step-by-step examples of how to fit multivariate adaptive regression splines (MARS) in both R and Python:
 Multivariate Adaptive Regression Splines in R 
 Multivariate Adaptive Regression Splines in Python 
<h2><span class="orange">How to Perform Multivariate Normality Tests in Python</span></h2>
When we’d like to test whether or not a single variable is normally distributed, we can create a  Q-Q plot  to visualize the distribution or we can perform a formal statistical test like an  Anderson Darling Test  or a  Jarque-Bera Test .
However, when we’d like to test whether or not <em>several </em>variables are normally distributed as a group we must perform a <b>multivariate normality test</b>.
This tutorial explains how to perform the Henze-Zirkler multivariate normality test for a given dataset in Python.
<b>Related: </b>If we’d like to identify outliers in a multivariate setting, we can use the  Mahalanobis distance .
<h3>Example: Henze-Zirkler Multivariate Normality Test in Python</h3>
The <b>Henze-Zirkler Multivariate Normality Test </b>determines whether or not a group of variables follows a multivariate normal distribution. The null and alternative hypotheses for the test are as follows:
H<sub>0</sub> (null): The variables follow a multivariate normal distribution.
H<sub>a</sub> (alternative): The variables <em>do not </em>follow a multivariate normal distribution.
To perform this test in Python we can use the  multivariate_normality()  function from the pingouin library.
First, we need to install pingouin:
<b>pip install pingouin
</b>
Next, we can import the <b>multivariate_normality() </b>function and use it to perform a Multivariate Test for Normality for a given dataset:
<b>#import necessary packages
from pingouin import multivariate_normality
import pandas as pd
import numpy as np
#create a dataset with three variables x1, x2, and x3
df = pd.DataFrame({'x1':np.random.normal(size=50),   'x2': np.random.normal(size=50),   'x3': np.random.normal(size=50)})
#perform the Henze-Zirkler Multivariate Normality Test
multivariate_normality(df, alpha=.05)
HZResults(hz=0.5956866563391165, pval=0.6461804077893423, normal=True)</b>
The results of the test are as follows:
<b>H-Z Test Statistic: </b>0.59569
<b>p-value: </b>0.64618
Since the p-value of the test is not less than our specified alpha value of .05, we fail to reject the null hypothesis. The dataset can be assumed to follow a multivariate normal distribution.
<em><b>Related:</b> Learn how the Henze-Zirkler test is used in real-life medical applications in  this research paper .</em>
<h2><span class="orange">How to Perform Multivariate Normality Tests in R</span></h2>
When we’d like to test whether or not a single variable is normally distributed, we can create a  Q-Q plot  to visualize the distribution or we can perform a formal statistical test like an  Anderson Darling Test  or a  Jarque-Bera Test .
However, when we’d like to test whether or not <em>several </em>variables are normally distributed as a group we must perform a <b>multivariate normality test</b>.
This tutorial explains how to perform the following multivariate normality tests for a given dataset in R:
Mardia’s Test
Energy Test
Multivariate Kurtosis and Skew Tests
<b>Related: </b>If we’d like to identify outliers in a multivariate setting, we can use the  Mahalanobis distance .
<h3>Example: Mardia’s Test in R</h3>
<b>Mardia’s Test</b> determines whether or not a group of variables follows a multivariate normal distribution. The null and alternative hypotheses for the test are as follows:
H<sub>0</sub> (null): The variables follow a multivariate normal distribution.
H<sub>a</sub> (alternative): The variables <em>do not </em>follow a multivariate normal distribution.
The following code shows how to perform this test in R using the <b>QuantPsyc</b> package:
<b>library(QuantPsyc)
#create dataset
set.seed(0)
data &lt;- data.frame(x1 = rnorm(50),   x2 = rnorm(50),   x3 = rnorm(50))
#perform Multivariate normality test
mult.norm(data)$mult.test
          Beta-hat      kappa     p-val
Skewness  1.630474 13.5872843 0.1926626
Kurtosis 13.895364 -0.7130395 0.4758213
</b>
The <b>mult.norm() </b>function tests for multivariate normality in both the skewness and kurtosis of the dataset. Since both p-values are not less than .05, we fail to reject the null hypothesis of the test. We don’t have evidence to say that the three variables in our dataset do not follow a multivariate distribution.
<h3>Example: Energy Test in R</h3>
An <b>Energy Test</b> is another statistical test that determines whether or not a group of variables follows a multivariate normal distribution. The null and alternative hypotheses for the test are as follows:
H<sub>0</sub> (null): The variables follow a multivariate normal distribution.
H<sub>a</sub> (alternative): The variables <em>do not </em>follow a multivariate normal distribution.
The following code shows how to perform this test in R using the <b>energy </b>package:
<b>library(energy)
#create dataset
set.seed(0)
data &lt;- data.frame(x1 = rnorm(50),   x2 = rnorm(50),   x3 = rnorm(50))
#perform Multivariate normality test
mvnorm.etest(data, R=100)
Energy test of multivariate normality: estimated parameters
data:  x, sample size 50, dimension 3, replicates 100
E-statistic = 0.90923, p-value = 0.31
</b>
The p-value of the test is <b>0.31</b>. Since this is not less than .05, we fail to reject the null hypothesis of the test. We don’t have evidence to say that the three variables in our dataset do not follow a multivariate distribution.
<em><b>Note: </b>The argument R=100 specifies 100 boostrapped replicates to be used when performing the test. For datasets with smaller sample sizes, you may increase this number to produce a more reliable estimate of the test statistic.</em>
<h2><span class="orange">Mutually Inclusive vs. Mutually Exclusive Events</span></h2>
Two events are <b>mutually exclusive</b> if they cannot occur at the same time.
For example, let event A be the event that a dice lands on an even number and let event B be the event that a dice lands on an odd number.
We would define the  sample space  for the events as follows:
A = {2, 4, 6}
B = {1, 3, 5}
Notice that there is no overlap between the two sample spaces. Thus, events A and B are mutually exclusive because they both cannot occur at the same time. The number that a dice lands on can’t be even <em>and</em> odd at the same time.
Conversely, two events are <b>mutually inclusive</b> if they <em>can</em> occur at the same time.
For example, let event C be the event that a dice lands on an even number and let event D be the event that a dice lands on a number greater than 3.
We would define the sample space for the events as follows:
C = {2, 4, 6}
D = {4, 5, 6}
Notice that there is overlap between the two sample spaces. Thus, events C and D are mutually inclusive because they can both occur at the same time. It’s possible for the dice to land on a number that is even <em>and</em> is greater than 3.
<h3>Probabilities of Events</h3>
If two events are <b>mutually exclusive</b>, then the probability that they both occur is zero.
For example, consider the two sample spaces for events A and B from earlier:
A = {2, 4, 6}
B = {1, 3, 5}
Since there is no overlap in the sample spaces, we would say P(A and B) = <b>0</b>.
But if two events are <b>mutually inclusive</b>, then the probability that they both occur will be some number greater than zero.
For example, consider the two sample spaces for events C and D from earlier:
C = {2, 4, 6}
D = {4, 5, 6}
Since there are 6 possible numbers that the dice could land on and two of those numbers (4 and 6) belong to both events C and D, we would calculate P(C and D) as 2/6, or <b>1/3</b>.
<h3>Visualizing Mutually Inclusive & Mutually Exclusive Events</h3>
We often use Venn diagrams to visualize the probabilities associated with events.
If two events are <b>mutually exclusive</b> then they would not overlap at all in a Venn diagram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/mutuallyInc1.png">
Conversely, if two events are <b>mutually inclusive</b> then there would be at least some overlap in the Venn diagram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/mutuallyInc2.png">
<h2><span class="orange">How to Use n() Function in R (With Examples)</span></h2>
You can use the <b>n()</b> function from the  dplyr  package in R to count the number of observations in a group.
Here are three common ways to use this function in practice:
<b>Method 1: Use n() to Count Observations by Group</b>
<b>df %>%
  group_by(group_variable) %>%
  summarise(count = n())
</b>
<b>Method 2: Use n() to Add Column that Shows Observations by Group</b>
<b>df %>%
  group_by(group_variable) %>%
  mutate(count = n())</b>
<b>Method 3: Use n() to Filter Based on Observations by Group</b>
<b>df %>%
  group_by(group_variable) %>%
  filter(n() > 15)</b>
The following examples show how to use each method in practice with the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'C'), points=c(22, 25, 25, 20, 29, 13), assists=c(10, 12, 9, 4, 11, 10), rebounds=c(9, 8, 5, 10, 14, 12))
#view data frame
df
  team points assists rebounds
1    A     22      10        9
2    A     25      12        8
3    A     25       9        5
4    B     20       4       10
5    B     29      11       14
6    C     13      10       12
</b>
<h2>Example 1: Use n() to Count Observations by Group</h2>
The following code shows how to use the <b>n()</b> function along with the <b>summarise()</b> function to count the number of observations by <b>team</b>:
<b>library(dplyr)
#count number of observations by team
df %>%
  group_by(team) %>%
  summarise(count = n())
# A tibble: 3 x 2
  team  count
   
1 A         3
2 B         2
3 C         1</b>
From the output we can see:
Team <b>A</b> occurs 3 times
Team <b>B</b> occurs 2 times
Team <b>C</b> occurs 1 time
<h2>Example 2: Use n() to Add Column that Shows Observations by Group</h2>
The following code shows how to use the <b>n()</b> function along with the <b>mutate()</b> function to add a column to the date frame that contains the number of observations by <b>team</b>:
<b>library(dplyr)
#add new column that shows number of observations by team
df %>%
  group_by(team) %>%
  mutate(count = n())
# A tibble: 6 x 5
# Groups:   team [3]
  team  points assists rebounds count
            
1 A         22      10        9     3
2 A         25      12        8     3
3 A         25       9        5     3
4 B         20       4       10     2
5 B         29      11       14     2
6 C         13      10       12     1</b>
The new column called <b>count</b> contains the team count for each row in the data frame.
<h2>Example 3: Use n() to Filter Based on Observations by Group</h2>
The following code shows how to use the <b>n()</b> function along with the <b>filter()</b> function to filter the data frame to only show rows where the team occurs greater than one time:
<b>library(dplyr)
#filter rows where team count is greater than 1
df %>%
  group_by(team) %>%
  filter(n() > 1)
# A tibble: 5 x 4
# Groups:   team [2]
  team  points assists rebounds
           
1 A         22      10        9
2 A         25      12        8
3 A         25       9        5
4 B         20       4       10
5 B         29      11       14</b>
Notice that the resulting data frame only contains rows where the team is “A” or “B” because these are the only teams that have a count greater than one.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use the across() Function in dplyr 
 How to Use the relocate() Function in dplyr 
 How to Use the slice() Function in dplyr 
<h2><span class="orange">How to Use na.omit in R (With Examples)</span></h2>
You can use the <b>na.omit()</b> function in R to remove any incomplete cases in a vector, matrix, or data frame.
This function uses the following basic syntax:
<b>#omit NA values from vector
x &lt;- na.omit(x)
#omit rows with NA in any column of data frame
df &lt;- na.omit(df)
#omit rows with NA in specific column of data frame
df &lt;- df[!(is.na(df$column)), ]
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Omit NA Values from Vector</h3>
The following code shows how to omit all NA values from a vector:
<b>#define vector
x &lt;- c(1, 24, NA, 6, NA, 9)
#omit NA values from vector
x &lt;- na.omit(x)
x
[1]  1 24  6  9
attr(,"na.action")
[1] 3 5
attr(,"class")
[1] "omit"
</b>
The first line in the output shows the vector without NA values while the next two lines show additional information about the location of the NA values.
We can use the following code to just return the vector without the NA values:
<b>#define vector
x &lt;- c(1, 24, NA, 6, NA, 9)
#omit NA values from vector
x &lt;- as.numeric(na.omit(x))
x
[1]  1 24  6  9
</b>
<h3>Example 2: Omit Rows with NA in Any Column of Data Frame</h3>
The following code shows how to omit all rows with NA values in any column of a data frame :
<b>#define data frame
df &lt;- data.frame(x=c(1, 24, NA, 6, NA, 9), y=c(NA, 3, 4, 8, NA, 12), z=c(NA, 7, 5, 15, 7, 14))
#view data frame
df
   x  y  z
1  1 NA NA
2 24  3  7
3 NA  4  5
4  6  8 15
5 NA NA  7
6  9 12 14
#omit rows with NA value in any column data frame
df &lt;- na.omit(df)
#view data frame 
df
   x  y  z
2 24  3  7
4  6  8 15
6  9 12 14
</b>
<h3>Example 3: Omit Rows with NA in Specific Column of Data Frame</h3>
To omit rows with NA values in a specific column of a data frame, it’s actually easier to use the <b>is.na()</b> function as follows:
<b>#define data frame
df &lt;- data.frame(x=c(1, 24, NA, 6, NA, 9), y=c(NA, 3, 4, 8, NA, 12), z=c(NA, 7, 5, 15, 7, 14))
#view data frame
df
   x  y  z
1  1 NA NA
2 24  3  7
3 NA  4  5
4  6  8 15
5 NA NA  7
6  9 12 14
#remove rows with NA value in <em>x</em> column
df &lt;- df[!(is.na(df$x)), ]
#view data frame 
df
   x  y  z
1  1 NA NA
2 24  3  7
4  6  8 15
6  9 12 14</b>
<h2><span class="orange">How to Use na.rm in R (With Examples)</span></h2>
You can use the argument <b>na.rm = TRUE</b> to exclude missing values when calculating descriptive statistics in R.
<b>#calculate mean and exclude missing values
mean(x, na.rm = TRUE)
#calculate sum and exclude missing values 
sum(x, na.rm = TRUE)
#calculate maximum and exclude missing values 
max(x, na.rm = TRUE)
#calculate standard deviation and exclude missing values 
sd(x, na.rm = TRUE)
</b>
The following examples show how to use this argument in practice with both vectors and data frames.
<h3>Example 1: Use na.rm with Vectors</h3>
Suppose we attempt to calculate the mean, sum, max, and standard deviation for the following vector in R that contains some missing values:
<b>#define vector with some missing values
x &lt;- c(3, 4, 5, 5, 7, NA, 12, NA, 16)
mean(x)
[1] NA
sum(x)
[1] NA
max(x)
[1] NA
sd(x)
[1] NA
</b>
Each of these functions returns a value of <b>NA</b>.
To exclude missing values when performing these calculations, we can simply include the argument <b>na.rm = TRUE</b> as follows:
<b>#define vector with some missing values
x &lt;- c(3, 4, 5, 5, 7, NA, 12, NA, 16)
mean(x, na.rm = TRUE)
[1] 7.428571
sum(x, na.rm = TRUE)
[1] 52
max(x, na.rm = TRUE)
[1] 16
sd(x, na.rm = TRUE)
[1] 4.790864
</b>
Notice that we were able to complete each calculation successfully while excluding the missing values.
<h3>Example 2: Use na.rm with Data Frames</h3>
Suppose we have the following data frame in R that contains some missing values:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, NA, 3, 2), var3=c(3, 3, NA, 6, 8), var4=c(1, 1, 2, 8, NA))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    7    3    1
3    3   NA   NA    2
4    4    3    6    8
5    5    2    8   NA
</b>
We can use the <b>apply()</b> function to calculate descriptive statistics for each column in the data frame and use the <b>na.rm = TRUE</b> argument to exclude missing values when performing these calculations:
<b>#calculate mean of each column
apply(df, 2, mean, na.rm = TRUE)
var1 var2 var3 var4 
3.20 4.75 5.00 3.00 
#calculate sum of each column
apply(df, 2, sum, na.rm = TRUE)
var1 var2 var3 var4 
  16   19   20   12 
#calculate max of each column
apply(df, 2, max, na.rm = TRUE)
var1 var2 var3 var4 
   5    7    8    8 
#calculate standard deviation of each column
apply(df, 2, sd, na.rm = TRUE)
    var1     var2     var3     var4 
1.483240 2.629956 2.449490 3.366502</b>
Once again, we were able to complete each calculation successfully while excluding the missing values.
<h2><span class="orange">Naive Forecasting in Excel: Step-by-Step Example</span></h2>
A <b>naive forecast</b> is one in which the forecast for a given period is simply equal to the value observed in the previous period.
For example, suppose we have the following sales of a given product during the first three months of the year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast1.png">
The forecast for sales in April would simply be equal to the actual sales from the previous month of March:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast2.png">
Although this method is simple, it tends to work surprisingly well in practice.
This tutorial provides a step-by-step example of how to perform naive forecasting in Excel.
<h3>Step 1: Enter the Data</h3>
First, we’ll enter the sales data for a 12-month period at some imaginary company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast3.png">
<h3>Step 2: Create the Forecasts</h3>
Next, we’ll use the following formulas to create naive forecasts for each month:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast4.png">
<h3>Step 3: Measure the Accuracy of the Forecasts</h3>
Lastly, we need to measure the accuracy of the forecasts. Two common metrics used to measure accuracy include:
Mean absolute percentage error
Mean Absolute Deviation
The following image shows how to calculate mean absolute percentage error:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast5.png">
The mean absolute percentage error turns out to be <b>9.9%</b>.
And the following image shows how to calculate mean absolute deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast6.png">
The mean absolute deviation turns out to be <b>3.45</b>.
To know if this forecast is useful, we can compare it to other forecasting models and see if the accuracy measurements are better or worse.
<h2><span class="orange">How to Perform Naive Forecasting in R (With Examples)</span></h2>
A <b>naive forecast</b> is one in which the forecast for a given period is simply equal to the value observed in the previous period.
For example, suppose we have the following sales of a given product during the first three months of the year:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast1.png">
The forecast for sales in April would simply be equal to the actual sales from the previous month of March:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/naive_forecast2.png">
Although this method is simple, it tends to work surprisingly well in practice.
This tutorial provides a step-by-step example of how to perform naive forecasting in R.
<h3>Step 1: Enter the Data</h3>
First, we’ll enter the sales data for a 12-month period at some imaginary company:
<b>#create vector to hold actual sales data
actual &lt;- c(34, 37, 44, 47, 48, 48, 46, 43, 32, 27, 26, 24)
</b>
<h3>Step 2: Generate the Naive Forecasts</h3>
Next, we’ll use the following formulas to create naive forecasts for each month:
<b>#generate naive forecasts
forecast &lt;- c(NA, actual[-length(actual)])
#view naive forecasts
forecast
[1] NA 34 37 44 47 48 48 46 43 32 27 26
</b>
Note that we simply used <b>NA</b> for the first forecasted value.
<h3>Step 3: Measure the Accuracy of the Forecasts</h3>
Lastly, we need to measure the accuracy of the forecasts. Two common metrics used to measure accuracy include:
Mean absolute percentage error (MAPE)
Mean absolute error (MAE)
We can use the following code to calculate both metrics:
<b>#calculate MAPE
mean(abs((actual-forecast)/actual), na.rm=T) * 100
[1] 9.898281
#calculate MAE
mean(abs(actual-forecast), na.rm=T)
[1] 3.454545
</b>
The mean absolute percentage error is <b>9.898%</b> and the mean absolute error is <b>3.45</b>
To know if this forecast is useful, we can compare it to other forecasting models and see if the accuracy measurements are better or worse.
<h3>Step 4: Visualize the Forecasts</h3>
Lastly, we can create a simple line plot to visualize the differences between the actual sales and the naive forecasts for the sales during each period:
<b>#plot actual sales
plot(actual, type='l', col = 'red', main='Actual vs. Forecasted Sales',
     xlab='Sales Period', ylab='Sales')
#add line for forecasted sales
lines(forecast, type='l', col = 'blue')
#add legend
legend('topright', legend=c('Actual', 'Forecasted'),
       col=c('red', 'blue'), lty=1)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/naiveR1.png">
Notice that the forecasted sales line is basically a lagged version of the actual sales line.
This is exactly what we would expect the plot to look like since the naive forecast simply forecasts the sales in the current period to be equal to the sales in the previous period.
<h2><span class="orange">How to Fix: NameError name ‘np’ is not defined</span></h2>
One of the most common errors you may encounter when using Python is:
<b>NameError: name 'np' is not defined</b>
This error occurs when you import the python library  NumPy , but fail to give it the alias of <b>np</b> when importing it.
The following examples illustrate how this problem occurs and how to fix it.
<h3>Example 1: import numpy</h3>
Suppose you import the NumPy library using the following code:
<b>import numpy</b>
If you then attempt to define a numpy array of values, you’ll get the following error:
<b>#define numpy array
x = np.random.normal(loc=0, scale=1, size=20)
#attempt to print values in arrary
print(x)
Traceback (most recent call last): 
----> 1 x = np.random.normal(loc=0, scale=1, size=20)
      2 print(x)
NameError: name 'np' is not defined
</b>
To fix this error, you need provide the alias of <b>np</b> when importing NumPy:
<b>import numpy as np
#define numpy array
x = np.random.normal(loc=0, scale=1, size=20)
#print values in arrary
print(x)
[-0.93937656 -0.49448118 -0.16772964  0.44939978 -0.80577905  0.48042484
  0.30175551 -0.15672656 -0.26931062  0.38226115  1.4472055  -0.13668984
 -0.74752684  1.6729974   2.25824518  0.77424489  0.67853607  1.46739364
  0.14647622  0.87787596]
</b>
<h3>Example 2: from numpy import *</h3>
Suppose you import all functions from the NumPy library using the following code:
<b>from numpy import *</b>
If you then attempt to define a numpy array of values, you’ll get the following error:
<b>#define numpy array
x = np.random.normal(loc=0, scale=1, size=20)
#attempt to print values in arrary
print(x)
Traceback (most recent call last): 
----> 1 x = np.random.normal(loc=0, scale=1, size=20)
      2 print(x)
NameError: name 'np' is not defined
</b>
To fix this error, you need provide the alias of <b>np</b> when importing NumPy:
<b>import numpy as np
#define numpy array
x = np.random.normal(loc=0, scale=1, size=20)
#print values in arrary
print(x)
[-0.93937656 -0.49448118 -0.16772964  0.44939978 -0.80577905  0.48042484
  0.30175551 -0.15672656 -0.26931062  0.38226115  1.4472055  -0.13668984
 -0.74752684  1.6729974   2.25824518  0.77424489  0.67853607  1.46739364
  0.14647622  0.87787596]</b>
Alternatively, you can choose to not use the <b>np</b> syntax at all:
<b>import numpy
#define numpy array
x = numpy.random.normal(loc=0, scale=1, size=20)
#print values in arrary
print(x)
[-0.93937656 -0.49448118 -0.16772964  0.44939978 -0.80577905  0.48042484
  0.30175551 -0.15672656 -0.26931062  0.38226115  1.4472055  -0.13668984
 -0.74752684  1.6729974   2.25824518  0.77424489  0.67853607  1.46739364
  0.14647622  0.87787596]</b>
<b>Note:</b> The syntax “import numpy as np” is commonly used because it offers a more concise way to use NumPy functions. Instead of typing “numpy” each time, you can simply type in “np” which is quicker and easier to read.
<h2><span class="orange">How to Fix: NameError name ‘pd’ is not defined</span></h2>
One common error you may encounter when using Python is:
<b>NameError: name 'pd' is not defined</b>
This error usually occurs when you import the python library  pandas , but fail to give it the alias of <b>pd</b> when importing it.
The following examples illustrate how this error occurs in practice and how you can quickly fix it.
<h3>Example 1: Use import pandas as pd</h3>
Suppose you import the pandas library using the following code:
<b>import pandas</b>
If you then attempt to create a pandas DataFrame, you’ll get the following error:
<b>#create pandas DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#attempt to print DataFrame
print(df)
Traceback (most recent call last):
      1 import pandas
----> 2 df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],
      3                    'assists': [5, 7, 7, 9, 12, 9, 9, 4],
      4                    'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
      5 
NameError: name 'pd' is not defined
</b>
To fix this error, you need provide the alias of <b>pd</b> when importing pandas
<b>import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#print DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
</b>
<h3>Example 2: Use import pandas</h3>
Suppose you import the pandas library using the following code:
<b>import pandas</b>
If you then attempt to create a pandas DataFrame, you’ll get the following error:
<b>#create pandas DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#attempt to print DataFrame
print(df)
Traceback (most recent call last):
      1 import pandas
----> 2 df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],
      3                    'assists': [5, 7, 7, 9, 12, 9, 9, 4],
      4                    'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
      5 
NameError: name 'pd' is not defined
</b>
To fix this error, you could simply choose not to use the alias of <b>pd</b> at all:
<b>import pandas
#create pandas DataFrame
df = pandas.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#print DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12</b>
<b>Note:</b> The syntax “import pandas as pd” is commonly used because it offers a more concise way to use pandas functions. Instead of typing “pandas” each time, you can simply type in “pd” which is quicker and easier to read.
<h2><span class="orange">How to Use the names Function in R (3 Examples)</span></h2>
You can use the <b>names()</b> function to set the names of an object or get the names of an object in R.
This function uses the following syntax:
<b>#get names of object
names(x)
#set names of object
names(x) &lt;- c('value1', 'value2', 'value3', ...)
</b>
The following examples show how to use the <b>names()</b> function with different objects.
<h3>Example 1: Use names() Function with Vector</h3>
We can use the <b>names()</b> function to set the names for a vector:
<b>#create vector
my_vector &lt;- c(5, 10, 15, 20, 25)
#view vector
my_vector
[1]  5 10 15 20 25
#set names for vector
names(my_vector) &lt;- c('A', 'B', 'C', 'D', 'E')
#view updated vector
my_vector
 A  B  C  D  E 
 5 10 15 20 25 </b>
We can then use brackets to access the values in a vector based off the name:
<b>#access value in vector that corresponds to 'B' name
my_vector['B']
 B 
10
</b>
<h3>Example 2: Use names() Function with List</h3>
We can use the <b>names()</b> function to set the names for a list:
<b>#create list
my_list &lt;- list(c(1, 2, 3), 'hello', 10)
#view list
my_list
[[1]]
[1] 1 2 3
[[2]]
[1] "hello"
[[3]]
[1] 10
#set names for list
names(my_list) &lt;- c('A', 'B', 'C')
#view updated list
my_list
$A
[1] 1 2 3
$B
[1] "hello"
$C
[1] 10
</b>
We can then use brackets to access the values in a list based off the name:
<b>#access value in list that corresponds to 'C' name
my_list['C']
$C
[1] 10
</b>
<h3>Example 3: Use names() Function with Data Frame</h3>
We can use the <b>names()</b> function to set the names for the columns of a data frame:
<b>#create data frame
df &lt;- data.frame(A=c('A', 'B', 'C', 'D', 'E'), B=c(99, 90, 86, 88, 95), C=c(33, 28, 31, 39, 34), D=c(30, 28, 24, 24, 28))
#get names of data frame
names(df)
[1] "A" "B" "C" "D"
#set names of data frame
names(df) &lt;- c('team', 'points', 'assists', 'rebounds')
#view updated names of data  frame
names(df)
[1] "team"     "points"   "assists"  "rebounds"
</b>
<h2><span class="orange">How to Handle NaN Values in R (With Examples)</span></h2>
In R, <b>NaN</b> stands for Not a Number.
Typically NaN values occur when you attempt to perform some calculation that results in an invalid result.
For example, dividing by zero or calculating the log of a negative number both produce NaN values:
<b>#attempt to divide by zero
0 / 0
[1] NaN
#attempt to calculate log of negative value
log(-12)
[1] NaN
</b>
Note that NaN values are different from <b>NA</b> values, which simply represent missing values.
You can use the following methods to handle NaN values in R:
<b>#identify positions in vector with NaN values
which(is.nan(x))
#count total NaN values in vector
sum(is.nan(x)) 
#remove NaN values in vector
x_new &lt;- x[!is.nan(x)]
#replace NaN values in vector
x[is.nan(x)] &lt;- 0 
</b>
The following examples show how to use each of these methods in practice.
<h3>Example 1: Identify Positions in Vector with NaN Values</h3>
The following code shows how to identify the positions in a vector that contain NaN values:
<b>#create vector with some NaN values
x &lt;- c(1, NaN, 12, NaN, 50, 30)
#identify positions with NaN values
which(is.nan(x))
[1] 2 4</b>
From the output we can see that the elements in positions <b>2</b> and <b>4</b> in the vector are NaN values.
<h3>Example 2: Count Total NaN Values in Vector</h3>
The following code shows how to count the total number of NaN values in a vector in R:
<b>#create vector with some NaN values
x &lt;- c(1, NaN, 12, NaN, 50, 30)
#identify positions with NaN values
sum(is.nan(x))
[1] 2</b>
From the output we can see that there are <b>2</b> total NaN values in the vector.
<h3>Example 3: Remove NaN Values in Vector</h3>
The following code shows how to create a new vector that has the NaN values removed from the original vector:
<b>#create vector with some NaN values
x &lt;- c(1, NaN, 12, NaN, 50, 30)
#define new vector with NaN values removed
x_new &lt;- x[!is.nan(x)]
#view new vector
x_new
[1]  1 12 50 30
</b>
Notice that both NaN values have been removed from the vector.
<h3>Example 4: Replace NaN Values in Vector</h3>
The following code shows how to replace NaN values in a vector with zeros:
<b>#create vector with some NaN values
x &lt;- c(1, NaN, 12, NaN, 50, 30)
#replace NaN values with zero
x[is.nan(x)] &lt;- 0
#view updated vector
x
[1]  1  0 12  0 50 30
</b>
Notice that both NaN values have been replaced by zeros in the vector.
<h2><span class="orange">How to Fix in R: NAs Introduced by Coercion</span></h2>
One common warning message you may encounter in R is:
<b>Warning message:
NAs introduced by coercion 
</b>
This warning message occurs when you use <b>as.numeric()</b> to convert a vector in R to a numeric vector and there happen to be non-numerical values in the original vector.
To be clear, <b>you don’t need to do anything to “fix” this warning message.</b> R is simply alerting you to the fact that some values in the original vector were converted to NAs because they couldn’t be converted to numeric values.
However, this tutorial shares the exact steps you can use if you don’t want to see this warning message displayed at all.
<h3>How to Reproduce the Warning Message</h3>
The following code converts a character vector to a numeric vector:
<b>#define character vector
x &lt;- c('1', '2', '3', NA, '4', 'Hey')
#convert to numeric vector
x_num &lt;- as.numeric(x)
#display numeric vector
x_num
Warning message:
NAs introduced by coercion 
[1]  1  2  3 NA  4 NA</b>
R converts the character vector to a numeric vector, but displays the warning message <b>NAs introduced by coercion</b> since two values in the original vector could not be converted to numeric values.
<h3>Method #1: Suppress Warnings</h3>
One way to deal with this warning message is to simply suppress it by using the <b>suppressWarnings()</b> function when converting the character vector to a numeric vector:
<b>#define character vector
x &lt;- c('1', '2', '3', NA, '4', 'Hey')
#convert to numeric vector, suppressing warnings
suppressWarnings(x_num &lt;- as.numeric(x))
#display numeric vector
x_num
[1]  1  2  3 NA  4 NA</b>
R successfully converts the character vector to a numeric vector without displaying any warning messages.
<h3>Method #2: Replace Non-Numeric Values</h3>
One way to avoid the warning message in the first place is by replacing non-numeric values in the original vector with blanks by using the <b>gsub()</b> function:
<b>#define character vector
x &lt;- c('1', '2', '3', '4', 'Hey')
#replace non-numeric values with 0
x &lt;- gsub("Hey", "0", x)
#convert to numeric vector
x_num &lt;- as.numeric(x)
#display numeric vector
x_num
[1]  1  2  3 4 0</b>
R successfully converts the character vector to a numeric vector without displaying any warning messages.
<h2><span class="orange">How to Interpret Negative AIC Values</span></h2>
The Akaike information criterion (AIC) is a metric that is used to compare the fit of different regression models.
It is calculated as:
AIC = 2K – 2<em>ln</em>(L)
where:
<b>K:</b> The number of model parameters.
<b><em>ln</em>(L)</b>: The log-likelihood of the model. This tells us how likely the model is, given the data.
Once you’ve fit several regression models, you can compare the AIC value of each model. The model with the lowest AIC offers the best fit.
One question students often have about AIC is: <em><b>How do I interpret negative AIC values?</b></em>
The simple answer: <b>The lower the value for AIC, the better the fit of the model. The absolute value of the AIC value is not important. It can be positive or negative.</b>
For example, if Model 1 has an AIC value of -56.5 and Model 2 has an AIC value of -103.3, then Model 2 offers a better fit. It doesn’t matter if both AIC values are negative.
<h3>Understanding Negative AIC Values</h3>
It’s easy to see how a given regression model could result in a negative AIC value if we simply look at the formula use to calculate AIC:
AIC = 2K – 2<em>ln</em>(L)
Suppose we have a model with 7 parameters and a log-likelihood of 70.
We would calculate the AIC of this model as:
AIC = 2*7 – 2*70 = <b>-126</b>
We could then compare this AIC value to that of other regression models to determine which model provides the best fit.
<h3>Textbook References on Negative AIC Values</h3>
A helpful textbook reference on negative AIC values comes from  <em>Model Selection and Multimodal Inference: A Practical Information-Theoretic Approach</em>  on page 62:
Usually, AIC is positive; however, it can be shifted by any additive constant, and some shifts can result in negative values of AIC… It is not the absolute size of the AIC value, it is the relative values over the set of models considered, and particularly the differences between AIC values, that are important.
Another useful reference comes from  <em>Serious Stats: A Guide to Advanced Statistics for the Behavioral Sciences</em>  on page 402:
As with likelihood, the absolute value of AIC is largely meaningless (being determined by the arbitrary constant). As this constant depends on the data, AIC can be used to compare models fitted on identical samples.
 
The best model from the set of plausible models being considered is therefore the one with the smallest AIC value (the least information loss relative to the true model).
As noted in both textbooks, the absolute value of the AIC is not important. We merely use AIC values to compare the fit of models and the model with the lowest AIC value is best.
<h2><span class="orange">Negative Binomial Distribution Calculator</span></h2>
This calculator uses the negative binomial distribution to find the probability of experiencing <b>k</b> failures before experiencing <b>r</b> successes when the probability of success on each trial is <b>p</b>.
<label><b>k</b> (number of failures)</label>
<input type="number" id="k" value="9">
<label><b>r</b> (number of successes)</label>
<input type="number" id="r" value="4">
<label><b>p</b> (probability of success on a given trial)</label>
<input type="number" id="p" value="0.3">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
P(X = 9): 0.07191
<script>
function calc() {
//get input values
var p = +document.getElementById('p').value;
var k = +document.getElementById('k').value;
var r = +document.getElementById('r').value;
//calculate SE
var exactP = jStat.negbin.pdf(k, r, p);
//output probabilities
document.getElementById('exactP').innerHTML = exactP.toFixed(5);
document.getElementById('x1').innerHTML = k;
}
</script>
<h2><span class="orange">An Introduction to the Negative Binomial Distribution</span></h2>
The <b>negative binomial distribution</b> describes the probability of experiencing a certain amount of failures before experiencing a certain amount of successes in a series of Bernoulli trials.
A <b>Bernoulli trial</b> is an experiment with only two possible outcomes – “success” or “failure” – and the probability of success is the same each time the experiment is conducted.
 
An example of a Bernoulli trial is a coin flip. The coin can only land on two sides (we could call heads a “success” and tails a “failure”) and the probability of success on each flip is 0.5, assuming the coin is fair.
If a  random variable  <em>X</em> follows a negative binomial distribution, then the probability of experiencing <em>k </em>failures before experiencing a total of <i>r</i> successes can be found by the following formula:
<b>P(X=k) = <sub>k+r-1</sub>C<sub>k</sub> * (1-p)<sup>r</sup> *p<sup>k</sup></b>
where:
<b>k: </b>number of failures
<b>r: </b>number of successes
<b>p: </b>probability of success on a given trial
<b><sub>k+r-1</sub>C<sub>k</sub>: </b>number of combinations of (k+r-1) things taken k at a time
For example, suppose we flip a coin and define a “successful” event as landing on heads. What is the probability of experiencing 6 failures before experiencing a total of 4 successes?
To answer this, we can use the negative binomial distribution with the following parameters:
<b>k: </b>number of failures = 6
<b>r: </b>number of successes = 4
<b>p: </b>probability of success on a given trial = 0.5
Plugging these numbers in the formula, we find the probability to be:
<b>P(X=6 failures) </b>= <sub>6+4-1</sub>C<sub>6</sub> * (1-.5)<sup>4</sup> *(.5)<sup>6</sup> = (84)*(.0625)*(.015625) = <b>0.08203</b>.
<h3>Properties of the Negative Binomial Distribution</h3>
The negative binomial distribution has the following properties:
The mean number of failures we expect before achieving <em>r </em>successes is <b>pr / (1-p)</b>.
The variance in the number of failures we expect before achieving <em>r </em>successes is <b>pr / (1-p)<sup>2</sup></b>.
For example, suppose we flip a coin and define a “successful” event as landing on heads. 
The mean number of failures (e.g. landing on tails) we expect before achieving 4 successes would be <b>pr/(1-p) </b> = (.5*4) / (1-.5) = <b>4</b>.
The variance in the number of failures we expect before achieving 4 successes would be <b>pr / (1-p)<sup>2</sup> </b>= (.5*4) / (1-.5)<sup>2</sup> = <b>8</b>.
<h3>Negative Binomial Distribution Practice Problems</h3>
Use the following practice problems to test your knowledge of the negative binomial distribution.
<em><b>Note: </b>We will use the  Negative Binomial Distribution Calculator  to calculate the answers to these questions.</em>
<b>Problem 1</b>
<b>Question: </b>Suppose we flip a coin and define a “successful” event as landing on heads. What is the probability of experiencing 3 failures before experiencing a total of 4 successes?
<b>Answer:</b> Using the Negative Binomial Distribution Calculator with k = 3 failures, r = 4 successes, and p = 0.5, we find that P(X=3) = <b>0.15625</b>.
<b>Problem 2</b>
<b>Question: </b>Suppose we go door to door selling candy. We consider it a “success” if someone buys a candy bar. The probability that any given person will buy a candy bar is 0.4. What is the probability of experiencing 8 failures before we experience a total of 5 successes?
<b>Answer:</b> Using the Negative Binomial Distribution Calculator with k = 8 failures, r = 5 successes, and p = 0.4, we find that P(X=8) = <b>0.08514</b>.
<b>Problem 3</b>
<b>Question: </b>Suppose we roll a die and define a “successful” roll as landing on the number 5. The probability that the die lands on a 5 on any given roll is 1/6 = 0.167. What is the probability of experiencing 4 failures before we experience a total of 3 successes?
<b>Answer:</b> Using the Negative Binomial Distribution Calculator with k = 4 failures, r = 3 successes, and p = 0.167, we find that P(X=4) = <b>0.03364</b>.
<h2><span class="orange">Negative Binomial vs. Poisson: How to Choose a Regression Model</span></h2>
<b>Negative binomial regression</b> and <b>Poisson regression</b> are two types of regression models that are appropriate to use when the  response variable  is represented by discrete count outcomes.
Here are a few examples of response variables that represent discrete count outcomes:
The number of students who graduate from a certain program
The number of traffic accidents at a certain intersection
The number of participants who finish a marathon
The number of returns in a given month at a retail store
If the variance is roughly equal to the mean, then a Poisson regression model typically fits a dataset well.
However, if the variance is significantly greater than the mean, then a negative binomial regression model is typically able to fit the data better.
There are two techniques we can use to determine if Poisson regression or negative binomial regression is more appropriate to use for a given dataset:
<b>1. Residual Plots</b>
We can create a residual plot of the standardized residuals vs. predicted values from a regression model.
If the majority of the standardized residuals fall within the range of -2 and 2 then a Poisson regression model is likely appropriate.
However, if many residuals fall outside of this range then a negative binomial regression model will likely provide a better fit.
<b>2. Likelihood Ratio Test</b>
We can fit a Poisson regression model and a negative binomial regression model to the same dataset and then perform a Likelihood Ratio Test.
If the p-value of the test is less than some significance level (e.g. 0.05) then we can conclude that the negative binomial regression model offers a significantly better fit.
The following example shows how to use both of these techniques in R to determine whether a Poisson regression or negative binomial regression model is better to use for a given dataset.
<h3>Example: Negative Binomial vs. Poisson Regression</h3>
Suppose we want to know how many scholarship offers a high school baseball player in a given county receives based on their school division (“A”, “B”, or “C”) and their college entrance exam score (measured from 0 to 100).
Use the following steps to determine if a negative binomial regression model or Poisson regression model offers a better fit to the data.
<b>Step 1: Create the Data</b>
The following code creates the dataset we will work with, which includes data on 1,000 baseball players:
<b>#make this example reproducible
set.seed(1)
#create dataset
data &lt;- data.frame(offers = c(rep(0, 700), rep(1, 100), rep(2, 100),              rep(3, 70), rep(4, 30)),   division = sample(c('A', 'B', 'C'), 100, replace = TRUE),   exam = c(runif(700, 60, 90), runif(100, 65, 95),            runif(200, 75, 95)))
#view first six rows of dataset
head(data)
  offers division     exam
1      0        A 66.22635
2      0        C 66.85974
3      0        A 77.87136
4      0        B 77.24617
5      0        A 62.31193
6      0        C 61.06622</b>
<b>Step 2: Fit a Poisson Regression Model & Negative Binomial Regression Model</b>
The following code shows how to fit both a Poisson regression model and negative binomial regression model to the data:
<b>#fit Poisson regression model
p_model &lt;- glm(offers ~ division + exam, family = 'poisson', data = data)
#fit negative binomial regression model
library(MASS)
nb_model &lt;- glm.nb(offers ~ division + exam, data = data)
</b>
<b>Step 3: Create Residual Plots</b>
The following code shows how to produce residual plots for both models.
<b>#Residual plot for Poisson regression
p_res &lt;- resid(p_model)
plot(fitted(p_model), p_res, col='steelblue', pch=16,
     xlab='Predicted Offers', ylab='Standardized Residuals', main='Poisson')
abline(0,0)
#Residual plot for negative binomial regression 
nb_res &lt;- resid(nb_model)
plot(fitted(nb_model), nb_res, col='steelblue', pch=16,
     xlab='Predicted Offers', ylab='Standardized Residuals', main='Negative Binomial')
abline(0,0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/negativeBinom1.png">
From the plots we can see that the residuals are more spread out for the Poisson regression model (notice that some residuals extend beyond 3) compared to the negative binomial regression model.
This is a sign that a negative binomial regression model is likely more appropriate since the residuals of that model are smaller. 
<b>Step 4: Perform a Likelihood Ratio Test</b>
Lastly, we can perform a likelihood ratio test to determine if there is a statistically significant difference in the fit of the two regression models:
<b>pchisq(2 * (logLik(nb_model) - logLik(p_model)), df = 1, lower.tail = FALSE)
'log Lik.' 3.508072e-29 (df=5)
</b>
The p-value of the test turns out to be <b>3.508072e-29</b>, which is significantly less than 0.05.
Thus, we would conclude that the negative binomial regression model offers a significantly better fit to the data compared to the Poisson regression model.
<h2><span class="orange">5 Examples of Negatively Skewed Distributions</span></h2>
<b>Skewness</b> is a way to describe the  symmetry  of a distribution.
A distribution is <b>negatively skewed</b> if it has a “tail” on the left side of the distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew2.png">
<b>Note:</b> Sometimes negatively skewed distributions are also called “left skewed” distributions.
In this article we share 5 examples of negatively skewed distributions in the real world.
<h3>Example 1: Distribution of Age of Deaths</h3>
The distribution of the age of deaths in most populations is negatively skewed. Most people live to be between 70 and 80 years old, with fewer and fewer living less than this age.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/leftSkew1.png">
<h3>Example 2: Distribution of Olympic Long Jumps</h3>
In most years, the distribution of long jump lengths for competitors in the Olympics is negatively skewed because most competitors land a jump around 7.5-8 meters, with a few landing a jump of just 5-6 meters.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/leftSkew2.png">
<h3>Example 3: Distribution of Scores on Easy Exams</h3>
The distribution of scores on easy exams or tests tend to be negatively skewed because most students score very high, while a few students score much lower than the average.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/leftSkew3.png">
<h3>Example 4: Distribution of Daily Stock Market Returns</h3>
The distribution of daily stock market returns is negatively skewed because the stock market delivers a slightly positive return on most days, but occasionally returns huge negative returns on a few days.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/leftSkew4.png">
<h3>Example 5: Distribution of GPA Values</h3>
The distribution of GPA among college students is negatively skewed since most students have a GPA between 2 and 4 with a few having a GPA much lower.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/leftSkew5.png">
<h2><span class="orange">How to Perform the Nemenyi Post-Hoc Test in Python</span></h2>
The  Friedman Test  is a non-parametric alternative to the  Repeated Measures ANOVA . It is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
If the p-value of the Friedman test is statistically significant, we can then perform the <b>Nemenyi post-hoc test</b> to determine exactly which groups are different.
The following step-by-step example shows how to perform the Nemenyi test in Python.
<h3>Step 1: Create the Data</h3>
Suppose a researcher wants to know if the reaction times of patients is equal on three different drugs. To test this, he measures the reaction time (in seconds) of 10 different patients on each of the three drugs.
We can create the following three arrays that contain the response times for each patient on each of the three drugs:
<b>group1 = [4, 6, 3, 4, 3, 2, 2, 7, 6, 5]
group2 = [5, 6, 8, 7, 7, 8, 4, 6, 4, 5]
group3 = [2, 2, 5, 3, 2, 2, 1, 4, 3, 2]</b>
<h3>Step 2: Perform the Friedman Test</h3>
Next, we’ll perform the Friedman Test using the  friedmanchisquare() function  from the scipy.stats library:
<b>from scipy import stats
#perform Friedman Test
stats.friedmanchisquare(group1, group2, group3)
FriedmanchisquareResult(statistic=13.3513513, pvalue=0.00126122012)</b>
The Friedman Test uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> The mean for each population is equal.
<b>The alternative hypothesis: (Ha):</b> At least one population mean is different from the rest.
In this example, the test statistic is <b>13.35135 </b>and the corresponding p-value is <b>0.00126</b>. Since this p-value is less than 0.05, we can reject the null hypothesis that the mean response time is the same for all three drugs.
In other words, we have sufficient evidence to conclude that the type of drug used leads to statistically significant differences in response time.
<h3>Step 3: Perform the Nemenyi Test</h3>
Next, we can perform the Nemenyi post-hoc test to determine exactly which groups have different means.
To do so, we need to install the scikit-posthocs library:
<b>pip install scikit-posthocs
</b>
Next, we’ll use the <b>posthoc_nemenyi_friedman()</b> function to perform the Nemenyi post-hoc test:
<b>import scikit_posthocs as sp
import numpy as np
#combine three groups into one array
data = np.array([group1, group2, group3])
#perform Nemenyi post-hoc test
sp.posthoc_nemenyi_friedman(data.T)
0        1        2
01.0000000.4374070.065303
10.4374071.0000000.001533
20.0653030.0015331.000000
</b>
<b>Note:</b> We had to transpose the numpy array (data.T) in order to perform the post-hoc test correctly.
The Nemeyi post-hoc test returns the p-values for each pairwise comparison of means. From the output we can see the following p-values:
P-value of group 0 vs. group 1: <b>0.4374</b>
P-value of group 0 vs. group 2: <b>0.0653</b>
P-value of group 1 vs. group 2: <b>0.0015</b>
At α = .05, the only two groups that have statistically significantly different means are group 1 and group 2.
<em><b>Note:</b> The Nemenyi test converted the group number from 1, 2, 3 into 0, 1, 2. Thus, the groups from the original data that are significantly different are groups 2 and 3.</em>
<h2><span class="orange">How to Perform a Nested ANOVA in Excel (Step-by-Step)</span></h2>
A  nested ANOVA  is a type of ANOVA (“analysis of variance”) in which at least one factor is <em>nested</em> inside another factor.
For example, suppose a researcher wants to know if three different fertilizers produce different levels of plant growth.
To test this, he has three different technicians sprinkle fertilizer A on four plants each, another three technicians sprinkle fertilizer B on four plants each, and another three technicians sprinkle fertilizer C on four plants each.
In this scenario, the  response variable  is plant growth and the two factors are technician and fertilizer. It turns out that technician is <em>nested</em> within fertilizer:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nested1-1.png">
The following step-by-step example shows how to perform this nested ANOVA in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the data in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nestedExcel1.png">
<h3>Step 2: Fit the Nested ANOVA</h3>
There is no built-in Nested ANOVA function in Excel, but we can use the <b>Anova: Two-Factor With Replication</b> option from the <b>Data Analysis ToolPak</b> to perform a nested ANOVA with some tweaks.
To do so, click the <b>Data</b> tab along the top ribbon. Then click the <b>Data Analysis</b> button within the <b>Analyze</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option, you need to first  load the Data Analysis ToolPak in Excel .
In the window that appears, click <b>Anova: Two-Factor With Replication</b> and then click <b>OK</b>. In the new window that appears, enter the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nestedExcel2-1.png">
Once you click <b>OK</b>, the following output will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nestedExcel3.png">
<h3>Step 3: Interpret the Output</h3>
The ANOVA table shown at the bottom of the output is the one table that we will focus on.
The row labeled <b>Sample</b> shows the results for fertilizer. The p-value in this row (4.27031E-10) is less than .05, so we can conclude that fertilizer is statistically significant.
To determine if the nested factor “technician” is statistically significant, we must perform the following manual calculations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nestedExcel4.png">
The p-value turns out to be <b>0.211</b>. Since this is not less than .05, we conclude that technician is not a statistically significant predictor of plant growth.
These results tell us that if we’d like to increase plant growth, we should focus on the fertilizer being used rather than the individual technician who is sprinkling the fertilizer.
<h2><span class="orange">How to Perform a Nested ANOVA in R (Step-by-Step)</span></h2>
A  nested ANOVA  is a type of ANOVA (“analysis of variance”) in which at least one factor is <em>nested</em> inside another factor.
For example, suppose a researcher wants to know if three different fertilizers produce different levels of plant growth.
To test this, he has three different technicians sprinkle fertilizer A on four plants each, another three technicians sprinkle fertilizer B on four plants each, and another three technicians sprinkle fertilizer C on four plants each.
In this scenario, the  response variable  is plant growth and the two factors are technician and fertilizer. It turns out that technician is <em>nested</em> within fertilizer:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nested1-1.png">
The following step-by-step example shows how to perform this nested ANOVA in R.
<h3>Step 1: Create the Data</h3>
First, let’s create a data frame to hold our data in R:
<b>#create data
df &lt;- data.frame(growth=c(13, 16, 16, 12, 15, 16, 19, 16, 15, 15, 12, 15,          19, 19, 20, 22, 23, 18, 16, 18, 19, 20, 21, 21,          21, 23, 24, 22, 25, 20, 20, 22, 24, 22, 25, 26), fertilizer=c(rep(c('A', 'B', 'C'), each=12)), tech=c(rep(1:9, each=4)))
#view first six rows of data
head(df)
  growth fertilizer tech
1     13          A    1
2     16          A    1
3     16          A    1
4     12          A    1
5     15          A    2
6     16          A    2</b>
<h3>Step 2: Fit the Nested ANOVA</h3>
We can use the following syntax to fit a nested ANOVA in R:
<b>aov(response ~ factor A / factor B)</b>
where:
<b>response:</b> The response variable
<b>factor A:</b> The first factor
<b>factor B:</b> The second factor nested within the first factor
The following code shows how to fit the nested ANOVA for our dataset:
<b>#fit nested ANOVA
nest &lt;- aov(df$growth ~ df$fertilizer / factor(df$tech))
#view summary of nested ANOVA
summary(nest)
              Df Sum Sq Mean Sq F value   Pr(>F)    
df$fertilizer                  2  372.7  186.33  53.238 4.27e-10 ***
df$fertilizer:factor(df$tech)  6   31.8    5.31   1.516    0.211    
Residuals                     27   94.5    3.50                     
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</b>
<h3>Step 3: Interpret the Output</h3>
We can look at the p-value column to determine whether or not each factor has a statistically significant effect on plant growth.
From the table above, we can see that fertilizer has a statistically significant effect on plant growth (p-value &lt; .05) but technician does not (p-value = 0.211).
This tells us that if we’d like to increase plant growth, we should focus on the fertilizer being used rather than the individual technician who is sprinkling the fertilizer.
<h3>Step 4: Visualize the Results</h3>
Lastly, we can use boxplots to visualize the distribution of plant growth by fertilizer and by technician:
<b>#load ggplot2 data visualization package
library(ggplot2)
#create boxplots to visualize plant growth
ggplot(df, aes(x=factor(tech), y=growth, fill=fertilizer)) +
  geom_boxplot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nestedR1.png">
From the chart we can see that there is significant variation in growth between the three different fertilizers, but not as much variation between the technicians within each fertilizer group.
This seems to match up with the results of the nested ANOVA and confirms that fertilizer has a significant effect on plant growth but individual technicians do not.
<h2><span class="orange">What is a Nested ANOVA? (Definition & Example)</span></h2>
A <b>nested ANOVA</b> is a type of ANOVA (“analysis of variance”) in which at least one factor is <em>nested</em> inside another factor.
Note:</b><i style="color: #000000;"> Sometimes a nested ANOVA is called a “hierarchical ANOVA.” These two terms are often used interchangeably.</i>
For example, suppose we would like to know if three different fertilizers produce different levels of plant growth.
To test this, we have three different technicians sprinkle fertilizer A on four plants each, another three technicians sprinkle fertilizer B on four plants each, and another three technicians sprinkle fertilizer C on four plants each.
In this scenario, the  response variable  is plant growth and the two factors are technician and fertilizer. It turns out that technician is <em>nested</em> within fertilizer:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nested1-1.png">
Here’s what the raw data would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nested2.png">
In this scenario, a nested ANOVA can test for two things:
Is plant growth equal across each level of factor 1 (fertilizer)?
Is plant growth equal across each level of factor 2 (technician)?
When we perform a nested ANOVA (using statistical software like R, Excel, SPSS, etc.) , the output will be in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/nested3.png">
Here’s how to interpret the output:
<b>Source:</b> The source of the variance
<b>Sum of Squares:</b> The sum of the squared deviations
<b>df:</b> The degrees of freedom
<b>Mean Square:</b> The mean square, calculatead as Sum of Squares / df
<b>F-Value:</b> The F-value, calculated as Mean Square / Mean Square Residuals
<b>p-value:</b> The p-value that corresponds to the F-value
We can look at the <b>p-value</b> column to determine whether or not each factor has a statistically significant effect on plant growth.
From the table above, we can see that fertilizer has a statistically significant effect on plant growth (p-value &lt; .05) but technician does not (p-value = 0.211).
This tells us that if we’d like to increase plant growth, we should focus on the fertilizer being used rather than the individual technician who is sprinkling the fertilizer.
<h3>Notes</h3>
Here are a few notes to keep in mind about nested ANOVA’s:
<b>1. Nested ANOVA’s can have more than two factors.</b>
In the previous example the nested ANOVA had two factors, one nested inside the other. However, a nested ANOVA could have more than two factors nested inside each other.
<b>2. Nested ANOVA’s are different than two-way ANOVA’s.</b>
In a nested ANOVA, at least one factor is nested <em>inside</em> another factor. This is different from a  two-way ANOVA  in which there are also two factors but neither factor is nested inside the other.
For example, in the previous scenario suppose each technician sprinkled each type of fertilizer. In this case, we could perform a two-way ANOVA because every possible combination of technique and fertilizer occurred in the dataset.
<h3>How to Perform a Nested ANOVA in Practice</h3>
The following tutorials explain how to perform a nested ANOVA in Excel and R:
 How to Perform a Nested ANOVA in R 
 How to Perform a Nested ANOVA in Excel 
<h2><span class="orange">How to Create a Nested For Loop in R (Including Examples)</span></h2>
A <b>nested for loop</b> allows you to loop through elements in multiple vectors (or multiple dimensions of a matrix) and perform some operations.
The basic structure of a <b>for loop</b> in R is:
<b>for(i in 1:4) {
  print (i)
}
[1] 1
[1] 2
[1] 3
[1] 4</b>
And the basic structure of a <b>nested for loop</b> is:
<b>for(i in 1:4) {
  for(j in 1:2) {
    print (i*j)
  }
}
[1] 1
[1] 2
[1] 2
[1] 4
[1] 3
[1] 6
[1] 4
[1] 8</b>
This tutorial shows a few examples of how to create nested for loops in R.
<h3>Example 1: Nested For Loop in R</h3>
The following code shows how to use a nested for loop to fill in the values of a 4×4 matrix:
<b>#create matrix
empty_mat &lt;- matrix(nrow=4, ncol=4)
#view empty matrix
empty_mat
     [,1] [,2] [,3] [,4]
[1,]   NA   NA   NA   NA
[2,]   NA   NA   NA   NA
[3,]   NA   NA   NA   NA
[4,]   NA   NA   NA   NA
#use nested for loop to fill in values of matrix
for(i in 1:4) {
  for(j in 1:4) {
    empty_mat[i, j] = (i*j)
  }
}
#view matrix
empty_mat
     [,1] [,2] [,3] [,4]
[1,]    1    2    3    4
[2,]    2    4    6    8
[3,]    3    6    9   12
[4,]    4    8   12   16
</b>
<h3>Example 2: Nested For Loop in R</h3>
The following code shows how to use a nested for loop to square each value in a data frame:
<b>#create empty data frame
df &lt;- data.frame(var1=c(1, 7, 4), var2=c(9, 13, 15))
#view empty data frame 
df
  var1 var2
1    1    9
2    7   13
3    4   15
#use nested for loop to square each value in the data frame
for(i in 1:nrow(df)) {
  for(j in 1:ncol(df)) {
    df[i, j] = df[i,j]^2
  }
}
#view new data frame
df
  var1 var2
1    1   81
2   49  169
3   16  225</b>
<h3>A Note on Looping</h3>
In general, nested for loops perform fine on small datasets or matrices but they tend to be fairly slow with larger data. 
For big data, the  family of apply functions  tend to be much quicker and the  data.table  package has many built-in functions that perform efficiently on larger datasets.
<h2><span class="orange">How to Write a Nested If Else Statement in R (With Examples)</span></h2>
The  ifelse()  function in base R can be used to write quick if-else statements. This function uses the following syntax:
<b>ifelse(test, yes, no)</b>
where:
<b>test:</b> A logical test
<b>yes:</b> The value to return if the logical test is True
<b>no:</b> The value to return if the logical test is False
This tutorial explains how to use this function to write if else statements along with nested if else statements in R, using the following data frame:
<b>#create data frame
df &lt;- data.frame(team = c('A', 'A', 'B', 'B', 'B', 'C', 'D'), points = c(4, 7, 8, 8, 8, 9, 12), rebounds = c(3, 3, 4, 4, 6, 7, 7))
#view data frame
df
  team points rebounds
1    A      4        3
2    A      7        3
3    B      8        4
4    B      8        4
5    B      8        6
6    C      9        7
7    D     12        7</b>
<h3>Example 1: How to Write a Basic If Else Statement</h3>
The following code shows how to create a new column in the data frame whose value is based off the value in the ‘team’ column:
<b>#create new column in data frame
df$rating &lt;- ifelse(df$team == 'A', 'great', 'bad')
#view data frame
df
  team points rebounds rating
1    A      4        3  great
2    A      7        3  great
3    B      8        4    bad
4    B      8        4    bad
5    B      8        6    bad
6    C      9        7    bad
7    D     12        7    bad
</b>
This simple ifelse statement tells R to do the following:
If the value in the team column is ‘A’ then give the player a rating of ‘great.’
Else, give the player a rating of ‘bad.’
<h3>Example 2: How to Write a Nested If Else Statement</h3>
The following code shows how to create a new column in the data frame by writing a nested if else statement:
<b>#create new column in data frame
df$rating &lt;- ifelse(df$team == 'A', 'great',
               ifelse(df$team == 'B', 'OK', 'bad'))
#view data frame
df
  team points rebounds rating
1    A      4        3  great
2    A      7        3  great
3    B      8        4     OK
4    B      8        4     OK
5    B      8        6     OK
6    C      9        7    bad
7    D     12        7    bad
</b>
This nested ifelse statement tells R to do the following:
If the value in the team column is ‘A’ then give the player a rating of ‘great.’
Else, if the value in the team column is ‘B’ then give the player a rating of ‘OK.’
Else, give the player a rating of ‘bad.’
<h3>Example 3: How to Write Longer Nested If Else Statements</h3>
The following code shows how to create a new column in the data frame by writing an even longer nested if else statement:
<b>#create new column in data frame
df$rating &lt;- ifelse(df$team == 'A', 'great',
               ifelse(df$team == 'B', 'OK', ifelse(df$team == 'C', 'decent', 'bad')))
#view data frame
df
  team points rebounds rating
1    A      4        3  great
2    A      7        3  great
3    B      8        4     OK
4    B      8        4     OK
5    B      8        6     OK
6    C      9        7 decent
7    D     12        7    bad
</b>
This nested ifelse statement tells R to do the following:
If the value in the team column is ‘A’ then give the player a rating of ‘great.’
Else, if the value in the team column is ‘B’ then give the player a rating of ‘OK.’
Else, if the value in the team column is ‘C’ then give the player a rating of ‘decent.’
Else, give the player a rating of ‘bad.’
Note that you can use this exact pattern to write nested ifelse statements as long as you’d like.
You can find more R tutorials  here .
<h2><span class="orange">What is a Nested Model? (Definition & Example)</span></h2>
A <b>nested model</b> is simply a  regression model  that contains a subset of the predictor variables in another regression model.
For example, suppose we have the following regression model (let’s call it Model A) that predicts the number of points scored by a basketball player based on four predictor variables:
Points = β<sub>0</sub> + β<sub>1</sub>(minutes) + β<sub>2</sub>(height) + β<sub>3</sub>(position) + β<sub>4</sub>(shots) + ε
One example of a nested model (let’s call it Model B) would be the following model with only two of the predictor variables from model A:
Points = β<sub>0</sub> + β<sub>1</sub>(minutes) + β<sub>2</sub>(height) + ε
We would say that <b>Model B is nested in Model A</b> because Model B contains a subset of the predictor variables from Model A.
However, consider if we had another model (let’s call it Model C) that contains three predictor variables:
Points = β<sub>0</sub> + β<sub>1</sub>(minutes) + β<sub>2</sub>(height) + β<sub>3</sub>(free throws attempted)
<b>We would not say that Model C is nested in Model A</b> because each model contains predictor variables that the other model does not.
<h3>The Importance of Nested Models</h3>
We often use nested models in practice when we want to know if a model with a full set of predictor variables can fit a dataset better than a model with a subset of those predictor variables.
For example, in the scenario above we might fit a <b>full model</b> using minutes played, height, position, and shots attempted to predict the number of points scored by basketball players.
However, we might suspect that position and shots attempted don’t do a very good job of predicting points scored.
Thus, we might fit a <b>nested model</b> that only uses minutes played and height to predict points scored. 
We can then compare the two models to determine if there is a statistically significant difference.
If there is no significant difference between the models, we can drop position and shots attempted as predictor variables since they don’t significantly improve the model.
<h3>How to Analyze Nested Models</h3>
To determine if a nested model is significantly different than a “full” model, we typically perform a likelihood ratio test which uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> The full model and the nested model fit the data equally well. Thus, you should <b>use the nested model</b>.
<b>H<sub>A</sub>:</b> The full model fits the data significantly better than the nested model. Thus, you should <b>use the full model</b>.
A likelihood ratio test produces a Chi-Square test statistic and a corresponding p-value.
If the  p-value  of the test is below a certain significance level (e.g. 0.05), then we can reject the null hypothesis and conclude that the full model offers a significantly better fit.
The following tutorials explain how to perform a likelihood ratio test using R and Python:
 How to Perform a Likelihood Ratio Test in R 
 How to Perform a Likelihood Ratio Test in Python 
<h2><span class="orange">Neyman Bias: Definition & Examples</span></h2>
<b>Neyman bias</b> (also known as <em>prevalence-incidence bias</em>) is a type of bias that can occur in research studies in which extremely sick individuals or extremely healthy individuals are excluded from the final results of the study which may lead to biased results.
There are two ways in which this bias can affect the results of a study:
<b>1.</b> If extremely sick individuals are excluded from the study because they’ve died, then the disease will appear less severe.
<b>2.</b> If extremely healthy individuals are excluded from the study because they have recovered and been sent home, then the disease will appear more severe.
<h3>Examples of Neyman Bias</h3>
Here are two examples of Neyman Bias occurring in different scenarios:
<b>Example 1: Sick individuals being excluded from a study.</b>
Suppose a group of researchers at a hospital want to study the severity of a certain strain of flu. They randomly select a sample of 40 individuals in the area who contract that strain of flu and monitor their outcomes. 
In this scenario, the individuals who contract a particularly severe case of the flu and happen to die from it will be excluded from the study. This means only individuals with mild cases will be included in the study, which will make the flu appear less severe.
<b>Example 2: Healthy individuals being excluded from a study.</b>
Suppose a group of researchers at a hospital want to study the severity of a certain seasonal cold. They randomly select a sample of 30 individuals in the area who contract the cold and monitor their outcomes. 
In this scenario, the individuals who already contracted the cold and recovered will not be included in the study, which means only individuals with more severe cases who have not recovered will be included in the study. This could cause the cold to appear more severe.
<h3>In What Types of Studies Does Neyman Bias Occur?</h3>
Neyman bias occurs most often in studies in which there is a long time period between individuals contracting a certain disease and then being included in a study simply because this gives them more time to either (1) recover and not be included in the study or (2) die and not be included in the study.
Case-control studies are most susceptible to this type of bias, but it can also occur in cohort studies and cross-sectional studies.
<h3>How to Prevent Neyman Bias</h3>
There are two ways to avoid the pitfalls of Neyman bias:
<b>1. Use incident cases rather than prevalent cases.</b>
An <b>incident case </b>is a newly diagnosed case of a disease. A <b>prevalent case </b>is an existing case of a disease, in which an individual has typically had it for a longer period of time and thus have a more progressed and serious version of the disease. By using incident cases, it’s less likely that individuals will be excluded from the study at some point since they’re a new case.
<b>2. Use follow-up studies.</b>
Another way to avoid Neyman bias is by using a follow-up study in which researchers follow up with individuals and examine their situation after the study is over. This can be particularly useful for monitoring individuals who left a study because they recovered from the disease, which allows researchers to gain a better understanding of the long-term effects of a disease.
<h2><span class="orange">4 Examples of No Correlation Between Variables</span></h2>
In statistics, <b>correlation</b> is a measure of the linear relationship between two variables.
The value for a correlation coefficient is always between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
If two variables have a correlation of zero, it indicates that they’re not related in any way. In other words, knowing the value of one variable doesn’t give us any idea of what the value of the other variable may be.
If we create a  scatterplot  of two variables that have zero correlation, there will be no clear pattern in the plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation1.png">
<h3>Examples of No Correlation</h3>
The following examples illustrate scenarios where two variables have no correlation.
<b>Example 1: Coffee Consumption vs. Intelligence</b>
The amount of coffee that individuals consume and their IQ level has a correlation of zero. In other words, knowing how much coffee an individual drinks doesn’t give us an idea of what their IQ level might be.
If we created a scatterplot of daily coffee consumption vs. IQ level, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation2.png">
<b>Example 2: Height & Exam Scores</b>
The height of students and their average exam scores has a correlation of zero. In other words, knowing the height of an individual doesn’t give us an idea of what their average exam score might be.
If we created a scatterplot of height vs. average exam score, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation3.png">
<b>Example 3: Shoe Size & Movies Watched</b>
The shoe size of individuals and the number of movies they watch per year has a correlation of zero. In other words, knowing the shoe size of an individual doesn’t give us an idea of how many movies they watch per year.
If we created a scatterplot of shoe size vs. number of movies watched, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation4.png">
<b>Example 4: Weight & Income</b>
The weight of individuals and their annual income has a correlation of zero. In other words, knowing the weight of a person doesn’t give us an idea of what their annual income might be.
If we created a scatterplot of weight vs. income, it would look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/noCorrelation5.png">
<h2><span class="orange">How to Fix in Python: no handles with labels found to put in legend</span></h2>
One warning you may encounter when using matplotlib is:
<b>No handles with labels found to put in legend.
</b>
This warning usually occurs for one of two reasons:
<b>1.</b> You failed to create labels for the data in the plot.
<b>2.</b> You attempted to create a legend before creating a plot.
The following examples shows how to avoid this warning in both scenarios.
<h3>Example 1: You failed to create labels for the data in the plot.</h3>
Suppose we attempt to use the following code to create a line chart in matplotlib with a legend and labels:
<b>import matplotlib.pyplot as plt
import pandas as pd
#define data values
df = pd.DataFrame({'x': [18, 22, 19, 14, 14, 11, 20, 28],   'y': [5, 7, 7, 9, 12, 9, 9, 4],   'z': [11, 8, 10, 6, 6, 5, 9, 12]})
#add multiple lines to matplotlib plot
plt.plot(df['x'], color='green')
plt.plot(df['y'], color='blue')
plt.plot(df['z'], color='purple')
#attempt to add legend to plot
plt.legend()
No handles with labels found to put in legend.
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/handle1.jpg"506">
Matplotlib creates the line plot, but we receive the warning of <b>No handles with labels found to put in legend</b>.
To avoid this warning, we must use the <b>label</b> argument to provide a label for each line in the plot:
<b>import matplotlib.pyplot as plt
import pandas as pd
#define data values
df = pd.DataFrame({'x': [18, 22, 19, 14, 14, 11, 20, 28],   'y': [5, 7, 7, 9, 12, 9, 9, 4],   'z': [11, 8, 10, 6, 6, 5, 9, 12]})
#add multiple lines to matplotlib plot
plt.plot(df['x'], label='x', color='green')
plt.plot(df['y'], label='y', color='blue')
plt.plot(df['z'], label='z', color='purple')
#attempt to add legend to plot
plt.legend()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/handle2.jpg"510">
Notice that a legend is created with labels and we don’t receive any warning this time.
<h3>Example 2: You attempted to create a legend before creating a plot.</h3>
Suppose we attempt to use the following code to create a line chart in matplotlib with a legend and labels:
<b>import matplotlib.pyplot as plt
import pandas as pd
#define data values
df = pd.DataFrame({'x': [18, 22, 19, 14, 14, 11, 20, 28],   'y': [5, 7, 7, 9, 12, 9, 9, 4],   'z': [11, 8, 10, 6, 6, 5, 9, 12]})
#attempt to add legend to plot
plt.legend()
#add multiple lines to matplotlib plot
plt.plot(df['x'], label='x', color='green')
plt.plot(df['y'], label='y', color='blue')
plt.plot(df['z'], label='z', color='purple')
No handles with labels found to put in legend.
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/handle1.jpg"506">
Matplotlib creates the line plot, but we receive the warning of <b>No handles with labels found to put in legend</b>.
To avoid this warning, we must use <b>plt.legend()</b> <em>after</em> adding the lines to the plot:
<b>import matplotlib.pyplot as plt
import pandas as pd
#define data values
df = pd.DataFrame({'x': [18, 22, 19, 14, 14, 11, 20, 28],   'y': [5, 7, 7, 9, 12, 9, 9, 4],   'z': [11, 8, 10, 6, 6, 5, 9, 12]})
#add multiple lines to matplotlib plot
plt.plot(df['x'], label='x', color='green')
plt.plot(df['y'], label='y', color='blue')
plt.plot(df['z'], label='z', color='purple')
#attempt to add legend to plot
plt.legend()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/handle2.jpg"510">
A legend is created with labels and we don’t receive any warning this time.
<h2><span class="orange">How to Fix: No module named matplotlib</span></h2>
One common error you may encounter when using Python is:
<b>no module named 'matplotlib'</b>
This error occurs when Python does not detect the  matplotlib  library in your current environment.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Step 1: pip install matplotlib</h3>
Since matplotlib doesn’t come installed automatically with Python, you’ll need to install it yourself. The easiest way to do so is by using <b>pip</b>, which is a package manager for Python.
You can run the following pip command to install matplotlib:
<b>pip install matplotlib</b>
In most cases, this will fix the error.
<h3>Step 2: Install pip</h3>
If you’re still getting an error, you may need to install pip. Use  these steps  to do so.
You can also use  these steps  to upgrade pip to the latest version to ensure that it works.
You can then run the same pip command as earlier to install matplotlib:
<b>pip install matplotlib</b>
At this point, the error should be resolved.
<h3>Step 3: Check matplotlib and pip Versions</h3>
If you’re still running into errors, you may be using a different version of matplotlib and pip.
You can use the following commands to check if your matplotlib and pip versions match:
<b>which python
python --version
which pip</b>
If the two versions don’t match, you need to either install an older version of matplotlib or upgrade your Python version.
<h3>Step 4: Check matplotlib Version</h3>
Once you’ve successfully installed matplotlib, you can use the following command to display the matplotlib version in your environment:
<b>pip show matplotlib
Name: matplotlib
Version: 3.1.3
Summary: Python plotting package
Home-page: https://matplotlib.org
Author: John D. Hunter, Michael Droettboom
Author-email: matplotlib-users@python.org
License: PSF
Location: /srv/conda/envs/notebook/lib/python3.7/site-packages
Requires: cycler, numpy, kiwisolver, python-dateutil, pyparsing
Required-by: seaborn, scikit-image
Note: you may need to restart the kernel to use updated packages.
</b>
<b>Note:</b> The easiest way to avoid errors with matplotlib and Python versions is to simply install  Anaconda , which is a toolkit that comes pre-installed with Python and matplotlib and is free to use.
<h2><span class="orange">How to Fix: No module named numpy</span></h2>
One common error you may encounter when using Python is:
<b>Import error: no module named 'numpy'</b>
This error occurs when Python does not detect the  NumPy  library in your current environment.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Step 1: pip install numpy</h3>
Since NumPy doesn’t come installed automatically with Python, you’ll need to install it yourself. The easiest way to do so is by using <b>pip</b>, which is a package manager for Python.
You can run the following pip command to install NumPy:
<b>pip install numpy</b>
For python 3 you can use:
<b>pip3 install numpy</b>
In most cases, this will fix the error.
<h3>Step 2: Install pip</h3>
If you’re still getting an error, you may need to install pip. Use  these steps  to do so.
You can also use  these steps  to upgrade pip to the latest version to ensure that it works.
You can then run the same pip command as earlier to install NumPy:
<b>pip install numpy</b>
At this point, the error should be resolved.
<h3>Step 3: Check NumPy Version</h3>
Once you’ve successfully installed NumPy, you can use the following command to display the NumPy version in your environment:
<b>pip show numpy
Name: numpy
Version: 1.20.3
Summary: NumPy is the fundamental package for array computing with Python.
Home-page: https://www.numpy.org
Author: Travis E. Oliphant et al.
Author-email: None
License: BSD
Location: /srv/conda/envs/notebook/lib/python3.7/site-packages
Requires: 
Required-by: tensorflow, tensorflow-estimator, tensorboard, statsmodels, seaborn,
scipy, scikit-learn, PyWavelets, patsy, pandas, matplotlib, Keras-Preprocessing,
Keras-Applications, imageio, h5py, bqplot, bokeh, altair
Note: you may need to restart the kernel to use updated packages.
</b>
<h2><span class="orange">How to Fix: No module named pandas</span></h2>
One common error you may encounter when using Python is:
<b>no module named 'pandas'</b>
This error occurs when Python does not detect the  pandas  library in your current environment.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Step 1: pip install pandas</h3>
Since pandas doesn’t come installed automatically with Python, you’ll need to install it yourself. The easiest way to do so is by using <b>pip</b>, which is a package manager for Python.
You can run the following pip command to install pandas:
<b>pip install pandas</b>
In most cases, this will fix the error.
<h3>Step 2: Install pip</h3>
If you’re still getting an error, you may need to install pip. Use  these steps  to do so.
You can also use  these steps  to upgrade pip to the latest version to ensure that it works.
You can then run the same pip command as earlier to install pandas:
<b>pip install pandas</b>
At this point, the error should be resolved.
<h3>Step 3: Check pandas and pip Versions</h3>
If you’re still running into errors, you may be using a different version of pandas and pip.
You can use the following commands to check if your pandas and pip versions match:
<b>which python
python --version
which pip</b>
If the two versions don’t match, you need to either install an older version of pandas or upgrade your Python version.
<h3>Step 4: Check pandas Version</h3>
Once you’ve successfully installed pandas, you can use the following command to display the pandas version in your environment:
<b>pip show pandas
Name: pandas
Version: 1.1.5
Summary: Powerful data structures for data analysis, time series, and statistics
Home-page: https://pandas.pydata.org
Author: None
Author-email: None
License: BSD
Location: /srv/conda/envs/notebook/lib/python3.6/site-packages
Requires: python-dateutil, pytz, numpy
Required-by: 
Note: you may need to restart the kernel to use updated packages.
</b>
<b>Note:</b> The easiest way to avoid errors with pandas and Python versions is to simply install  Anaconda , which is a toolkit that comes pre-installed with Python and pandas and is free to use.
<h2><span class="orange">How to Fix: No module named plotly</span></h2>
One common error you may encounter when using Python is:
<b>ModuleNotFoundError: No module named 'plotly'</b>
This error occurs when Python does not detect the  Plotly  library in your current environment.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Step 1: pip install plotly</h3>
Since Plotly doesn’t come installed automatically with Python, you’ll need to install it yourself.
The easiest way to do so is by using <b>pip</b>, which is a package manager for Python.
You can run the following pip command to install plotly:
<b>pip install plotly</b>
For python 3 you can use:
<b>pip3 install numpy</b>
You can then run the following code to see if Plotly was successfully installed:
<b>pip list | grep plotly
plotly                        5.3.1
</b>
If <b>plotly</b> is displayed with a version number, this means that it was successfully installed.
In most cases, this will fix the error.
<h3>Step 2: Install pip</h3>
If you’re still getting an error, you may need to install pip. Use  these steps  to do so.
You can also use  these steps  to upgrade pip to the latest version to ensure that it works.
You can then run the same pip command as earlier to install Plotly:
<b>pip install plotly</b>
At this point, the error should be resolved.
<h3>Step 3: Check Plotly Version</h3>
Once you’ve successfully installed Plotly, you can use the following command to display the Plotly version in your environment:
<b>pip show plotly
Name: plotly
Version: 5.3.1
Summary: An open-source, interactive data visualization library for Python
Home-page: https://plotly.com/python/
Author: Chris P
Author-email: chris@plot.ly
License: MIT
Location: /srv/conda/envs/notebook/lib/python3.7/site-packages
Requires: six, tenacity
Required-by: 
Note: you may need to restart the kernel to use updated packages.
</b>
<h2><span class="orange">How to Fix: No module named seaborn</span></h2>
One common error you may encounter when using Python is:
<b>no module named 'seaborn'</b>
This error occurs when Python does not detect the  seaborn  library in your current environment.
This tutorial shares the exact steps you can use to troubleshoot this error.
<h3>Step 1: pip install seaborn</h3>
Since seaborn doesn’t come installed automatically with Python, you’ll need to install it yourself. The easiest way to do so is by using <b>pip</b>, which is a package manager for Python.
You can run the following pip command to install seaborn:
<b>pip install seaborn</b>
In most cases, this will fix the error.
<h3>Step 2: Install pip</h3>
If you’re still getting an error, you may need to install pip. Use  these steps  to do so.
You can also use  these steps  to upgrade pip to the latest version to ensure that it works.
You can then run the same pip command as earlier to install seaborn:
<b>pip install seaborn</b>
At this point, the error should be resolved.
<h3>Step 3: Check seaborn and pip Versions</h3>
If you’re still running into errors, you may be using a different version of seaborn and pip.
You can use the following commands to check if your seaborn and pip versions match:
<b>which python
python --version
which pip</b>
If the two versions don’t match, you need to either install an older version of seaborn or upgrade your Python version.
<h3>Step 4: Check seaborn Version</h3>
Once you’ve successfully installed seaborn, you can use the following command to display the seaborn version in your environment:
<b>pip show seaborn
Name: seaborn
Version: 0.11.2
Summary: seaborn: statistical data visualization
Home-page: https://seaborn.pydata.org
Author: Michael Waskom
Author-email: mwaskom@gmail.com
License: BSD (3-clause)
Location: /srv/conda/envs/notebook/lib/python3.7/site-packages
Requires: numpy, scipy, matplotlib, pandas
Required-by: 
Note: you may need to restart the kernel to use updated packages.
</b>
<b>Note:</b> The easiest way to avoid errors with seaborn and Python versions is to simply install  Anaconda , which is a toolkit that comes pre-installed with Python and seaborn and is free to use.
<h2><span class="orange">How to Fix: No module named ‘sklearn.cross_validation’</span></h2>
One error you may encounter when using Python is:
<b>ModuleNotFoundError: No module named 'sklearn.cross_validation'
</b>
This error usually occurs when you attempt to import the <b>train_test_split</b> function from <b>sklearn</b> using the following line:
<b>from sklearn.cross_validation import train_test_split</b>
However, the <b>cross_validation</b> sub-module has been replaced with the <b>model_selection</b> sub-module, so you need to use the following line instead:
<b>from sklearn.model_selection import train_test_split</b>
The following example shows how to resolve this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we would like to use the <b>train_test_split</b> function from <b>sklearn</b> to split a pandas DataFrame into training and testing sets.
Suppose we attempt to use the following code to import the <b>train_test_split</b> function:
<b>from sklearn.cross_validation import train_test_split
ModuleNotFoundError: No module named 'sklearn.cross_validation' 
</b>
We receive an error because we used the wrong sub-module name when attempting to import the <b>train_test_split</b> function.
<h2>How to Fix the Error</h2>
To fix this error, we simply need to use the <b>model_selection</b> sub-module instead:
<b>from sklearn.model_selection import train_test_split
</b>
This time we don’t receive any error.
We could then proceed to use the <b>train_test_split</b> function to split a pandas DataFrame into a training and testing set:
<b>from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
#make this example reproducible
np.random.seed(1)
#create DataFrame with 1000 rows and 3 columns
df = pd.DataFrame({'x1': np.random.randint(30, size=1000),   'x2': np.random.randint(12, size=1000),   'y': np.random.randint(2, size=1000)})
#split original DataFrame into training and testing sets
train, test = train_test_split(df, test_size=0.2, random_state=0)
#view first few rows of each set
print(train.head())
     x1  x2  y
687  16   2  0
500  18   2  1
332   4  10  1
979   2   8  1
817  11   1  0
print(test.head())
     x1  x2  y
993  22   1  1
859  27   6  0
298  27   8  1
553  20   6  0
672   9   2  1</b>
We’re successfully able to use the <b>train_test_split</b> function without any error.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in Python:
 How to Fix: columns overlap but no suffix specified 
 How to Fix: ‘numpy.ndarray’ object has no attribute ‘append’ 
 How to Fix: if using all scalar values, you must pass an index 
 How to Fix: ValueError: cannot convert float NaN to integer 
<h2><span class="orange">How to Fix Error in R: non-conformable arguments</span></h2>
One error message you may encounter when using R is:
<b>Error in matrix2 %*% matrix1 : non-conformable arguments
</b>
This error occurs when you attempt to multiply two matrices but the number of <b>columns</b> in the left matrix does not match the number of <b>rows</b> in the right matrix.
The following example shows how to resolve this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following two matrices in R:
<b>#create first matrix
mat1 &lt;- matrix(1:10, nrow=5) 
mat1
     [,1] [,2]
[1,]    1    6
[2,]    2    7
[3,]    3    8
[4,]    4    9
[5,]    5   10
#create second matrix
mat2 &lt;- matrix(1:6, nrow=2)
mat2
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6</b>
Now suppose we attempt to multiply the second matrix by the first matrix:
<b>#attempt to multiply second matrix by first matrix
mat2 %*% mat1
Error in mat2 %*% mat1 : non-conformable arguments
</b>
We receive an error because the number of <b>columns</b> (3) in the left  matrix does not match the number of <b>rows</b> (5) in the right matrix.
<h3>How to Avoid the Error</h3>
To avoid the <b>non-conformable arguments</b> error, we must instead multiply the first matrix by the second matrix:
<b>multiply first matrix by second matrix
mat1 %*% mat2
     [,1] [,2] [,3]
[1,]   13   27   41
[2,]   16   34   52
[3,]   19   41   63
[4,]   22   48   74
[5,]   25   55   85
</b>
Notice that we’re able to successfully multiply the two matrices without any error because the number of <b>columns</b> (2) in the left matrix matches the number of <b>rows</b> (2) in the right matrix.
We can also use the<b> dim()</b> function to display the number of columns and rows in each matrix:
<b>#view dimensions of first matrix
dim(mat1)
[1] 5 2
#view dimensions of second matrix
dim(mat2)
[1] 2 3
</b>
From this output we can see:
The first matrix has <b>5</b> rows and <b>2</b> columns.
The second matrix has <b>2</b> rows and <b>3</b> columns.
This makes it obvious that we must use the first matrix on the left and the second matrix on the right when multiplying since the first matrix has <b>2</b> columns and the second matrix has <b>2</b> rows.
<h2><span class="orange">How to Perform Nonlinear Regression in Excel (Step-by-Step)</span></h2>
<b>Nonlinear regression</b> is a regression technique that is used when the relationship between a predictor variable and a  response variable  does not follow a linear pattern.
The following step-by-step example shows how to perform nonlinear regression in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset to work with:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/nonlinear1.png">
<h3>Step 2: Create a Scatterplot</h3>
Next, let’s create a scatterplot to visualize the data.
First, highlight the cells in the range <b>A1:B21</b>. Next, click the <b>Insert</b> tab along the top ribbon, and then click the first plot option under <b>Scatter</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel3.png">
The following scatterplot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/nonlinear2.png">
<h3>Step 3: Add a Trendline</h3>
Next, click anywhere on the scatterplot. Then click the <b>+</b> sign in the top right corner. In the dropdown menu, click the arrow next to <b>Trendline</b> and then click <b>More Options</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/nonlinear3.png">
In the window that appears to the right, click the button next to <b>Polynomial</b>. Then check the boxes next to <b>Display Equation on chart</b> and <b>Display R-squared value on chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/nonlinear4.png">
This produces the following curve on the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/nonlinear5.png">
Note that you may need to experiment with the value for the <b>Order</b> of the polynomial until you find the curve that best fits the data.
<h3>Step 4: Write the Regression Equation</h3>
From the plot we can see that the equation of the regression line is as follows:
y = -0.0048x<sup>4</sup> + 0.2259x<sup>3</sup> – 3.2132x<sup>2</sup> + 15.613x – 6.2654
The  R-squared  tells us the percentage of the variation in the response variable that can be explained by the predictor variables.
The R-squared for this particular curve is <b>0.9651</b>. This means that 96.51% of the variation in the response variable can be explained by the predictor variables in the model.
<h2><span class="orange">5 Examples of Nonlinear Relationships Between Variables</span></h2>
In most statistics courses, students learn about <b>linear relationships</b> between variables.
These are relationships where an increase in one variable is associated with a predictable increase in another variable.
One example of this might be minutes played in a basketball game vs. total points scored:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear11.jpg"543">
Players who play more minutes tend to score more points.
However, there can also exist <b>nonlinear relationships</b> between variables and these appear all the time in the real world.
This tutorial provides five examples of nonlinear relationships between variables in the real world.
<h3>Example 1: Quadratic Relationships</h3>
One of the most common nonlinear relationships in the real world is a <b>quadratic relationship</b> between variables.
When plotted on a scatterplot, this relationship typically exhibits a “U” shape.
One example might be total working hours per week vs. overall happiness:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear12.jpg"536">
As working hours increase from zero, overall happiness tends to increase, but beyond a certain threshold more working hours actually leads to decreased happiness.
This upside down “U” shape is the signature shape of a quadratic relationship between two variables.
<h3>Example 2: Cubic Relationships</h3>
Another common nonlinear relationship in the real world is a <b>cubic relationship</b> between variables.
When plotted on a scatterplot, this relationship typically has two distinct curves.
This type of relationship exists often between variables in the field of thermodynamics:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear13.jpg"541">
Notice that there are two distinct curves on the plot and the relationship between variable X and variable Y is clearly not linear.
<h3>Example 3: Exponential Relationships</h3>
Another common nonlinear relationship in the real world is an <b>exponential relationship</b> between variables.
When plotted on a scatterplot, this relationship exhibits a single curve that becomes more pronounced as the variable on the x-axis increases.
One well-known example of an exponential relationship is the lifespan of bamboo plants and their yearly growth:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear14.jpg"560">
During the first few years of growth, a bamboo plant grows very slowly but once it reaches a certain age it explodes in height and grows at a rapid pace.
<h3>Example 4: Logarithmic Relationships</h3>
Another common nonlinear relationship in the real world is a <b>logarithmic relationship</b> between variables.
When plotted on a scatterplot, this relationship exhibits a single curve that becomes less pronounced as the variable on the x-axis increases.
One example of a logarithmic relationship is between the efficiency of smart-home technologies and time:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear15.jpg"516">
When a new smart-home technology (like a self-operating vacuum or self-operating AC unit) is installed in a home, it learns rapidly how to become more efficient, but then once it reaches a certain point it hits a maximum threshold in efficiency.
<h3>Example 5: Cosine Relationships</h3>
Another common nonlinear relationship in the real world is a <b>cosine relationship</b> between variables.
When plotted on a scatterplot, this relationship exhibits a “wave” shape.
One example of a cosine relationship is between the frequency of sound waves and time:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/nonlinear16.jpg"536">
Notice how the relationship exhibits a “wave” shape, which is highly nonlinear.
<h2><span class="orange">Nonresponse Bias: Explanation & Examples</span></h2>
<b>Nonresponse bias </b>is the bias that occurs when the people who respond to a survey differ significantly from the people who do not respond to the survey.
Nonresponse bias can occur for several reasons:
The survey is poorly designed and leads to nonresponses. For example, excessively long surveys without incentives may cause a large percentage of people to not complete the survey.
Certain people are more likely to respond to a particular survey. For example, people who go rock climbing often are more likely to respond to a survey about a potential new rock climbing facility than people who don’t go rock climbing.
The survey didn’t reach all members of a population. For example, a survey sent out on a new phone app may only reach younger people who have the app, which leads to nonresponses from older members of the population.
The survey asks embarrassing questions about private information that make many people unwilling to respond.
Nonresponse bias can occur for all of these reasons.
<h2>Why is Nonresponse Bias a Problem?</h2>
Nonresponse bias is a problem for two main reasons:
<b>1. Nonresponse bias causes the sample to be unrepresentative of the population as a whole.</b> The whole point of collecting data for a sample is that it’s quicker and cheaper than collecting data for an entire population, and to be able to extrapolate the findings from the sample to the larger population.
In order to extrapolate the findings, though, the sample needs to be <em> representative  </em>of our population as a whole. Ideally we would like our sample to be a “mini” version of the population.
Unfortunately, nonresponse bias can cause the people in our sample to be significantly different than the people in the larger population.
For example, suppose a city is considering building a new rock climbing facility. To gauge how interested people in the city would be in using this type of facility, city officials send out a short survey via a new smartphone app.
Because of the method used to deliver the survey and because of the content on the survey (rock climbing questions), mostly young people who have the app and who are interested in rock climbing respond.
Thus, when the survey results come back it appears that an overwhelming majority of people in the city are interested in having this new facility built. Unfortunately, the results from the survey are not representative of the larger population.
The visual below illustrates this problem: suppose the green circles represent people who are interested in using the facility while the red circles represent people who are not interested in using the facility:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/nonResponseBias.jpg">
Notice how the sample is not representative of the larger population. The results of the survey would show that most people are excited about a new rock climbing facility. Unfortunately, if city officials assumed that this sample was representative of the population, they may decide to build the facility and then quickly realize that far fewer people would use it than they thought.
<b>2. Nonresponse bias can cause larger variance for estimates</b>. If the sample size of the survey turns out to be smaller than the sample size researchers had planned to use, the variance for the estimates of the study may be larger than planned.
For example, from  hypothesis testing  we know that the larger our sample size, the lower the variance on our estimate for a population mean or a population proportion. However, the smaller our sample size, the higher the variance on our estimates for population parameters, and the harder it is to find a statistically significant finding.
<h2>Examples of Nonresponse Bias</h2>
The following examples illustrate several cases in which nonresponse bias can occur.
<h3>Example 1</h3>
Researchers want to know how computer scientists perceive a new software program. There is pressure to get as much data as possible from the survey, so the researchers design a survey that takes roughly one hour to complete. When they distribute the survey, they find that many computer scientists either don’t respond at all or begin to respond but eventually quit before completing the entire survey.
When researchers get the data back, they find that the respondents perceive the software to be  excellent and high quality. However, once they roll out the new software to the entire population of computer scientists they find that they receive mostly negative feedback.
It turns out that the people who took the time to complete the entire survey turned out to be mostly entry-level computer scientists who were unable to assess the flaws of the program.
Because of this, respondents of the survey did not reflect the larger computer scientist population as whole and thus the results of the survey were unreliable.
<h3>Example 2</h3>
Researchers want to learn about alcohol consumption rates at a certain college. They decide to set up a booth on campus where students can stop and take a questionnaire in regards to how much and how often they consume alcohol. Unfortunately, the questionnaire is not anonymous so only students who drink very little or not at all choose to fill out the questionnaire. 
When the results come back, it appears that alcohol consumption is low and infrequent among students. Unfortunately, the respondents of the survey are not reflective of the larger population of students on campus and thus the findings are unreliable.
<h3>Example 3</h3>
One classic example of nonresponse bias is the 1936 Presidential Election. A popular publication at the time ran a poll that predicted Alf Landon would beat Franklin D. Roosevelt by a landslide. However, when the election took place Franklin D. Roosevelt actually won by a landslide.
It turns out that of the 10 million questionnaires sent out, only 2.3 million people responded. The 7.7 million who did not respond turned out to be significantly different in terms of political preference. 
Thus, the results of the questionnaire were not reflective of the population as a whole, which is why the prediction that Alf Landon would win turned out to be so incorrect.
<h2>How to Prevent Nonresponse Bias</h2>
Nonresponse bias can be prevented (or at least mitigated) by taking the following steps:
Design the survey to be relatively short. The longer a survey, the less likely people are to take time out of their day to respond.
Offer incentives for completing the survey. Incentives generally increase response rates.
Make sure that people know answers to the survey will be confidential or anonymous. This generally makes people more willing to respond.
Distribute the survey in such a way that it reaches a large percentage of the population, e.g. use traditional forms of distribution rather than a new app that few people have.
While it’s not always possible to completely eliminate the effects of nonresponse bias, it’s possible to minimize the effects by using a smart survey design and distribution method.
<h2><span class="orange">Normal Approximation to Binomial: Definition & Example</span></h2>
If <em>X</em> is a  random variable  that follows a  binomial distribution  with <em>n</em> trials and <em>p</em> probability of success on a given trial, then we can calculate the mean (μ) and standard deviation (σ) of <em>X</em> using the following formulas:
μ = np
σ = √np(1-p)
It turns out that if <em>n</em> is sufficiently large then we can actually use the  normal distribution  to approximate the probabilities related to the binomial distribution. This is known as the <b>normal approximation to the binomial</b>.
For <em>n</em> to be “sufficiently large” it needs to meet the following criteria:
np ≥ 5
n(1-p) ≥ 5
When both criteria are met, we can use the normal distribution to answer probability questions related to the binomial distribution.
However, the normal distribution is a continuous probability distribution while the binomial distribution is a discrete probability distribution, so we must apply a continuity correction when calculating probabilities.
In simple terms, a <b>continuity correction</b> is the name given to adding or subtracting 0.5 to a discrete x-value.
For example, suppose we would like to find the probability that a coin lands on heads less than or equal to 45 times during 100 flips. That is, we want to find P(X ≤ 45). To use the normal distribution to approximate the binomial distribution, we would instead find P(X ≤ 45.5).
The following table shows when you should add or subtract 0.5, based on the type of probability you’re trying to find:
<table><tbody>
<tr>
<th><b>Using Binomial Distribution</b></th>
<th><b>Using Normal Distribution with Continuity Correction</b></th>
</tr>
<tr>
<td style="text-align: center;">X = 45</td>
<td style="text-align: center;">44.5 &lt; X &lt; 45.5</td>
</tr>
<tr>
<td style="text-align: center;">X ≤ 45</td>
<td style="text-align: center;">X &lt; 45.5</td>
</tr>
<tr>
<td style="text-align: center;">X &lt; 45</td>
<td style="text-align: center;">X &lt; 44.5</td>
</tr>
<tr>
<td style="text-align: center;">X ≥ 45</td>
<td style="text-align: center;">X > 44.5</td>
</tr>
<tr>
<td style="text-align: center;">X > 45</td>
<td style="text-align: center;">X > 45.5</td>
</tr>
</tbody></table>
The following step-by-step example shows how to use the normal distribution to approximate the binomial distribution.
<h3>Example: Normal Approximation to the Binomial</h3>
Suppose we want to know the probability that a coin lands on heads less than or equal to 43 times during 100 flips.
In this situation we have the following values:
<b>n</b> (number of trials) = 100
<b>X</b> (number of successes) = 43
<b>p</b> (probability of success on a given trial) = 0.50
To calculate the probability of the coin landing on heads less than or equal to 43 times, we can use the following steps:
<b>Step 1: Verify that the sample size is large enough to use the normal approximation.</b>
First, we must verify that the following criteria are met:
np ≥ 5
n(1-p) ≥ 5
In this case, we have:
np = 100*0.5 = 50
n(1-p) = 100*(1 – 0.5) = 100*0.5 = 50
Both numbers are greater than 5, so we’re safe to use the normal approximation.
<b>Step 2: Determine the continuity correction to apply.</b>
Referring to the table above, we see that we should add 0.5 when we’re working with a probability in the form of X ≤ 43. Thus, we will be finding P(X&lt; 43.5).
<b>Step 3: Find the mean (μ) and standard deviation (σ) of the binomial distribution.</b>
<b>μ</b> = n*p = 100*0.5 = 50
<b>σ </b>= √n*p*(1-p) = √100*.5*(1-.5) = √25 = 5
<b>Step 4: Find the z-score using the mean and standard deviation found in the previous step.</b>
<b>z </b>= (x – μ) / σ = (43.5 – 50) / 5 = -6.5 / 5 = -1.3.
<b>Step 5: Find the probability associated with the z-score.</b>
We can use the  Normal CDF Calculator  to find that the area under the standard normal curve to the left of -1.3 is <b>.0968</b>.
Thus, the probability that a coin lands on heads less than or equal to 43 times during 100 flips is <b>.0968</b>.
This example illustrated the following:
We had a situation where a random variable followed a binomial distribution.
We wanted to find the probability of obtaining a certain value for this random variable.
Since the sample size (n = 100 trials) was sufficiently large, we were able to use the normal distribution to approximate the binomial distribution.
This is a complete example of how to use the normal approximation to find probabilities related to the binomial distribution.
<h2><span class="orange">How to Calculate & Plot the Normal CDF in Python</span></h2>
A cumulative distribution function (<b>CDF</b>) tells us the probability that a  random variable  takes on a value less than or equal to some value.
This tutorial explains how to calculate and plot values for the normal CDF in Python.
<h3>Example 1: Calculate Normal CDF Probabilities in Python</h3>
The easiest way to calculate normal CDF probabilities in Python is to use the <b>norm.cdf()</b> function from the  SciPy  library.
The following code shows how to calculate the probability that a random variable takes on a value less than 1.96 in a standard normal distribution:
<b>from scipy.stats import norm
#calculate probability that random value is less than 1.96 in normal CDF
norm.cdf(1.96)
0.9750021048517795
</b>
The probability that a random variables takes on a value less than 1.96 in a standard normal distribution is roughly <b>0.975</b>.
We can also find the probability that a random variable takes on a value greater than 1.96 by simply subtracting this value from 1:
<b>from scipy.stats import norm
#calculate probability that random value is greater than 1.96 in normal CDF
1 - norm.cdf(1.96)
0.024997895148220484
</b>
The probability that a random variables takes on a value greater than 1.96 in a standard normal distribution is roughly <b>0.025</b>.
<h3>Example 2: Plot the Normal CDF</h3>
The following code shows how to plot a normal CDF in Python:
<b>import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as ss
#define x and y values to use for CDF
x = np.linspace(-4, 4, 1000)
y = ss.norm.cdf(x)
#plot normal CDF
plt.plot(x, y)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/normcdf12.jpg">
The x-axis shows the values of a random variable that follows a standard normal distribution and the y-axis shows the probability that a random variable takes on a value less than the value shown on the x-axis.
For example, if we look at x = 1.96 then we’ll see that the cumulative probability that x is less than 1.96 is roughly <b>0.975</b>.
Feel free to modify the colors and the axis labels of the normal CDF plot as well:
<b>import matplotlib.pyplot as plt
import numpy as np
import scipy.stats as ss
#define x and y values to use for CDF
x = np.linspace(-4, 4, 1000)
y = ss.norm.cdf(x)
#plot normal CDF
plt.plot(x, y, color='red')
plt.title('Normal CDF')
plt.xlabel('x')
plt.ylabel('CDF')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/normcdf13.jpg"551">
<h2><span class="orange">How to Use the Normal CDF in R (With Examples)</span></h2>
You can use the following methods to work with the normal CDF (cumulative distribution function) in R:
<b>Method 1: Calculate Normal CDF Probabilities</b>
<b>#calculate probability that random value is less than 1.96 in normal CDF
pnorm(1.96)
#calculate probability that random value is greater than 1.96 in normal CDF
pnorm(1.96, lower.tail=FALSE)
</b>
<b>Method 2: Plot the Normal CDF</b>
<b>#define sequence of x-values
x &lt;- seq(-4, 4, .01)
#calculate normal CDF probabilities
prob &lt;- pnorm(x)
 
#plot normal CDF
plot(x, prob, type="l")</b>
The following examples show how to use these methods in practice.
<h3>
<b>Example 1: Calculate Normal CDF Probabilities </b>
</h3>
The following code shows how to calculate the probability that a random variable takes on a value less than 1.96 in a standard normal distribution:
<b>#calculate probability that random value is less than 1.96 in normal CDF
pnorm(1.96)
[1] 0.9750021
</b>
The probability that a random variables takes on a value less than 1.96 in a standard normal distribution is <b>0.975</b>.
We can also find the probability that a random variable takes on a value greater than 1.96 by using the <b>lower.tail</b> argument:
<b>#calculate probability that random value is greater than 1.96 in normal CDF
pnorm(1.96, lower.tail=FALSE)
[1] 0.0249979
</b>
And we can use the following syntax to find the probability that a random variable takes on a value between two values in a standard normal distribution:
<b>#calculate probability that random value takes on value between -1.96 and 1.96
pnorm(1.96) - pnorm(-1.96)
[1] 0.9500042
</b>
The probability that a random variable takes on a value between -1.96 and 1.96 in a standard normal distribution is <b>0.95</b>.
<h3>Example 2: Plot the Normal CDF</h3>
The following code shows how to plot a normal CDF:
<b>#define sequence of x-values
x &lt;- seq(-4, 4, .01)
#calculate normal CDF probabilities
prob &lt;- pnorm(x)
 
#plot normal CDF
plot(x, prob, type="l")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/normcdf1.png">
The x-axis shows the values of a random variable that follows a standard normal distribution and the y-axis shows the probability that a random variable takes on a value less than the value shown on the x-axis.
For example, if we look at x = 1.96 then we’ll see that the cumulative probability that x is less than 1.96 is roughly <b>0.975:</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/normalcdf4.png">
Note that you can modify the aesthetics of the normal CDF plot as well:
<b>#define sequence of x-values
x &lt;- seq(-4, 4, .01)
#calculate normal CDF probabilities
prob &lt;- pnorm(x)
 
#plot normal CDF
plot(x, prob, type='l', col='blue', lwd=2, main='Normal CDF', ylab='Cumulative Prob')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/normalcdf5.png">
<b>Related:</b>  How to Use seq Function in R 
<h2><span class="orange">Normal Distribution Dataset Generator</span></h2>
This tool automatically generates a normally distributed dataset based on a population mean and standard deviation.
To generate a normally distributed dataset, simply specify the values below and then click the “Generate” button.
<label for="n"><b>μ</b> (population mean)</label>
<input type="number" id="mean" value="0">
<label for="X"><b>σ</b> (population standard deviation)</label>
<input type="number" id="sd" value="1">
<label for="p"><b>n</b> (dataset size)</label>
<input type="number" id="n" min="0" value="15">
<input type="button" id="button" onclick="binomialCalc()" value="Generate">
<div>
Mean of dataset: 0.023
<div>
Standard deviation of dataset: 0.849
<textarea id="output_data">
-1.62
0.31
1.05
0.72
0.52
-0.77
0.62
0.95
0.14
-0.58
0.35
-0.04
0.28
0.15
-1.74
</textarea>
<script>
function binomialCalc() {
//get input values
var mean = document.getElementById('mean').value;
var sd = document.getElementById('sd').value;
var n = document.getElementById('n').value;
//define function to generate random variables
function gen_norm() {
    var u = 0, v = 0;
    while(u === 0) u = Math.random(); //Converting [0,1) to (0,1)
    while(v === 0) v = Math.random();
    return Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v );
 }
//fill array with values
var num = [];
for (i = 0; i < n; i++) {
num.push(parseFloat(gen_norm()*sd-(-1*mean)).toFixed(2))
}
//find mean and sd of values
var meanOut = math.mean(num);
var sdOut = math.std(num);
//output mean and sd
document.getElementById('meanOut').innerHTML = meanOut.toFixed(3);
document.getElementById('sdOut').innerHTML = sdOut.toFixed(3);
//output normally distributed data values
var textarea = document.getElementById("output_data");
textarea.value = num.join("\n");
}
</script>
<h2><span class="orange">How to Calculate Normal Distribution Probabilities in Excel</span></h2>
A  normal distribution  is the most commonly used distribution in all of statistics.
To calculate probabilities related to the normal distribution in Excel, you can use the  NORMDIST  function, which uses the following basic syntax:
<b>=NORMDIST(x, mean, standard_dev, cumulative)</b>
where:
<b>x</b>: The value of interest in the normal distribution
<b>mean</b>: The mean of the normal distribution
<b>standard_dev</b>: The standard deviation of the normal distribution
<b>cumulative</b>: Whether to calculate cumulative probabilities (this is usually TRUE)
The following examples show how to use this function to calculate probabilities related to the normal distribution.
<h3>Example 1: Calculate Probability Less than Some Value</h3>
Suppose the scores for an exam are normally distributed with a mean of 90 and a standard deviation of 10.
Find the probability that a randomly selected student receives a score less than 80.
The following screenshot shows how to use the <b>NORMDIST()</b> function in Excel to calculate this probability:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/norm11.jpg"515">
The probability that a randomly selected student receives a score less than 80 is <b>0.1587</b>.
<h3>Example 2: Calculate Probability Greater than Some Value</h3>
Suppose the scores for an exam are normally distributed with a mean of 90 and a standard deviation of 10.
Find the probability that a randomly selected student receives a score <em>greater</em> than 80.
To find this probability, we can simply do 1 – <b>NORMDIST()</b> in Excel as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/norm12.jpg"522">
The probability that a randomly selected student receives a score greater than 80 is <b>0.1587</b>.
<h3>Example 3: Calculate Probability Between Two Values</h3>
Suppose the scores for an exam are normally distributed with a mean of 90 and a standard deviation of 10.
Find the probability that a randomly selected student receives a score between 87 and 93.
To find this probability, we can subtract the larger value of <b>NORMDIST()</b> from the smaller value of another <b>NORMDIST()</b> in Excel as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/norm13.jpg"619">
The probability that a randomly selected student receives a score between 87 and 93 is <b>0.2358</b>.
<h2><span class="orange">Normal Distribution vs. Standard Normal Distribution: The Difference</span></h2>
The  normal distribution  is the most commonly used probability distribution in statistics.
It has the following properties:
 Symmetrical 
Bell-shaped
Mean and median are equal; both located at the center of the distribution
The mean of the normal distribution determines its location and the standard deviation determines its spread.
For example, the following plot shows three normal distributions with different means and standard deviations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardNorm2-1.png">
<b>The standard normal distribution is a specific type of normal distribution where the mean is equal to 0 and the standard deviation is equal to 1.</b>
The following plot shows a standard normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardNorm3.png">
<h3>How to Convert a Normal Distribution to Standard Normal Distribution</h3>
Any normal distribution can be converted into a standard normal distribution by converting the data values into z-scores, using the following formula:
<b>z = (x – μ) / σ</b>
where:
<b>x:</b> Individual data value
<b>μ:</b> Mean of the distribution
<b>σ:</b> Standard deviation of the distribution
For example, suppose we have the following dataset with a mean of 6 and a standard deviation of 2.152:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardNorm4.png">
We can convert each individual data value into a z-score by subtracting 6 from each value and dividing by 2.152:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardNorm5.png">
The <b>z-score</b> tells us how many standard deviations each data point lies from the mean. For example, the first data value of “3” lies 1.39 standard deviations below the mean.
The mean of this distribution of z-scores has a mean of zero and a standard deviation of one.
<h3>How to Use the Standard Normal Distribution</h3>
A standard normal distribution has the following properties:
About 68% of data falls within one standard deviation of the mean
About 95% of data falls within two standard deviations of the mean
About 99.7% of data falls within three standard deviations of the mean
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/10/normal_dist.png">
This is known as the <b>Empirical Rule</b> and is used to understand the distribution of values in a dataset.
For example, suppose the height of plants in a certain garden are normally distributed with a mean of 47.4 inches and a standard deviation of 2.4 inches.
<em>According to the Empirical Rule, what percentage of plants are less than 54.6 inches tall?</em>
The Empirical Rule states that for a given dataset with a normal distribution, 99.7% of data values fall within three standard deviations of the mean. This means that 49.85% of values fall between the mean and three standard deviations above the mean.
In this example, 54.6 is located three standard deviations above the mean. Since we know that 50% of data values fall below the mean in a normal distribution, a total of 50% + 49.85% = 99.85% of values fall below 54.6.
Thus, <b>99.85%</b> of plants are less than 54.6 inches tall. 
<h2><span class="orange">Normal Distribution vs. t-Distribution: What’s the Difference?</span></h2>
The  normal distribution  is the most commonly used distribution in all of statistics and is known for being symmetrical and bell-shaped.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/norm_vs_t1.png">
A closely related distribution is the <b>t-distribution</b>, which is also symmetrical and bell-shaped but it has heavier “tails” than the normal distribution.
That is, more values in the distribution are located in the tail ends than the center compared to the normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/norm_vs_t2.png">
In statistical jargon we use a metric called <b>kurtosis</b> to measure how “heavy-tailed” a distribution is. Thus, we would say that the kurtosis of a t-distribution is greater than a normal distribution.
In practice, we use the t-distribution most often when performing  hypothesis tests  or  constructing confidence intervals .
For example, the formula to calculate a confidence interval for a population mean is as follows:
<b>Confidence Interval = x  +/-  t<sub>1-α/2, n-1</sub>*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t: </b>the critical t-value, based on the significance level <em>α</em> and sample size <em>n</em>
<b>s: </b>sample standard deviation
<b>n: </b>sample size
In this formula we use the critical value from the  t table  instead of the critical value from the  z table  when either one of the following is true:
We do not know the population standard deviation.
The sample size is less than or equal to 30.
The following flow diagram provides a helpful way to know whether you should use the critical value from the t table or the z table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/z_table_vs_t_table.jpg">
The main difference between using the t-distribution compared to the normal distribution when constructing confidence intervals is that critical values from the t-distribution will be larger, which leads to <em>wider</em> confidence intervals.
For example, suppose we’d like to construct a 95% confidence interval for the mean weight for some population of turtles so we go out and collect a random sample of turtles with the following information:
Sample size <b>n = 25</b>
Sample mean weight <b>x = 300</b>
Sample standard deviation <b>s = 18.5</b>
The z-critical value for a 95% confidence level is <b>1.96</b> while a t-critical value for a 95% confidence interval with df = 25-1 = 24 degrees of freedom is <b>2.0639</b>.
Thus, a 95% confidence interval for the population mean using a z-critical value is:
95% C.I. = 300 +/- 1.96*(18.5/√<span>25) = <b>[ 292.75 , 307.25]</b>
While a 95% confidence interval for the population mean using a t-critical value is:
95% C.I. = 300 +/- 2.0639*(18.5/√25) = <b>[ 292.36 , 307.64]</b>
Notice that the confidence interval with the t-critical value is wider.
The idea here is that when we have small sample sizes, we’re less certain about the true population mean so it makes since to use the t-distribution to produce wider confidence intervals that have a higher chance of containing the true population mean.
<h3>Visualizing Degrees of Freedom for the t-Distribution</h3>
It’s worth noting that as the degrees of freedom increases, the t-distribution approaches the normal distribution.
To illustrate this, consider the following graph that shows the shape of the t-distribution with the following degrees of freedom:
df = 3
df = 10
df = 30
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/norm_vs_t3.png">
Beyond 30 degrees of freedom, the t-distribution and the normal distribution become so similar that the differences between using a t-critical value vs. a z-critical value in formulas becomes negligible.
<h2><span class="orange">How to Calculate Normal Probabilities on a TI-84 Calculator</span></h2>
The  normal distribution  is the most commonly used distributions in all of statistics. This tutorial explains how to use the following functions on a TI-84 calculator to find normal distribution probabilities:
<b>normalpdf(x, μ, σ) </b>returns the probability associated with the normal pdf where:
<b>x </b>= individual value
<b>μ </b>= population mean
<b>σ </b>= population standard deviation
<b>normalcdf(lower_x, upper_x, μ, σ) </b>returns the cumulative probability associated with the normal cdf between two values.
where:
<b>lower_x </b>= lower individual value
<b>upper_x </b>= upper individual value
<b>μ </b>= population mean
<b>σ </b>= population standard deviation
Both of these functions can be accessed on a TI-84 calculator by pressing 2nd and then pressing vars. This will take you to a <b>DISTR </b>screen where you can then use <b>normalpdf() </b>and <b>normalcdf()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/normalProbTI1.png">
The following examples illustrate how to use these functions to answer different questions.
<h3>Example 1: Normal probability greater than x</h3>
<b>Question: </b>For a normal distribution with mean = 40 and standard deviation = 6, find the probability that a value is greater than 45.
<b>Answer: </b>Use the function normalcdf(x, 10000, μ, σ):
<b>normalcdf(45, 10000, 40, 6) = 0.2023</b>
<em>Note: Since the function requires an upper_x value, we just use 10000.</em>
<h3>Example 2: Normal probability less than x</h3>
<b>Question: </b>For a normal distribution with mean = 100 and standard deviation = 11.3, find the probability that a value is less than 98.
<b>Answer: </b>Use the function normalcdf(-10000, x, μ, σ):
<b>normalcdf(-10000, 98, 100, 11.3) = 0.4298</b>
<em>Note: Since the function requires a lower_x value, we just use -10000.</em>
<h3>Example 3: Normal probability between two values</h3>
<b>Question: </b>For a normal distribution with mean = 50 and standard deviation = 4, find the probability that a value is between 48 and 52.
<b>Answer: </b>Use the function normalcdf(smaller_x, larger_x, μ, σ)
<b>normalcdf(48, 52, 50, 4) = 0.3829</b>
<h3>Example 4: Normal probability outside of two values</h3>
<b>Question: </b>For a normal distribution with mean = 22 and standard deviation = 4, find the probability that a value is less than 20 or greater than 24
<b>Answer: </b>Use the function normalcdf(-10000, smaller_x, μ, σ) + normalcdf(larger_x, 10000, μ, σ) 
<b>normalcdf(-10000, 20, 22, 4) + normalcdf(24, 10000, 22, 4) = 0.6171</b>
<h2><span class="orange">How to Create a Normal Probability Plot in Excel (Step-by-Step)</span></h2>
A <b>normal probability plot</b> can be used to determine if the values in a dataset are roughly  normally distributed .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel1.png">
This tutorial provides a step-by-step example of how to create a normal probability plot for a given dataset in Excel
<h3>Step 1: Create the Dataset</h3>
First, let’s create a fake dataset with 15 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel2.png">
<h3>Step 2: Calculate the Z-Values</h3>
Next, we’ll use the following formula to calculate the z-value that corresponds to the first data value:
<b>=NORM.S.INV((RANK(A2, $A$2:$A$16, 1)-0.5)/COUNT(A:A))
</b>
We’ll copy this formula down to each cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel3.png">
<h3>Step 3: Create the Normal Probability Plot</h3>
Next, we’ll create the normal probability plot.
First, highlight the cell range A2:B16 as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel4.png">
Along the top ribbon, click the <b>Insert</b> tab. Under the <b>Charts</b> section, click the first option under <b>Scatter</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/curveExcel3.png">
 This automatically produces the following chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel5.png">
The x-axis displays the ordered data values and the y-value displays their corresponding z-values.
Feel free to modify the title, axes, and labels to make the plot more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel1.png">
<h3>How to Interpret a Normal Probability Plot</h3>
The way to interpret a normal probability plot is simple: if the data values fall along a roughly straight line at a 45-degree angle, then the data is normally distributed.
In our plot above we can see that the values tend to deviate from a straight line at a 45-degree angle, especially on the tail ends. This likely indicates that the data is not normally distributed.
Although a normal probability plot isn’t a formal statistical test, it offers an easy way to visually check whether or not a data set is normally distributed.
If you’re looking for a formal normality test, read  this tutorial  on how to perform a normality test in Excel.
<h2><span class="orange">Normal vs. Uniform Distribution: What’s the Difference?</span></h2>
The  normal distribution  is the most commonly used probability distribution in statistics.
It has the following properties:
Symmetrical
Bell-shaped
If we create a plot of the normal distribution, it will look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/norm_unif1.png">
The  uniform distribution  is a probability distribution in which every value between an interval from <em>a</em> to <em>b</em> is equally likely to occur.
It has the following properties:
Symmetrical
Rectangular-shaped
If we create a plot of the uniform distribution, it will look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/norm_unif2.png">
The normal distribution and uniform distribution share the following <b>similarity</b>:
Both distributions are symmetrical. That is, if we were to draw a line down the center of the distribution, the left and right sides of the distribution would perfectly mirror each other:
However, the two distributions have the following <b>difference</b>:
The distributions have different shapes.
The normal distribution is bell-shaped, which means value near the center of the distribution are more likely to occur as opposed to values on the tails of the distribution.
The uniform distribution is rectangular-shaped, which means every value in the distribution is equally likely to occur.
<h3>Normal Distribution vs. Uniform Distribution: When to Use Each</h3>
The <b>normal distribution</b> is used to model phenomenon that tend to follow a “bell-curve” shape. For example, it’s well-documented that the birthweight of newborn babies is normally distributed with a mean of about 7.5 pounds.
The histogram of the birthweight of newborn babies in the U.S. displays a bell-shape that is typically of the normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/unimodal1-2.png">
Most babies are likely to weight around 7.5 pounds, with few weighing less than 7 pounds and few weighing more than 8 pounds.
Conversely, the <b>uniform distribution</b> is used to model scenarios where each potential outcome is equally likely. 
A classic example is rolling a die. If you roll a die one time, the probability that it falls on a number between 1 and 6 follows a uniform distribution because each number is equally likely to occur.
For example, there are 6 possible numbers the die can land on so the probability that you roll a 1 is 1/6.
Similarly, the probability that you roll a 2 is 1/6.
Similarly, the probability that you roll a 3 is 1/6.
And so on.
<h3>Bonus: How to Plot the Normal & Uniform Distribution</h3>
We used the following code in R to create plots of the normal and uniform distributions:
<b>#define x-axis
x &lt;- seq(-4, 4, length=100)
#calculate normal distribution probabilities
y &lt;- dnorm(x)
#plot normal distribution
plot(x, y, type = "l", lwd = 2)
#define x-axis
x &lt;- seq(-4, 4, length=100)
#calculate uniform distribution probabilities
y &lt;- dunif(x, min = -3, max = 3)
#plot uniform distribution
plot(x, y, type = "l", lwd = 2, xlim = c(-4, 4))
</b>
<h2><span class="orange">How to Calculate NormalCDF Probabilities in Excel</span></h2>
The  NormalCDF  function on a TI-83 or TI-84 calculator can be used to find the probability that a normally distributed  random variable  takes on a value in a certain range.
On a TI-83 or TI-84 calculator, this function uses the following syntax
<b>normalcdf(lower, upper, μ, σ)</b>
where:
<b>lower </b>= lower value of range
<b>upper </b>= upper value of range
<b>μ </b>= population mean
<b>σ </b>= population standard deviation
For example, suppose a random variable is  normally distributed  with a mean of 50 and a standard deviation of 4. The probability that a random variable takes on a value between 48 and 52 can be calculated as:
<b>normalcdf(48, 52, 50, 4) = 0.3829</b>
We can replicate this answer in Excel by using the <b>NORM.DIST()</b> function, which uses the following syntax:
<b>NORM.DIST(x, σ, μ, cumulative)</b>
where:
<b>x </b>= individual data value
<b>μ </b>= population mean
<b>σ </b>= population standard deviation
<b>cumulative = </b>FALSE calculate  the PDF; TRUE calculates the CDF 
The following examples show how to use this function in practice.
<h3>Example 1: Probability Between Two Values</h3>
Suppose a random variable is normally distributed with a mean of 50 and a standard deviation of 4. The probability that a random variable takes on a value <b>between</b> 48 and 52 can be calculated as:
<b>=NORM.DIST(52, 50, 4, TRUE) - NORM.DIST(48, 50, 4, TRUE)
</b>
The following image shows how to perform this calculation in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/normalcdf1.png">
The probability turns out to be <b>0.3829.</b>
<h3>Example 2: Probability Less Than One Value</h3>
Suppose a random variable is normally distributed with a mean of 50 and a standard deviation of 4. The probability that a random variable takes on a value <b>less than</b> 48 can be calculated as:
<b>=NORM.DIST(48, 50, 4, TRUE)
</b>
The following image shows how to perform this calculation in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/normalcdf2-1.png">
 
The probability turns out to be <b>0.3085.</b>
<h3>Example 3: Probability Greater Than One Value</h3>
Suppose a random variable is normally distributed with a mean of 50 and a standard deviation of 4. The probability that a random variable takes on a value <b>greater than</b> 55 can be calculated as:
<b>=1 - NORM.DIST(55, 50, 4, TRUE)
</b>
The following image shows how to perform this calculation in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/normalcdf3.png">
The probability turns out to be <b>0.1056.</b>
<h2><span class="orange">How to Perform a Normality Test in Excel (Step-by-Step)</span></h2>
Many statistical tests make the assumption that the values in a dataset are  normally distributed .
One of the easiest ways to test this assumption is to perform a <b>Jarque-Bera test</b>, which is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a normal distribution.
This test uses the following hypotheses:
H<sub>0</sub>: The data is normally distributed.
H<sub>A</sub>: The data is <em>not</em> normally distributed.
The test statistic <em><b>JB</b> </em>is defined as:
<em><b>JB</b> </em> =(n/6) * (S<sup>2</sup> + (C<sup>2</sup>/4))
where:
<b>n:</b> the number of  observations  in the sample
<b>S: </b>the sample skewness
<b>C:</b> the sample kurtosis
Under the null hypothesis of normality, <em>JB ~ </em>X<sup>2</sup>(2).
If the  p-value  that corresponds to the test statistic is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that the data is not normally distributed.
This tutorial provides a step-by-step example of how to perform a Jarque-Bera test for a given dataset in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset with 15 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/npp_excel2.png">
<h3>Step 2: Calculate the Test Statistic</h3>
Next, calculate the JB test statistic. Column E shows the formulas used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityExcel1.png">
The test statistic turns out to be <b>1.0175</b>.
<h3>Step 3: Calculate the P-Value</h3>
Under the null hypothesis of normality, the test statistic JB follows a Chi-Square distribution with 2 degrees of freedom.
So, to find the p-value for the test we will use the following function in Excel: <b>=CHISQ.DIST.RT(JB test statistic, 2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityExcel2.png">
The p-value of the test is <b>0.601244</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. We don’t have sufficient evidence to say that the dataset is not normally distributed.
In other words, we can assume that the data is normally distributed.
<h2><span class="orange">How to Perform a Normality Test in Google Sheets</span></h2>
Many statistical tests make the assumption that the values in a dataset are  normally distributed .
One of the easiest ways to test this assumption is to perform a <b>Jarque-Bera test</b>, which is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a normal distribution.
This test uses the following hypotheses:
<b>H<sub>0</sub></b>: The data is normally distributed.
<b>H<sub style="color: #000000;">A</sub></b>: The data is <em>not</em> normally distributed.
The test statistic <em><b>JB</b> </em>is defined as:
<em><b>JB</b> </em> =(n/6) * (S<sup>2</sup> + (C<sup>2</sup>/4))
where:
<b>n:</b> the number of  observations  in the sample
<b>S: </b>the sample skewness
<b>C:</b> the sample kurtosis
Under the null hypothesis of normality, <em>JB ~ </em>X<sup>2</sup>(2).
If the  p-value  that corresponds to the test statistic is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that the data is not normally distributed.
This tutorial provides a step-by-step example of how to perform a Jarque-Bera test for a given dataset in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s create a fake dataset with 15 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/normalitysheets1.jpg"466">
<h3>Step 2: Calculate the Test Statistic</h3>
Next, we will calculate the JB test statistic.
Column <b>E</b> shows the formulas used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/normalitysheets2.jpg"617">
The test statistic turns out to be <b>1.0175</b>.
<h3>Step 3: Calculate the P-Value</h3>
Under the null hypothesis of normality, the test statistic JB follows a Chi-Square distribution with 2 degrees of freedom.
So, to find the  p-value  for the test we will use the following formula:
<b>=CHISQ.DIST.RT(JB test statistic, 2)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/normalitysheets3.jpg">
The p-value of the test is <b>0.601244</b>.
Recall that this Jarque-Bera normality test uses the following hypotheses:
<b>H<sub>0</sub></b>: The data is normally distributed.
<b>H<sub style="color: #000000;">A</sub></b>: The data is <em>not</em> normally distributed.
Since this p-value is not less than 0.05, we fail to reject the null hypothesis.
This means we don’t have sufficient evidence to say that the dataset is not normally distributed.
In other words, we can assume that the data is normally distributed.
<h2><span class="orange">How to Test for Normality in Python (4 Methods)</span></h2>
Many statistical tests make the  assumption  that datasets are normally distributed. 
There are four common ways to check this assumption in Python:
<b>1. (Visual Method) Create a histogram.</b>
If the histogram is roughly “bell-shaped”, then the data is assumed to be normally distributed.
<b>2. (Visual Method) Create a Q-Q plot.</b>
If the points in the plot roughly fall along a straight diagonal line, then the data is assumed to be normally distributed.
<b>3. (Formal Statistical Test) Perform a Shapiro-Wilk Test.</b>
If the p-value of the test is greater than α = .05, then the data is assumed to be normally distributed.
<b>4. (Formal Statistical Test) Perform a Kolmogorov-Smirnov Test.</b>
If the p-value of the test is greater than α = .05, then the data is assumed to be normally distributed.
The following examples show how to use each of these methods in practice.
<h3>Method 1: Create a Histogram</h3>
The following code shows how to create a histogram for a dataset that follows a  log-normal distribution :
<b>import math
import numpy as np
from scipy.stats import lognorm
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#generate dataset that contains 1000 log-normal distributed values
lognorm_dataset = lognorm.rvs(s=.5, scale=math.exp(1), size=1000)
#create histogram to visualize values in dataset
plt.hist(lognorm_dataset, edgecolor='black', bins=20)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/normalitypython1.jpg"559">
By simply looking at this histogram, we can tell the dataset does not exhibit a “bell-shape” and is not normally distributed.
<h3>Method 2: Create a Q-Q plot</h3>
The following code shows how to create a Q-Q plot for a dataset that follows a log-normal distribution:
<b>import math
import numpy as np
from scipy.stats import lognorm
import statsmodels.api as sm
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#generate dataset that contains 1000 log-normal distributed values
lognorm_dataset = lognorm.rvs(s=.5, scale=math.exp(1), size=1000)
#create Q-Q plot with 45-degree line added to plot
fig = sm.qqplot(lognorm_dataset, line='45')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/normalitypython2.jpg"533">
If the points on the plot fall roughly along a straight diagonal line, then we typically assume a dataset is normally distributed.
However, the points on this plot clearly don’t fall along the red line, so we would not assume that this dataset is normally distributed.
This should make sense considering we generated the data using a log-normal distribution function.
<h3>Method 3: Perform a Shapiro-Wilk Test</h3>
The following code shows how to perform a Shapiro-Wilk for a dataset that follows a log-normal distribution:
<b>import math
import numpy as np
from scipy.stats import shapiro 
from scipy.stats import lognorm
#make this example reproducible
np.random.seed(1)
#generate dataset that contains 1000 log-normal distributed values
lognorm_dataset = lognorm.rvs(s=.5, scale=math.exp(1), size=1000)
#perform Shapiro-Wilk test for normality
shapiro(lognorm_dataset)
ShapiroResult(statistic=0.8573324680328369, pvalue=3.880663073872444e-29)
</b>
From the output we can see that the test statistic is <b>0.857 </b>and the corresponding p-value is <b>3.88e-29</b> (extremely close to zero).
Since the p-value is less than .05, we reject the null hypothesis of the Shapiro-Wilk test.
This means we have sufficient evidence to say that the sample data does not come from a normal distribution.
<h3>Method 4: Perform a Kolmogorov-Smirnov Test</h3>
The following code shows how to perform a Kolmogorov-Smirnov test for a dataset that follows a log-normal distribution:
<b>import math
import numpy as np
from scipy.stats import kstest
from scipy.stats import lognorm
#make this example reproducible
np.random.seed(1)
#generate dataset that contains 1000 log-normal distributed values
lognorm_dataset = lognorm.rvs(s=.5, scale=math.exp(1), size=1000)
#perform Kolmogorov-Smirnov test for normality
kstest(lognorm_dataset, 'norm')
KstestResult(statistic=0.84125708308077, pvalue=0.0)
</b>
From the output we can see that the test statistic is <b>0.841 </b>and the corresponding p-value is <b>0.0</b>.
Since the p-value is less than .05, we reject the null hypothesis of the Kolmogorov-Smirnov test.
This means we have sufficient evidence to say that the sample data does not come from a normal distribution.
<h3>How to Handle Non-Normal Data</h3>
If a given dataset is <em>not</em> normally distributed, we can often perform one of the following transformations to make it more normally distributed:
<b>1. Log Transformation: </b>Transform the values from x to <b>log(x)</b>.
<b>2. Square Root Transformation: </b>Transform the values from x to <b>√x</b>.
<b>3. Cube Root Transformation: </b>Transform the values from x to <b>x<sup>1/3</sup></b>.
By performing these transformations, the dataset typically becomes more normally distributed.
Read  this tutorial  to see how to perform these transformations in Python.
<h2><span class="orange">How to Test for Normality in SPSS</span></h2>
Many statistical tests require one or more variables to be normally distributed in order for the results of the test to be reliable.
This tutorial explains two different methods you can use to test for normality among variables in SPSS.
Each method will use the following dataset, which shows the average points per game scored by 20 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS1.png">
<h3>Method 1: Histograms</h3>
One way to see if a variable is normally distributed is to create a  histogram  to view the distribution of the variable. If the variable <em>is </em>normally distributed, the histogram should take on a “bell” shape with more values located near the center and fewer values located out on the tails.
To create a histogram for this basketball dataset, we can click on the <b>Graphs </b>tab, then <b>Chart Builder</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS2.png">
In the window that pops up, select <b>Histogram </b>in the <b>Choose from</b> list and drag it into the editing window. Then drag the variable <b>points</b> into the x-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS3.png">
Once you click <b>OK</b>, the following histogram will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/histSPSS4.png">
We can see that the variable points isn’t perfectly normally distributed, but it does follow roughly a bell shape with most players scoring between 10 and 20 points per game and fewer players scoring outside of this amount.
Although this isn’t a formal way to test for normality, it gives us a quick way to visualize the distribution of a variable and gives us a rough idea of whether or not the distribution is bell shaped.
<h3>Method 2: Formal Statistical Tests</h3>
We can also use formal statistical tests to determine whether or not a variable follows a normal distribution. SPSS offers the following tests for normality:
Shapiro-Wilk Test
Kolmogorov-Smirnov Test
The null hypothesis for each test is that a given variable is normally distributed. If the p-value of the test is less than some significance level (common choices include 0.01, 0.05, and 0.10), then we can reject the null hypothesis and conclude that there is sufficient evidence to say that the variable is not normally distributed.
To perform both of these tests in SPSS simultaneously, click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then<b> Explore</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/normalitySPSS1.png">
In the new window that pops up, drag the variable <b>points </b>into the box labelled Dependent List. Then click <b>Plots </b>and make sure the box next to <b>Normality plots with tests </b>is selected. Then click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/normalitySPSS2.png">
Once you click <b>OK</b>, the results of the normality tests will be shown in the following box:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/normalitySPSS3.png">
The test statistic and corresponding p-value for each test are shown:
<b>Kolmogorov Smirnov Test:</b>
Test statistic: <b>.113</b>
p-value: <b>.200</b>
<b>Shapiro-Wilk Test:</b>
Test statistic: <b>.967</b>
p-value: <b>.699</b>
The p-values for both tests are not less than 0.05, which means we do not have sufficient evidence to say the variable <b>points </b>is not normally distributed.
If we wanted to perform some statistical test that assumes variables are normally distributed, we would know that the variable <b>points </b>satisfies this assumption.
<h2><span class="orange">How to Test for Normality in Stata</span></h2>
Many statistical tests require one or more variables to be  normally distributed  in order for the results of the test to be reliable.
This tutorial explains several methods you can use to test for normality among variables in Stata.
For each of these methods, we will use the built-in Stata dataset called <em>auto</em>. You can load this dataset using the following command:
<b>sysuse auto</b>
<h3>Method 1: Histograms</h3>
One informal way to see if a variable is normally distributed is to create a  histogram  to view the distribution of the variable.
If the variable <em>is </em>normally distributed, the histogram should take on a “bell” shape with more values located near the center and fewer values located out on the tails.
We can use the <b>hist </b>command to create a histogram for the variable <em>displacement</em>:
<b>hist displacement</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata1.png">
We can add a normal  density curve  to a histogram by using the <b>normal </b>command:
<b>hist displacement, normal</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata2.png">
It’s pretty obvious that the variable <em>displacement </em>is skewed to the right (e.g. most values are concentrated on the left and a long “tail” of values extends to the right) and does not follow a normal distribution.
<b>Related:</b>  Left Skewed vs. Right Skewed Distributions 
<h3>Method 2: Shapiro-Wilk Test</h3>
A formal way to test for normality is to use the <b>Shapiro-Wilk Test</b>.
The null hypothesis for this test is that the variable is normally distributed. If the  p-value  of the test is less than some significance level (common choices include 0.01, 0.05, and 0.10), then we can reject the null hypothesis and conclude that there is sufficient evidence to say that the variable is not normally distributed.
<em>*This test can be used when the total number of observations is between 4 and 2,000.</em>
We can use the the <b>swilk </b>command to perform a Shapiro-Wilk Test on the variable <em>displacement</em>:
<b>swilk displacement</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata3.png">
Here is how to interpret the output of the test:
<b>Obs: 74. </b>This is the number of observations used in the test.
<b>W: 0.92542. </b>This is the test statistic for the test.
<b>Prob>z: 0.00031. </b>This is the p-value associated with the test statistic.
Since the p-value is less than 0.05, we can reject the null hypothesis of the test. We have sufficient evidence to say that the variable <em>displacement </em>is not normally distributed.
We can also perform the Shapiro-Wilk Test on more than one variable at once by listing several variables after the <b>swilk </b>command:
<b>swilk displacement mpg length</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata4.png">
Using a 0.05 significance level, we would conclude that <em>displacement </em>and <em>mpg </em>are both non-normally distributed, but we don’t have sufficient evidence to say that <em>length </em>is non-normally distributed.
<h3>Method 3: Shapiro-Francia Test</h3>
Another formal way to test for normality is to use the <b>Shapiro-Francia Test</b>.
The null hypothesis for this test is that the variable is normally distributed. If the p-value of the test is less than some significance level, then we can reject the null hypothesis and conclude that there is sufficient evidence to say that the variable is not normally distributed.
<em>*This test can be used when the total number of observations is between 10 and 5,000.</em>
We can use the the <b>sfrancia </b>command to perform a Shapiro-Wilk Test on the variable <em>displacement</em>:
<b>sfrancia displacement</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata5.png">
Here is how to interpret the output of the test:
<b>Obs: 74. </b>This is the number of observations used in the test.
<b>W’: 0.93011. </b>This is the test statistic for the test.
<b>Prob>z: 0.00094. </b>This is the p-value associated with the test statistic.
Since the p-value is less than 0.05, we can reject the null hypothesis of the test. We have sufficient evidence to say that the variable <em>displacement </em>is not normally distributed.
Similar to the Shapiro-Wilk Test, you can perform the Shapiro-Francia Test on more than one variable at once by listing several variables after the <b>sfrancia </b>command.
<h3>Method 4: Skewness and Kurtosis Test</h3>
Another way to test for normality is to use the <b>Skewness and Kurtosis Test</b>, which determines whether or not the skewness and kurtosis of a variable is consistent with the normal distribution.
The null hypothesis for this test is that the variable is normally distributed. If the p-value of the test is less than some significance level, then we can reject the null hypothesis and conclude that there is sufficient evidence to say that the variable is not normally distributed.
<em>*This test requires a minimum of 8 observations to be used.</em>
We can use the the <b>sktest </b>command to perform a Skewness and Kurtosis Test on the variable <em>displacement</em>:
<b>sktest displacement</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalityTestStata6.png">
Here is how to interpret the output of the test:
<b>Obs: 74. </b>This is the number of observations used in the test.
<b>adj chi(2): 5.81. </b>This is the Chi-Square test statistic for the test.
<b>Prob>chi2: 0.0547. </b>This is the p-value associated with the test statistic.
Since the p-value is not less than 0.05, we fail to reject the null hypothesis of the test. We don’t have sufficient evidence to say that <em>displacement </em>is not normally distributed.
Similar to the other normality tests, you can perform the Skewness and Kurtosis Test on more than one variable at once by listing several variables after the <b>sktest </b>command.
<h2><span class="orange">Normalization Calculator</span></h2>
}
svg:not(:root) {
  overflow: visible;
}
</style>
We can <b>normalize</b> values in a dataset by subtracting the mean and then dividing by the standard deviation. This is also known as converting data values into <i>z-scores</i>.
To normalize the values in a given dataset, enter your comma separated data in the box below, then click the “Normalize” button:
<textarea id="input_data" name="x" rows="5" cols="40">4, 14, 16, 22, 24, 25, 37, 38, 38, 40, 41, 41, 43, 44</textarea>
<input type="button" id="button" onclick="calc()" value="Normalize">
<b>Explanation:</b> The mean value of the dataset is 4. The standard deviation is 4. Thus, to find the normalized value for each value in the dataset, we subtract by 4 and divide by 4.
<script>
function calc() {
//remove current table if one exists
var element = document.getElementsByTagName('table')[0];
    if(element) {element.parentNode.removeChild(element)}
//remove current explanation if one exists
var div_table = document.getElementById('explanation');
div_table.style.display = 'block';
var input_data = document.getElementById('input_data').value.split(',').map(Number);
//find mean, sd, normalized data
var mean = math.mean(input_data);
var sd = jStat(input_data).stdev(true);
var norm = input_data.map(function(x) { return ((x-mean)/sd).toFixed(3); });
var size = input_data.length;
//generate table of frequencies
var table = document.createElement('table');
    function boldHTML(text) {
  var element = document.createElement("b");
  element.innerHTML = text;
  return element;
}
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    td1.appendChild(boldHTML('Raw Data'));
    td2.appendChild(boldHTML('Normalized Data'));
    tr.appendChild(td1);
    tr.appendChild(td2);
    table.appendChild(tr);
for (var i = 0; i < size; i++){
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    var text1 = document.createTextNode((input_data)[i]);
    var text2 = document.createTextNode(norm[i]);
    td1.appendChild(text1);
    td2.appendChild(text2);
    tr.appendChild(td1);
    tr.appendChild(td2);
    table.appendChild(tr);
}
document.getElementById('table_output').appendChild(table);
//output results
document.getElementById('mean').innerHTML = mean.toFixed(4);
document.getElementById('mean2').innerHTML = mean.toFixed(4);
document.getElementById('sd').innerHTML = sd.toFixed(4);
document.getElementById('sd2').innerHTML = sd.toFixed(4);
  
} //end calc function
</script>
<h2><span class="orange">How to Normalize Columns in a Pandas DataFrame</span></h2>
Often you may want to <b>normalize </b>the data values of one or more columns in a pandas DataFrame.
This tutorial explains two ways to do so:
<b>1. Min-Max Normalization</b>
<b>Objective: </b>Converts each data value to a value between 0 and 1.
<b>Formula: </b>New value = (value – min) / (max – min)
<b>2. Mean Normalization</b>
<b>Objective: </b>Scales values such that the mean of all values is 0 and std. dev. is 1. 
<b>Formula: </b>New value = (value – mean) / (standard deviation)
Let’s check out an example of how to use each method on a pandas DataFrame.
<h3>Example 1: Min-Max Normalization</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]}) 
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
</b>
We can use the following code to apply a min-max normalization to each column in the DataFrame:
<b>(df-df.min())/(df.max()-df.min())
        points        assists        rebounds
01.0000000.0000001.0
10.0000000.2857140.4
20.2307690.2857140.8
30.1538460.5714290.0
40.5384621.0000000.0
</b>
The max value in each column is now equal to <b>1 </b>and the min value in each column is now equal to <b>0</b>, with all other values ranging between 0 and 1.
<h3>Example 2: Mean Normalization</h3>
Once again suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]}) 
#view DataFrame
print(df)
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
</b>
We can use the following code to apply a mean normalization to each column in the DataFrame:
<b>(df-df.mean())/df.std()
        points        assists   rebounds
01.554057-1.1338931.227881
1-0.971286-0.377964-0.087706
2-0.388514-0.3779640.789352
3-0.5827720.377964-0.964764
40.3885141.511858-0.964764
</b>
The values in each column are now normalized such that the mean of the values in each column is 0 and the standard deviation of values in each column is 1.
If a particular data point has a normalized value greater than 0, it’s an indication that the data point is greater than the mean of its column. Conversely, a normalized value less than 0 is an indication that the data point is less than the mean of its column.
<h2><span class="orange">How to Normalize Data Between 0 and 1</span></h2>
To normalize the values in a dataset to be between 0 and 1, you can use the following formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x))</b>
where:
<b>z<sub>i</sub>:</b> The i<sup>th</sup> normalized value in the dataset
<b>x<sub>i</sub>: </b>The i<sup>th</sup> value in the dataset
<b>min(x)</b>: The minimum value in the dataset
<b>max(x):</b> The maximum value in the dataset
For example, suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalize01.png">
The minimum value in the dataset is 13 and the maximum value is 71.
To normalize the first value of <b>13</b>, we would apply the formula shared earlier:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) </b> = (13 – 13) / (71 – 13) = <b>0</b>
To normalize the second value of <b>16</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x))</b> = (16 – 13) / (71 – 13) = <b>.0517</b>
To normalize the third value of <b>19</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x))</b> = (19 – 13) / (71 – 13) = <b>.1034</b>
We can use this exact same formula to normalize each value in the original dataset to be between 0 and 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalize01-1.png">
Using this normalization method, the following statements will always be true:
The normalized value for the minimum value in the dataset will always be 0.
The normalized value for the maximum value in the dataset will always be 1.
The normalized values for all other values in the dataset will be between 0 and 1.
<h3>When to Normalize Data</h3>
Often we normalize variables when performing some type of analysis in which we have multiple variables that are measured on different scales and we want each of the variables to have the same range.
This prevents one variable from being overly influential, especially if it’s measured in different units (i.e. if one variable is measured in inches and another is measured in yards).
It’s also worth noting that we used a method known as <b>min-max normalization </b>in this tutorial to normalize the data values.
The two most common normalization methods are as follows:
<b>1. Min-Max Normalization</b>
<b>Objective: </b>Converts each data value to a value between 0 and 100.
<b>Formula: </b>New value = (value – min) / (max – min) * 100
<b>2. Mean Normalization</b>
<b>Objective: </b>Scales values such that the mean of all values is 0 and std. dev. is 1. 
<b>Formula: </b>New value = (value – mean) / (standard deviation)
<h2><span class="orange">How to Normalize Data Between 0 and 100</span></h2>
To normalize the values in a dataset to be between 0 and 100, you can use the following formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 100</b>
where:
<b>z<sub>i</sub>:</b> The i<sup>th</sup> normalized value in the dataset
<b>x<sub>i</sub>: </b>The i<sup>th</sup> value in the dataset
<b>min(x)</b>: The minimum value in the dataset
<b>max(x):</b> The maximum value in the dataset
For example, suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/normalize1.png">
The minimum value in the dataset is 12 and the maximum value is 68.
To normalize the first value of <b>12</b>, we would apply the formula shared earlier:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 100</b> = (12 – 12) / (68 – 12) * 100 = <b>0</b>
To normalize the second value of <b>19</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 100</b> = (19 – 12) / (68 – 12) * 100 = <b>12.5</b>
To normalize the third value of <b>21</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 100</b> = (21 – 12) / (68 – 12) * 100 = <b>16.07</b>
We can use this exact same formula to normalize each value in the original dataset to be between 0 and 100:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/normalize2.png">
<h3>How to Normalize Data Between Any Range</h3>
We can actually use this formula to normalize a dataset between 0 and any number:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * Q</b>
where <b>Q</b> is the maximum number you want for your normalized data values.
In the previous example we chose Q to be equal to 100, but we could easily normalize a range of data values between 0 and 1,000 by choosing Q to be 1,000:
To normalize the first value of <b>12</b>, we would apply the formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 1,000</b> = (12 – 12) / (68 – 12) * 100 = <b>0</b>
To normalize the second value of <b>19</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 1,000</b> = (19 – 12) / (68 – 12) * 100 = <b>125</b>
To normalize the third value of <b>21</b>, we would use the same formula:
<b>z<sub>i</sub> = (x<sub>i</sub> – min(x)) / (max(x) – min(x)) * 1,000</b> = (21 – 12) / (68 – 12) * 100 = <b>160.7</b>
We can use this exact same formula to normalize each value in the original dataset to be between 0 and 1,000:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/normalize3.png">
<h3>When to Normalize Data</h3>
Occasionally we normalize variables when performing some type of analysis in which we have multiple variables that are measured on different scales and we want each of the variables to have the same range.
This prevents one variable from being overly influential, especially if it’s measured in different units (i.e. if one variable is measured in inches and another is measured in yards).
It’s also worth noting that we used a method known as <b>min-max normalization </b>in this tutorial to normalize the data values.
The two most common normalization methods are as follows:
<b>1. Min-Max Normalization</b>
<b>Objective: </b>Converts each data value to a value between 0 and 100.
<b>Formula: </b>New value = (value – min) / (max – min) * 100
<b>2. Mean Normalization</b>
<b>Objective: </b>Scales values such that the mean of all values is 0 and std. dev. is 1. 
<b>Formula: </b>New value = (value – mean) / (standard deviation)
<h2><span class="orange">How to Normalize Data in Excel</span></h2>
To “normalize” a set of data values means to scale the values such that the mean of all of the values is 0 and the standard deviation is 1. 
This tutorial explains how to normalize data in Excel.
<h3>Example: How to Normalize Data in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel1.png">
Perform the following steps to normalize this set of data values.
<b>Step 1: Find the mean.</b>
First, we will use the <b>=AVERAGE(range of values) </b>function to find the mean of the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel2.png">
<b>Step 2: Find the standard deviation.</b>
Next, we will use the <b>=STDEV(range of values) </b>function to find the standard deviation of the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel3.png">
<b>Step 3: Normalize the values.</b>
Lastly, we will use the <b>STANDARDIZE(x, mean, standard_dev) </b>function to normalize each of the values in the dataset.
<b>NOTE:</b>
 
The <b>STANDARDIZE </b>function uses the following formula to normalize a given data value:
 
Normalized value = (x – x) / s
 
where:
x = data value
x = mean of dataset
s = standard deviation of dataset
The following image shows the formula used to normalize the first value in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel4.png">
Once we normalize the first value in cell B2, we can hover the mouse over the bottom right corner of cell B2 until a small <b>+ </b>appears. Double click the <b>+ </b>to copy the formula down to the remaining cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel5.png">
Now every value in the dataset is normalized.
<h3>How to Interpret Normalized Data</h3>
The formula that we used to normalize a given data value, x, was as follows:
Normalized value = (x – x) / s
where:
x = data value
x = mean of dataset
s = standard deviation of dataset
If a particular data point has a normalized value greater than 0, it’s an indication that the data point is greater than the mean. Conversely, a normalized value less than 0 is an indication that the data point is less than the mean.
In particular, the normalized value tells us how many standard deviations the original data point is from the mean. For example, consider the data point “12” in our original dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/normalizeExcel6.png">
The normalized value for “12” turned out to be -1.288, which was calculated as:
Normalized value = (x – x) / s = (12 – 22.267) / 7.968 = <b>-1.288</b>
This tells us that the value “12” is <b>1.288 standard deviations below the mean </b>in the original dataset.
Each of the normalized values in the dataset can help us understand how close or far a particular data value is from the mean. A small normalized value indicates that a value is close to the mean while a large normalized value indicates that a value is far from the mean.
<h2><span class="orange">How to Normalize Data in Google Sheets</span></h2>
To <b>normalize</b> a set of data values means to scale the values such that the mean of all of the values is 0 and the standard deviation is 1. 
This tutorial explains how to normalize data in Google Sheets.
<h3>Example: How to Normalize Data in Google Sheets</h3>
Suppose we have the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets1.png">
 
Use following steps to normalize this set of data values.
<b>Step 1: Calculate the mean.</b>
First, we will use the <b>=AVERAGE(range of values) </b>function to find the mean of the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets2.png">
<b>Step 2: Find the standard deviation.</b>
Next, we will use the <b>=STDEV(range of values) </b>function to find the standard deviation of the dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets3.png">
<b>Step 3: Normalize the values.</b>
Lastly, we will use the <b>STANDARDIZE(x, mean, standard_dev) </b>function to normalize each of the values in the dataset.
<b>NOTE:</b>
 
The <b>STANDARDIZE </b>function uses the following formula to normalize a given data value:
 
Normalized value = (x – x) / s
 
where:
x = data value
x = mean of dataset
s = standard deviation of dataset
The following image shows the formula used to normalize the first value in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets4-1.png">
Once we normalize the first value in cell B2, we can hover the mouse over the bottom right corner of cell B2 until a small <b>+ </b>appears. Double click the <b>+ </b>to copy the formula down to the remaining cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets5.png">
Now every value in the dataset is normalized.
<h3>How to Interpret Normalized Data</h3>
The formula that we used to normalize a given data value, x, was as follows:
Normalized value = (x – x) / s
where:
x = data value
x = mean of dataset
s = standard deviation of dataset
If a particular data point has a normalized value greater than 0, it means that the data point is greater than the mean. Conversely, a normalized value less than 0 is an indication that the data point is less than the mean.
In particular, the normalized value tells us how many standard deviations the original data point is from the mean. For example, consider the data point “12” in our original dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/normalizeSheets5.png">
The normalized value for “12” turned out to be -1.288, which was calculated as:
Normalized value = (x – x) / s = (12 – 22.267) / 7.968 = <b>-1.288</b>
This tells us that the value “12” is <b>1.288 standard deviations below the mean </b>in the original dataset.
Each of the normalized values in the dataset can help us understand how close or far a particular data value is from the mean. A small normalized value indicates that a value is close to the mean while a large normalized value indicates that a value is far from the mean.
<h2><span class="orange">How to Normalize Data in Python</span></h2>
Often in statistics and machine learning, we <b>normalize</b> variables such that the range of the values is between 0 and 1.
The most common reason to normalize variables is when we conduct some type of multivariate analysis (i.e. we want to understand the relationship between several predictor variables and a response variable) and we want each variable to contribute equally to the analysis.
When variables are measured at different scales, they often do not contribute equally to the analysis. For example, if the values of one variable range from 0 to 100,000 and the values of another variable range from 0 to 100, the variable with the larger range will be given a larger weight in the analysis.
By normalizing the variables, we can be sure that each variable contributes equally to the analysis.
To normalize the values to be between 0 and 1, we can use the following formula:
<b>x<sub>norm</sub> = (x<sub>i</sub> – x<sub>min</sub>) / (x<sub>max</sub> – x<sub>min</sub>)</b>
where:
<b>x<sub>norm</sub>:</b> The i<sup>th</sup> normalized value in the dataset
<b>x<sub>i</sub>: </b>The i<sup>th</sup> value in the dataset
<b>x<sub>max</sub></b>: The minimum value in the dataset
<b>x<sub>min</sub>:</b> The maximum value in the dataset
The following examples show how to normalize one or more variables in Python.
<h3>Example 1: Normalize a NumPy Array</h3>
The following code shows how to normalize all values in a NumPy array:
<b>import numpy as np 
#create NumPy array
data = np.array([[13, 16, 19, 22, 23, 38, 47, 56, 58, 63, 65, 70, 71]])
#normalize all values in array
data_norm = (data - data.min())/ (data.max() - data.min())
#view normalized values
data_norm
array([[0.        , 0.05172414, 0.10344828, 0.15517241, 0.17241379,
        0.43103448, 0.5862069 , 0.74137931, 0.77586207, 0.86206897,
        0.89655172, 0.98275862, 1.        ]])</b>
Each of the values in the normalized array are now between 0 and 1.
<h3>Example 2: Normalize All Variables in Pandas DataFrame</h3>
 The following code shows how to normalize all variables in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#normalize values in every column
df_norm = (df-df.min())/ (df.max() - df.min())
#view normalized DataFrame
df_norm
        points        assists rebounds
00.7647060.125 0.857143
10.0000000.375 0.428571
20.1764710.375 0.714286
30.1176470.625 0.142857
40.4117651.000 0.142857
50.6470590.625 0.000000
60.7647060.625 0.571429
71.0000000.000 1.000000
</b>
Each of the values in every column are now between 0 and1.
<h3>Example 3: Normalize Specific Variables in Pandas DataFrame</h3>
The following code shows how to normalize a specific variables in a pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
define columns to normalize
x = df.iloc[:,0:2]
#normalize values in first two columns only 
df.iloc[:,0:2] = (x-x.min())/ (x.max() - x.min())
#view normalized DataFrame 
df
points        assists rebounds
00.7647060.125 11
10.0000000.375 8
20.1764710.375 10
30.1176470.625 6
40.4117651.000 6
50.6470590.625 5
60.7647060.625 9
71.0000000.000 12
</b>
Notice that just the values in the first two columns are normalized.
<h2><span class="orange">How to Normalize Data in SAS</span></h2>
To “normalize” a set of data values means to scale the values such that the mean of all of the values is 0 and the standard deviation is 1. 
This tutorial explains how to normalize data in SAS.
<h3>Example: How to Normalize Data in SAS</h3>
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/norm1.jpg"79">
Perform the following steps to normalize this set of data values in SAS.
<b>Step 1: Create the Dataset</b>
First, let’s use the following code to create the dataset in SAS:
<b>/*create dataset*/
data original_data;
    input values;
    datalines;
12
14
15
15
16
17
18
20
24
25
26
29
32
34
37
;
run;
/*view mean and standard deviation of dataset*/
proc means data=original_data Mean StdDev ndec=3; 
   var values;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/norm3.jpg"191">
From the output we can see that the mean of the dataset is <b>22.267</b> and the standard deviation is <b>7.968</b>.
<b>Step 2: Normalize the Dataset</b>
Next, we’ll use <b>proc stdize</b> to normalize the dataset:
<b>/*normalize the dataset*/
proc stdize data=original_data out=normalized_data;
   var values;
run;
/*print normalized dataset*/
proc print data=normalized_data;
 
/*view mean and standard deviation of normalized dataset*/
proc means data=normalized_data Mean StdDev ndec=2; 
   var values;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/norm2.jpg"168">
From the output we can see that the mean of the normalized dataset is <b>0</b> and the standard deviation is <b>1</b>.
<b>Step 3: Interpret the Normalized Data</b>
SAS used the following formula to normalize the data values:
Normalized value = (x – x) / s
where:
x = data value
x = mean of dataset
s = standard deviation of dataset
Each normalized value tells us how many standard deviations the original data value was from the mean.
For example, consider the data point “12” in our original dataset. The original sample mean was 22.267 and the original sample standard deviation was 7.968.
The normalized value for “12” turned out to be -1.288, which was calculated as:
Normalized value = (x – x) / s = (12 – 22.267) / 7.968 = <b>-1.288</b>
This tells us that the value “12” is <b>1.288 standard deviations below the mean </b>in the original dataset.
Each of the normalized values in the dataset can help us understand how close or far a particular data value is from the mean.
A small normalized value indicates that a value is close to the mean while a large normalized value indicates that a value is far from the mean.
<h2><span class="orange">How to Use “NOT IN” Operator in R (With Examples)</span></h2>
You can use the following basic syntax to select all elements that are not in a list of values in R:
<b>!(data %in% c(value1, value2, value3, ...))
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: How to Use “NOT IN” with Vectors</h3>
The following code shows how to select all values in a vector in R that are not in a certain list of values:
<b>#define numeric vector
num_data &lt;- c(1, 2, 3, 3, 4, 4, 5, 5, 6)
#display all values in vector not equal to 3 or 4
num_data[!(num_data %in% c(3, 4))]
[1] 1 2 5 5 6
</b>
All values that are <b>not equal</b> to 3 or 4 are shown in the output.
Note that we can use the same syntax to select all elements in a vector that are not in a certain list of characters:
<b>#define vector of character data
char_data &lt;- c('A', 'A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'D')
#display all elements in vector not equal to 'A', or 'C'
char_data[!(char_data %in% c('A', 'C'))]
[1] "B" "B" "D" "D" "D"
</b>
All values that are <b>not equal</b> to ‘A’ or ‘C’ are shown in the output.
<h3>Example 2: How to Use “NOT IN” with Data Frames</h3>
The following code shows how to select all rows in a data frame in R in which a certain column is not equal to certain values:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'C', 'C', 'D'), points=c(77, 81, 89, 83, 99, 92, 97), assists=c(19, 22, 29, 15, 32, 39, 14))
#view data frame
df
  team points assists
1    A     77      19
2    A     81      22
3    B     89      29
4    B     83      15
5    C     99      32
6    C     92      39
7    D     97      14
#select all rows where team is not equal to 'A' or 'B'
subset(df, !(team %in% c('A', 'B')))
  team points assists
5    C     99      32
6    C     92      39
7    D     97      14</b>
Notice that all rows that do not have an ‘A’ or ‘B’ in the team column are returned.
We can also use similar syntax to select all rows in which a certain column is not equal to certain numeric values:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'C', 'C', 'D'), points=c(77, 81, 89, 83, 99, 92, 97), assists=c(19, 22, 29, 15, 32, 39, 14))
#view data frame
df
  team points assists
1    A     77      19
2    A     81      22
3    B     89      29
4    B     83      15
5    C     99      32
6    C     92      39
7    D     97      14
#select all rows where team is not equal to 'A' or 'B'
subset(df, !(points %in% c(89, 99)))
  team points assists
1    A     77      19
2    A     81      22
4    B     83      15
6    C     92      39
7    D     97      14</b>
Notice that all rows that are not equal to 89 or 99 in the points column are returned.
<h2><span class="orange">Pandas: How to Use Equivalent of np.where()</span></h2>
You can use the NumPy  where()  function to quickly update the values in a NumPy array using if-else logic.
For example, the following code shows how to update the values in a NumPy array that meet a certain condition:
<b>import numpy as np
#create NumPy array of values
x = np.array([1, 3, 3, 6, 7, 9])
#update valuesin array based on condition
x = np.where((x &lt; 5) | (x > 8), x/2, x)
#view updated array
x
array([0.5, 1.5, 1.5, 6. , 7. , 4.5])
</b>
<b>If</b> a given value in the array was less than 5 or greater than 8, we divided the value by 2.
<b>Else</b>, we left the value unchanged.
We can perform a similar operation in a pandas DataFrame by using the pandas  where()  function, but the syntax is slightly different.
Here’s the basic syntax using the NumPy where() function:
<b>x = np.where(condition, value_if_true, value_if_false)
</b>
And here’s the basic syntax using the pandas where() function:
<b>df['col'] = (value_if_false).where(condition, value_if_true)</b>
The following example shows how to use the pandas where() function in practice.
<h2>Example: The Equivalent of np.where() in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'A': [18, 22, 19, 14, 14, 11, 20, 28],   'B': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
    A   B
0  18   5
1  22   7
2  19   7
3  14   9
4  14  12
5  11   9
6  20   9
7  28   4</b>
We can use the following pandas <b>where()</b> function to update the values in column A based on a specific condition:
<b>#update values in column A based on condition
df['A'] = (df['A'] / 2).where(df['A'] &lt; 20, df['A'] * 2)
#view updated DataFrame
print(df)
      A   B
0   9.0   5
1  44.0   7
2   9.5   7
3   7.0   9
4   7.0  12
5   5.5   9
6  40.0   9
7  56.0   4
</b>
<b>If</b> a given value in column A was less than 20, we multiplied the value by 2.
<b>Else</b>, we divided the value by 2.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 Pandas: How to Count Values in Column with Condition 
 Pandas: How to Drop Rows in DataFrame Based on Condition 
 Pandas: How to Replace Values in Column Based on Condition 
<h2><span class="orange">How to Use nrow Function in R (With Examples)</span></h2>
You can use the <b>nrow()</b> function in R to count the number of rows in a data frame:
<b>#count number of rows in data frame
nrow(df) </b>
The following examples show how to use this function in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(x=c(1, 2, 3, 3, 5, NA), y=c(8, 14, NA, 25, 29, NA)) 
#view data frame
df
   x  y
1  1  8
2  2 14
3  3 NA
4  3 25
5  5 29
6 NA NA
</b>
<h3>Example 1: Count Rows in Data Frame</h3>
The following code shows how to count the total number of rows in the data frame:
<b>#count total rows in data frame
nrow(df)
[1] 6
</b>
There are <b>6</b> total rows.
<h3>Example 2: Count Rows with Condition in Data Frame</h3>
The following code shows how to count the number of rows where the value in the ‘x’ column is greater than 3 and is not blank:
<b>#count total rows in data frame where 'x' is greater than 3 and not blank
nrow(df[df$x>3 & !is.na(df$x), ])
[1] 1</b>
There is <b>1</b> row in the data frame that satisfies this condition.
<h3>Example 3: Count Rows with no Missing Values</h3>
The following code shows how to use the  complete.cases()  function to count the number of rows with no missing values in the data frame:
<b>#count total rows in data frame with no missing values in any column
nrow(df[complete.cases(df), ])
[1] 4</b>
There are <b>4</b> rows with no missing values in the data frame.
<h3>Example 4: Count Rows with Missing Values in Specific Column</h3>
The following code shows how to use the  is.na()  function to count the number of rows that have a missing value in the ‘y’ column specifically:
<b>#count total rows in with missing value in 'y' column
nrow(df[is.na(df$y), ])
[1] 2</b>
There are <b>2</b> rows with missing values in the ‘y’ column.
<h2><span class="orange">Understanding the Null Hypothesis for ANOVA Models</span></h2>
A  one-way ANOVA  is used to determine if there is a statistically significant difference between the mean of three or more independent groups.
A one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>= … = μ<sub>k </sub>(all of the group means are equal)
<b>H<sub>A</sub>:</b> At least one group mean is different<sub> </sub>from the rest
To decide if we should reject or fail to reject the null hypothesis, we must refer to the p-value in the output of the ANOVA table.
If the p-value is less than some significance level (e.g. 0.05) then we can reject the null hypothesis and conclude that not all group means are equal.
A  two-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups that have been split on two variables (sometimes called “factors”).
A two-way ANOVA tests three null hypotheses at the same time:
All group means are equal at each level of the first variable
All group means are equal at each level of the second variable
There is no interaction effect between the two variables
To decide if we should reject or fail to reject each null hypothesis, we must refer to the p-values in the output of the two-way ANOVA table.
The following examples show how to decide to reject or fail to reject the null hypothesis in both a one-way ANOVA and two-way ANOVA.
<h2>Example 1: One-Way ANOVA</h2>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a certain exam. To test this, we recruit 30 students to participate in a study and split them into three groups.
The students in each group are  randomly assigned  to use one of the three exam prep programs for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same exam. 
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png">
When we enter these values into the  One-Way ANOVA Calculator , we receive the following ANOVA table as the output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay4.png">
Notice that the p-value is <b>0.11385</b>.
For this particular example, we would use the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>(the mean exam score for each group is equal)
<b>H<sub>A</sub>:</b> At least one group mean is different<sub> </sub>from the rest
Since the p-value from the ANOVA table is not less than 0.05, we fail to reject the null hypothesis.
This means we don’t have sufficient evidence to say that there is a statistically significant difference between the mean exam scores of the three groups.
<h2>Example 2: Two-Way ANOVA</h2>
Suppose a botanist wants to know whether or not plant growth is influenced by sunlight exposure and watering frequency.
She plants 40 seeds and lets them grow for two months under different conditions for sunlight exposure and watering frequency. After two months, she records the height of each plant. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/2wayanova1.png"(max-width: 392px) 100vw, 392px">
In the table above, we see that there were five plants grown under each combination of conditions.
For example, there were five plants grown with daily watering and no sunlight and their heights after two months were 4.8 inches, 4.4 inches, 3.2 inches, 3.9 inches, and 4.4 inches:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/2wayanova2.png"(max-width: 392px) 100vw, 392px">
She performs a  two-way ANOVA in Excel  and ends up with the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/anovaa.jpg"605">
We can see the following p-values in the output of the two-way ANOVA table:
The p-value for watering frequency is <b>0.975975</b>. This is not statistically significant at a significance level of 0.05.
The p-value for sunlight exposure is <b>3.9E-8 (0.000000039)</b>. This is statistically significant at a significance level of 0.05.
The p-value for the interaction between watering  frequency and sunlight exposure is <b>0.310898</b>. This is not statistically significant at a significance level of 0.05.
These results indicate that sunlight exposure is the only factor that has a statistically significant effect on plant height.
And because there is no interaction effect, the effect of sunlight exposure is consistent across each level of watering frequency.
That is, whether a plant is watered daily or weekly has no impact on how sunlight exposure affects a plant.
<h2><span class="orange">Understanding the Null Hypothesis for Linear Regression</span></h2>
Linear regression is a technique we can use to understand the relationship between one or more predictor variables and a  response variable .
If we only have one predictor variable and one response variable, we can use  simple linear regression , which uses the following formula to estimate the relationship between the variables:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x
where:
<U+0177>: The estimated response value.
β<sub>0</sub>: The average value of y when x is zero.
β<sub>1</sub>: The average change in y associated with a one unit increase in x.
x: The value of the predictor variable.
Simple linear regression uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> β<sub>1</sub> = 0
<b>H<sub>A</sub>:</b> β<sub>1</sub> ≠ 0
The null hypothesis states that the coefficient β<sub>1</sub> is equal to zero. In other words, there is no statistically significant relationship between the predictor variable, x, and the response variable, y.
The alternative hypothesis states that β<sub>1</sub> is <em>not</em> equal to zero. In other words, there <em>is</em> a statistically significant relationship between x and y.
If we have multiple predictor variables and one response variable, we can use  multiple linear regression , which uses the following formula to estimate the relationship between the variables:
<U+0177> = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + … + β<sub>k</sub>x<sub>k</sub>
where:
<U+0177>: The estimated response value.
β<sub>0</sub>: The average value of y when all predictor variables are equal to zero.
β<sub>i</sub>: The average change in y associated with a one unit increase in x<sub>i</sub>.
x<sub>i</sub>: The value of the predictor variable x<sub>i</sub>.
Multiple linear regression uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> β<sub>1</sub> = β<sub>2</sub> = … = β<sub>k</sub> = 0
<b>H<sub>A</sub>:</b> β<sub>1</sub> = β<sub>2</sub> = … = β<sub>k</sub> ≠ 0
The null hypothesis states that all coefficients in the model are equal to zero. In other words, none of the predictor variables have a statistically significant relationship with the response variable, y.
The alternative hypothesis states that not every coefficient is simultaneously equal to zero.
The following examples show how to decide to reject or fail to reject the null hypothesis in both simple linear regression and multiple linear regression models.
<h3>Example 1: Simple Linear Regression</h3>
Suppose a professor would like to use the number of hours studied to predict the exam score that students will receive in his class. He collects data for 20 students and fits a simple linear regression model.
The following screenshot shows the output of the regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/simpleRegressionExcel5.png">
The fitted simple linear regression model is:
Exam Score = 67.1617 + 5.2503*(hours studied)
To determine if there is a statistically significant relationship between hours studied and exam score, we need to analyze the  overall F value  of the model and the corresponding p-value:
Overall F-Value: <b>47.9952</b>
P-value: <b>0.000</b>
Since this p-value is less than .05, we can reject the null hypothesis. In other words, there is a statistically significant relationship between hours studied and exam score received.
<h3>Example 2: Multiple Linear Regression</h3>
Suppose a professor would like to use the number of hours studied and the number of prep exams taken to predict the exam score that students will receive in his class. He collects data for 20 students and fits a multiple linear regression model.
The following screenshot shows the output of the regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
The fitted multiple linear regression model is:
Exam Score = 67.67 + 5.56*(hours studied) – 0.60*(prep exams taken)
To determine if there is a jointly statistically significant relationship between the two predictor variables and the response variable, we need to analyze the overall F value of the model and the corresponding p-value:
Overall F-Value: <b>23.46</b>
P-value: <b>0.00</b>
Since this p-value is less than .05, we can reject the null hypothesis. In other words, hours studied and prep exams taken have a jointly statistically significant relationship with exam score.
<b>Note:</b> Although the p-value for prep exams taken (p = 0.52) is not significant, prep exams <em>combined</em> with hours studied has a significant relationship with exam score.
<h2><span class="orange">Understanding the Null Hypothesis for Logistic Regression</span></h2>
 Logistic regression  is a type of regression model we can use to understand the relationship between one or more predictor variables and a  response variable  when the response variable is binary.
If we only have one predictor variable and one response variable, we can use <b>simple logistic regression</b>, which uses the following formula to estimate the relationship between the variables:
<b>log[p(X) / (1-p(X))]  =  β<sub>0</sub> + β<sub>1</sub>X</b>
The formula on the right side of the equation predicts the log odds of the response variable taking on a value of 1.
Simple logistic regression uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> β<sub>1</sub> = 0
<b>H<sub>A</sub>:</b> β<sub>1</sub> ≠ 0
The null hypothesis states that the coefficient β<sub>1</sub> is equal to zero. In other words, there is no statistically significant relationship between the predictor variable, x, and the response variable, y.
The alternative hypothesis states that β<sub>1</sub> is <em>not</em> equal to zero. In other words, there <em>is</em> a statistically significant relationship between x and y.
If we have multiple predictor variables and one response variable, we can use <b>multiple logistic regression</b>, which uses the following formula to estimate the relationship between the variables:
<b>log[p(X) / (1-p(X))] = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + … + β<sub>k</sub>x<sub>k</sub></b>
Multiple logistic regression uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> β<sub>1</sub> = β<sub>2</sub> = … = β<sub>k</sub> = 0
<b>H<sub>A</sub>:</b> β<sub>1</sub> = β<sub>2</sub> = … = β<sub>k</sub> ≠ 0
The null hypothesis states that all coefficients in the model are equal to zero. In other words, none of the predictor variables have a statistically significant relationship with the response variable, y.
The alternative hypothesis states that not every coefficient is simultaneously equal to zero.
The following examples show how to decide to reject or fail to reject the null hypothesis in both simple logistic regression and multiple logistic regression models.
<h3>Example 1: Simple Logistic Regression</h3>
Suppose a professor would like to use the number of hours studied to predict the exam score that students will receive in his class. He collects data for 20 students and fits a simple logistic regression model.
We can use the following code in R to fit a simple logistic regression model:
<b>#create data
df &lt;- data.frame(result=c(0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1), hours=c(1, 5, 5, 1, 2, 1, 3, 2, 2, 1, 2, 1, 3, 4, 4, 2, 1, 1, 4, 3))
#fit simple logistic regression model
model &lt;- glm(result~hours, family='binomial', data=df)
#view summary of model fit
summary(model)
Call:
glm(formula = result ~ hours, family = "binomial", data = df)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8244  -1.1738   0.7701   0.9460   1.2236  
Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)  -0.4987     0.9490  -0.526    0.599
hours         0.3906     0.3714   1.052    0.293
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 26.920  on 19  degrees of freedom
Residual deviance: 25.712  on 18  degrees of freedom
AIC: 29.712
Number of Fisher Scoring iterations: 4
#calculate p-value of overall Chi-Square statistic
1-pchisq(26.920-25.712, 19-18)
[1] 0.2717286
</b>
To determine if there is a statistically significant relationship between hours studied and exam score, we need to analyze the overall Chi-Square value of the model and the corresponding p-value.
We can use the following formula to calculate the overall Chi-Square value of the model:
X<sup>2</sup> = (Null deviance – Residual deviance) / (Null df – Residual df)
The p-value turns out to be <b>0.2717286</b>.
Since this p-value is not less than .05, we fail to reject the null hypothesis. In other words, there is not a statistically significant relationship between hours studied and exam score received.
<h3>Example 2: Multiple Logistic Regression</h3>
Suppose a professor would like to use the number of hours studied and the number of prep exams taken to predict the exam score that students will receive in his class. He collects data for 20 students and fits a multiple logistic regression model.
We can use the following code in R to fit a multiple logistic regression model:
<b>#create data
df &lt;- data.frame(result=c(0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1), hours=c(1, 5, 5, 1, 2, 1, 3, 2, 2, 1, 2, 1, 3, 4, 4, 2, 1, 1, 4, 3), exams=c(1, 2, 2, 1, 2, 1, 1, 3, 2, 4, 3, 2, 2, 4, 4, 5, 4, 4, 3, 5))
#fit simple logistic regression model
model &lt;- glm(result~hours+exams, family='binomial', data=df)
#view summary of model fit
summary(model)
Call:
glm(formula = result ~ hours + exams, family = "binomial", data = df)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5061  -0.6395   0.3347   0.6300   1.7014  
Coefficients:
            Estimate Std. Error z value Pr(>|z|)  
(Intercept)  -3.4873     1.8557  -1.879   0.0602 .
hours         0.3844     0.4145   0.927   0.3538  
exams         1.1549     0.5493   2.103   0.0355 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 26.920  on 19  degrees of freedom
Residual deviance: 19.067  on 17  degrees of freedom
AIC: 25.067
Number of Fisher Scoring iterations: 5
#calculate p-value of overall Chi-Square statistic
1-pchisq(26.920-19.067, 19-17)
[1] 0.01971255
</b>
The p-value for the overall Chi-Square statistic of the model turns out to be <b>0.01971255</b>.
Since this p-value is less than .05, we reject the null hypothesis. In other words, there is a statistically significant relationship between the combination of hours studied and prep exams taken and final exam score received.
<h2><span class="orange">How to Interpret Null & Residual Deviance (With Examples)</span></h2>
Whenever you fit a general linear model (like logistic regression, Poisson regression, etc.), most statistical software will produce values for the <b>null deviance</b> and <b>residual deviance </b>of the model.
The <b>null deviance</b> tells us how well the response variable can be predicted by a model with only an intercept term.
The <b>residual deviance</b> tells us how well the response variable can be predicted by a model with <em>p</em> predictor variables. The lower the value, the better the model is able to predict the value of the response variable.
To determine if a model is “useful” we can compute the Chi-Square statistic as:
<b>X<sup>2</sup></b> = Null deviance – Residual deviance
with <em>p</em> degrees of freedom.
We can then find the p-value associated with this Chi-Square statistic. The lower the p-value, the better the model is able to fit the dataset compared to a model with just an intercept term.
The following example shows how to interpret null and residual deviance for a logistic regression model in R.
<h3>Example: Interpreting Null & Residual Deviance</h3>
For this example, we’ll use the <b>Default</b> dataset from the ISLR package. We can use the following code to load and view a summary of the dataset:
<b>#load dataset
data &lt;- ISLR::Default
#view summary of dataset
summary(data)
 default    student       balance           income     
 No :9667   No :7056   Min.   :   0.0   Min.   :  772  
 Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340         Median : 823.6   Median :34553         Mean   : 835.4   Mean   :33517         3rd Qu.:1166.3   3rd Qu.:43808         Max.   :2654.3   Max.   :73554 
</b>
This dataset contains the following information about 10,000 individuals:
<b>default:</b> Indicates whether or not an individual defaulted.
<b>student:</b> Indicates whether or not an individual is a student.
<b>balance:</b> Average balance carried by an individual.
<b>income:</b> Income of the individual.
We will use student status, bank balance, and income to build a  logistic regression model  that predicts the probability that a given individual defaults:
<b>#fit logistic regression model
model &lt;- glm(default~balance+student+income, family="binomial", data=data)
#view model summary
summary(model)
Call:
glm(formula = default ~ balance + student + income, family = "binomial", 
    data = data)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4691  -0.1418  -0.0557  -0.0203   3.7383  
Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -1.087e+01  4.923e-01 -22.080  &lt; 2e-16 ***
balance      5.737e-03  2.319e-04  24.738  &lt; 2e-16 ***
studentYes  -6.468e-01  2.363e-01  -2.738  0.00619 ** 
income       3.033e-06  8.203e-06   0.370  0.71152    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 2920.6  on 9999  degrees of freedom
Residual deviance: 1571.5  on 9996  degrees of freedom
AIC: 1579.5
Number of Fisher Scoring iterations: 8
</b>
We can observe the following values in the output for the null and residual deviance:
<b>Null deviance</b>: 2920.6 with df = 9999
<b>Residual deviance</b>: 1571.5 with df = 9996
We can use these values to calculate the X<sup>2</sup> statistic of the model:
X<sup>2</sup> = Null deviance – Residual deviance
X<sup>2</sup> = 2910.6 – 1579.0
X<sup>2</sup> = 1331.6
There are <em>p</em> = 3 predictor variables degrees of freedom.
We can use the  Chi-Square to P-Value Calculator  to find that a X<sup>2</sup> value of 1331.6 with 3 degrees of freedom has a p-value of 0.000000.
Since this p-value is much less than .05, we would conclude that the model is highly useful for predicting the probability that a given individual defaults.
<h2><span class="orange">Number Needed to Harm Calculator</span></h2>
 Number needed to harm (NNH)  refers to the average number of patients who need to be exposed to some risk factor to cause harm in an average of one person who would not have been harmed otherwise.
The number needed to harm (NNH) is calculated as:
NNH = 1 / (I<sub>T</sub> – I<sub>C</sub>)
where:
I<sub>T</sub>: Incidence rate in treatment (or “exposed”) group
I<sub>C</sub>: Incidence rate in control group
To calculate NNH, simply fill in the boxes below and then click the “Calculate” button.
<label for="T"><b>I<sub>T</sub>: Incidence rate in treatment group</b></label>
<input type="number" id="T" value="0.05">
<label for="C"><b>I<sub>C</sub>: Incidence rate in control group</b></label>
<input type="number" id="C" value="0.03">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Number Needed to Harm (NNH): </b>50.00
<b>Interpretation: </b>50.00 patients need to be exposed in order for one to experience a harmful effect, on average.
<script>
function calc() {
//get input values
var T  = document.getElementById('T').value*1;
var C  = document.getElementById('C').value*1;
//find NNH
var NNH = 1/(T-C);
//output
document.getElementById('NNH').innerHTML = NNH.toFixed(2);
document.getElementById('NNH_out').innerHTML = NNH.toFixed(2);
}
</script>
<h2><span class="orange">What is Number Needed to Harm? (Definition & Example)</span></h2>
<b>Number needed to harm (NNH)</b> refers to the average number of patients who need to be exposed to some risk factor to cause harm in an average of one person who would not have been harmed otherwise.
For example, suppose doctors test out a new drug designed to lower blood pressure and find that one in every 250 patients experiences a heart attack as a side effect.
The number needed to harm for this particular drug would be NNH = <b>250</b>.
The higher the NNH for a given drug or treatment, the lower the risk factor of that drug or treatment. 
For example, if drug A has a NNH of <b>250</b> and drug B has a NNH of <b>600</b>, drug B would be preferred because it only harms one in every 600 patients, on average.
<h3>Formula to Calculate Number Needed to Harm</h3>
In practice, we use the following formula to calculate NNH:
<b>Number Needed to Harm (NNH) = 1 / (I<sub>T</sub> – I<sub>C</sub>)</b>
where:
<b>I<sub>T</sub></b> – The incidence rate in the treatment group
<b>I<sub>C</sub></b> – The incidence rate in the control group
For example, suppose 5% of patients who use a new blood pressure drug experience a heart attack compared to 3% of patients who simply took a placebo.
We would calculate the number needed to harm as:
NNH = 1 / (I<sub>T</sub> – I<sub>C</sub>)
NNH = 1 / (.05 – .03)
NNH = 50
This means that 50 patients, on average, need to be exposed to this drug in order for one of them to experience a heart attack who otherwise would not have experienced the heart attack.
<h3>NNH vs. NNT</h3>
A similar metric is known as the<b> number needed to treat (NNT)</b>, which refers to the average number of patients that need to be treated for a benefit to occur to one person.
It is calculated as:
<b>Number Needed to Treat (NNT) = 1 / (I<sub>C</sub> – I<sub>T</sub>)</b>
where:
<b>I<sub>T</sub></b> – The incidence rate in the treatment group
<b>I<sub>C</sub></b> – The incidence rate in the control group
An ideal new drug or treatment would have a low NNT and a high NNH because this means that only a few people need to be treated for a benefit to occur while a lot of people need to be treated for something harmful to occur.
Doctors and clinicians often look at both NNH and NNT when deciding whether or not it’s prudent to give patients certain drugs. However, the specific scenario also makes a difference.
For example, a drug may have a low NNH (meaning harmful effects occur often) but it may still be used if the alternative is something serious like a heart attack, stroke, or even death.
<h3>Caveats of Using NNH</h3>
Keep in mind the following caveats of using NNH as a metric:
<b>1. The NNH is not the same for all patients.</b>
Number needed to harm (NNH) only provides us with an average. However, some patients will naturally be at higher risk if they have pre-existing conditions or make poor lifestyle choices.
<b>2. Time frame matters.</b>
The time frame for a given treatment or drug is important. For example, if a treatment is given to a patient over the course of five years then this should be mentioned along with the value for NNH.
<h2><span class="orange">How to Count Number of Rows in R (With Examples)</span></h2>
You can use the  nrow()  function to count the number of rows in a data frame in R:
<b>#count total rows in data frame
nrow(df)
#count total rows with no NA values in any column of data frame
nrow(na.omit(df))
#count total rows with no NA values in specific column of data frame 
nrow(df[!is.na(df$column_name),])
</b>
The following examples show how to use the <b>nrow()</b> function in practice.
<h3>Example 1: Count Total Number of Rows</h3>
The following code shows how to count the total number of rows in a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 6, 2), var3=c(9, 9, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    9    1
2    3    7    9    1
3    3    8    6    2
4    4    6    6    8
5    5    2    8    9
#count total rows in data frame
nrow(df)
[1] 5</b>
There are <b>5</b> total rows in this data frame.
<h3>Example 2: Count Rows with No NA Values in Any Column</h3>
The following code shows how to count the total number of rows in a data frame with no NA values in any column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, NA, 2), var3=c(9, 9, NA, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    9    1
2    3    7    9    1
3    3    8   NA    2
4    4   NA    6    8
5    5    2    8    9
#count total rows in data frame with no NA values in any column of data frame
nrow(na.omit(df))
[1] 3</b>
There are <b>3</b> total rows in this data frame that have no NA values in any column.
<h3>Example 3: Count Rows with No NA Values in Specific Column</h3>
The following code shows how to count the total number of rows in a data frame with no NA values in any column:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, NA, 2), var3=c(9, NA, NA, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    9    1
2    3    7   NA    1
3    3    8   NA    2
4    4   NA    6    8
5    5    2    8    9
#count total rows in data frame with no NA values in 'var2' column of data frame
nrow(df[!is.na(df$var2),])
[1] 4</b>
There are <b>4</b> total rows in this data frame that have no NA values in the ‘var2’ column.
<h2><span class="orange">How to Convert Numeric to Character in R (With Examples)</span></h2>
We can use the following syntax to convert a numeric vector to a character vector in R:
<b>character_vector &lt;- as.character(numeric_vector)
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Convert a Vector from Numeric to Character</h3>
The following code shows how to convert a numeric vector to a character vector:
<b>#create numeric vector
chars &lt;- c(12, 14, 19, 22, 26)
#convert numeric vector to character vector
chars &lt;- as.character(chars)
#view character vector
chars
[1] "12" "14" "19" "22" "26"
#confirm class of character vector
class(chars)
[1] "character"
</b>
<h3>Example 2: Convert a Column from Numeric to Character</h3>
The following code shows how to convert a specific column in a data frame from numeric to character:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c(28, 34, 35, 36, 40))
#convert column 'b' from numeric to character
df$b &lt;- as.character(df$b)
#confirm class of character vector
class(df$b)
[1] "character"
</b>
<h3>Example 3: Convert Several Columns from Numeric to Character</h3>
The following code shows how to convert all numeric columns in a data frame from numeric to character:
<b>#create data frame
df &lt;- data.frame(a = c('12', '14', '19', '22', '26'), b = c('28', '34', '35', '36', '40'), c = as.factor(c(1, 2, 3, 4, 5)), d = c(45, 56, 54, 57, 59))
#display classes of each column
sapply(df, class)
          a           b           c           d 
  "numeric" "character"    "factor"   "numeric" 
#identify all numeric columns
nums&lt;- sapply(df, is.numeric)
#convert all numeric columns to character
df[ , nums] &lt;- as.data.frame(apply(df[ , nums], 2, as.character))
#display classes of each column
sapply(df, class)
          a           b           c           d 
"character" "character"    "factor" "character" </b>
This code made the following changes to the data frame columns:
<b>Column a:</b> From numeric to character
<b>Column b:</b> Unchanged (since it was already numeric)
<b>Column c:</b> Unchanged (since it was a factor)
<b>Column d:</b> From numeric to character
By using the  apply()  and  sapply()  functions, we were able to convert only the numeric columns to character columns and leave all other columns unchanged.
<h2><span class="orange">How to Add a Column to a NumPy Array (With Examples)</span></h2>
You can use one of the following methods to add a column to a NumPy array:
<b>Method 1: Append Column to End of Array</b>
<b>np.append(my_array, [[value1], [value2], [value3], ...], axis=1)
</b>
<b>Method 2: Insert Column in Specific Position of Array</b>
<b>np.insert(my_array, 3, [value1, value2, value3, ...], axis=1) </b>
The following examples show how to use each method in practice.
<h3>Example 1: Append Column to End of NumPy Array</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])
#view NumPy array
my_array
array([[1, 2, 3, 4],
       [5, 6, 7, 8]])</b>
We can use the following syntax to add a column to the end of the NumPy array:
<b>#append column to end of NumPy array
new_array = np.append(my_array, [[10], [13]], axis=1)
#view updated array
new_array
array([[ 1,  2,  3,  4, 10],
       [ 5,  6,  7,  8, 13]])</b>
<h3>Example 2: Insert Column in Specific Position of NumPy Array</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
#view NumPy array
my_array
array([[1, 2, 3],
       [4, 5, 6],
       [7, 8, 9]])</b>
We can use the following syntax to insert a new column before the column in index position 2 of the NumPy array:
<b>#insert new column before column in index position 2
new_array = np.insert(my_array, 2, [10, 13, 19], axis=1)
#view updated array
new_array
array([[ 1,  2, 10,  3],
       [ 4,  5, 13,  6],
       [ 7,  8, 19,  9]])</b>
Notice that the new column of values has been inserted before the column in index position 2.
<h2><span class="orange">How to Add Row to Matrix in NumPy (With Examples)</span></h2>
You can use the following syntax to add a row to a matrix in NumPy:
<b>#add new_row to current_matrix
current_matrix = np.vstack([current_matrix, new_row])
</b>
You can also use the following syntax to only add rows to a matrix that meet a certain condition:
<b>#only add rows where first element is less than 10
current_matrix = np.vstack((current_matrix, new_rows[new_rows[:,0] &lt; 10]))</b>
The following examples shows how to use this syntax in practice.
<h3>Example 1: Add Row to Matrix in NumPy</h3>
The following code shows how to add a new row to a matrix in NumPy:
<b>import numpy as np
#define matrix
current_matrix = np.array([[1 ,2 ,3], [4, 5, 6], [7, 8, 9]])
#define row to add
new_row = np.array([10, 11, 12])
#add new row to matrix
current_matrix = np.vstack([current_matrix, new_row])
#view updated matrix
current_matrix
array([[ 1,  2,  3],
       [ 4,  5,  6],
       [ 7,  8,  9],
       [10, 11, 12]])
</b>
Notice that the last row has been successfully added to the matrix.
<h3>Example 2: Add Rows to Matrix Based on Condition</h3>
The following code shows how to add several new rows to an existing matrix based on a specific condition:
<b>import numpy as np
#define matrix
current_matrix = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
#define potential new rows to add
new_rows = np.array([[6, 8, 10], [8, 10, 12], [10, 12, 14]])
#only add rows where first element in row is less than 10
current_matrix = np.vstack((current_matrix, new_rows[new_rows[:,0] &lt; 10]))
#view updated matrix
current_matrix
array([[ 1,  2,  3],
       [ 4,  5,  6],
       [ 7,  8,  9],
       [ 6,  8, 10],
       [ 8, 10, 12]])
</b>
Only the rows where the first element in the row was less than 10 were added.
<b>Note</b>: You can find the complete online documentation for the <b>vstack()</b> function  here .
<h2><span class="orange">How to Fix: All input arrays must have same number of dimensions</span></h2>
One error you may encounter when using NumPy is:
<b>ValueError: all the input arrays must have same number of dimensions</b>
This error occurs when you attempt to concatenate two NumPy arrays that have different dimensions.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following two NumPy arrays:
<b>import numpy as np
#create first array
array1 = np.array([[1, 2], [3, 4], [5,6], [7,8]])
print(array1) 
[[1 2]
 [3 4]
 [5 6]
 [7 8]]
#create second array 
array2 = np.array([9,10, 11, 12])
print(array2)
[ 9 10 11 12]</b>
Now suppose we attempt to use the <b>concatenate()</b> function to combine the two arrays into one array:
<b>#attempt to concatenate the two arrays
np.concatenate([array1, array2])
ValueError: all the input arrays must have same number of dimensions, but the array at
            index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)
</b>
We receive a <b>ValueError</b> because the two arrays have different dimensions.
<h3>How to Fix the Error</h3>
There are two methods we can use to fix this error.
<b>Method 1: Use np.column_stack</b>
One way to concatenate the two arrays while avoiding errors is to use the <b>column_stack()</b> function as follows:
<b>np.column_stack((array1, array2))
array([[ 1,  2,  9],
       [ 3,  4, 10],
       [ 5,  6, 11],
       [ 7,  8, 12]])
</b>
Notice that we’re able to successfully concatenate the two arrays without any errors.
<b>Method 2: Use np.c_</b>
We can also concatenate the two arrays while avoiding errors using the <b>np.c_</b> function as follows:
<b>np.c_[array1, array2]
array([[ 1,  2,  9],
       [ 3,  4, 10],
       [ 5,  6, 11],
       [ 7,  8, 12]])
</b>
Notice that this function returns the exact same result as the previous method.
<h2><span class="orange">How to Add Elements to NumPy Array (3 Examples)</span></h2>
You can use the following methods to add one or more elements to a NumPy array:
<b>Method 1: Append One Value to End of Array</b>
<b>#append one value to end of array
new_array = np.append(my_array, 15)
</b>
<b>Method 2: Append Multiple Values to End of Array</b>
<b>#append multiple values to end of array
new_array = np.append(my_array, [15, 17, 18])
</b>
<b>Method 3: Insert One Value at Specific Position in Array</b>
<b>#insert 95 into the index position 2
new_array = np.insert(my_array, 2, 95)</b>
<b>Method 4: Insert Multiple Values at Specific Position in Array</b>
<b>#insert 95 and 99 starting at index position 2 of the NumPy array
new_array = np.insert(my_array, 2, [95, 99]) 
</b>
This tutorial explains how to use each method in practice with the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([1, 2, 2, 3, 5, 6, 7, 10])
#view NumPy array
my_array
array([ 1,  2,  2,  3,  5,  6,  7, 10])
</b>
<h2>Example 1: Append One Value to End of Array</h2>
The following code shows how to use <b>np.append()</b> to append one value to the end of the NumPy array:
<b>#append one value to end of array
new_array = np.append(my_array, 15)
#view new array
new_array
array([ 1,  2,  2,  3,  5,  6,  7, 10, 15])
</b>
The value <b>15</b> has been added to the end of the NumPy array.
<h2>Example 2: Append Multiple Values to End of Array</h2>
The following code shows how to use <b>np.append()</b> to append multiple values to the end of the NumPy array:
<b>#append multiple values to end of array
new_array = np.append(my_array, [15, 17, 18])
#view new array
new_array
array([ 1,  2,  2,  3,  5,  6,  7, 10, 15, 17, 18])
</b>
The values <b>15</b>, <b>17</b>, and <b>18 </b>have been added to the end of the NumPy array.
<h2>Example 3: Insert One Value at Specific Position in Array</h2>
The following code shows how to insert one value into a specific position in the NumPy array:
<b>#insert 95 into the index position 2
new_array = np.insert(my_array, 2, 95)
#view new array
new_array
array([ 1,  2, 95,  2,  3,  5,  6,  7, 10])
</b>
The value <b>95</b> has been inserted into index position 2 of the NumPy array.
<h2>Example 4: Insert Multiple Values at Specific Position in Array</h2>
The following code shows how to insert multiple values starting at a specific position in the NumPy array:
<b>#insert 95 and 99 starting at index position 2 of the NumPy array
new_array = np.insert(my_array, 2, [95, 99]) 
#view new array
new_array
array([ 1,  2, 95, 99,  2,  3,  5,  6,  7, 10])
</b>
The values <b>95</b> and <b>99</b> have been inserted starting at index position 2 of the NumPy array.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Filter a NumPy Array 
 How to Remove NaN Values from NumPy Array 
 How to Compare Two NumPy Arrays 
<h2><span class="orange">How to Export a NumPy Array to a CSV File (With Examples)</span></h2>
You can use the following basic syntax to export a NumPy array to a CSV file:
<b>import numpy as np
#define NumPy array
data = np.array([[1,2,3],[4,5,6],[7,8,9]])
#export array to CSV file
np.savetxt("my_data.csv", data, delimiter=",")
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Export NumPy Array to CSV</h3>
The following code shows how to export a NumPy array to a CSV file:
<b>import numpy as np
#define NumPy array
data = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12], [13, 14, 15]])
#export array to CSV file
np.savetxt("my_data.csv", data, delimiter=",")</b>
If I navigate to the location where the CSV file is saved on my laptop, I can view the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/exportNumpy1.png">
<h3>Example 2: Export NumPy Array to CSV With Specific Format</h3>
The default format for numbers is “%.18e” – this displays 18 zeros. However, we can use the <b>fmt</b> argument to specify a different format.
For example, the following code exports a NumPy array to CSV and specifies two decimal places:
<b>import numpy as np
#define NumPy array
data = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12], [13, 14, 15]])
#export array to CSV file (using 2 decimal places)
np.savetxt("my_data.csv", data, delimiter=",", fmt="%.2f")</b>
If I navigate to the location where the CSV file is saved, I can view the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/exportNumpy2.png">
<h3>Example 3: Export NumPy Array to CSV With Headers</h3>
The following code shows how to export a NumPy array to a CSV file with custom column headers:
<b>import numpy as np
#define NumPy array
data = np.array([[1,2,3], [4,5,6], [7,8,9], [10, 11, 12], [13, 14, 15]])
#export array to CSV file (using 2 decimal places)
np.savetxt("my_data.csv", data, delimiter=",", fmt="%.2f",
           header="A, B, C", comments="")</b>
<b>Note</b>: The <b>comments</b> argument prevents a “#” symbol from being displayed in the headers.
If I navigate to the location where the CSV file is saved, I can view the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/exportNumpy.png">
<b> Note</b>: You can find the complete documentation for the <b>numpy.savetxt()</b> function  here .
<h2><span class="orange">How to Convert NumPy Array of Floats into Integers</span></h2>
You can use the following methods to convert a NumPy array of floats to an array of integers:
<b>Method 1: Convert Floats to Integers (Rounded Down)</b>
<b>rounded_down_integer_array = float_array.astype(int)
</b>
<b>Method 2: Convert Floats to Integers (Rounded to Nearest Integer)</b>
<b>rounded_integer_array = (np.rint(some_floats)).astype(int)
</b>
<b>Method 3: Convert Floats to Integers (Rounded Up)</b>
<b>rounded_up_integer_array = (np.ceil(float_array)).astype(int)
</b>
The following examples show how to use each method in practice with the following NumPy array of floats:
<b>import numpy as np
#create NumPy array of floats
float_array = np.array([2.33, 4.7, 5.1, 6.2356, 7.88, 8.5])
#view array
print(float_array)
[2.33   4.7    5.1    6.2356 7.88   8.5   ]
#view dtype of array
print(float_array.dtype)
float64</b>
<h2>Example 1: Convert Floats to Integers (Rounded Down)</h2>
The following code shows how to convert a NumPy array of floats to an array of integers in which each float is rounded down to the nearest integer:
<b>#convert NumPy array of floats to array of integers (rounded down)
rounded_down_integer_array = float_array.astype(int)
#view array
print(rounded_down_integer_array)
[2 4 5 6 7 8]
#view dtype of array
print(rounded_down_integer_array.dtype)
int32</b>
Notice that each float has been rounded down to the nearest integer and the new array has a dtype of <b>int32</b>.
<h2>Example 2: Convert Floats to Integers (Rounded to Nearest Integer)</h2>
The following code shows how to convert a NumPy array of floats to an array of integers in which each float is rounded to the nearest integer:
<b>#convert NumPy array of floats to array of integers (rounded to nearest)
rounded_integer_array = (np.rint(float_array)).astype(int)
#view array
print(rounded_integer_array)
[2 5 5 6 8 8]
#view dtype of array
print(rounded_integer_array.dtype)
int32</b>
Notice that each float has been rounded to the nearest integer and the new array has a dtype of <b>int32</b>.
<h2>Example 3: Convert Floats to Integers (Rounded Up)</h2>
The following code shows how to convert a NumPy array of floats to an array of integers in which each float is rounded up to the nearest integer:
<b>#convert NumPy array of floats to array of integers (rounded up)
rounded_up_integer_array = (np.ceil(float_array)).astype(int)
#view array
print(rounded_up_integer_array)
[3 5 6 7 8 9]
#view dtype of array
print(rounded_up_integer_array.dtype)
int32</b>
Notice that each float has been rounded up to the nearest integer and the new array has a dtype of <b>int32</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Fill NumPy Array with Values 
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Convert a NumPy Array to Pandas DataFrame</span></h2>
You can use the following syntax to convert a NumPy array into a pandas DataFrame:
<b>#create NumPy array
data = np.array([[1, 7, 6, 5, 6], [4, 4, 4, 3, 1]])
#convert NumPy array to pandas DataFrame
df = pd.DataFrame(data=data)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Convert NumPy Array to Pandas DataFrame</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 7, 6, 5, 6], [4, 4, 4, 3, 1]])
#print class of NumPy array
type(data)
numpy.ndarray
</b>
We can use the following syntax to convert the NumPy array into a pandas DataFrame:
<b>import pandas as pd
#convert NumPy array to pandas DataFrame
df = pd.DataFrame(data=data)
#print DataFrame
print(df)
   0  1  2  3  4
0  1  7  6  5  6
1  4  4  4  3  1
#print class of DataFrame
type(df)
pandas.core.frame.DataFrame
</b>
<h3>Specify Row & Column Names for Pandas DataFrame</h3>
We can also specify row names and column names for the DataFrame by using the <b>index</b> and <b>columns</b> arguments, respectively.
<b>#convert array to DataFrame and specify rows & columns
df = pd.DataFrame(data=data, index=["r1", "r2"], columns=["A", "B", "C", "D", "E"])
#print the DataFrame
print(df)
    A  B  C  D  E
r1  1  7  6  5  6
r2  4  4  4  3  1</b>
<h2><span class="orange">NumPy mean() vs. average(): What’s the Difference?</span></h2>
You can use the <b>np.mean()</b> or <b>np.average()</b> functions to calculate the average value of an array in Python.
Here is the subtle difference between the two functions:
<b>np.mean </b>always calculates the arithmetic mean.
<b>np.average </b>has an optional <b>weights</b> parameter that can be used to calculate a weighted average.
The following examples show how to use each function in practice.
<h3>Example 1: Use np.mean() and np.average() without Weights</h3>
Suppose we have the following array in Python that contains seven values:
<b>#create array of values
data = [1, 4, 5, 7, 8, 8, 10]
</b>
We can use <b>np.mean()</b> and <b>np.average()</b> to calculate the average value of this array:
<b>import numpy as np
#calculate average value of array
np.mean(data)
6.142857142857143
#calcualte average value of array
np.average(data)
6.142857142857143
</b>
Both functions return the exact same value.
Both functions used the following formula to calculate the average:
Average = (1 + 4 + 5 + 7 + 8 + 8 + 10) / 7 = <b>6.142857</b>…
<h3>Example 2: Use np.average() with Weights</h3>
Once again suppose we have the following array in Python that contains seven values:
<b>#create array of values
data = [1, 4, 5, 7, 8, 8, 10]
</b>
We can use <b>np.average()</b> to calculate a weighted average for this array by supplying a list of values to the <b>weights</b> parameters:
<b>import numpy as np
#calculate weighted average of array
np.average(data, weights=(.1, .2, .4, .05, .05, .1, .1))
5.45
</b>
The weighted average turns  out to be <b>5.45</b>.
Here is the formula that <b>np.average()</b> used to calculate this value:
Weighted Average = 1*.1 + 4*.2 + 5*.4 + 7*.05 + 8*.05 + 8*.1 + 10*.1 = <b>5.45</b>.
Note that we could not use <b>np.mean()</b> to perform this calculation since that function doesn’t have a <b>weights</b> parameter.
Refer to the NumPy documentation for a complete explanation of the  np.mean()  and  np.average()  functions.
<h2><span class="orange">A Simple Explanation of NumPy Axes (With Examples)</span></h2>
Many functions in  NumPy  require that you specify an axis along which to apply a certain calculation.
Typically the following rule of thumb applies:
<b>axis=0</b>: Apply the calculation “column-wise”
<b>axis=1</b>: Apply the calculation “row-wise”
The following image shows a visual representation of the axes on a NumPy matrix with 2 rows and 4 columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/numpyaxis1.jpg">
The following examples show how to use the <b>axis</b> argument in different scenarios with the following NumPy matrix:
<b>import numpy as np
#create NumPy matrix
my_matrix = np.matrix([[1, 4, 7, 8], [5, 10, 12, 14]])
#view NumPy matrix
my_matrix
matrix([[ 1,  4,  7,  8],
        [ 5, 10, 12, 14]])
</b>
<h2>Example 1: Find Mean Along Different Axes</h2>
We can use <b>axis=0</b> to find the mean of each column in the NumPy matrix:
<b>#find mean of each column in matrix
np.mean(my_matrix, axis=0)
matrix([[ 3. ,  7. ,  9.5, 11. ]])
</b>
The output shows the mean value of each column in the matrix.
For example:
The mean value of the first column is (1 + 5) / 2 = <b>3</b>.
The mean value of the second column is (4 + 10) / 2 = <b>7</b>.
And so on.
We can also use <b>axis=1</b> to find the mean of each row in the matrix:
<b>#find mean of each row in matrix
np.mean(my_matrix, axis=1)
matrix([[ 5.  ],
        [10.25]])</b>
The output shows the mean value of each row in the matrix.
For example:
The mean value in the first row is (1+4+7+8) / 4 = <b>5</b>.
The mean value in the second row is (5+10+12+14) / 4 = <b>10.25</b>.
<h2>Example 2: Find Sum Along Different Axes</h2>
We can use <b>axis=0</b> to find the sum of each column in the matrix:
<b>#find sum of each column in matrix
np.sum(my_matrix, axis=0)
matrix([[ 6, 14, 19, 22]])
</b>
The output shows the sum of each column in the matrix.
For example:
The sum of the first column is 1 + 5 = <b>6</b>.
The sum of the second column is 4 + 10 = <b>14</b>.
And so on.
We can also use <b>axis=1</b> to find the sum of each row in the matrix:
<b>#find sum of each row in matrix
np.sum(my_matrix, axis=1)
matrix([[20],
        [41]])</b>
The output shows the sum of each row in the matrix.
For example:
The sum of the first row is 1+4+7+8 = <b>20</b>.
The sum of the second row is 5+10+12+14 = <b>41</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in NumPy:
 How to Create a NumPy Matrix with Random Numbers 
 How to Normalize a NumPy Matrix 
 How to Add Row to Matrix in NumPy 
<h2><span class="orange">How to Count Unique Values in NumPy Array (3 Examples)</span></h2>
You can use the following methods to count unique values in a NumPy array:
<b>Method 1: Display Unique Values</b>
<b>np.unique(my_array)
</b>
<b>Method 2: Count Number of Unique Values</b>
<b>len(np.unique(my_array))</b>
<b>Method 3: Count Occurrences of Each Unique Value</b>
<b>np.unique(my_array, return_counts=True)</b>
The following examples show how to use each method in practice with the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([1, 3, 3, 4, 4, 7, 8, 8])
</b>
<h2>Example 1: Display Unique Values</h2>
The following code shows how to display the unique values in the NumPy array:
<b>#display unique values
np.unique(my_array)
array([1, 3, 4, 7, 8])
</b>
From the output we can see each of the unique values in the NumPy array: 1, 3, 4, 7, 8.
<h2>Example 2: Count Number of Unique Values</h2>
The following code shows how to count the total number of unique values in the NumPy array:
<b>#display total number of unique values
len(np.unique(my_array))
5</b>
From the output we can see there are <b>5</b> unique values in the NumPy array.
<h2>Example 3: Count Occurrences of Each Unique Value</h2>
The following code shows how to count the number of occurrences of each unique value in the NumPy array:
<b>#count occurrences of each unique value
np.unique(my_array, return_counts=True)
(array([1, 3, 4, 7, 8]), array([1, 2, 2, 1, 2]))
</b>
The first array in the output shows the unique values and the second array shows the count of each unique value.
We can use the following code to print this output in a format that is easier to read:
<b>#get unique values and counts of each value
unique, counts = np.unique(my_array, return_counts=True)
#display unique values and counts side by side
print(np.asarray((unique, counts)).T)
[[1 1]
 [3 2]
 [4 2]
 [7 1]
 [8 2]]
</b>
From the output we can see:
The value 1 occurs <b>1</b> time.
The value 3 occurs <b>2</b> times.
The value 4 occurs <b>2</b> times.
The value 7 occurs <b>1</b> time.
The value 8 occurs <b>2</b> times.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Python:
 How to Calculate the Mode of NumPy Array 
 How to Map a Function Over a NumPy Array 
 How to Sort a NumPy Array by Column 
<h2><span class="orange">How to Count Occurrences of Elements in NumPy</span></h2>
You can use the following methods to count the occurrences of elements in a NumPy array:
<b>Method 1: Count Occurrences of a Specific Value</b>
<b>np.count_nonzero(x == 2)
</b>
<b>Method 2: Count Occurrences of Values that Meet One Condition</b>
<b>np.count_nonzero(x &lt; 6)</b>
<b>Method 3: Count Occurrences of Values that Meet One of Several Conditions</b>
<b>np.count_nonzero((x == 2) | (x == 7))</b>
The following examples show how to use each method in practice with the following NumPy array:
<b>import numpy as np
#create NumPy array
x = np.array([2, 2, 2, 4, 5, 5, 5, 7, 8, 8, 10, 12])
</b>
<h3>Example 1: Count Occurrences of a Specific Value</h3>
The following code shows how to count the number of elements in the NumPy array that are equal to the value 2:
<b>#count number of values in array equal to 2
np.count_nonzero(x == 2)
3</b>
From the output we can see that <b>3</b> values in the NumPy array are equal to 2.
<h3>Example 2: Count Occurrences of Values that Meet One Condition</h3>
The following code shows how to count the number of elements in the NumPy array that have a value less than 6:
<b>#count number of values in array that are less than 6
np.count_nonzero(x &lt; 6)
7</b>
From the output we can see that <b>7</b> values in the NumPy array have a value less than 6.
<h3>Example 3: Count Occurrences of Values that Meet One of Several Conditions</h3>
The following code shows how to count the number of elements in the NumPy array that are equal to 2 or 7:
<b>#count number of values in array that are equal to 2 <em>or</em> 7
np.count_nonzero((x == 2) | (x == 7))
4</b>
From the output we can see that <b>4</b> values in the NumPy array are equal to 2 or 7.
<h2><span class="orange">How to Bin Variables in Python Using numpy.digitize()</span></h2>
Often you may be interested in placing the values of a variable into “bins” in Python.
Fortunately this is easy to do using the  numpy.digitize()  function, which uses the following syntax:
<b>numpy.digitize(x, bins, right=False)</b>
where:
<b>x: </b>Array to be binned.
<b>bins: </b>Array of bins.
<b>right: </b>Indicating whether the intervals include the right or the left bin edge. Default is that the interval does not include the right edge.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Place All Values into Two Bins</h3>
The following code shows how to place the values of an array into two bins:
<b>0</b> if x &lt; 20
<b>1 </b>if x ≥ 20
<b>import numpy as np
#create data
data = [2, 4, 4, 7, 12, 14, 19, 20, 24, 31, 34]
#place values into bins
np.digitize(data, bins=[20])
array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1])
</b>
<h3>Example 2: Place All Values into Three Bins</h3>
The following code shows how to place the values of an array into three bins:
<b>0</b> if x &lt; 10
<b>1 </b>if 10 ≤ x &lt; 20
<b>2 </b>if x ≥ 20
<b>import numpy as np
#create data
data = [2, 4, 4, 7, 12, 14, 20, 22, 24, 31, 34]
#place values into bins
np.digitize(data, bins=[10, 20])
array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2])</b>
Note that if we specify right=<b>True </b>then the values would be placed into the following bins:
<b>0</b> if x ≤ 10
<b>1 </b>if 10 &lt; x ≤ 20
<b>2 </b>if x > 20
Each interval would include the right bin edge. Here’s what that looks like:
<b>import numpy as np
#create data
data = [2, 4, 4, 7, 12, 14, 20, 22, 24, 31, 34]
#place values into bins
np.digitize(data, bins=[10, 20], right=True)
array([0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2])</b>
<h3>Example 3: Place All Values into Four Bins</h3>
The following code shows how to place the values of an array into three bins:
<b>0</b> if x &lt; 10
<b>1 </b>if 10 ≤ x &lt; 20
<b>2 </b>if 20 ≤ x &lt; 30
<b>3 </b>if x ≥ 30
<b>import numpy as np
#create data
data = [2, 4, 4, 7, 12, 14, 20, 22, 24, 31, 34]
#place values into bins
np.digitize(data, bins=[10, 20, 30])
array([0, 0, 0, 0, 1, 1, 2, 2, 2, 3, 3])
</b>
<h3>Example 4: Count the Frequency of Each Bin</h3>
Another useful NumPy function that complements the numpy.digitize() function is the  numpy.bincount()  function, which counts the frequencies of each bin.
The following code shows how to place the values of an array into three bins and then count the frequency of each bin:
<b>import numpy as np
#create data
data = [2, 4, 4, 7, 12, 14, 20, 22, 24, 31, 34]
#place values into bins
bin_data = np.digitize(data, bins=[10, 20])
#view binned data
bin_data
array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2])
#count frequency of each bin
np.bincount(bin_data)
array([4, 2, 5])
</b>
The output tells us that:
Bin “0” contains <b>4 </b>data values.
Bin “1” contains <b>2 </b>data values.
Bin “2” contains <b>5 </b>data values.
<em>Find more Python tutorials  here .</em>
<h2><span class="orange">How to Calculate Dot Product Using NumPy</span></h2>
Given vector <em>a</em> = [a<sub>1</sub>, a<sub>2</sub>, a<sub>3</sub>] and vector <em>b</em> = [b<sub>1</sub>, b<sub>2</sub>, b<sub>3</sub>], the <b>dot product</b> of the vectors, denoted as <b>a · b</b>, is given by:
<b>a · b</b> = a<sub>1</sub> * b<sub>1</sub> + a<sub>2</sub> * b<sub>2</sub> + a<sub>3</sub> * b<sub>3</sub>
For example, if <em>a</em> = [2, 5, 6] and <em>b</em> = [4, 3, 2], then the dot product of <em>a</em> and <em>b</em> would be equal to:
<b>a · b = </b>2*4 + 5*3 + 6*2
<b>a · b = </b>8 + 15 + 12
<b>a · b = </b>35
Simply put, the dot product is the sum of the products of the corresponding entries in two vectors.
In Python, you can use the <b>numpy.dot()</b> function to quickly calculate the dot product between two vectors:
<b>import numpy as np
np.dot(a, b)
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Calculate Dot Product Between Two Vectors</h3>
The following code shows how to use <b>numpy.dot()</b> to calculate the dot product between two vectors:
<b>import numpy as np
#define vectors
a = [7, 2, 2]
b = [1, 4, 9]
#calculate dot product between vectors
np.dot(a, b)
33
</b>
Here is how this value was calculated:
<b>a · b = </b>7*1 + 2*4 + 2*9
<b>a · b = </b>7 + 8 + 18
<b>a · b = </b>33
<h3>Example 2: Calculate Dot Product Between Two Columns</h3>
The following code shows how to use <b>numpy.dot()</b> to calculate the dot product between two columns in a pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'A': [4, 6, 7, 7, 9],   'B': [5, 7, 7, 2, 2],   'C': [11, 8, 9, 6, 1]})
#view DataFrame
df
ABC
04511
1678
2779
3726
4921
#calculate dot product between column A and column C
np.dot(df.A, df.C)
206
</b>
Here is how this value was calculated:
<b>A · C = </b>4*11 + 6*8 + 7*9 + 7*6 + 9*1
<b>A · C = </b>44 + 48 + 63 + 42 + 9
<b>A · C = </b>206
<b>Note:</b> Keep in mind that Python will throw an error if the two vectors you’re calculating the dot product for have different lengths.
<h2><span class="orange">How to Fill NumPy Array with Values (2 Examples)</span></h2>
You can use the following methods to fill a NumPy array with values:
<b>Method 1: Use np.full()</b>
<b>#create NumPy array of length 10 filled with 3's
my_array = np.full(10, 3)
</b>
<b>Method 2: Use fill()</b>
<b>#create empty NumPy array with length of 10
my_array = np.empty(10)
#fill NumPy array with 3's
my_array.fill(3)</b>
The following examples show how to use each function in practice.
<h2>Example 1: Use np.full()</h2>
The following code shows how to use the <b>np.full()</b> function to fill up a NumPy array of length 10 with the value 3 in each position:
<b>import numpy as np
#create NumPy array of length 10 filled with 3's
my_array = np.full(10, 3)
#view NumPy array
print(my_array)
[3 3 3 3 3 3 3 3 3 3]
</b>
The NumPy array is filled with a value of 3 in each position.
We can use similar syntax to create a NumPy array of any size.
For example, the following code shows how to create a NumPy array with 7 rows and 2 columns:
<b>import numpy as np
#create NumPy array filled with 3's
my_array = np.full((7, 2), 3)
#view NumPy array
print(my_array)
[[3 3]
 [3 3]
 [3 3]
 [3 3]
 [3 3]
 [3 3]
 [3 3]]
</b>
The result is a NumPy array with 7 rows and 2 columns where each position is filled with a value of 3.
<h2>Example 2: Use fill()</h2>
The following code shows how to use the <b>fill()</b> function to fill up an empty NumPy array with the value 3 in each position:
<b>#create empty NumPy array with length of 10
my_array = np.empty(10)
#fill NumPy array with 3's
my_array.fill(3)
#view NumPy array
print(my_array)
[3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]
</b>
The result is a NumPy array in which each position contains the value 3.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Replace Elements in NumPy Array 
 How to Count Unique Values in NumPy Array 
 How to Filter a NumPy Array 
<h2><span class="orange">How to Filter a NumPy Array (4 Examples)</span></h2>
You can use the following methods to filter the values in a NumPy array:
<b>Method 1: Filter Values Based on One Condition</b>
<b>#filter for values less than 5
my_array[my_array &lt; 5]
</b>
<b>Method 2: Filter Values Using “OR” Condition</b>
<b>#filter for values less than 5 <em>or</em> greater than 9
my_array[(my_array &lt; 5) | (my_array > 9)]
</b>
<b>Method 3: Filter Values Using “AND” Condition</b>
<b>#filter for values greater than 5 <i>and </i>less than 9
my_array[(my_array > 5) & (my_array &lt; 9)]</b>
<b>Method 4: Filter Values Contained in List</b>
<b>#filter for values that are equal to 2, 3, 5, or 12
my_array[np.in1d(my_array, [2, 3, 5, 12])]
</b>
This tutorial explains how to use each method in practice with the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([1, 2, 2, 3, 5, 6, 7, 10, 12, 14])
#view NumPy array
my_array
array([ 1,  2,  2,  3,  5,  6,  7, 10, 12, 14])
</b>
<h2>Example 1: Filter Values Based on One Condition</h2>
The following code shows how to filter values in the NumPy array based on just one condition:
<b>#filter for values less than 5
my_array[(my_array &lt; 5)]
array([1, 2, 2, 3])
#filter for values greater than 5
my_array[(my_array > 5)]
array([ 6,  7, 10, 12, 14])
#filter for values equal to 5
my_array[(my_array == 5)]
array([5])
</b>
<h2>Example 2: Filter Values Using “OR” Condition</h2>
The following code shows how to filter values in the NumPy array using an “OR” condition:
<b>#filter for values less than 5 <em>or</em> greater than 9
my_array[(my_array &lt; 5) | (my_array > 9)]
array([ 1,  2,  2,  3, 10, 12, 14])
</b>
This filter returns the values in the NumPy array that are less than 5 <b>or</b> greater than 9.
<h2>Example 3: Filter Values Using “AND” Condition</h2>
The following code shows how to filter values in the NumPy array using an “AND” condition:
<b>#filter for values greater than 5 <i>and </i>less than 9
my_array[(my_array > 5) & (my_array &lt; 9)]
array([6, 7])
</b>
This filter returns the values in the NumPy array that are greater than 5 <b>and </b>less than 9.
<h2>Example 4: Filter Values Contained in List</h2>
The following code shows how to filter values in the NumPy array that are contained in a list:
<b>#filter for values that are equal to 2, 3, 5, or 12
my_array[np.in1d(my_array, [2, 3, 5, 12])]
array([ 2,  2,  3,  5, 12])
</b>
This filter returns only the values that are equal to 2, 3, 5, or 12.
<b>Note</b>: You can find the complete documentation for the NumPy <b>in1d()</b> function  here .
<h2><span class="orange">How to Find Index of Value in NumPy Array (With Examples)</span></h2>
You can use the following methods to find the index position of specific values in a NumPy array:
<b>Method 1: Find All Index Positions of Value</b>
<b>np.where(x==value)
</b>
<b>Method 2: Find First Index Position of Value</b>
<b>np.where(x==value)[0][0]</b>
<b>Method 3: Find First Index Position of Several Values</b>
<b>#define values of interest
vals = np.array([value1, value2, value3])
#find index location of first occurrence of each value of interest
sorter = np.argsort(x)
sorter[np.searchsorted(x, vals, sorter=sorter)]</b>
The following examples show how to use each method in practice.
<h3>Method 1: Find All Index Positions of Value</h3>
The following code shows how to find every index position that is equal to a certain value in a NumPy array:
<b>import numpy as np
#define array of values
x = np.array([4, 7, 7, 7, 8, 8, 8])
#find all index positions where x is equal to 8
np.where(x==8)
(array([4, 5, 6]),)
</b>
From the output we can see that index positions 4, 5, and 6 are all equal to the value <b>8</b>.
<h3>Method 2: Find First Index Position of Value</h3>
The following code shows how to find the first index position that is equal to a certain value in a NumPy array:
<b>import numpy as np
#define array of values
x = np.array([4, 7, 7, 7, 8, 8, 8])
#find first index position where x is equal to 8
np.where(x==8)[0][0]
4
</b>
From the output we can see that the value <b>8</b> first occurs in index position 4.
<h3>Method 3: Find First Index Position of Several Values</h3>
The following code shows how to find the first index position of several values in a NumPy array:
<b>import numpy as np
#define array of values
x = np.array([4, 7, 7, 7, 8, 8, 8])
#define values of interest
vals = np.array([4, 7, 8])
#find index location of first occurrence of each value of interest
sorter = np.argsort(x)
sorter[np.searchsorted(x, vals, sorter=sorter)]
array([0, 1, 4])
</b>
From the output we can see:
The value <b>4</b> first occurs in index position 0.
The value <b>7</b> first occurs in index position 1.
The value <b>8</b> first occurs in index position 4.
<h2><span class="orange">How to Fix: ‘numpy.float64’ object cannot be interpreted as an integer</span></h2>
One error you may encounter when using NumPy is:
<b>TypeError: 'numpy.float64' object cannot be interpreted as an integer
</b>
This error occurs when you supply a float to some function that expects an integer.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to use the following for loop to print out various numbers in a NumPy array:
<b>import numpy as np
#define array of values
data = np.array([3.3, 4.2, 5.1, 7.7, 10.8, 11.4])
#use for loop to print out range of values at each index
for i in range(len(data)):
    print(range(data[i]))
TypeError: 'numpy.float64' object cannot be interpreted as an integer
</b>
We receive an error because the <b>range()</b> function expects an integer, but the values in the NumPy array are floats.
<h3>How to Fix the Error</h3>
There are two ways to quickly fix this error:
<b>Method 1: Use the int() Function</b>
One way to fix this error is to simply wrap the call with <b>int()</b> as follows:
<b>import numpy as np
#define array of values
data = np.array([3.3, 4.2, 5.1, 7.7, 10.8, 11.4])
#use for loop to print out range of values at each index
for i in range(len(data)):
    print(range(int(data[i])))
range(0, 3)
range(0, 4)
range(0, 5)
range(0, 7)
range(0, 10)
range(0, 11)
</b>
By using the <b>int()</b> function, we convert each float value in the NumPy array to an integer so we avoid the <b>TypeError</b> we encountered earlier.
<b>Method 2: Use the .astype(int) Function</b>
Another way to fix this error is to first convert the values in the NumPy array to integers:
<b>import numpy as np
#define array of values
data = np.array([3.3, 4.2, 5.1, 7.7, 10.8, 11.4])
#convert array of floats to array of integers
data_int = data.astype(int)
#use for loop to print out range of values at each index
for i in range(len(data)):
    print(range(data[i]))
range(0, 3)
range(0, 4)
range(0, 5)
range(0, 7)
range(0, 10)
range(0, 11)</b>
Using this method, we avoid the <b>TypeError</b> once again.
<h2><span class="orange">How to Fix: ‘numpy.float64’ object does not support item assignment</span></h2>
One common error you may encounter when using Python is:
<b>TypeError: 'numpy.float64' object does not support item assignment
</b>
This error usually occurs when you attempt to use brackets to assign a new value to a NumPy variable that has a type of <b>float64</b>.
The following example shows how to resolve this error in practice.
<h2>How to Reproduce the Error</h2>
Suppose we create some NumPy variable that has a value of <b>15.22</b> and we attempt to use brackets to assign it a new value of <b>13.7</b>:
<b>import numpy as np
#define some float value
one_float = np.float64(15.22)
#attempt to modify float value to be 13.7
one_float[0] = 13.7
TypeError: 'numpy.float64' object does not support item assignment
</b>
We receive the error that<b> ‘numpy.float64’ object does not support item assignment</b>.
We received this error because <b>one_float</b> is a scalar but we attempted to treat it like an array where we could use brackets to change the value in index position 0.
Since <b>one_float</b> is not an array, we can’t use brackets when attempting to change its value.
<h2>How to Fix the Error</h2>
The way to resolve this error is to simply not use brackets when assigning a new value to the float:
<b>#modify float value to be 13.7
one_float = 13.7
#view float
print(one_float)
13.7</b>
We’re able to successfully change the value from <b>15.22</b> to <b>13.7</b> because we didn’t use brackets.
Note that it’s fine to use brackets to change values in specific index positions as long as you’re working with an array.
For example, the following code shows how to change the first element in a NumPy array from <b>15.22</b> to <b>13.7</b> by using bracket notation:
<b>import numpy as np
#define a NumPy array of floats
many_floats = np.float64([15.22, 34.2, 15.4, 13.2, 33.4])
#modify float value in first index position of array to be 13.7
many_floats[0] = 13.7
#view updated array
print(many_floats)
[13.7 34.2 15.4 13.2 33.4]
</b>
This time we don’t receive an error either because we’re working with a NumPy array so it makes sense to use brackets.
<h2>Additional Resources</h2>
The following tutorials explain how to fix other common errors in Python:
 How to Fix in Python: ‘numpy.ndarray’ object is not callable 
 How to Fix: TypeError: ‘numpy.float64’ object is not callable 
 How to Fix: Typeerror: expected string or bytes-like object 
<h2><span class="orange">How to Fix: ‘numpy.float64’ object is not iterable</span></h2>
One error you may encounter when using NumPy is:
<b>TypeError: 'numpy.float64' object is not iterable
</b>
This error occurs when you attempt to perform some iterative operation on a a float value in NumPy, which isn’t possible.
The following example shows how to address this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#define array of data
data = np.array([1.3, 1.5, 1.6, 1.9, 2.2, 2.5])
#display array of data
print(data)
[1.3 1.5 1.6 1.9 2.2 2.5]</b>
Now suppose we attempt to print the sum of every value in the array:
<b>#attempt to print the sum of every value
for i in data:
    print(sum(i))
TypeError: 'numpy.float64' object is not iterable
</b>
We received an error because we attempted to perform an iterative operation (taking the sum of values) on each individual float value in the array.
<h3>How to Fix the Error</h3>
We can avoid this error in two ways:
<b>1. Performing a non-iterative operation on each value in the array.</b>
For example, we could print each value in the array:
<b>#print every value in array
for i in data:
    print(i)
1.3
1.5
1.6
1.9
2.2
2.5
</b>
We don’t receive an error because we didn’t attempt to perform an iterative operation on each value.
<b>2. Perform an iterative operation on a multi-dimensional array.</b>
 We could also avoid an error by performing an iterative operation on an array that is multi-dimensional:
<b>#create multi-dimensional array
data2 = np.array([[1.3, 1.5], [1.6, 1.9], [2.2, 2.5]])
#print sum of each element in array
for i in data2:
    print(sum(i))
2.8
3.5
4.7
</b>
We don’t receive an error because it made sense to use the <b>sum()</b> function on a multi-dimensional array.
In particular, here’s how NumPy calculated the sum values:
1.3 + 1.5 = <b>2.8</b>
1.6 + 1.9 = <b>3.5</b>
2.2 + 2.5 = <b>4.7</b>
<h2><span class="orange">How to Get Specific Column from NumPy Array (With Examples)</span></h2>
You can use the following syntax to get a specific column from a NumPy array:
<b>#get column in index position 2 from NumPy array
my_array[:, 2]
</b>
The following examples shows how to use this syntax in practice.
<h3>Example 1: Get One Column from NumPy Array</h3>
The following code shows how to get one specific column from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get column in index position 2
data[:, 2]
array([ 3,  7, 11])
</b>
If you’d like to get a column from a NumPy array and retrieve it as a <b>column vector</b>, you can use the following syntax:
<b>#get column in index position 2 (as a column vector)
data[:, [2]]
array([[ 3],
       [ 7],
       [11]])</b>
<h3>Example 2: Get Multiple Columns from NumPy Array</h3>
The following code shows how to get multiple columns from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get columns in index positions 1 and 3 from NumPy array
data[:, [1,3]]
array([[ 2,  4],
       [ 6,  8],
       [10, 12]])
</b>
<h3>Example 3: Get Columns in Range from NumPy Array</h3>
The following code shows how to get columns in a range from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get columns in index positions 0 through 3 (not including 3)
data[:, 0:3]
array([[ 1,  2,  3],
       [ 5,  6,  7],
       [ 9, 10, 11]])
</b>
Note that the last value in the range (in this case, 3) is not included in the range of columns that is returned.
<h2><span class="orange">NumPy: How to Get Indices Where Value is True</span></h2>
You can use the following methods to get the indices where some condition is true in NumPy:
<b>Method 1: Get Indices Where Condition is True in NumPy Array</b>
<b>#get indices of values greater than 10
np.asarray(my_array>10).nonzero()
</b>
<b>Method 2: Get Indices Where Condition is True in NumPy Matrix</b>
<b>#get indices of values greater than 10
np.transpose((my_matrix>10).nonzero())
</b>
<b>Method 3: Get Indices Where Condition is True in Any Row of NumPy Matrix</b>
<b>#get indices of rows where any value is greater than 10</b>
<b>np.asarray(np.any(my_matrix>10, axis=1)).nonzero()</b>
The following examples show how to use each method in practice.
<h2>Example 1: Get Indices Where Condition is True in NumPy Array</h2>
The following code shows how to get all indices in a NumPy array where the value is greater than 10:
<b>import numpy as np
#create NumPy array
my_array = np.array([2, 2, 4, 5, 7, 9, 11, 12, 3, 19])
#get index of values greater than 10
np.asarray(my_array>10).nonzero()
(array([6, 7, 9], dtype=int32),)
</b>
From the output we can see that the values in index positions <b>6</b>, <b>7</b>, and <b>9</b> of the original NumPy array have values greater than 10.
<h2>Example 2: Get Indices Where Condition is True in NumPy Matrix</h2>
The following code shows how to get all indices in a NumPy matrix where the value is greater than 10:
<b>import numpy as np
#create NumPy matrix
my_matrix = np.array([[2, 5, 9, 12],     [6, 7, 8, 8],     [2, 5, 7, 8],     [4, 1, 15, 11]])
#get index of values greater than 10
np.transpose((my_matrix>10).nonzero())
array([[0, 3],
       [3, 2],
       [3, 3]], dtype=int32)</b>
From the output we can see that the values in the following index positions of the matrix have values greater than 10:
[0, 3]
[3, 2]
[3, 3]
<h2>Example 3: Get Indices Where Condition is True in Any Row of NumPy Matrix</h2>
The following code shows how to get all row indices in a NumPy matrix where the value is greater than 10 in <em>any</em> element of the row:
<b>import numpy as np
#create NumPy matrix
my_matrix = np.array([[2, 5, 9, 12],     [6, 7, 8, 8],     [2, 5, 7, 8],     [4, 1, 15, 11]])
#get index of rows where any value is greater than 10
np.asarray(np.any(my_matrix>10, axis=1)).nonzero()
(array([0, 3], dtype=int32),)
</b>
From the output we can see that rows <b>0</b> and <b>3</b> have at least one value greater than 10.
<b>Note</b>: To get indices where a condition is true in a column, use<b> axis=0</b> instead.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Fill NumPy Array with Values 
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Get Specific Row from NumPy Array (With Examples)</span></h2>
You can use the following syntax to get a specific row from a NumPy array:
<b>#get row in index position 2 from NumPy array
my_array[2, :]
</b>
The following examples shows how to use this syntax in practice.
<h2>Example 1: Get One Row from NumPy Array</h2>
The following code shows how to get one specific row from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
print(data)
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get row in index position 2
data[2, :]
array([ 9, 10, 11, 12])</b>
Notice that only the row in index position 2 of the NumPy array is returned.
<h2>Example 2: Get Multiple Rows from NumPy Array</h2>
The following code shows how to get multiple rows from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get rows in index positions 0 and 2 from NumPy array
data[[0,2], :]
array([[ 1,  2,  3,  4],
       [ 9, 10, 11, 12]])
</b>
<h2>Example 3: Get Rows in Range from NumPy Array</h2>
The following code shows how to get rows in a range from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
#view NumPy array
data
array([[ 1,  2,  3,  4],
       [ 5,  6,  7,  8],
       [ 9, 10, 11, 12]])
#get rows in index positions 0 through 1
data[0:2, :]
array([[1, 2, 3, 4],
       [5, 6, 7, 8]])
</b>
Note that the last value in the range (in this case, 2) is not included in the range of rows that is returned.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in NumPy:
 How to Get Specific Column from NumPy Array 
 How to Map a Function Over a NumPy Array 
 How to Add a Column to a NumPy Array 
<h2><span class="orange">How to Perform Least Squares Fitting in NumPy (With Example)</span></h2>
The <b>method of least squares</b> is a method we can use to find the regression line that best fits a given dataset.
We can use the <b>linalg.lstsq(</b>) function in NumPy to perform least squares fitting.
The following step-by-step example shows how to use this function in practice.
<h2>Step 1: Enter the Values for X and Y</h2>
First, let’s create the following NumPy arrays:
<b>import numpy as np
#define x and y arrays
x = np.array([6, 7, 7, 8, 12, 14, 15, 16, 16, 19])
y = np.array([14, 15, 15, 17, 18, 18, 19, 24, 25, 29])
</b>
<h2>Step 2: Perform Least Squares Fitting</h2>
We can use the following code to perform least squares fitting and find the line that best “fits” the data:
<b>#perform least squares fitting
np.linalg.lstsq(np.vstack([x, np.ones(len(x))]).T, y, rcond=None)[0]
array([0.96938776, 7.76734694])
</b>
The result is an array that contains the <b>slope</b> and <b>intercept</b> values for the line of best fit.
From the output we can see:
Slope: <b>0.969</b>
Intercept: <b>7.767</b>
Using these two values, we can write the equation for the line of best fit:
<U+0177> = 7.767 + 0.969x
<h2>Step 3: Interpret the Results</h2>
Here’s how to interpret the line of best fit:
When x is equal to 0, the average value for y is <b>7.767</b>.
For each one unit increase in x, y increases by an average of <b>.969</b>.
We can also use the line of best fit to predict the value of y based on the value of x.
For example, if x has a value of 10 then we predict that the value of y would be <b>17.457</b>:
<U+0177> = 7.767 + 0.969x
<U+0177> = 7.767 + 0.969(10)
<U+0177> = 17.457
<h2>Bonus: Video Explanation of Least Squares Fitting</h2>
Refer to the video below for a simple explanation of least squares fitting:
<iframe title="Simple Linear Regression" src="https://www.youtube.com/embed/HKjFoBXeaGI?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Specific Elements from NumPy Array 
 How to Get the Index of Max Value in NumPy Array 
 How to Fill NumPy Array with Values 
<h2><span class="orange">NumPy: The Difference Between np.linspace and np.arange</span></h2>
When it comes to creating a sequence of values, <b>linspace</b> and <b>arange</b> are two commonly used NumPy functions.
Here is the subtle difference between the two functions:
<b>linspace </b>allows you to specify the <em>number </em>of steps
<b>arange </b>allows you to specify the <em>size </em>of the steps
The following examples show how to use each function in practice.
<h2>Example 1: How to Use np.linspace</h2>
The <b>np.linspace()</b> function uses the following basic syntax:
<b>np.linspace(start, stop, num, …)</b>
where:
<b>start</b>: The starting value of the sequence
<b>stop</b>: The end value of the sequence
<b>num</b>: the number of values to generate
The following code shows how to use <b>np.linspace()</b> to create 11 values evenly spaced between 0 and 20:
<b>import numpy as np
#create sequence of 11 evenly spaced values between 0 and 20
np.linspace(0, 20, 11)
array([ 0.,  2.,  4.,  6.,  8., 10., 12., 14., 16., 18., 20.])
</b>
The result is an array of 11 values that are evenly spaced between 0 and 20.
Using this method, <b>np.linspace()</b> automatically determines how far apart to space the values.
<h2>Example 2: How to Use np.arange</h2>
The <b>np.arange()</b> function uses the following basic syntax:
<b>np.arange(start, stop, step, …)</b>
where:
<b>start</b>: The starting value of the sequence
<b>stop</b>: The end value of the sequence
<b>step</b>: The spacing between values
The following code shows how to use <b>np.arange()</b> to create a sequence of values between 0 and 20 where the spacing between each value is 2:
<b>import numpy as np
#create sequence of values between 0 and 20 where spacing is 2
np.arange(0, 20, 2)
array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
</b>
The result is a sequence of values between 0 and 20 where the spacing between each value is 2.
Using this method, <b>np.arange()</b> automatically determines how many values to generate.
If we use a different step size (like 4) then<b> np.arange()</b> will automatically adjust the total number of values generated:
<b>import numpy as np
#create sequence of values between 0 and 20 where spacing is 4
np.arange(0, 20, 4)
array([ 0,  4,  8, 12, 16])
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Python:
 How to Fill NumPy Array with Values 
 How to Replace Elements in NumPy Array 
 How to Count Unique Values in NumPy Array 
<h2><span class="orange">How to Calculate the Magnitude of a Vector Using NumPy</span></h2>
The <b>magnitude</b> of a given vector, x, is calculated as:
||x|| = √x<sub>1</sub><sup>2</sup> + x<sub>2</sub><sup>2</sup> + x<sub>3</sub><sup>2</sup> + … + x<sub>n</sub><sup>2</sup>
For example, suppose x = [3, 7, 4]
The magnitude would be calculated as:
||x|| = √3<sup>2</sup> + 7<sup>2</sup> + 4<sup>2</sup> = √74 = 8.602
You can use one of the following two methods to calculate the magnitude of a vector using the  NumPy  package in Python:
<b>Method 1: Use linalg.norm()</b>
<b>np.linalg.norm(v)</b>
<b>Method 2: Use Custom NumPy Functions</b>
<b>np.sqrt(x.dot(x))
</b>
Both methods will return the exact same result, but the second method tends to be much faster especially for large vectors.
The following example shows how to use each method in practice.
<h3>Method 1: Use linalg.norm()</h3>
The following code shows how to use the <b>np.linalg.norm()</b> function to calculate the magnitude of a given vector:
<b>import numpy as np
#define vector
x = np.array([3, 6, 6, 4, 8, 12, 13])
#calculate magnitude of vector
np.linalg.norm(x)
21.77154105707724
</b>
The magnitude of the vector is <b>21.77</b>.
<h3>Method 2: Use Custom NumPy Functions</h3>
The following code shows how to use custom NumPy functions to calculate the magnitude of a given vector:
<b>import numpy as np
#define vector
x = np.array([3, 6, 6, 4, 8, 12, 13])
#calculate magnitude of vector
np.sqrt(x.dot(x))
21.77154105707724
</b>
The magnitude of the vector is <b>21.77</b>.
Notice that this matches the value that we calculated using the previous method.
<h2><span class="orange">How to Map a Function Over a NumPy Array (With Examples)</span></h2>
You can use the following basic syntax to map a function over a NumPy array:
<b>#define function
my_function = lambda x: x*5
#map function to every element in NumPy array
my_function(my_array)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Map Function Over 1-Dimensional NumPy Array</h3>
The following code shows how to map a function to a NumPy array that multiplies each value by 2 and then adds 5:
<b>import numpy as np
#create NumPy array
data = np.array([1, 3, 4, 4, 7, 8, 13, 15])
#define function
my_function = lambda x: x*2+5
#apply function to NumPy array
my_function(data)
array([ 7, 11, 13, 13, 19, 21, 31, 35])
</b>
Here is how each value in the new array was calculated:
First value: 1*2+5 = <b>7</b>
Second value: 3*2+5 = <b>11</b>
Third value: 4*2+5 = <b>13</b>
And so on.
<h3>Example 2: Map Function Over Multi-Dimensional NumPy Array</h3>
The following code shows how to map a function to a multi-dimensional NumPy array that multiplies each value by 2 and then adds 5:
<b>import numpy as np
#create NumPy array
data = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])
#view NumPy array
print(data)
[[1 2 3 4]
 [5 6 7 8]]
#define function
my_function = lambda x: x*2+5
#apply function to NumPy array
my_function(data)
array([[ 7,  9, 11, 13],
       [15, 17, 19, 21]])</b>
Notice that this syntax worked with a multi-dimensional array just as well as it worked with a one-dimensional array.
<h2><span class="orange">How to Convert NumPy Matrix to Array (With Examples)</span></h2>
You can use the following methods to convert a NumPy matrix to an array:
<b>Method 1: Use A1</b>
<b>my_array = my_matrix.A1
</b>
<b>Method 2: Use ravel()</b>
<b>my_array = np.asarray(my_matrix).ravel()
</b>
Both methods return the same result, but the second method simply requires more typing.
The following examples show how to use each method in practice.
<h2>Example 1: Convert NumPy Matrix to Array Using A1</h2>
The following code shows how to use the  A1 property  to convert a NumPy matrix to an array:
<b>import numpy as np
#create NumPy matrix with 3 columns and 5 rows
my_matrix = np.matrix(np.arange(15).reshape((5, 3)))
#view NumPy matrix
print(my_matrix)
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]
 [12 13 14]]
#convert matrix to array
my_array = my_matrix.A1
#view NumPy array
print(my_array)
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
</b>
We can see that the NumPy matrix has been converted to an array with 15 values.
We can confirm that it is NumPy array by using the <b>type()</b> function:
<b>#check type of my_array
type(my_array)
numpy.ndarray
</b>
It is indeed a NumPy array.
<h2>Example 2: Convert NumPy Matrix to Array Using ravel()</h2>
The following code shows how to use the  ravel()  function to convert a NumPy matrix to an array:
<b>import numpy as np
#create NumPy matrix with 3 columns and 5 rows
my_matrix = np.matrix(np.arange(15).reshape((5, 3)))
#view NumPy matrix
print(my_matrix)
[[ 0  1  2]
 [ 3  4  5]
 [ 6  7  8]
 [ 9 10 11]
 [12 13 14]]
#convert matrix to array
my_array = np.asarray(my_matrix).ravel()
#view NumPy array
print(my_array)
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
</b>
We can see that the NumPy matrix has been converted to an array with 15 values.
We can confirm that it is NumPy array by using the <b>type()</b> function:
<b>#check type of my_array
type(my_array)
numpy.ndarray
</b>
It is indeed a NumPy array.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Fill NumPy Array with Values 
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Calculate the Mode of NumPy Array (With Examples)</span></h2>
You can use the following basic syntax to find the mode of a NumPy array:
<b>#find unique values in array along with their counts
vals, counts = np.unique(array_name, return_counts=True)
#find mode
mode_value = np.argwhere(counts == np.max(counts))</b>
Recall that the <b>mode</b> is the value that occurs most often in an array.
Note that it’s possible for an array to have one mode or multiple modes.
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculating Mode of NumPy Array with Only One Mode</h3>
The following code shows how to find the mode of a NumPy array in which there is only one mode:
<b>import numpy as np
#create NumPy array of values with only one mode
x = np.array([2, 2, 2, 3, 4, 4, 5, 5, 5, 5, 7])
#find unique values in array along with their counts
vals, counts = np.unique(x, return_counts=True)
#find mode
mode_value = np.argwhere(counts == np.max(counts))
#print list of modes
print(vals[mode_value].flatten().tolist())
[5]
#find how often mode occurs
print(np.max(counts))
4</b>
From the output we can see that the mode is <b>5</b> and it occurs <b>4</b> times in the NumPy array.
<h3>Example 2: Calculating Mode of NumPy Array with Multiple Modes</h3>
The following code shows how to find the mode of a NumPy array in which there are multiple modes:
<b>import numpy as np
#create NumPy array of values with multiple modes
x = np.array([2, 2, 2, 3, 4, 4, 4, 5, 5, 5, 7])
#find unique values in array along with their counts
vals, counts = np.unique(x, return_counts=True)
#find mode
mode_value = np.argwhere(counts == np.max(counts))
#print list of modes
print(vals[mode_value].flatten().tolist())
[2, 4, 5]
#find how often mode occurs
print(np.max(counts))
3</b>
From the output we can see that this NumPy array has three modes: <b>2</b>, <b>4</b>, and <b>5</b>.
We can also see that each of these values occurs <b>3</b> times in the array.
<h2><span class="orange">How to Find Most Frequent Value in NumPy Array (With Examples)</span></h2>
You can use the following methods to find the most frequent value in a NumPy array:
<b>Method 1: Find Most Frequent Value</b>
<b>#find frequency of each value
values, counts = np.unique(my_array, return_counts=True)
#display value with highest frequency
values[counts.argmax()]
</b>
If there are multiple values that occur most frequently in the NumPy array, this method will only return the first value.
<b>Method 2: Find Each Most Frequent Value</b>
<b>#find frequency of each value
values, counts = np.unique(my_array, return_counts=True)
#display all values with highest frequencies
values[counts == counts.max()]
</b>
If there are multiple values that occur most frequently in the NumPy array, this method will return each of the most frequently occurring values.
The following examples show how to use each method in practice.
<h2>Example 1: Find Most Frequent Value in NumPy Array</h2>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([1, 2, 4, 4, 4, 5, 6, 7, 12])
</b>
Notice that there is only one value that occurs most frequently in this array: <b>4</b>.
We can use the <b>argmax()</b> function to return the value that occurs most frequently in the array:
<b>#find frequency of each value
values, counts = np.unique(my_array, return_counts=True)
#display value with highest frequency
values[counts.argmax()]
4</b>
The function correctly returns the value <b>4</b>.
<h2>Example 2: Find Each Most Frequent Value in NumPy Array</h2>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
my_array = np.array([1, 2, 4, 4, 4, 5, 6, 7, 12, 12, 12])
</b>
Notice that there are two values that occur most frequently in this array: <b>4</b> and <b>12</b>.
We can use the <b>max()</b> function to return each of the values that occur most frequently in the array:
<b>#find frequency of each value
values, counts = np.unique(my_array, return_counts=True)
#display each value with highest frequency
values[counts == counts.max()]
array([ 4, 12])
</b>
The function correctly returns the values <b>4</b> and <b>12</b>.
<b>Note</b>: You can find the complete documentation for the NumPy <b>unique()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Duplicate Elements in NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Rank Items in NumPy Array 
<h2><span class="orange">How to Fix: ‘numpy.ndarray’ object has no attribute ‘append’</span></h2>
One error you may encounter when using NumPy is:
<b>AttributeError: 'numpy.ndarray' object has no attribute 'append'
</b>
This error occurs when you attempt to append one or more values to the end of a NumPy array by using the <b>append()</b> function in regular Python.
Since NumPy doesn’t have an append attribute, an error is thrown. To fix this, you must use <b>np.append()</b> instead.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to append a new value to the end of a NumPy array using the <b>append()</b> function from regular Python:
<b>import numpy as np
#define NumPy array
x = np.array([1, 4, 4, 6, 7, 12, 13, 16, 19, 22, 23])
#attempt to append the value '25' to end of NumPy array
x.append(25)
AttributeError: 'numpy.ndarray' object has no attribute 'append'
</b>
We receive an error because NumPy doesn’t have an append attribute.
<h3>How to Fix the Error</h3>
To fix this error, we simply need to use <b>np.append()</b> instead:
<b>import numpy as np
#define NumPy array
x = np.array([1, 4, 4, 6, 7, 12, 13, 16, 19, 22, 23])
#append the value '25' to end of NumPy array
x = np.append(x, 25)
#view updated array
x
array([ 1,  4,  4,  6,  7, 12, 13, 16, 19, 22, 23, 25])
</b>
By using <b>np.append()</b> we were able to successfully append the value ’25’ to the end of the array.
Note that if you’d like to append one NumPy array to the end of another NumPy array, it’s best to use the <b>np.concatenate()</b> function:
<b>import numpy as np
#define two NumPy arrays
a = np.array([1, 4, 4, 6, 7, 12, 13, 16, 19, 22, 23])
b = np.array([25, 26, 26, 29])
#concatenate two arrays together
c = np.concatenate((a, b))
#view resulting array
c
array([ 1,  4,  4,  6,  7, 12, 13, 16, 19, 22, 23, 25, 26, 26, 29])
</b>
Refer to the online documentation for an in-depth explanation of both the array and concatenate functions:
 numpy.append() documentation 
 numpy.concatenate() documentation 
<h2><span class="orange">How to Fix: ‘numpy.ndarray’ object has no attribute ‘index’</span></h2>
One error you may encounter when using NumPy is:
<b>AttributeError: 'numpy.ndarray' object has no attribute 'index'
</b>
This error occurs when you attempt to use the <b>index()</b> function on a NumPy array, which does not have an index attribute available to use.
The following example shows how to address this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
x = np.array([4, 7, 3, 1, 5, 9, 9, 15, 9, 18])
</b>
We can use the following syntax to find the minimum and maximum values in the array:
<b>#find minimum and maximum values of array
min_val = np.min(x)
max_val = np.max(x)
#print minimum and maximum values
print(min_val, max_val)
1 18</b>
Now suppose we attempt to find the index position of the minimum and maximum values in the array:
<b>#attempt to print index position of minimum value
x.index(min_val)
AttributeError: 'numpy.ndarray' object has no attribute 'index'
</b>
We receive an error because we can’t apply an <b>index()</b> function to a NumPy array.
<h3>How to Address the Error</h3>
To find the index position of the minimum and maximum values in the NumPy array, we can use the NumPy <b>where()</b> function:
<b>#find index position of minimum value
np.where(x == min_val)
(array([3]),)
#find index position of maximum value
np.where(x == max_val)
(array([9]),)
</b>
From the output we can see:
The minimum value in the array is located in index position <b>3</b>.
The maximum value in the array is located in index position <b>9</b>.
We can use this same general syntax to find the index position of any value in a NumPy array.
For example, we can use the following syntax to find which index positions are equal to the value 9 in the NumPy array:
<b>#find index positions that are equal to the value 9
np.where(x == 9)
(array([5, 6, 8]),)
</b>
From the output we can see that the values in index positions 5, 6, and 8 are all equal to <b>9</b>.
<h2><span class="orange">How to Fix in Python: ‘numpy.ndarray’ object is not callable</span></h2>
One common error you may encounter when using NumPy in Python is:
<b>TypeError: 'numpy.ndarray' object is not callable
</b>
This error usually occurs when you attempt to call a NumPy array as a function by using round <b>()</b> brackets instead of square <b>[ ]</b> brackets.
The following example shows how to use this syntax in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
x = np.array([2, 4, 4, 5, 9, 12, 14, 17, 18, 20, 22, 25])
</b>
Now suppose we attempt to access the first element in the array:
<b>#attempt to access the first element in the array
x(0)
TypeError: 'numpy.ndarray' object is not callable
</b>
Since we used round <b>()</b> brackets Python thinks we’re attempting to call the NumPy array <b>x</b> as a function.
Since x is not a function, we receive an error.
<h3>How to Fix the Error</h3>
The way to resolve this error is to simply use square <b>[ ]</b> brackets when accessing elements of the NumPy array instead of round <b>()</b> brackets:
<b>#access the first element in the array
x[0]
2</b>
The first element in the array (2) is shown and we don’t receive any error because we used square <b>[ ]</b> brackets.
Also note that we can access multiple elements of the array at once as long as we use square <b>[ ]</b> brackets:
<b>#find sum of first three elements in array
x[0] + x[1] + x[2]
10</b>
<h2><span class="orange">How to Normalize a NumPy Matrix (With Examples)</span></h2>
To <b>normalize</b> a matrix means to scale the values such that that the range of the row or column values is between 0 and 1.
The easiest way to normalize the values of a NumPy matrix is to use the  normalize()  function from the sklearn package, which uses the following basic syntax:
<b>from sklearn.preprocessing import normalize
#normalize rows of matrix
normalize(x, axis=1, norm='l1')
#normalize columns of matrix
normalize(x, axis=0, norm='l1')
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Normalize Rows of NumPy Matrix</h3>
Suppose we have the following NumPy matrix:
<b>import numpy as np
#create matrix
x = np.arange(0, 36, 4).reshape(3,3)
#view matrix
print(x)
[[ 0  4  8]
 [12 16 20]
 [24 28 32]]
</b>
The following code shows how to normalize the rows of the NumPy matrix:
<b>from sklearn.preprocessing import normalize
#normalize matrix by rows
x_normed = normalize(x, axis=1, norm='l1')
#view normalized matrix
print(x_normed)
[[0.         0.33333333 0.66666667]
 [0.25       0.33333333 0.41666667]
 [0.28571429 0.33333333 0.38095238]]</b>
Notice that the values in each row now sum to one.
Sum of first row: 0 + 0.33 + 0.67 = <b>1</b>
Sum of second row: 0.25 + 0.33 + 0.417 = <b>1</b>
Sum of third row: 0.2857 + 0.3333 + 0.3809 = <b>1</b>
<h3>Example 2: Normalize Columns of NumPy Matrix</h3>
Suppose we have the following NumPy matrix:
<b>import numpy as np
#create matrix
x = np.arange(0, 36, 4).reshape(3,3)
#view matrix
print(x)
[[ 0  4  8]
 [12 16 20]
 [24 28 32]]
</b>
The following code shows how to normalize the rows of the NumPy matrix:
<b>from sklearn.preprocessing import normalize
#normalize matrix by columns
x_normed = normalize(x, axis=0, norm='l1')
#view normalized matrix
print(x_normed)
[[0.         0.08333333 0.13333333]
 [0.33333333 0.33333333 0.33333333]
 [0.66666667 0.58333333 0.53333333]]</b>
Notice that the values in each column now sum to one.
Sum of first column: 0 + 0.33 + 0.67 = <b>1</b>
Sum of second column: 0.083 + 0.333 + 0.583 = <b>1</b>
Sum of third column: 0.133 + 0.333 + 0.5333 = <b>1</b>
<h2><span class="orange">How to Create a NumPy Matrix with Random Numbers</span></h2>
You can use the following methods to create a NumPy matrix with random numbers:
<b>Method 1: Create NumPy Matrix of Random Integers</b>
<b>np.random.randint(low, high, (rows, columns))
</b>
<b>Method 2: Create NumPy Matrix of Random Floats</b>
<b>np.random.rand(rows, columns)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Create NumPy Matrix of Random Integers</h3>
The following code shows how to create a NumPy matrix of random values that ranges from <b>0</b> to <b>20</b> with a shape of <b>7 rows</b> and <b>2 columns</b>:
<b>import numpy as np
#create NumPy matrix of random integers
np.random.randint(0, 20, (7, 2))
array([[ 3,  7],
       [17, 10],
       [ 0, 10],
       [13, 16],
       [ 6, 14],
       [ 8,  7],
       [ 9, 15]])</b>
Notice that each value in the matrix ranges between 0 and 20 and the final shape of the matrix is 7 rows and 2 columns.
<h3>
<b>Example 2: </b>Create NumPy Matrix of Random Floats</b>
</h3>
The following code shows how to create a NumPy matrix with random float values between <b>0</b> and <b>1</b> and a shape of <b>7</b> columns and <b>2</b> rows:
<b>import numpy as np
#create NumPy matrix of random floats
np.random.rand(7, 2)
array([[0.64987774, 0.60099292],
       [0.13626106, 0.1859029 ],
       [0.77007972, 0.65179164],
       [0.33524707, 0.46201819],
       [0.1683    , 0.72960909],
       [0.76117417, 0.37212974],
       [0.18879731, 0.65723325]])
</b>
The result is a NumPy matrix that contains random float values between 0 and 1 with a shape of 7 rows and 2 columns.
Note that you can also use the NumPy <b>round()</b> function to round each float to a certain number of decimal places.
For example, the following code shows how to create a NumPy matrix of random floats each rounded to 2 decimal places:
<b>import numpy as np
#create NumPy matrix of random floats rounded to 2 decimal places
np.round(np.random.rand(5, 2), 2)
array([[0.37, 0.63],
       [0.51, 0.68],
       [0.23, 0.98],
       [0.62, 0.46],
       [0.02, 0.94]])
</b>
<b>Note</b>: You can find the complete documentation for the NumPy <b>rand()</b> function  here .
<h2><span class="orange">How to Rank Items in NumPy Array (With Examples)</span></h2>
You can use one of the following methods to calculate the rank of items in a NumPy array:
<b>Method 1: Use argsort() from NumPy</b>
<b>import numpy as np
ranks = np.array(my_array).argsort().argsort()
</b>
<b>Method 2: Use rankdata() from SciPy</b>
<b>from scipy.stats import rankdata
ranks = rankdata(my_array)
</b>
The following examples show how to use each method in practice with the following NumPy array:
<b>import numpy as np
#define array of values
my_array = np.array([3, 5, 2, 1, 9, 9])
#view array
print(my_array)
[3 5 2 1 9 9]</b>
<h2>Example 1: Rank Items in NumPy Array Using argsort()</h2>
The following code shows how to use the <b>argsort()</b> function from NumPy to rank the items in the array:
<b>#calculate rank of each item in array
ranks = np.array(my_array).argsort().argsort()
#view ranks
print(ranks)
[2 3 1 0 4 5]
</b>
The results show the rank of each item in the original array, with <b>0</b> representing the smallest value.
The benefit of this approach is that you don’t have to load any extra modules, but the drawback is that <b>argsort()</b> only has one method for handling ties.
By default, <b>argsort()</b> uses an ordinal method for handling ties which means the tied value that occurs first is automatically given the lower rank.
<h2>Example 2: Rank Items in NumPy Array Using rankdata()</h2>
The following code shows how to use the <b>rankdata()</b> function from SciPy to rank the items in the array:
<b><b>from scipy.stats import rankdata </b>
#calculate rank of each item in array
<b>ranks = rankdata(my_array)</b>
#view ranks
print(ranks)
array([3. , 4. , 2. , 1. , 5.5, 5.5])
</b>
The results show the rank of each item in the original array, with <b>1</b> representing the smallest value.
If you’d like <b>0</b> to represent the smallest value, simply subtract 1 from each value:
<b><b>from scipy.stats import rankdata </b>
#calculate rank of each item in array
ranks = rankdata(my_array) - 1
#view ranks
print(ranks)
[2.  3.  1.  0.  4.5 4.5]</b>
By default, the <b>rankdata()</b> function assigns average ranks to any values that have ties.
However, you can use the method argument to handle ties in a different way.
For example, the following code shows how to use <b>ordinal</b> as the method for handling ties:
<b><b>from scipy.stats import rankdata </b>
#calculate rank of each item in array
ranks = rankdata(my_array, method='ordinal') - 1
#view ranks
print(ranks)
[2 3 1 0 4 5]
</b>
This produces the same results as the <b>argsort()</b> method from NumPy.
Other methods for handling ties include <b>min</b>, <b>max</b>, and <b>dense</b>.
Read about each method in the  SciPy documentation .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Duplicate Elements from NumPy Array 
 How to Convert NumPy Array of Floats into Integers 
 How to Convert NumPy Matrix to Array 
<h2><span class="orange">How to Read CSV File with NumPy (Step-by-Step)</span></h2>
You can use the following basic syntax to read a CSV file into a record array in NumPy:
<b>from numpy import genfromtxt
my_data = genfromtxt('data.csv', delimiter=',', dtype=None)
</b>
The following step-by-step example shows how to use this syntax in practice.
<h2>Step 1: View the CSV File</h2>
Suppose we have the following CSV file called <b>data.csv</b> that we’d like to read into NumPy:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/numpy_csv1.png">
<h2>Step 2: Read in CSV File</h2>
The following code shows how to read in this CSV file into a Numpy array:
<b>from numpy import genfromtxt
#import CSV file
my_data = genfromtxt('data.csv', delimiter=',', dtype=None)
</b>
Note the following:
<b>delimiter</b>: This specifies the delimiter that separates the data values in the CSV file.
<b>dtype</b>: This specifies the data type for the NumPy array. By using <b>None</b>, we allow multiple data types to be imported at once within the array.
<h2>Example 3: View the NumPy Array</h2>
Once we’ve imported the CSV file, we can view it:
<b>#view imported CSV file
my_data
array([[1, 2, 2, 2, 3, 4],
       [5, 5, 6, 8, 9, 9]])
</b>
We can see that the data in the NumPy array matches the data shown in the CSV file.
<b>Note</b>: You can find the complete online documentation for the <b>genfromtxt()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions with CSV files in pandas:
 How to Read CSV Files with Pandas 
 How to Export Pandas DataFrame to CSV File 
 Pandas: How to Append Data to Existing CSV File 
<h2><span class="orange">How to Remove Duplicate Elements from NumPy Array</span></h2>
You can use the following methods to remove duplicate elements in NumPy:
<b>Method 1: Remove Duplicate Elements from NumPy Array</b>
<b>new_data = np.unique(data)</b>
<b>Method 2: Remove Duplicate Rows from NumPy Matrix</b>
<b>new_data = np.unique(data, axis=0)</b>
<b>Method 3: Remove Duplicate Columns from NumPy Matrix</b>
<b>new_data = np.unique(data, axis=1)</b>
The following examples show how to use each method in practice.
<h2>Example 1: Remove Duplicate Elements from NumPy Array</h2>
The following code shows how to remove duplicate elements from a NumPy array:
<b>import numpy as np
#create NumPy array
data = np.array([1, 1, 1, 2, 2, 4, 5, 5, 5, 5, 7, 8])
#create new array that removes duplicates
new_data = np.unique(data)
#view new array
print(new_data)
[1 2 4 5 7 8]
</b>
Notice that all duplicates have been removed from the NumPy array and only unique values remain.
<h2>Example 2: Remove Duplicate Rows from NumPy Matrix</h2>
The following code shows how to remove duplicate rows from a NumPy matrix:
<b>import numpy as np
#create NumPy matrix
data = np.array([[1, 5, 5, 8], [1, 5, 5, 8], [6, 2, 3, 4], [6, 2, 3, 4]])
#create new array that removes duplicate rows
new_data = np.unique(data, axis=0)
#view new matrix
print(new_data)
[[1 5 5 8]
 [6 2 3 4]]
</b>
Notice that all duplicate rows have been removed from the NumPy matrix and only unique rows remain.
<h2>Example 3: Remove Duplicate Columns from NumPy Matrix</h2>
The following code shows how to remove duplicate columns from a NumPy matrix:
<b>import numpy as np
#create NumPy matrix
data = np.array([[1, 1, 5, 8, 1], [1, 1, 2, 6, 1], [4, 4, 3, 8, 4]])
#create new matrix that removes duplicate columns
new_data = np.unique(data, axis=1)
#view new matrix
print(new_data)
[[1 5 8]
 [1 2 6]
 [4 3 8]]
</b>
Notice that all duplicate columns have been removed from the NumPy matrix and only unique columns remain.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Fill NumPy Array with Values 
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Remove Specific Elements from NumPy Array</span></h2>
You can use the following methods to remove specific elements from a NumPy array:
<b>Method 1: Remove Elements Equal to Specific Value</b>
<b>#remove elements whose value is equal to 12
new_array = np.delete(original_array, np.where(original_array == 12))
</b>
<b>Method 2: Remove Elements Equal to Some Value in List</b>
<b>#remove elements whose value is equal to 2, 5, or 12
new_array = np.setdiff1d(original_array, [2, 5, 12])</b>
<b>Method 3: Remove Elements Based on Index Position</b>
<b>#remove elements in index positions 0 and 6
new_array = np.delete(original_array, [0, 6])</b>
The following examples show how to use each method in practice.
<h2>Example 1: Remove Elements Equal to Specific Value</h2>
The following code shows how to remove all elements from a NumPy array whose value is equal to 12:
<b>import numpy as np
#define original array of values
original_array = np.array([1, 2, 2, 4, 5, 7, 9, 12, 12])
#remove elements whose value is equal to 12
new_array = np.delete(original_array, np.where(original_array == 12))
#view new array
print(new_array)
[1 2 2 4 5 7 9]
</b>
Notice that both elements in the array that were equal to 12 have been removed.
<h2>Example 2: Remove Elements Equal to Some Value in List</h2>
The following code shows how to remove all elements from a NumPy array whose values is equal to 2, 5, or 12:
<b>import numpy as np
#define original array of values
original_array = np.array([1, 2, 2, 4, 5, 7, 9, 12, 12])
#remove elements whose value is equal to 2, 5, or 12
new_array = np.setdiff1d(original_array, [2, 5, 12])
#view new array
print(new_array)
[1 4 7 9]
</b>
Notice that all elements whose value was 2, 5, or 12 have been removed.
<h2>Example 3: Remove Elements Based on Index Position</h2>
The following code shows how to remove the elements in index positions 0 and 6 from a NumPy array:
<b>import numpy as np
#define original array of values
original_array = np.array([1, 2, 2, 4, 5, 7, 9, 12, 12])
#remove elements in index positions 0 and 6
new_array = np.delete(original_array, [0, 6])
#view new array
print(new_array)
[ 2  2  4  5  7 12 12]
</b>
Notice that the elements in index position <b>0</b> (with value of 1) and index position <b>6</b> (with value of 9) have both been removed from the NumPy array.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Python:
 How to Fill NumPy Array with Values 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Remove NaN Values from NumPy Array (3 Methods)</span></h2>
You can use the following methods to remove NaN values from a NumPy array:
<b>Method 1: Use isnan()</b>
<b>new_data = data[~np.isnan(data)]
</b>
<b>Method 2: Use isfinite()</b>
<b>new_data = data[np.isfinite(data)]</b>
<b>Method 3: Use logical_not()</b>
<b>new_data = data[np.logical_not(np.isnan(data))]
</b>
Each of these methods produce the same result, but the first method is the shortest to type out so it tends to be used most often.
The following examples show how to use each method in practice.
<h3>Example 1: Remove NaN Values Using isnan()</h3>
The following code shows how to remove NaN values from a NumPy array by using the <b>isnan()</b> function:
<b>import numpy as np
#create array of data
data = np.array([4, np.nan, 6, np.nan, 10, 11, 14, 19, 22])
#define new array of data with nan values removed
new_data = data[~np.isnan(data)]
#view new array
print(new_data)
[ 4.  6. 10. 11. 14. 19. 22.]
</b>
Notice that the two NaN values have been successfully removed from the NumPy array.
This method simply keeps all of the elements in the array that are not (~) NaN values.
<h3>Example 2: Remove NaN Values Using isfinite()</h3>
The following code shows how to remove NaN values from a NumPy array by using the <b>isfinite()</b> function:
<b>import numpy as np
#create array of data
data = np.array([4, np.nan, 6, np.nan, 10, 11, 14, 19, 22])
#define new array of data with nan values removed
new_data = data[np.isfinite(data)]
#view new array
print(new_data)
[ 4.  6. 10. 11. 14. 19. 22.]
</b>
Notice that the two NaN values have been successfully removed from the NumPy array.
This method simply keeps all of the elements in the array that are finite values.
Since NaN values are not finite, they’re removed from the array.
<h3>Example 3: Remove NaN Values Using logical_not()</h3>
The following code shows how to remove NaN values from a NumPy array by using the <b>logical_not()</b> function:
<b>import numpy as np
#create array of data
data = np.array([4, np.nan, 6, np.nan, 10, 11, 14, 19, 22])
#define new array of data with nan values removed
new_data = data[np.logical_not(np.isnan(data))]
#view new array
print(new_data)
[ 4.  6. 10. 11. 14. 19. 22.]
</b>
Notice that the two NaN values have been successfully removed from the NumPy array.
While this method is equivalent to the previous two, it requires more typing so it’s not used as often.
<h2><span class="orange">How to Replace NaN Values with Zero in NumPy</span></h2>
You can use the following basic syntax to replace NaN values with zero in NumPy:
<b>my_array[np.isnan(my_array)] = 0</b>
This syntax works with both matrices and arrays.
The following examples show how to use this syntax in practice.
<h2>Example 1: Replace NaN Values with Zero in NumPy Array</h2>
The following code shows how to replace all NaN values with zero in a NumPy array:
<b>import numpy as np
#create array of data
my_array = np.array([4, np.nan, 6, np.nan, 10, 11, 14, 19, 22])
#replace nan values with zero in array
my_array[np.isnan(my_array)] = 0
#view updated array
print(my_array)
[ 4.  0.  6.  0. 10. 11. 14. 19. 22.]
</b>
Notice that both NaN values in the original array have been replaced with zero.
<h2>Example 2: Replace NaN Values with Zero in NumPy Matrix</h2>
Suppose we have the following NumPy matrix:
<b>import numpy as np
#create NumPy matrix
my_matrix = np.matrix(np.array([np.nan, 4, 3, np.nan, 8, 12]).reshape((3, 2)))
#view NumPy matrix
print(my_matrix)
[[nan  4.]
 [ 3. nan]
 [ 8. 12.]]
</b>
We can use the following code to replace all NaN values with zero in the NumPy matrix:
<b>#replace nan values with zero in matrix
my_matrix[np.isnan(my_matrix)] = 0
#view updated array
print(my_matrix)
[[ 0.  4.]
 [ 3.  0.]
 [ 8. 12.]]</b>
Notice that both NaN values in the original matrix have been replaced with zero.
<b>Related:</b>  How to Remove NaN Values from NumPy Array 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Fill NumPy Array with Values 
 How to Remove Specific Elements from NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Get Specific Row from NumPy Array 
<h2><span class="orange">How to Replace Elements in NumPy Array (3 Examples)</span></h2>
You can use the following methods to replace elements in a NumPy array:
<b>Method 1: Replace Elements Equal to Some Value</b>
<b>#replace all elements equal to 8 with a new value of 20
my_array[my_array == 8] = 20
</b>
<b>Method 2: Replace Elements Based on One Condition</b>
<b>#replace all elements greater than 8 with a new value of 20
my_array[my_array > 8] = 20</b>
<b>Method 3: Replace Elements Based on Multiple Conditions</b>
<b>#replace all elements greater than 8 or less than 6 with a new value of 20
my_array[(my_array > 8) | (my_array &lt; 6)] = 20
</b>
The following examples show how to use each method in practice with the following NumPy array:
<b>import numpy as np
#create array
my_array = np.array([4, 5, 5, 7, 8, 8, 9, 12])
#view array
print(my_array)
[ 4  5  5  7  8  8  9 12]
</b>
<h2>Method 1: Replace Elements Equal to Some Value</h2>
The following code shows how to replace all elements in the NumPy array equal to <b>8</b> with a new value of <b>20</b>:
<b>#replace all elements equal to 8 with 20
my_array[my_array == 8] = 20
#view updated array
print(my_array)
[ 4  5  5  7 20 20  9 12]
</b>
<h2>Method 2: Replace Elements Based on One Condition</h2>
The following code shows how to replace all elements in the NumPy array greater than <b>8</b> with a new value of <b>20</b>:
<b>#replace all elements greater than 8 with 20
my_array[my_array > 8] = 20
#view updated array
print(my_array)
[ 4  5  5  7  8  8 20 20]
</b>
<h2>Method 3: Replace Elements Based on Multiple Conditions</h2>
The following code shows how to replace all elements in the NumPy array greater than <b>8</b> or less than <b>6</b> with a new value of <b>20</b>:
<b>#replace all elements greater than 8 or less than 6 with a new value of 20
my_array[(my_array > 8) | (my_array &lt; 6)] = 20
#view updated array
print(my_array)
[20 20 20  7  8  8 20 20]
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in NumPy:
 How to Calculate the Mode of NumPy Array 
 How to Find Index of Value in NumPy Array 
 How to Map a Function Over a NumPy Array 
<h2><span class="orange">How to Shift Elements in NumPy Array (With Examples)</span></h2>
You can use one of the following methods to shift the elements in a NumPy array:
<b>Method 1: Shift Elements (Keep All Original Elements)</b>
<b>#shift each element two positions to the right
data_new = np.roll(data, 2)
</b>
<b>Method 2: Shift Elements (Allow Elements to Be Replaced)</b>
<b>#define shifting function
def shift_elements(arr, num, fill_value):
    result = np.empty_like(arr)
    if num > 0:
        result[:num] = fill_value
        result[num:] = arr[:-num]
    elif num &lt; 0:
        result[num:] = fill_value
        result[:num] = arr[-num:]
    else:
        result[:] = arr
    return result
#shift each element two positions to the right (replace shifted elements with zero)
data_new = shift_elements(data, 2, 0)
</b>
The following examples show how to use each method in practice.
<h3>Method 1: Shift Elements (Keep All Original Elements)</h3>
The following code shows how to use the  np.roll()  function to shift each element in a NumPy array two positions to the right:
<b>import numpy as np
#create NumPy array
data = np.array([1, 2, 3, 4, 5, 6])
#shift each element two positions to the right
data_new = np.roll(data, 2)
#view new NumPy array
data_new
array([5, 6, 1, 2, 3, 4])
</b>
Notice that each element was shifted two positions to the right and elements at the end of the array simply got moved to the front.
We could also use a negative number in the <b>np.roll()</b> function to shift elements to the left:
<b>import numpy as np
#create NumPy array
data = np.array([1, 2, 3, 4, 5, 6])
#shift each element three positions to the left
data_new = np.roll(data, -3)
#view new NumPy array
data_new
array([4, 5, 6, 1, 2, 3])</b>
<h3>Method 2: Shift Elements (Allow Elements to Be Replaced)</h3>
We can also define a custom function to shift the elements in a NumPy array and allow elements that are shifted to be replaced by a certain value.
For example, we can define the following function to shift elements and replace any shifted elements with the value 0:
<b>import numpy as np
#create NumPy array
data = np.array([1, 2, 3, 4, 5, 6])
#define custom function to shift elements
def shift_elements(arr, num, fill_value):
    result = np.empty_like(arr)
    if num > 0:
        result[:num] = fill_value
        result[num:] = arr[:-num]
    elif num &lt; 0:
        result[num:] = fill_value
        result[:num] = arr[-num:]
    else:
        result[:] = arr
    return result
#shift each element two positions to the right and replace shifted values with zero
data_new = shift_elements(data, 2, 0)
#view new NumPy array
data_new
array([0, 0, 1, 2, 3, 4])</b>
We can also use a negative number in the function to shift the elements to the left:
<b>import numpy as np
#create NumPy array
data = np.array([1, 2, 3, 4, 5, 6])
#define custom function to shift elements
def shift_elements(arr, num, fill_value):
    result = np.empty_like(arr)
    if num > 0:
        result[:num] = fill_value
        result[num:] = arr[:-num]
    elif num &lt; 0:
        result[num:] = fill_value
        result[:num] = arr[-num:]
    else:
        result[:] = arr
    return result
#shift each element three positions to the left and replace shifted values with 50
data_new = shift_elements(data, -3, 50)
#view new NumPy array
data_new
array([ 4,  5,  6, 50, 50, 50])</b>
<h2><span class="orange">How to Sort a NumPy Array by Column (With Examples)</span></h2>
You can use the following methods to sort the rows of a NumPy array by column values:
<b>Method 1: Sort by Column Values Ascending</b>
<b>x_sorted_asc = x[x[:, 1].argsort()]
</b>
<b>Method 2: Sort by Column Values Descending</b>
<b>x_sorted_desc = x[x[:, 1].argsort()[::-1]]</b>
The following examples show how to use each method in practice.
<h3>Example 1: Sort Numpy Array by Column Values Ascending</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create array
x = np.array([14, 12, 8, 10, 5, 7, 11, 9, 2]).reshape(3,3)
#view array
print(x)
[[14 12  8]
 [10  5  7]
 [11  9  2]]
</b>
We can use the following code to sort the rows of the NumPy array in ascending order based on the values in the second column:
<b>#define new matrix with rows sorted in ascending order by values in second column
x_sorted_asc = x[x[:, 1].argsort()]
#view sorted matrix
print(x_sorted_asc)
[[10  5  7]
 [11  9  2]
 [14 12  8]]</b>
Notice that the rows are now sorted in ascending order (smallest to largest) based on the values in the second column.
<h3>
<b>Example 2: Sort Numpy Array by Column Values Descending</b>
</h3>
Suppose we have the following NumPy array:
<b>import numpy as np
#create array
x = np.array([14, 12, 8, 10, 5, 7, 11, 9, 2]).reshape(3,3)
#view array
print(x)
[[14 12  8]
 [10  5  7]
 [11  9  2]]
</b>
We can use the following code to sort the rows of the NumPy array in descending order based on the values in the second column:
<b>#define new matrix with rows sorted in descending order by values in second column
x_sorted_desc = x[x[:, 1].argsort()[::-1]]
#view sorted matrix
print(x_sorted_desc)
[[14 12  8]
 [11  9  2]
 [10  5  7]]</b>
Notice that the rows are now sorted in descending order (largest to smallest) based on the values in the second column.
<h2><span class="orange">How to Swap Two Columns in a NumPy Array (With Example)</span></h2>
You can use the following basic syntax to swap two columns in a NumPy array:
<b>some_array[:, [0, 2]] = some_array[:, [2, 0]]
</b>
This particular example will swap the first and third columns in the NumPy array called <b>some_array</b>.
All other columns will remain in their original positions.
The following example shows how to use this syntax in practice.
<b>Related:</b>  How to Swap Two Rows in a NumPy Array 
<h2>Example: Swap Two Columns in NumPy Array</h2>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
some_array = np.array([[1, 1, 2], [3, 3, 7], [4, 3, 1], [9, 9, 5], [6, 7, 7]])
#view NumPy array
print(some_array)
[[1 1 2]
 [3 3 7]
 [4 3 1]
 [9 9 5]
 [6 7 7]]</b>
We can use the following syntax to swap the first and third columns in the NumPy array:
<b>#swap columns 1 and 3
some_array[:, [0, 2]] = some_array[:, [2, 0]]
#view updated NumPy array
print(some_array)
[[2 1 1]
 [7 3 3]
 [1 3 4]
 [5 9 9]
 [7 7 6]]
</b>
Notice that the first and third columns have been swapped.
All other columns remained in their original positions.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Duplicate Elements in NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Rank Items in NumPy Array 
<h2><span class="orange">How to Swap Two Rows in a NumPy Array (With Example)</span></h2>
You can use the following basic syntax to swap two rows in a NumPy array:
<b>some_array[[0, 3]] = some_array[[3, 0]]
</b>
This particular example will swap the first and fourth rows in the NumPy array called <b>some_array</b>.
All other rows will remain in their original positions.
The following example shows how to use this syntax in practice.
<h2>Example: Swap Two Rows in NumPy Array</h2>
Suppose we have the following NumPy array:
<b>import numpy as np
#create NumPy array
some_array = np.array([[1, 1, 2], [3, 3, 7], [4, 3, 1], [9, 9, 5], [6, 7, 7]])
#view NumPy array
print(some_array)
[[1 1 2]
 [3 3 7]
 [4 3 1]
 [9 9 5]
 [6 7 7]]</b>
We can use the following syntax to swap the first and fourth rows in the NumPy array:
<b>#swap rows 1 and 4
some_array[[0, 3]] = some_array[[3, 0]]
#view updated NumPy array
print(some_array)
[[9 9 5]
 [3 3 7]
 [4 3 1]
 [1 1 2]
 [6 7 7]]
</b>
Notice that the first and fourth rows have been swapped.
All other rows remained in their original positions.
Note that <b>some_array[[0,  3]]</b> is shorthand for <b>some_array[[0, 3],  :]</b> so we could also use the following syntax to get the same results:
<b>#swap rows 1 and 4
some_array[[0, 3], :] = some_array[[3, 0], :]
#view updated NumPy array
print(some_array)
[[9 9 5]
 [3 3 7]
 [4 3 1]
 [1 1 2]
 [6 7 7]]</b>
Notice that the first and fourth rows have been swapped.
This result matches the result from using the shorthand notation in the previous example.
Feel free to use whichever notation you prefer to swap two rows in a given NumPy array.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in NumPy:
 How to Remove Duplicate Elements in NumPy Array 
 How to Replace Elements in NumPy Array 
 How to Rank Items in NumPy Array 
<h2><span class="orange">How to Use NumPy where() With Multiple Conditions</span></h2>
You can use the following methods to use the NumPy  where()  function with multiple conditions:
<b>Method 1: Use where() with OR</b>
<b>#select values less than five <em>or</em> greater than 20
x[np.where((x &lt; 5) | (x > 20))]
</b>
<b>Method 2: Use where() with AND</b>
<b>#select values greater than five <i>and </i>less than 20
x[np.where((x > 5) & (x &lt; 20))]</b>
The following example shows how to use each method in practice.
<h3>Method 1: Use where() with OR</h3>
The following code shows how to select every value in a NumPy array that is less than 5 <b>or</b> greater than 20:
<b>import numpy as np
#define NumPy array of values
x = np.array([1, 3, 3, 6, 7, 9, 12, 13, 15, 18, 20, 22])
#select values that meet one of two conditions
x[np.where((x &lt; 5) | (x > 20))]
array([ 1,  3,  3, 22])
</b>
Notice that four values in the NumPy array were less than 5 <b>or</b> greater than 20.
You can also use the <b>size</b> function to simply find how many values meet one of the conditions:
<b>#find number of values that are less than 5 or greater than 20
(x[np.where((x &lt; 5) | (x > 20))]).size
4</b>
<h3>Method 2: Use where() with AND</h3>
The following code shows how to select every value in a NumPy array that is greater than 5 <b>and </b>less than 20:
<b>import numpy as np
#define NumPy array of values
x = np.array([1, 3, 3, 6, 7, 9, 12, 13, 15, 18, 20, 22])
#select values that meet two conditions
x[np.where((x > 5) & (x &lt; 20))]
array([6,  7,  9, 12, 13, 15, 18])
</b>
The output array shows the seven values in the original NumPy array that were greater than 5 <b>and</b> less than 20.
Once again, you can use the <b>size</b> function to find how many values meet both conditions:
<b>#find number of values that are greater than 5 and less than 20
(x[np.where((x > 5) & (x &lt; 20))]).size
7</b>
<h2><span class="orange">What is an Observation in Statistics?</span></h2>
In statistics, an <b>observation</b> is simply one occurrence of something you’re measuring.
For example, suppose you’re measuring the weight of a certain species of turtle. Each turtle that you collect the weight for counts as one single observation.
The following dataset contains the weight of 15 different turtles, so there are <b>15 </b>total observations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/obs1.png">
When viewing a dataset in statistical software like  Excel ,  R ,  Python , or  Stata , the number of rows in the dataset is equal to the number of observations. 
For example, a dataset with 100 rows has 100 observations.
It’s also interesting to note that a single observation can be associated with <b>multiple variables</b>. For example, in the following dataset there are 15 observations and 3 variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/obs5.png">
The first observation has the following values for the three variables:
Weight: 290 pounds, Length: 30 inches, Region: East
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/obs3.png">
The second observation has the following values for the three variables:
Weight: 296 pounds, Length: 35 inches, Region: East
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/obs4.png">
And so on.
It’s also worth noting that the total number of observations is equal to the <b>sample size</b> of the dataset. For example, a dataset that has 100 observations has a sample size of 100.
<h2><span class="orange">What is Observer Bias? (Definition & Examples)</span></h2>
<b>Observer bias</b> occurs in research when the beliefs or expectations of an observer (or investigator) can influence the data that’s collected in a study.
This causes the results of a study to be unreliable and hard to reproduce in other research settings.
In this article we share two famous examples of observer bias along with a strategy that can be used to minimize this type of bias in practice.
<h3>Example 1: Clever Hans</h3>
In the early 1900’s there was a horse named  Clever Hans  that was claimed to have the ability to do arithmetic extremely well.
The owner, Wilhelm Von Olson, would ask Clever Hans different questions that involved adding, subtracting, multiplying, and other arithmetic operations and Clever Hans would provide an answer by tapping his hoof a certain number of times.
<figure style="width: 730px"><img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/e/e3/CleverHans.jpg"730"><figcaption> Clever Hans in 1904 </figcaption></figure>Amazed by this, psychologist Oskar Pfungst investigated this situation and found that Clever Hans could only provide the correct answer when the owner actually knew the correct answer to the question.
It turns out that when Clever Hans would approach the correct number of taps to make, the owner Wilhelm Von Olson would start to react in a certain way which signaled that Hans should stop tapping.
Without realizing it, the owner was giving subtle cues to Hans about the correct number of taps to make. But when the owner himself didn’t know the answer to the questions he was asking, Hans was unable to produce the correct answer because the owner didn’t make any subtle cues on when to stop tapping.
This is an example of observer bias because the expectations of the owner caused Clever Hans to act in a certain way, which resulted in faulty data. 
<h3>Example 2: Smart & Dull Rats</h3>
In 1963, psychologist Robert Rosenthal had two groups of students test rats. The rats were categorized as being “bright” or “dull” in their ability to complete mazes, even though in reality they were all the same type of standard lab rat.
The results of the study showed that the students who thought they were handling “bright” rats behaved in certain ways to make sure that the rats had a better chance of completing the maze while the students who thought they were handling “dull” rats behaved in ways that reduced the rats chances of completing the mazes.
This is an example of observer bias because it turns out that the expectations of the students influenced how well the different groups of rats performed.
<h3> How to Minimize Observer Bias</h3>
The easiest way to minimize observer bias is to ensure that the observer doesn’t have any expectations of the subjects for which they’re gathering data.
In technical terms, we say that the observers should be <b>blinded</b> to the ability of the subjects or the expected outcomes of the subjects.
For example, the person asking arithmetic questions to Clever Hans should not know the answer to the question they’re asking. This will prevent them from giving subtle cues to Hans about what the correct answer should be.
Or, in the example with the rats, the students should not know which “type” of rat they’re handling. Instead, they should simply be told to test the rats in the maze and no distinction should be made on whether or not they’re handling “bright” or “dull” rats.
<h2><span class="orange">How to Calculate Odds Ratio and Relative Risk in Excel</span></h2>
We often use the <b>odds ratio </b>and <b>relative risk </b>when performing an analysis on a 2-by-2 table, which takes on the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
The <b>odds ratio </b>tells us the ratio of the odds of an event occurring in a treatment group to the odds of an event occurring in a control group. It is calculated as:
<b>Odds ratio </b>= (A*D) / (B*C)
The <b>relative risk </b>tells us the ratio of the probability of an event occurring in a treatment group to the probability of an event occurring in a control group. It is calculated as:
<b>Relative risk </b>= [A/(A+B)]  /  [C/(C+D)]
This tutorial explains how to calculate odds ratios and relative risk in Excel.
<h3>How to Calculate the Odds Ratio and Relative Risk</h3>
Suppose 50 basketball players use a new training program and 50 players use an old training program. At the end of the program we test each player to see if they pass a certain skills test.
The following table shows the number of players who passed and failed, based on the program they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel2.png">
The <b>odds ratio </b>is calculated as (34*11) / (16*39) = <b>0.599</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel3.png">
We would interpret this to mean that the odds that a player passes the test by using the new program are just <b>0.599 times the odds </b>that a player passes the test by using the old program.
In other words, the odds that a player passes the test are actually lowered by 40.1% by using the new program.
The <b>relative risk </b>is calculated as  [34/(34+16)]  /  [39/(39+11)] = <b>0.872</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel4.png">
We would interpret this to mean that the ratio of the probability of a player passing the test using the new program compared to the old program is <b>0.872</b>.
Because this value is less than 1, it indicates that the probability of passing is actually lower under the new program compared to the old program.
We could also see this by directly computing the probability that a player passes under each program:
Probability of passing under new program = 34 / 50 = <b>68%</b>
Probability of passing under old program = 39 / 50 = <b>78%</b>
<h3>How to Calculate Confidence Intervals</h3>
Once we calculate the odds ratio and relative risk, we may also be interested in computing confidence intervals for these two metrics.
A 95% confidence interval for the <b>odds ratio</b> can be calculated using the following formula:
<b>95% C.I. for odds ratio</b> = [ e^(ln(OR) – 1.96*SE(ln(OR))),  e^(ln(OR) – 1.96*SE(ln(OR))) ]
where SE(ln(OR)) =√1/A + 1/B + 1/C + 1/D
The 95% C.I. for the odds ratio turns out to be <b>(.245, 1.467)</b>. The image below shows the formula we used to calculate this confidence interval:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel6.png">
A 95% confidence interval for the <b>relative risk</b> can be calculated using the following formula:
<b>95% C.I. for relative risk</b> = exp(ln(RR) – 1.96*SE(ln(RR))) to exp(ln(RR) – 1.96*SE(ln(RR)))
where SE(ln(RR)) =√1/A + 1/C – 1/(A+B) – 1/(C+D)
The 95% C.I. for the relative risk turns out to be <b>(.685, 1.109)</b>. The image below shows the formula we used to calculate this confidence interval:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel7.png">
<h2><span class="orange">Odds Ratio vs. Relative Risk: What’s the Difference?</span></h2>
Two terms that students often confuse in statistics are <b>odds ratio</b> and <b>relative risk</b>. 
We often use these two metrics when performing an analysis on a 2-by-2 table, which takes on the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
The <b>odds ratio </b>tells us the ratio of the odds of an event occurring in a treatment group to the odds of an event occurring in a control group. It is calculated as:
<b>Odds ratio </b>= (A*D) / (B*C)
The <b>relative risk </b>tells us the ratio of the probability of an event occurring in a treatment group to the probability of an event occurring in a control group. It is calculated as:
<b>Relative risk </b>= [A/(A+B)]  /  [C/(C+D)]
In short, here’s the difference:
An odds ratio is a ratio of two <b>odds</b>.
Relative risk is a ratio of two <b>probabilities</b>.
The following example shows how to calculate and interpret an odds ratio and relative risk in a real-life situation.
<h3>Example: Calculating Odds Ratio and Relative Risk</h3>
Suppose 100 basketball players use a new training program and 100 players use an old training program. At the end of the program we test each player to see if they pass a certain skills test.
The following table shows the number of players who passed and failed, based on the program they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/odds_vs.png">
The <b>odds ratio </b>is calculated as:
Odds ratio = (A*D) / (B*C)
Odds ratio = (61*48) / (39*52)
Odds ratio = 1.44
We would interpret this to mean that the odds that a player passes the test by using the new program are <b>1.44 times the odds </b>that a player passes the test by using the old program.
In other words, the odds that a player passes the test are increased by using the new program.
The <b>relative risk </b>is calculated as
Relative risk = [A/(A+B)]  /  [C/(C+D)]
Relative risk = [61/(61+39)] / [52/(52+48)]
Relative risk = 1.17
We would interpret this to mean that the ratio of the probability of a player passing the test using the new program compared to the old program is <b>1.17</b>.
Since this value is greater than 1, it tells us that the probability of passing is higher under the new program compared to the old program.
We can also see this by directly computing the probability that a player passes under each program:
Probability of passing under new program = 61 / 100 = <b>61%</b>
Probability of passing under old program = 52 / 100 = <b>52%</b>
Taking the ratio of these probabilities, we can calculate the relative risk as 61% / 52% = <b>1.17</b>.
Note that the odds ratio and relative risk are both greater than 1, which tells us that the chances of experiencing some event (e.g. passing the skills test) is greater in the treatment group compared to the control group.
The odds ratio and relative risk give us similar information, but we interpret each value in slightly different ways.
In particular:
The <b>odds ratio</b> tells us that the odds of passing the skills test is higher under the new program.
The <b>relative risk</b> tells us that the probability of passing the skills test is higher under the new program.
Using either metric, we can easily see that the new program is better than the old program.
<h2><span class="orange">How to Calculate Odds Ratios in R (With Example)</span></h2>
In statistics, an <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group compared to the odds of an event occurring in a control group.
We often calculate an odds ratio when performing an analysis on a 2-by-2 table, which takes on the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
To calculate an odds ratio in R, we can use the <b>oddsratio()</b> function from the <b>epitools</b> package.
The following example shows how to use this syntax in practice.
<h2>Example: Calculating an Odds Ratio in R</h2>
Suppose 50 basketball players use a new training program and 50 players use an old training program. At the end of the program we test each player to see if they pass a certain skills test.
The following table shows the number of players who passed and failed, based on the program they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/odds1.png">
Suppose we would like to calculate an odds ratio to compare the odds of a player passing the skills test using the new program compared to using the old program.
Here is how to create this matrix in R:
<b>#create matrix
program &lt;- c('New Program', 'Old Program')
outcome &lt;- c('Pass', 'Fail')
data &lt;- matrix(c(34, 16, 39, 11), nrow=2, ncol=2, byrow=TRUE)
dimnames(data) &lt;- list('Program'=program, 'Outcome'=outcome)
#view matrix
data
             Outcome
Program       Pass Fail
  New Program   34   16
  Old Program   39   11</b>
And here is how to calculate the odds ratio using the <b>oddsratio()</b> function from the <b>epitools</b> package:
<b>install.packages('epitools')
library(epitools)
#calculate odds ratio
oddsratio(data)
$measure
             odds ratio with 95% C.I.
Program        estimate     lower    upper
  New Program 1.0000000        NA       NA
  Old Program 0.6045506 0.2395879 1.480143
$p.value
             two-sided
Program       midp.exact fisher.exact chi.square
  New Program         NA           NA         NA
  Old Program   0.271899    0.3678219  0.2600686
$correction
[1] FALSE
attr(,"method")
[1] "median-unbiased estimate & mid-p exact CI"
</b>
The odds ratio turns out to be <b>0.6045506</b>.
We would interpret this to mean that the odds that a player passes the test by using the new program are just <b>.6045506 times the odds </b>that a player passes the test by using the old program.
In other words, the odds that a player passes the test are actually lowered by about 39.6% by using the new program.
We can also use the values in the <b>lower</b> and <b>upper</b> columns of the output to construct the following 95% confidence interval for the odds ratio:
95% confidence interval for the odds ratio: <b>[0.24, 1.48]</b>.
We are 95% confident that the true odds ratio between the new and old training program is contained in this interval.
The <b>midp.exact</b> column in the output also displays the p-value associated with the odds ratio.
This p-value turns out to be <b>0.271899</b>. Since this value is not less than .05, we would conclude that the odds ratio is not statistically significant.
In other words, we know from the odds ratio that the odds of a player passing using the new program are lower than the odds of passing using the old program, but the difference between these odds is not actually statistically significant.
<h2>Additional Resources</h2>
The following tutorials provide additional information about odds ratios:
 Odds Ratio vs. Relative Risk: What’s the Difference? 
 The Complete Guide: How to Report Odds Ratios 
 How to Calculate a Confidence Interval for an Odds Ratio 
<h2><span class="orange">How to Create an Ogive Graph in Excel</span></h2>
An <b>ogive </b>is a graph that shows how many data values lie above or below a certain value in a dataset. This tutorial explains how to create an ogive in Excel.
<h3>Example: How to Create an Ogive in Excel</h3>
Perform the following steps to create an ogive for a dataset in Excel.
<b>Step 1: Enter the data.</b>
Enter the data values in a single column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel1.png">
<b>Step 2: Define the class limits.</b>
Next, define the class limits you’d like to use for the ogive. I’ll choose class widths of 10:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel2.png">
<b>Step 3: Find class frequencies.</b>
Next, we’ll use the following formula to calculate the frequencies for the first class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel3.png">
Copy this formula to the rest of the classes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel4.png">
<b>Step 4: Find cumulative frequencies.</b>
Next, we’ll use the following formulas to calculate the cumulative frequency for each class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel5.png">
<b>Step 5: Create the ogive graph.</b>
To create the ogive graph, hold down CTRL and highlight columns D and F.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel6.png">
Along the top ribbon in Excel, go to the <b>Insert </b>tab, then the <b>Charts </b>group. Click <b>Scatter Chart</b>, then click <b>Scatter with Straight Lines and Markers</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/ogive12.jpg"552">
This will automatically produce the following ogive graph:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel7.png">
Feel free to modify the axes and the title to make the graph more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ogiveExcel8.png">
<h2><span class="orange">How to Create an Ogive Graph in R</span></h2>
An <b>ogive </b>is a graph that shows how many data values lie above or below a certain value in a dataset. 
This tutorial explains how to create the following ogive graph in R:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/ogiveR1.jpg">
<h3>Example: Create Ogive Graph in R</h3>
First, let’s define a dataset that contains 20 values:
<b>#create dataset
data &lt;- c(6, 7, 7, 8, 9, 12, 14, 16, 16, 17, 22, 24, 28, 31, 34, 35, 39, 41, 42, 43)
</b>
Next, let’s use the <b>graph.freq()</b> and <b>ogive.freq()</b> functions from the <b>agricolae</b> package in R to create a simple ogive graph:
<b>library(agricolae)
#define values to plot
value_bins &lt;- graph.freq(data, plot=FALSE)
values &lt;- ogive.freq(value_bins, frame=FALSE)
#create ogive chart
plot(values, xlab='Values', ylab='Relative Cumulative Frequency',
     main='Ogive Chart', col='steelblue', type='b', pch=19, las=1, bty='l')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/ogiveR1.jpg">
The x-axis shows the values from the dataset and the y-axis shows the relative cumulative frequency of values that lie below the values shown on the x-axis.
Here is how to interpret some of the more obscure arguments in the <b>plot()</b> function:
<b>type=’b’</b>: Plot <em>both</em> lines and points
<b>pch=19</b>: Fill in the circles in the plot
<b>las=1</b>: Make labels perpendicular to axis
<b>bty=’l’</b>: Only show the border on the bottom and left sides of the plot
We can view the actual values in the plot by printing the values created from the <b>ogive.freq()</b> function:
<b>#view values in ogive
values
     x  RCF
1  6.0 0.00
2 13.4 0.30
3 20.8 0.50
4 28.2 0.65
5 35.6 0.80
6 43.0 1.00
7 50.4 1.00
</b>
Here’s how to interpret the values:
0% of all values in the dataset were less than or equal to <b>6</b>.
30% of all values in the dataset were less than or equal to <b>13.4</b>.
50% of all values in the dataset were less than or equal to <b>20.8</b>.
65% of all values in the dataset were less than or equal to <b>35.6</b>.
And so on.
<h2><span class="orange">How to Create an Ogive Graph in Python</span></h2>
An <b>ogive </b>is a graph that shows how many data values lie above or below a certain value in a dataset. This tutorial explains how to create an ogive in Python.
<h3>Example: How to Create an Ogive in Python</h3>
Perform the following steps to create an ogive for a dataset in Python.
<b>Step 1: Create a dataset.</b>
First, we can create a simple dataset.
<b>import numpy as np
#create array of 1,000 random integers between 0 and 10
np.random.seed(1)
data = np.random.randint(0, 10, 1000)
#view first ten values 
data[:10]
array([5, 8, 9, 5, 0, 0, 1, 7, 6, 9])
</b>
<b>Step 2: Create an ogive.</b>
Next, we can use the  numpy.histogram  function to automatically find the classes and the class frequencies. Then we can use matplotlib to actually create the ogive:
<b>import numpy as np
import matplotlib.pyplot as plt 
#obtain histogram values with 10 bins
values, base = np.histogram(data, bins=10)
#find the cumulative sums
cumulative = np.cumsum(values)
# plot the ogive
plt.plot(base[:-1], cumulative, 'ro-')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/ogivepython1.png">
The ogive chart will look different based on the number of bins that we specify in the <b>numpy.histogram</b> function. For example, here’s what the chart would look like if we used 30 bins:
<b>#obtain histogram values with 30 bins
values, base = np.histogram(data, bins=10)
#find the cumulative sums
cumulative = np.cumsum(values)
# plot the ogive
plt.plot(base[:-1], cumulative, 'ro-')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/ogivepython2.png">
The argument ‘<b>ro-‘ </b>specifies:
Use the color red (r)
Use circles at each class break (o)
Use lines to connect the circles (-)
Feel free to change these options to change the aesthetics of the chart.
<h2><span class="orange">How to Perform OLS Regression in R (With Example)</span></h2>
Ordinary least squares (OLS) regression is a method that allows us to find a line that best describes the relationship between one or more predictor variables and a  response variable .
This method allows us to find the following equation:
<b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x</b>
where:
<b><U+0177></b>: The estimated response value
<b>b<sub>0</sub></b>: The intercept of the regression line
<b>b<sub>1</sub></b>: The slope of the regression line
This equation can help us understand the relationship between the predictor and response variable, and it can be used to predict the value of a response variable given the value of the predictor variable.
The following step-by-step example shows how to perform OLS regression in R.
<h2>Step 1: Create the Data</h2>
For this example, we’ll create a dataset that contains the following two variables for 15 students:
Total hours studied
Exam score
We’ll perform OLS regression, using hours as the predictor variable and exam score as the response variable.
The following code shows how to create this fake dataset in R:
<b>#create dataset
df &lt;- data.frame(hours=c(1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14), score=c(64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89))
#view first six rows of dataset
head(df)
  hours score
1     1    64
2     2    66
3     4    76
4     5    73
5     5    74
6     6    81
</b>
<h2>Step 2: Visualize the Data</h2>
Before we perform OLS regression, let’s create a scatter plot to visualize the relationship between hours and exam score:
<b>library(ggplot2)
#create scatter plot
ggplot(df, aes(x=hours, y=score)) +
  geom_point(size=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/ols1.jpg"486">
One of the  four assumptions  of linear regression is that there is a linear relationship between the predictor and response variable.
From the plot we can see that the relationship does appear to be linear. As hours increases, score tends to increase as well in a linear fashion.
Next, we can create a boxplot to visualize the distribution of exam scores and check for outliers.
<b>Note</b>: R defines an observation to be an outlier if it is 1.5 times the interquartile range greater than the third quartile or 1.5 times the interquartile range less than the first quartile.
If an observation is an outlier, a tiny circle will appear in the boxplot:
<b>library(ggplot2)
#create scatter plot
ggplot(df, aes(y=score)) +
  geom_boxplot()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/ols2.jpg"333">
There are no tiny circles in the boxplot, which means there are no outliers in our dataset.
<h2>Step 3: Perform OLS Regression</h2>
Next, we can use the <b>lm()</b> function in R to perform OLS regression, using hours as the predictor variable and score as the response variable:
<b>#fit simple linear regression model
model &lt;- lm(score~hours, data=df)
#view model summary
summary(model)
Call:
lm(formula = score ~ hours)
Residuals:
   Min     1Q Median     3Q    Max 
-5.140 -3.219 -1.193  2.816  5.772 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   65.334      2.106  31.023 1.41e-13 ***
hours          1.982      0.248   7.995 2.25e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.641 on 13 degrees of freedom
Multiple R-squared:  0.831,Adjusted R-squared:  0.818 
F-statistic: 63.91 on 1 and 13 DF,  p-value: 2.253e-06
</b>
From the model summary we can see that the fitted regression equation is:
<b>Score = 65.334 + 1.982*(hours)</b>
This means that each additional hour studied is associated with an average increase in exam score of <b>1.982</b> points.
The intercept value of <b>65.334</b> tells us the average expected exam score for a student who studies zero hours.
We can also use this equation to find the expected exam score based on the number of hours that a student studies.
For example, a student who studies for 10 hours is expected to receive an exam score of <b>85.15</b>:
<b>Score = 65.334 + 1.982*(10) = 85.15</b>
Here is how to interpret the rest of the model summary:
<b>Pr(>|t|):</b> This is the p-value associated with the model coefficients. Since the p-value for <em>hours</em> (2.25e-06) is significantly less than .05, we can say that there is a statistically significant association between <em>hours</em> and <em>score</em>.
<b>Multiple R-squared:</b> This number tells us the percentage of the variation in the exam scores can be explained by the number of hours studied. In general, the larger the R-squared value of a regression model the better the predictor variables are able to predict the value of the response variable. In this case, <b>83.1%</b> of the variation in scores can be explained hours studied.
<b>Residual standard error:</b> This is the average distance that the observed values fall from the regression line. The lower this value, the more closely a regression line is able to match the observed data. In this case, the average observed exam score falls <b>3.641</b> points away from the score predicted by the regression line.
<b>F-statistic & p-value:</b> The F-statistic (<b>63.91</b>) and the corresponding p-value (<b>2.253e-06</b>) tell us the overall significance of the regression model, i.e. whether predictor variables in the model are useful for explaining the variation in the response variable. Since the p-value in this example is less than .05, our model is statistically significant and <em>hours</em> is deemed to be useful for explaining the variation in <em>score</em>.
<h2>Step 4: Create Residual Plots</h2>
Lastly, we need to create residual plots to check the assumptions of  homoscedasticity  and  normality .
The assumption of <b>homoscedasticity</b> is that the  residuals  of a regression model have roughly equal variance at each level of a predictor variable.
To verify that this assumption is met, we can create a <b>residuals vs. fitted plot</b>.
The x-axis displays the fitted values and the y-axis displays the residuals. As long as the residuals appear to be randomly and evenly distributed throughout the chart around the value zero, we can assume that homoscedasticity is not violated:
<b>#define residuals
res &lt;- resid(model)
#produce residual vs. fitted plot
plot(fitted(model), res)
#add a horizontal line at 0 
abline(0,0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/ols3.jpg"501">
The residuals appear to be randomly scatted around zero and don’t exhibit any noticeable patterns, so this assumption is met.
The assumption of <b>normality </b>states that the  residuals  of a regression model are roughly normally distributed.
To check if this assumption is met, we can create a <b>Q-Q plot</b>. If the points in the plot fall along a roughly straight line at a 45-degree angle, then the data is normally distributed:
<b>#create Q-Q plot for residuals
qqnorm(res)
#add a straight diagonal line to the plot
qqline(res) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/ols4.jpg"416">
The residuals stray from the 45-degree line a bit, but not enough to cause serious concern. We can assume that the normality assumption is met.
Since the residuals are normally distributed and homoscedastic, we’ve verified that the assumptions of the OLS regression model are met.
Thus, the output from our model is reliable.
<b>Note</b>: If one or more of the assumptions was not met, we could attempt  transforming  our data.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform Multiple Linear Regression in R 
 How to Perform Exponential Regression in R 
 How to Perform Weighted Least Squares Regression in R 
<h2><span class="orange">How to Perform OLS Regression in Python (With Example)</span></h2>
Ordinary least squares (OLS) regression is a method that allows us to find a line that best describes the relationship between one or more predictor variables and a  response variable .
This method allows us to find the following equation:
<b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x</b>
where:
<b><U+0177></b>: The estimated response value
<b>b<sub>0</sub></b>: The intercept of the regression line
<b>b<sub>1</sub></b>: The slope of the regression line
This equation can help us understand the relationship between the predictor and response variable, and it can be used to predict the value of a response variable given the value of the predictor variable.
The following step-by-step example shows how to perform OLS regression in Python.
<h2>Step 1: Create the Data</h2>
For this example, we’ll create a dataset that contains the following two variables for 15 students:
Total hours studied
Exam score
We’ll perform OLS regression, using hours as the predictor variable and exam score as the response variable.
The following code shows how to create this fake dataset in pandas:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'hours': [1, 2, 4, 5, 5, 6, 6, 7, 8, 10, 11, 11, 12, 12, 14],   'score': [64, 66, 76, 73, 74, 81, 83, 82, 80, 88, 84, 82, 91, 93, 89]})
#view DataFrame
print(df)
    hours  score
0       1     64
1       2     66
2       4     76
3       5     73
4       5     74
5       6     81
6       6     83
7       7     82
8       8     80
9      10     88
10     11     84
11     11     82
12     12     91
13     12     93
14     14     89</b>
<h2>Step 2: Perform OLS Regression</h2>
Next, we can use functions from the  statsmodels  module to perform OLS regression, using <b>hours</b> as the predictor variable and score as the <b>response </b>variable:
<b>import statsmodels.api as sm
#define predictor and response variables
y = df['score']
x = df['hours']
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.831
Model:                            OLS   Adj. R-squared:                  0.818
Method:                 Least Squares   F-statistic:                     63.91
Date:                Fri, 26 Aug 2022   Prob (F-statistic):           2.25e-06
Time:                        10:42:24   Log-Likelihood:                -39.594
No. Observations:                  15   AIC:                             83.19
Df Residuals:                      13   BIC:                             84.60
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         65.3340      2.106     31.023      0.000      60.784      69.884
hours          1.9824      0.248      7.995      0.000       1.447       2.518
==============================================================================
Omnibus:                        4.351   Durbin-Watson:                   1.677
Prob(Omnibus):                  0.114   Jarque-Bera (JB):                1.329
Skew:                           0.092   Prob(JB):                        0.515
Kurtosis:                       1.554   Cond. No.                         19.2
==============================================================================</b>
From the <b>coef</b> column we can see the regression coefficients and can write the following fitted regression equation is:
<b>Score = 65.334 + 1.9824*(hours)</b>
This means that each additional hour studied is associated with an average increase in exam score of <b>1.9824</b> points.
The intercept value of <b>65.334</b> tells us the average expected exam score for a student who studies zero hours.
We can also use this equation to find the expected exam score based on the number of hours that a student studies.
For example, a student who studies for 10 hours is expected to receive an exam score of <b>85.158</b>:
<b>Score = 65.334 + 1.9824*(10) = 85.158</b>
Here is how to interpret the rest of the model summary:
<b>P(>|t|):</b> This is the p-value associated with the model coefficients. Since the p-value for <em>hours</em> (0.000) is less than .05, we can say that there is a statistically significant association between <em>hours</em> and <em>score</em>.
<b>R-squared:</b> This tells us the percentage of the variation in the exam scores can be explained by the number of hours studied. In this case, <b>83.1%</b> of the variation in scores can be explained hours studied.
<b>F-statistic & p-value:</b> The F-statistic (<b>63.91</b>) and the corresponding p-value (<b>2.25e-06</b>) tell us the overall significance of the regression model, i.e. whether predictor variables in the model are useful for explaining the variation in the response variable. Since the p-value in this example is less than .05, our model is statistically significant and <em>hours</em> is deemed to be useful for explaining the variation in <em>score</em>.
<h2>Step 3: Visualize the Line of Best Fit</h2>
Lastly, we can use the <b>matplotlib</b> data visualization package to visualize the fitted regression line over the actual data points:
<b>import matplotlib.pyplot as plt
#find line of best fit
a, b = np.polyfit(df['hours'], df['score'], 1)
#add points to plot
plt.scatter(df['hours'], df['score'], color='purple')
#add line of best fit to plot
plt.plot(df['hours'], a*df['hours']+b)
#add fitted regression equation to plot
plt.text(1, 90, 'y = ' + '{:.3f}'.format(b) + ' + {:.3f}'.format(a) + 'x', size=12)
#add axis labels
plt.xlabel('Hours Studied')
plt.ylabel('Exam Score')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/line11.jpg"502">
The purple points represent the actual data points and the blue line represents the fitted regression line.
We also used the <b>plt.text()</b> function to add the fitted regression equation to the top left corner of the plot.
From looking at the plot, it looks like the fitted regression line does a pretty good job of capturing the relationship between the <b>hours</b> variable and the <b>score</b> variable.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Logistic Regression in Python 
 How to Perform Exponential Regression in Python 
 How to Calculate AIC of Regression Models in Python 
<h2><span class="orange">Omitted Variable Bias: Definition & Examples</span></h2>
<b>Omitted variable bias </b>occurs when a relevant  explanatory variable  is not included in a  regression model , which can cause the coefficient of one or more explanatory variables in the model to be biased.
An omitted variable is often left out of a regression model for one of two reasons:
<b>1.</b> Data for the variable is simply not available.
<b>2. </b>The effect of the explanatory variable on the  response variable  is unknown.
In order for the omitted variable to actually bias the coefficients in the model, the following two requirements must be met:
<b>1.</b> The omitted variable must be correlated with one or more explanatory variables in the model.
<b>2.</b> The omitted variable must be correlated with the response variable in the model.
<h3>The Effects of Omitted Variable Bias</h3>
Suppose we have two explanatory variables, A and B, and one response variable, Y. Suppose we fit a simple linear regression model with A as the only explanatory variable and we leave B out of the model.
If B is correlated with A <em>and </em>correlated with Y, then it will cause the coefficient estimate of A to be biased. The following diagram shows how the coefficient estimate of A will be biased, depending on the nature of the relationship with B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/omitted1.png">
<h3>Example: Omitted Variable Bias</h3>
Suppose we want to study the effect that square footage has on house price so we fit the following simple linear regression model:
House price = B<sub>0</sub> + B<sub>1</sub>(square footage)
Suppose we find the estimated model to be:
<b>House price =  40,203.91 + 118.31(square footage)</b>
The way we would interpret the coefficient for square footage is that <em>each additional one unit increase in square footage is associated with an increase in house price of $118.31, on average.</em>
However, suppose we leave out the explanatory variable <em>age</em> which turns out to be highly negatively correlated with square footage and highly negatively correlated with house price. This variable should be in the model, but it’s not. Thus, the coefficient estimate for square footage is likely biased.
Based on the fact that <em>age </em>is negatively correlated with both the explanatory variable and the response variable in the model, we would expect the coefficient estimate for square footage to be positively biased:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/omitted2.png">
Suppose we find data for house age and then include it in the model. The model then becomes:
House price = B<sub>0</sub> + B<sub>1</sub>(square footage) + B<sub>2</sub>(age) 
Suppose we find the estimated model to be:
<b>House price =  123,426.20 + 81.06(square footage) – 1,291.04(age)</b>
Note that the coefficient estimate for square footage went significantly down, which means it <em>was </em>positively biased in the previous model.
The way we would interpret the coefficient for square footage in this model is that <em>each additional one unit increase in square footage is associated with an average increase in house price of $81.06, assuming age is held constant.</em>
<h3>What to Do About Omitted Variable Bias</h3>
Unfortunately omitted variable bias occurs often in the real world because there are usually some variables that <em>should </em>be included in a regression model but aren’t because data for them isn’t available or the relationship between them and the response variable is unknown.
If possible, you should try to include any and all relevant explanatory variables in a regression model so that you can understand the true relationship between the explanatory variables and the response variable. 
Leaving relevant explanatory variables out of a model can significantly affect the interpretation of the model, as we saw in the previous example with house prices.
<h2><span class="orange">What is an Omnibus Test? (Definition & Examples)</span></h2>
In statistics, an <b>omnibus test</b> is any statistical test that tests for the significance of several parameters in a model at once.
For example, suppose we have the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub> = … = μ<sub>k </sub>(all the population means are equal)
<b>H<sub>A</sub>:</b> At least one population mean is different from the rest
This is an example of an omnibus test because the null hypothesis contains more than two parameters. 
If we reject the null hypothesis, we know that at least one of the population means is different from the rest, but we don’t specifically know which population means are different.
An omnibus test appears most commonly in  ANOVA models  and  multiple linear regression models .
This tutorial provides an example of an omnibus test in both a one-way ANOVA and a multiple linear regression model.
<h3>Omnibus Test in a One-Way ANOVA</h3>
Suppose a professor wants to know whether three different exam prep programs lead to different exam scores. To test this, he randomly assigns 10 students to use each exam prep program for one month and then administers the same exam to the students in each group.
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png">
To determine if each prep program leads to the same exam scores, he performs a one-way ANOVA using the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub>
<b>H<sub>A</sub>:</b> At least one exam prep program leads to different mean scores than the rest.
This is an example of an omnibus test because the null hypothesis has more than two parameters.
Using a  One-Way ANOVA Calculator , he is able to produce the following ANOVA table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay4.png">
To determine if he can reject or fail to reject the null hypothesis, he only needs to look at the F test statistic and the corresponding p-value in the table.
The F test statistic is <b>2.358</b> and the corresponding p-value is <b>0.11385</b>. Since this p-value is not less than .05, he fails to reject the null hypothesis.
In other words, he doesn’t have sufficient evidence to say that any of the exam prep programs lead to different average exam scores.
<b>Note:</b> If the p-value was less than .05, then the professor would reject the null hypothesis. He could then perform  post-hoc tests  to determine exactly which programs produced different average exam scores.
<h3>Omnibus Test in a Multiple Linear Regression Model</h3>
Suppose a professor wants to determine if the number of hours studied and the number of prep exams taken can predict the exam score that a student will receive.
To test this, he collects data for 20 students and fits the following multiple linear regression model:
Exam Score = β<sub>0</sub> + β<sub>1</sub>(hours) + β<sub>2</sub>(prep exams)
This regression model using the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> β<sub>1</sub> = β<sub>2</sub> = 0
<b>H<sub>A</sub>:</b> At least one coefficient is not equal to zero.
This is an example of an omnibus test because the null hypothesis is testing if several parameters are equal to zero at once.
The following  regression output in Excel  shows the results of this regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegExcel4.png">
To determine if he can reject or fail to reject the null hypothesis, he only needs to look at the F test statistic and the corresponding p-value in the table.
The F test statistic is <b>23.46</b> and the corresponding p-value is <b>0.00</b>. Since this p-value is less than .05, he can reject the null hypothesis and conclude that at least one of the model coefficients is not equal to zero.
However, simply rejecting the null hypothesis of this omnibus test doesn’t actually tell him which model coefficients are not equal to zero. To determine this, he must look at the p-values for the individual model coefficients:
P-value of hours: <b>0.00</b>
P-value of prep exams: <b>0.52</b>
This tells him that hours is a statistically significant predictor of exam score while prep exams is not.
<h2>Summary</h2>
Here’s a summary of what we learned in this article:
An <b>omnibus test</b> is used to test for the significance of several model parameters at once.
If we reject the null hypothesis of an omnibus test, we know that at least one model parameter is significant.
If we reject the null hypothesis of an ANOVA model, we can use  post-hoc tests  to determine which population means are actually different.
If we reject the null hypothesis of a multiple linear regression model, we can look at the p-values for the individual model coefficients to determine which ones are statistically significant.
<h2>Additional Resources</h2>
The following tutorials explain how to perform a one-way ANOVA and multiple linear regression in Excel:
 How to Perform a One-Way ANOVA in Excel 
 How to Perform Multiple Linear Regression in Excel 
<h2><span class="orange">How to Perform One-Hot Encoding in Python</span></h2>
<b>One-hot encoding</b> is used to convert categorical variables into a format that can be readily used by  machine learning algorithms .
The basic idea of one-hot encoding is to create new variables that take on values 0 and 1 to represent the original categorical values.
For example, the following image shows how we would perform one-hot encoding to convert a categorical variable that contains team names into new variables that contain only 0 and 1 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/oneHot1.png">
The following step-by-step example shows how to perform one-hot encoding for this exact dataset in Python.
<h3>Step 1: Create the Data</h3>
First, let’s create the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29]})
#view DataFrame
print(df)
  team  points
0    A      25
1    A      12
2    B      15
3    B      14
4    B      19
5    B      23
6    C      25
7    C      29</b>
<h3>Step 2: Perform One-Hot Encoding</h3>
Next, let’s import the <b>OneHotEncoder()</b> function from the <b>sklearn</b> library and use it to perform one-hot encoding on the ‘team’ variable in the pandas DataFrame:
<b>from sklearn.preprocessing import OneHotEncoder
#creating instance of one-hot-encoder
encoder = OneHotEncoder(handle_unknown='ignore')
#perform one-hot encoding on 'team' column 
encoder_df = pd.DataFrame(encoder.fit_transform(df[['team']]).toarray())
#merge one-hot encoded columns back with original DataFrame
final_df = df.join(encoder_df)
#view final df
print(final_df)
  team  points    0    1    2
0    A      25  1.0  0.0  0.0
1    A      12  1.0  0.0  0.0
2    B      15  0.0  1.0  0.0
3    B      14  0.0  1.0  0.0
4    B      19  0.0  1.0  0.0
5    B      23  0.0  1.0  0.0
6    C      25  0.0  0.0  1.0
7    C      29  0.0  0.0  1.0
</b>
Notice that three new columns were added to the DataFrame since the original ‘team’ column contained three unique values.
<b>Note</b>: You can find the complete documentation for the <b>OneHotEncoder()</b> function  here .
<h3>Step 3: Drop the Original Categorical Variable</h3>
Lastly, we can drop the original ‘team’ variable from the DataFrame since we no longer need it:
<b>#drop 'team' column
final_df.drop('team', axis=1, inplace=True)
#view final df
print(final_df)
   points    0    1    2
0      25  1.0  0.0  0.0
1      12  1.0  0.0  0.0
2      15  0.0  1.0  0.0
3      14  0.0  1.0  0.0
4      19  0.0  1.0  0.0
5      23  0.0  1.0  0.0
6      25  0.0  0.0  1.0
7      29  0.0  0.0  1.0
</b>
<b>Related:</b>  How to Drop Columns in Pandas (4 Methods) 
We could also rename the columns of the final DataFrame to make them easier to read:
<b>#rename columns
final_df.columns = ['points', 'teamA', 'teamB', 'teamC']
#view final df
print(final_df)
   points  teamA  teamB  teamC
0      25    1.0    0.0    0.0
1      12    1.0    0.0    0.0
2      15    0.0    1.0    0.0
3      14    0.0    1.0    0.0
4      19    0.0    1.0    0.0
5      23    0.0    1.0    0.0
6      25    0.0    0.0    1.0
7      29    0.0    0.0    1.0
</b>
The one-hot encoding is complete and we can now feed this pandas DataFrame into any machine learning algorithm that we’d like.
<h2><span class="orange">How to Perform One-Hot Encoding in R</span></h2>
<b>One-hot encoding</b> is used to convert categorical variables into a format that can be used by  machine learning algorithms .
The basic idea of one-hot encoding is to create new variables that take on values 0 and 1 to represent the original categorical values.
For example, the following image shows how we would perform one-hot encoding to convert a categorical variable that contains team names into new variables that contain only 0 and 1 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/oneHot1.png">
The following step-by-step example shows how to perform one-hot encoding for this exact dataset in R.
<h3>Step 1: Create the Data</h3>
First, let’s create the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'), points=c(25, 12, 15, 14, 19, 23, 25, 29))
#view data frame
df
  team points
1    A     25
2    A     12
3    B     15
4    B     14
5    B     19
6    B     23
7    C     25
8    C     29
</b>
<h3>Step 2: Perform One-Hot Encoding</h3>
Next, let’s use the <b>dummyVars()</b> function from the <b>caret </b>package to perform one-hot encoding on the ‘team’ variable in the data frame:
<b>library(caret)
#define one-hot encoding function
dummy &lt;- dummyVars(" ~ .", data=df)
#perform one-hot encoding on data frame
final_df &lt;- data.frame(predict(dummy, newdata=df))
#view final data frame
final_df
  teamA teamB teamC points
1     1     0     0     25
2     1     0     0     12
3     0     1     0     15
4     0     1     0     14
5     0     1     0     19
6     0     1     0     23
7     0     0     1     25
8     0     0     1     29 </b>
Notice that three new columns were added to the data frame since the original ‘team’ column contained three unique values.
Also notice that the original ‘team’ column was dropped from the data frame since it’s no longer needed.
The one-hot encoding is complete and we can now feed this dataset into any machine learning algorithm that we’d like.
<b>Note</b>: You can find the complete online documentation for the <b>dummyVars()</b> function  here .
<h2><span class="orange">One Proportion Z-Test Calculator</span></h2>
A <b>one proportion z-test</b> is used to compare an observed proportion to a theoretical one. The test statistic is calculated as:
<b>z</b> = (p-p<sub>0</sub>) / √(p<sub>0</sub>(1-p<sub>0</sub>)/n)
where:
p = observed sample proportion
p<sub>0</sub> = hypothesized population proportion
n = sample size
To perform a one proportion z-test, simply fill in the information below and then click the “Calculate” button.
<label><b>p<sub>0</sub></b> (hypothesized population proportion)</label>
<input type="number" id="p0" value="0.42">
<label><b>p</b> (observed sample proportion)</label>
<input type="number" id="p" value="0.47">
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="30">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
z-statistic: 0.55487
p-value (one-tailed): 0.28949
p-value (two-tailed): 0.57898
95% C.I. = [0.2914, 0.6486]
<script>
function calc() {
//get input values
var p0 = +document.getElementById('p0').value;
var p = +document.getElementById('p').value;
var n = +document.getElementById('n').value;
//calculate stuff
var z = (p-p0)/(Math.sqrt(p0*(1-p0)/n));
//calculate p-value
if (z<0) {
var p1 = jStat.normal.cdf(z, 0, 1);
var p2 = p1*2;
} else {
var p1 = 1-jStat.normal.cdf(z, 0, 1);
var p2 = p1*2;
}
//calculate C.I.
var zCrit = Math.abs(jStat.normal.inv(.975, 0, 1));
var se = Math.sqrt(p*(1-p)/n);
var low = p - (zCrit*se);
var high = p - (-1*zCrit*se);
//output probabilities
document.getElementById('z').innerHTML = z.toFixed(5);
document.getElementById('p1').innerHTML = p1.toFixed(5);
document.getElementById('p2').innerHTML = p2.toFixed(5);
document.getElementById('low').innerHTML = low.toFixed(4);
document.getElementById('high').innerHTML = high.toFixed(4);
}
</script>
<h2><span class="orange">How to Perform a One Proportion Z-Test in Excel</span></h2>
A <b>one proportion z-test </b>is used to compare an observed proportion to a theoretical one.
For example, suppose a phone company claims that 90% of its customers are satisfied with their service. To test this claim, an independent researcher gathered a simple random sample of 200 customers and asked them if they are satisfied with their service, to which 85% responded yes.
We can use a one proportion z-test to test whether or not the true percentage of customers who are satisfied with their service is actually 90%.
<h2>Steps to Perform a One Sample Z-Test</h2>
We can use the following steps to perform the one proportion z-test:
<b>Step 1. State the hypotheses. </b>
The null hypothesis (H0): P = 0.90
The alternative hypothesis: (Ha): P ≠ 0.90
<b>Step 2. Find the test statistic and the corresponding p-value.</b>
Test statistic <em>z</em>  =  (p-P) / (√P(1-P) / n)
where p is the sample proportion, P is the hypothesized population proportion, and n is the sample size.
z = (.85-.90) / (√.90(1-.90) / 200) = (-.05) / (.0212) = <b>-2.358</b>
Use the  Z Score to P Value Calculator  with a z score of -2.358 and a two-tailed test to find that the p-value = <b>0.018</b>.
<b>Step 3. Reject or fail to reject the null hypothesis.</b>
First, we need to choose a significance level to use for the test. Common choices are 0.01, 0.05, and 0.10. For this example, let’s use 0.05. Since the p-value is less than our significance level of .05, we reject the null hypothesis.
Since we rejected the null hypothesis, we have sufficient evidence to say that it’s not true that 90% of customers are satisfied with their service.
<h2>How to Perform a One Sample Z-Test in Excel</h2>
The following examples illustrate how to perform a one sample z-test in Excel.
<h3>One Sample Z Test (Two-tailed)</h3>
A phone company claims that 90% of its customers are satisfied with their service. To test this claim, an independent researcher gathered a simple random sample of 200 customers and asked them if they are satisfied with their service, to which 190 responded yes.
<em><b>Test the null hypothesis that 90% of customers are satisfied with their service against the alternative hypothesis that not 90% of customers are satisfied with their service. Use a 0.05 level of significance.</b></em>
The following screenshot shows how to perform a two-tailed one sample z test in Excel, along with the formulas used:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/oneSamp2.jpg">
You need to fill in the values for cells <b>B1:B3</b>. Then, the values for cells <b>B5:B7 </b>are automatically calculated using the formulas shown in cells <b>C5:C7</b>.
Note that the formulas shown do the following:
Formula in cell <b>C5</b>: This calculates the sample proportion using the formula <b>Frequency / Sample size</b>
Formula in cell <b>C6</b>: This calculates the test statistic using the formula <b>(p-P) / (√P(1-P) / n) </b>where p is the sample proportion, P is the hypothesized population proportion, and n is the sample size.
Formula in cell <b>C6</b>: This calculates the p-value associated with the test statistic calculated in cell <b>B6</b> using the Excel function <b>NORM.S.DIST</b>, which returns the cumulative probability for the normal distribution with mean = 0 and standard deviation = 1. We multiply this value by two since this is a two-tailed test.
Since the p-value (<b>0.018</b>) is less than our chosen significance level of  <b>0.05</b>, we reject the null hypothesis and conclude that the true percentage of customers who are satisfied with their service is not equal to 90%.
<h3>One Sample Z Test (One-tailed)</h3>
A phone company claims that <em>at least </em>90% of its customers are satisfied with their service. To test this claim, an independent researcher gathered a simple random sample of 200 customers and asked them if they are satisfied with their service, to which 176 responded yes.
<em><b>Test the null hypothesis that</b></em><b> <em>at least </em>90%</b><em><b> of customers are satisfied with their service against the alternative hypothesis that less than 90% of customers are satisfied with their service. Use a 0.1 level of significance.</b></em>
The following screenshot shows how to perform a one-tailed one sample z test in Excel, along with the formulas used:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/06/oneSamp3.jpg">
You need to fill in the values for cells <b>B1:B3</b>. Then, the values for cells <b>B5:B7 </b>are automatically calculated using the formulas shown in cells <b>C5:C7</b>.
Note that the formulas shown do the following:
Formula in cell <b>C5</b>: This calculates the sample proportion using the formula <b>Frequency / Sample size</b>
Formula in cell <b>C6</b>: This calculates the test statistic using the formula <b>(p-P) / (√P(1-P) / n) </b>where p is the sample proportion, P is the hypothesized population proportion, and n is the sample size.
Formula in cell <b>C6</b>: This calculates the p-value associated with the test statistic calculated in cell <b>B6</b> using the Excel function <b>NORM.S.DIST</b>, which returns the cumulative probability for the normal distribution with mean = 0 and standard deviation = 1.
Since the p-value (<b>0.17</b>) is greater than our chosen significance level of  <b>0.1</b>, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the true percentage of customers who are satisfied with their service is less than 90%.
<h2><span class="orange">How to Perform a One Proportion Z-Test in R (With Examples)</span></h2>
A  one proportion z-test  is used to compare an observed proportion to a theoretical one.
This test uses the following null hypotheses:
<b>H<sub>0</sub>: </b>p = p<sub>0</sub> (population proportion is equal to hypothesized proportion p<sub>0</sub>)
The alternative hypothesis can be either two-tailed, left-tailed, or right-tailed:
<b>H<sub>1</sub> (two-tailed): </b>p ≠ p<sub>0</sub> (population proportion is not equal to some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (left-tailed): </b>p &lt; p<sub>0</sub> (population proportion is less than some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (right-tailed): </b>p > p<sub>0</sub> (population proportion is greater than some hypothesized value p<sub>0</sub>)
The test statistic is calculated as:
z = (p-p<sub>0</sub>) / √p<sub>0</sub>(1-p<sub>0</sub>)/n
where:
<b>p: </b>observed sample proportion
<b>p<sub>0</sub>:</b> hypothesized population proportion
<b>n: </b>sample size
If the p-value that corresponds to the test statistic z is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h3>One Proportion Z-Test in R</h3>
To perform a one proportion z-test in R, we can use one of the following functions:
<b>If n ≤ 30: </b>binom.test(x, n, p = 0.5, alternative = “two.sided”)
<b>If n> 30: </b>prop.test(x, n, p = 0.5, alternative = “two.sided”, correct=TRUE)
where:
<b>x: </b>The number of successes
<b>n: </b>The number of trials
<b>p: </b>The hypothesized population proportion
<b>alternative: </b>The alternative hypothesis
<b>correct: </b>Whether or not to apply Yates’ continuity correction
The following example shows how to carry out a one proportion z-test in R.
<h3>Example: One Proportion Z-Test in R</h3>
Suppose we want to know whether or not the proportion of residents in a certain county who support a certain law is equal to 60%. To test this, we collect the following data on a random sample:
<b>p<sub>0</sub>:</b> hypothesized population proportion = 0.60
<b>x: </b>residents who support law: 64
<b>n: </b>sample size = 100
Since our sample size is greater than 30, we can use the <b>prop.test() </b>function to perform a one sample z-test:
<b>prop.test(x=64, n=100, p=0.60, alternative="two.sided")
1-sample proportions test with continuity correction
data:  64 out of 100, null probability 0.6
X-squared = 0.51042, df = 1, p-value = 0.475
alternative hypothesis: true p is not equal to 0.6
95 percent confidence interval:
 0.5372745 0.7318279
sample estimates:
   p 
0.64 </b>
From the output we can see that the p-value is <b>0.475</b>. Since this value is not less than α = 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the proportion of residents who support the law is different from 0.60.
The 95% confidence interval for the true proportion of residents in the county that support the law is also found to be:
<b>95% C.I. = [0.5373, 7318]</b>
Since this confidence interval contains the proportion <b>0.60</b>, we do not have evidence to say that the true proportion of residents who support the law is different from 0.60. This matches the conclusion we came to using just the p-value of the test.
<h2><span class="orange">How to Perform a One Proportion Z-Test in Python</span></h2>
A  one proportion z-test  is used to compare an observed proportion to a theoretical one.
This test uses the following null hypotheses:
<b>H<sub>0</sub>: </b>p = p<sub>0</sub> (population proportion is equal to hypothesized proportion p<sub>0</sub>)
The alternative hypothesis can be either two-tailed, left-tailed, or right-tailed:
<b>H<sub>1</sub> (two-tailed): </b>p ≠ p<sub>0</sub> (population proportion is not equal to some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (left-tailed): </b>p &lt; p<sub>0</sub> (population proportion is less than some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (right-tailed): </b>p > p<sub>0</sub> (population proportion is greater than some hypothesized value p<sub>0</sub>)
The test statistic is calculated as:
z = (p-p<sub>0</sub>) / √p<sub>0</sub>(1-p<sub>0</sub>)/n
where:
<b>p: </b>observed sample proportion
<b>p<sub>0</sub>:</b> hypothesized population proportion
<b>n: </b>sample size
If the p-value that corresponds to the test statistic z is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h3>One Proportion Z-Test in Python</h3>
To perform a one proportion z-test in Python, we can use the  proportions_ztest()  function from the <b>statsmodels</b> library, which uses the following syntax:
<b>proportions_ztest(count, nobs, value=None, alternative=’two-sided’) </b>
where:
<b>count: </b>The number of successes
<b>nobs: </b>The number of trials
<b>value: </b>The hypothesized population proportion
<b>alternative: </b>The alternative hypothesis
This function returns a z test-statistic and a corresponding p-value.
The following example shows how to use this function to perform a one proportion z-test in Python.
<h3>Example: One Proportion Z-Test in Python</h3>
Suppose we want to know whether or not the proportion of residents in a certain county who support a certain law is equal to 60%. To test this, we collect the following data on a random sample:
<b>p<sub>0</sub>:</b> hypothesized population proportion = 0.60
<b>x: </b>residents who support law: 64
<b>n: </b>sample size = 100
The following code shows how to use the <b>proportions_ztest</b> function to perform a one sample z-test:
<b>#import proportions_ztest function
from statsmodels.stats.proportion import proportions_ztest
#perform one proportion z-test
proportions_ztest(count=60, nobs=100, value=0.64)
(-0.8164965809277268, 0.41421617824252466)</b>
From the output we can see that the z test-statistic is <b>-0.8165 </b>and the corresponding p-value is <b>0.4142</b>. Since this value is not less than α = 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the proportion of residents who support the law is different from 0.60.
<h2><span class="orange">One Proportion Z-Test: Definition, Formula, and Example</span></h2>
A <b>one proportion z-test</b> is used to compare an observed proportion to a theoretical one.
This tutorial explains the following:
The motivation for performing a one proportion z-test.
The formula to perform a one proportion z-test.
An example of how to perform a one proportion z-test.
<h3>One Proportion Z-Test: Motivation</h3>
Suppose we want to know if the proportion of people in a certain county that are in favor of a certain law is equal to 60%. Since there are thousands of residents in the county, it would be too costly and time-consuming to go around and ask each resident about their stance on the law.
Instead, we might select a  simple random sample  of residents and ask each one whether or not they support the law:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CIprop1.png"(max-width: 429px) 100vw, 429px">
However, it’s virtually guaranteed that the proportion of residents in the sample who support the law will be at least a little different from the proportion of residents in the entire population who support the law. <b>The question is whether or not this difference is statistically significant</b>. Fortunately, a one proportion z-test allows us to answer this question.
<h3>
<b>One Proportion Z-Test: Formula</b>
</h3>
A one proportion z-test always uses the following null hypothesis:
<b>H<sub>0</sub>: </b>p = p<sub>0</sub> (population proportion is equal to some hypothesized population proportion p<sub>0</sub>)
The alternative hypothesis can be either two-tailed, left-tailed, or right-tailed:
<b>H<sub>1</sub> (two-tailed): </b>p ≠ p<sub>0</sub> (population proportion is not equal to some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (left-tailed): </b>p &lt; p<sub>0</sub> (population proportion is less than some hypothesized value p<sub>0</sub>)
<b>H<sub>1</sub> (right-tailed): </b>p > p<sub>0</sub> (population proportion is greater than some hypothesized value p<sub>0</sub>)
We use the following formula to calculate the test statistic z:
z = (p-p<sub>0</sub>) / √p<sub>0</sub>(1-p<sub>0</sub>)/n
where:
<b>p: </b>observed sample proportion
<b>p<sub>0</sub>:</b> hypothesized population proportion
<b>n: </b>sample size
If the p-value that corresponds to the test statistic z is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h3>
<b>One Proportion Z-Test: Example</b>
</h3>
Suppose we want to know whether or not the proportion of residents in a certain county who support a certain law is equal to 60%. To test this, will perform a one proportion z-test at significance level α = 0.05 using the following steps:
<b>Step 1: Gather the sample data.</b>
Suppose we survey a random sample of residents and end up with the following information:
<b>p: </b>observed sample proportion = 0.64
<b>p<sub>0</sub>:</b> hypothesized population proportion = 0.60
<b>n: </b>sample size = 100
<b>Step 2: Define the hypotheses.</b>
We will perform the one sample t-test with the following hypotheses:
<b>H<sub>0</sub>: </b>p = 0.60 (population proportion is equal to 0.60)
<b>H<sub>1</sub>: </b>p ≠ 0.60 (population proportion is not equal to 0.60)
<b>Step 3: Calculate the test statistic <em>z</em>.</b>
<b>z </b>= (p-p<sub>0</sub>) / √p<sub>0</sub>(1-p<sub>0</sub>)/n = (.64-.6) / √.6(1-.6)/100 = <b>0.816</b>
<b>Step 4: Calculate the p-value of the test statistic <em>z</em>.</b>
According to the  Z Score to P Value Calculator , the two-tailed p-value associated with z = 0.816 is <b>0.4145</b>.
<b>Step 5: Draw a conclusion.</b>
Since this p-value is not less than our significance level α = 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the proportion of residents who support the law is different from 0.60.
<em><b>Note: </b>You can also perform this entire one proportion z-test by simply using the  One Proportion Z-Test Calculator .</em>
<h2><span class="orange">One Sample t-test Calculator</span></h2>
A <b>one sample t-test</b> is used to test whether or not the mean of a population is equal to some value.
To perform a one sample t-test, simply fill in the information below and then click the “Calculate” button.
<label for="raw">Enter raw data</label>
<input type="radio" id="raw" name="tails" onclick="check()" checked><label for="summary">Enter summary data</label>
<input type="radio" id="summary" name="tails" onclick="check()">
<textarea id="rawData" rows="5" cols="40">301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304</textarea>
<label><b>x</b> (sample mean)</label>
<input type="number" id="x" value="300">
<label><b>s</b> (sample standard deviation)</label>
<input type="number" id="s" value="18.5">
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="40">
<label><b>μ<sub>0</sub></b> (hypothesized population mean)</label>
<input type="number" id="mu" value="310">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>t = </b> 0.3232
<b>df = </b> 39
<b>p-value (one-tailed) = </b> 0.1245
<b>p-value (two-tailed) = </b> 0.3232
<script>
//set summary table to hidden to start
var summary_display = document.getElementById("summary_table");
summary_display.style.display = "none";
//find which radio button is checked
function check() {
if (document.getElementById('raw').checked) {
var table_display = document.getElementById("words_table");
        table_display.style.display = "block";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "none";
} else {
var table_display = document.getElementById("words_table");
        table_display.style.display = "none";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "block";
}
} //end check
//perform one-sample t-test
function calc() {
if (document.getElementById('summary').checked) {
var mu = +document.getElementById('mu').value;
var x = +document.getElementById('x').value;
var s = +document.getElementById('s').value;
var n = +document.getElementById('n').value;
var t = (x-mu)/(s/Math.sqrt(n));
var df = n-1;
if (t<0) {
var p1 = jStat.studentt.cdf(t, df);
var p2 = p1*2;
} else {
var p1 = 1-jStat.studentt.cdf(t, df);
var p2 = p1*2;
}
document.getElementById('t').innerHTML = t.toFixed(6);
document.getElementById('df').innerHTML = df;
document.getElementById('p1').innerHTML = p1.toFixed(6);
document.getElementById('p2').innerHTML = p2.toFixed(6);
} else {
var raw = document.getElementById('rawData').value.split(',').map(Number);
var mu = +document.getElementById('mu').value;
var x = math.mean(raw)
var s = math.std(raw)
var n = raw.length;
var t = (x-mu)/(s/Math.sqrt(n));
var df = n-1;
if (t<0) {
var p1 = jStat.studentt.cdf(t, df);
var p2 = p1*2;
} else {
var p1 = 1-jStat.studentt.cdf(t, df);
var p2 = p1*2;
}
document.getElementById('t').innerHTML = t.toFixed(6);
document.getElementById('df').innerHTML = df;
document.getElementById('p1').innerHTML = p1.toFixed(6);
document.getElementById('p2').innerHTML = p2.toFixed(6);
}
//output results
}
</script>
<h2><span class="orange">How to Conduct a One Sample t-Test in Excel</span></h2>
A  <b>one sample t-test</b>  is used to test whether or not the mean of a population is equal to some value.
This tutorial explains how to conduct a one sample t-test in Excel.
<h2>How to Conduct a One Sample t-Test in Excel</h2>
Suppose a botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a random sample of 12 plants and records each of their heights in inches.
The following image shows the height (in inches) for each plant in the sample:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/oneSampExcel1.png">
We can use the following steps to conduct a one sample t-test to determine if the mean height for this species of plant is actually equal to 15 inches.
<b>Step 1: Find the sample size, sample mean, and sample standard deviation.</b>
First, we need to find the sample size, sample mean, and sample standard deviation, which will all be used to conduct the one sample t-test.
The following image shows the formulas we can use to calculate these values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/oneSampExcel2.png">
<b>Step 2: Calculate the test statistic <em>t</em>.</b>
Next, we will calculate the test statistic <em>t </em>using the following formula:
<em>t </em> = x – μ / (s/√n)
where:
x = sample mean
μ = hypothesized population mean
s = sample standard deviation
n = sample size
The following image shows how to calculate <em>t </em>in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/oneSampExcel3.png">
The test statistic <em>t </em>turns out to be <b>-1.68485</b>.
<b>Step 3: Calculate the p-value of the test statistic.</b>
Next, we need to calculate the p-value associated with the test statistic using the following function in Excel:
=T.DIST.2T(ABS(x), deg_freedom)
where:
x = test statistic <em>t</em>
deg_freedom = degrees of freedom for the test, which is calculated as n-1
<b>Technical Notes: </b>
 
The function <b>T.DIST.2T()</b> returns the p-value for a two-tailed t-test. If you’re instead conducting a left-tailed t-test or a right-tailed t-test, you would instead use the functions <b>T.DIST()</b> or <b>T.DIST.RT()</b>, respectively.
The following image shows how to calculate the p-value for our test statistic:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/oneSampExcel4.png">
The p-value turns out to be <b>0.120145</b>.
<b>Step 4: Interpret the results.</b>
The two hypotheses for this particular one sample t test are as follows:
<b>H<sub>0</sub>: </b>μ = 15 (the mean height for this species of plant is 15 inches)
<b>H<sub>A</sub>: </b>μ ≠15 (the mean height is <em>not </em>15 inches)
Because the p-value of our test<b> (0.120145) </b>is greater than alpha = 0.05, we fail to reject the null hypothesis of the test.
We do not have sufficient evidence to say that the mean height for this particular species of plant is different from 15 inches.
<h2><span class="orange">How to Perform a One Sample T-Test in R</span></h2>
A  one sample t-test  is used to determine whether or not the mean of a population is equal to some value.
You can use the following basic syntax in R to perform a one sample t-test:
<b>t.test(data, mu=10)</b>
The following example shows how to use this syntax in practice.
<h2>Example: One Sample T-Test in R</h2>
Suppose a botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. 
She collects a  simple random sample  of 12 plants and records each of their heights in inches.
She can use the following code to perform a one sample t-test in R to determine if the mean height for this species of plant is actually equal to 15 inches:
<b>#create vector to hold plant heights
my_data &lt;- c(14, 14, 16, 13, 12, 17, 15, 14, 15, 13, 15, 14)
#perform one sample t-test
t.test(my_data, mu=15)
One Sample t-test
data:  my_data
t = -1.6848, df = 11, p-value = 0.1201
alternative hypothesis: true mean is not equal to 15
95 percent confidence interval:
 13.46244 15.20423
sample estimates:
mean of x 
 14.33333 
</b>
Here is how to interpret each value in the output:
<b>data</b>: The name of the vector used in the t-test. In this example, we used <b>my_data</b>.
<b>t</b>: The t test-statistic, calculated as (x – μ) / (s√n) = (14.333-15)/(1.370689/√12) = <b>-1.6848</b>.
<b>df</b>: The degrees of freedom, calculated as n-1 = 12-1 = <b>11</b>.
<b>p-value</b>: The two-tailed p-value that corresponds to a t test-statistic of -1.6848 and 11 degrees of freedom. In this case, p = <b>0.1201</b>.
<b>95 percent confidence interval</b>: The 95% confidence for the true population mean, calculated to be <b>[13.46244, 15.20423]</b>.
The null and alternative hypotheses for this one sample t-test are as follows:
<b>H<sub>0</sub>: </b>μ = 15 (the mean height for this species of plant is 15 inches)
<b>H<sub>A</sub>: </b>μ ≠15 (the mean height is <em>not </em>15 inches)
Because the p-value of our test<b> (0.1201) </b>is greater than 0.05, we fail to reject the null hypothesis of the test.
This means we do not have sufficient evidence to say that the mean height for this particular species of plant is different from 15 inches.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tests in R:
 How to Perform a Two Sample T-Test in R 
 How to Perform a Paired Samples T-Test in R 
 How to Perform Welch’s T-Test in R 
<h2><span class="orange">How to Perform a One Sample t-Test in SAS</span></h2>
A  one sample t-test  is used to determine whether or not the mean of a  population  is equal to some value.
This tutorial explains how to perform a one sample t-test in SAS.
<h3>Example: One Sample t-Test in SAS</h3>
Suppose a botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a  random sample  of 12 plants and records each of their heights in inches.
The heights are as follows: 14, 14, 16, 13, 12, 17, 15, 14, 15, 13, 15, 14
Use the following steps to conduct a one sample t-test to determine if the mean height for this species of plant is actually equal to 15 inches.
<b>Step 1: Create the data.</b>
First, we’ll use the following code to create the dataset in SAS:
<b>/*create dataset*/
data my_data;
    input Height;
    datalines;
14
14
16
13
12
17
15
14
15
13
15
14
;
run;
/*print dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/oneSampleSAS1.jpg"110">
<b>Step 2: Perform a one sample t-test.</b>
Next, we’ll use <b>proc ttest</b> to perform the one sample t-test:
<b>/*perform one sample t-test*/
proc ttest data=my_data sides=2 alpha=0.05  h0=15;
    var Height;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/oneSampleSAS2.jpg"379">
The first table displays descriptive statistics for our sample, including:
N (total observations): <b>12</b>
Mean (sample mean): <b>14.3333</b>
Std Dev (sample standard deviation): <b>1.3707</b>
Std Error (standard error, calculated as s/√n): <b>.3957</b>
Minimum (the minimum value): <b>12</b>
Maximum (the maximum value) <b>17</b>
The second table displays the 95%  confidence interval  for the true population mean:
95% C.I. for μ: <b>[13.4624, 15.2042]</b>
The third table displays the t test statistic and corresponding p-value:
t test statistic: <b>-1.68</b>
p-value: <b>0.1201</b>
<b>Note</b>: The t test statistic was calculated as:
t test statistic = (x – μ) / (s/√n)
t test statistic = (14.3333-15) / (1.3707/√12)
t test statistic = -1.68
Recall that the one sample t-test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>μ = 15 inches
<b>H<sub>A</sub>: </b>μ ≠ 15 inches
Since the p-value (<b>.1201</b>) is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the mean height of this certain species of plant is different than 15 inches.
<h2><span class="orange">How to Conduct a One Sample T-Test in Python</span></h2>
A  one sample t-test  is used to determine whether or not the mean of a population is equal to some value.
This tutorial explains how to conduct a one sample t-test in Python.
<h3>Example: One Sample t-Test in Python</h3>
Suppose a botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a random sample of 12 plants and records each of their heights in inches.
Use the following steps to conduct a one sample t-test to determine if the mean height for this species of plant is actually equal to 15 inches.
<b>Step 1: Create the data.</b>
First, we’ll create an array to hold the measurements of the 12 plants:
<b>data = [14, 14, 16, 13, 12, 17, 15, 14, 15, 13, 15, 14]</b>
<b>Step 2: Conduct a one sample t-test.</b>
Next, we’ll use the  ttest_1samp() function  from the scipy.stats library to conduct a one sample t-test, which uses the following syntax:
<b>ttest_1samp(a, popmean)</b>
where:
<b>a: </b>an array of sample observations
<b>popmean: </b>the expected population mean
Here’s how to use this function in our specific example:
<b>import scipy.stats as stats
#perform one sample t-test
stats.ttest_1samp(a=data, popmean=15)
(statistic=-1.6848, pvalue=0.1201)
</b>
The t test statistic is <b>-1.6848 </b>and the corresponding two-sided p-value is <b>0.1201</b>.
<b>Step 3: Interpret the results.</b>
The two hypotheses for this particular one sample t-test are as follows:
<b>H<sub>0</sub>: </b>μ = 15 (the mean height for this species of plant is 15 inches)
<b>H<sub>A</sub>: </b>μ ≠15 (the mean height is <em>not </em>15 inches)
Because the p-value of our test<b> (0.1201) </b>is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height for this particular species of plant is different from 15 inches.
<h2><span class="orange">How to Perform a One Sample t-test in SPSS</span></h2>
A  one sample t-test  is used to test whether or not the mean of a population is equal to some value.
This tutorial explains how to conduct a one sample t-test in SPSS.
<h3>Example: One Sample t-test in SPSS</h3>
A botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a random sample of 12 plants and records each of their heights in inches:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS1.png">
Use the following steps to perform a one sample t-test to determine if the true mean height of this species of plant is equal to 15 inches, based on the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>μ = 15 (the true population mean is equal to 15 inches)
<b>H<sub>1</sub>: </b>μ ≠ 15 (the true population mean is not equal to 15 inches)
<em>Use a significance level of α = 0.05.</em>
<b>Step 1: Choose the One Sample t-test option.</b>
Click the <b>Analyze </b>tab, then <b>Compare Means</b>, then <b>One-Sample T Test</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS2.png">
<b>Step 2: Fill in the necessary values to perform the one sample t-test.</b>
Once you click <b>One-Sample T Test</b>, the following window will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS3.png">
Drag the variable <b>height </b>into the box labelled <b>Test Variable(s) </b>and change the <b>Test Value </b>to 15. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS4.png">
<b>Step 3: Interpret the results.</b>
Once you click <b>OK</b>, the results of the one sample t-test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/oneSampSPSS5.png">
The first table displays summary statistics for the variable height:
<b>N: </b>The sample size
<b>Mean: </b>The mean height of plants in the sample
<b>Std. Deviation: </b>The standard deviation of the height of plants in the sample.
<b>Std. Error Mean: </b>The standard error of the mean, calculated as s/√n
The second table displays the results of the one sample t-test:
<b>t: </b>The test statistic, calculated as (x – μ) / (s/√n) = (14.3333-15) / (1.37/√12) = -1.685
<b>df: </b>The degrees of freedom, calculated as n-1 = 12-1 = 11
<b>Sig. (2-tailed): </b>The two-sided p-value that corresponds to a t value of -1.685 with df=11
<b>Mean Difference: </b>The difference between the sample mean and the hypothesized mean
<b>95% C.I. of the Difference: </b>The 95%  confidence interval  for the true difference between the sample mean and the hypothesized mean
Since the p-value of the test (.120) is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the true mean height of this species of plant is different than 15 inches.
<h2><span class="orange">How to Perform a One Sample t-test in Stata</span></h2>
A  <b>one sample t-test</b>  is used to test whether or not the mean of a population is equal to some value.
This tutorial explains how to conduct a one sample t-test in Stata.
<h2>Example: One Sample t-test in Stata</h2>
Researchers want to know if automobiles, on average, get 20 miles per gallon. They collect a sample of 74 cars and wish to conduct a one sample t-test to determine if the true average mpg is 20 or not.
Perform the following steps to conduct a one sample t-test.
<b>Step 1: Load the data.</b>
First, load the data by typing <b>use http://www.stata-press.com/data/r13/auto </b>in the command box and clicking Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneSampStata1.png">
<b>Step 2: View the raw data.</b>
Before we perform a one sample t-test, let’s first view the raw data. Along the top menu bar, go to <b>Data > Data Editor > Data Editor (Browse)</b>. This will show us a bunch of information about each of the 74 cars, but keep in mind we’re only interested in miles per gallon (the <em>mpg </em>column):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneSampStata2.png">
<b>Step 3: Perform a one sample t-test.</b>
Along the top menu bar, go to <b>Statistics > Summaries, tables, and tests > Classical tests of hypotheses > t test (mean-comparison test)</b>.
Keep <em>One-sample </em>selected. For Variable name, choose <em>mpg</em>. For Hypothesized mean, type in <em>20</em>. For Confidence level, choose any level you’d like. A value of 95 corresponds to a significance level of 0.05. We will leave this at 95. Lastly, click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneSampStata3.png">
The results of the one sample t-test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneSampStata4.png">
Here is how to interpret the results:
<b>Obs: </b>The number of observations. In this case, there are 74 total cars.
<b>Mean: </b>The mean mpg of all cars. In this case, the mean is 21.2973 miles per gallon.
<b>Std. Err: </b>Calculated as σ / √n = 5.785503 / √74 = .6725511.
<b>Std. Dev: </b>The standard deviation of mpg. In this case, it’s 5.785503.
<b>95% Conf. Interval: </b>The 95% confidence interval for the true population mean.
<b>t: </b>The test statistic, calculated as (x – u) / (σ / √n)= (21.2973-20) / 5.785503 / √74 = 1.9289.
<b>degrees of freedom: </b>The degrees of freedom to be used for the t-test, calculated as n-1 = 74-1 = 73.
The p-values for three different one sample t-tests are displayed at the bottom of the results. Since we are interested in understanding if the true average mpg is 20 or not, we will look at the results of the middle test (in which the alternative hypothesis is Ha: mean !=20) which has a p-value of <b>0.0576</b>.
Since this value is not smaller than our significance level of 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the true mean mpg for this population of cars is different than 20 mpg.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our one sample t-test. Here is an example of how to do so:
A one sample t-test was conducted on 74 cars to determine if the true population mean mpg was different than 20 mpg.
 
Results showed that the true population mean was <em>not </em>different than 20 mpg (t = 1.9289 w/ df = 73, p = .0576) at a significance level of 0.05.
 
A 95% confidence interval for the true population mean resulted in the interval of (19.9569, 22.63769).
<h2><span class="orange">How to Perform a One Sample t-test on a TI-84 Calculator</span></h2>
A  <b>one sample t-test</b>  is used to test whether or not the mean of a population is equal to some value.
This tutorial explains how to conduct a one sample t-test on a TI-84 calculator.
<h3>Example: One Sample t-test on a TI-84 Calculator</h3>
Researchers want to know if a certain type of car gets 20 miles per gallon or not. They obtain a random sample of 74 cars and find that the mean is 21.29 mpg while the standard deviation is 5.78 mpg. Use this data to perform a one sample t-test to determine if the true mpg for this type of car is equal to 20 mpg.
<b>Step 1: Select T-Test.</b>
Press Stat. Scroll over to TESTS. Scroll down to T-Test and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oneSampTI1.png">
<b>Step 2: Fill in the necessary info.</b>
The calculator will ask for the following information:
<b>Inpt: </b>Choose whether you are working with raw data (Data) or summary statistics (Stats). In this case, we will highlight Stats and press ENTER.
<b>μ<sub>0</sub>:</b> The mean to be used in the null hypothesis. We will type 20 and press  ENTER.
<b>x:</b> The sample mean. We will type 21.29 and press  ENTER.
<b>s<sub>x</sub></b>: The sample standard deviation. We will type 5.78 and press ENTER.
<b>n</b>: The sample size. We will type 74 and press ENTER.
<b>μ</b>:The alternative hypothesis to be used. Since we are performing a two-tailed test, we will highlight<b> ≠μ<sub>0 </sub></b>and press ENTER. This indicates that our alternative hypothesis is μ≠20. The other two options would be used for left-tailed tests (&lt;μ<sub>0</sub>) and right-tailed tests (>μ<sub>0</sub>) .
Lastly, highlight Calculate and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oneSampTI2.png">
<b>Step 3: Interpret the results.</b>
Our calculator will automatically produce the results of the one-sample t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oneSampTI3.png">
Here is how to interpret the results:
<b>μ≠20</b>: This is the alternative hypothesis for the test.
<b>t=1.919896124</b>: This is the t test-statistic. 
<b>p=0.0587785895</b>: This is the p-value that corresponds to the test-statistic.
<b>x=21.59</b>. This is the sample mean that we entered.
<b>s<sub>x</sub>=5.78</b>. This is the sample standard deviation that we entered.
<b>n=74</b>: This is the sample size that we entered.
Because the p-value of the test (0.0587785895) is not less than 0.05, we fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the mean mpg for this type of car is different from 20 mpg.
<h2><span class="orange">One Sample t-test: Definition, Formula, and Example</span></h2>
A <b>one sample t-test</b> is used to test whether or not the mean of a  population  is equal to some value.
This tutorial explains the following:
The motivation for performing a one sample t-test.
The formula to perform a one sample t-test.
The assumptions that should be met to perform a one sample t-test.
An example of how to perform a one sample t-test.
<h2>One Sample t-test: Motivation</h2>
Suppose we want to know whether or not the mean weight of a certain species of turtle in Florida is equal to 310 pounds. Since there are thousands of turtles in Florida, it would be extremely time-consuming and costly to go around and weigh each individual turtle.
Instead, we might take a  simple random sample  of 40 turtles and use the mean weight of the turtles in this sample to estimate the true population mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CImean1.png">
However, it’s virtually guaranteed that the mean weight of turtles in our sample will differ from 310 pounds. <b>The question is whether or not this difference is statistically significant</b>. Fortunately, a one sample t-test allows us to answer this question.
<h2>
<b>One Sample t-test: Formula</b>
</h2>
A one-sample t-test always uses the following null hypothesis:
<b>H<sub>0</sub>: </b>μ = μ<sub>0</sub> (population mean is equal to some hypothesized value μ<sub>0</sub>)
The alternative hypothesis can be either two-tailed, left-tailed, or right-tailed:
<b>H<sub>1</sub> (two-tailed): </b>μ ≠ μ<sub>0</sub> (population mean is not equal to some hypothesized value μ<sub>0</sub>)
<b>H<sub>1</sub> (left-tailed): </b>μ &lt; μ<sub>0</sub> (population mean is less than some hypothesized value μ<sub>0</sub>)
<b>H<sub>1</sub> (right-tailed): </b>μ > μ<sub>0</sub> (population mean is greater than some hypothesized value μ<sub>0</sub>)
We use the following formula to calculate the test statistic t:
<b>t = (x – μ) / (s/√n)</b>
where:
<b>x: </b>sample mean
<b>μ<sub>0</sub>:</b> hypothesized population mean
<b>s: </b>sample standard deviation
<b>n: </b>sample size
If the p-value that corresponds to the test statistic t with (n-1) degrees of freedom is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can reject the null hypothesis.
<h2>One Sample t-test: Assumptions</h2>
For the results of a one sample t-test to be valid, the following assumptions should be met:
<b>The variable under study should be either an  interval or ratio variable .</b>
<b>The observations in the sample should be  independent .</b>
<b>The variable under study should be approximately normally distributed. </b>You can check this assumption by creating a histogram and visually checking if the distribution has roughly a “bell shape.”
<b>The variable under study should have no outliers.</b> You can check this assumption by creating a  boxplot  and visually checking for outliers.
<h2>
<b>One Sample t-test: Example</b>
</h2>
Suppose we want to know whether or not the mean weight of a certain species of turtle is equal to 310 pounds. To test this, will perform a one-sample t-test at significance level α = 0.05 using the following steps:
<b>Step 1: Gather the sample data.</b>
Suppose we collect a random sample of turtles with the following information:
Sample size n = 40
Sample mean weight x = 300
Sample standard deviation s = 18.5
<b>Step 2: Define the hypotheses.</b>
We will perform the one sample t-test with the following hypotheses:
<b>H<sub>0</sub>: </b>μ = 310 (population mean is equal to 310 pounds)
<b>H<sub>1</sub>: </b>μ ≠ 310 (population mean is not equal to 310 pounds)
<b>Step 3: Calculate the test statistic <em>t</em>.</b>
<b>t </b>= (x – μ) / (s/√n) = (300-310) / (18.5/√40) = <b>-3.4187</b>
<b>Step 4: Calculate the p-value of the test statistic <em>t</em>.</b>
According to the  T Score to P Value Calculator , the p-value associated with t = -3.4817 and degrees of freedom = n-1 = 40-1 = 39 is <b>0.00149</b>.
<b>Step 5: Draw a conclusion.</b>
Since this p-value is less than our significance level α = 0.05, we reject the null hypothesis. We have sufficient evidence to say that the mean weight of this species of turtle is not equal to 310 pounds.
<em><b>Note: </b>You can also perform this entire one sample t-test by simply using the  One Sample t-test calculator .</em>
<h2>Additional Resources</h2>
The following tutorials explain how to perform a one-sample t-test using different statistical programs:
 How to Perform a One Sample t-test in Excel 
 How to Perform a One Sample t-test in SPSS 
 How to Perform a One Sample t-test in Stata 
 How to Perform a One Sample t-test in R 
 How to Conduct a One Sample t-test in Python 
 How to Perform a One Sample t-test on a TI-84 Calculator 
<h2><span class="orange">One Sample Z-Test Calculator</span></h2>
A <b>one sample z-test</b> is used to test whether or not the mean of a population is equal to some value when the population standard deviation is known.
To perform a one sample z-test, simply fill in the information below and then click the “Calculate” button.
<label for="raw">Enter raw data</label>
<input type="radio" id="raw" name="tails" onclick="check()" checked><label for="summary">Enter summary data</label>
<input type="radio" id="summary" name="tails" onclick="check()">
<textarea id="rawData" rows="5" cols="40">301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304</textarea>
<label><b>x</b> (sample mean)</label>
<input type="number" id="x" value="300">
<label><b>n</b> (sample size)</label>
<input type="number" id="n" value="40">
<label><b>σ</b> (population standard deviation)</label>
<input type="number" id="s" value="18.5">
<label><b>μ<sub>0</sub></b> (hypothesized population mean)</label>
<input type="number" id="mu" value="310">
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>z = </b> 0.3232
<b>p-value (one-tailed) = </b> 0.1245
<b>p-value (two-tailed) = </b> 0.3232
<script>
//set summary table to hidden to start
var summary_display = document.getElementById("summary_table");
summary_display.style.display = "none";
//find which radio button is checked
function check() {
if (document.getElementById('raw').checked) {
var table_display = document.getElementById("words_table");
        table_display.style.display = "block";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "none";
} else {
var table_display = document.getElementById("words_table");
        table_display.style.display = "none";
        var summary_display = document.getElementById("summary_table");
        summary_display.style.display = "block";
}
} //end check
//perform one-sample z-test
function calc() {
if (document.getElementById('summary').checked) {
var mu = +document.getElementById('mu').value;
var x = +document.getElementById('x').value;
var s = +document.getElementById('s').value;
var n = +document.getElementById('n').value;
var z = (x-mu)/(s/Math.sqrt(n));
var p1 = jStat.ztest(z)/2;
var p2 = p1*2;
document.getElementById('z').innerHTML = z.toFixed(6);
document.getElementById('p1').innerHTML = p1.toFixed(6);
document.getElementById('p2').innerHTML = p2.toFixed(6);
} else {
var raw = document.getElementById('rawData').value.split(',').map(Number);
var mu = +document.getElementById('mu').value;
var s = +document.getElementById('s').value;
var x = math.mean(raw)
var n = raw.length;
var z = (x-mu)/(s/Math.sqrt(n));
var p1 = jStat.ztest(z)/2;
var p2 = p1*2;
document.getElementById('z').innerHTML = z.toFixed(6);
document.getElementById('p1').innerHTML = p1.toFixed(6);
document.getElementById('p2').innerHTML = p2.toFixed(6);
}
//output results
}
</script>
<h2><span class="orange">One Sample Z-Test: Definition, Formula, and Example</span></h2>
A <b>one sample z-test</b> is used to test whether the mean of a population is less than, greater than, or equal to some specific value.
This test assumes that the population standard deviation is known.
This tutorial explains the following:
The formula to perform a one sample z-test.
The assumptions of a one sample z-test.
An example of how to perform a one sample z-test.
Let’s jump in!
<h2>One Sample Z-Test: Formula</h2>
A one sample z-test will always use one of the following null and alternative hypotheses:
<b>1. Two-Tailed Z-Test</b>
<b>H<sub>0</sub>: </b>μ = μ<sub>0</sub> (population mean is equal to some hypothesized value μ<sub>0</sub>)
<b>H<sub>A</sub>: </b>μ ≠ μ<sub>0</sub> (population mean is not equal to some hypothesized value μ<sub>0</sub>)
<b>2. Left-Tailed Z-Test</b>
<b>H<sub>0</sub>: </b>μ ≥ μ<sub>0</sub> (population mean is greater than or equal to some hypothesized value μ<sub>0</sub>)
<b>H<sub>A</sub>: </b>μ &lt; μ<sub>0</sub> (population mean is less than some hypothesized value μ<sub>0</sub>)
<b>3. Right-Tailed Z-Test</b>
<b>H<sub>0</sub>: </b>μ ≤ μ<sub>0</sub> (population mean is less than or equal to some hypothesized value μ<sub>0</sub>)
<b>H<sub>A</sub>: </b>μ > μ<sub>0</sub> (population mean is greaer than some hypothesized value μ<sub>0</sub>)
We use the following formula to calculate the z test statistic:
<b>z = (x – μ<sub>0</sub>) / (σ/√n)</b>
where:
<b>x: </b>sample mean
<b>μ<sub>0</sub>:</b> hypothesized population mean
<b>σ: </b>population standard deviation
<b>n: </b>sample size
If the p-value that corresponds to the z test statistic is less than your chosen significance level (common choices are 0.10, 0.05, and 0.01) then you can  reject the null hypothesis .
<h2>One Sample Z-Test: Assumptions</h2>
For the results of a one sample z-test to be valid, the following assumptions should be met:
The data are continuous (not discrete).
The data is a  simple random sample  from the population of interest.
The data in the population is approximately  normally distributed .
The population standard deviation is known.
<h2>One Sample Z-Test: Example</h2>
Suppose the IQ in a population is normally distributed with a mean of μ = 100 and standard deviation of σ = 15.
A scientist wants to know if a new medication affects IQ levels, so she recruits 20 patients to use it for one month and records their IQ levels at the end of the month:
 To test this, she will perform a one sample z-test at significance level α = 0.05 using the following steps:
<b>Step 1: Gather the sample data.</b>
Suppose she collects a simple random sample with the following information:
<b>n</b> (sample size) = 20
 <b>x</b> (sample mean IQ) = 103.05
<b>Step 2: Define the hypotheses.</b>
She will perform the one sample z-test with the following hypotheses:
<b>H<sub>0</sub>: </b>μ = 100
<b>H<sub>A</sub>: </b>μ ≠ 100
<b>Step 3: Calculate the z test statistic.</b>
The z test statistic is calculated as:
z = (x – μ) / (σ√n)
z = (103.05 – 100) / (15/√20)
z = 0.90933
<b>Step 4: Calculate the p-value of the z test statistic.</b>
According to the  Z Score to P Value Calculator , the two-tailed p-value associated with z = 0.90933 is <b>0.36318</b>.
<b>Step 5: Draw a conclusion.</b>
Since the p-value (0.36318) is not less than the significance level (.05), the scientist will fail to reject the null hypothesis.
There is not sufficient evidence to say that the new medication significantly affects IQ level.
<b>Note: </b>You can also perform this entire one sample z-test by using the  One Sample Z-Test Calculator .
<h2>Additional Resources</h2>
The following tutorials explain how to perform a one sample z-test using different statistical software:
 How to Perform Z-Tests in Excel 
 How to Perform Z-Tests in R 
 How to Perform Z-Tests in Python 
<h2><span class="orange">How to Create One-Sided Confidence Intervals (With Examples)</span></h2>
A <b>confidence interval for a mean </b>is a range of values that is likely to contain a population mean with a certain level of confidence.
It is calculated as:
<b>Confidence Interval = x +/- t<sub>α/2, n-1</sub>*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t<sub>α/2, n-1</sub>: </b>t-value that corresponds to α/2 with n-1 degrees of freedom
<b>s: </b>sample standard deviation
<b>n: </b>sample size
The formula above describes how to create a typical <b>two-sided confidence interval</b>.
However, in some scenarios we’re only interested in creating <b>one-sided confidence intervals</b>.
We can use the following formulas to do so:
<b>Lower One-Sided Confidence Interval</b> = [-∞, x + t<sub>α, n-1</sub>*(s/√n) ]
<b>Upper One-Sided Confidence Interval</b> = [ x – t<sub>α, n-1</sub>*(s/√n), ∞ ]
The following examples show how to create lower one-sided and upper one-sided confidence intervals in practice.
<h3>Example 1: Create a Lower One-Sided Confidence Interval</h3>
Suppose we’d like to create a lower one-sided 95% confidence interval for a population mean in which we collect the following information for a sample:
<b>x: </b>20.5
<b>s: </b>3.2
<b>n: </b>18
According to the  Inverse t Distribution Calculator , the t-value that we should use for a one-sided 95% confidence interval with n-1 = 19 degrees of freedom is 1.7291.
We can then plug each of these values into the formula for a lower one-sided confidence interval:
<b>Lower One-Sided Confidence Interval</b> = [-∞, x + t<sub>α, n-1</sub>*(s/√n) ]
<b>Lower One-Sided Confidence Interval</b> = [-∞, 20.5 + 1.7291*(3.2/√18) ]
<b>Lower One-Sided Confidence Interval</b> = [-∞, 21.804 ]
We would interpret this interval as follows: We are 95% confident that the true population mean is equal to or less than <b>21.804</b>.
<h3>Example 2: Create an Upper One-Sided Confidence Interval</h3>
Suppose we’d like to create an upper one-sided 95% confidence interval for a population mean in which we collect the following information for a sample:
<b>x: </b>40
<b>s: </b>6.7
<b>n: </b>25
According to the  Inverse t Distribution Calculator , the t-value that we should use for a one-sided 95% confidence interval with n-1 = 24 degrees of freedom is 1.7109.
We can then plug each of these values into the formula for an upper one-sided confidence interval:
<b>Upper One-Sided Confidence Interval</b> = [ x – t<sub>α, n-1</sub>*(s/√n), ∞ ]
<b>Lower One-Sided Confidence Interval</b> = [ 40 – 1.7109*(6.7/√25), ∞ ]
<b>Lower One-Sided Confidence Interval</b> = [ 37.707, ∞ ]
We would interpret this interval as follows: We are 95% confident that the true population mean is greater than or equal to <b>37.707</b>.
<h2><span class="orange">One-Tailed Hypothesis Tests: 3 Example Problems</span></h2>
In statistics, we use  hypothesis tests  to determine whether some claim about a  population parameter  is true or not.
Whenever we perform a hypothesis test, we always write a null hypothesis and an alternative hypothesis, which take the following forms:
<b>H<sub>0</sub></b> (Null Hypothesis): Population parameter = ≤, ≥ some value
<b>H<sub>A</sub></b> (Alternative Hypothesis): Population parameter &lt;, >, ≠ some value
There are two types of hypothesis tests:
<b>Two-tailed test</b>: Alternative hypothesis contains the <b>≠</b> sign
<b>One-tailed test</b>: Alternative hypothesis contains either <b>&lt;</b> or <b>></b> sign
In a <b>one-tailed test</b>, the alternative hypothesis contains the less than (“&lt;“) or greater than (“>”) sign. This indicates that we’re testing whether or not there is a positive or negative effect.
Check out the following example problems to gain a better understanding of one-tailed tests.
<h2>Example 1: Factory Widgets</h2>
Suppose it’s assumed that the average weight of a certain widget produced at a factory is 20 grams. However, one engineer believes that a new method produces widgets that weigh less than 20 grams.
To test this, he can perform a one-tailed hypothesis test with the following null and alternative hypotheses:
<b>H<sub>0</sub></b> (Null Hypothesis): μ ≥ 20 grams
<b>H<sub>A</sub></b> (Alternative Hypothesis): μ &lt; 20 grams
<b>Note</b>: We can tell this is a one-tailed test because the alternative hypothesis contains the less than (<b>&lt;</b>) sign. Specifically, we would call this a left-tailed test because we’re testing if some population parameter is less than a specific value.
To test this, he uses the new method to produce 20 widgets and obtains the following information:
n = <b>20</b> widgets
x = <b>19.8</b> grams
s = <b>3.1</b> grams
Plugging these values into the  One Sample t-test Calculator , we obtain the following results:
t-test statistic:<b> -0.288525</b>
one-tailed p-value: <b>0.388</b>
Since the p-value is not less than .05, the engineer fails to reject the null hypothesis.
He does not have sufficient evidence to say that the true mean weight of widgets produced by the new method is less than 20 grams.
<h2>Example 2: Plant Growth</h2>
Suppose a standard fertilizer has been shown to cause a species of plants to grow by an average of 10 inches. However, one botanist believes a new fertilizer can cause this species of plants to grow by an average of greater than 10 inches.
To test this, she can perform a one-tailed hypothesis test with the following null and alternative hypotheses:
<b>H<sub>0</sub></b> (Null Hypothesis): μ ≤ 10 inches
<b>H<sub>A</sub></b> (Alternative Hypothesis): μ > 10 inches
<b>Note</b>: We can tell this is a one-tailed test because the alternative hypothesis contains the greater than (<b>></b>) sign. Specifically, we would call this a right-tailed test because we’re testing if some population parameter is greater than a specific value.
To test this claim, she applies the new fertilizer to a simple random sample of 15 plants and obtains the following information:
n = <b>15</b> plants
x = <b>11.4</b> inches
s = <b>2.5</b> inches
Plugging these values into the  One Sample t-test Calculator , we obtain the following results:
t-test statistic:<b> 2.1689</b>
one-tailed p-value: <b>0.0239</b>
Since the p-value is less than .05, the botanist rejects the null hypothesis.
She has sufficient evidence to conclude that the new fertilizer causes an average increase of greater than 10 inches.
<h2>Example 3: Studying Method</h2>
A professor currently teaches students to use a studying method that results in an average exam score of 82. However, he believes a new studying method can produce exam scores with an average value greater than 82.
To test this, he can perform a one-tailed hypothesis test with the following null and alternative hypotheses:
<b>H<sub>0</sub></b> (Null Hypothesis): μ ≤ 82
<b>H<sub>A</sub></b> (Alternative Hypothesis): μ > 82
<b>Note</b>: We can tell this is a one-tailed test because the alternative hypothesis contains the greater than (<b>></b>) sign. Specifically, we would call this a right-tailed test because we’re testing if some population parameter is greater than a specific value.
To test this claim, the professor has 25 students use the new studying method and then take the exam. He collects the following data on the exam scores for this sample of students:
n = <b>25</b>
x = <b>85</b>
s = <b>4.1</b>
Plugging these values into the  One Sample t-test Calculator , we obtain the following results:
t-test statistic:<b> 3.6586</b>
one-tailed p-value: <b>0.0006</b>
Since the p-value is less than .05, the professor rejects the null hypothesis.
He has sufficient evidence to conclude that the new studying method produces exam scores with an average score greater than 82.
<h2>Additional Resources</h2>
The following tutorials provide additional information about hypothesis testing:
 Introduction to Hypothesis Testing 
 What is a Directional Hypothesis? 
 When Do You Reject the Null Hypothesis? 
<h2><span class="orange">How to Perform a One-Way ANOVA by Hand</span></h2>
A  one-way ANOVA  (“analysis of variance”) compares the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means.
This tutorial explains how to perform a one-way ANOVA by hand.
<h3>Example: One-Way ANOVA by Hand</h3>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a certain exam. To test this, we recruit 30 students to participate in a study and split them into three groups.
The students in each group are randomly assigned to use one of the three exam prep programs for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same exam. 
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png"(max-width: 237px) 100vw, 237px">
Use the following steps to perform a one-way ANOVA by hand to determine if the mean exam score is different between the three groups:
<b>Step 1: Calculate the group means and the overall mean.</b>
First, we will calculate the mean for all three groups along with the overall mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVAhand1-1.png">
<b>Step 2: Calculate SSR.</b>
Next, we will calculate the regression sum of squares (SSR) using the following formula:
nΣ(X<sub>j</sub> – X..)<sup>2</sup> 
where:
<b>n</b>: the sample size of group j
<b>Σ</b>: a greek symbol that means “sum”
<b>X<sub>j</sub></b>: the mean of group j
<b>X..</b>: the overall mean
In our example, we calculate that SSR = 10(83.4-85.8)<sup>2</sup> + 10(89.3-85.8)<sup>2</sup> + 10(84.7-85.8)<sup>2</sup> = <b>192.2</b>
<b>Step 3: Calculate SSE.</b>
Next, we will calculate the error sum of squares (SSE) using the following formula:
Σ(X<sub>ij</sub> – X<sub>j</sub>)<sup>2</sup> 
where:
<b>Σ</b>: a greek symbol that means “sum”
<b>X<sub>ij</sub></b>: the i<sup>th</sup> observation in group j
<b>X<sub>j</sub></b>: the mean of group j
In our example, we calculate SSE as follows:
<b>Group 1: </b>(85-83.4)<sup>2</sup> + (86-83.4)<sup>2 </sup>+<b> </b>(88-83.4)<sup>2 </sup>+<b> </b>(75-83.4)<sup>2 </sup>+<b> </b>(78-83.4)<sup>2 </sup>+<b> </b>(94-83.4)<sup>2 </sup>+<b> </b>(98-83.4)<sup>2 </sup>+ <b> </b>(79-83.4)<sup>2 </sup>+<b> </b>(71-83.4)<sup>2 </sup>+<b> </b>(80-83.4)<sup>2 </sup>= <b>640.4</b>
<b>Group 2: </b>(91-89.3)<sup>2</sup> + (92-89.3)<sup>2 </sup>+<b> </b>(93-89.3)<sup>2 </sup>+<b> </b>(85-89.3)<sup>2 </sup>+<b> </b>(87-89.3)<sup>2 </sup>+<b> </b>(84-89.3)<sup>2 </sup>+<b> </b>(82-89.3)<sup>2 </sup>+ <b> </b>(88-89.3)<sup>2 </sup>+<b> </b>(95-89.3)<sup>2 </sup>+<b> </b>(96-89.3)<sup>2 </sup>= <b>208.1</b>
<b>Group 3: </b>(79-84.7)<sup>2</sup> + (78-84.7)<sup>2 </sup>+<b> </b>(88-84.7)<sup>2 </sup>+<b> </b>(94-84.7)<sup>2 </sup>+<b> </b>(92-84.7)<sup>2 </sup>+<b> </b>(85-84.7)<sup>2 </sup>+<b> </b>(83-84.7)<sup>2 </sup>+ <b> </b>(85-84.7)<sup>2 </sup>+<b> </b>(82-84.7)<sup>2 </sup>+<b> </b>(81-84.7)<sup>2 </sup>= <b>252.1</b>
<b>SSE: </b>640.4 + 208.1 + 252.1 = <b>1100.6</b>
<b>Step 4: Calculate SST.</b>
Next, we will calculate the total sum of squares (SST) using the following formula:
SST = SSR + SSE
In our example, SST = 192.2 + 1100.6 = <b>1292.8</b>
<b>Step 5: Fill in the ANOVA table.</b>
Now that we have SSR, SSE, and SST, we can fill in the ANOVA table:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F</b></th>
</tr>
<tr>
<td><b>Treatment</b></td>
<td style="text-align: center;">192.2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">2.358</td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">1100.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">40.8</td>
<td> </td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">1292.8</td>
<td style="text-align: center;">29</td>
<td> </td>
<td> </td>
</tr>
</tbody></table>
Here is how we calculated the various numbers in the table:
<b>df treatment: </b>k-1 = 3-1 = 2
<b>df error: </b>n-k = 30-3 = 27
<b>df total: </b>n-1 = 30-1 = 29
<b>MS treatment: </b>SST / df treatment = 192.2 / 2 = 96.1
<b>MS error: </b>SSE / df error = 1100.6 / 27 = 40.8
<b>F: </b>MS treatment / MS error = 96.1 / 40.8 = 2.358
<em><b>Note: </b>n = total observations, k = number of groups</em>
<b>Step 6: Interpret the results.</b>
The F test statistic for this one-way ANOVA is <b>2.358</b>. To determine if this is a statistically significant result, we must compare this to the F critical value found in the  F distribution table  with the following values:
α (significance level) = 0.05
DF1 (numerator degrees of freedom) = df treatment = 2
DF2 (denominator degrees of freedom) = df error = 27
We find that the F critical value is <b>3.3541</b>.
Since the F test statistic in the ANOVA table is less than the F critical value in the F distribution table, we fail to reject the null hypothesis. This means we don’t have sufficient evidence to say that there is a statistically significant difference between the mean exam scores of the three groups.
<b>Bonus Resource:</b> Use this  One-Way ANOVA Calculator  to automatically perform a one-way ANOVA for up to five samples.
<h2><span class="orange">One-Way ANOVA Calculator</span></h2>
This  one-way ANOVA  calculator compares the means of three or more independent samples.
Simply enter the values for up to five samples into the cells below, then press the “Calculate” button.
<b>Sample 1</b>
<textarea id="a" rows="3" cols="40">11, 12, 12, 14, 16, 19, 19, 20, 21, 21</textarea>
<b>Sample 2</b>
<textarea id="b" rows="3" cols="40">16, 16, 16, 17, 18, 19, 22, 24, 25, 32</textarea>
<b>Sample 3</b>
<textarea id="c" rows="3" cols="40">18, 18, 19, 21, 21, 22, 22, 23, 24, 26</textarea>
<b>Sample 4</b>
<textarea id="d" rows="3" cols="40"></textarea>
<b>Sample 5</b>
<textarea id="e" rows="3" cols="40"></textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<table><tbody>
<tr>
<th style="background-color: #bee3ff"><b>Source</b></th>
            <th style="background-color: #bee3ff"><b>SS</b></th>
            <th style="background-color: #bee3ff"><b>df</b></th>
            <th style="background-color: #bee3ff"><b>MS</b></th>
            <th style="background-color: #bee3ff"><b>F</b></th>
            <th style="background-color: #bee3ff"><b>P</b></th>
        </tr>
<tr>
<td>Treatment</td>
            <td>136.1</td>
            <td>2</td>
            <td>68.0</td>
            <td>4.069</td>
            <td>0.02853</td>
        </tr>
<tr>
<td>Error</td>
            <td>451.4</td>
            <td>27</td>
            <td>16.7</td>
            <td></td>
            <td></td>
        </tr>
<tr>
<td>Total</td>
            <td>587.5</td>
            <td>29</td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
</tbody></table>
<script>
var div_table = document.getElementById('words_table');
function calc() {
//define addition function
function add(a, b) {
    return a + b;
}
//get raw data for each group
var group_a  = document.getElementById('a').value.split(',').map(Number);
var group_b  = document.getElementById('b').value.split(',').map(Number);
var group_c  = document.getElementById('c').value.split(',').map(Number);
var group_d  = document.getElementById('d').value.split(',').map(Number);
var group_e  = document.getElementById('e').value.split(',').map(Number);
//verify they exist
if (group_a.length < 2){ group_a = null};
if (group_b.length < 2){ group_b = null};
if (group_c.length < 2){ group_c = null};
if (group_d.length < 2){ group_d = null};
if (group_e.length < 2){ group_e = null};
//get summary stats of each group
if (group_a != null) { var mean_group_a = math.mean(group_a); };
if (group_b != null) { var mean_group_b = math.mean(group_b); };
if (group_c != null) { var mean_group_c = math.mean(group_c); };
if (group_d != null) { var mean_group_d = math.mean(group_d); };
if (group_e != null) { var mean_group_e = math.mean(group_e); };
var all_groups_holder = group_a.concat(group_b, group_c, group_d, group_e);
var all_groups = all_groups_holder.filter(function (el) {
  return el != null;
});
var mean_all_groups = math.mean(all_groups);
var count_all_groups = all_groups.length;
if (group_a != null) { var mean_group_a_diff = mean_group_a - mean_all_groups; } else { var mean_group_a_diff = 0;};
if (group_b != null) { var mean_group_b_diff = mean_group_b - mean_all_groups; } else { var mean_group_b_diff = 0;};
if (group_c != null) { var mean_group_c_diff = mean_group_c - mean_all_groups; } else { var mean_group_c_diff = 0;};
if (group_d != null) { var mean_group_d_diff = mean_group_d - mean_all_groups; } else { var mean_group_d_diff = 0;};
if (group_e != null) { var mean_group_e_diff = mean_group_e - mean_all_groups; } else { var mean_group_e_diff = 0;};
if (group_a != null) { var count_group_a = group_a.length; } else { var count_group_a = 0;};
if (group_b != null) { var count_group_b = group_b.length; } else { var count_group_b = 0;};
if (group_c != null) { var count_group_c = group_c.length; } else { var count_group_c = 0;};
if (group_d != null) { var count_group_d = group_d.length; } else { var count_group_d = 0;};
if (group_e != null) { var count_group_e = group_e.length; } else { var count_group_e = 0;};
if (group_a != null) { var flag_group_a = 1; } else { var flag_group_a = 0;};
if (group_b != null) { var flag_group_b = 1; } else { var flag_group_b = 0;};
if (group_c != null) { var flag_group_c = 1; } else { var flag_group_c = 0;};
if (group_d != null) { var flag_group_d = 1; } else { var flag_group_d = 0;};
if (group_e != null) { var flag_group_e = 1; } else { var flag_group_e = 0;};
var total_treatments = [flag_group_a, flag_group_b, flag_group_c, flag_group_d, flag_group_e].reduce(add, 0);
//find sum of squares total, treatment, and residual
var ss_total = (all_groups.map(function(x) { return Math.pow(x-mean_all_groups, 2); })).reduce(add, 0);
var ss_treatment = Math.pow(mean_group_a_diff, 2)*(count_group_a) + Math.pow(mean_group_b_diff, 2)*(count_group_b) + Math.pow(mean_group_c_diff, 2)*(count_group_c) + Math.pow(mean_group_d_diff, 2)*(count_group_d) + Math.pow(mean_group_e_diff, 2)*(count_group_e);
var ss_residual = ss_total - ss_treatment;
//find degrees of freedom
df_total = count_all_groups - 1;
df_treatment = total_treatments - 1;
df_residual = df_total - df_treatment;
//find mean sum of squares
var ms_treatment = ss_treatment / df_treatment;
var ms_residual = ss_residual / df_residual;
//find f value and p value
        var f = ms_treatment / ms_residual;
        var p = 1 - jStat.centralF.cdf(f, df_treatment, df_residual);
        
//--------------OUTPUT RESULTS-----------//
document.getElementById('ss_treatment').innerHTML = ss_treatment.toFixed(1);
document.getElementById('ss_residual').innerHTML = ss_residual.toFixed(1);
document.getElementById('ss_total').innerHTML = ss_total.toFixed(1);
document.getElementById('df_treatment').innerHTML = df_treatment.toFixed(0);
document.getElementById('df_residual').innerHTML = df_residual.toFixed(0);
document.getElementById('df_total').innerHTML = df_total.toFixed(0);
document.getElementById('ms_treatment').innerHTML = ms_treatment.toFixed(1);
document.getElementById('ms_residual').innerHTML = ms_residual.toFixed(1);
document.getElementById('f').innerHTML = f.toFixed(3);
document.getElementById('p').innerHTML = p.toFixed(5);
}
</script>
<h2><span class="orange">How to Perform a One-Way ANOVA in Excel</span></h2>
A  one-way ANOVA  (“analysis of variance”) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial explains how to perform a one-way ANOVA in Excel.
<h3>Example: One-Way ANOVA in Excel</h3>
Suppose a researcher recruits 30 students to participate in a study. The students are randomly assigned to use one of three studying techniques for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same test. 
The test scores for the students are shown below:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova1.jpg"> 
The researcher wants to perform a one-way ANOVA to determine if the average scores are the same across all three groups.
To perform a one-way ANOVA in Excel, navigate to the <b>Data</b> tab, then click on the <b>Data Analysis</b> option within the <b>Analysis</b> group.
<em>If you don’t see the<b> Data Analysis</b> option, then you first need to load the free  Analysis ToolPak .</em>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/toolpak3.jpg"> 
Once you click this, a window will pop up with different Analysis Tools options. Select <b>Anova: Single Factor</b>, then click <b>OK</b>.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova2.jpg"> 
A new window pops up asking for an <b>Input Range</b>. You can either drag a box around your data or manually enter the data range. In this case, our data is in cells <b>C4:E13</b>.
Next, choose an <b>Alpha</b> level for the test. By default, this number is 0.05. In this case, I’ll leave it as 0.05.
Lastly, choose a cell for the <b>Output Range</b>, which is where the results of the one-way ANOVA will appear. In this case, I choose cell <b>G4</b>.
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova3.jpg"> 
Once you click <b>OK</b>, the output of the one-way ANOVA will appear:
<h2>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova4.jpg"> 
<b>Interpreting the Output</b>
</h2>
There are two tables shown in the output. The first is a summary table, which shows the count of test scores in each group, the sum of the test scores, the average of the test scores, and the variance of the test scores. 
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova5.jpg"> 
Recall that a one-way ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more groups.
From this first table, we can see that the mean score for each of the three groups is different, but to know if these differences are statistically significant, we need to look at the second table.
The second table shows the F test statistic, the F critical value, and the p-value:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/anova6.jpg"> 
In this case the <b>F test statistic</b> is<b> 2.3575</b> and the <b>F critical value</b> is<b> 3.3541</b>. Since the F test statistic is less than the F critical value, we do not have sufficient evidence to reject the null hypothesis that the means for the three groups are equal.
This means we do not have sufficient evidence to say that there is a difference in test scores among the three studying techniques. 
We could also use the  p-value  to reach the same conclusion. In this case the <b>p-value</b> is <b>0.1138</b>, which is greater than the alpha level of <b>0.05</b>.
This means we do not have sufficient evidence to reject the null hypothesis that the means for the three groups are equal. 
<b>Note: </b>In cases where you do reject the null hypothesis, you can perform a  Tukey-Kramer post hoc test  to determine exactly which group means are different.
<h2><span class="orange">One-Way ANOVA in Google Sheets (Step-by-Step)</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial provides a step-by-step example of how to perform a one-way ANOVA in Google Sheets.
<h3>Step 1: Install the XLMiner Analysis ToolPak</h3>
To perform a one-way ANOVA in Google Sheets, we need to first install the free XLMiner Analysis Toolpak.
To do so, click <b>Add-ons > Get add-ons</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets0.png">
Next, type <b>XLMiner Analysis ToolPak</b> in the search bar and click the icon that appears:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets1.png">
Lastly, click the <b>Install</b> button.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets2.png">
<h3>Step 2: Enter the Data</h3>
Next, we need to enter the data to use for the one-way ANOVA.
For this example, suppose a researcher recruits 30 students to participate in a study. The students are randomly assigned to use one of three studying methods to prepare for an exam. The exam results for each student are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets3.png">
<h3>Step 3: Perform the One-Way ANOVA</h3>
To perform a one-way ANOVA on this dataset, click <b>Add-ons > XLMiner Analysis ToolPak > Start</b>. The Analysis ToolPak will appear on the right side of the screen.
Click <b>Anova: Single Factor</b> and fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets4.png">
<h3>Step 4: Interpret the Results</h3>
Once you click <b>OK</b>, the results of the one-way ANOVA will appear starting in the cell you specified in <b>Output Range</b>. In our case, we chose to display the results starting in cell E1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets5.png">
There are two tables shown in the output.
The first table shows the count, sum, average, and variance of the test scores for each of the three groups.
The second table displays the results of the one-way ANOVA, including:
<b>F-statistic:</b> 2.3575
<b>F Critical</b> <b>value</b>: 3.3541
<b>P-value:</b> 0.1138
Recall that a one-way ANOVA has the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null hypothesis):</b> All group means are equal.
<b>H<sub>A </sub>(alternative hypothesis):</b> At least one group mean is different<sub> </sub>from the rest.
Since the p-value in the ANOVA table (.1138) is not less than .05, we do not have sufficient evidence to reject the null hypothesis.
Thus, we don’t have evidence to say that the three different studying methods lead to different exam scores.
<h2><span class="orange">How to Perform a One-Way ANOVA in SAS</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial provides a step-by-step example of how to perform a one-way ANOVA in SAS.
<h3>Step 1: Create the Data</h3>
Suppose a researcher recruits 30 students to participate in a study. The students are  randomly assigned  to use one of three studying methods to prepare for an exam.
The exam results for each student are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/anovaSAS1.jpg"219">
We can use the following code to create this dataset in SAS:
<b>/*create dataset*/
data my_data;
    input Method $ Score;
    datalines;
A 78
A 81
A 82
A 82
A 85
A 88
A 88
A 90
B 81
B 83
B 83
B 85
B 86
B 88
B 90
B 91
C 84
C 88
C 88
C 89
C 90
C 93
C 95
C 98
;
run;
</b>
<h3>Step 2: Perform the One-Way ANOVA</h3>
Next, we’ll use <b>proc ANOVA </b>to perform the one-way ANOVA:
<b>/*perform one-way ANOVA*/
proc ANOVA data=my_data;
class Method;
model Score = Method;
means Method / tukey cldiff;
run;</b>
<b>Note</b>: We used the <b>means</b> function to specify that a  Tukey post-hoc test  should be performed if the overall p-value of the one-way ANOVA is statistically significant.
<h3>Step 3: Interpret the Results</h3>
The first table we want to analyze in the results is the ANOVA table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/anovasas2.jpg">
From this table we can see:
The overall F Value: <b>5.26</b>
The corresponding p-value: <b>0.0140</b>
Recall that a one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> All group means are equal.
<b>H<sub>A</sub>:</b> At least one group mean is different<sub> </sub>from the rest.
Since the p-value from the ANOVA table (0.0140) is less than α = .05, we reject the null hypothesis.
This tells us that the mean exam score is not equal between the three studying methods.
<b>Related:</b>  How to Interpret the F-Value and P-Value in ANOVA 
SAS also provides  boxplots  to visualize the distribution of exam scores for each of the three studying methods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/anovasas3.jpg"531">
From the boxplots we can see that the exam scores tend to be higher among students who used studying method C compared to methods B and C.
To determine exactly which group means are different, we must refer to the final table in the output that shows the results of the Tukey post-hoc tests:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/anovasas4.jpg"442">
To tell which group means are different, we must look at which pairwise comparisons have stars (<b>***</b>) next to them.
From the table we can see that the mean values for groups A and C are statistically significantly different.
We can also see the 95% confidence interval for the difference in mean exam scores between group A and C:
95% Confidence Interval for Difference in Means: <b>[1.228, 11.522]</b>
<h3>Step 4: Report the Results</h3>
Lastly, we can  report the results  of the one-way ANOVA:
A one-way ANOVA was performed to compare the effect of three different studying methods on exam scores.
 
A one-way ANOVA revealed that there was a statistically significant difference in mean exam score between at least two groups (F(2, 21) = [5.26], p = 0.014).
 
Tukey’s HSD Test for multiple comparisons found that the mean value of exam score was significantly different between method C and method A (95% C.I. = [1.228,11.522]).
 
There was no statistically significant difference in mean exam scores between method A and method B or between method B and method C.
<h2><span class="orange">How to Perform a One-Way ANOVA in Python</span></h2>
A  one-way ANOVA  (“analysis of variance”) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial explains how to perform a one-way ANOVA in Python.
<h3>Example: One-Way ANOVA in Python</h3>
A researcher recruits 30 students to participate in a study. The students are  randomly assigned  to use one of three studying techniques for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same test. 
Use the following steps to perform a one-way ANOVA to determine if the average scores are the same across all three groups.
<b>Step 1: Enter the data.</b>
First, we’ll enter the exam scores for each group into three separate arrays:
<b>#enter exam scores for each group
group1 = [85, 86, 88, 75, 78, 94, 98, 79, 71, 80]
group2 = [91, 92, 93, 85, 87, 84, 82, 88, 95, 96]
group3 = [79, 78, 88, 94, 92, 85, 83, 85, 82, 81]
</b>
<b>Step 2: Perform the one-way ANOVA.</b>
Next, we’ll use the  f_oneway() function  from the SciPy library to perform the one-way ANOVA:
<b>from scipy.stats import f_oneway
#perform one-way ANOVA
f_oneway(group1, group2, group3)
(statistic=2.3575, pvalue=0.1138)
</b>
<b>Step 3: Interpret the results.</b>
A one-way ANOVA uses the following null and alternative  hypotheses :
<b>H<sub>0 </sub>(null hypothesis):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>= … = μ<sub>k </sub>(all the population means are equal)
<b>H<sub>1 </sub>(null hypothesis):</b> at least one population mean is different<sub> </sub>from the rest
The  F test statistic  is <b>2.3575</b> and the corresponding p-value is <b>0.1138</b>. Since the p-value is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that there is a difference in exam scores among the three studying techniques.
<h2><span class="orange">How to Conduct a One-Way ANOVA in R</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This type of test is called a <em>one-way</em> ANOVA because we are analyzing how <em>one</em> predictor variable impacts a response variable.
<b>Note</b>: If we were instead interested in how two predictor variables impact a response variable, we could conduct a  two-way ANOVA .
<h2>How to Conduct a One-Way ANOVA in R</h2>
The following example illustrates how to conduct a one-way ANOVA in R.
<h3>Background</h3>
Suppose we want to determine if three different exercise programs impact weight loss differently. The predictor variable we’re studying is <em>exercise program</em> and the  response variable  is <em>weight loss, </em>measured in pounds.
We can conduct a one-way ANOVA to determine if there is a statistically significant difference between the resulting weight loss from the three programs.
We recruit 90 people to participate in an experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month.
The following code creates the data frame we’ll be working with:
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = rep(c("A", "B", "C"), each = 30),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#view first six rows of data frame
head(data)
#  program weight_loss
#1       A   2.6900916
#2       A   0.7965260
#3       A   1.1163717
#4       A   1.7185601
#5       A   2.7246234
#6       A   0.6050458
</b>
The first column in the data frame shows the program that the person participated in for one month and the second column shows the total weight loss that person experienced by the end of the program, measured in pounds.
<h3>Exploring the Data</h3>
Before we even fit the one-way ANOVA model, we can gain a better understanding of the data by finding the mean and standard deviation of weight loss for each of the three programs using the <b>dplyr </b>package:
<b>#load <em>dplyr </em>package
library(dplyr)
#find mean and standard deviation of weight loss for each treatment group
data %>%
  group_by(program) %>%
  summarise(mean = mean(weight_loss),
            sd = sd(weight_loss))
#  A tibble: 3 x 3
#  program  mean    sd
#      
#1 A        1.58 0.905
#2 B        2.56 1.24 
#3 C        4.13 1.57  
</b>
We can also create a  boxplot  for each of the three programs to visualize the distribution of weight loss for each program:
<b>#create boxplots
boxplot(weight_loss ~ program,
data = data,
main = "Weight Loss Distribution by Program",
xlab = "Program",
ylab = "Weight Loss",
col = "steelblue",
border = "black")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/oneWayAnova1.jpg">
Just from these boxplots we can see that the the mean weight loss is highest for the participants in Program C and the mean weight loss is lowest for the participants in Program A.
We can also see that the standard deviation (the “length” of the boxplot) for weight loss is quite a bit higher in Program C compared to the other two programs.
Next, we’ll fit the one-way ANOVA model to our data to see if these visual differences are actually statistically significant. 
<h3>Fitting the One-Way ANOVA Model</h3>
The general syntax to fit a one-way ANOVA model in R is as follows:
<b>aov(response variable ~ predictor_variable, data = dataset)</b>
In our example, we can use the following code to fit the one-way ANOVA model, using <em>weight_loss </em>as the response variable and <em>program </em>as our predictor variable. We can then use the <b>summary() </b>function to view the output of our model:
<b>#fit the one-way ANOVA model
model &lt;- aov(weight_loss ~ program, data = data)
#view the model output
summary(model)
#            Df Sum Sq Mean Sq F value   Pr(>F)    
#program      2  98.93   49.46   30.83 7.55e-11 ***
#Residuals   87 139.57    1.60                     
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
From the model output we can see that the predictor variable <em>program </em>is statistically significant at the .05 significance level.
In other words, there is a statistically significant difference between the mean weight loss that results from the three programs.
<h3>Checking the Model Assumptions</h3>
Before we go any further, we should check to see that the  assumptions  of our model are met so that the our results from the model are reliable. In particular, a one-way ANOVA assumes:
<b>1. Independence</b> – the observations in each group need to be independent of each other. Since we used a randomized design (i.e. we assigned participants to the exercise programs randomly), this assumption should be met so we don’t need to worry too much about this.
<b>2. Normality</b> – the dependent variable should be approximately normally distributed for each level of the predictor variable. 
<b>3. Equal Variance</b> – the variances for each group are equal or approximately equal.
One way to check the assumptions of <b>normality </b>and <b>equal variance </b>is to use the function <b>plot()</b>, which produces four model-checking plots. In particular, we are most interested in the following two plots:
<b>Residuals vs Fitted</b> – this plot shows the relationship between the residuals and the fitted values. We can use this plot to roughly gauge whether or not the variance between the groups is approximately equal.
<b>Q-Q Plot</b> – this plot displays the standardized residuals against the theoretical quantiles. We can use this plot to roughly gauge whether or not the normality assumption is met.
The following code can be used to produce these model-checking plots:
<b>plot(model)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/oneWayAnova2.jpg">
The <em>Q-Q plot</em> above allows us to check the normality assumption. Ideally the standardized residuals would fall along the straight diagonal line in the plot. However, in the plot above we can see that the residuals stray from the line quite a bit towards the beginning and the end. This is an indication that our normality assumption may be violated.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/oneWayAnova3.jpg">
The <em>Residuals vs Fitted plot</em> above allows us to check our equal variances assumption. Ideally we’d like to see the residuals be equally spread out for each level of the fitted values.
We can see that the residuals are much more spread out for the higher fitted values, which is an indication that our  equal variances assumption  may be violated.
To formally test for equal variances, we could Levene’s Test using the <b>car </b>package:
<b>#load car package
library(car)
#conduct Levene's Test for equality of variances
leveneTest(weight_loss ~ program, data = data)
#Levene's Test for Homogeneity of Variance (center = median)
#      Df F value  Pr(>F)  
#group  2  4.1716 0.01862 *
#      87                  
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
The p-value of the test is <b>0.01862</b>. If we use a 0.05 significance level, we would reject the null hypothesis that the variances are equal across the three programs. However, if we use a 0.01 significance level, we would not reject the null hypothesis.
Although we could attempt to transform the data to make sure that our assumptions of normality and equality of variances are met, at the time being we won’t worry too much about this.
<h3>Analyzing Treatment Differences</h3>
Once we have verified that the model assumptions are met (or reasonably met), we can then conduct a  post hoc test  to determine exactly which treatment groups differ from one another.
For our post hoc test, we will use the function <b>TukeyHSD()</b> to conduct Tukey’s Test for multiple comparisons:
<b>#perform Tukey's Test for multiple comparisons
TukeyHSD(model, conf.level=.95) 
#  Tukey multiple comparisons of means
#    95% family-wise confidence level
#
#Fit: aov(formula = weight_loss ~ program, data = data)
#
#$program
#         diff       lwr      upr     p adj
#B-A 0.9777414 0.1979466 1.757536 0.0100545
#C-A 2.5454024 1.7656076 3.325197 0.0000000
#C-B 1.5676610 0.7878662 2.347456 0.0000199
</b>
The p-value indicates whether or not there is a statistically significant difference between each program. We can see from the output that there is a statistically significant difference between the mean weight loss of each program at the 0.05 significance level.
We can also visualize the 95% confidence intervals that result from the Tukey Test by using the <b>plot(TukeyHSD())</b> function in R:
<b>#create confidence interval for each comparison
plot(TukeyHSD(model, conf.level=.95), las = 2)
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/oneWayAnova4.jpg">
The results of the confidence intervals are consistent with the results of the hypothesis tests.
In particular, we can see that none of the confidence intervals for the mean weight loss between programs contain the value <em>zero</em>, which indicates that there is a statistically significant difference in mean loss between all three programs.
This is consistent with the fact that all of the  p-values  from our  hypothesis tests  are below 0.05.
<h3>Reporting the Results of the One-Way ANOVA</h3>
Lastly, we can report the results of the one-way ANOVA in such a way that summarizes the findings:
A one-way ANOVA was conducted to examine the effects of exercise program<em> </em>on weight loss <em>(measured in pounds).</em> There was a statistically significant difference between the effects of the three programs on weight loss (F(2, 87) = 30.83, p = 7.55e-11). Tukey’s HSD post hoc tests were carried out.
The mean weight loss for participants in program C is significantly higher than the mean weight loss for participants in program B (p &lt; 0.0001).
The mean weight loss for participants in program C is significantly higher than the mean weight loss for participants in program A (p &lt; 0.0001).
In addition, the mean weight loss for participants in program B is significantly higher than the mean weight loss for participants in program A (p  = 0.01).

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
