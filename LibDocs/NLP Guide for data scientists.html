<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #001000;  font-size: 20px;}
pre { color: gray; background-color: #001010}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
.topic{
    color: lime;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 28%;
	margin-right: 28%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head><body>

<center><h3>The Essential NLP Guide for data scientists</h3>
(with codes for top 10 common NLP tasks)
</center>
<div id="toc"><ul></ul></div>
<br>
<br>


<h2>Introduction</h2>
<p>Organizations today deal with huge amount and wide variety of data &#8211; calls from customers, their emails, tweets, data from mobile applications and what not. It takes a lot of effort and time to make this data useful. One of <b class="topic">the core skills in extracting information from text data is Natural Language Processing (NLP)</b>.</p>
<p>Natural Language Processing (NLP) is the art and science which helps us extract information from text and use it in our computations and algorithms. Given then increase in content on internet and social media, it is one of the must have still for all data scientists out there.</p>
<p>Whether you know NLP or not, this guide should help you as a ready reference for you. Through this guide, I have provided you with resources and codes to run the most common tasks in NLP.</p>
<p>

<h2>Why did I create this Guide?</h2>
<p>After having been working on NLP problems for some time now, I have encountered various situations where I needed to consult hundred of different of sources to study about the latest developments in the form of research papers, blogs and competitions for some of the common NLP tasks.</p>
<p>So, I decided to bring all these resources to one place and make it a One-Stop solution for the latest and the important resources for these common NLP tasks. Below is the list of tasks covered in this article along with their relevant resources. Let&#8217;s get started.</p>

<h2>Table of Contents</h2>
<ol>
<li>Stemming</li>
<li>Lemmatisation</li>
<li>Word Embeddings</li>
<li>Part-of-Speech Tagging</li>
<li>Named Entity Disambiguation</li>
<li>Named Entity Recognition</li>
<li>Sentiment Analysis</li>
<li>Semantic Text Similarity</li>
<li>Language Identification</li>
<li>Text Summarisation</li>
</ol>

<h2>1. Stemming</h2>
<p><strong>What is Stemming?: </strong>Stemming 来源 is the process of reducing the words(generally modified or derived) to their word stem or <b class="topic">root form</b>. The objective of stemming is to reduce related words to the same stem even if the stem is not a dictionary word. For example, in the English language-</p>
<ol>
<li><strong><em>beautiful</em></strong> and <strong><em>beautifully </em></strong>are stemmed to <strong><em>beauti</em> </strong></li>
<li><em><strong>good, better</strong></em> and <em><strong>best</strong></em> are stemmed to <em><strong>good, better</strong></em> and <em><strong>best</strong></em> respectively</li>
</ol>
<p><strong>Paper: </strong>The <a href="https://tartarus.org/martin/PorterStemmer/def.txt">original paper by Martin Porter</a> on Porter Algorithm for stemming.</p>
<p><strong>Algorithm: </strong><a href="https://bitbucket.org/mchaput/stemming/src/5c242aa592a6d4f0e9a0b2e1afdca4fd757b8e8a/stemming/porter2.py?at=default&amp;fileviewer=file-view-default" target="_blank" rel="noopener">Here is the Python implementation of Porter2 stemming algorithm.</a></p>
<p><strong>Implementation: </strong>Here is how you can stem a word using the Porter2 algorithm from the <em><strong>stemming </strong></em>library.</p>
<p><code>#!pip install stemming<br />
</code><code>from stemming.porter2 import stem<br />
</code><code>stem("casually")</code></p>

<h2>2. Lemmatisation</h2>
<p><strong>What is Lemmatisation?: </strong>Lemmatisation 词形还原 is the process of reducing a group of words into their lemma or dictionary form. It takes into account things like POS(Parts of Speech), the meaning of the word in the sentence, the meaning of the word in the nearby sentences etc. before reducing the word to its lemma. For example, in the English Language-</p>
<ol>
<li><em><strong>beautiful</strong></em> and <em><strong>beautifully</strong></em> are lemmatised to <em><strong>beautiful</strong></em> and <em><strong>beautifully</strong></em> respectively.</li>
<li><em><strong>good</strong></em>, <em><strong>better</strong></em> and <em><strong>best</strong></em> are lemmatised to <em><strong>good</strong></em>, <em><strong>good</strong></em> and <em><strong>good</strong></em> respectively.</li>
</ol>
<p><strong>Paper 1: </strong><a href="http://www.ijrat.org/downloads/icatest2015/ICATEST-2015127.pdf" target="_blank" rel="noopener">This paper</a> discusses different methods for performing lemmatisation in great detail. A must read if you want to know hoe traditional lemmatisers work.</p>
<p><strong>Paper 2</strong>: <a href="https://academic.oup.com/dsh/article-abstract/doi/10.1093/llc/fqw034/2669790/Lemmatization-for-variation-rich-languages-using">This is an excellent paper</a> which addresses the problem of lemmatisation for variation rich languages using Deep Learning.</p>
<p><strong>Dataset: </strong><a href="https://catalog.ldc.upenn.edu/ldc99t42">This is the link for Treebank-3 dataset</a> which you can use if you wish to create your own Lemmatiser.</p>
<p><strong>Implementation:</strong> Below is an implementation of an English Lemmatiser using spacy.</p>
<p><code>#!pip install spacy</code><br />
<code>#python -m spacy download en</code><br />
<code>import spacy</code><br />
<code>nlp=spacy.load("en")</code><br />
<code>doc="good better best"</code></p>
<p><code>for token in nlp(doc):</code><br />
<code>    print(token,token.lemma_)</code></p>

<h2>3. Word Embeddings</h2>
<p><strong>What is Word Embeddings?: </strong>Word Embeddings is the name of the techniques which are used to represent Natural Language in vector form of real numbers. They are useful because of computers&#8217; inability to process Natural Language. So these Word Embeddings capture the essence and relationship between words in a Natural Language using real numbers. In Word Embeddings, a word or a phrase is represented in a fixed dimension vector of length say 100.</p>
<p>So for example-</p>
<p>A word &#8220;man&#8221; might be represented in a 5-dimension vector as</p>
<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/25154235/word-vector-300x28.png">

<p>where each of these numbers is the magnitude of the word in a particular direction.</p>
<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/26091308/Word-Vectors.png" width="900">

<p><strong>Blog: </strong><a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">Here is an article</a> which explains Word Embeddings in great detail.</p>
<p><strong>Paper</strong>: <a href="https://arxiv.org/pdf/1411.2738.pdf">A very good paper</a> which explains Word Vectors in detail. A must-read for an in-depth understanding of Word Vectors.</p>
<p><strong>Tool</strong>: <a href="https://ronxin.github.io/wevi/">A browser based</a> tool for visualising Word Vectors.</p>
<p><strong>Pre-trained Word Vectors: </strong>Here is an exhaustive list of <a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md">pre-trained Word Vectors</a> in 294 languages by facebook.</p>
<p><strong>Implementation:</strong> Here is how you can obtain pre-trained Word Vector of a word using the gensim package.</p>
<p>Download the <a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit">Google News pre-trained Word Vectors from here.</a></p>
<p><code>#!pip install gensim</code><br />
<code>from gensim.models.keyedvectors</code><code> import KeyedVectors</code><br />
<code>word_vectors=KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)</code><br />
<code>word_vectors['human']</code></p>
<p><strong>Implementation: </strong>Here is how you can train your own word vectors using gensim</p>
<p><code>sentence=[['first','sentence'],['second','sentence']]</code><br />
<code class="python plain">model </code><code class="python keyword">=</code> <code class="python plain">gensim.models.Word2Vec(sentence, min_count</code><code class="python keyword">=</code><code class="python value">1,size=300,workers=4</code><code class="python plain">)</code></p>

<h2>4. Part-Of-Speech Tagging</h2>
<p><strong>What is Part-Of-Speech Tagging?: </strong>In Simplistic terms, Part-Of-Speech Tagging is the process of marking up of words in a sentence as <em>nouns</em>, <em>verbs</em>, <em>adjectives</em>, <em>adverbs etc</em>. For example, in the sentence-</p>
<p>&#8220;Ashok killed the snake with a stick&#8221;</p>
<p>The Parts-Of-Speech are identified as &#8211;</p>
<p>Ashok <em>PROPN</em><br />
killed <em>VERB</em><br />
the <em>DET</em><br />
snake <em>NOUN</em><br />
with <em>ADP</em><br />
a <em>DET</em><br />
stick <em>NOUN</em><br />
. <em>PUNCT</em></p>
<p><strong>Paper 1: </strong>This <a href="https://aclweb.org/anthology/N16-1031.pdf">paper by choi aptly titled <em>The Last Gist to the State-of-the-Art </em></a>presents a novel method called Dynamic Feature Induction which achieves state-of-the-art on POS Tagging task</p>
<p><strong>Paper 2: </strong><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/837/192">This paper</a> presents performing unsupervised POS Tagging using Anchor Hidden Markov Models.</p>
<p><strong>Implementation: </strong>Here is how we can perform POS Tagging using spacy.</p>
<p><code>#!pip install spacy</code><br />
<code>#!python -m spacy download en </code><br />
<code>nlp=spacy.load('en')</code><br />
<code>sentence="Ashok killed the snake with a stick"</code><br />
<code>for token</code><code> in nlp</code><code>(sentence):</code><br />
<code>   print(token,token.pos_)</code></p>

<h2>5. Named Entity Disambiguation</h2>
<p><strong>What is Named Entity Disambiguation?: </strong>Named Entity Disambiguation is the process of identifying the mentions of entities in a sentence. For example, in the sentence-</p>
<p>&#8220;Apple earned a revenue of 200 Billion USD in 2016&#8221;</p>
<p>It is the task of Named Entity Disambiguation to infer that Apple in the sentence is the company Apple and not a fruit.</p>
<p>Named Entity, in general, requires a knowledge base of entities which it can use to link entities in the sentence to the knowledge base.</p>
<p><strong>Paper 1:</strong> <a href="https://arxiv.org/pdf/1504.07678.pdf">This paper by Huang</a> makes use of Deep Semantic Relatedness models based on Deep Neural Networks along with Knowledgebase to achieve a state-of-the-art result on Named Entity Disambiguation.</p>
<p><strong>Paper 2:</strong> <a href="https://arxiv.org/pdf/1704.04920.pdf">This paper by Ganea and Hofmann</a> make use of Local Neural Attention along with Word Embeddings and no manually crafted features.</p>

<h2>6. Named Entity Recognition</h2>
<p><strong>What is Named Entity Recognition?:</strong> Named Entity Recognition is the task of identifying entities in a sentence and classifying them into categories like a person, organisation, date, location, time etc. For example, a NER would take in a sentence like &#8211;</p>
<p>&#8220;Ram of Apple Inc. travelled to Sydney on 5th October 2017&#8221;</p>
<p>and return something like</p>
<p>Ram<br />
of<br />
Apple <em>ORG</em><br />
Inc. <em>ORG</em><br />
travelled<br />
to<br />
Sydney <em>GPE</em><br />
on<br />
5th <em>DATE</em><br />
October <em>DATE</em><br />
2017 <em>DATE</em></p>
<p>Here, ORG stands for Organisation and GPE stands for location.</p>
<p>The problem with current NERs is that even state-of-the-art NER tend to perform poorly when they are used on a domain of data which is different from the data, the NER was trained on.</p>

<p>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/26091557/ner.png">

<p><strong>Paper:</strong> <a href="https://arxiv.org/pdf/1603.01360.pdf">This excellent paper</a> makes use of bi-directional LSTMs and combines Supervised and Unsupervised learning methods to achieve a state-of-the-art result in Named Entity Recognition in 4 languages.</p>
<p><strong>Implementation:</strong> Here is how you can perform Named Entity Recognition using spacy.</p>
<p><code>import spacy</code><br />
<code>nlp=spacy.load('en')sentence="Ram of Apple Inc. travelled to Sydney on 5th October 2017"</code><br />
<code>for token</code><code> in nlp</code><code>(sentence):</code><br />
<code>   print(token, token.ent_type_)</code></p>

<h2>7. Sentiment Analysis</h2>
<p><strong>What is Sentiment Analysis?:</strong> Sentiment Analysis is a broad range of subjective analysis which uses Natural Language processing techniques to perform tasks such as identifying the sentiment of a customer review, positive or negative feeling in a sentence, judging mood via voice analysis or written text analysis etc. For example-</p>
<p>&#8220;I did not like the chocolate ice-cream&#8221; &#8211; is a negative experience of ice-cream.</p>
<p>&#8220;I did not hate the chocolate ice-cream&#8221; &#8211; may be considered as a neutral experience</p>
<p>There is a wide range of methods which are used to perform sentiment analysis starting from taking a count of negative and positive words in a sentence to using LSTMs with Word Embeddings.</p>
<p><strong>Blog 1</strong>: <a href="https://www.analyticsvidhya.com/blog/2016/02/step-step-guide-building-sentiment-analysis-model-graphlab/">This article focuses on performing sentiment analysis on movie tweets</a></p>
<p><strong>Blog 2:</strong> <a href="https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/">This article focuses on performing sentiment analysis of tweets during the Chennai flood.</a></p>
<p><strong>Paper 1:</strong> <a href="https://arxiv.org/pdf/1305.6143.pdf">This paper</a> takes the Supervised Learning method approach with Naive Bayes method to classify IMDB reviews.</p>
<p><strong>Paper 2:</strong> <a href="http://www.cs.cmu.edu/~yohanj/research/papers/WSDM11.pdf">This paper</a> makes use of Unsupervised Learning method with LDA to identify aspects and sentiments of user-generated reviews. This paper is outstanding in the sense that it addresses the problem of shortage of annotated reviews.</p>
<p><strong>Repository:</strong> <a href="https://github.com/xiamx/awesome-sentiment-analysis">This is an awesome repository</a> of the research papers and implementation of sentiment analysis in various languages.</p>
<p><strong>Dataset 1:</strong> <a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/">Multi-Domain sentiment dataset version 2.0</a></p>
<p><strong>Dataset 2:</strong> <a href="http://www.sananalytics.com/lab/twitter-sentiment/">Twitter Sentiment analysis Dataset</a></p>
<p><strong>Competition:</strong> <a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews">A very good competition where you can check the performance of your models on the sentiment analysis task of movie reviews of rotten tomatoes.</a></p>

<h2>8. Semantic Text Similarity</h2>
<p><strong>What is Semantic Text Similarity?:</strong> Semantic Text Similarity is the process of analysing similarity between two pieces of text with respect to the meaning and essence of the text rather than analysing the syntax of the two pieces of text. Also, similarity is different than relatedness.</p>
<p>For example &#8211;</p>
<p>Car and Bus are similar but Car and fuel are related.</p>
<p><strong>Paper 1:</strong> <a href="https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf">This paper</a> presents the different approaches to measuring text similarity in detail. A must read paper to know about the existing approaches at a single place.</p>
<p><strong>Paper 2:</strong> <a href="http://casa.disi.unitn.it/~moschitt/since2013/2015_SIGIR_Severyn_LearningRankShort.pdf">This paper</a> introduces CNNs to rank a pair of two short texts</p>
<p><strong>Paper 3:</strong> <a href="https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf">This paper</a> makes use of Tree-LSTMs which achieve a state-of-the-art result on Semantic Relatedness of texts and Semantic Classification.</p>

<h2>9. Language Identification</h2>
<p><strong>What is Language Identification?: </strong>Language identification is the task of identifying the language in which the content is in.  It makes use of statistical as well as syntactical properties of the language to perform this task. It may also be considered as a special case of text classification.</p>
<p><strong>Blog: </strong><a href="https://fasttext.cc/blog/2017/10/02/blog-post.html">In this blog post by fastText</a>, they introduce a new tool which can identify 170 languages under 1MB of memory usage.</p>
<p><strong>Paper 1: </strong><a href="http://www.ep.liu.se/ecp/131/021/ecp17131021.pdf">This paper </a>discusses 7 methods of language identification of 285 languages.</p>
<p><strong>Paper 2:</strong> <a href="https://repositorio.uam.es/bitstream/handle/10486/666848/automatic_lopez-moreno_ICASSP_2014_ps.pdf?sequence=1">This paper</a> describes how Deep Neural Networks can be used to achieve state-of-the-art results on Automatic Language Identification.</p>

<h2>10.  Text Summarisation</h2>
<p><strong>What is Text Summarisation?: </strong>Text Summarisation is the process of shortening up of a text by identifying the important points of the text and creating a summary using these points. The goal of Text Summarisation is to retain maximum information along with maximum shortening of text without altering the meaning of the text.</p>
<p>Paper 1: <a href="https://arxiv.org/pdf/1509.00685.pdf">This paper</a> describes a Neural Attention Model based approach for Abstractive Sentence Summarization.</p>
<p>Paper 2: <a href="https://arxiv.org/pdf/1602.06023.pdf">This paper</a> describes how sequence-to-sequence RNNs can be used to achieve state-of-the-art results on Text Summarisation.</p>
<p>Repository: <a href="https://github.com/tensorflow/models/tree/master/research/textsum">This repository by Google Brain </a>team has the codes for using a sequence-to-sequence model customised for Text Summarisation. The model is trained on Gigaword dataset.</p>
<p>Application: <a href="https://www.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/">Reddit&#8217;s autotldr bot</a> uses Text Summarisation to summarise articles into the comments of a post. This feature turned out to be very famous amongst the Reddit users.</p>
<p>Implementation: Here is how you can quickly summarise your text using the gensim package.</p>
<p><code>from gensim.summarization import summarize<br />
</code><br />
<code>sentence="Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization."<br />
</code><br />
<code>summarize(sentence)</code></p>

<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
