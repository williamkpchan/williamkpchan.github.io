<base target="_blank"><html><head><title>statologyContents 19</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 19"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 19</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Sort a Matrix in R (With Examples)</span></h2>
You can use the following methods to sort a matrix by a particular column in R:
<b>Method 1: Sort Matrix by One Column Increasing</b>
<b>sorted_matrix &lt;- my_matrix[order(my_matrix[, 1]), ]
</b>
<b>Method 2: Sort Matrix by One Column Decreasing</b>
<b>sorted_matrix &lt;- my_matrix[order(my_matrix[, 1], decreasing=TRUE), ]
</b>
The following examples show how to use each method in practice with the following matrix:
<b>#create matrix
my_matrix &lt;- matrix(c(5, 4, 2, 2, 7, 9, 12, 10, 15, 4, 6, 3), ncol=2)
#view matrix
my_matrix
     [,1] [,2]
[1,]    5   12
[2,]    4   10
[3,]    2   15
[4,]    2    4
[5,]    7    6
[6,]    9    3</b>
<h2>Example 1: Sort Matrix by One Column Increasing</h2>
The following code shows how to sort the matrix by increasing values based on the first column:
<b>#sort matrix by first column increasing
sorted_matrix &lt;- my_matrix[order(my_matrix[, 1]), ]
#view sorted matrix
sorted_matrix
     [,1] [,2]
[1,]    2   15
[2,]    2    4
[3,]    4   10
[4,]    5   12
[5,]    7    6
[6,]    9    3
</b>
Notice that the matrix is now sorted by increasing values based on the first column.
We could just as easily sort by increasing values based on the second column by changing the <b>1</b> to a <b>2</b>:
<b>#sort matrix by second column increasing
sorted_matrix &lt;- my_matrix[order(my_matrix[, 2]), ]
#view sorted matrix
sorted_matrix
     [,1] [,2]
[1,]    9    3
[2,]    2    4
[3,]    7    6
[4,]    4   10
[5,]    5   12
[6,]    2   15
</b>
The matrix is now sorted by increasing values based on the second column.
<h2>Example 2: Sort Matrix by One Column Decreasing</h2>
The following code shows how to sort the matrix by decreasing values based on the first column:
<b>#sort matrix by first column decreasing
sorted_matrix &lt;- my_matrix[order(my_matrix[, 1], decreasing=TRUE), ]
#view sorted matrix
sorted_matrix
     [,1] [,2]
[1,]    2   15
[2,]    2    4
[3,]    4   10
[4,]    5   12
[5,]    7    6
[6,]    9    3
</b>
Notice that the matrix is now sorted by decreasing values based on the first column.
<b>Related</b>:  The Complete Guide to sort(), order(), and rank() in R 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common sorting operations in R:
 How to Sort Values Alphabetically in R 
 How to Sort a Data Frame by Date in R 
 How to Sort by Multiple Columns in R 
<h2><span class="orange">The Complete Guide: How to Use sort(), order(), and rank() in R</span></h2>
Three functions in R that people often get confused are <b>sort</b>, <b>order</b>, and <b>rank</b>.
Here’s the difference between these functions:
<b>sort()</b> will sort a vector in ascending order
<b>order()</b> will return the index of each element in a vector in sorted order
<b>rank()</b> will assign a rank to each element in a vector (smallest = 1)
The following example shows how to use each of these functions in practice.
<h3>Example: Use sort(), order(), & rank() with Vectors</h3>
The following code shows how to use <b>sort()</b>, <b>order()</b>, and<b> rank()</b> functions with a vector with four values:
<b>#create vector
x &lt;- c(0, 20, 10, 15)
#sort vector
sort(x)
[1]  0 10 15 20
#order vector
order(x)
[1] 1 3 4 2
#rank vector
rank(x)
[1] 1 4 2 3
</b>
Here’s what each function did:
<b>1.</b> The <b>sort()</b> function simply sorted the values in the vector in ascending order.
<b>2.</b> The <b>order()</b> function returned the index of each element in sorted order.
If you put the values from the original vector in order based on these index values, you’ll end up with a sorted vector.
For example, order() tells us to put the value in index position <b>1</b> first – this is 0 in the original vector.
Then order() tells us to put the value in index position <b>3</b> next – this is 10 in the original vector.
Then order() tells us to put the value in index position <b>4</b> next – this is 15 in the original vector.
Then order() tells us to put the value in index position <b>2</b> next – this is 20 in the original vector.
The end result is a sorted vector – 0, 10, 15, 20.
<b>3.</b> The <b>rank()</b> function assigned a rank to each element in the vector, using 1 for the smallest value.
For example, rank() tells us that the first value in the original vector is the smallest (rank = 1) and the second value in the original vector is the largest (rank = 4)
Note that we can use the following syntax to use <b>sort()</b>, <b>order()</b>, and r<b>ank()</b> in reverse order:
<b>#create vector
x &lt;- c(0, 20, 10, 15)
#sort vector in decreasing order
sort(x, decreasing=TRUE)
[1] 20 15 10  0
#order vector in decreasing order
order(x, decreasing=TRUE)
[1] 2 4 3 1
#rank vector in reverse order (largest value = 1)
rank(-x)
[1] 4 1 3 2
</b>
Notice that these results are the exact opposite of the ones produced in the previous examples.
<h3>Note: How to Handle Ties with rank() Function</h3>
We can use the <b>ties.method</b> argument to specify how we should handle ties when using the<b> rank()</b> function:
<b>rank(x, ties.method='average')
</b>
You can use one of the following options to specify how to handle ties:
<b>average</b>: (Default) Assigns each tied element to the average rank (elements ranked in the 3rd and 4th position would both receive a rank of 3.5)
<b>first</b>: Assigns the first tied element to the lowest rank (elements ranked in the 3rd and 4th positions would receive ranks 3 and 4 respectively)
<b>min</b>: Assigns every tied element to the lowest rank (elements ranked in the 3rd and 4th position would both receive a rank of 3)
<b>max</b>: Assigns every tied element to the highest rank (elements ranked in the 3rd and 4th position would both receive a rank of 4)
<b>random</b>: Assigns every tied element to a random rank (either element tied for the 3rd and 4th position could receive either rank)
Depending on your scenario, one of these methods might make more sense to use than the others.
<h2><span class="orange">How to Use the source Function in R (With Example)</span></h2>
You can use the <b>source</b> function in R to reuse functions that you create in another R script.
This function uses the following basic syntax:
<b>source("path/to/some/file.R")</b>
Simply add this line to the top of your R script and you’ll be able to use any functions defined in <b>file.R</b>.
The following example shows how to use the <b>source</b> function in practice.
<h3>Example: Using the source Function in R</h3>
Suppose we have the following R script called <b>some_functions.R</b> that contains two simple user-defined functions:
<b>#define function that divides values by 2
divide_by_two &lt;- function(x) {
  return(x/2)
}
#define function that multiplies values by 3
multiply_by_three &lt;- function(x) {
  return(x*3)
}
</b>
Now suppose we’re currently working with some R script called <b>main_script.R</b>.
Assuming<b> some_functions.R</b> and <b>main_script.R</b> are located within the same folder, we can use source at the top of our <b>main_script.R</b> to allow us to use the functions we defined in the <b>some_functions.R</b> script:
<b>source("some_functions.R")
#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F'), points=c(14, 19, 22, 15, 30, 40))
#view data frame
df
  team points
1    A     14
2    B     19
3    C     22
4    D     15
5    E     30
6    F     40
#create new columns using functions from some_functions.R
df$half_points &lt;- divide_by_two(df$points)
df$triple_points &lt;- multiply_by_three(df$points)
#view updated data frame
df
  team points half_points triple_points
1    A     14         7.0            42
2    B     19         9.5            57
3    C     22        11.0            66
4    D     15         7.5            45
5    E     30        15.0            90
6    F     40        20.0           120
</b>
Notice that we’re able to create two new columns in our data frame using functions that we defined in the <b>some_functions.R</b> script.
The <b>source</b> function allowed us to use the <b>divide_by_two</b> and <b>multiply_by_three</b> functions in our current script, even though these functions weren’t created in the current script.
<b>Note</b>: In this example, we only used one <b>source</b> function at the top of the file. However, we can use as many <b>source</b> functions as we’d like if we want to reuse functions defined in several different scripts.
<h2><span class="orange">The Spearman-Brown Formula: Definition & Example</span></h2>
The <b>Spearman-Brown formula</b> is used to predict the reliability of a test after changing the length of the test.
The formula is:
<b>Predicted reliability = kr / (1 + (k-1)r)</b>
where:
<b>k</b>: Factor by which the length of the test is changed. For example, if original test is 10 questions and new test is 15 questions, k = 15/10 = <b>1.5</b>.
<b>r</b>: Reliability of the original test. We typically use  Cronbach’s Alpha  for this, which is a value that ranges from 0 to 1 with higher values indicating higher reliability.
The following example shows how to use this formula in practice.
<h3>Example: How to Use the Spearman-Brown Formula</h3>
Suppose a company uses a 15-item test to assess employee satisfaction and the test is known to have a reliability of 0.74.
If the company increases the length of the test to 30 items, what is the predicted reliability of the new test?
We can use the Spearman-Brown formula to calculate the predicted reliability:
Predicted reliability = kr / (1 + (k-1)r)
Predicted reliability = 2*.74 / (1 + (2-1)*.74)
Predicted reliability = 0.85
The new test has a predicted reliability of <b>0.85</b>.
<b>Note</b>: We calculated k as 30/15 = 2.
<h3>Cautions on Using the Spearman-Brown Formula</h3>
Based on the Spearman-Brown formula, we can see that increasing the number of items on a test by <em>any</em> number will increase the predicted reliability of the test.
For example, suppose we increase the number of items on the test from the previous example from 15 to 16. Then we would calculate k as 16/15 = 1.067.
The predicted reliability would be:
Predicted reliability = kr / (1 + (k-1)r)
Predicted reliability = 1.067*.74 / (1 + (1.067-1)*.74)
Predicted reliability = 0.752
The new test has a predicted reliability of <b>0.752</b>, which is higher than the reliability of <b>0.74</b> on the original test.
Using this logic, we might think that increasing the length of the test by a massive amount of items is a good idea because we could push the reliability closer and closer to 1.
However, we should keep in mind the following:
<b>1. Using too many items can cause fatigue effects.</b>
If a test has too many questions then individuals may become fatigued as they answer more and more questions, causing them to produce less reliable answers as the test drags on.
<b>2. The new items added to the test should be of equal difficulty to the existing items.</b>
It’s important that if we do decide to increase the length of a test that we make sure the new items / questions we’re adding are of equal difficulty to the existing items otherwise the predicted reliability will not be accurate.
<h2><span class="orange">How to Calculate Spearman Rank Correlation in R</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with the following interpretations:
<b> -1: </b>a perfect negative relationship between two variables
<b>0: </b>no relationship between two variables
<b>1: </b>a perfect positive relationship between two variables
One special type of correlation is called <b>Spearman Rank Correlation</b>, which is used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class).
To calculate the Spearman rank correlation between two variables in R, we can use the following basic syntax:
<b>corr &lt;- cor.test(x, y, method = 'spearman')
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Spearman Rank Correlation Between Vectors</h3>
The following code shows how to calculate the Spearman rank correlation between two vectors in R:
<b>#define data
x &lt;- c(70, 78, 90, 87, 84, 86, 91, 74, 83, 85)
y &lt;- c(90, 94, 79, 86, 84, 83, 88, 92, 76, 75)
#calculate Spearman rank correlation between x and y
cor.test(x, y, method = 'spearman')
Spearman's rank correlation rho
data:  x and y
S = 234, p-value = 0.2324
alternative hypothesis: true rho is not equal to 0
sample estimates:
       rho 
-0.4181818 </b>
From the output we can see that the Spearman rank correlation is <b>-0.41818</b> and the corresponding p-value is <b>0.2324</b>.
This indicates that there is a negative correlation between the two vectors.
However, since the p-value of the correlation is not less than 0.05, the correlation is not statistically significant.
<h3>Example 2: Spearman Rank Correlation Between Columns in Data Frame</h3>
The following code shows how to calculate the Spearman rank correlation between two column in a data frame:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'), points=c(67, 70, 75, 78, 73, 89, 84, 99, 90, 91), assists=c(22, 27, 30, 23, 25, 31, 38, 35, 34, 32))
#calculate Spearman rank correlation between x and y
cor.test(df$points, df$assists, method = 'spearman')
Spearman's rank correlation rho
data:  df$points and df$assists
S = 36, p-value = 0.01165
alternative hypothesis: true rho is not equal to 0
sample estimates:
      rho 
0.7818182 
</b>
From the output we can see that the Spearman rank correlation is <b>0.7818</b> and the corresponding p-value is <b>0.01165</b>.
This indicates that there is a strong positive correlation between the two vectors.
Since the p-value of the correlation is less than 0.05, the correlation is statistically significant.
<h2><span class="orange">How to Calculate Spearman Rank Correlation in Python</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with the following interpretations:
<b> -1: </b>a perfect negative relationship between two variables
<b>0: </b>no relationship between two variables
<b>1: </b>a perfect positive relationship between two variables
One special type of correlation is called <b>Spearman Rank Correlation</b>, which is used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class).
This tutorial explains how to calculate the Spearman rank correlation between two variables in Python
<h3>Example: Spearman Rank Correlation in Python</h3>
Suppose we have the following pandas DataFrame that contains the math exam score and science exam score of 10 students in a particular class:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'student': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],   'math': [70, 78, 90, 87, 84, 86, 91, 74, 83, 85],   'science': [90, 94, 79, 86, 84, 83, 88, 92, 76, 75]})
</b>
To calculate the Spearman Rank correlation between the math and science scores, we can use the  spearmanr()  function from <b>scipy.stats</b>:
<b>from scipy.stats import spearmanr
#calculate Spearman Rank correlation and corresponding p-value
rho, p = spearmanr(df['math'], df['science'])
#print Spearman rank correlation and p-value
print(rho)
-0.41818181818181815
print(p)
0.22911284098281892
</b>
From the output we can see that the Spearman rank correlation is <b>-0.41818</b> and the corresponding p-value is <b>0.22911</b>.
This indicates that there is a negative correlation between the science and math exam scores.
However, since the p-value of the correlation is not less than 0.05, the correlation is not statistically significant.
Note that we could also use the following syntax to just extract the correlation coefficient or the p-value:
<b>#extract Spearman Rank correlation coefficient
spearmanr(df['math'], df['science'])[0]
-0.41818181818181815
#extract p-value of Spearman Rank correlation coefficient
spearmanr(df['math'], df['science'])[1] 
0.22911284098281892
</b>
<h2><span class="orange">How to Calculate Spearman Rank Correlation in Excel</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with the following interpretations:
<b> -1: </b>a perfect negative relationship between two variables
<b>0: </b>no relationship between two variables
<b>1: </b>a perfect positive relationship between two variables
One special type of correlation is called <b>Spearman Rank Correlation</b>, which is used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class).
This tutorial explains how to calculate the Spearman rank correlation between two variables in Excel.
<h3>Example: Spearman Rank Correlation in Excel</h3>
Perform the following steps to calculate the Spearman rank correlation between the math exam score and science exam score of 10 students in a particular class.
<b>Step 1: Enter the data.</b>
Enter the exam scores for each student in two separate columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel1.png">
<b>Step 2: Calculate the ranks for each exam score.</b>
Next, we will calculate the rank for each exam score. Use the following formulas in cells D2 and E2 to calculate the Math and Science ranks for the first student, Austin:
<b>Cell D2: </b>=RANK.AVG(B2, $B$2:$B$11, 0)
<b>Cell E2: </b>=RANK.AVG(C2, $C$2:$C$11, 0)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel2.png">
Next, highlight the remaining cells to be filled in:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel3.png">
Then click Ctrl+D to fill in the ranks for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel4.png">
<b>Step 3: Calculate the Spearman Rank Correlation Coefficient.</b>
Lastly, we will calculate the Spearman Rank Correlation Coefficient between Math scores and Science scores by using the <b>CORREL() </b>function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel5.png">
The Spearman rank correlation turns out to be <b>-0.41818</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel6.png">
<b>Step 4 (Optional): Determine if the Spearman rank correlation is statistically significant.</b>
In the previous step, we found the Spearman rank correlation between the Math and Science exam scores to be <b>-0.41818</b>, which indicates a negative correlation between the two variables.
However, to determine if this correlation is statistically significant, we would need to refer to a Spearman rank correlation table of critical values, which shows the critical values associated with various sample sizes (n) and significance levels (α).
 If the absolute value of our correlation coefficient is greater than the critical value in the table, then the correlation between the two variables is statistically significant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel7-1.png">
In our example, our sample size was n = 10 students. Using a significance level of 0.05, we find that the critical value is <b>0.564</b>.
Because the absolute value of the Spearman rank correlation coefficient that we calculated (<b>0.41818</b>) is not larger than this critical value, it means the correlation between Math and Science scores is not statistically significant.
<h2><span class="orange">How to Calculate Spearman Rank Correlation in Google Sheets</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with the following interpretations:
<b> -1: </b>a perfect negative relationship between two variables
<b>0: </b>no relationship between two variables
<b>1: </b>a perfect positive relationship between two variables
One special type of correlation is called <b>Spearman Rank Correlation</b>, which is used to measure the correlation between two ranked variables. (e.g. rank of a student’s math exam score vs. rank of their science exam score in a class).
This tutorial explains how to calculate the Spearman rank correlation between two variables in Google Sheets.
<h3>Example: Spearman Rank Correlation in Google Sheets</h3>
Perform the following steps to calculate the Spearman rank correlation between the math exam score and science exam score of 10 students in a particular class.
<b>Step 1: Enter the data.</b>
Enter the exam scores for each student in two separate columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/spearmanSheets1.png">
<b>Step 2: Calculate the ranks for each exam score.</b>
Next, we will calculate the rank for each exam score. Use the following formulas in cells D2 and E2 to calculate the Math and Science ranks for the first student:
<b>Cell D2: </b>=RANK.AVG(B2, $B$2:$B$11, 0)
<b>Cell E2: </b>=RANK.AVG(C2, $C$2:$C$11, 0)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/spearmanSheets2.png">
Next, highlight the remaining cells to be filled in:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/spearmanSheets3.png">
Then click Ctrl+D to fill in the ranks for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/spearmanSheets4.png">
<b>Step 3: Calculate the Spearman Rank Correlation Coefficient.</b>
Lastly, we will calculate the Spearman Rank Correlation Coefficient between Math scores and Science scores by using the <b>CORREL() </b>function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/spearmanSheets5.png">
The Spearman rank correlation turns out to be <b>-0.41818</b>.
<b>Step 4 (Optional): Determine if the Spearman rank correlation is statistically significant.</b>
In the previous step, we found the Spearman rank correlation between the Math and Science exam scores to be <b>-0.41818</b>, which indicates a negative correlation between the two variables.
However, to determine if this correlation is statistically significant, we would need to refer to a Spearman rank correlation table of critical values, which shows the critical values associated with various sample sizes (n) and significance levels (α).
 If the absolute value of our correlation coefficient is greater than the critical value in the table, then the correlation between the two variables is statistically significant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/spearmanExcel7-1.png">
In our example, our sample size was n = 10 students. Using a significance level of 0.05, we find that the critical value is <b>0.564</b>.
Because the absolute value of the Spearman rank correlation coefficient that we calculated (<b>0.41818</b>) is not larger than this critical value, it means the correlation between Math and Science scores is not statistically significant.
<b>Related: </b> How to Calculate Spearman Rank Correlation in Excel 
<h2><span class="orange">How to Perform Spline Regression in R (With Example)</span></h2>
<b>Spline regression</b> is a type of regression that is used when there are points or “knots” where the pattern in the data abruptly changes and  linear regression  and  polynomial regression  aren’t flexible enough to fit the data.
The following step-by-step example shows how to perform spline regression in R.
<h2>Step 1: Create the Data</h2>
First, let’s create a dataset in R with two variables and create a scatterplot to visualize the relationship between the variables:
<b>#create data frame
df &lt;- data.frame(x=1:20, y=c(2, 4, 7, 9, 13, 15, 19, 16, 13, 10,     11, 14, 15, 15, 16, 15, 17, 19, 18, 20))
#view head of data frame
head(df)
  x  y
1 1  2
2 2  4
3 3  7
4 4  9
5 5 13
6 6 15
#create scatterplot
plot(df$x, df$y, cex=1.5, pch=19)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/spline1.jpg"455">
Clearly the relationship between x and y is non-linear and there appear to be two points or “knots” where the pattern in the data abruptly changes at x = 7 and x = 10.
<h2>Step 2: Fit Simple Linear Regression Model</h2>
Next, let’s use the  lm() function  to fit a simple linear regression model to this dataset and plot the fitted regression line on the scatterplot:
<b>#fit simple linear regression model
linear_fit &lt;- lm(df$y ~ df$x)
#view model summary
summary(linear_fit)
Call:
lm(formula = df$y ~ df$x)
Residuals:
    Min      1Q  Median      3Q     Max 
-5.2143 -1.6327 -0.3534  0.6117  7.8789 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   6.5632     1.4643   4.482 0.000288 ***
df$x          0.6511     0.1222   5.327  4.6e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.152 on 18 degrees of freedom
Multiple R-squared:  0.6118,Adjusted R-squared:  0.5903 
F-statistic: 28.37 on 1 and 18 DF,  p-value: 4.603e-05
#create scatterplot
plot(df$x, df$y, cex=1.5, pch=19)
#add regression line to scatterplot
abline(linear_fit)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/spline2.jpg"461">
From the scatterplot we can see that the simple linear regression line doesn’t fit the data well.
From the model output we can also see that the  adjusted R-squared value  is <b>0.5903</b>.
We’ll compare this to the adjusted R-squared value of a spline model.
<h2>Step 3: Fit Spline Regression Model</h2>
Next, let’s use the <b>bs()</b> function from the <b>splines</b> package to fit a spline regression model with two knots and then plot the fitted model on the scatterplot:
<b>library(splines)
#fit spline regression model
spline_fit &lt;- lm(df$y ~ bs(df$x, knots=c(7, 10)))
#view summary of spline regression model
summary(spline_fit)
Call:
lm(formula = df$y ~ bs(df$x, knots = c(7, 10)))
Residuals:
     Min       1Q   Median       3Q      Max 
-2.84883 -0.94928  0.08675  0.78069  2.61073 
Coefficients:            Estimate Std. Error t value Pr(>|t|)    
(Intercept)                    2.073      1.451   1.429    0.175    
bs(df$x, knots = c(7, 10))1    2.173      3.247   0.669    0.514    
bs(df$x, knots = c(7, 10))2   19.737      2.205   8.949 3.63e-07 ***
bs(df$x, knots = c(7, 10))3    3.256      2.861   1.138    0.274    
bs(df$x, knots = c(7, 10))4   19.157      2.690   7.121 5.16e-06 ***
bs(df$x, knots = c(7, 10))5   16.771      1.999   8.391 7.83e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 1.568 on 14 degrees of freedom
Multiple R-squared:  0.9253,Adjusted R-squared:  0.8987 
F-statistic:  34.7 on 5 and 14 DF,  p-value: 2.081e-07
#calculate predictions using spline regression model
x_lim &lt;- range(df$x)
x_grid &lt;- seq(x_lim[1], x_lim[2])
preds &lt;- predict(spline_fit, newdata=list(x=x_grid))
#create scatter plot with spline regression predictions
plot(df$x, df$y, cex=1.5, pch=19)
lines(x_grid, preds)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/spline3.jpg"469">
From the scatterplot we can see that the spline regression model is able to fit the data quite well.
From the model output we can also see that the adjusted R-squared value is <b>0.8987</b>.
The adjusted R-squared value for this model is much higher than the simple linear regression model, which tells us that the spline regression model is able to fit the data much better.
Note that for this example we chose the knots to be located at x=7 and x=10.
In practice, you’ll have to pick the knot locations yourself based on where the patterns in the data appear to change and based on domain expertise.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform Multiple Linear Regression in R 
 How to Perform Exponential Regression in R 
 How to Perform Weighted Least Squares Regression in R 
<h2><span class="orange">How to Split Column Into Multiple Columns in R (With Examples)</span></h2>
You can use one of the following two methods to split one column into multiple columns in R:
<b>Method 1: Use str_split_fixed()</b>
<b>library(stringr)
df[c('col1', 'col2')] &lt;- str_split_fixed(df$original_column, 'sep', 2)
</b>
<b>Method 2: Use separate()</b>
<b>library(dplyr)
library(tidyr)
df %>% separate(original_column, c('col1', 'col2'))
</b>
The following examples show how to use each method in practice.
<h3>Method 1: Use str_split_fixed()</h3>
Suppose we have the following data frame:
<b>#create data frame
df &lt;- data.frame(player=c('John_Wall', 'Dirk_Nowitzki', 'Steve_Nash'), points=c(22, 29, 18), assists=c(8, 4, 15))
#view data frame
df
         player points assists
1     John_Wall     22       8
2 Dirk_Nowitzki     29       4
3    Steve_Nash     18      15</b>
We can use the <b>str_split_fixed()</b> function from the <b>stringr</b> package to separate the ‘player’ column into two new columns called ‘First’ and ‘Last’ as follows:
<b>library(stringr)
#split 'player' column using '_' as the separator
df[c('First', 'Last')] &lt;- str_split_fixed(df$player, '_', 2)
#view updated data frame
df
         player points assists First     Last
1     John_Wall     22       8  John     Wall
2 Dirk_Nowitzki     29       4  Dirk Nowitzki
3    Steve_Nash     18      15 Steve     Nash</b>
Notice that two new columns are added at the end of the data frame.
Feel free to rearrange the columns and drop the original ‘player’ columns if you’d like:
<b>#rearrange columns and leave out original 'player' column
df_final &lt;- df[c('First', 'Last', 'points', 'assists')]
#view updated data frame
df_final
  First     Last points assists
1  John     Wall     22       8
2  Dirk Nowitzki     29       4
3 Steve     Nash     18      15</b>
<h3>Method 2: Use separate()</h3>
The following code shows how to use the <b>separate()</b> function from the <b>tidyr</b> package to separate the ‘player’ column into ‘first’ and ‘last’ columns:
<b>library(dplyr)
library(tidyr)
#create data frame
df &lt;- data.frame(player=c('John_Wall', 'Dirk_Nowitzki', 'Steve_Nash'), points=c(22, 29, 18), assists=c(8, 4, 15))
#separate 'player' column into 'First' and 'Last'
df %>% separate(player, c('First', 'Last'))
  First     Last points assists
1  John     Wall     22       8
2  Dirk Nowitzki     29       4
3 Steve     Nash     18      15
</b>
Note that the <b>separate()</b> function will separate strings based on any non-alphanumeric value.
For example, if the first and last names were separated by a comma, the <b>separate()</b> function would automatically split based on the location of the comma:
<b>library(dplyr)
library(tidyr)
#create data frame
df &lt;- data.frame(player=c('John,Wall', 'Dirk,Nowitzki', 'Steve,Nash'), points=c(22, 29, 18), assists=c(8, 4, 15))
#separate 'player' column into 'First' and 'Last'
df %>% separate(player, c('First', 'Last'))
  First     Last points assists
1  John     Wall     22       8
2  Dirk Nowitzki     29       4
3 Steve     Nash     18      15</b>
You can find the complete online documentation for the <b>separate()</b> function  here .
<h2><span class="orange">How to Use split() Function in R to Split Data</span></h2>
The <b>split()</b> function in R can be used to split data into groups based on factor levels.
This function uses the following basic syntax:
<b>split(x, f, …)</b>
where:
<b>x</b>: Name of the vector or data frame to divide into groups
<b>f</b>: A factor that defines the groupings
The following examples show how to use this function to split vectors and data frames into groups.
<h3>Example 1: Use split() to Split Vector Into Groups</h3>
The following code shows how to split a vector of data values into groups based on a vector of factor levels:
<b>#create vector of data values
data &lt;- c(1, 2, 3, 4, 5, 6)
#create vector of groupings
groups &lt;- c('A', 'B', 'B', 'B', 'C', 'C')
#split vector of data values into groups
split(x = data, f = groups)
$A
[1] 1
$B
[1] 2 3 4
$C
[1] 5 6</b>
The result is three groups.
Note that you can use indexing to retrieve specific groups as well:
<b>#split vector of data values into groups and only display second group
split(x = data, f = groups)[2]
$B
[1] 2 3 4</b>
<h3>Example 2: Use split() to Split Data Frame Into Groups</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'B'), position=c('G', 'G', 'F', 'G', 'F', 'F'), points=c(33, 28, 31, 39, 34, 44), assists=c(30, 28, 24, 24, 28, 19))
#view data frame
df
  team position points assists
1    A        G     33      30
2    A        G     28      28
3    A        F     31      24
4    B        G     39      24
5    B        F     34      28
6    B        F     44      19
</b>
We can use the following code to split the data frame into groups based on the ‘team’ variable:
<b>#split data frame into groups based on 'team'
split(df, f = df$team)
$A
  team position points assists
1    A        G     33      30
2    A        G     28      28
3    A        F     31      24
$B
  team position points assists
4    B        G     39      24
5    B        F     34      28
6    B        F     44      19
</b>
The result is two groups. The first contains only rows where ‘team’ is equal to A and the second contains only rows where ‘team’ is equal to B.
Note that we can also split the data into groups using multiple factor variables. For example, the following code shows how to split the data into groups based on the ‘team’ and ‘position’ variables:
<b>#split data frame into groups based on 'team' and 'position' variables
split(df, f = list(df$team, df$position))
$A.F
  team position points assists
3    A        F     31      24
$B.F
  team position points assists
5    B        F     34      28
6    B        F     44      19
$A.G
  team position points assists
1    A        G     33      30
2    A        G     28      28
$B.G
  team position points assists
4    B        G     39      24
</b>
The result is four groups.
<h2><span class="orange">Split-Half Reliability: Definition + Examples</span></h2>
<b> Internal consistency  </b>refers to how well a survey, questionnaire, or test actually measures what you want it to measure. The higher the internal consistency, the more confident you can be that your survey or test is reliable.
One popular way to measure internal consistency is to use <b>split-half reliability</b>, which is a technique that involves the following steps:
<b>1. </b>Split a test into two halves. For example, one half may be composed of even-numbered questions while the other half is composed of odd-numbered questions.
<b>2. </b>Administer each half to the same individual.
<b>3. </b>Repeat for a large group of individuals.
<b>4. </b>Find the correlation between the scores for both halves.
The higher the correlation between the two halves, the higher the internal consistency of the test or survey. Ideally you would like the correlation between the halves to be high because this indicates that all parts of the test are contributing equally to what is being measured.
<h2>When to Use Split-Half Reliability</h2>
The split-half reliability method is an easy method to carry out if you want to measure internal consistency, but it should only be used if the following two conditions are present:
<b>1.</b> The test has a large number of questions. Split-half reliability works best for tests that have a large number of questions (e.g. 100 questions) because the number we calculate for the correlation will be more reliable.
<b>2. </b>All of the questions on the test or survey measure the same construct or knowledge area. If a particular test measures several different constructs like leadership skills, communications skills, programming skills, and other professional skills, then a split-half reliability would not be appropriate to use since many of the responses are not expected to be correlated anyway.
<h2>Example of Split-Half Reliability</h2>
Suppose researchers would like to measure the internal consistency of a particular test that has 100 questions all related to introverted personality traits. They carry out the following steps to measure the split-half reliability of the test.
<b>Step 1. </b>Split the test in half based on odd-numbered and even-numbered questions.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/splitHalf1-1.png">
<b>Step 2. </b>Administer each half of the test to the same individual.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/splitHalf2.png">
<b>Step 3. </b>Repeat for 50 individuals.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/splitHalf3.png">
<b>Step 4. </b>Find the correlation between the scores for both halves.
If the researchers find that the  correlation  is fairly high, they can be assured that all parts of the test are contributing equally to measuring the introverted personality traits they’re interested in.
Conversely, if the correlation is low then that could be an indication that the lowly correlated questions need to be either re-written or removed altogether to improved the internal consistency and the overall reliability of the test.
<h2><span class="orange">What is a Split-Plot Design? (Explanation & Example)</span></h2>
A <b>split-plot design </b>is an experimental design in which researchers are interested in studying two factors in which:
One of the factors is “easy” to change or vary.
One of the factors is “hard” to change or vary.
This type of design was developed in 1925 by mathematician Ronald Fisher for use in agricultural experiments.
To illustrate the idea of the split-plot design, consider an example in which researchers want to study the effects of two irrigation methods (Factor A) and two fertilizers (Factor B) on crop yield.
In this particular example, it’s not possible to apply different irrigation methods to areas smaller than one field, but it <em>is </em>possible to apply different fertilizers to small areas.
Thus, if we have four fields then we can randomly assign one of the irrigation methods (we’ll call them A<sub>1</sub> and A<sub>2</sub>) to each field:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/splitPlot1.png">
Then we can split each field in half and randomly assign one fertilizer (we’ll call them B<sub>1</sub> and B<sub>2</sub>) to each half:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/splitPlot2.png">
In this example, we have 4 “whole” plots and within each whole plot we have 2 “split” plots.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/splitPlot3.png">
<h3>Advantages of Split-Plot Designs</h3>
Split-plot designs have two advantages over completely randomized designs:
<b>1. Cost</b>
Since one of the factors in a split-plot design doesn’t have to be changed for each split-plot, this means this type of design tends to be cheaper to carry out in practice.
<b>2. Efficiency</b>
A split-plot design leads to an increase in precision in the estimates for all factor effects except for the whole-plot main effects.
<h3>Examples of Split-Plot Designs in Real Life</h3>
Split-plot designs are often used in manufacturing because there is often some variable that is produced in large quantities and thus it makes sense to carry out a split-plot design to reduce the cost of running an experiment.
Here are a few examples of split-plot designs in real life scenarios:
<b>Example 1: Baking</b>
A packaged-food manufacturer may be interested in identifying the optimal cake mix formulation. Since cake mixes are made in massive batches, it’s not feasible to change the combination of ingredients.
Thus, the ingredients act as the “whole” plot factors and other factors like temperature and baking time are used as the “split” plot factors.
<b>Example 2: Automobiles</b>
An automobile manufacturer may be interested in finding the optimal engine/fuel combination. Since engines take a long time to make, they may decide to create three new engines and test out three different fuels on each engine.
In this scenario, the engine type is the hard-to-change factor “whole” plot factor and the fuels are the easy-to-change “split” plot factor.
<b>Example 3: Woodworking</b>
A wood manufacturer wants to find the optimal mix of wood type and temperature to produce the most durable wood. Since the type of wood can take a long time to acquire, they may apply three different temperatures to two different wood types.
In this scenario, the wood type is the hard-to-change factor “whole” plot factor and the temperature is the easy-to-change “split” plot factor.
<h2><span class="orange">How to Use Spread Function in R (With Examples)</span></h2>
The <b>spread()</b> function from the  tidyr  package can be used to “spread” a key-value pair across multiple columns.
This function uses the following basic syntax:
<b>spread(data, key value)</b>
where:
<b>data</b>: Name of the data frame
<b>key</b>: Column whose values will become variable names
<b>value</b>: Column where values will fill under new variables created from key
The following examples show how to use this function in practice.
<h3>Example 1: Spread Values Across Two Columns</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(player=rep(c('A', 'B'), each=4), year=rep(c(1, 1, 2, 2), times=2), stat=rep(c('points', 'assists'), times=4), amount=c(14, 6, 18, 7, 22, 9, 38, 4))
#view data frame
df
  player year    stat amount
1      A    1  points     14
2      A    1 assists      6
3      A    2  points     18
4      A    2 assists      7
5      B    1  points     22
6      B    1 assists      9
7      B    2  points     38
8      B    2 assists      4</b>
We can use the <b>spread()</b> function to turn the values in the <b>stat</b> column into their own columns:
<b>library(tidyr)
#spread stat column across multiple columns
spread(df, key=stat, value=amount)
  player year assists points
1      A    1       6     14
2      A    2       7     18
3      B    1       9     22
4      B    2       4     38</b>
<h3>Example 2: Spread Values Across More Than Two Columns</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df2 &lt;- data.frame(player=rep(c('A'), times=8), year=rep(c(1, 2), each=4), stat=rep(c('points', 'assists', 'steals', 'blocks'), times=2), amount=c(14, 6, 2, 1, 29, 9, 3, 4))
#view data frame
df2
  player year    stat amount
1      A    1  points     14
2      A    1 assists      6
3      A    1  steals      2
4      A    1  blocks      1
5      A    2  points     29
6      A    2 assists      9
7      A    2  steals      3
8      A    2  blocks      4</b>
We can use the <b>spread()</b> function to turn the four unique values in the <b>stat</b> column into four new columns:
<b>library(tidyr)
#spread stat column across multiple columns
spread(df2, key=stat, value=amount)
  player year assists blocks points steals
1      A    1       6      1     14      2
2      A    2       9      4     29      3</b>
Every column is a variable.
Every row is an observation.
Every cell is a single value.
The tidyr package uses four core functions to create tidy data:
<b>1.</b> The <b>spread()</b> function.
<b>2.</b> The  <b>gather()</b>  function.
<b>3.</b> The  <b>separate()</b>  function.
4. The  <b>unite()</b>  function.
If you can master these four functions, you will be able to create “tidy” data from any data frame.
<h2><span class="orange">How to Use sprintf Function in R to Print Formatted Strings</span></h2>
You can use the <b>sprintf()</b> function in R to print formatted strings.
This function uses the following basic syntax:
<b>sprintf(fmt, x)</b>
where:
<b>fmt</b>: The format to use
<b>x</b>: The value to format
The following examples show how to use this function in practice.
<h3>Example 1: Format Digits After Decimal Point</h3>
The following code shows how to use <b>sprintf()</b> to only display two digits after a decimal point:
<b>#define value
x &lt;- 15.49347782
#only display 2 digits after decimal place
sprintf("%.2f", x)
[1] "15.49"
</b>
<h3>Example 2: Format Digits Before Decimal Point</h3>
The following code shows how to use <b>sprintf()</b> to display ten digits before the decimal point:
<b>#define value
x &lt;- 15435.4
#display 10 total digits before decimal place
sprintf("%10.f", x)
[1] "     15435"</b>
Since there were only five digits before the decimal point to start with, the <b>sprintf()</b> function added another five blank spaces at the beginning of the string to make a total of 10 digits before the decimal point.
<h3>Example 3: Format Value Using Scientific Notation</h3>
The following code shows how to use <b>sprintf()</b> to display a value in scientific notation:
<b>#define value
x &lt;- 15435.4
#display in scientific notation using lowercase e
sprintf("%e", x)
[1] "1.543540e+04"
#display in scientific notation using uppercase E
sprintf("%E", x)
[1] "1.543540E+04" 
</b>
<h3>Example 4: Format One Value in String</h3>
The following code shows how to use <b>sprintf()</b> to format a value in a string:
<b>#define value
x &lt;- 5.4431
#display string with formatted value
sprintf("I rode my bike about %.1f miles", x)
[1] "I rode my bike about 5.4 miles"
</b>
<h3>Example 5: Format Multiple Values in String</h3>
The following code shows how to use <b>sprintf()</b> to format multiple values in a string:
<b>#define values
x1 &lt;- 5.4431
x2 &lt;- 10.778342
#display string with formatted values
sprintf("I rode my bike %.1f miles and then ran %.2f miles", x1, x2)
[1] "I rode my bike 5.4 miles and then ran 10.78 miles"
</b>
<h2><span class="orange">5 Examples of Spurious Correlation in Real Life</span></h2>
In statistics, <b>spurious correlation</b> refers to a correlation between two variables that occurs purely by chance without one variable actually causing the other to occur.
This type of correlation is dangerous because it can sometimes make people think that one variable causes another, when in reality the correlation exists purely by chance.
It turns out that this type of correlation between variables happens all the time in real life.
The following examples share five different real-life examples of spurious correlation.
<h3>Example 1: Master’s Degrees vs. Box Office Revenue</h3>
If we collect data for the total number of Master’s degrees issued by universities each year and the total box office revenue generated by year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause2.png">
This doesn’t mean that issuing more Master’s degrees is causing the box office revenue to increase each year.
The more likely explanation is that the global population has been increasing each year, which means more Master’s degrees are issued each year and the sheer number of people attending movies each year are both increasing in roughly equal amounts. 
The correlation between the two variables is spurious.
<h3>Example 2: Measles Cases vs. Marriage Rate</h3>
If we collect data for the total number of measles cases in the U.S. each year and the marriage rate each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/corrCause4.png">
This doesn’t mean that reduced measles cases is somehow causing lower marriage rates. The two variables are independent.
Modern medicine is simply causing measles cases to drop and fewer people are getting married due to various reasons each year.
The correlation between the two variables is spurious.
<h3>Example 3: High School Graduates vs. Donut Consumption</h3>
If we collect data for the total number of high school graduates and total donut consumption in the U.S. each year, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/spurious3.jpg"580">
This doesn’t mean that an increased number of high school graduates is leading to more donut consumption in the United States.
The more likely explanation is that U.S. population has been increasing over time, which means that the number of people receiving a high school degree and the total donuts being consumed are both increasing as population increases.
This is a spurious correlation.
<h3>Example 4: Video Game Sales vs. Nuclear Energy Production</h3>
If we collect data for the total video game sales each year around the world and the total energy produced by nuclear power plants, we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/spurious4.jpg"574">
This doesn’t mean that somehow increased video game sales are leading to increased nuclear energy production.
Instead, more nuclear energy power plants are being built and more video games are being sold as the global population increases each year.
Although both variables increase steadily over time, one is not causing the other. The correlation between the two is spurious.
<h3>Example 5: Revenue from Arcades vs. Coal Mining Jobs</h3>
If we collect data for the total revenue generated from arcades in the U.S. and total number of coal mining jobs in the U.S., we would find that the two variables are highly correlated.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/spurious5.jpg"572">
This doesn’t mean that one variable is causing the other to decrease.
Instead, both arcades and coal mining have become less common over the years which explains why both variables have decreased at roughly the same rate.
The correlation between the two variables is spurious.
<h2><span class="orange">How to Calculate Square Root & Cube Root in Google Sheets</span></h2>
You can use the following formula to calculate the <b>square root</b> of a value in Google Sheets:
<b>=SQRT(A1)
</b>
And you can use the following formula to calculate the <b>cube root</b> of a value in Google Sheets:
<b>=A1^(1/3)</b>
The following examples show how to use each formula in practice.
<h3>Example 1: Calculate Square Root in Google Sheets</h3>
The following screenshot shows how to calculate the square root of values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/root1.jpg">
Note that we typed the following formula into cell B2:
<b>=SQRT(A2)</b>
We then copy and paste this formula down to every cell in column B.
<h3>Example 2: Calculate Cube Root in Google Sheets</h3>
The following screenshot shows how to calculate the cube root of values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/root2.jpg">
Note that we typed the following formula into cell B2:
<b>=A2^(1/3)</b>
We then copy and paste this formula down to every cell in column B.
<h3>Example 3: Calculate Nth Root in Google Sheets</h3>
We can use the following generic formula to calculate the nth root of a value in Google Sheets:
<b>=A2^(1/n)</b>
For example, we could use the following formula to calculate the fourth root of a value:
<b>=A2^(1/4)</b>
The following screenshot shows how to calculate the fourth root of values in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/root3.jpg"524">
Note that we typed the following formula into cell B2:
<b>=A2^(1/4)</b>
We then copy and paste this formula down to every cell in column B.
<h2><span class="orange">How to Use the Square Root Function in R (With Examples)</span></h2>
You can use the <b>sqrt()</b> function to find the square root of a numeric value in R:
<b>sqrt(x)
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Calculate Square Root of a Single Value</h3>
The following code shows how to calculate the square root of a single value in R:
<b>#define x
x &lt;- 25
#find square root of x
sqrt(x)
[1] 5
</b>
<h3>Example 2: Calculate Square Root of Values in Vector</h3>
The following code shows how to calculate the square root of every value in a vector in R:
<b>#define vector
x &lt;- c(1, 3, 4, 6, 9, 14, 16, 25)
#find square root of every value in vector
sqrt(x)
[1] 1.000000 1.732051 2.000000 2.449490 3.000000 3.741657 4.000000 5.000000
</b>
Note that if there are negative values in the vector, there will be a warning message. To avoid this warning message, you can first convert each value in the vector to an absolute value:
<b>#define vector with some negative values
x &lt;- c(1, 3, 4, 6, -9, 14, -16, 25)
#attempt to find square root of each value in vector
sqrt(x)
[1] 1.000000 1.732051 2.000000 2.449490      NaN 3.741657      NaN 5.000000
Warning message:
In sqrt(x) : NaNs produced
#convert each value to absolute value and then find square root of each value
sqrt(abs(x))
[1] 1.000000 1.732051 2.000000 2.449490 3.000000 3.741657 4.000000 5.000000
</b>
<h3>Example 3: Calculate Square Root of Column in Data Frame</h3>
The following code shows how to calculate the square root of a single column in a data frame:
<b>#create data frame
data &lt;- data.frame(a=c(1, 3, 4, 6, 8, 9),   b=c(7, 8, 8, 7, 13, 16),   c=c(11, 13, 13, 18, 19, 22),   d=c(12, 16, 18, 22, 29, 38))
#find square root of values in column a
sqrt(data$a)
[1] 1.000000 1.732051 2.000000 2.449490 2.828427 3.000000
</b>
<h3>Example 4: Calculate Square Root of Several Columns in Data Frame</h3>
The following code shows how to use the <b>apply()</b> function to calculate the square root of several columns in a data frame:
<b>#create data frame
data &lt;- data.frame(a=c(1, 3, 4, 6, 8, 9),   b=c(7, 8, 8, 7, 13, 16),   c=c(11, 13, 13, 18, 19, 22),   d=c(12, 16, 18, 22, 29, 38))
#find square root of values in columns a, b, and d
apply(data[ , c('a', 'b', 'd')], 2, sqrt)
            a        b        d
[1,] 1.000000 2.645751 3.464102
[2,] 1.732051 2.828427 4.000000
[3,] 2.000000 2.828427 4.242641
[4,] 2.449490 2.645751 4.690416
[5,] 2.828427 3.605551 5.385165
[6,] 3.000000 4.000000 6.164414</b>
<h2><span class="orange">How to Calculate SST, SSR, and SSE in Excel</span></h2>
We often use three different  sum of squares  values to measure how well a  regression line  actually fits a dataset:
<b>1. Sum of Squares Total (SST) – </b>The sum of squared differences between individual data points (y<sub>i</sub>) and the mean of the response variable (y).
SST = Σ(y<sub>i</sub> – y)<sup>2</sup>
<b>2. Sum of Squares Regression (SSR)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and the mean of the response variable(y).
SSR = Σ(<U+0177><sub>i</sub> – y)<sup>2</sup>
<b>3. Sum of Squares Error (SSE)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and observed data points (y<sub>i</sub>).
SSE = Σ(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup>
The following step-by-step example shows how to calculate each of these metrics for a given regression model in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that contains the number of hours studied and exam score received for 20 different students at a certain school:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquareExcel1.png">
<h3>Step 2: Fit a Regression Model</h3>
Along the top ribbon in Excel, click the <b>Data</b> tab and click on <b>Data Analysis</b>. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
Once you click on <b>Data Analysis,</b> a new window will pop up. Select <b>Regression </b>and click OK.
In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquareExcel2.png">
Once you click <b>OK</b>, the regression output will appear.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquareExcel3.png">
<h3>Step 3: Analyze the Output</h3>
The three sum of squares metrics – SST, SSR, and SSE – can be seen in the <b>SS</b> column of the <b>ANOVA</b> table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquareExcel4.png">
The metrics turn out to be:
<b>Sum of Squares Total (SST):</b> 1248.55
<b>Sum of Squares Regression (SSR):</b> 917.4751
<b>Sum of Squares Error (SSE):</b> 331.0749
We can verify that SST = SSR + SSE:
SST = SSR + SSE
1248.55 = 917.4751 + 331.0749
We can also manually calculate the  R-squared  of the regression model:
R-squared = SSR / SST
R-squared = 917.4751 / 1248.55
R-squared = 0.7348
This tells us that <b>73.48%</b> of the variation in exam scores can be explained by the number of hours studied.
<h2><span class="orange">How to Calculate SST, SSR, and SSE in Python</span></h2>
We often use three different  sum of squares  values to measure how well a  regression line  fits a dataset:
<b>1. Sum of Squares Total (SST) – </b>The sum of squared differences between individual data points (y<sub>i</sub>) and the mean of the response variable (y).
SST = Σ(y<sub>i</sub> – y)<sup>2</sup>
<b>2. Sum of Squares Regression (SSR)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and the mean of the response variable(y).
SSR = Σ(<U+0177><sub>i</sub> – y)<sup>2</sup>
<b>3. Sum of Squares Error (SSE)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and observed data points (y<sub>i</sub>).
SSE = Σ(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup>
The following step-by-step example shows how to calculate each of these metrics for a given regression model in Python.
<h2>Step 1: Create the Data</h2>
First, let’s create a dataset that contains the number of hours studied and exam score received for 20 different students at a certain university:
<b>import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'hours': [1, 1, 1, 2, 2, 2, 2, 2, 3, 3,             3, 4, 4, 4, 5, 5, 6, 7, 7, 8],   'score': [68, 76, 74, 80, 76, 78, 81, 84, 86, 83,             88, 85, 89, 94, 93, 94, 96, 89, 92, 97]})
#view first five rows of DataFrame
df.head()
hoursscore
0168
1176
2174
3280
4276
</b>
<h2>Step 2: Fit a Regression Model</h2>
Next, we’ll use the <b>OLS()</b> function from the  statsmodels  library to fit a simple linear regression model using score as the response variable and hours as the predictor variable:
<b>import statsmodels.api as sm
#define response variable
y = df['score']
#define predictor variable
x = df[['hours']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
</b>
<h2>Step 3: Calculate SST, SSR, and SSE</h2>
Lastly, we can use the following formulas to calculate the SST, SSR, and SSE values of the model:
<b>import numpy as np
#calculate sse
sse = np.sum((model.fittedvalues - df.score)**2)
print(sse)
331.07488479262696
#calculate ssr
ssr = np.sum((model.fittedvalues - df.score.mean())**2)
print(ssr)
917.4751152073725
#calculate sst
sst = ssr + sse
print(sst)
1248.5499999999995
</b>
The metrics turn out to be:
<b>Sum of Squares Total (SST):</b> 1248.55
<b>Sum of Squares Regression (SSR):</b> 917.4751
<b>Sum of Squares Error (SSE):</b> 331.0749
We can verify that SST = SSR + SSE:
SST = SSR + SSE
1248.55 = 917.4751 + 331.0749
<h2>Additional Resources</h2>
You can use the following calculators to automatically calculate SST, SSR, and SSE for any simple linear regression line:
 SST Calculator 
 SSR Calculator 
 SSE Calculator 
The following tutorials explain how to calculate SST, SSR, and SSE in other statistical software:
 How to Calculate SST, SSR, and SSE in R 
 How to Calculate SST, SSR, and SSE in Excel 
<h2><span class="orange">How to Calculate SST, SSR, and SSE in R</span></h2>
We often use three different  sum of squares  values to measure how well a  regression line  actually fits a dataset:
<b>1. Sum of Squares Total (SST) – </b>The sum of squared differences between individual data points (y<sub>i</sub>) and the mean of the response variable (y).
SST = Σ(y<sub>i</sub> – y)<sup>2</sup>
<b>2. Sum of Squares Regression (SSR)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and the mean of the response variable(y).
SSR = Σ(<U+0177><sub>i</sub> – y)<sup>2</sup>
<b>3. Sum of Squares Error (SSE)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and observed data points (y<sub>i</sub>).
SSE = Σ(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup>
The following step-by-step example shows how to calculate each of these metrics for a given regression model in R.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that contains the number of hours studied and exam score received for 20 different students at a certain college:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 1, 1, 2, 2, 2, 2, 2, 3, 3,         3, 4, 4, 4, 5, 5, 6, 7, 7, 8), score=c(68, 76, 74, 80, 76, 78, 81, 84, 86, 83,         88, 85, 89, 94, 93, 94, 96, 89, 92, 97))
#view first six rows of data frame
head(df)
  hours score
1     1    68
2     1    76
3     1    74
4     2    80
5     2    76
6     2    78</b>
<h3>Step 2: Fit a Regression Model</h3>
Next, we’ll use the <b>lm()</b> function to fit a simple linear regression model using score as the  response variable  and hours as the predictor variable:
<b>#fit regression model
model &lt;- lm(score ~ hours, data = df)
#view model summary
summary(model)
Call:
lm(formula = score ~ hours, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-8.6970 -2.5156 -0.0737  3.1100  7.5495 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  73.4459     1.9147  38.360  &lt; 2e-16 ***
hours         3.2512     0.4603   7.063 1.38e-06 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 4.289 on 18 degrees of freedom
Multiple R-squared:  0.7348,Adjusted R-squared:  0.7201 
F-statistic: 49.88 on 1 and 18 DF,  p-value: 1.378e-06
</b>
<h3>Step 3: Calculate SST, SSR, and SSE</h3>
We can use the following syntax to calculate SST, SSR, and SSE:
<b>#find sse
sse &lt;- sum((fitted(model) - df$score)^2)
sse
[1] 331.0749
#find ssr
ssr &lt;- sum((fitted(model) - mean(df$score))^2)
ssr
[1] 917.4751
#find sst
sst &lt;- ssr + sse
sst
[1] 1248.55
</b>
The metrics turn out to be:
<b>Sum of Squares Total (SST):</b> 1248.55
<b>Sum of Squares Regression (SSR):</b> 917.4751
<b>Sum of Squares Error (SSE):</b> 331.0749
We can verify that SST = SSR + SSE:
SST = SSR + SSE
1248.55 = 917.4751 + 331.0749
We can also manually calculate the  R-squared  of the regression model:
R-squared = SSR / SST
R-squared = 917.4751 / 1248.55
R-squared = 0.7348
This tells us that <b>73.48%</b> of the variation in exam scores can be explained by the number of hours studied.
<h2><span class="orange">A Gentle Guide to Sum of Squares: SST, SSR, SSE</span></h2>
 Linear regression  is used to find a line that best “fits” a dataset.
We often use three different <b>sum of squares</b> values to measure how well the regression line actually fits the data:
<b>1. Sum of Squares Total (SST) – </b>The sum of squared differences between individual data points (y<sub>i</sub>) and the mean of the response variable (y).
SST = Σ(y<sub>i</sub> – y)<sup>2</sup>
<b>2. Sum of Squares Regression (SSR)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and the mean of the response variable(y).
SSR = Σ(<U+0177><sub>i</sub> – y)<sup>2</sup>
<b>3. Sum of Squares Error (SSE)</b> – The sum of squared differences between predicted data points (<U+0177><sub>i</sub>) and observed data points (y<sub>i</sub>).
SSE = Σ(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup>
The following relationship exists between these three measures:
<b>SST = SSR + SSE</b>
Thus, if we know two of these measures then we can use some simple algebra to calculate the third.
<h3>SSR, SST & R-Squared</h3>
 R-squared , sometimes referred to as the coefficient of determination, is a measure of how well a linear regression model fits a dataset. It represents the proportion of the variance in the  response variable  that can be explained by the predictor variable.
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all. A value of 1 indicates that the response variable can be perfectly explained without error by the predictor variable.
Using SSR and SST, we can calculate R-squared as:
<b>R-squared = SSR / SST</b>
For example, if the SSR for a given regression model is 137.5 and SST is 156 then we would calculate R-squared as:
R-squared = 137.5 / 156 = 0.8814
This tells us that 88.14% of the variation in the response variable can be explained by the predictor variable.
<h3>
<b>Calculate SST, SSR, SSE: Step-by-Step Example</b>
</h3>
Suppose we have the following dataset that shows the number of hours studied by six different students along with their final exam scores:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare1.png">
Using some statistical software (like  R ,  Excel ,  Python ) or even  by hand , we can find that the line of best fit is:
<b>Score = 66.615 + 5.0769*(Hours)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare2.png">
Once we know the line of best fit equation, we can use the following steps to calculate SST, SSR, and SSE:
<b>Step 1: Calculate the mean of the response variable.</b>
The mean of the response variable (y) turns out to be <b>81</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare3.png">
<b>Step 2: Calculate the predicted value for each observation.</b>
Next, we can use the line of best fit equation to calculate the predicted exam score () for each student.
For example, the predicted exam score for the student who studied one hours is:
Score = 66.615 + 5.0769*(1) = <b>71.69</b>.
We can use the same approach to find the predicted score for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare4.png">
<b>Step 3: Calculate the sum of squares total (SST).</b>
Next, we can calculate the sum of squares total.
For example, the sum of squares total for the first student is:
(y<sub>i</sub> – y)<sup>2</sup> = (68 – 81)<sup>2</sup> = <b>169</b>.
We can use the same approach to find the sum of squares total for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare5.png">
The sum of squares total turns out to be <b>316</b>.
<b>Step 4: Calculate the sum of squares regression (SSR).</b>
Next, we can calculate the sum of squares regression.
For example, the sum of squares regression for the first student is:
(<U+0177><sub>i</sub> – y)<sup>2</sup> = (71.69 – 81)<sup>2</sup> = <b>86.64</b>.
We can use the same approach to find the sum of squares regression for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare6.png">
The sum of squares regression turns out to be <b>279.23</b>.
<b>Step 5: Calculate the sum of squares error (SSE).</b>
Next, we can calculate the sum of squares error.
For example, the sum of squares error for the first student is:
(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup> = (71.69 – 68)<sup>2</sup> = <b>13.63</b>.
We can use the same approach to find the sum of squares error for each student:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sumSquare7.png">
We can verify that SST = SSR + SSE
SST = SSR + SSE
316 = 279.23 + 36.77
We can also calculate the R-squared of the regression model by using the following equation:
R-squared = SSR / SST
R-squared = 279.23 / 316
R-squared = 0.8836
This tells us that <b>88.36%</b> of the variation in exam scores can be explained by the number of hours studied.
<h2><span class="orange">How to Stack Data Frame Columns in R</span></h2>
Often you may want to stack two or more data frame columns into one column in R.
For example, you may want to go from this:
<b>  person trial outcome1 outcome2
     A     1        7        4
     A     2        6        4
     B     1        6        5
     B     2        5        5
     C     1        4        3
     C     2        4        2
</b>
To this:
<b>   person trial outcomes  value
      A     1   outcome1     7
      A     2   outcome1     6
      B     1   outcome1     6
      B     2   outcome1     5
      C     1   outcome1     4
      C     2   outcome1     4
      A     1   outcome2     4
      A     2   outcome2     4
      B     1   outcome2     5
      B     2   outcome2     5
      C     1   outcome2     3
      C     2   outcome2     2</b>
This tutorial explains two methods you can use in R to do this.
<h3>Method 1: Use the Stack Function in Base R</h3>
The following code shows how to stack columns using the <b>stack </b>function in base R:
<b>#create original data frame
data &lt;- data.frame(person=c('A', 'A', 'B', 'B', 'C', 'C'),   trial=c(1, 2, 1, 2, 1, 2),   outcome1=c(7, 6, 6, 5, 4, 4),   outcome2=c(4, 4, 5, 5, 3, 2))
#stack the third and fourth columns
cbind(data[1:2], stack(data[3:4]))
   person trial values      ind
1       A     1      7 outcome1
2       A     2      6 outcome1
3       B     1      6 outcome1
4       B     2      5 outcome1
5       C     1      4 outcome1
6       C     2      4 outcome1
7       A     1      4 outcome2
8       A     2      4 outcome2
9       B     1      5 outcome2
10      B     2      5 outcome2
11      C     1      3 outcome2
12      C     2      2 outcome2
</b>
<h3>Method 2: Use the Melt Function from Reshape2</h3>
The following code shows how to stack columns using the <b>melt </b>function from the <b>reshape2 </b>library:
<b>#load library
library(reshape2)
#create original data frame
data &lt;- data.frame(person=c('A', 'A', 'B', 'B', 'C', 'C'),   trial=c(1, 2, 1, 2, 1, 2),   outcome1=c(7, 6, 6, 5, 4, 4),   outcome2=c(4, 4, 5, 5, 3, 2))
#melt columns of data frame
melt(data, id.var = c('person', 'trial'), variable.name = 'outcomes')
   person trial outcomes value
1       A     1 outcome1     7
2       A     2 outcome1     6
3       B     1 outcome1     6
4       B     2 outcome1     5
5       C     1 outcome1     4
6       C     2 outcome1     4
7       A     1 outcome2     4
8       A     2 outcome2     4
9       B     1 outcome2     5
10      B     2 outcome2     5
11      C     1 outcome2     3
12      C     2 outcome2     2
</b>
<em>You can find the complete documentation for the melt function  here .</em>
<h2><span class="orange">How to Stack Multiple Pandas DataFrames</span></h2>
Often you may wish to stack two or more pandas DataFrames. Fortunately this is easy to do using the pandas  concat()  function.
This tutorial shows several examples of how to do so.
<h3>Example 1: Stack Two Pandas DataFrames</h3>
The following code shows how to “stack” two pandas DataFrames on top of each other and create one DataFrame:
<b>import pandas as pd
#create two DataFrames
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],    'points':[12, 5, 13, 17, 27]})
df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],    'points':[24, 26, 27, 27, 12]})
#"stack" the two DataFrames together
df3 = pd.concat([df1,df2], ignore_index=True)
#view resulting DataFrame
df3
playerpoints
0A12
1B5
2C13
3D17
4E27
5F24
6G26
7H27
8I27
9J12
</b>
<h3>Example 2: Stack Three Pandas DataFrames</h3>
Similar code can be used to stack three pandas DataFrames on top of each other to create one DataFrame:
<b>import pandas as pd
#create three DataFrames
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],    'points':[12, 5, 13, 17, 27]})
df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],    'points':[24, 26, 27, 27, 12]})
df3 = pd.DataFrame({'player': ['K', 'L', 'M', 'N', 'O'],    'points':[9, 5, 5, 13, 17]})
#"stack" the two DataFrames together
df4 = pd.concat([df1,df2, df3], ignore_index=True)
#view resulting DataFrame
df4
        playerpoints
0A12
1B5
2C13
3D17
4E27
5F24
6G26
7H27
8I27
9J12
10K9
11L5
12M5
13N13
14O17</b>
<h3>The Importance of ignore_index</h3>
Note that in the previous examples we used <b>ignore_index=True</b>.
This tells pandas to ignore the index numbers in each DataFrame and to create a new index ranging from 0 to n-1 for the new DataFrame.
For example, consider what happens when we don’t use <b>ignore_index=True </b>when stacking the following two DataFrames:
<b>import pandas as pd
#create two DataFrames with indices
df1 = pd.DataFrame({'player': ['A', 'B', 'C', 'D', 'E'],    'points':[12, 5, 13, 17, 27]},    index=[0, 1, 2, 3, 4])
df2 = pd.DataFrame({'player': ['F', 'G', 'H', 'I', 'J'],    'points':[24, 26, 27, 27, 12]},    index=[2, 4, 5, 6, 9])
#stack the two DataFrames together
df3 = pd.concat([df1,df2])
#view resulting DataFrame
df3
        playerpoints
0A12
1B5
2C13
3D17
4E27
2F24
4G26
5H27
6I27
9J12
</b>
The resulting DataFrame kept its original index values from the two DataFrames.
Thus, you should typically use <b>ignore_index=True </b>when stacking two DataFrames unless you have a specific reason for keeping the original index values.
<h2><span class="orange">How to Create a Stacked Barplot in R (With Examples)</span></h2>
A <b>stacked barplot</b> is a type of chart that displays quantities for different variables, <i>stacked</i> by another variable.
This tutorial explains how to create stacked barplots in R using the data visualization library  ggplot2 .
<h3>Stacked Barplot in ggplot2</h3>
Suppose we have the following data frame that displays the average points scored per game for nine basketball players:
<b>#create data frame
df &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=3), position=rep(c('Guard', 'Forward', 'Center'), times=3), points=c(14, 8, 8, 16, 3, 7, 17, 22, 26))
#view data frame
df
  team position points
1    A    Guard     14
2    A  Forward      8
3    A   Center      8
4    B    Guard     16
5    B  Forward      3
6    B   Center      7
7    C    Guard     17
8    C  Forward     22
9    C   Center     26</b>
We can use the following code to create a stacked barplot that displays the points scored by each player, stacked by team and position:
<b>library(ggplot2)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='stack', stat='identity')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/groupedBarR1.png">
<h3>Customizing a Stacked Barplot</h3>
We can also customize the title, axes labels, theme, and colors of the stacked barplot to make it look however we’d like:
<b>library(ggplot2)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='stack', stat='identity') +
  theme_minimal() + 
  labs(x='Team', y='Points', title='Avg. Points Scored by Position & Team') +
  theme(plot.title = element_text(hjust=0.5, size=20, face='bold')) +
  scale_fill_manual('Position', values=c('coral2', 'steelblue', 'pink'))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/groupedBarR2.png">
We can also customize the appearance further by using one of the pre-defined themes in the <b>ggthemes</b> library. For example, we could use the Wall Street Journal Theme from this library:
<b>install.packages('ggthemes')
library(ggplot2)
library(ggthemes)
ggplot(df, aes(fill=position, y=points, x=team)) + 
  geom_bar(position='stack', stat='identity') +
  theme_wsj()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/groupedBarR3.png">
<em>Refer to our  Complete Guide to the Best ggplot2 Themes  for even more themes.</em>
<h2><span class="orange">How to Create a Stacked Dot Plot in R</span></h2>
A <b>stacked dot plot </b>is a type of plot that displays frequencies using dots.
There are two methods you can use to create a stacked dot plot in R:
<b>Method 1:</b> The  stripchart()  function in base R.
<b>Method 2:</b> The  geom_dotplot()  function in ggplot2.
This tutorial provides a brief example of how to use each of these methods to produce a stacked dot plot.
<h3>Example 1: Stacked Dot Plot in Base R</h3>
The following code shows how to make a basic stacked dot plot in base R:
<b>#create some fake data
set.seed(0)
data &lt;- sample(0:20, 100, replace = TRUE)
#create stacked dot plot
stripchart(data, method = "stack")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/dotR1.png">
And the following code shows how to customize the stacked dot plot to make it more aesthetically pleasing:
<b>#create some fake data
set.seed(0)
data &lt;- sample(0:20, 100, replace = TRUE)
#create stacked dot plot
stripchart(data, method = "stack", offset = .5, at = 0, pch = 19,
           col = "steelblue", main = "Stacked Dot Plot", xlab = "Data Values")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/dotR2.png">
<h3>Example 2: Stacked Dot Plot in ggplot2</h3>
The following code shows how to make a basic stacked dot plot in ggplot2:
<b>#load ggplot2
library(ggplot2)
#create some fake data
set.seed(0)
data &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))
#create stacked dot plot
ggplot(data, aes(x = x)) +
  geom_dotplot()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/dotR3.png">
And the following code shows how to customize the stacked dot plot to make it more aesthetically pleasing:
<b>#load ggplot2
library(ggplot2)
#create some fake data
set.seed(0)
data &lt;- data.frame(x = sample(0:20, 100, replace = TRUE))
#create customized stacked dot plot
ggplot(data, aes(x = x)) +
  geom_dotplot(dotsize = .75, stackratio = 1.2, fill = "steelblue") + 
  scale_y_continuous(NULL, breaks = NULL) +
  labs(title = "Stacked Dot Plot", x = "Data Values", y = "")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/dotR4.png">
You can find more R tutorials  here .
<h2><span class="orange">How to Calculate Standard Deviation by Group in R (With Examples)</span></h2>
You can use one of the following methods to calculate the standard deviation by group in R:
<b>Method 1: Use base R</b>
<b>aggregate(df$col_to_aggregate, list(df$col_to_group_by), FUN=sd) </b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
df %>%
  group_by(col_to_group_by) %>%
  summarise_at(vars(col_to_aggregate), list(name=sd))
</b>
<b>Method 3: Use data.table</b>
<b>library(data.table)
setDT(df)
dt[ ,list(sd=sd(col_to_aggregate)), by=col_to_group_by]
</b>
The following examples show how to use each of these methods in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=6), points=c(8, 10, 12, 12, 14, 15, 10, 11, 12,          18, 22, 24, 3, 5, 5, 6, 7, 9))
#view data frame
df
   team points
1     A      8
2     A     10
3     A     12
4     A     12
5     A     14
6     A     15
7     B     10
8     B     11
9     B     12
10    B     18
11    B     22
12    B     24
13    C      3
14    C      5
15    C      5
16    C      6
17    C      7
18    C      9</b>
<h2>Method 1: Calculate Standard Deviation by Group Using Base R</h2>
The following code shows how to use the <b>aggregate() </b>function from base R to calculate the standard deviation of points scored by team:
<b>#calculate standard deviation of points by team
aggregate(df$points, list(df$team), FUN=sd)
  Group.1        x
1       A 2.562551
2       B 6.013873
3       C 2.041241</b>
<h2>
<b>Method 2: Calculate Standard Deviation by Group Using dplyr</b>
</h2>
The following code shows how to use the <b>group_by()</b> and <b>summarise_at()</b> functions from the <b>dplyr</b> package to calculate the standard deviation of points scored by team:
<b><b>library(dplyr) </b>
#calculate standard deviation of points scored by team 
<b>df %>%
  group_by(team) %>%
  summarise_at(vars(points), list(name=sd))
</b># A tibble: 3 x 2
  team   name
   
1 A      2.56
2 B      6.01
3 C      2.04
</b>
<h2>Method 3: Calculate Standard Deviation by Group Using data.table</h2>
 The following code shows how to calculate the standard deviation of points scored by team using functions from the <b>data.table</b> package:
<b><b>library(data.table) </b>
#convert data frame to data table 
setDT(df)
#calculate standard deviation of points scored by team 
<b>df[ ,list(sd=sd(points)), by=team]
   team       sd
1:    A 2.562551
2:    B 6.013873
3:    C 2.041241</b></b>
Notice that all three methods return the same results.
<b>Note</b>: If you’re working with an extremely large data frame, it’s recommended to use the <b>dplyr</b> or <b>data.table</b> approach since these packages perform much faster than base R.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Calculate the Mean by Group in R 
 How to Calculate the Sum by Group in R 
 How to Calculate Quantiles by Group in R 
<h2><span class="orange">Standard Deviation in Google Sheets (Sample & Population)</span></h2>
<b>Standard deviation</b> is one of the most common ways to measure the  spread of values  in a dataset.
There are two different types of standard deviations you can calculate, depending on the type of data you’re working with.
<b>1. Population standard deviation</b>
You should calculate the population standard deviation when the dataset you’re working with represents an entire population, i.e. every value that you’re interested in.
The formula to calculate a population standard deviation, denoted as σ, is:
<b>σ = √Σ(x<sub>i</sub> – μ)<sup>2</sup> / N</b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in a dataset
<b>μ</b>: The population mean
<b>N</b>: The population size
<b>2. Sample standard deviation</b>
You should calculate the sample standard deviation when the dataset you’re working with represents a a sample taken from a larger population of interest.
The formula to calculate a sample standard deviation, denoted as <em>s</em>, is:
<b>s = √Σ(x<sub>i</sub> – x<U+0304>)<sup>2</sup> / (n – 1)</b>
where:
<b>Σ</b>: A symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in a dataset
<b>x<U+0304></b>: The sample mean
<b>n</b>: The sample size
The following examples show how to calculate the sample and population standard deviation in Google Sheets.
<h3>Example 1: Calculating Sample Standard Deviation in Google Sheets</h3>
Suppose a biologist wants to summarize the standard deviation of the weight of a particular species of turtles so she collects a  simple random sample  of 20 turtles from the population.
Since she’s using a sample to estimate the standard deviation of the population, she can calculate the sample standard deviation.
The following screenshot shows how to use the <b>STDEV.S()</b> function to calculate the sample standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/st2.png">
The sample standard deviation turns out to be <b>11.91</b>.
Note that <b>STDEV()</b> will also return the sample standard deviation.
<h3>Example 2: Calculating Population Standard Deviation in Google Sheets</h3>
Suppose a basketball coach wants to summarize the standard deviation of points scored by the 12 players on his team.
Since he is only interested in the points scored by his players and not any other players on any other team, he can calculate the population standard deviation.
The following screenshot shows how to use the <b>STDEV.P()</b> function to calculate the population standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/st1.png">
The population standard deviation turns out to be <b>7.331</b>.
 Population vs. Sample Standard Deviation: When to Use Each 
 Coefficient of Variation vs. Standard Deviation: The Difference 
 Why is Standard Deviation Important? 
The following tutorials explain how to calculate other measures of spread in Google Sheets:
 How to Calculate Interquartile Range in Google Sheets 
 How to Calculate Coefficient of Variation in Google Sheets 
<h2><span class="orange">How to Calculate Standard Deviation in R (With Examples)</span></h2>
You can use the following syntax to calculate the standard deviation of a vector in R:
<b>sd(x)
</b>
Note that this formula calculates the sample standard deviation using the following formula:
√Σ (x<sub>i</sub> – μ)<sup>2</sup>/ (n-1)
where:
<b>Σ</b>: A fancy symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in the dataset
<b>μ</b>: The mean value of the dataset
<b>n:</b> The sample size
The following examples show how to use this function in practice.
<h3>Example 1: Calculate Standard Deviation of Vector</h3>
The following code shows how to calculate the standard deviation of a single vector in R:
<b>#create dataset
data &lt;- c(1, 3, 4, 6, 11, 14, 17, 20, 22, 23)
#find standard deviation
sd(data)
[1] 8.279157</b>
Note that you must use <b>na.rm = TRUE</b> to calculate the standard deviation if there are missing values in the dataset:
<b>#create dataset with missing values
data &lt;- c(1, 3, 4, 6, NA, 14, NA, 20, 22, 23)
#attempt to find standard deviation
sd(data)
[1] NA
#find standard deviation and specify to ignore missing values
sd(data, na.rm = TRUE)
[1] 9.179753
</b>
<h3>Example 2: Calculate Standard Deviation of Column in Data Frame</h3>
The following code shows how to calculate the standard deviation of a single column in a data frame:
<b>#create data frame
data &lt;- data.frame(a=c(1, 3, 4, 6, 8, 9),   b=c(7, 8, 8, 7, 13, 16),   c=c(11, 13, 13, 18, 19, 22),   d=c(12, 16, 18, 22, 29, 38))
#find standard deviation of column a
sd(data$a)
[1] 3.060501
</b>
<h3>Example 3: Calculate Standard Deviation of Several Columns in Data Frame</h3>
The following code shows how to calculate the standard deviation of several columns in a data frame:
<b>#create data frame
data &lt;- data.frame(a=c(1, 3, 4, 6, 8, 9),   b=c(7, 8, 8, 7, 13, 16),   c=c(11, 13, 13, 18, 19, 22),   d=c(12, 16, 18, 22, 29, 38))
#find standard deviation of specific columns in data frame
apply(data[ , c('a', 'c', 'd')], 2, sd)
       a        c        d 
3.060501 4.289522 9.544632 </b>
<h2><span class="orange">How to Calculate Standard Deviation of Columns in R</span></h2>
You can use the following basic syntax to calculate the standard deviation of columns in R:
<b>#calculate standard deviation of one column
sd(df$col1)
#calculate standard deviation of all columns
sapply(df, sd)
#calculate standard deviation of specific columns
sapply(df[c('col1', 'col2', 'col5')], sd)</b>
The following examples show how to use this syntax in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28))
#view data frame
df
  team points assists rebounds
1    A     99      33       30
2    B     91      28       28
3    C     86      31       24
4    D     88      39       24
5    E     95      34       28
</b>
<h2>Example 1: Standard Deviation of One Column</h2>
The following code shows how to calculate the standard deviation of one column in the data frame:
<b>#calculate standard deviation of 'points' column
sd(df$points)
[1] 5.263079
</b>
The standard deviation of values in the ‘points’ column is <b>5.263079</b>.
<h2>Example 2: Standard Deviation of All Columns</h2>
The following code shows how to calculate the standard deviation of every column in the data frame:
<b>#calculate standard deviation of all columns in data frame
sapply(df, sd)
    team   points  assists rebounds 
      NA 5.263079 4.062019 2.683282 
Warning message:
In var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm = na.rm) :
  NAs introduced by coercion</b>
Since the ‘team’ column is a character variable, R returns NA and gives us a warning.
However, it successfully computes the standard deviation of the other three numeric columns.
<h2>Example 3: Standard Deviation of Specific Columns</h2>
The following code shows how to calculate the standard deviation of specific columns in the data frame:
<b>#calculate standard deviation of 'points' and 'rebounds' columns
sapply(df[c('points', 'rebounds')], sd)
  points rebounds 
5.263079 2.683282 </b>
Note that we could use column index values to select columns as well:
<b>#calculate standard deviation of 'points' and 'rebounds' columns
sapply(df[c(2, 4)], sd)
  points rebounds 
5.263079 2.683282 </b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in R:
 How to Calculate Standard Deviation of Rows in R 
 How to Calculate the Mean of Multiple Columns in R 
 How to Find the Max Value Across Multiple Columns in R 
 How to Select Specific Columns in R 
<h2><span class="orange">How to Calculate the Standard Deviation of a List in Python</span></h2>
You can use one of the following three methods to calculate the standard deviation of a list in Python:
<b>Method 1: Use NumPy Library</b>
<b>import numpy as np
#calculate standard deviation of list
<span>np.std<span>(<span>my_list<span>)
</b>
<b>Method 2: Use statistics Library</b>
<b>import statistics as stat
#calculate standard deviation of list
<span>stat.stdev<span>(<span>my_list<span>)</b>
<b>Method 3: Use Custom Formula</b>
<b>#calculate standard deviation of list
<span>st.stdev<span>(<span>my_list<span>)</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Calculate Standard Deviation Using NumPy Library</h3>
The following code shows how to calculate both the sample standard deviation and population standard deviation of a list using NumPy:
<b>import numpy as np
#define list
my_list = [3, 5, 5, 6, 7, 8, 13, 14, 14, 17, 18]
#calculate sample standard deviation of list
<span>np.std<span>(<span>my_list, ddof=1<span>)
5.310367218940701
#calculate population standard deviation of list 
<span>np.std(<span>my_list)
5.063236478416116
</b>
Note that the population standard deviation will always be smaller than the sample standard deviation for a given dataset.
<h3>Method 2: Calculate Standard Deviation Using statistics Library</h3>
The following code shows how to calculate both the sample standard deviation and population standard deviation of a list using the Python statistics library:
<b>import statistics as stat
#define list
my_list = [3, 5, 5, 6, 7, 8, 13, 14, 14, 17, 18]
#calculate sample standard deviation of list
<span>stat.stdev(my_list)
5.310367218940701
#calculate population standard deviation of list 
stat.pstdev(my_list)
5.063236478416116
</b>
<h3>Method 3: Calculate Standard Deviation Using Custom Formula</h3>
The following code shows how to calculate both the sample standard deviation and population standard deviation of a list without importing any Python libraries:
<b>#define list
my_list = [3, 5, 5, 6, 7, 8, 13, 14, 14, 17, 18]
#calculate sample standard deviation of list<span>
(sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / (len(my_list)-1))**0.5
5.310367218940701
#calculate population standard deviation of list 
(sum((x-(sum(my_list) / len(my_list)))**2 for x in my_list) / len(my_list))**0.5
5.063236478416116</b>
Notice that all three methods calculated the same values for the standard deviation of the list.
<h2><span class="orange">How to Find the Standard Deviation of a Probability Distribution</span></h2>
A probability distribution tells us the probability that a  random variable  takes on certain values.
For example, the following probability distribution tells us the probability that a certain soccer team scores a certain number of goals in a given game:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/meanDist1.png">
To find the <b>standard deviation </b>of a probability distribution, we can use the following formula:
<b>σ = √Σ(x<sub>i</sub>-μ)<sup>2</sup> * P(x<sub>i</sub>)</b>
where:
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value
<b>μ:</b> The mean of the distribution
<b>P(x<sub>i</sub>):</b> The probability of the i<sup>th</sup> value
For example, consider our probability distribution for the soccer team:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/meanDist1.png">
The mean number of goals for the soccer team would be calculated as:
μ = 0*0.18  +  1*0.34  +  2*0.35  +  3*0.11  +  4*0.02  =  <b>1.45</b> goals.
We could then calculate the standard deviation as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sdDist1.png">
The standard deviation is the square root of the sum of the values in the third column. Thus, we would calculate it as:
Standard deviation = √(.3785 + .0689 + .1059 + .2643 + .1301) = <b>0.9734</b>
The variance is simply the standard deviation squared, so:
Variance = .9734<sup>2</sup> = <b>0.9475</b>
The following examples show how to calculate the standard deviation of a probability distribution in a few other scenarios.
<h3>Example 1: Standard Deviation of Vehicle Failures</h3>
The following probability distribution tells us the probability that a given vehicle experiences a certain number of battery failures during a 10-year span:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/meanDist2.png">
<b>Question: </b>What is the standard deviation of the number of failures for this vehicle?
<b>Solution:</b> The mean number of expected failures is calculated as:
μ = 0*0.24  +  1*0.57  +  2*0.16  +  3*0.03 =  <b>0.98 </b>failures.
We could then calculate the standard deviation as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sdDist2.png">
The standard deviation is the square root of the sum of the values in the third column. Thus, we would calculate it as:
Standard deviation = √(.2305 + .0002 + .1665 + .1224) = <b>0.7208</b>
<h3>Example 2: Standard Deviation of Sales</h3>
The following probability distribution tells us the probability that a given salesman will make a certain number of sales in the upcoming month:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/meanDist4.png">
<b>Question: </b>What is the standard deviation of the number of sales for this salesman in the upcoming month?
<b>Solution:</b> The mean number of expected sales is calculated as:
μ = 10*.24  +  20*.31  +  30*0.39  +  40*0.06  =  <b>22.7 </b>sales.
We could then calculate the standard deviation as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sdDist3.png">
The standard deviation is the square root of the sum of the values in the third column. Thus, we would calculate it as:
Standard deviation = √(38.7096 + 2.2599 + 20.7831 + 17.9574) = <b>8.928</b>
<h2><span class="orange">How to Calculate Standard Deviation of Rows in R</span></h2>
You can use the following basic syntax to calculate the standard deviation of rows in R:
<b>row_stdev &lt;- apply(df, 1, sd, na.rm=TRUE)
</b>
The following example shows how to use this syntax in R.
<h2>Example: Calculate Standard Deviation of Rows in R</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(game1=c(12, 15, 15, 18, 29, 30, 31), game2=c(15, 17, 17, 16, 29, 8, 14), game3=c(8, 22, 27, 35, 29, 22, 17))
#view data frame
df
  game1 game2 game3
1    12    15     8
2    15    17    22
3    15    17    27
4    18    16    35
5    29    29    29
6    30     8    22
7    31    14    17</b>
We can use the following syntax to calculate the standard deviation of the values in each row:
<b>#calculate standard deviation of each row
row_stdev &lt;- apply(df, 1, sd, na.rm=TRUE)
#view standard deviation of each row
row_stdev
[1]  3.511885  3.605551  6.429101 10.440307  0.000000 11.135529  9.073772
</b>
From the output we can see:
The standard deviation of values in the first row is <b>3.511885</b>.
The standard deviation of values in the second row is <b>3.605551</b>.
The standard deviation of values in the third row is <b>6.429101</b>.
And so on.
If we’d like, we can also use the  transform()  function to add a new column to the data frame that shows the standard deviation of values in each row:
<b>#add column that displays standard deviation of each row
df &lt;- transform(df, row_stdev=apply(df, 1, sd, na.rm=TRUE))
#view updated data frame
df
  game1 game2 game3 row_stdev
1    12    15     8  3.511885
2    15    17    22  3.605551
3    15    17    27  6.429101
4    18    16    35 10.440307
5    29    29    29  0.000000
6    30     8    22 11.135529
7    31    14    17  9.073772
</b>
The new column called <b>row_stdev</b> displays the standard deviation of values in each row.
<b>Note:</b> The standard deviation of values in row 5 is equal to zero because each of the values is the same, thus there is no “deviation” at all in the values.
<b>Related:</b>  How to Interpret a Standard Deviation of Zero 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common functions in R:
 How to Calculate Standard Deviation Using dplyr 
 How to Calculate Weighted Standard Deviation in R 
 How to Calculate Pooled Standard Deviation in R 
<h2><span class="orange">How to Interpret a Standard Deviation of Zero</span></h2>
In statistics, the <b>standard deviation</b> is used to measure the spread of values in a sample.
We can use the following formula to calculate the standard deviation of a given sample:
√Σ(x<sub>i</sub> – x<sub>bar</sub>)<sup>2</sup> / (n-1)
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value in the sample
<b>x<sub>bar</sub>:</b> The mean of the sample
<b>n:</b> The sample size
The higher the value for the standard deviation, the more spread out the values are in a  sample .
The lower the value for the standard deviation, the more tightly packed together the values.
<b>If the standard deviation of a sample is zero, this means that every value in the sample is the exact same.</b>
In other words, there is zero spread in the values.
The following example shows how to interpret a standard deviation of zero in practice.
<h3>Example: How to Interpret a Standard Deviation of Zero</h3>
Suppose we collect a  simple random sample  of 10 lizards and measure their lengths (in inches):
<b>Lengths</b>: 7, 7, 7, 7, 7, 7, 7, 7, 7, 7
The mean length of lizards in the sample is 7 inches.
Knowing this, we can calculate the sample standard deviation (s) for this dataset:
s = √Σ(x<sub>i</sub> – x<sub>bar</sub>)<sup>2</sup> / (n-1)
s = √((7 – 7)<sup>2</sup> + (7 – 7)<sup>2 </sup>+ (7 – 7)<sup>2</sup> + … +  (7 – 7)<sup>2</sup>/ (10-1)
s = √0<sup>2</sup> + 0<sup>2 </sup>+ 0<sup>2</sup> + … + 0<sup>2</sup>/ 9
s = <b>0</b>
The sample standard deviation turns out to be <b>0</b>.
Since every lizard has the exact same length, the spread of values in the dataset is exactly zero.
<h3>Will Standard Deviation Ever Be Zero in the Real World?</h3>
It’s entirely possible that a real-world dataset could have a standard deviation of zero, but it’s rare.
The most likely scenario where you could encounter a standard deviation of zero would be when collecting small samples for rare events.
For example, suppose you collect data on the number of traffic accidents during a one-week interval in a certain town.
It’s entirely possible that you could collect the following data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/sd0.jpg"185">
In this scenario, the mean number of daily accidents would be zero and the standard deviation would also be zero.
Or perhaps you collect the following data on the number of monthly sales of some expensive product for some company during a 6-month time frame:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/sd0_1.jpg"182">
Since the product is so expensive, it just so happens that the company only sells exactly two each month.
In this scenario, the mean number of monthly products sold is two and the standard deviation of monthly products sold is zero.
<b>Whenever you encounter a standard deviation of zero in a real-world dataset, just know that this means every value in the dataset is the exact same.</b>
<h2><span class="orange">6 Examples of Using Standard Deviation in Real Life</span></h2>
The <b>standard deviation</b> is used to measure the spread of values in a dataset.
Individuals and companies use standard deviation all the time in different fields to gain a better understanding of datasets.
The following examples explain how the standard deviation is used in different real life scenarios.
<h3>Example 1: Standard Deviation in Weather Forecasting</h3>
Standard deviation is widely used in weather forecasting to understand how much variation exists in daily and monthly temperatures in different cities.
For example:
A weatherman who works in a city with a small standard deviation in temperatures year-round can confidently predict what the weather will be on a given day since temperatures don’t vary much from one day to the next.
A weatherman who works in a city with a high standard deviation in temperatures will be less confident in his predictions because there is much more variation in temperatures from one day to the next.
<h3>Example 2: Standard Deviation in Healthcare</h3>
Standard deviation is widely used by insurance analysts and actuaries in the healthcare industry.
For example:
Insurance analysts often calculate the standard deviation of the age of the individuals they provide insurance for so they can understand how much variation exists among the age of individuals they provide insurance for.
Actuaries calculate standard deviation of healthcare usage so they can know how much variation in usage to expect in a given month, quarter, or year.
<h3>Example 3: Standard Deviation in Real Estate</h3>
Standard deviation is a metric that is used often by real estate agents.
For example:
Real estate agents calculate the standard deviation of house prices in a particular area so they can inform their clients of the type of variation in house prices they can expect.
Real estate agents also calculate the standard deviation of the square footage of house prices in certain areas so they can inform their clients on what type of variation to expect in terms of square footage of houses in a particular area.
<h3>Example 4: Standard Deviation in Human Resources</h3>
Standard deviation is often used by individuals who work in Human Resource departments at companies.
For example:
Human Resource managers often calculate the standard deviation of salaries in a certain field so that they can know what type of variation in salaries to offer to new employees.
<h3>Example 5: Standard Deviation in Marketing</h3>
Standard deviation is often used by marketers to gain an understanding of how their advertisements perform.
For example:
Marketers often calculate the standard deviation of revenue earned per advertisement so they can understand how much variation to expect in revenue from a given ad.
Marketers also calculate the standard deviation of the number of ads used by competitors to understand whether or not competitors are using more or less ads than normal during a given period.
<h3>Example 6: Standard Deviation in Test Scores</h3>
Standard deviation is used by professors at universities to calculate the spread of test scores among students.
For example:
Professors can calculate the standard deviation of test scores on a final exam to better understand whether most students score close to the average or if there is a wide spread in test scores.
Professors can also calculate the standard deviation of test scores for multiple classes to understand which classes had the highest variation in test scores among students.
<h2><span class="orange">Standard Deviation vs. Standard Error: What’s the Difference?</span></h2>
Two terms that students often confuse in statistics are <b>standard deviation</b> and <b>standard error</b>.
The <b>standard deviation </b>measures how spread out values are in a dataset.
The <b>standard error </b>is the standard deviation of the mean in repeated samples from a population.
Let’s check out an example to clearly illustrate this idea.
<h2>Example: Standard Deviation vs. Standard Error</h2>
Suppose we measure the weight of 10 different turtles.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error1.png">
For this sample of 10 turtles, we can calculate the sample mean and the sample standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error2-1.png">
Suppose the standard deviation turns out to be 8.68. This gives us an idea of how spread out the weights are of these turtles.
But suppose we collect another  simple random sample  of 10 turtles and take their measurements as well.
More than likely, this sample of 10 turtles will have a slightly different mean and standard deviation, even if they’re taken from the same population:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error3.png">
Now if we imagine that we take repeated samples from the same population and record the sample mean and sample standard deviation for each sample:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error4.png">
Now imagine that we plot each of the sample means on the same line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error5.png">
<b>The standard deviation of these means is known as the standard error.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_dev_vs_standard_error6.png">
The formula to actually calculate the standard error is:
<b>Standard Error = s/ √n</b>
where:
<b>s:</b> sample standard deviation
<b>n:</b> sample size
<h2>What’s the Point of Using the Standard Error?</h2>
When we calculate the mean of a given sample, we’re not actually interested in knowing the mean of that particular sample, but rather the mean of the larger population that the sample comes from.
However, we use samples because they’re much easier to collect data for compared to an entire population.
And of course the sample mean will vary from sample to sample, so we use the <b>standard error of the mean </b>as a way to measure how precise our estimate is of the mean.
You’ll notice from the formula to calculate the standard error that as the sample size (n) increases, the standard error decreases:
<b>Standard Error = s/ √n</b>
This should make sense as larger sample sizes reduce variability and increase the chance that our sample mean is closer to the actual population mean.
<h2>When to Use Standard Deviation vs. Standard Error</h2>
If we are simply interested in measuring how spread out values are in a dataset, we can use the <b>standard deviation</b>.
However, if we’re interested in quantifying the uncertainty around an estimate of the mean, we can use the <b>standard error of the mean</b>.
Depending on your specific scenario and what you’re trying to accomplish, you may choose to use either the standard deviation or the standard error.
<h2><span class="orange">How to Calculate the Standard Error of the Mean in Excel</span></h2>
The <b>standard error of the mean</b> is a way to measure how spread out values are in a dataset. It is calculated as:
<b>Standard error = s / √n</b>
where:
<b>s</b>: sample standard deviation
<b>n</b>: sample size
You can calculate the standard error of the mean for any dataset in Excel by using the following formula:
<b>=STDEV(range of values) / SQRT(COUNT(range of values))</b>
The following example demonstrates how to use this formula.
<h3>Example: Standard Error in Excel</h3>
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_error_excel1.png">
The following screenshot shows how to calculate the standard error of the mean for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_error_excel2.png">
The standard error turns out to be <b>2.0014</b>.
Note that the function <b>=STDEV() </b>calculates the sample mean, which is equivalent to the function <b>=STDEV.S() </b>in Excel.
Thus, we could have used the following formula to get the same results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_error_excel3.png">
Once again the standard error turns out to be <b>2.0014</b>.
<h3>How to Interpret the Standard Error of the Mean</h3>
The standard error of the mean is simply a measure of how spread out values are around the mean. There are two things to keep in mind when interpreting the standard error of the mean:
<b>1. The larger the standard error of the mean, the more spread out values are around the mean in a dataset.</b>
To illustrate this, consider if we change the last value in the previous dataset to a much larger number:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_error_excel4.png">
Notice how the standard error jumps from <b>2.0014 </b>to <b>6.9783</b>. This is an indication that the values in this dataset are more spread out around the mean compared to the previous dataset.
<b>2. As the sample size increases, the standard error of the mean tends to decrease.</b>
To illustrate this, consider the standard error of the mean for the following two datasets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/standard_error_excel5.png">
The second dataset is simply the first dataset repeated twice. Thus, the two datasets have the same mean but the second dataset has a larger sample size so it has a smaller standard error.
<h2><span class="orange">What is the Standard Error of the Estimate? (Definition & Example)</span></h2>
The <b>standard error of the estimate</b> is a way to measure the accuracy of the predictions made by a regression model.
Often denoted σ<sub>est</sub>, it is calculated as:
<b>σ<sub>est</sub> = √Σ(y – <U+0177>)<sup>2</sup>/n</b>
where:
<b>y:</b> The observed value
<b><U+0177>:</b> The predicted value
<b>n:</b> The total number of observations
The standard error of the estimate gives us an idea of how well a regression model fits a dataset. In particular:
The smaller the value, the better the fit.
The larger the value, the worse the fit.
For a regression model that has a small standard error of the estimate, the data points will be closely packed around the estimated regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate1.png">
Conversely, for a regression model that has a large standard error of the estimate, the data points will be more loosely scattered around the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate2.png">
The following example shows how to calculate and interpret the standard error of the estimate for a regression model in Excel.
<h3>Example: Standard Error of the Estimate in Excel</h3>
Use the following steps to calculate the standard error of the estimate for a regression model in Excel.
<b>Step 1: Enter the Data</b>
First, enter the values for the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate3.png">
<b>Step 2: Perform Linear Regression</b>
Next, click the <b>Data</b> tab along the top ribbon. Then click the <b>Data Analysis</b> option within the <b>Analyze</b> group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option, you need to first  load the Analysis ToolPak .
In the new window that appears, click <b>Regression</b> and then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/regressionPak.png">
In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate4-1.png">
Once you click <b>OK</b>, the regression output will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/standardErrorEstimate5.png">
We can use the coefficients from the regression table to construct the estimated regression equation:
<b><U+0177> = 13.367 + 1.693(x)</b>
And we can see that the standard error of the estimate for this regression model turns out to be <b>6.006</b>. In simple terms, this tells us that the average data point falls <b>6.006</b> units from the regression line.
We can use the estimated regression equation and the standard error of the estimate to construct a 95% confidence interval for the predicted value of a certain data point.
For example, suppose x is equal to 10. Using the estimated regression equation, we would predict that y would be equal to:
<U+0177> = 13.367 + 1.693*(10) = 30.297
And we can obtain the 95% confidence interval for this estimate by using the following formula:
95% C.I. = [<U+0177> – 1.96*σ<sub>est</sub>, <U+0177> + 1.96*σ<sub>est</sub>]
For our example, the 95% confidence interval would be calculated as:
95% C.I. = [<U+0177> – 1.96*σ<sub>est</sub>, <U+0177> + 1.96*σ<sub>est</sub>]
95% C.I. = [30.297 – 1.96*6.006, 30.297 + 1.96*6.006]
95% C.I. = [18.525, 42.069]
<h2><span class="orange">How to Calculate Standard Error of the Mean in Google Sheets</span></h2>
The <b>standard error of the mean</b> is a way to measure how spread out values are in a dataset. It is calculated as:
<b>Standard error = s / √n</b>
where:
<b>s</b>: sample standard deviation
<b>n</b>: sample size
We can calculate the standard error of the mean for a given dataset in Google Sheets by using the following formula:
<b>=STDEV.S(range of values) / SQRT(COUNT(range of values))</b>
The following example demonstrates how to use this formula.
<h3>Example: Standard Error in Google Sheets</h3>
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/semGoogleSheets1.png">
The following formula shows how to calculate the standard error of the mean for this dataset:
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/semGoogleSheets2.png">
The standard error of the mean turns out to be <b>2.0014</b>.
<h3>How to Interpret the Standard Error of the Mean</h3>
The standard error of the mean is a measure of how spread out values are around the mean. There are two things to keep in mind when interpreting the standard error of the mean:
<b>1. The larger the standard error of the mean, the more spread out values are around the mean in a dataset.</b>
To illustrate this, consider if we change the last value in the previous dataset to a much larger number:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/semGoogleSheets3.png">
Notice how the standard error jumps from <b>2.0014 </b>to <b>6.9783</b>. This tells us that the values in this dataset are more spread out around the mean compared to the previous dataset.
<b>2. As the sample size increases, the standard error of the mean tends to decrease.</b>
To illustrate this, consider the standard error of the mean for the following two datasets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/semGoogleSheets4.png">
The second dataset is simply the first dataset repeated twice. Thus, the two datasets have the same mean but the second dataset has a larger sample size so it has a smaller standard error.
<h2><span class="orange">How to Calculate the Standard Error of the Mean in Python</span></h2>
The <b>standard error of the mean</b> is a way to measure how spread out values are in a dataset. It is calculated as:
<b>Standard error of the mean = s / √n</b>
where:
<b>s</b>: sample standard deviation
<b>n</b>: sample size
This tutorial explains two methods you can use to calculate the standard error of the mean for a dataset in Python. Note that both methods produce the exact same results.
<h3>Method 1: Use SciPy</h3>
The first way to calculate the standard error of the mean is to use the  sem()  function from the SciPy Stats library.
The following code shows how to use this function:
<b>from scipy.stats import sem
#define dataset 
data = [3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 29]
#calculate standard error of the mean 
sem(data)
2.001447</b>
The standard error of the mean turns out to be <b>2.001447</b>.
<h3>Method 2: Use NumPy</h3>
Another way to calculate the standard error of the mean for a dataset is to use the  std()  function from NumPy.
<em>Note that we must specify <b>ddof=1 </b>in the argument for this function to calculate the sample standard deviation as opposed to the population standard deviation.</em>
The following code shows how to do so:
<b>import numpy as np
#define dataset
data = np.array([3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 29])
#calculate standard error of the mean 
np.std(data, ddof=1) / np.sqrt(np.size(data))
2.001447
</b>
Once again, the standard error of the mean turns out to be <b>2.001447</b>.
<h3>How to Interpret the Standard Error of the Mean</h3>
The standard error of the mean is simply a measure of how spread out values are around the mean. There are two things to keep in mind when interpreting the standard error of the mean:
<b>1. The larger the standard error of the mean, the more spread out values are around the mean in a dataset.</b>
To illustrate this, consider if we change the last value in the previous dataset to a much larger number:
<b>from scipy.stats import sem
#define dataset 
data = [3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 150]
#calculate standard error of the mean 
sem(data)
6.978265
</b>
Notice how the standard error jumps from <b>2.001447 </b>to <b>6.978265</b>. This is an indication that the values in this dataset are more spread out around the mean compared to the previous dataset.
<b>2. As the sample size increases, the standard error of the mean tends to decrease.</b>
To illustrate this, consider the standard error of the mean for the following two datasets:
<b>from scipy.stats import sem 
#define first dataset and find SEM
data1 = [1, 2, 3, 4, 5]
sem(data1)
0.7071068
#define second dataset and find SEM
data2 = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
sem(data2)
0.4714045
</b>
The second dataset is simply the first dataset repeated twice. Thus, the two datasets have the same mean but the second dataset has a larger sample size so it has a smaller standard error.
<h2><span class="orange">How to Calculate Standard Error of the Mean in R</span></h2>
The <b>standard error of the mean</b> is a way to measure how spread out values are in a dataset. It is calculated as:
<b>Standard error = s / √n</b>
where:
<b>s</b>: sample standard deviation
<b>n</b>: sample size
This tutorial explains two methods you can use to calculate the standard error of a dataset in R.
<h2>Method 1: Use the Plotrix Library</h2>
The first way to calculate the standard error of the mean is to use the built-in  std.error()  function from the Plotrix library.
The following code shows how to use this function:
<b>library(plotrix)
#define dataset
data &lt;- c(3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 29)
#calculate standard error of the mean 
std.error(data)
2.001447</b>
The standard error of the mean turns out to be <b>2.001447</b>.
<h2>Method 2: Define Your Own Function</h2>
Another way to calculate the standard error of the mean for a dataset is to simply define your own function.
The following code shows how to do so:
<b>#define standard error of mean function
std.error &lt;- function(x) sd(x)/sqrt(length(x))
#define dataset
data &lt;- c(3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 29)
#calculate standard error of the mean 
std.error(data)
2.001447</b>
Once again, the standard error of the mean turns out to be <b>2.0014</b>.
<h2>How to Interpret the Standard Error of the Mean</h2>
The standard error of the mean is simply a measure of how spread out values are around the mean. 
There are two things to keep in mind when interpreting the standard error of the mean:
<b>1. The larger the standard error of the mean, the more spread out values are around the mean in a dataset.</b>
To illustrate this, consider if we change the last value in the previous dataset to a much larger number:
<b>#define dataset
data &lt;- c(3, 4, 4, 5, 7, 8, 12, 14, 14, 15, 17, 19, 22, 24, 24, 24, 25, 28, 28, 150)
#calculate standard error of the mean 
std.error(data)
6.978265
</b>
Notice how the standard error jumps from <b>2.001447 </b>to <b>6.978265</b>.
This is an indication that the values in this dataset are more spread out around the mean compared to the previous dataset.
<b>2. As the sample size increases, the standard error of the mean tends to decrease.</b>
To illustrate this, consider the standard error of the mean for the following two datasets:
<b>#define first dataset and find SEM
data1 &lt;- c(1, 2, 3, 4, 5)
std.error(data1)
0.7071068
#define second dataset and find SEM
data2 &lt;- c(1, 2, 3, 4, 5, 1, 2, 3, 4, 5)
std.error(data2)
0.4714045
</b>
The second dataset is simply the first dataset repeated twice.
Thus, the two datasets have the same mean but the second dataset has a larger sample size so it has a smaller standard error.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Calculate Sample & Population Variance in R 
 How to Calculate Pooled Variance in R 
 How to Calculate the Coefficient of Variation in R 
<h2><span class="orange">Standard Error of Measurement: Definition & Example</span></h2>
A <b>standard error of measurement</b>, often denoted <b>SE<sub>m</sub></b>, estimates the variation around a “true” score for an individual when repeated measures are taken.
It is calculated as:
<b>SE<sub>m</sub></b> = s√1-R
where:
<b>s:</b> The standard deviation of measurements
<b>R:</b> The reliability coefficient of a test
Note that a  reliability coefficient  ranges from 0 to 1 and is calculated by administering a test to many individuals twice and calculating the correlation between their test scores.
The higher the reliability coefficient, the more often a test produces consistent scores.
<h3>Example: Calculating a Standard Error of Measurement</h3>
Suppose an individual takes a certain test 10 times over the course of a week that aims to measure overall intelligence on a scale of 0 to 100. They receive the following scores:
<b>Scores:</b> 88, 90, 91, 94, 86, 88, 84, 90, 90, 94
The sample mean is 89.5 and the sample standard deviation is 3.17.
If the test is known to have a reliability coefficient of 0.88, then we would calculate the standard error of measurement as:
SE<sub>m</sub> = s√1-R = 3.17√1-.88 = <b>1.098</b>
<h3>How to Use SE<sub>m</sub> to Create Confidence Intervals</h3>
Using the standard error of measurement, we can create a confidence interval that is likely to contain the “true” score of an individual on a certain test with a certain degree of confidence.
If an individual receives a score of <em>x</em> on a test, we can use the following formulas to calculate various confidence intervals for this score:
68% Confidence Interval = [<em>x</em> – SE<sub>m</sub>, <em>x</em> + SE<sub>m</sub>]
95% Confidence Interval = [<em>x</em> – 2*SE<sub>m</sub>, <em>x</em> + 2*SE<sub>m</sub>]
99% Confidence Interval = [<em>x</em> – 3*SE<sub>m</sub>, <em>x</em> + 3*SE<sub>m</sub>]
For example, suppose an individual scores a 92 on a certain test that is known to have a SE<sub>m</sub> of 2.5. We could calculate a 95% confidence interval as:
95% Confidence Interval = [92 – 2*2.5, 92 + 2*2.5] = [87, 97]
This means we are <b>95% confident</b> that an individual’s “true” score on this test is between 87 and 97.
<h3>Reliability & Standard Error of Measurement</h3>
There exists a simple relationship between the reliability coefficient of a test and the standard error of measurement:
The higher the reliability coefficient, the lower the standard error of measurement.
The lower the reliability coefficient, the higher the standard error of measurement.
To illustrate this, consider an individual who takes a test 10 times and has a standard deviation of scores of <b>2</b>.
If the test has a reliability coefficient of <b>0.9</b>, then the standard error of measurement would be calculated as:
SE<sub>m</sub> = s√1-R = 2√1-.9 = <b>0.632</b>
However, if the test has a reliability coefficient of <b>0.5</b>, then the standard error of measurement would be calculated as:
SE<sub>m</sub> = s√1-R = 2√1-.5 = <b>1.414</b>
This should make sense intuitively: If the scores of a test are less reliable, then the error in the measurement of the “true” score will be higher.
<h2><span class="orange">Standard Error of the Proportion: Formula & Example</span></h2>
Often in statistics we’re interested in estimating the proportion of individuals in a  population  with a certain characteristic.
For example, we might be interested in estimating the proportion of residents in a certain city who support a new law.
Instead of going around and asking every individual resident if they support the law, we would instead collect a  simple random sample  and find out how many residents in the sample support the law.
We would then calculate the <b>sample proportion (p<U+0302>)</b> as:
<b>Sample Proportion Formula:</b>
 
<b>p<U+0302> = x / n</b>
where:
<b>x:</b> The count of individuals in the sample with a certain characteristic.
<b>n:</b> The total number of individuals in the sample.
We would then use this sample proportion to <em>estimate</em> the population proportion. For example, if 47 of the 300 residents in the sample supported the new law, the sample proportion would be calculated as 47 / 300 = <b>0.157</b>.
This means our best estimate for the proportion of residents in the population who supported the law would be <b>0.157</b>.
However, there’s no guarantee that this estimate will exactly match the true population proportion so we typically calculate the <b>standard error of the proportion</b> as well.
This is calculated as:
<b>Standard Error of the Proportion Formula:</b>
 
<b>Standard Error = √p<U+0302>(1-p<U+0302>) / n</b>
For example, if p<U+0302> = 0.157 and n = 300, then we would calculate the standard error of the proportion as:
Standard error of the proportion = √.157(1-.157) / 300 = <b>0.021</b>
We then typically use this standard error to calculate a confidence interval for the true proportion of residents who support the law.
This is calculated as:
<b>Confidence Interval for a Population Proportion Formula:</b>
 
<b>Confidence Interval = p<U+0302>  +/-  z*√p<U+0302>(1-p<U+0302>) / n</b>
Looking at this formula, it’s easy to see that <b>the larger the standard error of the proportion, the wider the confidence interval</b>.
Note that the <b>z</b> in the formula is the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
For example, here’s how to calculate a 95% confidence interval for the true proportion of residents in the city who support the new law:
95% C.I. = p<U+0302>  +/-  z*√p<U+0302>(1-p<U+0302>) / n
95% C.I. = .157  +/-  1.96*√.157(1-.157) / 300
95% C.I. = .157  +/-  1.96*(.021)
95% C.I. = [ .10884 , .19816]
Thus, we would say with 95% confidence that the true proportion of residents in the city who support the new law is between 10.884% and 19.816%.
<h2><span class="orange">How to Calculate the Standard Error of Regression in Excel</span></h2>
Whenever we fit a  linear regression model , the model takes on the following form:
Y = β<sub>0</sub> + β<sub>1</sub>X + … + β<sub>i</sub>X +<U+03F5>
where <U+03F5> is an error term that is independent of X.
No matter how well X can be used to predict the values of Y, there will always be some random error in the model.
One way to measure the dispersion of this random error is by using the <b>standard error of the regression model</b>, which is a way to measure the standard deviation of  the residuals  <U+03F5>.
This tutorial provides a step-by-step example of how to calculate the standard error of a regression model in Excel.
<h3>Step 1: Create the Data</h3>
For this example, we’ll create a dataset that contains the following variables for 12 different students:
Exam Score
Hours Spent Studying
Current Grade
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel1.png">
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll fit a  multiple linear regression  model using <em>Exam Score</em> as the  response variable  and <em>Study Hours</em> and <em>Current Grade</em> as the predictor variables.
To do so, click the <b>Data</b> tab along the top ribbon and then click <b>Data Analysis</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option available, you need to first  load the Data Analysis ToolPak .
In the window that pops up, select <b>Regression</b>. In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel2.png">
Once you click <b>OK</b>, the output of the regression model will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel3.png">
<h3>Step 3: Interpret the Standard Error of Regression</h3>
The standard error of the regression model is the number next to <b>Standard Error</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel4.png">
The standard error of this particular regression model turns out to be <b>2.790029</b>.
This number represents the average distance between the actual exam scores and the exam scores predicted by the model.
Note that some of the exam scores will be further than 2.79 units away from the predicted score while some will be closer. But, on average, the distance between the actual exam scores and the predicted scores is <b>2.790029</b>.
Also note that a smaller standard error of regression indicates that a regression model fits a dataset more closely.
Thus, if we fit a new regression model to the dataset and ended up with a standard error of, say, <b>4.53</b>, this new model would be worse at predicting exam scores than the previous model.
<h2><span class="orange">Understanding the Standard Error of a Regression Slope</span></h2>
The <b>standard error of a regression slope</b> is a way to measure the “uncertainty” in the estimate of a regression slope.
It is calculated as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand1.png">
where:
<b>n</b>: total sample size
<b>y<sub>i</sub></b>: actual value of response variable
<b><U+0177><sub>i</sub></b>: predicted value of response variable
<b>x<sub>i</sub></b>: actual value of predictor variable
<b>x<U+0304></b>: mean value of predictor variable
The smaller the standard error, the lower the variability around the coefficient estimate for the regression slope.
The standard error of the regression slope will be displayed in a “standard error” column in the regression output of most statistical software:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand2.png">
The following examples show how to interpret the standard error of a regression slope in two different scenarios.
<h3>Example 1: Interpreting a Small Standard Error of a Regression Slope</h3>
Suppose a professor wants to understand the relationship between the number of hours studied and the final exam score received for students in his class.
He collects data for 25 students and creates the following scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand3.png">
There is a clear positive association between the two variables. As hours studied increases, the exam score increases at a fairly predictable rate.
He then fits a simple linear regression model using hours studied as the predictor variable and final exam score as the response variable.
The following table shows the results of the regression:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand4.png">
The coefficient for the predictor variable ‘hours studied’ is 5.487. This tells us that each additional hour studied is associated with an average increase of <b>5.487</b> in exam score.
The standard error is <b>0.419</b>, which is a measure of the variability around this estimate for the regression slope.
We can use this value to calculate the t-statistic for the predictor variable ‘hours studied’:
t-statistic = coefficient estimate / standard error
t-statistic = 5.487 / .419
t-statistic = 13.112
The p-value that corresponds to this test statistic is 0.000, which indicates that ‘hours studied’ has a statistically significant relationship with final exam score.
Since the standard error of the regression slope was small relative to the coefficient estimate of the regression slope, the predictor variable was statistically significant.
<h3>Example 2: Interpreting a Large Standard Error of a Regression Slope</h3>
Suppose a different professor wants to understand the relationship between the number of hours studied and the final exam score received for students in her class.
She collects data for 25 students and creates the following scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand5.png">
There appears to be slight positive association between the two variables. As hours studied increases, the exam score generally increases but not at a predictable rate.
Suppose the professor then fits a simple linear regression model using hours studied as the predictor variable and final exam score as the response variable.
The following table shows the results of the regression:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/stand6.png">
The coefficient for the predictor variable ‘hours studied’ is 1.7919. This tells us that each additional hour studied is associated with an average increase of <b>1.7919</b> in exam score.
The standard error is <b>1.0675</b>, which is a measure of the variability around this estimate for the regression slope.
We can use this value to calculate the t-statistic for the predictor variable ‘hours studied’:
t-statistic = coefficient estimate / standard error
t-statistic = 1.7919 / 1.0675
t-statistic = 1.678
The p-value that corresponds to this test statistic is 0.107. Since this p-value is not less than .05, this indicates that ‘hours studied’ does not have a statistically significant relationship with final exam score.
Since the standard error of the regression slope was large relative to the coefficient estimate of the regression slope, the predictor variable was <em>not</em> statistically significant.
<h2><span class="orange">Standard Error of the Proportion Calculator</span></h2>
The standard error of a sample proportion can be calculated as:
<b>Standard error</b> = √p(1-p) / n
where:
p is the proportion of successes
n is the sample size
To find the standard error of a sample proportion, simply enter the necessary values below and then click the “Calculate” button.
<label for="z"><b>p</b> (proportion of successes)</label>
<input type="number" id="p" value="0.2">
<label for="mean"><b>n</b> (sample size)</label>
<input type="number" id="n" value="23">
<input type="button" id="button" onclick="calc()" value="Calculate">
<div>
Standard error = √p(1-p) / n
<div>
Standard error = √0.2(1-0.2) / 23 = <b>0.083406</b>
<script>
function calc() {
//get input values
var p = document.getElementById('p').value;
var n = document.getElementById('n').value;
//calculate SE
var SE = Math.sqrt(p*(1-p)/n);
//output probabilities
document.getElementById('p1').innerHTML = p;
document.getElementById('p2').innerHTML = p;
document.getElementById('nOut').innerHTML = n;
document.getElementById('SE').innerHTML = SE.toFixed(6);
}
</script>
<h2><span class="orange">Understanding the Standard Error of the Regression</span></h2>
When we fit a  regression model  to a dataset, we’re often interested in how well the regression model “fits” the dataset. Two metrics commonly used to measure goodness-of-fit include  R-squared  (R<sup>2</sup>) and <b>the</b> <b>standard error of the regression</b>, often denoted <em>S</em>.
This tutorial explains how to interpret the standard error of the regression (S) as well as why it may provide more useful information than R<sup>2</sup>.
<h2>Standard Error vs. R-Squared in Regression</h2>
Suppose we have a simple dataset that shows how many hours 12 students studied per day for a month leading up to an important exam along with their exam score:  
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression1.jpg">
If we fit a simple linear regression model to this dataset in Excel, we receive the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression2.jpg">
<b>R-squared</b> is the proportion of the variance in the response variable that can be explained by the predictor variable. In this case, <b>65.76%</b> of the variance in the exam scores can be explained by the number of hours spent studying.
<b>The standard error of the regression</b> is the average distance that the observed values fall from the regression line. In this case, the observed values fall an average of 4.89 units from the regression line.
If we plot the actual data points along with the regression line, we can see this more clearly:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression3.jpg" alt="">
Notice that some observations fall very close to the regression line, while others are not quite as close. But on average, the observed values fall<b> 4.19 units</b> from the regression line.
The standard error of the regression is particularly useful because it can be used to assess the precision of predictions. Roughly 95% of the observation should fall within +/- two standard error of the regression, which is a quick approximation of a 95% prediction interval. 
If we’re interested in making predictions using the regression model, the standard error of the regression can be a more useful metric to know than R-squared because it gives us an idea of how precise our predictions will be in terms of units.
To illustrate why the standard error of the regression can be a more useful metric in assessing the “fit” of a model, consider another example dataset that shows how many hours 12 students studied per day for a month leading up to an important exam along with their exam score: 
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression4.jpg" alt="">
Notice that this is the exact same dataset as before, <b>except all of the values are cut in half</b>. Thus, the students in this dataset studied for exactly half as long as the students in the previous dataset and received exactly half the exam score.
If we fit a simple linear regression model to this dataset in Excel, we receive the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression5.jpg">
Notice that the R-squared of <b>65.76%</b> is the exact same as the previous example.
However, the standard error of the regression is <b>2.095</b>, which is exactly half as large as the standard error of the regression in the previous example. 
If we plot the actual data points along with the regression line, we can see this more clearly:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/standardErrorRegression6.jpg">
Notice how the observations are packed much more closely around the regression line.  On average, the observed values fall<b> 2.095 units</b> from the regression line.
So, even though both regression models have an R-squared of <b>65.76%</b>, we know that the second model would provide more precise predictions because it has a lower standard error of the regression. 
<h2>The Advantages of Using the Standard Error</h2>
The standard error of the regression (S) is often more useful to know than the R-squared of the model because it provides us with actual units. If we’re interested in using a regression model to produce predictions, S can tell us very easily if a model is precise enough to use for prediction.
For example, suppose we want to produce a 95% prediction interval in which we can predict exam scores within 6 points of the actual score.
Our first model has an R-squared of 65.76%, but this doesn’t tell us anything about how precise our prediction interval will be. Luckily we also know that the first model has an S of 4.19. This means a 95% prediction interval would be roughly 2*4.19 = +/- 8.38 units wide, which is too wide for our prediction interval.
Our second model also has an R-squared of 65.76%, but again this doesn’t tell us anything about how precise our prediction interval will be. However, we know that the second model has an S of 2.095. This means a 95% prediction interval would be roughly 2*2.095= +/- 4.19 units wide, which is less than 6 and thus sufficiently precise to use for producing prediction intervals.
<b>Further Reading</b>
 Introduction to Simple Linear Regression 
 What is a Good R-squared Value? 
<h2><span class="orange">Standardization vs. Normalization: What’s the Difference?</span></h2>
<b>Standardization</b> and <b>normalization</b> are two ways to rescale data.
<b>Standardization</b> rescales a dataset to have a mean of 0 and a standard deviation of 1. It uses the following formula to do so:
<b>x<sub>new</sub> = (x<sub>i</sub> – x) / s</b>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in the dataset
<b>x</b>: The sample mean
<b>s</b>: The sample standard deviation
<b>Normalization</b> rescales a dataset so that each value falls between 0 and 1. It uses the following formula to do so:
<b>x<sub>new</sub> = (x<sub>i</sub> – x<sub>min</sub>) / (x<sub>max</sub> – x<sub>min</sub>)</b>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in the dataset
<b>x<sub>min</sub></b>: The minimum value in the dataset
<b>x<sub>max</sub></b>: The maximum value in the dataset
The following examples show how to standardize and normalize a dataset in practice.
<h3>Example: How to Standardize Data</h3>
Suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalize01.png">
The mean value in the dataset is 43.15 and the standard deviation is 22.13.
To normalize the first value of <b>13</b>, we would apply the formula shared earlier:
<b>x<sub>new</sub> = (x<sub>i</sub> – x) / s </b> = (13 – 43.15) / 22.13 = <b>-1.36</b>
To normalize the second value of <b>16</b>, we would use the same formula:
<b>x<sub>new</sub> = (x<sub>i</sub> – x) / s </b> = (16 – 43.15) / 22.13 = <b>-1.23</b>
To normalize the third value of<b>19</b>, we would use the same formula:
<b>x<sub>new</sub> = (x<sub>i</sub> – x) / s </b> = (19 – 43.15) / 22.13 = <b>-1.09</b>
We can use this exact same formula to standardize each value in the original dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/standardize0.png">
<h3>Example: How to Normalize Data</h3>
Once again, suppose we have the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalize01.png">
The minimum value in the dataset is 13 and the maximum value is 71.
To normalize the first value of <b>13</b>, we would apply the formula shared earlier:
<b>x<sub>new</sub> = (x<sub>i</sub> – x<sub>min</sub>) / (x<sub>max</sub> – x<sub>min</sub>) </b> = (13 – 13) / (71 – 13) = <b>0</b>
To normalize the second value of <b>16</b>, we would use the same formula:
<b>x<sub>new</sub> = (x<sub>i</sub> – x<sub>min</sub>) / (x<sub>max</sub> – x<sub>min</sub>)</b> = (16 – 13) / (71 – 13) = <b>.0517</b>
To normalize the third value of<b>19</b>, we would use the same formula:
<b>x<sub>new</sub> = (x<sub>i</sub> – x<sub>min</sub>) / (x<sub>max</sub> – x<sub>min</sub>)</b>= (19 – 13) / (71 – 13) = <b>.1034</b>
We can use this exact same formula to normalize each value in the original dataset to be between 0 and 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/normalize01-1.png">
<h3>Standardization vs. Normalization: When to Use Each</h3>
Typically we <b>normalize</b> data when performing some type of analysis in which we have multiple variables that are measured on different scales and we want each of the variables to have the same range.
This prevents one variable from being overly influential, especially if it’s measured in different units (i.e. if one variable is measured in inches and another is measured in yards).
On the other hand, we typically <b>standardize</b> data when we’d like to know how many standard deviations each value in a dataset lies from the mean.
For example, we might have a list of exam scores for 500 students at a particular school and we’d like to know how many standard deviations each exam score lies from the mean score.
In this case, we could standardize the raw data to find out this information. Then, a standardized score of 1.26 would tell us that the exam score of that particular student lies 1.26 standard deviations above the mean exam score.
Whether you decide to normalize or standardize your data, keep the following in mind:
A <b>normalized dataset</b> will always have values that range between 0 and 1.
A <b>standardized dataset</b> will have a mean of 0 and standard deviation of 1, but there is no specific upper or lower bound for the maximum and minimum values.
Depending on your particular scenario, it may make more sense to normalize or standardize the data.
<h2><span class="orange">How to Standardize Data in R (With Examples)</span></h2>
To <b>standardize</b> a dataset means to scale all of the values in the dataset such that the mean value is 0 and the standard deviation is 1.
The most common way to do this is by using the z-score standardization, which scales values using the following formula:
<b>(x<sub>i</sub> – x) / s</b>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in the dataset
<b>x</b>: The sample mean
<b>s</b>: The sample standard deviation
The following examples show how to use the  scale()  function in unison with the <b>dplyr</b> package in R to scale one or more variables in a data frame using the z-score standardization.
<h3>Standardize a Single Variable</h3>
The following code shows how to scale just one variable in a data frame with three variables:
<b>library(dplyr)
#make this example reproducible 
set.seed(1)
#create original data frame
df &lt;- data.frame(var1= runif(10, 0, 50),  var2= runif(10, 2, 23), var3= runif(10, 5, 38))
#view original data frame 
df
        var1      var2      var3
1  13.275433  6.325466 35.845273
2  18.606195  5.707692 12.000703
3  28.642668 16.427480 26.505234
4  45.410389 10.066178  9.143318
5  10.084097 18.166670 13.818282
6  44.919484 12.451684 17.741765
7  47.233763 17.069989  5.441881
8  33.039890 22.830028 17.618803
9  31.455702  9.980739 33.699798
10  3.089314 18.326350 16.231517
#scale <em>var1</em> to have mean = 0 and standard deviation = 1
df2 &lt;- df %>% mutate_at(c('var1'), ~(scale(.) %>% as.vector))
df2
          var1      var2      var3
1  -0.90606801  6.325466 35.845273
2  -0.56830963  5.707692 12.000703
3   0.06760377 16.427480 26.505234
4   1.13001072 10.066178  9.143318
5  -1.10827188 18.166670 13.818282
6   1.09890684 12.451684 17.741765
7   1.24554014 17.069989  5.441881
8   0.34621281 22.830028 17.618803
9   0.24583830  9.980739 33.699798
10 -1.55146305 18.326350 16.231517
</b>
Notice that just the first variable was scaled while the other two variables remained the same. We can quickly confirm that the new scaled variable has a mean value of 0 and a standard deviation of 1:
<b>#calculate mean of scaled variable
mean(df2$var1)
[1] -4.18502e-18 #basically zero
#calculate standard deviation of scaled variable 
sd(df2$var1)
[1] 1</b>
<h3>Standardize Multiple Variables</h3>
The following code shows how to scale several variables in a data frame at once:
<b>library(dplyr)
#make this example reproducible 
set.seed(1)
#create original data frame
df &lt;- data.frame(var1= runif(10, 0, 50),  var2= runif(10, 2, 23), var3= runif(10, 5, 38))
#scale <em>var1</em> and <em>var2</em> to have mean = 0 and standard deviation = 1
df3 &lt;- df %>% mutate_at(c('var1', 'var2'), ~(scale(.) %>% as.vector))
df3
          var1       var2      var3
1  -0.90606801 -1.3045574 35.845273
2  -0.56830963 -1.4133223 12.000703
3   0.06760377  0.4739961 26.505234
4   1.13001072 -0.6459703  9.143318
5  -1.10827188  0.7801967 13.818282
6   1.09890684 -0.2259798 17.741765
7   1.24554014  0.5871157  5.441881
8   0.34621281  1.6012242 17.618803
9   0.24583830 -0.6610127 33.699798
10 -1.55146305  0.8083098 16.231517</b>
<h3>Standardize All Variables</h3>
The following code shows how to scale <em>all</em> variables in a data frame using the <b>mutate_all</b> function:
<b>library(dplyr)
#make this example reproducible 
set.seed(1)
#create original data frame
df &lt;- data.frame(var1= runif(10, 0, 50),  var2= runif(10, 2, 23), var3= runif(10, 5, 38))
#scale all variables to have mean = 0 and standard deviation = 1
df4 &lt;- df %>% mutate_all(~(scale(.) %>% as.vector))
df4
          var1       var2       var3
1  -0.90606801 -1.3045574  1.6819976
2  -0.56830963 -1.4133223 -0.6715858
3   0.06760377  0.4739961  0.7600871
4   1.13001072 -0.6459703 -0.9536246
5  -1.10827188  0.7801967 -0.4921813
6   1.09890684 -0.2259798 -0.1049130
7   1.24554014  0.5871157 -1.3189757
8   0.34621281  1.6012242 -0.1170501
9   0.24583830 -0.6610127  1.4702281
10 -1.55146305  0.8083098 -0.2539824</b>
<h2><span class="orange">How to Standardize Data in Python (With Examples)</span></h2>
To <b>standardize</b> a dataset means to scale all of the values in the dataset such that the mean value is 0 and the standard deviation is 1.
We use the following formula to standardize the values in a dataset:
<b>x<sub>new</sub> = (x<sub>i</sub> – x) / s</b>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> value in the dataset
<b>x</b>: The sample mean
<b>s</b>: The sample standard deviation
We can use the following syntax to quickly standardize all of the columns of a pandas DataFrame in Python:
<b>(df-df.mean())/df.std()
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Standardize All Columns of DataFrame</h3>
The following code shows how to standardize all columns in a pandas DataFrame:
<b>import pandas as pd
#create data frame
df = pd.DataFrame({'y': [8, 12, 15, 14, 19, 23, 25, 29],   'x1': [5, 7, 7, 9, 12, 9, 9, 4],   'x2': [11, 8, 10, 6, 6, 5, 9, 12],   'x3': [2, 2, 3, 2, 5, 5, 7, 9]})
#view data frame
df
yx1x2x3
085112
112782
2157103
314962
4191265
523955
625997
7294129
#standardize the values in each column
df_new = (df-df.mean())/df.std()
#view new data frame
df_new
        y       x1       x2       x3
0-1.418032-1.078639 1.025393-0.908151
1-0.857822-0.294174-0.146485-0.908151
2-0.437664-0.294174 0.634767-0.525772
3-0.577717 0.490290-0.927736-0.908151
4 0.122546 1.666987-0.927736 0.238987
5 0.682756 0.490290-1.318362 0.238987
6 0.962861 0.490290 0.244141 1.003746
7 1.523071-1.470871 1.416019 1.768505</b>
We can verify that the mean and standard deviation of each column is equal to 0 and 1, respectively:
<b>#view mean of each column
df_new.mean()
y     0.000000e+00
x1    2.775558e-17
x2   -4.163336e-17
x3    5.551115e-17
dtype: float64
#view standard deviation of each column
df_new.std()
y     1.0
x1    1.0
x2    1.0
x3    1.0
dtype: float64
</b>
<h3>Example 2: Standardize Specific Columns of DataFrame</h3>
Sometimes you may only want to standardize specific columns in a DataFrame.
For example, for many  machine learning algorithms  you may only want to standardize the predictor variables before fitting a certain model to the data.
The following code shows how to standardize specific columns in a pandas DataFrame:
<b>import pandas as pd
#create data frame
df = pd.DataFrame({'y': [8, 12, 15, 14, 19, 23, 25, 29],   'x1': [5, 7, 7, 9, 12, 9, 9, 4],   'x2': [11, 8, 10, 6, 6, 5, 9, 12],   'x3': [2, 2, 3, 2, 5, 5, 7, 9]})
#view data frame
df
yx1x2x3
085112
112782
2157103
314962
4191265
523955
625997
7294129
#define predictor variable columns
df_x = df[['x1', 'x2', 'x3']]
#standardize the values for each predictor variable
df[['x1', 'x2', 'x3']] = (df_x-df_x.mean())/df_x.std()
#view new data frame
df
         y       x1       x2       x3
08-1.078639 1.025393-0.908151
112-0.294174-0.146485-0.908151
215-0.294174 0.634767-0.525772
314 0.490290-0.927736-0.908151
419 1.666987-0.927736 0.238987
523 0.490290-1.318362 0.238987
625 0.490290 0.244141 1.003746
729-1.470871 1.416019 1.768505</b>
Notice that the “y” column remains unchanged, but the columns “x1”, “x2”, and “x3” are all standardized.
We can verify that the mean and standard deviation of each predictor variable column is equal to 0 and 1, respectively:
<b>#view mean of each predictor variable column
df[['x1', 'x2', 'x3']].mean()
x1    2.775558e-17
x2   -4.163336e-17
x3    5.551115e-17
dtype: float64
#view standard deviation of each predictor variable column
df[['x1', 'x2', 'x3']].std()
x1    1.0
x2    1.0
x3    1.0
dtype: float64</b>
<h2><span class="orange">Standardized Residuals Calculator</span></h2>
A <b> standardized residual </b> is a residual that has been divided by its standard deviation. It is calculated as:
<b>r<sub>i</sub> = e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
where:
e<sub>i</sub>: The i<sup>th</sup> residual
RSE: The residual standard error of the model
h<sub>ii</sub>: The leverage of the i<sup>th</sup> observation
This calculator finds the standardized residuals for each observation in a simple linear regression model.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">8, 12, 12, 13, 14, 16, 17, 22, 24, 26, 29, 30</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">41, 42, 39, 37, 35, 39, 45, 46, 39, 49, 55, 57</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Linear Regression Equation:</b>
<U+0177> = 29.6309 + (0.7553)*x
<b>List of Standardized Residuals:</b>
-0.143
-3.104
1.896
-0.064
1.975
-0.906
1.133
-0.787
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope;
var b = lr.intercept;
//calculate residuals
residuals = [];
for (var obs = 0; obs < y.length; obs++) {
this_resid = (y[obs] - (b - (-1*a*x[obs]))).toFixed(3);
residuals.push(this_resid);
}
//calculate leverage
lev_n = x.length;
lev_mean = math.mean(x);
lev_ss = 0;
 for (var i = 0; i < x.length; i++) {
   lev_ss += (x[i]-lev_mean)*(x[i]-lev_mean);
} 
leverages = [];
for (var j = 0; j < x.length; j++) {
this_leverage = 1/lev_n-(-1*(Math.pow(x[j] -lev_mean, 2)))/lev_ss;
leverages.push(this_leverage);
}
//calculate RSE
var xbar = math.mean(x);
var ybar = math.mean(y);
let xbar2_hold = 0
for (let i = 0; i < x.length; i++) {
xbar2_hold += Math.pow(x[i], 2);
}
var xbar2 = xbar2_hold / x.length;
let sxx = 0
for (let i = 0; i < x.length; i++) {
sxx += Math.pow(x[i] - xbar, 2);
}
let syy = 0
for (let i = 0; i < y.length; i++) {
syy += Math.pow(y[i] - ybar, 2);
}
let sxy = 0
for (let i = 0; i < x.length; i++) {
sxy += (x[i] - xbar)*(y[i]-ybar);
}
let sxx2 = 0
for (let i = 0; i < x.length; i++) {
sxx2 += (x[i] - xbar)*(Math.pow(x[i], 2)-xbar2);
}
let sx2x2 = 0
for (let i = 0; i < x.length; i++) {
sx2x2 += Math.pow((Math.pow(x[i], 2)-xbar2), 2);
}
let sx2y = 0
for (let i = 0; i < x.length; i++) {
sx2y += (Math.pow(x[i], 2)-xbar2)*(y[i]-ybar);
}
var sst = syy;
var ssr = (sxy/sxx)*sxy;
var sse = sst-ssr;
var RSE = Math.sqrt(sse / (x.length - 2));
//calculate standardized residuals
stand_resids = [];
for (var k = 0; k < x.length; k++) {
this_sr = (residuals[k]/(RSE*Math.sqrt(1-leverages[k]))).toFixed(4);
stand_resids.push(this_sr);
}
//output results
document.getElementById('a').innerHTML = a.toFixed(4);
document.getElementById('b').innerHTML = b.toFixed(4);
document.getElementById('resids_out').innerHTML = stand_resids.toString().split(',').join("<br />");
}
//output error message if both lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate Standardized Residuals in Excel</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in a  regression model .
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
If we plot the observed values and overlay the fitted regression line, the residuals for each  observation  would be the vertical distance between the observation and the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals1-1.png">
One type of residual we often use to identify outliers in a regression model is known as a <b>standardized residual</b>.
It is calculated as:
<b>r<sub>i</sub>  =  e<sub>i</sub> / s(e<sub>i</sub>)</b>  =  <b>e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
where:
<b>e<sub>i</sub>:</b> The i<sup>th</sup> residual
<b>RSE:</b> The residual standard error of the model
<b>h<sub>ii</sub></b>: The leverage of the i<sup>th</sup> observation
In practice, we often consider any standardized residual with an absolute value greater than 3 to be an outlier.
This tutorial provides a step-by-step example of how to calculate standardized residuals in Excel.
<h3>Step 1: Enter the Data</h3>
First, we’ll enter the values for a small dataset into Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred1.png">
<h3>Step 2: Calculate the Residuals</h3>
Next, we’ll go to the <b>Data</b> tab along the top ribbon and click <b>Data Analysis</b> within the <b>Analysis</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
<em>If you haven’t installed this Add-in already, check out  this tutorial  on how to do so. It’s easy to install and completely free.</em>
Once you’ve clicked Data Analysis, click the option that says <b>Regression</b> and then click <b>OK</b>. In the new window that pops up, fill in the following information and click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred2-2.png">
The residual for each observation will appear in the output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred3.png">
Copy and paste these residuals in a new column next to the original data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred4.png">
<h3>Step 3: Calculate the Leverage</h3>
Next, we need to calculate the leverage of each observation.
The following image shows how to do so:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred5.png">
Here are the formulas used in the various cells:
<b>B14:</b> =COUNT(B2:B13)
<b>B15:</b> =AVERAGE(B2:B13)
<b>B16:</b> =DEVSQ(B2:B13) 
<b>E2:</b> =1/$B$14+(B2-$B$15)^2/$B$16
<h3>Step 4: Calculate the Standardized Residuals</h3>
Lastly, we can calculate the standardized residuals using the formula:
<b>r<sub>i</sub>  =  e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
The <b>RSE</b> for the model can be found in the model output from earlier. It turns out to be <b>4.44</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred6.png">
Thus, we can use the following formula to calculate the standardized residual for each observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sred7.png">
From the results we can see that none of the standardized residuals exceed an absolute value of 3. Thus, none of the observations appear to be outliers.
It’s worth noting in some cases that researchers consider observations with standardized residuals that exceed an absolute value of 2 to be considered outliers.
It’s up to you to decide whether to use an absolute value of 2 or 3 as the threshold for outliers, depending on the specific problem you’re working on.
<h2><span class="orange">How to Calculate Standardized Residuals in R</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in a  regression model .
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
If we plot the observed values and overlay the fitted regression line, the residuals for each  observation  would be the vertical distance between the observation and the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals1-1.png">
One type of residual we often use to identify outliers in a regression model is known as a <b>standardized residual</b>.
It is calculated as:
<b>r<sub>i</sub>  =  e<sub>i</sub> / s(e<sub>i</sub>)</b>  =  <b>e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
where:
<b>e<sub>i</sub>:</b> The i<sup>th</sup> residual
<b>RSE:</b> The residual standard error of the model
<b>h<sub>ii</sub></b>: The leverage of the i<sup>th</sup> observation
In practice, we often consider any standardized residual with an absolute value greater than 3 to be an outlier.
This tutorial provides a step-by-step example of how to calculate standardized residuals in R.
<h3>Step 1: Enter the Data</h3>
First, we’ll create a small dataset to work with in R:
<b>#create data
data &lt;- data.frame(x=c(8, 12, 12, 13, 14, 16, 17, 22, 24, 26, 29, 30),   y=c(41, 42, 39, 37, 35, 39, 45, 46, 39, 49, 55, 57))
#view data
data
    x  y
1   8 41
2  12 42
3  12 39
4  13 37
5  14 35
6  16 39
7  17 45
8  22 46
9  24 39
10 26 49
11 29 55
12 30 57</b>
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll use the <b>lm()</b> function to fit a  simple linear regression model :
<b>#fit model
model &lt;- lm(y ~ x, data=data)
#view model summary
summary(model) 
Call:
lm(formula = y ~ x, data = data)
Residuals:
    Min      1Q  Median      3Q     Max 
-8.7578 -2.5161  0.0292  3.3457  5.3268 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  29.6309     3.6189   8.188  9.6e-06 ***
x             0.7553     0.1821   4.148  0.00199 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 4.442 on 10 degrees of freedom
Multiple R-squared:  0.6324,Adjusted R-squared:  0.5956 
F-statistic:  17.2 on 1 and 10 DF,  p-value: 0.001988</b>
<h3>Step 3: Calculate the Standardized Residuals</h3>
Next, we’ll use the built-in <b>rstandard()</b> function to calculate the standardized residuals of the model:
<b>#calculate the standardized residuals
standard_res &lt;- rstandard(model)
#view the standardized residuals
standard_res
          1           2           3           4           5           6 
 1.40517322  0.81017562  0.07491009 -0.59323342 -1.24820530 -0.64248883 
          7           8           9          10          11          12 
 0.59610905 -0.05876884 -2.11711982 -0.06655600  0.91057211  1.26973888
</b>
We can add the standardized residuals back to the original data frame if we’d like:
<b>#column bind standardized residuals back to original data frame
final_data &lt;- cbind(data, standard_res)
#view data frame
    x  y standard_res
1   8 41   1.40517322
2  12 42   0.81017562
3  12 39   0.07491009
4  13 37  -0.59323342
5  14 35  -1.24820530
6  16 39  -0.64248883
7  17 45   0.59610905
8  22 46  -0.05876884
9  24 39  -2.11711982
10 26 49  -0.06655600
11 29 55   0.91057211
12 30 57   1.26973888
</b>
We can then sort each observation from largest to smallest according to its standardized residual to get an idea of which observations are closest to being outliers:
<b>#sort standardized residuals descending
final_data[order(-standard_res),]
    x  y standard_res
1   8 41   1.40517322
12 30 57   1.26973888
11 29 55   0.91057211
2  12 42   0.81017562
7  17 45   0.59610905
3  12 39   0.07491009
8  22 46  -0.05876884
10 26 49  -0.06655600
4  13 37  -0.59323342
6  16 39  -0.64248883
5  14 35  -1.24820530
9  24 39  -2.11711982</b>
From the results we can see that none of the standardized residuals exceed an absolute value of 3. Thus, none of the observations appear to be outliers.
<h3>Step 4: Visualize the Standardized Residuals</h3>
Lastly, we can create a scatterplot to visualize the values for the predictor variable vs. the standardized residuals:
<b>#plot predictor variable vs. standardized residuals
plot(final_data$x, standard_res, ylab='Standardized Residuals', xlab='x') 
#add horizontal line at 0
abline(0, 0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/standresR1.png">
<h2><span class="orange">How to Calculate Standardized Residuals in Python</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in a  regression model .
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
If we plot the observed values and overlay the fitted regression line, the residuals for each  observation  would be the vertical distance between the observation and the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals1-1.png">
One type of residual we often use to identify outliers in a regression model is known as a <b>standardized residual</b>.
It is calculated as:
<b>r<sub>i</sub>  =  e<sub>i</sub> / s(e<sub>i</sub>)</b>  =  <b>e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
where:
<b>e<sub>i</sub>:</b> The i<sup>th</sup> residual
<b>RSE:</b> The residual standard error of the model
<b>h<sub>ii</sub></b>: The leverage of the i<sup>th</sup> observation
In practice, we often consider any standardized residual with an absolute value greater than 3 to be an outlier.
This tutorial provides a step-by-step example of how to calculate standardized residuals in Python.
<h3>Step 1: Enter the Data</h3>
First, we’ll create a small dataset to work with in Python:
<b>import pandas as pd
#create dataset
df = pd.DataFrame({'x': [8, 12, 12, 13, 14, 16, 17, 22, 24, 26, 29, 30],   'y': [41, 42, 39, 37, 35, 39, 45, 46, 39, 49, 55, 57]})
</b>
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll fit a  simple linear regression model :
<b>import statsmodels.api as sm
#define response variable
y = df['y']
#define explanatory variable
x = df['x']
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit() </b>
<h3>Step 3: Calculate the Standardized Residuals</h3>
Next, we’ll calculate the standardized residuals of the model:
<b>#create instance of influence
influence = model.get_influence()
#obtain standardized residuals
standardized_residuals = influence.resid_studentized_internal
#display standardized residuals
print(standardized_residuals)
[ 1.40517322  0.81017562  0.07491009 -0.59323342 -1.2482053  -0.64248883
  0.59610905 -0.05876884 -2.11711982 -0.066556    0.91057211  1.26973888]</b>
From the results we can see that none of the standardized residuals exceed an absolute value of 3. Thus, none of the observations appear to be outliers.
<h3>Step 4: Visualize the Standardized Residuals</h3>
Lastly, we can create a scatterplot to visualize the values for the predictor variable vs. the standardized residuals:
<b>import matplotlib.pyplot as plt
plt.scatter(df.x, standardized_residuals)
plt.xlabel('x')
plt.ylabel('Standardized Residuals')
plt.axhline(y=0, color='black', linestyle='--', linewidth=1)
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/standresPython1.png">
<h2><span class="orange">What Are Standardized Residuals?</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in a  regression model .
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
If we plot the observed values and overlay the fitted regression line, the residuals for each  observation  would be the vertical distance between the observation and the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals1-1.png">
One type of residual we often use to identify outliers in a regression model is known as a <b>standardized residual</b>.
It is calculated as:
<b>r<sub>i</sub>  =  e<sub>i</sub> / s(e<sub>i</sub>)</b>  =  <b>e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
where:
<b>e<sub>i</sub>:</b> The i<sup>th</sup> residual
<b>RSE:</b> The residual standard error of the model
<b>h<sub>ii</sub></b>: The leverage of the i<sup>th</sup> observation
In practice, we often consider any standardized residual with an absolute value greater than 3 to be an outlier.
This doesn’t necessarily mean that we’ll remove these observations from the model, but we should at least investigate them further to verify that they’re not a result of a data entry error or some other odd occurrence. 
<em><b>Note:</b> Sometimes standardized residuals are also referred to as “internally studentized residuals.”</em>
<h3>Example: How to Calculate Standardized Residuals</h3>
Suppose we have the following dataset with 12 total observations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals3.png">
If we use some statistical software (like  R ,  Excel ,  Python ,  Stata , etc.) to fit a linear regression line to this dataset, we’ll find that the line of best fit turns out to be:
<b>y = 29.63 + 0.7553x</b>
Using this line, we can calculate the predicted value for each Y value based on the value of X. For example, the predicted value of the first observation would be:
y = 29.63 + 0.7553*(8) = <b>35.67</b>
We can then calculate the residual for this observation as:
Residual = Observed value – Predicted value = 41 – 35.67 = <b>5.33</b>
We can repeat this process to find the residual for every single observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals4.png">
We can also use statistical software to find that the residual standard error of the model is <b>4.44</b>.
And, although it’s beyond the scope of this tutorial, we can use software to find the leverage statistic (h<sub>ii</sub>) for each observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/standRes1.png">
We can then use the following formula to calculate the standardized residual for each observation:
<b>r<sub>i</sub></b>  =   <b>e<sub>i</sub> / RSE√1-h<sub>ii</sub></b>
For example, the standardized residual for the first observation is calculated as:
<b>r<sub>i</sub>   =  5.33 / 4.44√1-.27 = 1.404</b>
We can repeat this process to find the standardized residual for each observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/standRes2.png">
We can then create a quick scatterplot of the predictor values vs. standardized residuals to visually see if any of the standardized residuals exceed an absolute value threshold of 3:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/standRes3.png">
From the plot we can see that none of the standardized residuals exceed an absolute value of 3. Thus, none of the observations appear to be outliers.
It’s worth noting in some cases that researchers consider observations with standardized residuals that exceed an absolute value of 2 to be considered outliers.
It’s up to you to decide, depending on the field you’re working in and the specific problem you’re working on, whether to use an absolute value of 2 or 3 as the threshold for outliers.
<h2><span class="orange">What is a Standardized Test Statistic?</span></h2>
A statistical hypothesis is an assumption about a  population parameter . For example, we may assume that the mean height of a male in the U.S. is 70 inches. The assumption about the height is the<em> statistical hypothesis</em> and the true mean height of a male in the U.S. is the<em> population parameter</em>.
A  hypothesis test  is a formal statistical test we use to reject or fail to reject some statistical hypothesis.
The basic process for performing a hypothesis test is as follows:
<b>1.</b> Collect sample data.
<b>2.</b> Calculate the standardized test statistic for the sample data.
<b>3.</b> Compare the standardized test statistic to some critical value. If it’s more extreme than the critical value, reject the null hypothesis. Otherwise, fail to reject the null hypothesis test.
The formula that we use to calculate the <b>standardized test statistic </b>varies depending on the type of hypothesis test we perform.
The following table shows the formula to use to calculate the standardized test statistic for each of the four major types of hypothesis tests:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/standardizedTestStatistic1.png">
<h3>Hypothesis Test for One Mean</h3>
A <b>one sample t-test</b> is used to test whether or not the mean of a population is equal to some value.
The standardized test statistic for this type of test is calculated as follows:
<b>t = (x – μ) / (s/√n)</b>
where:
<b>x: </b>sample mean
<b>μ<sub>0</sub>:</b> hypothesized population mean
<b>s: </b>sample standard deviation
<b>n: </b>sample size
Refer to  this tutorial  for an example of how to calculate this standardized test statistic.
<h3>Hypothesis Test for a Difference in Means</h3>
A <b>two sample t-test</b> is used to test whether or not the means of two populations are equal.
The standardized test statistic for this type of test is calculated as follows:
<b>t = (x<sub>1</sub> – x<sub>2</sub>)  /  s<sub>p</sub>(√1/n<sub>1</sub> + 1/n<sub>2</sub>)</b>
where x<sub>1</sub> and x<sub>2</sub> are the sample means, n<sub>1 </sub>and n<sub>2 </sub>are the sample sizes, and where s<sub>p</sub> is calculated as:
<b>s<sub>p</sub></b> = √ (n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> /  (n<sub>1</sub>+n<sub>2</sub>-2)
where s<sub>1</sub><sup>2</sup> and s<sub>2</sub><sup>2</sup> are the sample variances.
Refer to  this tutorial  for an example of how to calculate this standardized test statistic.
<h3>Hypothesis Test for One Proportion</h3>
A <b>one proportion z-test</b> is used to compare an observed proportion to a theoretical one.
The standardized test statistic for this type of test is calculated as follows:
<b>z = (p-p<sub>0</sub>) / √p<sub>0</sub>(1-p<sub>0</sub>)/n</b>
where:
<b>p: </b>observed sample proportion
<b>p<sub>0</sub>:</b> hypothesized population proportion
<b>n: </b>sample size
Refer to  this tutorial  for an example of how to calculate this standardized test statistic.
<h3>Hypothesis Test for a Difference in Proportions</h3>
A <b>two proportion z-test</b> is used to test for a difference between two population proportions.
The standardized test statistic for this type of test is calculated as follows:
<b><b>z </b>= (p<sub>1</sub>-p<sub>2</sub>) / √p(1-p)(1/n<sub>1</sub>+1/n<sub>2</sub>)</b>
where p<sub>1</sub> and p<sub>2</sub> are the sample proportions, n<sub>1 </sub>and n<sub>2 </sub>are the sample sizes, and where p is the total pooled proportion calculated as:
p = (p<sub>1</sub>n<sub>1</sub> + p<sub>2</sub>n<sub>2</sub>)/(n<sub>1</sub>+n<sub>2</sub>)
Refer to  this tutorial  for an example of how to calculate this standardized test statistic.
<h2><span class="orange">Standardized vs. Unstandardized Regression Coefficients</span></h2>
<b>Multiple linear regression</b> is a useful way to quantify the relationship between two or more predictor variables and a  response variable .
Typically when we perform multiple linear regression, the resulting regression coefficients are <b>unstandardized</b>, meaning they use the raw data to find the line of best fit.
However, when the predictor variables are measured on drastically different scales it can be useful to perform multiple linear regression using standardized data, which results in <b>standardized</b> coefficients.
To help you wrap your head around this idea, let’s walk through a simple example.
<h3>Example: Standardized vs. Unstandardized Regression Coefficients</h3>
Suppose we have the following dataset that contains information about the age, square footage, and selling price of 12 houses:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/standardized1.png">
Suppose we then perform multiple linear regression, using <b>age</b> and <b>square footage</b> as the predictor variables and price as the <b>response variable</b>. Here is the  regression output :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/standardized2.png">
The regression coefficients in this table are <b>unstandardized</b>, meaning they used the raw data to fit this regression model. Upon first glance, it appears that <b>age </b>has a much larger effect on house price since it’s coefficient in the regression table is <b>-409.833</b> compared to just <b>100.866 </b>for the predictor variable <b>square footage</b>. 
However, the standard error is much larger for age compared to square footage, which is why the corresponding p-value is actually large for age (p=0.520) and small for square footage (p=0.000).
 The reason for the extreme differences in regression coefficients is because of the extreme differences in scales for the two variables:
The values for <b>age </b>range from 4 to 44.
The values for <b>square footage </b>range from 1,200 to 2,800.
Suppose we instead <b>standardize </b>the original raw data by converting each original data value to a z-score:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/standardized3.png">
If we then perform multiple linear regression using the standardized data, we’ll get the following regression output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/standardized4.png">
The regression coefficients in this table are <b>standardized</b>, meaning they used standardized data to fit this regression model. The way to interpret the coefficients in the table is as follows:
A one standard deviation increase in <b>age </b>is associated with a <b>0.92 </b>standard deviation decrease in house price, assuming square footage is held constant.
A one standard deviation increase in <b>square footage </b>is associated with a <b>0.885 </b>standard deviation increase in house price, assuming age is held constant.
Immediately we can see that square footage has a much larger effect on house price than age. Also note that the p-values for each predictor variable are the exact same as the previous regression model.
<b>Related: </b> How to Calculate Z-Scores in Excel 
<h3>When to Use Standardized vs. Unstandardized Regression Coefficients</h3>
Standardized and unstandardized regression coefficients can both be useful depending on the situation. In particular:
<b>Unstandardized regression coefficients</b> are useful when you want to interpret the effect that a one unit change on a predictor variable has on a response variable. In the example above, we could use the unstandardized regression coefficients from the first regression to understand the exact relationship between the predictor variables and the response variable:
A one unit increase in age was associated with an average <b>$409</b> decrease in house price, assuming square footage was held constant. This coefficient turned out to not be statistically significant (p=0.520).
A one unit increase in square footage was associated with an average <b>$100</b> increase in house price, assuming age was held constant. This coefficient also turned out to be statistically significant (p=0.000).
<b>Standardized regression coefficients</b> are useful when you want to compare the effect that different predictor variables have on a response variable. Since each variable is standardized, you’re able to see which variable has the <em>greatest </em>effect on the response variable.
One downside of standardized regression coefficients is that they’re a bit harder to interpret. For example, it’s easier to understand the effect that a one unit increase in age has on house price compared to the effect that a one standard deviation increase has on house price.
<h2><span class="orange">What is a Stanine Score? (Definition & Examples)</span></h2>
A <b>stanine score</b>, short for “standard nine” score, is a way to scale test scores on a nine-point standard scale.
Using this method, we can convert every test score from the original score (i.e. 0 to 100) to a number between 1 and 9.
We use a simple two-step process to scale test scores to stanine scores:
<b>1. </b>Rank each test score from lowest to highest.
<b>2.</b> Give the lowest 4% of scores a stanine score of 1, the next lowest 7% of scores a stanine score of 2, and so on according to the following table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/stanine1.png">
In general, we regard test scores as follows:
<b>Stanines 1, 2, 3:</b> Below average
<b>Stanines 4, 5, 6:</b> Average
<b>Stanines 7, 8, 9:</b> Above average
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/stanine2.png">
It turns out that a stanine scale has a mean of five and a standard deviation of two.
<h2>Pros & Cons of Stanine Scores</h2>
Stanine scores offer the follow pros and cons:
<b>Pro:</b> Stanine scores allow us to gain a quick understanding of where a given test score lies relative to all other test scores.
For example, we know that a student who receives a test score in stanine 5 belongs to the middle 20% of all test scores. And we know that a student who falls in stanine 9 received a test score in the top 4% of all scores.
<b>Con:</b> The drawback of using stanines is that each stanine is not equally sized and a test score in a given stanine could be closer to scores in the next stanine compared to scores within its own stanine.
For example, students who receive a score in the 40th through 60th percentile are all grouped together in stanine 5. However, a student whose test score falls in the 58th percentile would be closer to the scores received in stanine 6 compared to most of the scores received in stanine 5.
<h2>Alternatives to Stanine Scores</h2>
Two alternatives to stanine scores are percentiles and z-scores.
<b>1. </b>A <b>percentile</b> tells us the percentage of all scores that a given test score lies above.
For example, a test score at the 90th percentile is higher than 90% of all test scores. A test score that falls at the 50th percentile is exactly in the middle of all test scores.
<b>2. </b>A <b>z-score</b> tells us how many standard deviations a given score is from the mean. It is calculated as:
<b>z</b> = (X – μ) / σ
where:
X is a single raw data value
μ is the mean of the dataset
σ is the standard deviation of the dataset
We interpret z-scores as follows:
A positive z-score indicates that a test score is <em>above</em> the mean
A negative z-score indicates that a test score is <em>below</em> the mean
A z-score equal to zero indicates a test score that is exactly <em>equal to</em> the mean
The further away a z-score is from zero, the further a given test score is from the mean.
Both z-scores and percentiles give us a more precise idea of where certain test scores rank compared to stanine scores.
<h2><span class="orange">Statistic vs. Parameter: What’s the Difference?</span></h2>
There are two important terms in the field of  inferential statistics  that you should know the difference between: <b>statistic </b>and <b>parameter</b>.
This article provides the definition for each term along with a real-world example and several practice problems to help you better understand the difference between the two terms.
<h2>Statistic vs. Parameter: Definitions</h2>
A <b>statistic </b>is a number that describes some characteristic of a sample.
A <b>parameter </b>is a number that describes some characteristic of a population.
Recall that a  population  represents every possible individual element that you’re interested in measuring, while a sample simply represents a portion of the population.
For example, you may be interested in identifying the mean height of palm trees in Florida. There might be tens of thousands of palm trees around the state, which means it would be virtually impossible to go around and measure the height of every single one.
Instead, you may select  a random sample  of 100 palm trees and find the mean height of the trees in just that sample. Suppose the mean turns out to be 36 feet.
In this example, the population is every palm tree in Florida. The sample is the group of 100 trees that we randomly selected.
The <b>statistic </b>is the mean height of the trees in our sample – 36 feet.
The <b>parameter </b>is the true mean height of <em>all </em>palm trees in Florida, which is unknown since we will never be able to measure every single palm tree in the state.
The parameter is the value that we’re actually interested in measuring, but the statistic is the value that we use to estimate the value of the parameter since the statistic is so much easier to obtain.
<h2>Commonly Used Statistics and Parameters</h2>
In the previous example, we were interested in measuring the  population mean , but there are many other population parameters that we might be interested in measuring. 
The following table shows a list of common parameters we might be interested in measuring, along with its corresponding sample statistic.
<em>Note that we write parameters and statistics using different symbols.</em>
<table><tbody>
<tr>
<th style="text-align: left;"><b>Measurement</b></th>
<th><b>Sample statistic</b></th>
<th><b>Population parameter</b></th>
</tr>
<tr>
<td style="text-align: left;">Mean</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">μ (mu)</td>
</tr>
<tr>
<td style="text-align: left;">Standard deviation</td>
<td style="text-align: center;">s</td>
<td style="text-align: center;">σ (sigma)</td>
</tr>
<tr>
<td style="text-align: left;">Variance</td>
<td style="text-align: center;">s<sup>2</sup></td>
<td style="text-align: center;">σ<sup>2</sup> (sigma squared)</td>
</tr>
<tr>
<td style="text-align: left;">Proportion</td>
<td style="text-align: center;">p</td>
<td style="text-align: center;">π (pi)</td>
</tr>
<tr>
<td style="text-align: left;">Correlation</td>
<td style="text-align: center;">r</td>
<td style="text-align: center;">ρ (rho)</td>
</tr>
<tr>
<td style="text-align: left;">Regression coefficient</td>
<td style="text-align: center;">b</td>
<td style="text-align: center;">β (beta)</td>
</tr>
</tbody></table>
In any problem, we are always interested in measuring the population parameter. However, it’s often too time-consuming, too costly, or simply not possible to actually measure every single individual element in the population, which is why we instead calculate a sample statistic and use that statistic to estimate the true population parameter.
<b>Nerd notes:</b>
 
To ensure that our sample statistic is a good estimate for the true population parameter, we need to make sure that we obtain a representative sample – a sample in which the characteristics of the individuals closely match the characteristics of the overall population.
 
Read more about how to obtain a representative sample using various sampling methods in  this post .
<h2>Statistic vs. Parameter: Practice Problems</h2>
The following practice problems will help you gain a better understanding of the difference between statistics and parameters.
First, read the problem. Then, try to identify the statistic and the parameter in each problem. The correct answer will be listed below each problem so that you can check your work.
<h3>Problem #1</h3>
A researcher would like to find the mean wingspan of a certain bird species. She collects a random sample of 50 birds, measures the wingspan of each bird, and finds that the mean wingspan is 15.6 inches.
<b>Answer: </b>The <b>parameter </b>that the researcher is interested in measuring is the mean wingspan for the entire population of this particular bird species. The <b>statistic </b>is the sample mean, which turns out to be 15.6 inches.
<h3>Problem #2</h3>
An election council wants to understand what proportion of adults in a certain city are in favor of a particular tax law. They obtain a random sample of 1,000 adults and find that 34% are in favor of the law.
<b>Answer: </b>The <b>parameter </b>that the council is interested in measuring is the proportion of all adults in the city who are in favor of the tax law. The <b>statistic </b>is the sample proportion, which turns out to be 34%.
<h3>Problem #3</h3>
A team of economists wants to estimate the standard deviation of incomes among adults in a certain country. They obtain a random sample of 10,000 adults and find that the standard deviation among their incomes is $12,500.
<b>Answer: </b>The <b>parameter </b>that the team of economists is interested in measuring is the standard deviation of incomes among all adults in the country. The <b>statistic </b>is the sample standard deviation, which turns out to be $12,500.
<h3>Problem #4</h3>
A researcher wants to estimate the mean coffee consumption of students at a particular university. He obtains a random sample of 200 students and finds that the mean coffee consumption is 2.2 cups per day per student.
<b>Answer: </b>The <b>parameter </b>that the researcher wants to measure is the mean coffee consumption of all students at this university. The <b>statistic </b>is the sample mean, which turns out to be 2.2 cups per day per student.
<h2><span class="orange">A Simple Explanation of Statistical vs. Practical Significance</span></h2>
A <b>statistical hypothesis</b> is an assumption about a <b>population parameter</b>. For example, we may assume that the mean height of a male in a certain county is 68 inches. The assumption about the height is the<em> statistical hypothesis</em> and the true mean height of a male in the U.S. is the<em> population parameter</em>.
A  <b>hypothesis test</b>  is a formal statistical test we use to reject or fail to reject a statistical hypothesis. To perform a hypothesis test, we obtain a random sample from the population and determine if the sample data is likely to have occurred, given that the null hypothesis is indeed true.
If the sample data is sufficiently unlikely under that assumption, then we can reject the null hypothesis and conclude that an effect exists.
The way we determine whether or not the sample data is “sufficiently unlikely” under the assumption that the null is true is to define some significance level (typically chosen to be 0.01, 0.05, or 0.10) and then check to see if the p-value of the hypothesis test is less than that significance level.
If the p-value is less than the significance level, then we say that the results are <b>statistically significant</b>. This simply means that some effect exists, but it does not necessarily mean that the effect is actually practical in the real world. Results can be statistically significant without being <b>practically significant</b>.
<b>Related:</b>  An Explanation of P-Values and Statistical Significance 
<h2>Practical Significance</h2>
It’s possible for hypothesis tests to produce results that are statistically significant, despite having a small effect size. There are two main ways that small effect sizes can produce small (and thus statistically significant) p-values:
<b>1. The variability in the sample data is very low.</b> When your sample data has low variability, a hypothesis test is able to produce more precise estimates of the population’s effect, which allows the test to detect even small effects.
For example, suppose we want to perform an independent two-sample t test on the following two samples that show the test scores of 20 students from two different schools to determine if the mean test scores are significantly different between the schools:
<b>sample 1: 85 85 86 86 85 86 86 86 86 85 85 85 86 85 86 85 86 86 85 86
sample 2: 87 86 87 86 86 86 86 86 87 86 86 87 86 86 87 87 87 86 87 86</b>
The mean for sample 1 is <b>85.55</b> and the mean for sample 2 is <b>86.40</b> . When we perform an independent two-sample t test, it turns out that the test statistic is <b>-5.3065</b> and the corresponding p-value is<b> &lt;.0001</b>. The difference between the test scores is statistically significant.
The difference between the mean test scores for these two samples is only <b>0.85</b>, but the low variability in test scores for each school causes a statistically significant result. Note that the standard deviation for the scores is <b>0.51</b> for sample 1 and <b>0.50</b> for sample 2.
This low variability is what allowed the hypothesis test to detect the tiny difference in scores and allow the differences to be statistically significant.
The underlying reason that low variability can lead to statistically significant conclusions is because the test statistic <em>t </em>for a two sample independent t-test is calculated as:
<b>test statistic <em>t</em></b>  = [ (x<sub>1</sub> – x<sub>2</sub>) – d ]  /  (√s<sup>2</sup><sub>1</sub> / n<sub>1</sub> + s<sup>2</sup><sub>2</sub> / n<sub>2</sub>)
where s<sup>2</sup><sub>1</sub> and s<sup>2</sup><sub>2</sub> indicate the sample variation for sample 1 and sample 2, respectively. Notice that when these two numbers are small, the entire denominator of the test statistic <em>t </em>is small.
And when we divide by a small number, we end up with a large number. This means the test statistic <em>t </em>will be large and the corresponding p-value will be small, thus leading to statistically significant results.
<b>2. The sample size is very large. </b>The larger the sample size, the greater the statistical power of a hypothesis test, which enables it to detect even small effects. This can lead to statistically significant results, despite small effects that may have no practical significance.
For example, suppose we want to perform an independent two-sample t test on the following two samples that show the test scores of 20 students from two different schools to determine if the mean test scores are significantly different between the schools:
<b>Sample 1: 88 89 91 94 87 94 94 92 91 86 87 87 92 89 93 90 92 95 89 93
Sample 2: 95 88 93 87 89 90 86 90 95 89 91 92 91 88 94 93 94 87 93 90</b>
If we create a boxplot for each sample to display the distribution of scores, we can see that they look very similar:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/prac_sig1.jpg">
The mean for sample 1 is <b>90.65 </b>and the mean for sample 2 is <b>90.75</b>. The standard deviation for sample 1 is <b>2.77</b> and the standard deviation for sample 2 is <b>2.78</b>. When we perform an independent two-sample t test, it turns out that the test statistic is <b>-0.113 </b>and the corresponding p-value is<b> 0.91</b>. The difference between the mean test scores is not statistically significant.
However, consider if the sample sizes of the two samples were both <b>200</b>. In this case, an independent two-sample t test would reveal that the test statistic is <b>-1.97</b> and the corresponding p-value is just under <b>0.05</b>. The difference between the mean test scores is statistically significant.
The underlying reason that large sample sizes can lead to statistically significant conclusions once again goes back to the test statistic <em>t </em>for a two sample independent t-test:
<b>test statistic <em>t</em></b>  = [ (x<sub>1</sub> – x<sub>2</sub>) – d ]  /  (√s<sup>2</sup><sub>1</sub> / n<sub>1</sub> + s<sup>2</sup><sub>2</sub> / n<sub>2</sub>)
Notice that when n<sub>1</sub> and n<sub>2</sub> are small, the entire denominator of the test statistic <em>t </em>is small. And when we divide by a small number, we end up with a large number. This means the test statistic <em>t </em>will be large and the corresponding p-value will be small, thus leading to statistically significant results.
<h2>Using Subject Matter Expertise to Assess Practical Significance</h2>
To determine whether a statistically significant result from a hypothesis test is practically significant, subject matter expertise is often needed.
In the previous examples when we were testing for differences between test scores for two schools, it would help to have the expertise of someone who works in schools or who administers these types of tests to help us determine whether or not a mean difference of 1 point has practical implications.
For example, a mean difference of 1 point may be statistically significant at alpha level = 0.05, but does this mean that the school with the lower scores should adopt the curriculum that the school with the higher scores is using? Or would this involve too much administrative cost and be too expensive/timely to implement?
Just because there is a statistically significant difference in test scores between two schools does not mean that the effect size of the difference is big enough to enact some type of change in the education system.
<h2>Using Confidence Intervals to Assess Practical Significance</h2>
Another useful tool for determining practical significance is a  <b>confidence interval</b> . A confidence interval gives us a range of values that the true population parameter is likely to fall in.
For example, let’s go back to the example of comparing the difference in test scores between two schools. A principal may declare that a mean difference in scores of at least 5 points is needed in order for the school to adopt a new curriculum.
In one study, we may find that the mean difference in test scores is 8 points. However, the confidence interval around this mean may be [4, 12], which indicates that <em>4 </em>could be the true difference between the mean test scores. In this case, the principal may conclude that the school will not change the curriculum since the confidence interval indicates that the true difference could be less than 5.
However, in another study we may find that the mean difference in test scores is once again 8 points, but the confidence interval around the mean may be [6, 10]. Since this interval does not contain <em>5</em>, the principal will likely conclude that the true difference in test scores is greater than 5 and thus determine that it makes sense to change the curriculum.
<h2>Conclusion</h2>
In closing, here’s what we learned:
<b>Statistical significance</b> only indicates if there is an effect based on some significance level.
<b>Practical significance</b> is whether or not this effect has practical implications in the real world.
We use statistical analyses to determine statistical significance and subject-area expertise to assess practical significance.
Small effect sizes can produce small p-values when (1) the variability in the sample data is very low and when (2) the sample size is very large.
By defining a minimum effect size before we conduct a hypothesis test, we can better assess whether the result of a hypothesis test (even if it’s statistically significant) actually has real world practicality.
 <b>Confidence intervals</b>  can be useful for determining practical significance. If the minimum effect size is not contained within a confidence interval, then the results may be practically significant.
<h2><span class="orange">How to Create a Statistical Process Control Chart in Excel</span></h2>
A <b>statistical process control chart</b> is a type of chart that is used to visualize how a process changes over time and is used to determine whether or not a process remains in a state of control.
The following step-by-step example shows how to create a statistical process control chart in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for our sample data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/spc1.png">
<h3>Step 2: Calculate the Mean</h3>
Next, we can use the following formula to calculate the mean value of the dataset:
<b>=AVERAGE($A$2:$A$21)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/spc2.png">
<h3>Step 3: Calculate the Upper & Lower Limits</h3>
Next, we can use the following formula to calculate the upper and lower limits for the chart:
<b>#Upper limit calculation
=$B$2+3*STDEV.S($A$2:$A$21)
#Lower limit calculation
=$B$2-3*STDEV.S($A$2:$A$21)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/spc3.png">
<h3>Step 4: Create the Statistical Process Control Chart</h3>
Lastly, we can highlight every value in the cell range <b>A1:D21</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Insert Line Chart</b>.
The following statistical process control chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/spc4.png">
Here’s how to interpret the lines in the chart:
<b>Blue line:</b> The raw data
<b>Orange line:</b> The mean value of the data
<b>Grey line:</b> The upper limit on the process
<b>Yellow line:</b> The lower limit of the process
Since the blue line (the raw data) never crosses the upper limit or lower limit on the chart, we would say that the process remained in a state of control for the entire duration of the data collection.
In more technical terms, we would say that the data values never exceeded 3 standard deviations above or below the mean value of the dataset.
<h2><span class="orange">Statistician vs. Data Scientist: What’s the Difference?</span></h2>
<b>Statisticians</b> and <b>data scientists</b> both work heavily with data, but there are some key differences between the two professions:
<b>Difference #1 (Types of Data) –</b> Data scientists tend to spend more time gathering and cleaning imperfect data while statisticians are usually provided with tidy data.
<b>Difference #2 (End Goals)</b> – Data scientists tend to focus on creating models that predict outcomes while statisticians tend to focus on building models that accurately describe the relationship between variables.
<b>Difference #3 (Production)</b> – Data scientists tend to build models that are put into production at companies while statisticians tend to build models that can provide insights or explanations of phenomenon.
Keep reading for an in-depth explanation of these differences.
<h2>Difference #1: Types of Data</h2>
In general, data scientists often work with data that is messier, harder to extract, and much larger than the type of data used by statisticians.
For example, a data scientist that works at a real estate firm might have to extract datasets that contain millions of rows from several different external servers that are all in different formats.
She would need extensive knowledge of SQL and at least one programming language (like  R  or  Python ) in order to extract the data and wrangle it into a format that is suitable for modeling.
By contrast, statisticians tend to work with smaller datasets that are already in a neat format.
For example, a statistician that works for a biomedical company may be given an Excel file with 50 rows that contains information about blood pressure, heart rate, and cholesterol levels for 50 different patients.
Rather than spending their time extracting and cleaning data, they would likely spend more time deciding on a suitable  hypothesis test  or model to fit to the data and checking that the  assumptions  of their chosen test or statistical model are met.
<h2>Difference #2: End Goals</h2>
In many cases, a data scientist’s end goal is to create some type of model that can accurately predict some outcome.
For example, a data scientist who works for a financial company might attempt to build a  logistic regression model  that can accurately predict whether certain individuals will default on a loan.
They will fit a variety of models using different combinations of predictor variables and try to find the model that produces the most accurate predictions.
Their end goal is to create an accurate model rather than quantifying exactly how each predictor variable is related to the  response variable .
By contrast, statisticians tend to focus more on building models that can accurately describe the relationship between predictor variables and a response variable.
For example, a statistician that works at a university might recruit 30 students to participate in a study that quantifies exactly how different studying habits affect exam scores.
In this scenario, the statistician would be more concerned with interpreting the coefficients of the regression model and analyzing their corresponding  p-values  to understand whether they have a statistically significant relationship with the response variable.
<h2>Difference #3: Production</h2>
In general, data scientists tend to build statistical models that are put into production at companies far more often than statisticians.
For example, a data scientist that works at a large grocery chain might build a model that is able to accurately forecast sales of various products.
His end goal would be to work with developers at the company who can help him place his model into a server that runs on a nightly basis that can forecast the sales of products for each new day.
By contrast, statisticians rarely create models that are put into any type of production.
For example, a statistician that works at a healthcare company might build a model that describes the relationship between various lifestyle factors (smoking, exercise, diet, etc.) but their end goal is to simply <em>quantify</em> the relationship between these factors and some response variable like lifespan.
Their end goal is to build a model that provides them with some insights rather than a model that is put into a production environment.
<h2>Conclusion</h2>
Statisticians and data scientists both work with data in their everyday roles, but they do so in different ways.
Data scientists tend to work with a wider variety of data that is often messy and needs to be wrangled while statisticians often work with smaller and more tidy datasets.
Data scientists also tend to focus more on creating models that can accurately predict outcomes while statisticians tend to build models that can accurately explain the relationship between variables.
Lastly, data scientists tend to put models into production at companies while statisticians often summarize and report their findings to provide insights into real-world phenomena.
<h2>Additional Resources</h2>
The following articles explain the importance of statistics in various fields:
 Why is Statistics Important? (10 Reasons Statistics Matters!) 
 The Importance of Statistics in Business 
 The Importance of Statistics in Education 
 The Importance of Statistics in Healthcare 
 The Importance of Statistics in Finance 
<h2><span class="orange">8 Examples of How Statistics is Used in Real Life</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
When learning about statistics, students often ask:
<b><em>When is statistics actually used in real life?</em></b>
It turns out that it’s used in many different fields for a variety of applications.
In this article we share 8 examples of how statistics is used in real life.
<b>Related:</b>  Why is Statistics Important? (10 Reasons Statistics Matters!) 
<h3>Example 1: Weather Forecasting</h3>
Statistics is used heavily in the field of weather forecasting.
In particular, probability is used by weather forecasters to assess how likely it is that there will be rain, snow, clouds, etc. on a given day in a certain area.
Forecasters will regularly say things like “there is a 90% chance of rain today between after 5PM” to indicate that there’s a high likelihood of rain during certain hours.
<h3>Example 2: Sales Tracking</h3>
Retail companies often use  descriptive statistics  like the mean, median, mode, standard deviation, and interquartile range to track the sales behavior of certain products.
This gives companies an idea of how many products they can expect to sell during different time periods and allows them to know how much they should keep in inventory.
<h3>Example 3: Health Insurance</h3>
Health insurance companies often use statistics and probability to determine how likely it is that certain individuals will spend a certain amount on healthcare each year.
For example, an actuary at a health insurance company might use factors like age, existing medical conditions, current health status, etc. to determine that there’s a 80% probability that a certain individual will spend $10,000 or more on healthcare in a given year.
<h3>Example 4: Traffic</h3>
Traffic engineers regularly use statistics to monitor total traffic in different areas of a city, which allows them to decide whether or not they should add or remove roads to optimize traffic flow.
Also, traffic engineers often use  time series analysis  to monitor how traffic changes throughout the day so they can optimize the behavior of traffic lights.
<h3>Example 5: Investing</h3>
Investors use statistics and probability to assess how likely it is that a certain investment will pay off.
For example, a given investor might determine that there is a 5% chance that the stock of company A will increase 100x during the upcoming year. Based on this probability, they’ll decide how much of their portfolio to invest in the stock.
<h3>Example 6: Medical Studies</h3>
Statistics is regularly used in medical studies to understand how different factors are related.
For example, medical professions often use  correlation  to analyze how factors like weight, height, smoking habits, exercise habits, and diet are related.
If a certain diet and overall weight is found to be negatively correlated, a medical professional may recommend the diet to an individual who needs to lose weight.
<h3>Example 7: Manufacturing</h3>
Statistics is often used in manufacturing to monitor the efficiency of different processes.
For example, manufacturing engineers may collect a  random sample  of widgets from a certain assembly line and track how many of the widgets are defective.
They may then perform a  one proportion z-test  to determine if the proportion of widgets that are defective is lower than a certain value that is considered acceptable.
<h3>Example 8: Urban Planning</h3>
Statistics is regularly used by urban planners to decide how many apartments, shops, stores, etc. should be built in a certain area based on population growth patterns.
For example, if an urban planner sees that population growth in a certain part of the city is increasing at an exponential rate compared to other parts of the city then they may decide to prioritize building new apartment complexes in that part of town compared to another area.
<h2><span class="orange">SOCS: A Helpful Acronym for Describing Distributions</span></h2>
In statistics, we’re often interested in understanding how a dataset is distributed. In particular, there are four things that are helpful to know about a distribution:
<b>1.</b> <b>Shape</b>
Is the distribution symmetrical or skewed to one side?
Is the distribution unimodal (one peak) or  bimodal  (two peaks)?
<b>2. Outliers</b>
Are there any outliers present in the distribution?
<b>3. Center</b>
What is the mean, median, and mode of the distribution?
<b>4. Spread</b>
What is the range, interquartile range, standard deviation, and variance of the distribution?
<b>SOCS</b> is a useful acronym that we can use to remember these four things. It stands for “shape, outliers, center, spread.”
Let’s walk through a simple example of how to use SOCS to describe a distribution.
<h3>Example: How to Use SOCS to Describe a Distribution</h3>
Suppose we have the following dataset that shows the height of a sample of 20 different plants.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/SOCS0.png">
Here is how we can use SOCS to describe this distribution of data values.
<h3>Shape</h3>
First, we want to describe the shape of the distribution.
One helpful way to visualize the shape of the distribution is to create a histogram, which displays the frequencies of every value in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/SOCS00.png">
<b>Is the distribution symmetrical or skewed to one side?</b> From the histogram, we can see that the distribution is roughly symmetrical. That is, the values aren’t skewed to one side or the other.
<b>Is the distribution unimodal (one peak) or bimodal (two peaks)?</b> The distribution is unimodal. It has one peak at the value “7.”
<h3>Outliers</h3>
Next, we want to determine if there are any outliers in the dataset. From the histogram, we can visually inspect the distribution and see that 22 is potentially an outlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/SOCS1.png">
One common way to formally define an outlier is any value that is 1.5 times the interquartile range above the third quartile or below the first quartile.
Using  the Interquartile Range Calculator , we can input the 20 raw data values and find that the third quartile is <b>9</b>, the interquartile range is <b>3</b>, and thus any value above 9 + (1.5*3) = <b>13.5</b> is an outlier, by definition.
Since 22 is greater than 13.5, we can declare 22 to be an outlier.
<h3>Center</h3>
Next, we want to describe where the center of the distribution is located. Three common  measures of central tendency  we can use are the mean, median, and the mode.
<b>Mean: </b>This is the average value in the distribution. We find this by adding up all of the individual values, then dividing by the total number of values:
Mean = (8+4+6+7+7+6+7+8+6+11+8+22+10+9+9+7+5+7+6+4) / 20 = <b>7.85</b>
<b>Median: </b>This is the “middle” value in the distribution. We find this by arranging all of the values from smallest to greatest, then identifying the middle value. This turns out to be <b>7</b>.
4, 4, 5, 6, 6, 6, 6, 7, 7, <b>7</b>, <b>7</b>, 7, 8, 8, 8, 9, 9, 10, 11, 22
<b>Mode: </b>This is the value that occurs most frequently. This turns out to be <b>7</b>.
<h3>Spread</h3>
Next, we want to describe how spread out the values are in the distribution. Four common  measures of dispersion  we can use are the range, interquarile range, standard deviation, and the variance.
<b>Range: </b>This is the difference between the largest and smallest value in the dataset. This turns out to be 22 – 4 = <b>18</b>.
<b>Interquartile Range: </b>This measures the width of the middle 50% of the data values. From inputting the 20 raw data values into  the Interquartile Range Calculator , we can see that this is equal to <b>3</b>.
<b>Standard Deviation: </b>This is a measure of how spread out the data values are, on average. From inputting the 20 raw data values into the Variance and Standard Devation calculator, we can see that the standard deviation is equal to <b>3.69</b>.
<b>Variance: </b>This is simply the standard deviation, squared. This is equal to 3.69<sup>2</sup> = <b>13.63</b>.
<h3>Conclusion</h3>
From using <b>SOCS </b>as a guide, we were able to describe the distribution of plant heights in the following manner:
The distribution was unimodal and symmetrical, meaning it only had one peak and it was not skewed to one side or the other.
The distribution had one outlier: 22.
The distribution had a mean of 7.85, a median of 7, and a mode of 7.
The distribution had a range of 18, an interquartile range of 3, a standard deviation of 3.69, and a variance of 13.63.
Note that we can use SOCS to describe any distribution, which is a helpful way for us to gain a good understanding of the shape of a distribution, if it has any outliers, where the center is roughly located, and how spread out the data values are.
<h2><span class="orange">Statistics vs. Analytics: What’s the Difference?</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
The field of <b>analytics</b> is concerned with applying statistical methods to practical business problems.
There is much overlap between these two fields, but here is the main difference:
A <b>statistician</b> is more likely to work in a clinical setting or research setting where study design,  hypothesis testing ,  ANOVA models , and  confidence intervals  are more commonly used.
An <b>analyst</b> is more likely to work in a business setting where descriptive statistics, data visualizations, and regression models are more commonly used.
Both statisticians and analysts work with data in their daily roles, but statisticians tend to be more focused on testing statistical hypotheses while analysts tend to be more focused on understanding data and patterns underlying business operations.
Keep reading to see how statistics and analytics are used in real-world scenarios.
<h2>The Use of Statistics in the Real World</h2>
Here are a few examples of how statistics is used in real-world scenarios.
<h3>Example 1: Hypothesis Testing</h3>
Statisticians working in clinical settings often use hypothesis tests to determine if a new drug causes improved outcomes in patients.
For example, a biostatistician may administer a blood pressure drug to 30 patients for one month and then administer a second blood pressure drug to the same 30 patients for another month.
Then, they may perform a  paired samples t-test  to determine if there is a statistically significant difference in blood pressure reduction between the two drugs.
<h3>Example 2: ANOVA Models</h3>
Statisticians working in agricultural settings often use ANOVA models to determine if there is a statistically significant difference in crop yield between three or more types of fertilizers.
For example, a statistician may apply three different fertilizers to different fields for one month and then collect data to measure the mean crop yield.
They could then perform a  one-way ANOVA  to determine if there is a  statistically significant difference  between the mean yield.
<h3>Example 3: Confidence Intervals</h3>
Statisticians working in medical settings often use confidence intervals to quantify the mean value of different biometrics.
For example, a statistician may collect data on the blood pressure of 50 patients who all use the same blood pressure medication to come up with a range of values that is likely to contain the true mean reduction in blood pressure for patients in the overall population who use this particular medication.
<h2>The Use of Analytics in the Real World</h2>
Here are a few examples of how analytics is used in real-world scenarios.
<h3>Example 1: Descriptive Statistics</h3>
Business analysts often use descriptive statistics to summarize data related to the finances of companies.
For example, a business analyst who works for a retail company may calculate the following descriptive statistics during one business quarter:
Mean number of daily sales
Median number of daily sales
Standard deviation of daily sales
Total revenue
Total expenses
Percentage change in new customers
Percentage of products returned by customers
Using these metrics, the analyst can gain an understanding of the financial state of the company and also compare these metrics to previous quarters to understand how the metrics are trending over time.
They can then use these metrics to inform the organization on areas that could use improvement to help the company increases revenue or reduce expenses.
<h3>Example 2: Data Visualizations</h3>
Analysts who work for retail companies often created data visualizations such as line charts, bar charts, heat maps, box plots, scatter plots, and other charts to visualize the total sales, revenue, expenses, refunds, etc. during different business quarters.
In the real world, many analysts often create interactive dashboards using software like  Tableau  so that business leaders can interactively dig into different metrics and explore data trends and patterns to better understand how the business is performing.
<h3>Example 3: Regression Models</h3>
Financial analysts often use regression models to quantify the relationship between one or more predictor variables and a  response variable .
For example, an analyst may have access to data on total money spent on TV advertising, online advertising, and total revenue generated.
They might then build the following multiple linear regression model:
Revenue = 76.4 + 4.6(online advertising) + 0.8(TV advertising)
Here’s how to interpret the  regression coefficients  in this model:
For each additional dollar spent on online advertising, revenue increases by an average of $4.60 (assuming dollars spent on TV advertising is held constant).
For each additional dollar spent on TV advertising, revenue increases by an average of $0.80 (assuming dollars spent on online advertising is held constant).
Using this model, a financial analyst can quickly understand that money spent on online advertising results in much higher average revenue compared to money spent on TV advertising.
<h2>Conclusion</h2>
Statistics and analytics are two fields that share much overlap.
However, statisticians tend to be more focused on testing statistical hypotheses while analysts tend to be more focused on understanding data and patterns underlying business operations.
In the real world, statisticians and analysts often work side by side and it’s not uncommon for the two professions to collaborate to solve real-world problems.
<h2><span class="orange">Statistics vs. Biostatistics: What’s the Difference?</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
<b>Biostatistics</b> is simply the application of statistical methods to topics in biology.
For example, a student who takes an introductory statistics course may learn about the following topics:
How to calculate  descriptive statistics 
How to  visualize data 
How to construct  confidence intervals 
How to perform  hypothesis tests 
How to fit  regression models 
How to fit  ANOVA models 
A student who then takes a biostatistics course would learn how to apply each of these statistical methods to answer research questions in biology, public health, and medicine.
If a student wants to become a biostatistician, they must first learn about the concepts taught in an introductory statistics course.
They can then take a biostatistics course to learn how to apply statistical methods to specific research questions in the field of biology.
<h2>Common Statistical Methods Used in Biostatistics</h2>
The field of biostatistics uses many statistical methods.
The following examples illustrate some methods that are commonly used.
<h3>Example 1: Hypothesis Tests</h3>
Biostatisticians frequently use hypothesis tests to determine if a new drug causes improved outcomes in patients.
For example, a biostatistician may administer a blood pressure drug to 30 patients for one month and then administer a second blood pressure drug to the same 30 patients for another month.
Then, they may perform a  paired samples t-test  to determine if there is a statistically significant difference in blood pressure reduction between the two drugs.
<h3>Example 2: Logistic Regression Models</h3>
Biostatisticians often use  logistic regression models  to predict whether or not individuals or animals will have a particular binary outcome (yes or no).
For example, researchers can measure a variety of variables including weight, height, age, etc. so they can fit a logistic regression model that tells them the likelihood that a tumor on an animal will be malignant.
<h3>Example 3: Survival Curves</h3>
Biostatisticians often use  survival curves  to understand the proportion of individuals in a certain population that are expected to be alive after a certain age, or at a given time after contracting some type of disease.
For example, survival curves are frequently used to calculate the probability that individuals will live an additional number of months or years after being diagnosed with a specific type of cancer or other serious illness.
In practice, survival curves are used by biostatisticians, doctors, and epidemiologists all the time to gain a better understanding of the behavior of different viruses, diseases, and illnesses.
<h2>Conclusion</h2>
In conclusion:
The field of <b>statistics</b> encompasses a wide variety of methods that can be used with many different types of data.
The field of <b>biostatistics</b> is simply the application of these statistical methods to various topics in biology.
<h2><span class="orange">Statistics vs. Econometrics: What’s the Difference?</span></h2>
The field of <b>statistics</b> is concerned with collecting, analyzing, interpreting, and presenting data.
<b>Econometrics </b>is simply the application of statistical methods to topics in economics.
For example, a student who takes an introductory statistics course may learn about the following topics:
How to calculate  descriptive statistics 
How to  visualize data 
How to construct  confidence intervals 
How to perform  hypothesis tests 
How to fit  regression models 
How to fit  ANOVA models 
A student who then takes an econometrics course would learn how to apply each of these statistical methods to answer research questions related to the economy.
If a student wants to become an econometrician, they must first learn about the concepts taught in an introductory statistics course.
They can then take an econometrics course to learn how to apply statistical methods to specific research questions in the field of economics.
<h2>Common Statistical Methods Used in Econometrics</h2>
The field of econometrics uses many statistical methods.
The following examples illustrate some methods that are commonly used.
<h3>Example 1: Descriptive Statistics</h3>
Econometricians frequently use descriptive statistics to summarize the current state of an economy in a particular area.
For example, an econometrician might collect the following data about individuals in a particular city:
Population size: 85,000
Mean household income: $71,200
Median household income: $56,400
Standard deviation of household income: $12,200
Using these descriptive statistics, the econometrician can gain a solid understanding of the income distribution in this city.
The econometrician could also compare these values to other cities or even compare these values to the same city during a different time period.
In practice, econometricians use descriptive statistics all the time to gain a better understanding of the economic standing in different towns, cities, states, and countries.
<h3>Example 2: Regression Models</h3>
Econometricians often use  multiple regression models  to understand how various factors affect certain  response variables .
For example, an econometrician who studies houses might fit the following regression model:
<b>Response variable</b>:
House price
<b>Predictor variables</b>:
Square footage
Number of bedrooms
Number of bathrooms
Yard size
They can then use this regression model to understand exactly how the various predictor variables affect the response variable.
For example, they might find that for each additional one square foot increase in house size (holding all other variables constant) the house price increases by an average of $150.
Or they may find that for each additional bathroom (holding all other variables constant) the house price increases by an average of $8,500.
They can also use this regression model to predict the selling house of a price based on the values of the predictor variables in the model.
<h3>Example 3: Time Series Forecasting</h3>
Econometricians often use  time series analysis  to forecast the state of the economy for a given county, city, state, or country at some point in the future.
For example, an econometrician may use historical data to predict the GDP, unemployment rate, interest rate, or some other metric for a given country at some point in the future.
<b>Related:</b>  How to Plot a Time Series in R (With Examples) 
<h2>Conclusion</h2>
In conclusion:
The field of <b>statistics</b> encompasses a wide variety of methods that can be used with many different types of data.
The field of <b>econometrics </b>is simply the application of these statistical methods to various topics in economics.
<h2><span class="orange">Statistics vs. Probability: What’s the Difference?</span></h2>
<b>Probability </b>and <b>statistics </b>are two fields that both use data to answer questions, but they do so in slightly different ways.
The field of <b>probability </b>uses existing known data to predict the likelihood of future events.
Example: If 3 out of 5 marbles in a bag are red, what is the probability of picking two red marbles in repeated pulls without replacement?
The field of <b>statistics</b> uses data from a sample to draw inferences about a larger population.
Example: We collect a random sample of 50 turtles and measure each of their weights. We then use the sample data to infer a range of values that are likely to contain the true mean weight of all turtles in this population.
Keep reading to see how statistics and probability are used in real-world scenarios.
<h2>The Use of Statistics in the Real World</h2>
Here are a few examples of how statistics is used in real-world scenarios.
<h3>Example 1: Confidence Intervals</h3>
Statisticians working in finance often use confidence intervals to estimate the true value of different financial metrics.
For example, a statistician may collect data for the annual income of 200 randomly selected households in a certain city and then use this sample data to construct a  confidence interval for the mean  income of all households in this city.
By using data from a sample, the statistician can draw inferences about the overall population of interest.
<h3>Example 2: Hypothesis Testing</h3>
Statisticians working in clinical settings often use hypothesis tests to determine if a new drug causes improved outcomes in patients.
For example, a biostatistician may administer a blood pressure drug to 30 patients for one month and then administer a second blood pressure drug to the same 30 patients for another month.
Then, they may perform a  paired samples t-test  to determine if there is a statistically significant difference in blood pressure reduction between the two drugs.
By using sample data, the statistician can draw conclusions about these two drugs in the overall population.
<h2>The Use of Probability in the Real World</h2>
Here are a few examples of how probability is used in real-world scenarios.
<h3>Example 1: Predicting Natural Disasters</h3>
Suppose it’s known that the probability of a category 5 hurricane hitting a certain coastal area in a given year is .02.
Knowing this, a local government can predict the  probability that at least one  of these types of hurricanes will hit in the next 10 years:
P(at least one success) = 1 – P(failure in a given trial)<sup>n</sup>
P(at least one success) = 1 – (0.98)<sup>10</sup>
P(at least one success): 0.18293
The probability that at least one of these types of hurricanes will occur in the next 10 years is<b> 0.18293</b>.
By using existing known data, the local government can predict the likelihood of future events.
<h3>Example 2: Card Games</h3>
Professional poker players often use probability to predict the likelihood that certain cards will be flipped during a game.
For example, there are 4 kings in a standard deck of 52 cards.
Suppose the poker player knows that 3 kings have been dealt in the first 26 cards dealt already.
They can then calculate the probability of being dealt a king on the next card:
P(king) = number of kings / number of cards left
P(king) = 1 / 26
P(king) = .038
The probability that a king is dealt on the next card is roughly <b>.038</b>.
By using existing known data, the poker player can predict the likelihood of a specific future event.
<h2>Conclusion</h2>
Statistics and probability are two fields that both use data to answer questions, but they do so in different ways.
The field of <b>probability </b>uses existing known data to predict the likelihood of future events.
The field of <b>statistics</b> uses data from a sample to draw inferences about a larger population.
<h2>Additional Resources</h2>
The following articles explain the importance of statistics in various fields:
 Why is Statistics Important? (10 Reasons Statistics Matters!) 
 The Importance of Statistics in Business 
 The Importance of Statistics in Education 
 The Importance of Statistics in Healthcare 
 The Importance of Statistics in Finance 
<h2><span class="orange">How to Extract P-Values from Linear Regression in Statsmodels</span></h2>
You can use the following methods to extract p-values for the coefficients in a linear regression model fit using the  statsmodels  module in Python:
<b>#extract p-values for all predictor variables
for x in range (0, 3):
    print(model.pvalues[x])
#extract p-value for specific predictor variable name
model.pvalues.loc['predictor1']
#extract p-value for specific predictor variable position
model.pvalues[0]
</b>
The following examples show how to use each method in practice.
<h2>Example: Extract P-Values from Linear Regression in Statsmodels</h2>
Suppose we have the following pandas DataFrame that contains information about hours studied, prep exams taken, and final score received by students in a certain class:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'hours': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6],   'exams': [1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2],   'score': [76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96]})
#view head of DataFrame
df.head()
hoursexamsscore
01176
12378
22385
34588
42272</b>
We can use the <b>OLS()</b> function from the statsmodels module to fit a  multiple linear regression model , using “hours” and “exams” as the predictor variables and “score” as the  response variable :
<b>import statsmodels.api as sm
#define predictor and response variables
y = df['score']
x = df[['hours', 'exams']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.718
Model:                            OLS   Adj. R-squared:                  0.661
Method:                 Least Squares   F-statistic:                     12.70
Date:                Fri, 05 Aug 2022   Prob (F-statistic):            0.00180
Time:                        09:24:38   Log-Likelihood:                -38.618
No. Observations:                  13   AIC:                             83.24
Df Residuals:                      10   BIC:                             84.93
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         71.4048      4.001     17.847      0.000      62.490      80.319
hours          5.1275      1.018      5.038      0.001       2.860       7.395
exams         -1.2121      1.147     -1.057      0.315      -3.768       1.344
==============================================================================
Omnibus:                        1.103   Durbin-Watson:                   1.248
Prob(Omnibus):                  0.576   Jarque-Bera (JB):                0.803
Skew:                          -0.289   Prob(JB):                        0.669
Kurtosis:                       1.928   Cond. No.                         11.7
==============================================================================
</b>
By default, the <b>summary()</b> function displays the p-values of each predictor variable up to three decimal places:
P-value for intercept: <b>0.000</b>
P-value for hours: <b>0.001</b>
P-value for exams: <b>0.315</b>
However, we can extract the full p-values for each predictor variable in the model by using the following syntax:
<b>#extract p-values for all predictor variables
for x in range (0, 3):
    print(model.pvalues[x])
6.514115622692573e-09
0.0005077783375870773
0.3154807854805659
</b>
This allows us to see the p-values to more decimal places:
P-value for intercept: <b>0.00000000651411562269257</b>
P-value for hours: <b>0.0005077783375870773</b>
P-value for exams: <b>0.3154807854805659</b>
<b>Note</b>: We used <b>3</b> in our <b>range()</b> function because there were three total coefficients in our regression model.
We can also use the following syntax to extract the p-value for the ‘hours’ variable specifically:
<b>#extract p-value for 'hours' only
model.pvalues.loc['hours']
0.0005077783375870773
</b>
Or we could use the following syntax to extract the p-value for the coefficient of a variable in a specific position of the regression model:
<b>#extract p-value for coefficient in index position 0
model.pvalues[0]
6.514115622692573e-09</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Logistic Regression in Python 
 How to Calculate AIC of Regression Models in Python 
 How to Calculate Adjusted R-Squared in Python 
<h2><span class="orange">How to Perform Logistic Regression Using Statsmodels</span></h2>
The  statsmodels  module in Python offers a variety of functions and classes that allow you to fit various statistical models.
The following step-by-step example shows how to perform  logistic regression  using functions from statsmodels.
<h2>Step 1: Create the Data</h2>
First, let’s create a pandas DataFrame that contains three variables:
Hours Studied (Integer value)
Study Method (Method A or B)
Exam Result (Pass or Fail)
We’ll fit a logistic regression model using hours studied and study method to predict whether or not a student passes a given exam.
The following code shows how to create the pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'result': [0, 1, 0, 0, 0, 0, 0, 1, 1, 0,              0, 1, 1, 1, 0, 1, 1, 1, 1, 1],   'hours': [1, 2, 2, 2, 3, 2, 5, 4, 3, 6,            5, 8, 8, 7, 6, 7, 5, 4, 8, 9],   'method': ['A', 'A', 'A', 'B', 'B', 'B', 'B',             'B', 'B', 'A', 'B', 'A', 'B', 'B',             'A', 'A', 'B', 'A', 'B', 'A']})
#view first five rows of DataFrame
df.head()
resulthoursmethod
001A
112A
202A
302B
403B</b>
<h2>Step 2: Fit the Logistic Regression Model</h2>
Next, we’ll fit the logistic regression model using the <b>logit()</b> function:
<b>import statsmodels.formula.api as smf
#fit logistic regression model
model = smf.logit('result ~ hours + method', data=df).fit()
#view model summary
print(model.summary())
Optimization terminated successfully.
         Current function value: 0.557786
         Iterations 5           Logit Regression Results                           
==============================================================================
Dep. Variable:                 result   No. Observations:                   20
Model:                          Logit   Df Residuals:                       17
Method:                           MLE   Df Model:                            2
Date:                Mon, 22 Aug 2022   Pseudo R-squ.:                  0.1894
Time:                        09:53:35   Log-Likelihood:                -11.156
converged:                       True   LL-Null:                       -13.763
Covariance Type:            nonrobust   LLR p-value:                   0.07375
===============================================================================  coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------
Intercept      -2.1569      1.416     -1.523      0.128      -4.932       0.618
method[T.B]     0.0875      1.051      0.083      0.934      -1.973       2.148
hours           0.4909      0.245      2.002      0.045       0.010       0.972
===============================================================================
</b>
The values in the <b>coef</b> column of the output tell us the average change in the log odds of passing the exam.
For example:
Using study method B is associated with an average increase of <b>.0875</b> in the log odds of passing the exam compared to using study method A.
Each additional hour studied is associated with an average increase of <b>.4909</b> in the log odds of passing the exam.
The values in the <b>P>|z|</b> column represent the p-values for each coefficient.
For example:
Studying method has a p-value of <b>.934</b>. Since this value is not less than .05, it means there is not a statistically significant relationship between hours studied and whether or not  a student passes the exam.
Hours studied has a p-value of <b>.045</b>. Since this value is less than .05, it means there is a statistically significant relationship between hours studied and whether or not  a student passes the exam.
<h2>Step 3: Evaluate Model Performance</h2>
To assess the quality of the logistic regression model, we can look at two metrics in the output:
<b>1. Pseudo R-Squared</b>
This value can be thought of as the substitute to the R-squared value for a linear regression model.
It is calculated as the ratio of the maximized log-likelihood function of the null model to the full model.
This value can range from 0 to 1, with higher values indicating a better model fit.
In this example, the pseudo R-squared value is <b>.1894</b>, which is quite low. This tells us that the predictor variables in the model don’t do a very good job of predicting the value of the response variable.
<b>2. LLR p-value</b>
This value can be thought of as the substitute to the p-value for the  overall F-value  of a linear regression model.
If this value is below a certain threshold (e.g. α = .05) then we can conclude that the model overall is “useful” and is better at predicting the values of the response variable compared to a model with no predictor variables.
In this example, the LLR p-value is <b>.07375</b>. Depending on the significance level we choose (e.g. .01, .05, .1) we may or may not conclude that the model as a whole is useful.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Linear Regression in Python 
 How to Perform Logarithmic Regression in Python 
 How to Perform Quantile Regression in Python 
<h2><span class="orange">How to Make Predictions Using Regression Model in Statsmodels</span></h2>
You can use the following basic syntax to use a regression model fit using the  statsmodels  module in Python to make predictions on new observations:
<b>model.predict(df_new)
</b>
This particular syntax will calculate the predicted response values for each row in a new DataFrame called <b>df_new</b>, using a regression model fit with statsmodels called <b>model</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Make Predictions Using Regression Model in Statsmodels</h2>
Suppose we have the following pandas DataFrame that contains information about hours studied, prep exams taken, and final score received by students in a certain class:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'hours': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6],   'exams': [1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2],   'score': [76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96]})
#view head of DataFrame
df.head()
hoursexamsscore
01176
12378
22385
34588
42272</b>
We can use the <b>OLS()</b> function from the statsmodels module to fit a  multiple linear regression model , using “hours” and “exams” as the predictor variables and “score” as the response variable:
<b>import statsmodels.api as sm
#define predictor and response variables
y = df['score']
x = df[['hours', 'exams']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.718
Model:                            OLS   Adj. R-squared:                  0.661
Method:                 Least Squares   F-statistic:                     12.70
Date:                Fri, 05 Aug 2022   Prob (F-statistic):            0.00180
Time:                        09:24:38   Log-Likelihood:                -38.618
No. Observations:                  13   AIC:                             83.24
Df Residuals:                      10   BIC:                             84.93
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         71.4048      4.001     17.847      0.000      62.490      80.319
hours          5.1275      1.018      5.038      0.001       2.860       7.395
exams         -1.2121      1.147     -1.057      0.315      -3.768       1.344
==============================================================================
Omnibus:                        1.103   Durbin-Watson:                   1.248
Prob(Omnibus):                  0.576   Jarque-Bera (JB):                0.803
Skew:                          -0.289   Prob(JB):                        0.669
Kurtosis:                       1.928   Cond. No.                         11.7
==============================================================================
</b>
From the <b>coef</b> column in the output, we can write the fitted regression model:
Score = 71.4048 + 5.1275(hours) – 1.2121(exams)
Now suppose we would like to use the fitted regression model to predict the “score” for five new students.
First, let’s create a DataFrame to hold the five new observations:
<b>#create new DataFrame
df_new = pd.DataFrame({'hours': [1, 2, 2, 4, 5],       'exams': [1, 1, 4, 3, 3]})
#add column for constant
df_new = sm.add_constant(df_new)
#view new DataFrame
print(df_new)
   const  hours  exams
0    1.0      1      1
1    1.0      2      1
2    1.0      2      4
3    1.0      4      3
4    1.0      5      3</b>
Next, we can use the <b>predict()</b> function to predict the “score” for each of these students, using “hours” and “exams” as the values for the predictor variables in our fitted regression model:
<b>#predict scores for the five new students
model.predict(df_new)
0    75.320242
1    80.447734
2    76.811480
3    88.278550
4    93.406042
dtype: float64
</b>
Here’s how to interpret the output:
The first student in the new DataFrame is predicted to get a score of <b>75.32</b>.
The second student in the new DataFrame is predicted to get a score of <b>80.45</b>.
And so on.
To understand how these predictions were calculated, we need to refer to the fitted regression model from earlier:
Score = 71.4048 + 5.1275(hours) – 1.2121(exams)
By plugging in the values for “hours” and “exams” for the new students, we can calculate their predicted score.
For example, the first student in the new DataFrame had a value of <b>1</b> for hours and a value of <b>1</b> for exams.
Thus, their predicted score was calculated as:
Score = 71.4048 + 5.1275(1) – 1.2121(1) = <b>75.32</b>.
The score of each student in the new DataFrame was calculated in a similar manner.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Logistic Regression in Python 
 How to Calculate AIC of Regression Models in Python 
 How to Calculate Adjusted R-Squared in Python 
<h2><span class="orange">STDEV.P vs. STDEV.S in Excel: What’s the Difference?</span></h2>
There are three different functions you can use to calculate standard deviation in Excel:
<b>1. STDEV.P:</b> This function calculates the population standard deviation. Use this function when the range of values represents the entire population.
This function uses the following formula:
Population standard deviation = √Σ (x<sub>i</sub> – μ)<sup>2</sup> / N
where:
<b>Σ:</b> A greek symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value in the dataset
<b>μ:</b> The population mean
<b>N:</b> The total number of observations
<b>2. STDEV.S:</b> This function calculates the sample standard deviation. Use this function when the range of values represents a sample of values, rather than an entire population.
This function uses the following formula:
Sample standard deviation = √Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>Σ:</b> A greek symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value in the dataset
<b> x:</b> The sample mean
<b>N:</b> The total number of observations
<b>3. STDEV:</b> This function calculates the sample standard deviation as well. It will return the exact same value as the <b>STDEV.S</b> function.
<b>Technical Note:</b>
 
Since the formula for the population standard deviation divides by <em>N</em> instead of <em>n-1</em>, the population standard deviation will always be smaller than the sample standard deviation.
 
The reason the population standard deviation will be smaller is because if we know every value in the population, then we know the exact standard deviation.
 
However, when we only have a sample of the population then we have more uncertainty around the exact standard deviation of the overall population, so our estimate for the standard deviation needs to be larger.
The following example shows how to use these functions in practice.
<h3>Example: STDEV.P vs. STDEV.S in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sd_formulas1.png">
The following screenshot shows how to calculate the standard deviation for the dataset using the three different standard deviation formulas:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/sd_formulas2.png">
The sample standard deviation turns out to be <b>9.127</b> and the population standard deviation turns out to be <b>8.896</b>.
As mentioned earlier, the population standard deviation will always be smaller than the sample standard deviation.
<h3>When to Use STDEV.P vs. STDEV.S</h3>
In most cases, we’re unable to collect data for an entire population so we instead collect data for just a  sample  of the population.
Thus, we almost always use <b>STDEV.S</b> to calculate the standard deviation of a dataset because our dataset typically represents a sample.
Note that <b>STDEV</b> and <b>STDEV.S</b> return the exact same values, so we can use either function to calculate the sample standard deviation of a given dataset.
<h2><span class="orange">How to Create a Stem-and-Leaf Plot in Excel</span></h2>
A <b>stem-and-leaf plot</b> is a chart we can use to display data by splitting up each value in a dataset into a <em>stem </em>and a <em>leaf</em>.
Here is an example of a stem-and-leaf plot for a given dataset, created by the  Statology Stem-and-Leaf Plot Generator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata1.png"(max-width: 351px) 100vw, 351px">
The <em>stem </em>for each value is simply the first digit of the value while the <em>leaf </em>is the second digit of the value.
Now let’s find out how to create a stem-and-leaf plot in Excel.
<h2>Example: Stem-and-Leaf Plot in Excel</h2>
Use the following steps to create a stem-and-leaf plot in Excel.
<b>Step 1: Enter the data.</b>
Enter the data values in a single column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel1.png">
<b>Step 2: Identify the minimum and maximum values.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel2.png">
<b>Step 3: Manually enter the “stems” based on the minimum and maximum values.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel3.png">
<b>Step 4: Calculate the “leaves” for the first row.</b>
The following calculation shows how to compute the leaves for the first row. Don’t be intimidated by the length of the formula – it’s actually very simple, just repetitive.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel4.png">
Once you finish typing the formula and click Enter, you will get the following result:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel5.png">
<b>Step 5: Repeat the calculation for each row.</b>
To repeat this calculation for each row, simply click on cell D7, hover over the bottom-right hand corner of the cell until a tiny <b>+ </b>appears, then double-click. This will copy the formula to the rest of the rows in the Stem-and-Leaf plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafExcel6.png">
To double check that your results are correct, you can verify three numbers:
<b>Make sure the number of individual “leaves” matches the number of observations.</b> In our example, we have 10 total “leaves” which matches the 10 observations in our original dataset.
<b>Verify the minimum number. </b>The very first leaf should match the minimum value in your dataset. In our example, we see that the first leaf is “4” and is paired with a stem of “1” which matches the minimum number of “14” in our dataset.
<b>Verify the maximum number. </b>The very last leaf should match the maximum value in your dataset. In our example, we see that the last leaf is “5” and is paired with a stem of “3” which matches the minimum number of “35” in our dataset.
Once you verify these three numbers, you can be confident that your Stem-and-Leaf plot is correct.
<h2><span class="orange">Stem and Leaf Plot Generator</span></h2>
A stem and leaf plot displays data by splitting up each value in a dataset into a “stem” and a “leaf.”
To learn how to make a stem and leaf plot by hand, read  this tutorial .
To create a stem and leaf plot for a given dataset, enter your comma separated data in the box below:
<textarea id="input_data" name="x" rows="5" cols="40"></textarea>
<input type="button" id="button" onclick="calc()" value="Generate Stem and Leaf Plot">
<script>
//code to generate stem and leaf plot comes from https://www.rosettacode.org/wiki/Stem-and-leaf_plot
function calc() {
//remove current stem and leaf plot
var element = document.getElementsByTagName('table')[0];
    if(element) {element.parentNode.removeChild(element)}
var data = document.getElementById('input_data').value.match(/\d+/g).map(Number);
function has_property(obj, propname) {
        return typeof(obj[propname]) === "undefined" ? false : true;
    }
 
    function compare_numbers(a, b) {return a-b;}
 
    function stemplot(data, target) {
        var stem_data = {};
        var all_stems = [];
        for (var i = 0; i < data.length; i++) {
            var stem = Math.floor(data[i] / 10);
            var leaf = Math.round(data[i] % 10);
            if (has_property(stem_data, stem)) {
                stem_data[stem].push(leaf);
            } else {
                stem_data[stem] = [leaf];
                all_stems.push(stem);
            }
        }
        all_stems.sort(compare_numbers);
 
        var min_stem = all_stems[0];
        var max_stem = all_stems[all_stems.length - 1];
 
        var table = document.createElement('table');
        for (var stem = min_stem; stem <= max_stem; stem++) {
            var row = document.createElement('tr');
            var label = document.createElement('th');
            row.appendChild(label);
            label.appendChild(document.createTextNode(stem));
            if (has_property(stem_data, stem)) {
                stem_data[stem].sort(compare_numbers);
                for (var i = 0; i < stem_data[stem].length; i++) {
                    var cell = document.createElement('td');
                    cell.appendChild(document.createTextNode(stem_data[stem][i]));
                    row.appendChild(cell);
                }
            }
            table.appendChild(row);
        }
        target.appendChild(table);
    }
    
    stemplot(data, document.getElementById('target'));
    
//insert header in table
var the_table = document.getElementsByTagName('table')[0];
var header_insert = the_table.createTHead();
var row_insert = header_insert.insertRow(0);
var cell_insert = row_insert.insertCell(0);
var cell_insert2 = row_insert.insertCell(1);
cell_insert.innerHTML = "<b>Stem</b>";
cell_insert2.innerHTML = "<b>Leaf</b>";
  
} //end calc function
</script>
<h2><span class="orange">Stem-and-Leaf Plots: How to Find Mean, Median, & Mode</span></h2>
A  stem-and-leaf plot  is a type of plot that displays data by splitting up each value in a dataset into a <em>stem</em> and a <em>leaf</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/stemMean1.png">
This tutorial explains how to calculate the mean, median, and mode of a stem-and-leaf plot.
<h3>Example: Mean, Median & Mode of Stem-and-Leaf Plot</h3>
Suppose we have the following stem-and-leaf plot that shows the height of 19 different plants:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/stemMean2.png">
<b>Mean:</b>
To find the mean of this dataset, we can add up all of the individual values and divide by the total sample size of 19:
Mean = (11+12+13+26+26+30+31+31+42+45+46+51+52+61+78+82+82+93+94) / 19 = <b>47.68</b>.
The mean turns out to be <b>47.68</b>. This is the average value of the dataset.
<b>Median:</b>
To find the median of this dataset, we can write out all of the individual values in order and identify the value that lies directly in the middle:
11, 12, 13, 26, 26, 30, 31, 31, 42, <b>45</b>, 46, 51, 52, 61, 78, 82, 82, 93, 94
The median turns out to be <b>45</b>. This is the value located directly in the middle of the dataset.
<b>Mode:</b>
To find the mode of this dataset, we can identify the values that occur most often:
11, 12, 13, <b>26</b>, <b>26</b>, 30, 31, 31, 42, 45, 46, 51, 52, 61, 78, <b>82</b>, <b>82</b>, 93, 94
This dataset has two modes: <b>26</b> and <b>82</b>. Each of these values occurs twice in the dataset while every other value only occurs once.
<h2><span class="orange">How to Create a Stem-and-Leaf Plot in Python</span></h2>
A  stem-and-leaf plot  is a chart that displays data by splitting up each value in a dataset into a <em>stem </em>and a <em>leaf</em>. It’s a unique plot because it helps us visualize the shape of a distribution while still displaying the raw individual data values.
This tutorial explains how to create a stem-and-leaf plot in Python.
<h3>Example: Stem-and-Leaf Plot in Python</h3>
Suppose we have the following dataset in Python:
<b>x = [32, 34, 35, 41, 44, 46, 47, 52, 52, 53, 56, 61, 62]
</b>
To create a stem-and-leaf plot for this dataset, we can use the <b>stemgraphic </b>library:
<b>pip install stemgraphic</b>
Once this is installed, we can use the following code to create a stem-and-leaf plot for our dataset:
<b>import stemgraphic
#create stem-and-leaf plot
fig, ax = stemgraphic.stem_graphic(x)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/stemleafplotPython1.png">
The way to interpret this plot is as follows:
The number in the red box at the bottom of the plot displays the minimum number in the dataset (<b>32</b>).
The number in the red box at the top of the plot displays the maximum number in the dataset (<b>62</b>).
The numbers in the far left display the aggregated count of values in the plot. For example, the first row contains <b>2 </b>aggregated values, the second row contains <b>3 </b>aggregated values, the third row contains <b>5 </b>aggregated values, and so on.
The numbers in the middle column display the <em>stems</em>, which are <b>3</b>, <b>4</b>, <b>5</b>, and <b>6</b>.
The numbers in the far right column display the <em>leaves</em>.
This single plot provides us with a ton of information about the distribution of values in this dataset.
<h2><span class="orange">How to Create a Stem-and-Leaf Plot in SPSS</span></h2>
A  stem-and-leaf plot  displays data by splitting up each value in a dataset into a <em>stem </em>and a <em>leaf</em>. It’s a useful plot for easily visualizing the distribution of a dataset.
This tutorial explains how to create a stem-and-leaf plot in SPSS.
<h3>Example: Stem-and-Leaf Plot in SPSS</h3>
Suppose we have the following dataset that shows the average points per game for 16 basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS1.png">
To create a stem-and-leaf plot for this dataset, click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Explore</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS2.png">
This will bring up the following window:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS3.png">
To create a stem-and-leaf plot, we need to drag the variable <b>points </b>into the box labelled <b>Dependent List</b>. Then we need to make sure <b>Plots </b>is selected under the option that says <b>Display </b>near the bottom of the box.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS4.png">
Once we click <b>OK</b>, the following stem-and-leaf plot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS5.png">
The <b>Stem </b>column displays the first digit for each data value while the <b>Leaf </b>column displays the second digit.
For example, the first leaf shown in the first row represents the player who averages 5 points per game:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS6.png">
And the last leaf shown in the last row represents the player who averages 31 points per game:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS7.png">
This simple plot helps us get an idea of the distribution of the points scored by the 16 players in this dataset.
<h2><span class="orange">How to Create a Stem-and-Leaf Plot in Stata</span></h2>
A  <b>stem-and-leaf plot</b>  is a chart we can use to display data by splitting up each value in a dataset into a <em>stem </em>and a <em>leaf</em>.
Here is an example of a stem-and-leaf plot for a given dataset, created by the  Statology Stem-and-Leaf Plot Generator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata1.png">
The <em>stem </em>for each value is simply the first digit of the value while the <em>leaf </em>is the second digit of the value.
Now let’s find out how to create a stem-and-leaf plot in Stata.
<h2>Example: Stem-and-Leaf Plot in Stata</h2>
Use the following steps to create a stem-and-leaf plot in Stata.
<b>Step 1: Load the data.</b>
We’ll use a built-in Stata dataset called <em>auto</em> for this example. Load this dataset by typing the following into the command box:
<b>use http://www.stata-press.com/data/r13/auto</b>
<b>Step 2: Create a stem-and-leaf plot for the variable <em>mpg</em>.</b>
Type the following into the Command box and click <em>Enter</em>:
<b>stem mpg</b>
This produces the following stem-and-leaf plot for all of the values for <em>mpg</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata5.png">
By default, Stata splits the stems into multiple lines. You can specify that each stem only uses one line by using the <b>lines() </b>command:
<b>stem mpg, lines(1)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata6.png">
Notice how each stem now has all of its values on one line.
We can also create a stem-and-leaf plot for another variable in the dataset called <em>price</em>, which represents the price of each car in the dataset and takes on values in the thousands.
<b>stem price</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata7.png">
We can also use the <b>round() </b>command to round numbers. For example, we can use round(100) to specify that each value of price should be rounded to the hundreds place:
<b>stem price, round(100)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata9.png">
Or we can specify each price to be rounded to the tens place:
<b>stem price, round(10)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata8.png">
Lastly, we can use the command <b>prune </b>to avoid printing any stems that have no leaves:
<b>stem price, round(10) prune</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/stemLeafStata10.png">
<h3>Additional Resources:</h3>
 What are Stem-and-leaf plots? 
 Stem and Leaf Plot Generator 
<h2><span class="orange">How to Make a Stem and Leaf Plot with Decimals</span></h2>
A  stem and leaf plot  is a type of plot that displays data by splitting up each value in a dataset into a “stem” and a “leaf.”
For example, suppose we have the following dataset:
<b>Dataset:</b> 12, 14, 18, 22, 22, 23, 25, 25, 28, 45, 47, 48
If we define the first digit in each value as the “stem” and the second digit as the “leaf” then we can create the following stem and leaf plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/stemLeafDecimal1-1.png">
While stem and leaf plots are typically used with integer values, they can also be used for values with decimals as well.
The following examples illustrate how to create stem and leaf plots with decimals.
<h3>Example 1: Stem and Leaf Plot with One Decimal</h3>
Suppose we have the following dataset:
<b>Dataset:</b> 11.6, 12.2, 12.5, 12.6, 13.7, 13.8, 14.1, 15.2
If we define the digits in front of the decimal as the stem, and the digits after the decimal as the leaf, then we can create the following stem and leaf plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/stemLeafDecimal2.png">
When creating this stem and leaf plot, it’s important to include a <b>key</b> at the bottom so the reader knows how to interpret the values in both the stem and the leaf.
<h3>Example 2: Stem and Leaf Plot with Multiple Decimals</h3>
Suppose we have the following dataset:
<b>Dataset:</b> 3.26, 3.28, 3.34, 3.38, 3.41, 3.42, 3.44, 3.59, 3.63
If we define the integer and the first value after the decimal place as the stem, and the second value after the decimal as the leaf, then we can create the following stem and leaf plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/stemLeafDecimal3.png">
Once again, the key at the bottom of the plot tells us how to interpret the values in the plot.
<h3>Example 3: Interpreting a Stem and Leaf Plot with Decimals</h3>
Suppose we have the following stem and leaf plot with decimals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/stemLeafDecimal4.png">
<b>Question 1:</b> What is the max value in the dataset?
The max value would be <b>8.3</b>.
<b>Question 2:</b> What is the range of the dataset?
The range of the dataset is the difference between the largest and smallest value. Thus, the range would be 8.3 – 4.5 = <b>3.8</b>.
<b>Question 3:</b> What is the mode of the dataset?
The mode is the value that occurs most often. This would be <b>8.2</b>.
<b>Question 4:</b> What is the median of the dataset?
The median is defined as the “middle” value of the dataset. To find this value, we can write out each of the individual values in the dataset and find the middle value:
Dataset: 4.5, 4.7, 4.9, 5.2, 5.4, <b>6.1</b>, 7.8, 8.2, 8.2, 8.2, 8.3
The median is <b>6.1</b>.
<h2><span class="orange">Stem-and-Leaf Plots: Definition & Examples</span></h2>
A <b>stem-and-leaf plot</b> displays data by splitting up each value in a dataset into a “stem” and a “leaf.”
This tutorial explains how to create and interpret stem-and-leaf plots.
<h3>How to Make a Stem-and-leaf Plot</h3>
The following two examples illustrate how to create a stem-and-leaf plot from scratch for a given dataset.
<b>Example 1</b>
Suppose we have the following dataset:
12, 14, 18, 22, 22, 23, 25, 25, 28, 45, 47, 48
Here is what the stem and leaf plot looks like:
1 | 2  4  8
2 | 2  2  3  5  5  8
3 |
4 | 5  7  8
And here’s how to make it:
<b>Step 1:</b> <b>split each value in the dataset into a “stem” and a “leaf.”</b>
The <b>“stem”</b> of each value is the first digit:
<b>1</b>2, <b>1</b>4, <b>1</b>8, <b>2</b>2, <b>2</b>2, <b>2</b>3, <b>2</b>5,<b> 2</b>5, <b>2</b>8, <b>4</b>5, <b>4</b>7, <b>4</b>8
The <b>“leaf”</b> of each value is the second digit:
1<b>2</b>, 1<b>4</b>, 1<b>8</b>, 2<b>2</b>, 2<b>2</b>, 2<b>3</b>, 2<b>5</b>, 2<b>5</b>, 2<b>8</b>, 4<b>5</b>, 4<b>7</b>, 4<b>8</b>
<b>Step 2: To make the plot, place the “stems” along the vertical axis and the “leaves” along the horizontal axis:</b>
<b>1</b> | <b>2  4  8</b>
<b>2</b> |<b> 2  2  3  5  5  8</b>
<b>3</b> |
<b>4</b> |<b> 5  7  8</b>
<b>Example 2</b>
Suppose we have the following dataset:
134, 156, 158, 159, 160, 162, 164
Here is what the stem and leaf plot looks like:
13 | 4
14 |
15 | 6  8  9
16 | 0  2  4
And here’s how to make it:
<b>Step 1:</b> <b>split each value in the dataset into a “stem” and a “leaf.”</b>
In this case, since the values in this dataset have three digits, the <b>“stem”</b> of each value is the first two digits:
<b>13</b>4, <b>15</b>6, <b>15</b>8, <b>15</b>9, <b>16</b>0, <b>16</b>2, <b>16</b>4
The <b>“leaf”</b> of each value is the last digit:
13<b>4</b>, 15<b>6</b>, 15<b>8</b>, 15<b>9</b>, 16<b>0</b>, 16<b>2</b>, 16<b>4</b>
<b>Step 2: To make the plot, place the “stems” along the vertical axis and the “leaves” along the horizontal axis:</b>
<b>13</b> | <b>4</b>
<b>14</b> |
<b>15</b> |<b> 6  8  9</b>
<b>16</b> | <b>0  2  4</b>
<h3>How to Interpret a stem-and-leaf plot</h3>
Suppose we have the following stem and leaf plot that shows the number of ice cream cones that Marie sold at her ice cream shop during each of the past 14 days:
7 | 3  3  3  7  9
8 | 4  5  8  8
9 | 1  2  5  8  9
<b><em>Question: </em></b><em>What is the most cones that Marie sold on any given day?</em>
Answer: 99 cones
7 | 3  3  3  7  9
8 | 4  5  8  8
<b>9</b> | 1  2  5  8  <b>9</b>
<b><em>Question: </em></b><em>During how many days did Marie sell more than 80 cones?</em>
Answer: nine days
7 | 3  3  3  7  9
<b>8</b> | <b>4  5  8  8</b>
<b>9</b> | <b>1  2  5  8  9</b>
(during these nine days, she sold 84, 85, 88, 88, 91, 92, 95, 98, and 99 cones)
<h2><span class="orange">A Complete Guide to Stepwise Regression in R</span></h2>
 Stepwise regression  is a procedure we can use to build a  regression model  from a set of predictor variables by entering and removing predictors in a stepwise manner into the model until there is no statistically valid reason to enter or remove any more.
The goal of stepwise regression is to build a regression model that includes all of the predictor variables that are statistically significantly related to the  response variable .
This tutorial explains how to perform the following stepwise regression procedures in R:
Forward Stepwise Selection
Backward Stepwise Selection
Both-Direction Stepwise Selection
For each example we’ll use the built-in <b>mtcars</b> dataset:
<b>#view first six rows of <em>mtcars
</em>head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
We will fit a multiple linear regression model using <em>mpg </em>(miles per gallon) as our response variable and all of the other 10 variables in the dataset as potential predictors variables.
For each example will use the built-in  step()  function from the stats package to perform stepwise selection, which uses the following syntax:
<b>step(intercept-only model, direction, scope)</b>
where:
<b>intercept-only model</b>: the formula for the intercept-only model
<b>direction: </b>the mode of stepwise search, can be either “both”, “backward”, or “forward”
<b>scope: </b>a formula that specifies which predictors we’d like to attempt to enter into the model
<h3>Example 1: Forward Stepwise Selection</h3>
The following code shows how to perform forward stepwise selection:
<b>#define intercept-only model
intercept_only &lt;- lm(mpg ~ 1, data=mtcars)
#define model with all predictors
all &lt;- lm(mpg ~ ., data=mtcars)
#perform forward stepwise regression
forward &lt;- step(intercept_only, direction='forward', scope=formula(all), trace=0)
#view results of forward stepwise regression
forward$anova
   Step Df  Deviance Resid. Df Resid. Dev       AIC
1       NA        NA        31  1126.0472 115.94345
2  + wt -1 847.72525        30   278.3219  73.21736
3 + cyl -1  87.14997        29   191.1720  63.19800
4  + hp -1  14.55145        28   176.6205  62.66456
#view final model
forward$coefficients
(Intercept)          wt         cyl          hp 
 38.7517874  -3.1669731  -0.9416168  -0.0180381 
</b>
<em><b>Note:</b> The argument trace=0 tells R not to display the full results of the stepwise selection. This can take up quite a bit of space if there are a large number of predictor variables.</em>
Here is how to interpret the results:
First, we fit the intercept-only model. This model had an AIC of <b>115.94345</b>.
Next, we fit every possible one-predictor model. The model that produced the lowest AIC and also had a statistically significant reduction in AIC compared to the intercept-only model used the predictor <em>wt</em>. This model had an AIC of <b>73.21736</b>.
Next, we fit every possible two-predictor model. The model that produced the lowest AIC and also had a statistically significant reduction in AIC compared to the single-predictor model added the predictor <em>cyl</em>. This model had an AIC of <b>63.19800</b>.
Next, we fit every possible three-predictor model. The model that produced the lowest AIC and also had a statistically significant reduction in AIC compared to the two-predictor model added the predictor <em>hp</em>. This model had an AIC of <b>62.66456</b>.
Next, we fit every possible four-predictor model. It turned out that none of these models produced a significant reduction in AIC, thus we stopped the procedure.
The final model turns out to be:
<b>mpg ~ 38.75 – 3.17*wt – 0.94*cyl – 0.02*hyp</b>
<h3>Example 2: Backward Stepwise Selection</h3>
The following code shows how to perform backward stepwise selection:
<b>#define intercept-only model
intercept_only &lt;- lm(mpg ~ 1, data=mtcars)
#define model with all predictors
all &lt;- lm(mpg ~ ., data=mtcars)
#perform backward stepwise regression
backward &lt;- step(all, direction='backward', scope=formula(all), trace=0)
#view results of backward stepwise regression
backward$anova
    Step Df   Deviance Resid. Df Resid. Dev      AIC
1        NA         NA        21   147.4944 70.89774
2  - cyl  1 0.07987121        22   147.5743 68.91507
3   - vs  1 0.26852280        23   147.8428 66.97324
4 - carb  1 0.68546077        24   148.5283 65.12126
5 - gear  1 1.56497053        25   150.0933 63.45667
6 - drat  1 3.34455117        26   153.4378 62.16190
7 - disp  1 6.62865369        27   160.0665 61.51530
8   - hp  1 9.21946935        28   169.2859 61.30730
#view final model
backward$coefficients
(Intercept)          wt        qsec          am 
   9.617781   -3.916504    1.225886    2.935837
</b>
Here is how to interpret the results:
First, we fit a model using all <em>p</em> predictors. Define this as M<sub>p</sub>.
Next, for k = p, p-1, … 1, we fit all k models that contain all but one of the predictors in M<sub>k</sub>, for a total of k-1 predictor variables. Next, pick the best among these k models and call it M<sub>k-1</sub>.
Lastly, we pick a single best model from among M<sub>0</sub>…M<sub>p</sub> using AIC.
The final model turns out to be:
<b>mpg ~ 9.62 – 3.92*wt + 1.23*qsec + 2.94*am</b>
<h3>Example 3: Both-Direction Stepwise Selection</h3>
The following code shows how to perform both-direction stepwise selection:
<b>#define intercept-only model
intercept_only &lt;- lm(mpg ~ 1, data=mtcars)
#define model with all predictors
all &lt;- lm(mpg ~ ., data=mtcars)
#perform backward stepwise regression
both &lt;- step(intercept_only, direction='both', scope=formula(all), trace=0)
#view results of backward stepwise regression
both$anova
   Step Df  Deviance Resid. Df Resid. Dev       AIC
1       NA        NA        31  1126.0472 115.94345
2  + wt -1 847.72525        30   278.3219  73.21736
3 + cyl -1  87.14997        29   191.1720  63.19800
4  + hp -1  14.55145        28   176.6205  62.66456
#view final model
both$coefficients
(Intercept)          wt         cyl          hp 
 38.7517874  -3.1669731  -0.9416168  -0.0180381 
</b>
Here is how to interpret the results:
First, we fit the intercept-only model.
Next, we added predictors to the model sequentially just like we did in forward-stepwise selection. However, after adding each predictor we also removed any predictors that no longer provided an improvement in model fit.
We repeated this process until we reached a final model.
The final model turns out to be:
<b>mpg ~ 9.62 – 3.92*wt + 1.23*qsec + 2.94*am</b>
Note that forward stepwise selection and both-direction stepwise selection produced the same final model while backward stepwise selection produced a different model.
<h2><span class="orange">What is Stepwise Selection? (Explanation & Examples)</span></h2>
In the field of machine learning, our goal is to build a model that can effectively use a set of predictor variables to predict the value of some  response variable .
Given a set of <em>p</em> total predictor variables, there are many models that we could potentially build. One method that we can use to pick the best model is known as  best subset selection , which attempts to choose the best model from <em>all</em> possible models that could be built with the set of predictors.
Unfortunately this method suffers from two drawbacks:
It can be computationally intense. For a set of <em>p</em> predictor variables, there are 2<sup>p</sup> possible models. For example, with 10 predictor variables there are 2<sup>10</sup> = 1,000 possible models to consider.
Because it considers such a large number of models, it could potentially find a model that performs well on training data but not on future data. This could result in  overfitting .
An alternative to best subset selection is known as <b>stepwise selection</b>, which compares a much more restricted set of models.
There are two types of stepwise selection methods: forward stepwise selection and backward stepwise selection.
<h3>Forward Stepwise Selection</h3>
Forward stepwise selection works as follows:
<b>1.</b> Let M<sub>0</sub> denote the null model, which contains no predictor variables. 
<b>2.</b> For k = 0, 2, … p-1:
Fit all p-k models that augment the predictors in M<sub>k</sub> with one additional predictor variable.
Pick the best among these p-k models and call it M<sub>k+1</sub>. Define “best” as the model with the highest R<sup>2</sup> or equivalently the lowest RSS.
<b>3.</b> Select a single best model from among M<sub>0</sub>…M<sub>p</sub> using cross-validation prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>.
<h3>Backward Stepwise Selection</h3>
Backward stepwise selection works as follows:
<b>1.</b> Let M<sub>p</sub> denote the full model, which contains all <em>p</em> predictor variables. 
<b>2.</b> For k = p, p-1, … 1:
Fit all k models that contain all but one of the predictors in M<sub>k</sub>, for a total of k-1 predictor variables.
Pick the best among these k models and call it M<sub>k-1</sub>. Define “best” as the model with the highest R<sup>2</sup> or equivalently the lowest RSS.
<b>3.</b> Select a single best model from among M<sub>0</sub>…M<sub>p</sub> using cross-validation prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>.
<h3>Criteria for Choosing the “Best” Model</h3>
The last step of both forward and backward stepwise selection involves choosing the model with the lowest prediction error, lowest Cp, lowest BIC, lowest AIC, or highest adjusted R<sup>2</sup>.
Here are the formulas used to calculate each of these metrics:
<b>Cp:</b> (RSS+2dσ<U+0302>) / n
<b>AIC: </b>(RSS+2dσ<U+0302><sup>2</sup>) / (nσ<U+0302><sup>2</sup>)
<b>BIC: </b>(RSS+log(n)dσ<U+0302><sup>2</sup>) / n
<b>Adjusted R<sup>2</sup>:</b> 1 – ( (RSS/(n-d-1)) / (TSS / (n-1)) )
where:
<b>d:</b> The number of predictors
<b>n:</b> Total observations
<b>σ<U+0302>:</b> Estimate of the variance of the error associate with each response measurement in a regression model
<b>RSS:</b> Residual sum of squares of the regression model
<b>TSS:</b> Total sum of squares of the regression model
<h3>Pros & Cons of Stepwise Selection</h3>
Stepwise selection offers the following <b>benefit</b>:
It is more computationally efficient than best subset selection. Given <em>p</em> predictor variables, best subset selection must fit 2<sup>p</sup> models.
Conversely, stepwise selection only has to fit 1+p(p+ 1)/2 models. For p = 10 predictor variables, best subset selection must fit 1,000 models while stepwise selection only has to fit 56 models.
However, stepwise selection has the following potential <b>drawback:</b>
It is not guaranteed to find the best possible model out of all 2<sup>p</sup> potential models.
For example, suppose we have a dataset with p = 3 predictors. The best possible one-predictor model may contain x<sub>1</sub> and the best possible two-predictor model may instead contain x<sub>1</sub> and x<sub>2</sub>.
 In this case, forward stepwise selection will fail to select the best possible two-predictor model because M<sub>1</sub> will contain x<sub>1</sub>, so M<sub>2</sub> must also contain x<sub>1</sub> along with some other variable.
<h2><span class="orange">How to Use str() Function in R (4 Examples)</span></h2>
You can use the <b>str() </b>function in R to display the internal structure of any R object in a compact way.
This function uses the following basic syntax:
<b>str(object)
</b>
where:
<b>x</b>: The name of the object to display the structure for
The following examples show how to use this function in different scenarios.
<h3>Example 1: Use str() with Vector</h3>
The following code shows how to use the <b>str()</b> function to display the internal structure of a vector in a compact way:
<b>#create vector
x &lt;- c(2, 4, 4, 5, 8, 10, NA, 15, 12, 12, 19, 24)
#display internal structure of vector
str(x)
num [1:12] 2 4 4 5 8 10 NA 15 12 12 ...
</b>
From the output we can see:
The vector has a class of numeric
The vector has a length of 12
By default, the <b>str()</b> function also displays the first 10 items in the vector.
<h3>Example 2: Use str() with Data Frame</h3>
The following code shows how to use the <b>str()</b> function to display the internal structure of a data frame in a compact way:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 90, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28))
#display internal structure of data frame
str(df)
'data.frame':5 obs. of  4 variables:
 $ team    : chr  "A" "B" "C" "D" ...
 $ points  : num  99 90 86 88 95
 $ assists : num  33 28 31 39 34
 $ rebounds: num  30 28 24 24 28
</b>
From the output we can see:
The object has a class of data.frame
The data frame has 5 observations (rows) and 4 variables (columns)
The name of each variable in the data frame is also shown along with the class of each variable and the first few values.
Using the <b>str()</b> function is an excellent way to gain a quick understanding of a data frame, especially if the data frame is very large.
In practice, the <b>str()</b> function is one of the first functions used after loading a data frame into R, even before performing any exploratory analysis or statistical modeling.
<h3>Example 3: Use str() with Matrix</h3>
The following code shows how to use the <b>str()</b> function to display the internal structure of a matrix in a compact way:
<b>#create matrix
mat &lt;- matrix(1:15, nrow=5)
#view matrix
mat
     [,1] [,2] [,3]
[1,]    1    6   11
[2,]    2    7   12
[3,]    3    8   13
[4,]    4    9   14
[5,]    5   10   15
#display internal structure of matrix
str(mat)
 int [1:5, 1:3] 1 2 3 4 5 6 7 8 9 10 ...
</b>
From the output we can see:
The matrix has a class of integer
The matrix has 5 rows and 3 columns
By default, the <b>str()</b> function also displays the first 10 values in the vector.
<h3>Example 4: Use str() with List</h3>
The following code shows how to use the <b>str()</b> function to display the internal structure of a list in a compact way:
<b>#create list
my_list &lt;- list(A=1:5, B=c(2, 9), C=c('hey', 'hello'))
#view list
my_list
$A
[1] 1 2 3 4 5
$B
[1] 2 9
$C
[1] "hey"   "hello"
#display internal structure of list
str(my_list)
List of 3
 $ A: int [1:5] 1 2 3 4 5
 $ B: num [1:2] 2 9
 $ C: chr [1:2] "hey" "hello"</b>
From the output we can see:
The list has 3 elements
The first element has a name of A, a class of integer, a length of 5, and all 5 values are shown.
The second element has a name of B, a class of numeric, a length of 2, and the 2 values are shown.
The third element has a name of C, a class of character, a length of 2, and the 2 values are shown.
By just using the <b>str()</b> function, we’re able to gain a full understanding of the structure of the list.
<h2><span class="orange">How to Use str_c in R (With Examples)</span></h2>
The <b>str_c()</b> function from the  stringr  package in R can be used to join two or more vectors element-wise into a single character vector.
This function uses the following syntax:
<b>str_c(. . ., sep = “”)</b>
where:
<b>. . .:</b> One or more character vectors
<b>sep:</b> String to insert between vectors
The following examples show how to use this function in practice
<h2>Example 1: Use str_c with No Separator</h2>
The following code shows how to use the <b>str_c()</b> function to join together two vectors element-wise into a single character vector:
<b>library(stringr)
#define two vectors
vec1 &lt;- c('Mike', 'Tony', 'Will', 'Chad', 'Rick')
vec2 &lt;- c('Douglas', 'Atkins', 'Durant', 'Johnson', 'Flair')
#join vectors together element-wise
str_c(vec1, vec2)
[1] "MikeDouglas" "TonyAtkins"  "WillDurant"  "ChadJohnson" "RickFlair"  
</b>
The result is a single character vector.
Note that the vectors have been joined together element-wise with no separator between the elements.
<h2>Example 2: Use str_c with Separator</h2>
The following code shows how to use the <b>str_c()</b> function to join together two vectors element-wise into a single character vector with an underscore as a separator:
<b>library(stringr)
#define two vectors
vec1 &lt;- c('Mike', 'Tony', 'Will', 'Chad', 'Rick')
vec2 &lt;- c('Douglas', 'Atkins', 'Durant', 'Johnson', 'Flair')
#join vectors together element-wise
str_c(vec1, vec2, sep="_")
[1] "Mike_Douglas" "Tony_Atkins"  "Will_Durant"  "Chad_Johnson" "Rick_Flair"    
</b>
The result is a single character vector in which the elements from each vector have been joined with an underscore.
Feel free to use any character you’d like for the <b>sep</b> argument.
For example, you may choose to use a dash:
<b>library(stringr)
#define two vectors
vec1 &lt;- c('Mike', 'Tony', 'Will', 'Chad', 'Rick')
vec2 &lt;- c('Douglas', 'Atkins', 'Durant', 'Johnson', 'Flair')
#join vectors together element-wise
str_c(vec1, vec2, sep="-")
[1] "Mike-Douglas" "Tony-Atkins"  "Will-Durant"  "Chad-Johnson" "Rick-Flair"  
</b>
The result is a single character vector in which the elements from each vector have been joined with a dash.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
<h2><span class="orange">How to Use str_count in R (With Examples)</span></h2>
The <b>str_count()</b> function from the  stringr  package in R can be used to count the number of matches in a string.
This function uses the following syntax:
<b>str_count(string, pattern = “”)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to look for
The following examples show how to use this function in practice
<h2>Example 1: Use str_count with One Pattern</h2>
The following code shows how to use the <b>str_count()</b> function to count the number of times the letter ‘a’ occurs in each element in a character vector:
<b>library(stringr)
#create character vector
x &lt;- c('Mavs', 'Cavs', 'Nets', 'Trailblazers', 'Heat')
#count number of times 'a' occurs in each element in vector
str_count(x, 'a')
[1] 1 1 0 2 1
</b>
Here’s how to interpret the output:
The pattern ‘a’ occurs 1 time in ‘Mavs’
The pattern ‘a’ occurs 1 time in ‘Cavs’
The pattern ‘a’ occurs 0 times in ‘Nets’
And so on.
Note that <b>str_count()</b> is also case-sensitive, so a capital ‘A’ would return <b>0</b> for each element in the character vector.
<h2>
<b>Example 2: Use </b>str_count with Multiple Patterns</b>
</h2>
The following code shows how to use the <b>str_count()</b> function to count the number of times the letter ‘a’ or the letter ‘s’ occurs in each element in a character vector:
<b>library(stringr)
#create character vector
x &lt;- c('Mavs', 'Cavs', 'Nets', 'Trailblazers', 'Heat')
#count number of times 'a' or 's' occurs in each element in vector
str_count(x, 'a|s')
[1] 2 2 1 3 1
</b>
Here’s how to interpret the output:
The pattern ‘a’ or ‘s’ occurs 2 times in ‘Mavs’
The pattern ‘a’ or ‘s’ occurs 2 times in ‘Cavs’
The pattern ‘a’ or ‘s’ occurs 1 time in ‘Nets’
<b>Note:</b> The <b>|</b> symbol represents an “OR” operator in R.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
<h2><span class="orange">How to Use str_detect() Function in R (3 Examples)</span></h2>
You can use the <b>str_detect() </b>function from the <b>stringr</b> function R to detect the presence or absence of a certain pattern in a string.
This function uses the following basic syntax:
<b>library(stringr)
#check if "hey" exists in object named x
str_detect(x, "hey")
</b>
This function returns <b>TRUE</b> if the pattern is present in the string or <b>FALSE</b> if it is not.
The following examples show how to use this function in different scenarios.
<h3>Example 1: Use str_detect() with String</h3>
The following code shows how to use the <b>str_detect()</b> function to detect if the pattern “hey” is present in a certain string:
<b>library(stringr)
#create string
x &lt;- "hey there everyone"
#determine if "hey" is present in string
str_detect(x, "hey")
[1] TRUE
</b>
From the output we can see that “hey” is present in the string.
Note that <b>str_detect()</b> is case-sensitive as well:
<b>library(stringr)
#create string
x &lt;- "hey there everyone"
#determine if "Hey" is present in string
str_detect(x, "Hey")
[1] FALSE</b>
From the output we can see that “Hey” is not present in the string.
<h3>Example 2: Use str_detect() with Vector</h3>
The following code shows how to use the <b>str_detect()</b> function to detect if the pattern “hey” is present in each individual element of a vector:
<b>library(stringr)
#create vector
x &lt;- c("hello", "heyo", "hi", "hey")
#determine if "hey" is present in each element of vector
str_detect(x, "hey")
[1] FALSE  TRUE FALSE  TRUE
</b>
From the output we can see that “hey” in just the second and fourth elements of the vector.
<h3>Example 3: Use str_detect() with Data Frame</h3>
The following code shows how to use the <b>str_detect()</b> function to subset a data frame based on the values in one column having “avs” in the name:
<b>library(stringr)
#create data frame
df &lt;- data.frame(team=c("Mavs", "Heat", "Pacers", "Cavs"), points=c(99, 90, 86, 103))
#subset data frame based on teams that have "avs" in the name
df[str_detect(df$team, "avs"), ]
  team points
1 Mavs     99
4 Cavs    103</b>
Notice that only the teams that have “avs” in the name are included in the final data frame.
<h2><span class="orange">How to Use str_extract in R (With Examples)</span></h2>
The <b>str_extract()</b> function from the  stringr  package in R can be used to extract matched patterns in a string.
This function uses the following syntax:
<b>str_extract(string, pattern)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to extract
The following examples show how to use this function in practice.
<h3>Example 1: Extract One Pattern from String</h3>
The following code shows how to extract the string “ther” from a particular string in R:
<b>library(stringr)
#define string
some_string &lt;- "Hey there my name is Doug"
#extract "ther" from string
str_extract(some_string, "ther")
[1] "ther"
</b>
The pattern “ther” was successfully extracted from the string.
Note that if we attempt to extract some pattern that doesn’t exist in the string, we’ll simply receive <b>NA</b> as a result:
<b>library(stringr)
#define string
some_string &lt;- "Hey there my name is Doug"
#attempt to extract "apple" from string
str_extract(some_string, "apple")
[1] NA</b>
Since the pattern “apple” did not exist in the string, a value of <b>NA</b> was returned.
<h3>Example 2: Extract Numeric Values from String</h3>
The following code shows how to use the regex<b> \\d+</b> to extract only the numeric values from a string:
<b>library(stringr)
#define string
some_string &lt;- "There are 350 apples over there"
#extract only numeric values from string
str_extract(some_string, "\\d+")
[1] "350"
</b>
<h3>Example 3: Extract Characters from Vector of Strings</h3>
The following code shows how to use the regex <b>[a-z]+</b> to extract only characters from a vector of strings:
<b>library(stringr)
#define vector of strings
some_strings &lt;- c("4 apples", "3 bananas", "7 oranges")
#extract only characters from each string in vector
str_extract(some_strings, "[a-z]+")
[1] "apples"  "bananas" "oranges"
</b>
Notice that only the characters from each string are returned.
<h2><span class="orange">How to Use str_match in R (With Examples)</span></h2>
The <b>str_match()</b> function from the  stringr  package in R can be used to extract matched groups from a string.
This function uses the following syntax:
<b>str_match(string, pattern)
</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to look for
The following examples show how to use this function in practice
<h2>Example 1: Use str_match with Vector</h2>
The following code shows how to use the <b>str_match()</b> function to extract matched patterns from a character vector:
<b>library(stringr)
#create vector of strings
x &lt;- c('Mavs', 'Cavs', 'Heat', 'Thunder', 'Blazers')
#extract strings that contain 'avs'
str_match(x, pattern='avs')
     [,1] 
[1,] "avs"
[2,] "avs"
[3,] NA   
[4,] NA   
[5,] NA  
</b>
The result is a matrix in which each row displays the matched pattern or an <b>NA</b> value if the pattern was not found.
For example:
The pattern ‘avs’ was found in the first element ‘Mavs’, so ‘avs’ was returned.
The pattern ‘avs’ was found in the second element ‘Cavs’, so ‘avs’ was returned.
The pattern ‘avs was not found in the third element ‘Heat’ so NA was returned.
And so on.
<h2>Example 2: Use str_match with Data Frame</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Cavs', 'Heat', 'Thunder', 'Blazers'), points=c(99, 104, 110, 103, 115))
#view data frame
df
     team points
1    Mavs     99
2    Cavs    104
3    Heat    110
4 Thunder    103
5 Blazers    115
</b>
The following code shows how to use the <b>str_match()</b> function to add a new column to the data frame that either does or does not contain a matched pattern for each team name:
<b>library(stringr)
#create new column
df$match &lt;- str_match(df$team, pattern='avs')
#view updated data frame
df
     team points match
1    Mavs     99   avs
2    Cavs    104   avs
3    Heat    110  &lt;NA>
4 Thunder    103  &lt;NA>
5 Blazers    115  &lt;NA></b>
The new column titled <b>match</b> contains either the pattern ‘avs’ or NA, depending on whether the pattern is found in the <b>team</b> column.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
 How to Use str_count in R 
 How to Use str_pad in R 
<h2><span class="orange">How to Use str_pad in R (With Examples)</span></h2>
The <b>str_pad()</b> function from the  stringr  package in R can be used to pad characters to a string.
This function uses the following syntax:
<b>str_pad(string, width, side = c(“left”, “right”, “both”), pad = ” “)
</b>
where:
<b>string:</b> Character vector
<b>width:</b> Minimum width of padded strings
<b>side:</b> Side to add padding character (Default is left)
<b>pad:</b> Character to use for padding (Default is space)
The following examples show how to use this function in practice
<h2>Example 1: Pad String with Spaces</h2>
The following code shows how to use the <b>str_pad()</b> function to pad the left side of a string with spaces until the string has 10 total characters:
<b>library(stringr)
#create string
my_string &lt;- "Rhino"
#pad string to length of 10
str_pad(my_string, width=10)
[1] "     Rhino"
</b>
Notice that five spaces have been added to the left side of the string so that the string has a total length of 10.
Use the <b>side</b> argument to instead pad the right side of the string:
<b>library(stringr)
#create string
my_string &lt;- "Rhino"
#pad string to length of 10
str_pad(my_string, width=10, side="right")
[1] "Rhino     "
</b>
<h2>Example 2: Pad String with Specific Character</h2>
The following code shows how to use the <b>str_pad()</b> function to pad the left side of a string with underscores until the string has 10 total characters:
<b>library(stringr)
#create string
my_string &lt;- "Rhino"
#pad string to length of 10 using underscores
str_pad(my_string, width=10, pad="_")
[1] "_____Rhino"
</b>
Notice that five underscores have been added to the left side of the string so that the string has a total length of 10.
<h2>Example 3: Pad String with Specific Number of Characters</h2>
The following code shows how to use the <b>str_pad()</b> function along with the <b>nchar()</b> function to pad the left side of a string with a specific number (<b>5</b>) of characters:
<b>library(stringr)
#create string
my_string &lt;- "Rhino"
#pad string with 5 A's
str_pad(my_string, width=nchar(my_string)+5, pad="A")
[1] "AAAAARhino"
</b>
Notice that five A’s have been padded to the left side of the string.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
 How to Use str_count in R 
<h2><span class="orange">How to Use str_remove in R (With Examples)</span></h2>
The <b>str_remove()</b> function from the  stringr  package in R can be used to remove matched patterns from a string.
This function uses the following syntax:
<b>str_remove(string, pattern)
</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to look for
The following examples show how to use this function in practice
<h2>Example 1: Use str_remove with Vector</h2>
The following code shows how to use the <b>str_remove()</b> function to remove the first occurrence of the pattern “e” in a vector:
<b>library(stringr)
#create character vector
my_vector &lt;- "Hey there everyone."
#remove first occurrence of "e" from vector
str_remove(my_vector, "e")
[1] "Hy there everyone."
</b>
Notice that the first “e” has been removed from the vector, but all other “e” occurrences have remained.
To remove each occurrence of “e”, you can instead use the <b>str_remove_all()</b> function:
<b>library(stringr)
#create character vector
my_vector &lt;- "Hey there everyone."
#remove all occurrences of "e" from vector
str_remove_all(my_vector, "e")
[1] "Hy thr vryon."
</b>
Notice that every occurrence of “e” has bee removed from the string this time.
<h2>Example 2: Use str_remove with Data Frame</h2>
The following code shows how to use the <b>str_remove()</b> function to remove the pattern “avs” from every string in a particular column of a data frame:
<b>library(stringr)
#create data frame
df &lt;- data.frame(team=c('Mavs', 'Cavs', 'Heat', 'Hawks'), points=c(99, 94, 105, 122))
#view data frame
df
   team points
1  Mavs     99
2  Cavs     94
3  Heat    105
4 Hawks    122
#remove every occurrence of "avs" in the team column
df$team &lt;- str_remove(df$team, "avs")
#view updated data frame
df
   team points
1     M     99
2     C     94
3  Heat    105
4 Hawks    122
</b>
Notice that the pattern “avs” has been removed from the first two team names.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
 How to Use str_count in R 
 How to Use str_pad in R 
<h2><span class="orange">How to Use str_replace in R (With Examples)</span></h2>
The <b>str_replace()</b> function from the  stringr  package in R can be used to replace matched patterns in a string. This function uses the following syntax:
<b>str_replace(string, pattern, replacement)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to look for
<b>replacement:</b> A character vector of replacements
This tutorial provides several examples of how to use this function in practice on the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('team_A', 'team_B', 'team_C', 'team_D'), conference=c('West', 'West', 'East', 'East'), points=c(88, 97, 94, 104))
#view data frame
df
    team conference points
1 team_A       West     88
2 team_B       West     97
3 team_C       East     94
4 team_D       East    104</b>
<h2>Example 1: Replace String with Pattern</h2>
The following code shows how to replace the string “West” with “Western” in the conference column:
<b>library(stringr)
#replace "West" with "Western" in the conference column
df$conference &lt;- str_replace(df$conference, "West", "Western")
#view data frame
df
    team conference points
1 team_A    Western     88
2 team_B    Western     97
3 team_C       East     94
4 team_D       East    104
</b>
<h2>Example 2: Replace String with Nothing</h2>
The following code shows how to replace the string “team_” with nothing in the team column:
<b>#replace "team_" with nothing in the team column
df$team&lt;- str_replace(df$team, "team_", "")
#view data frame
df
  team conference points
1    A       West     88
2    B       West     97
3    C       East     94
4    D       East    104
</b>
<h2>Example 3: Replace Multiple Strings</h2>
The following code shows how to replace multiple strings in a single column. Specifically:
Replace “West” with “W”
Replace “East” with “E”
Since we’re replacing multiple strings, we use the <b>str_replace_all()</b> function:
<b>#replace multiple words in the conference column
df$conference &lt;- str_replace_all(df$conference, c("West" = "W", "East" = "E"))
#view data frame
df
    team conference points
1 team_A          W     88
2 team_B          W     97
3 team_C          E     94
4 team_D          E    104</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform Partial String Matching in R 
 How to Convert Strings to Dates in R 
 How to Convert Character to Numeric in R 
<h2><span class="orange">How to Use str_split in R (With Examples)</span></h2>
The <b>str_split()</b> function from the  stringr  package in R can be used to split a string into multiple pieces. This function uses the following syntax:
<b>str_split(string, pattern)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to split on
Similarly, the <b>str_split_fixed()</b> function from the stringr package can be used to split a string into a fixed number of pieces. This function uses the following syntax:
<b>str_split_fixed(string, pattern, n)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to split on
<b>n:</b> Number of pieces to return
This tutorial provides examples of how to use each of these functions on the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('andy & bob', 'carl & doug', 'eric & frank'), points=c(14, 17, 19))
#view data frame
df
          team points
1   andy & bob     14
2  carl & doug     17
3 eric & frank     19
</b>
<h3>Example 1: Split String Using str_split()</h3>
The following code shows how to split the string in the “team” column using the <b>str_split()</b> function:
<b>library(stringr)
#split the string in the <em>team</em> column on " & "
str_split(df$team, " & ")
[[1]]
[1] "andy" "bob" 
[[2]]
[1] "carl" "doug"
[[3]]
[1] "eric"  "frank"</b>
The result is a list of three elements that show the individual player names on each team.
<h3>
<b>Example 2: Split String Using  str_split_fixed()</b>
</h3>
The following code shows how to split the string in the “team” column into two fixed pieces using the <b>str_split_fixed()</b> function:
<b>library(stringr)
#split the string in the <em>team</em> column on " & "
str_split_fixed(df$team, " & ", 2)
     [,1]   [,2]   
[1,] "andy" "bob"  
[2,] "carl" "doug" 
[3,] "eric" "frank"</b>
The result is a matrix with two columns and three rows.
Once useful application of the <b>str_split_fixed()</b> function is to append the resulting matrix to the end of the data frame. For example:
<b>library(stringr)
#split the string in the <em>team</em> column and append resulting matrix to data frame
df[ , 3:4] &lt;- str_split_fixed(df$team, " & ", 2)
#view data frame
df
          team points   V3    V4
1   andy & bob     14 andy   bob
2  carl & doug     17 carl  doug
3 eric & frank     19 eric frank</b>
The column titled ‘V3’ shows the name of the first player on the team and the column titled ‘V4’ shows the name of the second player on the team.
<h2><span class="orange">How to Use str_sub in R (With Examples)</span></h2>
The <b>str_sub()</b> function from the  stringr  package in R can be used to extract or replace substrings in a string.
This function uses the following syntax:
<b>str_sub(string, start, end)</b>
where:
<b>string:</b> Character vector
<b>start:</b> Position of the first character
<b>end:</b> Position of the last character
This tutorial provides several examples of how to use this function in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('team_A', 'team_B', 'team_C', 'team_D'), conference=c('West', 'West', 'East', 'East'), points=c(88, 97, 94, 104))
#view data frame
df
    team conference points
1 team_A       West     88
2 team_B       West     97
3 team_C       East     94
4 team_D       East    104</b>
<h2>Example 1: Extract Substring in a String</h2>
The following code shows how to extract the substring that starts in position 5 and ends in position 6 for each string in the “team” column:
<b>library(stringr)
#extract characters in positions 5 through 6 of 'team' column
str_sub(string=df$team, start=5, end=6)
[1] "_A" "_B" "_C" "_D"
</b>
<h2>Example 2: Extract Substring Up to Specific Position</h2>
The following code shows how to extract every character up to position 4 for each string in the “team” column:
<b>library(stringr)
#extract all characters up to position 4 in 'team' column
str_sub(string=df$team, end=4)
[1] "team" "team" "team" "team"</b>
<h2>Example 3: Extract Substring Starting at Specific Position</h2>
The following code shows how to extract every character after position 3 for each string in the “team” column:
<b>library(stringr)
#extract all characters after position 2 in 'team' column
str_sub(string=df$team, start=3)
[1] "am_A" "am_B" "am_C" "am_D"
</b>
<h2>Example 4: Replace Substring in a String</h2>
The following code shows how to replace the substring starting at position 1 and ending at position 5 for each string in the “team” column:
<b>library(stringr)
#replace all characters between position 1 and 5 in 'team' column
str_sub(string=df$team, start=1, end=5) &lt;- 'TEAM'
#view updated data frame
df
   team conference points
1 TEAMA       West     88
2 TEAMB       West     97
3 TEAMC       East     94
4 TEAMD       East    104</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
<h2><span class="orange">How to Use str_trim in R (With Examples)</span></h2>
The <b>str_trim()</b> function from the  stringr  package in R can be used to trim whitespace from a string.
This function uses the following syntax:
<b>str_trim(string, side = c(“both”, “left”, “right”))</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Side on which to remove whitespace
The following examples show how to use this function in practice
<h2>Example 1: Trim Whitespace from Left Side</h2>
The following code shows how to use the <b>str_trim()</b> function to trim whitespace from the left side of a string:
<b>library(stringr)
#create string
my_string &lt;- "   Hey there everyone.  "
#view string
my_string
[1] "   Hey there everyone.  "
#create new string with white space removed from left
new_string &lt;- str_trim(my_string, side="left")
#view new string
new_string
[1] "Hey there everyone.  "
</b>
Notice that all of the whitespace on the left side of the string has been trimmed.
<h2>Example 2: Trim Whitespace from Right Side</h2>
The following code shows how to use the <b>str_trim()</b> function to trim whitespace from the right side of a string:
<b>library(stringr)
#create string
my_string &lt;- "   Hey there everyone.  "
#view string
my_string
[1] "   Hey there everyone.  "
#create new string with white space removed from right
new_string &lt;- str_trim(my_string, side="right")
#view new string
new_string
[1] "   Hey there everyone."
</b>
Notice that all of the whitespace on the right side of the string has been trimmed.
<h2>Example 3: Trim Whitespace from Both Sides</h2>
The following code shows how to use the <b>str_trim()</b> function to trim whitespace from both sides of a string:
<b>library(stringr)
#create string
my_string &lt;- "   Hey there everyone.  "
#view string
my_string
[1] "   Hey there everyone.  "
#create new string with white space removed from both sides
new_string &lt;- str_trim(my_string, side="both")
#view new string
new_string
[1] "Hey there everyone."
</b>
Notice that all of the whitespace on both sides of the string have been trimmed.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Use str_replace in R 
 How to Use str_split in R 
 How to Use str_detect in R 
 How to Use str_count in R 
<h2><span class="orange">How to Perform Stratified Sampling in Excel (Step-by-Step)</span></h2>
In statistics, we often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>stratified random sampling</b>, in which a population is split into groups and a certain number of members from each group are randomly selected to be included in the sample.
The following step-by-step example shows how to perform stratified random sampling in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following dataset into Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/strat1.png">
Next, we’ll perform stratified random sampling in which we randomly select two players from each basketball team to be included in the final sample.
<h3>Step 2: Enter Random Values for Each Row</h3>
Next, let’s create a new column titled <b>Random</b> and type in <b>=RAND()</b> for the first value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/strat2.png">
This generates a random value between 0 and 1.
Next, hover over the bottom right corner of the cell until a tiny cross ( <b>+</b> ) appears and double click it to paste the <b>=RAND()</b> formula to all remaining cells in the column.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/strat3.png">
Unfortunately, each time we hit Enter the random cell values will change. To prevent this, copy every value in column E then right click and choose <b>Paste Values</b> into the same column so that the random values will no longer change.
<h3>Step 3: Sort Data Values</h3>
Next, highlight all of the data. Then click the <b>Data</b> tab along the top ribbon. Then click the <b>Sort</b> button within the <b>Sort & Filter</b> group.
In the new window that appears, sort first by <b>Team</b> from A to Z, then sort by <b>Random</b> from Smallest to Largest.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/strat4.png">
Once you click <b>OK</b>, the data will be sorted accordingly.
<h3>Step 4: Select the Final Sample</h3>
The final sample will simply be the first two rows from each team:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/strat5.png">
The final sample will include the following player ID’s from each team:
<b>Team A</b>: 3, 5
<b>Team B</b>: 9, 6
<b>Team C</b>: 15, 11
Our stratified random sampling is complete because we have now chosen two players from each team.
<h2><span class="orange">Stratified Sampling in Pandas (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>stratified random sampling</b>, in which a population is split into groups and a certain number of members from each group are randomly selected to be included in the sample.
This tutorial explains two methods for performing stratified random sampling in Python.
<h3>Example 1: Stratified Sampling Using Counts</h3>
Suppose we have the following pandas DataFrame that contains data about 8 basketball players on 2 different teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'G', 'F', 'F', 'C', 'C'],   'assists': [5, 7, 7, 8, 5, 7, 6, 9],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10]})
#view DataFrame
df
        teamposition assists rebounds
0AG 5 11
1AG 7 8
2AF 7 10
3AG 8 6
4BF 5 6
5BF 7 9
6BC 6 6
7BC 9 10
</b>
The following code shows how to perform stratified random sampling by randomly selecting 2 players from each team to be included in the sample:
<b>df.groupby('team', group_keys=False).apply(lambda x: x.sample(2))
        teamposition assists rebounds
0AG 5 11
3AG 8 6
6BC 6 6
5BF 7 9
</b>
Notice that two players from each team are included in the stratified sample.
<h3>Example 2: Stratified Sampling Using Proportions</h3>
Once again suppose we have the following pandas DataFrame that contains data about 8 basketball players on 2 different teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'G', 'F', 'F', 'C', 'C'],   'assists': [5, 7, 7, 8, 5, 7, 6, 9],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10]})
#view DataFrame
df
        teamposition assists rebounds
0AG 5 11
1AG 7 8
2AF 7 10
3AG 8 6
4BF 5 6
5BF 7 9
6BC 6 6
7BC 9 10
</b>
Notice that 6 of the 8 players (75%) in the DataFrame are on team A and 2 out of the 8 players (25%) are on team B.
The following code shows how to perform stratified random sampling such that the proportion of players in the sample from each team matches the proportion of players from each team in the larger DataFrame:
<b>import numpy as np
#define total sample size desired
N = 4
#perform stratified random sampling
df.groupby('team', group_keys=False).apply(lambda x: x.sample(int(np.rint(N*len(x)/len(df))))).sample(frac=1).reset_index(drop=True)
        teamposition  assists  rebounds
0BF  7   9
1BG  8   6
2BC  6   6
3AG  7   8
</b>
Notice that the proportion of players from team A in the stratified sample (25%) matches the proportion of players from team A in the larger DataFrame.
Similarly, the proportion of players from team B in the stratified sample (75%) matches the proportion of players from team B in the larger DataFrame. 
<h2><span class="orange">Stratified Sampling in R (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>stratified random sampling</b>, in which a population is split into groups and a certain number of members from each group are randomly selected to be included in the sample.
This tutorial explains how to perform stratified random sampling in R.
<h2>Example: Stratified Sampling in R</h2>
A high school is composed of 400 students who are either Freshman, Sophomores, Juniors, or Seniors. Suppose we’d like to take a stratified sample of 40 students such that 10 students from each grade are included in the sample.
The following code shows how to generate a sample data frame of 400 students:
<b>#make this example reproducible
set.seed(1)
#create data frame
df &lt;- data.frame(grade = rep(c('Freshman', 'Sophomore', 'Junior', 'Senior'), each=100), gpa = rnorm(400, mean=85, sd=3))
#view first six rows of data frame
head(df)
     grade      gpa
1 Freshman 83.12064
2 Freshman 85.55093
3 Freshman 82.49311
4 Freshman 89.78584
5 Freshman 85.98852
6 Freshman 82.53859</b>
<h3>Stratified Sampling Using Number of Rows</h3>
The following code shows how to use the <b>group_by() </b>and <b>sample_n()</b> functions from the  dplyr  package to obtain a stratified random sample of 40 total students with 10 students from each grade:
<b>library(dplyr)
#obtain stratified sample
strat_sample &lt;- df %>%  group_by(grade) %>%  sample_n(size=10)
#find frequency of students from each grade
table(strat_sample$grade)
 Freshman    Junior    Senior Sophomore 
       10        10        10        10 
</b>
<h3>Stratified Sampling Using Fraction of Rows</h3>
The following code shows how to use the <b>group_by() </b>and <b>sample_frac()</b> functions from the  dplyr  package to obtain a stratified random sample in which we randomly select 15% of students from each grade:
<b>library(dplyr)
#obtain stratified sample
strat_sample &lt;- df %>%  group_by(grade) %>%  sample_frac(size=.15)
#find frequency of students from each grade
table(strat_sample$grade)
 Freshman    Junior    Senior Sophomore 
       15        15        15        15 </b>
<h2><span class="orange">How to Create a Strip Chart in R</span></h2>
A <b>strip chart</b> is a type of chart that displays numerical data along a single strip. Similar to  boxplots , strip charts can help you visualize the distribution of data. Strip charts can be a good alternative to boxplots when the sample sizes are small so that you can see the individual data points.
 This tutorial explains how to create a strip chart in R using the built-in <b>stripchart()</b> function.
<h2>The stripchart() function</h2>
The basic syntax to create a strip chart in R is as follows:
stripchart(x, method, jitter, main, xlab, ylab, col, pch, vertical, group.names)
<b>x</b>: a numeric vector or a list of numeric vectors to be plotted. This is the only required argument to produce a plot.
<b>method</b>: the method to be used to separate points that have identical values. The default method “overplot” causes such points to be overplotted, but it’s possible to specify “jitter” to jitter the points or “stack” to stack the points.
<b>jitter</b>: when method = “jitter” is used, this provides the amount of jittering to be applied.
main: title of the chart
<b>xlab</b>: x-axis label
<b>ylab</b>: y-axis label
<b>col</b>: color of the points in the plot
<b>pch</b>: shape of the points in the plot
<b>vertical</b>: when vertical is “TRUE”, the plot is drawn vertically rather than the default horizontal
<b>group.names</b>: group labels to be printed alongside the plot, if more than one numeric vector is being plotted.
<h2>Strip Chart for a Single Numeric Vector</h2>
The following example uses the built-in R dataset <b>iris </b>to create a strip chart for a single numeric vector.
<b>#view first six rows of <em>iris </em>dataset
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
</b>
The following code creates a basic strip chart for the variable <b>Sepal.Length</b>:
<b>stripchart(iris$Sepal.Length)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart1.jpg">
We can also add additional arguments to add a title and x-axis label, change the color of the points, change the shape of the points, and use the method “jitter” so that individual points don’t overlap each other:
<b>stripchart(iris$Sepal.Length,
           main = 'Sepal Length Distribution',
           xlab = 'Sepal Length',
           col = 'red',
           pch = 1,
           method = 'jitter')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart2.jpg">
Instead of jittering the points, we can “stack” them instead:
<b>stripchart(iris$Sepal.Length,
           main = 'Sepal Length Distribution',
           xlab = 'Sepal Length',
           col = 'red',
           pch = 1,
           method = 'stack')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart3.jpg">
We can also display the plot vertically instead of the default horizontal, and change the axis label to be on the y-axis instead:
<b>stripchart(iris$Sepal.Length,
           main = 'Sepal Length Distribution',
           ylab = 'Sepal Length',
           col = 'red',
           pch = 1,
           method = 'jitter',
           vertical = TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart4.jpg">
<h2>Strip Chart for Multiple Numeric Vectors</h2>
We can also draw multiple strip charts in a single plot by passing in a list of numeric vectors. 
The following code creates a list that contains the variables <em>Sepal Length </em>and <em>Sepal Width</em> in the <b>iris </b>dataset and produces a strip chart for each variable in a single plot:
<b>#create list of variables
x &lt;- list('Sepal Length' = iris$Sepal.Length, 'Sepal Width' = iris$Sepal.Width)
#create plot that contains one strip chart per variable
stripchart(x,
           main = 'Sepal Width & Length Distributions',
           xlab = 'Measurement', 
           ylab = 'Variable',
           col = c('steelblue', 'coral2'),
           pch = 16,
           method = 'jitter')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart5.jpg">
Just as in the example before, we can choose to plot the strip charts vertically instead of the default horizontal:
<b>stripchart(x, main = 'Sepal Width & Length Distributions',
           xlab = 'Measurement', 
           ylab = 'Variable',
           col = c('steelblue', 'coral2'),
           pch = 16,
           method = 'jitter',
           vertical = TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart6.jpg">
In addition, we can pass a formula in the form of y~x into the stripchart() function, where <em>y </em>is a numeric vector grouped by the value of <em>x</em>. 
For example, in the <b>iris </b>dataset we could group the data according to <em>Species</em> which has three distinct values (“setosa”, “versicolor”, and “virginica”) and then plot the <em>Sepal Length </em>for each species in a strip chart:
<b>stripchart(Sepal.Length ~ Species,
           data = iris,
           main = 'Sepal Length by Species',
           xlab = 'Species', 
           ylab = 'Sepal Length',
           col = c('steelblue', 'coral2', 'purple'),
           pch = 16,
           method = 'jitter',
           vertical = TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/stripchart7.jpg">
To view the full documentation on the stripchart() function in R, simply type in:
<b>?stripchart</b>
<h2><span class="orange">How to Use strptime and strftime Functions in R</span></h2>
You can use the <b>strptime</b> and <b>strftime</b> functions in R to convert between character and time objects.
The <b>strptime</b> function converts characters to time objects and uses the following basic syntax:
<b>strptime(character_object, format="%Y-%m-%d")</b>
The <b>strftime</b> function converts time objects to characters and uses the following basic syntax:
<b>strftime(time_object)</b>
The following examples show how to use each function in practice.
<h3>Example 1: Use strptime Function in R</h3>
Suppose we have the following character vector in R:
<b>#create character vector
char_data &lt;- c("2022-01-01", "2022-01-25", "2022-02-14", "2022-03-19")
#view class of vector
class(char_data)
[1] "character"
</b>
We can use the <b>strptime</b> function to convert the characters to time objects:
<b>#convert characters to time objects
time_data &lt;- strptime(char_data, format="%Y-%m-%d")
#view new vector
time_data
[1] "2022-01-01 UTC" "2022-01-25 UTC" "2022-02-14 UTC" "2022-03-19 UTC"
#view class of new vector
class(time_data)
[1] "POSIXlt" "POSIXt"
</b>
We can see that the characters have been converted to time objects.
Note that we can also use the <b>tz</b> argument to convert the characters to time objects with a specific time zone.
For example, we could specify “EST” to convert the characters to time objects in the Eastern Time Zone:
<b>#convert characters to time objects in EST time zone
time_data &lt;- strptime(char_data, format="%Y-%m-%d", tz="EST")
#view new vector
time_data
[1] "2022-01-01 EST" "2022-01-25 EST" "2022-02-14 EST" "2022-03-19 EST"
</b>
Notice that each of the time objects now end with <b>EST</b>, which indicates an Eastern Time Zone.
<h3>Example 2: Use strftime Function in R</h3>
Suppose we have the following vector of time objects in R:
<b>#create vector of time objects
time_data &lt;- as.POSIXct(c("2022-01-01", "2022-01-25", "2022-02-14"))
#view class of vector
class(time_data)
[1] "POSIXct" "POSIXt"
</b>
We can use the <b>strftime</b> function to convert the time objects to characters:
<b>#convert time objects to characters
char_data &lt;- strftime(time_data)
#view new vector
char_data
[1] "2022-01-01" "2022-01-25" "2022-02-14"
#view class of new vector
class(char_data)
[1] "character"
</b>
We can see that the time objects have been converted to characters.
<h2><span class="orange">How to Use strsplit() Function in R to Split Elements of String</span></h2>
The <b>strsplit()</b> function in R can be used to split a string into multiple pieces. This function uses the following syntax:
<b>strsplit(string, pattern)</b>
where:
<b>string:</b> Character vector
<b>pattern:</b> Pattern to split on
The following examples show how to use this function in practice.
<h3>Example 1: Split String Based on Spaces</h3>
The following code shows how to use the <b>strsplit()</b> function to split a string based on spaces:
<b>#split string based on spaces
split_up &lt;- strsplit("Hey there people", split=" ")
#view results
split_up
[[1]]
[1] "Hey"    "there"  "people"
#view class of split_up
class(split_up)
[1] "list"
</b>
The result is a list of three elements that are split based on the spaces in the original string.
We can use the <b>unlist()</b> function if we would instead like to produce a <b>vector</b> as the result:
<b>#split string based on spaces
split_up &lt;- unlist(strsplit("Hey there people", split=" "))
#view results
split_up
[1] "Hey"    "there"  "people"
#view class of split_up
class(split_up)
[1] "character"</b>
We can see that the result is a character vector.
<h3>
<b>Example 2: Split String Based on Custom Delimiter</b>
</h3>
We can also use the <b>strplit()</b> function to split a string based on a custom delimiter, such as a dash:
<b>#split string based on dashes
strsplit("Hey-there-people", split="-")
[[1]]
[1] "Hey"    "there"  "people"</b>
The result is a list of three elements that are split based on the dashes in the original string.
<h3>
<b>Example 3: Split String Based on Several Delimiters</b>
</h3>
We can also use brackets within the <b>split</b> argument of the <b>strplit()</b> function to split a string based on several different delimiters:
<b>#split string based on several delimiters
strsplit("Hey&there-you/people", split="[&-/]")
[[1]]
[1] "Hey"    "there"  "you"    "people"
</b>
The result is a list of elements that were split whenever any of the following delimiters were present in the original string:
Ampersand (<b>&</b>)
Dash (<b>–</b>)
Slash (<b>/</b>)
<h2><span class="orange">How to Calculate Studentized Residuals in Python</span></h2>
A <b>studentized residual</b> is simply a residual divided by its estimated standard deviation.
In practice, we typically say that any  observation  in a dataset that has a studentized residual greater than an absolute value of 3 is an outlier.
We can quickly obtain the studentized residuals of a regression model in Python by using the  OLSResults.outlier_test()  function from statsmodels, which uses the following syntax:
<b>OLSResults.outlier_test()</b>
where <i>OLSResults </i>is the name of a linear model fit using the <b>ols()</b> function from statsmodels.
<h3>Example: Calculating Studentized Residuals in Python</h3>
Suppose we build the following  simple linear regression  model in Python:
<b>#import necessary packages and functions
import numpy as np
import pandas as pd
import statsmodels.api as sm
from statsmodels.formula.api import ols
#create dataset
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19]})
#fit simple linear regression model
model = ols('rating ~ points', data=df).fit()
</b>
We can use the <b>outlier_test()</b> function to produce a DataFrame that contains the studentized residuals for each observation in the dataset:
<b>#calculate studentized residuals
stud_res = model.outlier_test()
#display studentized residuals
print(stud_res)
    student_resid unadj_p bonf(p)
0-0.4864710.6414941.000000
1-0.4919370.6378141.000000
2  0.1720060.8683001.000000
3 1.2877110.2387811.000000
4 0.1069230.9178501.000000
5 0.7488420.4783551.000000
6-0.9681240.3652341.000000
7-2.4099110.0467800.467801
8 1.6880460.1352581.000000
9-0.0141630.9890951.000000
</b>
This DataFrame displays the following values for each observation in the dataset:
The studentized residual
The unadjusted p-value of the studentized residual
The Bonferroni-corrected p-value of the studentized residual
We can see that the studentized residual for the first observation in the dataset is <b>-0.486471</b>, the studentized residual for the second observation is <b>-0.491937</b>, and so on.
We can also create a quick plot of the predictor variable values vs. the corresponding studentized residuals:
<b>import matplotlib.pyplot as plt
#define predictor variable values and studentized residuals
x = df['points']
y = stud_res['student_resid']
#create scatterplot of predictor variable vs. studentized residuals
plt.scatter(x, y)
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel('Points')
plt.ylabel('Studentized Residuals') 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/studres1.png">
From the plot we can see that none of the observations have a studentized residual with an absolute value greater than 3, thus there are no clear outliers in the dataset.
<h2><span class="orange">How to Calculate Studentized Residuals in R</span></h2>
A <b>studentized residual</b> is simply a residual divided by its estimated standard deviation.
In practice, we typically say that any  observation  in a dataset that has a studentized residual greater than an absolute value of 3 is an outlier.
We can quickly obtain the studentized residuals of any regression model in R by using the <b>studres()</b> function from the MASS package, which uses the following syntax:
<b>studres(model)</b>
where <em>model</em> represents any linear model.
<h3>Example: Calculating Studentized Residuals in R</h3>
Suppose we build the following  simple linear regression  model in R, using the built-in <em>mtcars</em> dataset:
<b>#build simple linear regression model
model &lt;- lm(mpg ~ disp, data=mtcars)
</b>
We can use the <b>studres()</b> function from the MASS package to calculate the studentized residuals for each observation in the dataset:
<b>library(MASS)
#calculate studentized residuals
stud_resids &lt;- studres(model)
#view first three studentized residuals
head(stud_resids, 3)
    Mazda RX4 Mazda RX4 Wag    Datsun 710 
   -0.6236250    -0.6236250    -0.7405315 
</b>
We can also create a quick plot of the predictor variable values vs. the corresponding studentized residuals:
<b>#plot predictor variable vs. studentized residuals
plot(mtcars$disp, stud_resids,  ylab='Studentized Residuals', xlab='Displacement') 
#add horizontal line at 0
abline(0, 0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/studentized1.png">
From the plot we can see that none of the observations have a studentized residual with an absolute value greater than 3, thus there are no clear outliers in the dataset.
We can also add the studentized residuals of each observation back into the original dataset if we’d like:
<b>#add studentized residuals to orignal dataset
final_data &lt;- cbind(mtcars[c('mpg', 'disp')], stud_resids)
#view final dataset
head(final_data)
   mpg disp stud_resids
Mazda RX4         21.0  160  -0.6236250
Mazda RX4 Wag     21.0  160  -0.6236250
Datsun 710        22.8  108  -0.7405315
Hornet 4 Drive    21.4  258   0.7556078
Hornet Sportabout 18.7  360   1.2658336
Valiant           18.1  225  -0.6896297</b>
We can then sort each observation from largest to smallest according to its studentized residual to get an idea of which observations are closest to being outliers:
<b>#sort studentized residuals descending
final_data[order(-stud_resids),]
     mpg  disp stud_resids
Toyota Corolla      33.9  71.1  2.52397102
Pontiac Firebird    19.2 400.0  2.06825391
Fiat 128            32.4  78.7  2.03684699
Lotus Europa        30.4  95.1  1.53905536
Honda Civic         30.4  75.7  1.27099586
Hornet Sportabout   18.7 360.0  1.26583364
Chrysler Imperial   14.7 440.0  1.06486066
Hornet 4 Drive      21.4 258.0  0.75560776
Porsche 914-2       26.0 120.3  0.42424678
Fiat X1-9           27.3  79.0  0.30183728
Merc 240D           24.4 146.7  0.26235893
Ford Pantera L      15.8 351.0  0.20825609
Cadillac Fleetwood  10.4 472.0  0.08338531
Lincoln Continental 10.4 460.0 -0.07863385
Duster 360          14.3 360.0 -0.14476167
Merc 450SL          17.3 275.8 -0.28759769
Dodge Challenger    15.5 318.0 -0.30826585
Merc 230            22.8 140.8 -0.30945955
Merc 450SE          16.4 275.8 -0.56742476
AMC Javelin         15.2 304.0 -0.58138205
Camaro Z28          13.3 350.0 -0.58848471
Mazda RX4 Wag       21.0 160.0 -0.62362497
Mazda RX4           21.0 160.0 -0.62362497
Maserati Bora       15.0 301.0 -0.68315010
Valiant             18.1 225.0 -0.68962974
Datsun 710          22.8 108.0 -0.74053152
Merc 450SLC         15.2 275.8 -0.94814699
Toyota Corona       21.5 120.1 -0.99751166
Volvo 142E          21.4 121.0 -1.01790487
Merc 280            19.2 167.6 -1.09979261
Ferrari Dino        19.7 145.0 -1.24732999
Merc 280C           17.8 167.6 -1.57258064
</b>
<h2><span class="orange">Sturges’ Rule Calculator</span></h2>
<b> Sturges’ Rule </b> uses the following formula to determine the optimal number of bins to use in a histogram:
Number of bins = <U+2308>log<sub>2</sub>n + 1<U+2309>
To apply Sturges’ Rule to a given dataset, simply enter the sample size of the dataset in the box below and then click the “Calculate” button.
<label for="n"><b>n (the sample size)</b></label>
<input type="number" id="n" value="31">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Number of Bins to Use: </b> 6
<script>
function calc() {
//get input values
var n  = document.getElementById('n').value*1;
//find number of bins
var bins = Math.ceil( Math.log2(n) - (-1) );
//output
document.getElementById('n_out').innerHTML = bins;
}
</script>
<h2><span class="orange">What is Sturges’ Rule? (Definition & Example)</span></h2>
A <b>histogram</b> is a chart that helps us visualize the distribution of values in a dataset.
It turns out that the number of bins used in a histogram can have a huge impact on how we interpret the data.
If we use too few bins, the true underlying pattern in the data can be hidden:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/sturges1.png">
And if we use too many bins, we may just be visualizing the noise in a dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/sturges2.png">
Fortunately, we can use a method known as Sturges’ Rule to determine the optimal number of bins to use in a histogram.
<b>Sturges’ Rule</b> uses the following formula to determine the optimal number of bins to use in a histogram:
<b>Optimal Bins = <U+2308>log<sub>2</sub>n + 1<U+2309></b>
where:
<b>n: </b>The total number of  observations  in the dataset.
<b><U+2308> <U+2309>:</b> Symbols that mean “ceiling” – i.e. round the answer up to the nearest integer.
<h3>Example: Sturges’ Rule</h3>
Suppose we have the following dataset with n = 31 total observations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/sturges3.png">
We can use Sturges’ Rule to determine the optimal number of bins to use to visualize these values in a histogram:
<b>Optimal Bins</b> = <U+2308>log<sub>2</sub>(31) + 1<U+2309> = <U+2308>4.954 + 1<U+2309> = <U+2308>5.954<U+2309> = <b>6</b>.
According to Sturges’ Rule, we should use 6 bins in the histogram we use to visualize this distribution of values.
Here’s what a histogram with 6 bins would look like for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/sturges4.png">
Notice how this seems to be enough bins to get a good idea of the underlying distribution of values without being too many that we’re just visualizing the noise in the data.
<h3>Common Values for Sturges’ Rule</h3>
The following table shows the optimal number of bins to use in a histogram based on the total number of observations in a dataset, according to Sturges’ Rule:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/sturges5.png">
<h3>Alternatives to Sturges’ Rule</h3>
Sturges’ Rule is the most common method for determining the optimal number of bins to use in a histogram, but there are several alternative methods including:
<b>The Square-root Rule</b>: Number of bins = <U+2308>√n<U+2309>
<b>The Rice Rule:</b> Number of bins = <U+2308>2 * <sup>3</sup>√n<U+2309>
<b>The Freedman-Diaconis’ Rule:</b> Number of bins = (2*IQR) / <sup>3</sup>√n where <em>IQR</em> is the interquartile range.
<h3>Bonus: Sturges’ Rule Calculator</h3>
Use  this free online calculator  to automatically apply Sturges’ Rule to determine the optimal number of bins to use for a histogram based on the size of a dataset.
<h2><span class="orange">How to Use sub() Function in R (With Examples)</span></h2>
The <b>sub()</b> function in R can be used to replace the first occurrence of certain text within a string in R.
This function uses the following basic syntax:
<b>sub(pattern, replacement, x) </b>
where:
<b>pattern</b>: The pattern to look for
<b>replacement</b>: The replacement for the pattern
<b>x</b>: The string to search
The following examples show how to use this function in practice.
<b>Note</b>: To replace all occurrences of certain text in a string, use the  gsub()  function instead.
<h2>Example 1: Replace One Specific Text in String</h2>
The following code shows how to replace the text “cool” with “nice” in a string in R:
<b>#create string
my_string &lt;- 'This is a cool string'
#replace 'cool' with 'nice'
my_string &lt;- sub('cool', 'nice', my_string)
#view updated string
my_string
[1] "This is a nice string"
</b>
Notice that “cool” has been replaced with “nice” in the string.
<h2>Example 2: Replace One of Several Specific Texts in String</h2>
The following code shows how to replace the texts “zebra”, “walrus”, and “peacock” with “dog” if any of them occur in a string:
<b>#create string
my_string &lt;- 'My favorite animal is a walrus'
#replace either zebra, walrus, or peacock with dog
my_string &lt;- sub('zebra|walrus|peacock', 'dog', my_string)
#view updated string
my_string
[1] "My favorite animal is a dog"
</b>
Notice that “walrus” has been replaced with “dog” in the string.
<b>Note</b>: The <b>|</b> operator stands for “OR” in R.
<h2>Example 3: Replace Numeric Values in String</h2>
The following code shows how to replace all numeric values in a string with the text “a lot”:
<b>#create string
my_string &lt;- 'There are 400 dogs out here'
#replace numeric values with 'a lot'
my_string &lt;- sub('[[:digit:]]+', 'a lot of', my_string)
#view updated string
my_string
[1] "There are a lot of dogs out here"
</b>
Notice that the numeric value of 400 has been replaced with “a lot” in the string.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use diff Function in R 
 How to Use seq Function in R 
 How to Use diff Function in R 
<h2><span class="orange">How to Adjust Subplot Size in Matplotlib</span></h2>
You can use the following syntax to adjust the size of subplots in Matplotlib:
<b>#specify one size for all subplots
fig, ax = plt.subplots(2, 2, figsize=(10,7))
#specify individual sizes for subplots
fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]})
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Specify One Size for All Subplots</h3>
The following code shows how to specify one size for all subplots:
<b>import matplotlib.pyplot as plt
#define subplots
fig, ax = plt.subplots(2, 2, figsize=(10,7))
fig.tight_layout()
#define data
x = [1, 2, 3]
y = [7, 13, 24]
#create subplots
ax[0, 0].plot(x, y, color='red')
ax[0, 1].plot(x, y, color='blue')
ax[1, 0].plot(x, y, color='green')
ax[1, 1].plot(x, y, color='purple')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/subplot1.png">
We can easily change the size of the subplots by changing the values in the <b>figsize</b> argument:
<b>import matplotlib.pyplot as plt
#define subplots
fig, ax = plt.subplots(2, 2, figsize=(5,5))
fig.tight_layout()
#define data
x = [1, 2, 3]
y = [7, 13, 24]
#create subplots
ax[0, 0].plot(x, y, color='red')
ax[0, 1].plot(x, y, color='blue')
ax[1, 0].plot(x, y, color='green')
ax[1, 1].plot(x, y, color='purple')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/subplot2.png">
<h3>
<b>Example 2: Specify Sizes for Individual Subplots</b>
</h3>
The following code shows how to specify different sizes for individual subplots:
<b>import matplotlib.pyplot as plt
#define subplots
fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [3, 1]})
fig.tight_layout()
#define data
x = [1, 2, 3]
y = [7, 13, 24]
#create subplots
ax[0].plot(x, y, color='red')
ax[1].plot(x, y, color='blue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/subplot3.png">
We can easily change the size of the subplots by changing the values in the <b>width_ratios</b> argument:
<b>import matplotlib.pyplot as plt
#define subplots
fig, ax = plt.subplots(1, 2, gridspec_kw={'width_ratios': [1, 3]})
fig.tight_layout()
#define data
x = [1, 2, 3]
y = [7, 13, 24]
#create subplots
ax[0].plot(x, y, color='red')
ax[1].plot(x, y, color='blue')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/subplot4.png">
<h2><span class="orange">How to Subset by a Date Range in R (With Examples)</span></h2>
The easiest way to subset a data frame by a date range in R is to use the following syntax:
<b>df[df$date >= "some date" & df$date &lt;= "some date", ]
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Subset Between Two Dates</h3>
The following code shows how to select the rows of a data frame that fall between two dates, inclusive:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(date = as.Date("2021-01-01") - 0:19, sales = runif(20, 10, 500) + seq(50, 69)^2)
#view first six rows
head(df)
        date    sales
1 2021-01-01 2949.382
2 2020-12-31 2741.099
3 2020-12-30 2896.341
4 2020-12-29 3099.698
5 2020-12-28 3371.022
6 2020-12-27 3133.824
#subset between two dates, inclusive
df[df$date >= "2020-12-25" & df$date &lt;= "2020-12-28", ]
        date    sales
5 2020-12-28 3371.022
6 2020-12-27 3133.824
7 2020-12-26 3586.211
8 2020-12-25 3721.891
</b>
You only need to modify the greater and less than signs to select the rows that fall between two dates, exclusive:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(date = as.Date("2021-01-01") - 0:19, sales = runif(20, 10, 500) + seq(50, 69)^2)
#subset between two dates, exclusive
df[df$date > "2020-12-25" & df$date &lt; "2020-12-28", ]
        date    sales
6 2020-12-27 3133.824
7 2020-12-26 3586.211
</b>
<h3>Example 2: Subset After a Certain Date</h3>
The following code shows how to select the rows of a data frame that occur after a certain date:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(date = as.Date("2021-01-01") - 0:19, sales = runif(20, 10, 500) + seq(50, 69)^2)
#subset after a certain date
df[df$date >= "2020-12-22", ]
         date    sales
1  2021-01-01 2949.382
2  2020-12-31 2741.099
3  2020-12-30 2896.341
4  2020-12-29 3099.698
5  2020-12-28 3371.022
6  2020-12-27 3133.824
7  2020-12-26 3586.211
8  2020-12-25 3721.891
9  2020-12-24 3697.791
10 2020-12-23 3799.266
11 2020-12-22 3640.275</b>
<h3>Example 3: Subset Before a Certain Date</h3>
The following code shows how to select the rows of a data frame that occur before a certain date:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(date = as.Date("2021-01-01") - 0:19, sales = runif(20, 10, 500) + seq(50, 69)^2)
#subset before a certain date
df[df$date &lt; "2020-12-22", ]
         date    sales
12 2020-12-21 3831.928
13 2020-12-20 3940.513
14 2020-12-19 4315.641
15 2020-12-18 4294.211
16 2020-12-17 4612.222
17 2020-12-16 4609.873
18 2020-12-15 4850.633
19 2020-12-14 5120.034
20 2020-12-13 4957.217</b>
<h2><span class="orange">How to Subset Data Frame by Factor Levels in R</span></h2>
You can use one of the following methods to subset a data frame by factor levels in R:
<b>Method 1: Subset by One Factor Level</b>
<b>#subset rows where team is equal to 'B'</b>
<b>df_sub &lt;- df[df$team == 'B', ]</b>
<b>Method 2: Subset by Multiple Factor Levels</b>
<b>#subset rows where team is equal to 'A' or 'C'
df_sub &lt;- df[df$team %in% c('A', 'C'), ]
</b>
The following examples show how to use each of these methods in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=factor(c('A', 'A', 'B', 'B', 'B', 'C')), points=c(22, 35, 19, 15, 29, 23))
#view data frame
df
  team points
1    A     22
2    A     35
3    B     19
4    B     15
5    B     29
6    C     23</b>
<h2>Method 1: Subset by One Factor Level</h2>
The following code shows how to create a new data frame that subsets by the rows where the value in the <b>team</b> column is equal to ‘B’:
<b>#subset rows where team is equal to 'B'
df_sub &lt;- df[df$team == 'B', ]
#view updated data frame
df_sub
</b>
<b>  team points
3    B     19
4    B     15
5    B     29</b>
Notice that the new data frame only contains rows where the value in the <b>team</b> column is equal to ‘B’.
<h2>Example 2: Subset by Multiple Factor Levels</h2>
The following code shows how to create a new data frame that subsets by the rows where the value in the <b>team</b> column is equal to ‘A’ or ‘C’:
<b>#subset rows where team is equal to 'A' or 'C'
df_sub &lt;- df[df$team %in% c('A', 'C'), ]
#view updated data frame
df_sub
</b>
<b>  team points
1    A     22
2    A     35
6    C     23</b>
Notice that the new data frame only contains rows where the value in the <b>team</b> column is equal to ‘A’ or ‘C’.
Using this syntax, you can include as many factor levels as you’d like in the vector following the<b> %in%</b> operator to subset by even more factor levels.
<b>Related:</b>  How to Use %in% Operator in R (With Examples) 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Convert Factor to Numeric in R 
 How to Convert Factor to Character in R 
 How to Reorder Factor Levels in R 
<h2><span class="orange">How to Subset a Data Frame in R (4 Examples)</span></h2>
You can use the following basic syntax to subset a data frame in R:
<b>df[rows, columns]</b>
The following examples show how to use this syntax in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'C', 'C', 'C'), points=c(77, 81, 89, 83, 99, 92, 97), assists=c(19, 22, 29, 15, 32, 39, 14))
#view data frame
df
  team points assists
1    A     77      19
2    A     81      22
3    B     89      29
4    B     83      15
5    C     99      32
6    C     92      39
7    C     97      14
</b>
<h3>Example 1: Subset Data Frame by Selecting Columns</h3>
The following code shows how to subset a data frame by column names:
<b>#select all rows for columns 'team' and 'assists'
df[ , c('team', 'assists')]
  team assists
1    A      19
2    A      22
3    B      29
4    B      15
5    C      32
6    C      39
7    C      14
</b>
We can also subset a data frame by column index values:
<b>#select all rows for columns 1 and 3
df[ , c(1, 3)]
  team assists
1    A      19
2    A      22
3    B      29
4    B      15
5    C      32
6    C      39
7    C      14</b>
<h3>Example 2: Subset Data Frame by Excluding Columns</h3>
The following code shows how to subset a data frame by excluding specific column names:
<b>#define columns to exclude
cols &lt;- names(df) %in% c('points')
#exclude points column
df[!cols]
  team assists
1    A      19
2    A      22
3    B      29
4    B      15
5    C      32
6    C      39
7    C      14
</b>
We can also exclude columns using index values
<b>#exclude column 2
df[ , c(-2)]
  team assists
1    A      19
2    A      22
3    B      29
4    B      15
5    C      32
6    C      39
7    C      14</b>
<h3>Example 3: Subset Data Frame by Selecting Rows</h3>
The following code shows how to subset a data frame by specific rows:
<b>#select rows 1, 5, and 7
df[c(1, 5, 7), ]
  team points assists
1    A     77      19
5    C     99      32
7    C     97      14
</b>
We can also subset a data frame by selecting a range of rows:
<b>#select rows 1 through 5
df[1:5, ]
  team points assists
1    A     77      19
2    A     81      22
3    B     89      29
4    B     83      15
5    C     99      32</b>
<h3>Example 4: Subset Data Frame Based on Conditions</h3>
The following code shows how to use the <b>subset()</b> function to select rows and columns that meet certain conditions:
<b>#select rows where points is greater than 90
subset(df, points > 90)
  team points assists
5    C     99      32
6    C     92      39
7    C     97      14
</b>
We can also use the <b>|</b> (“or”) operator to select rows that meet one of several conditions:
<b>#select rows where points is greater than 90 or less than 80
subset(df, points > 90 | points &lt; 80)
  team points assists
1    A     77      19
5    C     99      32
6    C     92      39
7    C     97      14</b>
We can also use the <b>&</b> (“and”) operator to select rows that meet multiple conditions:
<b>#select rows where points is greater than 90 <em>and</em> assists is greater than 30
subset(df, points > 90 & assists > 30)
  team points assists
5    C     99      32
6    C     92      39</b>
We can also use the select argument to only select certain columns based on a condition:
<b>#select rows where points is greater than 90 and only show 'team' column
subset(df, points > 90, select=c('team'))
  team
5    C
6    C
7    C</b>
<h2><span class="orange">How to Subset Data in SAS (3 Examples)</span></h2>
Here are the three most common ways to subset a dataset in SAS:
<b>Method 1: Choose Which Columns to Keep</b>
<b>data new_data;
    set original_data;
    keep var1 var3;
run;
</b>
<b>Method 2: Choose Which Columns to Drop</b>
<b>data new_data;
    set original_data;
    drop var4;
run;</b>
<b>Method 3: Choose Which Rows to Keep Based on Condition</b>
<b>data new_data;
    set original_data;
    if var1 &lt; 25 then delete;
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input team $ points rebounds;
    datalines;
Warriors 25 8
Wizards 18 12
Rockets 22 6
Celtics 24 11
Thunder 27 14
Spurs 33 19
Nets 31 20
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset3.jpg"253">
<h3>Example 1: <b>Choose Which Columns to Keep</b></h3>
The following code shows how to subset a dataset by using the <b>KEEP</b> statement to keep only certain columns:
<b>/*create new dataset*/
data new_data;
    set original_data;
    keep team points;
run;
/*view new dataset*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset1.jpg"167">
<h3>Example 2: <b>Choose Which Columns to Drop</b></h3>
The following code shows how to subset a dataset by using the <b>DROP </b>statement to drop specific columns:
<b>/*create new dataset*/
data new_data;
    set original_data;
    drop points;
run;
/*view new dataset*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset2.jpg"194">
<h3>Example 3: <b>Choose Which Rows to Keep Based on Condition</b></h3>
The following code shows how to subset a dataset by using the <b>DELETE </b>statement to drop specific rows from the dataset where the value in the <b>points</b> column is less than 25:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if points &lt; 25 then delete;
run;
/*view new dataset*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset4.jpg"251">
You can also use the <b>OR</b> “<b>|</b>” operator to drop the rows where <b>points</b> is less than 25 <em>or</em> <b>rebounds</b> is less than 10:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if points &lt; 25 | rebounds &lt; 10 then delete;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset5.jpg"257">
You can also use the <b>AND</b> “<b>&</b>” operator to drop the rows where <b>points</b> is less than 25 <i>and </i><b>rebounds</b> is less than 10:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if points &lt; 25 & rebounds &lt; 10 then delete;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/subset6.jpg"258">
<h2><span class="orange">How to Subset Lists in R (With Examples)</span></h2>
You can use the following syntax to subset lists in R:
<b>#extract first list item
my_list[[1]]
#extract first and third list item
my_list[c(1, 3)]
#extract third element from the first item
my_list[[c(1, 3)]] 
</b>
The following examples show how to this syntax with the following list:
<b>#create list
my_list &lt;- list(a = 1:3, b = 7, c = "hey")
#view list
my_list
$a
[1] 1 2 3
$b
[1] 7
$c
[1] "hey"
</b>
<h3>Example 1: Extract One List Item</h3>
The following code shows various ways to extract one list item:
<b>#extract first list item using index value
my_list[[1]]
[1] 1 2 3
#extract first list item using name
my_list[["a"]]
[1] 1 2 3
#extract first list item using name with $ operator
my_list$a
[1] 1 2 3
</b>
Notice that all three methods lead to the same result.
<h3>Example 2: Extract Multiple List Items</h3>
The following code shows various ways to extract multiple list items:
<b>#extract first and third list item using index values
my_list[c(1, 3)]
$a
[1] 1 2 3
$c
[1] "hey"
#extract first and third list item using names
my_list[c("a", "c")]
$a [1] 1 2 3
$c [1] "hey"
</b>
Both methods lead to the same result.
<h3>Example 3: Extract Specific Element from List Item</h3>
The following code shows various ways to extract a specific element from a list item:
<b>#extract third element from the first item using index values
my_list[[c(1, 3)]] 
[1] 3
#extract third element from the first item using double brackets
my_list[[1]][[3]]
[1] 3
</b>
Both methods lead to the same result.
<h2><span class="orange">How to Use substring Function in R (4 Examples)</span></h2>
The <b>substring()</b> function in R can be used to extract a substring in a character vector.
This function uses the following syntax:
<b>substring(text, first, last)</b>
where:
<b>text:</b> Name of the character vector
<b>first:</b> The first element to be extracted
<b>last:</b> The last element to be extracted
Also note that the <b>substr()</b> function does the exact same thing, but with slightly different argument names:
<b>substr(text, first, last)</b>
where:
<b>x:</b> Name of the character vector
<b>start:</b> The first element to be extracted
<b>stop:</b> The last element to be extracted
The examples in this tutorial show how to use the<b> substring()</b> function in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('Mavericks', 'Hornets', 'Rockets', 'Grizzlies'))
#view data frame
df
       team
1 Mavericks
2   Hornets
3   Rockets
4 Grizzlies
</b>
<h3>Example 1: Extract Characters Between Certain Positions</h3>
The following code shows how to use the <b>substring()</b> function to extract the characters between positions 2 and 5 of the “team” column:
<b>#create new column that contains characters between positions 2 and 5
df$between2_5 &lt;- substring(df$team, first=2, last=5)
#view updated data frame
df
       team  between2_5
1 Mavericks        aver
2   Hornets        orne
3   Rockets        ocke
4 Grizzlies        rizz</b>
Notice that the new column contains the characters between positions 2 and 5 of the “team” column.
<h3>Example 2: Extract First N Characters</h3>
The following code shows how to use the <b>substring()</b> function to extract the first 3 characters of the “team” column:
<b>#create new column that contains first 3 characters
df$first3 &lt;- substring(df$team, first=1, last=3)
#view updated data frame
df
       team first3
1 Mavericks    Mav
2   Hornets    Hor
3   Rockets    Roc
4 Grizzlies    Gri</b>
Notice that the new column contains the first three characters of the “team” column.
<h3>Example 3: Extract Last N Characters</h3>
The following code shows how to use the <b>substring()</b> function to extract the last 3 characters of the “team” column:
<b>#create new column that contains last 3 characters
df$last3 &lt;- substring(df$team, nchar(df$team)-3+1, nchar(df$team))
#view updated data frame
df
       team last3
1 Mavericks   cks
2   Hornets   ets
3   Rockets   ets
4 Grizzlies   ies</b>
Notice that the new column contains the last three characters of the “team” column.
<h3>Example 4: Replace a Substring</h3>
The following code shows how to use the <b>substring()</b> function to replace the first 3 characters of the values in the “team” column with 3 asterisks:
<b>#replace first 3 characters with asterisks in team column
substring(df$team, first=1, last=3) &lt;- "***"
#view updated data frame
df
       team
1 ***ericks
2   ***nets
3   ***kets
4 ***zzlies</b>
Notice that the first three characters of each team name has been replaced with asterisks.
<h2><span class="orange">What is the Success/Failure Condition in Statistics?</span></h2>
A <b>Bernoulli trial</b> is an experiment with only two possible outcomes – “success” or “failure” – and the probability of success is the same each time the experiment is conducted.
An example of a Bernoulli trial is a coin flip. The coin can only land on two sides (we could call heads a “success” and tails a “failure”) and the probability of success on each flip is 0.5, assuming the coin is fair.
Often in statistics when we want to calculate probabilities involving more than just a few Bernoulli trials, we use the  normal distribution  as an approximation. However, in order to do so we must check that the <b>Success/Failure Condition </b>is met:
<b>Success/Failure Condition: </b>There should be at least 10 expected successes and 10 expected failures in a sample in order to use the normal distribution as an approximation.
Written using notation, we must verify both of the following:
Expected number of successes is at least 10: <b>np ≥ 10</b>
Expected number of failures is at least 10: <b>n(1-p) ≥ 10</b>
where <em>n </em>is the sample size and <em>p </em>is the probability of success on a given trial.
<em><b>Note: </b>Some textbooks instead say that only 5 expected successes and 5 expected failures are needed in order to use the normal approximation. However, 10 is more commonly used and it is a more conservative number, thus we’ll use that number in this tutorial.</em>
<h3>Example: Checking the Success/Failure Condition</h3>
Suppose we would like to create a confidence interval for the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and ask them about their stance on the law. Here are the results:
Sample size <b>n = 100</b>
Proportion in favor of law <b>p = 0.56</b>
We would like to use the following formula to calculate the confidence interval:
<b>Confidence Interval = p  +/-  z*√p(1-p) / n</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the z-value that corresponds to the normal distribution
<b>n: </b>sample size
This formula uses a z-value, which comes from the normal distribution. Thus, in this formula we’re using the normal distribution to approximate the binomial distribution.
However, in order to do so we need to check that the <b>Success/Failure Condition </b>is met. Let’s verify that both the number of successes and the number of failures in the sample are at least 10:
Number of successes: np = 100*.56 = <b>56</b>
Number of failures: n(1-p) = 100*(1-.56) = <b>44</b>
Both numbers are equal to or greater than 10, so we’re okay to proceed with the formula shown above to calculate the confidence interval.
<h2><span class="orange">How to Sum Across Multiple Sheets in Excel</span></h2>
You can use the following basic syntax to sum values across multiple sheets in Excel:
<b>=SUM(Sheet1!A1, Sheet2!B5, Sheet3!A12, ...)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Sum Across Multiple Sheets in Excel</h3>
Suppose we have three sheets titled <b>week1</b>, <b>week2</b>, and <b>week3</b> that each contain data about eight basketball players and their total points scored during that week:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/sumacross11.jpg"470">
Each sheet has the exact same layout with “Player” in column A and “Points” in column B.
Now suppose we’d like to take the sum of points scored for each player during each week and display the sum in a new sheet called <b>total</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/sumacross12.jpg"490">
We can use the following formula to do so:
<b>=SUM(week1!B2, week2!B2, week3!B2)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/sumacross13.jpg"555">
The “Total Points” column contains the sum of the points scored for each player across <b>week1</b>, <b>week2</b>, and <b>week3</b>.
For example:
Player A scored a total of <b>20</b> points across the three weeks.
Player B scored a total of <b>18</b> points across the three weeks.
Player C scored a total of <b>21</b> points across the three weeks.
And so on.
Note that if each cell you’d like to sum is in the exact same position in every sheet, you can use the following shortcut to take the sum of the value in cell B2 for every sheet between <b>week1</b> and <b>week3</b>:
<b>=SUM(week1:week3!B2)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/sumacross14.jpg"482">
Notice that the values for the <b>Total Points</b> column match the ones we calculated earlier.
<h2><span class="orange">Google Sheets: How to Sum Across Multiple Sheets</span></h2>
You can use the following basic syntax to sum values across multiple sheets in Google Sheets:
<b>=SUM(Sheet1!A1, Sheet2!B5, Sheet3!A12, ...)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Sum Across Multiple Sheets</h3>
Suppose we have three sheets titled <b>week1</b>, <b>week2</b>, and <b>week3</b> that each contain data about eight basketball players and their total points scored during that week:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/sumAcross1.png">
Each sheet has the exact same layout with “Player” in column A and “Points” in column B.
Now suppose we’d like to take the sum of points scored for each player during each week and display the sum in a new sheet called <b>total</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/sumAcross2.png">
We can use the following formula to do so:
<b>=SUM(week1!B2, week2!B2, week3!B2)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/sumAcross3.png">
The “Total Points” column contains the sum of the points scored for each player across <b>week1</b>, <b>week2</b>, and <b>week3</b>.
For example:
Player A scored a total of <b>23</b> points across the three weeks.
Player B scored a total of <b>24</b> points across the three weeks.
Player C scored a total of <b>26</b> points across the three weeks.
And so on.
You can use this exact syntax to sum values across any number of sheets you’d like in Google Sheets.
<h2><span class="orange">How to Calculate the Sum by Group in Excel</span></h2>
You can use the following simple formula to calculate the sum of values by group in an Excel spreadsheet:
<b>=SUMIF(group_range, "group name", sum_range)
</b>
The following example shows how to use this formula in practice.
<h3>Example: Sum by Group in Excel</h3>
Suppose we have the following dataset that shows the total points scored by 15 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel00.png">
Now suppose we’d like to find the sum of the points scored, grouped by team.
To do so, we can use the <b>=UNIQUE()</b> function to first create a list of the unique teams. We’ll type the following formula into cell F2:
<b>=UNIQUE(B2:B16)</b>
Once we press enter, a list of unique team names will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel0.png">
Next, we can use the <b>=SUMIF()</b> function to find the sum of points scored by players on each team.
We’ll type in the following formula into cell G2:
<b>=SUMIF(B2:B16, F2, C2:C16)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel3-1.png">
We’ll then copy and paste this formula into the remaining cells in column G:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/sumgroupExcel4.png">
That’s it!
Column F displays each of the unique teams and column G displays the sum of the points scored by each team.
<h2><span class="orange">How to Calculate the Sum by Group in R (With Examples)</span></h2>
Often you may want to calculate the sum by group in R. There are three methods you can use to do so:
<b>Method 1: Use base R.</b>
<b>aggregate(df$col_to_aggregate, list(df$col_to_group_by), FUN=sum) </b>
<b>Method 2: Use the dplyr() package.</b>
<b>library(dplyr)
df %>%
  group_by(col_to_group_by) %>%
  summarise(Freq = sum(col_to_aggregate))
</b>
<b>Method 3: Use the data.table package.</b>
<b>library(data.table)
dt[ ,list(sum=sum(col_to_aggregate)), by=col_to_group_by]
</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Calculate Sum by Group Using Base R</h3>
The following code shows how to use the <b>aggregate() </b>function from base R to calculate the sum of the points scored by team in the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('a', 'a', 'b', 'b', 'b', 'c', 'c'), pts=c(5, 8, 14, 18, 5, 7, 7), rebs=c(8, 8, 9, 3, 8, 7, 4))
#view data frame
df
  team pts rebs
1    a   5    8
2    a   8    8
3    b  14    9
4    b  18    3
5    b   5    8
6    c   7    7
7    c   7    4
#find sum of points scored by team
aggregate(df$pts, list(df$team), FUN=sum)
  Group.1  x
1       a 13
2       b 37
3       c 14
</b>
<h3>Method 2: Calculate Sum by Group Using dplyr</h3>
The following code shows how to use the <b>group_by()</b> and <b>summarise()</b> functions from the <b>dplyr</b> package to calculate the sum of points scored by team in the following data frame:
<b><b>library(dplyr) </b>
#create data frame
df &lt;- data.frame(team=c('a', 'a', 'b', 'b', 'b', 'c', 'c'), pts=c(5, 8, 14, 18, 5, 7, 7), rebs=c(8, 8, 9, 3, 8, 7, 4))
#find sum of points scored by team 
<b>df %>%
  group_by(team) %>%
  summarise(Freq = sum(pts))
# A tibble: 3 x 2
  team   Freq
  &lt;chr> &lt;dbl>
1 a        13
2 b        37
3 c        14  </b>
</b>
<h3>Method 3: Calculate Sum by Group Using data.table</h3>
 The following code shows how to use the <b>data.table</b> package to calculate the sum of points scored by team in the following data frame:
<b><b>library(data.table) </b>
#create data frame
df &lt;- data.frame(team=c('a', 'a', 'b', 'b', 'b', 'c', 'c'), pts=c(5, 8, 14, 18, 5, 7, 7), rebs=c(8, 8, 9, 3, 8, 7, 4))
#convert data frame to data table 
setDT(df)
#find sum of points scored by team 
<b>df[ ,list(sum=sum(pts)), by=team]
   team sum
1:    a  13
2:    b  37
3:    c  14</b></b>
Notice that all three methods return identical results.
<b>Note:</b> If you have an extremely large dataset, the data.table method will work the fastest among the three methods listed here.
<h2><span class="orange">How to Calculate the Sum of Columns in Pandas</span></h2>
Often you may be interested in calculating the sum of one or more columns in a pandas DataFrame. Fortunately you can do this easily in pandas using the  sum()  function.
This tutorial shows several examples of how to use this function.
<h3>Example 1: Find the Sum of a Single Column</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [np.nan, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame 
df
ratingpointsassistsrebounds
090255NaN
1852078
28214710
3881686
4942756
5902079
6761266
77515910
88714910
9861957
</b>
We can find the sum of the column titled “points” by using the following syntax:
<b>df['points'].sum()
182</b>
The sum() function will also exclude NA’s by default. For example, if we find the sum of the “rebounds” column, the first value of “NaN” will simply be excluded from the calculation:
<b>df['rebounds'].sum()
72.0</b>
<h3>Example 2: Find the Sum of Multiple Columns</h3>
We can find the sum of multiple columns by using the following syntax:
<b>#find sum of points and rebounds columns
df[['rebounds', 'points']].sum()
rebounds     72.0
points      182.0
dtype: float64
</b>
<h3>Example 3: Find the Sum of All Columns</h3>
We can find also find the sum of all columns by using the following syntax:
<b>#find sum of all columns in DataFrame
df.sum()
rating      853.0
points      182.0
assists      68.0
rebounds     72.0
dtype: float64
</b>
For columns that are not numeric, the sum() function will simply not calculate the sum of those columns.
<em>You can find the complete documentation for the sum() function  here .</em>
<h2><span class="orange">How to Use sum() Function in R (With Examples)</span></h2>
You can use the <b>sum()</b> function in R to find the sum of values in a vector.
This function uses the following basic syntax:
<b>sum(x, na.rm=FALSE)</b>
where:
<b>x</b>: Name of the vector.
<b>na.rm</b>: Whether to ignore NA values. Default is FALSE.
The following examples show how to use this function in practice.
<h3>Example 1: Sum Values in Vector</h3>
The following code shows how to sum the values in a vector:
<b>#create vector
x &lt;- c(3, 6, 7, 12, 15)
#sum values in vector
sum(x)
[1] 43</b>
If there happen to be NA values in the vector, you can use <b>na.rm=TRUE</b> to ignore the missing values when calculating the mean:
<b>#create vector with some NA values
x &lt;- c(3, NA, 7, NA, 15)
#sum values in vector
sum(x, na.rm=TRUE)
[1] 25</b>
<h3>Example 2: Sum Values in Data Frame Column</h3>
The following code shows how to sum the values in a specific column of a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    7    3    1
3    3    8    6    2
4    4    3    6    8
5    5    2    8    9
#sum values in 'var1' column
sum(df$var1)
[1] 16</b>
<h3>Example 3: Sum Values in Several Data Frame Columns</h3>
The following code shows how to use the <b>sapply()</b> function to sum the values in several columns of a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    7    3    1
3    3    8    6    2
4    4    3    6    8
5    5    2    8    9
#sum values in 'var1' and 'var3' columns
sapply(df[ , c('var1', 'var3')], sum)
var1 var3 
  16   26</b>
<h2><span class="orange">How to Calculate Sum of Squares in ANOVA (With Example)</span></h2>
In statistics, a  one-way ANOVA  is used to compare the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means.
Whenever you perform a one-way ANOVA, you will always compute three sum of squares values:
<b>1. Sum of Squares Regression (SSR)</b>
This is the sum of the squared differences between each group mean and the  grand mean .
<b>2. Sum of Squares Error (SSE)</b>
This is the sum of the squared differences between each individual observation and the group mean of that observation.
<b>3. Sum of Squares Total (SST)</b>
This is the sum of the squared differences between each individual observation and the grand mean.
Each of these three values are placed in the final ANOVA table, which we use to determine whether or not there is a statistically significant difference between the group means.
The following example shows how to calculate each of these sum of squares values for a one-way ANOVA in practice.
<h3>Example: How to Calculate Sum of Squares in ANOVA</h3>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a certain exam. To test this, we recruit 30 students to participate in a study and split them into three groups.
The students in each group are randomly assigned to use one of the three exam prep programs for the next three weeks to prepare for an exam. At the end of the three weeks, all of the students take the same exam. 
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png"(max-width: 237px) 100vw, 237px">
The following steps show how to calculate the sum of squares values for this one-way ANOVA.
<b>Step 1: Calculate the group means and the grand mean.</b>
First, we will calculate the mean for all three groups along with the grand (or “overall”) mean:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVAhand1-1.png">
<b>Step 2: Calculate SSR.</b>
Next, we will calculate the sum of squares regression (SSR) using the following formula:
nΣ(X<sub>j</sub> – X..)<sup>2</sup> 
where:
<b>n</b>: the sample size of group j
<b>Σ</b>: a greek symbol that means “sum”
<b>X<sub>j</sub></b>: the mean of group j
<b>X..</b>: the overall mean
In our example, we calculate that SSR = 10(83.4-85.8)<sup>2</sup> + 10(89.3-85.8)<sup>2</sup> + 10(84.7-85.8)<sup>2</sup> = <b>192.2</b>
<b>Step 3: Calculate SSE.</b>
Next, we will calculate the sum of squares error (SSE) using the following formula:
Σ(X<sub>ij</sub> – X<sub>j</sub>)<sup>2</sup> 
where:
<b>Σ</b>: a greek symbol that means “sum”
<b>X<sub>ij</sub></b>: the i<sup>th</sup> observation in group j
<b>X<sub>j</sub></b>: the mean of group j
In our example, we calculate SSE as follows:
<b>Group 1: </b>(85-83.4)<sup>2</sup> + (86-83.4)<sup>2 </sup>+<b> </b>(88-83.4)<sup>2 </sup>+<b> </b>(75-83.4)<sup>2 </sup>+<b> </b>(78-83.4)<sup>2 </sup>+<b> </b>(94-83.4)<sup>2 </sup>+<b> </b>(98-83.4)<sup>2 </sup>+ <b> </b>(79-83.4)<sup>2 </sup>+<b> </b>(71-83.4)<sup>2 </sup>+<b> </b>(80-83.4)<sup>2 </sup>= <b>640.4</b>
<b>Group 2: </b>(91-89.3)<sup>2</sup> + (92-89.3)<sup>2 </sup>+<b> </b>(93-89.3)<sup>2 </sup>+<b> </b>(85-89.3)<sup>2 </sup>+<b> </b>(87-89.3)<sup>2 </sup>+<b> </b>(84-89.3)<sup>2 </sup>+<b> </b>(82-89.3)<sup>2 </sup>+ <b> </b>(88-89.3)<sup>2 </sup>+<b> </b>(95-89.3)<sup>2 </sup>+<b> </b>(96-89.3)<sup>2 </sup>= <b>208.1</b>
<b>Group 3: </b>(79-84.7)<sup>2</sup> + (78-84.7)<sup>2 </sup>+<b> </b>(88-84.7)<sup>2 </sup>+<b> </b>(94-84.7)<sup>2 </sup>+<b> </b>(92-84.7)<sup>2 </sup>+<b> </b>(85-84.7)<sup>2 </sup>+<b> </b>(83-84.7)<sup>2 </sup>+ <b> </b>(85-84.7)<sup>2 </sup>+<b> </b>(82-84.7)<sup>2 </sup>+<b> </b>(81-84.7)<sup>2 </sup>= <b>252.1</b>
<b>SSE: </b>640.4 + 208.1 + 252.1 = <b>1100.6</b>
<b>Step 4: Calculate SST.</b>
Next, we will calculate the sum of squares total (SST) using the following formula:
SST = SSR + SSE
In our example, SST = 192.2 + 1100.6 = <b>1292.8</b>
Once we have calculated the values for SSR, SSE, and SST, each of these values will eventually be placed in the ANOVA table:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F-value</b></th>
<th style="text-align: center;"><b>p-value</b></th>
</tr>
<tr>
<td><b>Regression</b></td>
<td style="text-align: center;">192.2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">2.358</td>
<td style="text-align: center;">0.1138</td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">1100.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">40.8</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">1292.8</td>
<td style="text-align: center;">29</td>
<td> </td>
<td> </td>
<td> </td>
</tr>
</tbody></table>
Here is how we calculated the various numbers in the table:
<b>df regression: </b>k-1 = 3-1 = 2
<b>df error: </b>n-k = 30-3 = 27
<b>df total: </b>n-1 = 30-1 = 29
<b>MS treatment: </b>SST / df treatment = 192.2 / 2 = 96.1
<b>MS error: </b>SSE / df error = 1100.6 / 27 = 40.8
<b>F-value: </b>MS treatment / MS error = 96.1 / 40.8 = 2.358
<b>p-value</b>: p-value that corresponds to F value.
<em><b>Note: </b>n = total observations, k = number of groups</em>
Check out  this tutorial  for how to interpret the F-Value and p-value in the ANOVA table.
<h2><span class="orange">How to Find the Sum of Rows in a Pandas DataFrame</span></h2>
Often you may be interested in calculating the sum of one or more rows in a pandas DataFrame. Fortunately you can do this easily in pandas using the  sum(axis=1)  function.
This tutorial shows several examples of how to use this function on the following DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [8, np.nan, 10, 6, 6, 9, 6, 10, 10, 7]})
#view DataFrame 
df
        ratingpointsassistsrebounds
0902558.0
185207NaN
28214710.0
3881686.0
4942756.0
5902079.0
6761266.0
77515910.0
88714910.0
9861957.07
</b>
<h3>Example 1: Find the Sum of Each Row</h3>
We can find the sum of each row in the DataFrame by using the following syntax:
<b>df.sum(axis=1)
0    128.0
1    112.0
2    113.0
3    118.0
4    132.0
5    126.0
6    100.0
7    109.0
8    120.0
9    117.0
dtype: float64</b>
The output tells us:
The sum of values in the first row is <b>128</b>.
The sum of values in the second row is <b>112</b>.
The sum of values in the third row is <b>113</b>.
And so on.
<h3>Example 2: Place the Row Sums in a New Column</h3>
We can use the following code to add a column to our DataFrame to hold the row sums:
<b>#define new DataFrame column 'row_sum' as the sum of each row
df['row_sum'] = df.sum(axis=1)
#view DataFrame
df
ratingpointsassistsreboundsrow_sum
0902558.0128.0
185207NaN112.0
28214710.0113.0
3881686.0118.0
4942756.0132.0
5902079.0126.0
6761266.0100.0
77515910.0109.0
88714910.0120.0
9861957.0117.0
</b>
<h3>Example 3: Find the Row Sums for a Short List of Specific Columns</h3>
We can use the following code to find the row sum for a short list of specific columns:
<b>#define new DataFrame column as sum of points and assists columns
df['sum_pa'] = df['points'] + df['assists']
#view DataFrame
df
ratingpointsassistsrebounds  sum_pa
0902558.0  30
185207NaN  27
28214710.0  21
3881686.0  24
4942756.0  32
5902079.0  27
6761266.0  18
77515910.0  24
88714910.0  23
9861957.0  24</b>
<h3>Example 4: Find the Row Sums for a Long List of Specific Columns</h3>
We can use the following code to find the row sum for a longer list of specific columns:
<b>#define col_list as a list of all DataFrame column names
col_list= list(df)
#remove the column 'rating' from the list
col_list.remove('rating')
#define new DataFrame column as sum of rows in col_list 
df['new_sum'] = df[col_list].sum(axis=1)
#view DataFrame
df
        ratingpointsassistsrebounds new_sum
0902558.0 38.0
185207NaN 27.0
28214710.0 31.0
3881686.0 30.0
4942756.0 38.0
5902079.0 36.0
6761266.0 24.0
77515910.0 34.0
88714910.0 33.0
9861957.0 31.0</b>
<em>You can find the complete documentation for the pandas sum() function  here .</em>
<h2><span class="orange">How to Sum Specific Columns in R (With Examples)</span></h2>
Often you may want to find the sum of a specific set of columns in a data frame in R. Fortunately this is easy to do using the <b>rowSums() </b>function.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Find the Sum of Specific Columns</h3>
The following code shows how to create a data frame with three columns and find the sum of the first and third columns:
<b>#create data frame
data &lt;- data.frame(var1 = c(0, NA, 2, 2, 5),   var2 = c(5, 5, 7, 8, 9),   var3 = c(2, 7, 9, 9, 7))
#view data frame
data
  var1 var2 var3
1    0    5    2
2   NA    5    7
3    2    7    9
4    2    8    9
5    5    9    7
#find sum of first and third columns
rowSums(data[ , c(1,3)], na.rm=TRUE)
[1]  2  7 11 11 12
</b>
The way to interpret the output is as follows:
The sum of values in the first row for the first and third columns is <b>2</b>.
The sum of values in the first row for the first and third columns is <b>7</b>.
The sum of values in the first row for the first and third columns is <b>11</b>.
The sum of values in the first row for the first and third columns is <b>11</b>.
The sum of values in the first row for the first and third columns is <b>12</b>.
You can also assign the row sums of these specific columns to a new variable in the data frame:
<b>#assign row sums to new variable named <em>row_sum</em>
data$row_sum &lt;- rowSums(data[ , c(1,3)], na.rm=TRUE)
#view data frame
data
  var1 var2 var3 row_sum
1    0    5    2       2
2   NA    5    7       7
3    2    7    9      11
4    2    8    9      11
5    5    9    7      12
</b>
<h3>Example 2: Find the Sum of All Columns</h3>
It’s also possible to find the sum across all columns in a data frame. The following code shows how to do so:
<b>#find row sums across all columns
data$new &lt;- rowSums(data, na.rm=TRUE)
#view data frame
data
  var1 var2 var3 new
1    0    5    2   7
2   NA    5    7  12
3    2    7    9  18
4    2    8    9  19
5    5    9    7  21</b>
We can see that:
The sum of values in the first row across all three columns is <b>7</b>.
The sum of values in the second row across all three columns is <b>12</b>.
And so on.
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">How to Sum Specific Rows in R (With Examples)</span></h2>
We can use the following syntax to sum specific rows of a data frame in R:
<b>with(df, sum(column_1[column_2 == 'some value']))
</b>
This syntax finds the sum of the rows in column 1 in which column 2 is equal to some value, where the data frame is called <b>df</b>.
This tutorial provides several examples of how to use this function in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team = c('A', 'A', 'B', 'B', 'B', 'C', 'C'), points = c(4, 7, 8, 8, 8, 9, 12), rebounds = c(3, 3, 4, 4, 6, 7, 7))
#view data frame
df
  team points rebounds
1    A      4        3
2    A      7        3
3    B      8        4
4    B      8        4
5    B      8        6
6    C      9        7
7    C     12        7
</b>
<h3>Example 1: Sum Rows Based on the Value of One Column</h3>
The following code shows how to find the sum of all rows in the points column where team is equal to C:
<b>#find sum of points where team is equal to 'C'
with(df, sum(points[team == 'C']))
[1] 21
</b>
And the following code shows how to find the sum of all rows in the rebounds column where the value in the points column is greater than 7:
<b>#find sum of rebounds where points is greater than 7
with(df, sum(rebounds[points > 7]))
[1] 28</b>
<h3>Example 2: Sum Rows Based on the Value of Multiple Columns</h3>
The following code shows how to find the sum of the rows in the rebounds column where the value in the points column is less than 8 <b>or</b> the value in the team column is equal to C:
<b>with(df, sum(rebounds[points &lt; 8 | team == 'C']))
[1] 20
</b>
And the following code shows how to find the sum of the rows in the rebounds column where the value in the points column is less than 10 <b>and</b> the value in the team column is equal to B:
<b>with(df, sum(rebounds[points &lt; 10 & team == 'B']))
[1] 14</b>
<h2><span class="orange">How to Use SUMIF Contains in Google Sheets</span></h2>
You can use the following formulas in Google Sheets to calculate the sum of cells that contain certain strings:
<b>Method 1: SUMIF Contains (One Criteria)</b>
<b>=SUMIF(A2:A11, "*string*", C2:C11)
</b>
This formula takes the sum of values in <b>C2:C11</b> where the corresponding cell in the range <b>A2:A11</b> contains “string.”
<b>Method 2: SUMIF Contains (Multiple Criteria)</b>
<b>=SUMIFS(C2:C11, A2:A11, "*string1*", B2:B11, "*string2*")</b>
This formula takes the sum of values in <b>C2:C11</b> where the cell in the range <b>A2:A11</b> contains “string1” and the cell in the range <b>B2:B11</b> contains “string2.”
The following examples show how to use each method in practice.
<h3>Example 1: SUMIF Contains (One Criteria)</h3>
We can use the following formula to calculate the sum of values in the <b>Points</b> column for the rows where the <b>Team</b> column contains “Mav”:
<b>=SUMIF(A2:A11,"*Mav*",C2:C11)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumifcontain1.jpg"562">
From the output we can see that the sum of points for the rows where the team name contains “Mav” is <b>112</b>.
We can also manually verify this is correct by taking the sum of points for the rows where the team name contains “Mav”:
Sum of Points: 21 + 14 + 19 + 30 + 28 = <b>112</b>.
<h3>Example 2: SUMIF Contains (Multiple Criteria)</h3>
We can use the following formula to calculate the sum of values in the <b>Points</b> column for the rows where the <b>Team</b> column contains “Mav” and the <b>Position</b> column contains “Guar”:
<b>=SUMIFS(C2:C11, A2:A11, "*Mav*", B2:B11, "*Gua*")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumifcontain2.jpg"595">
From the output we can see that the sum of points for the rows where the team contains “Mav” and the position contains “Guar” is <b>35</b>.
We can also manually verify this is correct by taking the sum of points for the rows where the team name contains “Mav” and the position contains “Guar”:
Sum of Points: 21 + 14 = <b>35</b>.
<b>Related:</b>  How to Use COUNTIF Contains in Google Sheets 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 How to Use SUMIF with OR in Google Sheets 
 How to Use SUMIFS with a Date Range in Google Sheets 
 How to Use SUMIF From Another Sheet in Google Sheets 
<h2><span class="orange">How to Perform a SUMIF Function in R</span></h2>
Often you may be interested in <em>only</em> finding the sum of rows in an R data frame that meet some criteria. Fortunately this is easy to do using the following basic syntax:
<b>aggregate(col_to_sum ~ col_to_group_by, data=df, sum)
</b>
The following examples show how to use this syntax  on the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('a', 'a', 'b', 'b', 'b', 'c', 'c'), pts=c(5, 8, 14, 18, 5, 7, 7), rebs=c(8, 8, 9, 3, 8, 7, 4), blocks=c(1, 2, 2, 1, 0, 4, 1))
#view data frame
df
  team pts rebs blocks
1    a   5    8      1
2    a   8    8      2
3    b  14    9      2
4    b  18    3      1
5    b   5    8      0
6    c   7    7      4
7    c   7    4      1</b>
<h3>Example 1: Perform a SUMIF Function on One Column</h3>
The following code shows how to find the sum of points for each team:
<b>aggregate(pts ~ team, data=df, sum)
  team pts
1    a  13
2    b  37
3    c  14
</b>
<h3>Example 2: Perform a SUMIF Function on Multiple Columns</h3>
The following code shows how to find the sum of points and rebounds for each team:
<b>aggregate(cbind(pts, rebs) ~ team, data=df, sum)
  team pts rebs
1    a  13   16
2    b  37   20
3    c  14   11
</b>
<h3>Example 3: Perform a SUMIF Function on All Columns</h3>
The following code shows how to find the sum of all columns in the data frame for each team:
<b><b>aggregate(. ~ team, data=df, sum)
  team pts rebs blocks
1    a  13   16      3
2    b  37   20      3
3    c  14   11      5
</b></b>
<b>Note:</b> The period (<b>.</b>) is used in R to represent “all” columns.
<h2><span class="orange">How to Use SUMIF with OR in Excel</span></h2>
You can use the following formulas to combine the <b>SUMIF</b> function with the <b>OR</b> function in Excel:
<b>Method 1: SUMIF with OR (One Column)</b>
<b>=SUM(SUMIFS(B2:B13, A2:A13,{"Value1","Value2", "Value3"}))
</b>
This particular formula finds the sum of values in <b>B2:B13</b> where the corresponding value in <b>A2:A13</b> contains “Value1”, “Value2”, or “Value3.”
<b>Method 2: SUMIF with OR (Multiple Columns)</b>
<b>=SUMIF(A2:A13,"Value1", C2:C13)+SUMIF(B2:B13,"Value2", C2:C13)
</b>
This particular formula finds the sum of values in <b>C2:C13</b> where the corresponding value in <b>A2:A13</b> contains “Value1” or the corresponding value in <b>B2:B13</b> contains “Value2.”
The following example shows how to use each method in practice with the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/orexcel1.jpg"459">
<h3>Example 1: SUMIF with OR (One Column)</h3>
We can use the following formula to sum the values in the <b>Points</b> column where the value in the <b>Team</b> column is equal to “Mavs” or “Rockets”:
<b>=SUM(SUMIFS(C2:C13, A2:A13,{"Mavs","Rockets"}))</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/orexcel2.jpg"578">
The sum of the values in the <b>Points</b> column where the value in the <b>Team</b> column is equal to “Mavs” or “Rockets” is <b>53</b>.
<h3>Example 2: SUMIF with OR (Multiple Columns)</h3>
We can use the following formula to sum the values in the <b>Points</b> column where the value in the <b>Team</b> column is equal to “Mavs” or the value in the <b>Position</b> column is “Center”:
<b>=SUMIF(A2:A13,"Mavs",C2:C13)+SUMIF(B2:B13,"Center",C2:C13)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/orexcel3.jpg"659">
The sum of the values in the <b>Points</b> column where the value in the <b>Team</b> column is equal to “Mavs” or the value in the <b>Position</b> column is “Center” is <b>87</b>.
<h2><span class="orange">How to Use SUMIF with OR in Google Sheets</span></h2>
Often you may want to sum the cells in Google Sheets that meet one of several criteria.
You can use the following basic syntax to do so:
<b>=SUMIFS(B2:B11, A2:A11, "value1") + SUMIFS(B2:B11, A2:A11, "value2")
</b>
This particular formula takes the sum of values in the range <b>B2:B11</b> where the corresponding value in the range <b>A2:A11</b> is equal to “value1” or “value2.”
The following example shows how to use this syntax in practice.
<h3>Example: Use SUMIF with OR in Google Sheets</h3>
Suppose we have the following data in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumifor1.jpg"480">
We can use the following formula to count the number of cells in column A that have a value of “Mavs” or “Jazz”:
<b>=SUMIFS(B2:B11, A2:A11, "Mavs") + SUMIFS(B2:B11, A2:A11, "Jazz")</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumifor2.jpg">
We can see that the sum of values in the <b>Points</b> column where the corresponding value in the <b>Team</b> column is “Mavs” or “Jazz” is <b>77</b>.
We can manually verify that this is correct by taking the sum of points for the Mavs and Jazz:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumifor3.jpg"579">
Sum of Points: 23 + 13 + 19 + 22 = <b>77</b>.
<b>Note</b>: You can find the complete documentation for the <b>SUMIFS</b> function in Google Sheets  here .
<h2><span class="orange">How to Use summary() Function in R (With Examples)</span></h2>
The <b>summary()</b> function in R can be used to quickly summarize the values in a vector, data frame, regression model, or ANOVA model in R.
This syntax uses the following basic syntax:
<b>summary(data)</b>
The following examples show how to use this function in practice.
<h3>Example 1: Using summary() with Vector</h3>
The following code shows how to use the <b>summary()</b> function to summarize the values in a vector:
<b>#define vector
x &lt;- c(3, 4, 4, 5, 7, 8, 9, 12, 13, 13, 15, 19, 21)
#summarize values in vector
summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   3.00    5.00    9.00   10.23   13.00   21.00 
</b>
The <b>summary()</b> function automatically calculates the following summary statistics for the vector:
Min: The minimum value
1st Qu: The value of the 1st quartile (25th percentile)
Median: The median value
3rd Qu: The value of the 3rd quartile (75th percentile)
Max: The maximum value
Note that if there are any missing values (NA) in the vector, the <b>summary()</b> function will automatically exclude them when calculating the summary statistics:
<b>#define vector
x &lt;- c(3, 4, 4, 5, 7, 8, 9, 12, 13, 13, 15, 19, 21, NA, NA)
#summarize values in vector
summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
   3.00    5.00    9.00   10.23   13.00   21.00       2</b>
<h3>Example 2: Using summary() with Data Frame</h3>
The following code shows how to use the <b>summary()</b> function to summarize every column in a data frame:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 90, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28))
#summarize every column in data frame
summary(df)
     team               points        assists      rebounds   
 Length:5           Min.   :86.0   Min.   :28   Min.   :24.0  
 Class :character   1st Qu.:88.0   1st Qu.:31   1st Qu.:24.0  
 Mode  :character   Median :90.0   Median :33   Median :28.0      Mean   :91.6   Mean   :33   Mean   :26.8      3rd Qu.:95.0   3rd Qu.:34   3rd Qu.:28.0      Max.   :99.0   Max.   :39   Max.   :30.0 
</b>
<h3>Example 3: Using summary() with Specific Data Frame Columns</h3>
The following code shows how to use the <b>summary()</b> function to summarize specific columns in a data frame:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 90, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28))
#summarize every column in data frame
summary(df[c('points', 'rebounds')])
     points        rebounds   
 Min.   :86.0   Min.   :24.0  
 1st Qu.:88.0   1st Qu.:24.0  
 Median :90.0   Median :28.0  
 Mean   :91.6   Mean   :26.8  
 3rd Qu.:95.0   3rd Qu.:28.0  
 Max.   :99.0   Max.   :30.0 </b>
<h3>Example 4: Using summary() with Regression Model</h3>
The following code shows how to use the <b>summary()</b> function to summarize the results of a linear regression model:
<b>#define data
df &lt;- data.frame(y=c(99, 90, 86, 88, 95, 99, 91), x=c(33, 28, 31, 39, 34, 35, 36))
#fit linear regression model
model &lt;- lm(y~x, data=df)
#summarize model fit
summary(model)
Call:
lm(formula = y ~ x, data = df)
Residuals:
     1      2      3      4      5      6      7 
 6.515 -1.879 -6.242 -5.212  2.394  6.273 -1.848 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)  
(Intercept)  88.4848    22.1050   4.003   0.0103 *
x             0.1212     0.6526   0.186   0.8599  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 5.668 on 5 degrees of freedom
Multiple R-squared:  0.006853,Adjusted R-squared:  -0.1918 
F-statistic: 0.0345 on 1 and 5 DF,  p-value: 0.8599
</b>
<b>Related:</b>  How to Interpret Regression Output in R 
<h3>Example 5: Using summary() with ANOVA Model</h3>
The following code shows how to use the <b>summary()</b> function to summarize the results of an ANOVA model in R:
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = rep(c("A", "B", "C"), each = 30),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#fit ANOVA model
model &lt;- aov(weight_loss ~ program, data = data)
#summarize model fit
summary(model)
            Df Sum Sq Mean Sq F value   Pr(>F)    
program      2  98.93   49.46   30.83 7.55e-11 ***
Residuals   87 139.57    1.60                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
<b>Related:</b>  How to Interpret ANOVA Results in R 
<h2><span class="orange">How to Calculate Summary Statistics for a Pandas DataFrame</span></h2>
You can use the following methods to calculate summary statistics for variables in a pandas DataFrame:
<b>Method 1: Calculate Summary Statistics for All Numeric Variables</b>
<b>df.describe()
</b>
<b>Method 2: Calculate Summary Statistics for All String Variables</b>
<b>df.describe(include='object')</b>
<b>Method 3: Calculate Summary Statistics Grouped by a Variable</b>
<b>df.groupby('group_column').mean()
df.groupby('group_column').median()
df.groupby('group_column').max()
...</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],   'points': [18, 22, 19, 14, 14, 11, 20, 28, 30],   'assists': [5, np.nan, 7, 9, 12, 9, 9, 4, 5],   'rebounds': [11, 8, 10, 6, 6, 5, 9, np.nan, 6]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18      5.0      11.0
1    A      22      NaN       8.0
2    A      19      7.0      10.0
3    A      14      9.0       6.0
4    B      14     12.0       6.0
5    B      11      9.0       5.0
6    B      20      9.0       9.0
7    B      28      4.0       NaN
8    B      30      5.0       6.0
</b>
<h3>Example 1: Calculate Summary Statistics for All Numeric Variables</h3>
The following code shows how to calculate the summary statistics for each numeric variable in the DataFrame:
<b>df.describe()
   points assistsrebounds
count9.0000008.0000008.000000
mean19.5555567.5000007.625000
std6.3661432.7255412.199838
min11.0000004.0000005.000000
25%14.0000005.0000006.000000
50%19.0000008.0000007.000000
75%22.0000009.0000009.250000
max30.00000012.00000011.000000</b>
We can see the following summary statistics for each of the three numeric variables:
<b>count:</b> The count of non-null values
<b>mean</b>: The mean value
<b>std</b>: The standard deviation
<b>min:</b> The minimum value
<b>25%</b>: The value at the 25th percentile
<b>50%</b>: The value at the 50th percentile (also the median)
<b>75%</b>: The value at the 75th percentile
<b>max</b>: The maximum value
<h3>Example 2: Calculate Summary Statistics for All String Variables</h3>
The following code shows how to calculate the summary statistics for each string variable in the DataFrame:
<b>df.describe(include='object')
team
count   9
unique   2
top   B
freq   5</b>
We can see the following summary statistics for the one string variable in our DataFrame:
<b>count</b>: The count of non-null values
<b>unique</b>: The number of unique values
<b>top:</b> The most frequently occurring value
<b>freq</b>: The count of the most frequently occurring value
<h3>Example 3: Calculate Summary Statistics Grouped by a Variable</h3>
The following code shows how to calculate the mean value for all numeric variables, grouped by the <b>team</b> variable:
<b>df.groupby('team').mean()
pointsassistsrebounds
team
A18.257.08.75
B20.607.86.50
</b>
The output displays the mean value for the <b>points</b>, <b>assists</b>, and <b>rebounds</b> variables, grouped by the <b>team</b> variable.
Note that we can use similar syntax to calculate a different summary statistic, such as the median:
<b>df.groupby('team').median()
pointsassistsrebounds
team
A18.57.09.0
B20.09.06.0</b>
The output displays the median value for the <b>points</b>, <b>assists</b>, and <b>rebounds</b> variables, grouped by the <b>team</b> variable.
<b>Note</b>: You can find the complete documentation for the <b>describe</b> function in pandas  here .
<h2><span class="orange">The Easiest Way to Create Summary Tables in R</span></h2>
The easiest way to create summary tables in R is to use the <b>describe()</b> and <b>describeBy()</b> functions from the <b>psych</b> library.
<b>library(psych)
#create summary table
describe(df)
#create summary table, grouped by a specific variable
describeBy(df, group=df$var_name)
</b>
The following examples show how to use these functions in practice.
<h3>Example 1: Create Basic Summary Table</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'C', 'C', 'C'), points=c(15, 22, 29, 41, 30, 11, 19), rebounds=c(7, 8, 6, 6, 7, 9, 13), steals=c(1, 1, 2, 3, 5, 7, 5))
#view data frame
df
  team points rebounds steals
1    A     15        7      1
2    A     22        8      1
3    B     29        6      2
4    B     41        6      3
5    C     30        7      5
6    C     11        9      7
7    C     19       13      5</b>
We can use the <b>describe()</b> function to create a summary table for each variable in the data frame:
<b>library(psych) 
#create summary table
describe(df)
         vars n  mean    sd median trimmed   mad min max range  skew kurtosis
team*       1 7  2.14  0.90      2    2.14  1.48   1   3     2 -0.22    -1.90
points      2 7 23.86 10.24     22   23.86 10.38  11  41    30  0.33    -1.41
rebounds    3 7  8.00  2.45      7    8.00  1.48   6  13     7  1.05    -0.38
steals      4 7  3.43  2.30      3    3.43  2.97   1   7     6  0.25    -1.73
           se
team*    0.34
points   3.87
rebounds 0.93
steals   0.87
</b>
Here’s how to interpret each value in the output:
<b>vars</b>: column number
<b>n</b>: Number of valid cases
<b>mean</b>: The mean value
<b>median</b>: The median value
<b>trimmed</b>: The trimmed mean (default trims 10% of observations from each end)
<b>mad</b>: The median absolute deviation (from the median)
<b>min</b>: The minimum value
<b>max</b>: The maximum value
<b>range</b>: The range of values (max – min)
<b>skew</b>: The skewness
<b>kurtosis</b>: The kurtosis
<b>se</b>: The standard error
It’s important to note that any variable with an asterisk (*) symbol next to it is a categorical or logical variable that has been converted to a numerical variable with values that represent the numerical ordering of the values.
In our example, the variable ‘team’ has been converted to a numerical variable so we shouldn’t interpret the summary statistics for it literally.
Also note that you can use the argument <b>fast=TRUE</b> to only calculate the most common summary statistics:
<b>#create smaller summary table
describe(df, fast=TRUE)
         vars n  mean    sd min  max range   se
team        1 7   NaN    NA Inf -Inf  -Inf   NA
points      2 7 23.86 10.24  11   41    30 3.87
rebounds    3 7  8.00  2.45   6   13     7 0.93
steals      4 7  3.43  2.30   1    7     6 0.87</b>
We can also choose to only compute the summary statistics for certain variables in the data frame:
<b>#create summary table for just 'points' and 'rebounds' columns
describe(df[ , c('points', 'rebounds')], fast=TRUE)
         vars n  mean    sd min max range   se
points      1 7 23.86 10.24  11  41    30 3.87
rebounds    2 7  8.00  2.45   6  13     7 0.93</b>
<h3>Example 2: Create Summary Table, Grouped by Specific Variable</h3>
The following code shows how to use the <b>describeBy()</b> function to create a summary table for the data frame, grouped by the ‘team’ variable:
<b>#create summary table, grouped by 'team' variable
describeBy(df, group=df$team, fast=TRUE)
 Descriptive statistics by group 
group: A
         vars n mean   sd min  max range  se
team        1 2  NaN   NA Inf -Inf  -Inf  NA
points      2 2 18.5 4.95  15   22     7 3.5
rebounds    3 2  7.5 0.71   7    8     1 0.5
steals      4 2  1.0 0.00   1    1     0 0.0
------------------------------------------------------------ 
group: B
         vars n mean   sd min  max range  se
team        1 2  NaN   NA Inf -Inf  -Inf  NA
points      2 2 35.0 8.49  29   41    12 6.0
rebounds    3 2  6.0 0.00   6    6     0 0.0
steals      4 2  2.5 0.71   2    3     1 0.5
------------------------------------------------------------ 
group: C
         vars n  mean   sd min  max range   se
team        1 3   NaN   NA Inf -Inf  -Inf   NA
points      2 3 20.00 9.54  11   30    19 5.51
rebounds    3 3  9.67 3.06   7   13     6 1.76
steals      4 3  5.67 1.15   5    7     2 0.67
</b>
The output shows the summary statistics for each of the three teams in the data frame.
<h2><span class="orange">How to Use a SUMPRODUCT IF Formula in Google Sheets</span></h2>
You can use the following methods to create a <b>SUMPRODUCT IF</b> formula in Google Sheets:
<b>Method 1: SUMPRODUCT IF with One Criteria</b>
<b>=SUMPRODUCT(--(A2:A12="value"), C2:C12, D2:D12)
</b>
This formula finds the sum of the products between columns C and D only where the value in column A is equal to “value.”
<b>Method 2: SUMPRODUCT IF with Multiple Criteria</b>
<b>=SUMPRODUCT(--(A2:A12="value1"),--(B2:B12="value2"), C2:C12, D2:D12)
</b>
This formula finds the sum of the products between columns C and D only where the value in column A is equal to “value1” <em>and</em> the value in column B is equal to “value2.”
The following examples show how to use each method with the following dataset in in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumprod_if1.jpg"531">
<h3>Example 1: SUMPRODUCT IF with One Criteria</h3>
We can use the following formula to calculate the sum of the products between the <b>Quantity</b> and <b>Price</b> columns only for the rows where the <b>Store</b> column is equal to “A”:
<b>=SUMPRODUCT(--(A2:A12="A"), C2:C12, D2:D12)</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumprod_if2.jpg"690">
The sum of the products between <b>Quantity</b> and <b>Price</b> for <b>Store</b> A turns out to be <b>86</b>.
We can manually verify this is correct by taking the sum of the products between quantity and price for store A only:
SUMPRODUCT for Store A: 1*6 + 3*12 + 4*6 + 4*4 + 1*4 = <b>86</b>.
<h3>Example 2: SUMPRODUCT IF with Multiple Criteria</h3>
We can use the following formula to calculate the sum of the products between the <b>Quantity</b> and <b>Price</b> columns only for the rows where the <b>Store</b> column is equal to “A” and the <b>Product</b> column is equal to “Gloves”:
<b>=SUMPRODUCT(--(A2:A12="A"),--(B2:B12="Gloves"), C2:C12, D2:D12)</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/sumprod_if3.jpg"679">
The sum of the products between <b>Quantity</b> and <b>Price</b> for “Gloves” in store A turns out to be <b>30</b>.
We can manually verify this is correct by taking the sum of the products between quantity and price for “Gloves” in store A only:
SUMPRODUCT for “Gloves” in Store A: 1*6 + 4*6 = <b>30</b>.
<h2><span class="orange">How to Use SUMSQ in Excel (With Example)</span></h2>
You can use the <b>SUMSQ </b>function in Excel to calculate the sum of squares for a given sample.
This function uses the following basic syntax:
<b>=SUMSQ(value1, value2, value3, ...)</b>
Here’s the formula that <b>SUMSQ </b>actually uses:
Sum of squares = Σx<sub>i</sub><sup>2</sup>
where:
<b>Σ</b>: A fancy symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> data value
The following example shows how to use this function in practice.
<h2>Example: How to Use DEVSQ in Excel</h2>
Suppose we have the following dataset in Excel
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq3.jpg"476">
We can use the following formula to calculated the sum of squares for this dataset:
<b>=SUMSQ(A2:A13)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/sumsq2.jpg">
The sum of squares turns out to be <b>1,402</b>.
We can confirm this is correct by manually calculating the sum of squares for this dataset:
Sum of squares = Σx<sub>i</sub><sup>2</sup>
Sum of squares = 2<sup>2</sup> + 3<sup>2</sup> + 5<sup>2</sup> + 5<sup>2</sup> + 7<sup>2</sup> + 8<sup>2</sup> + 9<sup>2</sup> + 12<sup>2</sup> + 14<sup>2</sup> + 15<sup>2</sup> + 16<sup>2</sup> + 18<sup>2</sup>
Sum of squares = 4 + 9 + 25 + 25 + 49 + 64 + 81 + 144 + 196 + 225 + 256 + 324
Sum of squares = <b>1,402</b>
The sum of squares turns out to be <b>1,402</b>.
This matches the value that we calculated using the <b>SUMSQ</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Excel:
 How to Use DEVSQ in Excel 
 How to Calculate SST, SSR, and SSE in Excel 
 How to Use SUMIF From Another Sheet in Excel 
<h2><span class="orange">How to Use SUMSQ in Google Sheets (With Example)</span></h2>
You can use the <b>SUMSQ </b>function in Google Sheets to calculate the sum of squares for a given sample.
This function uses the following basic syntax:
<b>=SUMSQ(value1, value2, value3, ...)</b>
Here’s the formula that <b>SUMSQ </b>actually uses:
Sum of squares = Σx<sub>i</sub><sup>2</sup>
where:
<b>Σ</b>: A fancy symbol that means “sum”
<b>x<sub>i</sub></b>: The i<sup>th</sup> data value
The following example shows how to use this function in practice.
<h2>Example: How to Use DEVSQ in Google Sheets</h2>
Suppose we have the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/devsq1.jpg"500">
We can use the following formula to calculated the sum of squares for this dataset:
<b>=SUMSQ(A2:A13)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/sumsq1.jpg">
The sum of squares turns out to be <b>1,402</b>.
We can confirm this is correct by manually calculating the sum of squares for this dataset:
Sum of squares = Σx<sub>i</sub><sup>2</sup>
Sum of squares = 2<sup>2</sup> + 3<sup>2</sup> + 5<sup>2</sup> + 5<sup>2</sup> + 7<sup>2</sup> + 8<sup>2</sup> + 9<sup>2</sup> + 12<sup>2</sup> + 14<sup>2</sup> + 15<sup>2</sup> + 16<sup>2</sup> + 18<sup>2</sup>
Sum of squares = 4 + 9 + 25 + 25 + 49 + 64 + 81 + 144 + 196 + 225 + 256 + 324
Sum of squares = <b>1,402</b>
The sum of squares turns out to be <b>1,402</b>.
This matches the value that we calculated using the <b>SUMSQ</b> function.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 How to Use DEVSQ in Google Sheets 
 How to Use SUMIF with Multiple Columns in Google Sheets 
 How to Sum Across Multiple Sheets in Google Sheets 
<h2><span class="orange">How to Add Superscripts & Subscripts to Plots in R</span></h2>
You can use the following basic syntax to add superscripts or subscripts to plots in R:
<b>#define expression with superscript
x_expression &lt;- expression(x^3 ~ variable ~ label)
#define expression with subscript
y_expression &lt;- expression(y[3] ~ variable ~ label)
#add expressions to axis labels
plot(x, y, xlab = x_expression, ylab = y_expression)
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Add Superscripts to Axis Labels</h3>
The following code shows how to add superscripts to the axis labels of a plot in R:
<b>#define data
x &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)
y &lt;- c(9, 12, 14, 16, 15, 19, 26, 29)
#define x and y-axis labels with superscripts
x_expression &lt;- expression(x^3 ~ variable ~ label)
y_expression &lt;- expression(y^3 ~ variable ~ label)
#create plot
plot(x, y, xlab = x_expression, ylab = y_expression)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sup1.png">
Notice that both the x-axis and y-axis have a superscript in their label.
The y-axis superscript is a bit cut off in the plot. To move the axis labels closer to the plot, we can use the  par()  function in R:
<b>#adjust par values (default is (3, 0, 0))
par(mgp=c(2.5, 1, 0)) 
#create plot
plot(x, y, xlab = x_expression, ylab = y_expression)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sup2.png">
<b>Note</b>: We chose “3” as a random value to place in the superscript. Feel free to place any numeric value or character in the superscript.
<h3>Example 2: Add Subscripts to Axis Labels</h3>
The following code shows how to add subscripts to the axis labels of a plot in R:
<b>#define data
x &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)
y &lt;- c(9, 12, 14, 16, 15, 19, 26, 29)
#define x and y-axis labels with superscripts
x_expression &lt;- expression(x[3] ~ variable ~ label)
y_expression &lt;- expression(y[3] ~ variable ~ label)
#create plot
plot(x, y, xlab = x_expression, ylab = y_expression)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sup3.png">
<h3>Example 3: Add Superscripts & Subscripts Inside Plot</h3>
The following code shows how to add a superscript to a text element inside a plot:
<b>#define data
x &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)
y &lt;- c(9, 12, 14, 16, 15, 19, 26, 29)
#create plot
plot(x, y)
#define label with superscript to add to plot
R2_expression &lt;- expression(paste(" ", R^2 , "= ", .905))
#add text to plot
text(x = 2, y = 25, label = R2_expression)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sup4.png">
<h2><span class="orange">A Quick Introduction to Supervised vs. Unsupervised Learning</span></h2>
The field of machine learning contains a massive set of algorithms that can be used for understanding data. These algorithms can be classified into one of two categories:
<b>1. Supervised Learning Algorithms:</b> Involves building a model to estimate or predict an output based on one or more inputs.
<b>2. Unsupervised Learning Algorithms:</b> Involves finding structure and relationships from inputs. There is no “supervising” output.
This tutorial explains the difference between these two types of algorithms along with several examples of each.
<h2>Supervised Learning Algorithms</h2>
A <b>supervised learning algorithm</b> can be used when we have one or more explanatory variables (X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, …, X<sub>p</sub>) and a  response variable  (Y) and we would like to find some function that describes the relationship between the explanatory variables and the response variable:
<b>Y = <em>f</em>(X) + ε</b>
where <em>f</em> represents systematic information that X provides about Y and where ε is a random error term independent of X with a mean of zero.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/supervised1.png">
There are two main types of supervised learning algorithms:
<b>1. Regression:</b> The output variable is continuous (e.g. weight, height, time, etc.)
<b>2. Classification:</b> The output variable is categorical (e.g. male or female, pass or fail, benign or malignant, etc.)
There are two main reasons that we use supervised learning algorithms:
<b>1. Prediction:</b> We often use a set of explanatory variables to predict the value of some response variable (e.g. using <em>square footage</em> and <em>number of bedrooms</em> to predict <em>home price</em>)
<b>2. Inference:</b> We may be interested in understanding the way that a response variable is affected as the value of the explanatory variables change (e.g. how much does home price increase, on average, when the number of bedrooms increases by one?)
Depending on whether our goal is inference or prediction (or a mix of both), we may use different methods for estimating the function <em>f</em>. For example, linear models offer easier interpretation but non-linear models that are difficult to interpret may offer more accurate prediction.
Here is a list of the most commonly used supervised learning algorithms:
Linear regression
Logistic regression
Linear discriminant analysis
Quadratic discriminant analysis
Decision trees
Naive bayes
Support vector machines
Neural networks
<h2>Unsupervised Learning Algorithms</h2>
An <b>unsupervised learning algorithm</b> can be used when we have a list of variables (X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>, …, X<sub>p</sub>) and we would simply like to find underlying structure or patterns within the data.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/supervised2.png">
There are two main types of unsupervised learning algorithms:
<b>1. Clustering:</b> Using these types of algorithms, we attempt to find “clusters” of  observations  in a dataset that are similar to each other. This is often used in retail when a company would like to identify clusters of customers who have similar shopping habits so that they can create specific marketing strategies that target certain clusters of customers.
<b>2. Association:</b> Using these types of algorithms, we attempt to find “rules” that can be used to draw associations. For example, retailers may develop an association algorithm that says “if a customer buys product X they are highly likely to also buy product Y.”
Here is a list of the most commonly used unsupervised learning algorithms:
Principal component analysis
K-means clustering
K-medoids clustering
Hierarchical clustering
Apriori algorithm
<h2>Summary: Supervised vs. Unsupervised Learning</h2>
The following table summarizes the differences between supervised and unsupervised learning algorithms:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/supervised3.png">
And the following diagram summarizes the types of machine learning algorithms:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/supervised4.png">
<h2><span class="orange">How to Suppress Warnings in R (With Examples)</span></h2>
You can use the following methods to suppress warnings in R:
<b>Method 1: Suppress Warnings on Specific Line</b>
<b>suppressWarnings(one line of code)
</b>
<b>Method 2: Suppress Warnings Globally</b>
<b>suppressWarnings({
several lines of code
just a bunch of code
lots and lots of code
})</b>
The following examples show how to use each method in practice with the following code, which produces two warning messages:
<b>#define character vector
x &lt;- c('1', '2', '3', NA, '4', 'Hey')
#convert to numeric vector
x_numeric &lt;- as.numeric(x)
#display numeric vector
print(x_numeric)
Warning message:
NAs introduced by coercion 
[1]  1  2  3 NA  4 NA
#define two vectors
a &lt;- c(1, 2, 3, 4, 5)
b &lt;- c(6, 7, 8, 9)
#add the two vectors
a + b
[1]  7  9 11 13 11
Warning message:
In a + b : longer object length is not a multiple of shorter object length
</b>
<h2>Method 1: Suppress Warnings on Specific Line</h2>
We can wrap the <b>suppressWarnings()</b> function around the <b>as.numeric()</b> function to suppress only the first warning in the code:
<b>#define character vector
x &lt;- c('1', '2', '3', NA, '4', 'Hey')
#convert to numeric vector
suppressWarnings(x_numeric &lt;- as.numeric(x))
#display numeric vector
print(x_numeric)
[1]  1  2  3 NA  4 NA
#define two vectors
a &lt;- c(1, 2, 3, 4, 5)
b &lt;- c(6, 7, 8, 9)
#add the two vectors
a + b
[1]  7  9 11 13 11
Warning message:
In a + b : longer object length is not a multiple of shorter object length</b>
Notice that the first warning message no longer appears but the second warning message still appears.
<h2>Method 2: Suppress Warnings Globally</h2>
We can wrap the <b>suppressWarnings({})</b> function around the entire chunk of code to suppress all warnings globally:
<b>suppressWarnings({
#define character vector
x &lt;- c('1', '2', '3', NA, '4', 'Hey')
#convert to numeric vector
suppressWarnings(x_numeric &lt;- as.numeric(x))
#display numeric vector
print(x_numeric)
[1]  1  2  3 NA  4 NA
#define two vectors
a &lt;- c(1, 2, 3, 4, 5)
b &lt;- c(6, 7, 8, 9)
#add the two vectors
a + b
[1]  7  9 11 13 11
})</b>
Notice that we don’t receive any warnings this time because we wrapped the <b>suppressWarnings({})</b> function around the entire chunk of code.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Avoid R Warning: reached getOption(“max.print”) 
 How to Handle R Warning: glm.fit: algorithm did not converge 
 How to Fix: runtimewarning: invalid value encountered in double_scalars 
<h2><span class="orange">How to Swap Two Columns in Pandas (With Example)</span></h2>
You can use the following custom function to swap the position of two columns in a pandas DataFrame:
<b>def swap_columns(df, col1, col2):
    col_list = list(df.columns)
    x, y = col_list.index(col1), col_list.index(col2)
    col_list[y], col_list[x] = col_list[x], col_list[y]
    df = df[col_list]
    return df
</b>
This function will swap the positions of columns <b>col1</b> and <b>col2</b> in the DataFrame.
The following example shows how to use this function in practice.
<h3>Example: Swap Two Columns in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'points': [18, 22, 19, 14, 14, 11, 20, 28],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      18        5        11
1    B      22        7         8
2    C      19        7        10
3    D      14        9         6
4    E      14       12         6
5    F      11        9         5
6    G      20        9         9
7    H      28        4        12</b>
We can define a <b>swap_columns()</b> function to swap the positions of the “points” and “rebounds” columns:
<b>#define function to swap columns
def swap_columns(df, col1, col2):
    col_list = list(df.columns)
    x, y = col_list.index(col1), col_list.index(col2)
    col_list[y], col_list[x] = col_list[x], col_list[y]
    df = df[col_list]
    return df
#swap points and rebounds columns
df = swap_columns(df, 'points', 'rebounds'):
#view updated DataFrame
print(df)
  team  rebounds  assists  points
0    A        11        5      18
1    B         8        7      22
2    C        10        7      19
3    D         6        9      14
4    E         6       12      14
5    F         5        9      11
6    G         9        9      20
7    H        12        4      28
</b>
Notice that the “points” and “rebounds” columns have been swapped while every other column has remained in the same position.
<h2><span class="orange">How to Swap Two Rows in Pandas (With Example)</span></h2>
You can use the following custom function to swap the position of two rows in a pandas DataFrame:
<b>def swap_rows(df, row1, row2):
    df.iloc[row1], df.iloc[row2] =  df.iloc[row2].copy(), df.iloc[row1].copy()
    return df
</b>
This function will swap the positions of rows in index positions <b>row1</b> and <b>row2 </b>in the DataFrame.
The following example shows how to use this function in practice.
<h2>Example: Swap Two Rows in Pandas</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team' : ['Mavs', 'Nets', 'Kings', 'Cavs', 'Heat', 'Magic'],   'points' : [12, 15, 22, 29, 24, 22],   'assists': [4, 5, 10, 8, 7, 10]})
#view DataFrame
print(df)
    team  points  assists
0   Mavs      12        4
1   Nets      15        5
2  Kings      22       10
3   Cavs      29        8
4   Heat      24        7
5  Magic      22       10
</b>
We can define a <b>swap_rows()</b> function to swap the rows in index positions 0 and 4 in the DataFrame:
<b>#define function to swap rows
def swap_rows(df, row1, row2):
    df.iloc[row1], df.iloc[row2] =  df.iloc[row2].copy(), df.iloc[row1].copy()
    return df
#swap rows in index positions 0 and 4
df = swap_rows(df, 0, 4)
#view updated DataFrame
print(df)
    team  points  assists
0   Heat      24        7
1   Nets      15        5
2  Kings      22       10
3   Cavs      29        8
4   Mavs      12        4
5  Magic      22       10
</b>
Notice that the rows in index positions 0 and 4 have been swapped while every other row has remained in the same position.
<b>Note</b>: Within the <b>swap_rows()</b> function, we used the  .iloc  function to select rows in the DataFrame based on their index position.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 Pandas: How to Count Occurrences of Specific Value in Column 
 Pandas: Get Index of Rows Whose Column Matches Value 
 Pandas: How to Count Missing Values in DataFrame 
<h2><span class="orange">How to Switch Two Columns in R (With Examples)</span></h2>
Occasionally you may want to switch the position of two columns in an R data frame. Fortunately this is easy to do using one of the two following bits of code:
<b>Option 1: Use column syntax.</b>
<b>#define order of data frame columns
df &lt;- df[c("col1", "col2", "col3", "col4")]
</b>
<b>Option 2: Use row and column syntax.</b>
<b>#define order of data frame columns
df &lt;- df[ , c("col1", "col2", "col3", "col4")]</b>
The following examples illustrate how to use these two bits of code in practice.
<h3>Example 1: Switch Two Columns Using Column Syntax</h3>
The following code shows how to create a data frame with four columns and then switch the position of the first and third column:
<b>#create data frame
df &lt;- data.frame(col1=c(1, 2, 6, 3, 6, 6), col2=c(4, 4, 5, 4, 3, 2), col3=c(7, 7, 8, 7, 3, 3), col4=c(9, 9, 9, 5, 5, 3))
#view data frame
df
  col1 col2 col3 col4
1    1    4    7    9
2    2    4    7    9
3    6    5    8    9
4    3    4    7    5
5    6    3    3    5
6    6    2    3    3
#switch positions of first and third column
df &lt;- df[c("col3", "col2", "col1", "col4")]
#view new data frame
df
  col3 col2 col1 col4
1    7    4    1    9
2    7    4    2    9
3    8    5    6    9
4    7    4    3    5
5    3    3    6    5
6    3    2    6    3
</b>
<h3>Example 2: Switch Two Columns Using Row & Column Syntax</h3>
The following code shows how to create a data frame with four columns and then switch the position of the first and third column:
<b>#create data frame
df &lt;- data.frame(col1=c(1, 2, 6, 3, 6, 6), col2=c(4, 4, 5, 4, 3, 2), col3=c(7, 7, 8, 7, 3, 3), col4=c(9, 9, 9, 5, 5, 3))
#view data frame
df
  col1 col2 col3 col4
1    1    4    7    9
2    2    4    7    9
3    6    5    8    9
4    3    4    7    5
5    6    3    3    5
6    6    2    3    3
#switch positions of first and third column
df &lt;- df[ , c("col3", "col2", "col1", "col4")]
#view new data frame
df
  col3 col2 col1 col4
1    7    4    1    9
2    7    4    2    9
3    8    5    6    9
4    7    4    3    5
5    3    3    6    5
6    3    2    6    3</b>
Notice that both methods lead to the same results.
<h2><span class="orange">Sxx Calculator for Linear Regression</span></h2>
In statistics, <b>Sxx</b> represents the sum of squared deviations from the mean value of x.
This value is often calculated when fitting a linear regression model by hand.
To calculate Sxx for a given regression model, simply enter the list of the comma-separated values for the x-values of the dataset in the box below, then click the “Calculate” button:
<b>x values:</b>
<textarea id="x" rows="5" cols="40">1, 2, 2, 3, 5, 8</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Sxx: 33.50000</b>
<script>
function calc() {
//calculate sample mean
var x = document.getElementById('x').value.split(',').map(Number);
var sxx = jStat.variance(x) *x.length;
//output sxx
document.getElementById('sxx').innerHTML = sxx.toFixed(5);
  
} //end calc function
</script>
<h2><span class="orange">Sxy Calculator for Linear Regression</span></h2>
This calculator automatically finds the value for <b>Sxy</b> for a linear regression model based on the x values and y values in a dataset.
To calculate Sxy, simply enter a list of comma-separated values for x and y in the boxes below, then click the “Calculate” button:
<b>x values:</b>
<textarea id="x" rows="5" cols="40">1, 2, 2, 3, 5, 8</textarea>
<b>y values:</b>
<textarea id="y" rows="5" cols="40">8, 12, 14, 19, 22, 21</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
Sxy = 59.00000
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        var sxx = jStat.variance(x) *x.length;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['sxx'] = sxx;
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        lr['r2'] = Math.pow((n*sum_xy - sum_x*sum_y)/Math.sqrt((n*sum_xx-sum_x*sum_x)*(n*sum_yy-sum_y*sum_y)),2);
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope*lr.sxx;
var b = lr.intercept;
document.getElementById('a').innerHTML = a.toFixed(5);
}
//output error message if boths lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">Symmetric Distribution: Definition + Examples</span></h2>
In statistics, a <b>symmetric distribution</b> is a distribution in which the left and right sides mirror each other.
The most well-known symmetric distribution is the  normal distribution , which has a distinct bell-shape.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric1.png">
If you were to draw a line down the center of the distribution, the left and right sides of the distribution would perfectly mirror each other:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric5.png">
In statistics, <b>skewness</b> is a way to describe the symmetry of a distribution. This value can be negative, zero, or positive.
<b>For symmetric distributions, the skewness is zero.</b>
This is in contrast to left-skewed distributions, which have negative skewness:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew2.png">
This is also in contrast to right-skewed distributions, which have positive skewness:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew1.png">
<h3>Properties of Symmetric Distributions</h3>
In a symmetrical distribution, the mean, median, and mode are all equal.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew5.png">
Recall the following definitions for each:
<b>Mean:</b> The average value.
<b>Median:</b> The middle value.
<b>Mode:</b> The value that occurs most often.
In a symmetrical distribution, each of these values is equal to each other.
In each of the examples up to this point, we’ve used unimodal distributions as examples – distributions with only one “peak.” However, a distribution can also be bimodal and be symmetrical.
A  bimodal distribution  is a distribution that has two peaks.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric6.png">
Notice that if we drew a line down the center of this distribution, the left and right sides would still mirror each other.
For these distributions, the mean and the median are equal. However, the mode is located in the two peaks.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric7.png">
<h3>Other Examples of Symmetric Distributions</h3>
Along with the normal distribution, the following distributions are also symmetrical:
<b>The t-Distribution</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric4.png">
<b>The Uniform Distribution</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric3.png">
<b>The Cauchy Distribution</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/symmetric2.png">
If you drew a line down the center of any of these distributions, the left and right sides of each distribution would perfectly mirror each other.
<h3>Symmetric Distributions & The Central Limit Theorem</h3>
One of the most important theorems in all of statistics is the central limit theorem, which states that  the sampling distribution of a sample mean  is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>.
In order to apply the central limit theorem, a sample size must be sufficiently large. It turns out that the exact number for “sufficiently large” depends on the underlying shape of the population distribution.
In particular:
If the population distribution is symmetric, sometimes a sample size as small as 15 is sufficient.
If the population distribution is skewed, generally a sample size of at least 30 is needed.
If the population distribution is extremely skewed, then a sample size of 40 or higher may be necessary.
Thus, the benefit of symmetric distributions is that we require smaller sample sizes to apply the central limit theorem when calculating  confidence intervals  or performing  hypothesis tests .
<h2><span class="orange">What is a Symmetric Histogram? (Definition & Examples)</span></h2>
A <b>histogram</b> is a type of chart that helps us visualize the frequency of values in a dataset.
A <b>symmetric histogram</b> is a type of histogram that has perfectly identical halves if we were to draw a line down the center of it.
There are two common types of symmetric histograms:
<b>Unimodal symmetric histogram</b>: A histogram with one peak
<b>Bimodal symmetric histogram</b>: A histogram with two peaks
The following examples show what each of these histograms looks like.
<h2>Example 1: Unimodal Symmetric Histogram</h2>
The following histogram is an example of a unimodal symmetric histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/symm1.jpg"520">
If we were to draw a line down the center of the histogram, both the left and right sides would look the exact same:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/symm2.jpg">
We refer to this as a <b>unimodal</b> symmetric histogram because “uni” means “one” and this histogram only has one peak directly in the middle.
<h2>Example 2: Bimodal Symmetric Histogram</h2>
The following histogram is an example of a bimodal symmetric histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/symm3.jpg"544">
If we were to draw a line down the center of the histogram, both the left and right sides would look the exact same:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/symm4.jpg"551">
We refer to this as a <b>bimodal</b> symmetric histogram because “bi” means “two” and this histogram has two peaks.
<b>Related:</b>  An Introduction to Bimodal Distributions 
<h2>What is a Roughly Symmetric Histogram?</h2>
In the real world, there are rarely perfectly symmetrical histograms but there are often <b>roughly symmetrical histograms</b>.
These are histograms that are “roughly” symmetrical, meaning the two sides look roughly the same if you draw a line down the center of the histogram.
One example of this would be the distribution of the weights of newborn babies.
It’s well known that newborn weights follow a unimodal distribution with an average around 7.5 lbs.
If we create a histogram of baby weights, we’ll see a “peak” at 7.5 lbs with some babies weighing more and some weighing less:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/unimodal1-2.png">
This is a roughly symmetrical histogram. If we drew a vertical line down the center, each side would look roughly the same.
Another real-world example is the distribution of ACT scores for high school students in the U.S.
The average score is about 21 with some students scoring less and some scoring higher. If we create a histogram of ACT scores for all students in the U.S. we’ll see a single “peak” at 21 with some students scoring higher and some scoring lower:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/unimodal2.png">
This is also a roughly symmetrical histogram. If we drew a vertical line down the center, each side would look roughly the same.
When working with real-world datasets, you’ll rarely encounter perfectly symmetrical histograms but you will often encounter roughly symmetrical histograms.
<h2>Additional Resources</h2>
The following tutorials provide additional information about histograms:
 How to Describe the Shape of Histograms 
 How to Compare Histograms 
 How to Estimate the Mean and Median of Any Histogram 
<h2><span class="orange">How to Perform Systematic Sampling in Excel (Step-by-Step)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>systematic sampling</b>, which is implemented with a simple two step process:
<b>1.</b> Place each member of a population in some order.
<b>2.</b> Choose a random starting point and select every n<sup>th</sup> member to be in the sample.
The following step-by-step example shows how to perform systematic sampling in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for some dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel1.png">
<h3>Step 2: Enter Parameter Values</h3>
Next, we need to decide how many values we’d like to randomly sample. For this example, we’ll choose a sample size of n = 4.
Next, we’ll use the following formulas to calculate the population size, step size for our systematic sampling, and random starting point:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel2.png">
<h3>Step 3: Label Each Data Value</h3>
Our <b>RANDBETWEEN()</b> function randomly chose the value 3 as the starting point in our dataset. Thus, give the data value at position 3 a label of 1 and then number down to the step size of 5:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel3.png">
Next, type <b>=D4</b> in the next available cell:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel4.png">
Next, copy and paste this formula down to all remaining cells in column D. Then manually fill in the values at the top of the of column D:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel5.png">
<h3>Step 4: Filter Values</h3>
Lastly, we need to filter the dataset to only include data values that have a label of 1.
To do so, click the <b>Data</b> tab along the top ribbon. Then click the <b>Filter</b> icon. Then click the column called <b>Labels</b> and filter to only include rows that have a label with value 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel6.png">
Once you do so, the data will be filtered to only include data values that have a Label of 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/sysExcel7.png">
This represents our sample of size 4.
<h2><span class="orange">Systematic Sampling in Pandas (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>systematic sampling</b>, which is implemented with a simple two step process:
<b>1.</b> Place each member of a population in some order.
<b>2.</b> Choose a random starting point and select every n<sup>th</sup> member to be in the sample.
This tutorial explains how to perform systematic sampling on a pandas DataFrame in Python.
<h2>Example: Systematic Sampling in Pandas</h2>
Suppose a teacher wants to obtain a sample of 100 students from a school that has 500 total students. She chooses to use systematic sampling in which she places each student in alphabetical order according to their last name, randomly chooses a starting point, and picks every 5th student to be in the sample.
The following code shows how to create a fake data frame to work with in Python:
<b>import pandas as pd
import numpy as np
import string
import random
#make this example reproducible
np.random.seed(0)
#create simple function to generate random last names
def randomNames(size=6, chars=string.ascii_uppercase):
    return ''.join(random.choice(chars) for _ in range(size))
#create DataFrame
df = pd.DataFrame({'last_name': [randomNames() for _ in range(500)],   'GPA': np.random.normal(loc=85, scale=3, size=500)})
#view first six rows of DataFrame
df.head()
last_nameGPA
0PXGPIV86.667888
1JKRRQI87.677422
2TRIZTC83.733056
3YHUGIN85.314142
4ZVUNVK85.684160</b>
And the following code shows how to obtain a sample of 100 students through systematic sampling:
<b>#obtain systematic sample by selecting every 5th row
sys_sample_df = df.iloc[::5]
#view first six rows of DataFrame
sys_sample_df.head()
   last_name      gpa
3      ORJFW 88.78065
8      RWPSB 81.96988
13     RACZU 79.21433
18     ZOHKA 80.47246
23     QJETK 87.09991
28     JTHWB 83.87300
#view dimensions of data frame
sys_sample_df.shape
(100, 2)
</b>
Notice that the first member included in the sample was in the first row of the original data frame. Each subsequent member in the sample is located 5 rows after the previous member.
And from using <b>shape() </b>we can see that the systematic sample we obtained is a data frame with 100 rows and 2 columns.
<h2><span class="orange">Systematic Sampling in R (With Examples)</span></h2>
Researchers often take  samples  from a population and use the data from the sample to draw conclusions about the population as a whole.
One commonly used sampling method is <b>systematic sampling</b>, which is implemented with a simple two step process:
<b>1.</b> Place each member of a population in some order.
<b>2.</b> Choose a random starting point and select every n<sup>th</sup> member to be in the sample.
This tutorial explains how to perform systematic sampling in R.
<h2>Example: Systematic Sampling in R</h2>
Suppose a superintendent wants to obtain a sample of 100 students from a school that has 500 total students. She chooses to use systematic sampling in which she places each student in alphabetical order according to their last name, randomly chooses a starting point, and picks every 5th student to be in the sample.
The following code shows how to create a fake data frame to work with in R:
<b>#make this example reproducible
set.seed(1)
#create simple function to generate random last names
randomNames &lt;- function(n = 5000) {
  do.call(paste0, replicate(5, sample(LETTERS, n, TRUE), FALSE))
}
#create data frame
df &lt;- data.frame(last_name = randomNames(500), gpa = rnorm(500, mean=82, sd=3))
#view first six rows of data frame
head(df)
  last_name      gpa
1     GONBW 82.19580
2     JRRWZ 85.10598
3     ORJFW 88.78065
4     XRYNL 85.94409
5     FMDCE 79.38993
6     XZBJC 80.49061</b>
And the following code shows how to obtain a sample of 100 students through systematic sampling:
<b>#define function to obtain systematic sample
obtain_sys = function(N,n){
  k = ceiling(N/n)
  r = sample(1:k, 1)
  seq(r, r + k*(n-1), k)
}
#obtain systematic sample
sys_sample_df = df[obtain_sys(nrow(df), 100), ]
#view first six rows of data frame
head(sys_sample_df)
   last_name      gpa
3      ORJFW 88.78065
8      RWPSB 81.96988
13     RACZU 79.21433
18     ZOHKA 80.47246
23     QJETK 87.09991
28     JTHWB 83.87300
#view dimensions of data frame
dim(sys_sample_df)
[1] 100   2</b>
Notice that the first member included in the sample was in row 3 of the original data frame. Each subsequent member in the sample is located 5 rows after the previous member.
And from using <b>dim() </b>we can see that the systematic sample we obtained is a data frame with 100 rows and 2 columns.
<h2><span class="orange">How to Find t Alpha/2 Values</span></h2>
Whenever you come across the term <b>t<sub>α/2</sub></b> in statistics, it is simply referring to the <b>t critical value </b>from the  t-distribution table  that corresponds to α/2.
This tutorial explains the following:
How to find t<sub>α/2</sub> using a z table.
How to find t<sub>α/2</sub> using a calculator.
How to use t<sub>α/2</sub> values.
Let’s jump in!
<h3>How to find t<sub>α/2</sub> using a t table</h3>
Suppose we want to find t<sub>α/2</sub> for some test that is using the following values:
Alpha Level: 0.10
Types of test: Two-tailed
Degrees of freedom: 20
Using a t-distribution table, we can find that the t critical value is <b>1.725</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/tAlpha1.png">
<h3>How to find t<sub>α/2</sub> using a calculator</h3>
We can also use the  Inverse t Distribution Calculator  to find t<sub>α/2</sub> for some test.
For example, suppose we once again want to find t<sub>α/2</sub> for some test that is using the following values:
Alpha Level: 0.10
Types of test: Two-tailed
Degrees of freedom: 20
We can enter the following values into the calculator and find that the t critical value is <b>1.7247</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/tAlpha2.png">
This matches the t critical value that we found in the t distribution table.
<h3>How to Use t<sub>α/2</sub> Values</h3>
In practice, t critical values are used in hypothesis tests to determine whether or not the results of a test are statistically significant.
The basic process for doing so is as follows:
<b>Step 1:</b> Calculate the test statistic using raw data.
<b>Step 2:</b> Compare the test statistic to the t critical value (t<sub>α/2</sub>).
<b>Step 3:</b> Reject or fail to reject the null hypothesis of the test.
If the absolute value of the t test statistic is greater than the t critical value, then we can reject the null hypothesis of the test.
Otherwise, if the absolute value of the t test statistic is less than the t critical value, then we fail to reject the null hypothesis.
<h2><span class="orange">How to Find t Critical Values in R</span></h2>
Whenever you conduct a t-test, you will get a test statistic as a result. To determine if the results of the t-test are statistically significant, you can compare the test statistic to a <b>t critical value</b>. If the absolute value of the test statistic is greater than the t critical value, then the results of the test are statistically significant.
The t critical value can be found by using a  t distribution table  or by using statistical software.
To find the t critical value, you need to specify:
A significance level (common choices are 0.01, 0.05, and 0.10)
The degrees of freedom
Using these two values, you can determine the t critical value to be compared with the test statistic.
<h3>How to Find the T Critical Value in R</h3>
To find the T critical value in R, you can use the qt() function, which uses the following syntax:
<b>qt(p, df, lower.tail=TRUE)</b>
where:
<b>p: </b>The significance level to use
<b>df</b>: The degrees of freedom
<b>lower.tail:</b> If TRUE, the probability to the left of <b>p </b>in the t distribution is returned. If FALSE, the probability to the right is returned. Default is TRUE.
The following examples illustrate how to find the t critical value for a left-tailed test, right-tailed test, and a two-tailed test.
<h3>Left-tailed test </h3>
Suppose we want to find the t critical value for a left-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>#find t critical value
qt(p=.05, df=22, lower.tail=TRUE)
[1] -1.717144
</b>
The t critical value is <b>-1.7171</b>. Thus, if the test statistic is less than this value, the results of the test are statistically significant.
<h3>Right-tailed test </h3>
Suppose we want to find the t critical value for a right-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>#find t critical value
qt(p=.05, df=22, lower.tail=FALSE)
[1] 1.717144 
</b>
The t critical value is <b>1.7171</b>. Thus, if the test statistic is greater than this value, the results of the test are statistically significant.
<h3>Two-tailed test </h3>
Suppose we want to find the t critical values for a two-tailed test with a significance level of .05 and degrees of freedom = 22:
<b>#find two-tailed t critical values
qt(p=.05/2, df=22, lower.tail=FALSE)
[1] 2.073873
</b>
Whenever you perform a two-tailed test, there will be two critical values. In this case, the T critical values are <b>2.0739 </b>and <b>-2.0739</b>. Thus, if the test statistic is less than -2.0739 or greater than 2.0739, the results of the test are statistically significant.
<i>You can find more R tutorials  here .</i>
<h2><span class="orange">How to Create a t-Distribution Graph in Excel</span></h2>
A <b>t-distribution</b> is a type of continuous probability distribution. It has the following properties:
It is continuous
It is bell-shaped
It is symmetric around zero
It is defined by one parameter: the number of degrees of freedom
The t-distribution converges to the standard normal distribution as the number of degrees of freedom converges to infinity
The t-distribution is often used in various hypothesis tests when sample sizes are small (n &lt; 30) in place of  the normal distribution .
<b>Related: </b> How to Make a Bell Curve in Excel 
<h2>How to Create a t-Distribution Graph in Excel</h2>
Often we are interested in visualizing the t-distribution. Fortunately, it’s easy to create a t-distribution graph in Excel by using the <b>T.DIST() </b>function which uses the following syntax:
<b>T.DIST(x, deg_freedom, cumulative)</b>
<b>x: </b>the value for the random variable in the t-distribution
<b>deg_freedom: </b>an integer that indicates the number of degrees of freedom in the t-distribution
<b>cumulative: </b>when set to TRUE, it returns the value for the cumulative density function; when set to FALSE, it returns the value for the probability density function
Next, we’ll show how to create the following t-distribution graph in Excel:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel3.jpg">
To create a t-distribution graph in Excel, we can perform the following steps:
<b>1. Enter the number of degrees of freedom (df) in cell A2.</b> In this case, we will use 12.
<b>2. Create a column for the range of values for the random variable in the t-distribution</b>. In this case, we will create a range of values from -4 to 4 by increments of 2 in cells B2 through B42.
<b>3. Create a column for the pdf of the t-distribution associated with the random values. </b>In cell C2, type the formula <b>T.DIST(B2, $A$2, FALSE)</b>. Then hover over the bottom right of cell C2 until the <b>+ </b>sign appears. Click and drag down to autofill the values for cells C2 through C42.
<b>4. Create the graph. </b>Highlight the two columns (B2:C42). Click the <em>INSERT</em> tab. In the <em>Charts</em> area, click <em>scatter with smooth lines</em>. The following chart will appear:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel4.jpg">
<b>5. Change the graph appearance. </b>By default, the y-axis appears in the middle of the graph and the gridlines show up in the background. We can change this by using the following steps:
Right click on the x-axis. Click <em>Format Axis</em>. Under <em>Vertical axis crosses</em>, click <em>Axis Value </em>and type in <b>-5</b>.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel5.jpg">
Click inside the chart. A <b>+ </b>sign will appear in the top right corner. Click it to remove the gridlines (if you’d like) and add axes titles. In this example, we choose to label the x-axis as <em>t, </em>labelthe y-axis as <em>f(t)</em>, and remove the title entirely. The picture below shows the end result:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel3.jpg">
<h2>How to Create Several t-Distribution Graphs in Excel</h2>
We can also display several t-distribution curves in one graph if we’d like. This can be useful if we want to see how the shape of the t-distribution changes for various values for the degrees of freedom.
In order to display several t-distribution curves, we simply need to add three new columns for a t-distribution with a different value for the degrees of freedom. For example, we can create t-distribution curves for degrees of freedom = 6 and degrees of freedom = 60:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel6.jpg">
To create the t-distribution curve for df = 60, we can use the exact same steps we used before. To add a curve for df = 6, we can perform the following steps:
Right click inside the chart. Click <em>Select Data</em>. 
Under <em>Legend Entries (Series)</em>, click <em>Edit</em>. 
Choose the cells for the <em>X Values </em>and <em>Y Values </em>that contain the values in columns F and G. Then click <em>OK</em>. The following curve will be added to the chart:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/TDistExcel7.jpg">
You’ll notice a pattern for t-distribution graphs:
The higher the degrees of freedom, the more narrow the t-distribution curve will be. That is, it will have a higher peak.
Conversely, the lower the degrees of freedom, the more flattened out the curve will be and the “fatter” the tails of the graph will be.
As the degrees of freedom approaches infinity, the curve will converge to the standard normal distribution curve.
<h2>Modifying the Aesthetics of the Graph</h2>
Note that you can also modify the aesthetics of the graph by changing the following features:
Modify the size and color of the title
Modify the size and color of the axes labels
Choose whether or not to display gridlines in the background
Modify the background color of the graph
Modify the color of the curve itself
Choose whether or not to display the tick marks along the axes
Depending on how you would like the graph to appear, Excel gives you the ability to modify the chart quite a bit.
<b>Find more Excel tutorials on Statology  here .</b>
<h2><span class="orange">How to Use the t Distribution in Python</span></h2>
The <b>t distribution</b> is a probability distribution that is similar to the  normal distribution  except it has heavier “tails” than the normal distribution.
That is, more values in the distribution are located in the tail ends than the center compared to the normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/norm_vs_t2.png">
This tutorial explains how to use the t distribution in Python.
<h3>How to Generate a t Distribution</h3>
You can use the <b>t.rvs(df, size)</b> function to generate random values from a t distribution with a specific degrees of freedom and sample size:
<b>from scipy.stats import t
#generate random values from t distribution with df=6 and sample size=10
t.rvs(df=6, size=10)
array([-3.95799716, -0.01099963, -0.55953846, -1.53420055, -1.41775611,
       -0.45384974, -0.2767931 , -0.40177789, -0.3602592 ,  0.38262431])
</b>
The result is an array of 10 values that follow a t distribution with 6 degrees of freedom.
<h3>How to Calculate P-Values Using t Distribution</h3>
We can use the <b>t.cdf(x, df, loc=0, scale=1) </b>function to find the p-value associated with some t test statistic.
<b>Example 1: Find One-Tailed P-Value</b>
Suppose we perform a one-tailed  hypothesis test  and end up with a t test statistic of <b>-1.5</b> and degrees of freedom = <b>10</b>.
We can use the following syntax to calculate the p-value that corresponds to this test statistic:
<b>from scipy.stats import t
#calculate p-value
t.cdf(x=-1.5, df=10)
0.08225366322272008
</b>
The one-tailed p-value that corresponds to a t test statistic of -1.5 with 10 degrees of freedom is <b>0.0822</b>.
<b>Example 2: Find Two-Tailed P-Value</b>
Suppose we perform a two-tailed  hypothesis test  and end up with a t test statistic of <b>2.14</b> and degrees of freedom = <b>20</b>.
We can use the following syntax to calculate the p-value that corresponds to this test statistic:
<b>from scipy.stats import t
#calculate p-value
(1 - t.cdf(x=2.14, df=20)) * 2
0.04486555082549959
</b>
The two-tailed p-value that corresponds to a t test statistic of 2.14 with 20 degrees of freedom is <b>0.0448</b>.
<b>Note</b>: You can double check these answers by using the  Inverse t Distribution Calculator .
<h3>How to Plot a t Distribution</h3>
You can use the following syntax to plot a t distribution with a specific degrees of freedom:
<b>from scipy.stats import t
import matplotlib.pyplot as plt
#generate t distribution with sample size 10000
x = t.rvs(df=12, size=10000)
#create plot of t distribution
plt.hist(x, density=True, edgecolor='black', bins=20)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/tdist1.png">
Alternatively, you can create a  density curve  using the  seaborn  visualization package:
<b>import seaborn as sns
#create density curve
sns.kdeplot(x)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/tdist2.png">
<h2><span class="orange">t-Distribution Table</span></h2>
  
<h2><span class="orange">T Score to P Value Calculator</span></h2>
<label for="t"><b>t score</b></label>
<input type="number" id="t" min="0" value="1.5">
<label for="df"><b>Degrees of freedom</b></label>
<input type="number" id="df" min="0" value="11">
<b>One-tailed or two-tailed hypothesis?</b>
<label for="one_tailed">One-tailed</label>
<input type="radio" id="one_tailed" name="tails"><label for="two_tailed">Two-tailed</label>
<input type="radio" id="two_tailed" name="tails" checked>
<b>Significance level</b>
<label for="one">0.01</label>
<input type="radio" id="one" name="sig" value="0.01"><label for="five">0.05</label>
<input type="radio" id="five" name="sig" value="0.05" checked><label for="ten">0.10</label>
<input type="radio" id="ten" name="sig" value="0.10">
<input type="button" id="button" onclick="getPVALUE()" value="Calculate">
<div>
P-value: 0.08088
<div>
The result is NOT SIGNIFICANT at p &lt; 0.05
<script>
//the following function originated from https://stackoverflow.com/questions/16194730/seeking-a-statistical-javascript-function-to-return-p-value-from-a-z-score/30435852
function getPVALUE() {
    
//get input degrees of freedom, t-value
var df = document.getElementById('df').value*1;
var t_input = document.getElementById('t').value*1;
var t = Math.abs(t_input);
//calculate p-value
var p_value = 1 - jStat.studentt.cdf(t, df);
//get tails input
if (document.getElementById('two_tailed').checked) {
p_value = p_value * 2;
}
//get significance level input
var sig_level = '';
if (document.getElementById('one').checked) {
sig_level = 0.01;
} else if (document.getElementById('five').checked) {
sig_level = 0.05;
} else {
    sig_level = 0.10;
}
//get significance verdict
var sig_verdict = '';
if (p_value < sig_level) {
sig_verdict = 'SIGNIFICANT';
} else {
sig_verdict = 'NOT SIGNIFICANT';
}
//output values
    document.getElementById('exactProb').innerHTML = p_value.toFixed(5);
document.getElementById('sig_level').innerHTML = sig_level.toFixed(2);
document.getElementById('sig_verdict').innerHTML = sig_verdict;
  }
</script>
<h2><span class="orange">T-Score vs. Z-Score: When to Use Each</span></h2>
Two terms that often confuse students in statistics classes are <b>t-scores</b> and <b>z-scores</b>.
Both are used extensively when performing  hypothesis tests  or constructing  confidence intervals , but they’re slightly different.
Here’s the formula for each:
<b>t-score = (x – μ) / (s/√n)</b>
where:
<b>x</b>: Sample mean
<b>μ</b>: Population mean
<b>s</b>: Sample standard deviation
<b>n</b>: Sample size
<b>z-score = (x – μ) / σ</b>
where:
<b>x</b>: Raw data value
<b>μ</b>: Population mean
<b>σ</b>: Population standard deviation
This flow chart shows when you should use each, depending on your data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/zscore_vs_tscore1.png">
The following examples show how to calculate a t-score and z-score in practice.
<h3>Example 1: Calculating a T-Score</h3>
Suppose a restaurant makes burgers that claim to have a mean weight of μ = 0.25 pounds.
Suppose we take a random sample of n = 20 burgers and find that the sample mean weight is x = 0.22 pounds with a standard deviation of s = 0.05 pounds. Perform a hypothesis test to determine if the true mean weight of all burgers produced by this restaurant is equal to 0.25 pounds.
For this example, we would use a <b>t-score</b> to perform the hypothesis test because neither of the following two conditions are met.
The population standard deviation (σ) is known. (σ is not provided in this example)
The sample size is greater than 30. (n = 20 in this example)
Thus, we would calculate the t-score as:
t-score = (x – μ) / (s/√n)
t-score = (.22 – .25) / (.05 / √20)
t- score = -2.68
According to the  T Score to P Value Calculator , the p-value that corresponds to this t-score is <b>0.01481</b>. 
Since this p-value is less than .05, we have sufficient evidence to say that the mean weight of burgers produced at this restaurant is not equal to 0.25 pounds.
<h3>Example 2: Calculating a Z-Score</h3>
Suppose a company manufactures batteries that are known to have a lifespan that follows a normal distribution with a mean of μ = 20 hours and a standard deviation of σ = 5 hours.
Suppose we take a random sample of n = 50 batteries and find that the sample mean is x = 21 hours. Perform a hypothesis test to determine if the true mean lifespan of all batteries manufactured by this company is equal to 20 hours.
For this example, we would use a <b>z-score</b> to perform the hypothesis test because the following two conditions are met:
The population standard deviation (σ) is known. (σ is equal to 5 in this example)
The sample size is greater than 30. (n = 50 in this example)
Thus, we would calculate the z-score as:
z-score = (x – μ) / σ
z-score = (21 – 20) / 5
z- score = 0.2
According to the  Z Score to P Value Calculator , the p-value that corresponds to this z-score is <b>0.84184</b>. 
Since this p-value is not less than .05, we don’t have sufficient evidence to say that the mean lifespan of all batteries manufactured by this company is different than 20 hours.
<h2><span class="orange">The Four Assumptions Made in a T-Test</span></h2>
A  two sample t-test  is used to test whether or not the means of two populations are equal.
This type of test makes the following assumptions about the data:
<b>1. Independence: </b>The observations in one sample are independent of the observations in the other sample.
<b>2. Normality: </b>Both samples are approximately normally distributed.
<b>3. Homogeneity of Variances: </b>Both samples have approximately the same variance.
<b>4. Random Sampling: </b>Both samples were obtained using a random sampling method.
If one or more of these assumptions are violated, then the results of the two sample t-test may be unreliable or even misleading.
In this tutorial we provide an explanation of each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2>Assumption 1: Independence</h2>
A two sample t-test makes the assumption that the  observations  in one sample are independent of the observations in the other sample.
This is a crucial assumption because if the same individuals appear in both samples then it isn’t valid to draw conclusions about the differences between the samples.
<h3>How to Check this Assumption</h3>
The easiest way to check this assumption is to verify that each observation only appears in each sample once and that the observations in each sample were collected using random sampling.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated, the results of the two sample t-test are completely invalid. In this scenario, it’s best to collect two new samples using a random sampling method and ensure that each individual in one sample does not belong to the other sample.
<h2>Assumption 2: Normality</h2>
A two sample t-test makes the assumption that both samples are approximately normally distributed.
This is a crucial assumption because if the samples are not normally distributed then it isn’t valid to use the p-values from the test to draw conclusions about the differences between the samples.
<h3>How to Check this Assumption</h3>
If the sample sizes are small (n &lt; 50), then we can use a Shapiro-Wilk test to determine if each sample size is normally distributed. If the p-value of the test is less than a certain significance level, then the data is likely not normally distributed.
If the sample sizes are large, then it’s better to use a  Q-Q plot  to visually check if the data is normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/qqplot.jpg"459">
If the data points roughly fall along a straight diagonal line in a Q-Q plot, then the dataset likely follows a normal distribution.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated then we can perform a  Mann-Whitney U test , which is considered the non-parametric equivalent to the two sample t-test and does not make the assumption that the two samples are normally distributed.
<h2>Assumption 3: Homogeneity of Variances</h2>
A two sample t-test makes the assumption that the two samples have roughly equal variances.
<h3>How to Check this Assumption</h3>
We use the following rule of thumb to determine if the variances between the two samples are equal: If the ratio of the larger variance to the smaller variance is less than 4, then we can assume the variances are approximately equal and use the two sample t-test.
For example, suppose sample 1 has a variance of 24.5 and sample 2 has a variance of 15.2. The ratio of the larger sample variance to the smaller sample variance would be calculated as:
<b>Ratio:</b> 24.5 / 15.2 = 1.61
Since this ratio is less than 4, we could assume that the variances between the two groups are approximately equal.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated then we can perform  Welch’s t-test , which is a non-parametric version of the two sample t-test and does not make the assumption that the two samples have equal variances.
<h2>Assumption 4: Random Sampling</h2>
A two sample t-test makes the assumption that both samples were obtained using a random sampling method.
<h3>How to Check this Assumption</h3>
There is no formal statistical test we can use to test this assumption. Instead, we just need to make sure that both samples were obtained use a  random sampling method  such that each individual in the population of interest had an equal probability of being included in either sample.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated, then it’s unlikely that our two samples are  representative  of the population of interest. In this case, we can’t generalize the findings from the two sample t-test to the overall  population  with reliability.
In this scenario, it’s best to collect two new samples using a random sampling method.
<h2><span class="orange">How to Perform a t-Test for Correlation</span></h2>
A  Pearson correlation coefficient  is used to quantify the linear association between two variables.
It always takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation.
0 indicates no linear correlation.
1 indicates a perfectly positive linear correlation.
To determine if a correlation coefficient is statistically significant you can perform a t-test, which involves calculating a t-score and a corresponding p-value.
The formula to calculate the t-score is:
<b>t = r√(n-2) / (1-r<sup>2</sup>)</b>
where:
<b>r:</b> The correlation coefficient
<b>n:</b> The sample size
The p-value is calculated as the corresponding two-sided p-value for the t-distribution with n-2 degrees of freedom.
The following example shows how to perform a t-test for a correlation coefficient.
<h3>Example: Performing a t-Test for Correlation</h3>
Suppose we have the following dataset with two variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/tcor1-1.png">
Using some statistical software (Excel, R, Python, etc.) we can calculate the correlation coefficient between the two variables to be <b>0.707</b>.
This is a highly positive correlation, but to determine if it’s statistically significant we need to calculate the corresponding t-score and p-value.
We can calculate the t-score as:
t = r√(n-2) / (1-r<sup>2</sup>)
t = .707√(10-2) / (1-.707<sup>2</sup>)
t = <b>2.828</b>
Using a  T Score to P Value Calculator , we find that the corresponding p-value is <b>0.022</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/tcor2.png">
Since this p-value is less than .05, we would conclude that the correlation between these two variables is statistically significant.
<h2><span class="orange">How to Perform t-Tests in Google Sheets</span></h2>
Broadly speaking, there are three types of t-tests:
One sample t-test
Two sample t-test
Paired samples t-test
This tutorial provides examples of how to perform each of these tests in Google Sheets.
<h3>Example: One Sample t-Test</h3>
<b>Definition: </b>A  one sample t-test  is used to test whether or not the mean of a population is equal to some value.
<b>Example: </b>A botanist wants to know if the mean height of a certain species of plant is equal to 15 inches. She collects a  random sample  of 12 plants and records each of their heights in inches.
The following screenshot shows how to perform a one sample t-test to determine if the true population mean height is equal to 15 inches:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/t_test_sheets1.png">
The two hypotheses for this particular one sample t test are as follows:
<b>H<sub>0</sub>: </b>μ = 15 (the mean height for this species of plant is 15 inches)
<b>H<sub>A</sub>: </b>μ ≠15 (the mean height is <em>not </em>15 inches)
Because the p-value of our test<b> (0.120145) </b>is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height for this particular species of plant is different from 15 inches.
<h3>Example: Two Sample t-Test</h3>
<b>Definition: </b>A  two sample t-test  is used to test whether or not the means of two populations are equal.
<b>Example: </b>Researchers want to know whether or not two different species of plants in a particular country have the same mean height. They collect a random sample of 20 plants from each species and record each plant height in inches.
The following screenshot shows how to perform a two sample t-test using the <b>T.TEST()</b> function to determine if the two population mean heights are equal:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/t_test_sheets2.png">
<em><b>Note: </b>It’s also possible to perform a one-tailed two sample t-test with or without the assumption that both samples have the same variance. Refer to the  T.TEST documentation  to see how to adjust the assumptions for the test.</em>
The two hypotheses for this two sample t test are as follows:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>1</sub>: </b>μ<b><sub>1</sub></b> ≠ μ<sub>2</sub> (the two population means are not equal)
Because the p-value of our test<b> (0.530047) </b>is greater than alpha = 0.05, we fail to reject the null hypothesis of the test. We do not have sufficient evidence to say that the mean height for this particular species of plant is different from 15 inches.
<h3>Example: Paired Samples t-Test</h3>
<b>Definition: </b>A  paired samples t-test  is used to compare the means of two samples when each observation in one sample can be paired with an observation in the other sample.
<b>Example: </b>We want to know whether a study program significantly impacts student performance on a particular exam. To test this, we have 20 students in a class take a pre-test. Then, we have each of the students participate in the study program for two weeks. Then, the students retake a test of similar difficulty.
The following screenshot shows how to perform a paired sample t-test to compare the difference between the mean scores on the first and second test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/t_test_sheets3.png">
<em><b>Note: </b>It’s also possible to perform a one-tailed two sample t-test with or without the assumption that both samples have the same variance. Refer to the  T.TEST documentation  to see how to adjust the assumptions for the test.</em>
The two hypotheses for this paired samples t test are as follows:
<b>H<sub>0</sub>:</b> μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>1</sub>: </b>μ<b><sub>1</sub></b> ≠ μ<sub>2</sub> (the two population means are not equal)
Because the p-value of our test<b> (0.011907) </b>is less than alpha = 0.05, we reject the null hypothesis of the test. We have sufficient evidence to say that there is a statistically significant difference between the mean pre-test and post-test score.
<h2><span class="orange">Understanding the t-Test in Linear Regression</span></h2>
<b>Linear regression</b> is used to quantify the relationship between a predictor variable and a response variable.
Whenever we perform linear regression, we want to know if there is a statistically significant relationship between the predictor variable and the response variable.
We test for significance by performing a t-test for the regression slope. We use the following null and alternative hypothesis for this t-test:
<b>H<sub>0</sub></b>: β<sub>1</sub> = 0 (the slope is equal to zero)
<b>H<sub>A</sub></b>: β<sub>1</sub> ≠ 0 (the slope is not equal to zero)
We then calculate the test statistic as follows:
<b><em>t</em> = <em>b</em> / SE<sub><em>b</em></sub></b>
where:
<em><b>b</b>:</em> coefficient estimate
<b>SE<sub><em>b</em></sub></b>: standard error of the coefficient estimate
If the p-value that corresponds to <em>t</em> is less than some threshold (e.g. α = .05) then we reject the null hypothesis and conclude that there is a statistically significant relationship between the predictor variable and the response variable.
The following example shows how to perform a t-test for a linear regression model in practice.
<h3>Example: Performing a t-Test for Linear Regression</h3>
Suppose a professor wants to analyze the relationship between hours studied and exam score received for 40 of his students.
He performs simple linear regression using hours studied as the predictor variable and exam score received as the response variable.
The following table shows the results of the regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/testReg1-1.png">
To determine if hours studied has a statistically significant relationship with final exam score, we can perform a t-test.
We use the following null and alternative hypothesis for this t-test:
<b>H<sub>0</sub></b>: β<sub>1</sub> = 0 (the slope for hours studied is equal to zero)
<b>H<sub>A</sub></b>: β<sub>1</sub> ≠ 0 (the slope for hours studied is not equal to zero)
We then calculate the test statistic as follows:
<em>t</em> = <em>b</em> / SE<sub><em>b</em></sub>
<em>t</em> = 1.117 / 1.025
<em>t</em> = 1.089
The p-value that corresponds to<em> t</em> = 1.089 with df = n-2 = 40 – 2 = 38 is <b>0.283</b>.
Note that we can also use the  T Score to P Value Calculator  to calculate this p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/testReg2.png">
Since this p-value is not less than .05, we fail to reject the null hypothesis.
This means that hours studied <em>does not</em> have a statistically significant relationship between final exam score.
<h2><span class="orange">6 Examples of Using T-Tests in Real Life</span></h2>
In statistics, there are three commonly used t-tests:
 One Sample t-test : Used to compare a population mean to some value.
 Independent Two Sample t-test : Used to compare two population means.
 Paired Samples t-test : Used to compare two population means when each observation in one sample can be paired with an observation in the other sample.
This article shares several examples of how each of these types of t-tests are used in real life situations.
<h2>Examples: One Sample t-tests in Real Life</h2>
<b>Example 1: Manufacturing</b>
A manufacturing engineer wants to know if some new process leads to a significant improvement in mean battery life of some product. 
To test this, he measures the mean battery life for 50 products created using the new process and performs a one sample t-test to determine if the mean battery life is different from the mean battery life of products made using the current process.
<b>Example 2: Medicine</b>
A doctor may want to know if some new drug leads to a significant reduction in blood pressure compared to the current standard drug used.
To test this, he recruits 20 subjects to participate in a study in which they each take the new drug for one month. He can perform a one sample t-test to determine if the mean reduction in blood pressure is significantly greater than the mean reduction that results from the current standard drug.
<h2>Examples: Independent Two Sample t-tests in Real Life</h2>
<b>Example 1: Studying Techniques</b>
A professor wants to know if two studying techniques lead to different mean exam scores.
To test this, he assigns 30 students to use one studying technique and 30 students to use a different studying technique in preparation for an exam. He then has each student take the same exam. He can use an independent two sample t-test to determine if the mean is different between the two groups.
<b>Example 2: Weight Loss</b>
A dietician wants to know if two different diets lead to different mean weight loss amounts.
To test this, she assigns 20 subjects to use diet A for one month and 20 subjects to use diet B for one month. She then measures the total weight loss of each subject at the end of the month. She can use an independent two sample t-test to determine if the mean weight loss is different between the two groups.
<h2>Examples: Paired Samples t-tests in Real Life</h2>
<b>Example 1: Fuel Treatment</b>
Researchers want to know if a new fuel treatment leads to a change in the mean miles per gallon of a certain car. To test this, they conduct an experiment in which they measure the mpg of 11 cars with and without the fuel treatment.
Since each car is used in each sample, the researchers can use a paired samples t-test to determine if the mean mpg is different with and without the fuel treatment.
<b>Example 2: Plant Growth</b>
A botanist wants to know if two different soils lead to different levels of evaporation in plants.
To test this, she measures the mean amount of evaporation for 20 plants in soil A for one month. Then, she transfers each of the 20 plants to soil B and measures the mean amount of evaporation for one month.
Since each of the plants is used in both soil types, she can use a paired samples t-test to determine if the mean evaporation is different between the two soils.
<h2>Additional Resources</h2>
 One Sample t-test Calculator 
 Two Sample t-test Calculator 
 Paired Samples t-test Calculator 
<h2><span class="orange">How to Perform a t-test with Unequal Sample Sizes</span></h2>
One question students often have in statistics is:
<b><em>Is it possible to perform a t-test when the sample sizes of each group are not equal?</em></b>
The short answer:
<b>Yes, you can perform a t-test when the sample sizes are not equal. Equal sample sizes is not  one of the assumptions  made in a t-test.</b>
The real issues arise when the two samples do not have equal variances, which <em>is</em> one of the assumptions made in a t-test.
When this occurs, it’s recommended that you use  Welch’s t-test  instead, which does not make the assumption of equal variances.
The following examples demonstrate how to perform t-tests with unequal sample sizes when the variances are equal and when they’re not equal.
<h2>Example 1: Unequal Sample Sizes and Equal Variances</h2>
Suppose we administer two programs designed to help students score higher on some exam.
The results are as follows:
<b>Program 1:</b>
<b>n</b> (sample size): 500
<b>x</b> (sample mean): 80
<b>s</b> (sample standard deviation): 5
<b>Program 2:</b>
<b>n</b> (sample size): 20
<b>x</b> (sample mean): 85
<b>s</b> (sample standard deviation): 5
The following code shows how to create a boxplot in R to visualize the distribution of exam scores for each program:
<b>#make this example reproducible
set.seed(1)
#create vectors to hold exam scores
program1 &lt;- rnorm(500, mean=80, sd=5)
program2 &lt;- rnorm(20, mean=85, sd=5)
#create boxplots to visualize distribution of exam scores
boxplot(program1, program2, names=c("Program 1","Program 2"))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/unequal1.jpg"446">
The mean exam score for Program 2 appears to be higher, but the variance of exam scores between the two programs is roughly equal. 
The following code shows how to perform an independent samples t-test along with a Welch’s t-test:
<b>#perform independent samples t-test
t.test(program1, program2, var.equal=TRUE)
Two Sample t-test
data:  program1 and program2
t = -3.3348, df = 518, p-value = 0.0009148
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.111504 -1.580245
sample estimates:
mean of x mean of y 
 80.11322  83.95910
#perform Welch's two sample t-test
t.test(program1, program2, var.equal=FALSE)
Welch Two Sample t-test
data:  program1 and program2
t = -3.3735, df = 20.589, p-value = 0.00293
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.219551 -1.472199
sample estimates:
mean of x mean of y 
 80.11322  83.95910 
</b>
The independent samples t-test returns a p-value of<b> .0009</b> and Welch’s t-test returns a p-value of <b>.0029</b>.
Since the p-value of each test is less than .05, we would reject the null hypothesis in each test and conclude that there is a statistically significant difference in mean exam scores between the two programs.
<b>Even though the sample sizes are unequal, the independent samples t-test and Welch’s t-test both return similar results since the two samples had equal variances.</b>
<h2>Example 2: Unequal Sample Sizes and Unequal Variances</h2>
Suppose we administer two programs designed to help students score higher on some exam.
The results are as follows:
<b>Program 1:</b>
<b>n</b> (sample size): 500
<b>x</b> (sample mean): 80
<b>s</b> (sample standard deviation): 25
<b>Program 2:</b>
<b>n</b> (sample size): 20
<b>x</b> (sample mean): 85
<b>s</b> (sample standard deviation): 5
The following code shows how to create a boxplot in R to visualize the distribution of exam scores for each program:
<b>#make this example reproducible
set.seed(1)
#create vectors to hold exam scores
program1 &lt;- rnorm(500, mean=80, sd=25)
program2 &lt;- rnorm(20, mean=85, sd=5)
#create boxplots to visualize distribution of exam scores
boxplot(program1, program2, names=c("Program 1","Program 2"))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/unequal2.jpg"447">
The mean exam score for Program 2 appears to be higher, but the variance of exam scores for Program 1 is much higher than Program 2.
The following code shows how to perform an independent samples t-test along with a Welch’s t-test:
<b>#perform independent samples t-test
t.test(program1, program2, var.equal=TRUE)
Two Sample t-test
data:  program1 and program2
t = -0.5988, df = 518, p-value = 0.5496
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -14.52474   7.73875
sample estimates:
mean of x mean of y 
  80.5661   83.9591
#perform Welch's two sample t-test
t.test(program1, program2, var.equal=FALSE)
Welch Two Sample t-test
data:  program1 and program2
t = -2.1338, df = 74.934, p-value = 0.03613
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -6.560690 -0.225296
sample estimates:
mean of x mean of y 
  80.5661   83.9591 
</b>
The independent samples t-test returns a p-value of<b> .5496</b> and Welch’s t-test returns a p-value of <b>.0361</b>.
The independent samples t-test is not able to detect a difference in mean exam scores, but the Welch’s t-test is able to detect a statistically significant difference.
<b>Since the two samples had unequal variances, only Welch’s t-test was able to detect the statistically significant difference in mean exam scores since this test does not make the assumption of equal variances between samples.</b>
<h2>Additional Resources</h2>
The following tutorials provide additional information about t-tests:
 Introduction to the One Sample t-test 
 Introduction to the Two Sample t-test 
 Introduction to the Paired Samples t-test 
<h2><span class="orange">The Difference Between T-Values and P-Values in Statistics</span></h2>
Two terms that students often get confused in statistics are <b>t-values</b> and <b>p-values</b>.
To understand the difference between these terms, it helps to understand <b>t-tests</b>.
Broadly speaking, there are three different types of t-tests:
<b>One-sample t-test</b>: Used to test whether a population mean is equal to some value.
<b>Two-sample t-test</b>: Used to test whether two population means are equal.
<b>Paired samples t-test</b>: Used to test whether two population means are equal when each observation in one sample can be paired with an observation in the other sample.
We use the following steps to perform each test:
<b>Step 1:</b> State the null and alternative hypothesis.
<b>Step 2:</b> Calculate the t-value.
<b>Step 3:</b> Calculate the p-value that corresponds to the t-value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/tval1.png">
For each test, the <b>t-value</b> is a way to quantify the difference between the population means and the <b>p-value</b> is the probability of obtaining a t-value with an absolute value at least as large as the one we actually observed in the sample data if the null hypothesis is actually true.
If the p-value is less than a certain value (e.g. 0.05) then we reject the null hypothesis of the test.
For each type of t-test, we’re interested in the <b>p-value</b> and we simply use the <b>t-value</b> as an intermediate step to calculating the p-value.
The following example shows how to calculate and interpret a t-value and corresponding p-value for a two-sample t-test.
<h3>Example: Calculate & Interpret T-Values and P-Values</h3>
Suppose we want to know whether or not the mean weight between two different species of turtles is equal. We go out and collect a simple random sample of 12 turtles from each population with the following weights:
<b>Species #1</b>: 301, 298, 295, 297, 304, 305, 309, 298, 291, 299, 293, 304
<b>Species #2</b>: 302, 309, 324, 313, 312, 310, 305, 298, 299, 300, 289, 294
Here’s how to perform a two-sample t-test using this data:
<b>Step 1: State the null and alternative hypothesis.</b>
First, we’ll state the null and alternative hypotheses:
<b>H<sub>0</sub>: </b>μ<sub>1</sub> = μ<sub>2</sub> (the two population means are equal)
<b>H<sub>1</sub>: </b>μ<sub>1</sub> ≠ μ<sub>2</sub> (the two population means are not equal)
<b>Step 2: Calculate the t-value.</b>
Next, we’ll enter the weights of each sample of turtles into the  Two Sample t-test Calculator  and find that the t-value is <b>-1.608761</b>.
<b>Step 3: Calculate the p-value.</b>
We can also use the  Two Sample t-test Calculator  to find that the p-value that corresponds to a t-value of -1.608761 is <b>0.121926</b>.
Since this p-value is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the mean weight of turtles between the two populations are different.
Notice that we simply used the t-value as an intermediate step to calculating the p-value. The p-value is the true value that we were interested in, but we had to first calculate the t-value.
<h2><span class="orange">How to Use the Table Function in R (With Examples)</span></h2>
The <b>table()</b> function in R can be used to quickly create frequency tables.
This tutorial provides examples of how to use this function with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(player = c('AJ', 'Bob', 'Chad', 'Dan', 'Eric', 'Frank'), position = c('A', 'B', 'B', 'B', 'B', 'A'), points = c(1, 2, 2, 1, 0, 0))
#view data frame
df
  player position points
1     AJ        A      1
2    Bob        B      2
3   Chad        B      2
4    Dan        B      1
5   Eric        B      0
6  Frank        A      0</b>
<h3>Example 1: Frequency Table for One Variable</h3>
The following code shows how to create a frequency table for the <b>position</b> variable in our data frame:
<b>#calculate frequency table for <em>position</em> variable
table(df$position)
A B 
2 4
</b>
From the output we can observe:
2 players in the data frame have a position of ‘<b>A</b>‘
4 players in the data frame have a position of ‘<b>B</b>‘
<h3>Example 2: Frequency Table of Proportions for One Variable</h3>
The following code shows how to use <b>prop.table()</b> to create a frequency table of proportions for the <b>position</b> variable in our data frame:
<b>#calculate frequency table of proportions for <em>position</em> variable
prop.table(table(df$position))
        A         B 
0.3333333 0.6666667
</b>
From the output we can observe:
33.33% of players in the data frame have a position of ‘<b>A</b>‘
66.67% of players in the data frame have a position of ‘<b>B</b>‘
Note that in a proportion table the sum of the proportions will always be equal to 1.
<h3>Example 3: Frequency Table for Two Variables</h3>
The following code shows how to create a frequency table for the <b>position</b> and <b>points </b>variable in our data frame:
<b>#calculate frequency table for <em>position</em> and <em>points</em> variable
table(df$position, df$points)
    0 1 2
  A 1 1 0
  B 1 1 2
</b>
From the output we can observe:
1 player in the data frame has a position of ‘<b>A</b>‘ and <b>0</b> points
1 player in the data frame has a position of ‘<b>A</b>‘ and <b>1</b> point
0 players in the data frame have a position of ‘<b>A</b>‘ and <b>2</b> points
1 player in the data frame has a position of ‘<b>B</b>‘ and <b>0</b> points
1 player in the data frame has a position of ‘<b>B</b>‘ and <b>1</b> point
2 players in the data frame have a position of ‘<b>B</b>‘ and <b>2</b> points
<h3>Example 4: Frequency Table of Proportions for Two Variables</h3>
The following code shows how to create a frequency table of proportions for the <b>position</b> and <b>points </b>variable in our data frame:
<b>#calculate frequency table of proportions for <em>position</em> and <em>points</em> variable
prop.table(table(df$position, df$points))
            0         1         2
  A 0.1666667 0.1666667 0.0000000
  B 0.1666667 0.1666667 0.3333333
</b>
From the output we can observe:
16.67% of players in the data frame have a position of ‘<b>A</b>‘ and <b>0</b> points
16.67% of players in the data frame have a position of ‘<b>A</b>‘ and <b>1</b> point
0% of players in the data frame have a position of ‘<b>A</b>‘ and <b>2</b> points
16.67% of players in the data frame have a position of ‘<b>B</b>‘ and <b>0</b> points
16.67% of players in the data frame have a position of ‘<b>B</b>‘ and <b>1</b> point
33.3% of players in the data frame have a position of ‘<b>B</b>‘ and <b>2</b> points
Note that we can also use the <b>options()</b> function to specify how many decimals to show in the proportion table:
<b>#only display two decimal places
options(digits=2)
#calculate frequency table of proportions for <em>position</em> and <em>points</em> variable
prop.table(table(df$position, df$points))
       0    1    2
  A 0.17 0.17 0.00
  B 0.17 0.17 0.33</b>
<h2><span class="orange">What is Tabular Data? (Definition & Example)</span></h2>
In statistics, <b>tabular data</b> refers to data that is organized in a table with rows and columns.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/tabular2.jpg">
Within the table, the <b>rows</b> represent observations and the <b>columns</b> represent attributes for those observations.
For example, the following table represents tabular data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/tabular1.jpg">
This dataset has <b>9</b> rows and <b>5</b> columns.
Each row represents one basketball player and the five columns describe different attributes about the player including:
Player name
Minutes played
Point
Rebounds
Assists
The opposite of <b>tabular data</b> would be <b>visual data</b>, which would be some type of plot or chart that helps us <em>visualize</em> the values in a dataset.
For example, we might have the following bar chart that helps us visualize the total minutes played by each player in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/tabular3.jpg">
This would be an example of <b>visual data</b>.
It contains the exact same information about player names and minutes played for the players in the dataset, but it’s simply displayed in a visual form instead of a tabular form.
Or we might have the following scatterplot that helps us visualize the relationship between minutes played and points scored for each player:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/tabular4.jpg"504">
This is another example of<b> visual data</b>.
<h2>When is Tabular Data Used in Practice?</h2>
In practice, tabular data is the most common type of data that you’ll run across in the real world.
In the real world, most data that is saved in an Excel spreadsheet is considered tabular data because the rows represent observations and the columns represent attributes for those observations.
For example, here’s what our basketball dataset from earlier might look like in an Excel spreadsheet:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/tabular5.jpg"537">
This format is one of the most natural ways to collect and store values in a dataset, which is why it’s used so often.
<h2>Additional Resources</h2>
The following tutorials explain other common terms in statistics:
 Why is Statistics Important? 
 Why is Sample Size Important in Statistics? 
 What is an Observation in Statistics? 
 What is Considered Raw Data in Statistics? 
<h2><span class="orange">How to Use tabulate() Function in R to Count Integer Occurrences</span></h2>
The <b>tabulate()</b> function in R can be used to count the occurrences of integer values in a vector.
This function uses the following basic syntax:
<b>tabulate(bin, nbins=max(1, bin, na.rm=TRUE))</b>
where:
<b>bin</b>: Name of the vector
<b>nbins</b>: The number of bins to be used
The following examples show how to use this function in practice.
<h3>Example 1: Count Integer Occurrences in Vector</h3>
The following code shows how to use the <b>tabulate()</b> function to count the occurrences of integers in a given vector:
<b>#create vector of data values
data &lt;- c(1, 1, 1, 2, 3, 3, 3, 4, 7, 8)
#count occurrences of integers in vector
tabulate(data)
[1] 3 1 3 1 0 0 1 1
</b>
By default, the <b>tabulate()</b> function uses 1 as the minimum integer value and displays the occurrences of each successive integer in the vector.
For example:
The integer 1 occurs <b>3</b> times in the vector.
The integer 2 occurs <b>1</b> time in the vector.
The integer 3 occurs <b>3</b> times in the vector.
The integer 4 occurs <b>1</b> time in the vector.
The integer 5 occurs <b>0</b> times in the vector.
And so on.
Note that if you use the <b>nbins</b> argument, you simply limit the number of integers that the <b>tabulate()</b> function counts:
<b>#count occurrences of integers but limit output to 5
tabulate(data, nbins=5)
[1] 3 1 3 1 0</b>
<h3>
<b>Example 2: Count Integer Occurrences in Vector with Decimals</b>
</h3>
If we use the <b>tabulate()</b> function with a vector that contains decimals, the function will simply tell us how often each integer value occurs:
<b>#create vector of data values with decimals
data &lt;- c(1.2, 1.4, 1.7, 2, 3.1, 3.5)
#count occurrences of integers
tabulate(data)
[1] 3 1 2</b>
From the output we can see:
The integer value 1 occurred <b>3</b> times.
The integer value 2 occurred <b>1</b> time.
The integer value 3 occurred <b>2</b> times.
<h3>
<b>Example 3: Count Integer Occurrences in Vector with Negative Values</b>
</h3>
If we use the <b>tabulate()</b> function with a vector that contains negative values or zeros, the function will simply ignore the negative values and the zeros:
<b>#create vector with some negative values and zeros
data &lt;- c(-5, -5, -2, 0, 1, 1, 2, 4)
#count occurrences of integers
tabulate(data)
[1] 2 1 0 1</b>
From the output we can see:
The integer value 1 occurred <b>2</b> times.
The integer value 2 occurred <b>1</b> time.
The integer value 3 occurred <b>0</b> times.
The integer value 4 occurred <b>1</b> time.
<h3>An Alternative to Tabulate: The table() Function</h3>
If you’d like to count the occurrence of every value in a vector, it’s better to use the <b>table()</b> function:
<b>#create vector with a variety of numbers
data &lt;- c(-5, -5, -2, 0, 1, 1, 2.5, 4)
#count occurrences of each unique value in vector
table(data)
data
 -5  -2   0   1 2.5   4 
  2   1   1   2   1   1 </b>
From the output we can see:
The value -5 occurred <b>2</b> times.
The value -2 occurred <b>1</b> time.
The value 0 occurred <b>1</b> time.
The value 1 occurred <b>2</b> times.
The value 2.5 occurred <b>1</b> time.
The value 4 occurred <b>1</b> time.
Notice that the <b>table()</b> function counts the occurrence of every value, not just the integer values.
<h2><span class="orange">How to Use the tapply() Function in R (With Examples)</span></h2>
The <b>tapply()</b> function in R can be used to apply some function to a vector, grouped by another vector.
This function uses the following basic syntax:
<b>tapply(X, INDEX, FUN, ..)</b>
where:
<b>X</b>: A vector to apply a function to
<b>INDEX</b>: A vector to group by
<b>FUN</b>: The function to apply
The following examples show how to use this function in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'), position=c('G', 'G', 'F', 'F', 'G', 'G', 'F', 'F'), points=c(14, 19, 13, 8, 15, 15, 17, 19), assists=c(4, 3, 3, 5, 9, 14, 15, 12))
#view data frame
df
  team position points assists
1    A        G     14       4
2    A        G     19       3
3    A        F     13       3
4    A        F      8       5
5    B        G     15       9
6    B        G     15      14
7    B        F     17      15
8    B        F     19      12</b>
<h2>Example 1: Apply Function to One Variable, Grouped by One Variable</h2>
The following code shows how to use the <b>tapply()</b> function to calculate the mean value of <b>points</b>, grouped by <b>team</b>:
<b>#calculate mean of points, grouped by team
tapply(df$points, df$team, mean)
   A    B 
13.5 16.5</b>
From the output we can see:
The mean value of points for team A is <b>13.5</b>.
The mean value of points for team B is <b>16.5</b>.
Note that you can also include additional arguments after the function, such as<b> na.rm</b>, to indicate that you wish to calculate the mean while ignoring NA values in the data frame:
<b>#calculate mean of points, grouped by team
tapply(df$points, df$team, mean, na.rm=TRUE)
   A    B 
13.5 16.5</b>
<h2>Example 2: Apply Function to One Variable, Grouped by Multiple Variables</h2>
The following code shows how to use the <b>tapply()</b> function to calculate the mean value of <b>points</b>, grouped by <b>team</b> and <b>position</b>:
<b>#calculate mean of points, grouped by team and position
tapply(df$points, list(df$team, df$position), mean, na.rm=TRUE)
     F    G
A 10.5 16.5
B 18.0 15.0
</b>
From the output we can see:
The mean value of points for team A and position F is <b>10.5</b>.
The mean value of points for team A and position G is <b>16.5</b>.
The mean value of points for team B and position F is <b>18.0</b>.
The mean value of points for team B and position G is <b>15.0</b>.
<b>Note</b>: In this example we grouped by two variables, but we can include as many variables as we’d like in the <b>list()</b> function to group by even more variables.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use the dim() Function in R 
 How to Use the table() Function in R 
 How to Use sign() Function in R 
<h2><span class="orange">How to Fit a TBATS Model in R (With Example)</span></h2>
One popular time series forecasting method is known as <b>TBATS</b>, which is an acronym for:
<b>T</b>rigonometric seasonality
<b>B</b>ox-Cox transformation
<b>A</b>RMA errors
<b>T</b>rend
<b>S</b>easonal components.
This method fits a variety of models both with and without:
Seasonality
A Box-Cox transformation
ARMA(p, q) process
Various trends
Various seasonal effects
This method will choose the model with the lowest value for the  Akaike Information Criterion  (AIC) value as the final model.
The easiest way to fit a TBATS model to a time series dataset in R is to use the <b>tbats</b> function from the <b>forecast</b> package.
The following example shows how to use this function in practice.
<h3>Example: How to Fit a TBATS Model in R</h3>
For this example, we’ll use the built-in R dataset called <b>USAccDeaths</b>, which contains values for the total monthly accidental deaths in the USA from 1973 to 1978:
<b>#view USAccDeaths dataset
USAccDeaths
       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
1973  9007  8106  8928  9137 10017 10826 11317 10744  9713  9938  9161  8927
1974  7750  6981  8038  8422  8714  9512 10120  9823  8743  9129  8710  8680
1975  8162  7306  8124  7870  9387  9556 10093  9620  8285  8466  8160  8034
1976  7717  7461  7767  7925  8623  8945 10078  9179  8037  8488  7874  8647
1977  7792  6957  7726  8106  8890  9299 10625  9302  8314  8850  8265  8796
1978  7836  6892  7791  8192  9115  9434 10484  9827  9110  9070  8633  9240
</b>
We can use the following code to fit a TBATS model to this dataset and make predictions for the values of future months:
<b>library(forecast)
#fit TBATS model
fit &lt;- tbats(USAccDeaths)
#use model to make predictions
predict &lt;- predict(fit)
#view predictions      
predict
         Point Forecast     Lo 80     Hi 80    Lo 95     Hi 95
Jan 1979       8307.597  7982.943  8632.251 7811.081  8804.113
Feb 1979       7533.680  7165.539  7901.822 6970.656  8096.704
Mar 1979       8305.196  7882.740  8727.651 7659.106  8951.286
Apr 1979       8616.921  8150.753  9083.089 7903.978  9329.864
May 1979       9430.088  8924.028  9936.147 8656.137 10204.038
Jun 1979       9946.448  9403.364 10489.532 9115.873 10777.023
Jul 1979      10744.690 10167.936 11321.445 9862.621 11626.760
Aug 1979      10108.781  9499.282 10718.280 9176.632 11040.929
Sep 1979       9034.784  8395.710  9673.857 8057.405 10012.162
Oct 1979       9336.862  8668.087 10005.636 8314.060 10359.664
Nov 1979       8819.681  8124.604  9514.759 7756.652  9882.711
Dec 1979       9099.344  8376.864  9821.824 7994.407 10204.282
Jan 1980       8307.597  7563.245  9051.950 7169.208  9445.986
Feb 1980       7533.680  6769.358  8298.002 6364.750  8702.610
Mar 1980       8305.196  7513.281  9097.111 7094.067  9516.325
Apr 1980       8616.921  7800.849  9432.993 7368.847  9864.995
May 1980       9430.088  8590.590 10269.585 8146.187 10713.988
Jun 1980       9946.448  9084.125 10808.771 8627.639 11265.257
Jul 1980      10744.690  9860.776 11628.605 9392.859 12096.522
Aug 1980      10108.781  9203.160 11014.402 8723.753 11493.809
Sep 1980       9034.784  8109.000  9960.567 7618.920 10450.647
Oct 1980       9336.862  8390.331 10283.392 7889.269 10784.455
Nov 1980       8819.681  7854.387  9784.976 7343.391 10295.972
Dec 1980       9099.344  8114.135 10084.554 7592.597 10606.092
</b>
The output shows the forecasted number of deaths for upcoming months along with the 80% and 95% confidence intervals.
For example, we can see the following predictions for January 1979:
Predicted number of deaths: <b>8,307.597</b>
80% Confidence Interval for number of deaths: <b>[7,982.943, 8,632.251]</b>
95% Confidence Interval for number of deaths: <b>[7,811.081, 8,804.113]</b>
We can also use the <b>plot()</b> function to plot these predicted future values:
<b>#plot the predicted values
plot(forecast(fit))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/tbats1.jpg"443">
The blue line represents the future predicted values and the grey bands represent the confidence interval limits.
<h2><span class="orange">How to Test for Normality in R (4 Methods)</span></h2>
Many statistical tests make  the assumption  that datasets are normally distributed. 
There are four common ways to check this assumption in R:
<b>1. (Visual Method) Create a histogram.</b>
If the histogram is roughly “bell-shaped”, then the data is assumed to be normally distributed.
<b>2. (Visual Method) Create a Q-Q plot.</b>
If the points in the plot roughly fall along a straight diagonal line, then the data is assumed to be normally distributed.
<b>3. (Formal Statistical Test) Perform a Shapiro-Wilk Test.</b>
If the p-value of the test is greater than α = .05, then the data is assumed to be normally distributed.
<b>4. (Formal Statistical Test) Perform a Kolmogorov-Smirnov Test.</b>
If the p-value of the test is greater than α = .05, then the data is assumed to be normally distributed.
The following examples show how to use each of these methods in practice.
<h3>Method 1: Create a Histogram</h3>
The following code shows how to create a histogram for a normally distributed and non-normally distributed dataset in R:
<b>#make this example reproducible
set.seed(0)
#create data that follows a normal distribution
normal_data &lt;- rnorm(200)
#create data that follows an exponential distribution
non_normal_data &lt;- rexp(200, rate=3)
#define plotting region
par(mfrow=c(1,2)) 
#create histogram for both datasets
hist(normal_data, col='steelblue', main='Normal')
hist(non_normal_data, col='steelblue', main='Non-normal')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/normR1.png">
The histogram on the left exhibits a dataset that is normally distributed (roughly a “bell-shape”) and the one on the right exhibits a dataset that is not normally distributed.
<h3>Method 2: Create a Q-Q plot</h3>
The following code shows how to create a Q-Q plot for a normally distributed and non-normally distributed dataset in R:
<b>#make this example reproducible
set.seed(0)
#create data that follows a normal distribution
normal_data &lt;- rnorm(200)
#create data that follows an exponential distribution
non_normal_data &lt;- rexp(200, rate=3)
#define plotting region
par(mfrow=c(1,2)) 
#create Q-Q plot for both datasets
qqnorm(normal_data, main='Normal')
qqline(normal_data)
qqnorm(non_normal_data, main='Non-normal')
qqline(non_normal_data)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/normR2.png">
The Q-Q plot on the left exhibits a dataset that is normally distributed (the points fall along a straight diagonal line) and the Q-Q plot on the right exhibits a dataset that is not normally distributed.
<h3>Method 3: Perform a Shapiro-Wilk Test</h3>
The following code shows how to perform a Shapiro-Wilk test on a normally distributed and non-normally distributed dataset in R:
<b>#make this example reproducible
set.seed(0)
#create data that follows a normal distribution
normal_data &lt;- rnorm(200)
#perform shapiro-wilk test
shapiro.test(normal_data)
Shapiro-Wilk normality test
data:  normal_data
W = 0.99248, p-value = 0.3952
#create data that follows an exponential distribution
non_normal_data &lt;- rexp(200, rate=3)
#perform shapiro-wilk test
shapiro.test(non_normal_data)
Shapiro-Wilk normality test
data:  non_normal_data
W = 0.84153, p-value = 1.698e-13</b>
The p-value of the first test is not less than .05, which indicates that the data is normally distributed.
The p-value of the second test <em>is</em> less than .05, which indicates that the data is not normally distributed.
<h3>Method 4: Perform a Kolmogorov-Smirnov Test</h3>
The following code shows how to perform a Kolmogorov-Smirnov test on a normally distributed and non-normally distributed dataset in R:
<b>#make this example reproducible
set.seed(0)
#create data that follows a normal distribution
normal_data &lt;- rnorm(200)
#perform kolmogorov-smirnov test
ks.test(normal_data, 'pnorm')
One-sample Kolmogorov-Smirnov test
data:  normal_data
D = 0.073535, p-value = 0.2296
alternative hypothesis: two-sided
#create data that follows an exponential distribution
non_normal_data &lt;- rexp(200, rate=3)
#perform kolmogorov-smirnov test
ks.test(non_normal_data, 'pnorm') 
One-sample Kolmogorov-Smirnov test
data:  non_normal_data
D = 0.50115, p-value &lt; 2.2e-16
alternative hypothesis: two-sided</b>
The p-value of the first test is not less than .05, which indicates that the data is normally distributed.
The p-value of the second test <em>is</em> less than .05, which indicates that the data is not normally distributed.
<h3>How to Handle Non-Normal Data</h3>
If a given dataset is <em>not</em> normally distributed, we can often perform one of the following transformations to make it more normally distributed:
<b>1. Log Transformation: </b>Transform the values from x to <b>log(x)</b>.
<b>2. Square Root Transformation: </b>Transform the values from x to <b>√x</b>.
<b>3. Cube Root Transformation: </b>Transform the values from x to <b>x<sup>1/3</sup></b>.
By performing these transformations, the dataset typically becomes more normally distributed.
Read  this tutorial  to see how to perform these transformations in R.
<h2><span class="orange">What is Test-Retest Reliability? (Definition & Example)</span></h2>
Often researchers want to use some type of test to measure a construct like intelligence, aptitude, educational ability, etc. in individuals of some population.
When administering any type of test, it’s important that the test has <b>reliability</b>. In other words, it’s important that the results of a test can be reproduced under the same conditions at two different points in time.
<b>Test-retest reliability</b> is a specific way to measure reliability of a test and it refers to the extent that a test produces similar results over time.
We calculate the test-retest reliability by using the  Pearson Correlation Coefficient , which takes on a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two scores
0 indicates no linear correlation between two scores
1 indicates a perfectly positive linear correlation between two scores
For example, we may give an IQ test to 50 participants on January 1st and then give the same type of IQ test of similar difficulty to the same group of 50 participants one month later.
We could calculate the correlation of scores between the two tests to determine if the test has good test-retest reliability.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/test_retest1.png">
Generally a test-retest reliability correlation of at least<b> 0.80 or higher</b> indicates good reliability.
<h3>Example: Calculating Test-Retest Reliability</h3>
Suppose researchers give a test to 20 individuals and then give the same type of test one month later to the same 20 individuals.
Their scores are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/test_retest2.png">
We can use a  correlation calculator  to find that the Pearson Correlation Coefficient between the two sets of scores is <b>0.836</b>.
Since this correlation is greater than 0.80, researchers could conclude that the test has good test-retest reliability.
In other words, the test produces reliable results that can be replicated at different points in time.
<h3>Potential Bias in Test-Retest Reliability</h3>
Test-retest reliability is a useful metric to calculate, but be aware of the following potential biases that could affect this metric:
<b>1. Practice Effect</b>
A practice effect occurs when participants simply gets better at some test due to practice. This means they’re likely to show better results during later tests because they’ve had time to practice and improve.
The way to prevent this type of bias is to give individuals tests that are of equal difficulty but have a different variety of questions so that they can’t memorize the answers to the types of questions asked on the first test.
<b>2. Fatigue Effect</b>
A fatigue effect occurs when participants gets worse at some test because they get mentally drained or fatigued from taking previous tests.
The way to prevent this type of bias is to provide plenty of time in between tests (ideally weeks or even months) so that participants are fresh when taking both tests.
<b>3. Differences in Conditions</b>
When participants take the two tests under different conditions (i.e. different lighting, different time of day, different time allowed to complete the test, etc.) it’s possible that they score differently on the tests simply due to differences in the testing environment. 
The way to prevent this type of bias is to ensure that participants take both tests under identical conditions, i.e. during the same time of day, with the same general lighting and environment, and given the same amount of time to complete the test.
<h2><span class="orange">How to Test the Significance of a Regression Slope</span></h2>
Suppose we have the following dataset that shows the square feet and price of 12 different houses:
  
We want to know if there is a significant relationship between square feet and price.
To get an idea of what the data looks like, we first create  a scatterplot  with <em>square feet </em>on the x-axis and <em>price </em>on the y-axis:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/regression3.jpg"> 
We can clearly see that there is a positive correlation between square feet and price. As square feet increases, the price of the house tends to increase as well.
However, to know if there is a <b>statistically significant</b> <b>relationship</b> between square feet and price, we need to run a simple linear regression.
So, we run a  simple linear regression  using <em>square feet </em>as the predictor and <em>price </em>as the response and get the following output:
  
<em>Whether you run a simple linear regression in Excel, SPSS, R, or some other software, you will get a similar output to the one shown above.</em>
Recall that a simple linear regression will produce the line of best fit, which is the equation for the line that best “fits” the data on our scatterplot. This line of best fit is defined as:
 <b><U+0177> = b<sub>0</sub> + b<sub>1</sub>x </b>
where <U+0177> is the predicted value of the response variable, b<sub>0</sub> is the y-intercept, b<sub>1</sub> is the regression coefficient, and x is the value of the predictor variable.
The value for b<sub>0</sub> is given by the coefficient for the intercept, which is <b>47588.70.</b>
The value for b<sub>1</sub> is given by the coefficient for the predictor variable <em>Square Feet</em>, which is <b>93.57.</b>
Thus, the line of best fit in this example is <b><U+0177> = 47588.70+ 93.57x</b>
Here is how to interpret this line of best fit:
<b>b<sub>0</sub>: </b>When the value for square feet is zero, the average expected value for price is $47,588.70. (In this case, it doesn’t really make sense to interpret the intercept, since a house can never have zero square feet)
<b>b<sub>1</sub>: </b>For each additional square foot, the average expected increase in price is $93.57. 
So, now we know that for each additional square foot, the average expected increase in price is $93.57. 
To find out if this increase is statistically significant, we need to conduct a hypothesis test for B<sub>1</sub> or construct a confidence interval for B<sub>1</sub>.
<b>Note</b>: A hypothesis test and a confidence interval will always give the same results.
<h2>Constructing a Confidence Interval for a Regression Slope</h2>
To construct a confidence interval for a regression slope, we use the following formula:
Confidence Interval = b<sub>1</sub>  +/-  (t<sub>1-∝/2, n-2</sub>) * (standard error of b<sub>1</sub>)
where:
 b<sub>1</sub> is the slope coefficient given in the regression output
(t<sub>1-∝/2, n-2</sub>) is the t critical value for confidence level 1-∝ with n-2 degrees of freedom where <em>n </em>is the total number of observations in our dataset
(standard error of b<sub>1</sub>) is the standard error of b<sub>1</sub> given in the regression output
For our example, here is how to construct a 95% confidence interval for B<sub>1</sub>:
b<sub>1</sub> is 93.57 from the regression output.
Since we are using a 95% confidence interval, ∝ = .05 and n-2 = 12-2 = 10, thus t<sub>.975, 10</sub> is 2.228 according to the  t-distribution table 
(standard error of b<sub>1</sub>) is 11.45 from the regression output
Thus, our 95% confidence interval for B<sub>1 </sub>is:
93.57  +/-  (2.228) * (11.45) = <b>(68.06 , 119.08)</b>
This means we are 95% confident that the true average increase in price for each additional square foot is between $68.06 and $119.08.
Notice that $0 is not in this interval, so the relationship between square feet and price is statistically significant at the 95% confidence level.
<h2>Conducting a Hypothesis Test for a Regression Slope</h2>
To conduct a hypothesis test for a regression slope, we follow the  standard five steps for any hypothesis test :
<b>Step 1. State the hypotheses. </b>
The null hypothesis (H0): B<sub>1</sub> = 0
The alternative hypothesis: (Ha): B<sub>1</sub> ≠ 0
<b>Step 2. Determine a significance level to use.</b>
Since we constructed a 95% confidence interval in the previous example, we will use the equivalent approach here and  choose to use a .05 level of significance.
<b>Step 3. Find the test statistic and the corresponding p-value.</b>
In this case, the test statistic is <em>t </em>=  coefficient  of b<sub>1</sub> / standard error of b<sub>1</sub> with n-2 degrees of freedom.  We can find these values from the regression output:
  
Thus, test statistic <em>t </em>= 92.89 / 13.88 = 6.69. 
Using the  T Score to P Value Calculator  with a t score of 6.69 with 10 degrees of freedom and a two-tailed test, the p-value = <b>0.000</b>.
<b>Step 4. Reject or fail to reject the null hypothesis.</b>
Since the p-value is less than our significance level of .05, we reject the null hypothesis.
<b>Step 5. Interpret the results. </b>
Since we rejected the null hypothesis, we have sufficient evidence to say that the true average increase in price for each additional square foot is not zero.
<h2><span class="orange">What is Tetrachoric Correlation?</span></h2>
<b>Tetrachoric correlation</b> is a measure of the correlation between two binary variables – that is, variables that can only take on two values like “yes” and “no” or “good” and “bad.”
This type of correlation is often used in surveys and personality tests in which the questions being asked only have two possible response values.
The value for a tetrachoric correlation can range from -1 to 1 where:
<b>-1</b> indicates a strong negative correlation between the two variables.
<b>0</b> indicates no correlation between the two variables.
<b>1</b> indicates a strong positive correlation between the two variables.
<em><b>Note:</b> For this correlation to be reliable, it’s assumed that both variables come from a  normal distribution .</em>
<h3>How to Calculate Tetrachoric Correlation</h3>
Suppose we have the following 2×2 table with two variables, x and y, that both take on two values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/tetra0.png">
The formula to calculate the tetrachoric correlation between the two variables in this table is:
<b>Tetrachoric correlation = COS(π/(1+√(ad/b/c)))</b>
where:
<b>COS</b> represents the cosine function
<b>π</b> represents the numerical value Pi, equal to 3.141592…
<b>a, b, c, d</b> represent the numerical values in the cells of the 2×2 table
<b>Note:</b> If you have two  ordinal variables  (that can take on more than just two values) then you can instead calculate the  polychoric correlation .
<h3>Example: Calculating Tetrachoric Correlation</h3>
Suppose we want to know whether or not gender is associated with political party preference so we take a  simple random sample  of 100 voters and survey them on their political party preference.
The following table shows the results of the survey:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/tetra2.png">
We would calculate the tetrachoric correlation between these two variables as:
Tetrachoric correlation = COS(π/(1+√(19*39/30/12))) = <b>0.277</b>.
This correlation is fairly low, which indicates that there is a weak association between gender and political party preference.
<h2><span class="orange">How to Use text() Function in R to Add Text to Plot</span></h2>
You can use the <b>text()</b> function to add text to a plot in base R.
This function uses the following basic syntax:
<b>text(x, y, “my text”)</b>
where:
<b>x, y</b>: The (x, y) coordinates where the text should be placed.
The following examples show how to use this function in practice.
<h3>Example 1: Add One Text Element to Plot</h3>
The following code shows how to use <b>text()</b> to add one text element to a plot at (x, y) coordinates of (5, 18):
<b>#create data frame with values to plot
df &lt;- data.frame(x=c(1, 4, 7, 8, 8, 10), y=c(4, 9, 16, 14, 12, 20))
#create scatterplot
plot(df$x, df$y)
#add text element at (5, 18)
text(x=5, y=18, "this is my text")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/textr1.jpg"449">
Notice that our text element has been added to the (x, y) coordinates of (5, 18) in the plot.
<h3>Example 2: Add Multiple Text Elements to Plot</h3>
To add multiple text elements to a plot, we can simply use multiple <b>text()</b> functions:
<b>#create data frame with values to plot
df &lt;- data.frame(x=c(1, 4, 7, 8, 8, 10), y=c(4, 9, 16, 14, 12, 20))
#create scatterplot
plot(df$x, df$y)
#add text elements
text(x=5, y=18, "first text")
text(x=5, y=10, "second text") 
text(x=5, y=5, "third text") 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/textr2.jpg">
Notice that three text elements have been added to the plot, each at the (x, y) coordinates that we specified.
<h3>Example 3: Customize Text Elements in Plot</h3>
We can use the <b>cex</b>, <b>col</b>, and <b>font</b> arguments to customize the size, color, and font style of the text elements in the plot, respectively:
<b>#create data frame with values to plot
df &lt;- data.frame(x=c(1, 4, 7, 8, 8, 10), y=c(4, 9, 16, 14, 12, 20))
#create scatterplot
plot(df$x, df$y)
#add text elements with custom appearance
text(x=5, y=18, "first text", col='red')
text(x=5, y=10, "second text", col='blue', cex=3) 
text(x=5, y=5, "third text", col='green', cex=5, font=3) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/textr3.jpg">
Notice that each of the three text elements have a custom appearance.
Also note that the <b>font</b> argument takes four possible values:
<b>1</b>: plain
<b>2</b>: bold
<b>3</b>: italic
<b>4</b>: bold-italic
Since we specified <b>font=3</b> for our third text element, the font is italic.
<h3>Example 4: Add Text Labels to Each Point in Plot</h3>
We can use the <b>labels </b>arguments to add a text label to each point in the plot:
<b>#create data frame with values to plot
df &lt;- data.frame(teams=c('A', 'B', 'C', 'D', 'E', 'F'), x=c(1, 4, 7, 8, 8, 10), y=c(4, 9, 16, 14, 12, 20))
#create scatterplot
plot(df$x, df$y)
#add text label to each point in plot
text(df$x, df$y, labels=df$teams, pos=4)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/textr4.jpg">
Notice that each of the points in the plot now have a text label.
Also note that the <b>pos </b>argument controls where the text label is placed relative to the point and takes four possible values:
<b>1</b>: below text
<b>2</b>: left of text
<b>3</b>: above text
<b>4</b>: right of text
Since we specified <b>pos=4</b> each text label is placed to the right of the points in the plot.
<h2><span class="orange">The Normal Distribution</span></h2>
The <b>normal distribution </b>is the most common probability distribution in statistics.
  
Normal distributions have the following features:
Bell shape
 Symmetrical 
Mean and median are equal; both are located at the center of the distribution
About 68% of data falls within one standard deviation of the mean
About 95% of data falls within two standard deviations of the mean
About 99.7% of data falls within three standard deviations of the mean
The last three bullet points are known as the <b>Empirical Rule</b>, sometimes called the <b>68-95-99.7 rule</b>.
<b>Related:  </b> Empirical Rule (Practice Problems) 
<h3>How to Draw a Normal Curve</h3>
To draw a normal curve, we need to know the mean and the standard deviation.
<b>Example 1:</b> <em><b>Suppose the height of males at a certain school is normally distributed with mean of <span><span><span>μ<span>=70 inches and a standard deviation of <span><span><span>σ = 2 inches. Sketch the normal curve.</b></em>
<b>Step 1:</b> Sketch a normal curve.
<b>Step 2:</b> The mean of 70 inches goes in the middle.
<b>Step 3:</b> Each standard deviation is a distance of 2 inches.  
  
<b>Example 2:</b> <em><b>Suppose the weight of a certain species of otters is normally distributed with mean of <span><span><span>μ<span>=30 lbs  and a standard deviation of <span><span><span>σ = 5 lbs. Sketch the normal curve.</b></em>
<b>Step 1:</b> Sketch a normal curve.
<b>Step 2:</b> The mean of 30 lbs goes in the middle.
<b>Step 3:</b> Each standard deviation is a distance of 5 lbs
<h3>  
<b>How to Find Percentages Using the Normal Distribution</h3>
The <b>empirical rule</b>, sometimes called the <b>68-95-99.7 rule</b>, says that for a random variable that is normally distributed, 68% of data falls within one standard deviation of the mean, 95% falls within two standard deviations of the mean, and 99.7% falls within three standard deviations of the mean.
  
Using this rule, we can answer questions about percentages.
<b>Example:</b> <em><b>Suppose the height of males at a certain school is normally distributed with mean of <span><span><span>μ<span>=70 inches and a standard deviation of <span><span><span>σ = 2 inches.</b></em>
<em><b><span><span><span>Approximately what percentage of males at this school are taller than 74 inches?</b></em>
<b>Solution:</b>
<b>Step 1:</b> Sketch a normal distribution with a mean of <span><span><span>μ<span>=70 inches and a standard deviation of <span><span><span>σ = 2 inches.
<b>Step 2: </b>A height of 74 inches is two standard deviations above the mean. Add the percentages above that point in the normal distribution.
  
2.35% + 0.15% = <b>2.5%</b>
Approximately <b>2.5%</b> of males at this school are taller than 74 inches.
<em><b><span><span><span>Approximately what percentage of males at this school are between 68 inches and 72 inches tall?</b></em>
<b>Solution:</b>
<b>Step 1:</b> Sketch a normal distribution with a mean of <span><span><span>μ<span>=70 inches and a standard deviation of <span><span><span>σ = 2 inches.
<b>Step 2: </b>A height of 68 inches and 72 inches is one standard deviation below and above the mean, respectively. Simply add the percentages between these two points in the normal distribution.
  
34% + 34% = <b>68%</b>
Approximately <b>68%</b> of males at this school are between 68 inches and 72 inches tall.
<h3>How to Find Counts Using the Normal Distribution</h3>
We can also use the empirical rule to answer questions about counts.
Example: <em><b>Suppose the weight of a certain species of otters is normally distributed with mean of <span><span><span>μ<span>=30 lbs  and a standard deviation of <span><span><span>σ = 5 lbs. </b></em>
<em><b>A certain colony has 200 of these otters. Approximately how many of these otters weigh more than 35 lbs?</b></em>
<b>Solution:</b>
<b>Step 1:</b> Sketch a normal distribution with a mean of <span><span><span>μ<span>=30 lbs and a standard deviation of <span><span><span>σ = 5 lbs.
<b>Step 2: </b>A weight of 35 lbs is one standard deviation above the mean. Add the percentages above that point in the normal distribution.
  
13.5% + 2.35% + 0.15% = <b>16%</b>
<b>Step 3:</b> Since there are 200 otters in the colony, 16% of 200 = 0.16 * 200 = <b>32</b>
Approximately 32 of the otters in this colony weight more than 35 lbs.
<em><b>Approximately how many of the otters in this colony weigh less than 30 lbs?</b></em>
Instead of walking through all of the steps we just did above, we can recognize that the median of a normal distribution is equal to the mean, which is 30 lbs in this case.
This means that half of the otters weight more than 30 lbs and half weight less than 30 lbs. This means that 50% of the 200 otters weight less than 30 lbs, so 0.5 * 200 = <b>100 otters</b>.
<h2><span class="orange">Theoretical Probability: Definition + Examples</span></h2>
Probability is a topic in statistics that describes the likelihood of certain events happening. When we talk about probability, we’re often referring to one of two types:
<b>1. Theoretical probability</b>
Theoretical probability is the likelihood that an event will happen based on pure mathematics. The formula to calculate the theoretical probability of event <em>A </em>happening is:
P(<em>A</em>) = number of desired outcomes / total number of possible outcomes
For example, the theoretical probability that a dice lands on “2” after one roll can be calculated as:
P(<em>land on 2</em>) = (only one way the dice can land on 2) / (six possible sides the dice can land on) = <b>1/6</b>
<b>2. Experimental probability</b>
Experimental probability is the actual probability of an event occurring that you directly observe in an experiment. The formula to calculate the experimental probability of event <em>A </em>happening is:
P(<em>A</em>) = number of times event occurs / total number of trials
For example, suppose we roll a dice 11 times and it lands on a “2” three times. The experimental probability for the dice landing on “2” can be calculated as:
P(<em>land on 2</em>) = (lands on 2 three times) / (rolled the dice 11 times) = <b>3/11</b>
<h2>How to Remember the Difference</h2>
You can remember the difference between theoretical probability and experimental probability using the following trick:
The theoretical probability of an event occurring can be calculated <b>in theory</b> using math.
The experimental probability of an event occurring can be calculated by directly observing the results of <b>an experiment</b>.
<h2>The Benefit of Using Theoretical Probability</h2>
Statisticians often like to calculate the theoretical probability of events because it’s much easier and faster to calculate compared to actually conducting an experiment.
For example, suppose it’s known that 1 out of every 30 students at a particular school will need additional help with their math homework after school. Instead of waiting to see how many students show up for homework help after school, a school administrator could instead calculate the total number of students at the school (suppose it’s 300) and multiply by the theoretical probability (1/30) to know that he will likely need 10 people present to help each of the students one-on-one.
<h2>Examples of Theoretical Probability</h2>
Experimental probabilities are usually easier to calculate than theoretical probabilities because it just involves counting the number of times that a certain event actually occurred relative to the total number of trials.
Conversely, theoretical probabilities can be trickier to calculate. So, here are several examples of how to calculate theoretical probabilities to help you master the topic.
<h3>Example 1</h3>
A bag contains the following:
3 red balls
4 green balls
2 purple balls
<b>Question: </b>If you close your eyes and randomly pull out one ball, what is the probability that it will be green?
<b>Answer: </b>We can use the following formula to calculate the theoretical probability of pulling out a green ball:
P(<em>green</em>) = (4 green balls) / (9 total balls) = <b>4/9</b>
<h3>Example 2</h3>
You own a 9-sided dice that contains the numbers 1 through 9 on the sides.
<b>Question: </b>What is the probability that the dice lands on “7” if you were to roll it one time?
<b>Answer: </b>We can use the following formula to calculate the theoretical probability that the dice lands on 7:
P(<em>lands on 7</em>) = (only one way the dice can land on 7) / (9 possible sides) = <b>1/9</b>
<h3>Example 3</h3>
A bag contains the name of 3 boys and 7 seven girls.
<b>Question: </b>If you close your eyes and randomly pull one name out of the bag, what is the probability that you pull out a girl’s name?
<b>Answer: </b>We can use the following formula to calculate the theoretical probability that you pull out a girl’s name:
P(<em>girls name</em>) = (7 possible girl names) / (10 total names) = <b>7/10</b>
<h2><span class="orange">Third Variable Problem: Definition & Example</span></h2>
In statistics, a <b>third variable problem</b> occurs when an observed correlation between two variables can actually be explained by a third variable that hasn’t been accounted for.
When this third variable is not taken into account, the correlation between the two variables under study can be misleading and even confusing.
This tutorial provides several examples of third variable problems in different settings.
<h3>Example 1: Dogs & Fire Hydrants</h3>
A researcher observes that cities with more fire hydrants tend to also have more dogs.
However, these two variables are only correlated because they both have a high correlation with a third variable: <b>population size</b>.
Larger cities tend to have both more fire hydrants <em>and</em> more dogs. Conversely, smaller cities tend to have fewer fire hydrants <em>and</em> fewer dogs.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/thirdVariable1.png">
<h3>Example 2: Ice Cream Sales & Shark Attacks</h3>
A researcher finds that ice cream sales and shark attacks are highly positively correlated.
However, these two variables are only correlated because they both have a high correlation with a third variable: <b>temperature</b>.
When it’s warmer out, more people buy ice cream and more people swim in the ocean which explains why the values for both ice cream sales and shark attacks tend to increase during the same times of the year.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/thirdVariable2.png">
<h3>Example 3: Volunteers & Natural Disasters</h3>
A study finds that the more volunteers that show up after a natural disaster, the greater the damage. 
However, these two variables are only correlated because they both have a high correlation with a third variable: <b>size of the natural disaster</b>.
Larger natural disasters are highly correlated with more damage done as well as an increase in the number of volunteers.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/thirdVariable3.png">
<h3>Related Articles</h3>
 What is Omitted Variable Bias? 
 What is Undercoverage Bias? 
 What is Aggregation Bias? 
 What is a Confounding Variable? 
<h2><span class="orange">Three Sigma Calculator</span></h2>
The <b>standard deviation</b> is a common way to measure how “spread out” data values are. In statistics, the standard deviation is often referred to as <b>sigma</b>, which is written as σ.
Values that lie outside of three sigmas from the mean are often denoted as outliers or unusual values in a dataset. Thus, finding the value of three sigmas from the mean is often of interest to statisticians and researchers.
To find the value of sigma as well as three sigma for a given dataset, enter your comma separated data in the box below, then click the “Calculate” button:
<textarea id="input_data" name="x" rows="5" cols="40">4, 14, 16, 22, 24, 25, 37, 38, 38, 40, 41, 41, 43, 44</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<b>Standard Deviation (Sigma):</b> 12.3448
<b>Three Sigma:</b> 37.0343
<script>
function calc() {
var input_data = document.getElementById('input_data').value.match(/\d+/g).map(Number);
//var input_numbers = //document.getElementById('text_area').value.split(',').map(Number);
var sigma = jStat(input_data).stdev();
var sigma3 = sigma * 3;
//output results
document.getElementById('sigma').innerHTML = sigma.toFixed(4);
document.getElementById('sigma3').innerHTML = sigma3.toFixed(4);
  
} //end calc function
</script>
<h2><span class="orange">How to Perform a Three-Way ANOVA in R</span></h2>
A <b>three-way ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups that have been split on three factors.
The following example shows how to perform a three-way ANOVA in R.
<h2>Example: Three-Way ANOVA in R</h2>
Suppose a researcher wants to determine if two training programs lead to different mean improvements in jumping height among college basketball players.
The researcher suspects that gender and division (Division I or II) may also affect jumping height so he collects data for these factors as well.
His goal is to perform a three-way ANOVA to determine how training program, gender, and division affect jumping height.
Use the following steps to perform this three-way ANOVA in R:
<b>Step 1: Create the Data</b>
First, let’s create a data frame to hold the data:
<b>#create dataset
df &lt;- data.frame(program=rep(c(1, 2), each=20), gender=rep(c('M', 'F'), each=10, times=2), division=rep(c(1, 2), each=5, times=4), height=c(7, 7, 8, 8, 7, 6, 6, 5, 6, 5,          5, 5, 4, 5, 4, 3, 3, 4, 3, 3,          6, 6, 5, 4, 5, 4, 5, 4, 4, 3,          2, 2, 1, 4, 4, 2, 1, 1, 2, 1)) 
#view first six rows of dataset
head(df)
  program gender division height
1       1      M        1      7
2       1      M        1      7
3       1      M        1      8
4       1      M        1      8
5       1      M        1      7
6       1      M        2      6</b>
<b>Step 2: View Descriptive Statistics</b>
Before performing the three-way ANOVA, we can use  dplyr  to quickly summarize the mean jumping height increase grouped by training program, gender, and divison:
<b>library(dplyr)
#calculate mean jumping height increase grouped by program, gender, and division
df %>%
  group_by(program, gender, division) %>%
  summarize(mean_height = mean(height))
# A tibble: 8 x 4
# Groups:   program, gender [4]
  program gender division mean_height 
1       1 F             1         4.6
2       1 F             2         3.2
3       1 M             1         7.4
4       1 M             2         5.6
5       2 F             1         2.6
6       2 F             2         1.4
7       2 M             1         5.2
8       2 M             2         4  
</b>
Here’s how to interpret the output:
The mean jumping height increase among division I females who used training program 1 was <b>4.6 inches</b>.
The mean jumping height increase among division II females who used training program 1 was <b>3.2 inches</b>.
The mean jumping height increase among division I males who used training program 1 was <b>7.4 inches</b>.
And so on.
<b>Step 3: Perform the Three-Way ANOVA</b>
Next, we can use the <b>aov()</b> function to perform the three-way ANOVA:
<b>#perform three-way ANOVA
model &lt;- aov(height ~ program * gender * division, data=df)
#view summary of three-way ANOVA
summary(model)
        Df Sum Sq Mean Sq F value   Pr(>F)    
program                  1   36.1   36.10  65.636 2.98e-09 ***
gender                   1   67.6   67.60 122.909 1.71e-12 ***
division                 1   19.6   19.60  35.636 1.19e-06 ***
program:gender           1    0.0    0.00   0.000    1.000    
program:division         1    0.4    0.40   0.727    0.400    
gender:division          1    0.1    0.10   0.182    0.673    
program:gender:division  1    0.1    0.10   0.182    0.673    
Residuals               32   17.6    0.55                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
The <b>Pr(>F)</b> column shows the p-value for each individual factor and the interactions between the factors.
From the output we can see that none of the interactions between the three factors were statistically significant.
We can also see that each of the three factors – program, gender, and division – were statistically significant.
We can now use dplyr once again to find the mean jumping height increase for program, gender, and division separately:
<b>library(dplyr)
#find mean jumping increase by program
df %>%
  group_by(program) %>%
  summarize(mean_height = mean(height))
# A tibble: 2 x 2
  program mean_height
           
1       1         5.2
2       2         3.3
#find mean jumping increase by gender
df %>%
  group_by(gender) %>%
  summarize(mean_height = mean(height))
# A tibble: 2 x 2
  gender mean_height
          
1 F             2.95
2 M             5.55
#find mean jumping increase by division
df %>%
group_by(division) %>%
summarize(mean_height = mean(height))
# A tibble: 2 x 2
  division mean_height
            
1        1        4.95
2        2        3.55</b>
From the output we can observe the following:
The mean jumping height increase among individuals who used training program 1 (<b>5.2 inches</b>) was higher than the mean increase among individuals who used training program 2 <b>(3.3 inches</b>).
The mean jumping height increase among males (<b>5.55 inches</b>) was higher than the mean increase among females <b>(2.95 inches</b>).
The mean jumping height increase among division 1 players (<b>4.95 inches</b>) was higher than the mean increase among division 2 players <b>(3.55 inches</b>).
In conclusion, we would state that training program, gender, and division are all significant predictors of the jumping height increase among players.
We would also state that there are no significant interaction effects between these three factors.
<h2>Additional Resources</h2>
The following tutorials explain how to fit other ANOVA models in R:
 How to Perform a One-Way ANOVA in R 
 How to Perform a Two-Way ANOVA in R 
<h2><span class="orange">How to Perform a Three-Way ANOVA in Python</span></h2>
A <b>three-way ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups that have been split on three factors.
The following example shows how to perform a three-way ANOVA in Python.
<h2>Example: Three-Way ANOVA in Python</h2>
Suppose a researcher wants to determine if two training programs lead to different mean improvements in jumping height among college basketball players.
The researcher suspects that gender and division (Division I or II) may also affect jumping height so he collects data for these factors as well.
His goal is to perform a three-way ANOVA to determine how training program, gender, and division affect jumping height.
Use the following steps to perform this three-way ANOVA in Python:
<b>Step 1: Create the Data</b>
First, let’s create a pandas DataFrame to hold the data:
<b>import numpy as np
import pandas as pd
#create DataFrame
df = pd.DataFrame({'program': np.repeat([1, 2], 20),   'gender': np.tile(np.repeat(['M', 'F'], 10), 2),   'division': np.tile(np.repeat([1, 2], 5), 4),   'height': [7, 7, 8, 8, 7, 6, 6, 5, 6, 5,              5, 5, 4, 5, 4, 3, 3, 4, 3, 3,              6, 6, 5, 4, 5, 4, 5, 4, 4, 3,              2, 2, 1, 4, 4, 2, 1, 1, 2, 1]})
#view first ten rows of DataFrame 
df[:10]
programgenderdivision  height
01M1  7
11M1  7
21M1  8
31M1  8
41M1  7
51M2  6
61M2  6
71M2  5
81M2  6
91M2  5
</b>
<b>Step 2: Perform the Three-Way ANOVA</b>
Next, we can use the <b>anova_lm()</b> function from the <b>statsmodels</b> library to perform the three-way ANOVA:
<b>import statsmodels.api as sm
from statsmodels.formula.api import ols
#perform three-way ANOVA
model = ols("""height ~ C(program) + C(gender) + C(division) +
               C(program):C(gender) + C(program):C(division) + C(gender):C(division) +
               C(program):C(gender):C(division)""", data=df).fit()
sm.stats.anova_lm(model, typ=2)
          sum_sqdfF        PR(>F)
C(program)                  3.610000e+011.06.563636e+012.983934e-09
C(gender)                  6.760000e+011.01.229091e+021.714432e-12
C(division)                  1.960000e+011.03.563636e+011.185218e-06
C(program):C(gender)          2.621672e-301.04.766677e-301.000000e+00
C(program):C(division)          4.000000e-011.07.272727e-014.001069e-01
C(gender):C(division)          1.000000e-011.01.818182e-016.726702e-01
C(program):C(gender):C(division)  1.000000e-011.01.818182e-016.726702e-01
Residual                  1.760000e+0132.0NaN        NaN</b>
<b>Step 3: Interpret the Results</b>
The <b>Pr(>F)</b> column shows the p-value for each individual factor and the interactions between the factors.
From the output we can see that none of the interactions between the three factors were statistically significant.
We can also see that each of the three factors (program, gender, and division) were statistically significant with the following p-values:
P-value of <b>program</b>: 0.00000000298
P-value of <b>gender</b>: 0.00000000000171
P-value of <b>division</b>: 0.00000185
In conclusion, we would state that training program, gender, and division are all significant predictors of the jumping height increase among players.
We would also state that there are no significant interaction effects between these three factors.
<h2>Additional Resources</h2>
The following tutorials explain how to fit other ANOVA models in Python:
 How to Perform a One-Way ANOVA in Python 
 How to Perform a Two-Way ANOVA in Python 
 How to Perform a Repeated Measures ANOVA in Python 
<h2><span class="orange">Three-Way ANOVA: Definition & Example</span></h2>
A <b>three-way ANOVA</b> is used to determine how three different factors affect some response variable.
Three-way ANOVAs are less common than a  one-way ANOVA  (with only one factor) or  two-way ANOVA  (with only two factors) but they are still used in a variety of fields.
Whenever we perform a three-way ANOVA, we’re interested in whether there is a statistically significant relationship between each factor and the response variable along with whether there are any interaction effects between the factors.
This tutorial shares several scenarios in which you might use a three-way ANOVA along with an example of how to perform one.
<h2>When to Use a Three-Way ANOVA</h2>
Here are a few scenarios where you may use a three-way ANOVA:
<b>Scenario 1: Botany</b>
A botanist may want to determine how (1) sunlight exposure, (2) watering frequency, and (3) fertilizer type affect plant growth.
In this scenario, she could perform a three-way ANOVA since there are three factors and one response variable.
<b>Scenario 2: Retail</b>
An executive at a retail store may want to determine how (1) day of the week, (2) store location, and (3) advertising campaigns affect total sales.
In this scenario, he could perform a three-way ANOVA since there are three factors and one response variable.
<b>Scenario 3: Medical</b>
A doctor may want to determine how (1) gender, (2) diet, and (3) exercise habits affect weight.
In this scenario, she could perform a three-way ANOVA since there are three factors and one response variable.
<h2>Three-Way ANOVA: Example</h2>
Suppose a researcher wants to determine if training program, gender, and athletic division affect jumping height.
To test this, he can perform a three-way ANOVA with the following factors:
<b>1. Training program</b> (program 1 vs. program 2)
<b>2. Gender</b> (male vs. female)
<b>3. Athletic Division</b> (division I vs. division II)
The one response variable will be <b>jumping height</b>.
Suppose he collects this data on 40 individuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/three1.jpg"377">
He then uses statistical software to perform a three-way ANOVA and receives the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/three2.jpg">
The <b>P-value</b> column shows the p-value for each individual factor and the interactions between the factors.
From the output we can see that none of the interactions between the three factors were statistically significant.
We can also see that each of the three factors (Program, Gender, and Division) were statistically significant.
In conclusion, we would state that training program, gender, and division are all significant predictors of the jumping height increase among players.
We would also state that there are no significant interaction effects between these three factors.
<b>Note</b>: In practice, we would also calculate the mean jumping height for each program, gender, and division so that we could determine which  levels  of each factor are associated with increased jumping height.
<h2>Additional Resources</h2>
The following tutorials explain how to perform a three-way ANOVA in R and Python:
 How to Perform a Three-Way ANOVA in R 
 How to Perform a Three-Way ANOVA in Python 
<h2><span class="orange">How to Use the Tilde Operator (~) in R</span></h2>
You can use the tilde operator (<b>~</b>) in R to separate the left hand side of an equation from the right hand side.
This operator is most commonly used with the  lm()  function in R, which is used to fit  linear regression models .
The basic syntax for the lm() function is:
<b>model &lt;- lm(y ~ x1 + x2, data=df)</b>
The variable name on the <b>left side</b> of the tilde operator (y) represents the <b>response variable</b>.
The variable names on the <b>right side</b> of the tilde operator (x1, x2) represent the <b>predictor variables</b>.
The following examples show how to use this tilde operator in different scenarios.
<h3>Example 1: Use Tilde Operator with One Predictor Variable</h3>
Suppose we fit the following simple linear regression model in R:
<b>model &lt;- lm(y ~ x, data=df)</b>
This particular regression model has one response variable (y) and one predictor variable (x).
If we wrote out this regression equation in statistical notation it would look like this:
y = β<sub>0</sub> + β<sub>1</sub>x
<h3>Example 2: Use Tilde Operator with Multiple Predictor Variables</h3>
Suppose we fit the following multiple linear regression model in R:
<b>model &lt;- lm(y ~ x1 + x2 + x3, data=df)</b>
This particular regression model has one response variable (y) and three predictor variables (x1, x2, x3).
If we wrote out this regression equation in statistical notation it would look like this:
y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub>
<h3>Example 3: Use Tilde Operator with Unknown Number of Predictor Variables</h3>
Suppose we fit the following multiple linear regression model in R:
<b>model &lt;- lm(y ~ ., data=df)</b>
This particular syntax indicates that we would like to use <b>y</b> as the response variable and every other variable in the data frame as predictor variables.
This syntax is useful when we want to fit a regression model with tons of predictor variables but we don’t want to type out the individual name of every single predictor variable.
<h2><span class="orange">5 Examples of Time Series Analysis in Real Life</span></h2>
<b>Time series analysis</b> is used to understand how the value of some variable changes over time.
In this article, we share five examples of how time series analysis is commonly used in real life situations.
<h3>Example 1: Retail Sales</h3>
Retail stores often use time series analysis to analyze how their total sales is trending over time.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/timeseries_real1.png">
Time series analysis is particularly useful for analyzing monthly, seasonal, and yearly trends in sales.
This allows retail stores to be able to more accurately predict what their sales will be during an upcoming period and be able to more accurately predict how much inventory and staff they’ll need during different periods of the year.
<h3>Example 2: Stock Prices</h3>
Time series analysis is also used frequently by stock traders so they can gain a better understanding of the patterns in various stock prices.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/timeseries_real2.png">
Time series plots in particular are helpful because they allow stock analysts and traders to understand the trend and direction of a certain stock price.
<h3>Example 3: Weather</h3>
Time series analysis is also used frequently by weatherman to predict what the temperatures will be during different months and seasons throughout the year.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/timeseries_real3.png">
<h3>Example 4: Heart Rate</h3>
Time series analysis is also used in the medical field to monitor the heart rate of patients who may be on certain medications to make sure that heart rate doesn’t fluctuate too wildly during any given time of the day.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/timeseries_real4.png">
<h3>Example 5: Subscribers</h3>
Time series analysis is often used by online publications to analyze trends in the total number of subscribers from one year to the next.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/timeseries_real5.png">
Time series plots can be particularly useful for identifying whether or not growth in the number of subscribers is increasing, decreasing, or hitting a plateau.
<h2><span class="orange">How to Convert UNIX Timestamp to Date in R (3 Methods)</span></h2>
You can use one of the following three methods to convert a  UNIX timestamp  to a date object in R:
<b>Method 1: Use Base R</b>
<b>#convert UNIX timestamp to date 
as.Date(as.POSIXct(x, origin="1970-01-01"))
</b>
<b>Method 2: Use anytime Package</b>
<b>library(anytime)
#convert UNIX timestamp to date
anydate(x)
</b>
<b>Method 3: Use lubridate Package</b>
<b>library(lubridate)
#convert UNIX timestamp to date 
as_date(as_datetime(x))</b>
The following examples show how to use each function in practice. 
<h3>Example 1: Convert Timestamp to Date Using Base R</h3>
We can use the following code to convert a UNIX timestamp to a date using only functions from base R:
<b>#define UNIX timestamp
value &lt;- 1648565400
#convert UNIX timestamp to date object
new_date &lt;- as.Date(as.POSIXct(value, origin="1970-01-01"))
#view date object
new_date
[1] "2022-03-29"
#view class of date object
class(new_date)
[1] "Date"</b>
The UNIX timestamp has successfully been converted to a date object.
<h3>Example 2: Convert Timestamp to Date Using anytime Package</h3>
We can also use the <b>anydate()</b> function from the <b>anytime</b> package to convert a UNIX timestamp to a date object in R:
<b>library(anytime)
#define UNIX timestamp
value &lt;- 1648565400
#convert UNIX timestamp to date object
new_date &lt;- anydate(value)
#view date object
new_date
[1] "2022-03-29"
#view class of date object
class(new_date)
[1] "Date"</b>
The UNIX timestamp has successfully been converted to a date object.
<h3>Example 3: Convert Timestamp to Date Using lubridate Package</h3>
We can also use the <b>as_date()</b> function from the  lubridate  package to convert a UNIX timestamp to a date object in R:
<b>library(lubridate)
#define UNIX timestamp
value &lt;- 1648565400
#convert UNIX timestamp to date object
new_date &lt;- as_date(as_datetime(value))
#view date object
new_date
[1] "2022-03-29"
#view class of date object
class(new_date)
[1] "Date"</b>
Once again, the UNIX timestamp has successfully been converted to a date object.
<h2><span class="orange">Tolerance Interval Calculator</span></h2>
A <b>tolerance interval</b> is a range that is likely to contain a specific proportion of a population with a certain level of confidence.
This calculator creates a tolerance interval based on a given sample.
Simply enter a list of sample values below, then choose a proportion of the population you’d like the tolerance interval to contain, then choose a confidence level, then click the “Calculate” button:
<b>Sample values:</b>
<textarea id="x" rows="5" cols="40">33, 35, 36, 38, 39, 40, 40, 45, 46, 48, 51, 52, 53, 54, 55</textarea>
<b>Proportion of Population:</b>
<input type="number" id="pop" value="0.99">
<b>Confidence Level:</b>
<input type="number" id="CI" value="0.95" min="0" max="1">
<input type="button" id="button" onclick="calc()" value="Calculate">
95% Tolerance Interval: <b>(15.102, 73.565)</b>
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var pop = +document.getElementById('pop').value;
var CI = +document.getElementById('CI').value;
var CI_out = CI*100
var xbar = math.mean(x);
var n = x.length;
var s = math.std(x);
var z = Math.abs(jStat.normal.inv((1-pop)/2, 0, 1));
var blob = Math.sqrt(((n-1)*(1+1/n))/(jStat.chisquare.inv(1-CI, n-1)));
console.log(xbar, n, s, z, blob);
//calculate lower and upper bounds of prediction interval
var lowCI = xbar-s*z*blob;
var highCI = xbar-(-1*s*z*blob);
//output results
document.getElementById('lowCI').innerHTML = lowCI.toFixed(3);
document.getElementById('highCI').innerHTML = highCI.toFixed(3);
document.getElementById('CI_out').innerHTML = CI_out.toFixed(0);
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate a Tolerance Interval in Excel</span></h2>
A <b>tolerance interval</b> is a range that is likely to contain a specific proportion of a population with a certain level of confidence.
We can use the following formula to calculate the lower and upper limits of a two-sided tolerance interval:
<b>Tolerance Interval Limits</b>: x<U+0304> ± z<sub>(1-p)/2</sub>√(n-1)(1+1/n)/X<sup>2</sup><sub>1-α, n-1</sub>
where:
<b>x<U+0304></b>: sample mean
<b>z</b>: z critical value
<b>p</b>: proportion of population to be contained in interval
<b>n</b>: sample size
<b>X<sup>2</sup></b>: Chi-Square critical value with 1-α significance level and n-1 degrees of freedom
The following example shows how to calculate a tolerance interval in Excel for a given sample.
<h3>Example: Calculating a Tolerance Interval in Excel</h3>
Suppose we collect the following sample data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tol1.png">
Suppose we would like to create a tolerance interval that contains 99% of the population values with a 95% level of confidence.
We can use the following formulas in Excel to calculate this tolerance interval:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/tol2.png">
The tolerance interval turns out to be <b>[15.102, 73.565]</b>.
We would interpret this to mean that this interval contains 99% of the population values with a 95% level of confidence.
Note that if we change the values for the confidence level or the proportion of the population in the interval, the lower and upper limits of the tolerance interval will automatically change.
<b>Note</b>: You can automatically calculate a tolerance interval for a given sample by using this online  Tolerance Interval Calculator .
<h2><span class="orange">Total Sum of Squares Calculator</span></h2>
This calculator finds the total sum of squares of a regression equation based on values for a predictor variable and a response variable.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">6, 7, 7, 8, 12, 14, 15, 16, 16, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">14, 15, 15, 17, 18, 18, 16, 14, 11, 8</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Total Sum of Squares (SST): 88.4000</b>
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
var xbar = math.mean(x);
var ybar = math.mean(y);
let xbar2_hold = 0
for (let i = 0; i < x.length; i++) {
xbar2_hold += Math.pow(x[i], 2);
}
var xbar2 = xbar2_hold / x.length;
let sxx = 0
for (let i = 0; i < x.length; i++) {
sxx += Math.pow(x[i] - xbar, 2);
}
let syy = 0
for (let i = 0; i < y.length; i++) {
syy += Math.pow(y[i] - ybar, 2);
}
let sxy = 0
for (let i = 0; i < x.length; i++) {
sxy += (x[i] - xbar)*(y[i]-ybar);
}
let sxx2 = 0
for (let i = 0; i < x.length; i++) {
sxx2 += (x[i] - xbar)*(Math.pow(x[i], 2)-xbar2);
}
let sx2x2 = 0
for (let i = 0; i < x.length; i++) {
sx2x2 += Math.pow((Math.pow(x[i], 2)-xbar2), 2);
}
let sx2y = 0
for (let i = 0; i < x.length; i++) {
sx2y += (Math.pow(x[i], 2)-xbar2)*(y[i]-ybar);
}
var b = ((sxy*sx2x2)-(sx2y*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var c = ((sx2y*sxx)-(sxy*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var a = ybar - (b*xbar) - (c*xbar2);
var sst = syy;
var ssr = (sxy/sxx)*sxy;
var sse = sst-ssr;
document.getElementById('sst').innerHTML = sst.toFixed(4);
}
//output error message if boths lists are not equal
else {
 //document.getElementById('out').innerHTML = '';
 document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Split Data into Training & Test Sets in R (3 Methods)</span></h2>
Often when we fit  machine learning algorithms  to datasets, we first split the dataset into a training set and a test set.
There are three common ways to split data into training and test sets in R:
<b>Method 1: Use Base R</b>
<b>#make this example reproducible
set.seed(1)
#use 70% of dataset as training set and 30% as test set
sample &lt;- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.7,0.3))
train  &lt;- df[sample, ]
test   &lt;- df[!sample, ]</b>
<b>Method 2: Use caTools package</b>
<b>library(caTools)
#make this example reproducible
set.seed(1)
#use 70% of dataset as training set and 30% as test set
sample &lt;- sample.split(df$any_column_name, SplitRatio = 0.7)
train  &lt;- subset(df, sample == TRUE)
test   &lt;- subset(df, sample == FALSE)</b>
<b>Method 3: Use dplyr package</b>
<b>library(dplyr)
#make this example reproducible
set.seed(1)
#create ID column
df$id &lt;- 1:nrow(df)
#use 70% of dataset as training set and 30% as test set 
train &lt;- df %>% dplyr::sample_frac(0.70)
test  &lt;- dplyr::anti_join(df, train, by = 'id')</b>
The following examples show how to use each method in practice with the built-in  iris dataset  in R.
<h3>Example 1: Split Data Into Training & Test Set Using Base R</h3>
The following code shows how to use base R to split the iris dataset into a training and test set, using 70% of the rows as the training set and the remaining 30% as the test set:
<b>#load iris dataset
data(iris)
#make this example reproducible
set.seed(1)
#Use 70% of dataset as training set and remaining 30% as testing set
sample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7,0.3))
train  &lt;- iris[sample, ]
test   &lt;- iris[!sample, ]
#view dimensions of training set
dim(train)
[1] 106   5
#view dimensions of test set
dim(test)
[1] 44 5</b>
From the output we can see:
The training set is a data frame with 106 rows and 5 columns.
The test is a data frame with 44 rows and 5 columns.
Since the original data frame had 150 total rows, the training set contains roughly 106 / 150 = 70.6% of the original rows.
We can also view the first few rows of the training set if we’d like:
<b>#view first few rows of training set
head(train)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
8          5.0         3.4          1.5         0.2  setosa
9          4.4         2.9          1.4         0.2  setosa
</b>
<h3>Example 2: Split Data Into Training & Test Set Using caTools</h3>
The following code shows how to use the <b>caTools</b> package in R to split the iris dataset into a training and test set, using 70% of the rows as the training set and the remaining 30% as the test set:
<b>library(caTools)
#load iris dataset
data(iris)
#make this example reproducible
set.seed(1)
#Use 70% of dataset as training set and remaining 30% as testing set
sample &lt;- sample.split(iris$Species, SplitRatio = 0.7)
train  &lt;- subset(iris, sample == TRUE)
test   &lt;- subset(iris, sample == FALSE)
#view dimensions of training set
dim(train)
[1] 105   5
#view dimensions of test set
dim(test)
[1] 45 5</b>
From the output we can see:
The training set is a data frame with 105 rows and 5 columns.
The test is a data frame with 45 rows and 5 columns.
<h3>Example 3: Split Data Into Training & Test Set Using dplyr</h3>
The following code shows how to use the <b>caTools</b> package in R to split the iris dataset into a training and test set, using 70% of the rows as the training set and the remaining 30% as the test set:
<b>library(dplyr)
#load iris dataset
data(iris)
#make this example reproducible
set.seed(1)
#create ID variable
iris$id &lt;- 1:nrow(iris)
#Use 70% of dataset as training set and remaining 30% as testing set 
train &lt;- iris %>% dplyr::sample_frac(0.7)
test  &lt;- dplyr::anti_join(iris, train, by = 'id')
#view dimensions of training set
dim(train)
[1] 105 6
#view dimensions of test set
dim(test)
[1] 45 6
</b>
From the output we can see:
The training set is a data frame with 105 rows and 6 columns.
The test is a data frame with 45 rows and 6 columns.
Note that these training and test sets contain one extra ‘id’ column that we created.
Be sure not to use this column (or drop it entirely from the data frames) when fitting your machine learning algorithm.
<h2><span class="orange">How to Transform Data in Excel (Log, Square Root, Cube Root)</span></h2>
Many statistical tests make the assumption that datasets are  normally distributed .
However, this assumption is often violated in practice. One way to address this issue is to transform the values of the dataset using one of the following three transformations:
<b>1. Log Transformation: </b>Transform the values from y to <b>log(y)</b>.
<b>2. Square Root Transformation: </b>Transform the values from y to <b>√y</b>.
<b>3. Cube Root Transformation: </b>Transform the values from y to <b>y<sup>1/3</sup></b>.
By performing these transformations, the data typically becomes closer to normally distributed. The following examples show how to perform these transformations in Excel.
<h3>Log Transformation in Excel</h3>
To apply a log transformation to a dataset in Excel, we can use the <b>=LOG10()</b> function.
The following screenshot shows how to apply a log transformation to a dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/transformExcel1.png">
To determine if this transformation made the dataset more normally distributed, we can perform a  Jarque-Bera normality test in Excel .
The test statistic for this test is defined as:
<em><b>JB</b> </em> =(n/6) * (S<sup>2</sup> + (C<sup>2</sup>/4))
where:
<b>n:</b> the number of  observations  in the sample
<b>S: </b>the sample skewness
<b>C:</b> the sample kurtosis
Under the null hypothesis of normality, <em>JB ~ </em>X<sup>2</sup>(2).
If the  p-value  that corresponds to the test statistic is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that the data is not normally distributed.
The following screenshot shows how to perform a Jarque-Bera test for the  raw data  and the transformed data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/transformExcel2.png">
Notice that the p-value for the raw data is less than .05, which indicates that it is <em>not</em> normally distributed.
However, the p-value for the transformed data is not less than .05, so we can assume that it <em>is</em> normally distributed. This tells us that the log transformation worked.
<h3>Square Root Transformation in Excel</h3>
To apply a square root transformation to a dataset in Excel, we can use the <b>=SQRT()</b> function.
The following screenshot shows how to apply a square root transformation to a dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/transformExcel3.png">
Notice that the p-value of the Jarque-Bera normality test for the transformed data is not less than .05, which indicates that the square root transformation was effective.
<h3>Cube Root Transformation in Excel</h3>
To apply a cube root transformation to a dataset in Excel, we can use the <b>=DATA^(1/3)</b> function.
The following screenshot shows how to apply a cube root transformation to a dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/transformExcel4.png">
The p-value of the Jarque-Bera normality test for the transformed data is not less than .05, which indicates that the cube root transformation was effective.
All three data transformations effectively made the raw data more normally distributed.
Out of the three transformations, the log transformation resulted in the largest p-value in the Jarque-Bera normality test, which tells us that it likely made the data the “most” normally distributed out of the three transformation methods.
<h2><span class="orange">How to Transform Data in Python (Log, Square Root, Cube Root)</span></h2>
Many statistical tests make the assumption that datasets are normally distributed. However, this is often not the case in practice.
One way to address this issue is to transform the distribution of values in a dataset using one of the three transformations:
<b>1. Log Transformation: </b>Transform the response variable from y to <b>log(y)</b>.
<b>2. Square Root Transformation: </b>Transform the response variable from y to <b>√y</b>.
<b>3. Cube Root Transformation: </b>Transform the response variable from y to <b>y<sup>1/3</sup></b>.
By performing these transformations, the dataset typically becomes more normally distributed.
The following examples show how to perform these transformations in Python.
<h3>Log Transformation in Python</h3>
The following code shows how to perform a <b>log transformation</b> on a variable and create side-by-side plots to view the original distribution and the log-transformed distribution of the data:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(0)
#create beta distributed random variable with 200 values
data = np.random.beta(a=4, b=15, size=300)
#create log-transformed data
data_log = np.log(data)
#define grid of plots
fig, axs = plt.subplots(nrows=1, ncols=2)
#create histograms
axs[0].hist(data, edgecolor='black')
axs[1].hist(data_log, edgecolor='black')
#add title to each histogram
axs[0].set_title('Original Data')
axs[1].set_title('Log-Transformed Data')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/trans11.png">
Notice how the log-transformed distribution is more normally distributed compared to the original distribution.
It’s still not a perfect “bell shape” but it’s closer to a normal distribution that the original distribution.
<h3>Square Root Transformation in Python</h3>
The following code shows how to perform a <b>square root transformation</b> on a variable and create side-by-side plots to view the original distribution and the square root transformed distribution of the data:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(0)
#create beta distributed random variable with 200 values
data = np.random.beta(a=1, b=5, size=300)
#create log-transformed data
data_log = np.sqrt(data)
#define grid of plots
fig, axs = plt.subplots(nrows=1, ncols=2)
#create histograms
axs[0].hist(data, edgecolor='black')
axs[1].hist(data_log, edgecolor='black')
#add title to each histogram
axs[0].set_title('Original Data')
axs[1].set_title('Square Root Transformed Data')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/trans12.png">
Notice how the square root transformed data is much more normally distributed than the original data.
<h3>Cube Root Transformation in Python</h3>
The following code shows how to perform a <b>cube root transformation</b> on a variable and create side-by-side plots to view the original distribution and the cube root transformed distribution of the data:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(0)
#create beta distributed random variable with 200 values
data = np.random.beta(a=1, b=5, size=300)
#create log-transformed data
data_log = np.cbrt(data)
#define grid of plots
fig, axs = plt.subplots(nrows=1, ncols=2)
#create histograms
axs[0].hist(data, edgecolor='black')
axs[1].hist(data_log, edgecolor='black')
#add title to each histogram
axs[0].set_title('Original Data')
axs[1].set_title('Cube Root Transformed Data')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/trans13.png">
Notice how the cube root transformed data is much more normally distributed than the original data.
<h2><span class="orange">How to Transform Data in R (Log, Square Root, Cube Root)</span></h2>
Many statistical tests make the assumption that the residuals of a  response variable  are normally distributed.
However, often the residuals are <em>not </em>normally distributed. One way to address this issue is to transform the response variable using one of the three transformations:
<b>1. Log Transformation: </b>Transform the response variable from y to <b>log(y)</b>.
<b>2. Square Root Transformation: </b>Transform the response variable from y to <b>√y</b>.
<b>3. Cube Root Transformation: </b>Transform the response variable from y to <b>y<sup>1/3</sup></b>.
By performing these transformations, the response variable typically becomes closer to normally distributed. The following examples show how to perform these transformations in R.
<h3>Log Transformation in R</h3>
The following code shows how to perform a log transformation on a response variable:
<b>#create data frame
df &lt;- data.frame(y=c(1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 7, 8), x1=c(7, 7, 8, 3, 2, 4, 4, 6, 6, 7, 5, 3, 3, 5, 8), x2=c(3, 3, 6, 6, 8, 9, 9, 8, 8, 7, 4, 3, 3, 2, 7))
#perform log transformation
log_y &lt;- log10(df$y)
</b>
The following code shows how to create histograms to view the distribution of <em>y </em>before and after performing a log transformation:
<b>#create histogram for original distribution
hist(df$y, col='steelblue', main='Original')
#create histogram for log-transformed distribution 
hist(log_y, col='coral2', main='Log Transformed')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/transformR1.png">
Notice how the log-transformed distribution is much more normal compared to the original distribution. It’s still not a perfect “bell shape” but it’s closer to a normal distribution that the original distribution.
In fact, if we perform a  Shapiro-Wilk test  on each distribution we’ll find that the original distribution fails the normality assumption while the log-transformed distribution does not (at α = .05):
<b>#perform Shapiro-Wilk Test on original data
shapiro.test(df$y)
Shapiro-Wilk normality test
data:  df$y
W = 0.77225, p-value = 0.001655
#perform Shapiro-Wilk Test on log-transformed data 
shapiro.test(log_y)
Shapiro-Wilk normality test
data:  log_y
W = 0.89089, p-value = 0.06917</b>
<h3>Square Root Transformation in R</h3>
The following code shows how to perform a square root transformation on a response variable:
<b>#create data frame
df &lt;- data.frame(y=c(1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 7, 8), x1=c(7, 7, 8, 3, 2, 4, 4, 6, 6, 7, 5, 3, 3, 5, 8), x2=c(3, 3, 6, 6, 8, 9, 9, 8, 8, 7, 4, 3, 3, 2, 7))
#perform square root transformation
sqrt_y &lt;- sqrt(df$y)
</b>
The following code shows how to create histograms to view the distribution of <em>y </em>before and after performing a square root transformation:
<b>#create histogram for original distribution
hist(df$y, col='steelblue', main='Original')
#create histogram for square root-transformed distribution 
hist(sqrt_y, col='coral2', main='Square Root Transformed')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/transformR2.png">
Notice how the square root-transformed distribution is much more normally distributed compared to the original distribution.
<h3>Cube Root Transformation in R</h3>
The following code shows how to perform a cube root transformation on a response variable:
<b>#create data frame
df &lt;- data.frame(y=c(1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 7, 8), x1=c(7, 7, 8, 3, 2, 4, 4, 6, 6, 7, 5, 3, 3, 5, 8), x2=c(3, 3, 6, 6, 8, 9, 9, 8, 8, 7, 4, 3, 3, 2, 7))
#perform square root transformation
cube_y &lt;- df$y^(1/3)
</b>
The following code shows how to create histograms to view the distribution of <em>y </em>before and after performing a square root transformation:
<b>#create histogram for original distribution
hist(df$y, col='steelblue', main='Original')
#create histogram for square root-transformed distribution 
hist(cube_y, col='coral2', main='Cube Root Transformed')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/transformR3.png">
Depending on your dataset, one of these transformations may produce a new dataset that is more normally distributed than the others.
<h2><span class="orange">How to Use the transform Function in R (3 Examples)</span></h2>
You can use the <b>transform() </b>function in base R to modify existing columns or add new columns to a data frame.
This function uses the following basic syntax:
<b>transform(df, my_column = my_column_transformed)
</b>
The following examples show how to use this function in different scenarios with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(pos=c('G', 'G', 'F', 'F', 'C'), points=c(23, 29, 33, 14, 10), assists=c(7, 7, 5, 9, 14))
#view data frame
df
  pos points assists
1   G     23       7
2   G     29       7
3   F     33       5
4   F     14       9
5   C     10      14
</b>
<h3>Example 1: Use transform() to Modify Existing Column</h3>
The following code shows how to use the <b>transform()</b> function to modify the existing points column:
<b>#divide existing points column by 2
df_new &lt;- transform(df, points = points / 2)
#view new data frame
df_new
  pos points assists
1   G   11.5       7
2   G   14.5       7
3   F   16.5       5
4   F    7.0       9
5   C    5.0      14</b>
Notice that each value in the existing points column was divided by two and all other columns remained unchanged.
<h3>Example 2: Use transform() to Add One New Column</h3>
The following code shows how to use the <b>transform()</b> function to add a new column called <b>points2</b>:
<b>#add new column called points2
df_new &lt;- transform(df, points2 = points * 2)
#view new data frame
df_new
  pos points assists points2
1   G     23       7      46
2   G     29       7      58
3   F     33       5      66
4   F     14       9      28
5   C     10      14      20</b>
Notice that the new column has been added to the data frame and all other existing columns remained the same.
<h3>Example 3: Use transform() to Add Multiple New Columns</h3>
The following code shows how to use the <b>transform()</b> function to add two new columns called <b>points2</b> and <b>assists2</b>:
<b>#add new columns called points2 and assists2
df_new &lt;- transform(df,    points2 = points * 2,    assists2 = assists * 2)
#view new data frame
df_new
  pos points assists points2 assists2
1   G     23       7      46       14
2   G     29       7      58       14
3   F     33       5      66       10
4   F     14       9      28       18
5   C     10      14      20       28</b>
Notice that two new columns have been added to the data frame and all other existing columns remained the same.
<h2><span class="orange">How to Transpose a Data Frame in R (With Examples)</span></h2>
There are two common methods you can use to transpose a data frame in R:
<b>Method 1: Use Base R</b>
<b>#transpose data frame
t(df)
</b>
<b>Method 2: Use data.table</b>
<b>library(data.table)
#transpose data frame
df_t &lt;- transpose(df)
#redefine row and column names
rownames(df_t) &lt;- colnames(df)
colnames(df_t) &lt;- rownames(df)
</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Transpose Data Frame Using Base R</h3>
Suppose we have the following data frame:
<b>#create data frame
df &lt;- data.frame(A = c(1, 2, 3, 4, 5), B = c(6, 7, 8, 9, 10), C = c(11, 12, 13, 14, 15))
#define row names
row.names(df) &lt;- c('One', 'Two', 'Three', 'Four', 'Five')
#view data frame
df
      A  B  C
One   1  6 11
Two   2  7 12
Three 3  8 13
Four  4  9 14
Five  5 10 15</b>
We can use the <b>t()</b> function from base R to quickly transpose the data frame:
<b>#transpose data frame
t(df)
  One Two Three Four Five
A   1   2     3    4    5
B   6   7     8    9   10
C  11  12    13   14   15
</b>
The rows and the columns are now switched.
<h3>Method 2: Transpose Data Frame Using data.table</h3>
Once again suppose we have the following data frame:
<b>#create data frame
df &lt;- data.frame(A = c(1, 2, 3, 4, 5), B = c(6, 7, 8, 9, 10), C = c(11, 12, 13, 14, 15))
#define row names
row.names(df) &lt;- c('One', 'Two', 'Three', 'Four', 'Five')
#view data frame
df
      A  B  C
One   1  6 11
Two   2  7 12
Three 3  8 13
Four  4  9 14
Five  5 10 15</b>
We can use the <b>transpose()</b> function from the data.table package to quickly transpose the data frame:
<b>library(data.table)
#transpose data frame
df_t &lt;- transpose(df)
#redefine row and column names
rownames(df_t) &lt;- colnames(df)
colnames(df_t) &lt;- rownames(df)
#display transposed data frame
df_t
  One Two Three Four Five
A   1   2     3    4    5
B   6   7     8    9   10
C  11  12    13   14   15
</b>
The result matches the transposed data frame from the previous example.
<b>Note</b>: The data.table method will be much faster than base R if you are working with an extremely large data frame.
<h2><span class="orange">Treatment Diffusion: Definition + Example</span></h2>
When researchers conduct an experiment, they’re often interested in understanding whether a certain “treatment” affects an outcome.
To test this, they’ll often recruit a  random sample  of individuals to be part of a control group and another random sample of individuals to be part of a treatment group. 
At the end of the experiment, they’ll record the outcomes for each group and conduct some statistical test to determine if the treatment significantly affected the outcome.
One potential issue that researchers could run into when conducting an experiment is <b>treatment diffusion</b>.
<b>Treatment diffusion: </b>When the control group in an experiment is affected by the treatment in some way.
In this post we’ll share:
An example of treatment diffusion
Why treatment diffusion can be a serious problem
How to prevent treatment diffusion from occurring
<h3>An Example of Treatment Diffusion</h3>
Suppose researchers are interested in finding out if a certain studying technique leads to a significant improvement in exam scores among students at a certain school.
To test this, they may recruit the following two  random samples :
<b>Control group:</b> 25 students who are told to continue to use their current studying technique.
<b>Treatment group: </b>25 students who are taught how to use the new studying technique.
After one month, each of the students take the same exam. The researchers had planned on conducting a  two sample t-test  to determine if their was a significant difference in mean exam scores between the two groups, but it was discovered that students in the treatment group were sharing the studying technique with students in the control group during lunch.
Because the students in the control group found out about the exact studying technique that students in the treatment group were using, it’s possible that they also started using the technique to potentially boost their own exam scores.
This is an example of <b>treatment diffusion </b>– the individuals in the control group became affected by the treatment.
Thus, the researchers will have a much harder time determining if any differences in exam scores can be attributable to the studying technique because students in both groups likely used the technique.
<h3>Why Treatment Diffusion Can Be a Problem</h3>
When treatment diffusion occurs, it causes two specific problems:
<b>1.</b> It makes it difficult for researchers to know if the difference in outcomes between two groups can be attributed to the treatment.
<b>2.</b> It makes it difficult for researchers to quantify <em>how much </em>of an impact that a treatment had on the outcome.
Essentially, treatment diffusion threatens the overall validity of the experiment and makes it hard for researchers to be confident in the conclusions that they draw.
<h3>How to Prevent Treatment Diffusion</h3>
Treatment diffusion is typically a problem in experiments when researchers provide some type of training or information to the individuals in the experiment.
In these situations, there’s always a possibility that information can spread from individuals in the treatment group to individuals in the control group.
Individuals may talk to each other about the information they learned, or even share information via texting, emailing, or calling. And when individuals in the control group learn about this information, they may decide to modify their own behavior.
To prevent treatment diffusion from occurring, researchers may decide to place the control group and the treatment group in two different locations to prevent the spread of information.
For example, researchers may decide to use a group of students at one school as the control group and a group of students from a different school as the treatment group. This makes it much less likely for treatment diffusion to occur.
One drawback of separating individuals based on location is that it’s possible that the two groups aren’t similar enough to each other.
For example, students at one school might have a different socioeconomic background, educational standards, and even school hours compared to students at the other school.
This increases the chances that any differences in exam scores could be due to other factors besides the studying technique.

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
