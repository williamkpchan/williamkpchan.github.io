<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: lightgray; background-color: #002010;  font-size: 18px;}
pre { color: lightgray; background-color: #001010;  font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
.topic{
    color: lime;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 28%;
	margin-right: 28%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head><body>

<center><b class="goldword">Tree-Based Models</b></center>
<div id="toc"><ul></ul></div>
<br>



<p>Recursive partitioning is a fundamental tool in data mining. It helps us explore the stucture of a set of data, while developing easy to visualize decision rules  for predicting  a categorical (classification tree) or continuous (regression tree) outcome. This section briefly describes CART modeling, conditional inference trees, and random forests.</p>

<h2>CART Modeling via rpart </h2>
<p><b class="goldword">Classification and regression trees</b> (as described by Brieman, Freidman, Olshen, and Stone) can be generated through the  <a href="http://cran.r-project.org/web/packages/rpart/index.html">rpart</a> package. Detailed information on rpart is available in <a href="http://www.mayo.edu/hsr/techrpt/61.pdf">An Introduction to Recursive Partitioning
 Using the RPART Routines</a>. <b class="goldword">The general steps</b> are provided below followed by two examples. </p>

<h3>1. Grow the Tree </h3>
<p>To grow a tree, use<br />
<strong>rpart(</strong><em>formula</em>, <strong>data=</strong>, <strong>method=,control=)</strong> where</p>
<p>
<table width="85%">
<tr>
<td>
<strong><em>formula</em></strong></td>
<td>is in the format <em><br />
outcome</em> ~ <em>predictor1</em>+<em>predictor2</em>+<em>predictor3</em>+ect.</td>
</tr>
<tr>
  <td><strong>data=</strong></td>
  <td>specifies the data frame</td>
  </tr>
<tr>
  <td><strong>method=</strong></td>
  <td><strong>&quot;class&quot;</strong> for a classification tree <strong><br />
  &quot;anova&quot;</strong> for a regression tree </td>
  </tr>
<tr>
  <td><strong>control=</strong></td>
  <td>optional parameters for controlling tree growth. For example, control=rpart.control(minsplit=30, cp=0.001) requires that the minimum number of observations in a node be 30 before attempting a split and that a split must decrease  the overall lack of fit by a factor of 0.001 (cost complexity factor) before being attempted. </td>
  </tr>
</table></p>

<h3>2. Examine the results </h3>
<p>The following functions help us to examine the results.</p>
<table width="85%">
<tr>
<td><strong>printcp(</strong><em>fit</em><strong>) </strong></td>
<td>display cp table </td>
</tr>
<tr>
  <td><strong>plotcp(</strong><em>fit</em><strong>) </strong></td>
  <td>plot cross-validation results</td>
  </tr>
<tr>
  <td><strong>rsq.rpart(</strong><em>fit</em><strong>)</strong></td>
  <td>plot approximate R-squared and relative error for different splits (2 plots). labels are only appropriate for the &quot;anova&quot; method. </td>
  </tr>
<tr>
  <td><strong>print(</strong><em>fit</em><strong>) </strong></td>
  <td>print results </td>
  </tr>
<tr>
  <td><strong>summary(</strong><em>fit</em><strong>)</strong></td>
  <td>detailed results including surrogate splits </td>
  </tr>
<tr>
  <td><strong>plot(</strong><em>fit</em><strong>) </strong></td>
  <td>plot decision tree </td>
  </tr>
<tr>
  <td><strong>text(</strong><em>fit</em><strong>) </strong></td>
  <td>label the decision tree plot </td>
  </tr>
<tr>
  <td><strong>post(</strong><em>fit</em>, <strong>file=)</strong> </td>
  <td>create postscript plot of decision tree </td>
  </tr>
</table>
<p>
In trees created by <strong>rpart( )</strong>, move to the <strong>LEFT</strong> branch when the stated condition is true (see the graphs below).</p>
     
<h3>3. prune tree</h3>
<p>Prune back the tree to avoid overfitting the data. Typically, you will want to select a tree size that minimizes the cross-validated error,  the <strong>xerror </strong>column printed by <strong>printcp( )</strong>. </p>
<p>Prune the tree to the desired size using<br />
<strong>prune(</strong><em>fit</em>, <strong>cp=</strong> <strong>) </strong></p>
<p>Specifically, use <strong>printcp( )</strong> to examine the cross-validated error results, select the complexity parameter associated with minimum error, and place it into the <strong>prune( )</strong> function. Alternatively, you can use the code fragment </p>
<p>&nbsp;&nbsp;&nbsp;<strong>&nbsp;&nbsp;fit$cptable[which.min(fit$cptable[,&quot;xerror&quot;]),&quot;CP&quot;]</strong></p>
<p>to automatically select the complexity parameter associated with the smallest cross-validated error. Thanks to <a href="../about/books.html">HSAUR</a> for this idea. </p>

<h3>Classification Tree example </h3>
<p>Let's use the data frame <strong>kyphosis</strong>  to predict a type of deformation (kyphosis) after surgery, from age in months (Age), number of vertebrae involved (Number), and the highest vertebrae operated on (Start).</p><p> <code># Classification Tree with rpart<br />
  library(rpart)<br />
  <br />
  # grow tree <br />
  fit &lt;- rpart(Kyphosis ~ Age + Number + Start,<br />
 &nbsp;&nbsp;	  method=&quot;class&quot;, data=kyphosis)<br />
 <br />
 printcp(fit) # display the results <br />
  plotcp(fit) # visualize cross-validation results <br />
summary(fit) # detailed summary of splits<br />
<br />
# plot tree <br />
  plot(fit, uniform=TRUE, <br />
 &nbsp;&nbsp;	  main=&quot;Classification Tree for Kyphosis&quot;)<br />
  text(fit, use.n=TRUE, all=TRUE, cex=.8)<br />
  <br />
  # create attractive postscript plot of tree <br />
  post(fit, file = &quot;c:/tree.ps&quot;, <br />
 &nbsp;&nbsp;	  title = &quot;Classification Tree for Kyphosis&quot;)</code></p>
<p><a href="images/cpPlot.png"><img src="images/smcpPlot.jpg" alt="cp Plot" width="103" height="103" /></a> <a href="images/ctree.png"><img src="images/smctree.jpg" alt="Classification Tree" width="103" height="103" /></a> <a href="images/tree.pdf"><img src="images/smtree.jpg" alt="Classification Tree in Postscript" width="133" height="103" /></a> click to view </p>
<p><code>
  # prune the tree <br />
  pfit&lt;- prune(fit, cp= &nbsp;&nbsp;fit$cptable[which.min(fit$cptable[,&quot;xerror&quot;]),&quot;CP&quot;])<br />
  <br />
  # plot the pruned tree <br />
  plot(pfit, uniform=TRUE, <br />
 &nbsp;&nbsp;	  main=&quot;Pruned Classification Tree for Kyphosis&quot;)<br />
  text(pfit, use.n=TRUE, all=TRUE, cex=.8)<br />
 post(pfit, file = &quot;c:/ptree.ps&quot;, <br />
 &nbsp;&nbsp;	  title = &quot;Pruned Classification Tree for Kyphosis&quot;)</code></p>
<p><a href="images/pctree.png"><img src="images/smpctree.jpg" alt="Pruned Classificaiton Tree" width="103" height="103" /></a> <a href="images/ptree.pdf"><img src="images/smptree.jpg" alt="Pruned Classification Tree in Postscript" width="133" height="103" /></a> click to view </p>

<h3>Regression Tree example</h3>
<p>In this example we will predict car mileage from price, country, reliability, and car type. The data frame is <strong>cu.summary</strong>. </p>
 <p><code># Regression Tree Example<br />
  library(rpart)<br />
  <br />
 # grow tree <br />
 fit &lt;- rpart(Mileage~Price + Country + Reliability + Type, <br />
 &nbsp;&nbsp; method=&quot;anova&quot;, data=cu.summary)<br />
 <br />
 printcp(fit) # display the results <br />
 plotcp(fit) # visualize cross-validation results
 <br />
 summary(fit) # detailed summary of splits<br />
 <br />
 # create additional plots
 <br />
par(mfrow=c(1,2)) # two plots on one page
<br />
rsq.rpart(fit) # visualize cross-validation results
 &nbsp;	  <br />
 <br />
 # plot tree <br />
 plot(fit, uniform=TRUE, <br />
 &nbsp;&nbsp;	  main=&quot;Regression Tree for Mileage &quot;)<br />
 text(fit, use.n=TRUE, all=TRUE, cex=.8)<br />
 <br />
 # create attractive postcript plot of tree <br />
 post(fit, file = &quot;c:/tree2.ps&quot;, <br />
 &nbsp;&nbsp;	  title = &quot;Regression Tree for Mileage &quot;)</code></p><p> <a href="images/plotCP2.png"><img src="images/smplotCP2.jpg" alt="cp plot for regression tree" width="103" height="103" /></a> <a href="images/rsqplot.png"><img src="images/smrsqplot.jpg" alt="rsquare plot for regression tree" width="103" height="103" /></a><a href="images/rtree.png"><img src="images/smrtree.jpg" alt="regression tree" width="103" height="103" /></a></a> <a href="images/tree2.pdf"><img src="images/smtree2.jpg" alt="Regressio Tree in Post Script" width="133" height="103" /></a> click to view </p>
<p><code># prune the tree <br />
 pfit&lt;- prune(fit, cp=0.01160389) # from cptable &nbsp;&nbsp;<br />
 <br />
 # plot the pruned tree <br />
 plot(pfit, uniform=TRUE, <br />
 &nbsp;&nbsp;	  main=&quot;Pruned Regression Tree for Mileage&quot;)<br />
 text(pfit, use.n=TRUE, all=TRUE, cex=.8)<br />
 post(pfit, file = &quot;c:/ptree2.ps&quot;, <br />
 &nbsp;&nbsp;	  title = &quot;Pruned Regression Tree for Mileage&quot;)</code></p>
<p>It turns out that this produces the same tree as the original. </p>

<h2>Conditional inference trees via party </h2>
<p>The <a href="http://cran.r-project.org/web/packages/party/index.html">party</a> package provides nonparametric regression trees for nominal, ordinal, numeric, censored, and multivariate responses. <a href="http://cran.r-project.org/web/packages/party/vignettes/party.pdf">party: A laboratory for recursive partitioning</a>, provides details. </p>
 <p>You can create a regression or classification tree via the function</p>
<p><strong>ctree(</strong><em>formula</em>, <strong>data=)</strong><br />
The type of tree created will depend on the outcome variable (nominal factor, ordered factor, numeric, etc.). Tree growth is based on statistical stopping rules, so pruning should not be required. </p>
<p>The previous two examples are re-analyzed below.  </p>
<p><code># Conditional Inference Tree for Kyphosis<br />
 library(party)<br />
 fit &lt;- ctree(Kyphosis ~ Age + Number + Start, <br />
 &nbsp;&nbsp; data=kyphosis)<br />
 plot(fit, main=&quot;Conditional Inference Tree for Kyphosis&quot;)</code></p>
<p><a href="images/citree.png"><img src="images/smcitree.jpg" alt="Condiitional Inference Tree for Kyphosis" width="103" height="103" /></a> click to view </p>
<p><code># Conditional Inference Tree for Mileage<br />
  library(party)<br />
 fit2 &lt;- ctree(Mileage~Price + Country + Reliability + Type, <br />
 &nbsp;&nbsp; data=na.omit(cu.summary))</code></p>
<p><a href="images/citree2.png"><img src="images/smcitree2.jpg" alt="Conditional Inference Tree for Mileage" width="103" height="103" /></a> click to view</p>

<h2>Random Forests </h2>
<p>Random forests improve predictive accuracy by generating a large number of bootstrapped trees (based on random samples of variables), classifying a case using each tree in this new &quot;forest&quot;, and deciding a final predicted outcome by combining the results across all of the trees (an average in regression, a majority vote in classification). Breiman and Cutler's random forest approach is implimented via the <a href="http://cran.r-project.org/web/packages/randomForest/index.html">randomForest</a> package. </p>
<p>Here is an example.</p>
<p><code># Random Forest prediction of Kyphosis data<br />
  library(randomForest)<br />
  fit &lt;- randomForest(Kyphosis ~ Age + Number + Start, &nbsp;&nbsp;data=kyphosis)<br />
  print(fit) # view results
     <br />
importance(fit) # importance of each predictor </code></p>
<p>For more details see the comprehensive <a href="https://www.stat.berkeley.edu/~breiman/RandomForests/">Random Forest website</a>. </p>

<h2>Going Further</h2>
<p>This section has only touched on the options available. To learn more, see the CRAN Task View on <a href="http://cran.r-project.org/web/views/MachineLearning.html">Machine &amp; Statistical Learning</a>. </p>

<h2>To Practice</h2>
<p>Try <a href="https://www.datacamp.com/community/open-courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic"> the Kaggle R Tutorial on Machine Learning</a> which includes an exercise with Random Forests.</p>
<br><br><br><br><br>


<center><h2>Classification Trees using the rpart function</h2></center>
<br>



<p>
In a previous 
<a href="http://www.wekaleamstudios.co.uk/posts/classification-trees/" rel="nofollow" target="_blank">
post
</a>
 on classification trees we considered using the 
<strong>
tree
</strong>
 package to fit a classification tree to data divided into known classes. In this post we will look at the alternative function rpart that is available within the base 
<strong>
R
</strong>
 distribution.
</p>
<p>

<a title="Click here to watch this video!" href="http://www.wekaleamstudios.co.uk/posts/classification-trees-using-the-rpart-function/#m3mLNpeke0I" rel="nofollow" target="_blank">
<img src="https://i0.wp.com/www.r-bloggers.com/wp-content/uploads/2010/09/02.jpg">
</a>
<br />

<small>
Fast Tube by 
<a title="Casper's Blog" href="http://blog.caspie.net/" rel="nofollow" target="_blank">
Casper
</a>
</small>

</p>
<p>
A classification tree can be fitted using the 
<strong>
rpart
</strong>
 function using a similar syntax to the 
<strong>
tree
</strong>
 function. For the ecoli data set discussed in the previous 
<a href="http://www.wekaleamstudios.co.uk/posts/classification-trees/" rel="nofollow" target="_blank">
post
</a>
 we would use:
</p>
<pre class="text" style="font-family:monospace;">
>
 require(rpart)
>
 ecoli.df = read.csv("ecoli.txt")
</pre>
<p>
followed by
</p>
<pre class="text" style="font-family:monospace;">
>
 ecoli.rpart1 = rpart(class ~ mcv + gvh + lip + chg + aac + alm1 + alm2, 
  data = ecoli.df)
</pre>
<p>
We would then consider whether the tree could be simplified by pruning and make use of the 
<strong>
plotcp
</strong>
 function:
</p>
<pre class="text" style="font-family:monospace;">
>
 plotcp(ecoli.rpart1)
</pre>
<p>
Once the amount of pruning has been determined from this graph or by looking at the output from the 
<strong>
printcp
</strong>
 function:
</p>
<pre class="text" style="font-family:monospace;">
>
 printcp(ecoli.rpart1)
 
Classification tree:
rpart(formula = class ~ mcv + gvh + lip + chg + aac + alm1 + 
    alm2, data = ecoli.df)
 
Variables actually used in tree construction:
[1] aac  alm1 gvh  mcv 
 
Root node error: 193/336 = 0.5744
 
n= 336 
 
        CP nsplit rel error  xerror     xstd
1 0.388601      0   1.00000 1.00000 0.046959
2 0.207254      1   0.61140 0.61658 0.045423
3 0.062176      2   0.40415 0.45596 0.041758
4 0.051813      3   0.34197 0.38342 0.039359
5 0.031088      4   0.29016 0.36269 0.038571
6 0.015544      5   0.25907 0.30570 0.036136
7 0.010000      6   0.24352 0.31088 0.036375
</pre>
<p>
The 
<strong>
prune
</strong>
 function is used to simplify the tree based on a 
<em>
cp
</em>
 identified from the graph or printed output threshold.
</p>
<pre class="text" style="font-family:monospace;">
>
 ecoli.rpart2 = prune(ecoli.rpart1, cp = 0.02)
</pre>
<p>
The classification tree can be visualised with the plot function and then the text function adds labels to the graph:
</p>
<pre class="text" style="font-family:monospace;">
>
 plot(ecoli.rpart2, uniform = TRUE)
>
 text(ecoli.rpart2, use.n = TRUE, cex = 0.75)
</pre>
<p>
Other useful resources are provided on the 
<a href="http://www.wekaleamstudios.co.uk/supplementary-material/" rel="nofollow" target="_blank">
Supplementary Material
</a>
 page.
</p>

<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<li><a href="' + href + '" target="_self">' + text + '</a></li>');
    }

    $('h2, h3').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
