<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="..\maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .apply, div.title').click(function(){
    parent.history.back();
    return false;
    });
});
</script>
<style>
body{font-size: x-large; width: 80%; margin-left: 10%}
h1, h2 {color: gold;}
</style>
</head><body>
<center><h1>Gradient Boosting Machines</h1>

<div id="toc"></div></center>
<br>
<br>
<br>

  
  <p><img src="http://uc-r.github.io/public/images/analytics/gbm/boosted_stumps.gif" style="float:right; margin: 2px 5px 0px 20px; width: 30%; height: 30%;" />
Gradient boosted machines (GBMs) are an extremely popular machine learning algorithm that have proven successful across many domains and is one of the leading methods for winning Kaggle competitions.  Whereas <a href="random_forests">random forests</a> build an ensemble of deep independent trees, GBMs build an ensemble of shallow and weak successive trees with each tree learning and improving on the previous.  When combined, these many weak successive trees produce a powerful “committee” that are often hard to beat with other algorithms.  This tutorial will cover the fundamentals of GBMs for regression problems.</p>

<h2></h2>
<h2 id="tldr">Introduction</h2>

<p>This tutorial serves as an introduction to the GBMs.  This tutorial will cover the following material:</p>

<ul>
  <li><a href="#prereq">Replication Requirements</a>: What you’ll need to reproduce the analysis in this tutorial.</li>
  <li><a href="#proscons">Advantages &amp; Disadvantages</a>: Primary strengths and weaknesses of GBMs.</li>
  <li><a href="#idea">The idea</a>: A quick overview of how GBMs work.</li>
  <li><a href="#gbm">gbm</a>: Training and tuning with the <code class="highlighter-rouge">gbm</code> package</li>
  <li><a href="#xgboost">xgboost</a>: Training and tuning with the <code class="highlighter-rouge">xgboost</code> package</li>
  <li><a href="#h2o">h2o</a>: Training and tuning with the <code class="highlighter-rouge">h2o</code> package</li>
  <li><a href="#learn">Learning more</a>: Where you can learn more.</li>
</ul>

<h2></h2>
<h2 id="prereq">Replication Requirements</h2>

<p>This tutorial leverages the following packages. Some of these packages play a supporting role; however, we demonstrate how to implement GBMs with several different packages and discuss the pros and cons to each.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">rsample</span><span class="p">)</span><span class="w">      </span><span class="c1"># data splitting </span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">gbm</span><span class="p">)</span><span class="w">          </span><span class="c1"># basic implementation</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">xgboost</span><span class="p">)</span><span class="w">      </span><span class="c1"># a faster implementation of gbm</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">caret</span><span class="p">)</span><span class="w">        </span><span class="c1"># an aggregator package for performing many machine learning models</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o</span><span class="p">)</span><span class="w">          </span><span class="c1"># a java-based platform</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">pdp</span><span class="p">)</span><span class="w">          </span><span class="c1"># model visualization</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">ggplot2</span><span class="p">)</span><span class="w">      </span><span class="c1"># model visualization</span><span class="w">
</span><span class="n">library</span><span class="p">(</span><span class="n">lime</span><span class="p">)</span><span class="w">         </span><span class="c1"># model visualization</span><span class="w">
</span></code></pre></div></div>

<p>To illustrate various GBM concepts we will use the Ames Housing data that has been included in the <code class="highlighter-rouge">AmesHousing</code> package.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.</span><span class="w">
</span><span class="c1"># Use set.seed for reproducibility</span><span class="w">

</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
</span><span class="n">ames_split</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">initial_split</span><span class="p">(</span><span class="n">AmesHousing</span><span class="o">::</span><span class="n">make_ames</span><span class="p">(),</span><span class="w"> </span><span class="n">prop</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.7</span><span class="p">)</span><span class="w">
</span><span class="n">ames_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">training</span><span class="p">(</span><span class="n">ames_split</span><span class="p">)</span><span class="w">
</span><span class="n">ames_test</span><span class="w">  </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">testing</span><span class="p">(</span><span class="n">ames_split</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><strong>Important note:</strong> tree-based methods tend to perform well on unprocessed data (i.e. without normalizing, centering, scaling features).  In this tutorial I focus on how to implement GBMs with various packages. Although I do not pre-process the data, realize that you <strong><em>can</em></strong> improve model performance by spending time processing variable attributes.</p>

<h2></h2>
<h2 id="proscons">Advantages &amp; Disadvantages</h2>

<p><strong>Advantages:</strong></p>

<ul>
  <li>Often provides predictive accuracy that cannot be beat.</li>
  <li>Lots of flexibility - can optimize on different loss functions and provides several hyperparameter tuning options that make the function fit very flexible.</li>
  <li>No data pre-processing required - often works great with categorical and numerical values as is.</li>
  <li>Handles missing data - imputation not required.</li>
</ul>

<p><strong>Disdvantages:</strong></p>

<ul>
  <li>GBMs will continue improving to minimize all errors.  This can overemphasize outliers and cause overfitting. Must use cross-validation to neutralize.</li>
  <li>Computationally expensive - GBMs often require many trees (&gt;1000) which can be time and memory exhaustive.</li>
  <li>The high flexibility results in many parameters that interact and influence heavily the behavior of the approach (number of iterations, tree depth, regularization parameters, etc.). This requires a large grid search during tuning.</li>
  <li>Less interpretable although this is easily addressed with various tools (variable importance, partial dependence plots, LIME, etc.).</li>
</ul>

<h2></h2>
<h2 id="idea">The idea</h2>

<p>Several supervised machine learning models are founded on a single predictive model (i.e. <a href="http://uc-r.github.io/linear_regression">linear regression</a>, <a href="http://uc-r.github.io/regularized_regression">penalized models</a>, <a href="http://uc-r.github.io/naive_bayes">naive Bayes</a>, <a href="http://uc-r.github.io/svm">support vector machines</a>). Alternatively, other approaches such as <a href="http://uc-r.github.io/regression_trees">bagging</a> and <a href="http://uc-r.github.io/random_forests">random forests</a> are built on the idea of building an ensemble of models where each individual model predicts the outcome and then the ensemble simply averages the predicted values.  The family of boosting methods is based on a different, constructive strategy of ensemble formation.</p>

<p>The main idea of boosting is to add new models to the ensemble <strong><em>sequentially</em></strong>. At each particular iteration, a new weak, base-learner model is trained with respect to the error of the whole ensemble learnt so far.</p>

<center>

<img src="http://uc-r.github.io/public/images/analytics/gbm/boosted-trees-process.png" alt="Fig 1. Sequential ensemble approach." width="50%" height="40%" />
<figcaption>Fig 1. Sequential ensemble approach.</figcaption>
</center>
<p><br /></p>

<p>Let’s discuss each component of the previous sentence in closer detail because they are important.</p>

<p><strong>Base-learning models</strong>:  Boosting is a framework that iteratively improves <em>any</em> weak learning model.  Many gradient boosting applications allow you to “plug in” various classes of weak learners at your disposal. In practice however, boosted algorithms almost always use decision trees as the base-learner. Consequently, this tutorial will discuss boosting in the context of regression trees.</p>

<p><strong>Training weak models</strong>: A weak model is one whose error rate is only slightly better than random guessing.  The idea behind boosting is that each sequential model builds a simple weak model to slightly improve the remaining errors.  With regards to decision trees, shallow trees represent a weak learner.  Commonly, trees with only 1-6 splits are used. Combining many weak models (versus strong ones) has a few benefits:</p>

<ul>
  <li>Speed: Constructing weak models is computationally cheap.</li>
  <li>Accuracy improvement: Weak models allow the algorithm to <em>learn slowly</em>; making minor adjustments in new areas where it does not perform well. In general,  statistical approaches that learn slowly tend to perform well.</li>
  <li>Avoids overfitting: Due to making only small incremental improvements with each model in the ensemble, this allows us to stop the learning process as soon as overfitting has been detected (typically by using cross-validation).</li>
</ul>

<p><strong>Sequential training with respect to errors</strong>: Boosted trees are grown sequentially; each tree is grown using information from previously grown trees. The basic algorithm for boosted regression trees can be generalized to the following where <em>x</em> represents our features and <em>y</em> represents our response:</p>

<ol>
  <li>Fit a decision tree to the data: <script type="math/tex">F_1(x) = y</script>,</li>
  <li>We then fit the next decision tree to the residuals of the previous: <script type="math/tex">h_1(x) = y - F_1(x)</script>,</li>
  <li>Add this new tree to our algorithm: <script type="math/tex">F_2(x) = F_1(x) + h_1(x)</script>,</li>
  <li>Fit the next decision tree to the residuals of <script type="math/tex">F_2</script>: <script type="math/tex">h_2(x) = y - F_2(x)</script>,</li>
  <li>Add this new tree to our algorithm: <script type="math/tex">F_3(x) = F_2(x) + h_1(x)</script>,</li>
  <li>Continue this process until some mechanism (i.e. cross validation) tells us to stop.</li>
</ol>

<p>The basic algorithm for boosted regression trees can be generalized to the following where the final model is simply a stagewise additive model of <em>b</em> individual regression trees:</p>

<script type="math/tex; mode=display">f(x) =  \sum^B_{b=1}f^b(x) \tag{1}</script>

<p>To illustrate the behavior, assume the following <em>x</em> and <em>y</em> observations.  The blue sine wave represents the true underlying function and the points represent observations that include some irriducible error (noise).  The boosted prediction illustrates the adjusted predictions after each additional sequential tree is added to the algorithm.  Initially, there are large errors which the boosted algorithm improves upon immediately but as the predictions get closer to the true underlying function you see each additional tree make small improvements in different areas across the feature space where errors remain. Towards the end of the gif, the predicted values nearly converge to the true underlying function.</p>

<center>
<img src="http://uc-r.github.io/public/images/analytics/gbm/boosted_stumps.gif" alt="Fig 2. Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))" width="50%" height="70%" />
<figcaption>Fig 2. Boosted regression tree predictions (courtesy of [Brandon Greenwell](https://github.com/bgreenwell))</figcaption>
</center>
<p><br /></p>

<h3 id="gradient-descent">Gradient descent</h3>

<p>Many algorithms, including decision trees, focus on minimizing the residuals and, therefore, emphasize the MSE loss function.  The algorithm discussed in the previous section outlines the approach of sequentially fitting regression trees to minimize the errors.  This specific approach is how gradient boosting minimizes the mean squared error (MSE) loss function.  However, often we wish to focus on other loss functions such as mean absolute error (MAE) or to be able to apply the method to a classification problem with a loss function such as deviance. The name <strong><em>gradient</em></strong> boosting machines come from the fact that this procedure can be generalized to loss functions other than MSE.</p>

<p>Gradient boosting is considered a <strong><em>gradient descent</em></strong> algorithm. Gradient descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are a downhill skier racing your friend.  A good strategy to beat your friend to the bottom is to take the path with the steepest slope. This is exactly what gradient descent does - it measures the local gradient of the loss (cost) function for a given set of parameters (<script type="math/tex">\Theta</script>) and takes steps in the direction of the descending gradient. Once the gradient is zero, we have reached the minimum.</p>

<center>
<img src="http://uc-r.github.io/public/images/analytics/gbm/gradient_descent.png" alt="Fig 3. Gradient descent (Geron, 2017)." width="50%" height="50%" />
<figcaption>Fig 3. Gradient descent (Geron, 2017).</figcaption>
</center>
<p><br /></p>

<p>Gradient descent can be performed on any loss function that is differentiable.  Consequently, this allows GBMs to optimize different loss functions as desired (see <a href="http://web.stanford.edu/~hastie/ElemStatLearn/">ESL, p. 360</a> for common loss functions). An important parameter in gradient descent is the size of the steps which is determined by the <em>learning rate</em>. If the learning rate is too small, then the algorithm will take many iterations to find the minimum. On the other hand, if the learning rate is too high, you might jump cross the minimum and end up further away than when you started.</p>

<center>
<img src="http://uc-r.github.io/public/images/analytics/gbm/learning_rate_comparison.png" alt="Fig 4. Learning rate comparisons (Geron, 2017)." width="90%" height="70%" />
<figcaption>Fig 4. Learning rate comparisons (Geron, 2017).</figcaption>
</center>
<p><br /></p>

<p>Moreover, not all cost functions are convex (bowl shaped). There may be local minimas, plateaus, and other irregular terrain of the loss function that makes finding the global minimum difficult.  <strong><em>Stochastic gradient descent</em></strong> can help us address this problem by sampling a fraction of the training observations (typically without replacement) and growing the next tree using that subsample.  This makes the algorithm faster but the stochastic nature of random sampling also adds some random nature in descending the loss function gradient.  Although this randomness does not allow the algorithm to find the absolute global minimum,  it can actually help the algorithm jump out of local minima and off plateaus and get near the global minimum.</p>

<center>
<img src="http://uc-r.github.io/public/images/analytics/gbm/stochastic_gradient_descent.png" alt="Fig 5. Stochastic gradient descent (Geron, 2017)." width="35%" height="40%" />
<figcaption>Fig 5. Stochastic gradient descent (Geron, 2017).</figcaption>
</center>
<p><br /></p>

<p>As we’ll see in the next section, there are several hyperparameter tuning options that allow us to address how we approach the gradient descent of our loss function.</p>

<h3 id="tuning">Tuning</h3>

<p>Part of the beauty and challenges of GBM is that they offer several tuning parameters.  The beauty in this is GBMs are highly flexible.  The challenge is that they can be time consuming to tune and find the optimal combination of hyperparamters.  The most common hyperparameters that you will find in most GBM implementations include:</p>

<ul>
  <li><strong>Number of trees:</strong> The total number of trees to fit. GBMs often require many trees; however, unlike random forests GBMs can overfit so the goal is to find the optimal number of trees that minimize the loss function of interest with cross validation.</li>
  <li><strong>Depth of trees:</strong> The number <em>d</em> of splits in each tree, which controls the complexity of the boosted ensemble. Often <script type="math/tex">d = 1</script> works well, in which case each tree is a <em>stump</em> consisting of a single split. More commonly, d is greater than 1 but it is unlikely <script type="math/tex">d > 10</script> will be required.</li>
  <li><strong>Learning rate:</strong> Controls how quickly the algorithm proceeds down the gradient descent. Smaller values reduce the chance of overfitting but also increases the time to find the optimal fit. This is also called <em>shrinkage</em>.</li>
  <li><strong>Subsampling:</strong> Controls whether or not you use a fraction of the available training observations. Using less than 100% of the training observations means you are implementing stochastic gradient descent.  This can help to minimize overfitting and keep from getting stuck in a local minimum or plateau of the loss function gradient.</li>
</ul>

<p>Throughout this tutorial you’ll be exposed to additional hyperparameters that are specific to certain packages and can improve performance and/or the efficiency of training and tuning models.</p>

<h3 id="package-implementation">Package implementation</h3>

<p>There are many packages that implement GBMs and GBM variants. You can find a fairly comprehensive list <a href="https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html#gbm-software-in-r">here</a> and at the <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN Machine Learning Task View</a>. However, the most popular implementations which we will cover in this post include:</p>

<ul>
  <li><a href="https://cran.r-project.org/web/packages/gbm/index.html">gbm</a>: The original R implementation of GBMs</li>
  <li><a href="https://cran.r-project.org/web/packages/xgboost/index.html">xgboost</a>: A fast and efficient gradient boosting framework (C++ backend).</li>
  <li><a href="https://cran.r-project.org/web/packages/gamboostLSS/index.html">h2o</a>: A powerful java-based interface that provides parallel distributed algorithms and efficient productionalization.</li>
</ul>

<h2></h2>
<h2 id="gbm">gbm</h2>

<p>The <a href="https://github.com/gbm-developers/gbm"><code class="highlighter-rouge">gbm</code></a> R package is an implementation of extensions to Freund and Schapire’s <a href="http://www.site.uottawa.ca/~stan/csi5387/boost-tut-ppr.pdf">AdaBoost algorithm</a> and Friedman’s <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">gradient boosting machine</a>. This is the original R implementation of GBM. A presentation is available <a href="https://www.slideshare.net/mark_landry/gbm-package-in-r">here</a> by Mark Landry.</p>

<p>Features include<sup id="fnref:ledell"><a href="#fn:ledell" class="footnote">1</a></sup>:</p>

<ul>
  <li>Stochastic GBM.</li>
  <li>Supports up to 1024 factor levels.</li>
  <li>Supports Classification and regression trees.</li>
  <li>Can incorporate many loss functions.</li>
  <li>Out-of-bag estimator for the optimal number of iterations is provided.</li>
  <li>Easy to overfit since early stopping functionality is not automated in this package.</li>
  <li>If internal cross-validation is used, this can be parallelized to all cores on the machine.</li>
  <li>Currently undergoing a major refactoring &amp; rewrite (and has been for some time).</li>
  <li>GPL-2/3 License.</li>
</ul>

<h3 id="basic-implementation">Basic implementation</h3>

<p><code class="highlighter-rouge">gbm</code> has two primary training functions - <code class="highlighter-rouge">gbm::gbm</code> and <code class="highlighter-rouge">gbm::gbm.fit</code>. The primary difference is that <code class="highlighter-rouge">gbm::gbm</code> uses the formula interface to specify your model whereas <code class="highlighter-rouge">gbm::gbm.fit</code> requires the separated <code class="highlighter-rouge">x</code> and <code class="highlighter-rouge">y</code> matrices.  When working with <em>many</em> variables it is more efficient to use the matrix rather than formula interface.</p>

<p>The default settings in <code class="highlighter-rouge">gbm</code> includes a learning rate (<code class="highlighter-rouge">shrinkage</code>) of 0.001. This is a very small learning rate and typically requires a large number of trees to find the minimum MSE.  However, <code class="highlighter-rouge">gbm</code> uses a default number of trees of 100, which is rarely sufficient.  Consequently, I crank it up to 10,000 trees.  The default depth of each tree (<code class="highlighter-rouge">interaction.depth</code>) is 1, which means we are ensembling a bunch of stumps. Lastly, I also include <code class="highlighter-rouge">cv.folds</code> to perform a 5 fold cross validation.  The model took about 90 seconds to run and the results show that our MSE loss function is minimized with 10,000 trees.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="c1"># train GBM model</span><span class="w">
</span><span class="n">gbm.fit</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sale_Price</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w">
  </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w">
  </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10000</span><span class="p">,</span><span class="w">
  </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
  </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.001</span><span class="p">,</span><span class="w">
  </span><span class="n">cv.folds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">n.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="c1"># will use all cores by default</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">  

</span><span class="c1"># print results</span><span class="w">
</span><span class="n">print</span><span class="p">(</span><span class="n">gbm.fit</span><span class="p">)</span><span class="w">
</span><span class="c1">## gbm(formula = Sale_Price ~ ., distribution = "gaussian", data = ames_train, </span><span class="w">
</span><span class="c1">##     n.trees = 10000, interaction.depth = 1, shrinkage = 0.001, </span><span class="w">
</span><span class="c1">##     cv.folds = 5, verbose = FALSE, n.cores = NULL)</span><span class="w">
</span><span class="c1">## A gradient boosted model with gaussian loss function.</span><span class="w">
</span><span class="c1">## 10000 iterations were performed.</span><span class="w">
</span><span class="c1">## The best cross-validation iteration was 10000.</span><span class="w">
</span><span class="c1">## There were 80 predictors of which 45 had non-zero influence.</span><span class="w">
</span></code></pre></div></div>

<p>The output object is a list containing several modelling and results information.  We can access this information with regular indexing; I recommend you take some time to dig around in the object to get comfortable with its components.  Here, we see that the minimum CV RMSE is 29133 (this means on average our model is about $29,133 off from the actual sales price) but the plot also illustrates that the CV error is still decreasing at 10,000 trees.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get MSE and compute RMSE</span><span class="w">
</span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">gbm.fit</span><span class="o">$</span><span class="n">cv.error</span><span class="p">))</span><span class="w">
</span><span class="c1">## [1] 29133.33</span><span class="w">

</span><span class="c1"># plot loss function as a result of n trees added to the ensemble</span><span class="w">
</span><span class="n">gbm.perf</span><span class="p">(</span><span class="n">gbm.fit</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"cv"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/gbm-basic-results-1.png" style="display: block; margin: auto;" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 10000
</code></pre></div></div>

<p>In this case, the small learning rate is resulting in very small incremental improvements which means <strong><em>many</em></strong> trees are required.  In fact, for the default learning rate and tree depth settings it takes 39,906 trees for the CV error to minimize (~ 5 minutes of run time)!</p>

<h3 id="tuning-1">Tuning</h3>

<p>However, rarely do the default settings suffice.  We could tune parameters one at a time to see how the results change.  For example, here, I increase the learning rate to take larger steps down the gradient descent, reduce the number of trees (since we are reducing the learning rate), and increase the depth of each tree from using a single split to 3 splits. This model takes about 90 seconds to run and achieves a significantly lower RMSE than our initial model with only 1,260 trees.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="c1"># train GBM model</span><span class="w">
</span><span class="n">gbm.fit2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sale_Price</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w">
  </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w">
  </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
  </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w">
  </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">,</span><span class="w">
  </span><span class="n">cv.folds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">n.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="c1"># will use all cores by default</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">  

</span><span class="c1"># find index for n trees with minimum CV error</span><span class="w">
</span><span class="n">min_MSE</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">gbm.fit2</span><span class="o">$</span><span class="n">cv.error</span><span class="p">)</span><span class="w">

</span><span class="c1"># get MSE and compute RMSE</span><span class="w">
</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">gbm.fit2</span><span class="o">$</span><span class="n">cv.error</span><span class="p">[</span><span class="n">min_MSE</span><span class="p">])</span><span class="w">
</span><span class="c1">## [1] 23112.1</span><span class="w">

</span><span class="c1"># plot loss function as a result of n trees added to the ensemble</span><span class="w">
</span><span class="n">gbm.perf</span><span class="p">(</span><span class="n">gbm.fit2</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"cv"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/gbm-tune1-1.png" style="display: block; margin: auto;" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## [1] 1260
</code></pre></div></div>

<p>However, a better option than manually tweaking hyperparameters one at a time is to perform a grid search which iterates over every combination of hyperparameter values and allows us to assess which combination tends to perform well.  To perform a manual grid search, first we want to construct our grid of hyperparameter combinations.  We’re going to search across 81 models with varying learning rates and tree depth.  I also vary the minimum number of observations allowed in the trees terminal nodes (<code class="highlighter-rouge">n.minobsinnode</code>) and introduce stochastic gradient descent by allowing <code class="highlighter-rouge">bag.fraction</code> &lt; 1.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create hyperparameter grid</span><span class="w">
</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">expand.grid</span><span class="p">(</span><span class="w">
  </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.01</span><span class="p">,</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="m">.3</span><span class="p">),</span><span class="w">
  </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w">
  </span><span class="n">n.minobsinnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">15</span><span class="p">),</span><span class="w">
  </span><span class="n">bag.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.65</span><span class="p">,</span><span class="w"> </span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> 
  </span><span class="n">optimal_trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># a place to dump results</span><span class="w">
  </span><span class="n">min_RMSE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">                     </span><span class="c1"># a place to dump results</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># total number of combinations</span><span class="w">
</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 81</span><span class="w">
</span></code></pre></div></div>

<p>We loop through each hyperparameter combination and apply 5,000 trees.  However, to speed up the tuning process, instead of performing 5-fold CV I train on 75% of the training observations and evaluate performance on the remaining 25%. <strong>Important note:</strong> when using <code class="highlighter-rouge">train.fraction</code> it will take the first XX% of the data so its important to randomize your rows in case their is any logic behind the ordering of the data (i.e. ordered by neighborhood).</p>

<p>After about 30 minutes of training time our grid search ends and we see a few important results pop out.  First, our top model has better performance than our previously fitted model above, with the RMSE nearly $3,000 lower. Second, looking at the top 10 models we see that:</p>

<ul>
  <li>none of the top models used a learning rate of 0.3; small incremental steps down the gradient descent appears to work best,</li>
  <li>none of the top models used stumps (<code class="highlighter-rouge">interaction.depth = 1</code>); there are likely stome important interactions that the deeper trees are able to capture,</li>
  <li>adding a stochastic component with <code class="highlighter-rouge">bag.fraction</code> &lt; 1 seems to help; there may be some local minimas in our loss function gradient,</li>
  <li>none of the top models used <code class="highlighter-rouge">n.minobsinnode</code> = 15; the smaller nodes may allow us to capture pockets of unique feature-price point instances,</li>
  <li>in a few instances we appear to use nearly all 5,000 trees; maybe we should increase this parameter in our next search?</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># randomize data</span><span class="w">
</span><span class="n">random_index</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">ames_train</span><span class="p">),</span><span class="w"> </span><span class="n">nrow</span><span class="p">(</span><span class="n">ames_train</span><span class="p">))</span><span class="w">
</span><span class="n">random_ames_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ames_train</span><span class="p">[</span><span class="n">random_index</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">

</span><span class="c1"># grid search </span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  
  </span><span class="c1"># reproducibility</span><span class="w">
  </span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># train model</span><span class="w">
  </span><span class="n">gbm.tune</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="w">
    </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sale_Price</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w">
    </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random_ames_train</span><span class="p">,</span><span class="w">
    </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
    </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">interaction.depth</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">shrinkage</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">n.minobsinnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">n.minobsinnode</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">bag.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">bag.fraction</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">train.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.75</span><span class="p">,</span><span class="w">
    </span><span class="n">n.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="c1"># will use all cores by default</span><span class="w">
    </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># add min training error and trees to grid</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">optimal_trees</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">gbm.tune</span><span class="o">$</span><span class="n">valid.error</span><span class="p">)</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">min_RMSE</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">gbm.tune</span><span class="o">$</span><span class="n">valid.error</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">arrange</span><span class="p">(</span><span class="n">min_RMSE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">head</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="c1">##    shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees  min_RMSE</span><span class="w">
</span><span class="c1">## 1       0.01                 5              5         0.65          3867  16647.87</span><span class="w">
</span><span class="c1">## 2       0.01                 5              5         0.80          4209  16960.78</span><span class="w">
</span><span class="c1">## 3       0.01                 5              5         1.00          4281  17084.29</span><span class="w">
</span><span class="c1">## 4       0.10                 3             10         0.80           489  17093.77</span><span class="w">
</span><span class="c1">## 5       0.01                 3              5         0.80          4777  17121.26</span><span class="w">
</span><span class="c1">## 6       0.01                 3             10         0.80          4919  17139.59</span><span class="w">
</span><span class="c1">## 7       0.01                 3              5         0.65          4997  17139.88</span><span class="w">
</span><span class="c1">## 8       0.01                 5             10         0.80          4123  17162.60</span><span class="w">
</span><span class="c1">## 9       0.01                 5             10         0.65          4850  17247.72</span><span class="w">
</span><span class="c1">## 10      0.01                 3             10         1.00          4794  17353.36</span><span class="w">
</span></code></pre></div></div>

<p>These results help us to zoom into areas where we can refine our search.  Let’s adjust our grid and zoom into closer regions of the values that appear to produce the best results in our previous grid search.  This grid contains 81 combinations that we’ll search across.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># modify hyperparameter grid</span><span class="w">
</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">expand.grid</span><span class="p">(</span><span class="w">
  </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.01</span><span class="p">,</span><span class="w"> </span><span class="m">.05</span><span class="p">,</span><span class="w"> </span><span class="m">.1</span><span class="p">),</span><span class="w">
  </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">),</span><span class="w">
  </span><span class="n">n.minobsinnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w">
  </span><span class="n">bag.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.65</span><span class="p">,</span><span class="w"> </span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> 
  </span><span class="n">optimal_trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># a place to dump results</span><span class="w">
  </span><span class="n">min_RMSE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">                     </span><span class="c1"># a place to dump results</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># total number of combinations</span><span class="w">
</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 81</span><span class="w">
</span></code></pre></div></div>

<p>We can use the same <code class="highlighter-rouge">for</code> loop as before and perform our grid search. We get pretty similar results as before and, actually, our best model is the same as the best model above with an RMSE just above $20K.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># grid search </span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  
  </span><span class="c1"># reproducibility</span><span class="w">
  </span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># train model</span><span class="w">
  </span><span class="n">gbm.tune</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="w">
    </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sale_Price</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w">
    </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">random_ames_train</span><span class="p">,</span><span class="w">
    </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6000</span><span class="p">,</span><span class="w">
    </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">interaction.depth</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">shrinkage</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">n.minobsinnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">n.minobsinnode</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">bag.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">bag.fraction</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">train.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.75</span><span class="p">,</span><span class="w">
    </span><span class="n">n.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="c1"># will use all cores by default</span><span class="w">
    </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># add min training error and trees to grid</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">optimal_trees</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">gbm.tune</span><span class="o">$</span><span class="n">valid.error</span><span class="p">)</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">min_RMSE</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="nf">min</span><span class="p">(</span><span class="n">gbm.tune</span><span class="o">$</span><span class="n">valid.error</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">arrange</span><span class="p">(</span><span class="n">min_RMSE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">head</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="c1">##    n.trees shrinkage interaction.depth n.minobsinnode bag.fraction optimal_trees min_RMSE</span><span class="w">
</span><span class="c1">## 1     6000      0.10                 5              5         0.65           483 20407.76</span><span class="w">
</span><span class="c1">## 2     6000      0.01                 5              7         0.65          4999 20598.62</span><span class="w">
</span><span class="c1">## 3     6000      0.01                 5              5         0.65          4644 20608.75</span><span class="w">
</span><span class="c1">## 4     6000      0.05                 5              7         0.80          1420 20614.77</span><span class="w">
</span><span class="c1">## 5     6000      0.01                 7              7         0.65          4977 20762.26</span><span class="w">
</span><span class="c1">## 6     6000      0.10                 3             10         0.80          1076 20822.23</span><span class="w">
</span><span class="c1">## 7     6000      0.01                 7             10         0.80          4995 20830.03</span><span class="w">
</span><span class="c1">## 8     6000      0.01                 7              5         0.80          4636 20830.18</span><span class="w">
</span><span class="c1">## 9     6000      0.10                 3              7         0.80           949 20839.92</span><span class="w">
</span><span class="c1">## 10    6000      0.01                 5             10         0.65          4980 20840.43</span><span class="w">
</span></code></pre></div></div>

<p>Once we have found our top model we train a model with those specific parameters. And since the model converged at 483 trees I train a cross validated model (to provide a more robust error estimate) with 1000 trees. The cross-validated error of ~$22K is a better representation of the error we might expect on a new unseen data set.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># for reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="c1"># train GBM model</span><span class="w">
</span><span class="n">gbm.fit.final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">formula</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sale_Price</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">,</span><span class="w">
  </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gaussian"</span><span class="p">,</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w">
  </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">483</span><span class="p">,</span><span class="w">
  </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">shrinkage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">,</span><span class="w">
  </span><span class="n">n.minobsinnode</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">bag.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.65</span><span class="p">,</span><span class="w"> 
  </span><span class="n">train.fraction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
  </span><span class="n">n.cores</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="c1"># will use all cores by default</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">  
</span></code></pre></div></div>

<h3 id="visualizing">Visualizing</h3>

<h4 id="variable-importance">Variable importance</h4>

<p>After re-running our final model we likely want to understand the variables that have the largest influence on sale price.  The <code class="highlighter-rouge">summary</code> method for <code class="highlighter-rouge">gbm</code> will output a data frame and a plot that shows the most influential variables.  <code class="highlighter-rouge">cBars</code> allows you to adjust the number of variables to show (in order of influence). The default method for computing variable importance is with relative influence</p>

<ol>
  <li><code class="highlighter-rouge">method = relative.influence</code>: At each split in each tree, <code class="highlighter-rouge">gbm</code> computes the improvement in the split-criterion (MSE for regression). <code class="highlighter-rouge">gbm</code> then averages the improvement made by each variable across all the trees that the variable is used. The variables with the largest average decrease in MSE are considered most important.</li>
  <li><code class="highlighter-rouge">method = permutation.test.gbm</code>: For each tree, the OOB sample is passed down the tree and the prediction accuracy is recorded. Then the values for each variable (one at a time) are randomly permuted and the accuracy is again computed. The decrease in accuracy as a result of this randomly “shaking up” of variable values is averaged over all the trees for each variable. The variables with the largest average decrease in accuracy are considered most important.</li>
</ol>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">par</span><span class="p">(</span><span class="n">mar</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">summary</span><span class="p">(</span><span class="w">
  </span><span class="n">gbm.fit.final</span><span class="p">,</span><span class="w"> 
  </span><span class="n">cBars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">relative.influence</span><span class="p">,</span><span class="w"> </span><span class="c1"># also can use permutation.test.gbm</span><span class="w">
  </span><span class="n">las</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/vip1-1.png" style="display: block; margin: auto;" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>##                                   var      rel.inf
## Overall_Qual             Overall_Qual 4.084734e+01
## Gr_Liv_Area               Gr_Liv_Area 1.323956e+01
## Neighborhood             Neighborhood 1.100911e+01
## Total_Bsmt_SF           Total_Bsmt_SF 5.513300e+00
## Bsmt_Qual                   Bsmt_Qual 5.149919e+00
## First_Flr_SF             First_Flr_SF 3.884696e+00
## Garage_Cars               Garage_Cars 2.354694e+00
## Full_Bath                   Full_Bath 1.953775e+00
## MS_SubClass               MS_SubClass 1.169509e+00
## Kitchen_Qual             Kitchen_Qual 1.137581e+00
...truncated...
</code></pre></div></div>

<p>An alternative approach is to use the underdevelopment <a href="https://github.com/koalaverse/vip"><code class="highlighter-rouge">vip</code></a> package, which provides <code class="highlighter-rouge">ggplot2</code> plots.  <code class="highlighter-rouge">vip</code> also provides an additional measure of variable importance based on partial dependence measures and is a common variable importance plotting framework for many machine learning models.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># devtools::install_github("koalaverse/vip")</span><span class="w">
</span><span class="n">vip</span><span class="o">::</span><span class="n">vip</span><span class="p">(</span><span class="n">gbm.fit.final</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/vip2-1.png" style="display: block; margin: auto;" /></p>

<h4 id="partial-dependence-plots">Partial dependence plots</h4>

<p>After the most relevant variables have been identified, the next step is to attempt to understand how the response variable changes based on these variables. For this we can use partial dependence plots (PDPs) and individual conditional expectation (ICE) curves.</p>

<p>PDPs plot the change in the average predicted value as specified feature(s) vary over their marginal distribution. For example, consider the <code class="highlighter-rouge">Gr_Liv_Area</code> variable. The PDP plot below displays the average change in predicted sales price as we vary <code class="highlighter-rouge">Gr_Liv_Area</code> while holding all other variables constant. This is done by holding all variables constant for each observation in our training data set but then apply the unique values of <code class="highlighter-rouge">Gr_Liv_Area</code> for each observation. We then average the sale price across all the observations. This PDP illustrates how the predicted sales price increases as the square footage of the ground floor in a house increases.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gbm.fit.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area"</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="o">$</span><span class="n">n.trees</span><span class="p">,</span><span class="w"> </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/pdp1-1.png" style="display: block; margin: auto;" /></p>

<p>ICE curves are an extension of PDP plots but, rather than plot the average marginal effect on the response variable, we plot the change in the predicted response variable for each observation as we vary each predictor variable. Below shows the regular ICE curve plot (left) and the centered ICE curves (right). When the curves have a wide range of intercepts and are consequently “stacked” on each other, heterogeneity in the response variable values due to marginal changes in the predictor variable of interest can be difficult to discern. The centered ICE can help draw these inferences out and can highlight any strong heterogeneity in our results. The resuts show that most observations follow a common trend as <code class="highlighter-rouge">Gr_Liv_Area</code> increases; however, the centered ICE plot highlights a few observations that deviate from the common trend.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">ice1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="o">$</span><span class="n">n.trees</span><span class="p">,</span><span class="w"> 
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w">
    </span><span class="n">ice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Non-centered"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w">

</span><span class="n">ice2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="o">$</span><span class="n">n.trees</span><span class="p">,</span><span class="w"> 
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w">
    </span><span class="n">ice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"Centered"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w">

</span><span class="n">gridExtra</span><span class="o">::</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">ice1</span><span class="p">,</span><span class="w"> </span><span class="n">ice2</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/ice-plots-1.png" style="display: block; margin: auto;" /></p>

<h4 id="lime">LIME</h4>

<p>LIME is a newer procedure for understanding why a prediction resulted in a given value for a single observation.  You can read more about LIME <a href="http://uc-r.github.io/lime">here</a>. To use the <code class="highlighter-rouge">lime</code> package on a <code class="highlighter-rouge">gbm</code> model we need to define model type and prediction methods.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_type.gbm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">...</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="s2">"regression"</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">predict_model.gbm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="p">,</span><span class="w"> </span><span class="n">...</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="o">$</span><span class="n">n.trees</span><span class="p">)</span><span class="w">
  </span><span class="nf">return</span><span class="p">(</span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>We can now apply to our two observations.  The results show the predicted value (Case 1: $118K, Case 2: $161K), local model fit (both are relatively poor), and the most influential variables driving the predicted value for each observation.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get a few observations to perform local interpretation on</span><span class="w">
</span><span class="n">local_obs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ames_test</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="p">]</span><span class="w">

</span><span class="c1"># apply LIME</span><span class="w">
</span><span class="n">explainer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lime</span><span class="p">(</span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="p">)</span><span class="w">
</span><span class="n">explanation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">explain</span><span class="p">(</span><span class="n">local_obs</span><span class="p">,</span><span class="w"> </span><span class="n">explainer</span><span class="p">,</span><span class="w"> </span><span class="n">n_features</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">plot_features</span><span class="p">(</span><span class="n">explanation</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/lime-plot-gbm-1.png" style="display: block; margin: auto;" /></p>

<h3 id="predicting">Predicting</h3>

<p>Once you have decided on a final model you will likely want to use the model to predict on new observations.  Like most models, we simply use the <code class="highlighter-rouge">predict</code> function; however, we also need to supply the number of trees to use (see <code class="highlighter-rouge">?predict.gbm</code> for details).  We see that our RMSE for our test set is very close to the RMSE we obtained on our best <code class="highlighter-rouge">gbm</code> model.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># predict values for test data</span><span class="w">
</span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">gbm.fit.final</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">gbm.fit.final</span><span class="o">$</span><span class="n">n.trees</span><span class="p">,</span><span class="w"> </span><span class="n">ames_test</span><span class="p">)</span><span class="w">

</span><span class="c1"># results</span><span class="w">
</span><span class="n">caret</span><span class="o">::</span><span class="n">RMSE</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">ames_test</span><span class="o">$</span><span class="n">Sale_Price</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 20681.88</span><span class="w">
</span></code></pre></div></div>

<h2></h2>
<h2 id="xgboost">xgboost</h2>

<p>The <a href="https://cran.r-project.org/web/packages/xgboost/index.html"><code class="highlighter-rouge">xgboost</code></a> R package provides an R API to “Extreme Gradient Boosting”, which is an efficient implementation of gradient boosting framework (apprx 10x faster than <code class="highlighter-rouge">gbm</code>). The <a href="https://github.com/dmlc/xgboost/tree/master/demo">xgboost/demo</a> repository provides a wealth of information. You can also find a fairly comprehensive parameter tuning guide <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">here</a>. The <code class="highlighter-rouge">xgboost</code> package has been quite popular and successful on <a href="http://blog.kaggle.com/tag/xgboost/">Kaggle</a> for data mining competitions.</p>

<p>Features include:</p>

<ul>
  <li>Provides built-in k-fold cross-validation</li>
  <li>Stochastic GBM with column and row sampling (per split and per tree) for better generalization.</li>
  <li>Includes efficient linear model solver and tree learning algorithms.</li>
  <li>Parallel computation on a single machine.</li>
  <li>Supports various objective functions, including regression, classification and ranking.</li>
  <li>The package is made to be extensible, so that users are also allowed to define their own objectives easily.</li>
  <li>Apache 2.0 License.</li>
</ul>

<h3 id="basic-implementation-1">Basic implementation</h3>

<p>XGBoost only works with matrices that contain all numeric variables; consequently, we need to one hot encode our data.  There are different ways to do this in R (i.e. <code class="highlighter-rouge">Matrix::sparse.model.matrix</code>, <code class="highlighter-rouge">caret::dummyVars</code>) but here we will use the <code class="highlighter-rouge">vtreat</code> package.  <code class="highlighter-rouge">vtreat</code> is a robust package for data prep and helps to eliminate problems caused by missing values, novel categorical levels that appear in future data sets that were not in the training data, etc.  However, <code class="highlighter-rouge">vtreat</code> is not very intuitive.  I will not explain the functionalities but you can find more information <a href="https://arxiv.org/abs/1611.09477">here</a>, <a href="https://www.r-bloggers.com/a-demonstration-of-vtreat-data-preparation/">here</a>, and <a href="https://github.com/WinVector/vtreat">here</a>.</p>

<p>The following applies <code class="highlighter-rouge">vtreat</code> to one-hot encode the training and testing data sets.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># variable names</span><span class="w">
</span><span class="n">features</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">setdiff</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">ames_train</span><span class="p">),</span><span class="w"> </span><span class="s2">"Sale_Price"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Create the treatment plan from the training data</span><span class="w">
</span><span class="n">treatplan</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vtreat</span><span class="o">::</span><span class="n">designTreatmentsZ</span><span class="p">(</span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">features</span><span class="p">,</span><span class="w"> </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span><span class="w">

</span><span class="c1"># Get the "clean" variable names from the scoreFrame</span><span class="w">
</span><span class="n">new_vars</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">treatplan</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">magrittr</span><span class="o">::</span><span class="n">use_series</span><span class="p">(</span><span class="n">scoreFrame</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">        
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">filter</span><span class="p">(</span><span class="n">code</span><span class="w"> </span><span class="o">%in%</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s2">"clean"</span><span class="p">,</span><span class="w"> </span><span class="s2">"lev"</span><span class="p">))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
  </span><span class="n">magrittr</span><span class="o">::</span><span class="n">use_series</span><span class="p">(</span><span class="n">varName</span><span class="p">)</span><span class="w">     

</span><span class="c1"># Prepare the training data</span><span class="w">
</span><span class="n">features_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vtreat</span><span class="o">::</span><span class="n">prepare</span><span class="p">(</span><span class="n">treatplan</span><span class="p">,</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">varRestriction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_vars</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">()</span><span class="w">
</span><span class="n">response_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ames_train</span><span class="o">$</span><span class="n">Sale_Price</span><span class="w">

</span><span class="c1"># Prepare the test data</span><span class="w">
</span><span class="n">features_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vtreat</span><span class="o">::</span><span class="n">prepare</span><span class="p">(</span><span class="n">treatplan</span><span class="p">,</span><span class="w"> </span><span class="n">ames_test</span><span class="p">,</span><span class="w"> </span><span class="n">varRestriction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_vars</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">()</span><span class="w">
</span><span class="n">response_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ames_test</span><span class="o">$</span><span class="n">Sale_Price</span><span class="w">

</span><span class="c1"># dimensions of one-hot encoded data</span><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">features_train</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 2051  208</span><span class="w">
</span><span class="nf">dim</span><span class="p">(</span><span class="n">features_test</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 879 208</span><span class="w">
</span></code></pre></div></div>

<p><code class="highlighter-rouge">xgboost</code> provides different training functions (i.e. <code class="highlighter-rouge">xgb.train</code> which is just a wrapper for <code class="highlighter-rouge">xgboost</code>). However, to train an XGBoost we typically want to use <code class="highlighter-rouge">xgb.cv</code>, which incorporates cross-validation.  The following trains a basic 5-fold cross validated XGBoost model with 1,000 trees.  There are many parameters available in <code class="highlighter-rouge">xgb.cv</code> but the ones you have become more familiar with in this tutorial include the following default values:</p>

<ul>
  <li>learning rate (<code class="highlighter-rouge">eta</code>): 0.3</li>
  <li>tree depth (<code class="highlighter-rouge">max_depth</code>): 6</li>
  <li>minimum node size (<code class="highlighter-rouge">min_child_weight</code>): 1</li>
  <li>percent of training data to sample for each tree (<code class="highlighter-rouge">subsample</code> –&gt; equivalent to <code class="highlighter-rouge">gbm</code>’s <code class="highlighter-rouge">bag.fraction</code>): 100%</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">xgb.fit1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.cv</span><span class="p">(</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w">
  </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response_train</span><span class="p">,</span><span class="w">
  </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
  </span><span class="n">nfold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reg:linear"</span><span class="p">,</span><span class="w">  </span><span class="c1"># for regression models</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">               </span><span class="c1"># silent,</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p>The <code class="highlighter-rouge">xgb.fit1</code> object contains lots of good information. In particular we can assess the <code class="highlighter-rouge">xgb.fit1$evaluation_log</code> to identify the minimum RMSE and the optimal number of trees for both the training data and the cross-validated error. We can see that the training error continues to decrease to 965 trees where the RMSE nearly reaches zero; however, the cross validated error reaches a minimum RMSE of $27,572 with only 60 trees.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get number of trees that minimize error</span><span class="w">
</span><span class="n">xgb.fit1</span><span class="o">$</span><span class="n">evaluation_log</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">summarise</span><span class="p">(</span><span class="w">
    </span><span class="n">ntrees.train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="p">))[</span><span class="m">1</span><span class="p">],</span><span class="w">
    </span><span class="n">rmse.train</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="p">),</span><span class="w">
    </span><span class="n">ntrees.test</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="p">))[</span><span class="m">1</span><span class="p">],</span><span class="w">
    </span><span class="n">rmse.test</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="p">),</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span><span class="c1">##   ntrees.train rmse.train ntrees.test rmse.test</span><span class="w">
</span><span class="c1">## 1          965  0.5022836          60  27572.31</span><span class="w">

</span><span class="c1"># plot error vs number trees</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">xgb.fit1</span><span class="o">$</span><span class="n">evaluation_log</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">train_rmse_mean</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">test_rmse_mean</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/xgb-find-min-error-1.png" style="display: block; margin: auto;" /></p>

<p>A nice feature provided by <code class="highlighter-rouge">xgb.cv</code> is early stopping.  This allows us to tell the function to stop running if the cross validated error does not improve for <em>n</em> continuous trees.  For example, the above model could be re-run with the following where we tell it stop if we see no improvement for 10 consecutive trees.  This feature will help us speed up the tuning process in the next section.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="n">xgb.fit2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.cv</span><span class="p">(</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w">
  </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response_train</span><span class="p">,</span><span class="w">
  </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
  </span><span class="n">nfold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reg:linear"</span><span class="p">,</span><span class="w">  </span><span class="c1"># for regression models</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># silent,</span><span class="w">
  </span><span class="n">early_stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="c1"># stop if no improvement for 10 consecutive trees</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># plot error vs number trees</span><span class="w">
</span><span class="n">ggplot</span><span class="p">(</span><span class="n">xgb.fit2</span><span class="o">$</span><span class="n">evaluation_log</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">train_rmse_mean</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">geom_line</span><span class="p">(</span><span class="n">aes</span><span class="p">(</span><span class="n">iter</span><span class="p">,</span><span class="w"> </span><span class="n">test_rmse_mean</span><span class="p">),</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /></p>

<h3 id="tuning-2">Tuning</h3>

<p>To tune the XGBoost model we pass parameters as a list object to the <code class="highlighter-rouge">params</code> argument.  The most common parameters include:</p>

<ul>
  <li><code class="highlighter-rouge">eta</code>:controls the learning rate</li>
  <li><code class="highlighter-rouge">max_depth</code>: tree depth</li>
  <li><code class="highlighter-rouge">min_child_weight</code>: minimum number of observations required in each terminal node</li>
  <li><code class="highlighter-rouge">subsample</code>: percent of training data to sample for each tree</li>
  <li><code class="highlighter-rouge">colsample_bytrees</code>: percent of columns to sample from for each tree</li>
</ul>

<p>For example, if we wanted to specify specific values for these parameters we would extend the above model with the following parameters.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create parameter list</span><span class="w">
  </span><span class="n">params</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w">
    </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
    </span><span class="n">min_child_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w">
    </span><span class="n">subsample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.8</span><span class="p">,</span><span class="w">
    </span><span class="n">colsample_bytree</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.9</span><span class="w">
  </span><span class="p">)</span><span class="w">

</span><span class="c1"># reproducibility</span><span class="w">
</span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">

</span><span class="c1"># train model</span><span class="w">
</span><span class="n">xgb.fit3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.cv</span><span class="p">(</span><span class="w">
  </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w">
  </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response_train</span><span class="p">,</span><span class="w">
  </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1000</span><span class="p">,</span><span class="w">
  </span><span class="n">nfold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reg:linear"</span><span class="p">,</span><span class="w">  </span><span class="c1"># for regression models</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># silent,</span><span class="w">
  </span><span class="n">early_stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="c1"># stop if no improvement for 10 consecutive trees</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># assess results</span><span class="w">
</span><span class="n">xgb.fit3</span><span class="o">$</span><span class="n">evaluation_log</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">summarise</span><span class="p">(</span><span class="w">
    </span><span class="n">ntrees.train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="p">))[</span><span class="m">1</span><span class="p">],</span><span class="w">
    </span><span class="n">rmse.train</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">train_rmse_mean</span><span class="p">),</span><span class="w">
    </span><span class="n">ntrees.test</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="p">))[</span><span class="m">1</span><span class="p">],</span><span class="w">
    </span><span class="n">rmse.test</span><span class="w">   </span><span class="o">=</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">test_rmse_mean</span><span class="p">),</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span><span class="c1">##   ntrees.train rmse.train ntrees.test rmse.test</span><span class="w">
</span><span class="c1">## 1          180   5891.703         170  24650.17</span><span class="w">
</span></code></pre></div></div>

<p>To perform a large search grid, we can follow the same procedure we did with <code class="highlighter-rouge">gbm</code>.  We create our hyperparameter search grid along with columns to dump our results in.  Here, I create a pretty large search grid consisting of 576 different hyperparameter combinations to model.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create hyperparameter grid</span><span class="w">
</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">expand.grid</span><span class="p">(</span><span class="w">
  </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.01</span><span class="p">,</span><span class="w"> </span><span class="m">.05</span><span class="p">,</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="m">.3</span><span class="p">),</span><span class="w">
  </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">),</span><span class="w">
  </span><span class="n">min_child_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">),</span><span class="w">
  </span><span class="n">subsample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.65</span><span class="p">,</span><span class="w"> </span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w"> 
  </span><span class="n">colsample_bytree</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="m">.9</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
  </span><span class="n">optimal_trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># a place to dump results</span><span class="w">
  </span><span class="n">min_RMSE</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">                     </span><span class="c1"># a place to dump results</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 576</span><span class="w">
</span></code></pre></div></div>

<p>Now I apply the same for loop procedure to loop through and apply a XGBoost model for each hyperparameter combination and dump the results in the <code class="highlighter-rouge">hyper_grid</code> data frame.  <strong>Important note:</strong> if you plan to run this code be prepared to run it before going out to eat or going to bed as it the full search grid took 6 hours to run!</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># grid search </span><span class="w">
</span><span class="k">for</span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">nrow</span><span class="p">(</span><span class="n">hyper_grid</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w">
  
  </span><span class="c1"># create parameter list</span><span class="w">
  </span><span class="n">params</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">eta</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">max_depth</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">min_child_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">min_child_weight</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">subsample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">subsample</span><span class="p">[</span><span class="n">i</span><span class="p">],</span><span class="w">
    </span><span class="n">colsample_bytree</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">colsample_bytree</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># reproducibility</span><span class="w">
  </span><span class="n">set.seed</span><span class="p">(</span><span class="m">123</span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># train model</span><span class="w">
  </span><span class="n">xgb.tune</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.cv</span><span class="p">(</span><span class="w">
    </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w">
    </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w">
    </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response_train</span><span class="p">,</span><span class="w">
    </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
    </span><span class="n">nfold</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
    </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reg:linear"</span><span class="p">,</span><span class="w">  </span><span class="c1"># for regression models</span><span class="w">
    </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">               </span><span class="c1"># silent,</span><span class="w">
    </span><span class="n">early_stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="c1"># stop if no improvement for 10 consecutive trees</span><span class="w">
  </span><span class="p">)</span><span class="w">
  
  </span><span class="c1"># add min training error and trees to grid</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">optimal_trees</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which.min</span><span class="p">(</span><span class="n">xgb.tune</span><span class="o">$</span><span class="n">evaluation_log</span><span class="o">$</span><span class="n">test_rmse_mean</span><span class="p">)</span><span class="w">
  </span><span class="n">hyper_grid</span><span class="o">$</span><span class="n">min_RMSE</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">xgb.tune</span><span class="o">$</span><span class="n">evaluation_log</span><span class="o">$</span><span class="n">test_rmse_mean</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">dplyr</span><span class="o">::</span><span class="n">arrange</span><span class="p">(</span><span class="n">min_RMSE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">head</span><span class="p">(</span><span class="m">10</span><span class="p">)</span><span class="w">
</span><span class="c1">##     eta max_depth min_child_weight subsample colsample_bytree optimal_trees min_RMSE</span><span class="w">
</span><span class="c1">## 1  0.01         5                5      0.65                1          1576 23548.84</span><span class="w">
</span><span class="c1">## 2  0.01         5                3      0.80                1          1626 23587.16</span><span class="w">
</span><span class="c1">## 3  0.01         5                3      0.65                1          1451 23602.96</span><span class="w">
</span><span class="c1">## 4  0.01         5                1      0.65                1          1480 23608.65</span><span class="w">
</span><span class="c1">## 5  0.05         5                3      0.65                1           305 23743.54</span><span class="w">
</span><span class="c1">## 6  0.01         5                1      0.80                1          1851 23772.90</span><span class="w">
</span><span class="c1">## 7  0.05         3                3      0.65                1           552 23783.55</span><span class="w">
</span><span class="c1">## 8  0.01         7                5      0.65                1          1248 23792.65</span><span class="w">
</span><span class="c1">## 9  0.01         3                3      0.80                1          1923 23794.78</span><span class="w">
</span><span class="c1">## 10 0.01         7                1      0.65                1          1070 23800.80</span><span class="w">
</span></code></pre></div></div>

<p>After assessing the results you would likely perform a few more grid searches to hone in on the parameters that appear to influence the model the most.  In fact, <a href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/">here is a link</a> to a great blog post that discusses a strategic approach to tuning with <code class="highlighter-rouge">xgboost</code>. However, for brevity, we’ll just assume the top model in the above search is the globally optimal model.  Once you’ve found the optimal model, we can fit our final model with <code class="highlighter-rouge">xgb.train</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># parameter list</span><span class="w">
</span><span class="n">params</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">eta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">,</span><span class="w">
  </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">min_child_weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">subsample</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.65</span><span class="p">,</span><span class="w">
  </span><span class="n">colsample_bytree</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># train final model</span><span class="w">
</span><span class="n">xgb.fit.final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgboost</span><span class="p">(</span><span class="w">
  </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">params</span><span class="p">,</span><span class="w">
  </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w">
  </span><span class="n">label</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">response_train</span><span class="p">,</span><span class="w">
  </span><span class="n">nrounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1576</span><span class="p">,</span><span class="w">
  </span><span class="n">objective</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"reg:linear"</span><span class="p">,</span><span class="w">
  </span><span class="n">verbose</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="w">
</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<h3 id="visualizing-1">Visualizing</h3>

<h4 id="variable-importance-1">Variable importance</h4>

<p><code class="highlighter-rouge">xgboost</code> provides built-in variable importance plotting.  First, you need to create the importance matrix with <code class="highlighter-rouge">xgb.importance</code> and then feed this matrix into <code class="highlighter-rouge">xgb.plot.importance</code>.  There are 3 variable importance measure:</p>

<ul>
  <li>Gain: the relative contribution of the corresponding feature to the model calculated by taking each feature’s contribution for each tree in the model. This is synonymous with <code class="highlighter-rouge">gbm</code>’s <code class="highlighter-rouge">relative.influence</code>.</li>
  <li>Cover: the relative number of observations related to this feature. For example, if you have 100 observations, 4 features and 3 trees, and suppose feature1 is used to decide the leaf node for 10, 5, and 2 observations in tree1, tree2 and tree3 respectively; then the metric will count cover for this feature as 10+5+2 = 17 observations. This will be calculated for all the 4 features and the cover will be 17 expressed as a percentage for all features’ cover metrics.</li>
  <li>Frequency: the percentage representing the relative number of times a particular feature occurs in the trees of the model. In the above example, if feature1 occurred in 2 splits, 1 split and 3 splits in each of tree1, tree2 and tree3; then the weightage for feature1 will be 2+1+3 = 6. The frequency for feature1 is calculated as its percentage weight over weights of all features.</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create importance matrix</span><span class="w">
</span><span class="n">importance_matrix</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.importance</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">xgb.fit.final</span><span class="p">)</span><span class="w">

</span><span class="c1"># variable importance plot</span><span class="w">
</span><span class="n">xgb.plot.importance</span><span class="p">(</span><span class="n">importance_matrix</span><span class="p">,</span><span class="w"> </span><span class="n">top_n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">measure</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gain"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/xgb-vip-1.png" style="display: block; margin: auto;" /></p>

<h4 id="partial-dependence-plots-1">Partial dependence plots</h4>

<p>PDP and ICE plots work similarly to how we implemented them with <code class="highlighter-rouge">gbm</code>. The only difference is you need to incorporate the training data within the <code class="highlighter-rouge">partial</code> function.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pdp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.fit.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area_clean"</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1576</span><span class="p">,</span><span class="w"> </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"PDP"</span><span class="p">)</span><span class="w">

</span><span class="n">ice</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xgb.fit.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area_clean"</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1576</span><span class="p">,</span><span class="w"> </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w"> </span><span class="n">ice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">features_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"ICE"</span><span class="p">)</span><span class="w">

</span><span class="n">gridExtra</span><span class="o">::</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">pdp</span><span class="p">,</span><span class="w"> </span><span class="n">ice</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/xgb-pdp-ice-1.png" style="display: block; margin: auto;" /></p>

<h4 id="lime-1">LIME</h4>

<p>LIME provides built-in functionality for <code class="highlighter-rouge">xgboost</code> objects (see <code class="highlighter-rouge">?model_type</code>).  However, just keep in mind that the local observations being analyzed need to be one-hot encoded in the same manner as we prepared the training and test data.  Also, when you feed the training data into the <code class="highlighter-rouge">lime::lime</code> function be sure that you coerce it from a matrix to a data frame.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># one-hot encode the local observations to be assessed.</span><span class="w">
</span><span class="n">local_obs_onehot</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vtreat</span><span class="o">::</span><span class="n">prepare</span><span class="p">(</span><span class="n">treatplan</span><span class="p">,</span><span class="w"> </span><span class="n">local_obs</span><span class="p">,</span><span class="w"> </span><span class="n">varRestriction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_vars</span><span class="p">)</span><span class="w">

</span><span class="c1"># apply LIME</span><span class="w">
</span><span class="n">explainer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lime</span><span class="p">(</span><span class="n">data.frame</span><span class="p">(</span><span class="n">features_train</span><span class="p">),</span><span class="w"> </span><span class="n">xgb.fit.final</span><span class="p">)</span><span class="w">
</span><span class="n">explanation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">explain</span><span class="p">(</span><span class="n">local_obs_onehot</span><span class="p">,</span><span class="w"> </span><span class="n">explainer</span><span class="p">,</span><span class="w"> </span><span class="n">n_features</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">plot_features</span><span class="p">(</span><span class="n">explanation</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/xgb-lime-1.png" style="display: block; margin: auto;" /></p>

<h3 id="predicting-1">Predicting</h3>

<p>Lastly, we use <code class="highlighter-rouge">predict</code> to predict on new observations; however, unlike <code class="highlighter-rouge">gbm</code> we do not need to provide the number of trees. Our test set RMSE is only about $600 different than that produced by our <code class="highlighter-rouge">gbm</code> model.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># predict values for test data</span><span class="w">
</span><span class="n">pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">xgb.fit.final</span><span class="p">,</span><span class="w"> </span><span class="n">features_test</span><span class="p">)</span><span class="w">

</span><span class="c1"># results</span><span class="w">
</span><span class="n">caret</span><span class="o">::</span><span class="n">RMSE</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span><span class="w"> </span><span class="n">response_test</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 21319.3</span><span class="w">
</span></code></pre></div></div>

<h2></h2>
<h2 id="h2o">h2o</h2>

<p>The <a href="https://cran.r-project.org/web/packages/h2o/index.html"><code class="highlighter-rouge">h2o</code></a> R package is a powerful and efficient java-based interface that allows for local and cluster-based deployment. It comes with a fairly comprehensive <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/index.html">online resource</a> that includes methodology and code documentation along with tutorials.</p>

<p>Features include:</p>

<ul>
  <li>Distributed and parallelized computation on either a single node or a multi-node cluster.</li>
  <li>Automatic early stopping based on convergence of user-specified metrics to user-specified relative tolerance.</li>
  <li>Stochastic GBM with column and row sampling (per split and per tree) for better generalization.</li>
  <li>Support for exponential families (Poisson, Gamma, Tweedie) and loss functions in addition to binomial (Bernoulli), Gaussian and multinomial distributions, such as Quantile regression (including Laplace).</li>
  <li>Grid search for hyperparameter optimization and model selection.</li>
  <li>Data-distributed, which means the entire dataset does not need to fit into memory on a single node, hence scales to any size training set.</li>
  <li>Uses histogram approximations of continuous variables for speedup.</li>
  <li>Uses dynamic binning - bin limits are reset at each tree level based on the split bins’ min and max values discovered during the last pass.</li>
  <li>Uses squared error to determine optimal splits.</li>
  <li>Distributed implementation details outlined in a <a href="http://blog.h2o.ai/2013/10/building-distributed-gbm-h2o/">blog post</a> by Cliff Click.</li>
  <li>Unlimited factor levels.</li>
  <li>Multiclass trees (one for each class) built in parallel with each other.</li>
  <li>Apache 2.0 Licensed.</li>
  <li>Model export in plain Java code for deployment in production environments.</li>
</ul>

<h3 id="basic-implementation-2">Basic implementation</h3>

<p>Lets go ahead and start up h2o:</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h</span><span class="m">2</span><span class="n">o.no_progress</span><span class="p">()</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.init</span><span class="p">(</span><span class="n">max_mem_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"5g"</span><span class="p">)</span><span class="w">
</span><span class="c1">##  Connection successful!</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## R is connected to the H2O cluster: </span><span class="w">
</span><span class="c1">##     H2O cluster uptime:         1 days 8 hours </span><span class="w">
</span><span class="c1">##     H2O cluster timezone:       America/New_York </span><span class="w">
</span><span class="c1">##     H2O data parsing timezone:  UTC </span><span class="w">
</span><span class="c1">##     H2O cluster version:        3.18.0.11 </span><span class="w">
</span><span class="c1">##     H2O cluster version age:    20 days  </span><span class="w">
</span><span class="c1">##     H2O cluster name:           H2O_started_from_R_bradboehmke_zvs686 </span><span class="w">
</span><span class="c1">##     H2O cluster total nodes:    1 </span><span class="w">
</span><span class="c1">##     H2O cluster total memory:   2.70 GB </span><span class="w">
</span><span class="c1">##     H2O cluster total cores:    4 </span><span class="w">
</span><span class="c1">##     H2O cluster allowed cores:  4 </span><span class="w">
</span><span class="c1">##     H2O cluster healthy:        TRUE </span><span class="w">
</span><span class="c1">##     H2O Connection ip:          localhost </span><span class="w">
</span><span class="c1">##     H2O Connection port:        54321 </span><span class="w">
</span><span class="c1">##     H2O Connection proxy:       NA </span><span class="w">
</span><span class="c1">##     H2O Internal Security:      FALSE </span><span class="w">
</span><span class="c1">##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 </span><span class="w">
</span><span class="c1">##     R Version:                  R version 3.5.0 (2018-04-23)</span><span class="w">
</span></code></pre></div></div>

<p><code class="highlighter-rouge">h2o.gbm</code> allows us to perform a GBM with H2O. However, prior to running our initial model we need to convert our training data to an h2o object. By default, <code class="highlighter-rouge">h2o.gbm</code> applies a GBM model with the following parameters:</p>

<ul>
  <li>number of trees (<code class="highlighter-rouge">ntrees</code>): 50</li>
  <li>learning rate (<code class="highlighter-rouge">learn_rate</code>): 0.1</li>
  <li>tree depth (<code class="highlighter-rouge">max_depth</code>): 5</li>
  <li>minimum observations in a terminal node (<code class="highlighter-rouge">min_rows</code>): 10</li>
  <li>no sampling of observations or columns</li>
</ul>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create feature names</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"Sale_Price"</span><span class="w">
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">setdiff</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">ames_train</span><span class="p">),</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">

</span><span class="c1"># turn training set into h2o object</span><span class="w">
</span><span class="n">train.h2o</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.h2o</span><span class="p">(</span><span class="n">ames_train</span><span class="p">)</span><span class="w">

</span><span class="c1"># training basic GBM model with defaults</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.fit1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w">
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w">
  </span><span class="n">training_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train.h2o</span><span class="p">,</span><span class="w">
  </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># assess model results</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.fit1</span><span class="w">
</span><span class="c1">## Model Details:</span><span class="w">
</span><span class="c1">## ==============</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## H2ORegressionModel: gbm</span><span class="w">
</span><span class="c1">## Model ID:  GBM_model_R_1528813224809_1 </span><span class="w">
</span><span class="c1">## Model Summary: </span><span class="w">
</span><span class="c1">##   number_of_trees number_of_internal_trees model_size_in_bytes min_depth max_depth mean_depth min_leaves max_leaves</span><span class="w">
</span><span class="c1">## 1              50                       50               17360         5         5    5.00000         10         31</span><span class="w">
</span><span class="c1">##   mean_leaves</span><span class="w">
</span><span class="c1">## 1    22.60000</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## H2ORegressionMetrics: gbm</span><span class="w">
</span><span class="c1">## ** Reported on training data. **</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## MSE:  165078993</span><span class="w">
</span><span class="c1">## RMSE:  12848.31</span><span class="w">
</span><span class="c1">## MAE:  9243.007</span><span class="w">
</span><span class="c1">## RMSLE:  0.08504509</span><span class="w">
</span><span class="c1">## Mean Residual Deviance :  165078993</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## H2ORegressionMetrics: gbm</span><span class="w">
</span><span class="c1">## ** Reported on cross-validation data. **</span><span class="w">
</span><span class="c1">## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## MSE:  690559339</span><span class="w">
</span><span class="c1">## RMSE:  26278.5</span><span class="w">
</span><span class="c1">## MAE:  15706.57</span><span class="w">
</span><span class="c1">## RMSLE:  0.1418509</span><span class="w">
</span><span class="c1">## Mean Residual Deviance :  690559339</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## Cross-Validation Metrics Summary: </span><span class="w">
</span><span class="c1">##                               mean          sd  cv_1_valid   cv_2_valid  cv_3_valid  cv_4_valid cv_5_valid</span><span class="w">
</span><span class="c1">## mae                      15693.389   416.34943    15772.27    14608.483   15983.741    16375.13   15727.32</span><span class="w">
</span><span class="c1">## mean_residual_deviance 6.8786246E8 1.0933132E8 6.4370349E8 4.80721376E8 8.2373146E8 9.0298413E8 5.881719E8</span><span class="w">
</span><span class="c1">## mse                    6.8786246E8 1.0933132E8 6.4370349E8 4.80721376E8 8.2373146E8 9.0298413E8 5.881719E8</span><span class="w">
</span><span class="c1">## r2                      0.89403546 0.014769712   0.9059133   0.92479074   0.8778974  0.86524695  0.8963289</span><span class="w">
</span><span class="c1">## residual_deviance      6.8786246E8 1.0933132E8 6.4370349E8 4.80721376E8 8.2373146E8 9.0298413E8 5.881719E8</span><span class="w">
</span><span class="c1">## rmse                      26059.87   2091.1375   25371.312     21925.36   28700.723   30049.693  24252.256</span><span class="w">
</span><span class="c1">## rmsle                   0.13967283 0.015356901  0.13289417  0.110673144  0.13735695  0.13947856 0.17796135</span><span class="w">
</span></code></pre></div></div>

<p>Similar to XGBoost, we can incorporate automated stopping so that we can crank up the number of trees but terminate training once model improvement decreases or stops.  There is also an option to terminate training after so much time has passed (see <code class="highlighter-rouge">max_runtime_secs</code>). In this example, I train a default model with 5,000 trees but stop training after 10 consecutive trees have no improvement on the cross-validated error. In this case, training stops after 3828 trees and has a cross-validated RMSE of $24,684.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># training basic GBM model with defaults</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.fit2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w">
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w">
  </span><span class="n">training_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train.h2o</span><span class="p">,</span><span class="w">
  </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">ntrees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_tolerance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
  </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">123</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># model stopped after xx trees</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.fit2</span><span class="o">@</span><span class="n">parameters</span><span class="o">$</span><span class="n">ntrees</span><span class="w">
</span><span class="c1">## [1] 3828</span><span class="w">

</span><span class="c1"># cross validated RMSE</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.rmse</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.fit2</span><span class="p">,</span><span class="w"> </span><span class="n">xval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 24684.09</span><span class="w">
</span></code></pre></div></div>

<h3 id="tuning-3">Tuning</h3>

<p>H2O provides <strong><em>many</em></strong> parameters that can be adjusted.  It is well worth your time to check out the available documentation at <a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#gbm-tuning-guide">H2O.ai</a>.  For this tutorial, we’ll focus on the more common hyperparameters that I typically apply.  This includes:</p>

<ul>
  <li>Tree complexity:
    <ul>
      <li><code class="highlighter-rouge">ntrees</code>: number of trees to train</li>
      <li><code class="highlighter-rouge">max_depth</code>: depth of each tree</li>
      <li><code class="highlighter-rouge">min_rows</code>: Fewest observations allowed in a terminal node</li>
    </ul>
  </li>
  <li>Learning rate:
    <ul>
      <li><code class="highlighter-rouge">learn_rate</code>: rate to descend the loss function gradient</li>
      <li><code class="highlighter-rouge">learn_rate_annealing</code>: allows you to have a high initial <code class="highlighter-rouge">learn_rate</code>, then gradually reduce as trees are added (speeds up training).</li>
    </ul>
  </li>
  <li>Adding stochastic nature:
    <ul>
      <li><code class="highlighter-rouge">sample_rate</code>: row sample rate per tree</li>
      <li><code class="highlighter-rouge">col_sample_rate</code>: column sample rate per tree (synonymous with <code class="highlighter-rouge">xgboost</code>’s <code class="highlighter-rouge">colsample_bytree</code>)</li>
    </ul>
  </li>
</ul>

<p>Note that there are parameters that control how categorical and continuous variables are encoded, binned, and split.  The defaults tend to perform quite well but I have been able to gain small improvements in certain circumstances by adjusting these.  I will not cover them but they are work reviewing.</p>

<p>To perform grid search tuning with H2O we have two options: perform a full or random discrete grid search.</p>

<h4 id="full-grid-search">Full grid search</h4>

<p>A <strong><em>full cartesian grid search</em></strong> examines every combination of hyperparameter settings that we specify in a tuning grid.  This has been the type of tuning we have been performing with our manual <code class="highlighter-rouge">for</code> loops with <code class="highlighter-rouge">gbm</code> and <code class="highlighter-rouge">xgboost</code>.  However, to speed up training with H2O I’ll use a validation set rather than perform k-fold cross validation.  The following creates a hyperparameter grid consisting of 486 hyperparameter combinations. We apply <code class="highlighter-rouge">h2o.grid</code> to perform a grid search while also incorporating stopping parameters to reduce training time. Total grid search time was about 90 minutes.</p>

<p>A few characteristics pop out when we assess the results - models with trees deeper than one split with a low learning rate, no annealing, and stochastic observation sampling tend to perform best.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create training &amp; validation sets</span><span class="w">
</span><span class="n">split</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.splitFrame</span><span class="p">(</span><span class="n">train.h2o</span><span class="p">,</span><span class="w"> </span><span class="n">ratios</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.75</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">split</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w">
</span><span class="n">valid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">split</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span><span class="w">

</span><span class="c1"># create hyperparameter grid</span><span class="w">
</span><span class="n">hyper_grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">),</span><span class="w">
  </span><span class="n">min_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">),</span><span class="w">
  </span><span class="n">learn_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.01</span><span class="p">,</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="m">0.1</span><span class="p">),</span><span class="w">
  </span><span class="n">learn_rate_annealing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.99</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
  </span><span class="n">sample_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.5</span><span class="p">,</span><span class="w"> </span><span class="m">.75</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">),</span><span class="w">
  </span><span class="n">col_sample_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">.8</span><span class="p">,</span><span class="w"> </span><span class="m">.9</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># perform grid search </span><span class="w">
</span><span class="n">grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.grid</span><span class="p">(</span><span class="w">
  </span><span class="n">algorithm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm"</span><span class="p">,</span><span class="w">
  </span><span class="n">grid_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm_grid1"</span><span class="p">,</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> 
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> 
  </span><span class="n">training_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w">
  </span><span class="n">validation_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">valid</span><span class="p">,</span><span class="w">
  </span><span class="n">hyper_params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="p">,</span><span class="w">
  </span><span class="n">ntrees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_tolerance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
  </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">123</span><span class="w">
  </span><span class="p">)</span><span class="w">

</span><span class="c1"># collect the results and sort by our model performance metric of choice</span><span class="w">
</span><span class="n">grid_perf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.getGrid</span><span class="p">(</span><span class="w">
  </span><span class="n">grid_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm_grid1"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">sort_by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"mse"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">decreasing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span><span class="n">grid_perf</span><span class="w">
</span><span class="c1">## H2O Grid Details</span><span class="w">
</span><span class="c1">## ================</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## Grid ID: gbm_grid1 </span><span class="w">
</span><span class="c1">## Used hyper parameters: </span><span class="w">
</span><span class="c1">##   -  col_sample_rate </span><span class="w">
</span><span class="c1">##   -  learn_rate </span><span class="w">
</span><span class="c1">##   -  learn_rate_annealing </span><span class="w">
</span><span class="c1">##   -  max_depth </span><span class="w">
</span><span class="c1">##   -  min_rows </span><span class="w">
</span><span class="c1">##   -  sample_rate </span><span class="w">
</span><span class="c1">## Number of models: 486 </span><span class="w">
</span><span class="c1">## Number of failed models: 0 </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## Hyper-Parameter Search Summary: ordered by increasing mse</span><span class="w">
</span><span class="c1">##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse</span><span class="w">
</span><span class="c1">## 1             1.0       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_299 3.6209830674536294E8</span><span class="w">
</span><span class="c1">## 2             0.8       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_297 3.6380633209494674E8</span><span class="w">
</span><span class="c1">## 3             0.8       0.01                  1.0         3      1.0         0.5  gbm_grid1_model_27 3.6672773986842275E8</span><span class="w">
</span><span class="c1">## 4             0.8       0.01                  1.0         5      1.0         0.5  gbm_grid1_model_45  3.683498830618852E8</span><span class="w">
</span><span class="c1">## 5             0.9       0.01                  1.0         3     10.0        0.75 gbm_grid1_model_298  3.686060225554216E8</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## ---</span><span class="w">
</span><span class="c1">##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse</span><span class="w">
</span><span class="c1">## 481             0.9       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_433  2.824716768094968E9</span><span class="w">
</span><span class="c1">## 482             0.9       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_325  2.824716768094968E9</span><span class="w">
</span><span class="c1">## 483             0.9       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_379  2.824716768094968E9</span><span class="w">
</span><span class="c1">## 484             1.0       0.01                 0.99         1      5.0         1.0 gbm_grid1_model_380 2.8252384874380198E9</span><span class="w">
</span><span class="c1">## 485             1.0       0.01                 0.99         1      1.0         1.0 gbm_grid1_model_326 2.8252384874380198E9</span><span class="w">
</span><span class="c1">## 486             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid1_model_434 2.8252384874380198E9</span><span class="w">
</span></code></pre></div></div>

<p>We can check out more details of the best performing model. The top model achieves a validation RMSE of $19,029.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Grab the model_id for the top model, chosen by validation error</span><span class="w">
</span><span class="n">best_model_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">grid_perf</span><span class="o">@</span><span class="n">model_ids</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w">
</span><span class="n">best_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.getModel</span><span class="p">(</span><span class="n">best_model_id</span><span class="p">)</span><span class="w">

</span><span class="c1"># Now let’s get performance metrics on the best model</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.performance</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_model</span><span class="p">,</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1">## H2ORegressionMetrics: gbm</span><span class="w">
</span><span class="c1">## ** Reported on validation data. **</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## MSE:  362098307</span><span class="w">
</span><span class="c1">## RMSE:  19028.88</span><span class="w">
</span><span class="c1">## MAE:  12427.99</span><span class="w">
</span><span class="c1">## RMSLE:  0.1403692</span><span class="w">
</span><span class="c1">## Mean Residual Deviance :  362098307</span><span class="w">
</span></code></pre></div></div>

<h4 id="random-discrete-grid-search">Random discrete grid search</h4>

<p>Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, <code class="highlighter-rouge">h2o</code> provides an additional grid search path called <strong><em>“RandomDiscrete”</em></strong>, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model.</p>

<p>The following performs a random discrete grid search using the same hyperparameter grid we used above.  However, in this example we add a search criteria (which is preferred when using a random search) that stops the grid search if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 3600 seconds (60 minutes). In this example, our search went for the entire 60 minutes and evaluated 291 of the 486 potential models.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># random grid search criteria</span><span class="w">
</span><span class="n">search_criteria</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">list</span><span class="p">(</span><span class="w">
  </span><span class="n">strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"RandomDiscrete"</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_metric</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"mse"</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_tolerance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.005</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">max_runtime_secs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">60</span><span class="o">*</span><span class="m">60</span><span class="w">
  </span><span class="p">)</span><span class="w">

</span><span class="c1"># perform grid search </span><span class="w">
</span><span class="n">grid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.grid</span><span class="p">(</span><span class="w">
  </span><span class="n">algorithm</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm"</span><span class="p">,</span><span class="w">
  </span><span class="n">grid_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm_grid2"</span><span class="p">,</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> 
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w"> 
  </span><span class="n">training_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w">
  </span><span class="n">validation_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">valid</span><span class="p">,</span><span class="w">
  </span><span class="n">hyper_params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hyper_grid</span><span class="p">,</span><span class="w">
  </span><span class="n">search_criteria</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">search_criteria</span><span class="p">,</span><span class="w"> </span><span class="c1"># add search criteria</span><span class="w">
  </span><span class="n">ntrees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_tolerance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
  </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">123</span><span class="w">
  </span><span class="p">)</span><span class="w">

</span><span class="c1"># collect the results and sort by our model performance metric of choice</span><span class="w">
</span><span class="n">grid_perf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.getGrid</span><span class="p">(</span><span class="w">
  </span><span class="n">grid_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"gbm_grid2"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">sort_by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"mse"</span><span class="p">,</span><span class="w"> 
  </span><span class="n">decreasing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="w">
  </span><span class="p">)</span><span class="w">
</span><span class="n">grid_perf</span><span class="w">
</span><span class="c1">## H2O Grid Details</span><span class="w">
</span><span class="c1">## ================</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## Grid ID: gbm_grid2 </span><span class="w">
</span><span class="c1">## Used hyper parameters: </span><span class="w">
</span><span class="c1">##   -  col_sample_rate </span><span class="w">
</span><span class="c1">##   -  learn_rate </span><span class="w">
</span><span class="c1">##   -  learn_rate_annealing </span><span class="w">
</span><span class="c1">##   -  max_depth </span><span class="w">
</span><span class="c1">##   -  min_rows </span><span class="w">
</span><span class="c1">##   -  sample_rate </span><span class="w">
</span><span class="c1">## Number of models: 291 </span><span class="w">
</span><span class="c1">## Number of failed models: 0 </span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## Hyper-Parameter Search Summary: ordered by increasing mse</span><span class="w">
</span><span class="c1">##   col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse</span><span class="w">
</span><span class="c1">## 1             0.8       0.05                  1.0         3     10.0         1.0  gbm_grid2_model_74  5.150720254988258E8</span><span class="w">
</span><span class="c1">## 2             0.9       0.01                  1.0         3      5.0         0.5 gbm_grid2_model_146 5.1889115659740096E8</span><span class="w">
</span><span class="c1">## 3             0.9       0.05                  1.0         3      5.0         0.5 gbm_grid2_model_114 5.2062049083883923E8</span><span class="w">
</span><span class="c1">## 4             0.8       0.05                  1.0         3      5.0        0.75  gbm_grid2_model_37 5.2124226584496534E8</span><span class="w">
</span><span class="c1">## 5             0.9       0.05                  1.0         3     10.0         1.0 gbm_grid2_model_157  5.212796449846914E8</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## ---</span><span class="w">
</span><span class="c1">##     col_sample_rate learn_rate learn_rate_annealing max_depth min_rows sample_rate           model_ids                  mse</span><span class="w">
</span><span class="c1">## 286             0.9       0.01                 0.99         1     10.0         1.0 gbm_grid2_model_179  3.323851889022955E9</span><span class="w">
</span><span class="c1">## 287             1.0       0.01                 0.99         1     10.0         1.0 gbm_grid2_model_260 3.3243159009633546E9</span><span class="w">
</span><span class="c1">## 288             0.9       0.01                 0.99         1      5.0         0.5 gbm_grid2_model_199 3.3243216930611935E9</span><span class="w">
</span><span class="c1">## 289             0.8       0.01                 0.99         1     10.0         0.5  gbm_grid2_model_80 3.3244630344508557E9</span><span class="w">
</span><span class="c1">## 290             0.8       0.01                 0.99         1      1.0         0.5  gbm_grid2_model_71 3.3244630344508557E9</span><span class="w">
</span><span class="c1">## 291             0.8       0.01                 0.99         1      5.0         0.5 gbm_grid2_model_227 3.3244630344508557E9</span><span class="w">
</span></code></pre></div></div>

<p>In this example, the best model obtained a cross-validated RMSE of $22,695.  Not quite as good as the full grid search; however, often the results come much closer.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Grab the model_id for the top model, chosen by validation error</span><span class="w">
</span><span class="n">best_model_id</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">grid_perf</span><span class="o">@</span><span class="n">model_ids</span><span class="p">[[</span><span class="m">1</span><span class="p">]]</span><span class="w">
</span><span class="n">best_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.getModel</span><span class="p">(</span><span class="n">best_model_id</span><span class="p">)</span><span class="w">

</span><span class="c1"># Now let’s get performance metrics on the best model</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.performance</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">best_model</span><span class="p">,</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1">## H2ORegressionMetrics: gbm</span><span class="w">
</span><span class="c1">## ** Reported on validation data. **</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## MSE:  515072025</span><span class="w">
</span><span class="c1">## RMSE:  22695.2</span><span class="w">
</span><span class="c1">## MAE:  13841.13</span><span class="w">
</span><span class="c1">## RMSLE:  0.1427291</span><span class="w">
</span><span class="c1">## Mean Residual Deviance :  515072025</span><span class="w">
</span></code></pre></div></div>

<p>Once we’ve found our preferred model, we’ll go ahead and retrain a new model with the full training data.  I’ll use the best model from the full grid search and perform a 5-fold CV to get a robust estimate of the expected error.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># train final model</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.gbm</span><span class="p">(</span><span class="w">
  </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w">
  </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">,</span><span class="w">
  </span><span class="n">training_frame</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train.h2o</span><span class="p">,</span><span class="w">
  </span><span class="n">nfolds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w">
  </span><span class="n">ntrees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10000</span><span class="p">,</span><span class="w">
  </span><span class="n">learn_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.01</span><span class="p">,</span><span class="w">
  </span><span class="n">learn_rate_annealing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
  </span><span class="n">max_depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w">
  </span><span class="n">min_rows</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">sample_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.75</span><span class="p">,</span><span class="w">
  </span><span class="n">col_sample_rate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_rounds</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w">
  </span><span class="n">stopping_tolerance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w">
  </span><span class="n">seed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">123</span><span class="w">
</span><span class="p">)</span><span class="w">

</span><span class="c1"># model stopped after xx trees</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="o">@</span><span class="n">parameters</span><span class="o">$</span><span class="n">ntrees</span><span class="w">
</span><span class="c1">## [1] 9385</span><span class="w">

</span><span class="c1"># cross validated RMSE</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.rmse</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">xval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w">
</span><span class="c1">## [1] 23218.45</span><span class="w">
</span></code></pre></div></div>

<h3 id="visualizing-2">Visualizing</h3>

<h4 id="variable-importance-2">Variable importance</h4>

<p><code class="highlighter-rouge">h2o</code> provides a built function that plots variable importance.  It only has one measure of variable importance, relative importance, which measures the average impact each variable has across all the trees on the loss function. The variable with the largest is most importance and the impact of all other variables are provided relative to the most important variable.  The <code class="highlighter-rouge">vip</code> package also works with <code class="highlighter-rouge">h2o</code> objects to plot variable importance.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h</span><span class="m">2</span><span class="n">o.varimp_plot</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">num_of_features</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">10</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/h2o-vip-1.png" style="display: block; margin: auto;" /></p>

<h4 id="partial-dependence-plots-2">Partial dependence plots</h4>

<p>We can also create similar PDP and ICE plots as before.  We only need to incorporate a specialty function that converts the supplied data to an <code class="highlighter-rouge">h2o</code> object and then formats the predicted output as a data frame.  We feed this into the <code class="highlighter-rouge">partial</code> function and the rest is standard.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pfun</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">object</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
  </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">object</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.h2o</span><span class="p">(</span><span class="n">newdata</span><span class="p">)))[[</span><span class="m">1L</span><span class="p">]]</span><span class="w">
</span><span class="p">}</span><span class="w">

</span><span class="n">pdp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">pred.fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pfun</span><span class="p">,</span><span class="w">
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> 
    </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"PDP"</span><span class="p">)</span><span class="w">

</span><span class="n">ice</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Gr_Liv_Area"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">pred.fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pfun</span><span class="p">,</span><span class="w">
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> 
    </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w">
    </span><span class="n">ice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"ICE"</span><span class="p">)</span><span class="w">

</span><span class="n">gridExtra</span><span class="o">::</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">pdp</span><span class="p">,</span><span class="w"> </span><span class="n">ice</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/h2o-pdp-ice-1.png" style="display: block; margin: auto;" /></p>

<p><code class="highlighter-rouge">h2o</code> does not provide built-in ICE plots but it does provide a PDP plot that plots not only the mean marginal impact (as in a normal PDP) but also one standard error to show the variability.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h</span><span class="m">2</span><span class="n">o.partialPlot</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train.h2o</span><span class="p">,</span><span class="w"> </span><span class="n">cols</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Overall_Qual"</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/h2o-built-in-partial-1.png" style="display: block; margin: auto;" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>## PartialDependence: Partial Dependence Plot of model GBM_model_R_1528826179077_5 on column 'Overall_Qual'
##      Overall_Qual mean_response stddev_response
## 1   Above_Average 173455.738331    59312.490353
## 2         Average 169106.725259    58783.969565
## 3   Below_Average 166036.746826    61278.387041
## 4       Excellent 227580.796151    66397.833661
## 5            Fair 161148.080639    62201.228046
## 6            Good 185388.598046    62306.233255
## 7            Poor 156493.400673    63340.558363
## 8  Very_Excellent 227965.543212    66833.666984
## 9       Very_Good 206703.390125    64790.060632
## 10      Very_Poor 151256.551340    64421.926029
</code></pre></div></div>

<p>Unfortunately, <code class="highlighter-rouge">h2o</code>’s function plots the categorical levels in alphabetical order whereas <code class="highlighter-rouge">pdp</code> will plot them in their specified level order making inference more intuitive.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pdp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Overall_Qual"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">pred.fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pfun</span><span class="p">,</span><span class="w">
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> 
    </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">ames_train</span><span class="p">)</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"PDP"</span><span class="p">)</span><span class="w">

</span><span class="n">ice</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">partial</span><span class="p">(</span><span class="w">
    </span><span class="n">pred.var</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"Overall_Qual"</span><span class="p">,</span><span class="w"> 
    </span><span class="n">pred.fun</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">pfun</span><span class="p">,</span><span class="w">
    </span><span class="n">grid.resolution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> 
    </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">as.data.frame</span><span class="p">(</span><span class="n">ames_train</span><span class="p">),</span><span class="w">
    </span><span class="n">ice</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="w">
    </span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w">
  </span><span class="n">autoplot</span><span class="p">(</span><span class="n">rug</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">train</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">.1</span><span class="p">,</span><span class="w"> </span><span class="n">center</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">scale_y_continuous</span><span class="p">(</span><span class="n">labels</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">scales</span><span class="o">::</span><span class="n">dollar</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w">
  </span><span class="n">ggtitle</span><span class="p">(</span><span class="s2">"ICE"</span><span class="p">)</span><span class="w">

</span><span class="n">gridExtra</span><span class="o">::</span><span class="n">grid.arrange</span><span class="p">(</span><span class="n">pdp</span><span class="p">,</span><span class="w"> </span><span class="n">ice</span><span class="p">,</span><span class="w"> </span><span class="n">nrow</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/h2o-pdp-categorical-1.png" style="display: block; margin: auto;" /></p>

<h4 id="lime-2">LIME</h4>

<p>LIME also provides built-in functionality for <code class="highlighter-rouge">h2o</code> objects (see <code class="highlighter-rouge">?model_type</code>).</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># apply LIME</span><span class="w">
</span><span class="n">explainer</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">lime</span><span class="p">(</span><span class="n">ames_train</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">)</span><span class="w">
</span><span class="n">explanation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">explain</span><span class="p">(</span><span class="n">local_obs</span><span class="p">,</span><span class="w"> </span><span class="n">explainer</span><span class="p">,</span><span class="w"> </span><span class="n">n_features</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span><span class="p">)</span><span class="w">
</span><span class="n">plot_features</span><span class="p">(</span><span class="n">explanation</span><span class="p">)</span><span class="w">
</span></code></pre></div></div>

<p><img src="http://uc-r.github.io/public/images/analytics/gbm/h2o-lime-1.png" style="display: block; margin: auto;" /></p>

<h3 id="predicting-2">Predicting</h3>

<p>Lastly, we use <code class="highlighter-rouge">h2o.predict</code> or <code class="highlighter-rouge">predict</code> to predict on new observations and we can also evaluate the performance of our model on our test set easily with <code class="highlighter-rouge">h2o.performance</code>.  Results are quite similar to both <code class="highlighter-rouge">gmb</code> and <code class="highlighter-rouge">xgboost</code>.</p>

<div class="language-r highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># convert test set to h2o object</span><span class="w">
</span><span class="n">test.h2o</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.h2o</span><span class="p">(</span><span class="n">ames_test</span><span class="p">)</span><span class="w">

</span><span class="c1"># evaluate performance on new data</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.performance</span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test.h2o</span><span class="p">)</span><span class="w">
</span><span class="c1">## H2ORegressionMetrics: gbm</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## MSE:  407532539</span><span class="w">
</span><span class="c1">## RMSE:  20187.44</span><span class="w">
</span><span class="c1">## MAE:  12683.01</span><span class="w">
</span><span class="c1">## RMSLE:  0.100829</span><span class="w">
</span><span class="c1">## Mean Residual Deviance :  407532539</span><span class="w">

</span><span class="c1"># predict with h2o.predict</span><span class="w">
</span><span class="n">h</span><span class="m">2</span><span class="n">o.predict</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test.h2o</span><span class="p">)</span><span class="w">
</span><span class="c1">##    predict</span><span class="w">
</span><span class="c1">## 1 130114.9</span><span class="w">
</span><span class="c1">## 2 162136.7</span><span class="w">
</span><span class="c1">## 3 263438.5</span><span class="w">
</span><span class="c1">## 4 484853.0</span><span class="w">
</span><span class="c1">## 5 219152.9</span><span class="w">
</span><span class="c1">## 6 208616.2</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## [879 rows x 1 column]</span><span class="w">

</span><span class="c1"># predict values with predict</span><span class="w">
</span><span class="n">predict</span><span class="p">(</span><span class="n">h</span><span class="m">2</span><span class="n">o.final</span><span class="p">,</span><span class="w"> </span><span class="n">test.h2o</span><span class="p">)</span><span class="w">
</span><span class="c1">##    predict</span><span class="w">
</span><span class="c1">## 1 130114.9</span><span class="w">
</span><span class="c1">## 2 162136.7</span><span class="w">
</span><span class="c1">## 3 263438.5</span><span class="w">
</span><span class="c1">## 4 484853.0</span><span class="w">
</span><span class="c1">## 5 219152.9</span><span class="w">
</span><span class="c1">## 6 208616.2</span><span class="w">
</span><span class="c1">## </span><span class="w">
</span><span class="c1">## [879 rows x 1 column]</span><span class="w">
</span></code></pre></div></div>

<h2></h2>
<h2 id="learning-more">Learning more</h2>

<p>GBMs are one of the most powerful ensemble algorithms that are often first-in-class with predictive accuracy. Although they are less intuitive and more computationally demanding than many other machine learning algorithms, they are essential to have in your toolbox. To learn more I would start with the following resources:</p>

<p><strong>Traditional book resources:</strong></p>

<ul>
  <li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a></li>
  <li><a href="http://appliedpredictivemodeling.com/">Applied Predictive Modeling</a></li>
  <li><a href="https://www.amazon.com/Computer-Age-Statistical-Inference-Mathematical/dp/1107149894">Computer Age Statistical Inference</a></li>
  <li><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">The Elements of Statistical Learning</a></li>
</ul>

<p><strong>Alternative online resources:</strong></p>

<ul>
  <li><a href="https://koalaverse.github.io/machine-learning-in-R/%20//www.youtube.com/watch?v=wPqtzj5VZus&amp;index=16&amp;list=PLNtMya54qvOFQhSZ4IKKXRbMkyL%20Mn0caa">Trevor Hastie - Gradient Boosting &amp; Random Forests at H2O World 2014</a> (YouTube)</li>
  <li><a href="http://www.slideshare.net/0xdata/gbm-27891077">Trevor Hastie - Data Science of GBM (2013)</a> (slides)</li>
  <li><a href="https://www.youtube.com/watch?v=9wn1f-30_ZY">Mark Landry - Gradient Boosting Method and Random Forest at H2O World 2015</a> (YouTube)</li>
  <li><a href="https://www.youtube.com/watch?v=IXZKgIsZRm0">Peter Prettenhofer - Gradient Boosted Regression Trees in scikit-learn at PyData London 2014</a> (YouTube)</li>
  <li><a href="http://journal.frontiersin.org/article/10.3389/fnbot.2013.00021/full">Alexey Natekin1 and Alois Knoll - Gradient boosting machines, a tutorial</a> (blog post)</li>
</ul>
<br>
<br>
<br>
<br>

<script>
  $(function() {
    var toc = $('#toc');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2,h3,h4').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
