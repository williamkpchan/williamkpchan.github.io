<!DOCTYPE html>
<head>
<meta charset="utf-8">
<title>Nodejs server Tips</title>
<link href="https://fonts.googleapis.com/css?family=Patrick Hand:400" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src='../mainscript.js'></script>

</head>
<style>
body { background-color: black; font-family: 'Patrick Hand', sans-serif; font-size: 24px; color: gray; width: 80%; margin-left: 10%;}

.tip { width: 100%;}
h2 {color: #10C030; }
.tip-number { margin: auto; color: #20A020; text-transform: uppercase; letter-spacing: 0.1rem; font-weight: bolder; font-size: 26px;}
.js-tip, .cssTip, .cssExplain { margin: auto; padding: 2px 2px; font-size: 24px; line-height: 1.5;}
.tip-button { background-color: #003020; outline: none; padding: 5px 5px; display: inline-block; margin: auto; font-size: 1rem; margin-top: 5px; cursor: pointer; font-weight: bolder; border: none; border-radius: 8px; color: #10C030;}
.disabled { background-color: #D8D8D8 !important; color: #888; cursor: not-allowed !important;}
a { text-decoration: none; color: #58D858;}
a:visited { color: #88C898;}
A:hover {  color: yellow;}
A:focus {  color: red;}
code { color: gray; background-color: #000a05; font-size: 18px;}
pre { color: gray; background-color: #000500; font-size: 18px; width:80%; white-space: pre-wrap; background-image:inherit;}
k {color: #205080;}
img{max-width:100%;}
</style>

<body onkeypress="chkKey()">
<span id="dateAndTime" onclick="showDateAndTime()"><script>showDateAndTime();</script></span>
<div class="tip-number">Nodejs server Tips</div>
<pre class="js-tip"> </pre><br>
<button class="tip-button" onclick="forward()">Tips Left: <span class="tip-limit-count"></span></button>

<script>
// List of JavaScript tips
var tipsList = [
'<h2>nodejs server</h2>var http = require(\'http\');\n\nhttp.createServer(function (req, res) {\n    res.writeHead(200, {\'Content-Type\': \'text/html\'});\n    res.end(\'Hello World!\');\n}).listen(8080);',
'<h2>Web Scraping nodejs server</h2><a href="http://stackoverflow.com/questions/6084360/using-node-js-as-a-simple-web-server" target="_blank">Using node.js as a simple web server</a>\nSimplest Node.js server is just:\n$ npm install http-server -g\n\nNow you can run a server via the following commands:\n$ cd MyApp\n$ http-server\n\nIf you\'re using NPM 5.2.0 or newer, you can use http-server without installing it with npx. \nThis isn\'t recommended for use in production but is a great way to quickly get a server running on localhost.\n\n$ npx http-server\n\nOr, you can try this, which opens your web browser and enables CORS requests:\n$ http-server -o --cors\n\nFor more options, check out the documentation for http-server on GitHub, or run:\n$ http-server --help\n\nLots of other nice features and brain-dead-simple deployment to NodeJitsu.\n\nFeature Forks\n\nOf course, you can easily top up the features with your own fork. \nYou might find it\'s already been done in one of the existing 800+ forks of this project:\n\nhttps://github.com/nodeapps/http-server/network\n\nLight Server: An Auto Refreshing Alternative\n\nA nice alternative to http-server is light-server. \nIt supports file watching and auto-refreshing and many other features.\n\n$ npm install -g light-server \n$ light-server\n\nAdd to your directory context menu in Windows Explorer\n\n reg.exe add HKCR\Directory\shell\LightServer\command /ve /t REG_EXPAND_SZ /f /d "\"C:\nodejs\light-server.cmd\" \"-o\" \"-s\" \"%V\""\n\nSimple JSON REST server\n\nIf you need to create a simple REST server for a prototype project then json-server might be what you\'re looking for.\n\nAuto Refreshing Editors\n\nMost web page editors and IDE tools now include a web server that will watch your source files and auto refresh your web page when they change.\nI use Live Server with Visual Studio Code.\nThe open source text editor Brackets also includes a NodeJS static web server. \nJust open any HTML file in Brackets, press "Live Preview" and it starts a static server and opens your browser at the page. \nThe browser will **auto refresh whenever you edit and save the HTML file. \nThis especially useful when testing adaptive web sites. \nOpen your HTML page on multiple browsers/window sizes/devices. \nSave your HTML page and instantly see if your adaptive stuff is working as they all auto refresh.\n\nPhoneGap Developers\n\nIf you\'re coding a hybrid mobile app, you may be interested to know that the PhoneGap team took this auto refresh concept on board with their new PhoneGap App. \nThis is a generic mobile app that can load the HTML5 files from a server during development. \nThis is a very slick trick since now you can skip the slow compile/deploy steps in your development cycle for hybrid mobile apps if you\'re changing JS/CSS/HTML files — which is what you\'re doing most of the time. \nThey also provide the static NodeJS web server (run phonegap serve) that detects file changes.\n\nPhoneGap + Sencha Touch Developers\n\nI\'ve now extensively adapted the PhoneGap static server & PhoneGap Developer App for Sencha Touch & jQuery Mobile developers. \nCheck it out at Sencha Touch Live. \nSupports --qr QR Codes and --localtunnel that proxies your static server from your desktop computer to a URL outside your firewall! Tons of uses. \nMassive speedup for hybrid mobile devs.\n\nCordova + Ionic Framework Developers\n\nLocal server and auto refresh features are baked into the ionic tool. \nJust run ionic serve from your app folder. \nEven better ... \nionic serve --lab to view auto-refreshing side by side views of both iOS and Android.\n\n<a href="http://hackprogramming.com/web-scraping-in-node-js-with-multiple-examples/" target="_blank">Web Scraping In Node Js With Multiple Examples</a>\n<a href="https://scotch.io/tutorials/scraping-the-web-with-node-js" target="_blank">Scraping the Web With Node.js</a>\n<a href="https://codeburst.io/an-introduction-to-web-scraping-with-node-js-1045b55c63f7" target="_blank">An Introduction to Web Scraping with Node JS</a>',
'<h3>What will we need?</h3>\nFor this project we’ll be using <a href="https://nodejs.org/en/" target="_blank">Node.js</a>. \n\nWe’ll also be using two open-sourced <a href="https://www.npmjs.com/" target="_blank"><i>npm</i></a><i> </i>modules to make today’s task a little easier:\n<a href="https://github.com/request/request-promise" target="_blank"><i>request-promise </i></a>— Request is a simple HTTP client that allows us to make quick and easy HTTP calls.\n<a href="https://github.com/cheeriojs/cheerio" target="_blank"><i>cheerio</i></a> — jQuery for Node.js. \nCheerio makes it easy to select, edit, and view DOM elements.',
'<h3>Project Setup.</h3>Create a new project folder. \nWithin that folder create an <code>index.js</code> file. \nWe’ll need to install and require our dependencies. \nOpen up your command line, and install and save:<i> request, request-promise, and cheerio</i>\nnpm install --save request request-promise cheerio\nThen require them in our <code>index.js</code> file:\nconst rp = require("request-promise");\nconst cheerio = require("cheerio");',
'<h3>Setting up the Request</h3><code>request-promise</code> accepts an object as input, and returns a promise. \nThe <code>options</code> object needs to do two things:\nPass in the url we want to scrape.\nTell Cheerio to load the returned HTML so that we can use it.\n\nHere’s what that looks like:\nconst options = {\n  uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`" target="_blank">https://www.yourURLhere.com`</a>,\n  transform: function (body) { return cheerio.load(body); }\n};\nThe <code>uri</code> key is simply the website we want to scrape.\nThe <code>transform</code> key tells <code>request-promise</code> to take the returned body and load it into Cheerio before returning it to us.\nAwesome. \nWe’ve successfully set up our HTTP request options! Here’s what your code should look like so far:\nconst rp = require("request-promise");\nconst cheerio = require("cheerio");const options = {\n  uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`" target="_blank">https://www.yourURLhere.com`</a>,\n  transform: function (body) { return cheerio.load(body); }\n};',
'<h3>Make the Request</h3>Now that the options are taken care of, we can actually make our request. \nThe boilerplate in the documentation for that looks like this:\nrp(<i>OPTIONS</i>)\n    .then(function (data) {\n        // <i>REQUEST SUCCEEDED:</i> <i>DO SOMETHING</i>\n    })\n    .catch(function (err) {\n        // <i>REQUEST FAILED: ERROR OF SOME KIND</i>\n    });\nWe pass in our <code>options</code> object to <code>request-promise</code>, then wait to see if our request succeeds or fails. \nEither way, we do something with the returned data.\nKnowing what the documentation says to do, lets create our own version:\nrp(options)\n  .then(($) => { console.log($); })\n  .catch((err) => { console.log(err); });\nThe code is pretty similar. \nThe big difference is I’ve used arrow functions. \nI’ve also logged out the returned data from our HTTP request. \nWe’re going to test to make sure everything is working so far.\nReplace the placeholder <code>uri</code> with the website you want to scrape. \nThen, open up your console and type:\n<i>node index.js</i>// LOGS THE FOLLOWING:\n{ [Function: initialize]\n  fn:\n   initialize {\n     constructor: [Circular],\n     _originalRoot:\n      { type: "root",\n        name: "root",\n        namespace: "<a href="http://www.w3.org/1999/xhtml"" target="_blank">http://www.w3.org/1999/xhtml"</a>,\n        attribs: {},\n        ...\nIf you don’t see an error, then everything is working so far — and you just made your first scrape!\n\nHere is the full code of our boilerplate:\nconst rp = require(\'request-promise\');\nconst cheerio = require(\'cheerio\');\nconst options = {\n  uri: `https://www.google.com`,\n  transform: function (body) { return cheerio.load(body); }\n};\n\nrp(options)\n  .then(($) => {\n    console.log($);\n  })\n  .catch((err) => {\n    console.log(err);\n  });\n\nBoilerplate web scraping code',
'<h2>Selectors</h2>First and foremost, Cheerio’s selector implementation is nearly identical to jQuery’s. \nSo if you know jQuery, this will be a breeze. \nThe selector method allows you to traverse and select elements in the document. \nYou can get data and set data using a selector. \nImagine we have the following HTML in the website we want to scrape:\n<i>&lt;ul id="cities">\n  &lt;li class="large">New York&lt;/li>\n  &lt;li id="medium">Portland&lt;/li>\n  &lt;li class="small">Salem&lt;/li>\n&lt;/ul></i>\nWe can select id’s using (<code>#</code>), classes using (<code>.</code>), and elements by their tag names, ex: <code>div</code>.\n$(".large").text() // New York\n$("#medium").text() // Portland\n$("li[class=small]").html() // &lt;li class="small">Salem&lt;/li>',
'<h2>Looping</h2>Just like jQuery, we can also iterate through multiple elements with the <k>each()</k> function. \nUsing the same HTML code as above, we can return the inner text of each <code>li</code> with the following code:\n$("li").each(function(i, elem) {\n  cities[i] = $(this).text();\n});// New York Portland Salem',
'<h2>Finding</h2><k>find()</k>\nImagine we have two lists on our web site:\n<i>&lt;ul id="cities">\n  &lt;li class="large">New York&lt;/li>\n  &lt;li id="c-medium">Portland&lt;/li>\n  &lt;li class="small">Salem&lt;/li>\n&lt;/ul>\n&lt;ul id="towns">\n  &lt;li class="large">Bend&lt;/li>\n  &lt;li id="t-medium">Hood River&lt;/li>\n  &lt;li class="small">Madras&lt;/li>\n&lt;/ul></i>\nWe can select each list using their respective ID’s, then find the <em>small </em>city/town within each list:\n$("#cities").find(".small").text() // Salem\n$("#towns").find(".small").text() // Madras\n<blockquote>Finding will search all descendant DOM elements, not just immediate children as shown in this example.</blockquote>',
'<h2>Children</h2><k>.children()</k>\nChildren is similar to find. \nThe difference is that children <i>only </i>searches for immediate children of the selected element.\n$("#cities").children("#c-medium").text();\n// Portland',
'<h2>Text & HTML</h2>Up until this point, all of my examples have included the <k>.text()</k> function. \nHopefully you’ve been able to figure out that this function is what gets the text of the selected element. \nYou can also use <k>.html()</k> to return the html of the given element:\n$(".large")<i>.text()</i> // Bend\n$(".large")<i>.html()</i> // &lt;li class="large">Bend&lt;/li>\n<h3>Additional Methods</h3>There are more methods than I can count, and the documentation for all of them is available <a href="https://github.com/cheeriojs/cheerio" target="_blank"><i>here</i></a>.',
'<h3>Chrome Developer Tools</h3>Don’t forget, the Chrome Developer Tools are your friend. \nIn Google Chrome, you can easily find element, class, and ID names using: <i>CTRL + SHIFT + F</i>',
'<h3>Limitations</h3>MOST websites modify the DOM using JavaScript. \nUnfortunately Cheerio doesn’t resolve parsing a modified DOM. \nDynamically generated content from procedures leveraging AJAX, client-side logic, and other async procedures are not available to Cheerio.\nRemember this is an introduction to basic scraping. \nIn order to get started you’ll need to find a static website with minimal DOM manipulation.\n\n<a href="https://www.freecodecamp.org/news/the-ultimate-guide-to-web-scraping-with-node-js-daa2027dcd3/" target="_blank">The Ultimate Guide to Web Scraping with Node.js</a>\n<a href="https://stackabuse.com/web-scraping-with-node-js/" target="_blank">Web Scraping with Node.js</a>\n<a href="https://levelup.gitconnected.com/web-scraping-with-node-js-c93dcf76fe2b" target="_blank">Web Scraping with Node.js</a>\n<a href="https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27" target="_blank">Debugging Node.js with Chrome DevTools</a>\n<a href="https://medium.com/the-node-js-collection/debugging-node-js-with-google-chrome-4965b5f910f4" target="_blank">Debugging Node.js with Google Chrome</a>\n<a href="https://www.google.com/search?q=chrome+node.js&oq=chrome+node.js&aqs=chrome..69i57.5657j0j7&sourceid=chrome&ie=UTF-8" target="_blank">chrome node.js</a>',
'<h2>web scraping</h2>you might want to collect prices from various e-commerce sites for a price comparison site.\nOr perhaps you need flight times and hotel/AirBNB listings for a travel site.\nMaybe you want to collect emails from various directories for sales leads, or use data from the internet to train machine learning/AI models.\nOr you could even be wanting to build a search engine like Google!\nGetting started with web scraping is easy, and the process can be broken down into two main parts:\n\nacquiring the data using an HTML request library or a headless browser,\nand parsing the data to get the exact information you want.\nThis guide will walk you through the process with the popular Node.js request-promise module, CheerioJS, and Puppeteer.\n\nWe will be gathering a list of all the names and birthdays of U.S.presidents from Wikipedia and the titles of all the posts on the front page of Reddit.\nFirst things first: Let’s install the libraries we’ll be using in this guide (Puppeteer will take a while to install as it needs to download Chromium as well).',
'<h3>Making your first request</h3>\nnpm install --save request request-promise cheerio puppeteer\n\nNext, let’s open a new text file (name the file potusScraper.js), and write a quick function to get the HTML of the Wikipedia “List of Presidents” page.\nconst rp = require(\'request-promise\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log(html);\n  })\n  .catch(function(err){\n    //handle error\n  });',
'<h3>Using Chrome DevTools</h3>Cool, we got the raw HTML from the web page! But now we need to make sense of this giant blob of text.\nTo do that, we’ll need to use Chrome DevTools to allow us to easily search through the HTML of a web page.\nUsing Chrome DevTools is easy: simply open Google Chrome, and right click on the element you would like to scrape (in this case I am right clicking on George Washington, because we want to get links to all of the individual presidents’ Wikipedia pages):\n\nNow, simply click inspect, and Chrome will bring up its DevTools pane, allowing you to easily inspect the page’s source HTML.',
'<h3>Parsing HTML with Cheerio.js</h3>Let’s use Cheerio.js to parse the HTML we received earlier to return a list of links to the individual Wikipedia pages of U.S.presidents.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log($(\'big > a\', html).length);\n    console.log($(\'big > a\', html));\n  })\n  .catch(function(err){\n    //handle error\n  });\n\nOutput:\n{ \'0\':\n  { type: \'tag\',\n    name: \'a\',\n    attribs: { href: \'/wiki/George_Washington\', title: \'George Washington\' },\n    children: [ [Object] ],\n    next: null,\n    prev: null,\n    parent:\n      { type: \'tag\',\n        name: \'big\',\n        attribs: {},\n        children: [Array],\n        next: null,\n        prev: null,\n        parent: [Object] } },\n  \'1\':\n    { type: \'tag\'\n  ...\n\nWe check to make sure there are exactly 45 elements returned (the number of U.S.presidents), meaning there aren’t any extra hidden “big” tags elsewhere on the page.\nNow, we can go through and grab a list of links to all 45 presidential Wikipedia pages by getting them from the “attribs” section of each element.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    const wikiUrls = [];\n    for (let i = 0; i < 45; i++) {\n      wikiUrls.push($(\'big > a\', html)[i].attribs.href);\n    }\n    console.log(wikiUrls);\n  })\n  .catch(function(err){\n    //handle error\n  });\n\nNow we have a list of all 45 presidential Wikipedia pages.\nLet’s create a new file (named potusParse.js), which will contain a function to take a presidential Wikipedia page and return the president’s name and birthday.\nFirst things first, let’s get the raw HTML from George Washington’s Wikipedia page.\n\nconst rp = require(\'request-promise\');\nconst url = \'https://en.wikipedia.org/wiki/George_Washington\';\n\nrp(url)\n  .then(function(html) {\n    console.log(html);\n  })\n  .catch(function(err) {\n    //handle error\n  });\nLet’s once again use Chrome DevTools to find the syntax of the code we want to parse, so that we can extract the name and birthday with Cheerio.js.\n\nSo we see that the name is in a class called “firstHeading” and the birthday is in a class called “bday”.\nLet’s modify our code to use Cheerio.js to extract these two classes.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst url = \'https://en.wikipedia.org/wiki/George_Washington\';\n\nrp(url)\n  .then(function(html) {\n    console.log($(\'.firstHeading\', html).text());\n    console.log($(\'.bday\', html).text());\n  })\n  .catch(function(err) {\n    //handle error\n  });',
'<h3>Putting it all together</h3>Perfect! Now let’s wrap this up into a function and export it from this module.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\n\nconst potusParse = function(url) {\n  return rp(url)\n    .then(function(html) {\n      return {\n        name: $(\'.firstHeading\', html).text(),\n        birthday: $(\'.bday\', html).text(),\n      };\n    })\n    .catch(function(err) {\n      //handle error\n    });\n};\n\nmodule.exports = potusParse;\n\nNow let’s return to our original file potusScraper.js and require the potusParse.js module.\nWe’ll then apply it to the list of wikiUrls we gathered earlier.\n\nconst rp = require(\'request-promise\');\nconst $ = require(\'cheerio\');\nconst potusParse = require(\'./potusParse\');\nconst url = \'https://en.wikipedia.org/wiki/List_of_Presidents_of_the_United_States\';\n\nrp(url)\n  .then(function(html) {\n    //success!\n    const wikiUrls = [];\n    for (let i = 0; i < 45; i++) {\n      wikiUrls.push($(\'big > a\', html)[i].attribs.href);\n    }\n    return Promise.all(\n      wikiUrls.map(function(url) {\n        return potusParse(\'https://en.wikipedia.org\' + url);\n      })\n    );\n  })\n  .then(function(presidents) {\n    console.log(presidents);\n  })\n  .catch(function(err) {\n    //handle error\n    console.log(err);\n  });',
'<h3>Rendering JavaScript Pages</h3>Voilà! A list of the names and birthdays of all 45 U.S.presidents.\nUsing just the request-promise module and Cheerio.js should allow you to scrape the vast majority of sites on the internet.\nRecently, however, many sites have begun using JavaScript to generate dynamic content on their websites.\nThis causes a problem for request-promise and other similar HTTP request libraries (such as axios and fetch), because they only get the response from the initial request, but they cannot execute the JavaScript the way a web browser can.\nThus, to scrape sites that require JavaScript execution, we need another solution.\nIn our next example, we will get the titles for all of the posts on the front page of Reddit.\nLet’s see what happens when we try to use request-promise as we did in the previous example.\nOutput:\n\nconst rp = require(\'request-promise\');\nconst url = \'https://www.reddit.com\';\n\nrp(url)\n  .then(function(html){\n    //success!\n    console.log(html);\n  })\n  .catch(function(err){\n    //handle error\n  });\n}\n\nHmmm…not quite what we want.\nThat’s because getting the actual content requires you to run the JavaScript on the page! With Puppeteer, that’s no problem.\nPuppeteer is an extremely popular new module brought to you by the Google Chrome team that allows you to control a headless browser.\nThis is perfect for programmatically scraping pages that require JavaScript execution.\nLet’s get the HTML from the front page of Reddit using Puppeteer instead of request-promise.\n\nconst puppeteer = require(\'puppeteer\');\nconst url = \'https://www.reddit.com\';\n\npuppeteer\n  .launch()\n  .then(function(browser) {\n    return browser.newPage();\n  })\n  .then(function(page) {\n    return page.goto(url).then(function() {\n      return page.content();\n    });\n  })\n  .then(function(html) {\n    console.log(html);\n  })\n  .catch(function(err) {\n    //handle error\n  });\n\nNice! The page is filled with the correct content!\nNow we can use Chrome DevTools like we did in the previous example.\nIt looks like Reddit is putting the titles inside “h2” tags.\nLet’s use Cheerio.js to extract the h2 tags from the page.\n\nconst puppeteer = require(\'puppeteer\');\nconst $ = require(\'cheerio\');\nconst url = \'https://www.reddit.com\';\n\npuppeteer\n  .launch()\n  .then(function(browser) {\n    return browser.newPage();\n  })\n  .then(function(page) {\n    return page.goto(url).then(function() {\n      return page.content();\n    });\n  })\n  .then(function(html) {\n    $(\'h2\', html).each(function() {\n      console.log($(this).text());\n    });\n  })\n  .catch(function(err) {\n    //handle error\n  });',
'<h3>Additional Resources</h3>And there’s the list! At this point you should feel comfortable writing your first web scraper to gather data from any website.\nHere are a few additional resources that you may find helpful during your web scraping journey:\n\n<a href="https://www.scraperapi.com/blog/the-10-best-rotating-proxy-services-for-web-scraping" rel="noopener" target="_blank">List of web scraping proxy services</a>\n<a href="https://www.scraperapi.com/blog/the-10-best-web-scraping-tools" rel="noopener" target="_blank">List of handy web scraping tools</a>\n<a href="https://www.scraperapi.com/blog/5-tips-for-web-scraping" target="_blank">List of web scraping tips</a>\n',
'<h3>Introduction</h3>\nWeb scraping is the process of automating data collection from the web. The process typically deploys a “crawler” that automatically surfs the web and scrapes data from selected pages. There are many reasons why you might want to scrape data. Primarily, it makes data collection much faster by eliminating the manual data-gathering process. Scraping is also a solution when data collection is desired or needed but the website does not provide an API.\nIn this tutorial, you will build a web scraping application using <a href="https://www.npmjs.com/">Node.js</a> and <a href="https://pptr.dev/">Puppeteer</a>. Your app will grow in complexity as you progress. First, you will code your app to open <a href="https://www.chromium.org/getting-involved/download-chromium">Chromium</a> and load a special website designed as a web-scraping sandbox: <a href="http://books.toscrape.com">books.toscrape.com</a>. In the next two steps, you will scrape all the books on a single page of books.toscrape and then all the books across multiple pages. In the remaining steps, you will filter your scraping by book category and then save your data as a JSON file.\n<i>Warning:</i> The ethics and legality of web scraping are very complex and constantly evolving. They also differ based on your location, the data’s location, and the website in question. This tutorial scrapes a special website, <a href="http://books.toscrape.com">books.toscrape.com</a>, which was specifically designed to test scraper applications. <i>Scraping any other domain falls outside the scope of this tutorial.</i>\n',
'<h2>Prerequisites</h2>Node.js installed on your development machine. This tutorial was tested on Node.js version 12.18.3 and npm version 6.14.6. <a href="https://www.digitalocean.com/community/tutorials/how-to-install-node-js-and-create-a-local-development-environment-on-macos">You can follow this guide to install Node.js on macOS or Ubuntu 18.04</a>, or you can <a href="https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-ubuntu-18-04">follow this guide to install Node.js on Ubuntu 18.04 using a PPA</a>.\n',
'<h2>Step 1 — Setting Up the Web Scraper</h2>With Node.js installed, you can begin setting up your web scraper. First, you will create a project root directory and then install the required dependencies. This tutorial requires just one dependency, and you will install it using Node.js’s default package manager <a href="https://www.digitalocean.com/community/tutorial_series/how-to-code-in-node-js">npm</a>. npm comes preinstalled with Node.js, so you don’t need to install it.\nCreate a folder for this project and then move inside:\n<code>mkdir book-scraper\ncd book-scraper</code>\nYou will run all subsequent commands from this directory.\nWe need to install one package using npm, or the node package manager. First initialize npm in order to create a <code>packages.json</code> file, which will manage your project’s dependencies and metadata.\nInitialize npm for your project:\n<code>npm init</code>\nnpm will present a sequence of prompts. You can press <code>ENTER</code> to every prompt, or you can add personalized descriptions. Make sure to press <code>ENTER</code> and leave the default values in place when prompted for <code>entry point:</code> and <code>test command:</code>. Alternately, you can pass the <code>y</code> flag to <code>npm</code>—<code>npm init -y</code>—and it will submit all the default values for you.\nYour output will look something like this:\n<code>Output{\n  "name": "<k>sammy_scraper</k>",\n  "version": "<k>1.0.0</k>",\n  "description": "<k>a web scraper</k>",\n  "main": "index.js",\n  "scripts": {\n    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"\n  },\n  "keywords": [],\n  "author": "<k>sammy the shark</k>",\n  "license": "<k>ISC</k>"\n}\n<k>Is this OK? (yes) yes</k></code>\nType <code>yes</code> and press <code>ENTER</code>. npm will save this output as your <code>package.json</code> file.\nNow use npm to install Puppeteer:\n<code>npm install --save puppeteer</code>\nThis command installs both Puppeteer and a version of Chromium that the Puppeteer team knows will work with their API.\nOn Linux machines, Puppeteer might require some additional dependencies.\nIf you are using Ubuntu 18.04, <a href="https://github.com/puppeteer/puppeteer/blob/main/docs/troubleshooting.md#chrome-headless-doesnt-launch-on-unix">check the ‘Debian Dependencies’ dropdown inside the ‘Chrome headless doesn’t launch on UNIX’ section of Puppeteer’s troubleshooting docs</a>. You can use the following command to help find any missing dependencies:\n<code>ldd chrome | grep not</code>\nWith npm, Puppeteer, and any additional dependencies installed, your <code>package.json</code> file requires one last configuration before you start coding. In this tutorial, you will launch your app from the command line with <code>npm run start</code>. You must add some information about this <code>start</code> script to <code>package.json</code>. Specifically, you must add one line under the <code>scripts</code> directive regarding your <code>start</code> command.\nOpen the file in your preferred text editor:\n<code>nano package.json</code>\nFind the <code>scripts:</code> section and add the following configurations. Remember to place a comma at the end of the <code>test</code> script line, or your file will not parse correctly.\n<code>Output{\n  . . .\n  "scripts": {\n    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"<k>,</k>\n  <k>"start": "node index.js"</k>\n  },\n  . . .\n  "dependencies": {\n    "puppeteer": "^5.2.1"\n  }\n}</code>\nYou will also notice that <code>puppeteer</code> now appears under <code>dependencies</code> near the end of the file. Your <code>package.json</code> file will not require any more revisions. Save your changes and close your editor.\nYou are now ready to start coding your scraper. In the next step, you will set up a browser instance and test your scraper’s basic functionality.\n',
'<h2>Step 2 — Setting Up the Browser Instance</h2>When you open a traditional browser, you can do things like click buttons, navigate with your mouse, type, open the dev tools, and more. A headless browser like Chromium allows you to do these same things, but programmatically and without a user interface. In this step, you will set up your scraper’s browser instance. When you launch your application, it will automatically open Chromium and navigate to <a href="http://books.toscrape.com">books.toscrape.com</a>. These initial actions will form the basis of your program.\nYour web scraper will require four <code>.js</code> files: <code>browser.js</code>, <code>index,js</code>, <code>pageController.js</code>, and <code>pageScraper.js</code>. In this step, you will create all four files and then continually update them as your program grows in sophistication. Start with <code>browser.js</code>; this file will contain the script that starts your browser.\nFrom your project’s root directory, create and open <code>browser.js</code> in a text editor:\n<code>nano browser.js</code>\nFirst, you will <code>require</code> Puppeteer and then create an <code>async</code> function called <code>startBrowser()</code>. This function will start the browser and return an instance of it. Add the following code:\n./book-scraper/browser.js\n<code>const puppeteer = require(\'puppeteer\');\nasync function startBrowser(){\n  let browser;\n  try {\n      console.log("Opening the browser......");\n      browser = await puppeteer.launch({\n          headless: false,\n          args: ["--disable-setuid-sandbox"],\n          \'ignoreHTTPSErrors\': true\n      });\n  } catch (err) {\n      console.log("Could not create a browser instance =&gt; : ", err);\n  }\n  return browser;\n}\nmodule.exports = {\n  startBrowser\n};</code>\n<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#puppeteerlaunchoptions">Puppeteer has a <code>.launch()</code> method</a> that launches an instance of a browser. This method returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>, so you have to <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Async_await">make sure the Promise resolves by using a <code>.then</code> or <code>await</code> block</a>.\nYou are using <code>await</code> to make sure the Promise resolves, wrapping this instance around <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch">a <code>try-catch</code> code block</a>, and then returning an instance of the browser.\nNotice that the <code>.launch()</code> method takes a JSON parameter with several values:\n<i>headless</i> - <code>false</code> means the browser will run with an Interface so you can watch your script execute, while <code>true</code> means the browser will run in headless mode. Note well, however, that if you want to deploy your scraper to the cloud, set <code>headless</code> back to <code>true</code>. Most virtual machines are headless and do not include a user interface, and hence can only run the browser in headless mode. Puppeteer also includes a <code>headful</code> mode, but that should be used solely for testing purposes.\n<i>ignoreHTTPSErrors</i> - <code>true</code> allows you to visit websites that aren’t hosted over a secure HTTPS protocol and ignore any HTTPS-related errors.\nSave and close the file.\nNow create your second <code>.js</code> file, <code>index.js</code>:\n<code>nano index.js</code>\nHere you will <code>require browser.js</code> and <code>pageController.js</code>. You will then call the <k>startBrowser()</k> function and pass the created browser instance to our page controller, which will direct its actions. Add the following code:\n./book-scraper/index.js\n<code>const browserObject = require(\'./browser\');\nconst scraperController = require(\'./pageController\');\n//Start the browser and create a browser instance\nlet browserInstance = browserObject.startBrowser();\n// Pass the browser instance to the scraper controller\nscraperController(browserInstance)</code>\nSave and close the file.\nCreate your third <code>.js</code> file, <code>pageController.js</code>:\n<code>nano pageController.js</code>\n<code>pageController.js</code> controls your scraping process. It uses the browser instance to control the <code>pageScraper.js</code> file, which is where all the scraping scripts execute. Eventually, you will use it to specify what book category you want to scrape. For now, however, you just want to make sure that you can open Chromium and navigate to a web page:\n./book-scraper/pageController.js\n<code>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    await pageScraper.scraper(browser);  \n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>\nThis code exports a function that takes in the browser instance and passes it to a function called <code>scrapeAll()</code>. This function, in turn, passes this instance to <code>pageScraper.scraper()</code> as an argument which uses it to scrape pages.\nSave and close the file.\nFinally, create your last <code>.js</code> file, <code>pageScraper.js</code>:\n<code>nano pageScraper.js</code>\nHere you will create an object literal with a <code>url</code> property and a <code>scraper()</code> method. The <code>url</code> is the web URL of the web page you want to scrape, while the <code>scraper()</code> method contains the code that will perform your actual scraping, although at this stage it merely navigates to a URL. Add the following code:\n./book-scraper/pageScraper.js\n<code>const scraperObject = {\n  url: \'http://books.toscrape.com\',\n  async scraper(browser){\n    let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    await page.goto(this.url);\n  }\n}\nmodule.exports = scraperObject;</code>\n<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#browsernewpage">Puppeteer has a <code>newPage()</code> method</a> that creates a new page instance in the browser, and these page instances can do quite a few things. In our <code>scraper()</code> method, you created a page instance and then used the <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagegotourl-options"><code>page.goto()</code> method</a> to navigate to <a href="http://books.toscrape.com">the books.toscrape.com homepage</a>.\nSave and close the file.\nYour program’s file-structure is now complete. The first level of your project’s directory tree will look like this:\n<code>Output.\n├── browser.js\n├── index.js\n├── node_modules\n├── package-lock.json\n├── package.json\n├── pageController.js\n└── pageScraper.js</code>\nNow run the command <code>npm run start</code> and watch your scraper application execute:\n<code>npm run start</code>\nIt will automatically open a Chromium browser instance, open a new page in the browser, and navigate to <a href="http://books.toscrape.com">books.toscrape.com</a>.\nIn this step, you created a Puppeteer application that opened Chromium and loaded the homepage for a dummy online bookstore—books.toscrape.com. In the next step, you will scrape the data for every book on that homepage.\n',
'<h2>Step 3 — Scraping Data from a Single Page</h2>Before adding more functionality to your scraper application, open your preferred web browser and manually navigate to the <a href="http://books.toscrape.com/">books to scrape homepage</a>. Browse the site and get a sense of how data is structured.\n<img src="https://assets.digitalocean.com/articles/67187/web_scraper.png" alt="Books to scrape websites image">\nYou will find a category section on the left and books displayed on the right. When you click on a book, the browser navigates to a new URL that displays relevant information regarding that particular book.\nIn this step, you will replicate this behavior, but with code; you will automate the business of navigating the website and consuming its data.\nFirst, if you inspect the source code for the homepage using the Dev Tools inside your browser, you will notice that the page lists each book’s data under a <code>section</code> tag. Inside the <code>section</code> tag every book is under a <code>list</code> (<code>li</code>) tag, and it is here that you find the link to the book’s dedicated page, the price, and the in-stock availability.\n<img src="https://assets.digitalocean.com/articles/67187/bookstoscrape_devtools.png" alt="books.toscrape source code viewed in dev tools">\nYou’ll be scraping these book URLs, filtering for books that are in-stock, navigating to each individual book page, and scraping that book’s data.\nReopen your <code>pageScraper.js</code> file:\n<code>nano pageScraper.js</code>\nAdd the following highlighted content. You will nest another <code>await</code> block inside <code>await page.goto(this.url);</code>:\n./book-scraper/pageScraper.js\n<code>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n        let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    <k>// Wait for the required DOM to be rendered</k>\n    <k>await page.waitForSelector(\'.page_inner\');</k>\n    <k>// Get the link to all the required books</k>\n    <k>let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {</k>\n      <k>// Make sure the book to be scraped is in stock</k>\n      <k>links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")</k>\n      <k>// Extract the links from the data</k>\n      <k>links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)</k>\n      <k>return links;</k>\n    <k>});</k>\n    <k>console.log(urls);</k>\n    }\n}\nmodule.exports = scraperObject;</code>\nIn this code block, you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagewaitforselectorselector-options">the <code>page.waitForSelector()</code> method</a>. This waited for the div that contains all the book-related information to be rendered in the DOM, and then you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pageevalselector-pagefunction-args">the <code>page.$$eval()</code> method</a>. This method gets the URL element with the selector <code>section ol li</code> (be sure that you always return only a string or a number from the <code>page.$eval()</code> and <code>page.$$eval()</code> methods).\nEvery book has two statuses; a book is either <code>In Stock</code> or <code>Out of stock</code>. You only want to scrape books that are <code>In Stock</code>. Because <code>page.$$eval()</code> returns an array of all matching elements, you have filtered this array to ensure that you are only working with in-stock books. You did this by searching for and evaluating the class <code>.instock.availability</code>. You then mapped out the <code>href</code> property of the book links and returned it from the method.\nSave and close the file.\nRe-run your application:\n<code>npm run start</code>\nThe browser will open, navigate to the web page, and then close once the task completes. Now check your console; it will contain all the scraped URLs:\n<code>Output&gt; book-scraper@1.0.0 start <k>/Users/sammy/book-scraper</k>\n&gt; node index.js\nOpening the browser......\nNavigating to http://books.toscrape.com...\n[\n  \'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\',\n  ...]</code>\nThis is a great start, but you want to scrape all the relevant data for a particular book and not only its URL. You will now use these URLs to open each page and scrape the book’s title, author, price, availability, UPC, description, and image URL.\nReopen <code>pageScraper.js</code>:\n<code>nano pageScraper.js</code>\nAdd the following code, which will loop through each scraped link, open a new page instance, and then retrieve the relevant data:\n./book-scraper/pageScraper.js\n<code>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n        let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    // Wait for the required DOM to be rendered\n    await page.waitForSelector(\'.page_inner\');\n    // Get the link to all the required books\n    let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n      // Make sure the book to be scraped is in stock\n      links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n      // Extract the links from the data\n      links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n      return links;\n    });\n        <k>// Loop through each of those links, open a new page instance and get the relevant data from them</k>\n    <k>let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {</k>\n      <k>let dataObj = {};</k>\n      <k>let newPage = await browser.newPage();</k>\n      <k>await newPage.goto(link);</k>\n      <k>dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);</k>\n      <k>dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);</k>\n      <k>dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {</k>\n        <k>// Strip new line and tab spaces</k>\n        <k>text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");</k>\n        <k>// Get the number of stock available</k>\n        <k>let regexp = /^.*\((.*)\).*$/i;</k>\n        <k>let stockAvailable = regexp.exec(text)[1].split(\' \')[0];</k>\n        <k>return stockAvailable;</k>\n      <k>});</k>\n      <k>dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);</k>\n      <k>dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);</k>\n      <k>dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);</k>\n      <k>resolve(dataObj);</k>\n      <k>await newPage.close();</k>\n    <k>});</k>\n    <k>for(link in urls){</k>\n      <k>let currentPageData = await pagePromise(urls[link]);</k>\n      <k>// scrapedData.push(currentPageData);</k>\n      <k>console.log(currentPageData);</k>\n    <k>}</k>\n    }\n}\nmodule.exports = scraperObject; </code>\nYou have an array of all URLs. You want to loop through this array, open up the URL in a new page, scrape data on that page, close that page, and open a new page for the next URL in the array. Notice that you wrapped this code in a Promise. This is because you want to be able to wait for each action in your loop to complete. Therefore each Promise opens a new URL and won’t resolve until the program has scraped all the data on the URL, and then that page instance has closed.\n<i>Warning:</i> note well that you waited for the Promise using a <code>for-in</code> loop. Any other loop will be sufficient but avoid iterating over your URL arrays using an array-iteration method like <code>forEach</code>, or any other method that uses a callback function. This is because the callback function will have to go through the callback queue and event loop first, hence, multiple page instances will open all at once. This will place a much larger strain on your memory.\nTake a closer look at your <code>pagePromise</code> function. Your scraper first created a new page for each URL, and then you used the <k>page.$eval()</k> function to target selectors for relevant details that you wanted to scrape on the new page. Some of the texts contain whitespaces, tabs, newlines, and other non-alphanumeric characters, which you stripped off using a regular expression. You then appended the value for every piece of data scraped in this page to an Object and resolved that object.\nSave and close the file.\nRun the script again:\n<code>npm run start</code>\nThe browser opens the homepage and then opens each book page and logs the scraped data from each of those pages. This output will print to your console:\n<code>OutputOpening the browser......\nNavigating to http://books.toscrape.com...\n{\n  bookTitle: \'A Light in the Attic\',\n  bookPrice: \'£51.77\',\n  noAvailable: \'22\',\n  imageUrl: \'http://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg\',\n  bookDescription: "It\'s hard to imagine a world without A Light in the Attic. [...]\',\n  upc: \'a897fe39b1053632\'\n}\n{\n  bookTitle: \'Tipping the Velvet\',\n  bookPrice: \'£53.74\',\n  noAvailable: \'20\',\n  imageUrl: \'http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg\',\n  bookDescription: `"Erotic and absorbing...Written with starling power."--"The New York Times Book Review " Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler [...]`,\n  upc: \'90fa61229261140a\'\n}\n{\n  bookTitle: \'Soumission\',\n  bookPrice: \'£50.10\',\n  noAvailable: \'20\',\n  imageUrl: \'http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg\',\n  bookDescription: \'Dans une France assez proche de la nôtre, [...]\',\n  upc: \'6957f44c3847a760\'\n}\n...</code>\nIn this step, you scraped relevant data for every book on the homepage of <a href="http://books.toscrape.com">books.toscrape.com</a>, but you could add much more functionality. Each page of books, for instance, is paginated; how do you get books from these other pages? Also, on the left side of the website you found book categories; what if you don’t want all the books, but you just want books from a particular genre? You will now add these features.\n',
'<h2>Step 4 — Scraping Data From Multiple Pages</h2>Pages on <a href="http://books.toscrape.com">books.toscrape.com</a> that are paginated have a <code>next</code> button beneath their content, while pages that are not paginated do not.\nYou will use the presence of this button to determine if the page is paginated or not. Since the data on each page is of the same structure and has the same markup, you won’t be writing a scraper for every possible page. Rather, you will use the practice of <a href="https://www.digitalocean.com/community/tutorials/js-understanding-recursion">recursion</a>.\nFirst, you need to change the structure of your code a bit to accommodate recursively navigating to several pages.\nReopen <code>pagescraper.js</code>:\n<code>nano pagescraper.js</code>\nYou will add a new function called <code>scrapeCurrentPage()</code> to your <code>scraper()</code> method. This function will contain all the code that scrapes data from a particular page and then click the next button if it exists. Add the following highlighted code:\n./book-scraper/pageScraper.js scraper()\n<code>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    async scraper(browser){\n    let page = await browser.newPage();\n    console.log(`Navigating to ${this.url}...`);\n    // Navigate to the selected page\n    await page.goto(this.url);\n    <k>let scrapedData = [];</k>\n    // Wait for the required DOM to be rendered\n    <k>async function scrapeCurrentPage(){</k>\n      await page.waitForSelector(\'.page_inner\');\n      // Get the link to all the required books\n      let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n        // Make sure the book to be scraped is in stock\n        links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n        // Extract the links from the data\n        links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n        return links;\n      });\n      // Loop through each of those links, open a new page instance and get the relevant data from them\n      let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {\n        let dataObj = {};\n        let newPage = await browser.newPage();\n        await newPage.goto(link);\n        dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);\n        dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);\n        dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {\n          // Strip new line and tab spaces\n          text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");\n          // Get the number of stock available\n          let regexp = /^.*\((.*)\).*$/i;\n          let stockAvailable = regexp.exec(text)[1].split(\' \')[0];\n          return stockAvailable;\n        });\n        dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);\n        dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);\n        dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);\n        resolve(dataObj);\n        await newPage.close();\n      });\n      for(link in urls){\n        let currentPageData = await pagePromise(urls[link]);\n        <k>scrapedData.push(currentPageData);</k>\n        // console.log(currentPageData);\n      }\n      // When all the data on this page is done, click the next button and start the scraping of the next page\n      // You are going to check if this button exist first, so you know if there really is a next page.\n      <k>let nextButtonExist = false;</k>\n      <k>try{</k>\n        <k>const nextButton = await page.$eval(\'.next &gt; a\', a =&gt; a.textContent);</k>\n        <k>nextButtonExist = true;</k>\n      <k>}</k>\n      <k>catch(err){</k>\n        <k>nextButtonExist = false;</k>\n      }\n      <k>if(nextButtonExist){</k>\n        <k>await page.click(\'.next &gt; a\');  </k>\n        <k>return scrapeCurrentPage(); // Call this function recursively</k>\n      <k>}</k>\n      <k>await page.close();</k>\n      <k>return scrapedData;</k>\n    <k>}</k>\n    <k>let data = await scrapeCurrentPage();</k>\n    <k>console.log(data);</k>\n    <k>return data;</k>\n  }\n}\nmodule.exports = scraperObject;</code>\nYou set the <code>nextButtonExist</code> variable to false initially, and then check if the button exists. If the <code>next</code> button exists, you set <code>nextButtonExists</code> to <code>true</code> and proceed to click the <code>next</code> button, and then call this function recursively.\nIf <code>nextButtonExists</code> is false, it returns the <code>scrapedData</code> array as usual.\nSave and close the file.\nRun your script again:\n<code>npm run start</code>\nThis might take a while to complete; your application, after all, is now scraping the data from over 800 books. Feel free to either close the browser or press <code>CTRL + C</code> to cancel the process.\nYou have now maximized your scraper’s capabilities, but you’ve created a new problem in the process. Now the issue is not too little data but too much data. In the next step, you will fine-tune your application to filter your scraping by book category.\n',
'<h2>Step 5 — Scraping Data by Category</h2>To scrape data by category, you will need to modify both your <code>pageScraper.js</code> file and your <code>pageController.js</code> file.\nOpen <code>pageController.js</code> in a text editor. Call the scraper so that it only scrapes travel books. Add the following code:\n./book-scraper/pageController.js\n<code>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    <k>let scrapedData = {};</k>\n    // Call the scraper for different set of books to be scraped\n    <k>scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');</k>\n    <k>await browser.close();</k>\n    <k>console.log(scrapedData)</k>\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>\nYou are now passing two parameters into your <code>pageScraper.scraper()</code> method, with the second parameter being the category of books you want to scrape, which in this example is <code>Travel</code>. But your <code>pageScraper.js</code> file does not recognize this parameter yet. You will need to adjust this file, too.\nSave and close the file.\nOpen <code>pageScraper.js</code>:\n<code>nano pageScraper.js</code>\nAdd the following code, which will add your category parameter, navigate to that category page, and then begin scraping through the paginated results:\n./book-scraper/pageScraper.js\n<code>const scraperObject = {\n    url: \'http://books.toscrape.com\',\n    <k>async scraper(browser, category){</k>\n        let page = await browser.newPage();\n        console.log(`Navigating to ${this.url}...`);\n        // Navigate to the selected page\n        await page.goto(this.url);\n        // Select the category of book to be displayed\n    <k>let selectedCategory = await page.$$eval(\'.side_categories &gt; ul &gt; li &gt; ul &gt; li &gt; a\', (links, _category) =&gt; {</k>\n      // Search for the element that has the matching text\n      <k>links = links.map(a =&gt; a.textContent.replace(/(\r\n\t|\n|\r|\t|^\s|\s$|\B\s|\s\B)/gm, "") === _category ? a : null);</k>\n      <k>let link = links.filter(tx =&gt; tx !== null)[0];</k>\n      <k>return link.href;</k>\n    <k>}, category);</k>\n    // Navigate to the selected category\n    <k>await page.goto(selectedCategory);</k>\n        let scrapedData = [];\n        // Wait for the required DOM to be rendered\n        async function scrapeCurrentPage(){\n            await page.waitForSelector(\'.page_inner\');\n            // Get the link to all the required books\n            let urls = await page.$$eval(\'section ol &gt; li\', links =&gt; {\n                // Make sure the book to be scraped is in stock\n                links = links.filter(link =&gt; link.querySelector(\'.instock.availability &gt; i\').textContent !== "In stock")\n                // Extract the links from the data\n                links = links.map(el =&gt; el.querySelector(\'h3 &gt; a\').href)\n                return links;\n            });\n            // Loop through each of those links, open a new page instance and get the relevant data from them\n            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {\n                let dataObj = {};\n                let newPage = await browser.newPage();\n                await newPage.goto(link);\n                dataObj[\'bookTitle\'] = await newPage.$eval(\'.product_main &gt; h1\', text =&gt; text.textContent);\n                dataObj[\'bookPrice\'] = await newPage.$eval(\'.price_color\', text =&gt; text.textContent);\n                dataObj[\'noAvailable\'] = await newPage.$eval(\'.instock.availability\', text =&gt; {\n                    // Strip new line and tab spaces\n                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");\n                    // Get the number of stock available\n                    let regexp = /^.*\((.*)\).*$/i;\n                    let stockAvailable = regexp.exec(text)[1].split(\' \')[0];\n                    return stockAvailable;\n                });\n                dataObj[\'imageUrl\'] = await newPage.$eval(\'#product_gallery img\', img =&gt; img.src);\n                dataObj[\'bookDescription\'] = await newPage.$eval(\'#product_description\', div =&gt; div.nextSibling.nextSibling.textContent);\n                dataObj[\'upc\'] = await newPage.$eval(\'.table.table-striped &gt; tbody &gt; tr &gt; td\', table =&gt; table.textContent);\n                resolve(dataObj);\n                await newPage.close();\n            });\n            for(link in urls){\n                let currentPageData = await pagePromise(urls[link]);\n                scrapedData.push(currentPageData);\n                // console.log(currentPageData);\n            }\n            // When all the data on this page is done, click the next button and start the scraping of the next page\n            // You are going to check if this button exist first, so you know if there really is a next page.\n            let nextButtonExist = false;\n            try{\n                const nextButton = await page.$eval(\'.next &gt; a\', a =&gt; a.textContent);\n                nextButtonExist = true;\n            }\n            catch(err){\n                nextButtonExist = false;\n            }\n            if(nextButtonExist){\n                await page.click(\'.next &gt; a\');   \n                return scrapeCurrentPage(); // Call this function recursively\n            }\n            await page.close();\n            return scrapedData;\n        }\n        let data = await scrapeCurrentPage();\n        console.log(data);\n        return data;\n    }\n}\nmodule.exports = scraperObject;</code>\nThis code block uses the category that you passed in to get the URL where the books of that category reside.\nThe <code>page.$$eval()</code> can take in arguments by passing the argument as a third parameter to the <code>$$eval()</code> method, and defining it as the third parameter in the callback as such:\nexample page.$$eval() function\n<code>page.$$eval(\'selector\', function(elem, args){\n  // .......\n}, args)</code>\nThis was what you did in your code; you passed the category of books you wanted to scrape, mapped through all the categories to check which one matches, and then returned the URL of this category.\nThis URL is then used to navigate to the page that displays the category of books you want to scrape using the <code>page.goto(selectedCategory)</code> method.\nSave and close the file.\nRun your application again. You will notice that it navigates to the <code>Travel</code> category, recursively opens books in that category page by page, and logs the results:\n<code>npm run start</code>\nIn this step, you scraped data across multiple pages and then scraped data across multiple pages from one particular category. In the final step, you will modify your script to scrape data across multiple categories and then save this scraped data to a stringified JSON file.\n',
'<h2>Step 6 — Scraping Data from Multiple Categories and Saving the Data as JSON</h2>In this final step, you will make your script scrape data off of as many categories as you want and then change the manner of your output. Rather than logging the results, you will save them in a structured file called <code>data.json</code>.\nYou can quickly add more categories to scrape; doing so requires only one additional line per genre.\nOpen <code>pageController.js</code>:\n<code>nano pageController.js</code>\nAdjust your code to include additional categories. The example below adds <code>HistoricalFiction</code> and <code>Mystery</code> to our existing <code>Travel</code> category:\n./book-scraper/pageController.js\n<code>const pageScraper = require(\'./pageScraper\');\nasync function scrapeAll(browserInstance){\n    let browser;\n    try{\n    browser = await browserInstance;\n    let scrapedData = {};\n    // Call the scraper for different set of books to be scraped\n    <k>scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');</k>\n    <k>scrapedData[\'HistoricalFiction\'] = await pageScraper.scraper(browser, \'Historical Fiction\');</k>\n    <k>scrapedData[\'Mystery\'] = await pageScraper.scraper(browser, \'Mystery\');</k>\n    await browser.close();\n    console.log(scrapedData)\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>\nSave and close the file.\nRun the script again and watch it scrape data for all three categories:\n<code>npm run start</code>\nWith the scraper fully-functional, your final step involves saving your data in a more useful format. You will now store it in a JSON file using <a href="https://nodejs.org/api/fs.html">the <code>fs</code> module in Node.js</a>.\nFirst, reopen <code>pageController.js</code>:\n<code>nano pageController.js</code>\nAdd the following highlighted code:\n./book-scraper/pageController.js\n<code>const pageScraper = require(\'./pageScraper\');\n<k>const fs = require(\'fs\');</k>\nasync function scrapeAll(browserInstance){\n  let browser;\n  try{\n    browser = await browserInstance;\n    let scrapedData = {};\n    // Call the scraper for different set of books to be scraped\n    scrapedData[\'Travel\'] = await pageScraper.scraper(browser, \'Travel\');\n    scrapedData[\'HistoricalFiction\'] = await pageScraper.scraper(browser, \'Historical Fiction\');\n    scrapedData[\'Mystery\'] = await pageScraper.scraper(browser, \'Mystery\');\n    await browser.close();\n    <k>fs.writeFile("data.json", JSON.stringify(scrapedData), \'utf8\', function(err) {</k>\n        <k>if(err) {</k>\n            <k>return console.log(err);</k>\n        }\n        <k>console.log("The data has been scraped and saved successfully! View it at \'./data.json\'");</k>\n    <k>});</k>\n  }\n  catch(err){\n    console.log("Could not resolve the browser instance =&gt; ", err);\n  }\n}\nmodule.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>\nFirst, you are requiring Node,js’s <code>fs</code> module in <code>pageController.js</code>. This ensures that you can save your data as a JSON file. Then you are adding code so that when the scraping completes and the browser closes, the program will create a new file called <code>data.json</code>. Note that the contents of <code>data.json</code> are <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify">stringified JSON</a>. Therefore, when reading the content of <code>data.json</code>, always <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON">parse it as JSON</a> before reusing the data.\nSave and close the file.\nYou have now built a web-scraping application that scrapes books across multiple categories and then stores your scraped data in a JSON file. As your application grows in complexity, you might want to store this scraped data in a database or serve it over an API. How this data is consumed is really up to you.\n',









];

bookid = "Nodejs server Tips"
</script>

<script src='https://williamkpchan.github.io/showTips.js'></script>
<!--script src='../showTips.js'></script-->


</body>
</html>