<base target="_blank"><html><head><title>Puppeteer Examples</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "Puppeteer Examples"
  var markerName = "h2"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>Puppeteer Examples</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2>Getting to Know Puppeteer Using Practical Examples</h2>
17 min read
An overview, concrete guide and kinda cheat sheet for the popular browser automation library, based on Node.js, which provides a high-level API over the Chrome DevTools Protocol.

Contents
</pre>
<ul>
<li><a href="#how-to-install">How to Install</a>
<ul>
<li><a href="#library-package">Library Package</a></li>
<li><a href="#product-package">Product Package</a></li>
</ul></li>
<li><a href="#interacting-browser">Interacting Browser</a>
<ul>
<li><a href="#launching-chromium">Launching Chromium</a></li>
<li><a href="#connecting-chromium">Connecting Chromium</a></li>
<li><a href="#launching-firefox">Launching Firefox</a></li>
<li><a href="#browser-context">Browser Context</a></li>
<li><a href="#headful-mode">Headful Mode</a></li>
<li><a href="#debugging">Debugging</a></li>
</ul></li>
<li><a href="#interacting-page">Interacting Page</a>
<ul>
<li><a href="#navigating-by-url">Navigating by URL</a></li>
<li><a href="#emulating-devices">Emulating Devices</a></li>
<li><a href="#handling-events">Handling Events</a></li>
<li><a href="#operating-mouse">Operating Mouse</a></li>
<li><a href="#operating-keyboard">Operating Keyboard</a></li>
<li><a href="#taking-screenshots">Taking Screenshots</a></li>
<li><a href="#generating-pdf">Generating PDF</a></li>
<li><a href="#faking-geolocation">Faking Geolocation</a></li>
<li><a href="#accessibility">Accessibility</a></li>
<li><a href="#code-coverage">Code Coverage</a></li>
<li><a href="#measuring-performance">Measuring Performance</a></li>
</ul></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#vs-code-snippets">VS Code Snippets</a></li>
</ul>
<pre>
<a href="https://pptr.dev" target="_blank">Puppeteer</a> is a project from the Google Chrome team which enables us to control a Chrome (or any other Chrome DevTools Protocol based browser) and execute common actions, much like in a real browser - programmatically, through a decent API.
Put simply, it&rsquo;s a super useful and easy tool for <strong>automating, testing and scraping</strong> web pages over a headless mode or headful either.

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/58bbf88ce5160641f8775322e78cbffbf2898b8f/4b6fd/images/posts/2018-08-26-debugging-nodejs-application-in-chrome-devtools-using-ndb/puppeteer-logo.png"
         alt="Puppeteer&#39;s logo" width="200"/> <figcaption>
            <h4>Puppeteer&#39;s logo</h4>
        </figcaption>
</figure>

In this article we&rsquo;re going to try out Puppeteer and demonstrate a variety of the available capabilities, through concrete examples.

<em><strong>Disclaimer:</strong> This article doesn&rsquo;t claim to replace <a href="https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md" target="_blank">the official documentation</a> but rather elaborate it - you definitely should go over it in order to be aligned with the most updated API specification.</em>

<h2 id="how-to-install">How to Install</h2>

To begin with, we&rsquo;ll have to install one of Puppeteer&rsquo;s <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-puppeteer-vs-puppeteer-core" target="_blank">packages</a>.

<h2 id="library-package">Library Package</h2>

A lightweight package, called <code>puppeteer-core</code>, which is a <strong>library</strong> that interacts with any browser that&rsquo;s based on DevTools protocol - without actually installing Chromium. 
It comes in handy mainly when we don‚Äôt need a downloaded version of Chromium, for instance, bundling this library within a project that interacts with a browser remotely.

In order to install, just run:

<code>npm install puppeteer-core
</code>

<h2 id="product-package">Product Package</h2>

The main package, called <code>puppeteer</code>, which is actually a full <strong>product</strong> for browser automation on top of <code>puppeteer-core</code>. 
Once it&rsquo;s installed, the most recent version of Chromium is placed inside <code>node_modules</code>, what guarantees that the downloaded version is compatible with the host operating system.

Simply run the following to install:

<code>npm install puppeteer
</code>
<br>

Now, we&rsquo;re absolutely ready to go! ü§ì

<h2 id="interacting-browser">Interacting Browser</h2>

As mentioned before, Puppeteer is just an API over the Chrome DevTools Protocol. 
Naturally, it should have a Chromium instance to interact with. 
This is the reason why Puppeteer&rsquo;s ecosystem provides methods to launch a new Chromium instance and connect an existing instance also.

Let&rsquo;s examine a few cases.

<h2 id="launching-chromium">Launching Chromium</h2>

The easiest way to interact with the browser is by launching a Chromium instance using Puppeteer:

<script type="application/javascript" src="//gist.github.com/nitayneeman/08c100ef25dffa0ddce9bcacd1925377.js"></script>

The <code>launch</code> method initializes the instance at first, and then attaching Puppeteer to that.
Notice this method is <strong>asynchronous</strong> (like most Puppeteer&rsquo;s methods) which, as we know, returns a <code>Promise</code>. 
Once it‚Äôs resolved, we get a <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-class-browser" target="_blank">browser</a> instance that represents our initialized instance.

<h2 id="connecting-chromium">Connecting Chromium</h2>

Sometimes we want to interact with an existing Chromium instance - whether using <code>puppeteer-core</code> or just attaching a remote instance:
<script type="application/javascript" src="//gist.github.com/nitayneeman/69876fea604aed196ad6cdf4c3e25f97.js"></script>

Well, it&rsquo;s easy to see that we use <a href="https://github.com/GoogleChrome/chrome-launcher" target="_blank">chrome-launcher</a> in order to launch a Chrome instance <strong>manually</strong>.
Then, we simply fetch the <code>webSocketDebuggerUrl</code> value of the created instance.

The <code>connect</code> method attaches the instance we just created to Puppeteer. 
All we&rsquo;ve to do is supplying the WebSocket endpoint of our instance.

<em><strong>Note:</strong> Of course, chrome-launcher is only to demonstrate an instance creation. 
We absolutely could connect an instance in other ways, as long as we have the appropriate WebSocket endpoint.</em>

<h2 id="launching-firefox">Launching Firefox</h2>

Some of you might wonder - could Puppeteer interact with other browsers besides Chromium? ü§î

Although there are projects that claim to support the variety browsers - the official team has started to maintain an <a href="https://github.com/GoogleChrome/puppeteer/tree/master/experimental/puppeteer-firefox" target="_blank">experimental project</a> that interacts with <strong>Firefox</strong>, specifically:

<code>npm install puppeteer-firefox
</code>
This project exposes the same decent API, what means we&rsquo;re already familiar with how to launch the browser:
<script type="application/javascript" src="//gist.github.com/nitayneeman/4c1cf0ad571891a94564bb39020c4975.js"></script>

‚ö†Ô∏è Pay attention - the API isn&rsquo;t totally ready yet and implemented progressively. 
Also, it&rsquo;s better to check out the implementation status <a href="https://aslushnikov.github.io/ispuppeteerfirefoxready/" target="_blank">here</a>.

<h2 id="browser-context">Browser Context</h2>

Imagine that instead of recreating a browser instance each time, which is pretty expensive operation, we could use the same instance but separate it into different individual sessions which belong to this shared browser.

It&rsquo;s actually possible, and these sessions are known as <strong><a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-class-browsercontext" target="_blank">Browser Contexts</a></strong>.

A default browser context is created as soon as creating a browser instance, but we can create additional browser contexts as necessary:
<script type="application/javascript" src="//gist.github.com/nitayneeman/1d7782b6430b280b5e64956d2ffabff8.js"></script>

Apart from the fact that we demonstrate how to access each context, we need to know that the only way to terminate the default context is by closing the browser instance - which, in fact, terminates all the contexts that belong to the browser.

Better yet, the browser context also come in handy when we want to apply a specific configuration on the session isolatedly - for instance, granting additional permissions.

<h2 id="headful-mode">Headful Mode</h2>

As opposed to the <strong>headless</strong> mode - which merely uses the command line,
the <strong>headful</strong> mode opens the browser with a graphical user interface during the instruction:
<script type="application/javascript" src="//gist.github.com/nitayneeman/da8135d4035a964cb251920a0c56944d.js"></script>

Because of the fact that the browser is launched in headless mode by default, we demonstrate how to launch it in a headful way.

In case you wonder - headless mode is mostly useful for environments that don&rsquo;t really need the UI or neither support such an interface. 
The cool thing is that we can headless almost everything in Puppeteer. 
üí™

<em><strong>Note:</strong> We&rsquo;re going to launch the browser in a headful mode for most of the upcoming examples, which will allow us to notice the result clearly.</em>

<h2 id="debugging">Debugging</h2>

When writing code, we should be aware of what kinds of ways are available to debug our program. 
The documentation lists several <a href="https://developers.google.com/web/tools/puppeteer/debugging" target="_blank">tips</a> about debugging Puppeteer.

Let&rsquo;s cover the core principles:

<strong>1Ô∏è‚É£ - Checking how the browser is operated</strong>

That&rsquo;s fairly probable we would like to see how our script instructs the browser and what&rsquo;s actually displayed, at some point.

The headful mode, which we&rsquo;re already familiar with, helps us to practically do that:
<script type="application/javascript" src="//gist.github.com/nitayneeman/a43a27099aa3e1d2a3d6efc16203974f.js"></script>

Beyond that the browser is truly opened, we can notice now the operated instructions clearly - due to <code>slowMo</code> which slows down Puppeteer when performing each operation.

<strong>2Ô∏è‚É£ - Debugging our application code in the browser</strong>

In case we want to debug the application itself in the opened browser - it basically means to open the DevTools and start debugging as usual:
<script type="application/javascript" src="//gist.github.com/nitayneeman/2264841aaee50c8621536a8f39f81576.js"></script>

Notice that we use <code>devtools</code> which launches the browser in a headful mode by default and opens the DevTools automatically.
On top of that, we utilize <code>waitForTarget</code> in order to hold the browser process until we terminate it <strong>explicitly</strong>.

Apparently - some of you may wonder if it&rsquo;s possible to <strong>sleep</strong> the browser with a specified time period, so:
<script type="application/javascript" src="//gist.github.com/nitayneeman/7d52c5724b19ac0e28196ee0b9574aaa.js"></script>

The first approach is merely a function that resolves a promise when <code>setTimeout</code> finishes.
The second approach, however, is much simpler but demands having a page instance (we&rsquo;ll get to that later).

<strong>3Ô∏è‚É£ - Debugging the process that uses Puppeteer</strong>

As we know, Puppeteer is executed in a Node.js process - which is absolutely separated from the browser process.
Hence, in this case, we should treat it as much as we debug a regular Node.js application.

Whether we connect to an <a href="https://nodejs.org/de/docs/guides/debugging-getting-started/#inspector-clients" target="_blank">inspector client</a> or prefer using <a href="/posts/debugging-nodejs-application-in-chrome-devtools-using-ndb/" target="_blank">ndb</a> -
it&rsquo;s all about placing the breakpoints right before Puppeteer&rsquo;s operation. 
Adding them programmatically is possible either, simply by inserting the <code>debugger;</code> statement, obviously.

<h2 id="interacting-page">Interacting Page</h2>

Now that Puppeteer is attached to a browser instance - which, as we already mentioned, represents our browser instance (Chromium, Firefox, whatever),
allows us creating easily a page (or multiple pages):
<script type="application/javascript" src="//gist.github.com/nitayneeman/9f082cbf5fa5bab43a6f0458369a057b.js"></script>

In the code example above we plainly create a new page by invoking the <code>newPage</code> method. 
Notice it&rsquo;s created on the default browser context.

Basically, <code>Page</code> is a class that represents a single tab in the browser (or an <a href="https://developer.chrome.com/extensions/background_pages" target="_blank">extension background</a>).
As you guess, this class provides handy methods and events in order to interact with the page (such as selecting elements, retrieving information, waiting for elements, etc.).

Well, it&rsquo;s about time to present a list of practical examples, as promised. 
To do this, we&rsquo;re going to scrape data from <a href="https://pptr.dev" target="_blank">the official Puppeteer website</a> and operate it.üïµ

<h2 id="navigating-by-url">Navigating by URL</h2>

One of the earliest things is, intuitively, instructing the blank page to navigate to a specified URL:
<script type="application/javascript" src="//gist.github.com/nitayneeman/c40b7dcb7752f961e2301bde52751ffd.js"></script>

We use <code>goto</code> to drive the created page to navigate Puppeteer‚Äôs website. 
Afterward, we just take the title of Page‚Äôs main frame, print it, and expect to get that as an output:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/a9ddcfd11c579a020459f942ee7a7ac41cf2fc93/63d90/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer.example.gif"
         alt="Navigating by a URL and scraping the title"/> <figcaption>
            <h4>Navigating by a URL and scraping the title</h4>
        </figcaption>
</figure>


As we notice, the title is unexpectedly missing. 
üßê

This example shows us which there&rsquo;s no guarantee that our page would render the selected element at the right moment, and if anything.
To clarify - possible reasons could be that the page is loaded slowly, part of the page is lazy-loaded, or perhaps it&rsquo;s navigated immediately to another page.

That&rsquo;s exactly why Puppeteer provides methods to wait for stuff like <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitforselectorselector-options" target="_blank">elements</a>, <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitfornavigationoptions" target="_blank">navigation</a>, <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitforfunctionpagefunction-options-args" target="_blank">functions</a>, <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitforrequesturlorpredicate-options" target="_blank">requests</a>, <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitforresponseurlorpredicate-options" target="_blank">responses</a> or simply a certain <a href="https://pptr.dev/#?product=Puppeteer&amp;show=api-pagewaitforselectororfunctionortimeout-options-args" target="_blank">predicate</a> - mainly to deal with an <strong>asynchronous</strong> flow.

Anyway, it turns out that Puppeteer&rsquo;s website has an entry page, which immediately redirects us to the well-known website&rsquo;s index page.
The thing is, that entry page in question doesn&rsquo;t render a <code>title</code> meta element:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/bee4443f4c56ef79a8a669da3a374b1f23339947/9ae10/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer2.example.gif"
         alt="Evaluating the title meta element"/> <figcaption>
            <h4>Evaluating the title meta element</h4>
        </figcaption>
</figure>


When navigating to Puppeteer&rsquo;s website, the <code>title</code> element is evaluated as an empty string. 
However, a few moments later, the page is really navigated to the website&rsquo;s index page and rendered with a title.

This means that the invoked <code>title</code> method is actually applied too early, on the entry page, instead of the website&rsquo;s index page. 
Thus, the entry page is considered as the first main frame, and eventually its title, which is an empty string, is returned.

Let&rsquo;s solve that case in a simple way:
<script type="application/javascript" src="//gist.github.com/nitayneeman/33efd11d066b17dcd42e6aebca06e65f.js"></script>

All we do, is instructing Puppeteer to wait until the page renders a <code>title</code> meta element, which is achieved by invoking <code>waitForSelector</code>. 
This method basically waits until the selected element is rendered within the page.

In that way - we can easily deal with asynchronous rendering and ensure that elements are visible on the page.

<h2 id="emulating-devices">Emulating Devices</h2>

Puppeteer&rsquo;s library provides tools for approximating how the page looks and behaves on various devices, which are pretty useful when testing a website&rsquo;s responsiveness.

Let&rsquo;s emulate a mobile device and navigate to the official website:
<script type="application/javascript" src="//gist.github.com/nitayneeman/c90a7fb97bd7cf25a9afda84e44086ac.js"></script>

We choose to emulate an iPhone X - which means changing the user agent appropriately.
Furthermore, we adjust the viewport size according to the display points that appear <a href="https://developer.apple.com/library/archive/documentation/DeviceInformation/Reference/iOSDeviceCompatibility/Displays/Displays.html" target="_blank">here</a>.

It&rsquo;s easy to understand that <code>setUserAgent</code> defines a specific user agent for the page, whereas <code>setViewport</code> modifies the viewport definition of the page. 
In case of multiple pages, each one has its own user agent and viewport definition.

Here&rsquo;s the result of the code example above:

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/e3ece42c36997561f49f75036eaec5205b31a0d9/4736d/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer3.example.png"
         alt="Emulating an iPhone X" width="300"/> <figcaption>
            <h4>Emulating an iPhone X</h4>
        </figcaption>
</figure>

Indeed, the console panel shows us that the page is opened with the right user agent and viewport size.

The truth is that we don&rsquo;t have to specify the iPhone X&rsquo;s descriptions explicitly, because the library arrives with a built-in list of <a href="https://github.com/GoogleChrome/puppeteer/blob/master/DeviceDescriptors.js" target="_blank">device descriptors</a>. 
On top of that, it provides a method called <code>emulate</code> which is practically a shortcut for invoking <code>setUserAgent</code> and <code>setViewport</code>, one after another.

Let&rsquo;s use that:
<script type="application/javascript" src="//gist.github.com/nitayneeman/4d5aff2659bd6eb62446da749a8d49d1.js"></script>

It&rsquo;s merely changed to pass the <a href="https://github.com/GoogleChrome/puppeteer/blob/master/DeviceDescriptors.js#L451" target="_blank">boilerplate descriptor</a> to <code>emulate</code> (instead of declaring that explicitly).
Notice we import the descriptors out of <code>puppeteer/DeviceDescriptors</code>.

<h2 id="handling-events">Handling Events</h2>

The <code>Page</code> class supports emitting of various events by actually extending the Node.js&rsquo;s <code>EventEmitter</code> object.
This means we can use the <a href="https://nodejs.org/api/events.html#events_class_eventemitter" target="_blank">natively supported methods</a> in order to handle these events - such as: <code>on</code>, <code>once</code>, <code>removeListener</code> and so on.

Here&rsquo;s the list of the supported events:
<script type="application/javascript" src="//gist.github.com/nitayneeman/34f212f8727e27521f48b864e78b4771.js"></script>

From looking at the list above - we clearly understand that the supported events include aspects of loading, frames, metrics, console, errors, requests, responses and even more!

Let&rsquo;s simulate and trigger part of the events by adding this script:
<script type="application/javascript" src="//gist.github.com/nitayneeman/7546990595cb946c8a45dc44869814c8.js"></script>

As we probably know, <code>evaluate</code> just executes the supplied script within the page context.

Though, the output is going to reflect the events we listen:

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/34f535b0b1adec45010489abc0235da6180e110c/005f6/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer4.example.gif"
         alt="Listening the page events"/> <figcaption>
            <h4>Listening the page events</h4>
        </figcaption>
</figure>

In case you wonder - it&rsquo;s possible to listen for <strong>custom events</strong> that are triggered in the page. 
Basically it means to define the event handler on page&rsquo;s window using the <code>exposeFunction</code> method.
Check out <a href="https://github.com/GoogleChrome/puppeteer/blob/master/examples/custom-event.js" target="_blank">this</a> example to understand exactly how to implement it.

<h2 id="operating-mouse">Operating Mouse</h2>

In general, the mouse controls the motion of a pointer in two dimensions within a viewport.
Unsurprisingly, Puppeteer represents the mouse by a class called <code>Mouse</code>.

Moreover, every <code>Page</code> instance has a <code>Mouse</code> - which allows performing operations such as changing its position and clicking within the viewport.

Let&rsquo;s start with changing the mouse position:
<script type="application/javascript" src="//gist.github.com/nitayneeman/c8460dd50235c93d3ceb57517980b068.js"></script>

The scenario we simulate is moving the mouse over the second link of the left API sidebar.
We set a viewport size and wait explicitly for the sidebar component to ensure it&rsquo;s really rendered.

Then, we invoke <code>move</code> in order to position the mouse with appropriate coordinates, that actually represent the center of the second link.

This is the expected result:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/590fda7859240dfbb59565fb182ad971d89611f2/f4a7d/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer5.example.png"
         alt="Hovering the second link"/> <figcaption>
            <h4>Hovering the second link</h4>
        </figcaption>
</figure>


Although it&rsquo;s hard to see, the second link is hovered as we planned.

The next step is simply clicking on the link by the respective coordinates:
<script type="application/javascript" src="//gist.github.com/nitayneeman/d15e2a83f245f778cf4a75aa94e4b576.js"></script>

Instead of changing the position explicitly, we just use <code>click</code> - which basically triggers <code>mousemove</code>, <code>mousedown</code> and <code>mouseup</code> events, one after another.

<em><strong>Note:</strong> We delay the pressing in order to demonstrate how to modify the click behavior, nothing more. 
It‚Äôs worth pointing out that we can also control the mouse buttons (left, center, right) and the number of clicks.</em>

Another nice thing is the ability to simulate a <strong>drag and drop</strong> behavior easily:
<script type="application/javascript" src="//gist.github.com/nitayneeman/d8cd4b051e7c419bbc959cc4267da286.js"></script>

All we do is using the <code>Mouse</code> methods for grabbing the mouse, from one position to another, and afterward releasing it.

<h2 id="operating-keyboard">Operating Keyboard</h2>

The keyboard is another way to interact with the page, mostly for input purposes.

Similar to the mouse, Puppeteer represents the keyboard by a class called <code>Keyboard</code> - and every <code>Page</code> instance holds such an instance.

Let&rsquo;s type some text within the search input:
<script type="application/javascript" src="//gist.github.com/nitayneeman/6e67f7662d46c462adc174ebe76654e9.js"></script>

Notice that we wait for the toolbar (instead of the API sidebar).
Then, we focus the search input element and simply type a text into it.

On top of typing text, it&rsquo;s obviously possible to trigger keyboard events:
<script type="application/javascript" src="//gist.github.com/nitayneeman/4aa1c17b612ce08f6049735166fd36bb.js"></script>

Basically, we press <code>ArrowDown</code> twice and <code>Enter</code> in order to choose the third search result.

See that in action:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/12f476512961c9ed6d32506b86dcb1cffc2100da/60d7f/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer6.example.gif"
         alt="Choosing a search result using the keyboard"/> <figcaption>
            <h4>Choosing a search result using the keyboard</h4>
        </figcaption>
</figure>


By the way, it&rsquo;s nice to know that there is a <a href="https://github.com/GoogleChrome/puppeteer/blob/master/lib/USKeyboardLayout.js" target="_blank">list</a> of the key codes.

<h2 id="taking-screenshots">Taking Screenshots</h2>

Taking screenshots through Puppeteer is a quite easy mission.

The API provides us a dedicated method for that:
<script type="application/javascript" src="//gist.github.com/nitayneeman/d058aeb61e73d6d4d0c1b9580da90b68.js"></script>

As we see, the <code>screenshot</code> method makes all the charm - whereas we just have to insert a path for the output.

Moreover, it&rsquo;s also possible to control the type, quality and even clipping the image:
<script type="application/javascript" src="//gist.github.com/nitayneeman/8cff466fa9c094e0fb80a8ca18dc9371.js"></script>

Here&rsquo;s the output:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/0130bb3e10ca459bf52270cb592e7247291f5124/b7e54/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer7.example.jpg"
         alt="Capturing an area within the page"/> <figcaption>
            <h4>Capturing an area within the page</h4>
        </figcaption>
</figure>


<h2 id="generating-pdf">Generating PDF</h2>

Puppeteer is either useful for generating a PDF file from the page content.

Let&rsquo;s demonstrate that:
<script type="application/javascript" src="//gist.github.com/nitayneeman/357b53391fe7afbdd4f4a2e9084fb31f.js"></script>

Running the <code>pdf</code> method simply generates us the following file:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/459fa3b18f9b1b40223039f4b82727a02600187b/9eaf7/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer8.example.gif"
         alt="Generating a PDF file from the content"/> <figcaption>
            <h4>Generating a PDF file from the content</h4>
        </figcaption>
</figure>


<h2 id="faking-geolocation">Faking Geolocation</h2>

Many websites customize their content based on the user&rsquo;s geolocation.

Modifying the geolocation of a page is pretty obvious:

<script type="application/javascript" src="//gist.github.com/nitayneeman/bb00805785011013d4117a8e59a4aa8a.js"></script>

First, we grants the browser context the appropriate permissions. 
Then, we use <code>setGeolocation</code> to override the current geolocation with the coordinates of the north pole.

Here&rsquo;s what we get when printing the location through <code>navigator</code>:

<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/24b351dc0af93891b977a39e6d490eb612662455/f39dd/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer9.example.png"
         alt="Changing the geolocation of the page" width="640"/> <figcaption>
            <h4>Changing the geolocation of the page</h4>
        </figcaption>
</figure>

<h2 id="accessibility">Accessibility</h2>

The <a href="https://developers.google.com/web/fundamentals/accessibility/semantics-builtin/the-accessibility-tree" target="_blank">accessibility tree</a> is a subset of the DOM that includes only elements with relevant information for assistive technologies such as screen readers, voice controls and so on.
Having the accessibility tree means we can analyze and test the accessibility support in the page.

When it comes to Puppeteer, it enables to capture the current state of the tree:
<script type="application/javascript" src="//gist.github.com/nitayneeman/06e407d4e7f40fee2199dc02f8d2595c.js"></script>

The snapshot doesn&rsquo;t pretend to be the full tree, but rather including just the interesting nodes (those which are acceptable by most of the assistive technologies).

<em><strong>Note:</strong> We can obtain the full tree through setting <code>interestingOnly</code> to false.</em>

<h2 id="code-coverage">Code Coverage</h2>

The code coverage feature was introduced officially as part of <a href="https://developers.google.com/web/updates/2017/04/devtools-release-notes#coverage" target="_blank">Chrome v59</a> - and provides the ability to measure how much code is being used, compared to the code that is actually loaded.
In this manner, we can reduce the dead code and eventually speed up the loading time of the pages.

With Puppeteer, we can manipulate the same feature programmatically:
<script type="application/javascript" src="//gist.github.com/nitayneeman/acd0c653f5bf3dea1a0fd34ce1018099.js"></script>

We instruct Puppeteer to gather coverage information for JavaScript and CSS files, until the page is loaded.
Thereafter, we define <code>calculateUsedBytes</code> which goes through a collected coverage data and calculates how many bytes are being used (based on the coverage).
At last, we merely invoke the created function on both coverages.

Let&rsquo;s look at the output:

<script type="application/javascript" src="//gist.github.com/nitayneeman/48b9f0f7f26210667f3ae1fcc64d2950.js"></script>

As expected, the output contains <code>usedBytes</code> and <code>totalBytes</code> for each file.

<h2 id="measuring-performance">Measuring Performance</h2>

One objective of measuring performance in terms of websites is to analyze how a page performs, during load and runtime - intending to make it faster.

Let&rsquo;s see how we use Puppeteer to measure our page performance:

<strong>1Ô∏è‚É£ - Analyzing load time through metrics</strong>

<a href="https://www.w3.org/TR/navigation-timing/" target="_blank">Navigation Timing</a> is a Web API that provides information and metrics relating to page navigation and load events, and accessible by <code>window.performance</code>.

In order to benefit from it, we should evaluate this API within the page context:
<script type="application/javascript" src="//gist.github.com/nitayneeman/b75273777f14a37f8ec4c04a0a01f9ec.js"></script>

Notice that if <code>evaluate</code> receives a function which returns a non-serializable value - then <code>evaluate</code> returns eventually <code>undefined</code>.
That&rsquo;s exactly why we stringify <code>window.performance</code> when evaluating within the page context.

The result is transformed into a comfy object, which looks like the following:
<script type="application/javascript" src="//gist.github.com/nitayneeman/5b8967e8acdadcee36e774ea24187275.js"></script>

Now we can simply combine these metrics and calculate different load times over the loading timeline.
For instance, <code>loadEventEnd - navigationStart</code> represents the time since the navigation started until the page is loaded.

<em><strong>Note:</strong> All explanations about the different timings above are available <a href="https://www.w3.org/TR/navigation-timing/#processing-model" target="_blank">here</a>.</em>

<strong>2Ô∏è‚É£ - Analyzing runtime through metrics</strong>

As far as the runtime metrics, unlike load time, Puppeteer provides a neat API:
<script type="application/javascript" src="//gist.github.com/nitayneeman/2b2c92450d5f7a5a8ba177293df25282.js"></script>

We invoke the <code>metrics</code> method and get the following result:
<script type="application/javascript" src="//gist.github.com/nitayneeman/10f3528f9f549d68627196e1ba9012a5.js"></script>

The interesting metric above is apparently <code>JSHeapUsedSize</code> which represents, in other words, the actual memory usage of the page.
Notice that the result is actually the output of <code>Performance.getMetrics</code>, which is part of <a href="https://chromedevtools.github.io/devtools-protocol/tot/Performance#method-getMetrics" target="_blank">Chrome DevTools Protocol</a>.

<strong>3Ô∏è‚É£ - Analyzing browser activities through tracing</strong>

<a href="http://www.chromium.org/developers/how-tos/trace-event-profiling-tool" target="_blank">Chromium Tracing</a> is a profiling tool that allows recording what the browser is really doing under the hood - with an emphasis on every thread, tab, and process.
And yet, it&rsquo;s reflected in Chrome DevTools as part of the <a href="https://developers.google.com/web/tools/chrome-devtools/evaluate-performance/timeline-tool" target="_blank">Timeline</a> panel.

Furthermore, this tracing ability is possible with Puppeteer either - which, as we might guess, practically uses the <a href="https://chromedevtools.github.io/devtools-protocol/tot/Tracing" target="_blank">Chrome DevTools Protocol</a>.

For example, let&rsquo;s record the browser activities during navigation:
<script type="application/javascript" src="//gist.github.com/nitayneeman/a7aa5b461498da58790c8b8b0ea1e54e.js"></script>

When the recording is stopped, a file called <code>trace.json</code> is created and contains the output that looks like:
<script type="application/javascript" src="//gist.github.com/nitayneeman/05b903701d236406d98537ddb08d86e3.js"></script>

Now that we&rsquo;ve the trace file, we can open it using Chrome DevTools, <a href="chrome://tracing" target="_blank">chrome://tracing</a> or <a href="https://chromedevtools.github.io/timeline-viewer/" target="_blank">Timeline Viewer</a>.

Here&rsquo;s the Performance panel after importing the trace file into the DevTools:
<figure>
    <img src="https://d33wubrfki0l68.cloudfront.net/e5fee1863ff460eb709686601c2c2d4b723a073f/567c5/images/posts/2019-03-16-getting-to-know-puppeteer-using-practical-examples/getting-to-know-puppeteer11.example.png"
         alt="Importing a trace file" width="640"/> <figcaption>
            <h4>Importing a trace file</h4>
        </figcaption>
</figure>


<h2 id="summary">Summary</h2>

We introduced today the Puppeteer&rsquo;s API through concrete examples.

Let&rsquo;s recap the main points:

<ul>
<li>Puppeteer is a Node.js library for automating, testing and scraping web pages on top of the Chrome DevTools Protocol.</li>
<li>Puppeteer&rsquo;s ecosystem provides a lightweight package, <code>puppeteer-core</code>, which is a library for browser automation - that interacts with any browser, which is based on DevTools protocol, without installing Chromium.</li>
<li>Puppeteer&rsquo;s ecosystem provides a package, which is actually the full product, that installs Chromium in addition to the browser automation library.</li>
<li>Puppeteer provides the ability to launch a Chromium browser instance or just connect an existing instance.</li>
<li>Puppeteer&rsquo;s ecosystem provides an experimental package, <code>puppeteer-firefox</code>, that interacts with Firefox.</li>
<li>The browser context allows separating different sessions for a single browser instance.</li>
<li>Puppeteer launches the browser in a headless mode by default, which merely uses the command line. 
Also - a headful mode, for opening the browser with a GUI, is supported either.</li>
<li>Puppeteer provides several ways to debug our application in the browser, whereas, debugging the process that executes Puppeteer is obviously the same as debugging a regular Node.js process.</li>
<li>Puppeteer allows navigating to a page by a URL and operating the page through the mouse and keyboard.</li>
<li>Puppeteer allows examining a page&rsquo;s visibility, behavior and responsiveness on various devices.</li>
<li>Puppeteer allows taking screenshots of the page and generating PDFs from the content, easily.</li>
<li>Puppeteer allows analyzing and testing the accessibility support in the page.</li>
<li>Puppeteer allows speeding up the page performance by providing information about the dead code, handy metrics and manually tracing ability.</li>
</ul>

And finally, Puppeteer is a powerful browser automation tool with a pretty simple API.
A decent number of capabilities are supported, including such we haven&rsquo;t covered at all - and that&rsquo;s why your next step could definitely be <a href="https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md" target="_blank">the official documentation</a>. 
üòâ

Here&rsquo;s attached the final <a href="https://github.com/nitayneeman/puppeteer-examples" target="_blank">project</a>:
<iframe
        type="text/html"
        async-src="https://stackblitz.com/edit/puppeteer-examples?ctl=1&embed=1&file=02-interacting-page%2fcreating-a-page.js&view=editor"
        frameborder="0">
</iframe>

<h2 id="vs-code-snippets">VS Code Snippets</h2>

Well, if you wish to get some useful code snippets of Puppeteer API for Visual Studio Code - then the following extension might interest you:

<figure>
    <img src="https://github.com/nitayneeman/vscode-puppeteer-snippets/blob/master/images/preview.gif?raw=true"
         alt="Using the snippets to generate a bsic Puppeteer script"/> <figcaption>
            <h4>Using the snippets to generate a basic Puppeteer script</h4>
        </figcaption>
</figure>

You&rsquo;re welcome to take a look at <a href="https://marketplace.visualstudio.com/items?itemName=nitayneeman.puppeteer-snippets" target="_blank">the extension page</a>.

<h2><span class="orange">Puppeteer Tutorial</span></h2>

<em>Most of the browsers are now available with a ‚Äúheadless‚Äù version where a user can interact with a website without any UI</em>. 
You can scrape websites too on these<em> headless browsers</em> using packages like a <em>puppeteer</em> and nodeJS.
Web development heavily relies on testing mechanisms for the quality checks before we push them into the production environment. 
A complex website will require a complex structure of test suites before we deploy it anywhere. 
<em>Headless browsers considerably reduce the testing time involved in web development as there is no overhead of any UI</em>. 
These browsers allow us to crunch more web pages in lesser time.
In this blog, we will learn to scrape websites on these <em>headless browsers</em> using <a href="https://nycdatascience.com/blog/meetup/node-js-workshop-ii-toolkit-to-make-your-work-efficiently/">nodeJS</a> and <a href="https://blog.datahut.co/asynchronous-web-scraping-using-python/">asynchronous programming</a>. 
Before we start with scraping websites, let us learn more about the headless browsers in a bit more detail. 
Furthermore, if you are concerned about the <a href="https://blog.datahut.co/is-web-data-scraping-legal/">legalities of scraping</a>, you can clear your <a href="https://blog.datahut.co/busting-8-myths-about-web-scraping/">myths about web scraping</a>.
 
<h2>What is a headless browser</h2>
<em>A headless browser is simply a browser just without any user interface</em>. 
A <em>headless browser</em>, like a normal browser, consists of all the capabilities of rendering a website. 
Since no GUI is available, one needs to use the command-line utility to interact with the browser. 
 
Headless browsers are designed for tasks like automation testing.
<em>Headless browsers are more flexible, fast and optimised in performing tasks like web-based automation testing. 
Since there is no overhead of any UI, headless browsers are suitable for automated stress testing and web scraping as these tasks can be run more quickly</em>. 
Although vendors like <b>PhantomJS, HtmlUnit</b> have been in the market offering headless browser capabilities for long, browser players like chrome and firefox are also now offering a ‚Äúheadless‚Äù version of their browsers. 
Hence one need not install an extra browser for headless capabilities.
<h2>The need for a headless browser</h2>
With the advancement in the web development frameworks, browsers have become smarter as well to load all the javascript libraries. 
With all the evolution in the web development technologies, testing of the websites has been evolved and has emerged out to be one of the essentials of the web development industry. 
Evolution of headless browsers allow us to perform the following applications

<li>Test automation for web applications<br />End-to-end testing is a methodology used to test whether the flow of an application is performing as designed from start to finish. 
The purpose of carrying out end-to-end tests is to identify system dependencies and to ensure that the right information is passed between various system components and systems. 
<em>Headless browsers</em> were designed to cater to this use case as they enable faster web testing using CLI.</li>
<li>Scraping websites<br /><em>Headless browsers</em> enable faster scraping of the websites as they do not have to deal with the overhead of opening any UI. 
With <em>headless browsers</em>, one can simply automate the scrapping mechanism and extract data in a much more optimised manner.</li>
<li>Taking web screenshots<br /><em>Headless browsers</em> may not offer any GUI experience but they do allow users to take snapshots of the websites they are rendering. 
It certainly helps in cases where one is performing automation testing and want to visualise code effects on the website and store results in the form of screenshots. 
Taking a large number of screenshots without any actual UI is a cakewalk using <em>headless browsers</em>.</li>
<li>Mapping user journey across the websites<br />Companies who successfully deliver outstanding customer experiences consistently do better than their competitors. 
<em>Headless browsers</em> allow us to run programs mapping customer journey test cases to optimise the user experience throughout their decision-making process on the website.</li>

<h2>What is Puppeteer</h2>
<em>Puppeteer is an API library with the DevTools protocol to control Chrome or Chromium. 
It is usually headless but can be set to operate Chrome or Chromium in its whole (non-headless)</em>. 
 
Furthermore, Puppeteer is a library of nodes that we can use to monitor a Chrome instance without heads (UI).
We use Chrome under the hood, but it will be JavaScript programmatically. 
<em>Puppeteer</em> is the Google Chrome team‚Äôs official Chrome <em>headless browser</em>. 
It may not be most effective as it breaks up a fresh Chrome example when it is initialized. 
This is the most accurate way to automate Chrome testing, though because it uses the actual navigator.
<h2>Web scraping using Puppeteer</h2>
In this article, we will be using puppeteer to <a href="https://blog.datahut.co/scraping-amazon-reviews-python-scrapy/">scrape the product listing from a website</a>. 
<em>Puppeteer will use the headless chrome browser to open the web page and query back all the results</em>. 
Before we start actually implementing puppeteer for web scraping, we will look into its setup and installation.
After that, we will implement a simple use case where we will go to an <a href="https://blog.datahut.co/scraping-yahoo-finance-data-using-python/">e-commerce website and search for a product and scrape all the results</a>. 
All the above tasks will be programmatically handled by using puppeteer library. 
Furthermore, we will use the nodeJS language to accomplish the above-defined task.
<h2>Installing puppeteer</h2>
Let us begin with the installation. 
 
<em>Puppeteer</em> is a node javascript library and hence, we will need node js installed on our machine. 
Node js comes with npm (node package manager) which will help us to install the <em>puppeteer</em> package.
The following code snippet will help you in the installation of node js

## Updating the system libraries ##
sudo apt-get update
## Installing node js in the system ##
sudo apt-get install nodejs 
You can use the below command to install the puppeteer package
npm install --save puppeteer
Since we have all the dependencies installed now, we can start implementing our scraping use case using puppeteer. 
We will be controlling actions on the website using our node JS program powered by the puppeteer package.
<h2>Scraping products list using puppeteer</h2>
Step1: Visiting the page and searching for a product
In this section, we will initialise a <em>puppeteer</em> object first. 
This object has access to all the utility functions available in the <em>puppeteer</em> package. 
In this section, our program visits the website, then it searches for the product search bar on the website. 
Upon finding the search elements, it types the product name in the search bar and loads the result. 
We gave the product name to the program using the command line arguments
const puppeteer = require('puppeteer');
const browser = await puppeteer.launch();
const page = await browser.newPage();
var args = process.argv[2]
await page.goto("<a href="https://www.croma.com/">https://www.croma.com/</a>");
await page.click('button.mobile__nav__row--btn-search')
await page.type('input#js-site-search-input', args)
await page.keyboard.press('Enter');
await page.screenshot({path: 'sample.png'})

Step 2: Scraping the list of items
In this section, we are scraping the product listings which we got after searching for our given product. 
HTML selectors were used for capturing web content. 
All the scrapped results were collated together to make the dataset. 
The querySelector function allows us to extract the content from the web page using the HTML selector. 
The querySelectorAll functions get all the content marked with the particular selector whereas querySelector function just returns the first matching element.
let urls = await page.evaluate(() = {
let results = [];
let items = document.querySelectorAll('li.product__list--item');
items.forEach((item) = {
let name = item.querySelector('a.product__list--name').innerText
let price = item.querySelector('span.pdpPrice').innerText
let discount = item.querySelector('div.listingDiscnt').innerText
results.push({
prod_name:  name,
prod_price: price,
prod_discount: discount
});
});
return results;
})
Full Code
Here is the full working sample of the implementation. 
We have wrapped up the entire login in a run function and are logging the scrapped results in the console.
const puppeteer = require('puppeteer');
function run () {
    return new Promise(async (resolve, reject) ={
        try {
            const browser = await puppeteer.launch();
            const page = await browser.newPage();
            var args = process.argv[2]
            await page.goto("<a href="https://www.croma.com/">https://www.croma.com/</a>");
            await page.click('button.mobile__nav__row--btn-search')
            await page.type('input#js-site-search-input', args)
            await page.keyboard.press('Enter');
            await page.screenshot({path: 'sample.png'})
            
            let urls = await page.evaluate(() = {
                let results = [];
                let items = document.querySelectorAll('li.product__list--item');
                items.forEach((item) = {
                    let name = item.querySelector('a.product__list--name').innerText
                    let price = item.querySelector('span.pdpPrice').innerText
                    let discount = item.querySelector('div.listingDiscnt').innerText
                    results.push({
                        prod_name:  name,
                        prod_price: price,
                        prod_discount: discount
                    });
                });
                return results;
            })
            browser.close();
            return resolve(urls);
        } catch (e) {
            return reject(e);
        }
    })
}
run().then(console.log).catch(console.error);
Running the script
You can use the below command to run the above puppeteer script with a headless browser. 
We will use the nodejs to run our code. 
You just have to mention the keyword node and the filename followed by the product name whose data you need to search on the given website and scrape the results.
In this example, we are searching for the iPhones on the Croma website and then we are scrapping the product listings.
node headlessScrape.js iphones

<h2>Scrape a Website Using Node.js and Puppeteer</h2>

<h3>Introduction</h3>
In this tutorial, you will build a web scraping application using <a href="https://www.npmjs.com/">Node.js</a> and <a href="https://pptr.dev/">Puppeteer</a>. 
First, you will code your app to open <a href="https://www.chromium.org/getting-involved/download-chromium">Chromium</a> and load a special website designed as a web-scraping sandbox: <a href="http://books.toscrape.com">books.toscrape.com</a>. 
In the next two steps, you will scrape all the books on a single page of books.toscrape and then all the books across multiple pages. 
In the remaining steps, you will filter your scraping by book category and then save your data as a JSON file.
This tutorial scrapes a special website, <a href="http://books.toscrape.com">books.toscrape.com</a>, which was specifically designed to test scraper applications. 

<h2>Prerequisites</h2>
Node.js installed on your development machine. 

<h2>Step 1 ‚Äî Setting Up the Web Scraper</h2>
With Node.js installed, you can begin setting up your web scraper. 
First, you will create a project root directory and then install the required dependencies. 
This tutorial requires just one dependency, and you will install it using Node.js&rsquo;s default package manager <a href="https://www.digitalocean.com/community/tutorial_series/how-to-code-in-node-js">npm</a>. 
npm comes preinstalled with Node.js, so you don‚Äôt need to install it.
Create a folder for this project and then move inside:
<code>mkdir book-scraper
cd book-scraper</code>
You will run all subsequent commands from this directory.
We need to install one package using npm, or the node package manager. 
First initialize npm in order to create a <code>packages.json</code> file, which will manage your project&rsquo;s dependencies and metadata.
Initialize npm for your project:
<code>npm init</code>
npm will present a sequence of prompts. 
You can press <code>ENTER</code> to every prompt, or you can add personalized descriptions. 
Make sure to press <code>ENTER</code> and leave the default values in place when prompted for <code>entry point:</code> and <code>test command:</code>. 
Alternately, you can pass the <code>y</code> flag to <code>npm</code>‚Äî<code>npm init -y</code>‚Äîand it will submit all the default values for you.
Your output will look something like this:
<code>Output
{
  "name": "sammy_scraper",
  "version": "1.0.0",
  "description": "a web scraper",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1"
  },
  "keywords": [],
  "author": "sammy the shark",
  "license": "ISC"
}
Is this OK? (yes) yes</code>
Type <code>yes</code> and press <code>ENTER</code>. 
npm will save this output as your <code>package.json</code> file.
Now use npm to install Puppeteer:
<code>npm install --save puppeteer</code>
This command installs both Puppeteer and a version of Chromium that the Puppeteer team knows will work with their API.

With npm, Puppeteer, and any additional dependencies installed, your <code>package.json</code> file requires one last configuration before you start coding. 
In this tutorial, you will launch your app from the command line with <code>npm run start</code>. 
You must add some information about this <code>start</code> script to <code>package.json</code>. 
Specifically, you must add one line under the <code>scripts</code> directive regarding your <code>start</code> command.
Open the file in your preferred text editor:
<code>nano package.json</code>
Find the <code>scripts:</code> section and add the following configurations. 
Remember to place a comma at the end of the <code>test</code> script line, or your file will not parse correctly. 

<code>Output
{
  . . .
  "scripts": {
    "test": "echo \"Error: no test specified\" &amp;&amp; exit 1",
    "start": "node index.js"
  },
  . . .
  "dependencies": {
    "puppeteer": "^5.2.1"
  }
}</code>
You will also notice that <code>puppeteer</code> now appears under <code>dependencies</code> near the end of the file. 
Your <code>package.json</code> file will not require any more revisions. 
Save your changes and close your editor. 

You are now ready to start coding your scraper. 
In the next step, you will set up a browser instance and test your scraper&rsquo;s basic functionality.
<h2>Step 2 ‚Äî Setting Up the Browser Instance</h2>
When you open a traditional browser, you can do things like click buttons, navigate with your mouse, type, open the dev tools, and more. 
A headless browser like Chromium allows you to do these same things, but programmatically and without a user interface. 
In this step, you will set up your scraper&rsquo;s browser instance. 
When you launch your application, it will automatically open Chromium and navigate to books.toscrape.com. 
These initial actions will form the basis of your program.
Your web scraper will require four <code>.js</code> files: <code>browser.js, index,js, pageController.js</code>, and <code>pageScraper.js</code>. 
In this step, you will create all four files and then continually update them as your program grows in sophistication. 
Start with <code>browser.js</code>; this file will contain the script that starts your browser.
From your project&rsquo;s root directory, create and open <code>browser.js</code> in a text editor:
<code>nano browser.js</code>
First, you will <code>require</code> Puppeteer and then create an <code>async</code> function called <code>startBrowser()</code>. 
This function will start the browser and return an instance of it. 
Add the following code:
./book-scraper/browser.js
<code>const puppeteer = require('puppeteer');
async function startBrowser(){
    let browser;
    try {
        console.log("Opening the browser......");
        browser = await puppeteer.launch({
            headless: false,
            args: ["--disable-setuid-sandbox"],
            'ignoreHTTPSErrors': true
        });
    } catch (err) {
        console.log("Could not create a browser instance =&gt; : ", err);
    }
    return browser;
}
module.exports = {
    startBrowser
};</code>
<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#puppeteerlaunchoptions">Puppeteer has a <code>.launch()</code> method</a> that launches an instance of a browser. 
This method returns a <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promise</a>, so you have to <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Asynchronous/Async_await">make sure the Promise resolves by using a <code>.then</code> or <code>await</code> block</a>.
You are using <code>await</code> to make sure the Promise resolves, wrapping this instance around <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/try...catch">a <code>try-catch</code> code block</a>, and then returning an instance of the browser.
Notice that the <code>.launch()</code> method takes a JSON parameter with several values:
<strong>headless</strong> - <code>false</code> means the browser will run with an Interface so you can watch your script execute, while <code>true</code> means the browser will run in headless mode. 
Note well, however, that if you want to deploy your scraper to the cloud, set <code>headless</code> back to <code>true</code>. 
Most virtual machines are headless and do not include a user interface, and hence can only run the browser in headless mode. 
Puppeteer also includes a <code>headful</code> mode, but that should be used solely for testing purposes.
<strong>ignoreHTTPSErrors</strong> - <code>true</code> allows you to visit websites that aren&rsquo;t hosted over a secure HTTPS protocol and ignore any HTTPS-related errors.
Save and close the file.
Now create your second <code>.js</code> file, <code>index.js</code>:
<code>nano index.js</code>
Here you will <code>require</code> <code>browser.js</code> and <code>pageController.js</code>. 
You will then call the <code>startBrowser()</code> function and pass the created browser instance to our page controller, which will direct its actions. 
Add the following code:
./book-scraper/index.js
<code>const browserObject = require('./browser');
const scraperController = require('./pageController');
//Start the browser and create a browser instance
let browserInstance = browserObject.startBrowser();
// Pass the browser instance to the scraper controller
scraperController(browserInstance)</code>
Save and close the file.
Create your third <code>.js</code> file, <code>pageController.js</code>:
<code>nano pageController.js</code>
<code>pageController.js</code> controls your scraping process. 
It uses the browser instance to control the <code>pageScraper.js</code> file, which is where all the scraping scripts execute. 
Eventually, you will use it to specify what book category you want to scrape. 
For now, however, you just want to make sure that you can open Chromium and navigate to a web page:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        await pageScraper.scraper(browser); 
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
This code exports a function that takes in the browser instance and passes it to a function called <code>scrapeAll()</code>. 
This function, in turn, passes this instance to <code>pageScraper.scraper()</code> as an argument which uses it to scrape pages.
Save and close the file.
Finally, create your last <code>.js</code> file, <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Here you will create an object literal with a <code>url</code> property and a <code>scraper()</code> method. 
The <code>url</code> is the web URL of the web page you want to scrape, while the <code>scraper()</code> method contains the code that will perform your actual scraping, although at this stage it merely navigates to a URL. 
Add the following code:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        await page.goto(this.url);
    }
}
module.exports = scraperObject;</code>
<a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#browsernewpage">Puppeteer has a <code>newPage()</code> method</a> that creates a new page instance in the browser, and these page instances can do quite a few things. 
In our <code>scraper()</code> method, you created a page instance and then used the <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagegotourl-options"><code>page.goto()</code> method</a> to navigate to <a href="http://books.toscrape.com">the books.toscrape.com homepage</a>.
Save and close the file.
Your program&rsquo;s file-structure is now complete. 
The first level of your project&rsquo;s directory tree will look like this:
<code>Output.
‚îú‚îÄ‚îÄ browser.js
‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ node_modules
‚îú‚îÄ‚îÄ package-lock.json
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ pageController.js
‚îî‚îÄ‚îÄ pageScraper.js</code>
Now run the command <code>npm run start</code> and watch your scraper application execute:
<code>npm run start</code>
It will automatically open a Chromium browser instance, open a new page in the browser, and navigate to books.toscrape.com.
In this step, you created a Puppeteer application that opened Chromium and loaded the homepage for a dummy online bookstore‚Äîbooks.toscrape.com. 
In the next step, you will scrape the data for every book on that homepage.
<h2>Step 3 ‚Äî Scraping Data from a Single Page</h2>
Before adding more functionality to your scraper application, open your preferred web browser and manually navigate to the <a href="http://books.toscrape.com/">books to scrape homepage</a>. 
Browse the site and get a sense of how data is structured.
<img src="https://assets.digitalocean.com/articles/67187/web_scraper.png" alt="Books to scrape websites image">
You will find a category section on the left and books displayed on the right. 
When you click on a book, the browser navigates to a new URL that displays relevant information regarding that particular book.
In this step, you will replicate this behavior, but with code; you will automate the business of navigating the website and consuming its data.
First, if you inspect the source code for the homepage using the Dev Tools inside your browser, you will notice that the page lists each book&rsquo;s data under a <code>section</code> tag. 
Inside the <code>section</code> tag every book is under a <code>list</code> (<code>li</code>) tag, and it is here that you find the link to the book&rsquo;s dedicated page, the price, and the in-stock availability.
<img src="https://assets.digitalocean.com/articles/67187/bookstoscrape_devtools.png" alt="books.toscrape source code viewed in dev tools">
You&rsquo;ll be scraping these book URLs, filtering for books that are in-stock, navigating to each individual book page, and scraping that book&rsquo;s data.
Reopen your <code>pageScraper.js</code> file:
<code>nano pageScraper.js</code>
Add the following highlighted content. 
You will nest another <code>await</code> block inside <code>await page.goto(this.url);</code>:
./book-scraper/pageScraper.js
<code>
const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Wait for the required DOM to be rendered
        await page.waitForSelector('.page_inner');
        // Get the link to all the required books
        let urls = await page.$$eval('section ol &gt; li', links =&gt; {
            // Make sure the book to be scraped is in stock
            links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
            // Extract the links from the data
            links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
            return links;
        });
        console.log(urls);
    }
}
module.exports = scraperObject;</code>
In this code block, you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pagewaitforselectorselector-options">the <code>page.waitForSelector()</code> method</a>. 
This waited for the div that contains all the book-related information to be rendered in the DOM, and then you called <a href="https://github.com/puppeteer/puppeteer/blob/v5.2.1/docs/api.md#pageevalselector-pagefunction-args">the <code>page.$$eval()</code> method</a>. 
This method gets the URL element with the selector <code>section ol li</code> (be sure that you always return only a string or a number from the <code>page.$eval()</code> and <code>page.$$eval()</code> methods).
Every book has two statuses; a book is either <code>In Stock</code> or <code>Out of stock</code>. 
You only want to scrape books that are <code>In Stock</code>. 
Because <code>page.$$eval()</code> returns an array of all matching elements, you have filtered this array to ensure that you are only working with in-stock books. 
You did this by searching for and evaluating the class <code>.instock.availability</code>. 
You then mapped out the <code>href</code> property of the book links and returned it from the method.
Save and close the file.
Re-run your application:
<code>npm run start</code>
The browser will open, navigate to the web page, and then close once the task completes. 
Now check your console; it will contain all the scraped URLs:
<code>Output
&gt; book-scraper@1.0.0 start /Users/sammy/book-scraper
&gt; node index.js
Opening the browser......
Navigating to http://books.toscrape.com...
[
  'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html',
  'http://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html',
  'http://books.toscrape.com/catalogue/soumission_998/index.html',
  'http://books.toscrape.com/catalogue/sharp-objects_997/index.html',
  'http://books.toscrape.com/catalogue/sapiens-a-brief-history-of-humankind_996/index.html',
  'http://books.toscrape.com/catalogue/the-requiem-red_995/index.html',
  'http://books.toscrape.com/catalogue/the-dirty-little-secrets-of-getting-your-dream-job_994/index.html',
  'http://books.toscrape.com/catalogue/the-coming-woman-a-novel-based-on-the-life-of-the-infamous-feminist-victoria-woodhull_993/index.html',
  'http://books.toscrape.com/catalogue/the-boys-in-the-boat-nine-americans-and-their-epic-quest-for-gold-at-the-1936-berlin-olympics_992/index.html',
  'http://books.toscrape.com/catalogue/the-black-maria_991/index.html',
  'http://books.toscrape.com/catalogue/starving-hearts-triangular-trade-trilogy-1_990/index.html',
  'http://books.toscrape.com/catalogue/shakespeares-sonnets_989/index.html',
  'http://books.toscrape.com/catalogue/set-me-free_988/index.html',
  'http://books.toscrape.com/catalogue/scott-pilgrims-precious-little-life-scott-pilgrim-1_987/index.html',
  'http://books.toscrape.com/catalogue/rip-it-up-and-start-again_986/index.html',
  'http://books.toscrape.com/catalogue/our-band-could-be-your-life-scenes-from-the-american-indie-underground-1981-1991_985/index.html',
  'http://books.toscrape.com/catalogue/olio_984/index.html',
  'http://books.toscrape.com/catalogue/mesaerion-the-best-science-fiction-stories-1800-1849_983/index.html',
  'http://books.toscrape.com/catalogue/libertarianism-for-beginners_982/index.html',
  'http://books.toscrape.com/catalogue/its-only-the-himalayas_981/index.html'
]</code>
This is a great start, but you want to scrape all the relevant data for a particular book and not only its URL. 
You will now use these URLs to open each page and scrape the book&rsquo;s title, author, price, availability, UPC, description, and image URL.
Reopen <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Add the following code, which will loop through each scraped link, open a new page instance, and then retrieve the relevant data:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Wait for the required DOM to be rendered
        await page.waitForSelector('.page_inner');
        // Get the link to all the required books
        let urls = await page.$$eval('section ol &gt; li', links =&gt; {
            // Make sure the book to be scraped is in stock
            links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
            // Extract the links from the data
            links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
            return links;
        });
        // Loop through each of those links, open a new page instance and get the relevant data from them
        let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
            let dataObj = {};
            let newPage = await browser.newPage();
            await newPage.goto(link);
            dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
            dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
            dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                // Strip new line and tab spaces
                text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                // Get the number of stock available
                let regexp = /^.*\((.*)\).*$/i;
                let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                return stockAvailable;
            });
            dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
            dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
            dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
            resolve(dataObj);
            await newPage.close();
        });
        for(link in urls){
            let currentPageData = await pagePromise(urls[link]);
            // scrapedData.push(currentPageData);
            console.log(currentPageData);
        }
    }
}
module.exports = scraperObject; </code>
You have an array of all URLs. 
You want to loop through this array, open up the URL in a new page, scrape data on that page, close that page, and open a new page for the next URL in the array. 
Notice that you wrapped this code in a Promise. 
This is because you want to be able to wait for each action in your loop to complete. 
Therefore each Promise opens a new URL and won&rsquo;t resolve until the program has scraped all the data on the URL, and then that page instance has closed.
<strong>Warning:</strong> note well that you waited for the Promise using a <code>for-in</code> loop. 
Any other loop will be sufficient but avoid iterating over your URL arrays using an array-iteration method like <code>forEach</code>, or any other method that uses a callback function. 
This is because the callback function will have to go through the callback queue and event loop first, hence, multiple page instances will open all at once. 
This will place a much larger strain on your memory.

Take a closer look at your <code>pagePromise</code> function. 
Your scraper first created a new page for each URL, and then you used the <code>page.$eval()</code> function to target selectors for relevant details that you wanted to scrape on the new page. 
Some of the texts contain whitespaces, tabs, newlines, and other non-alphanumeric characters, which you stripped off using a regular expression. 
You then appended the value for every piece of data scraped in this page to an Object and resolved that object.
Save and close the file.
Run the script again:
<code>npm run start</code>
The browser opens the homepage and then opens each book page and logs the scraped data from each of those pages. 
This output will print to your console:
<code>Output
Opening the browser......
Navigating to http://books.toscrape.com...
{
  bookTitle: 'A Light in the Attic',
  bookPrice: '¬£51.77',
  noAvailable: '22',
  imageUrl: 'http://books.toscrape.com/media/cache/fe/72/fe72f0532301ec28892ae79a629a293c.jpg',
  bookDescription: "It's hard to imagine a world without A Light in the Attic. 
[...]',
  upc: 'a897fe39b1053632'
}
{
  bookTitle: 'Tipping the Velvet',
  bookPrice: '¬£53.74',
  noAvailable: '20',
  imageUrl: 'http://books.toscrape.com/media/cache/08/e9/08e94f3731d7d6b760dfbfbc02ca5c62.jpg',
  bookDescription: `"Erotic and absorbing...Written with starling power."--"The New York Times Book Review " Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler [...]`,
  upc: '90fa61229261140a'
}
{
  bookTitle: 'Soumission',
  bookPrice: '¬£50.10',
  noAvailable: '20',
  imageUrl: 'http://books.toscrape.com/media/cache/ee/cf/eecfe998905e455df12064dba399c075.jpg',
  bookDescription: 'Dans une France assez proche de la n√¥tre, [...]',
  upc: '6957f44c3847a760'
}
...</code>
In this step, you scraped relevant data for every book on the homepage of books.toscrape.com, but you could add much more functionality. 
Each page of books, for instance, is paginated; how do you get books from these other pages? Also, on the left side of the website you found book categories; what if you don&rsquo;t want all the books, but you just want books from a particular genre? You will now add these features.
<h2>Step 4 ‚Äî Scraping Data From Multiple Pages</h2>
Pages on books.toscrape.com that are paginated have a <code>next</code> button beneath their content, while pages that are not paginated do not.
You will use the presence of this button to determine if the page is paginated or not. 
Since the data on each page is of the same structure and has the same markup, you won&rsquo;t be writing a scraper for every possible page. 
Rather, you will use the practice of <a href="https://www.digitalocean.com/community/tutorials/js-understanding-recursion">recursion</a>.
First, you need to change the structure of your code a bit to accommodate recursively navigating to several pages.
Reopen <code>pagescraper.js</code>:
<code>nano pagescraper.js</code>
You will add a new function called <code>scrapeCurrentPage()</code> to your <code>scraper()</code> method. 
This function will contain all the code that scrapes data from a particular page and then click the next button if it exists. 
Add the following highlighted code:
./book-scraper/pageScraper.js scraper()
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        let scrapedData = [];
        // Wait for the required DOM to be rendered
        async function scrapeCurrentPage(){
            await page.waitForSelector('.page_inner');
            // Get the link to all the required books
            let urls = await page.$$eval('section ol &gt; li', links =&gt; {
                // Make sure the book to be scraped is in stock
                links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
                // Extract the links from the data
                links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
                return links;
            });
            // Loop through each of those links, open a new page instance and get the relevant data from them
            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
                let dataObj = {};
                let newPage = await browser.newPage();
                await newPage.goto(link);
                dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
                dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
                dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                    // Strip new line and tab spaces
                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                    // Get the number of stock available
                    let regexp = /^.*\((.*)\).*$/i;
                    let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                    return stockAvailable;
                });
                dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
                dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
                dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
                resolve(dataObj);
                await newPage.close();
            });
            for(link in urls){
                let currentPageData = await pagePromise(urls[link]);
                scrapedData.push(currentPageData);
                // console.log(currentPageData);
            }
            // When all the data on this page is done, click the next button and start the scraping of the next page
            // You are going to check if this button exist first, so you know if there really is a next page.
            let nextButtonExist = false;
            try{
                const nextButton = await page.$eval('.next &gt; a', a =&gt; a.textContent);
                nextButtonExist = true;
            }
            catch(err){
                nextButtonExist = false;
            }
            if(nextButtonExist){
                await page.click('.next &gt; a');   
                return scrapeCurrentPage(); // Call this function recursively
            }
            await page.close();
            return scrapedData;
        }
        let data = await scrapeCurrentPage();
        console.log(data);
        return data;
    }
}
module.exports = scraperObject;</code>
You set the <code>nextButtonExist</code> variable to false initially, and then check if the button exists. 
If the <code>next</code> button exists, you set <code>nextButtonExists</code> to <code>true</code> and proceed to click the <code>next</code> button, and then call this function recursively.
If <code>nextButtonExists</code> is false, it returns the <code>scrapedData</code> array as usual.
Save and close the file.
Run your script again:
<code>npm run start</code>
This might take a while to complete; your application, after all, is now scraping the data from over 800 books. 
Feel free to either close the browser or press <code>CTRL + C</code> to cancel the process.
You have now maximized your scraper&rsquo;s capabilities, but you&rsquo;ve created a new problem in the process. 
Now the issue is not too little data but too much data. 
In the next step, you will fine-tune your application to filter your scraping by book category.
<h2>Step 5 ‚Äî Scraping Data by Category</h2>
To scrape data by category, you will need to modify both your <code>pageScraper.js</code> file and your <code>pageController.js</code> file.
Open <code>pageController.js</code> in a text editor:
<code>nano pageController.js</code>
Call the scraper so that it only scrapes travel books. 
Add the following code:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        await browser.close();
        console.log(scrapedData)
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
You are now passing two parameters into your <code>pageScraper.scraper()</code> method, with the second parameter being the category of books you want to scrape, which in this example is <code>Travel</code>. 
But your <code>pageScraper.js</code> file does not recognize this parameter yet. 
You will need to adjust this file, too.
Save and close the file.
Open <code>pageScraper.js</code>:
<code>nano pageScraper.js</code>
Add the following code, which will add your category parameter, navigate to that category page, and then begin scraping through the paginated results:
./book-scraper/pageScraper.js
<code>const scraperObject = {
    url: 'http://books.toscrape.com',
    async scraper(browser, category){
        let page = await browser.newPage();
        console.log(`Navigating to ${this.url}...`);
        // Navigate to the selected page
        await page.goto(this.url);
        // Select the category of book to be displayed
        let selectedCategory = await page.$$eval('.side_categories &gt; ul &gt; li &gt; ul &gt; li &gt; a', (links, _category) =&gt; {
            // Search for the element that has the matching text
            links = links.map(a =&gt; a.textContent.replace(/(\r\n\t|\n|\r|\t|^\s|\s$|\B\s|\s\B)/gm, "") === _category ? a : null);
            let link = links.filter(tx =&gt; tx !== null)[0];
            return link.href;
        }, category);
        // Navigate to the selected category
        await page.goto(selectedCategory);
        let scrapedData = [];
        // Wait for the required DOM to be rendered
        async function scrapeCurrentPage(){
            await page.waitForSelector('.page_inner');
            // Get the link to all the required books
            let urls = await page.$$eval('section ol &gt; li', links =&gt; {
                // Make sure the book to be scraped is in stock
                links = links.filter(link =&gt; link.querySelector('.instock.availability &gt; i').textContent !== "In stock")
                // Extract the links from the data
                links = links.map(el =&gt; el.querySelector('h3 &gt; a').href)
                return links;
            });
            // Loop through each of those links, open a new page instance and get the relevant data from them
            let pagePromise = (link) =&gt; new Promise(async(resolve, reject) =&gt; {
                let dataObj = {};
                let newPage = await browser.newPage();
                await newPage.goto(link);
                dataObj['bookTitle'] = await newPage.$eval('.product_main &gt; h1', text =&gt; text.textContent);
                dataObj['bookPrice'] = await newPage.$eval('.price_color', text =&gt; text.textContent);
                dataObj['noAvailable'] = await newPage.$eval('.instock.availability', text =&gt; {
                    // Strip new line and tab spaces
                    text = text.textContent.replace(/(\r\n\t|\n|\r|\t)/gm, "");
                    // Get the number of stock available
                    let regexp = /^.*\((.*)\).*$/i;
                    let stockAvailable = regexp.exec(text)[1].split(' ')[0];
                    return stockAvailable;
                });
                dataObj['imageUrl'] = await newPage.$eval('#product_gallery img', img =&gt; img.src);
                dataObj['bookDescription'] = await newPage.$eval('#product_description', div =&gt; div.nextSibling.nextSibling.textContent);
                dataObj['upc'] = await newPage.$eval('.table.table-striped &gt; tbody &gt; tr &gt; td', table =&gt; table.textContent);
                resolve(dataObj);
                await newPage.close();
            });
            for(link in urls){
                let currentPageData = await pagePromise(urls[link]);
                scrapedData.push(currentPageData);
                // console.log(currentPageData);
            }
            // When all the data on this page is done, click the next button and start the scraping of the next page
            // You are going to check if this button exist first, so you know if there really is a next page.
            let nextButtonExist = false;
            try{
                const nextButton = await page.$eval('.next &gt; a', a =&gt; a.textContent);
                nextButtonExist = true;
            }
            catch(err){
                nextButtonExist = false;
            }
            if(nextButtonExist){
                await page.click('.next &gt; a');   
                return scrapeCurrentPage(); // Call this function recursively
            }
            await page.close();
            return scrapedData;
        }
        let data = await scrapeCurrentPage();
        console.log(data);
        return data;
    }
}
module.exports = scraperObject;</code>
This code block uses the category that you passed in to get the URL where the books of that category reside.
The <code>page.$$eval()</code> can take in arguments by passing the argument as a third parameter to the <code>$$eval()</code> method, and defining it as the third parameter in the callback as such:
example page.$$eval() function
<code>page.$$eval('selector', function(elem, args){
    // .......
}, args)</code>
This was what you did in your code; you passed the category of books you wanted to scrape, mapped through all the categories to check which one matches, and then returned the URL of this category.
This URL is then used to navigate to the page that displays the category of books you want to scrape using the <code>page.goto(selectedCategory)</code> method.
Save and close the file.
Run your application again. 
You will notice that it navigates to the <code>Travel</code> category, recursively opens books in that category page by page, and logs the results:
<code>npm run start</code>
In this step, you scraped data across multiple pages and then scraped data across multiple pages from one particular category. 
In the final step, you will modify your script to scrape data across multiple categories and then save this scraped data to a stringified JSON file.
<h2>Step 6 ‚Äî Scraping Data from Multiple Categories and Saving the Data as JSON</h2>
In this final step, you will make your script scrape data off of as many categories as you want and then change the manner of your output. 
Rather than logging the results, you will save them in a structured file called <code>data.json</code>.
You can quickly add more categories to scrape; doing so requires only one additional line per genre.
Open <code>pageController.js</code>:
<code>nano pageController.js</code>
Adjust your code to include additional categories. 
The example below adds <code>HistoricalFiction</code> and <code>Mystery</code> to our existing <code>Travel</code> category:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        scrapedData['HistoricalFiction'] = await pageScraper.scraper(browser, 'Historical Fiction');
        scrapedData['Mystery'] = await pageScraper.scraper(browser, 'Mystery');
        await browser.close();
        console.log(scrapedData)
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
Save and close the file.
Run the script again and watch it scrape data for all three categories:
<code>npm run start</code>
With the scraper fully-functional, your final step involves saving your data in a more useful format. 
You will now store it in a JSON file using <a href="https://nodejs.org/api/fs.html">the <code>fs</code> module in Node.js</a>.
First, reopen <code>pageController.js</code>:
<code>nano pageController.js</code>
Add the following highlighted code:
./book-scraper/pageController.js
<code>const pageScraper = require('./pageScraper');
const fs = require('fs');
async function scrapeAll(browserInstance){
    let browser;
    try{
        browser = await browserInstance;
        let scrapedData = {};
        // Call the scraper for different set of books to be scraped
        scrapedData['Travel'] = await pageScraper.scraper(browser, 'Travel');
        scrapedData['HistoricalFiction'] = await pageScraper.scraper(browser, 'Historical Fiction');
        scrapedData['Mystery'] = await pageScraper.scraper(browser, 'Mystery');
        await browser.close();
        fs.writeFile("data.json", JSON.stringify(scrapedData), 'utf8', function(err) {
            if(err) {
                return console.log(err);
            }
            console.log("The data has been scraped and saved successfully! View it at './data.json'");
        });
    }
    catch(err){
        console.log("Could not resolve the browser instance =&gt; ", err);
    }
}
module.exports = (browserInstance) =&gt; scrapeAll(browserInstance)</code>
First, you are requiring Node,js&rsquo;s <code>fs</code> module in <code>pageController.js</code>. 
This ensures that you can save your data as a JSON file. 
Then you are adding code so that when the scraping completes and the browser closes, the program will create a new file called <code>data.json</code>. 
Note that the contents of <code>data.json</code> are <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON/stringify">stringified JSON</a>. 
Therefore, when reading the content of <code>data.json</code>, always <a href="https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON">parse it as JSON</a> before reusing the data.
Save and close the file.
You have now built a web-scraping application that scrapes books across multiple categories and then stores your scraped data in a JSON file. 
As your application grows in complexity, you might want to store this scraped data in a database or serve it over an API. 
How this data is consumed is really up to you.
<h2>Conclusion</h2>
In this tutorial, you built a web crawler that scraped data across multiple pages recursively and then saved it in a JSON file. 
In short, you learned a new way to automate data-gathering from websites.
Puppeteer has quite a lot of features that were not within the scope of this tutorial. 
To learn more, check out <a href="https://www.digitalocean.com/community/tutorials/tooling-puppeteer">Using Puppeteer for Easy Control Over Headless Chrome</a>. 
You can also visit <a href="http://pptr.dev/">Puppeteer&rsquo;s official documentation</a>.


<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});

</script>
</pre></body></html>
