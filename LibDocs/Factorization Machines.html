<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<style>

a:hover,a:active{color:red}
table.w3-table-all{margin:20px 0}
.top {
 position:relative;
 background-color:black;
 height:68px;
 padding-top:20px;
 line-height:50px;
 overflow:hidden;
 z-index:2;
}
body {
 background-color: #000000;
 color: MediumSeaGreen;
 margin-left: 14%;
 margin-right: 14%;
 font-size: 24px;
}
a { text-decoration: none;
	color: #58D858;}
a:visited { color: #88C898;}
A:hover {	color: yellow;}
A:focus {	color: red;}
code { color: gray; background-color: #001010; font-size: 18px;}
pre { color: gray; background-color: #001010; font-size: 18px;}
h1, h2, h3, h4, h5, .goldword {
	color: gold;
}
table{
	width: 100%;
	font-size: 20px;
	border-collapse: collapse;
	border: 1px solid gray;
}
th{
	border: 1px solid gray;
	font-weight:bold;
	color: lightgreen;
}
td{
	padding:10px;
	border: 1px dotted dimgray;
}
tr>th:first-child{
	width:40%;
}
tr>td:first-child{
	color: lime;

}
img{
	background-color: white;
	margin-top:1%;
	margin-bottom:2%;
}
.topic{
    color: lime;
}
.goldsha {
    color: white;
    border: 1px solid gold;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px gold inset;
}
.redsha {
    color: gold;
    border: 1px solid red;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px red inset;
}
.whitesha {
    color: red;
    border: 1px solid white;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -3px -2px 3px white inset;
}
.orangesha {
    color: yellow;
    border: 1px solid orange;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px orange inset;
}
.yellowsha {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
	display: inline-block;
}
.greensha {
    color: lightblue;
    border: 1px solid green;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px green inset;
}
.left {
    position: absolute;
    left: 100px;
    color: GoldenRod;
    border: 1px solid GoldenRod;
    padding: 2px;
    font-size: 60%;
}
.bord {
    color: redpink;
    border: 1px solid GoldenRod;
    padding: 1px;
    font-size: 90%;
}
.yellowbord {
    color: lime;
    border: 1px solid yellow;
    padding: 2px;
    border-radius: 3px;
	box-shadow: 3px 3px 3px silver;
}
.bluebord {
    color: white;
    border: 1px solid lightblue;
    padding: 2px;
    border-radius: 3px;
	box-shadow: -2px -2px 3px silver inset;
}
.highlight { 
    color: white;
    background-color: #002030
  }
hr {width: 50%;}
li{
	list-style-type: decimal;
}
#toc, #tang, #san, #pill {
	margin-left: 15%;
	margin-right: 15%;
	color: gold;
	padding: 1%;
	text-align: left;
	box-shadow: 5px 5px 15px silver;
	border-radius: 5px;
	border: 1px solid DarkSlateGray;
    font-size: 90%;
}
.mywords{
    color: Crimson;
}
.orangeword{
    color: orange;
}
.remarks {
	font-size: 22px;
	color: MediumSeaGreen;
}
</STYLE>

<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, .topic').click(function(){
    parent.history.back();
    return false;
    });
});
</script>


</head>
<body>

<center><h3>Factorization Machines &amp; their application on huge datasets (with codes in Python)</h3></center>
<div id="toc"><ul></ul></div>


<h2>
Introduction</h2>


<p>
I had even procured a machine with 16 GB RAM and i7 processor. But the first look at the dataset gave me jitters. The data when unzipped was over 50 GB &#8211; I had no clue how to predict a click on such a dataset. Thankfully Factorization machines came to my rescue.</em>
</p>

<p>
However, in most cases these datasets are sparse (only a few variables for each training example are non zero) due to which there are several features which are not important for prediction, this is where factorization helps to extract the most important latent or hidden features from the existing raw ones.</p>

<p>
Factorization helps in representing  approximately the same relationship between the target and predictors using a lower dimension dense matrix. In this article, I discuss Factorization Machines(FM) and Field Aware Factorization Machines(FFM) which allows us to take advantage of factorization in a regression/classification problem with an implementation using python.</p>


<h2>
Table of Contents</h2>

<ol>

<li>
Intuition behind Factorization</li>

<li>
How Factorization Machines trump Polynomial and linear models?</li>

<li>
Field Aware Factorization Machines (FFMs)</li>

<li>
Implementation using xLearn Library in Python</li>

</ol>


<h2>
Intuition behind Factorization</h2>

<p>
To get an intuitive understanding of matrix factorization, Let us consider an example: Suppose we have a user-movie matrix of ratings(1-5) where each value of the matrix represents rating (1-5) given by the user to the movie.</p>

<table class=" aligncenter" style="height: 146px;" width="621">

<tbody>

<tr>

<td style="text-align: center;">
</td>

<td style="text-align: center;">
Star Wars I</td>

<td style="text-align: center;">
Inception</td>

<td style="text-align: center;">
Godfather</td>

<td style="text-align: center;">
The Notebook</td>

</tr>

<tr>

<td style="text-align: center;">
U1</td>

<td style="text-align: center;">
5</td>

<td style="text-align: center;">
3</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
1</td>

</tr>

<tr>

<td style="text-align: center;">
U2</td>

<td style="text-align: center;">
4</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
1</td>

</tr>

<tr>

<td style="text-align: center;">
U3</td>

<td style="text-align: center;">
1</td>

<td style="text-align: center;">
1</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
5</td>

</tr>

<tr>

<td style="text-align: center;">
U4</td>

<td style="text-align: center;">
1</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
4</td>

</tr>

<tr>

<td style="text-align: center;">
U5</td>

<td style="text-align: center;">
&#8211;</td>

<td style="text-align: center;">
1</td>

<td style="text-align: center;">
5</td>

<td style="text-align: center;">
4</td>

</tr>

</tbody>

</table>


<p class="c2">
We observe from the table above that some of the ratings are missing and we would like to devise a method to predict these missing ratings. The intuition behind using matrix factorization to solve this problem is that there should be some latent features that determines how a user rates a movie. <span class="mywords">For example &#8211;  users A and B would rate an Al Pacino movie highly if both of them are fans of actor Al Pacino, here a preference towards a particular actor would be a hidden feature since we are not explicitly including it in the rating matrix</span>.</p>

<p class="c2">
Suppose we want to compute K hidden or latent features. Our task is to find out the matrices P(U x K) and Q(D x K) (U &#8211; Users, D &#8211; Movies) such that P x Q<sup>
T</sup>
 approximates R which is the rating matrix. 
<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20174727/user_movie.jpg"><br>
 Now, each row of P will represent strength of association between user and the feature while each row of Q represents the same strength w.r.t. the movie. To get the rating of a movie d<sub>
j</sub>
 rated by user u<sub>
i</sub>
, we can calculate the dot product of 2 vectors corresponding to u<sub>
i</sub>
 and d<sub>
j</sub>
 
<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20190955/rating_eqn.jpg"><br>
 All we need to do now is calculate P and Q matrices. <span class="c3">
We use <b class="mywords">gradient descent algorithm</b> for doing this. The objective is to minimize the squared error between the actual rating and the one estimated by P and Q. The squared error is given by the following equation.</p>

<p>

<br>
<img src="https://lh6.googleusercontent.com/XiAvQAeN307wZkPtseFLWUEjTsX09uqRJK0x_0JCirTfkq7FevkwiUFJCuVk9yOnc1-FUOMJOx5Y8Td_kH-D0bxm3ctXFpkNRxEovnI0EH8dEs8OwNbgOeXWU-d8mr-VHH4B2t7W">
</p>

<p>
<span class="c3">
Now, we need to define an update rule for p<sub>
ik</sub>
 and q<sub>
kj</sub>
. The update rule in gradient descent is defined by the gradient of the error to be minimized. </p>

<p>

<br>
<img src="https://lh5.googleusercontent.com/mzntHYSM-QRFJf07tPXNOiPfx_ObBtddkaJZP41NSmH9RGzB54La_snj2rQRLs4n4s5jO-_YwpmZIWOMGXfbyJTBd1KE0Ini7qwsQ2TXMjgYlIyvBfyIdHpx8AEmmC1aForGCHjb">
</p>

<p>
<span class="c3">
Having obtained the gradient, we can now formulate the update rules for both p<sub>
ik</sub>
 and q<sub>
kj</sub>
</p>

<p>

<br>
<img src="https://lh4.googleusercontent.com/vNZyLjLaoTHgsX9tOb9ajPgQSk_kmDwUOgHHZqZ_pQBY0ENQqCrYKGtawmpLy0ddiT0kFDZqW3fg9dB8t8qYhJE4da_kqyjm5v4EHM0t4BN4v7XRiD96ppNLYiukR4Ph0yg7YZSL">
</p>

<p class="c0">
<span class="c3">
Here, Î± is the learning rate which can control the size of updates. Using the above update rules, we can then iteratively perform the operation until the error converges to its minimum. We can check the overall error as calculated using the following equation and determine when we should stop the process.</p>

<p>

<br>
<img src="https://lh4.googleusercontent.com/D-oSvZYqsQjady0R-HOrc3GHPEg33DeX2obZ1VDv4Va3jHMMhwTnVQnT4W74GXDAMlAX4u0zv-j1xYx_9szQkWj0U6T-FrPQY2ZbkLHdlS9jb81S9x59P8VJH0rDIYRObqHthDAt">
</p>

<p class="c2">
The above solution is simple and often leads to overfitting where the existing ratings are predicted accurately but it does not generalize well on unseen data. To tackle this we can introduce a <span class="c14">
<a class="c18" href="https://www.google.com/url?q=https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/&amp;sa=D&amp;ust=1515084014508000&amp;usg=AFQjCNGeovNeJeL5YcXc3vBpiSgiIsAP1A">
regularization</a>
 parameter Î² which will control the user-feature and movie-feature vectors in P and Q respectively and give a good approximation for the ratings.</p>

<p class="c2">
For anyone interested in python implementation and exact details of the same may go to this<a class="c18" href="https://www.google.com/url?q=http://www.quuxlabs.com/wp-content/uploads/2010/09/mf.py_.txt&amp;sa=D&amp;ust=1515084014509000&amp;usg=AFQjCNEziNtvHvuydZedRoy4ufVXpcZkNg">
 </a>
<span class="c14">
<a class="c18" href="https://www.google.com/url?q=http://www.quuxlabs.com/wp-content/uploads/2010/09/mf.py_.txt&amp;sa=D&amp;ust=1515084014509000&amp;usg=AFQjCNEziNtvHvuydZedRoy4ufVXpcZkNg">
link</a>
. Once we have calculated P and Q using the above methodology, we get the approximate rating matrix as:</p>

<table class=" aligncenter" style="height: 130px;" width="457">

<tbody>

<tr>

<td style="text-align: center;">
</td>

<td style="text-align: center;">
Star Wars I</td>

<td style="text-align: center;">
Inception</td>

<td style="text-align: center;">
Godfather</td>

<td style="text-align: center;">
The Notebook</td>

</tr>

<tr>

<td style="text-align: center;">
U1</td>

<td style="text-align: center;">
4.97</td>

<td style="text-align: center;">
2.98</td>

<td style="text-align: center;">
2.18</td>

<td style="text-align: center;">
0.98</td>

</tr>

<tr>

<td style="text-align: center;">
U2</td>

<td style="text-align: center;">
3.97</td>

<td style="text-align: center;">
2.4</td>

<td style="text-align: center;">
1.97</td>

<td style="text-align: center;">
0.99</td>

</tr>

<tr>

<td style="text-align: center;">
U3</td>

<td style="text-align: center;">
1.02</td>

<td style="text-align: center;">
0.93</td>

<td style="text-align: center;">
5.32</td>

<td style="text-align: center;">
4.93</td>

</tr>

<tr>

<td style="text-align: center;">
U4</td>

<td style="text-align: center;">
1.00</td>

<td style="text-align: center;">
0.85</td>

<td style="text-align: center;">
4.59</td>

<td style="text-align: center;">
3.93</td>

</tr>

<tr>

<td style="text-align: center;">
U5</td>

<td style="text-align: center;">
1.36</td>

<td style="text-align: center;">
1.07</td>

<td style="text-align: center;">
4.89</td>

<td style="text-align: center;">
4.12</td>

</tr>

</tbody>

</table>


<p>
Notice how we are able to regenerate the existing ratings, moreover we are now able to get a fair approximation to the unknown rating values.</p>


<h2>
How Factorization Machines trump Polynomial and linear models?</h2>

<p>
Let us consider a couple of training examples from a click prediction dataset. The dataset is click through related sports news website (publisher) and sports gear firms (advertiser).</p>

<table class=" aligncenter" style="height: 114px;" width="352">

<tbody>

<tr>

<td style="text-align: center;">
Clicked</td>

<td style="text-align: center;">
Publisher (P)</td>

<td style="text-align: center;">
Advertiser (A)</td>

<td style="text-align: center;">
Gender (G)</td>

</tr>

<tr>

<td style="text-align: center;">
Yes</td>

<td style="text-align: center;">
ESPN</td>

<td style="text-align: center;">
Nike</td>

<td style="text-align: center;">
Male</td>

</tr>

<tr>

<td style="text-align: center;">
No</td>

<td style="text-align: center;">
NBC</td>

<td style="text-align: center;">
Adidas</td>

<td style="text-align: center;">
Male</td>

</tr>

</tbody>

</table>


<p>
When we talk about FMs or FFMs, each column (Publisher, Advertiser&#8230;) in the dataset would be referred to as a <strong>
field </strong>
and each value (ESPN, Nike&#8230;.) would be referred to as a <strong>
feature</strong>
.</p>

<p>
A linear or a logistic modeling technique is great and does well in a variety of problems but the drawback is that the model only learns the effect of all variables or features individually rather than in combination. </p>

<p>

<br>
<img src="https://latex.codecogs.com/gif.latex?%5Cdpi%7B150%7D%20%5Ctiny%20%5Cphi%20_%7BLM%7D%20%3D%20w_%7B0%7D%20&amp;plus;%20w_%7BESPN%7Dx_%7BESPN%7D%20&amp;plus;%20w_%7BNike%7Dx_%7BNike%7D%20&amp;plus;%20w_%7BAdidas%7Dx_%7BAdidas%7D%20&amp;plus;%20w_%7BNBC%7Dx_%7BNBC%7D%20&amp;plus;%20w_%7BMale%7Dx_%7BMale%7D" />
</p>

<p>
Where w<sub>
0</sub>
, w<sub>
ESPN</sub>
 etc.  represent the parameters and x<sub>
ESPN</sub>
, x<sub>
Nike </sub>
represent the individual features in the dataset. By minimizing the log-loss for the above function we get <a href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/">
logistic regression</a>
. One way to capture the feature interactions is a polynomial function that learns a separate parameter for the product of each pair of features treating each product as a separate variable. </p>

<p>

<br>
<img src="https://latex.codecogs.com/gif.latex?%5Cdpi%7B150%7D%20%5Ctiny%20%5Cphi%20_%7BPoly2%7D%20%3D%20w_%7B0%7D%20&amp;plus;%20w_%7BESPN%7Dx_%7BESPN%7D%20&amp;plus;%20w_%7BNike%7Dx_%7BNike%7D%20&amp;plus;%20w_%7BAdidas%7Dx_%7BAdidas%7D%20&amp;plus;%20w_%7BNBC%7Dx_%7BNBC%7D%20&amp;plus;%20w_%7BMale%7Dx_%7BMale%7D" width="767" height="16" />
</p>

<p>

<br>
<img src="https://latex.codecogs.com/gif.latex?&amp;plus;%20w_%7BESPN%2CNike%7Dx_%7BESPN%7Dx_%7BNike%7D%20&amp;plus;%20w_%7BESPN%2CMale%7Dx_%7BESPN%7Dx_%7BMale%7D%20&amp;plus;%20............." width="479" height="17" />
</p>

<p>
This can also be referred to as Poly2 model as we are only considering combination of 2 features for a term.</p>

<ul>

<li>
The problem with this is that even for a medium sized dataset, we have a huge model that has terrible implications for both the amount of memory needed to store the model and the time it takes to train the model. </li>

<li>
Secondly, for a sparse dataset this technique will not do well to learn all the weights or parameters reliably i.e. we will not have enough training examples for each pair of features in order for each weight to be reliable.</li>

</ul>


<h3>
FM to the rescue</h3>

<p>
FM solves the problem of considering pairwise feature interactions. It allows us to train, based on reliable information (latent features) from every pairwise combination of features in the model. FM also allows us to do this in an efficient way both in terms of time and space complexity. It models pairwise feature interactions as the dot product of low dimensional vectors(length = k). This is illustrated with the following equation for a degree = 2 factorization machine: 
<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20232823/fm_eqn1.jpg"><br>
 Each parameter in FMs (k=3) can be described as follows: 
<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20233722/fm_eqn_3.jpg"><br>
 Here, for each term we have calculated the dot product of the 2 latent factors of size 3 corresponding to the 2 features. </p>

<p class="c2">
From a modeling perspective, this is powerful because each feature ends up transformed to a space where similar features are embedded near one another. In simple words, the dot product basically represents similarity of the hidden features and it is higher when the features are in the neighborhood.</p>

<p class="c2">

<br>
<img src="https://lh6.googleusercontent.com/jiKwNexUqpNwKlFAk9pmyLQiYF7Nt1a3kHlG_DDSdHRCma9Rj_la1xOyeqOsI-4pVefTfYdpASFzu23JphaDgNIzwTaBo_Go38j3vyRxiOObXWhTHKFaECMAri0MlcZT-KCyNp5K">
 </p>

<p>
The cosine function is 1 (maximum) when theta is 0 and decreases to -1 when theta is 180 degrees. It is clear that the similarity is maximum when theta approaches 0.</p>

<p>
Another big advantage of FMs is that we are able to compute the term that models all pairwise interactions in <strong>
linear time complexity</strong>
 using a simple mathematical manipulation to the above equation. If you want to have a look at the exact steps required for this, please refer to the original Factorization Machines research paper at this <a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">
link</a>
.</p>


<h3>
Example: Demonstration of how FM is better than POLY2</h3>

<p>
Consider the following artificial Click Through Rate (CTR) data:</p>

<p>

<br>
<img src="https://lh3.googleusercontent.com/DRuBIo8ZJjO6XZ9QBV5celB1sigpIaK9-fqL9UZ5n7rSJ4G64BcxZbT1pjqv2VlsEppEBk8Pg4dgCGpp3pWqfoSHHP-gFb_ouOJ64bFNmXlY4ZVKQrt3tgnPNTAWs2Q06SKuQgfR">
</p>

<p>
This is a dataset comprising of sports websites as publishers and sports gear brands as publishers. The ad appears as a popup and the user has an option of clicking (clicks)the ad or closing it (unclicks).</p>

<ul>

<li>
There is only one negative training data for the pair (ESPN, Adidas).  For Poly2, a very negative weight w<sub>
ESPN,Adidas</sub>
 might be learned for this pair.  For FMs, because the prediction of (ESPN, Adidas) is determined by w<sub>
ESPN</sub>
Â·w<sub>
Adidas</sub>
, and because w<sub>
ESPN</sub>
 and w<sub>
Adidas</sub>
 are also learned from other pairs of features as well (e.g., (ESPN, Nike), (NBC, Adidas)), the prediction may be more accurate.  </li>

<li>
Another example is that there is no training data for the pair (NBC, Gucci). For Poly2, the prediction on this pair is 0, but for FMs, because w<sub>
NBC</sub>
 and w<sub>
Gucci</sub>
 can  be  learned  from  other  pairs,  it  is  still  possible to do meaningful prediction.</li>

</ul>


<h2>
Field-Aware Factorization Machines</h2>

<table style="height: 91px;" width="381">

<tbody>

<tr>

<td style="text-align: center;">
Clicked</td>

<td style="text-align: center;">
Publisher (P)</td>

<td style="text-align: center;">
Advertisor (A)</td>

<td style="text-align: center;">
Gender (G)</td>

</tr>

<tr>

<td style="text-align: center;">
Yes</td>

<td style="text-align: center;">
ESPN</td>

<td style="text-align: center;">
Nike</td>

<td style="text-align: center;">
Male</td>

</tr>

</tbody>

</table>

<p>
In order to understand FFMs, we need to realize the meaning of field. Field is typically the broader category which contains a particular feature. In the above training example, the fields are Publisher (P), Advertiser (A) and Gender(G).</p>

<ul>

<li>
In  FMs,  every  feature  has  only  one  latent  vector v  to  learn the  latent  effect  with  any  other  features. Take  ESPN  as an  example, w<sub>
ESPN</sub>
 is  used  to  learn  the  latent  effect  with Nike (w<sub>
ESPN</sub>
Â·w<sub>
Nike</sub>
) and Male (w<sub>
ESPN</sub>
.w<sub>
Male</sub>
).  </li>

<li>
However, because ESPN and Male belong to different fields, the latent effects of (ESPN, Nike) and (ESPN, Male) may be different. This is not captured by factorization machines as it will use the same parameters for dot product in both cases. </li>

<li>
In FFMs, each feature has several latent vectors. For example, when we consider the interaction term for ESPN and Nike, the hidden feature for ESPN would have the notation w<sub>
ESPN,A</sub>
 where A(Advertiser) represents the field for the feature Nike. Similarly for Gender field a different parameter w<sub>
ESPN,G</sub>
 would be learnt.</li>

</ul>

<p class="c0">

<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/Screenshot-2018-1-4-Edit-Post-%E2%80%B9-Analytics-Vidhya-%E2%80%94-WordPress.png">
</p>

<p class="c2">
FFMs have proved to be vital for winning the first prize of three CTR (Click through Rate) competitions hosted by Criteo, Avazu, Outbrain, it also won the third prize of RecSys Challenge 2015. Datasets for the CTR can be accessed from<a class="c18" href="https://www.google.com/url?q=https://www.kaggle.com/&amp;sa=D&amp;ust=1515084014529000&amp;usg=AFQjCNH2MwLc_lrIZAfCAR0DnKi9_xd__Q">
 </a>
<span class="c14">
<a class="c18" href="https://www.google.com/url?q=https://www.kaggle.com/&amp;sa=D&amp;ust=1515084014529000&amp;usg=AFQjCNH2MwLc_lrIZAfCAR0DnKi9_xd__Q">
Kaggle</a>
.</p>

<p style="text-align: left;">
 </p>

<h2>
Implementation using xLearn Library in Python</h2>

<p style="text-align: left;">
Some of the most popular libraries for its implementation in Python are as follows:</p>

<table class=" alignleft" style="height: 179px;" width="497">

<tbody>

<tr>

<td style="text-align: center;">
Package Name</td>

<td style="text-align: center;">
Description</td>

</tr>

<tr>

<td style="text-align: center;">
LibFM</td>

<td style="text-align: center;">
Earliest library by the author himself for FMs</td>

</tr>

<tr>

<td style="text-align: center;">
LibFFM</td>

<td style="text-align: center;">
Library exclusively FFMs</td>

</tr>

<tr>

<td style="text-align: center;">
xLearn</td>

<td style="text-align: center;">
Scalable ML package for both FM &amp; FFMs</td>

</tr>

<tr>

<td style="text-align: center;">
tffm</td>

<td style="text-align: center;">
Tensorflow implementation of arbitrary order FMs</td>

</tr>

</tbody>

</table>






<p style="text-align: left;">
For using FMs on datasets, it needs to be converted to a specific format called the libSVM format. The format of training and testing data file is:</p>

<p style="text-align: left;">
&lt;label&gt; &lt;feature1&gt;:&lt;value1&gt; &lt;feature2&gt;:&lt;value2&gt; &#8230;</p>

<p style="text-align: left;">
.</p>

<p style="text-align: left;">

<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20183750/libsvm_format.jpg"><br>
</p>

<p class="c2">
In case of a categorical field, the feature is uniquely encoded and a value of 1 is assigned to it. In the above figure ESPN is represented by code 1, Nike is represented by code 2 and so on. Each line contains an equivalent training example and is ended by a &#8216;\n&#8217; or a new line character.</p>

<ul>

<li>
For classification(binary/multiclass), &lt;label&gt; is an integer indicating the class label.</li>

<li>
For regression, &lt;label&gt; is the target value which can be any real number.</li>

<li>
Labels in the test file are only used to calculate accuracy or errors. If they are unknown, you can just fill the first column with any number.</li>

</ul>

<p class="c2">
Similarly for FFMs, the data needs to be transformed to a libffm format. Here, we also need to encode the field since ffm requires the information of field for learning. The format for the same is:</p>

<p class="c2">
&lt;label&gt; &lt;field1&gt;:&lt;feature1&gt;:&lt;value1&gt; &lt;field2&gt;:&lt;feature2&gt;:&lt;value2&gt; &#8230;..</p>


<p>

<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/20183838/ffm_format.jpg"><br>
</p>

<h3 style="text-align: left;">
Important note on numerical features</h3>

<p style="text-align: left;">
Numerical features either need to be discretized (transformed to categorical features by breaking the entire range of a particular numerical feature into smaller ranges and label encoding each range separately) and then converted to libffm format as described above. </p>

<p style="text-align: left;">
Another possibility is to add a dummy field which is the same as feature value will be numeric feature for that particular row (For example a feature with value 45.3 can be transformed to 1:1:45.3). However, the dummy fields may not be informative because they are merely duplicates of features.</p>

<h3>
xLearn</h3>

<p style="text-align: left;">
Recently launched xLearn library provides a fast solution to implementing FM and FFM models on a variety of datasets. It is much faster than libfm and libffm libraries and provide a better functionality for model testing and tuning.</p>

<p>

<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/21000403/xlearn.jpg"><br>
</p>

<p style="text-align: left;">
Here, we will illustrate with an example of FFM for a tiny sample(1%) of CTR dataset from Criteoâs click prediction challenge. You can download this dataset from here.</p>

<p style="text-align: left;">
But first we need to convert it to libffm format which is necessary for xLearn to fit the model. Following function does the job of converting dataset in standard dataframe format to libffm format. </p>

<p style="text-align: left;">
df = Dataframe to be converted to ffm format</p>

<p style="text-align: left;">
Type = Train/Test/Val</p>

<p style="text-align: left;">
Numerics = list of all numeric fields</p>

<p style="text-align: left;">
Categories = list of all categorical fields</p>

<p style="text-align: left;">
Features = list of all features except the Label and Id </p>

<pre style="text-align: left;">
# Based on Kaggle <a href="https://www.kaggle.com/scirpus/libffm-generator-lb-280">
kernel</a>
 by Scirpus
def convert_to_ffm(df,type,numerics,categories,features):
    currentcode = len(numerics)
    catdict = {}
    catcodes = {}
    # Flagging categorical and numerical fields
    for x in numerics:
         catdict[x] = 0
    for x in categories:
         catdict[x] = 1
    
    nrows = df.shape[0]
    ncolumns = len(features)
    with open(str(type) + "_ffm.txt", "w") as text_file:

    # Looping over rows to convert each row to libffm format
    for n, r in enumerate(range(nrows)):
         datastring = ""
         datarow = df.iloc[r].to_dict()
         datastring += str(int(datarow['Label']))
         # For numerical fields, we are creating a dummy field here
         for i, x in enumerate(catdict.keys()):
             if(catdict[x]==0):
                 datastring = datastring + " "+str(i)+":"+ str(i)+":"+ str(datarow[x])
             else:
         # For a new field appearing in a training example
                 if(x not in catcodes):
                     catcodes[x] = {}
                     currentcode +=1
                     catcodes[x][datarow[x]] = currentcode #encoding the feature
         # For already encoded fields
                 elif(datarow[x] not in catcodes[x]):
                     currentcode +=1
                     catcodes[x][datarow[x]] = currentcode #encoding the feature
                 code = catcodes[x][datarow[x]]
                 datastring = datastring + " "+str(i)+":"+ str(int(code))+":1"

         datastring += '\n'
         text_file.write(datastring)</pre>

<p class="c2">
xLearn can handle csv as well as libsvm format for implementation of FMs while we necessarily need to convert it to libffm format for using FFM.</p>

<p class="c2">
Once we have the dataset in libffm format, we could train the model using the xLearn library.</p>

<p class="c2">
Similar to any other ML algorithm, the dataset is divided into a training set and a validation set. xLearn automatically performs early stopping using the validation/test logloss and we can also declare another metric and monitor on the validation set for each iteration of the stochastic gradient descent.</p>

<p class="c2">
The following python script could be used for training and tuning hyperparameters of FFM model using xlearn on a dataset in ffm format.</p>

<pre style="text-align: left;">
import xlearn as xl

ffm_model = xl.create_ffm()
ffm_model.setTrain("criteo.tr.r100.gbdt0.ffm")
ffm_model.setValidate("criteo.va.r100.gbdt0.ffm")

param = {'task':'binary', # âbinaryâ for classification, âregâ for Regression
         'k':2,           # Size of latent factor
         'lr':0.1,        # Learning rate for GD
         'lambda':0.0002, # L2 Regularization Parameter
         'Metric':'auc',  # Metric for monitoring validation set performance
         'epoch':25       # Maximum number of Epochs
        }

ffm_model.fit(param, "model.out")


</pre>

<p>

<br>
<img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/12/trpro1.jpg"><br>
</p>

<p style="text-align: left;">
The library also allows us to use cross-validation using the cv() function:</p>

<pre style="text-align: left;">
ffm_model.cv(param)</pre>

<p style="text-align: left;">
Predictions can be done on the test set with the following code snippet:</p>

<pre style="text-align: left;">
# Convert output to 0/1
ffm_model.setTest("criteo.va.r100.gbdt0.ffm")
ffm_model.setSigmoid()
ffm_model.predict("model.out", "output.txt")

</pre>

<br>
<br><br><br><br><br>
<script>
  $(function() {
    var toc = $('#toc>ul');

    function makeLi(text, href) {
      return $('<a href="' + href + '" target="_self">' + text + '</a><br>');
    }

    $('h2').each(function(i) {
      var chapter = $(this), chapterNumber = i + 1;
      toc.append(
        makeLi(chapter.text(), '#chapter-' + chapterNumber)
      );
      chapter.attr('id', 'chapter-' + chapterNumber);
    });

  });
</script>
</body>
</html>
