<base target="_blank"><html><head><title>statologyContents 1</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 1"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 1</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">The 10% Condition in Statistics: Definition & Example</span></h2>
A <b>Bernoulli trial</b> is an experiment with only two possible outcomes – “success” or “failure” – and the probability of success is the same each time the experiment is conducted.
An example of a Bernoulli trial is a coin flip. The coin can only land on two sides (we could call heads a “success” and tails a “failure”) and the probability of success on each flip is 0.5, assuming the coin is fair.
Often in statistics when we want to calculate probabilities involving more than just a few Bernoulli trials, we use the  normal distribution  as an approximation. However, in order to do so we must assume that the trials are independent.
In cases where the trials are <em>not </em>actually independent, we can still assume that they are if the sample size we’re working with does not exceed 10% of the population size. This is known as <b>The 10% Condition</b>.
<b>The 10% Condition:</b> As long as the sample size is less than or equal to 10% of the population size, we can still make the assumption that Bernoulli trials are independent.
<h3>Intuition Behind The 10% Condition</h3>
To develop an intuition behind The 10% Condition, consider the following example.
Suppose the true proportion of students in a certain class who prefer football over basketball is 50%. Let  random variable  <em>X</em> be the number of students randomly selected in 4 trials who prefer football over basketball. Let’s say we’re interested in understanding the probability that all 4 randomly selected students prefer football over basketball.
If our classroom size is 20 and our trials were independent (e.g. we could take repeated samples of all 20 students), then the probability that each student would prefer football over basketball could be calculated as:
P(All 4 students prefer football) = 10/20 * 10/20 * 10/20 * 10/20 = <b>.0625</b>.
However, if our trials are <em>not </em>independent (e.g. once we sample one student, they can’t be placed back in the classroom) then the probability that all 4 students would prefer football would be calculated as:
P(All 4 students prefer football) = 10/20 * 9/19 * 8/18 * 7/17 = <b>.0433</b>.
These two probabilities are quite different. Consider that in this example our sample size (4 students) is not less than or equal to 10% of the population (20 students), thus we wouldn’t be able to use The 10% Condition.
However, consider the following table that shows the probability that all 4 randomly selected students prefer football, based on classroom size:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/tenpercentCondition1.png">
As the sample size relative to the population size (e.g. “classroom size” in this example) decreases, the calculated probability between independent trials and non-independent trials gets closer and closer.
Note that when the sample size is exactly 10% of the population size, the difference between the probabilities of independent trials and non-independent trials are relatively similar.
And when the sample size is much less than 10% of the population size (e.g. just 0.4% of the population size in the last row of the table), the probabilities between independent and non-independent trials are extremely close.
<h3>Conclusion</h3>
<b>The 10% Condition</b> says that our sample size should be less than or equal to 10% of the population size in order to safely make the assumption that a set of Bernoulli trials is independent.
Of course, it’s best if our sample size is much less than 10% of the population size so that our inferences about the population are as accurate as possible. For example, we’d prefer that our sample size is only 5% of the population compared to 10%.
<h2><span class="orange">A Complete Guide: The 2×2 Factorial Design</span></h2>
A <b>2×2 factorial design</b> is a type of experimental design that allows researchers to understand the effects of two independent variables (each with two  levels ) on a single dependent variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/2x2_1.png">
For example, suppose a botanist wants to understand the effects of sunlight (low vs. high) and watering frequency (daily vs. weekly) on the growth of a certain species of plant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/2x2_2.png">
This is an example of a 2×2 factorial design because there are two independent variables, each with two levels:
<b>Independent variable #1:</b> Sunlight
<b>Levels:</b> Low, High
<b>Independent variable #2:</b> Watering Frequency
<b>Levels:</b> Daily, Weekly
And there is one dependent variable: Plant growth.
<h3>The Purpose of a 2×2 Factorial Design</h3>
A 2×2 factorial design allows you to analyze the following effects:
<b>Main Effects:</b> These are the effects that just one independent variable has on the dependent variable.
For example, in our previous scenario we could analyze the following main effects:
Main effect of sunlight on plant growth.
We can find the mean plant growth of all plants that received low sunlight.
We can find the mean plant growth of all plants that received high sunlight.
Main effect of watering frequency on plant growth.
We can find the mean plant growth of all plants that were watered daily.
We can find the mean plant growth of all plants that were watered weekly.
<b>Interaction Effects:</b> These occur when the effect that one independent variable has on the dependent variable depends on the level of the other independent variable.
For example, in our previous scenario we could analyze the following interaction effects:
Does the effect of sunlight on plant growth depend on watering frequency?
Does the effect of watering frequency on plant growth depend on the amount of sunlight?
<h3>Visualizing Main Effects & Interaction Effects</h3>
When we use a 2×2 factorial design, we often graph the means to gain a better understanding of the effects that the independent variables have on the dependent variable.
For example, consider the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/2x2_4.png">
Here’s how to interpret the values in the plot:
The mean growth for plants that received high sunlight and daily watering was about <b>8.2</b> inches.
The mean growth for plants that received high sunlight and weekly watering was about <b>9.6</b> inches.
The mean growth for plants that received low sunlight and daily watering was about <b>5.3</b> inches.
The mean growth for plants that received low sunlight and weekly watering was about <b>5.8</b> inches.
To determine if there is an interaction effect between the two independent variables, we simply need to inspect whether or not the lines are parallel:
If the two lines in the plot are parallel, there is no interaction effect.
If the two lines in the plot are <em>not</em> parallel, there is an interaction effect.
In the previous plot, the two lines were roughly parallel so there is likely no interaction effect between watering frequency and sunlight exposure.
However, consider the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/2x2_5.png">
The two lines are not parallel at all (in fact, they cross!), which indicates that there is likely an interaction effect between them.
For example, this means the effect that sunlight has on plant growth <em>depends</em> on the watering frequency.
In other words, sunlight and watering frequency do not affect plant growth independently. Rather, there is an <em>interaction effect</em> between the two independent variables.
<h3>How to Analyze a 2×2 Factorial Design</h3>
Plotting the means is a visualize way to inspect the effects that the independent variables have on the dependent variable.
However, we can also perform a  two-way ANOVA  to formally test whether or not the independent variables have a statistically significant relationship with the dependent variable.
For example, the following code shows how to perform a two-way ANOVA for our hypothetical plant scenario in R:
<b>#make this example reproducible
set.seed(0)
df &lt;- data.frame(sunlight = rep(c('Low', 'High'), each = 30), water = rep(c('Daily', 'Weekly'), each = 15, times = 2), growth = c(rnorm(15, 6, 2), rnorm(15, 7, 3), rnorm(15, 7, 2),                   rnorm(15, 10, 3)))
#fit the two-way ANOVA model
model &lt;- aov(growth ~ sunlight * water, data = df)
#view the model output
summary(model)
               Df Sum Sq Mean Sq F value  Pr(>F)   
sunlight        1   52.5   52.48   8.440 0.00525 **
water           1   31.6   31.59   5.081 0.02813 * 
sunlight:water  1   12.8   12.85   2.066 0.15620   
Residuals      56  348.2    6.22                   
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</b>
Here’s how to interpret the output of the ANOVA:
The p-value associated with sunlight is <b>.005</b>. Since this is less than .05, this means sunlight exposure has a statistically significant effect on plant growth.
The p-value associated with water is <b>.028</b>. Since this is less than .05, this means watering frequency also has a statistically significant effect on plant growth.
The p-value for the interaction between sunlight and water is <b>.156</b>. Since this is not less than .05, this means there is no interaction effect between sunlight and water.
<h2><span class="orange">A Complete Guide: The 2×3 Factorial Design</span></h2>
A <b>2×3 factorial design</b> is a type of experimental design that allows researchers to understand the effects of two independent variables on a single dependent variable.
In this type of design, one independent variable has two  levels  and the other independent variable has three levels.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/2x3_1.png">
For example, suppose a botanist wants to understand the effects of sunlight (low vs. medium vs. high) and watering frequency (daily vs. weekly) on the growth of a certain species of plant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/2x3_2.png">
This is an example of a 2×3 factorial design because there are two independent variables, one having two levels and the other having three levels:
<b>Independent variable #1:</b> Sunlight
<b>Levels:</b> Low, Medium, High
<b>Independent variable #2:</b> Watering Frequency
<b>Levels:</b> Daily, Weekly
And there is one dependent variable: Plant growth.
<h3>The Purpose of a 2×3 Factorial Design</h3>
A 2×3 factorial design allows you to analyze the following effects:
<b>Main Effects:</b> These are the effects that just one independent variable has on the dependent variable.
For example, in our previous scenario we could analyze the following main effects:
Main effect of sunlight on plant growth.
Mean growth of all plants that received low sunlight.
Mean growth of all plants that received medium sunlight.
Mean growth of all plants that received high sunlight.
Main effect of watering frequency on plant growth.
Mean growth of all plants that were watered daily.
Mean growth of all plants that were watered weekly.
<b>Interaction Effects:</b> These occur when the effect that one independent variable has on the dependent variable depends on the level of the other independent variable.
For example, in our previous scenario we could analyze the following interaction effects:
Does the effect of sunlight on plant growth depend on watering frequency?
Does the effect of watering frequency on plant growth depend on the amount of sunlight?
<h3>How to Analyze a 2×3 Factorial Design</h3>
We can perform a  two-way ANOVA  to formally test whether or not the independent variables have a statistically significant relationship with the dependent variable.
For example, the following code shows how to perform a two-way ANOVA for our hypothetical plant scenario in R:
<b>#make this example reproducible
set.seed(0)
#create data
df &lt;- data.frame(sunlight = rep(c('Low', 'Medium', 'High'), each = 15, times = 2), water = rep(c('Daily', 'Weekly'), each = 45, times = 2), growth = c(rnorm(15, 9, 2), rnorm(15, 10, 3), rnorm(15, 13, 2),            rnorm(15, 8, 3), rnorm(15, 10, 4), rnorm(15, 12, 3)))
#fit the two-way ANOVA model
model &lt;- aov(growth ~ sunlight * water, data = df)
#view the model output
summary(model)
                Df Sum Sq Mean Sq F value Pr(>F)    
sunlight         2  602.3  301.15  50.811 &lt;2e-16 ***
water            1   39.6   39.62   6.685 0.0105 *  
sunlight:water   2   15.1    7.56   1.275 0.2819    
Residuals      174 1031.3    5.93                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
Here’s how to interpret the output of the ANOVA:
The p-value associated with sunlight is <b>&lt;2e-16</b>. Since this is less than .05, this means sunlight exposure has a statistically significant effect on plant growth.
The p-value associated with water is <b>.0105</b>. Since this is less than .05, this means watering frequency also has a statistically significant effect on plant growth.
The p-value for the interaction between sunlight and water is <b>.2819</b>. Since this is not less than .05, this means there is no interaction effect between sunlight and water.
<h2><span class="orange">A Complete Guide: The 2×4 Factorial Design</span></h2>
A <b>2×4 factorial design</b> is a type of experimental design that allows researchers to understand the effects of two independent variables on a single dependent variable.
In this type of design, one independent variable has two  levels  and the other independent variable has four levels.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/2x4_1.jpg"344">
For example, suppose a botanist wants to understand the effects of sunlight (none vs. low vs. medium vs. high) and watering frequency (daily vs. weekly) on the growth of a certain species of plant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/2x4_2.jpg"324">
This is an example of a 2×4 factorial design because there are two independent variables, one having two levels and the other having four levels:
<b>Independent variable #1:</b> Sunlight
<b>Levels:</b> None, Low, Medium, High
<b>Independent variable #2:</b> Watering Frequency
<b>Levels:</b> Daily, Weekly
And there is one dependent variable: Plant growth.
<h2>The Purpose of a 2×4 Factorial Design</h2>
A 2×4 factorial design allows you to analyze the following effects:
<b>Main Effects:</b> These are the effects that just one independent variable has on the dependent variable.
For example, in our previous scenario we could analyze the following main effects:
Main effect of sunlight on plant growth.
Mean growth of all plants that received no sunlight.
Mean growth of all plants that received low sunlight.
Mean growth of all plants that received medium sunlight.
Mean growth of all plants that received high sunlight.
Main effect of watering frequency on plant growth.
Mean growth of all plants that were watered daily.
Mean growth of all plants that were watered weekly.
<b>Interaction Effects:</b> These occur when the effect that one independent variable has on the dependent variable depends on the level of the other independent variable.
For example, in our previous scenario we could analyze the following interaction effects:
Does the effect of sunlight on plant growth depend on watering frequency?
Does the effect of watering frequency on plant growth depend on the amount of sunlight?
<h2>How to Analyze a 2×4 Factorial Design</h2>
We can perform a  two-way ANOVA  to formally test whether or not the independent variables have a statistically significant relationship with the dependent variable.
For example, the following code shows how to perform a two-way ANOVA for our hypothetical plant scenario in R:
<b>#make this example reproducible
set.seed(0)
#create data
df &lt;- data.frame(sunlight = rep(c('None', 'Low', 'Medium', 'High'), each=10, times=2), water = rep(c('Daily', 'Weekly'), each=40, times=2), growth = c(rnorm(10, 8, 2), rnorm(10, 8, 3), rnorm(10, 13, 2),            rnorm(10, 14, 3), rnorm(10, 10, 4), rnorm(10, 12, 3),            rnorm(10, 13, 2), rnorm(10, 14, 4)))
#fit the two-way ANOVA model
model &lt;- aov(growth ~ sunlight * water, data = df)
#view the model output
summary(model)
                Df Sum Sq Mean Sq F value   Pr(>F)    
sunlight         3  744.1  248.04   34.16  &lt; 2e-16 ***
water            1   43.1   43.05    5.93    0.016 *  
sunlight:water   3  195.8   65.27    8.99 1.61e-05 ***
Residuals      152 1103.5    7.26                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
Here’s how to interpret the output of the ANOVA:
<b>Main Effect #1 (Sunlight)</b>: The p-value associated with sunlight is <b>&lt;2e-16</b>. Since this is less than .05, this means sunlight exposure has a statistically significant effect on plant growth.
<b>Main Effect #2 (Water)</b>: The p-value associated with water is <b>.016</b>. Since this is less than .05, this means watering frequency also has a statistically significant effect on plant growth.
<b>Interaction Effect</b>: The p-value for the interaction between sunlight and water is <b>.000061</b>. Since this is less than .05, this means there is an interaction effect between sunlight and water.
<h2>Additional Resources</h2>
The following tutorials provide additional information on experimental design and analysis:
 A Complete Guide: The 2×2 Factorial Design 
 A Complete Guide: The 2×3 Factorial Design 
 What is a Factorial ANOVA? 
<h2><span class="orange">How to Create 3D Plots in R (With Examples)</span></h2>
The easiest way to create a 3D plot in R is to use the <b>persp()</b> function.
<b>persp(x, y, z)</b>
The following examples show how to use this function in practice.
<h2>Example 1: Basic 3D Plot</h2>
The following code shows how to create a basic 3D plot:
<b>#define x and y
x &lt;- -10:10
y &lt;- -10:10
#define function to create z-values
z_values &lt;- function(x, y) {
  sqrt(x ^ 2 + y ^ 2)
}
#create z-values
z = outer(x, y, z_values)
#create 3D plot
persp(x, y, z)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/3dplot_r.png">
<h2>Example 2: Custom 3D Plot</h2>
The following code shows how to customize the axis labels, title, color, and shade of the plot:
<b>#define x and y
x &lt;- -10:10
y &lt;- -10:10
#define function to create z-values
z_values &lt;- function(x, y) {
  sqrt(x ^ 2 + y ^ 2)
}
#create z-values
z = outer(x, y, z_values)
#create 3D plot
persp(x, y, z, xlab='X Variable', ylab='Y Variable', zlab='Z Variable',
      main='3D Plot', col='pink', shade=.4)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/3dplot_r2.png">
<h2>Example 3: Rotate the 3D Plot</h2>
The following code shows how to rotate the 3D plot to make it easier to view, using the <b>theta</b> and <b>phi</b> arguments:
<b>#define x and y
x &lt;- -10:10
y &lt;- -10:10
#define function to create z-values
z_values &lt;- function(x, y) {
  sqrt(x ^ 2 + y ^ 2)
}
#create z-values
z = outer(x, y, z_values)
#create 3D plot
persp(x, y, z, xlab='X Variable', ylab='Y Variable', zlab='Z Variable',
      main='3D Plot', col='pink', shade=.4, theta = 30, phi = 15)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/3dplot_r3.png">
<h2>Example 4: Add Tick Marks to the 3D Plot</h2>
The following code shows how to use the <b>ticktype</b> argument to add tick marks with labels to each axis:
<b>#define x and y
x &lt;- -10:10
y &lt;- -10:10
#define function to create z-values
z_values &lt;- function(x, y) {
  sqrt(x ^ 2 + y ^ 2)
}
#create z-values
z = outer(x, y, z_values)
#create 3D plot
persp(x, y, z, xlab='X Variable', ylab='Y Variable', zlab='Z Variable',
      main='3D Plot', col='pink', shade=.4, theta = 30, phi = 15, ticktype='detailed')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/3dplot_r4.png">
<h2>Additional Resources</h2>
The following tutorials explain how to create other common charts in R:
 How to Plot Multiple Lines in One Chart in R 
 How to Plot Multiple Boxplots in One Chart in R 
 How to Create a Pareto Chart in R 
 How to Create Radar Charts in R 
<h2><span class="orange">90th Percentile Calculator</span></h2>
The <b>90<sup>th</sup> percentile</b> of a dataset is the value that cuts off the first 90 percent of the data values when all of the values are sorted from least to greatest.
This calculator finds the 90<sup>th</sup> percentile for a given dataset.
Simply enter a list of the comma-separated values for the dataset, then click the “Calculate” button:
<b>Dataset values:</b>
<textarea id="x" rows="5" cols="40">14, 15, 15, 17, 22, 23, 23, 24, 25, 25, 26, 30, 31, 31, 32, 33, 34, 36, 38, 41</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>90<sup>th</sup> percentile: 36.2000</b>
<script>
function calc() {
var x = document.getElementById('x').value.split(',').map(Number);
var Q1 = jStat.percentile(x, 0.90);
document.getElementById('Q1').innerHTML = Q1.toFixed(4);
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate the 90th Percentile in Excel</span></h2>
The 90th percentile of a dataset is the value that cuts off the bottom 90 percent of the data values from the top 10 percent of data values when all of the values are sorted from least to greatest.
To find the 90th percentile of a dataset in Excel, you can use one of the following two functions:
<b>=PERCENTILE(array, k)</b>
<b>=PERCENTILE.INC(array, k)</b>
Both functions will return the same value. For both functions, the <b>array</b> is the list of values in your dataset and <b>k</b> is the percentile you’d like to find between 0 and 1.
To find the 90th percentile, we will use <b>0.9</b> for k.
Note that there is also a function called <b>=PERCENTILE.EXC</b> that calculates percentiles between 0 and 1, <em>exclusive</em>. This function is rarely used in practice.
The following example shows how to calculate the 90th percentile of a dataset in Excel.
<h3>Example: Calculating the 90th Percentile in Excel</h3>
Suppose we have the following dataset that shows the final exam scores of 20 students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/90th.png">
We can use the following syntax to find the 90th percentile of the exam scores:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/90th_1.png">
The 90th percentile turns out to be <b>94.1</b>.
This is the score that a student must receive in order to have a score that is greater than 90% of the exam scores in the entire class.
<h3>Notes</h3>
Keep in mind the following notes when calculating percentiles in Excel:
The value for <b>k</b> must always be between 0 and 1.
The percentile function will display a #VALUE! Error if you enter a non-numeric value for <b>k</b>.
The data in our example was sorted from lowest to highest exam scores, but a dataset does not need to be pre-sorted in this manner for the percentile function to work.
<h2><span class="orange">How to Calculate the 90th Percentile in Google Sheets</span></h2>
The <b>90th percentile</b> of a dataset is the value that cuts off the bottom 90 percent of the data values from the top 10 percent of data values when all of the values are sorted from least to greatest.
To find the 90th percentile of a dataset in Google Sheets, you can use one of the following two functions:
<b>=PERCENTILE(data, percentile)</b>
<b>=PERCENTILE.INC(data, percentile)</b>
Both functions will return the same value.
For both functions, the <b>data</b> is the list of values in your dataset and <b>percentile</b> is the percentile you’d like to find between 0 and 1.
To find the 90th percentile, we will use <b>0.9</b> for k.
Note that there is also a function called <b>=PERCENTILE.EXC</b> that calculates percentiles between 0 and 1, <em>exclusive</em>. This function is rarely used in practice.
The following example shows how to calculate the 90th percentile of a dataset in Google Sheets.
<h2>Example: Calculating the 90th Percentile in Google Sheets</h2>
Suppose we have the following dataset that shows the exam scores of 20 students in a particular class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/perc1.jpg"474">
We can use the following formula to find the 90th percentile of the exam scores:
<b>=PERCENTILE(A2:A21, 0.9)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/perc2.jpg">
The 90th percentile turns out to be <b>94.1</b>.
This is the score that a student must receive in order to have a score that is greater than 90% of the exam scores in the entire class.
<h2>Notes</h2>
Keep in mind the following notes when calculating percentiles in Google Sheets:
The value for <b>percentile</b> must always be between 0 and 1.
The percentile function will display a #VALUE! Error if you enter a non-numeric value for <b>k</b>.
The data in our example was sorted from lowest to highest exam scores, but a dataset does not need to be pre-sorted in this manner for the percentile function to work.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Calculate Deciles in Google Sheets 
 How to Use a Percentile IF Formula in Google Sheets 
 How to Calculate a Weighted Percentage in Google Sheets 
<h2><span class="orange">A Guide to apply(), lapply(), sapply(), and tapply() in R</span></h2>
This tutorial explains the differences between the built-in R functions <b>apply()</b>, <b>sapply()</b>, <b>lapply()</b>, and <b>tapply()</b> along with examples of when and how to use each function.
<h2>apply()</h2>
Use the <b>apply()</b> function when you want to apply a function to the rows or columns of a matrix or data frame.
The basic syntax for the apply() function is as follows:
<b>apply(X, MARGIN, FUN)</b>
X is the name of the matrix or data frame
MARGIN indicates which dimension to perform an operation across (1 = row, 2 = column)
FUN is the specific operation you want to perform (e.g. min, max, sum, mean, etc.)
The following code illustrates several examples of <b>apply()</b> in action.
<b>#create a data frame with three columns and five rows
data &lt;- data.frame(a = c(1, 3, 7, 12, 9),   b = c(4, 4, 6, 7, 8),   c = c(14, 15, 11, 10, 6))
data
#   a b  c
#1  1 4 14
#2  3 4 15
#3  7 6 11
#4 12 7 10
#5  9 8  6
#find the sum of each row
apply(data, 1, sum)
#[1] 19 22 24 29 23
#find the sum of each column
apply(data, 2, sum)
# a  b  c 
#32 29 56 
#find the mean of each row
apply(data, 1, mean)
#[1] 6.333333 7.333333 8.000000 9.666667 7.666667
#find the mean of each column, rounded to one decimal place
round(apply(data, 2, mean), 1)
#  a    b     c 
#6.4  5.8  11.2 
#find the standard deviation of each row
apply(data, 1, sd)
#[1] 6.806859 6.658328 2.645751 2.516611 1.527525
#find the standard deviation of each column
apply(data, 2, sd)
#       a        b        c 
#4.449719 1.788854 3.563706 
</b>
<h2>lapply()</h2>
Use the<b> lapply()</b> function when you want to apply a function to each element of a list, vector, or data frame and obtain a list as a result.
The basic syntax for the lapply() function is as follows:
<b>lapply(X, FUN)</b>
X is the name of the list, vector, or data frame
FUN is the specific operation you want to perform
The following code illustrates several examples of using <b>lapply()</b> on the columns of a data frame.
<b>#create a data frame with three columns and five rows
data &lt;- data.frame(a = c(1, 3, 7, 12, 9),   b = c(4, 4, 6, 7, 8),   c = c(14, 15, 11, 10, 6))
data
#   a b  c
#1  1 4 14
#2  3 4 15
#3  7 6 11
#4 12 7 10
#5  9 8  6
#find mean of each column and return results as a list
lapply(data, mean)
# $a
# [1] 6.4
#
# $b
# [1] 5.8
#
# $c
# [1] 11.2
#multiply values in each column by 2 and return results as a list
lapply(data, function(data) data*2)
# $a
# [1]  2  6 14 24 18
#
# $b
# [1]  8  8 12 14 16
#
# $c
# [1] 28 30 22 20 12
</b>
We can also use <b>lapply()</b> to perform operations on lists. The following examples show how to do so.
<b>#create a list
x &lt;- list(a=1, b=1:5, c=1:10) 
x
# $a
# [1] 1
#
# $b
# [1] 1 2 3 4 5
#
# $c
# [1]  1  2  3  4  5  6  7  8  9 10
#find the sum of each element in the list
lapply(x, sum)
# $a
# [1] 1
#
# $b
# [1] 15
#
# $c
# [1] 55
#find the mean of each element in the list
lapply(x, mean)
# $a
# [1] 1
#
# $b
# [1] 3
#
# $c
# [1] 5.5
#multiply values of each element by 5 and return results as a list
lapply(x, function(x) x*5)
# $a
# [1] 5
#
# $b
# [1]  5 10 15 20 25
#
# $c
# [1]  5 10 15 20 25 30 35 40 45 50
</b>
<h2>sapply()</h2>
Use the<b> sapply()</b> function when you want to apply a function to each element of a list, vector, or data frame and obtain a <b>vector</b> instead of a list as a result.
The basic syntax for the sapply() function is as follows:
<b>sapply(X, FUN)</b>
X is the name of the list, vector, or data frame
FUN is the specific operation you want to perform
The following code illustrates several examples of using <b>sapply()</b> on the columns of a data frame.
<b>#create a data frame with three columns and five rows
data &lt;- data.frame(a = c(1, 3, 7, 12, 9),   b = c(4, 4, 6, 7, 8),   c = c(14, 15, 11, 10, 6))
data
#   a b  c
#1  1 4 14
#2  3 4 15
#3  7 6 11
#4 12 7 10
#5  9 8  6
#find mean of each column and return results as a vector
sapply(data, mean)
#  a   b    c 
#6.4 5.8 11.2 
#multiply values in each column by 2 and return results as a matrix
sapply(data, function(data) data*2)
#      a  b  c
#[1,]  2  8 28
#[2,]  6  8 30
#[3,] 14 12 22
#[4,] 24 14 20
#[5,] 18 16 12</b>
We can also use <b>sapply()</b> to perform operations on lists. The following examples show how to do so.
<b>#create a list
x &lt;- list(a=1, b=1:5, c=1:10) 
x
# $a
# [1] 1
#
# $b
# [1] 1 2 3 4 5
#
# $c
# [1]  1  2  3  4  5  6  7  8  9 10
#find the sum of each element in the list
sapply(x, sum)
# a  b  c 
# 1 15 55 
#find the mean of each element in the list
sapply(x, mean)
#  a   b   c 
#1.0 3.0 5.5
</b>
<h2>tapply()</h2>
Use the<b> tapply()</b> function when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor.
The basic syntax for the tapply() function is as follows:
<b>tapply(X, INDEX, FUN)</b>
X is the name of the object, typically a vector
INDEX is a list of one or more factors
FUN is the specific operation you want to perform
The following code illustrates an example of using <b>tapply()</b> on the built-in R dataset <b>iris<em>.</em></b>
<b>#view first six lines of <em>iris </em>dataset
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
#find the max Sepal.Length of each of the three Species
tapply(iris$Sepal.Length, iris$Species, max)
#setosa versicolor  virginica 
#   5.8        7.0        7.9 
#find the mean Sepal.Width of each of the three Species
tapply(iris$Sepal.Width, iris$Species, mean)
# setosa versicolor virginica 
#  3.428      2.770     2.974 
#find the minimum Petal.Width of each of the three Species
tapply(iris$Petal.Width, iris$Species, min)
#  setosa versicolor virginica 
#     0.1        1.0       1.4 
</b>
<h2><span class="orange">A Simple Explanation of How to Interpret Variance</span></h2>
In statistics, we are often interested in understanding how “spread out” values are in a dataset. To measure this, we often use the following  measures of dispersion :
<b>The range: </b>the difference between the largest and smallest value in a dataset.
<b>The interquartile range: </b>the difference between the first quartile and the third quartile in a dataset (quartiles are simply values that split up a dataset into four equal parts).
<b>The standard deviation: </b>a way to measure the typical distance that values are from the mean.
<b>The variance: </b>the standard deviation squared.
Out of these four measures, <b>the variance</b> tends to be the one that is the hardest to understand intuitively. This post aims to provide a simple explanation of the variance.
<h2>Understanding Standard Deviation</h2>
Before we can understand the variance, we first need to understand <b>the standard deviation</b>, typically denoted as <b>σ</b>.
The formula to calculate the standard deviation is:
<b>σ</b> = √(Σ (x<sub>i</sub> – μ)<sup>2</sup> / N)
where μ is the population mean, x<sub>i</sub> is the <i>i</i>th element from the population, N is the population size, and Σ is just a fancy symbol that means “sum.”
In practice, you will rarely need to calculate the standard deviation by hand; instead, you can use statistical software or a calculator.
At its most basic level, the standard deviation tells us how spread out the data values are in a dataset. To illustrate this, consider the following three datasets along with their corresponding standard deviations:
[5, 5, 5]  standard deviation = <b>0 </b> (no spread at all)
[3, 5, 7]  standard deviation = <b>1.63 </b> (some spread)
[1, 5, 99]  standard deviation = <b>45.28 </b> (a lot of spread)
The term “standard deviation” can be understood by looking at the two words that make it up:
“deviation” – this refers to the distance from the mean.
“standard” – this refers to the “standard” or “typical”distance that a value is from the mean.
Once you understand standard deviation, it’s much easier to understand variance.
<h2>Understanding Variance</h2>
The variance, typically denoted as <b>σ<sup>2</sup></b>, is simply the standard deviation squared. The formula to find the variance of a dataset is:
<b>σ<sup>2</sup></b> = Σ (x<sub>i</sub> – μ)<sup>2</sup> / N
where μ is the population mean, x<sub>i</sub> is the <i>i</i>th element from the population, N is the population size, and Σ is just a fancy symbol that means “sum.”
So, if the standard deviation of a dataset is 8, then the variation would be  8<sup>2</sup> = 64.
Or, if the standard deviation of a dataset is 10, then the variation would be  10<sup>2</sup> = 100.
Or, if the standard deviation of a dataset is 3.7, then the variation would be  3.7<sup>2</sup> = 13.69.
The more spread out the values are in a dataset, the higher the variance. To illustrate this, consider the following three datasets along with their corresponding variances:
[5, 5, 5]  variance = <b>0 </b> (no spread at all)
[3, 5, 7]  variance = <b>2.67 </b>(some spread)
[1, 5, 99]  variance = <b>2,050.67 </b>(a lot of spread)
<h2>When Would You use Variance Instead of Standard Deviation?</h2>
After reading the above explanations for standard deviation and variance, you might be wondering when you would ever use the variance instead of the standard deviation to describe a dataset.
After all, the standard deviation tells us the average distance that a value lies from the mean while the variance tells us the square of this value. It would seem that the standard deviation is much easier to understand and interpret.
In reality, you will almost always use the standard deviation to describe how spread out the values are in a dataset.
However, the variance can be useful when you’re using a technique like  ANOVA  or  Regression  and you’re trying to explain the total variance in a model due to specific factors. 
For example, you might want to understand how much variance in test scores can be explained by IQ and how much variance can be explained by hours studied.
If 36% of the variation is due to IQ and 64% is due to hours studied, that’s easy to understand. But if we use the standard deviations of 6 and 8, that’s much less intuitive and doesn’t make much sense in the context of the problem.
Another case in which the variance may be better to use than the standard deviation is when you’re doing theoretical statistical work.
In this case, it’s much easier to use the variance when doing calculations since you don’t have to use a square root sign.
<h2><span class="orange">A Simple Guide to Understanding the F-Test of Overall Significance in Regression</span></h2>
This tutorial explains how to identify the F-statistic in the output of a regression table as well as how to interpret this statistic and its corresponding p-value.
<h2>Understanding the F-Test of Overall Significance</h2>
The <b>F-Test of overall significance</b> in regression is a test of whether or not your linear regression model provides a better fit to a dataset than a model with no predictor variables. 
The F-Test of overall significance has the following two hypotheses:
<b>Null hypothesis (H<sub>0</sub>) :</b> The model with no predictor variables (also known as an <em>intercept-only model</em>) fits the data as well as your regression model.
<b>Alternative hypothesis (H<sub>A</sub>) :</b> Your regression model fits the data better than the intercept-only model.
When you fit a regression model to a dataset, you will receive  a regression table  as output, which will tell you the F-statistic along with the corresponding p-value for that F-statistic.
If the p-value is less than the significance level you’ve chosen (<em>common choices are .01, .05, and .10</em>), then you have sufficient evidence to conclude that your regression model fits the data better than the intercept-only model.
<h2>Example: F-Test in Regression</h2>
Suppose we have the following dataset that shows the total number of hours studied, total prep exams taken, and final exam score received for 12 different students:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable1.jpg">
To analyze the relationship between hours studied and prep exams taken with the final exam score that a student receives, we run a multiple linear regression using <em>hours studied </em>and <em>prep </em><em>exams taken </em>as the predictor variables and <em>final exam score </em>as the response variable.
We receive the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable2.jpg">
From these results, we will focus on the F-statistic given in the ANOVA table as well as the p-value of that F-statistic, which is labeled as <em>Significance F</em> in the table. We will choose .05 as our significance level.
<b>F-statistic:</b> 5.090515
<b>P-value:</b> 0.0332
<em><b>Technical note:</b> The F-statistic is calculated as MS regression divided by MS residual. In this case MS regression / MS residual =273.2665 / 53.68151 = <b>5.090515</b>.</em>
Since the p-value is less than the significance level, we can conclude that our regression model fits the data better than the intercept-only model.
In the context of this specific problem, it means that using our predictor variables <em>Study Hours </em>and <em>Prep Exams </em>in the model allows us to fit the data better than if we left them out and simply used the intercept-only model.
<h2>Notes on Interpreting the F-Test of Overall Significance</h2>
In general, if none of your predictor variables are statistically significant, the overall F-test will also not be statistically significant.
However, it’s possible on some occasions that this doesn’t hold because the F-test of overall significance tests whether all of the predictor variables are <em>jointly </em>significant while the t-test of significance for each individual predictor variable merely tests whether each predictor variable is <em>individually </em>significant. 
Thus, the F-test determines whether or not <em>all </em>of the predictor variables are jointly significant.
It’s possible that each predictor variable is not significant and yet the F-test says that all of the predictor variables combined are jointly significant. 
<em><b>Technical note:</b> In general, the more predictor variables you have in the model, the higher the likelihood that the The F-statistic and corresponding p-value will be statistically significant.</em>
Another metric that you’ll likely see in the output of a regression is  R-squared , which measures the strength of the linear relationship between the predictor variables and the response variable is another.
Although R-squared can give you an idea of how strongly associated the predictor variables are with the response variable, it doesn’t provide a formal statistical test for this relationship.
This is why the F-Test is useful since it is a formal statistical test. In addition, if the overall F-test is significant, you can conclude that R-squared is not equal to zero and that the correlation between the predictor variable(s) and response variable is statistically significant.
<h2>Additional Resources</h2>
The following tutorials explain how to interpret other common values in regression models:
 How to Read and Interpret a Regression Table 
 Understanding the Standard Error of the Regression 
 What is a Good R-squared Value? 
<h2><span class="orange">How to Use abline() in R to Add Straight Lines to Plots</span></h2>
The <b>abline()</b> function in R can be used to add one or more straight lines to a plot in R.
This function uses the following syntax:
abline(a=NULL, b=NULL, h=NULL, v=NULL, …)
<div> 
<div>where:
<div> 
<b>a, b: </b>single values that specify the intercept and slope of the line
<b>h: </b>the y-value for the horizontal line
<b>v: </b>the x-value for the vertical line
The following examples show how to use this function in practice.
<h3>How to Add Horizontal Lines</h3>
The basic code to add a horizontal line to a plot in R is: <b>abline(h = some value)</b>
Suppose we have the following scatterplot that displays the values for <em>x </em>and <em>y </em>in a dataset:
<b>#define dataset
data &lt;- data.frame(x = c(1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11),   y = c(13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33, 35, 40, 41))
#plot <em>x</em> and <em>y</em> values in dataset
plot(data$x, data$y, pch = 16)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline1.jpg">
To add a horizontal line at the value y = 20, we can use the following code:
<b>abline(h = 20, col = 'coral2', lwd = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline2.jpg">
The following code illustrates how to add a horizontal solid line at the mean value of <em>y </em>along with two horizontal dashed lines at one standard deviation above and below the mean value:
<b>#create scatterplot for <em>x </em>and <em>y</em>
plot(data$x, data$y, pch = 16)
#create horizontal line at mean value of <em>y</em>
abline(h = mean(data$y), lwd = 2)
#create horizontal lines at one standard deviation above and below the mean value
abline(h = mean(data$y) + sd(data$y), col = 'steelblue', lwd = 3, lty = 2)
abline(h = mean(data$y) - sd(data$y), col = 'steelblue', lwd = 3, lty = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline3.jpg">
<h3>How to Add Vertical Lines</h3>
The basic code to add a vertical line to a plot in R is: <b>abline(v = some value)</b>
The following code illustrates how to add a vertical line at the mean value on a histogram:
<b>#make this example reproducible
set.seed(0)
#create dataset with 1000 random values normally distributed with mean = 10, sd = 2
data &lt;- rnorm(1000, mean = 10, sd = 2)
#create histogram of data values
hist(data, col = 'steelblue')
#draw a vertical dashed line at the mean value
abline(v = mean(data), lwd = 3, lty = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline4.jpg">
<h2>How to Add Regression Lines </h2>
The basic code to add a simple linear regression line to a plot in R is: <b>abline(model)</b>
The following code illustrates how to add a fitted linear regression line to a scatterplot:
<b>#define dataset
data &lt;- data.frame(x = c(1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11),   y = c(13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33, 35, 40, 41))
#create scatterplot of <em>x</em> and <em>y</em> values
plot(data$x, data$y, pch = 16)
#fit a linear regression model to the data
reg_model &lt;- lm(y ~ x, data = data)
#add the fitted regression line to the scatterplot
abline(reg_model, col="steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline5.jpg">
Note that we simply need a value for the intercept and the slope to fit a simple linear regression line to the data using the abline() function.
Thus, another way of using <b>abline()</b> to add a regression line is to explicitly specify the intercept and slope coefficients of the regression model:
<b>#define dataset
data &lt;- data.frame(x = c(1, 1, 2, 3, 4, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11),   y = c(13, 14, 17, 12, 23, 24, 25, 25, 24, 28, 32, 33, 35, 40, 41))
#create scatterplot of <em>x</em> and <em>y</em> values
plot(data$x, data$y, pch = 16)
#fit a linear regression model to the data
reg_model &lt;- lm(y ~ x, data = data)
#define intercept and slope values
a &lt;- coefficients(reg_model)[1] #intercept
b &lt;- coefficients(reg_model)[2] #slope
#add the fitted regression line to the scatterplot
abline(a=a, b=b, col="steelblue")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/abline5.jpg">
Notice that this produces the same line as before.
You can find more R tutorials on  this page .
<h2><span class="orange">How to Add a Column to a Pandas DataFrame</span></h2>
You can use the <b>assign()</b> function to add a new column to the end of a pandas DataFrame:
<b>df = df.assign(col_name=[value1, value2, value3, ...])
</b>
And you can use the <b>insert()</b> function to add a new column to a specific location in a pandas DataFrame:
<b>df.insert(position, 'col_name', [value1, value2, value3, ...])</b>
The following examples show how to use this syntax in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23],   'assists': [5, 7, 7, 9, 12, 9],   'rebounds': [11, 8, 10, 6, 6, 5]})
#view DataFrame
df
        pointsassistsrebounds
025511
11278
215710
31496
419126
52395
</b>
<h3>Example 1: Add New Column to End of DataFrame</h3>
The following code shows how to add a new column to the end of the DataFrame:
<b>#add 'steals' column to end of DataFrame
df = df.assign(steals=[2, 2, 4, 7, 4, 1])
#view DataFrame
df
        pointsassistsrebounds steals
025511 2
11278 2
215710 4
31496 7
419126 4
52395 1
</b>
<h3>Example 2: Add Multiple Columns to End of DataFrame</h3>
The following code shows how to add multiple new columns to the end of the DataFrame:
<b>#add 'steals' and 'blocks' columns to end of DataFrame
df = df.assign(steals=[2, 2, 4, 7, 4, 1],
               blocks=[0, 1, 1, 3, 2, 5])
#view DataFrame
df
pointsassistsrebounds stealsblocks
025511 20
11278 21
215710 41
31496 73
419126 42
52395 15</b>
<h3>Example 3: Add New Column Using Existing Column</h3>
The following code shows how to add a new column to the end of the DataFrame, based on the values in an existing column:
<b>#add 'half_pts' to end of DataFrame
df = df.assign(half_pts=lambda x: x.points / 2)
#view DataFrame
df
        pointsassistsrebounds half_pts
025511 12.5
11278 6.0
215710 7.5
31496 7.0
419126 9.5
52395 11.5</b>
<h3>Example 4: Add New Column in Specific Location of DataFrame</h3>
The following code shows how to add a new column by inserting it into a specific location in the DataFrame:
<b>#add 'steals' to column index position 2 in DataFrame
df.insert(2, 'steals', [2, 2, 4, 7, 4, 1])
#view DataFrame
df
        pointsassistsstealsrebounds
0255211
112728
2157410
314976
4191246
523915</b>
<h2><span class="orange">How to Add an Empty Column to a Data Frame in R</span></h2>
You can use the following basic syntax to add one or more empty columns to a data frame in R:
<b>#add one empty column called 'column1' to data frame
df[ , 'column1'] &lt;- NA
#add several empty columns to data frame
empty_cols &lt;- c('column1', 'column2', 'column3')
df[ , empty_cols] &lt;- NA
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Add One Empty Column to Data Frame</h3>
The following code shows how to add one empty column to a data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Mavs', 'Spurs', 'Nets'), points=c(99, 90, 84, 96), assists=c(22, 19, 16, 20))
#view data frame
df
   team points assists
1  Mavs     99      22
2  Mavs     90      19
3 Spurs     84      16
4  Nets     96      20
#add new empty column
df[ , 'blocks'] &lt;- NA
#view updated data frame
df
   team points assists blocks
1  Mavs     99      22     NA
2  Mavs     90      19     NA
3 Spurs     84      16     NA
4  Nets     96      20     NA
</b>
<h3>Example 2: Add Multiple Empty Columns to Data Frame</h3>
The following code shows how to add multiple empty columns to a data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Mavs', 'Spurs', 'Nets'), points=c(99, 90, 84, 96), assists=c(22, 19, 16, 20))
#view data frame
df
   team points assists
1  Mavs     99      22
2  Mavs     90      19
3 Spurs     84      16
4  Nets     96      20
#define names of empty columns to add
empty_cols &lt;- c('blocks', 'rebounds', 'steals')
#add multiple empty columns
df[ , empty_cols] &lt;- NA
#view updated data frame
df
   team points assists blocks rebounds steals
1  Mavs     99      22     NA       NA     NA
2  Mavs     90      19     NA       NA     NA
3 Spurs     84      16     NA       NA     NA
4  Nets     96      20     NA       NA     NA</b>
<h2><span class="orange">How to Add an Empty Column to a Pandas DataFrame</span></h2>
Occasionally you may want to add an empty column to a pandas DataFrame.
Fortunately this is fairly easy to do and this tutorial shows several examples of how to do so using the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#view DataFrame
df
pointsassistsrebounds
025511
11278
215710
31496
419126
</b>
<h3>Example 1: Add an Empty Column Using “”</h3>
The first way to add an empty column is to use<b> </b>quotations as follows:
<b>#add new column titled 'steals' 
df['steals'] = ""
#view DataFrame
df
pointsassistsrebounds steals
025511
11278
215710
31496
419126
</b>
<h3>Example 2: Add an Empty Column Using Numpy</h3>
Another way to add an empty column is to use <b>np.nan </b>as follows:
<b>#add new column titled 'steals'
df['steals'] = np.nan
#view DataFrame
df
        pointsassistsrebounds steals
025511 NaN
11278 NaN
215710 NaN
31496 NaN
419126 NaN
</b>
<h3>Example 3: Add an Empty Column Using Pandas Series</h3>
Another way to add an empty column is to use <b>pd.Series() </b>as follows:
<b>#add new column titled 'steals'
df['steals'] = pd.Series()
#view DataFrame
df
        pointsassistsrebounds steals
025511 NaN
11278 NaN
215710 NaN
31496 NaN
419126 NaN</b>
<h3>Example 4: Add an Empty Column Using Pandas Insert</h3>
Another way to add an empty column is to use the <b>insert() </b>function as follows:
<b>#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#insert empty column titled 'steals' into index position 2
df.insert(2, "steals", np.nan)
#view DataFrame
df
pointsassistsstealsrebounds
0255NaN11
1127NaN8
2157NaN10
3149NaN6
41912NaN6</b>
The nice part about this approach is that you can insert the empty column into any position you’d like in the DataFrame.
<h3>Example 5: Add Multiple Empty Columns at Once</h3>
To add multiple empty columns at once, you can use the <b>reindex() </b>function as follows:
<b>#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19],   'assists': [5, 7, 7, 9, 12],   'rebounds': [11, 8, 10, 6, 6]})
#add empty columns titled 'empty1' and 'empty2'
df = df.reindex(columns = df.columns.tolist() + ['empty1', 'empty2'])
#view DataFrame
df
        pointsassistsrebounds empty1empty2
025511 NaNNaN
11278 NaNNaN
215710 NaNNaN
31496 NaNNaN
419126 NaNNaN
</b>
<em>You can find more Python tutorials  here .</em>
<h2><span class="orange">How to Add a Horizontal Line to a Chart in Google Sheets</span></h2>
Occasionally you may want to add a horizontal line to a chart in Google Sheets to represent a target line, an average line, or some other metric.
This tutorial provides a step-by-step example of how to quickly add a horizontal line to a chart in Google Sheets.
<h3>Step 1: Create the Data</h3>
For this example, we’ll create the following fake dataset that shows the total sales and the goal for total sales in five different regions for a certain company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets1.png">
<h3>Step 2: Create a Combo Chart</h3>
Next, we’ll highlight cells <b>A1:C6</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets2.png">
Next, click the <b>Insert </b>tab. Then click <b>Chart</b> from the dropdown menu:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets3.png">
In the <b>Chart Editor</b> that appears to the right, click <b>Chart type</b> and select <b>Combo chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets4.png">
The following chart will appear that displays a bar for the sales of each region and a horizontal line that displays the goal for the sales:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets5.png">
This chart allows us to quickly see which regions have met (or exceeded) the sales goal and which regions have fallen short.
Note that we can also customize the color and the style of the line in the chart. For example, we could modify the line to be purple and dashed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets7.png">
This results in the following line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets8.png">
<h3>Step 3: Modify the Horizontal Line Value (Optional)</h3>
If we modify the values in the <b>Goal</b> column, the red horizontal line will automatically change in the chart.
For example, if we change the sales goal to 45 in the Goal column, the red line will automatically increase to 45 on the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/horizontalLineSheets6.png">
<h2><span class="orange">How to Add an Index (numeric ID) Column to a Data Frame in R</span></h2>
Suppose you have the following data frame:
<b>data &lt;- data.frame(team = c('Spurs', 'Lakers', 'Pistons', 'Mavs'),</b>
<b>                   avg_points = c(102, 104, 96, 97))</b>
<b>data
#     team avg_points
#1   Spurs        102
#2  Lakers        104
#3 Pistons         96
#4    Mavs         97</b>
In order to add an index column to give each row in this data frame a unique numeric ID, you can use the following code:
<b>#add index column to data frame
data$index &lt;- 1:nrow(data)
data
#     team avg_points index
#1   Spurs        102     1
#2  Lakers        104     2
#3 Pistons         96     3
#4    Mavs         97     4
</b>
Another way to add a unique ID to each row in the data frame is by using the<b> tibble::rowid_to_column</b> function from the <b>tidyverse </b>package:
<b>#load tidyverse package
library(tidyverse)
#create data frame
data &lt;- data.frame(team = c('Spurs', 'Lakers', 'Pistons', 'Mavs'),   avg_points = c(102, 104, 96, 97))
#add index column to data frame
data &lt;- tibble::rowid_to_column(data, "index")
data
#  index team avg_points
#1  1   Spurs        102
#2  2  Lakers        104
#3  3 Pistons         96
#4  4    Mavs         97
</b>
Notice that both techniques produce the same result: a new column that gives each row in the data frame a unique ID. 
<h2><span class="orange">How to Add Multiple Columns to Data Frame in R</span></h2>
You can use the following methods to add multiple columns to a data frame in R:
<b>Method 1: Add Multiple Columns to data.frame Object</b>
<b>df[c('new_col1', 'new_col2', 'new_col3')] &lt;- NA
</b>
<b>Method 2: Add Multiple Columns to data.table Object</b>
<b>library(data.table)
df[ , ':='(new_col1 = new_col1, new_col2 = new_col2,  new_col3 = new_col3)] 
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Add Multiple Columns to data.frame Object</h2>
Suppose we have the following data frame in R:
<b>#define data frama
df &lt;- data.frame(A=c(4, 8, 10, 2, 15, 12, 7, 22), B=c(6, 3, 9, 7, 6, 8, 14, 10), C=c(10, 9, 4, 4, 3, 7, 10, 11))
#view data frame
df
   A  B  C
1  4  6 10
2  8  3  9
3 10  9  4
4  2  7  4
5 15  6  3
6 12  8  7
7  7 14 10
8 22 10 11</b>
We can use the following syntax to add three new columns to the data frame that each contain NA values:
<b>#add three new columns to data frame
df[c('D', 'E', 'F')] &lt;- NA
#view updated data frame
df
   A  B  C  D  E  F
1  4  6 10 NA NA NA
2  8  3  9 NA NA NA
3 10  9  4 NA NA NA
4  2  7  4 NA NA NA
5 15  6  3 NA NA NA
6 12  8  7 NA NA NA
7  7 14 10 NA NA NA
8 22 10 11 NA NA NA</b>
Three new columns with all NA values have been added to the data frame.
<h2>Example 2: Add Multiple Columns to data.table Object</h2>
Suppose we have the following data table in R:
<b>library(data.table)
#create data table
dt &lt;- data.table(A=c(4, 8, 10, 2, 15, 12, 7, 22), B=c(6, 3, 9, 7, 6, 8, 14, 10), C=c(10, 9, 4, 4, 3, 7, 10, 11))
#view data table
dt
    A  B  C
1:  4  6 10
2:  8  3  9
3: 10  9  4
4:  2  7  4
5: 15  6  3
6: 12  8  7
7:  7 14 10
8: 22 10 11
</b>
We can use the following syntax to add two new columns to the data table:
<b>#define two columns to add
D = c(4, 5, 5, 4, 7, 8, 12, 10)
E = c(2, 2, 5, 7, 6, 5, 10, 13)
#add two columns to data table
dt[ , ':='(D = D, E = E)]
#view updated data table
dt
    A  B  C  D  E
1:  4  6 10  4  2
2:  8  3  9  5  2
3: 10  9  4  5  5
4:  2  7  4  4  7
5: 15  6  3  7  6
6: 12  8  7  8  5
7:  7 14 10 12 10
8: 22 10 11 10 13
</b>
Notice that two new columns have been added to the data table.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Add Column to Data Frame Based on Other Columns in R 
 How to Sort by Multiple Columns in R 
 How to Reorder Columns in R 
<h2><span class="orange">How to Add a Numpy Array to a Pandas DataFrame</span></h2>
Occasionally you may want to add a NumPy array as a new column to a pandas DataFrame.
Fortunately you can easily do this using the following syntax:
<b>df['new_column'] = array_name.tolist()
</b>
This tutorial shows a couple examples of how to use this syntax in practice.
<h3>Example 1: Add NumPy Array as New Column in DataFrame</h3>
The following code shows how to create a pandas DataFrame to hold some stats for basketball players and append a NumPy array as a new column titled ‘blocks’:
<b>import numpy as np
import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#create NumPy array for 'blocks'
blocks = np.array([2, 3, 1, 0, 2, 7, 8, 2])
#add 'blocks' array as new column in DataFrame
df['blocks'] = blocks.tolist()
#display the DataFrame
print(df)
   points  assists  rebounds  blocks
0      25        5        11       2
1      12        7         8       3
2      15        7        10       1
3      14        9         6       0
4      19       12         6       2
5      23        9         5       7
6      25        9         9       8
7      29        4        12       2
</b>
Note that the new DataFrame now has an extra column titled <em>blocks</em>.
<h3>Example 2: Add NumPy Matrix as New Columns in DataFrame</h3>
The following code shows how to create a pandas DataFrame to hold some stats for basketball players and append a NumPy array as a new column titled ‘blocks’:
<b>import numpy as np
import pandas as pd
#create pandas DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23
#create NumPy matrix
mat = np.matrix([[2, 3], [1, 0], [2, 7], [8, 2], [3, 4], [7, 7], [7, 5], [6, 3]])
#add NumPy matrix as new columns in DataFrame
df_new = pd.concat([df, pd.DataFrame(mat)], axis=1)
#display new DataFrame
print(df_new)
   points  assists  rebounds  0  1
0      25        5        11  2  3
1      12        7         8  1  0
2      15        7        10  2  7
3      14        9         6  8  2
4      19       12         6  3  4
5      23        9         5  7  7
6      25        9         9  7  5
7      29        4        12  6  3
</b>
Note that the names of the columns for the matrix that we added to the DataFrame are given default column names of <b>0</b> and <b>1</b>.
We can easily rename these columns using the <b>df.columns</b> function:
<b>#rename columns
df_new.columns = ['pts', 'ast', 'rebs', 'new1', 'new2']
#display DataFrame
print(df_new)
   pts  ast  rebs  new1  new2
0   25    5    11     2     3
1   12    7     8     1     0
2   15    7    10     2     7
3   14    9     6     8     2
4   19   12     6     3     4
5   23    9     5     7     7
6   25    9     9     7     5
7   29    4    12     6     3
</b>
<h2><span class="orange">How to Add a Regression Equation to a Plot in R</span></h2>
Often you may want to add a regression equation to a plot in R as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/regEquationR1.png">
Fortunately this is fairly easy to do using functions from the <b>ggplot2</b> and<b> ggpubr</b> packages.
This tutorial provides a step-by-step example of how to use functions from these packages to add a regression equation to a plot in R.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data to work with:
<b>#make this example reproducible
set.seed(1)
#create data frame
df &lt;- data.frame(x = c(1:100))
df$y &lt;- 4*df$x + rnorm(100, sd=20)
#view head of data frame
head(df)
  x         y
1 1 -8.529076
2 2 11.672866
3 3 -4.712572
4 4 47.905616
5 5 26.590155
6 6  7.590632</b>
<h3>Step 2: Create the Plot with Regression Equation</h3>
Next, we’ll use the following syntax to create a scatterplot with a fitted regression line and equation:
<b>#load necessary libraries
library(ggplot2)
library(ggpubr)
#create plot with regression line and regression equation
ggplot(data=df, aes(x=x, y=y)) +
        geom_smooth(method="lm") +
        geom_point() +
        stat_regline_equation(label.x=30, label.y=310)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/regEquationR1.png">
This tells us that the fitted regression equation is:
<b>y = 2.6 + 4*(x)</b>
Note that <b>label.x</b> and <b>label.y</b> specify the (x,y) coordinates for the regression equation to be displayed.
<h3>Step 3: Add R-Squared to the Plot (Optional)</h3>
You can also add the R-squared value of the regression model if you’d like using the following syntax:
<b>#load necessary libraries
library(ggplot2)
library(ggpubr)
#create plot with regression line, regression equation, and R-squared
ggplot(data=df, aes(x=x, y=y)) +
        geom_smooth(method="lm") +
        geom_point() +
        stat_regline_equation(label.x=30, label.y=310) +
        stat_cor(aes(label=..rr.label..), label.x=30, label.y=290)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/regEquationR2.png">
The  R-squared  for this model turns out to be <b>0.98</b>.
You can find more R tutorials on  this page .
<h2><span class="orange">How to Add a Regression Line to a Scatterplot in Excel</span></h2>
A  simple linear regression line  represents the line that best “fits” a dataset.
This tutorial provides a step-by-step example of how to quickly add a simple linear regression line to a  scatterplot  in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a simple dataset to work with:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine1.png">
<h3>Step 2: Create a Scatterplot</h3>
Next, highlight the cell range <b>A2:B21</b>. On the top ribbon, click the <b>INSERT </b>tab, then click <b>INSERT Scatter (X, Y)  or Bubble Chart</b> in the <b>Charts</b> group and click the first option to create a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine2.png">
The following scatterplot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine3.png">
<h3>Step 3: Add a Regression Line</h3>
Next, click anywhere on the scatterplot. Then click the plus (+) sign in the top right corner of the plot and check the box that says <b>Trendline</b>.
This will automatically add a simple linear regression line to your scatterplot:
<h3><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine4.png"></h3>
<h3>Step 4: Add a Regression Line Equation</h3>
You can also add the equation of the regression line to the chart by clicking <b>More Options.</b> In the box that appears to the right, check the box next to <b>Display Equation on chart</b>.
The simple linear regression equation will automatically appear on the scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/regLine5.png">
For this particular example, the regression line turns out to be:
<b>y = 0.917x + 12.462</b>
The way to interpret this is as follows:
For each additional one unit increase in the <em>x</em> variable, the average increase in the <em>y</em> variable is <b>0.917</b>.
When the <em>x</em> variable is equal to zero, the average value for the <em>y</em> variable is <b>12.462</b>.
We can also use this equation to estimate the value of <em>y</em> based on the value of <em>x</em>. For example, when <em>x</em> is equal to 15, the expected value for <em>y</em> is 26.217:
y = 0.917*(15) + 12.462 = <b>26.217</b>
Find more Excel tutorials  here .
<h2><span class="orange">How to Add and Subtract Days from a Date in Pandas</span></h2>
You can use the following methods to add and subtract days from a date in pandas:
<b>Method 1: Add Days to Date</b>
<b>df['date_column'] + pd.Timedelta(days=5)
</b>
<b>Method 2: Subtract Days from Date</b>
<b>df['date_column'] - pd.Timedelta(days=5) 
</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': ['2022-10-01', '2022-10-23', '2022-10-30', '2022-11-05'],   'sales': [450, 567, 612, 701]})
#convert date column to datetime
df['date'] = pd.to_datetime(df['date'])
#view DataFrame
print(df)
        date  sales
0 2022-10-01    450
1 2022-10-23    567
2 2022-10-30    612
3 2022-11-05    701</b>
<h2>Example 1: Add Days to Date in Pandas</h2>
The following code shows how to create a new column that adds five days to the value in the <b>date</b> column:
<b>#create new column that adds 5 days to value in date column
df['date_plus_five'] = df['date'] + pd.Timedelta(days=5)
#view updated DataFrame
print(df)
        date  sales date_plus_five
0 2022-10-01    450     2022-10-06
1 2022-10-23    567     2022-10-28
2 2022-10-30    612     2022-11-04
3 2022-11-05    701     2022-11-10
</b>
The new column <b>date_plus_five</b> represents the values in the <b>date</b> column with five days added to each value.
We can also use the <b>dtypes</b> function to confirm that the new column is indeed a datetime column:
<b>#check data type of each column
df.dtypes
date              datetime64[ns]
sales                      int64
date_plus_five    datetime64[ns]
dtype: object
</b>
Both the <b>date</b> and <b>date_plus_five</b> columns are recognized as datetime formats.
<h2>Example 2: Subtract Days from Date in Pandas</h2>
The following code shows how to create a new column that subtracts five days from the value in the <b>date</b> column:
<b>#create new column that subtracts five days from date
df['date_minus_five'] = df['date'] - pd.Timedelta(days=5)
#view updated DataFrame
print(df)
        date  sales date_minus_five
0 2022-10-01    450      2022-09-26
1 2022-10-23    567      2022-10-18
2 2022-10-30    612      2022-10-25
3 2022-11-05    701      2022-10-31
</b>
The new column <b>date_minus_five</b> represents the values in the <b>date</b> column with five days subtracted from each value.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Convert Columns to DateTime in Pandas 
 How to Convert Datetime to Date in Pandas 
 How to Extract Month from Date in Pandas 
<h2><span class="orange">How to Add & Subtract Days in Google Sheets (With Examples)</span></h2>
We can use the following simple formulas to <b>add or subtract days</b> to a date in Google Sheets:
<b>#add 10 days to the date in cell A2
=A2+10
#subtract 10 days from the date in cell A2
=A2-10
</b>
And we can use the following formulas to <b>add or subtract workdays</b> (Monday through Friday) to a date:
<b>#add 10 workdays to the date in cell A2
=workday(A2, 10)
#subtract 10 workdays from the date in cell A2
=workday(A2, -10)
</b>
The following examples show how to use each of these formulas in practice.
<h3>Example 1: Add & Subtract Days in Google Sheets</h3>
Suppose we have the following list of dates in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/add1.png">
We can use the following formula to add 10 days to each date:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/add2.png">
And we can use the following formula to subtract 10 days from each date:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/add3.png">
<h3>Example 2: Add & Subtract Workdays in Google Sheets</h3>
In some scenarios, we may only want to add or subtract workdays to dates in Google Sheets. Fortunately we can use the <b>workday()</b> function to do so.
The following formula shows how to add 10 workdays to each date:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/add4.png">
And the following formula shows how to subtract 10 workdays from each date:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/add5.png">
<b>Related:</b>  How to Calculate the Difference Between Two Dates in Google Sheets 
<h2><span class="orange">How to Add and Subtract Months from a Date in Pandas</span></h2>
You can use the following methods to add and subtract months from a date in pandas:
<b>Method 1: Add Months to Date</b>
<b>from pandas.tseries.offsets import DateOffset
df['date_column'] + DateOffset(months=3)
</b>
<b>Method 2: Subtract Months from Date</b>
<b>from pandas.tseries.offsets import DateOffset
df['date_column'] - DateOffset(months=3)</b>
The following examples show how to use each method in practice with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'date': pd.date_range(start='1/5/2022', freq='M', periods=10),   'sales': [6, 8, 9, 5, 4, 8, 8, 3, 5, 9]})
#view DataFrame
print(df)
        date  sales
0 2022-01-31      6
1 2022-02-28      8
2 2022-03-31      9
3 2022-04-30      5
4 2022-05-31      4
5 2022-06-30      8
6 2022-07-31      8
7 2022-08-31      3
8 2022-09-30      5
9 2022-10-31      9
</b>
<h2>Example 1: Add Months to Date in Pandas</h2>
The following code shows how to create a new column that adds 3 months to the value in the <b>date</b> column:
<b>from pandas.tseries.offsets import DateOffset
#create new column that adds 3 months to date
df['date_plus3'] = df.date + DateOffset(months=3)
#view updated DataFrame
print(df)
        date  sales date_plus3
0 2022-01-31      6 2022-04-30
1 2022-02-28      8 2022-05-28
2 2022-03-31      9 2022-06-30
3 2022-04-30      5 2022-07-30
4 2022-05-31      4 2022-08-31
5 2022-06-30      8 2022-09-30
6 2022-07-31      8 2022-10-31
7 2022-08-31      3 2022-11-30
8 2022-09-30      5 2022-12-30
9 2022-10-31      9 2023-01-31</b>
The new column <b>date_plus3</b> represents the values in the <b>date</b> column with three months added to each value.
<h2>Example 2: Subtract Months from Date in Pandas</h2>
The following code shows how to create a new column that subtracts 3 months to the value in the <b>date</b> column:
<b>from pandas.tseries.offsets import DateOffset
#create new column that subtracts 3 months from date
df['date_minus3'] = df.date + DateOffset(months=3)
#view updated DataFrame
print(df)
        date  sales date_minus3
0 2022-01-31      6  2021-10-31
1 2022-02-28      8  2021-11-28
2 2022-03-31      9  2021-12-31
3 2022-04-30      5  2022-01-30
4 2022-05-31      4  2022-02-28
5 2022-06-30      8  2022-03-30
6 2022-07-31      8  2022-04-30
7 2022-08-31      3  2022-05-31
8 2022-09-30      5  2022-06-30
9 2022-10-31      9  2022-07-31</b>
The new column <b>date_minus3</b> represents the values in the <b>date</b> column with three months subtracted from each value.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Convert Columns to DateTime in Pandas 
 How to Convert Datetime to Date in Pandas 
 How to Extract Month from Date in Pandas 
<h2><span class="orange">How to Add and Subtract Months from a Date in R</span></h2>
You can use the following functions from the  lubridate  package in R to quickly add and subtract months from a date:
<b>Method 1: Add Months</b>
<b>#add two months to date
my_date %m+% months(2)
</b>
<b>Method 2: Subtract Months</b>
<b>#subtract two months from date
my_date %m-% months(2)</b>
The following examples show how to use each method in practice.
<h2>Example 1: Add Months to Date</h2>
The following code shows how to add two months to a date in R:
<b>library(lubridate)
#define date
my_date &lt;- as.Date("2022-7-15")
#add two months to date
my_date %m+% months(2)
[1] "2022-09-15"</b>
Notice that two months have been added to the original date of 7/15/2022 to produce a new date of 9/15/2022.
<h2>Example 2: Subtract Months from Date</h2>
The following code shows how to subtract two months from a date in R:
<b>library(lubridate)
#define date
my_date &lt;- as.Date("2022-7-15")
#subtract two months from date
my_date %m-% months(2)
[1] "2022-05-15"</b>
Notice that two months have been subtracted from the original date of 7/15/2022 to produce a new date of 5/15/2022.
<h2>Example 3: Add & Subtract Months in Data Frame</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(date=as.Date(c("2022-3-14", "2022-5-29", "2022-7-15")), sales=c(140, 119, 138))
#view data frame
df
        date sales
1 2022-03-14   140
2 2022-05-29   119
3 2022-07-15   138</b>
We can use the following code to create new columns in the data frame by adding or subtracting months from the original value in the <b>date</b> column:
<b>library(lubridate)
#create new column that adds two months to each date
df$two_months_after &lt;- df$date %m+% months(2)
#create new column that subtracts two months from each date
df$two_months_before &lt;- df$date %m-% months(2)
#view updated data frame
df
        date sales two_months_after two_months_before
1 2022-03-14   140       2022-05-14        2022-01-14
2 2022-05-29   119       2022-07-29        2022-03-29
3 2022-07-15   138       2022-09-15        2022-05-15
</b>
Notice that two new columns have been added to the data frame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in R:
 How to Extract Year from Date in R 
 How to Group Data by Month in R (W 
 How to Calculate Number of Months Between Dates in R 
<h2><span class="orange">How to Add Text to ggplot2 Plots (With Examples)</span></h2>
You can use the <b>annotate()</b> function to add text to plots in ggplot2.
This function uses the following basic syntax:
<b>p +
  annotate("text", x=6, y=10, label= "hello")
</b>
where:
<b>x, y</b>: The (x, y) coordinates where the text should be placed.
<b>label</b>: The text to display.
The following examples show how to use this function in practice.
<h3>Example 1: Add One Text Element to ggplot2</h3>
The following code shows how to use <b>annotate()</b> to add one text element to a ggplot2 scatterplot:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 3, 3, 5, 7, 8, 10, 11), y=c(3, 5, 7, 5, 8, 10, 14, 19))
#create scatter plot with one text element
ggplot(df, aes(x=x, y=y)) +
  geom_point()
  annotate("text", x=6, y=10, label= "hello")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/annotate1.jpg"586">
Notice that our text element has been added to the (x, y) coordinates of (6, 10) in the plot.
<h3>Example 2: Add Multiple Text Elements to ggplot2</h3>
The following code shows how to use <b>annotate()</b> to add multiple text elements to a ggplot2 scatterplot:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 3, 3, 5, 7, 8, 10, 11), y=c(3, 5, 7, 5, 8, 10, 14, 19))
#create scatter plot with one text element
ggplot(df, aes(x=x, y=y)) +
  geom_point() +
  annotate("text", x=6, y=10, label= "hello") +
  annotate("text", x=3, y=15, label= "hello again")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/annotate2.jpg"575">
Notice that two text elements have been added to the plot at the coordinates that we specified.
<h3>Example 3: Customize Text Elements in Plot</h3>
We can use the <b>size</b>, <b>col</b>, and <b>italic</b> or <b>bold</b> arguments to customize the size, color, and font style of the text elements in the plot, respectively:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 3, 3, 5, 7, 8, 10, 11), y=c(3, 5, 7, 5, 8, 10, 14, 19))
#create scatter plot with custom text element
ggplot(df, aes(x=x, y=y)) +
  geom_point() +
  annotate("text", x=6, y=10, label= "bolditalic(hello)",
           col="blue", size=10, parse=TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/04/annotate3.jpg"570">
Notice that the one text element in the plot is now bold, italic, blue and has a size of 10.
<b>Note</b>: You can find the complete documentation for the <b>annotate()</b> function in ggplot2  here .
<h2><span class="orange">How to Add a Vertical Line to Charts in Excel</span></h2>
Occasionally you may want to add a vertical line to a chart in Excel at a specific position.
This tutorial provides a step-by-step example of how to add a vertical line to the following line chart in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical5.jpg">
Let’s jump in!
<h3>Step 1: Enter the Data</h3>
Suppose we would like to create a line chart using the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical1.jpg"406">
<h3>Step 2: Add Data for Vertical Line</h3>
Now suppose we would like to add a vertical line located at x = 6 on the plot.
We can add in the following artificial (x, y) coordinates to the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical2.jpg"461">
<h3>Step 3: Create Line Chart with Vertical Line</h3>
Lastly, we can highlight the cells in the range <b>A2:C14</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Scatter with Smooth Lines</b> within the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical3.jpg"555">
The following line chart will be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical4.jpg"668">
Notice that the vertical line is located at <b>x = 6</b>, which we specified at the end of our original dataset.
The vertical line ranges from <b>y = 0</b> to<b> y =25</b>, which we also specified in our original dataset.
To change the height of the line, simply change the y-values to use whatever starting and ending points you’d like.
<h3>Step 4: Customize the Chart (Optional)</h3>
Lastly, feel free to modify the range of each axis along with the style of the vertical line to make it more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/vertical5.jpg">
<h2><span class="orange">How to Add a Vertical Line to a Chart in Google Sheets</span></h2>
Occasionally you may want to add a vertical line to a chart in Google Sheets to represent a target line, an average line, or some other metric.
This tutorial provides a step-by-step example of how to to add a vertical line to the following line chart in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle5.jpg">
<h3>Step 1: Enter the Data</h3>
Suppose we would like to create a line chart using the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle1.jpg"454">
<h3>Step 2: Add Data for Vertical Line</h3>
Now suppose we would like to add a vertical line located at x = 6 on the plot.
We can add in the following artificial (x, y) coordinates to the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle2.jpg"472">
<h3>Step 3: Create Line Chart with Vertical Line</h3>
Lastly, we can highlight the cells in the range <b>A1:C13</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle3.jpg"481">
The following line chart will be automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle4.jpg"547">
Notice that the vertical line is located at <b>x = 6</b>, which we specified at the end of our original dataset.
The vertical line ranges from <b>y = 0</b> to<b> y =25</b>, which we also specified in our original dataset.
To change the height of the line, simply change the y-values to use whatever starting and ending points you’d like.
<h3>Step 4: Customize the Chart (Optional)</h3>
Feel free to double click on the vertical line to customize the color and style to make it more aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/verticalgoogle5.jpg">
<h2><span class="orange">How to Create Added Variable Plots in R</span></h2>
In statistics, <b>added variable plots</b> are individual plots that display the relationship between a  response variable  and one predictor variable in a multiple linear regression model, while controlling for the presence of other predictor variables in the model.
<b>Note:</b> Sometimes these plots are also called “partial regression plots.”
These type of plots allow us to observe the relationship between each individual predictor variable and the response variable in a model while holding other predictor variables constant.
To create added variable plots in R, we can use the <b>avPlots()</b> function from the <b>car</b> package:
<b>#load <em>car</em> package
library(car) 
#fit multiple linear regression model
model &lt;- lm(y ~ x1 + x2 + ..., data = df)
#create added variable plots
avPlots(model)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Added Variable Plots in R</h3>
Suppose we fit the following multiple linear regression model in R, using data from the <b>mtcars</b> dataset:
<b>#fit multiple linear regression model
model &lt;- lm(mpg ~ disp + hp + drat, data = mtcars)
#view summary of model
summary(model)
Call:
lm(formula = mpg ~ disp + hp + drat, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-5.1225 -1.8454 -0.4456  1.1342  6.4958 
Coefficients:
             Estimate Std. Error t value Pr(>|t|)   
(Intercept) 19.344293   6.370882   3.036  0.00513 **
disp        -0.019232   0.009371  -2.052  0.04960 * 
hp          -0.031229   0.013345  -2.340  0.02663 * 
drat         2.714975   1.487366   1.825  0.07863 . 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 3.008 on 28 degrees of freedom
Multiple R-squared:  0.775,Adjusted R-squared:  0.7509 
F-statistic: 32.15 on 3 and 28 DF,  p-value: 3.28e-09</b>
To visualize the relationship between the response variable “mpg” and each individual predictor variable in the model, we can produce added variable plots using the <b>avPlots()</b> function:
<b>#load car package
library(car)
#produce added variable plots
avPlots(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/addedVarPlots1.png">
Here is how to interpret each plot:
The x-axis displays a single predictor variable and the y-axis displays the response variable.
The blue line shows the association between the predictor variable and the response variable, <em>while holding the value of all other predictor variables constant</em>.
The points that are labelled in each plot represent the two observations with the largest  residuals  and the two observations with the largest partial leverage.
Note that the angle of the line in each plot matches the sign of the coefficient from the estimated regression equation.
For example, here are the estimated coefficients for each predictor variable from the model:
<b>disp: </b>-0.019232
<b>hp: </b>-0.031229
<b>drat: </b>2.714975
Notice that the angle of the line is positive in the added variable plot for <em>drat</em> while negative for both <em>disp</em> and <em>hp</em>, which matches the signs of their estimated coefficients:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/addedVarPlots2.png">
These plots allow us to conveniently visualize the relationship between each individual predictor variable and the response variable.
<h2><span class="orange">Adjusted Odds Ratio: Definition + Examples</span></h2>
In statistics, an <b>odds ratio</b> tells us the ratio of the odds of an event occurring in a treatment group to the odds of an event occurring in a control group.
Odds ratios appear most often in  logistic regression , which is a method we use to fit a regression model that has one or more predictor variables and a binary response variable.
An <b>adjusted odds ratio</b> is an odds ratio that has been adjusted to account for other predictor variables in a model.
It’s particularly useful for helping us understand how a predictor variable affects the odds of an event occurring, <em>after</em> adjusting for the effect of other predictor variables.
The following example illustrates the difference between an odds ratio and an adjusted odds ratio.
<h3>Example: Calculating Adjusted Odds Ratios</h3>
Suppose we are interested in understanding whether a mother’s age affects the probability of having a baby with a low birthweight.
To explore this, we can perform logistic regression using age as a predictor variable and low birthweight (yes or no) as a  response variable .
Suppose we collect data for 300 mothers and fit a logistic regression model. Here are the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/adjusted_odds_ratio1.png">
To obtain the odds ratio for age, we simply need to exponentiate the coefficient estimate from the table: e<sup>0.173</sup> = <b>1.189</b>.
This tells us that an increase of one year in age is associated with an increase of <b>1.189 </b>in the odds of a baby having low birthweight. In other words, the odds of having a baby with low birthweight are increased by <b>18.9%</b> for each additional yearly increase in age.
This odds ratio is known as a “crude” odds ratio or an “unadjusted” odds ratio because it has not been adjusted to account for other predictor variables in the model since it is the <em>only</em> predictor variable in the model.
But suppose we were interested in understanding whether a mother’s age <em>and</em> her smoking habits affect the probability of having a baby with a low birthweight.
To explore this, we can perform logistic regression using age and smoking (yes or no) as predictor variables and low birthweight as a  response variable .
Suppose we collect data for 300 mothers and fit a logistic regression model. Here are the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/adjusted_odds_ratio2.png">
Here is how to interpret the results:
<b>Age:</b> The adjusted odds ratio for age is calculated as e<sup>.045</sup> = <b>1.046</b>. This means the odds of having a baby with low birthweight are increased by 4.6% for each additional yearly increase in age, assuming the variable <em>smoking</em> is held constant.
For example, suppose mother A and mother B are both smokers. If mother A is one year older than mother B, then the odds that mother A has a low birthweight baby are 1.046 times the odds that mother B has a low birthweight baby.
<b>Smoking</b>: The adjusted odds ratio for smoking is calculated as e<sup>.485</sup> = <b>1.624</b>. This means the odds of having a baby with low birthweight are increased by 62.4% if the mother smokes (compared to not smoking), assuming the variable <i>age </i>is held constant.
For example, suppose mother A and mother B are both 30 years old. If mother A smokes during pregnancy and mother B does not, then the odds that mother A has a low birthweight baby are 62.4% higher than the odds that mother B has a low birthweight baby.
Note that the adjusted odds ratio for age is lower than the unadjusted odds ratio from the previous example. This is because when other predictor variables increase the odds of the response variable occurring, the adjusted odds ratio for a predictor variable already in the model will always decrease.
<h3>Summary: Odds Ratio vs. Adjusted Odds Ratio</h3>
An <b>odds ratio</b> (sometimes called a “crude” odds ratio) is useful for telling us how changes in one predictor variable affect the odds of some response variable occurring.
An <b>adjusted odds ratio</b> is useful for telling us how changes in one predictor variable affect the odds of a response variable occurring, <em>after</em> controlling for other predictor variables in a model.
<h2><span class="orange">How to Calculate Adjusted R-Squared in Excel</span></h2>
<b>R-squared</b>, often written R<sup>2</sup>, is the proportion of the variance in the  response variable  that can be explained by the predictor variables in a  linear regression model .
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all while a value of 1 indicates that the response variable can be perfectly explained without error by the predictor variables.
The <b>adjusted R-squared</b> is a modified version of R-squared that adjusts for the number of predictors in a regression model. It is calculated as:
<b>Adjusted R<sup>2</sup> = 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)]</b>
where:
<b>R<sup>2</sup></b>: The R<sup>2</sup> of the model
<b>n</b>: The number of observations
<b>k</b>: The number of predictor variables
Because R<sup>2</sup> always increases as you add more predictors to a model, adjusted R<sup>2</sup> can serve as a metric that tells you how useful a model is, <em>adjusted for the number of predictors in a model</em>.
This tutorial provides a step-by-step example of how to calculate adjusted R<sup>2</sup> for a regression model in R.
<h3>Step 1: Create the Data</h3>
For this example, we’ll create a dataset that contains the following variables for 12 different students:
Exam Score
Hours Spent Studying
Current Grade
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel1.png">
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll fit a  multiple linear regression  model using <em>Exam Score</em> as the  response variable  and <em>Study Hours</em> and <em>Current Grade</em> as the predictor variables.
To fit this model, click the <b>Data</b> tab along the top ribbon and then click <b>Data Analysis</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option available, you need to first  load the Data Analysis ToolPak .
In the window that pops up, select <b>Regression</b>. In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel2.png">
Once you click <b>OK</b>, the output of the regression model will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/standRegExcel3.png">
<h3>Step 3: Interpret the Adjusted R-Squared</h3>
The adjusted R-squared of the regression model is the number next to <b>Adjusted R Square</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/adjRExcel1.png">
The adjusted R-squared for this model turns out to be <b>0.946019</b>.
This value is extremely high, which indicates that the predictor variables <em>Study Hours</em> and <em>Current Grade</em> do a good job of predicting <em>Exam Score</em>.
<footer>
<imghttps://secure.gravatar.com/avatar/f9bf2177feb3943066b872bbbff44a0d?s=68&d=mm&r=g"><b>J T M</b> <span>says:
<!-- .comment-author -->
 <time datetime="2021-02-12T11:44:51-05:00">February 12, 2021 at 11:44 am</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
While it is true that adjusting R^2 (or partial eta^2) takes the number of predictors into account, the reason that this should always be done, even when you aren’t comparing models with different numbers of predictors, is that the adjustment removes (almost all of) the positive bias in the unadjusted value.  This was the original purpose of the adjustment and is rather important when the value of effect size will be used in a subsequent power analysis.
see Mordkoff, J.T.  (2019).  A simple method for removing bias from a popular measure of standardized effect size: Adjusted partial eta squared.  Advances in Methods and Practices in Psychological Science, 2, 228-232.
<!-- .comment-content -->
 Reply </article>
<h2><span class="orange">How to Calculate Adjusted R-Squared in Python</span></h2>
<b>R-squared</b>, often written R<sup>2</sup>, is the proportion of the variance in the  response variable  that can be explained by the predictor variables in a  linear regression model .
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all while a value of 1 indicates that the response variable can be perfectly explained without error by the predictor variables.
The <b>adjusted R-squared</b> is a modified version of R-squared that adjusts for the number of predictors in a regression model. It is calculated as:
<b>Adjusted R<sup>2</sup> = 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)]</b>
where:
<b>R<sup>2</sup></b>: The R<sup>2</sup> of the model
<b>n</b>: The number of observations
<b>k</b>: The number of predictor variables
Since R<sup>2</sup> always increases as you add more predictors to a model, adjusted R<sup>2</sup> can serve as a metric that tells you how useful a model is, <em>adjusted for the number of predictors in a model</em>.
This tutorial shows two examples of how to calculate adjusted R<sup>2</sup> for a regression model in Python.
<b>Related:</b>  What is a Good R-squared Value? 
<h3>Example 1: Calculate Adjusted R-Squared with sklearn</h3>
The following code shows how to fit a multiple linear regression model and calculate the adjusted R-squared of the model using sklearn:
<b>from sklearn.linear_model import LinearRegression
import pandas as pd
#define URL where dataset is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data = pd.read_csv(url)
#fit regression model
model = LinearRegression()
X, y = data[["mpg", "wt", "drat", "qsec"]], data.hp
model.fit(X, y)
#display adjusted R-squared
1 - (1-model.score(X, y))*(len(y)-1)/(len(y)-X.shape[1]-1)
0.7787005290062521</b>
The adjusted R-squared of the model turns out to be <b>0.7787</b>.
<h3>Example 2: Calculate Adjusted R-Squared with statsmodels</h3>
The following code shows how to fit a multiple linear regression model and calculate the adjusted R-squared of the model using statsmodels:
<b>import statsmodels.api as sm
import pandas as pd
#define URL where dataset is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data = pd.read_csv(url)
#fit regression model
X, y = data[["mpg", "wt", "drat", "qsec"]], data.hp
X = sm.add_constant(X)
model = sm.OLS(y, X).fit()
#display adjusted R-squared
print(model.rsquared_adj)
0.7787005290062521</b>
The adjusted R-squared of the model turns out to be <b>0.7787</b>, which matches the result from the previous example.
<h2><span class="orange">How to Calculate Adjusted R-Squared in R</span></h2>
<b>R-squared</b>, often written R<sup>2</sup>, is the proportion of the variance in the  response variable  that can be explained by the predictor variables in a  linear regression model .
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all while a value of 1 indicates that the response variable can be perfectly explained without error by the predictor variables.
The <b>adjusted R-squared</b> is a modified version of R-squared that adjusts for the number of predictors in a regression model. It is calculated as:
<b>Adjusted R<sup>2</sup> = 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)]</b>
where:
<b>R<sup>2</sup></b>: The R<sup>2</sup> of the model
<b>n</b>: The number of observations
<b>k</b>: The number of predictor variables
Because R<sup>2</sup> always increases as you add more predictors to a model, adjusted R<sup>2</sup> can serve as a metric that tells you how useful a model is, <em>adjusted for the number of predictors in a model</em>.
This tutorial explains how to calculate adjusted R<sup>2</sup> for a regression model in R.
<b>Related:</b>  What is a Good R-squared Value? 
<h3>Example: How to Calculate Adjusted R-Squared in R</h3>
We can use the following code to build a multiple linear regression model in R using the built-in dataset called <b>mtcars</b>:
<b>model &lt;- lm(hp ~ mpg + wt + drat + qsec, data=mtcars)
</b>
And we can use one of the following three methods to find the adjusted R-squared of the model:
<b>Method 1: Use the summary() function</b>
We can view both the R-squared and the adjusted R-squared of the model by simply using the <b>summary()</b> function:
<b>summary(model)
Call:
lm(formula = hp ~ mpg + wt + drat + qsec, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-48.801 -16.007  -5.482  11.614  97.338 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  473.779    105.213   4.503 0.000116 ***
mpg           -2.877      2.381  -1.209 0.237319    
wt            26.037     13.514   1.927 0.064600 .  
drat           4.819     15.952   0.302 0.764910    
qsec         -20.751      3.993  -5.197 1.79e-05 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 32.25 on 27 degrees of freedom
Multiple R-squared:  0.8073,Adjusted R-squared:  0.7787 
F-statistic: 28.27 on 4 and 27 DF,  p-value: 2.647e-09
</b>
At the bottom of the output we can see the following:
Multiple R-squared: <b>0.8073</b>
Adjusted R-squared: <b>0.7787</b>
<b>Method 2: Use summary(model)$adj.r.squared </b>
If we simply wanted to obtain the adjusted R-squared of the model, we could use the following function:
<b>summary(model)$adj.r.squared
[1] 0.7787005
</b>
<b>Method 3: Use a custom function</b>
Yet another way to find the adjusted R-squared of the model is to write a custom function:
<b>#define function to calculate adjusted R-squared
adj_r2 &lt;- function(x) {
   return (1 - ((1-x$adj.r.squared)*(nobs(x)-1)/(nobs(x)-length(x$coefficients)-1)))
}
#use function to calculate adjusted R-squared of the model
adj_r2(model)
[1] 0.7787005
numeric(0)</b>
Notice that each of the three methods shared here result in the same value for adjusted R-squared.
<h2><span class="orange">How to Interpret Adjusted R-Squared (With Examples)</span></h2>
When we fit  linear regression models  we often calculate the <b>R-squared</b> value of the model.
The R-squared value is the proportion of the variance in the  response variable  that can be explained by the predictor variables in the model.
The value for R-squared can range from 0 to 1 where:
A value of <b>0</b> indicates that the response variable cannot be explained by the predictor variables at all.
A value of <b>1</b> indicates that the response variable can be perfectly explained by the predictor variables.
Although this metric is commonly used to assess how well a regression model fits a dataset, it has one serious drawback:
<b>The drawback of R-squared:</b>
 
R-squared will always increase when a new predictor variable is added to the regression model.
Even if a new predictor variable is almost completely unrelated to the response variable, the R-squared value of the model will increase, if only by a small amount.
For this reason, it’s possible that a regression model with a large number of predictor variables has a high R-squared value, even if the model doesn’t fit the data well.
Fortunately there is an alternative to R-squared known as <b>adjusted R-squared</b>.
The <b>adjusted R-squared</b> is a modified version of R-squared that adjusts for the number of predictors in a regression model.
It is calculated as:
<b>Adjusted R<sup>2</sup> = 1 – [(1-R<sup>2</sup>)*(n-1)/(n-k-1)]</b>
where:
<b>R<sup>2</sup></b>: The R<sup>2</sup> of the model
<b>n</b>: The number of observations
<b>k</b>: The number of predictor variables
Because R-squared always increases as you add more predictors to a model, the adjusted R-squared can tell you how useful a model is, <em>adjusted for the number of predictors in a model</em>.
<b>The advantage of Adjusted R-squared:</b>
 
Adjusted R-squared tells us how well a set of predictor variables is able to explain the variation in the response variable, <em>adjusted for the number of predictors in a model</em>.
 
Because of the way it’s calculated, adjusted R-squared can be used to compare the fit of regression models with different numbers of predictor variables.
To gain a better understanding of adjusted R-squared, check out the following example.
<h3>Example: Understanding Adjusted R-Squared in Regression Models</h3>
Suppose a professor collects data on students in his class and fits the following regression model to understand how hours spent studying and current grade in the class affect the score a student receives on the final exam.
Exam Score = β<sub>0</sub> + β<sub>1</sub>(hours spent studying) + β<sub>2</sub>(current grade)
Suppose this regression model has the following metrics:
R-squared: <b>0.955</b>
Adjusted R-squared: <b>0.946</b>
Now suppose the professor decides to collect data on another variable for each student: shoe size.
Although this variable should be completely unrelated to the final exam score, he decides to fit the following regression model:
Exam Score = β<sub>0</sub> + β<sub>1</sub>(hours spent studying) + β<sub>2</sub>(current grade) + β<sub>3</sub>(shoe size)
Suppose this regression model has the following metrics:
R-squared: <b>0.965</b>
Adjusted R-squared: <b>0.902</b>
If we only looked at the <b>R-squared</b> values for each of these two regression models, we would conclude that the second model is better to use because it has a higher R-squared value!
However, if we look at the <b>adjusted R-squared</b> values then we come to a different conclusion: The first model is better to use because it has a higher adjusted R-squared value.
The second model only has a higher R-squared value because it has more predictor variables than the first model.
However, the predictor variable that we added (shoe size) was a poor predictor of final exam score, so the adjusted R-squared value penalized the model for adding this predictor variable.
This example illustrates why adjusted R-squared is a better metric to use when comparing the fit of regression models with different numbers of predictor variables.
<h2><span class="orange">How to Fix: aesthetics must be either length 1 or the same as the data</span></h2>
One error you may encounter in R is:
<b>Error: Aesthetics must be either length 1 or the same as the data (5): fill
</b>
This error occurs when you attempt to specify the <b>fill</b> colors to use in a ggplot2 plot, yet the number of colors you specified is different than 1 or different than the total number of objects to be filled.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we’re working with the built-in R dataset called <b>airquality</b>:
<b>#view first six lines of <em>airquality</em> dataset
head(airquality)
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA      NA 14.3   56     5   5
6    28      NA 14.9   66     5   6
</b>
Now suppose we attempt to create multiple boxplots to visualize the distribution of Ozone values for each Month:
<b>library(ggplot2)
#attempt to create multiple boxplots
ggplot(data = airquality, aes(x=as.character(Month), y=Temp)) +
    geom_boxplot(fill=c('steelblue', 'red'))
Error: Aesthetics must be either length 1 or the same as the data (5): fill
</b>
We receive an error because there are 5 unique Months in the dataset (thus, we will create 5 boxplots) but we only supplied two colors to the <b>fill</b> argument.
<h3>How to Fix the Error</h3>
There are two ways to fix this error:
<b>Method 1: Only Use One Color in Fill Argument</b>
We could choose to use just one color in the fill argument:
<b>library(ggplot2)
ggplot(data = airquality, aes(x=as.character(Month), y=Temp)) +
    geom_boxplot(fill=c('steelblue'))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/aes1.png">
This allows us to fill in each boxplot with the same color.
<b>Method 2: Use the Same Number of Colors as the Number of Boxplots</b>
We could also specify five colors to use since this matches the number of boxplots we will create:
<b>library(ggplot2)
ggplot(data = airquality, aes(x=as.character(Month), y=Temp)) +
    geom_boxplot(fill=c('steelblue', 'red', 'purple', 'green', 'orange'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/aes2.png">
We don’t receive and error because the number of colors we supplied matches the number of boxplots.
<h2><span class="orange">How to Aggregate Daily Data to Monthly and Yearly in R</span></h2>
Occasionally you may want to aggregate daily data to weekly, monthly, or yearly data in R.
This tutorial explains how to easily do so using the <b>lubridate</b> and <b>dplyr</b> packages.
<h2>Example: Aggregate Daily Data in R</h2>
Suppose we have the following data frame in R that shows the daily sales of some item over the course of 100 consecutive days:
<b>#make this example reproducible
set.seed(1)
#create data frame 
df &lt;- data.frame(date = as.Date("2020-12-01") + 0:99, sales = runif(100, 20, 50))
#view first six rows
head(df)
        date    sales
1 2020-12-01 27.96526
2 2020-12-02 31.16372
3 2020-12-03 37.18560
4 2020-12-04 47.24623
5 2020-12-05 26.05046
6 2020-12-06 46.95169</b>
To aggregate this data, we can use the  floor_date()  function from the <b>lubridate </b>package which uses the following syntax:
<b>floor_date(x, unit)</b>
where:
<b>x: </b>A vector of date objects.
<b>unit: </b>A time unit to round to. Options include second, minute, hour, day, week, month, bimonth, quarter, halfyear, and year.
The following code snippets show how to use this function along with the  group_by()  and  summarize()  functions from the <b>dplyr </b>package to find the mean sales by week, month, and year:
<b>Mean Sales by Week</b>
<b>library(lubridate)
library(dplyr)
#round dates down to week
df$week &lt;- floor_date(df$date, "week")
#find mean sales by week
df %>%
  group_by(week) %>%
  summarize(mean = mean(sales))
# A tibble: 15 x 2
   week        mean
        
 1 2020-11-29  33.9
 2 2020-12-06  35.3
 3 2020-12-13  39.0
 4 2020-12-20  34.4
 5 2020-12-27  33.6
 6 2021-01-03  35.9
 7 2021-01-10  37.8
 8 2021-01-17  36.8
 9 2021-01-24  32.8
10 2021-01-31  33.9
11 2021-02-07  34.1
12 2021-02-14  41.6
13 2021-02-21  31.8
14 2021-02-28  35.2
15 2021-03-07  37.1</b>
<b>Mean Sales by Month</b>
<b>library(lubridate)
library(dplyr)
#round dates down to week
df$month &lt;- floor_date(df$date, "month")
#find mean sales by month
df %>%
  group_by(month) %>%
  summarize(mean = mean(sales))
# A tibble: 4 x 2
  month       mean
       
1 2020-12-01  35.3
2 2021-01-01  35.6
3 2021-02-01  35.2
4 2021-03-01  37.0
</b>
<b>Mean Sales by Year</b>
<b>library(lubridate)
library(dplyr)
#round dates down to week
df$year &lt;- floor_date(df$date, "year")
#find mean sales by month
df %>%
  group_by(year) %>%
  summarize(mean = mean(sales))
# A tibble: 2 x 2
  year        mean
       
1 2020-01-01  35.3
2 2021-01-01  35.7
</b>
Note that we chose to aggregate by the mean, but we could use any summary statistic we’d like such as the median, mode, max, min, etc.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Calculate the Mean by Group in R 
 How to Calculate Cumulative Sums in R 
 How to Plot a Time Series in R 
<h2><span class="orange">The Complete Guide: How to Use the aggregate() Function in R</span></h2>
The <b>aggregate()</b> function in R can be used to calculate summary statistics for a dataset.
This function uses the following basic syntax: 
<b>aggregate(x, by, FUN)</b>
where:
<b>x</b>: A variable to aggregate
<b>by</b>: A list of variables to group by
<b>FUN</b>: The summary statistic to compute
The following examples show how to use this function in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'B'), position=c('G', 'G', 'F', 'G', 'F', 'F'), points=c(99, 90, 86, 88, 95, 99), assists=c(33, 28, 31, 39, 34, 23), rebounds=c(30, 28, 24, 24, 28, 33))
#view data frame
df
  team position points assists rebounds
1    A        G     99      33       30
2    A        G     90      28       28
3    A        F     86      31       24
4    B        G     88      39       24
5    B        F     95      34       28
6    B        F     99      23       33
</b>
<h3>Example 1: Aggregate Mean by Group</h3>
The following code shows how to use the <b>aggregate()</b> function to calculate the mean number of points scored by team:
<b>#find mean points by team
aggregate(df$points, by=list(df$team), FUN=mean)
  Group.1        x
1       A 91.66667
2       B 94.00000
</b>
This tells us:
Players on team A scored an average of <b>91.67</b> points per game.
Players on team B scored an average of <b>94</b> points per game.
Note that you can also change the names of the columns in the output by using the <b>colnames()</b> function:
<b>#find mean points by team
agg &lt;- aggregate(df$points, by=list(df$team), FUN=mean)
#rename columns in output
colnames(agg) &lt;- c('Team', 'Mean_Points')
#view output
agg
  Team Mean_Points
1    A    91.66667
2    B    94.00000</b>
<h3>Example 2: Aggregate Count by Group</h3>
The following code shows how to use the <b>aggregate()</b> function to count the number of players by team:
<b>#count number of players by team
aggregate(df$points, by=list(df$team), FUN=length)
  Group.1 x
1       A 3
2       B 3
</b>
This tells us:
Team A has <b>3</b> players.
Team B has <b>3</b> players.
<h3>Example 3: Aggregate Sum by Group</h3>
The following code shows how to use the <b>aggregate()</b> function to calculate the sum of points scored by each team:
<b>#find sum of points scored by team
aggregate(df$points, by=list(df$team), FUN=sum)
  Group.1   x
1       A 275
2       B 282
</b>
This tells us:
Team A scored a total of <b>275</b> points.
Team B scored a total of <b>282</b> points.
<h3>Example 4: Aggregate Multiple Columns</h3>
The following code shows how to use the <b>aggregate()</b> function to find the mean number of points scored, grouped by team and position:
<b>#find mean of points scored, grouped by team and position
aggregate(df$points, by=list(df$team, df$position), FUN=mean)
  Group.1 Group.2    x
1       A       F 86.0
2       B       F 97.0
3       A       G 94.5
4       B       G 88.0
</b>
This tells us:
Players in the ‘F’ position on Team A scored an average of <b>86 </b>points.
Players in the ‘F’ position on Team B scored an average of <b>97 </b>points.
Players in the ‘G’ position on Team A scored an average of <b>94.5 </b>points.
Players in the ‘G’ position on Team B scored an average of <b>88 </b>points.
<h2><span class="orange">What is Aggregation Bias? (Explanation & Example)</span></h2>
<b>Aggregation bias </b>occurs when it is wrongly assumed that the trends seen in aggregated data also apply to individual data points.
The easiest way to understand this type of bias is with a simple example.
<h3>Example: Aggregation Bias</h3>
Suppose researchers want to understand the relationship between the average number of years of education and average household income in a certain state. They obtain aggregated data for 4 different cities within the state and calculate the correlation between average education and average household income.
It turns out that  the correlation  between average number of years of education and average household income is <b>0.9632</b>. This is a highly positive correlation coefficient.
The researchers even create a  scatterplot  to visualize the relationship between average number of years of education and average household income: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/aggregationBias1.png">
Without actually looking at the individual data, they may publish a report that claims that more years of education is strongly positively correlated with household income.
However, suppose a new researcher comes along a year later and obtains data for individual households across the same set of cities. Suppose she creates the following scatterplot of the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/aggregationBias2.png">
She calculates the correlation between the two variables and finds that it’s actually only <b>0.1788</b> – still a positive correlation but not nearly as strong as the correlation found by the previous researchers.
It turns out that when the data became aggregated, it covered the true trend between education and income that was taking place at the individual level.
In fact, when we look at a city-by-city basis in the scatterplot the relationship between education and income is actually negative!
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/aggregationBias3.png">
<h3>Effects of Aggregation Bias</h3>
Aggregation bias occurs quite often in research simply because it’s often wrongly assumed that the trends that appear at an aggregate level must also appear at an individual level. Unfortunately, this is not always the case as the previous example showed.
Aggregation bias can cause the results of a study to draw the wrong conclusions and can be misleading. This type of bias is particularly harmful when it relates to correlations between variables.
Even if the correlation between aggregated data of two variables is positive, the underlying correlation between the two variables at an individual observation level can actually be:
Negative correlation
No correlation
Positive correlation 
The way to avoid this type of bias is to conduct studies using individual data points as opposed to aggregated data points so that the true relationship between two variables can be discovered.
<h2><span class="orange">How to Calculate AIC of Regression Models in Python</span></h2>
The Akaike information criterion (AIC) is a metric that is used to compare the fit of different regression models.
It is calculated as:
AIC = 2K – 2<em>ln</em>(L)
where:
<b>K:</b> The number of model parameters. The default value of K is 2, so a model with just one predictor variable will have a K value of 2+1 = 3.
<b><em>ln</em>(L)</b>: The log-likelihood of the model. This tells us how likely the model is, given the data.
The AIC is designed to find the model that explains the most variation in the data, while penalizing for models that use an excessive number of parameters.
Once you’ve fit several regression models, you can compare the AIC value of each model. The model with the lowest AIC offers the best fit.
To calculate the AIC of several regression models in Python, we can use the <b>statsmodels.regression.linear_model.OLS()</b> function, which has a property called <b>aic</b> that tells us the AIC value for a given model.
The following example shows how to use this function to calculate and interpret the AIC for various regression models in Python.
<h3>Example: Calculate & Interpret AIC in Python</h3>
Suppose we would like to fit two different  multiple linear regression models  using variables from the <b>mtcars</b> dataset.
First, we’ll load this dataset:
<b>from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import pandas as pd
#define URL where dataset is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data = pd.read_csv(url)
#view head of data
data.head()
        model          mpg  cyldisp  hp    drat wt   qsec  vs am gear carb
0Mazda RX4  21.0  6     160.0 110  3.90 2.620 16.46   0  1  44
1Mazda RX4 Wag  21.0  6     160.0 110  3.90 2.875 17.02   0  1  44
2Datsun 710  22.8  4     108.0 93   3.85 2.320 18.61   1  1  41
3Hornet 4 Drive  21.4  6     258.0 110  3.08 3.215 19.44   1  0  31
4Hornet Sportabout 18.78     360.0 175  3.15 3.440 17.02   0  0  32
</b>
Here are the predictor variables we’ll use in each model:
Predictor variables in Model 1: disp, hp, wt, qsec
Predictor variables in Model 2: disp, qsec
The following code shows how to fit the first model and calculate the AIC:
<b>#define response variable
y = data['mpg']
#define predictor variables
x = data[['disp', 'hp', 'wt', 'qsec']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit regression model
model = sm.OLS(y, x).fit()
#view AIC of model
print(model.aic)
157.06960941462438</b>
The AIC of this model turns out to be <b>157.07</b>.
Next, we’ll fit the second model and calculate the AIC:
<b>#define response variable
y = data['mpg']
#define predictor variables
x = data[['disp', 'qsec']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit regression model
model = sm.OLS(y, x).fit()
#view AIC of model
print(model.aic)
169.84184864154588</b>
The AIC of this model turns out to be <b>169.84</b>.
Since the first model has a lower AIC value, it is the better fitting model. 
Once we’ve identified this model as the best, we can proceed to fit the model and analyze the results including the R-squared value and the beta coefficients to determine the exact relationship between the set of predictor variables and the  response variable .
<h2><span class="orange">How to Calculate AIC in R (Including Examples)</span></h2>
The Akaike information criterion (AIC) is a metric that is used to compare the fit of several regression models.
It is calculated as:
AIC = 2K – 2<em>ln</em>(L)
where:
<b>K:</b> The number of model parameters. The default value of K is 2, so a model with just one predictor variable will have a K value of 2+1 = 3.
<b><em>ln</em>(L)</b>: The log-likelihood of the model. Most statistical software can automatically calculate this value for you.
The AIC is designed to find the model that explains the most variation in the data, while penalizing for models that use an excessive number of parameters.
Once you’ve fit several regression models, you can compare the AIC value of each model. The lower the AIC, the better the model fit.
To calculate the AIC of several regression models in R, we can use the <b>aictab()</b> function from the <b>AICcmodavg</b> package.
The following example shows how to use this function to calculate and interpret the AIC for various regression models in R.
<h3>Example: Calculate & Interpret AIC in R</h3>
Suppose we would like to fit three different  multiple linear regression models  using variables from the <b>mtcars</b> dataset.
Here are the predictor variables we’ll use in each model:
Predictor variables in Model 1: disp, hp, wt, qsec
Predictor variables in Model 2: disp, qsec
Predictor variables in Model 3: disp, wt
The following code shows how to fit each of these regression models:
<b>#fit three models
model1 &lt;- lm(mpg ~ disp + hp + wt + qsec, data = mtcars)
model2 &lt;- lm(mpg ~ disp + qsec, data = mtcars)
model3 &lt;- lm(mpg ~ disp + wt, data = mtcars)
</b>
Next, we’ll put the models into a list and use the <b>aictab()</b> function to calculate the AIC of each model:
<b>library(AICcmodavg)
#define list of models
models &lt;- list(model1, model2, model3)
#specify model names
mod.names &lt;- c('disp.hp.wt.qsec', 'disp.qsec', 'disp.wt')
#calculate AIC of each model
aictab(cand.set = models, modnames = mod.names)
Model selection based on AICc:
                K   AICc Delta_AICc AICcWt Cum.Wt     LL
disp.hp.wt.qsec 6 162.43       0.00   0.83   0.83 -73.53
disp.wt         4 165.65       3.22   0.17   1.00 -78.08
disp.qsec       4 173.32      10.89   0.00   1.00 -81.92
</b>
Here’s how to interpret the output:
<b>K:</b> The number of parameters in the model.
<b>AICc:</b> The AIC value of the model. The lowercase ‘c’ indicates that the AIC has been calculated from the AIC corrected for small sample sizes.
<b>Delta_AICc:</b> The difference between the AIC of the best model compared to the current model being compared.
<b>AICcWt:</b> The proportion of the total predictive power that can be found in the model.
<b>Cum.Wt</b>: The cumulative sum of the AIC weights.
<b>LL:</b> The log-likelihood of the model. This tells us how likely the model is, given the data we used.
The model with the lowest AIC value is always listed first. From the output we can see that the following model has the lowest AIC value and is thus the best fitting model:
mpg = β<sub>0</sub> + β<sub>1</sub>(disp) + β<sub>2</sub>(hp) + β<sub>3</sub>(wt) + β<sub>4</sub>(qsec)
Once we’ve identified this model as the best, we can proceed to fit the model and analyze the results including the R-squared value and the beta coefficients to determine the exact relationship between the set of predictor variables and the  response variable .
<h2><span class="orange">How to Use all() and any() Functions in R (With Examples)</span></h2>
The <b>all()</b> and <b>any()</b> functions in R can be used to check if all or any values in a vector evaluate to TRUE for some expression.
These functions use the following syntax:
<b>#check if <em>all</em> values in x are less than 10
all(x &lt; 10)
#check if <em>any</em> values in x are less than 10
any(x &lt; 10)
</b>
The following examples show how to use each function in practice. 
<h3>Example 1: Use all() and any() with Vector</h3>
We can use the following <b>all()</b> and <b>any()</b> functions to check if all or any values in a vector are less than 10:
<b>#define vector of data values
data &lt;- c(3, 4, 4, 8, 12, 15)
#check if all values are less than 10
all(data &lt; 10)
[1] FALSE
#check if any values are less than 10
any(data &lt; 10)
[1] TRUE
</b>
The <b>all()</b> function evaluates to <b>FALSE</b> because not all values in the vector are less than 10.
The <b>any()</b> function evaluates to <b>TRUE</b> because at least one value in the vector is less than 10.
<h3>Example 2: Use all() with NA Values</h3>
If we use the <b>all()</b> function with a vector that has NA values, we may receive <b>NA</b> as a result: 
<b>#define vector of data values with some NA values
data &lt;- c(3, 4, 4, 8, NA, NA)
#check if all values are less than 10
all(data &lt; 10)
[1] NA
</b>
To avoid this, we must specify <b>na.rm=TRUE</b> to first remove the NA values from the vector before checking if all values meet some condition:
<b>#define vector of data values with some NA values
data &lt;- c(3, 4, 4, 8, NA, NA)
#check if all values are less than 10 (and ignore NA values)
all(data &lt; 10, na.rm=TRUE)
[1] TRUE
</b>
The <b>all()</b> function now evaluates to <b>TRUE</b> because every value in the vector is less than 10, assuming we ignore NA values.
<h3>Example 3: Use all() and any() with Data Frame Columns</h3>
We can also use the <b>all()</b> and <b>any()</b> functions to evaluate expressions for data frame columns.
For example, suppose we have the following data frame in R:
<b>#define data frame
df &lt;- data.frame(points=c(30, 22, 19, 20, 14, NA), assists=c(7, 8, 13, 13, 10, 6), rebounds=c(8, 12, NA, NA, 5, 8))
#view data frame
df
  points assists rebounds
1     30       7        8
2     22       8       12
3     19      13       NA
4     20      13       NA
5     14      10        5
6     NA       6        8</b>
We can use the <b>all()</b> and <b>any()</b> functions to evaluate different expressions for the values in the “rebounds” column:
<b>#check if all values are less than 10 in rebounds column
all(df$rebounds &lt; 10, na.rm=TRUE)
[1] FALSE
#check if any values are less than 10 in rebounds column
any(df$rebounds &lt; 10, na.rm=TRUE)
[1] TRUE
#check if there are any NA values in rebounds column
any(is.na(df$rebounds))
[1] TRUE
</b>
From the output we can see:
Not all values are less than 10 in the rebounds column.
At least one value is less than 10 in the rebounds column.
There is at least one NA value in the rebounds column.
<b>Related:</b>  How to Use is.na in R (With Examples) 
<h2><span class="orange">How to Use alpha with geom_point() in ggplot2</span></h2>
You can use the <b>alpha</b> argument within the <b>geom_point()</b> function in ggplot2 to modify the transparency of the points in a plot.
This argument uses the following basic syntax:
<b>ggplot(df, aes(x=x, y=y)) +
  geom_point(alpha=1)
</b>
The value for <b>alpha</b> can range between 0 and 1 where:
<b>0</b> is completely transparent
<b>1</b> is completely solid
The default value for <b>alpha</b> is <b>1</b>.
By setting <b>alpha</b> to a value less than 1 it becomes easier to view overlapping points in a plot, which is particularly useful when plotting the points of a large dataset.
The following examples shows how to use the <b>alpha</b> argument in practice with the following data frame that contains 5,000 rows:
<b>#make thise example reproducible
set.seed(1)
#create data frame with 5000 rows
df &lt;- data.frame(x=runif(n=5000, min=1, max=100))
df$y = df$x*3 + runif(5000)*df$x^2
#view head of data frame
head(df)
         x         y
1 27.28536  108.2851
2 37.84027  622.8478
3 57.71248 1002.0662
4 90.91257 7539.2476
5 20.96651  202.6813
6 89.94058 2867.4643
</b>
<b>Related:</b>  How to Use runif Function in R 
<h2>Example 1: Using alpha = 1 in geom_point()</h2>
The following code shows how to create a scatter plot in ggplot2 by using the default value of 1 for the <b>alpha</b> argument:
<b>library(ggplot2)
#create scatter plot with default alpha value
ggplot(df, aes(x=x, y=y)) +
  geom_point()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/alpha3.jpg"587">
Notice that the points in the plot are completely solid, which corresponds to an <b>alpha</b> value of 1.
<h2>Example 2: Using alpha = 0 in geom_point()</h2>
The following code shows how to create a scatter plot in ggplot2 by using a value of 0 for the <b>alpha</b> argument:
<b>library(ggplot2)
#create scatter plot with alpha value of 0
ggplot(df, aes(x=x, y=y)) +
  geom_point(alpha=0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/alpha4.jpg"586">
All of the points in the plot are completely transparent, which corresponds to an <b>alpha</b> value of 0.
<h2>Example 3: Using alpha = 0.1 in geom_point()</h2>
The following code shows how to create a scatter plot in ggplot2 by using a value of 0.1 for the <b>alpha</b> argument:
<b>library(ggplot2)
#create scatter plot with alpha value of 0.1
ggplot(df, aes(x=x, y=y)) +
  geom_point(alpha=0.1)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/alpha5.jpg"606">
By setting <b>alpha</b> to 0.1, the points have a level of transparency that allows us to see where the points overlap the most on the plot.
Feel free to play around with the value of <b>alpha</b> within the <b>geom_point()</b> function to achieve a level of transparency that makes your plot the easiest to read.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Remove Axis Labels in ggplot2 
 How to Remove a Legend in ggplot2 
 How to Remove Gridlines in ggplot2 
<h2><span class="orange">What is an Alternative Hypothesis in Statistics?</span></h2>
Often in statistics we want to test whether or not some assumption is true about a  population parameter . For example, we might assume that the mean weight of a certain population of turtle is 300 pounds.
To determine if this assumption is true, we’ll go out and collect a  sample  of turtles and weigh each of them. Using this sample data, we’ll conduct a  hypothesis test .
The first step in a hypothesis test is to define the <b>null</b> and <b>alternative hypotheses</b>. These two hypotheses need to be mutually exclusive, so if one is true then the other must be false.
These two hypotheses are defined as follows:
<b>Null hypothesis (H<sub>0</sub>):</b> The sample data is consistent with the prevailing belief about the population parameter.
<b>Alternative hypothesis (H<sub>A</sub>):</b> The sample data suggests that the assumption made in the null hypothesis is not true. In other words, there is some non-random cause influencing the data.
<h3>Types of Alternative Hypotheses</h3>
There are two types of alternative hypotheses:
A <b>one-tailed hypothesis</b> involves making a “greater than” or “less than ” statement. For example, suppose we assume the mean height of a male in the U.S. is greater than or equal to 70 inches.
The null and alternative hypotheses in this case would be:
<b>Null hypothesis:</b> μ ≥ 70 inches
<b>Alternative hypothesis:</b> μ &lt; 70 inches
A <b>two-tailed hypothesis</b> involves making an “equal to” or “not equal to” statement. For example, suppose we assume the mean height of a male in the U.S. is equal to 70 inches.
The null and alternative hypotheses in this case would be:
<b>Null hypothesis:</b> μ = 70 inches
<b>Alternative hypothesis:</b> μ ≠ 70 inches
<em>Note:</em> The “equal” sign is always included in the null hypothesis, whether it is =, ≥, or ≤.
<h3>Examples of Alternative Hypotheses</h3>
The following examples illustrate how to define the null and alternative hypotheses for different research problems.
<b>Example 1:</b> A biologist wants to test if the mean weight of a certain population of turtle is different from the widely-accepted mean weight of 300 pounds.
The null and alternative hypothesis for this research study would be:
<b>Null hypothesis:</b> μ = 300 pounds
<b>Alternative hypothesis:</b> μ ≠ 300 pounds
If we reject the null hypothesis, this means we have sufficient evidence from the sample data to say that the true mean weight of this population of turtles is different from 300 pounds.
<b>Example 2:</b> An engineer wants to test whether a new battery can produce higher mean watts than the current industry standard of 50 watts.
The null and alternative hypothesis for this research study would be:
<b>Null hypothesis:</b> μ ≤ 50 watts
<b>Alternative hypothesis:</b> μ > 50 watts
If we reject the null hypothesis, this means we have sufficient evidence from the sample data to say that the true mean watts produced by the new battery is greater than the current industry standard of 50 watts.
<b>Example 3:</b> A botanist wants to know if a new gardening method produces less waste than the standard gardening method that produces 20 pounds of waste.
The null and alternative hypothesis for this research study would be:
<b>Null hypothesis:</b> μ ≥ 20 pounds
<b>Alternative hypothesis:</b> μ &lt; 20 pounds
If we reject the null hypothesis, this means we have sufficient evidence from the sample data to say that the true mean weight produced by this new gardening method is less than 20 pounds.
<h3>When to Reject the Null Hypothesis</h3>
Whenever we conduct a hypothesis test, we use sample data to calculate a test-statistic and a corresponding p-value.
If the p-value is less than some significance level (common choices are 0.10, 0.05, and 0.01), then we reject the null hypothesis. This means we have sufficient evidence from the sample data to say that the assumption made by the null hypothesis is not true.
If the p-value is <em>not</em> less than some significance level, then we fail to reject the null hypothesis. This means our sample data did not provide us with evidence that the assumption made by the null hypothesis was not true.
<b>Additional Resource:</b>  An Explanation of P-Values and Statistical Significance 
<h2><span class="orange">How to Perform an ANCOVA in Excel</span></h2>
An  ANCOVA  (“analysis of covariance”) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups, after controlling for one or more  covariates .
This tutorial explains how to perform an ANCOVA in Excel.
<h3>Example: ANCOVA in Excel</h3>
A teacher wants to know if three different studying techniques have an impact on exam scores, but she wants to account for the current grade that the student already has in the class.
She will perform an ANCOVA using the following variables:
<b>Factor variable:</b> studying technique
<b>Covariate:</b> current grade
<b>Response variable:</b> exam score
The following table shows the dataset for the 15 students that were recruited to participate in the study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVA1.png">
Use the following steps to perform an ANCOVA on this dataset:
<b>Step 1: Input the data.</b>
First, input the data in the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel1.png">
<b>Step 2: Calculate the mean and variance for each column.</b>
Next, calculate the mean and variance for each column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel2.png">
<b>Step 3: Calculate the slopes of the regression lines.</b>
Next, we will calculate the slopes of the regression lines of the exam scores for each studying technique. 
<em><b>Note: </b>Cells B21:E28 display the formulas used to obtain the values in cells B13:E19.</em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel3.png">
<b>Step 4: Perform a one-way ANOVA on Exam Scores and Current Grade separately.</b>
Next, we will perform a one-way ANOVA on the exam scores:
<em><b>Reference: </b> How to Perform a One-Way ANOVA in Excel </em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel4.png">
Then, we will perform a one-way ANOVA on the current grades:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel5.png">
<b>Step 5: Fill in the ANCOVA table.</b>
Next, we will fill in the ANCOVA table.
<em><b>Note: </b>Cells H39:M43 show the formulas used to obtain the values in cells B39:F43.</em>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ancovaExcel6-1.png">
<b>Step 6: Interpret the results.</b>
From the ANCOVA table we see that the p-value for study technique is <b>0.032</b>. Since this value is less than 0.05, we can reject the null hypothesis that each of the studying techniques leads to the same average exam score, <em>even after accounting for the student’s current grade in the class</em>.
<h2><span class="orange">How to Conduct an ANCOVA in R</span></h2>
This tutorial provides an example of how to perform an  ANCOVA  in R.
<h3>Example: ANCOVA in R</h3>
We will conduct an ANCOVA to test whether or not studying technique has an impact on exam scores by using the following variables:
<b>Studying technique</b>: The independent variable we are interested in analyzing
<b>Student’s current grade</b>: The covariate that we want to take into account
<b>Exam score</b>: The response variables we are interested in analyzing
The following dataset contains information for 90 students that were randomly split into three groups of 30.
The dataset shows the studying technique each student used <em>(A, B,</em> or <em>C)</em><em>,</em> their current grade in the class when they started using the studying technique, and their exam score they received after using the studying technique for one month to prepare for the exam:
<b>#make this example reproducible 
set.seed(10)
#create dataset
data &lt;- data.frame(technique = rep(c("A", "B", "C"), each = 30),   current_grade = runif(90, 65, 95),   exam = c(runif(30, 80, 95), runif(30, 70, 95), runif(30, 70, 90)))
#view first six lines of dataset
head(data)
#  technique current_grade     exam
#1         A      80.22435 87.32759
#2         A      74.20306 90.67114
#3         A      77.80723 88.87902
#4         A      85.79306 87.75735
#5         A      67.55408 85.72442
#6         A      71.76310 92.52167
</b>
<h3>Step 1: Explore the Data</h3>
Before we fit the ANCOVA model, we should first explore the data to gain a better understanding of it and verify that there aren’t any extreme outliers that could skew the results.
First, we can view a summary of each variable in the dataset:
<b>summary(data)
# technique current_grade        exam      
# A:30      Min.   :65.43   Min.   :71.17  
# B:30      1st Qu.:71.79   1st Qu.:77.27  
# C:30      Median :77.84   Median :84.69  
#           Mean   :78.15   Mean   :83.38  
#           3rd Qu.:83.65   3rd Qu.:89.22  
#           Max.   :93.84   Max.   :94.76  
</b>
We can see that each value for studying technique (<em>A, B, </em>and <em>C) </em>shows up 30 times in the data.
We can also see how the current student scores were distributed at the beginning of the study. The minimum score in the class was 65.43, the max was 93.84, and the mean was 78.15.
Likewise, we can see that the minimum score received on the exam was 71.17, the max was 94.76, and the mean was 83.38.
Next, we can use the <b>dplyr </b>package to easily find the mean and the standard deviation of both the current grades and the exam scores for each studying technique:
<b>#load <em>dplyr</em>
library(dplyr)
data %>%
  group_by(technique) %>%
  summarise(mean_grade = mean(current_grade),
            sd_grade = sd(current_grade),
            mean_exam = mean(exam),
            sd_exam = sd(exam))
# A tibble: 3 x 5
#  technique mean_grade sd_grade mean_exam sd_exam                      
#1 A               79.0     7.00      88.5    3.88
#2 B               78.5     8.33      81.8    7.62
#3 C               76.9     8.24      79.9    5.71</b>
We can see that the mean and the standard deviations of the current grade for the students using each studying technique is roughly similar. 
We can also see that the mean exam score is noticeably higher for the students who used studying technique <em>A</em> compared to techniques <em>B </em>and <em>C</em>.
We can also visualize the distribution of exam scores based on studying technique by using  boxplots :
<b>boxplot(exam ~ technique,
data = data,
main = "Exam Score by Studying Technique",
xlab = "Studying Technique",
ylab = "Exam Score",
col = "steelblue",
border = "black"
)</b>
<h3><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/ancova_1.jpg"></h3>
Similarly, we can also use boxplots to visualize the distribution of <em>current grades</em> based on studying technique:
<b>boxplot(current_grade ~ technique,
data = data,
main = "Current Grade by Studying Technique",
xlab = "Studying Technique",
ylab = "Current Grade",
col = "steelblue",
border = "black"
)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/ancova_2.jpg">
<h3>Step 2: Check the Model Assumptions</h3>
Once we’ve done some basic data exploration and are familiar with the data, we need to verify that the following assumptions for ANCOVA are met:
<b>The covariate and the treatment are independent</b> – we need to verify that the covariate (<em>current grade)</em> and the treatment <em>(studying technique)</em> are independent of each other, since adding a covariate term into the model only makes sense if the covariate and the treatment act independently on the response variable (<em>exam</em>). 
<b>Homogeneity of variance</b> – we need to verify that the variances among the groups is equal
To verify that the covariate and the treatment are independent, we can run an ANOVA using <em>current grade </em>as the response variable and <em>studying technique </em>as the predictor variable:
<b>#fit anova model
anova_model &lt;- aov(current_grade ~ technique, data = data)
#view summary of anova model
summary(anova_model)
#            Df Sum Sq Mean Sq F value Pr(>F)
#technique    2     74   37.21   0.599  0.552
#Residuals   87   5406   62.14    
</b>
The p-value is greater than 0.05, so the covariate (<em>current grade) </em>and the treatment (<em>studying technique</em>) seem to be independent.
Next, to verify that there is homogeneity of variance among the groups, we can conduct Levene’s Test:
<b>#load <em>car </em>library to conduct Levene's Test
libary(car)
#conduct Levene's Test
leveneTest(exam~technique, data = data)
#Levene's Test for Homogeneity of Variance (center = median)
#      Df F value    Pr(>F)    
#group  2  9.4324 0.0001961 ***
#      87   
</b>
The p-value from the test is equal to .0001961, which indicates that the variances among the groups are not equal. Although we could attempt a transformation on the data to correct this problem, we won’t worry too much about the differences in variance for the time being.
<h3>Step 3: Fit the ANCOVA Model</h3>
Next, we’ll fit the ANCOVA model using <em>exam score</em> as the response variable, <em>studying technique </em>as the predictor (or “treatment”) variable, and <em>current grade </em>as the covariate.
We’ll use the Anova() function in the car package to do so, just so we can specify that we’d like to use type III sum of squares for the model, since type I sum of squares is dependent upon the order that the predictors are entered into the model:
<b>#load <em>car </em>library
library(car)
#fit ANCOVA model
ancova_model &lt;- aov(exam ~ technique + current_grade, data = data)
#view summary of model
Anova(ancova_model, type="III") 
#Response: exam
#              Sum Sq Df  F value    Pr(>F)    
#(Intercept)   7161.2  1 201.4621 &lt; 2.2e-16 ***
#technique     1242.9  2  17.4830 4.255e-07 ***
#current_grade   12.3  1   0.3467    0.5576    
#Residuals     3057.0 86         
</b>
We can see that the p-value for <em>technique </em>is extremely small, which indicates that studying technique has a statistically significant effect on exam scores, even after controlling for the current grade.
<h3>Step 4: Post Hoc Tests</h3>
Although the ANCOVA results told us that <em>studying technique </em>had a statistically significant effect on exam scores, we need to run  post hoc tests  to actually find out which studying techniques differ from each other.
To do so, we can use the glht() function within the <b>multcomp </b>package in R to perform Tukey’s Test for multiple comparisons:
<b>#load the <em>multcomp </em>library
library(multcomp)
#fit the ANCOVA model
ancova_model &lt;- aov(exam ~ technique + current_grade, data = data)
#define the post hoc comparisons to make
postHocs &lt;- glht(ancova_model, linfct = mcp(technique = "Tukey"))
#view a summary of the post hoc comparisons
summary(postHocs)
#Multiple Comparisons of Means: Tukey Contrasts
#
#Fit: aov(formula = exam ~ technique + current_grade, data = data)
#
#Linear Hypotheses:
#           Estimate Std. Error t value Pr(>|t|)    
#B - A == 0   -6.711      1.540  -4.358 0.000109 ***
#C - A == 0   -8.736      1.549  -5.640  &lt; 1e-04 ***
#C - B == 0   -2.025      1.545  -1.311 0.393089    
#view the confidence intervals associated with the multiple comparisons
confint(postHocs)
# Simultaneous Confidence Intervals
#
#Multiple Comparisons of Means: Tukey Contrasts
#
#Fit: aov(formula = exam ~ technique + current_grade, data = data)
#
#Quantile = 2.3845
#95% family-wise confidence level
#
#Linear Hypotheses:
#           Estimate lwr      upr     
#B - A == 0  -6.7112 -10.3832  -3.0392
#C - A == 0  -8.7364 -12.4302  -5.0426
#C - B == 0  -2.0252  -5.7091   1.6588
</b>
From the output, we can see that there is a statistically significant difference (at α = .05) in exam scores between studying technique <em>A</em> and studying technique<em> B</em> (p-value: .000109) as well as between technique <em>A</em> and technique <em>C</em> (p-value: &lt;1e-04).
We can also see that there is <em>not </em>a statistically significant difference (at α = .05) between techniques <em>B</em> and <em>C</em>. The confidence intervals between the techniques confirm these conclusions as well.
Thus, we can conclude that using studying technique <em>A </em>leads to a statistically significantly greater exam score for students compared to techniques <em>B </em>and <em>C</em>, even after controlling for the student’s current grade in the class.
<h2><span class="orange">How to Perform an ANCOVA in Python</span></h2>
An  ANCOVA  (“analysis of covariance”) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups, after controlling for one or more  covariates .
This tutorial explains how to perform an ANCOVA in Python.
<h3>Example: ANCOVA in Python</h3>
A teacher wants to know if three different studying techniques have an impact on exam scores, but she wants to account for the current grade that the student already has in the class.
She will perform an ANCOVA using the following variables:
<b>Factor variable:</b> studying technique
<b>Covariate:</b> current grade
<b>Response variable:</b> exam score
Use the following steps to perform an ANCOVA on this dataset:
<b>Step 1: Enter the data.</b>
First, we’ll create a pandas DataFrame to hold our data:
<b>import numpy as np
import pandas as pd
#create data
df = pd.DataFrame({'technique': np.repeat(['A', 'B', 'C'], 5),   'current_grade': [67, 88, 75, 77, 85,                     92, 69, 77, 74, 88,                      96, 91, 88, 82, 80],   'exam_score': [77, 89, 72, 74, 69,                  78, 88, 93, 94, 90,                  85, 81, 83, 88, 79]})
#view data 
df
   techniquecurrent_gradeexam_score
0   A           67        77
1   A           88        89
2   A           75        72
3   A           77        74
4   A           85        69
5   B           92        78
6   B           69        88
7   B           77        93
8   B           74        94
9   B           88        90
10   C           96        85
11   C           91        81
12   C           88        83
13   C           82        88
14   C           80        79
</b>
<b>Step 2: Perform the ANCOVA.</b>
Next, we’ll perform an ANCOVA using the  ancova() function  from the pingouin library:
<b>pip install pingouin 
from pingouin import ancova
#perform ANCOVA
ancova(data=df, dv='exam_score', covar='current_grade', between='technique')
        Source        SS        DFF   p-uncnp2
0technique390.57513024.80997    0.031550.46653
1current_grade4.19388610.10329   0.753930.00930
2Residual446.60611411NaN   NaN        NaN
</b>
<b>Step 3: Interpret the results.</b>
From the ANCOVA table we see that the p-value (p-unc = “uncorrected p-value”) for study technique is <b>0.03155</b>. Since this value is less than 0.05, we can reject the null hypothesis that each of the studying techniques leads to the same average exam score, <em>even after accounting for the student’s current grade in the class</em>.
<h2><span class="orange">An Introduction to ANCOVA (Analysis of Variance)</span></h2>
<b>ANCOVA </b>stands for “analysis of covariance.” To understand how an ANCOVA works, it helps to first understand the ANOVA.
An  ANOVA  (analysis of variance) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. 
For example, suppose we want to know whether or not studying technique has an impact on exam scores for a class of students. We randomly split the class into three groups. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam.
To find out if studying technique impacts exam scores, we can conduct a one-way ANOVA, which will tell us if if there is a statistically significant difference between the mean scores of the three groups.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA1.jpg">
An <b>ANCOVA</b> is an extension of an ANOVA in which we’d like to determine if there is a statistically significant difference between three or more independent groups <em>after accounting for one or more  covariates .</em>
A <b>covariate </b>is a continuous variable that co-varies with the response variable.
For example, suppose we want to know whether or not studying technique has an impact on exam scores, <em>but we want to account for the grade that the student already has in the class</em>. We can use their current grade as a covariate and conduct an ANCOVA to determine if there is a statistically significant difference between the mean exam scores of the three groups.
This allows us to test whether or not studying technique has an impact on exam scores after the influence of the covariate has been removed.
Thus, if we find that there is a statistically significant difference in exam scores between the three studying techniques, we can be sure that this difference exists <em>even after accounting for the students current grade in the class (i.e. if they’re already doing well or not in the class)</em>.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA3.jpg">
<h3>Assumptions of ANCOVA</h3>
Before performing an ANCOVA, it’s important to make sure the following assumptions are met:
<b>The covariate(s) and the factor variable(s) are independent</b> – The covariate and the factor variable should be independent of each other, since adding a covariate term into the model only makes sense if the covariate and the factor variable act independently on the response variable.
<b>The covariate(s) are continuous data. </b>The covariates should be continuous (i.e. either  interval or ratio data ).
<b>Homogeneity of variances</b> – The variances among the groups should be roughly equal.
<b>Independence </b>– The observations in each group should be independent.
<b>Normality </b>– The data should be roughly normally distributed in each group.
<b>No extreme outliers </b>– There should be no extreme outliers in any of the groups that could significantly affect the results of the ANCOVA.
<h3>ANCOVA: Example</h3>
A teacher wants to know if three different studying techniques have an impact on exam scores, but she wants to account for the current grade that the student already has in the class.
She will perform an ANCOVA using the following variables:
<b>Factor variable:</b> studying technique
<b>Covariate:</b> current grade
<b>Response variable:</b> exam score
The following table shows the dataset for the 15 students that were recruited to participate in the study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVA1.png">
After performing an ANCOVA on the dataset, the teacher ends up with the following results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVA2.png">
The p-value for study technique is <b>0.03155</b>. Since this value is less than 0.05, we can reject the null hypothesis that each of the studying techniques leads to the same average exam score, <em>even after accounting for the student’s current grade in the class</em>.
To determine exactly which studying techniques produce different average exam scores, the teacher would need to run  post-hoc tests .
<h2><span class="orange">How to Use “AND” Operator in Pandas (With Examples)</span></h2>
You can use the <b>&</b> symbol as an “AND” operator in pandas.
For example, you can use the following basic syntax to filter for rows in a pandas DataFrame that satisfy condition 1 <b>and </b>condition 2:
<b>df[(condition1) & (condition2)]
</b>
The following examples show how to use this “AND” operator in different scenarios.
<h3>Example 1: Use “AND” Operator to Filter Rows Based on Numeric Values in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team  points  assists  rebounds
0    A      25        5        11
1    A      12        7         8
2    B      15        7        10
3    B      14        9         6
4    B      19       12         6
5    B      23        9         5
6    C      25        9         9
7    C      29        4        12</b>
We can use the following syntax to filter for rows in the DataFrame where the value in the points column is greater than 20 <b>and </b>the value in the assists column is equal to 9:
<b>#filter rows where points > 20 and assists = 9
df[(df.points > 20) & (df.assists == 9)]
        teampointsassistsrebounds
5B2395
6C2599</b>
The only rows returned are the ones where the points value is greater than 20 <b>and </b>the assists value is equal to 9.
<h3>Example 2: Use “AND” Operator to Filter Rows Based on String Values in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],   'position': ['G', 'G', 'F', 'F', 'C', 'F', 'C', 'C'],   'conference': ['W', 'W', 'W', 'W', 'E', 'E', 'E', 'E'],   'points': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
print(df)
  team position conference  points
0    A        G          W      11
1    B        G          W       8
2    C        F          W      10
3    D        F          W       6
4    E        C          E       6
5    F        F          E       5
6    G        C          E       9
7    H        C          E      12</b>
We can use the following syntax to filter for rows in the DataFrame where the value in the position column is equal to G <b>and</b> the value in the conference column is equal to W:
<b>#filter rows based on string values
df[(df.position == 'G') & (df.conference == 'W')]
        teamposition  conference points
0AG  W     11
1BG  W     8</b>
The only rows returned are the ones where the position column is equal to G <b>and</b> the conference column is equal to W.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in pandas:
 How to Use “OR” Operator in Pandas 
 How to Filter Pandas DataFrame Rows by Date 
 How to Filter a Pandas DataFrame by Column Values 
<h2><span class="orange">How to Perform an Anderson-Darling Test in Python</span></h2>
An <b>Anderson-Darling Test</b> is a goodness of fit test that measures how well your data fit a specified distribution.
This test is most commonly used to determine whether or not your data follow a  normal distribution .
This type of test is useful for testing for normality, which is a common assumption used in many statistical tests including  regression ,  ANOVA ,  t-tests , and many others.
<h3>Example: Anderson-Darling Test in Python</h3>
To conduct an Anderson-Darling Test in Python, we can use the  anderson() function  from the scipy.stats library, which uses the following syntax:
<b>anderson(x, dist=’norm’)</b>
where:
<b>x</b>: array of sample data
<b>dist</b>: the type of distribution to test against. Default is ‘norm’ but you can also specify ‘expon’ or ‘logistic.’
For example, here’s how to perform an Anderson-Darling Test on a sample of 50 normally distributed random variables:
<b>import numpy as np
#create data
np.random.seed(0)
data = np.random.normal(size=50)
#perform Anderson-Darling Test 
from scipy.stats import anderson
anderson(data)
AndersonResult(statistic=0.15006999533388665,
               critical_values=array([0.538, 0.613, 0.736, 0.858, 1.021]),
               significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))
</b>
The test statistic is <b>0.150</b>. We can compare this value to each critical value that corresponds to each significance level to see if the test results are significant. For example:
The critical value for α = 0.01 is <b>1.021</b>. Because the test statistic (0.150) is not greater than this critical value, the results are not significant at a significance level of 0.01.
The critical value for α = 0.025 is <b>0.858</b>. Because the test statistic (0.150) is not greater than this critical value, the results are not significant at a significance level of 0.025.
And so on.
We can see that the test results are not significant at any significance level, which means we would not reject the null hypothesis of the test. Thus, we don’t have sufficient evidence to say that the sample data is not normally distributed.
This result shouldn’t be surprising considering we used the <b>np.rand.normal()</b> function to generate a sample of 50 normally distributed values.
Consider instead if we performed the Anderson-Darling Test on a sample of 50 random integers between 0 and 10:
<b>import numpy as np
#create data
np.random.seed(0)
data = np.random.randint(0, 10, size=50)
#perform Anderson-Darling Test 
from scipy.stats import anderson
anderson(data)
AndersonResult(statistic=1.1926463985076836,
               critical_values=array([0.538, 0.613, 0.736, 0.858, 1.021]),
               significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]))
</b>
The test statistic is <b>1.1926</b>. We can compare this value to each critical value that corresponds to each significance level to see if the test results are significant. For example:
The critical value for α = 0.01 is <b>1.021</b>. Because the test statistic (1.1926) is greater than this critical value, the results are significant at a significance level of 0.01.
The critical value for α = 0.025 is <b>0.858</b>. Because the test statistic (1.1926) is greater than this critical value, the results are significant at a significance level of 0.025.
And so on.
We can see that the test results are significant at every significance level, which means we would reject the null hypothesis of the test no matter which significance level we choose to use. Thus, we have sufficient evidence to say that the sample data is not normally distributed.
This result also shouldn’t be surprising considering we used the <b>np.rand.randint()</b> function to generate a sample of 50 random integers between 0 and 10, which is unlikely to follow a normal distribution.
<em>You can find more Python tutorials  here </em>.
<h2><span class="orange">How to Conduct an Anderson-Darling Test in R</span></h2>
An <b>Anderson-Darling Test</b> is a goodness of fit test that measures how well your data fit a specified distribution. This test is most commonly used to determine whether or not your data follow a  normal distribution .
This type of test is useful for testing for normality, which is a common assumption used in many statistical tests including regression, ANOVA, t-tests, and many others.
<h3>Example: Anderson-Darling Test in R</h3>
To conduct an Anderson-Darling Test in R, we can use the <b>ad.test()</b> function within the <b>nortest </b>library.
The following code illustrates how to conduct an A-D test to test whether or not a vector of 100 values follows a normal distribution:
<b>#install (if not already installed) and load <em>nortest </em>library
install.packages('nortest')
library(nortest)
#make this example reproducible
set.seed(1)
#defined vector of 100 values that are normally distributed
x &lt;- rnorm(100, 0, 1)
#conduct Anderson-Darling Test to test for normality
ad.test(x)
#Anderson-Darling normality test
#
#data:  x
#A = 0.16021, p-value = 0.9471
</b>
This test returns two values:
<b>A</b>: the test statistic. 
<b>p-value</b>: the corresponding p-value of the test statistic.
The null hypothesis for the A-D test is that the data <em>does </em>follow a normal distribution. Thus, if our p-value for the test is below our significance level (common choices are 0.10, 0.05, and 0.01), then we can reject the null hypothesis and conclude that we have sufficient evidence to say our data does not follow a normal distribution.
In this case, our p-value is 0.9471. Since this is not below our significance level (let’s say .05), we do not have sufficient evidence to reject the null hypothesis. It’s safe to say that our data follows a normal distribution, which makes sense considering we generated 100 values that follow a normal distribution with a mean of 0 and standard deviation of 1 using the <b>rnorm()</b> function in R.
<b>Related:</b>  A Guide to dnorm, pnorm, qnorm, and rnorm in R 
Suppose instead we generate a vector of 100 values that follow a uniform distribution between 0 and 1. We can conduct an A-D test once again to see if this data follows a normal distribution:
<b>#make this example reproducible
set.seed(1)
#defined vector of 100 values that are uniformly distributed
x &lt;- runif(100, 0, 1)
#conduct Anderson-Darling Test to test for normality
ad.test(x)
#Anderson-Darling normality test
#
#data:  x
#A = 1.1472, p-value = 0.005086
</b>
Our test statistic <em>A </em>equals 1.1472 and the corresponding p-value equals 0.005086. Since our p-value is less than 0.05, we can reject the null hypothesis and conclude that we have sufficient evidence to say this data does not follow a normal distribution. This matches the result we expected since we know that our data actually follows a uniform distribution.
<h2>Conducting an Anderson-Darling Test on One Column of a Data Frame in R</h2>
We can also conduct an AD-test for a specified column of a data frame in R. For example, consider the built-in <b>iris </b>dataset:
<b>#view first six lines of <em>iris </em>dataset
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
</b>
Suppose we want to know whether or not the variable <em>Petal.Width </em>is normally distributed. We could first create a histogram to visualize the distribution of values:
<b>hist(iris$Petal.Width, col = 'steelblue', main = 'Distribution of Petal Widths',
     xlab = 'Petal Width')
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/AD_test.jpg">
The data does not appear to be normally distributed. To confirm this, we can conduct an A-D test to formally test whether or not the data is normally distributed:
<b>#conduct Anderson-Darling Test to test for normality
ad.test(iris$Petal.Width)
#Anderson-Darling normality test
#
#data:  iris$Petal.Width
#A = 5.1057, p-value = 1.125e-12
</b>
The p-value of the test is less than 0.05, thus we have sufficient evidence to reject the null hypothesis and conclude that <em>Petal.Width</em> does not follow a normal distribution.
<h2><span class="orange">How to Check ANOVA Assumptions</span></h2>
A  one-way ANOVA  is a statistical test used to determine whether or not there is a significant difference between the means of three or more independent groups. 
Here’s an example of when we might use a one-way ANOVA:
You randomly split up a class of 90 students into three groups of 30. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam.
 
You want to know whether or not the studying technique has an impact on exam scores so you conduct a <b>one-way ANOVA</b> to determine if there is a statistically significant difference between the mean scores of the three groups.
Before we can conduct a one-way ANOVA, we must first check to make sure that three assumptions are met.
<b>1. Normality </b>– Each sample was drawn from a normally distributed population.
<b>2. Equal Variances </b>– The variances of the populations that the samples come from are equal.
<b>3. Independence </b>– The observations in each group are independent of each other and the observations within groups were obtained by a random sample.
If these assumptions aren’t met, then the results of our one-way ANOVA could be unreliable.
In this post, we explain how to check these assumptions along with what to do if any of the assumptions are violated.
<h2>Assumption #1: Normality</h2>
ANOVA assumes that each sample was drawn from a normally distributed population.
<h3>How to check this assumption in R:</h3>
To check this assumption, we can use two approaches:
Check the assumption visually using histograms or  Q-Q plots .
Check the assumption using formal statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson.
For example, suppose we recruit 90 people to participate in a weight-loss experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month. To see if the program has an impact on weight loss, we want to conduct a one-way ANOVA. The following code illustrates how to check the normality assumption using histograms, Q-Q plots, and a Shapiro-Wilk test.
<b>1. Fit ANOVA Model.</b>
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = rep(c("A", "B", "C"), each = 30),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#fit the one-way ANOVA model
model &lt;- aov(weight_loss ~ program, data = data)
</b>
<b>2. Create histogram of response values.</b>
<b>#create histogram
hist(data$weight_loss)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/histogram.jpg"541">
The distribution doesn’t look very normally distributed (e.g. it doesn’t have a “bell” shape), but we can also create a Q-Q plot to get another look at the distribution.
<b>3. Create Q-Q plot of residuals</b>
<b>#create Q-Q plot to compare this dataset to a theoretical normal distribution 
qqnorm(model$residuals)
#add straight diagonal line to plot
qqline(model$residuals)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/qqplot.jpg">
In general, if the data points fall along a straight diagonal line in a Q-Q plot, then the dataset likely follows a normal distribution. In this case, we can see that there is some noticeable departure from the line along the tail ends which might indicate that the data is not normally distributed.
<b>4. Conduct Shapiro-Wilk Test for Normality.</b>
<b>#Conduct Shapiro-Wilk Test for normality 
shapiro.test(data$weight_loss)
#Shapiro-Wilk normality test
#
#data:  data$weight_loss
#W = 0.9587, p-value = 0.005999
</b>
The Shapiro-Wilk Test tests the null hypothesis that the samples come from a normal distribution vs. the alternative hypothesis that the samples do not come from a normal distribution. In this case, the p-value of the test is <b>0.005999</b>, which is less than the alpha level of 0.05. This suggests that the samples do not come a normal distribution.
<h3>What to do if this assumption is violated:</h3>
In general, a one-way ANOVA is considered to be fairly robust against violations of the normality assumption as long as the sample sizes are sufficiently large. 
Also, if you have extremely large sample sizes then statistical tests like the Shapiro-Wilk test will almost always tell you that your data is non-normal. For this reason, it’s often best to inspect your data visually using graphs like histograms and Q-Q plots. By simply looking at the graphs, you can get a pretty good idea of whether or not the data is normally distributed.
If the normality assumption is <em>severely </em>violated or if you just want to be extra conservative, you have two choices:
<b>(1)</b> Transform the response values of your data so that the distributions are more normally distributed.
<b>(2) </b>Perform an equivalent non-parametric test such as a  Kruskal-Wallis Test  that doesn’t require the assumption of normality.
<h2>Assumption #2: Equal Variance</h2>
ANOVA assumes that the variances of the populations that the samples come from are equal.
<h3>How to check this assumption in R:</h3>
We can check this assumption in R using two approaches:
Check the assumption visually using boxplots.
Check the assumption using a formal statistical tests like Bartlett’s Test.
The following code illustrates how to do so, using the same fake weight-loss dataset we created earlier.
<b>1. Create boxplots.</b>
<b>#Create box plots that show distribution of weight loss for each group
boxplot(weight_loss ~ program, xlab='Program', ylab='Weight Loss', data=data)
</b>
<h3><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/boxplot.jpg"557"></h3>
The variance of weight loss in each group can be seen by the length of each box plot. The longer the box, the higher the variance. For example, we can see that the variance is a bit higher for participants in program C compared to both program A and program B.
<b>2. Conduct Bartlett’s Test.</b>
<b>#Create box plots that show distribution of weight loss for each group
bartlett.test(weight_loss ~ program, data=data)
#Bartlett test of homogeneity of variances
#
#data:  weight_loss by program
#Bartlett's K-squared = 8.2713, df = 2, p-value = 0.01599</b>
 Bartlett’s Test  tests the null hypothesis that the samples have equal variances vs. the alternative hypothesis that the samples do not have equal variances. In this case, the p-value of the test is <b>0.01599</b>, which is less than the alpha level of 0.05. This suggests that the samples do not all have equal variances.
<h3>What to do if this assumption is violated:</h3>
In general, a one-way ANOVA is considered to be fairly robust against violations of the equal variances assumption as long as each group has the same sample size.
However, if the sample sizes are not the same and this assumption is severely violated, you could instead run a  Kruskal-Wallis Test , which is the non-parametric version of the one-way ANOVA.
<h2>Assumption #3: Independence</h2>
ANOVA assumes:
The observations in each group are independent of the observations in every other group.
The observations within each group were obtained by a random sample.
<h3>How to check this assumption:</h3>
There is no formal test you can use to verify that the observations in each group are independent and that they were obtained by a random sample. The only way this assumption can be satisfied is if a randomized design was used.
<h3>What to do if this assumption is violated:</h3>
Unfortunately, there is very little you can do if this assumption is violated. Simply put, if the data was collected in a way where the observations in each group are not independent of observations in other groups, or if the observations within each group were not obtained through a randomized process, the results of the ANOVA will be unreliable.
If this assumption is violated, the best thing to do is to set up the experiment again in a way that uses a randomized design.
<b>Further Reading:</b>
 How to Conduct a One-Way ANOVA in R 
 How to Conduct a One-Way ANOVA in Excel 
<h2><span class="orange">How to Interpret the F-Value and P-Value in ANOVA</span></h2>
An <b>ANOVA</b> (“analysis of variance”) is used to determine whether or not the means of three or more independent groups are equal.
An ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> All group means are equal.
<b>H<sub>A</sub>:</b> At least one group mean is different from the rest.
Whenever you perform an ANOVA, you will end up with a summary table that looks like the following:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F</b></th>
<th style="text-align: center;"><b>P-value</b></th>
</tr>
<tr>
<td><b>Treatment</b></td>
<td style="text-align: center;">192.2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">2.358</td>
<td style="text-align: center;">0.1138</td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">1100.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">40.8</td>
<td> </td>
<td> </td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">1292.8</td>
<td style="text-align: center;">29</td>
<td> </td>
<td> </td>
<td> </td>
</tr>
</tbody></table>
Two values that we immediately analyze in the table are the <b>F-statistic</b> and the corresponding <b>p-value</b>.
<h3>Understanding the F-Statistic in ANOVA</h3>
The <b>F-statistic</b> is the ratio of the mean squares treatment to the mean squares error:
F-statistic: Mean Squares Treatment / Mean Squares Error
Another way to write this is:
F-statistic: Variation between sample means / Variation within samples
The larger the F-statistic, the greater the variation between sample means relative to the variation within the samples.
Thus, the larger the F-statistic, the greater the evidence that there is a difference between the group means.
<h3>Understanding the P-Value in ANOVA</h3>
To determine if the difference between group means is statistically significant, we can look at the <b>p-value</b> that corresponds to the F-statistic.
To find the  p-value  that corresponds to this F-value, we can use an  F Distribution Calculator  with numerator degrees of freedom = df Treatment and denominator degrees of freedom = df Error.
For example, the p-value that corresponds to an F-value of 2.358, numerator df = 2, and denominator df = 27 is <b>0.1138</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/anova_f1.png">
If this p-value is less than α = .05, we reject the null hypothesis of the ANOVA and conclude that there is a statistically significant difference between the means of the three groups.
Otherwise, if the p-value is not less than α = .05 then we fail to reject the null hypothesis and conclude that we do not have sufficient evidence to say that there is a statistically significant difference between the means of the three groups.
In this particular example, the p-value is 0.1138 so we would fail to reject the null hypothesis. This means we don’t have sufficient evidence to say that there is a statistically significant difference between the group means.
<h3>On Using Post-Hoc Tests with an ANOVA</h3>
If the p-value of an ANOVA is less than .05, then we reject the null hypothesis that each group mean is equal.
In this scenario, we can then perform  post-hoc tests  to determine exactly which groups differ from each other.
There are several potential post-hoc tests we can use following an ANOVA, but the most popular ones include:
Tukey Test
Bonferroni Test
Scheffe Test
Refer to  this guide  to understand which post-hoc test you should use depending on your particular situation.
<h2><span class="orange">How to Calculate the Grand Mean in ANOVA (With Example)</span></h2>
In statistics, a  one-way ANOVA  is used to compare the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means.
One metric we always calculate when using an ANOVA is the <b>grand mean</b>, which represents the mean value for all  observations  in the dataset.
It is calculated as:
<b>Grand Mean = Σx<sub>i</sub> / n</b>
where:
<b>x<sub>i</sub></b>: The i<sup>th</sup> observation in the dataset
<b>n</b>: The total number of observations in the dataset
The grand mean is important because it’s used in the formula to calculate the total sum of squares, which is an important value that ends up in the final ANOVA table.
The following example shows how to calculate the grand mean for an ANOVA in practice.
<h3>Example: Calculating the Grand Mean in ANOVA</h3>
Suppose we want to know whether or not three different exam prep programs lead to different mean scores on a certain exam. To test this, we recruit 30 students to participate in a study and split them into three groups.
The students in each group are randomly assigned to use one of the three exam prep programs for a month to prepare for an exam. At the end of the month, all of the students take the same exam. 
The exam scores for each group are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png"(max-width: 237px) 100vw, 237px">
To calculate the <b>grand mean</b> for this dataset, we simply add up all of the observations and then divide by the total number of observations:
Grand Mean: (85 + 86 + 88 + 75 + 78 + 94 + 98 + 79 + 71 + 80 + 91 + 92 + 93 + 85 + 87 + 84 + 82 + 88 + 95 + 96 + 79 + 78 + 88 + 94 + 92 + 85 + 83 + 85 + 82 + 81) / 30 = <b>85.8</b>.
The grand mean is 85.8. This represents the average exam score for all 30 students.
Note that this value won’t necessarily match the individual group means.
For example, if we calculate the mean for each group of students, we’ll find that none of the group means actually match the grand mean (or “overall” mean):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/ANOVAhand1-1.png">
This grand mean is then used in the formula to calculate the <b>total sum of squares</b>, which is calculated as the sum of the squared deviations between each individual observation and the grand mean:
Total Sum of Squares: (85 – 85.8)<sup>2</sup> + (86 – 85.8)<sup>2</sup> + (88 – 85.8)<sup>2</sup> + . . . + (82 – 85.8)<sup>2</sup> + (81 – 85.8)<sup>2</sup> = <b>1292.8</b>.
This value is then eventually used in the final ANOVA table:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F</b></th>
</tr>
<tr>
<td><b>Treatment</b></td>
<td style="text-align: center;">192.2</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">96.1</td>
<td style="text-align: center;">2.358</td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">1100.6</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">40.8</td>
<td> </td>
</tr>
<tr>
<td><b>Total</b></td>
<td style="text-align: center;">1292.8</td>
<td style="text-align: center;">29</td>
<td> </td>
<td> </td>
</tr>
</tbody></table>
<b>Related:</b>  How to Interpret the F-Value and P-Value in ANOVA 
The good news is that you will rarely have to calculate the grand mean for an ANOVA by hand since most statistical software can do it for you.
However, it’s good to know how the grand mean is calculated and how it’s actually used in the ANOVA table.
<h2><span class="orange">A Guide to Using Post Hoc Tests with ANOVA</span></h2>
An <b>ANOVA</b> is a statistical test that is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. 
The  hypotheses  used in an ANOVA are as follows:
The null hypothesis (H<sub>0</sub>): μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3 </sub>= … = μ<sub>k  </sub>(the means are equal for each group)
The alternative hypothesis: (Ha): at least one of the means is different from the others
If the  p-value  from the ANOVA is less than the significance level, we can reject the null hypothesis and conclude that we have sufficient evidence to say that at least one of the means of the groups is different from the others.
However, this doesn’t tell us <em>which </em>groups are different from each other. It simply tells us that not all of the group means are equal.
In order to find out exactly which groups are different from each other, we must conduct a <b>post hoc test </b>(also known as a multiple comparison test), which will allow us to explore the difference between multiple group means while also controlling for the family-wise error rate.
<b>Technical Note:</b> It’s important to note that we only need to conduct a post hoc test when the p-value for the ANOVA is statistically significant. If the p-value is not statistically significant, this indicates that the means for all of the groups are not different from each other, so there is no need to conduct a post hoc test to find out which groups are different from each other.
<h2>The Family-Wise Error Rate</h2>
As mentioned before, post hoc tests allow us to test for difference between multiple group means while also controlling for the <b>family-wise error rate</b>. 
In a  hypothesis test , there is always a type I error rate, which is defined by our significance level (alpha) and tells us the probability of rejecting a null hypothesis that is actually true. In other words, it’s the probability of getting a “false positive”, i.e. when we claim there is a statistically significant difference among groups, but there actually isn’t. 
When we perform one hypothesis test, the type I error rate is equal to the significance level, which is commonly chosen to be 0.01, 0.05, or 0.10. However, when we conduct multiple hypothesis tests at once, the probability of getting a false positive increases.
For example, imagine that we roll a 20-sided dice. The probability that the dice lands on a “1” is just 5%. But if we roll two dice at once, the probability that one of the dice will land on a “1” increases to 9.75%. If we roll five dice at once, the probability increases to 22.6%. 
The more dice we roll, the higher the probability that one of the dice will land on a “1.” Similarly, if we conduct several hypothesis tests at once using a significance level of .05, the probability that we get a false positive increases to beyond just 0.05.
<h2>Multiple Comparisons in ANOVA</h2>
When we conduct an ANOVA, there are often three or more groups that we are comparing to one another. Thus, when we conduct a post hoc test to explore the difference between the group means, there are several <b>pairwise </b>comparisons we want to explore.
For example, suppose we have four groups: A, B, C, and D. This means there are a total of six pairwise comparisons we want to look at with a post hoc test:
A – B (the difference between the group A mean and the group B mean)
A – C
A – D
B – C
B – D
C – D
If we have more than four groups, the number of pairwise comparisons we will want to look at will only increase even more. The following table illustrates how many pairwise comparisons are associated with each number of groups along with the family-wise error rate:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/post_hoc1.jpg">
Notice that the family-wise error rate increases rapidly as the number of groups (and consequently the number of pairwise comparisons) increases. In fact, once we reach six groups, the probability of us getting a false positive is actually above 50%!
This means we would have serious doubts about our results if we were to make this many pairwise comparisons, knowing that our family-wise error rate was so high.
Fortunately, post hoc tests provide us with a way to make multiple comparisons between groups while controlling the family-wise error rate.
<h2>Example: One-Way ANOVA with Post Hoc Tests</h2>
The following example illustrates how to perform a  one-way ANOVA  with post hoc tests.
<em><b>Note:</b> This example uses the programming language R, but you don’t need to know R to understand the results of the test or the big takeaways.</em>
First, we’ll create a dataset that contains four groups (A, B, C, D) with 20 observations per group:
<b>#make this example reproducible
set.seed(1)
#load <em>tidyr </em>library to convert data from wide to long format
library(tidyr)
#create wide dataset
data &lt;- data.frame(A = runif(20, 2, 5),   B = runif(20, 3, 5),   C = runif(20, 3, 6),   D = runif(20, 4, 6))
#convert to long dataset for ANOVA
data_long &lt;- gather(data, key = "group", value = "amount", A, B, C, D)
#view first six lines of dataset
head(data_long)
#  group   amount
#1     A 2.796526
#2     A 3.116372
#3     A 3.718560
#4     A 4.724623
#5     A 2.605046
#6     A 4.695169
</b>
Next, we’ll fit a one-way ANOVA to the dataset:
<b>#fit anova model
anova_model &lt;- aov(amount ~ group, data = data_long)
#view summary of anova model
summary(anova_model)
#            Df Sum Sq Mean Sq F value   Pr(>F)    
#group        3  25.37   8.458   17.66 8.53e-09 ***
#Residuals   76  36.39   0.479            
</b>
From the ANOVA table output, we see that the F-statistic is 17.66 and the corresponding p-value is extremely small.
This means we have sufficient evidence to reject the null hypothesis that all of the group means are equal. Next, we can use a post hoc test to find which group means are different from each other.
We will walk through examples of the following post hoc tests:
<b>Tukey’s Test</b> – useful when you want to make every possible pairwise comparison
<b>Holm’s Method</b> – a slightly more conservative test compared to Tukey’s Test
 <b>Dunnett’s Correction</b>  – useful when you want to compare every group mean to a control mean, and you’re not interested in comparing the treatment means with one another.
<h2>Tukey’s Test</h2>
We can perform Tukey’s Test for multiple comparisons by using the built-in R function <b>TukeyHSD()</b> as follows:
<b>#perform Tukey's Test for multiple comparisons
TukeyHSD(anova_model, conf.level=.95) 
#  Tukey multiple comparisons of means
#    95% family-wise confidence level
#
#Fit: aov(formula = amount ~ group, data = data_long)
#
#$group
#         diff          lwr       upr     p adj
#B-A 0.2822630 -0.292540425 0.8570664 0.5721402
#C-A 0.8561388  0.281335427 1.4309423 0.0011117
#D-A 1.4676027  0.892799258 2.0424061 0.0000000
#C-B 0.5738759 -0.000927561 1.1486793 0.0505270
#D-B 1.1853397  0.610536271 1.7601431 0.0000041
#D-C 0.6114638  0.036660419 1.1862672 0.0326371
</b>
Notice that we specified our confidence level to be 95%, which means we want our family-wise error rate to be .05. R gives us two metrics to compare each pairwise difference:
Confidence interval for the mean difference (given by the values of <em>lwr </em>and <em>upr</em>)
Adjusted p-value for the mean difference
Both the confidence interval and the p-value will lead to the same conclusion.
For example, the 95% confidence interval for the mean difference between group C and group A is (0.2813, 1.4309), and since this interval doesn’t contain zero we know that the difference between these two group means is statistically significant. In particular, we know that the difference is positive, since the lower bound of the confidence interval is greater than zero.
Likewise, the p-value for the mean difference between group C and group A is 0.0011, which is less than our significance level of 0.05, so this also indicates that the difference between these two group means is statistically significant.
We can also visualize the 95% confidence intervals that result from the Tukey Test by using the <b>plot()</b> function in R:
<b>plot(TukeyHSD(anova_model, conf.level=.95))
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/post_hoc2.jpg">
If the interval contains zero, then we know that the difference in group means is not statistically significant. In the example above, the differences for B-A and C-B are not statistically significant, but the differences for the other four pairwise comparisons are statistically significant. 
<h2>Holm’s Method</h2>
Another post hoc test we can perform is holm’s method. This is generally viewed as a more conservative test compared to Tukey’s Test.
We can use the following code in R to perform holm’s method for multiple pairwise comparisons:
<b>#perform holm's method for multiple comparisons
pairwise.t.test(data_long$amount, data_long$group, p.adjust="holm") 
#Pairwise comparisons using t tests with pooled SD 
#
#data:  data_long$amount and data_long$group 
#
#  A       B       C      
#B 0.20099 -       -      
#C 0.00079 0.02108 -      
#D 1.9e-08 3.4e-06 0.01974
#
#P value adjustment method: holm </b>
This test provides a grid of p-values for each pairwise comparison. For example, the p-value for the difference between the group A and group B mean is 0.20099. 
If you compare the p-values of this test with the p-values from Tukey’s Test, you’ll notice that each of the pairwise comparisons lead to the same conclusion, except for the difference between group C and D. The p-value for this difference was .0505 in Tukey’s Test compared to .02108 in Holm’s Method.
Thus, using Tukey’s Test we concluded that the difference between group C and group D was not statistically significant at the .05 significance level, but using Holm’s Method we concluded that the difference between group C and group D <em>was </em>statistically significant. 
In general, the p-values produced by Holm’s Method tend to be lower than those produced by Tukey’s Test.
<h2>Dunnett’s Correction</h2>
Yet another method we can use for multiple comparisons is Dunett’s Correction. We would use this approach when we want to compare every group mean to a control mean, and we’re not interested in comparing the treatment means with one another.
For example, using the code below we compare the group means of B, C, and D all to that of group A. So, we use group A as our control group and we aren’t interested in the differences between groups B, C, and D. 
<b>#load multcomp library necessary for using Dunnett's Correction
library(multcomp)
#convert group variable to factor 
data_long$group &lt;- as.factor(data_long$group)
#fit anova model
anova_model &lt;- aov(amount ~ group, data = data_long)
#perform comparisons
dunnet_comparison &lt;- glht(anova_model, linfct = mcp(group = "Dunnett"))
#view summary of comparisons
summary(dunnet_comparison)
#Multiple Comparisons of Means: Dunnett Contrasts
#
#Fit: aov(formula = amount ~ group, data = data_long)
#
#Linear Hypotheses:
#           Estimate Std. Error t value Pr(>|t|)    
#B - A == 0   0.2823     0.2188   1.290 0.432445    
#C - A == 0   0.8561     0.2188   3.912 0.000545 ***
#D - A == 0   1.4676     0.2188   6.707  &lt; 1e-04 ***</b>
From the p-values in the output we can see the following:
The difference between the group B and group A mean<em> is not</em> statistically significant at a significance level of .05. The p-value for this test is <b>0.4324</b>.
The difference between the group C and group A mean<em> is </em>statistically significant at a significance level of .05. The p-value for this test is <b>0.0005</b>.
The difference between the group D and group A mean<em> is </em>statistically significant at a significance level of .05. The p-value for this test is<b> 0.00004</b>.
As we stated earlier, this approach treats group A as the “control” group and simply compares every other group mean to that of group A. Notice that there are no tests performed for the differences between groups B, C, and D because we aren’t interested in the differences between those groups.
<h2>A Note on Post Hoc Tests & Statistical Power</h2>
Post hoc tests do a great job of controlling the family-wise error rate, but the tradeoff is that they reduce the statistical power of the comparisons. This is because the only way to lower the family-wise error rate is to use a lower significance level for all of the individual comparisons. 
For example, when we use Tukey’s Test for six pairwise comparisons and we want to maintain a family-wise error rate of .05, we must use a significance level of approximately 0.011 for each individual significance level. The more pairwise comparisons we have, the lower the significance level we must use for each individual significance level.
The problem with this is that lower significance levels correspond to lower statistical power. This means that if a difference between group means actually does exist in the population, a study with lower power is less likely to detect it. 
One way to reduce the effects of this tradeoff is to simply reduce the number of pairwise comparisons we make. For example, in the previous examples we performed six pairwise comparisons for the four different groups. However, depending on the needs of your study, you may only be interested in making a few comparisons.
By making fewer comparisons, you don’t have to lower the statistical power as much.
It’s important to note that you should determine <em>before </em>you perform the ANOVA exactly which groups you want to make comparisons between and which post hoc test you will use to make these comparisons. Otherwise, if you simply see which post hoc test produces statistically significant results, that reduces the integrity of the study.
<h2>Conclusion</h2>
In this post, we learned the following things:
An ANOVA is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
If an ANOVA produces a p-value that is less than our significance level, we can use post hoc tests to find out which group means differ from one another.
Post hoc tests allow us to control the family-wise error rate while performing multiple pairwise comparisons.
The tradeoff of controlling the family-wise error rate is lower statistical power. We can reduce the effects of lower statistical power by making fewer pairwise comparisons.
You should determine beforehand which groups you’d like to make pairwise comparisons on and which post hoc test you will use to do so.
<h2><span class="orange">4 Examples of Using ANOVA in Real Life</span></h2>
Often when students learn about a certain topic in school, they’re inclined to ask:
<em><b>“When is this ever used in real life?”</b></em>
This is often the case in statistics, when certain techniques and methods seem so obscure that it’s hard to imagine them actually being applied in real-life situations.
However, the <b>ANOVA</b> (short for “analysis of variance”) is a technique that is actually used all the time in a variety of fields in real life.
In this post, we’ll share a quick refresher on what an ANOVA is along with four examples of how it is used in real life situations.
<h2>What is an ANOVA?</h2>
An <b>ANOVA</b> (“Analysis of Variance”) is a statistical technique that is used to determine whether or not there is a significant difference between the means of three or more independent groups. The two most common types of ANOVAs are the one-way ANOVA and two-way ANOVA.
A <b>One-Way ANOVA </b>is used to determine how one factor impacts a response variable. For example, we might want to know if three different studying techniques lead to different mean exam scores. To see if there is a statistically significant difference in mean exam scores, we can conduct a one-way ANOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA1.jpg">
A <b>Two-Way ANOVA </b>is used to determine how two factors impact a response variable, and to determine whether or not there is an interaction between the two factors on the response variable. For example, we might want to know how gender and how different levels of exercise impact average weight loss. We would conduct a two-way ANOVA to find out.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA2.jpg">
It’s also possible to conduct a three-way ANOVA, four-way ANOVA, etc. but these are much more uncommon and it can be difficult to interpret ANOVA results if too many factors are used.
Now we will share four different examples of when ANOVA’s are actually used in real life.
<h3>ANOVA Real Life Example #1</h3>
A large scale farm is interested in understanding which of three different fertilizers leads to the highest crop yield. They sprinkle each fertilizer on ten different fields and measure the total yield at the end of the growing season.
To understand whether there is a statistically significant difference in the mean yield that results from these three fertilizers, researchers can conduct a one-way ANOVA, using “type of fertilizer” as the factor and “crop yield” as the response.
If the overall p-value of the ANOVA is lower than our significance level (typically chosen to be 0.10, 0.05, 0.01) then we can conclude that there is a statistically significant difference in mean crop yield between the three fertilizers. We can then conduct  post hoc tests  to determine exactly which fertilizer lead to the highest mean yield.
<h3>ANOVA Real Life Example #2</h3>
Medical researchers want to know if four different medications lead to different mean blood pressure reductions in patients. They randomly assign 20 patients to use each medication for one month, then measure the blood pressure both before and after the patient started using the medication to find the mean blood pressure reduction for each medication.
To understand whether there is a statistically significant difference in the mean blood pressure reduction that results from these medications, researchers can conduct a one-way ANOVA, using “type of medication” as the factor and “blood pressure reduction” as the response.
If the overall p-value of the ANOVA is lower than our significance level, then we can conclude that there is a statistically significant difference in mean blood pressure reduction between the four medications. We can then conduct  post hoc tests  to determine exactly which medications lead to significantly different results.
<h3>ANOVA Real Life Example #3</h3>
A grocery chain wants to know if three different types of advertisements affect mean sales differently. They use each type of advertisement at 10 different stores for one month and measure total sales for each store at the end of the month.
To see if there is a statistically significant difference in mean sales between these three types of advertisements, researchers can conduct a one-way ANOVA, using “type of advertisement” as the factor and “sales” as the response variable.
If the overall p-value of the ANOVA is lower than our significance level, then we can conclude that there is a statistically significant difference in mean sales between the three types of advertisements. We can then conduct  post hoc tests  to determine exactly which types of advertisements lead to significantly different results.
<h3>ANOVA Real Life Example #4</h3>
Biologists want to know how different levels of sunlight exposure (no sunlight, low sunlight, medium sunlight, high sunlight) and watering frequency (daily, weekly) impact the growth of a certain plant. In this case, two factors are involved (level of sunlight exposure and water frequency), so they will conduct a two-way ANOVA to see if either factor significantly impacts plant growth and whether or not the two factors are related to each other.
The results of the ANOVA will tell us whether each individual factor has a significant effect on plant growth. Using this information, the biologists can better understand which level of sunlight exposure and/or watering frequency leads to optimal growth.
<h2>Conclusion</h2>
ANOVA is used in a wide variety of real-life situations, but the most common include:
<b>Retail: </b>Store are often interested in understanding whether different types of promotions, store layouts, advertisement tactics, etc. lead to different sales. This is the exact type of analysis that ANOVA is built for.
<b>Medical: </b>Researchers are often interested in whether or not different medications affect patients differently, which is why they often use one-way or two-way ANOVA’s in these situations.
<b>Environmental Sciences: </b>Researchers are often interested in understanding how different levels of factors affect plants and wildlife. Because of the nature of these types of analyses, ANOVA’s are often used.
So, next time someone asks you when an ANOVA is actually used in real life, feel free to reference these examples!
<h2><span class="orange">How to Analyze Residuals in an ANOVA Model</span></h2>
An <b>ANOVA</b> (“analysis of variance”) is a type of model that is used to determine whether or not there is a significant difference between the means of three or more independent groups.
Whenever we fit an ANOVA model to a dataset, there will always be residuals – these represent the difference between each individual observation and the mean of the group that the observation came from.
The following example shows how to calculate residuals for an ANOVA model in practice.
<h3>Example: Calculating Residuals in ANOVA</h3>
Suppose we recruit 90 people to participate in a weight-loss experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month.
We can conduct a one-way ANOVA to determine if there is a statistically significant difference between the resulting weight loss from the three programs.
Suppose we calculate the mean weight loss for individuals in each program to be:
<b>Program A</b>: 1.58 pounds
<b>Program B</b>: 2.56 pounds
<b>Program C</b>: 4.13 pounds
The residuals for the ANOVA model would be the difference between each individual’s weight loss and the mean weight loss in their program.
For example, the following table shows how to calculate the residuals for 10 different individuals in the study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/anova_residuals1-1.png">
Notice the following pattern:
Individuals who had a value greater than their group mean had a <b>positive residual</b>.
Individuals who had a value less than their group mean had a <b>negative residual</b>.
In practice, we would calculate the residuals for all 90 individuals.
<h3>How to Use Residuals to Check Normality</h3>
One of the  assumptions of an ANOVA  is that the residuals are normally distributed.
The most common way to check this assumption is by creating a  Q-Q plot .
If the residuals are normally distributed, then the points in a Q-Q plot will lie on a straight diagonal line.
Here’s what a Q-Q plot would look like for our previous example:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/qqnorm1.png">
The points deviate a bit from the straight diagonal line on the tail ends, but in general the points fall follow the diagonal line quite well. This tells us that the assumption of normality is likely met.
As reference, here’s what a Q-Q plot for non-normally distributed residuals might look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/qqnorm2.png">
The points deviate wildly from the straight diagonal line, which indicates that the residuals are not normally distributed.
Refer to the following tutorials to learn how to create Q-Q plots in different software:
 How to Create a Q-Q Plot in Excel 
 How to Create a Q-Q Plot in R 
 How to Create a Q-Q Plot in Python 
<h2><span class="orange">How to Perform a One-Way ANOVA on a TI-84 Calculator</span></h2>
A <b>one-way ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
This tutorial explains how to conduct a one-way ANOVA on a TI-84 calculator.
<h3>Example: One-Way ANOVA on a TI-84 Calculator</h3>
Suppose we recruit 30 students to participate in a study. The students are randomly assigned to use one of three studying techniques for one month to prepare for an exam. At the end of the month, all of the students take the same test. 
Use the following steps to perform a one-way ANOVA to determine if the average scores are the same across all three groups.
<b>Step 1: Input the data.</b>
First, we will input the data values for both the explanatory and the response variable. Press Stat and then press EDIT. Enter the following exam scores for the students who used the first study technique in column L1, the second studying technique in column L2, and the third studying technique in column L3:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/anovaTI1.png">
<b>Step 2: Perform the one-way ANOVA.</b>
Next, we will perform the one-way ANOVA. Press Stat and then scroll over to <b>TESTS</b>. Then scroll down to <b>ANOVA</b> and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/anovaTI2.png">
Enter the lists where the data is stored separated by commas, then add a closing parenthesis <b>)</b> and then press Enter.
<b>Note: </b>To make L1 appear, press 2nd and then press 1. To make L2 appear, press 2nd and then press 2. To make L3 appear, press 2nd and then press 3. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/anovaTI3.png">
The following results will appear once you press Enter:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/anovaTI4.png">
<b>Step 3: Interpret the results.</b>
The F-statistic for the test is <b>2.3575 </b>and the corresponding p-vaule is <b>0.1138</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis.
Thus, we do not have sufficient evidence to say that the mean exam score is different between the three groups. That is, we don’t have sufficient evidence to say that the studying technique leads to different exam scores.
<h2><span class="orange">How to Perform an ANOVA with Unequal Sample Sizes</span></h2>
One question students often have in statistics is:
<b><em>Is it possible to perform a one-way ANOVA when the sample sizes of each group are not equal?</em></b>
The short answer:
<b>Yes, you can perform a one-way ANOVA when the sample sizes are not equal. Equal sample sizes is not  one of the assumptions  made in an ANOVA.</b>
However, there are two potential issues to be aware of when performing a one-way ANOVA with unequal sample sizes:
<b>(1)</b> Reduced statistical power.
<b>(2)</b> Reduced robustness to unequal variance.
The following sections explain both of these potential issues in detail.
<h3>Issue #1: Reduced Statistical Power</h3>
When we use any type of statistical test to compare groups, the statistical power of the test is highest when each group has an equal sample size.
Recall that statistical power refers to the probability that a test will detect some effect when there actually is one.
It can be shown that the greater the differences in sample sizes between the groups, the lower the statistical power of an ANOVA.
This is why researchers typically want equal sample sizes so that they have higher power and thus a greater probability of detecting true differences.
It’s certainly possible to perform a one-way ANOVA with unequal sample sizes, but you should be aware that the power of the one-way ANOVA will be reduced.
<h3>Issue #2: Reduced Robustness to Unequal Variance</h3>
One of the assumptions of a one-way ANOVA is that the variance between each group is equal.
In general, a one-way ANOVA is considered to be robust against violations of the equal variances assumption, <em>but only if each group has the same sample size</em>.
Thus, if you have unequal sample sizes and unequal variances between the groups, then the results of the one-way ANOVA can be hard to trust.
<h3>How to Decide to Use a One-Way ANOVA with Unequal Sample Sizes</h3>
If you have unequal sample sizes and you’d like to perform a one-way ANOVA to test for differences between group means, you can use the following flow chart to decide how to proceed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/unequal1.png">
Here’s a brief explanation of the flow chart:
<b>Step 1: Determine if each group has the same variance.</b>
To determine if each group has the same variance, you can use one of two approaches:
Create boxplots for each group and see if the spread of values in each group is roughly equal.
Perform a formal statistical test for equal variances like  Bartlett’s Test .
If the variances are not equal, perform a  Kruskal-Wallis Test , which is considered the non-parametric equivalent to a one-way ANOVA.
If the variances are equal, proceed to the next step.
<b>Step 2: Determine if each group is normally distributed.</b>
To determine if the values in each group are roughly normally distributed, you can use one of two approaches:
Create histograms or  Q-Q plots  for each group.
Perform formal statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson.
If each group is normally distributed, you can proceed to perform a one-way ANOVA and interpret the results as you would with any ordinary one-way ANOVA.
If each group is not normally distributed, perform a Kruskal-Wallis Test instead.
<h2><span class="orange">ANOVA vs. Regression: What’s the Difference?</span></h2>
Two commonly used models in statistics are ANOVA and regression models.
These two types of models share the following <b>similarity:</b>
The  response variable  in each model is continuous. Examples of continuous variables include weight, height, length, width, time, age, etc.
However, these two types of models share the following <b>difference</b>:
ANOVA models are used when the predictor variables are categorical. Examples of categorical variables include level of education, eye color, marital status, etc.
Regression models are used when the predictor variables are continuous.*
*Regression models can be used with categorical predictor variables, but we have to create  dummy variables  in order to use them.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/anova_reg2.png">
The following examples show when to use ANOVA vs. regression models in practice.
<h3>Example 1: ANOVA Model Preferred</h3>
Suppose a biologist wants to understand whether or not four different fertilizers lead to the same average plant growth (in inches) during a one-month period. To test this, she applies each fertilizer to 20 plants and records the growth of each plant after one month.
In this scenario, the biologist should use a one-way ANOVA model to analyze the differences between the fertilizers because there is one predictor variable and it is categorical.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/anova_reg1.png">
In other words, the values for the predictor variable can be classified into the following “categories”:
Fertilizer 1
Fertilizer 2
Fertilizer 3
Fertilizer 4
A one-way ANOVA will tell the biologist whether or not the mean plant growth is equal between the four different fertilizers.
<h3>Example 2: Regression Model Preferred</h3>
Suppose a real estate agent wants to understand the relationship between square footage and house price. To analyze this relationship, he collects data on square footage and house price for 200 houses in a particular city.
In this scenario, the real estate agent should use a simple linear regression model to analyze the relationship between these two variables because the predictor variable (square footage) is continuous.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/anova_reg3.png">
Using simple linear regression, the real estate agent can fit the following regression model:
House price = β<sub>0</sub> + β<sub>1</sub>(square footage)
The value for β<sub>1</sub> will represent the average change in house price associated with each additional square foot.
This will allow the real estate agent to quantify the relationship between square footage and house price.
<h3>Example 3: Regression Model with Dummy Variables Preferred</h3>
Suppose a real estate agent wants to understand the relationship between the predictor variables “square footage” and “home type” (single-family, apartment, townhome) with the response variable of house price.
In this scenario, the real estate agent can use multiple linear regression by converting “home type” into a dummy variable since it’s currently a categorical variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/anova_reg4.png">
The real estate agent can then fit the following multiple linear regression model:
House price = β<sub>0</sub> + β<sub>1</sub>(square footage) + β<sub>2</sub>(single-family) + β<sub>3</sub>(apartment)
Here’s how we would interpret the coefficients in the model:
<b>β<sub>1</sub>:</b> The average change in house price associated with one extra square foot.
<b>β<sub>2</sub>:</b> The average difference in price between a single-family home and a townhome, assuming square footage is held constant.
<b>β<sub>3</sub>:</b> The average difference in price between a single-family home and an apartment, assuming square footage is held constant.
Check out the following tutorials to see how to create dummy variables in different statistical software:
 How to Create Dummy Variables in R 
 How to Create Dummy Variables in Excel 
 Introduction to the One-Way ANOVA 
 Introduction to the Two-Way ANOVA 
The following tutorials offer an in-depth introduction to linear regression models:
 Introduction to Simple Linear Regression 
 Introduction to Multiple Linear Regression 
<h2><span class="orange">What is an Antecedent Variable? (Explanation & Example)</span></h2>
In statistics, researchers are often interested in understanding the relationship between some independent variable and a dependent variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/antecedant1-1.png">
However, sometimes an <b>antecedent variable </b>can be present.
An <b>antecedent variable</b> is a variable that occurs <em>before </em>the independent and dependent variables under study and can help explain the relationship between the two.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/antecedant2.png">
You can remember this definition by remembering that the word <em>antecedent</em> literally means “previous or preexisting.”
<h3>Examples of Antecedent Variables</h3>
Antecedent variables can be present in a variety of research scenarios. Some examples include:
<b>Example 1: Age & Income</b>
Suppose researchers are interested in studying the relationship between age and annual income. However, an antecedent variable that could help explain (or partially explain) the relationship between the two variables that should be considered is <em>education level</em>, since this tends to have a correlation with both age and income.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/antecedant3.png">
<b>Example 2: Meditation & Happiness</b>
Suppose researchers are interested in studying the relationship between meditation and reported happiness levels. However, an antecedent variable that could help explain (or partially explain) the relationship between the two variables that should be considered is <em>work stress</em>, since this can have an effect on both free time available to meditate and reported happiness.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/antecedant4.png">
<h3>How to Control for Antecedent Variables</h3>
In an experiment, researchers could potentially control for antecedent variables by using them as  blocking factors . For example, they could divide participants into “blocks” based on their education level, then study the relationship between age and income with each block.
In regression analysis, researchers could include antecedent variables in a regression model to control for their effects. For example, researchers could include education level as a variable in the regression model so that the  regression coefficient  for age could be interpreted as the average change in income while holding education level constant.
In both of these scenarios, it’s assumed that data is readily available for these antecedent variables which isn’t always the case. For example, it could be hard to quantify “work stress” even though we know it might be an antecedent variable that could affect ability to meditate and reported happiness.
<h3>Related Variables</h3>
Two variables that are similar to antecedent variables and that can also affect the relationship between an independent variable and dependent variable include:
<b>1. </b> Extraneous variables : Variables that are not of interest in a study, but can affect both the independent and dependent variables.
<b>2. </b> Intervening variables : Variables that come between independent and dependent variables and have a direct effect on the relationship between the two.
Be wary of each of these types of variables when conducting an experiment or a study.
<h2><span class="orange">How to Find the Antilog of Values in Excel</span></h2>
The <b>antilog</b> of a number is the inverse of the log of a number.
So, if you calculate the log of a number you can then use the antilog to get back the original number.
For example, suppose we start with the number 7. If we take the log (base 10) of 7 then we would get .845:
log<sub>10</sub>(7) = <b>.845</b>
The antilog (base 10) of the value 0.845 can be found by taking 10 raised to the power of 0.845:
10<sup>.845</sup> = <b>7</b>
The antilog allowed us to get back the original number.
The following table shows how to calculate the antilog of values in Excel according to their base:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Base</b></th>
<th style="text-align: center;"><b>Number</b></th>
<th style="text-align: center;"><b>Log</b></th>
<th style="text-align: center;"><b>Antilog</b></th>
</tr>
<tr>
<td style="text-align: center;">x</td>
<td style="text-align: center;">y</td>
<td style="text-align: center;">=LOG(y, x)</td>
<td style="text-align: center;">=x^y</td>
</tr>
<tr>
<td style="text-align: center;">e</td>
<td style="text-align: center;">y</td>
<td style="text-align: center;">=LN(y)</td>
<td style="text-align: center;">=exp(y)</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">y</td>
<td style="text-align: center;">=LOG10(y)</td>
<td style="text-align: center;">=10^y</td>
</tr>
</tbody></table>
The following examples show how to calculate the antilog of values in Excel using different values for the base.
<h3>Example 1: Calculating the Antilog of Base 10</h3>
The following screenshot shows how to calculate the log base 10 of a list of values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog1.png">
To obtain the antilog of the values in column B, we simply need to use the formula <b>=10^value</b> in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog2.png">
By taking the antilog of the values in column B, we were able to obtain all of the original values.
<h3>Example 2: Calculating the Antilog of a Natural Log</h3>
The following screenshot shows how to calculate the natural log of a list of values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog3.png">
To obtain the antilog of the values in column B, we can use the formula <b>=exp(value)</b> in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog4.png">
By taking the antilog of the values in column B, we were able to obtain all of the original values.
<h3>Example 3: Calculating the Antilog of Base x</h3>
The following screenshot shows how to calculate the log base 7 of a list of values in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog5.png">
To obtain the antilog of the values in column B, we can use the formula <b>=7^value</b> in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/antilog6.png">
By taking the antilog of the values in column B, we were able to obtain all of the original values.
<h2><span class="orange">How to Find the Antilog of Values in Python</span></h2>
The <b>antilog</b> of a number is the inverse of the log of a number.
So, if you calculate the log of a number you can then use the antilog to get back the original number.
For example, suppose we start with the number 7. If we take the log (base 10) of 7 then we would get .845:
log<sub>10</sub>(7) = <b>.845</b>
The antilog (base 10) of the value 0.845 can be found by taking 10 raised to the power of 0.845:
10<sup>.845</sup> = <b>7</b>
The antilog allowed us to get back the original number.
The following table shows how to calculate the antilog of values in Python according to their base:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Base</b></th>
<th style="text-align: center;"><b>Number</b></th>
<th style="text-align: center;"><b>Log</b></th>
<th style="text-align: center;"><b>Antilog</b></th>
</tr>
<tr>
<td style="text-align: center;">e</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">np.log(x)</td>
<td style="text-align: center;">np.exp(x)</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">np.log10(x)</td>
<td style="text-align: center;">10 ** x</td>
</tr>
</tbody></table>
The following examples show how to calculate the antilog of values in Python using different values for the base.
<h3>Example 1: Calculating the Antilog of Base 10</h3>
Suppose we take the log (base 10) of the value 7:
<b>import numpy as np
#define original value
original = 7
#take log (base 10) of original value
log_original = np.log10(original)
#display log (base 10) of original value
log_original
0.845098
</b>
In order to get back the original value of 7, we can take the antilog by raising 10 to the power of 0.845098:
<b>#take the antilog
10 ** log_original
7.0
</b>
By taking the antilog, we were able to obtain the original value of 7.
<h3>Example 2: Calculating the Antilog of a Natural Log</h3>
Suppose we take the natural log of the value 7:
<b>#define original value
original = 7
#take natural log of original value
log_original = np.log(original)
#display natural log of original value
log_original
[1] 1.94591
</b>
In order to get back the original value of 7, we can take the antilog by raising <em>e</em> to the power of 1.94591:
<b>#take the antilog
np.exp(log_original)
7.0
</b>
By taking the antilog, we were able to obtain the original value of 7.
<h2><span class="orange">How to Find the Antilog of Values in R</span></h2>
The <b>antilog</b> of a number is the inverse of the log of a number.
So, if you calculate the log of a number you can then use the antilog to get back the original number.
For example, suppose we start with the number 7. If we take the log (base 10) of 7 then we would get .845:
log<sub>10</sub>(7) = <b>.845</b>
The antilog (base 10) of the value 0.845 can be found by taking 10 raised to the power of 0.845:
10<sup>.845</sup> = <b>7</b>
The antilog allowed us to get back the original number.
The following table shows how to calculate the antilog of values in R according to their base:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Base</b></th>
<th style="text-align: center;"><b>Number</b></th>
<th style="text-align: center;"><b>Log</b></th>
<th style="text-align: center;"><b>Antilog</b></th>
</tr>
<tr>
<td style="text-align: center;">n</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">log(x, n)</td>
<td style="text-align: center;">x^n</td>
</tr>
<tr>
<td style="text-align: center;">e</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">log(x)</td>
<td style="text-align: center;">exp(x)</td>
</tr>
<tr>
<td style="text-align: center;">10</td>
<td style="text-align: center;">x</td>
<td style="text-align: center;">log10(x)</td>
<td style="text-align: center;">10^x</td>
</tr>
</tbody></table>
The following examples show how to calculate the antilog of values in R using different values for the base.
<h3>Example 1: Calculating the Antilog of Base 10</h3>
Suppose we take the log (base 10) of the value 7:
<b>#define original value
original = 7
#take log (base 10) of original value
log_original = log10(original)
#display log (base 10) of original value
log_original
[1] 0.845098
</b>
In order to get back the original value of 7, we can take the antilog by raising 10 to the power of 0.845098:
<b>#take the antilog
10^log_original
[1] 7
</b>
By taking the antilog, we were able to obtain the original value of 7.
<h3>Example 2: Calculating the Antilog of a Natural Log</h3>
Suppose we take the natural log of the value 7:
<b>#define original value
original = 7
#take natural log of original value
log_original = log(original)
#display natural log of original value
log_original
[1] 1.94591
</b>
In order to get back the original value of 7, we can take the antilog by raising <em>e</em> to the power of 1.94591:
<b>#take the antilog
exp(log_original)
[1] 7
</b>
By taking the antilog, we were able to obtain the original value of 7.
<h3>Example 3: Calculating the Antilog of Base x</h3>
Suppose we take the log (base 5) of the value 7:
<b>#define original value
original = 7
#take log (base 5) of original value
log_original = log(original, 5)
#display log (base 10) of original value
log_original
[1] 1.209062
</b>
In order to get back the original value of 7, we can take the antilog by raising 5 to the power of 1.209062:
<b>#take the antilog
5^log_original
[1] 7
</b>
By taking the antilog, we were able to obtain the original value of 7.
<h2><span class="orange">When to Use aov() vs. anova() in R</span></h2>
The <b>aov()</b> and<b> anova()</b> functions in R seem similar, but we actually use them in two different scenarios.
We use <b>aov()</b> when we would like to fit an ANOVA model and view the results in an ANOVA summary table.
We use <b>anova()</b> when we would like to compare the fit of nested regression models to determine if a regression model with a certain set of coefficients offers a significantly better fit than a model with only a subset of the coefficients.
The following examples show how to use each function in practice.
<h2>Example 1: How to Use aov() in R</h2>
Suppose we would like to perform a  one-way ANOVA  to determine if three different exercise programs impact weight loss differently.
We recruit 90 people to participate in an experiment in which we randomly assign 30 people to follow either program A, program B, or program C for one month.
The following code shows how to use the <b>aov()</b> function in R to perform this one-way ANOVA:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(program = rep(c("A", "B", "C"), each=30), weight_loss = c(runif(30, 0, 3),                 runif(30, 0, 5),                 runif(30, 1, 7)))
#fit one-way anova using aov()
fit &lt;- aov(weight_loss ~ program, data=df)
#view results
summary(fit)
            Df Sum Sq Mean Sq F value   Pr(>F)    
program      2  98.93   49.46   30.83 7.55e-11 ***
Residuals   87 139.57    1.60                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
From the model output we can see that the p-value for program (.0000000000755) is less than .05, which means there is a statistically significant difference in mean weight loss between the three programs.
<h2>Example 2: How to Use anova() in R</h2>
Suppose we would like to use <b>number of hours studied</b> to predict <b>exam score</b> for students at a certain college. We may decide to fit the following two regression models:
<b>Full Model:</b> Score = β<sub>0</sub> + B<sub>1</sub>(hours) + B<sub>2</sub>(hours)<sup>2</sup>
<b>Reduced Model:</b> Score = β<sub>0</sub> + B<sub>1</sub>(hours)
The following code shows how to use the<b> anova()</b> function in R to perform a lack of fit test to determine if the full model offers a significantly better fit than the reduced model:
<b>#make this example reproducible
set.seed(1)
#create dataset
df &lt;- data.frame(hours = runif(50, 5, 15), score=50)
df$score = df$score + df$hours^3/150 + df$hours*runif(50, 1, 2)
#view head of data
head(df)
      hours    score
1  7.655087 64.30191
2  8.721239 70.65430
3 10.728534 73.66114
4 14.082078 86.14630
5  7.016819 59.81595
6 13.983897 83.60510
#fit full model
full &lt;- lm(score ~ poly(hours,2), data=df)
#fit reduced model
reduced &lt;- lm(score ~ hours, data=df)
#perform lack of fit test using anova()
anova(full, reduced)
Analysis of Variance Table
Model 1: score ~ poly(hours, 2)
Model 2: score ~ hours
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     47 368.48                                
2     48 451.22 -1   -82.744 10.554 0.002144 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
Since the p-value in the output table (.002144) is less than .05, we can reject the null hypothesis of the test and conclude that the full model offers a statistically significantly better fit than the reduced model.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform a One-Way ANOVA in R 
 How to Perform a Two-Way ANOVA in R 
 How to Perform a Repeated Measures ANOVA in R 
<h2><span class="orange">How to Append a List to a Pandas DataFrame (With Example)</span></h2>
You can use the following basic syntax to append a list to a pandas DataFrame:
<b>#define list
new_list = ['value1', 'value2', value3, value4]
#append list to DataFrame
df.loc[len(df)] = new_list</b>
The following example shows how to use this syntax in practice.
<h3>Example: Append List to Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame that contains information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'],   'points': [18, 22, 19, 14, 14, 11, 20, 28, 22],   'assists': [5, 7, 7, 9, 12, 9, 9, 4, 8],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12, 9]})
#view DataFrame
df
teampointsassistsrebounds
0A18511
1B2278
2C19710
3D1496
4E14126
5F1195
6G2099
7H28412
8I2289
</b>
We can use the following code to append a list that contains information about a new basketball team to the end of the DataFrame:
<b>#define list of values
new_team = ['J', 30, 10, 12]
#append list to DataFrame
df.loc[len(df)] = new_team
#view updated DataFrame
df
teampointsassistsrebounds
0A18511
1B2278
2C19710
3D1496
4E14126
5F1195
6G2099
7H28412
8I2289
9J301012</b>
Notice that the list of values has been appended as a new row to the end of the DataFrame.
Note that you’ll receive an error if the number of values in the list does not match the number of columns in the existing DataFrame.
For example, suppose we attempt to append the following list to the end of the DataFrame:
<b>#define list of values
new_team = ['J', 30]
#append list to DataFrame
df.loc[len(df)] = new_team
#view updated DataFrame
df
ValueError: cannot set a row with mismatched columns
</b>
We receive an error because our list contains two values, but the existing pandas DataFrame contains four columns.
To append a list to the end of this DataFrame, the list must also contain four values.
<h2><span class="orange">Arcsine Transformation in Excel (With Examples)</span></h2>
An <b>arcsine transformation</b> can be used to “stretch out” data points that range between the values 0 and 1.
This type of transformation is typically used when dealing with proportions and percentages.
We can use the following formula to perform an arcsine transformation in Excel:
<b>#perform arcsine transformation on value in cell A1
=ASIN(SQRT(A1))
</b>
The following examples shows how to perform an arcsine transformation on datasets in Excel.
<h3>Example 1: Arcsine Transformation of Values in Range 0 to 1</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc1.png">
We can use the following formula to perform an arcsine transformation on the value in cell A2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc2.png">
We can then hover over the bottom right corner of cell A2 until a tiny plus sign “<b>+</b>” appears, then double click it to copy and paste the arcsine formula down to every cell in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc3.png">
<h3>Example 2: Arcsine Transformation of Values Outside Range 0 to 1</h3>
Note that the arcsine transformation only works on values between the range of 0 to 1. So, if we have a vector with values outside of this range then we need to first convert each value to be in the range of 0 to 1.
For example, suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc4.png">
Before we perform an arcsine transformation on the values, we must first divide each value by the max value in the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc5.png">
Now each value in the dataset ranges between 0 and 1. We can then perform an arcsine transformation on the values in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/arc6.png">
<h2><span class="orange">Arcsine Transformation in R (With Examples)</span></h2>
An <b>arcsine transformation</b> can be used to “stretch out” data points that range between the values 0 and 1.
This type of transformation is typically used when dealing with proportions and percentages.
We can use the following syntax to perform an arcsine transformation in R:
<b>asin(sqrt(x))
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Arcsine Transformation of Values in Range 0 to 1</h3>
The following code shows how to perform an arcsine transformation on values in a vector that range between 0 and 1:
<b>#define vector 
x &lt;- c(0.1, 0.33, 0.43, 0.5, 0.7)
#perform arcsine transformation on values in vector
asin(sqrt(x))
[1] 0.3217506 0.6119397 0.7151675 0.7853982 0.9911566
</b>
<h3>Example 2: Arcsine Transformation of Values Outside Range 0 to 1</h3>
Note that the arcsine transformation only works on values between the range of 0 to 1. Thus, if we have a vector with values outside of this range, we need to first convert each value to be in the range of 0 to 1.
<b>#define vector with values outside of range 0 to 1
x &lt;- c(2, 14, 16, 30, 48, 78)
#create new vector where each value is divided by max value
y &lt;- x / max(x)
#view new vector
y
[1] 0.02564103 0.17948718 0.20512821 0.38461538 0.61538462 1.00000000
#perform arcsine transformation on new vector
asin(sqrt(y))
[1] 0.1608205 0.4374812 0.4700275 0.6689641 0.9018323 1.5707963
</b>
<h3>Example 3: Arcsine Transformation of Values in Data Frame</h3>
The following code shows how to perform an arcsine transformation of values in a specific column of a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(.2, .3, .4, .4, .7), var2=c(.1, .2, .2, .2, .3), var3=c(.04, .09, .1, .12, .2))
#perform arcsine transformation on values in 'var1' column
asin(sqrt(df$var1))
[1] 0.4636476 0.5796397 0.6847192 0.6847192 0.9911566
</b>
And the following code shows how to perform an arcsine transformation of values in multiple columns of a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(.2, .3, .4, .4, .7), var2=c(.1, .2, .2, .2, .3), var3=c(.04, .09, .1, .12, .2))
#perform arcsine transformation on values in 'var1' and 'var3' columns
sapply(df[ c('var1', 'var3')], function(x) asin(sqrt(x)))
          var1      var3
[1,] 0.4636476 0.2013579
[2,] 0.5796397 0.3046927
[3,] 0.6847192 0.3217506
[4,] 0.6847192 0.3537416
[5,] 0.9911566 0.4636476</b>
<h2><span class="orange">Area Between Two Z-Scores Calculator</span></h2>
This calculator finds the area under the normal distribution between two  z-scores .
Simply enter the two z-scores below and then click the “Calculate” button.
<label for="z1"><b>Left Bound Z-Score</b></label>
<input type="number" id="z1" value="-.34">
<label for="z2"><b>Right Bound Z-Score</b></label>
<input type="number" id="z2" value="0.8">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Area: </b> 0.42122
<script>
function calc() {
//get input values
var z1  = document.getElementById('z1').value*1;
var z2  = document.getElementById('z2').value*1;
var area = 1;
//find z-score
if (z1<z2) {
area = jStat.normal.cdf(z2, 0, 1 ) - jStat.normal.cdf(z1, 0, 1 );
} else {
area = jStat.normal.cdf(z1, 0, 1 ) - jStat.normal.cdf(z2, 0, 1 );
}
//output
document.getElementById('area').innerHTML = area.toFixed(5);
}
</script>
<h2><span class="orange">How to Create an Area Chart in Google Sheets (Step-by-Step)</span></h2>
An <b>area chart</b> is a type of chart that displays the quantitative values of several variables over a certain time period.
This tutorial provides a step-by-step example of how to create the following area chart in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle7.jpg">
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that shows the sales of three different products during a 5-year period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle1.jpg"604">
<h3>Step 2: Create the Area Chart</h3>
Next, highlight the cells in the range <b>A1:F4</b>. Then click <b>Insert</b> along the top ribbon and then click <b>Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle2.jpg"536">
In the <b>Chart editor</b> panel that appears on the right side of the screen, click the dropdown menu next to <b>Chart type</b> and click on the icon titled <b>Stacked area chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle3.jpg"310">
The following area chart will automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle4.jpg"608">
The x-axis displays the year and the y-axis displays the total sales of each of the three products.
<h3>Step 3: Modify the Area Chart</h3>
Next, we can modify the appearance of the area chart to make it easier to read.
In the <b>Chart editor</b> panel, click the <b>Customize</b> tab, then click <b>Chart & axis titles</b> and then type in a custom title for the chart in the <b>Title text</b> box:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle4-1.jpg"316">
Next, click the dropdown arrow next to  <b>Legend</b> and change the position to <b>Bottom</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle5.jpg"310">
Next, click the dropdown arrow next to <b>Series</b> and change the color of each Series to whatever you’d like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle6.jpg"296">
Here is what the final area chart will look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/areagoogle7.jpg">
<h2><span class="orange">Area To The Left of Z-Score Calculator</span></h2>
This calculator finds the area to the left of a certain  z-score  in the normal distribution.
Simply enter the z-score below and then click the “Calculate” button.
<label for="z1"><b>Z-Score</b></label>
<input type="number" id="z1" value="-.34">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Area to the Left of Z-Score: </b> 0.36693
<script>
function calc() {
//get input values
var z1  = document.getElementById('z1').value*1;
var area = 1;
//find area
area = jStat.normal.cdf(z1, 0, 1 );
//output
document.getElementById('area').innerHTML = area.toFixed(5);
}
</script>
<h2><span class="orange">Area To The Right of Z-Score Calculator</span></h2>
This calculator finds the area to the right of a certain  z-score  in the normal distribution.
Simply enter the z-score below and then click the “Calculate” button.
<label for="z1"><b>Z-Score</b></label>
<input type="number" id="z1" value="0.22">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Area to the Right of Z-Score: </b>0.41294
<script>
function calc() {
//get input values
var z1  = document.getElementById('z1').value*1;
var area = 1;
//find area
area = 1-jStat.normal.cdf(z1, 0, 1 );
//output
document.getElementById('area').innerHTML = area.toFixed(5);
}
</script>
<h2><span class="orange">How to Arrange Rows in R</span></h2>
Often you may be interested in arranging the rows of a data frame in R in a specific order. Fortunately this is easy to do using the  arrange()  function from the  dplyr  library.
This tutorial explains several examples of how to use this function in practice using the following data frame:
<b>#create data frame
df &lt;- data.frame(player = c('A', 'B', 'C', 'D', 'E', 'F', 'G'), points = c(12, 14, 14, 15, 20, 18, 29), assists = c(3, 5, 7, 8, 14, NA, 9))
#view data frame 
df
  player points assists
1      A     12       3
2      B     14       5
3      C     14       7
4      D     15       8
5      E     20      14
6      F     18      NA
7      G     29       9
</b>
<h3>Example 1: Arrange by One Column</h3>
The following code shows how to arrange the data frame in ascending order based on the values in the ‘points’ column:
<b>library(dplyr)
df %>% arrange(points)
  player points assists
1      A     12       3
2      B     14       5
3      C     14       7
4      D     15       8
5      F     18      NA
6      E     20      14
7      G     29       9
</b>
To sort in descending order, we can use the <b>desc() </b>function:
<b>df %>% arrange(desc(points))
  player points assists
1      G     29       9
2      E     20      14
3      F     18      NA
4      D     15       8
5      B     14       5
6      C     14       5
7      A     12       3
</b>
Note that NA’s will be sorted to the end, whether or not you sort ascending or decsending:
<b>df %>% arrange(assists)
  player points assists
1      A     12       3
2      B     14       5
3      C     14       7
4      D     15       8
5      G     29       9
6      E     20      14
7      F     18      NA
df %>% arrange(desc(assists))
  player points assists
1      E     20      14
2      G     29       9
3      D     15       8
4      C     14       7
5      B     14       5
6      A     12       3
7      F     18      NA</b>
<h3>Example 2: Arrange by Multiple Columns</h3>
To arrange the rows by multiple columns, we can simply provide more column names as arguments:
<b>#sort by points, then assists
df %>% arrange(points, assists)
  player points assists
1      A     12       3
2      B     14       5
3      C     14       7
4      D     15       8
5      F     18      NA
6      E     20      14
7      G     29       9
</b>
We can also arrange the rows by one column ascending and another descending:
<b>#sort by points ascending, then assists descending
df %>% arrange(points, desc(assists))
  player points assists
1      A     12       3
2      C     14       7
3      B     14       5
4      D     15       8
5      F     18      NA
6      E     20      14
7      G     29       9</b>
<h3>Example 3: Arrange Rows in a Custom order</h3>
 Occasionally you may also want to sort the rows in a custom order. You can easily do this by using a factor with specific levels:
<b>#sort by player with custom order
df %>% arrange(factor(player, levels = c('D', 'C', 'A', 'B', 'E', 'F', 'G')))
  player points assists
1      D     15       8
2      C     14       7
3      A     12       3
4      B     14       5
5      E     20      14
6      F     18      NA
7      G     29       9</b>
<em>You can find the complete documentation for the <b>arrange()</b> function  here .</em>
<h2><span class="orange">How to Use ARRAYFORMULA with VLOOKUP in Google Sheets</span></h2>
You can use the following syntax to use the <b>ARRAYFORMULA</b> function with the <b>VLOOKUP</b> function in Google Sheets:
<b>=ARRAYFORMULA(VLOOKUP(E2:E11,A2:C11,3,FALSE))
</b>
This particular formula searches for the values in <b>E2:E11</b> in the range <b>A2:C11</b> and returns the value from the third column in the range.
The benefit of using <b>ARRAYFORMULA</b> is that we can perform a <b>VLOOKUP</b> for each value in E2:E11 without writing an individual <b>VLOOKUP</b> formula for each cell in column E.
The following example shows how to use this formula in practice.
<h3>Example: Using ARRAYFORMULA with VLOOKUP in Google Sheets</h3>
Suppose we have the following dataset that contains information about various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/array1.jpg"505">
We can use <b>ARRAYFORMULA</b> with <b>VLOOKUP</b> to look up every team in column E and return their corresponding rebounds values in column F:
<b>=ARRAYFORMULA(VLOOKUP(E2:E11,A2:C11,3,FALSE))</b>
We can type this formula one time into cell <b>F2</b> and press <b>ENTER</b>, and the rebounds values for each team will be returned:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/array2.jpg">
This offers a much quicker way to return the rebounds values for each team as opposed to typing an individual <b>VLOOKUP</b> function in each cell of column F or even clicking and dragging the formula in cell <b>F2</b> down to each cell in column F.
<b>Note:</b> You can find the complete documentation for the <b>ARRAYFORMULA</b> function in Google Sheets  here .
<h2><span class="orange">How to Use as.Date() Function in R (With Examples)</span></h2>
You can use the <b>as.Date() </b>function in R to quickly convert character objects to date objects.
This function uses the following basic syntax:
<b>as.Date(x, format, tryFormats = c("%Y-%m-%d", "%Y/%m/%d")
</b>
where:
<b>x</b>: The name of the object to be converted to date.
<b>format</b>: The format of the date string. If not specified, it will try one of the tryFormats.
<b>tryFormats</b>: Formats to try. 
The following examples show how to use this function in different scenarios.
<h3>Example 1: Use as.Date() with Recognizable Date Formats</h3>
By default, the <b>as.Date()</b> function can easily convert character objects to date objects if the character objects are formatted in one of the following ways:
<b>%Y-%m-%d</b>
<b>%Y/%m/%d</b>
The following code shows how to use the <b>as.Date()</b> function to convert a character object with a <b>%Y-%m-%d</b> format to a date object:
<b>#define character object in %Y-%m-%d format
x &lt;- "2022-10-15"
#view class of x
class(x)
[1] "character"
#convert character object to date object
my_date &lt;- as.Date(x)
#view new date object
my_date
[1] "2022-10-15"
#view class of my_date
class(my_date)
[1] "Date"
</b>
We can see that the character object has been converted to a date object.
The following code shows how to use the <b>as.Date()</b> function to convert a character object with a <b>%Y/%m/%d</b> format to a date object:
<b>#define character object in %Y/%m/%d format
x &lt;- "2022/10/15"
#convert character object to date object
my_date &lt;- as.Date(x)
#view class of my_date
class(my_date)
[1] "Date"
</b>
We can see that the character object has been converted to a date object.
For both of these examples, we didn’t need to use the <b>format</b> argument in the <b>as.Date()</b> function because both date formats were recognized by R.
<h3>Example 2: Use as.Date() with Unrecognizable Date Formats</h3>
When character objects have an unrecognizable date format, you must use the <b>format</b> argument to specify the format.
For example, the following code shows how to use the <b>as.Date()</b> function to convert a character object with a <b>%m/%d/%Y</b> format to a date object:
<b>#define character object in %m/%d/%Y format
x &lt;- "10/15/2022"
#convert character object to date object
my_date &lt;- as.Date(x, format="%m/%d/%Y")
#view new date object
my_date
[1] "2022-10-15"
#view class of my_date
class(my_date)
[1] "Date"
</b>
We can see that the character object has been converted to a date object.
And the following code shows how to use the <b>as.Date()</b> function to convert a character object with a <b>%m%d%Y</b> format to a date object:
<b>#define character object in %m%d%Y format
x &lt;- "10152022"
#convert character object to date object
my_date &lt;- as.Date(x, format="%m%d%Y")
#view new date object
my_date
[1] "2022-10-15"
#view class of my_date
class(my_date)
[1] "Date"
</b>
The character object has successfully been converted to a date object.
<h2><span class="orange">What is Ascertainment Bias?</span></h2>
<b>Ascertainment bias</b> occurs when data for a study are collected such that some members of a population are more likely to be included in the sample than others.
This can result in samples that are  not representative of the target population , which makes it hard to generalize the findings from the sample to the population.
<h3>Examples of Ascertainment Bias</h3>
Here are a couple examples of ascertainment bias in different settings:
<b>1. Prevalance of Diseases</b>
Suppose researchers are interested in estimating how prevalent a disease is in a certain country. To collect data, they ask residents around the country to visit their nearest hospital and get tested for the disease.
Ascertainment bias is likely to occur because residents who are richer and more capable of getting to a hospital/live in an area that has a hospital are more likely to get tested. This means that the disease is likely to appear far more prevalant in rich populations compared to poor ones in this country.
However, this result is misleading because it turns out that richer residents are simply more likely to be included in the sample data.
<b>2. Support of Tax Increases</b>
Suppose a school board is interested in estimating the proportion of households in the school district that would support a tax increase to provide more funding for the school sports teams. To collect data, they go around and survey parents at the school football game on a Friday night.
Ascertainment bias is likely to occur because the parents that are present at the game are likely to have a child who is on the football team, which means they’re far more likely to support a tax increase compared to the typical household in the school district.
This means the proportion of households in the survey that support the tax increase is unlikely to match the proportion of households that support the tax increase in the overall population.
<h3>How to Prevent Ascertainment Bias</h3>
The easiest way to prevent ascertainment bias is to use a  sampling method  that gives each member of a population an equal chance of being included in the sample.
Examples of appropriate sampling methods include:
Simple random sample
Stratified random sample
Cluster random sample
Systematic random sample
In each of these methods, the probability that a given member of the population is included in the sample is equal.
This means each of these methods maximizes the chances that the sample obtained is representative of the target population. Thus, the findings from the sample can be generalized to the overall population with confidence.
 Nonresponse Bias 
 Undercoverage Bias 
 Omitted Variable Bias 
 Referral Bias 
 Aggregation Bias 
<h2><span class="orange">How to Use the assign() Function in R (3 Examples)</span></h2>
The <b>assign()</b> function in R can be used to assign values to variables.
This function uses the following basic syntax:
<b>assign(x, value)</b>
where:
<b>x</b>: A variable name, given as a character string.
<b>value</b>: The value(s) to be assigned to x.
The following examples show how to use this function in practice.
<h2>Example 1: Assign One Value to One Variable</h2>
The following code shows how to use the <b>assign()</b> function to assign the value of 5 to a variable called new_variable:
<b>#assign one value to new_variable
assign('new_variable', 5)
#print new_variable
new_variable
[1] 5
</b>
When we print the variable called <b>new_variable</b>, we can see that a value of <b>5</b> appears.
<h2>Example 2: Assign Vector of Values to One Variable</h2>
The following code shows how to use the <b>assign()</b> function to assign a vector of values to a variable called new_variable:
<b>#assign vector of values to new_variable
assign('new_variable', c(5, 6, 10, 12))
#print new_variable
new_variable
[1]  5  6 10 12
</b>
When we print the variable called <b>new_variable</b>, we can see that a vector of values appears.
<h2>Example 3: Assign Values to Several Variables</h2>
The following code shows how to use the <b>assign()</b> function within a  for loop  to assign specific values to several new variables:
<b>#use for loop to assign values to different variables
for(i in 1:4) {
  assign(paste0("var_", i), i*2)
}
#view variables created in for loop
var_1
[1] 2
var_2
[1] 4
var_3
[1] 6
var_4
[1] 8
</b>
By using the <b>assign()</b> function with a for loop, we were able to create four new variables.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use the dim() Function in R 
 How to Use the table() Function in R 
 How to Use sign() Function in R 
<h2><span class="orange">What is the Assumption of Independence in Statistics?</span></h2>
Many statistical tests make the assumption that  observations  are independent. This means that no two observations in a dataset are related to each other or affect each other in any way.
For example, suppose we want to test whether or not there is a difference in mean weight between two species of cats. If we measure the weight of 10 cats from species A and 10 cats from species B, we would violate the assumption of independence if each of the groups of cats came from the same litter.
It’s possible that the mother cat of species A simply had all low-weight kittens while the mother cat of species B had heavy kittens. In this regard, the observations in each sample are not independent of each other.
There are three common types of statistical tests that make this assumption of independence:
<b>1. </b> Two Sample t-test 
<b>2. </b> ANOVA (Analysis of Variance) 
<b>3.</b>  Linear Regression 
In the following sections, we explain <em>why</em> this assumption is made for each type of test along with how to determine whether or not this assumption is met.
<h3>Assumption of Independence in t-tests</h3>
A <b>two sample t-test</b> is used to test whether or not the means of two populations are equal.
<b>Assumption:</b> This type of test assumes that the observations <em>within</em> each sample are independent of each other and that the observations <em>between</em> samples are also independent of each other.
<b>Test this Assumption:</b> The easiest way to check this assumption is to verify that each observation only appears in each sample once and that the observations in each sample were collected using random sampling.
<h3>Assumption of Independence in ANOVA</h3>
An <b>ANOVA</b> is used to determine whether or not there is a significant difference between the means of three or more independent groups. 
<b>Assumption:</b> An ANOVA assumes that the observations in each group are independent of each other and the observations within groups were obtained by a random sample.
<b>Test this Assumption:</b> Similar to a t-test, the easiest way to check this assumption is to verify that each observation only appears in each sample once and that the observations in each sample were collected using random sampling.
<h3>Assumption of Independence in Regression</h3>
<b>Linear regression </b>is used to understand the relationship between one or more predictor variables and a  response variable .
<b>Assumption:</b> Linear regression assumes that the  residuals  in the fitted model are independent.
<b>Test this Assumption:</b> The easiest way to check this assumption is to look at a residual time series plot, which is a plot of residuals vs. time. Ideally, most of the residual autocorrelations should fall within the 95% confidence bands around zero, which are located at about +/- 2-over the square root of <em>n</em>, where <em>n</em> is the sample size. You can also formally test if this assumption is met using the  Durbin-Watson test .
<h3>Common Sources of Non-Independence</h3>
There are three common sources of non-independence in datasets:
<b>1. Observations are close together in time.</b>
For example, a researcher may be collecting data on the average speed of cars on a certain road. If he chooses to track the speeds during the evening hours, he may find that the average speed is much higher than he expected simply because each driver is rushing home from work.
This data violates the assumption that each observation is independent. Since each observation was observed during the same time of day, the speed of each car is likely to be similar to each other.
<b>2. Observations are close together in space.</b>
For example, a researcher may collect data on the annual income of individuals who happen to all live in the same high-income neighborhood because it’s convenient to do so.
In this regard, all of the individuals included in the sample data are likely to have similar incomes since they all live near each other. This violates the assumption that each observation is independent.
<b>3. Observations appear multiple times in the same dataset.</b>
For example, a researcher may need to collect data for 50 individuals but instead decides to collect data on 25 individuals twice because it’s much easier to do so.
This violates the assumption of independence because each observation in the dataset will be related to itself.
<h3>How to Avoid Violating the Assumption of Independence</h3>
The easiest way to avoid violating the assumption of independence is to simply use  simple random sampling  when obtaining a sample from a population.
Using this method, every individual in the  population  of interest has an equal chance of being included in the sample.
For example, if our population of interest contains 10,000 individuals then we may randomly assign a number to every individual in the population and then use a random number generator to select 40 random numbers. The individuals who match up with these numbers would then be included in the sample.
Using this method, we minimize the chances that we select two individuals who may be in close proximity to each other or who may be related in some way.
This is in direct contrast to other sampling methods such as:
<b>Convenience sampling:</b> Including individuals in a sample who are simply convenient to reach.
<b>Voluntary sampling:</b> Including individuals in a sample who  volunteer  to be included.
By using a random sampling method, we can minimize the chances that we violate the assumption of independence.
<h2><span class="orange">What is the Assumption of Normality in Statistics?</span></h2>
Many statistical tests rely on something called the <b>assumption of normality</b>.
This assumption states that if we collect many independent random samples from a population and calculate some value of interest (like the  sample mean ) and then create a histogram to visualize the distribution of sample means, we should observe a perfect  bell curve .
Many statistical techniques make this assumption about the data, including:
<b>1.</b>  One sample t-test : It’s assumed that the sample data is normally distributed.
<b>2.</b>  Two sample t-test : It’s assumed that both samples are normally distributed.
<b>3.</b>  ANOVA : It’s assumed that the residuals from the model are normally distributed.
<b>4.</b>  Linear regression : It’s assumed that the residuals from the model are normally distributed.
If this assumption is violated then the results of these tests become unreliable and we’re unable to generalize our findings from the sample data to the overall  population  with confidence. This is why it’s import to check if this assumption is met.
There are two common ways to check if this assumption of normality is met:
<b>1. Visualize Normality</b>
<b>2. Perform a Formal Statistical Test</b>
The following sections explain the specific graphs you can create and the specific statistical tests you can perform to check for normality.
<h3>Visualize Normality</h3>
A quick and informal way to check if a dataset is normally distributed is to create a histogram or a Q-Q plot.
<b>1. Histogram</b>
If a histogram for a dataset is roughly bell-shaped, then it’s likely that the data is normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityAssume1.png">
<b>2. Q-Q Plot</b>
A Q-Q plot, short for “quantile-quantile” plot, is a type of plot that displays theoretical quantiles along the x-axis (i.e. where your data would lie if it did follow a normal distribution) and sample quantiles along the y-axis (i.e. where your data actually lies).
If the data values fall along a roughly straight line at a 45-degree angle, then the data is assumed to be normally distributed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/normalityAssume2.png">
<h3>Perform a Formal Statistical Test</h3>
You can also perform a formal statistical test to determine if a dataset is normally distributed.
If the  p-value  of the test is less than a certain significance level (like α = 0.05) then you have sufficient evidence to say that the data is <em>not</em> normally distributed.
There are three statistical tests that are commonly used to test for normality:
<b>1. The Jarque-Bera Test</b>
 How to Perform a Jarque-Bera Test in Excel 
 How to Perform a Jarque-Bera Test in R 
 How to Perform a Jarque-Bera Test in Python 
<b>2. The Shapiro-Wilk Test</b>
 How to Perform a Shapiro-Wilk Test in R 
 How to Perform a Shapiro-Wilk Test in Python 
<b>3. The Kolmogorov-Smirnov Test</b>
 How to Perform a Kolmogorov-Smirnov Test in Excel 
 How to Perform a Kolmogorov-Smirnov Test in R 
 How to Perform a Kolmogorov-Smirnov Test in Python 
<h3>What to Do if the Assumption of Normality is Violated</h3>
If it turns out that your data is not normally distributed then you have two options:
<b>1. Transform the data.</b>
One option is to simply <em>transform</em> the data to make it more normally distributed. Common transformations include:
<b>Log Transformation: </b>Transform the data from y to <b>log(y)</b>.
<b>Square Root Transformation: </b>Transform the data from y to <b>√y</b>
<b>Cube Root Transformation: </b>Transform the data from y to <b>y<sup>1/3</sup></b>
<b>Box-Cox Transformation:</b> Transform the data using a  Box-Cox procedure 
By performing these transformations, the distribution of data values typically becomes more normally distributed.
<b>2. Perform a Non-Parametric Test</b>
Statistical tests that make the assumption of normality are known as <em>parametric tests</em>. But there are also a family of tests known as <em>non-parametric tests</em> that do not make this assumption of normality.
If it turns out that your data is not normally distributed, you could simply perform a non-parametric test. Here are a few non-parametric versions of common statistical tests:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Parametric Test</b></th>
<th style="text-align: center;"><b>Non-Parametric Equivalent</b></th>
</tr>
<tr>
<td style="text-align: center;">One Sample t-test</td>
<td style="text-align: center;">One Sample Wilcoxon Signed Rank Test</td>
</tr>
<tr>
<td style="text-align: center;">Two Sample t-test</td>
<td style="text-align: center;"> Mann-Whitney U Test </td>
</tr>
<tr>
<td style="text-align: center;">Paired Samples t-test</td>
<td style="text-align: center;"> Two Sample Wilcoxon Signed Rank Test </td>
</tr>
<tr>
<td style="text-align: center;">One-Way ANOVA</td>
<td style="text-align: center;"> Kruskal-Wallis Test </td>
</tr>
</tbody></table>
Each of these non-parametric tests allow you to perform a statistical test without satisfying the assumption of normality.
<h2><span class="orange">The 6 Assumptions of Logistic Regression (With Examples)</span></h2>
<b>Logistic regression </b>is a method that we can use to fit a regression model when the  response variable  is binary.
Before fitting a model to a dataset, logistic regression makes the following assumptions:
<h3>Assumption #1: The Response Variable is Binary</h3>
Logistic regression assumes that the response variable only takes on two possible outcomes. Some examples include:
Yes or No
Male or Female
Pass or Fail
Drafted or Not Drafted
Malignant or Benign
<b>How to check this assumption: </b>Simply count how many unique outcomes occur in the response variable. If there are more than two possible outcomes, you will need to perform  ordinal regression  instead.
<h3>Assumption #2: The Observations are Independent</h3>
Logistic regression assumes that the observations in the dataset are independent of each other. That is, the observations should not come from repeated measurements of the same individual or be related to each other in any way.
<b>How to check this assumption: </b>The easiest way to check this assumption is to create a plot of residuals against time (i.e. the order of the observations) and observe whether or not there is a random pattern. If there is <em>not </em>a random pattern, then this assumption may be violated. 
<h3>Assumption #3: There is No Multicollinearity Among Explanatory Variables</h3>
Logistic regression assumes that there is no severe  multicollinearity  among the  explanatory variables . 
Multicollinearity occurs when two or more explanatory variables are highly correlated to each other, such that they do not provide unique or independent information in the regression model. If the degree of correlation is high enough between variables, it can cause problems when fitting and interpreting the model. 
For example, suppose you want to perform logistic regression using <b>max vertical jump </b>as the response variable and the following variables as explanatory variables:
<b>Player height</b>
<b>Player shoe size</b>
<b>Hours spent practicing per day</b>
In this case, <b>height</b> and <b>shoe size</b> are likely to be highly correlated since taller people tend to have larger shoe sizes. This means that multicollinearity is likely to be a problem if we use both of these variables in the regression.
<b>How to check  this assumption: </b>The most common way to detect multicollinearity is by using the variance inflation factor (VIF), which measures the correlation and strength of correlation between the predictor variables in a regression model. Check out  this tutorial  for an in-depth explanation of how to calculate and interpret VIF values.
<h3>Assumption #4: There are No Extreme Outliers</h3>
Logistic regression assumes that there are no extreme outliers or influential observations in the dataset.
<b>How to check this assumption: </b>The most common way to test for extreme outliers and influential observations in a dataset is to calculate  Cook’s distance  for each observation. If there are indeed outliers, you can choose to (1) remove them, (2) replace them with a value like the mean or median, or (3) simply keep them in the model but make a note about this when reporting the regression results.
<h3>Assumption #5: There is a Linear Relationship Between Explanatory Variables and the Logit of the Response Variable</h3>
Logistic regression assumes that there exists a linear relationship between each explanatory variable and the logit of the response variable. Recall that the logit is defined as:
Logit(p)  = log(p / (1-p)) where p is the probability of a positive outcome.
<b>How to check this assumption: </b>The easiest way to see if this assumption is met is to use a Box-Tidwell test.
<h3>Assumption #6: The Sample Size is Sufficiently Large</h3>
Logistic regression assumes that the sample size of the dataset if large enough to draw valid conclusions from the fitted logistic regression model.
<b>How to check this assumption: </b>As a rule of thumb, you should have a minimum of 10 cases with the least frequent outcome for each explanatory variable. For example, if you have 3 explanatory variables and the expected probability of the least frequent outcome is 0.20, then you should have a sample size of at least (10*3) / 0.20 = <b>150</b>.
<h3>Assumptions of Logistic Regression vs. Linear Regression</h3>
In contrast to linear regression, logistic regression does not require:
A linear relationship between the explanatory variable(s) and the response variable.
The residuals of the model to be normally distributed.
The residuals to have constant variance, also known as  homoscedasticity .
<b>Related: </b> The Four Assumptions of Linear Regression 
<h2><span class="orange">Attributable Risk Calculator</span></h2>
svg:not(:root) {
  overflow: visible;
}
td input {
  max-width:60px;
  max-height:30px;
}
</style>
This calculator finds the attributable risk, attributable risk percentage, and population attributable risk percentage for a given 2×2 contingency table.
Simply fill in the cells of the table below and then click “Calculate.”
<table><tbody>
<tr style="max-height:10px">
<th style="min-width:120px"></th>
<th><b><span>Disease</b></th>
<th><b><span>No Disease</b></th>
</tr>
<tr>
<td>Exposed</td>
<td><input type="text" id="A" value="25"></td>
<td><input type="text" id="B" value="140"></td>
</tr>
<tr>
<td>Not Exposed</td>
<td><input type="text" id="C" value="52"></td>
<td><input type="text" id="D" value="683"></td>
</tr>
</tbody></table>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
Attributable Risk: <b>0.08077</b>
Attributable Risk %: <b>53.30612</b>
Population Attributable Risk %: <b>17.30718%</b>
<script>
function calc() {
//get input data
var A = document.getElementById('A').value;
var B = document.getElementById('B').value;
var C = document.getElementById('C').value;
var D = document.getElementById('D').value;
var N = A - (-1*B) - (-1*C) - (-1*D);
//calculate stuff
var AR = (A/(A-(-1*B)))  -  (C/(C-(-1*D)));
var ARP = AR / (A/(A-(-1*B))) * 100;
var PAR = ( ((A-(-1*C)) / N) - (C/(C-(-1*D))) )  /  ((A-(-1*C)) / N)  * 100;
//output results
document.getElementById('AR').innerHTML = AR.toFixed(5);
document.getElementById('ARP').innerHTML = ARP.toFixed(5);
document.getElementById('PAR').innerHTML = PAR.toFixed(5);
  
} //end calc function
</script>
<h2><span class="orange">What is Attributable Risk? (Definition & Example)</span></h2>
<b>Attributable risk</b> refers to the difference in incidence rates between people exposed to some risk factor vs. people not exposed to the risk factor.
For example, we might use this metric to understand the difference in cardiovascular disease (the incidence) between smokers (people exposed) and non-smokers (people not exposed).
Typically we calculate <b>attributable risk percentage</b>, which refers to the percentage of an incidence rate that can be attributed to some risk factor.
We can also calculate <b>population attributable risk percentage</b>, which refers to the percentage of an incidence rate in the overall population that can be attributed to some risk factor.
Given the following contingency table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/attributable_risk1.png">
<b>Attributable risk</b> can be calculated as:
AR = (A/(A+B))  –  (C/(C+D))
<b>Attributable risk percentage</b> can be calculated as:
AR % = AR / (A/(A+B)) * 100
<b>Population attributable risk percentage</b> can be calculated as:
PAR % = [ ((A+C) / N) – (C/(C+D)) ]  /  ((A+C) / N)  * 100
where <em>N</em> is the sum of all cells in the contingency table.
The following example shows how to calculate each of these metrics.
<h3>Example: Calculating Attributable Risk & Other Metrics</h3>
The following table shows the number of people who developed cardiovascular disease based on their smoking history:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/attributable_risk2.png">
The <b>attributable risk</b> of smoking can be calculated as:
AR = (A/(A+B))  –  (C/(C+D))
AR = (25/(25+140))  –  (52/(52+683))
AR = <b>.08077</b>
The <b>attributable risk percentage</b> of smoking can be calculated as:
AR % = AR / (A/(A+B)) * 100
AR % = .08077 / (25/(25+140)) * 100
AR % = <b>53.31%</b>
This means 53.31% of incidence of cardiovascular disease among smokers is attributable to their smoking.
The <b>population attributable risk percentage</b> of smoking can be calculated as:
PAR % = [ ((A+C)/N) – (C/(C+D)) ]  /  ((A+C) / N)  * 100
PAR % = [ ((25+52)/900) – (52/(52+683)) ]  /  ((25+52) / 900)  * 100
PAR % = <b>17.31%</b>
This means 17.31% of incidence of cardiovascular disease in the population is attributable to smoking.
In other words, if smoking were eliminated in the population then we would expect to see a 17.31% reduction in cases of cardiovascular disease.
<h3>Bonus: Attributable Risk Calculator</h3>
Feel free to use the  Attributable Risk Calculator  to automatically calculate the attributable risk, attributable risk percentage, and population attributable risk percentage for any 2×2 contingency table.
<h2><span class="orange">How to Calculate AUC (Area Under Curve) in Python</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary.
To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the “true positive rate.”
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative. This is also called the “true negative rate.”
One way to visualize these two metrics is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve.
This is a plot that displays the sensitivity along the y-axis and (1 – specificity) along the x-axis.
One way to quantify how well the logistic regression model does at classifying data is to calculate <b>AUC</b>, which stands for “area under curve.”
The closer the AUC is to 1, the better the model.
The following step-by-step example shows how to calculate AUC for a logistic regression model in Python.
<h3>Step 1: Import Packages</h3>
First, we’ll import the packages necessary to perform logistic regression in Python:
<b>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
</b>
<h3>Step 2: Fit the Logistic Regression Model</h3>
Next, we’ll import a dataset and fit a logistic regression model to it:
<b>#import dataset from CSV file on Github
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/default.csv"
data = pd.read_csv(url)
#define the predictor variables and the response variable
X = data[['student', 'balance', 'income']]
y = data['default']
#split the dataset into training (70%) and testing (30%) sets
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) 
#instantiate the model
log_regression = LogisticRegression()
#fit the model using the training data
log_regression.fit(X_train,y_train)</b>
<h3>Step 3: Calculate the AUC</h3>
We can use the <b>metrics.roc_auc_score()</b> function to calculate the AUC of the model:
<b>#use model to predict probability that given y value is 1
y_pred_proba = log_regression.predict_proba(X_test)[::,1]
#calculate AUC of model
auc = metrics.roc_auc_score(y_test, y_pred_proba)
#print AUC score
print(auc)
0.5602104030579559
</b>
The AUC (area under curve) for this particular model is <b>0.5602</b>.
Recall that a model with an AUC score of <b>0.5</b> is no better than a model that performs random guessing. 
Thus, in most cases a model with an AUC score of <b>0.5602</b> would be considered poor at classifying observations into the correct classes.
<h2><span class="orange">How to Calculate AUC (Area Under Curve) in R</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the “true positive rate.”
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative. This is also called the “true negative rate.”
One way to visualize these two metrics is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve.
This is a plot that displays the sensitivity along the y-axis and (1 – specificity) along the x-axis. One way to quantify how well the logistic regression model does at classifying data is to calculate <b>AUC</b>, which stands for “area under curve.”
The closer the AUC is to 1, the better the model.
The following step-by-step example shows how to calculate AUC for a logistic regression model in R.
<h3>Step 1: Load the Data</h3>
First, we’ll load the <b>Default</b> dataset from the <b>ISLR</b> package, which contains information about whether or not various individuals defaulted on a loan.
<b>#load dataset
data &lt;- ISLR::Default
#view first six rows of dataset
head(data)
  default student   balance    income
1      No      No  729.5265 44361.625
2      No     Yes  817.1804 12106.135
3      No      No 1073.5492 31767.139
4      No      No  529.2506 35704.494
5      No      No  785.6559 38463.496
6      No     Yes  919.5885  7491.559
</b>
<h3>Step 2: Fit the Logistic Regression Model</h3>
Next, we’ll fit a logistic regression model to predict the probability that an individual defaults:
<b>#make this example reproducible
set.seed(1)
#Use 70% of dataset as training set and remaining 30% as testing set
sample &lt;- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train &lt;- data[sample, ]
test &lt;- data[!sample, ] 
#fit logistic regression model
model &lt;- glm(default~student+balance+income, family="binomial", data=train)</b>
<h3>Step 3: Calculate the AUC of the Model</h3>
Next, we’ll use the <b>auc()</b> function from the <b>pROC</b> package to calculate the AUC of the model. This function uses the following syntax:
<b>auc(response, predicted)</b>
Here’s how to use this function in our example:
<b>#calculate probability of default for each individual in test dataset
predicted &lt;- predict(model, test, type="response")
#calculate AUC
library(pROC)
auc(test$default, predicted)
Setting levels: control = No, case = Yes
Setting direction: controls &lt; cases
Area under the curve: 0.9437
</b>
The AUC of the model turns out to be <b>0.9437</b>.
Since this value is close to 1, this indicates that the model does a very good job of predicting whether or not an individual will default on their loan.
<h2><span class="orange">How to Calculate Autocorrelation in Excel</span></h2>
<b>Autocorrelation</b> measures the degree of similarity between a time series and a lagged version of itself over successive time intervals.
It’s also sometimes referred to as “serial correlation” or “lagged correlation” since it measures the relationship between a variable’s current values and its historical values.
When the autocorrelation in a time series is high, it becomes easy to predict future values by simply referring to past values.
<h3>Autocorrelation in Excel</h3>
There is no built-in function to calculate autocorrelation in Excel, but we can use a single formula to calculate the autocorrelation for a time series for a given lag value.
For example, suppose we have the following time series that shows the value of a certain variable during 15 different time periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationexcel1.png">
We can use the following formula to calculate the autocorrelation at lag k =2.
<b>=(SUMPRODUCT(B2:B14-AVERAGE(B2:B16), B4:B16-AVERAGE(B2:B16))/COUNT(B2:B16))/VAR.P(B2:B16)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationexcel2.png">
This results in a value of <b>0.656325</b>. This is the autocorrelation at lag k = 2.
We can calculate the autocorrelation at lag k = 3 by changing the range of values in the formula:
<b>=(SUMPRODUCT(B2:B13-AVERAGE(B2:B16), B5:B16-AVERAGE(B2:B16))/COUNT(B2:B16))/VAR.P(B2:B16)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationexcel3-1.png">
This results in a value of <b>0.49105</b>. This is the autocorrelation at lag k = 3.
We can find the autocorrelation at each lag by using a similar formula. You’ll notice that the higher the lag, the lower the autocorrelation. This is typical of an autoregressive time series process.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationexcel4.png">
<em>You can find more Excel time series tutorials on  this page .</em>
<h2><span class="orange">How to Calculate Autocorrelation in R</span></h2>
<b>Autocorrelation</b> measures the degree of similarity between a time series and a lagged version of itself over successive time intervals.
It’s also sometimes referred to as “serial correlation” or “lagged correlation” since it measures the relationship between a variable’s current values and its historical values.
When the autocorrelation in a time series is high, it becomes easy to predict future values by simply referring to past values.
<h3>How to Calculate Autocorrelation in R</h3>
Suppose we have the following time series in R  that shows the value of a certain variable during 15 different time periods:
<b>#define data
x &lt;- c(22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51)
</b>
We can calculate the autocorrelation for every lag in the time series by using the <b>acf()</b> function from the <b>tseries </b>library:
<b>library(tseries)
#calculate autocorrelations
acf(x, pl=FALSE)
     0      1      2      3      4      5      6      7      8      9     10 
 1.000  0.832  0.656  0.491  0.279  0.031 -0.165 -0.304 -0.401 -0.458 -0.450 
    11 
-0.369 
</b>
The way to interpret the output is as follows:
The autocorrelation at lag 0 is <b>1</b>.
The autocorrelation at lag 1 is <b>0.832</b>.
The autocorrelation at lag 2 is <b>0.656</b>.
The autocorrelation at lag 3 is <b>0.491</b>.
And so on.
We can also specify the number of lags to display with the <b>lag </b>argument:
<b>#calculate autocorrelations up to lag=5
acf(x, lag=5, pl=FALSE)
Autocorrelations of series 'x', by lag
    0     1     2     3     4     5 
1.000 0.832 0.656 0.491 0.279 0.031 </b>
<h3>How to Plot the Autocorrelation Function in R</h3>
We can plot the autocorrelation function for a time series in R by simply not using the <b>pl=FALSE</b> argument:
<b>#plot autocorrelation function
acf(x)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/autocorrelationR1.png">
The x-axis displays the number of lags and the y-axis displays the autocorrelation at that number of lags. By default, the plot starts at lag = 0 and the autocorrelation will always be <b>1 </b>at lag = 0.
You can also specify a different title for the plot by using the <b>main </b>argument:
<b>#plot autocorrelation function with custom title
acf(x, main='Autocorrelation by Lag')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/autocorrelationR2.png">
<h2><span class="orange">How to Calculate Autocorrelation in Python</span></h2>
<b>Autocorrelation</b> measures the degree of similarity between a time series and a lagged version of itself over successive time intervals.
It’s also sometimes referred to as “serial correlation” or “lagged correlation” since it measures the relationship between a variable’s current values and its historical values.
When the autocorrelation in a time series is high, it becomes easy to predict future values by simply referring to past values.
<h3>How to Calculate Autocorrelation in Python</h3>
Suppose we have the following time series in Python that shows the value of a certain variable during 15 different time periods:
<b>#define data
x = [22, 24, 25, 25, 28, 29, 34, 37, 40, 44, 51, 48, 47, 50, 51]
</b>
We can calculate the autocorrelation for every lag in the time series by using the  acf() function  from the statsmodels library:
<b>import statsmodels.api as sm
#calculate autocorrelations
sm.tsa.acf(x)
array([ 1.        ,  0.83174224,  0.65632458,  0.49105012,  0.27863962,
        0.03102625, -0.16527446, -0.30369928, -0.40095465, -0.45823389,
       -0.45047733])
</b>
The way to interpret the output is as follows:
The autocorrelation at lag 0 is <b>1</b>.
The autocorrelation at lag 1 is <b>0.8317</b>.
The autocorrelation at lag 2 is <b>0.6563</b>.
The autocorrelation at lag 3 is <b>0.4910</b>.
And so on.
We can also specify the number of lags to use with the <b>nlags </b>argument:
<b>sm.tsa.acf(x, nlags=5)
array([1.0, 0.83174224, 0.65632458, 0.49105012, 0.27863962, 0.03102625])</b>
<h3>How to Plot the Autocorrelation Function in Python</h3>
We can plot the autocorrelation function for a time series in Python by using the  tsaplots.plot_acf() function  from the statsmodels library:
<b>from statsmodels.graphics import tsaplots
import matplotlib.pyplot as plt
#plot autocorrelation function
fig = tsaplots.plot_acf(x, lags=10)
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationpython1.png">
The x-axis displays the number of lags and the y-axis displays the autocorrelation at that number of lags. By default, the plot starts at lag = 0 and the autocorrelation will always be <b>1 </b>at lag = 0.
We can also zoom in on the first few lags by choosing to use fewer lags with the <b>lags </b>argument:
<b>from statsmodels.graphics import tsaplots
import matplotlib.pyplot as plt
#plot autocorrelation function
fig = tsaplots.plot_acf(x, lags=5)
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationpython2.png">
We can also change the title and the color of the circles used in the plot with the <b>title</b> and <b>color</b> arguments:
<b>from statsmodels.graphics import tsaplots
import matplotlib.pyplot as plt
#plot autocorrelation function
fig = tsaplots.plot_acf(x, lags=5, color='g', title='Autocorrelation function')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/autocorrelationpython4.png">
<em>You can find more Python tutorials on  this page .</em>
<h2><span class="orange">How to Average Across Columns in R (With Examples)</span></h2>
Often you may want to calculate the average of values across several columns in R. Fortunately this is easy to do using the <b>rowMeans() </b>function. 
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Find the Average Across All Columns</h3>
The following code shows how to calculate the average value of each row across <em>all</em> columns in a data frame:
<b>#create data frame
data &lt;- data.frame(var1 = c(0, NA, 2, 2, 5),   var2 = c(5, 5, 7, 8, 9),   var3 = c(2, 7, 9, 9, 7))
#view data frame
data
  var1 var2 var3
1    0    5    2
2   NA    5    7
3    2    7    9
4    2    8    9
5    5    9    7
#find average value in each row
rowMeans(data, na.rm=TRUE)
[1] 2.333333 6.000000 6.000000 6.333333 7.000000
</b>
The way to interpret the output is as follows:
The average value in the first row is <b>2.333</b>.
The average value in the second row is <b>6</b>.
The average value in the third row is <b>6</b>.
The average value in the fourth row is <b>6.333</b>.
The average value in the fifth row is <b>7</b>.
You can also assign these row averages to a new variable in the data frame:
<b>#assign row averages to new variable named <em>row_mean</em>
data$row_mean &lt;- rowMeans(data, na.rm=TRUE)
#view data frame
data
  var1 var2 var3 row_mean
1    0    5    2 2.333333
2   NA    5    7 6.000000
3    2    7    9 6.000000
4    2    8    9 6.333333
5    5    9    7 7.000000
</b>
<h3>Example 2: Find the Average Across Specific Columns</h3>
It’s also possible to find the average across only specific columns in a data frame. For example, the following code shows how to calculate the row averages across just the first two columns:
<b>#find row averages across first two columns
data$new &lt;- rowMeans(data[ , c(1,2)], na.rm=TRUE)
#view data frame
data
  var1 var2 var3 new
1    0    5    2 2.5
2   NA    5    7 5.0
3    2    7    9 4.5
4    2    8    9 5.0
5    5    9    7 7.0</b>
We can see that:
The average value in the first row across the first two columns is <b>2.5</b>.
The average value in the second row across the first two columns is <b>5</b>.
And so on.
You can use similar syntax to find the row averages for any set of columns. For example, the following code shows how to calculate the row averages across just the first and third columns:
<b>#find row averages across first and third columns
data$new &lt;- rowMeans(data[ , c(1,3)], na.rm=TRUE)
#view data frame
data
  var1 var2 var3 new
1    0    5    2 1.0
2   NA    5    7 7.0
3    2    7    9 5.5
4    2    8    9 5.5
5    5    9    7 6.0</b>
We can see that:
The average value in the first row across the first and third columns is <b>1</b>.
The average value in the second row across the first and third columns is <b>7</b>.
And so on.
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">How to Add Average Line to Chart in Google Sheets</span></h2>
This tutorial provides a step-by-step example of how to create the following chart with an average line in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/avgline4.jpg">
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following data that shows the total sales for some company during 10 consecutive weeks:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/avgline1.jpg"495">
<h3>Step 2: Calculate the Average</h3>
Next, we can type the following formula into cell <b>C2</b>:
<b>=AVERAGE($B$2:$B$11)
</b>
We can then copy and paste this formula to every remaining cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/avgline2.jpg"477">
Column C now contains the average of the sales values during the 10 weeks.
<h3>Step 3: Create Chart with Average Line</h3>
Next, highlight the cells in the range <b>A1:C11</b>, then click the <b>Insert</b> tab, then click <b>Chart</b>.
In the <b>Chart editor</b> panel that appears on the right side of the screen, click the <b>Setup</b> tab, then click the dropdown arrow under <b>Chart type</b> and choose <b>Combo chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/avgline3.jpg"314">
The following chart will automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/avgline4.jpg">
The blue bars show the sales made during each week and the red line shows the average sales made during the 10 weeks.
<h2><span class="orange">How to Calculate Average Time in Excel (With Examples)</span></h2>
You can use the following formula to calculate an average time value in Excel:
<b>=AVERAGE(A2:A11)
</b>
This particular formula calculates the average time value in the range <b>A2:A11</b> and assumes that each value in the range is in a valid time format.
The following example shows how to use this formula in practice.
<h2>Example: Calculate Average Time in Excel</h2>
Suppose we have the following list of times in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime1.jpg"458">
To verify that the times are in a valid format, we can highlight the range <b>A2:A11</b>, then click the <b>Number Format</b> dropdown menu from the <b>Number</b> tab within the <b>Home</b> tab on the top ribbon:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime2.jpg"675">
Once we click<b> More Number Formats</b>, a new screen will appear that shows which format Excel has chosen for the values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime3.jpg"610">
In this example, Excel has chosen <b>h:mm:ss AM/PM</b>, which is the correct time format.
If your time values are in another format, simply click on the appropriate format from the list of options.
We can then click <b>OK</b>.
Next, we can use the following formula to calculate the average time value:
<b>=AVERAGE(A2:A11)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime4.jpg">
We can see that the average time is <b>12:11:51 PM</b>.
<h2>Example: Calculate Average Time with Condition in Excel</h2>
We can also use an <b>AVERAGEIF</b> function to calculate the average time based on a condition in Excel.
For example, we can use the following formula to calculate the average time only for the times that occur after 12 PM:
<b>=AVERAGEIF(A2:A11, ">12:00:00 PM")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime5.jpg"552">
Excel returns a numerical value, but we can click the <b>Number Format</b> dropdown menu again and click <b>Time</b> to convert this numeric value to a time value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/avgtime6.jpg"528">
The average time among the times that occur after 12 PM is <b>5:18:19 PM</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Excel:
 How to Calculate Average If Cell Contains Number in Excel 
 How to Use AVERAGEIF with Multiple Ranges in Excel 
 How to Calculate Average Excluding Outliers in Excel 
<h2><span class="orange">How to Use AVERAGEIFS in Google Sheets (3 Examples)</span></h2>
The <b>AVERAGEIFS </b>function in Google Sheets can be used to find the average value in a range if the corresponding values in another range meet certain criteria.
This function uses the following basic syntax:
<b>AVERAGEIFS(average_range, criteria_range1, criterion1, [criteria_range2, criterion2, …])</b>
where:
<b>average_range</b>: The range to calculate average from
<b>criteria_range1</b>: The first range to check for certain criteria
<b>criterion1</b>: The criteria to analyze in the first range
The following examples show how to use this function in different scenarios with the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/averageifs1.jpg"485">
<h3>Example 1: AVERAGEIFS with One Character Column</h3>
We can use the following formula to calculate the average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Mavs”:
<b>=AVERAGEIFS(C2:C11, A2:A11, "Mavs")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/averageifs2.jpg"548">
The average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Mavs” is <b>24.6</b>.
We can manually verify this is correct by calculating the average points for the Mavs players:
Average: (22 + 28 + 25 + 30 + 18) / 5 = <b>24.6</b>.
<h3>Example 2: AVERAGEIFS with Two Character Columns</h3>
We can use the following formula to calculate the average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Mavs” and the <b>Position</b> column is equal to “Guard”:
<b>=AVERAGEIFS(C2:C11, A2:A11, "Mavs", B2:B11, "Guard")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/averageifs3.jpg">
The average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Mavs” and the <b>Position</b> column is equal to “Guard” is <b>29</b>.
We can manually verify this is correct by calculating the average points for the Mavs players who are Guards:
Average: (22 + 28) / 2 = <b>25</b>.
<h3>Example 3: AVERAGEIFS with Character & Numeric Columns</h3>
We can use the following formula to calculate the average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Spurs” and the <b>Points</b> column is greater than 15:
<b>=AVERAGEIFS(C2:C11, A2:A11, "Spurs", C2:C11, ">15")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/averageifs4.jpg"569">
The average value in the <b>Points</b> column where the <b>Team</b> column is equal to “Spurs” and the <b>Points </b>column is greater than 15 is <b>23.67</b>.
We can manually verify this is correct by calculating the average points for the Spurs players who scored more than 15 points:
Average: (19 + 22 + 30) / 3 = <b>23.67</b>.
<b>Note</b>: You can find the complete online documentation for the <b>AVERAGEIFS</b> function  here .
<h2><span class="orange">How to Find the Average of Several Standard Deviations</span></h2>
Occasionally you may be interested in finding the average of two or more standard deviations.
You can use one of two formulas to do so, depending on your data:
<b>Method 1: Equal Sample Sizes</b>
If you want to find the average standard deviation among <em>k</em> groups and each group has the same sample size, you can use the following formula:
Average S.D. = √ (s<sub>1</sub><sup>2</sup> +  s<sub>2</sub><sup>2</sup> + … + s<sub>k</sub><sup>2</sup>) / k
where:
<b>s<sub>k</sub>:</b> Standard deviation for k<sup>th</sup> group
<b>k</b>: Total number of groups
<b>Method 2: Unequal Sample Sizes</b>
If you want to find the average standard deviation among <em>k</em> groups and each group does not have the same sample size, you can use the following formula:
Average S.D. = √ ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> + … +  (n<sub>k</sub>-1)s<sub>k</sub><sup>2</sup>) /  (n<sub>1</sub>+n<sub>2</sub> + … + n<sub>k</sub> – k)
where:
<b>n<sub>k</sub>:</b> Sample size for k<sup>th</sup> group
<b>s<sub>k</sub>:</b> Standard deviation for k<sup>th</sup> group
<b>k</b>: Total number of groups
The following examples show how to use each formula in practice.
<h3>Method 1: Averaging Standard Deviations for Equal Sample Sizes</h3>
Suppose we’d like to calculate the average standard deviation of sales during the following six sales periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/averaging_sd1.png">
Let’s assume that we made the same number of sales transactions in each sales period. We can use the following formula to calculate the average standard deviation of sales per period:
Average standard deviation = √ (s<sub>1</sub><sup>2</sup> +  s<sub>2</sub><sup>2</sup> + … + s<sub>k</sub><sup>2</sup>) / k
Average standard deviation = √ (12<sup>2</sup> + 11<sup>2</sup> + 8<sup>2</sup> + 8<sup>2</sup> + 6<sup>2</sup> + 14<sup>2</sup>) / 6
Average standard deviation = 10.21
The average standard deviation of sales per period is <b>10.21</b>.
<h3>Method 2: Averaging Standard Deviations for Unequal Sample Sizes</h3>
Suppose we’d like to calculate the average standard deviation of sales during the following six sales periods:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/averaging_sd2.png">
Since the sample size (the total transactions) is not equal in each sales period, we’ll use the following formula to calculate the average standard deviation of sales per period:
Average S.D. = √ ((n<sub>1</sub>-1)s<sub>1</sub><sup>2</sup> +  (n<sub>2</sub>-1)s<sub>2</sub><sup>2</sup> + … +  (n<sub>k</sub>-1)s<sub>k</sub><sup>2</sup>) /  (n<sub>1</sub>+n<sub>2</sub> + … + n<sub>k</sub> – k)
Average S.D. = √ ((21)12<sup>2</sup> + (16)11<sup>2</sup> + (14)8<sup>2</sup> + (18)8<sup>2</sup> + (19)6<sup>2</sup> + (18)14<sup>2</sup>) / 106
Average S.D. = 10.29
The average standard deviation of sales per period is <b>10.29</b>.
Notice that the average standard deviation in both examples were quite similar. This is because the sample sizes (total transactions) in the second example were all fairly close together.
The two methods for calculating the average standard deviation will only differ greatly when the sample sizes differ greatly between the groups.
<h2><span class="orange">The Difference Between axis=0 and axis=1 in Pandas</span></h2>
Many functions in  pandas  require that you specify an axis along which to apply a certain calculation.
Typically the following rule of thumb applies:
<b>axis=0</b>: Apply the calculation “column-wise”
<b>axis=1</b>: Apply the calculation “row-wise”
The following examples show how to use the <b>axis</b> argument in different scenarios with the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
teampointsassistsrebounds
0A25511
1A1278
2B15710
3B1496
4B19126
5B2395
6C2599
7C29412</b>
<h3>Example 1: Find Mean Along Different Axes</h3>
We can use <b>axis=0</b> to find the mean of each column in the DataFrame:
<b>#find mean of each column
df.mean(axis=0)
points      20.250
assists      7.750
rebounds     8.375
dtype: float64
</b>
The output shows the mean value of each numeric column in the DataFrame.
Notice that pandas automatically avoids calculating the mean of the ‘team’ column because it’s a character column.
We can also use <b>axis=1</b> to find the mean of each row in the DataFrame:
<b>#find mean of each row
df.mean(axis=1)
0    13.666667
1     9.000000
2    10.666667
3     9.666667
4    12.333333
5    12.333333
6    14.333333
7    15.000000
dtype: float64</b>
From the output we can see:
The mean value in the first row is <b>13.667</b>.
The mean value in the second row is <b>9.000</b>.
The mean value in the third row is <b>10.667</b>.
And so on.
<h3>Example 2: Find Sum Along Different Axes</h3>
We can use <b>axis=0</b> to find the sum of specific columns in the DataFrame:
<b>#find sum of 'points' and 'assists' columns
df[['points', 'assists']].sum(axis=0)
points     162
assists     62
dtype: int64</b>
We can also use <b>axis=1</b> to find the sum of each row in the DataFrame:
<b>#find sum of each row
df.sum(axis=1)
0    41
1    27
2    32
3    29
4    37
5    37
6    43
7    45
dtype: int64</b>
<h3>Example 3: Find Max Along Different Axes</h3>
We can use <b>axis=0</b> to find the max value of specific columns in the DataFrame:
<b>#find max of 'points', 'assists', and 'rebounds' columns
df[['points', 'assists', 'rebounds']].max(axis=0)
points      29
assists     12
rebounds    12
dtype: int64</b>
We can also use <b>axis=1</b> to find the max value of each row in the DataFrame:
<b>#find max of each row
df.max(axis=1)
0    25
1    12
2    15
3    14
4    19
5    23
6    25
7    29
dtype: int64</b>
From the output we can see:
The max value in the first row is <b>25</b>.
The max value in the second row is <b>12</b>.
The max value in the third row is <b>15</b>.
And so on.
<h2><span class="orange">How to Create Back to Back Stem-and-Leaf Plots</span></h2>
A  stem-and-leaf plot  is a type of plot that displays data by splitting up each value in a dataset into a “stem” and a “leaf.”
For example, suppose we have the following dataset:
<b>Dataset:</b> 12, 14, 18, 22, 22, 23, 25, 25, 28, 45, 47, 48
If we define the first digit in each value as the “stem” and the second digit as the “leaf” then we can create the following stem and leaf plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/stemLeafDecimal1-1.png">
An extension of the stem-and-leaf plot is the <b>back to back stem-and-leaf plot</b>, which can be used to display values for <b>two datasets</b>.
This tutorial explains how to create a back to back stem-and-leaf plot along with how to use it to answer questions about two datasets.
<h3>How to Create a Back to Back Stem-and-Leaf Plot</h3>
Suppose we have the following two datasets that show the number of points scored by members of two basketball teams:
<b>Mavericks:</b> 2, 4, 8, 12, 12, 12, 15, 19, 23, 25, 31, 35, 38
<b>Lakers:</b> 6, 6, 7, 12, 13, 15, 16, 20, 22, 24, 28, 30, 31
To create a back to back stem-and-leaf plot for these datasets, we can create a single “stem” that represents the first digit in each value and a set of “leaves” that branch off to each side of the stem that represent the second digit in each value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/backbackstem1.png">
The points scored by the members of the Mavericks are shown on the right side of the stem and the points scored by the members of the Lakers are shown on the left side of the stem.
Notice that there are 13 individual values shown on each side of the stem, which represent the 13 data values for each dataset.
A back to back stem-and-leaf plot is useful because it allows us to easily visualize the distribution of values in two datasets at the same time and quickly compare two distributions. 
<h3>How to Interpret a Back to Back Stem-and-Leaf Plot</h3>
Once we have a back to back stem-and-leaf plot, we can use it to answer questions about each the two datasets shown in the plot.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/backbackstem1.png">
<b>Question 1:</b> What is the range for number of points scored for each team?
<em>Recall that the range is the difference between the largest and smallest value.</em>
Range for the Mavericks: 38 – 2 = 36
Range for the Lakers: 31 – 6 = 25
<b>Question 2:</b> What is the mode for number of points scored for each team?
<em>Recall that the mode is the value that occurs most often.</em>
Mode for the Mavericks: 12
Mode for the Lakers: 6
<b>Question 3:</b> What is the median for number of points scored for each team?
<em>Recall that the median is the “middle” value in a dataset.</em>
Median for the Mavericks: 15
Median for the Lakers: 16
<b>Question 4:</b> Which team had more players score 20 or more points?
Players who scored 20 or more for the Mavericks: 5
Players who scored 20 or more for the Lakers: 6
The Lakers had more players score 20 or more points.
<b>Question 5:</b> Which team had the highest-scoring player?
Highest scorer for the Mavericks: 38
Highest scorer for the Lakers: 31
The Mavericks had the highest-scoring player.
<h2><span class="orange">What is Backward Selection? (Definition & Example)</span></h2>
In statistics, <b>stepwise selection</b> is a procedure we can use to build a  regression model  from a set of predictor variables by entering and removing predictors in a stepwise manner into the model until there is no statistically valid reason to enter or remove any more.
The goal of stepwise selection is to build a regression model that includes all of the predictor variables that are statistically significantly related to the  response variable .
One of the most commonly used stepwise selection methods is known as <b>backward selection</b>, which works as follows:
<b>Step 1:</b> Fit a regression model using all <em>p</em> predictor variables. Calculate the AIC<b>*</b> value for the model.
<b>Step 2:</b> Remove the predictor variable that leads to the largest reduction in AIC and also leads to a statistically significant reduction in AIC compared to the model with all <em>p</em> predictor variables.
<b>Step 3:</b> Remove the predictor variable that leads to the largest reduction in AIC and also leads to a statistically significant reduction in AIC compared to the model with <em>p-1</em> predictor variables.
Repeat the process until removing any predictor variable no longer longer leads to a statistically significant reduction in AIC.
<b>*</b>There are several metrics you could use to calculate the quality of fit of a regression model including cross-validation prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>. In the example below we choose to use AIC.
The following example shows how to perform backward selection in R.
<h3>Example: Backward Selection in R</h3>
For this example we’ll use the built-in  mtcars dataset  in R:
<b>#view first six rows of <em>mtcars
</em>head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
We will fit a multiple linear regression model using <em>mpg </em>(miles per gallon) as our response variable and all of the other 10 variables in the dataset as potential predictors variables.
The following code shows how to perform backward stepwise selection:
<b>#define intercept-only model
intercept_only &lt;- lm(mpg ~ 1, data=mtcars)
#define model with all predictors
all &lt;- lm(mpg ~ ., data=mtcars)
#perform backward stepwise regression
backward &lt;- step(all, direction='backward', scope=formula(all), trace=0)
#view results of backward stepwise regression
backward$anova
    Step Df   Deviance Resid. Df Resid. Dev      AIC
1        NA         NA        21   147.4944 70.89774
2  - cyl  1 0.07987121        22   147.5743 68.91507
3   - vs  1 0.26852280        23   147.8428 66.97324
4 - carb  1 0.68546077        24   148.5283 65.12126
5 - gear  1 1.56497053        25   150.0933 63.45667
6 - drat  1 3.34455117        26   153.4378 62.16190
7 - disp  1 6.62865369        27   160.0665 61.51530
8   - hp  1 9.21946935        28   169.2859 61.30730
#view final model
backward$coefficients
(Intercept)          wt        qsec          am 
   9.617781   -3.916504    1.225886    2.935837
</b>
Here is how to interpret the results:
First, we fit a model using all 10 predictor variables and calculate the AIC of the model.
Next, we removed the variable (<b>cyl</b>) that lead to the greatest reduction in AIC and also had a statistically significant reduction in AIC compared to the 10-predictor variable model.
Next, we removed the variable (<b>vs</b>) that lead to the greatest reduction in AIC and also had a statistically significant reduction in AIC compared to the 9-predictor variable model.
Next, we removed the variable (<b>carb</b>) that lead to the greatest reduction in AIC and also had a statistically significant reduction in AIC compared to the 8-predictor variable model.
We repeated this process until removing any variable no longer led to a statistically significant reduction in AIC.
The final model turns out to be:
<b>mpg = 9.62 – 3.92*wt + 1.23*qsec + 2.94*am</b>
<h3>A Note on Using AIC</h3>
In the previous example, we chose to use AIC as the metric for evaluating the fit of various regression models.
AIC stands for <b>Akaike information criterion</b> and is calculated as:
AIC = 2K – 2<em>ln</em>(L)
where:
<b>K:</b> The number of model parameters.
<b><em>ln</em>(L)</b>: The log-likelihood of the model. This tells us how likely the model is, given the data.
However, there are other metrics you might choose to use to evaluate the fit of regression models including cross-validation prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>.
Fortunately, most statistical software allows you to specify which metric you would like to use when performing backward selection.
<h2><span class="orange">How to Perform Bagging in R (Step-by-Step)</span></h2>
When we create a  decision tree  for a given dataset, we only use one training dataset to build the model.
However, the downside of using a single decision tree is that it tends to suffer from  high variance . That is, if we split the dataset into two halves and apply the decision tree to both halves, the results could be quite different.
One method that we can use to reduce the variance of a single decision tree is known as  bagging , sometimes referred to as <em>bootstrap aggregating</em>.
Bagging works as follows:
<b>1.</b> Take <em>b</em> bootstrapped samples from the original dataset.
<b>2. </b>Build a decision tree for each bootstrapped sample.
<b>3. </b>Average the predictions of each tree to come up with a final model.
Through building hundreds or even thousands of individual decision trees and taking the average predictions from all of the trees, we often end up with a fitted bagged model that produces a much lower test error rate compared to a single decision tree.
This tutorial provides a step-by-step example of how to create a bagged model in R.
<h3>Step 1: Load the Necessary Packages</h3>
First, we’ll load the necessary packages for this example:
<b>library(dplyr)       #for data wrangling
library(e1071)       #for calculating variable importance
library(caret)       #for general model fitting
library(rpart)       #for fitting decision trees
library(ipred)       #for fitting bagged decision trees
</b>
<h3>Step 2: Fit the Bagged Model</h3>
For this example, we’ll use a built-in R dataset called <b>airquality</b> which contains air quality measurements in New York on 153 individual days.
<b>#view structure of airquality dataset
str(airquality)
'data.frame':153 obs. of  6 variables:
 $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...
 $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...
 $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...
 $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...
 $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...
 $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...
</b>
The following code shows how to fit a bagged model in R using the <b>bagging()</b> function from the <b>ipred</b> library.
<b>#make this example reproducible
set.seed(1)
#fit the bagged model
bag &lt;- bagging(
  formula = Ozone ~ .,
  data = airquality,
  nbagg = 150,   
  coob = TRUE,
  control = rpart.control(minsplit = 2, cp = 0)
)
#display fitted bagged model
bag
Bagging regression trees with 150 bootstrap replications 
Call: bagging.data.frame(formula = Ozone ~ ., data = airquality, nbagg = 150, 
    coob = TRUE, control = rpart.control(minsplit = 2, cp = 0))
Out-of-bag estimate of root mean squared error:  17.4973 </b>
Note that we chose to use <b>150</b> bootstrapped samples to build the bagged model and we specified <b>coob</b> to be <b>TRUE</b> to obtain the estimated out-of-bag error.
We also used the following specifications in the <b>rpart.control()</b> function:
<b>minsplit = 2: </b>This tells the model to only require 2 observations in a node to split.
<b>cp = 0</b>. This is the complexity parameter. By setting it to 0, we don’t require the model to be able to improve the overall fit by any amount in order to perform a split.
Essentially these two arguments allow the individual trees to grow extremely deep, which leads to trees with high variance but low bias. Then when we apply bagging we’re able to reduce the variance of the final model while keeping the bias low.
From the output of the model we can see that the out-of-bag estimated RMSE is <b>17.4973</b>. This is the average difference between the predicted value for Ozone and the actual observed value.
<h3>Step 3: Visualize the Importance of the Predictors</h3>
Although bagged models tend to provide more accurate predictions compared to individual decision trees, it’s difficult to interpret and visualize the results of fitted bagged models. 
We can, however, visualize the importance of the predictor variables by calculating the total reduction in RSS (residual sum of squares) due to the split over a given predictor, averaged over all of the trees. The larger the value, the more important the predictor.
The following code shows how to create a variable importance plot for the fitted bagged model, using the <b>varImp()</b> function from the <b>caret</b> library:
<b>#calculate variable importance
VI &lt;- data.frame(var=names(airquality[,-1]), imp=varImp(bag))
#sort variable importance descending
VI_plot &lt;- VI[order(VI$Overall, decreasing=TRUE),]
#visualize variable importance with horizontal bar plot
barplot(VI_plot$Overall,
        names.arg=rownames(VI_plot),
        horiz=TRUE,
        col='steelblue',
        xlab='Variable Importance')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/bag1.png">
We can see that <b>Solar.R</b> is the most importance predictor variable in the model while <b>Month</b> is the least important.
<h3>Step 4: Use the Model to Make Predictions</h3>
Lastly, we can use the fitted bagged model to make predictions on new observations.
<b>#define new observation
new &lt;- data.frame(Solar.R=150, Wind=8, Temp=70, Month=5, Day=5)
#use fitted bagged model to predict Ozone value of new observation
predict(bag, newdata=new)
24.4866666666667
</b>
Based on the values of the predictor variables, the fitted bagged model predicts that the Ozone value will be <b>24.487</b> on this particular day.
The complete R code used in this example can be found  here .
<h2><span class="orange">An Introduction to Bagging in Machine Learning</span></h2>
When the relationship between a set of predictor variables and a  response variable  is linear, we can use methods like  multiple linear regression  to model the relationship between the variables.
However, when the relationship is more complex then we often need to rely on non-linear methods. 
One such method is  classification and regression trees  (often abbreviated CART), which use a set of predictor variables to build <em>decision trees</em> that predict the value of a response variable.
<figure id="attachment_12094" aria-describedby="caption-attachment-12094" style="width: 453px"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/tree3.png"><figcaption id="caption-attachment-12094"><b>Example of a regression tree that uses years of experience and average home runs to predict the salary of a professional baseball player.</b></figcaption></figure>However, the downside of CART models is that they tend to suffer from  high variance . That is, if we split a dataset into two halves and apply a decision tree to both halves, the results could be quite different.
One method that we can use to reduce the variance of CART models is known as <b>bagging</b>, sometimes referred to as <em>bootstrap aggregating</em>.
<h3>What is Bagging?</h3>
When we create a single decision tree, we only use one training dataset to build the model.
However, <b>bagging</b> uses the following method:
<b>1.</b> Take <em>b</em> bootstrapped samples from the original dataset.
Recall that a <em>bootstrapped sample</em> is a sample of the original dataset in which the observations are taken with replacement.
<b>2. </b>Build a decision tree for each bootstrapped sample.
<b>3. </b>Average the predictions of each tree to come up with a final model.
For regression trees, we take the average of the prediction made by the <em>B</em> trees.
For classification trees, we take the most commonly occurring prediction made by the <em>B</em> trees.
Bagging can be used with any machine learning algorithm, but it’s particularly useful for decision trees because they inherently have high variance and bagging is able to dramatically reduce the variance, which leads to lower test error.
To apply bagging to decision trees, we grow <em>B </em>individual trees deeply without pruning them. This results in individual trees that have high variance, but low bias. Then when we take the average predictions from these trees we’re able to reduce the variance.
In practice, optimal performance typically occurs with 50 to 500 trees, but it’s possible to fit thousands of trees to produce a final model.
Just keep in mind that fitting more trees will require more computational power, which may or may not be an issue depending on the size of the dataset.
<h3>Out-of-Bag Error Estimation</h3>
It turns out that we can calculate the test error of a bagged model without relying on  k-fold cross-validation .
The reason is because it can be shown that each bootstrapped sample contains about 2/3 of the observations from the original dataset. The remaining 1/3 of the observations not used to fit the bagged tree are referred to as <b>out-of-bag (OOB) observations</b>. 
We can predict the value for the ith observation in the original dataset by taking the average prediction from each of the trees in which that observation was OOB.
We can use this approach to make a prediction for all <em>n</em> observations in the original dataset and thus calculate an error rate, which is a valid estimate of the test error.
The benefit of using this approach to estimate the test error is that it’s much quicker than k-fold cross-validation, especially when the dataset is large.
<h3>Understanding the Importance of Predictors</h3>
Recall that one of the benefits of decision trees is that they’re easy to interpret and visualize.
When we instead use bagging, we’re no longer able to interpret or visualize an individual tree since the final bagged model is the resulting of averaging many different trees. We gain prediction accuracy at the expense of interpretability.
However, we can still understand the importance of each predictor variable by calculating the total reduction in RSS (residual sum of squares) due to the split over a given predictor, averaged over all <em>B</em> trees. The larger the value, the more important the predictor.
<figure id="attachment_12115" aria-describedby="caption-attachment-12115" style="width: 411px"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/bag1.png"caption-attachment-12115"><b>Example of a variable importance plot.</b></figcaption></figure>Similarly, for classification models we can calculate the total reduction in the Gini Index due to the split over a given predictor, averaged over all <em>B</em> trees. The larger the value, the more important the predictor.
So, although we can’t exactly interpret a final bagged model we can still get an idea of how important each predictor variable is when predicting the response.
<h3>Going Beyond Bagging</h3>
The benefit of bagging is that it typically offers an improvement in test error rate compared to a single decision tree.
The downside is that the predictions from the collection of bagged trees can be highly correlated if there happens to be a very strong predictor in the dataset.
In this case, most or all of the bagged trees will use this predictor for the first split, which will result in trees that are similar to each other and have highly correlated predictions.
One way to get around this issue is to instead use random forests, which use a similar method as bagging but are able to produce decorrelated trees, which often leads to lower test error rates.
You can read a simple introduction to random forests  here .
<h2><span class="orange">How to Calculate Balanced Accuracy in Python Using sklearn</span></h2>
<b>Balanced accuracy</b> is a metric we can use to assess the performance of a  classification model .
It is calculated as:
<b>Balanced accuracy</b> = (Sensitivity + Specificity) / 2
where:
<b>Sensitivity</b>: The “true positive rate” – the percentage of positive cases the model is able to detect.
<b>Specificity</b>: The “true negative rate” – the percentage of negative cases the model is able to detect.
This metric is particularly useful when the two classes are imbalanced – that is, one class appears much more than the other.
For example, suppose a sports analyst uses a  logistic regression model  to predict whether or not 400 different college basketball players get drafted into the NBA.
The following confusion matrix summarizes the predictions made by the model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/balanced1.png">
To calculate the balanced accuracy of the model, we’ll first calculate the sensitivity and specificity:
<b>Sensitivity</b>: The “true positive rate” = 15 / (15 + 5) = 0.75
<b>Specificity</b>: The “true negative rate” = 375 / (375 + 5) = 0.9868
We can then calculate the balanced accuracy as:
Balanced accuracy = (Sensitivity + Specificity) / 2
Balanced accuracy = (0.75 + 9868) / 2
Balanced accuracy = 0.8684
The balanced accuracy for the model turns out to be <b>0.8684</b>.
The following example shows how to calculate the balanced accuracy for this exact scenario using the <b>balanced_accuracy_score()</b> function from the <b>sklearn</b> library in Python.
<h3>Example: Calculating Balanced Accuracy in Python</h3>
The following code shows how to define an array of predicted classes and an array of actual classes, then calculate the balanced accuracy of a model in Python:
<b>import numpy as np
from sklearn.metrics import balanced_accuracy_score
#define array of actual classes
actual = np.repeat([1, 0], repeats=[20, 380])
#define array of predicted classes
pred = np.repeat([1, 0, 1, 0], repeats=[15, 5, 5, 375])
#calculate balanced accuracy score
balanced_accuracy_score(actual, pred)
0.868421052631579</b>
The balanced accuracy is <b>0.8684</b>. This matches the value that we calculated earlier by hand.
<b>Note</b>: You can find the complete documentation for the <b>balanced_accuracy_score()</b> function  here .
<h2><span class="orange">What is Balanced Accuracy? (Definition & Example)</span></h2>
<b>Balanced accuracy</b> is a metric we can use to assess the performance of a  classification model .
It is calculated as:
<b>Balanced accuracy</b> = (Sensitivity + Specificity) / 2
where:
<b>Sensitivity</b>: The “true positive rate” – the percentage of positive cases the model is able to detect.
<b>Specificity</b>: The “true negative rate” – the percentage of negative cases the model is able to detect.
This metric is particularly useful when the two classes are imbalanced – that is, one class appears much more than the other.
The following example shows how to calculate balanced accuracy in practice and demonstrates why it’s such a useful metric.
<h3>Example: Calculating Balanced Accuracy</h3>
Suppose a sports analyst uses a  logistic regression model  to predict whether or not 400 different college basketball players get drafted into the NBA.
The following confusion matrix summarizes the predictions made by the model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/balanced1.png">
To calculate the balanced accuracy of the model, we’ll first calculate the sensitivity and specificity:
<b>Sensitivity</b>: The “true positive rate” = 15 / (15 + 5) = 0.75
<b>Specificity</b>: The “true negative rate” = 375 / (375 + 5) = 0.9868
We can then calculate the balanced accuracy as:
Balanced accuracy = (Sensitivity + Specificity) / 2
Balanced accuracy = (0.75 + 9868) / 2
Balanced accuracy = 0.8684
The balanced accuracy for the model turns out to be <b>0.8684</b>.
Note that the closer the balanced accuracy is to 1, the better the model is able to correctly classify observations.
In this example, the balanced accuracy is quite high which tells us that the logistic regression model does a pretty good job of predicting whether or not college players will get drafted into the NBA.
In this scenario, since the classes are so imbalanced (20 players got drafted and 380 players did not) the balanced accuracy gives us a more realistic picture of how well the model performs compared to an overall accuracy metric.
For example, we would calculate the accuracy of the model as:
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Accuracy = (15 + 375) / (15 + 375 + 5 + 5)
Accuracy = 0.975
The accuracy of the model is <b>0.975</b>, which sounds extremely high.
However, consider a model that just predicts every player to not get drafted. It would have an accuracy of 380 / 400 = <b>0.95</b>. This is only slightly lower than the accuracy of our model.
The balanced accuracy score of <b>0.8684</b> gives us a better idea of how well the model is able to predict both classes.
That is, it gives us a better idea of how well the model is able to predict players who won’t get drafted <em>and</em> those who will get drafted.
<h2><span class="orange">Balanced vs. Unbalanced Designs: What’s the Difference?</span></h2>
In statistics, ANOVA (“analysis of variance”) models are used to determine whether or not the means of different treatment levels are equal.
An ANOVA has a <b>balanced design</b> if the sample sizes are equal across all treatment combinations.
Conversely, an ANOVA has an <b>unbalanced design</b> if the sample sizes are <em>not</em> equal across all treatment combinations.
For example, suppose we want to perform a  one-way ANOVA  to determine if three different fertilizers cause the same mean growth in plants.
The following graphic shows an example of a balanced and unbalanced design for this one-way ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/balance1.png">
In the balanced design, there are an equal number of plants in each treatment. In the unbalanced design, there are unequal sample sizes.
Or suppose we want to perform a  two-way ANOVA  to determine if different combinations of fertilizer and sunlight cause the same mean growth in plants.
The following graphic shows and example of a balanced and unbalanced design for this two-way ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/balance2.png">
<b>Related:</b>  One-Way vs. Two-Way ANOVA: When to Use Each 
<h3>Why is a Balanced Design Preferred?</h3>
Balanced designs offer the following advantages over unbalanced designs:
<b>1.</b> The power of an ANOVA is highest when sample sizes are equal across all treatment combinations. When the power is highest, we have the best chance of detecting differences among the means across treatment combinations when the means truly are different.
<b>2.</b> The overall F-statistic of the ANOVA is less sensitive to violations of the  assumption of equal variance .
<h3>How do Unbalanced Designs Occur?</h3>
Even if researchers attempt to set up a balanced design for an ANOVA, there are several reasons why an unbalanced design could occur, including:
Individuals may decide to opt out of a study halfway through
Plants may simply die during the course of the study
A manufacturing plant may shut down and not be able to deliver certain components needed for a study.
There are tons of reasons why an experiment may suddenly become an unbalanced design.
<h3>How to Handle Unbalanced Designs</h3>
As mentioned earlier, balanced designs are preferred because they offer higher statistical power and more reliable test statistics.
However, if you do have to perform an experiment using an unbalanced design, you have three choices:
<b>1. Proceed with an ANOVA anyway.</b>
If the sample sizes across treatment combinations are not equal, but the assumption of equal variances is met, you can still proceed to perform an ANOVA anyway.
It’s well-known that ANOVA’s are fairly robust to unequal sample sizes if the variances across each treatment combination are still equal.
<b>2. Impute missing values.</b>
If there are only slight differences among sample sizes between treatment combinations, you could impute missing values using the mean or median of the treatment levels.
However, this approach should be used with caution and should only be used when sample sizes are nearly equal to begin with.
<b>3. Perform a non-parametric test.</b>
If the sample sizes are not equal and the assumption of equal variances is violated, you could instead perform a non-parametric equivalent to an ANOVA such as the  Kruskal-Wallis test .
This type of test is much more robust to unequal sample sizes and unequal variances across treatment combinations.
<h2><span class="orange">How to Create a Bar of Pie Chart in Excel (With Example)</span></h2>
A <b>bar of pie chart</b> is a pie chart that combines the smallest slices in the chart into one slice and then explodes that slice into a bar chart.
The benefit of this type of chart is that it provides an easier way to visualize the smallest slices of the pie chart.
This tutorial provides a step-by-step example of how to create the following bar of pie chart in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar1.jpg">
<h2>Step 1: Enter the Data</h2>
First, let’s enter the following dataset that shows the total sales made by 10 different employees at a company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar2.jpg"413">
<h2>Step 2: Create a Bar of Pie Chart</h2>
To create a pie of bar chart to visualize this dataset, highlight the cell range <b>A1:B11</b>, then click the <b>Insert</b> tab along the top ribbon, then click the Pie icon and then click <b>Bar of Pie</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar3.jpg"671">
Excel will automatically insert the following bar of pie chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar4.jpg"669">
<h2>Step 3: Customize the Bar of Pie Chart</h2>
By default, Excel has chosen to group the four smallest slices in the pie into one slice and then explode that slice into a bar chart.
To group together a different number of slices, simply double click any element in the bar chart.
This will bring up a panel called <b>Format Data Series</b>.
Suppose we change the number for <b>Values in second plot</b> to <b>2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar5.jpg"318">
The number of values shown in the bar chart will change to <b>2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar6.jpg"590">
We can also choose to group together all slices in the pie that are less than a certain percentage of the total pie.
For example, we could group together all slices that make up less than 10% of the total pie:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar7.jpg"338">
Excel will automatically calculate the number of slices to include in the bar chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar8.jpg"657">
In this dataset, the total Sales values sum to <b>116</b>. Thus, 10% of 116 is <b>11.6</b>
This means that any employee with less than <b>11.6</b> sales is automatically included in the bar chart.
Once you’ve decided how many slices to include in the bar chart, click anywhere on the chart to bring up the tiny plus “<b>+</b>” sign in the top right corner and check the box next to <b>Data Labels</b> to add labels to the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/piebar9.jpg"669">
The bar of pie chart is now complete.
<h2>Additional Resources</h2>
The following tutorials explain how to create other common visualizations in Excel:
 How to Create a Quadrant Chart in Excel 
 How to Create a Bubble Chart in Excel 
 How to Create a Double Doughnut Chart in Excel 
<h2><span class="orange">Bartlett’s Test Calculator</span></h2>
<b>Bartlett’s test</b> is used to test if samples are from populations with equal variances. Some statistical tests, like the  One-Way ANOVA , assume that variances are equal across samples. Bartlett’s test can be used to verify that assumption.
To conduct a Bartlett’s test for up to five samples, simply enter the data values below and click the “Calculate” button. A test statistic and corresponding p-value will be shown below. A p-value less than 0.05 is strong evidence that the variances are not equal across the samples.
Sample 1 || Sample 2 || Sample 3 || Sample 4 || Sample 5
<textarea id="a" name="a" rows="15" cols="4"></textarea><textarea id="b" name="b" rows="15" cols="4"></textarea><textarea id="c" name="c" rows="15" cols="4"></textarea><textarea id="d" name="d" rows="15" cols="4"></textarea><textarea id="e" name="e" rows="15" cols="4"></textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<div>
<div>
<script>
//create function that performs t test calculations
function calc() {
    
//define addition function
function add(a, b) {
    return a + b;
}
    
//get raw data
    //group a
    //    var values_a_input = document.getElementsByClassName('a');
//var values_a_array = [];
//for (var i = 0; i < values_a_input.length; i++) {
    //  values_a_array[i] = values_a_input[i].innerText;
    //    }
//values_a_array = values_a_array.filter(n => n);
//var group_a = values_a_array.map(Number);
if(document.getElementById("a").value == '') {
    var group_a = '';
} else {
    var group_a = document.getElementById('a').value.match(/\d+/g).map(Number);
}
if(document.getElementById("b").value == '') {
    var group_b = '';
} else {
    var group_b = document.getElementById('b').value.match(/\d+/g).map(Number);
}
if(document.getElementById("c").value == '') {
    var group_c = '';
} else {
    var group_c = document.getElementById('c').value.match(/\d+/g).map(Number);
}
if(document.getElementById("d").value == '') {
    var group_d = '';
} else {
    var group_d = document.getElementById('d').value.match(/\d+/g).map(Number);
}
if(document.getElementById("e").value == '') {
    var group_e = '';
} else {
    var group_e = document.getElementById('e').value.match(/\d+/g).map(Number);
}
var all_groups = (group_a.concat(group_b, group_c, group_d, group_e)).filter(n => n);
//get summary stats of each group
if (group_a.length > 0) { var mean_group_a = math.mean(group_a); };
if (group_b.length > 0) { var mean_group_b = math.mean(group_b); };
if (group_c.length > 0) { var mean_group_c = math.mean(group_c); };
if (group_d.length > 0) { var mean_group_d = math.mean(group_d); };
if (group_e.length > 0) { var mean_group_e = math.mean(group_e); };
if (group_a.length > 0) { var var_group_a = math.var(group_a); };
if (group_b.length > 0) { var var_group_b = math.var(group_b); };
if (group_c.length > 0) { var var_group_c = math.var(group_c); };
if (group_d.length > 0) { var var_group_d = math.var(group_d); };
if (group_e.length > 0) { var var_group_e = math.var(group_e); };
if (group_a.length > 0) { var lnvar_group_a = Math.log(var_group_a); } else { var lnvar_group_a = 0;};
if (group_b.length > 0) { var lnvar_group_b = Math.log(var_group_b); } else { var lnvar_group_b = 0;};
if (group_c.length > 0) { var lnvar_group_c = Math.log(var_group_c); } else { var lnvar_group_c = 0;};
if (group_d.length > 0) { var lnvar_group_d = Math.log(var_group_d); } else { var lnvar_group_d = 0;};
if (group_e.length > 0) { var lnvar_group_e = Math.log(var_group_e); } else { var lnvar_group_e = 0;};
if (group_a.length > 0) { var df_group_a = group_a.length-1; } else { var df_group_a = 0;};
if (group_b.length > 0) { var df_group_b = group_b.length-1; } else { var df_group_b = 0;};
if (group_c.length > 0) { var df_group_c = group_c.length-1; } else { var df_group_c = 0;};
if (group_d.length > 0) { var df_group_d = group_d.length-1; } else { var df_group_d = 0;};
if (group_e.length > 0) { var df_group_e = group_e.length-1; } else { var df_group_e = 0;};
var total_df = [df_group_a, df_group_b, df_group_c, df_group_d, df_group_e].reduce(add, 0);
var total_df1 = 1 / total_df;
var lnvar_array = [lnvar_group_a, lnvar_group_b, lnvar_group_c, lnvar_group_d, lnvar_group_e];
var df_array = [df_group_a, df_group_b, df_group_c, df_group_d, df_group_e];
var lnvar_df_array = []
for (var i = 0; i < lnvar_array.length; i++) {
    lnvar_df_array.push(lnvar_array[i] * df_array[i]);
}
var sum_lnvar_df_array = lnvar_df_array.reduce(add, 0);
if (group_a.length > 0) { var flag_group_a = 1; } else { var flag_group_a = 0;};
if (group_b.length > 0) { var flag_group_b = 1; } else { var flag_group_b = 0;};
if (group_c.length > 0) { var flag_group_c = 1; } else { var flag_group_c = 0;};
if (group_d.length > 0) { var flag_group_d = 1; } else { var flag_group_d = 0;};
if (group_e.length > 0) { var flag_group_e = 1; } else { var flag_group_e = 0;};
if (group_a.length > 0) { var df1_group_a = 1/df_group_a; } else { var df1_group_a = 0;};
if (group_b.length > 0) { var df1_group_b = 1/df_group_b; } else { var df1_group_b = 0;};
if (group_c.length > 0) { var df1_group_c = 1/df_group_c; } else { var df1_group_c = 0;};
if (group_d.length > 0) { var df1_group_d = 1/df_group_d; } else { var df1_group_d = 0;};
if (group_e.length > 0) { var df1_group_e = 1/df_group_e; } else { var df1_group_e = 0;};
var df1_array = [df1_group_a, df1_group_b, df1_group_c, df1_group_d, df1_group_e];
var df1_array_sum = df1_array.reduce(add, 0);
var treatments = [flag_group_a, flag_group_b, flag_group_c, flag_group_d, flag_group_e].reduce(add, 0) - 1;
if (treatments == 1) {
    var total_var = jStat.pooledvariance([group_a, group_b]);
    var total_lnvar = Math.log(total_var);
} else if (treatments == 2) {
   var total_var = jStat.pooledvariance([group_a, group_b, group_c]);
   var total_lnvar = Math.log(total_var);
} else if (treatments == 3) {
   var total_var = jStat.pooledvariance([group_a, group_b, group_c, group_d]);
   var total_lnvar = Math.log(total_var);
} else if (treatments == 4) {
   var total_var = jStat.pooledvariance([group_a, group_b, group_c, group_d, group_e]);
   var total_lnvar = Math.log(total_var);
}
//calculate test statistic
    var b_numerator = total_df * total_lnvar - sum_lnvar_df_array;
    var b_denominator = 1 - (-1 / (3*treatments)) * (df1_array_sum - total_df1);
    var b = b_numerator / b_denominator;
    var p = 1 - jStat.chisquare.cdf(b, treatments);
console.log(df1_array, lnvar_array, total_df, total_var, total_lnvar, total_df1);
//output results
document.getElementById('B').innerHTML = "Test Statistic B: " + b.toFixed(5);
document.getElementById('p').innerHTML = "p-value: " + p.toFixed(5);
}
</script>
<h2><span class="orange">How to Perform Bartlett’s Test in R (Step-by-Step)</span></h2>
 Bartlett’s test  is a statistical test that is used to determine whether or not the variances between several groups are equal.
Many statistical tests (like a  one-way ANOVA ) assume that variances are equal across samples. Bartlett’s test can be used to verify that assumption.
This test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>:</b> The variance among each group is equal.
<b>H<sub>A</sub>:</b> At least one group has a variance that is not equal to the rest.
The test statistic follows a Chi-Square distribution with <em>k-1</em> degrees of freedom where <em>k</em> is the number of groups.
If the corresponding  p-value  of the test statistic is less than some significance level (like α = .05) then we can reject the null hypothesis and conclude that not all groups have the same variance.
The following step-by-step example explains how to perform Bartlett’s test in R.
<h3>Step 1: Create the Data</h3>
To determine if three different studying techniques lead to different exam scores, a professor randomly assigns 10 students to use each technique (Technique A, B, or C) for one week and then makes each student take an exam of equal difficulty. 
The exam scores of the 30 students are shown below:
<b>#create data frame
df &lt;-data.frame(group = rep(c('A','B', 'C'), each=10),
                score = c(85, 86, 88, 75, 78, 94, 98, 79, 71, 80,          91, 92, 93, 85, 87, 84, 82, 88, 95, 96,          79, 78, 88, 94, 92, 85, 83, 85, 82, 81))
#view data frame
df
   group score
1      A    85
2      A    86
3      A    88
4      A    75
5      A    78
6      A    94
7      A    98
8      A    79
9      A    71
10     A    80
11     B    91
12     B    92
13     B    93
14     B    85
15     B    87
16     B    84
17     B    82
18     B    88
19     B    95
20     B    96
21     C    79
22     C    78
23     C    88
24     C    94
25     C    92
26     C    85
27     C    83
28     C    85
29     C    82
30     C    81</b>
<h3>Step 2: Perform Bartlett’s Test</h3>
To perform Bartlett’s test, we can use the <b>bartlett.test</b> function in base R, which uses the following syntax:
<b>bartlett.test(formula, data)</b>
Here’s how to use this function in our example:
<b>#perform Bartlett's test
bartlett.test(score ~ group, data = df)
Bartlett test of homogeneity of variances
data:  score by group
Bartlett's K-squared = 3.3024, df = 2, p-value = 0.1918</b>
The test returns the following results:
Test statistic <em>B</em>: <b>3.3024</b>
P-value: <b>0.1918</b>
Since the p-value is not less than 0.05, the professor will fail to reject the null hypothesis.
In other words, she doesn’t have sufficient evidence to say that the three groups have different variances.
Thus, she can proceed to perform the one-way ANOVA.
<h2><span class="orange">A Guide to Bartlett’s Test of Sphericity</span></h2>
<b>Bartlett’s Test of Sphericity</b> compares an observed correlation matrix to the identity matrix. Essentially it checks to see if there is a certain redundancy between the variables that we can summarize with a few number of factors. 
The null hypothesis of the test is that the variables are orthogonal, i.e. not correlated. The alternative hypothesis is that the variables are not orthogonal, i.e. they are correlated enough to where the correlation matrix diverges significantly from the identity matrix. 
This test is often performed before we use a data reduction technique such as principal component analysis or factor analysis to verify that a data reduction technique can actually compress the data in a meaningful way. 
<b>Note:</b> Bartlett’s Test of Sphericity is not the same as  Bartlett’s Test for Equality of Variances . This is a common confusion, since the two have similar names.
<h2>Correlation Matrix vs. Identity Matrix</h2>
A <b>correlation matrix</b> is simply a matrix of values that shows the correlation coefficients between variables. For example, the following correlation matrix shows the correlation coefficients between different variables for professional basketball teams.
<img class="lazy" data-src="https://i2.wp.com/fourpillarfreedom.com/wp-content/uploads/2018/12/NBA_4-1.png?resize=463%2C149&ssl=1">
 Correlation coefficients  can vary from -1 to 1. The further a value is from 0, the higher the correlation between two variables.
An<b> identity matrix</b> is a matrix in which all of the values along the diagonal are 1 and all of the other values are 0. 
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/identity_matrix.jpg">
In this case, if the numbers in this matrix represent correlation coefficients it means that each variable is perfectly orthogonal (i.e. “uncorrelated”) to every other variable and thus a data reduction technique like PCA or factor analysis would not be able to “compress” the data in any meaningful way. 
Thus, the reason we conduct Bartlett’s Test of Sphericity is to make sure that the correlation matrix of the variables in our dataset diverges significantly from the identity matrix, so that we know a data reduction technique is suitable to use.
If the p-value from Bartlett’s Test of Sphericity is lower than our chosen significance level (common choices are 0.10, 0.05, and 0.01), then our dataset is suitable for a data reduction technique.
<h2>How to Conduct Bartlett’s Test of Sphericity in R</h2>
To conduct Bartlett’s Test of Sphericity in R, we can use the <b>cortest.bartlett()</b> function from the <b>psych </b>library. The general syntax for this function is as follows:
cortest.bartlett(R, n)
R: a correlation matrix of the dataset
n: sample size of the dataset
The following code illustrates how to conduct this test on a fake dataset we created:
<b>#make this example reproducible
set.seed(0)
#create fake data
data &lt;- data.frame(A = rnorm(50, 1, 4), B = rnorm(50, 3, 6), C = rnorm(50, 5, 8))
#view first six rows of data
head(data)
#           A          B           C
#1  6.0518171  4.5968242 11.25487348
#2 -0.3049334  0.7397837 -1.21421297
#3  6.3191971 17.6481878  0.07208074
#4  6.0897173 -1.7720347  5.37264242
#5  2.6585657  2.6707352 -4.04308622
#6 -5.1598002  4.5008479  9.61375026
#find correlation matrix of data
cor_matrix &lt;- cor(data)
#view correlation matrix
cor_matrix
#          A            B            C
#A 1.0000000 0.1600155667 0.2825308511
#B 0.1600156 1.0000000000 0.0005358384
#C 0.2825309 0.0005358384 1.0000000000
#load psych library
library(psych)
#perform Bartlett's Test of Sphericity
cortest.bartlett(cor_matrix, n = nrow(data))
#$chisq
#[1] 5.252329
#
#$p.value
#[1] 0.1542258
#
#$df
#[1] 3</b>
The Chi-Square test statistic is 5.252329 and the corresponding p-value is 0.1542258, which is not smaller than our significance level (let’s use 0.05). Thus, this data is likely not suitable for PCA or factor analysis. 
To put this in layman’s terms, the three variables in our dataset are fairly uncorrelated so a data reduction technique like PCA or factor analysis would have a hard time compressing these variables into linear combinations that are able to capture significant variance present in the data.
<footer>
<imghttps://secure.gravatar.com/avatar/4b7515422516b81d9f05258e8e2b8b76?s=68&d=mm&r=g"><b>Stefan</b> <span>says:
<!-- .comment-author -->
 <time datetime="2020-01-08T11:12:34-05:00">January 8, 2020 at 11:12 am</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
Correction: The further a value is from “0”, the higher the correlation between two variables.
<!-- .comment-content -->
 Reply </article>
<footer>
<imghttps://secure.gravatar.com/avatar/958f66fbe89e80753a8c8a0492bf5cc8?s=68&d=mm&r=g"><b>admin</b> <span>says:
<!-- .comment-author -->
 <time datetime="2020-01-09T15:30:25-05:00">January 9, 2020 at 3:30 pm</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
Thanks for the pointing this out! Just fixed the typo <U+0001F642>
<!-- .comment-content -->
 Reply </article>
<h2><span class="orange">How to Perform Bartlett’s Test in Python (Step-by-Step)</span></h2>
 Bartlett’s test  is a statistical test that is used to determine whether or not the variances between several groups are equal.
Many statistical tests (like a  one-way ANOVA ) assume that variances are equal across samples. Bartlett’s test can be used to verify that assumption.
This test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>:</b> The variance among each group is equal.
<b>H<sub>A</sub>:</b> At least one group has a variance that is not equal to the rest.
The test statistic follows a Chi-Square distribution with <em>k-1</em> degrees of freedom where <em>k</em> is the number of groups.
If the corresponding  p-value  of the test statistic is less than some significance level (like α = .05) then we can reject the null hypothesis and conclude that not all groups have the same variance.
The following step-by-step example explains how to perform Bartlett’s test in Python.
<h3>Step 1: Create the Data</h3>
To determine if three different studying techniques lead to different exam scores, a professor randomly assigns 10 students to use each technique (Technique A, B, or C) for one week and then makes each student take an exam of equal difficulty. 
The exam scores of the 30 students are shown below:
<b>#create data
A = [85, 86, 88, 75, 78, 94, 98, 79, 71, 80]
B = [91, 92, 93, 85, 87, 84, 82, 88, 95, 96]
C = [79, 78, 88, 94, 92, 85, 83, 85, 82, 81]
</b>
<h3>Step 2: Perform Bartlett’s Test</h3>
To perform Bartlett’s test, we can use the <b>scipy.stats.bartlett()</b> function.
Here’s how to use this function in our example:
<b>import scipy.stats as stats
#perform Bartlett's test 
stats.bartlett(A, B, C)
BartlettResult(statistic=3.30243757, pvalue=0.191815983)
</b>
The test returns the following results:
Test statistic <em>B</em>: <b>3.3024</b>
P-value: <b>0.1918</b>
Since the p-value is not less than 0.05, the professor will fail to reject the null hypothesis. In other words, she doesn’t have sufficient evidence to say that the three groups have different variances.
Thus, she can proceed to perform the one-way ANOVA.
<h2><span class="orange">Bartlett’s Test for Homogeneity of Variances (Definition & Example)</span></h2>
<b>Bartlett’s Test</b> is a statistical test that is used to determine whether or not the variances between several groups are equal.
Many statistical tests (like a  one-way ANOVA ) assume that variances are equal across samples. Bartlett’s test can be used to verify that assumption.
The following steps explain how to perform Bartlett’s test.
<em><b>Note:</b> Don’t confuse this test with  Bartlett’s Test of Sphericity , which is used to compare an observed correlation matrix to the identity matrix.</em>
<h3>Steps to Perform Bartlett’s Test</h3>
Bartlett’s Test uses the following null and alternative  hypotheses :
<b>H<sub>0</sub>:</b> The variance among each group is equal.
<b>H<sub>A</sub>:</b> At least one group has a variance that is not equal to the rest.
The test statistic can be calculated as follows:
<b>B = (n-k)lns<sup>2</sup> – Σ(n<sub>j</sub>-1)lns<sub>j</sub><sup>2</sup> / c</b>
where:
n: The total number of observations across all groups
k: The total number of groups
ln: This stands for “natural log”
s<sup>2</sup>: The pooled variance
n<sub>j</sub>: The number of observations in group j
s<sub>j</sub><sup>2</sup>: The variance of group j
And where <em>c</em> is calculated as:
c = 1 + (1/3(k-1))*(Σ(1/(n<sub>j</sub>-1)) – (1/(n-k))
This test statistic follows a Chi-Square distribution with k-1 degrees of freedom. That is, B ~ X<sup>2</sup>(k-1).
If the  p-value  that corresponds to the test statistic is less than some significance level (like α = 0.05) then we can reject the null hypothesis and conclude that not all groups have the same variance.
<h3>Example: Bartlett’s Test</h3>
Suppose a professor wants to know if three different studying techniques lead to different average exam scores.
She randomly assigns 10 students to use each technique for one week, then makes each student take an exam of equal difficulty. 
The exam scores of the 30 students are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/oneWay2.png">
The professor wants to conduct a one-way ANOVA to see if the three techniques lead to different average exam scores, but she first must conduct Bartlett’s Test to verify that the three groups have equal variances.
It’s cumbersome to perform Bartlett’s Test by hand, so we’ll enter the following data values into the  Bartlett’s Test Calculator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/bartlett1.png">
The test returns the following results:
Test statistic <em>B</em>: <b>3.30244</b>
P-value: <b>0.19182</b>
Since the p-value is not less than 0.05, the professor will fail to reject the null hypothesis. In other words, she doesn’t have sufficient evidence to say that the three groups have different variances.
Thus, she can proceed to perform the one-way ANOVA.
<h2><span class="orange">Bayes Factor: Definition + Interpretation</span></h2>
When we conduct a  hypothesis test , we typically end up with a p-value that we compare to some alpha level to decide if we should reject or fail to reject the null hypothesis.
For example, we may conduct a  two sample t-test  using an alpha level of 0.05 to determine if two population means are equal. Suppose we conduct the test and end up with a p-value of 0.0023. In this case, we would reject the null hypothesis that the two population means are equal since the p-value is less than our chosen alpha level.
P-values are a common metric used to reject or fail to reject some hypothesis, but there is another metric that can also be used: <b>Bayes Factor</b>.
Bayes Factor is defined as the ratio of the likelihood of one particular hypothesis to the likelihood of another hypothesis. Typically it is used to find the ratio of the likelihood of an alternative hypothesis to a null hypothesis:
<b>Bayes Factor = </b>likelihood of data given H<sub>A</sub> / likelihood of data given H<sub>0</sub>
For example, if the Bayes Factor is 5 then it means the alternative hypothesis is 5 times as likely as the null hypothesis given the data.
Conversely, if the Bayes Factor is 1/5 then it means that the null hypothesis is 5 times as likely as the alternative hypothesis given the data.
Similar to p-values, we can use thresholds to decide when we should reject a null hypothesis. For example, we may decide that a Bayes Factor of 10 or higher is strong enough evidence to reject the null hypothesis.
Lee and Wagenmaker proposed the following interpretations of Bayes Factor in a  2015 paper :
<table><tbody>
<tr>
<th style="text-align: center;"><b>Bayes Factor</b></th>
<th style="text-align: center;"><b>Interpretation</b></th>
</tr>
<tr>
<td style="text-align: center;">> 100</td>
<td style="text-align: center;">Extreme evidence for alternative hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">30 – 100</td>
<td style="text-align: center;">Very strong evidence for alternative hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">10 – 30</td>
<td style="text-align: center;">Strong evidence for alternative hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">3 – 10</td>
<td style="text-align: center;">Moderate evidence for alternative hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">1 – 3</td>
<td style="text-align: center;">Anecdotal evidence for alternative hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">No evidence</td>
</tr>
<tr>
<td style="text-align: center;">1/3 – 1</td>
<td style="text-align: center;">Anecdotal evidence for null hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">1/3 – 1/10</td>
<td style="text-align: center;">Moderate evidence for null hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">1/10 – 1/30</td>
<td style="text-align: center;">Strong evidence for null hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">1/30 – 1/100</td>
<td style="text-align: center;">Very strong evidence for null hypothesis</td>
</tr>
<tr>
<td style="text-align: center;">&lt; 1/100</td>
<td style="text-align: center;">Extreme evidence for null hypothesis</td>
</tr>
</tbody></table>
<h2>Bayes Factor vs. P-Values</h2>
Bayes Factor and p-values have different interpretations.
<b>P-values:</b>
A p-value is interpreted as the probability of obtaining results as extreme as the observed results of a hypothesis test, assuming that the null hypothesis is correct.
For example, suppose you conduct a two sample t-test to determine if two population means are equal. If the test results in a p-value of 0.0023, this means the probability of obtaining this result is just <b>0.0023 </b>if the two population means are actually equal. Because this value is so small, we reject the null hypothesis and conclude that we have sufficient evidence to say that the two population means aren’t equal.
<b>Bayes Factor:</b>
Bayes Factor is interpreted as the ratio of the likelihood of the observed data occurring under the alternative hypothesis to the likelihood of the observed data occurring under the null hypothesis. 
For example, suppose you conduct a hypothesis test and end up with a Bayes Factor of 4. This means the alternative hypothesis is 4 times as likely as the null hypothesis given the data that you actually observed.
<h2>Conclusion</h2>
Some statisticians believe that the Bayes Factor offers an advantage over p-values because it allows you to quantify the evidence for and against two competing hypotheses. For example, evidence can be quantified in favor of or against a null hypothesis, which can’t be done using a p-value.
No matter which approach you use – Bayes Factor or p-values – you still have to decide on a cut-off value if you wish to reject or fail to reject some null hypothesis.
For example, in the table above we saw that a Bayes Factor of 9 would be classified as “moderate evidence for the alternative hypothesis” while a Bayes Factor of 10 would be classified as “strong evidence for the alternative hypothesis.”
In this sense, the Bayes Factor suffers from the same problem as a p-value of 0.06 being considered “not significant” while a p-value of 0.05 may be considered significant.
<b>Further Reading:</b>
 An Explanation of P-Values and Statistical Significance 
 A Simple Explanation of Statistical vs. Practical Significance 
<h2><span class="orange">How to Apply Bayes’ Theorem in Excel</span></h2>
<b>Bayes’ Theorem</b> states the following for any two events <em>A</em> and <em>B</em><em>:</em>
P(A|B) = P(A)*P(B|A) / P(B)
where:
P(A|B): The probability of event A, given event B has occurred.
P(B|A): The probability of event B, given event A has occurred.
P(A): The probability of event A.
P(B): The probability of event B.
For example, suppose the probability of the weather being cloudy is 40%.  Also suppose the probability of rain on a given day is 20% and that the probability of clouds on a rainy day is 85%. 
If it’s cloudy outside on a given day, what is the probability that it will rain that day?
<b>Solution</b>:
P(cloudy) = 0.40
P(rain) = 0.20
P(cloudy | rain) = 0.85
Thus, we can calculate:
P(rain | cloudy) = P(rain) * P(cloudy | rain) / P(cloudy)
P(rain | cloudy) = 0.20 * 0.85 / 0.40
P(rain | cloudy) = 0.425
If it’s cloudy outside on a given day, the probability that it will rain that day is <b>0.425</b> or <b>42.5%</b>.
The following example shows how to solve this exact problem using Bayes’ Theorem in Excel.
<h3>Example: Bayes’ Theorem in Excel</h3>
The following formula shows how to apply Bayes’ Theorem in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bayesExcel1.png">
For example, if we know the following probabilities:
P(cloudy) = 0.40
P(rain) = 0.20
P(cloudy | rain) = 0.85
Then we can simply plug these into the cells in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bayesExcel2.png">
This tells us that if it’s cloudy outside on a given day, the probability that it will rain that day is <b>0.425</b> or <b>42.5%</b>.
<h2><span class="orange">How to Apply Bayes’ Theorem in Python</span></h2>
<b>Bayes’ Theorem</b> states the following for any two events <em>A</em> and <em>B</em><em>:</em>
P(A|B) = P(A)*P(B|A) / P(B)
where:
P(A|B): The probability of event A, given event B has occurred.
P(B|A): The probability of event B, given event A has occurred.
P(A): The probability of event A.
P(B): The probability of event B.
For example, suppose the probability of the weather being cloudy is 40%. 
Also suppose the probability of rain on a given day is 20%.
Also suppose the probability of clouds on a rainy day is 85%. 
If it’s cloudy outside on a given day, what is the probability that it will rain that day?
<b>Solution</b>:
P(cloudy) = 0.40
P(rain) = 0.20
P(cloudy | rain) = 0.85
Thus, we can calculate:
P(rain | cloudy) = P(rain) * P(cloudy | rain) / P(cloudy)
P(rain | cloudy) = 0.20 * 0.85 / 0.40
P(rain | cloudy) = 0.425
If it’s cloudy outside on a given day, the probability that it will rain that day is <b>42.5%</b>.
We can create the following simple function to apply Bayes’ Theorem in Python:
<b>def bayesTheorem(pA, pB, pBA):
    return pA * pBA / pB
</b>
The following example shows how to use this function in practice.
<h2>Example: Bayes’ Theorem in Python</h2>
Suppose we know the following probabilities:
P(rain) = 0.20
P(cloudy) = 0.40
P(cloudy | rain) = 0.85
To calculate P(rain | cloudy), we can use the following syntax:
<b>#define function for Bayes' theorem
def bayesTheorem(pA, pB, pBA):
    return pA * pBA / pB
#define probabilities
pRain = 0.2
pCloudy = 0.4
pCloudyRain = 0.85
#use function to calculate conditional probability
bayesTheorem(pRain, pCloudy, pCloudyRain)
0.425
</b>
This tells us that if it’s cloudy outside on a given day, the probability that it will rain that day is <b>0.425</b> or <b>42.5%</b>.
This matches the value that we calculated earlier by hand.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Calculate Conditional Probability in Python 
 How to Calculate Expected Value in Python 
 How to Calculate a Trimmed Mean in Python 
<h2><span class="orange">How to Apply Bayes’ Theorem in R</span></h2>
<b>Bayes’ Theorem</b> states the following for any two events <em>A</em> and <em>B</em><em>:</em>
P(A|B) = P(A)*P(B|A) / P(B)
where:
P(A|B): The probability of event A, given event B has occurred.
P(B|A): The probability of event B, given event A has occurred.
P(A): The probability of event A.
P(B): The probability of event B.
For example, suppose the probability of the weather being cloudy is 40%.  Also suppose the probability of rain on a given day is 20% and that the probability of clouds on a rainy day is 85%. 
If it’s cloudy outside on a given day, what is the probability that it will rain that day?
<b>Solution</b>:
P(cloudy) = 0.40
P(rain) = 0.20
P(cloudy | rain) = 0.85
Thus, we can calculate:
P(rain | cloudy) = P(rain) * P(cloudy | rain) / P(cloudy)
P(rain | cloudy) = 0.20 * 0.85 / 0.40
P(rain | cloudy) = 0.425
If it’s cloudy outside on a given day, the probability that it will rain that day is <b>42.5%</b>.
We can create the following simple function to apply Bayes’ Theorem in R:
<b>bayesTheorem &lt;- function(pA, pB, pBA) {
  pAB &lt;- pA * pBA / pB
  return(pAB)
}
</b>
The following example shows how to use this function in practice.
<h3>Example : Bayes’ Theorem in R</h3>
Suppose we know the following probabilities:
P(rain) = 0.20
P(cloudy) = 0.40
P(cloudy | rain) = 0.85
To calculate P(rain | cloudy), we can use the following syntax:
<b>#define function for Bayes' Theorem
bayesTheorem &lt;- function(pA, pB, pBA) {
  pAB &lt;- pA * pBA / pB
  return(pAB)
}
#define probabilities
pRain &lt;- 0.2
pCloudy &lt;- 0.4
pCloudyRain &lt;- .85
#use function to calculate conditional probability
bayesTheorem(pRain, pCloudy, pCloudyRain)
[1] 0.425
</b>
This tells us that if it’s cloudy outside on a given day, the probability that it will rain that day is <b>0.425</b> or <b>42.5%</b>.
This matches the value that we calculated earlier by hand.
<h2><span class="orange">How to Make a Bell Curve in Excel: Example + Template</span></h2>
A “bell curve” is the nickname given to the shape of a  normal distribution , which has a distinct “bell” shape:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve1.png">
This tutorial explains how to make a bell curve in Excel for a given mean and standard deviation and even provides a free downloadable template that you can use to make your own bell curve in Excel.
<h3>Example: Bell Curve in Excel</h3>
Use the following steps to make a bell curve in Excel.
<b>Step 1: Create cells for the mean and standard deviation.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve.png">
<b>Step 2: Create cells for percentiles from -4 to 4, in increments of 0.1.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve2.png">
<b>. . .</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve3.png">
<b>Step 3: Create a column of data values to be used in the graph.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve4.png">
<b>Step 4: Find the values for the normal distribution pdf.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve5.png">
<b>Step 5: Create x-axis plot labels for only the integer percentiles.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve6.png">
<b>Step 6: Make the bell curve.</b>
First, highlight all of the values in the <b>pdf </b>column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve7.png">
Then, in the <b>Charts </b>group on the <b>Insert </b>tab, click the first plot option in the <b>Insert Line or Area Chart </b>category:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve8.png">
A bell curve will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve9.png">
<b>Step 7: Modify the x-axis labels.</b>
Right click anywhere on the chart and click <b>Select Data</b>. A new window will pop up. Click on the <b>Edit </b>button under Horizontal Axis Labels:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve10.png">
Select the range of cells where the x-axis labels are located. In our case, it’s cell range <b>D5:D85</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve11.png">
The x-axis labels will update automatically :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve12.png">
You’ll notice that if you change the mean and standard deviation, the bell curve will update automatically. For example, here’s what the bell curve turns into if we use mean = 10 and standard deviation = 2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve13.png">
Feel free to modify the chart title, add axis labels, and change the color if you’d like to make the chart more aesthetically pleasing.
<h3>Free Template</h3>
Feel free to download  this free template  that was used to make the exact bell curve in this tutorial.
<h2><span class="orange">How to Create a Bell Curve in Google Sheets (Step-by-Step)</span></h2>
A “bell curve” is the nickname given to the shape of a  normal distribution , which has a distinct “bell” shape:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/bellCurve1.png">
The following step-by-step example shows how to make a bell curve in Google Sheets for a given mean and standard deviation.
<h3>Step 1: Define the Mean & Standard Deviation</h3>
First, we’ll define the values for the mean and standard deviation of a given normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell1.png">
<h3>Step 2: Define Percentiles</h3>
Next, we’ll define the percentiles to use in the plot ranging from -4 to 4 in increments of 0.1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell2.png">
<h3>Step 3: Define Data Values</h3>
Next, we’ll create a column of data values to use in the plot using the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell3-1.png">
<h3>Step 4: Find the values for the Normal Distribution PDF</h3>
Next, we’ll use the following formula to find the values for the normal distribution probability density function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell4.png">
<h3>Step 5: Create the Bell Curve</h3>
Lastly, we can highlight the values in the range <b>B5:C85</b>, then click <b>Insert</b> and then click <b>Chart</b>.
The following bell curve will automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell5.png">
You’ll notice that if you change the mean and standard deviation, the bell curve will update automatically.
For example, here’s what the bell curve turns into if we use mean = 10 and standard deviation = 2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/bell6.png">
Feel free to modify the chart title, add axis labels, and change the color if you’d like to make the chart more aesthetically pleasing.
<h2><span class="orange">How to Make a Bell Curve in Python</span></h2>
A “bell curve” is the nickname given to the shape of a  normal distribution , which has a distinct “bell” shape:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/bellcurvePython1.png">
This tutorial explains how to make a bell curve in Python.
<h3>How to Create a Bell Curve in Python</h3>
The following code shows how to create a bell curve using the <b>numpy</b>, <b>scipy</b>, and <b>matplotlib</b> libraries:
<b>import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm
#create range of x-values from -4 to 4 in increments of .001
x = np.arange(-4, 4, 0.001)
#create range of y-values that correspond to normal pdf with mean=0 and sd=1 
y = norm.pdf(x,0,1)
#define plot 
fig, ax = plt.subplots(figsize=(9,6))
ax.plot(x,y)
#choose plot style and display the bell curve 
plt.style.use('fivethirtyeight')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/bellcurvePython1.png">
<h3>How to Fill in a Bell Curve in Python</h3>
The following code illustrates how to fill in the area under the bell curve ranging from -1 to 1:
<b>x = np.arange(-4, 4, 0.001)
y = norm.pdf(x,0,1)
fig, ax = plt.subplots(figsize=(9,6))
ax.plot(x,y)
#specify the region of the bell curve to fill in 
x_fill = np.arange(-1, 1, 0.001)
y_fill = norm.pdf(x_fill,0,1)
ax.fill_between(x_fill,y_fill,0, alpha=0.2, color='blue')
plt.style.use('fivethirtyeight')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/bellcurvePython2.png">
Note that you can also style the graph in any way you’d like using the many <b>matplotlib </b>styling options. For example, you could use a “solar light” theme with a green line and green shading:
<b>x = np.arange(-4, 4, 0.001)
y = norm.pdf(x,0,1)
fig, ax = plt.subplots(figsize=(9,6))
ax.plot(x,y, color='green')
#specify the region of the bell curve to fill in 
x_fill = np.arange(-1, 1, 0.001)
y_fill = norm.pdf(x_fill,0,1)
ax.fill_between(x_fill,y_fill,0, alpha=0.2, color='green')
plt.style.use('Solarize_Light2')
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/bellcurvePython3.png">
<em>You can find the complete style sheet reference guide for matplotlib  here .</em>
<h2><span class="orange">Bench Press Calculator (Find Your 1 Rep Max)</span></h2>
Your <b>one rep max (1RM)</b> for bench press is calculated using the following formula:
<b>1RM</b> = a * (1 + (r/30))
where <b>a</b> is an amount you can bench press for <b>r</b> repetitions.
For example, if you can bench press 180 lbs for 5 reps, then your one rep max is 180 * (1 + (5/30)) = 210.
To find the your one rep max, simply fill in the values below and then click the “Calculate” button.
<label for="amount"><b>I can bench press this amount</b></label>
<input type="number" id="amount" min="0" value="180">
<label for="reps"><b>For this many reps</b></label>
<input type="number" id="reps" min="0" value="5">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
Your one rep max is: <b>210.00</b>
<script>
function calc() {
//get input value and find log base 2
var amount = document.getElementById('amount').value*1;
var reps = document.getElementById('reps').value*1;
//calculate one rep max
var answer = amount * (1 - (-reps / 30));
//output answers
document.getElementById('answer').innerHTML = answer.toFixed(2);
}
</script>
<h2><span class="orange">A Guide to the Benjamini-Hochberg Procedure</span></h2>
Whenever you conduct a statistical test, it’s possible that you could get a p-value that is less than 0.05 purely by chance, even if your null hypothesis is true.
For example, suppose you want to know if a certain plant has a mean height greater than 10 inches. Your null and alternative hypotheses for the test would be:
<b>H<sub>0</sub>:</b> μ = 10 inches
<b>H<sub>A</sub>:</b> μ > 10 inches
To test this hypothesis, you may go out and collect a  random sample  of 20 plants to measure. Even if the true mean height of this species of plant is 10 inches, it’s possible that you could have selected a sample of 20 plants that were unusually tall, which will lead you to reject the null hypothesis.
Although the null hypothesis was true (the mean height of this plant really was 10 inches), you rejected it. In statistics, we call this a “false discovery.” You claimed to have made a discovery – a “significant result” – but it’s actually a false one.
Now imagine that you conduct 100 statistical tests at once. Using an  alpha level  of 0.05, there’s only a 5% chance of making a false discovery with any individual test, but because you’re conducting such a large amount of tests, you would expect about 5 of the 100 to lead to false discoveries.
In the modern world, false discoveries can be a common problem since technology has enabled researchers to conduct hundreds or even thousands of statistical tests at once.
For example, medical researchers can run statistical tests on tens of thousands of genes at once. Even with a false discovery rate of just 5%, this means hundreds of tests could result in false discoveries.
One way to control the false discovery rate is to use something known as the <b>Benjamini-Hochberg Procedure.</b>
<h2>The Benjamini-Hochberg Procedure</h2>
The <b>Benjamini-Hochberg Procedure</b> works as follows:
<b>Step 1: </b>Conduct all of your statistical tests and find the p-value for each test.
<b>Step 2: </b>Arrange the p-values in order from smallest to largest, assigning a rank to each one – the smallest p-value has a rank of 1, the next smallest has a rank of 2, etc.
<b>Step 3: </b>Calculate the Benjamini-Hochberg critical value for each p-value, using the formula<em> (i/m)*Q</em>
where:
<em>i </em>= rank of p-value
<em>m </em>= total number of tests
<em>Q</em> = your chosen false discovery rate
<b>Step 4: Find the largest p-value that is less than the critical value. Designate every p-value that is smaller than this p-value to be significant.</b>
The following example illustrates how to conduct this procedure with concrete values.
<h2>Example </h2>
Suppose researchers are interested in determining whether or not 20 different variables are linked to heart disease. They conduct 20 individual statistical tests at once and receive a p-value for each test. The following table shows the p-values for each test, ranked in order from smallest to largest.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/hochberg1-1.png">
Suppose researchers are willing to accept a 20% false discovery rate. Thus, to calculate the Benjamini-Hochberg critical value for each p-value, we can use the following formula: (i/20)*0.2 where <em>i </em>= rank of p-value.
The following table shows the Benjamini-Hochberg critical value for each individual p-value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/hochberg2-1.png">
The test with the largest p-value that is less than its Benjamini-Hochberg critical value is Variable #11, which has a p-value of 0.039 and a B-H critical value of 0.040.
Thus, this test and all tests with a smaller p-value will be considered significant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/hochberg3.png">
Note that even though Variable #17 and Variable #3 didn’t have p-values that were smaller than their B-H critical values, they are still considered to be significant since they have smaller p-values than Variable #11.
<h3>How to choose a false discovery rate</h3>
One of the most important steps in the Benjamini-Hochberg procedure is choosing a false discovery rate. You should choose your false discovery rate before you actually collect any data or conduct any statistical tests.
Typically you will conduct a large number of statistical tests during the exploratory phase of your analysis, which you will then follow up with more tests to further investigate your findings.
If the follow-up tests are low-cost, then you may consider setting a higher false discovery rate because even if you have a few false discoveries you’re likely to uncover these false discoveries with future tests.
Also, if the cost of missing an important discovery is high then you may want to set your false discovery rate higher so that you don’t miss anything important.
Depending on the costs of your research and the importance of not missing any important discoveries, the false discovery rate will vary from one situation to the next.
<h2><span class="orange">Berkson’s Bias: Definition + Examples</span></h2>
<b>Berkson’s bias</b> is a type of bias that occurs in research when two variables appear to be negatively correlated in sample data, but are actually positively correlated in the overall  population .
For example, suppose Tom wants to study the correlation between the quality of burgers and the quality of milkshakes at local restaurants.
He goes out and collects the following data on seven different restaurants:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson1.png">
He creates a  scatterplot  to visualize the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson2.png">
The  Pearson correlation coefficient  between these two variables is <b>-0.75</b>, which is a strong negative correlation.
This finding is counterintuitive to Tom – He would think that restaurants that make good burgers also make good milkshakes.
However, it turns out that Tom simply skipped over all of the restaurants in town that make both bad burgers <em>and</em> bad milkshakes.
If he had visited these restaurants, he would have collected the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson3.png">
And here’s what a scatterplot for this dataset looks like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson4.png">
The  Pearson correlation coefficient  between the two variables turns out to be <b>0.46</b>, which is a moderately strong positive correlation.
By only looking at a subset of the restaurants in town, Tom incorrectly concluded that there was a negative correlation between burger quality and milkshake quality.
In reality, there turns out to be a positive relationship (as one would expect) between these two variables. This is a classic example of Berkson’s bias. 
Check out the following examples for more scenarios where Berkson’s bias occurs in practice.
<h3>Example 1: College Admissions</h3>
Suppose a college only admits students who have a high enough GPA and high enough ACT score.
It’s well known that these two variables are positively correlated, but it turns out that among the students who decide to go to a particular college, there appears to be a negative correlation between the two.
However, this negative correlation only occurs because the students who have <em>both</em> a high GPA and ACT score may go to an elite university while students who have <em>both</em> a low GPA and ACT score do not get admitted at all.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson6.png">
Although the correlation between ACT and GPA is positive in the population, the correlation appears to be negative in the sample. This is a case of Berkson’s bias.
<h3>Example 2: Dating Preferences</h3>
Many individuals will only date partners who are both attractive and have a good personality.
In the real world, there might be no correlation at all between these two variables, but when narrowing down the dating pool, an individual may completely ignore potential partners who are both unattractive and have a good personality.
Thus, among the potential partners it may seem like there is a negative correlation between these two variables: More attractive people have a worse personality and people with better personalities seem less attractive.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/berkson7.png">
Although there is no correlation between these two variables in the population, there appears to be a negative correlation in the sample of potential partners. This is simply a case of Berkson’s bias.
<h3>How to Prevent Berkson’s Bias</h3>
The most obvious way to prevent Berkson’s bias in research studies is to collect a  simple random sample  from a population. That is, make sure that every member of the population of interest has an equal chance of being included in the sample.
For example, if you’re studying the prevalence of diseases in a certain country then you should collect a sample of individuals from around the entire country, not just those individuals who are convenient to reach in hospitals.
By using a simple random sample, researchers can maximize the chances that their sample is  representative of the population  which means they can generalize their findings from the sample to the overall population with confidence.
<h2><span class="orange">Bernoulli vs Binomial Distribution: What’s the Difference?</span></h2>
A  random variable  follows a <b>Bernoulli distribution</b> if it only has two possible outcomes: 0 or 1.
For example, suppose we flip a coin one time. Let the probability that it lands on heads be <em>p</em>. This means the probability that it lands on tails is 1-<em>p</em>.
Thus, we could write:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/bernoulli1.png">
In this case, random variable <em>X</em> follows a Bernoulli distribution. It can only take on two possible values.
<b>Now, if we flip a coin multiple times then the sum of the Bernoulli random variables will follow a Binomial distribution.</b>
For example, suppose we flip a coin 5 times and we want to know the probability of obtaining heads <em>k </em>times. We would say that the random variable <em>X</em> follows a Binomial distribution.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/bernoulli2-1.png">
If a random variable <em>X</em> follows a Binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0 heads during these 3 flips:
P(X=0) = <sub>3</sub>C<sub>0</sub> * .5<sup>0</sup> * (1-.5)<sup>3-0</sup> = 1 * 1 * (.5)<sup>3</sup> = <b>0.125</b>
<b>When <em>n</em> = 1 trial, the Binomial distribution is equivalent to the Bernoulli distribution.</b>
<h3>Important Notes</h3>
Here are a couple important notes in regards to the Bernoulli and Binomial distribution:
<b>1. A random variables that follows a Bernoulli distribution can only take on two possible values, but a random variable that follows a Binomial distribution can take on several values.</b>
For example, in a single coin flip we will either have 0 or 1 heads. However, in a series of 5 coin flips we could have 0, 1, 2, 3, 4, or 5 heads.
<b>2. In order for a random variable to follow a Binomial distribution, the probability of “success” in each Bernoulli trial must be equal and independent.</b>
For example, if we define “success” as landing on heads, then the probability of success on each coin flip is equal to 0.5 and each flip is independent – the outcome of one coin flip does not affect the outcome of another.
<h2><span class="orange">A Complete Guide to the Best ggplot2 Themes</span></h2>
This tutorial provides a complete guide to the best ggplot2 themes, including:
How to modify the appearance of plots using built-in ggplot2 themes.
How to modify the appearance of plots using predefined themes from the <em>ggthemes</em> library.
How to modify specific components of the theme including the plot panel background and the gridlines.
<h2>How to Modify Plot Appearance Using Built-in ggplot2 Themes</h2>
For each of the following examples, we’ll use the built-in R dataset iris:
<b>#view first six rows of <em>iris </em>dataset
head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa</b>
First, we’ll load the <em>ggplot2 </em>library and create a  scatterplot  that shows <em>Sepal.Length</em> on the x-axis and <em>Sepal.Width</em> on the y-axis, colored according to Species:
<b>#load <em>ggplot2 </em>library
library(ggplot2)
#create scatterplot
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes1.jpg">
Next, we’ll show how each of the built-in ggplot2 themes impact the appearance of the plot.
<h2>theme_gray</h2>
The default theme, featuring a gray background and white gridlines.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_gray()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes1.jpg">
<h2>theme_bw</h2>
A black on white theme.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_bw()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes2.jpg">
<h2>theme_linedraw</h2>
A theme with only black lines of various widths on white backgrounds.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_linedraw()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes3.jpg"></h2>
<h2>theme_light</h2>
A theme similar to <b>theme_linedraw</b> but with grey lines and axes designed to draw more attention to the data.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_light()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes4.jpg"></h2>
<h2>theme_dark</h2>
A theme similar to <b>theme_light</b>, but with a dark background. A useful theme for making thin colored lines stand out.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_dark()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes5.jpg"></h2>
<h2>theme_minimal</h2>
A theme with no background annotations.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_minimal()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes6.jpg"></h2>
<h2>theme_classic</h2>
A theme with no gridlines.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_classic()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes7.jpg"></h2>
<h2>theme_void</h2>
A completely empty theme.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_void()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes8.jpg"></h2>
<h2>How to modify the appearance of plots using predefined themes from the ggthemes library</h2>
In addition to using the built-in ggplot2 themes, we can use the predefined themes from the ggthemes library to modify the aesthetics of plots.
First, we’ll load the ggthemes library:
<b>library(ggthemes)</b>
Next, we’ll show a few examples of how to use the predefined themes to modify the aesthetics of plots:
<h2>theme_wsj</h2>
A Wall Street Journal theme.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_wsj()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes9.jpg">
<h2>theme_tufte</h2>
A minimalist theme inspired by the work of statistician Edward Tufte.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_tufte()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes10.jpg"></h2>
<h2>theme_solarized</h2>
A theme that uses colors based on  the solarized palette .
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_solarized()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes11.jpg">
Note that we can also use the argument <b>light = FALSE</b> to use a dark background on the plot:
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_solarized(light = FALSE)</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes12.jpg"></h2>
<h2>theme_gdocs</h2>
A theme with Google Docs Chart defaults.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_gdocs()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes13.jpg"></h2>
<h2>theme_fivethirtyeight</h2>
Theme inspired by  fivethirtyeight.com  plots.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_fivethirtyeight()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes14.jpg"></h2>
<h2>theme_economist</h2>
Theme inspired by The Economist.
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme_economist()</b>
<h2><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes15.jpg"></h2>
<h2>How to Modify Specific Components of Plots</h2>
We can use the <b>theme()</b> and <b>element_rect()</b> functions to change the plot panel background color:
<b>theme(panel.background = element_rect(fill, color, size))</b>
<b>fill:</b> fill color for rectangle
<b>color:</b> border color
<b>size:</b> border size
We can also use the <b>element_line()</b> function to change the size and appearance of the gridlines:
<b>theme(panel.grid.major = element_line(color, size, linetype),
      panel.grid.minor = element_line(color, size, linetype))</b>
<b>color:</b> border color
<b>size:</b> border size
<b>linetype:</b> line type (“blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”)
The following code illustrates how to remove the plot panel borders and the gridlines:
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme(panel.border = element_blank(),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank())</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes16.jpg">
The following code illustrates how to modify the plot panel background and the gridlines:
<b>ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +
  geom_point() +
  theme(
    panel.background = element_rect(fill = "powderblue",
    color = "powderblue",
    size = 0.5, linetype = "solid"),
    panel.grid.major = element_line(size = 0.5, linetype = 'solid', color = "white"),
    panel.grid.minor = element_line(size = 0.25, linetype = 'solid', color = "white")
  )</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/ggthemes17.jpg">
<h2><span class="orange">Best Subset Selection in Machine Learning (Explanation & Examples)</span></h2>
In the field of machine learning, we’re often interested in building models using a set of predictor variables and a  response variable . Our goal is to build a model that can effectively use the predictor variables to predict the value of the response variable.
Given a set of <em>p</em> total predictor variables, there are many models that we could potentially build. One method that we can use to pick the <em>best</em> model is known as <b>best subset selection</b> and it works as follows:
<b>1.</b> Let M<sub>0</sub> denote the null model, which contains no predictor variables. 
<b>2.</b> For k = 1, 2, … p:
Fit all <sub>p</sub>C<sub>k</sub> models that contain exactly <em>k</em> predictors.
Pick the best among these <sub>p</sub>C<sub>k</sub> models and call it M<sub>k</sub>. Define “best” as the model with the highest R<sup>2</sup> or equivalently the lowest RSS.
<b>3.</b> Select a single best model from among M<sub>0</sub>…M<sub>p</sub> using cross-validation prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>.
Note that for a set of <em>p</em> predictor variables, there are 2<sup>p</sup> possible models. 
<h3>Example of Best Subset Selection</h3>
Suppose we have a dataset with p = 3 predictor variables and one response variable, y. To perform best subset selection with this dataset, we would fit the following 2<sup>p</sup> = 2<sup>3</sup> = 8 models:
A model with no predictors
A model with predictor x<sub>1</sub>
A model with predictor x<sub>2</sub>
A model with predictor x<sub>3</sub>
A model with predictors x<sub>1</sub>, x<sub>2</sub>
A model with predictors x<sub>1</sub>, x<sub>3</sub>
A model with predictors x<sub>2</sub>, x<sub>3</sub>
A model with predictors x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>
Next, we’d choose the model with the highest R<sup>2</sup> among each set of models with <em>k</em> predictors. For example, we might end up choosing:
A model with no predictors
A model with predictor x<sub>2</sub>
A model with predictors x<sub>1</sub>, x<sub>2</sub>
A model with predictors x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>
Next, we’d perform  cross-validation  and choose the best model to be the one that results in the lowest prediction error, Cp, BIC, AIC, or adjusted R<sup>2</sup>.
For example, we might end up choosing the following model as the “best” model because it produced the lowest cross-validated prediction error:
A model with predictors x<sub>1</sub>, x<sub>2</sub>
<h3>Criteria for Choosing the “Best” Model</h3>
The last step of best subset selection involves choosing the model with the lowest prediction error, lowest Cp, lowest BIC, lowest AIC, or highest adjusted R<sup>2</sup>.
Here are the formulas used to calculate each of these metrics:
<b>Cp:</b> (RSS+2dσ<U+0302>) / n
<b>AIC: </b>(RSS+2dσ<U+0302><sup>2</sup>) / (nσ<U+0302><sup>2</sup>)
<b>BIC: </b>(RSS+log(n)dσ<U+0302><sup>2</sup>) / n
<b>Adjusted R<sup>2</sup>:</b> 1 – ( (RSS/(n-d-1)) / (TSS / (n-1)) )
where:
<b>d:</b> The number of predictors
<b>n:</b> Total observations
<b>σ<U+0302>:</b> Estimate of the variance of the error associate with each response measurement in a regression model
<b>RSS:</b> Residual sum of squares of the regression model
<b>TSS:</b> Total sum of squares of the regression model
<h3>Pros & Cons of Best Subset Selection</h3>
Best subset selection offers the following <b>pros:</b>
It’s a straightforward approach to understand and interpret.
It allows us to identify the best possible model since we consider all combinations of predictor variables.
However, this method comes with the following <b>cons:</b>
It can be computationally intense. For a set of <em>p</em> predictor variables, there are 2<sup>p</sup> possible models. For example, with 10 predictor variables there are 2<sup>10</sup> = 1,000 possible models to consider.
Because it considers such a large number of models, it could potentially find a model that performs well on training data but not on future data. This could result in  overfitting .
<h3>Conclusion</h3>
While best subset selection is straightforward to implement and understand, it can be unfeasible if you’re working with a dataset that has a large number of predictors and it could potentially lead to overfitting.
An alternative to this method is known as  stepwise selection , which is more computationally efficient.
<h2><span class="orange">What is a Beta Level in Statistics? (Definition & Example)</span></h2>
In statistics, we use  hypothesis tests  to determine if some assumption about a  population parameter  is true.
A hypothesis test always has the following two hypotheses:
<b>Null hypothesis (H<sub>0</sub>):</b> The sample data is consistent with the prevailing belief about the population parameter.
<b>Alternative hypothesis (H<sub>A</sub>):</b> The sample data suggests that the assumption made in the null hypothesis is not true. In other words, there is some non-random cause influencing the data.
Whenever we conduct a hypothesis test, there are always four possible outcomes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/beta1.png">
There are two types of errors we can commit:
<b>Type I Error:</b> We reject the null hypothesis when it is actually true. The probability of committing this type of error is denoted as <b>α</b>.
<b>Type II Error:</b> We fail to reject the null hypothesis when it is actually false. The probability of committing this type of error is denoted as <b>β</b>.
<h3>The Relationship Between Alpha and Beta</h3>
Ideally researchers want both the probability of committing a type I error <em>and</em> the probability of committing a type II error to be low.
However, a tradeoff exists between these two probabilities. If we decrease the alpha level, we can decrease the probability of rejecting a null hypothesis when it’s actually true, but this actually increases the beta level – the probability that we fail to reject the null hypothesis when it actually is false.
<h3>The Relationship Between Power and Beta</h3>
The <b>power</b> of a hypothesis test refers to the probability of detecting an effect or difference when an effect or difference is actually present. In other words, it’s the probability of correctly rejecting a false null hypothesis.
It is calculated as:
<b>Power = 1 – β</b>
In general, researchers want the power of a test to be high so that if some effect or difference does exist, the test is able to detect it.
From the equation above, we can see that the best way to raise the power of a test is to reduce the beta level. And the best way to reduce the beta level is typically to increase the sample size.
The following examples shows how to calculate the beta level of a hypothesis test and demonstrate why increasing the sample size can lower the beta level.
<h3>Example 1: Calculate Beta for a Hypothesis Test</h3>
Suppose a researcher wants to test if the mean weight of widgets produced at a factory is less than 500 ounces. It is known that the standard deviation of the weights is 24 ounces and the researcher decides to collect a random sample of 40 widgets.
He will perform the following hypothesis at α = 0.05:
<b>H<sub>0</sub>:</b> μ = 500
<b>H<sub>A</sub>:</b> μ &lt; 500
Now imagine that the mean weight of widgets being produced is actually 490 ounces. In other words, the null hypothesis should be rejected.
We can use the following steps to calculate the beta level – the probability of failing to reject the null hypothesis when it actually should be rejected:
<b>Step 1: Find the non-rejection region.</b>
According to the  Critical Z Value Calculator , the left-tailed critical value at α = 0.05 is <b>-1.645</b>.
<b>Step 2: Find the minimum sample mean we will fail to reject.</b>
The test statistic is calculated as z = (x – μ) / (s/√n)
Thus, we can solve this equation for the sample mean:
x = μ – z*(s/√n)
x = 500 – 1.645*(24/√40)
x = <b>493.758</b>
<b>Step 3: Find the probability of the minimum sample mean actually occurring.</b>
We can calculate this probability as:
P(Z ≥ (493.758 – 490) / (24/√40))
P(Z ≥ 0.99)
According to the  Normal CDF Calculator , the probability that Z ≥ 0.99 is <b>0.1611</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/beta2.png">
Thus, the beta level for this test is <b>β = 0.1611.</b> This means there is a 16.11% chance of failing to detect the difference if the real mean is 490 ounces.
<h3>Example 2: Calculate Beta for a Test with a Larger Sample Size</h3>
Now suppose the researcher performs the exact same hypothesis test but instead uses a sample size of n = 100 widgets. We can repeat the same three steps to calculate the beta level for this test:
<b>Step 1: Find the non-rejection region.</b>
According to the  Critical Z Value Calculator , the left-tailed critical value at α = 0.05 is <b>-1.645</b>.
<b>Step 2: Find the minimum sample mean we will fail to reject.</b>
The test statistic is calculated as z = (x – μ) / (s/√n)
Thus, we can solve this equation for the sample mean:
x = μ – z*(s/√n)
x = 500 – 1.645*(24/√100)
x = <b>496.05</b>
<b>Step 3: Find the probability of the minimum sample mean actually occurring.</b>
We can calculate this probability as:
P(Z ≥ (496.05 – 490) / (24/√100))
P(Z ≥ 2.52)
According to the  Normal CDF Calculator , the probability that Z ≥ 2.52 is <b>0.0059.</b>
Thus, the beta level for this test is <b>β = 0.0059.</b> This means there is only a .59% chance of failing to detect the difference if the real mean is 490 ounces.
Notice that by simply increasing the sample size from 40 to 100, the researcher was able to reduce the beta level from 0.1611 all the way down to .0059.
<b>Bonus:</b> Use this  Type II Error Calculator  to automatically calculate the beta level of a test.
<h2><span class="orange">What is the Bias-Variance Tradeoff in Machine Learning?</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the model predictions match the observed data.
For  regression models , the most commonly used metric is the mean squared error (MSE), which is calculated as:
MSE = (1/n)*Σ(y<sub>i</sub> – f(x<sub>i</sub>))<sup>2</sup>
where:
<b>n:</b> Total number of observations
<b>y<sub>i</sub>:</b> The response value of the i<sup>th</sup> observation
<b>f(x<sub>i</sub>):</b> The predicted response value of the i<sup>th</sup> observation
The closer the model predictions are to the observations, the smaller the MSE will be.
However, we only care about <b>test MSE</b> – the MSE when our model is applied to unseen data. This is because we only care about how the model will perform on unseen data, not existing data.
For example, it’s nice if a model that predicts stock market prices has a low MSE on historical data, but we <em>really</em> want to be able to use the model to accurately forecast future data.
It turns out that the test MSE can always be decomposed into two parts:
<b>(1) The variance:</b> Refers to the amount by which our function <em>f</em> would change if we estimated it using a different training set.
<b>(2) The bias:</b> Refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.
Written in mathematical terms:
Test MSE = Var(<em>f<U+0302>(</em>x<sub>0</sub>)) + [Bias(<em>f<U+0302>(</em>x<sub>0</sub>))]<sup>2</sup> + Var(ε)
Test MSE = Variance + Bias<sup>2</sup> + Irreducible error
The third term, the irreducible error, is the error that cannot be reduced by any model simply because there always exists <em>some</em> noise in the relationship between the set of explanatory variables and the  response variable .
Models that have <b>high bias</b> tend to have <b>low variance</b>. For example, linear regression models tend to have high bias (assumes a simple linear relationship between explanatory variables and response variable) and low variance (model estimates won’t change much from one sample to the next).
However, models that have <b>low bias</b> tend to have <b>high variance</b>. For example, complex non-linear models tend to have low bias (does not assume a certain relationship between explanatory variables and response variable) with high variance (model estimates can change a lot from one training sample to the next).
<h3>The Bias-Variance Tradeoff</h3>
The <b>bias-variance tradeoff </b>refers to the tradeoff that takes place when we choose to lower bias which typically increases variance, or lower variance which typically increases bias.
The following chart offers a way to visualize this tradeoff:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/bias_variance1.png">
The total error decreases as the complexity of a model increases but only up to a certain point. Past a certain point, variance begins to increase and total error also begins to increase.
In practice, we only care about minimizing the total error of a model, not necessarily minimizing the variance or bias. It turns out that the way to minimize the total error is to strike the right balance between variance and bias.
In other words, we want a model that is complex enough to capture the true relationship between the explanatory variables and the response variable, but not overly complex such that it finds patterns that don’t really exist.
When a model is too complex, it  <b>overfits</b>  the data. This happens because it works too hard to find patterns in the training data that are just caused by random chance. This type of model is likely to perform poorly on unseen data.
But when a model is too simple, it <b>underfits</b> the data. This happens because it assumes the true relationship between the explanatory variables and the response variable is more simple than it actually is.
The way to pick optimal models in machine learning is to strike the balance between bias and variance such that we can minimize the test error of the model on future unseen data.
In practice, the most common way to minimize test MSE is to use  cross-validation .
<h2><span class="orange">How to Calculate BIC in Python</span></h2>
The <b>Bayesian Information Criterion</b>, often abbreviated <b>BIC</b>, is a metric that is used to compare the goodness of fit of different regression models.
In practice, we fit several regression models to the same dataset and choose the model with the lowest BIC value as the model that best fits the data.
We use the following formula to calculate BIC:
<b>BIC: </b>(RSS+log(n)dσ<U+0302><sup>2</sup>) / n
where:
<b>d:</b> The number of predictors
<b>n:</b> Total observations
<b>σ<U+0302>:</b> Estimate of the variance of the error associate with each response measurement in a regression model
<b>RSS:</b> Residual sum of squares of the regression model
<b>TSS:</b> Total sum of squares of the regression model
To calculate the BIC of several regression models in Python, we can use the <b>statsmodels.regression.linear_model.OLS()</b> function, which has a property called bic that tells us the BIC value for a given model.
The following example shows how to use this function to calculate and interpret the BIC for various regression models in Python.
<h3>Example: Calculate BIC of Regression Models in Python</h3>
Suppose we would like to fit two different  multiple linear regression models  using variables from the <b>mtcars</b> dataset.
First, we’ll load this dataset:
<b>from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import pandas as pd
#define URL where dataset is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data = pd.read_csv(url)
#view head of data
data.head()
        model          mpg  cyldisp  hp    drat wt   qsec  vs am gear carb
0Mazda RX4  21.0  6     160.0 110  3.90 2.620 16.46   0  1  44
1Mazda RX4 Wag  21.0  6     160.0 110  3.90 2.875 17.02   0  1  44
2Datsun 710  22.8  4     108.0 93   3.85 2.320 18.61   1  1  41
3Hornet 4 Drive  21.4  6     258.0 110  3.08 3.215 19.44   1  0  31
4Hornet Sportabout 18.78     360.0 175  3.15 3.440 17.02   0  0  32
</b>
Next, we’ll fit the following two regression models:
<b>Model 1</b>: mpg = β<sub>0</sub> + β<sub>1</sub>(disp)+ β<sub>2</sub>(qsec)
<b>Model 2</b>: mpg = β<sub>0</sub> + β<sub>1</sub>(disp)+ β<sub>2</sub>(wt)
The following code shows how to fit the first model and calculate the BIC:
<b>#define response variable
y = data['mpg']
#define predictor variables
x = data[['disp', 'qsec']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit regression model
model = sm.OLS(y, x).fit()
#view BIC of model
print(model.bic)
174.23905634994506</b>
The BIC of this model turns out to be <b>174.239</b>.
Next, we’ll fit the second model and calculate the BIC:
<b>#define response variable
y = data['mpg']
#define predictor variables
x = data[['disp', 'wt']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit regression model
model = sm.OLS(y, x).fit()
#view BIC of model
print(model.bic)
166.56499196301334</b>
The BIC of this model turns out to be <b>166.565</b>.
Since the second model has a lower BIC value, it is the better fitting model. 
Once we’ve identified this model as the best, we can proceed to fit the model and analyze the results including the R-squared value and the beta coefficients to determine the exact relationship between the set of predictor variables and the  response variable .
<h2><span class="orange">How to Calculate BIC in R</span></h2>
The <b>Bayesian Information Criterion</b>, often abbreviated <b>BIC</b>, is a metric that is used to compare the goodness of fit of different regression models.
In practice, we fit several regression models to the same dataset and choose the model with the lowest BIC value as the model that best fits the data.
We use the following formula to calculate BIC:
<b>BIC: </b>(RSS+log(n)dσ<U+0302><sup>2</sup>) / n
where:
<b>d:</b> The number of predictors
<b>n:</b> Total observations
<b>σ<U+0302>:</b> Estimate of the variance of the error associate with each response measurement in a regression model
<b>RSS:</b> Residual sum of squares of the regression model
<b>TSS:</b> Total sum of squares of the regression model
The following step-by-step example shows how to calculate BIC values for regression models in R.
<h3>Step 1: View the Data</h3>
For this example, we’ll use the built-in <b>mtcars</b> dataset:
<b>#view first six rows of mtcars dataset
head(mtcars)   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
<h3>Step 2: Fit Several Models</h3>
Next, we’ll fit several different regression models using this dataset:
<b>#fit three different regression models
model1 &lt;- lm(mpg ~ disp + hp, data = mtcars)
model2 &lt;- lm(mpg ~ disp + qsec, data = mtcars)
model3 &lt;- lm(mpg ~ disp + wt, data = mtcars)
</b>
<h3>Step 3: Choose Model with Lowest BIC</h3>
To calculate the BIC value for each model, we can use the <b>BIC()</b> function from the <b>flexmix</b> package:
<b>library(flexmix)
#calculate BIC of model1
BIC(model1)
[1] 174.4815
#calculate BIC of model2
BIC(model2)
[1] 177.7048
#calculate BIC of model3
BIC(model3)
[1] 170.0307
</b>
We can see the BIC values for each model:
BIC of <b>model 1</b>: 174.4815
BIC of <b>model 2</b>: 177.7048
BIC of <b>model 3</b>: 170.0307
Since model 3 has the lowest BIC value, we will choose it as the model that best fits the dataset.
<h2><span class="orange">What is a Bimodal Distribution?</span></h2>
A <b>bimodal distribution </b>is a probability distribution with two modes.
We often use the term “mode” in  descriptive statistics  to refer to the most commonly occurring value in a dataset, but in this case the term “mode” refers to a local maximum in a chart.
When you visualize a bimodal distribution, you will notice two distinct “peaks” that represent these two modes.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal1-1.png">
This is different than a unimodal distribution that only has one peak:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal2.png">
You can remember the difference between the two by remembering:
“bi” = two
“uni” = one
Although most statistics courses use unimodal distributions like  the normal distribution  to explain different topics, bimodal distributions actually show up fairly often in practice so it’s useful to know how to recognize and interpret them.
<em><b>Note:</b> A bimodal distribution is a specific type of  multimodal distribution .</em>
<h3>Examples of Bimodal Distributions</h3>
Here are some examples of bimodal distributions:
<b>Example #1: Peak restaurant hours</b>
If you created a graph to visualize the distribution of customers at a certain restaurant by hour, you’d likely find that it follows a bimodal distribution with a peak during lunch hours and another peak during dinner hours:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal3.png">
<b>Example #2: Average height of two plant species</b>
Suppose you went around a field and measured the height of different plants. Without realizing it, you measure the height of two different species – one that is quite tall and another that is quite short. If you created a graph to visualize the distribution of heights, it would follow a bimodal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal4.png">
<b>Example #3: Exam scores</b>
Suppose a teacher gives an exam to his class of students. Some of the students studied for the exam, while others did not. When the teacher creates a graph of the exam scores, it follows a bimodal distribution with one peak around low scores for students who didn’t study and another peak around high scores for students who did study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal5.png">
<h3>What Causes Bimodal Distributions?</h3>
There are typically two things that cause bimodal distributions:
<b>1. Some underlying phenomena.</b>
Often bimodal distributions occur because of some underlying phenomena.
For example, the number of customers who visit a restaurant each hour follows a bimodal distribution since people tend to eat out during two distinct times: lunch and dinner. This underlying human behavior is what causes the bimodal distribution.
<b>2. Two different groups being lumped together.</b>
Bimodal distributions can also occur when you’re simply analyzing two different groups of things without realizing it.
For example, if you measure the height of plants in a certain field without realizing that two different species are growing in the same field, you’ll see a bimodal distribution when you create a chart.
<h3>How to Analyze Bimodal Distributions</h3>
We often describe distributions using  the mean or median  since this gives us an idea of where the “center” of the distribution is located.
Unfortunately, the mean and median aren’t useful to know for a bimodal distribution. For example, the mean exam score for students in the example above is 81:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/bimodal6.png">
However, very few students actually scored close to 81. In this case, the mean is misleading. Most students actually scored around 74 or around 88.
A better way to analyze and interpret bimodal distributions is to simply break the data into two separate groups, then analyze the center and the spread for each group.
For example, we may break up the exam scores into “low scores” and “high scores” and then find the mean and standard deviation for each group.
If you’re sharing the results of some analysis and your data does follow a bimodal distribution, it’s helpful to create a histogram like the ones shown above so that your audience can clearly see that the distribution has two distinct “peaks” and that it only makes sense to analyze each peak separately rather than as one large dataset.
<h2><span class="orange">How to Calculate a Binomial Confidence Interval in Python</span></h2>
A <b>confidence interval for a binomial probability</b> is calculated using the following formula:
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
where:
<b>p:</b> proportion of “successes”
<b>z: </b>the chosen z-value
<b>n: </b>sample size
The easiest way to calculate this type of confidence interval in Python is to use the <b>proportion_confint()</b> function from the <b>statsmodels</b> package:
<b><span><span>proportion_confint<span>(<span><span>count, <span><span>nobs, <span><span>alpha<span><span>=<span>0.05, <span><span>method<span><span>=<span><span>'normal'<span>)
</b>
where:
<b>count</b>: Number of successes
<b>nobs</b>: Total number of trials
<b>alpha</b>: Significance level (default is 0.05)
<b>method</b>: Method to use for confidence interval (default is “normal”)
The following example shows how to use this function in practice.
<h3>Example: Calculate Binomial Confidence Interval in Python</h3>
Suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. 
We decide to select a random sample of 100 residents and find that 56 of them are in favor of the law.
We can use the <b>proportion_confint()</b> function to calculate the 95% confidence interval for the true proportion of residents who suppose this law in the entire county:
<b>from statsmodels.stats.proportion import proportion_confint
#calculate 95% confidence interval with 56 successes in 100 trials
proportion_confint(count=56, nobs=100)
(0.4627099463758483, 0.6572900536241518)
</b>
The 95% confidence interval for the true proportion of residents in the county that support the law is <b>[.4627, .6573]</b>.
By default, this function uses the asymptotic normal approximation to calculate the confidence interval. However, we can use the <b>method</b> argument to use a different method.
For example, the default function used in the R programming language to calculate a binomial confidence interval is the Wilson Score Interval.
We can use the following syntax to specify this method when calculating the confidence interval in Python:
<b>from statsmodels.stats.proportion import proportion_confint
#calculate 95% confidence interval with 56 successes in 100 trials
proportion_confint(count=56, nobs=100, method='wilson')
(0.4622810465167698, 0.6532797336983921)
</b>
This tells us that the 95% confidence interval for the true proportion of residents in the county that support the law is <b>[.4623, .6533]</b>.
This confidence interval is just slightly different than the one calculated using the normal approximation.
Note that we can also adjust the <b>alpha</b> value to calculate a different confidence interval.
For example, we can set alpha to be 0.10 to calculate a 90% confidence interval:
<b>from statsmodels.stats.proportion import proportion_confint
#calculate 90% confidence interval with 56 successes in 100 trials
proportion_confint(count=56, nobs=100, alpha=0.10, method='wilson')
(0.47783814499647415, 0.6390007285095451)
</b>
This tells us that the 90% confidence interval for the true proportion of residents in the county that support the law is <b>[.4778, .6390]</b>.
<b>Note</b>: You can find the complete documentation for the <b>proportion_confint()</b> function  here .
<h2><span class="orange">How to Calculate a Binomial Confidence Interval in R</span></h2>
A <b>confidence interval for a binomial probability</b> is calculated using the following formula:
<b>Confidence Interval = p  +/-  z*(√p(1-p) / n)</b>
where:
<b>p:</b> proportion of “successes”
<b>z: </b>the chosen z-value
<b>n: </b>sample size
The z-value that you will use is dependent on the confidence level that you choose. The following table shows the z-value that corresponds to popular confidence level choices:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Confidence Level</b></th>
<th style="text-align: center;"><b>z-value</b></th>
</tr>
<tr>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">1.645</td>
</tr>
<tr>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">1.96</td>
</tr>
<tr>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">2.58</td>
</tr>
</tbody></table>
For example, suppose we want to estimate the proportion of residents in a county that are in favor of a certain law. We select a random sample of 100 residents and find that 56 of them are in favor of the law.
This tutorial explains three different ways to calculate a confidence interval for the true proportion of residents in the entire county that support the law.
<h3>Method 1: Use the prop.test() function</h3>
One way to calculate the 95% binomial confidence interval is to use the <b>prop.test()</b> function in base R:
<b>#calculate 95% confidence interval
prop.test(x=56, n=100, conf.level=.95, correct=FALSE)
1-sample proportions test without continuity correction
data:  56 out of 100, null probability 0.5
X-squared = 1.44, df = 1, p-value = 0.2301
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.4622810 0.6532797
sample estimates:
   p 
0.56 
</b>
The 95% C.I. for the true proportion of residents in the county that support the law is <b>[.46228, .65328]</b>.
<h3>Method 2: Use the binconf() function</h3>
Another way to calculate the confidence interval is to use the <b>binconf()</b> function from the <b>Hmisc</b> package:
<b>library(Hmisc)
#calculate 95% confidence interval
binconf(x=56, n=100, alpha=.05)
 PointEst    Lower     Upper
     0.56 0.462281 0.6532797
</b>
Notice that this confidence interval matches the one calculated in the previous example.
<h3>Method 3: Calculate the Confidence Interval Manually</h3>
Another way to calculate the 95% binomial confidence interval in R is to do it manually:
<b>#define proportion
p &lt;- 56/100
#define significance level
a &lt;- .05
#calculate 95% confidence interval
p + c(-qnorm(1-a/2), qnorm(1-a/2))*sqrt((1/100)*p*(1-p))
[1] 0.4627099 0.6572901</b>
Learn more about the <b>qnorm()</b> function here:  A Guide to dnorm, pnorm, qnorm, and rnorm in R 
<h2><span class="orange">The Three Assumptions of the Binomial Distribution</span></h2>
The  binomial distribution  is a probability distribution that is used to model the probability that a certain number of “successes” occur during a fixed number of trials.
The binomial distribution is appropriate to use if the following three assumptions are met:
<b>Assumption 1: Each trial only has two possible outcomes.</b>
We assume that each trial only has possible two outcomes. For example, if we flip a coin 100 times, each time there can only be two possible outcomes – heads or tails.
<b>Assumption 2: The probability of success is the same for each trial.</b>
We assume that the probability of achieving a “success” is the same for each trial. For example, the probability of a coin landing on heads is 0.5 for any given flip. This probability does not change from one coin flip to the next.
<b>Assumption 3: Each trial is independent.</b>
We assume that each trial is independent of every other trial. For example, the outcome of one coin flip does not affect the outcome of another coin flip. The flips are independent.
The following examples show various scenarios that meet the assumptions of the binomial distribution.
<h3>Example 1: Number of Free Throws Made</h3>
Suppose a basketball player is known to make 70% of his free throws attempts. If he makes 20 attempts, this scenario can be modeled using the binomial distribution.
This scenario meets each of the assumptions of the binomial distribution:
<b>Assumption 1: Each trial only has two possible outcomes.</b>
For each free throw attempt, there are only two possible outcomes – a make or a miss.
<b>Assumption 2: The probability of success is the same for each trial.</b>
The probability that the player makes a free throw on each attempt is the same – 70%. This does not change from one attempt to the next.
<b>Assumption 3: Each trial is independent.</b>
Each free throw attempt is independent of every other attempt. Whether or not a player makes one attempt does not affect whether he makes another attempt.
<h3>Example 2: Number of Side Effects</h3>
Suppose it’s known that 5% of adults that take a certain medication experience negative side effects. Suppose a medical profession then gives this medication to 100 adults in a given month.
This scenario meets each of the assumptions of the binomial distribution:
<b>Assumption 1: Each trial only has two possible outcomes.</b>
For each adult that receives the medication, there is only two possible outcomes – they experience negative side effects or they do not.
<b>Assumption 2: The probability of success is the same for each trial.</b>
The probability that each adult experiences a negative side effect is the same – 5%.
<b>Assumption 3: Each trial is independent.</b>
The outcome for each adult is independent. Whether or not one adult experiences negative side effects does not affect whether or not another adult does as well.
<h3>Example 3: Number of Shopping Returns</h3>
Suppose it’s known that 10% of all customers who walk into a shop are there to make a return. Suppose 200 people enter a store in a given day and the manager records the number who are there to make a return.
This scenario meets each of the assumptions of the binomial distribution:
<b>Assumption 1: Each trial only has two possible outcomes.</b>
Each time a customer enters the shop, there are only two reasons they may be there – to make a return or not.
<b>Assumption 2: The probability of success is the same for each trial.</b>
The probability that a given customer is there to make a return is the same – 10%.
<b>Assumption 3: Each trial is independent.</b>
The outcome for each customer is independent. Whether or not one customer is there to make a return does not affect whether or not another customer is there to make a return.
<h2><span class="orange">Binomial Distribution Calculator</span></h2>
The  Binomial distribution  is one of the most commonly used distributions in statistics.
To find probabilities related to the Binomial distribution, simply fill in the values below and then click the “Calculate” button.
<label for="p"><b>p</b> (probability of success on a given trial)</label> <input id="p" min="0" type="number" value="0.5">
<label for="n"><b>n</b> (number of trials)</label> <input id="n" min="0" type="number" value="100">
<label for="k"><b>k</b> (number of successes)</label> <input id="k" min="0" type="number" value="43">
<input id="buttonCalc" type="button" value="Calculate" onclick="pvalue()">
P(X=43) = 0.03007
P(X&lt;43) = 0.06661
P(X≤43) = 0.09667
P(X>43) = 0.90333
P(X≥43) = 0.93339
<script>
function pvalue() {
//get input values
var p = document.getElementById('p').value*1;
var n = document.getElementById('n').value*1;
var k = document.getElementById('k').value*1;
//assign probabilities to variable names
var exactProb = jStat.binomial.pdf(k,n,p);
var lessProb = jStat.binomial.cdf(k-1,n,p);
var lessEProb = jStat.binomial.cdf(k,n,p);
var greaterProb = 1-jStat.binomial.cdf(k,n,p);
var greaterEProb = 1-jStat.binomial.cdf(k-1,n,p);
//output probabilities
document.getElementById('k1').innerHTML = k;
document.getElementById('k2').innerHTML = k;
document.getElementById('k3').innerHTML = k;
document.getElementById('k4').innerHTML = k;
document.getElementById('k5').innerHTML = k;
document.getElementById('exactProb').innerHTML = exactProb.toFixed(5);
document.getElementById('lessProb').innerHTML = lessProb.toFixed(5);
document.getElementById('lessEProb').innerHTML = lessEProb.toFixed(5);
document.getElementById('greaterProb').innerHTML = greaterProb.toFixed(5);
document.getElementById('greaterEProb').innerHTML = greaterEProb.toFixed(5);
}
</script>
<h2><span class="orange">How to Use the Binomial Distribution in Excel</span></h2>
The  <b>binomial distribution</b>  is one of the most commonly used distributions in statistics. This tutorial explains how to use the following functions in Excel to solve questions about binomial probabilities:
<b>BINOM.DIST</b>
<b>BINOM.DIST.RANGE</b>
<b>BINOM.INV</b>
<h2>BINOM.DIST</h2>
The function <b>BINOM.DIST </b>finds the probability of getting a certain number of<em> </em>successes in a certain number of trials where the probability of success on each trial is fixed.
The syntax for <b>BINOM.DIST </b>is as follows:
<b>BINOM.DIST</b>(number_s, trials, probability_s_cumulative)
<b>number_s:</b> number of successes
<b>trials: </b>total number of trials
<b>probability_s: </b>probability of success on each trial
<b>probability_s_cumulative: </b>TRUE returns the cumulative probability; FALSE returns the exact probability
The following examples illustrate how to solve binomial probability questions using <b>BINOM.DIST</b>:
<h3>Example 1</h3>
<em>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.DIST(10, 12, 0.6, FALSE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom1.jpg">
The probability that Nathan makes exactly 10 free throw attempts out of 12 is <b>0.063852</b>.
<h3>Example 2</h3>
<em>Marty flips a fair coin 5 times. What is the probability that the coin lands on heads 2 times or fewer?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.DIST(2, 5, 0.5, TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom3.jpg">
The probability that the coin lands on heads 2 times or fewer is <b>0.5</b>.
<h3>Example 3</h3>
<em>Mike flips a fair coin 5 times. What is the probability that the coin lands on heads more than 3 times?</em>
To answer this question, we can use the following formula in Excel: <b>1 –</b> <b>BINOM.DIST(3, 5, 0.5, TRUE)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom4.jpg">
The probability that the coin lands on heads more than 3 times is <b>0.1875</b>.
<em><b>Note:</b> In this example, BINOM.DIST(3, 5, 0.5, TRUE) returns the probability that the coin lands on heads 3 times or fewer. So, to find the probability that the coin lands on heads more than 3 times, we simply use 1 – BINOM.DIST(3, 5, 0.5, TRUE).</em>
<h2>BINOM.DIST.RANGE</h2>
The function <b>BINOM.DIST.RANGE </b>finds the probability of getting a certain number of<em> </em>successes in a certain range, based on a certain number of trials where the probability of success on each trial is fixed.
The syntax for <b>BINOM.DIST.RANGE </b>is as follows:
<b>BINOM.DIST.RANGE</b>(trials, probability_s, number_s, number_s2)
<b>trials: </b>total number of trials
<b>probability_s: </b>probability of success on each trial
<b>number_s:</b> minimum number of successes
<b>number_s2:</b> maximum number of successes
The following examples illustrate how to solve binomial probability questions using <b>BINOM.DIST.RANGE</b>:
<h3>EXAMPLE 1</h3>
<em>Debra flips a fair coin 5 times. What is the probability that the coin lands on heads between 2 and 4 times?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.DIST.RANGE(5, 0.5, 2, 4)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom5.jpg">
The probability that the coin lands on heads between 2 and 4 times is <b>0.78125</b>.
<h3>EXAMPLE 2</h3>
<em>It is known that 70% of men support a certain law. If 10 men are randomly selected, what is the probability that between 4 and 6 of them support the law?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.DIST.RANGE(10, 0.7, 4, 6)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom6.jpg">
The probability that between 4 and 6 of the randomly selected men support the law is <b>0.339797</b>.
<h3>EXAMPLE 3</h3>
<em>Teri makes 90% of her free-throw attempts. If she shoots 30 free throws, what is the probability that she makes between 15 and 25?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.DIST.RANGE(30, .9, 15, 25)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom7.jpg">
The probability that she makes between 15 and 25 free throws is <b>0.175495</b>.
<h2>BINOM.INV</h2>
The function <b>BINOM.INV </b>finds the smallest value for which the cumulative binomial distribution is greater than or equal to a criterion value.
The syntax for <b>BINOM.INV </b>is as follows:
<b>BINOM.INV</b>(trials, probability_s, alpha)
<b>trials: </b>total number of trials
<b>probability_s: </b>probability of success on each trial
<b>alpha:</b> criterion value between 0 and 1
The following examples illustrate how to solve binomial probability questions using <b>BINOM.INV</b>:
<h3>EXAMPLE 1</h3>
<em>Duane flips a fair coin 10 times. What is the smallest number of times the coin could land on heads so that the cumulative binomial distribution is greater than or equal to 0.4?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.INV(10, 0.5, 0.4)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom8.jpg">
The smallest number of times the coin could land on heads so that the cumulative binomial distribution is greater than or equal to 0.4 is <b>5</b>.
<h3>EXAMPLE 2</h3>
<em>Duane flips a fair coin 20 times. What is the smallest number of times the coin could land on heads so that the cumulative binomial distribution is greater than or equal to 0.4?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.INV(20, 0.5, 0.4)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom9.jpg">
The smallest number of times the coin could land on heads so that the cumulative binomial distribution is greater than or equal to 0.4 is <b>9</b>.
<h3>EXAMPLE 3</h3>
<em>Duane flips a fair coin 30 times. What is the smallest number of times the coin could land on tails so that the cumulative binomial distribution is greater than or equal to 0.7?</em>
To answer this question, we can use the following formula in Excel: <b>BINOM.INV(20, 0.5, 0.4)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/binom10.jpg">
The smallest number of times the coin could land on tails so that the cumulative binomial distribution is greater than or equal to 0.7 is <b>16</b>.
<h2><span class="orange">How to Use the Binomial Distribution in Google Sheets</span></h2>
The  binomial distribution  in statistics describes the probability of obtaining <em>k</em> successes in <em>n</em> trials when the probability of success in a single experiment is <em>p</em>.
To calculate binomial distribution probabilities in Google Sheets, we can use the <b>BINOMDIST</b> function, which uses the following basic syntax:
<b>BINOMDIST(k, n, p, cumulative)</b>
where:
<b>k</b>: Number of successes
<b>n</b>: Number of trials
<b>p</b>: Probability of success on a given trial
<b>cumulative</b>: Whether to calculate a cumulative probability (Default is FALSE)
The following examples show how to use this function in practice.
<h3>Example 1: Probability of Exactly k Successes</h3>
Ty makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?
To answer this question, we can use the following formula in Google Sheets:
<b>=BINOMDIST(10, 12, 0.6, FALSE)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/binomgoogle1.jpg">
The probability that Ty makes exactly 10 free throw attempts out of 12 is <b>0.0639</b>.
<h3>Example 2: Probability of Less Than k Successes</h3>
Ty makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes less than 10?
To answer this question, we can use the following formula in Google Sheets:
<b>=BINOMDIST(9, 12, 0.6, TRUE)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/binomgoogle2.jpg"448">
The probability that Ty makes less than 10 free throw attempts out of 12 is <b>0.9166</b>.
<h3>Example 3: Probability of Less Than Or Equal to k Successes</h3>
Ty makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes less than or equal to 10?
To answer this question, we can use the following formula in Google Sheets:
<b>=BINOMDIST(10, 12, 0.6, TRUE)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/binomgoogle3.jpg"451">
The probability that Ty makes less than or equal to 10 free throw attempts out of 12 is <b>0.9166</b>.
<h3>Example 4: Probability of Greater Than k Successes</h3>
Ty makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes greater than 10?
To answer this question, we can use the following formula in Google Sheets:
<b>=1-BINOMDIST(10, 12, 0.6, TRUE)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/binomgoogle4.jpg"449">
The probability that Ty makes greater than 10 free throw attempts out of 12 is <b>0.0196</b>.
<h3>Example 5: Probability of Greater Than or Equal to k Successes</h3>
Ty makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes greater than or equal to 10?
To answer this question, we can use the following formula in Google Sheets:
<b>=1-BINOMDIST(9, 12, 0.6, TRUE)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/binomgoogle5.jpg"455">
The probability that Ty makes greater than or equal to 10 free throw attempts out of 12 is <b>0.0834</b>.
<b>Bonus</b>: You can use the  Binomial Distribution Calculator  to automatically calculate binomial probabilities for any values for <em>n</em>, <em>k</em>, and <em>p</em>.
<h2><span class="orange">How to Create a Binomial Distribution Graph in Excel</span></h2>
The  binomial distribution  is used to describe the probability of obtaining <em>k</em> successes in <em>n</em> binomial experiments.
A binomial experiment is an experiment that has the following properties:
The experiment consists of n repeated trials.
Each trial has only two possible outcomes.
The probability of success, denoted p, is the same for each trial.
Each trial is independent.
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n</b>: number of trials
<b>k</b>: number of successes
<b>p</b>: probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub></b>: the number of ways to obtain k successes in n trials
The following example explains how to create a binomial distribution graph in Excel.
<h3>Example: Binomial Distribution Graph in Excel</h3>
To create a binomial distribution graph, we need to first decide on a value for <em>n</em> (number of trials) and <em>p</em> (probability of success in a given trial):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/binomGraph1.png">
Next, we need to create a column for each possible number of successes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/binomGraph2.png">
Next, we can use the <b>BINOM.DIST()</b> function to calculate the binomial probability for the first number of successes:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/binomGraph3.png">
We can then copy and paste this formula to the remaining cells in column B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/binomGraph4.png">
Lastly, we can highlight each of the binomial probabilities, then click the <b>Insert</b> tab along the top ribbon, then click the <b>Insert Column or Bar Chart</b> icon in the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/binomGraph5.png">
The x-axis of the graph shows the number of successes in 8 trials and the y-axis shows the corresponding probability of that many successes.
Note that if you change the value for either <em>n</em> or <em>p</em>, the graph will automatically change to reflect the new probabilities.
<h2><span class="orange">How to Use the Binomial Distribution in Python</span></h2>
The  binomial distribution  is one of the most commonly used distributions in statistics. It describes the probability of obtaining <em>k</em> successes in <em>n</em> binomial experiments.
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
This tutorial explains how to use the binomial distribution in Python.
<h3>How to Generate a Binomial Distribution</h3>
You can generate an array of values that follow a binomial distribution by using the  random.binomial  function from the numpy library:
<b>from numpy import random
#generate an array of 10 values that follow a binomial distribution
random.binomial(n=10, p=.25, size=10)
array([5, 2, 1, 3, 3, 3, 2, 2, 1, 4])
</b>
Each number in the resulting array represents the number of “successes” experienced during <b>10</b> trials where the probability of success in a given trial was <b>.25</b>.
<h3>How to Calculate Probabilities Using a Binomial Distribution</h3>
You can also answer questions about binomial probabilities by using the  binom function  from the scipy library.
<b>Question 1:</b> Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?
<b>from scipy.stats import binom
#calculate binomial probability
binom.pmf(k=10, n=12, p=0.6)
0.0639
</b>
The probability that Nathan makes exactly 10 free throws is <b>0.0639</b>.
<b>Question 2:</b> Marty flips a fair coin 5 times. What is the probability that the coin lands on heads 2 times or fewer?
<b>from scipy.stats import binom
#calculate binomial probability
binom.cdf(k=2, n=5, p=0.5)
0.5
</b>
The probability that the coin lands on heads 2 times or fewer is <b>0.5</b>.
<b>Question 3:</b> It is known that 70% of individuals support a certain law. If 10 individuals are randomly selected, what is the probability that between 4 and 6 of them support the law?
<b>from scipy.stats import binom
#calculate binomial probability
binom.cdf(k=6, n=10, p=0.7) - binom.cdf(k=3, n=10, p=0.7)
0.3398
</b>
The probability that between 4 and 6 of the randomly selected individuals support the law is <b>0.3398</b>.
<h3>How to Visualize a Binomial Distribution</h3>
You can visualize a binomial distribution in Python by using the <b>seaborn</b> and <b>matplotlib</b> libraries:
<b>from numpy import random
import matplotlib.pyplot as plt
import seaborn as sns
x = random.binomial(n=10, p=0.5, size=1000)
sns.distplot(x, hist=True, kde=False)
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/binomialPython1.png">
The x-axis describes the number of successes during 10 trials and the y-axis displays the number of times each number of successes occurred during 1,000 experiments.
<h2><span class="orange">5 Real-Life Examples of the Binomial Distribution</span></h2>
The  Binomial distribution  is a probability distribution that is used to model the probability that a certain number of “successes” occur during a certain number of trials.
In this article we share 5 examples of how the Binomial distribution is used in the real world.
<h3>Example 1: Number of Side Effects from Medications</h3>
Medical professionals use the binomial distribution to model the probability that a certain number of patients will experience side effects as a result of taking new medications.
For example, suppose it is known that 5% of adults who take a certain medication experience negative side effects. We can use a  Binomial Distribution Calculator  to find the probability that more than a certain number of patients in a random sample of 100 will experience negative side effects.
P(X > 5 patients experience side effects) = <b>0.38400</b>
P(X > 10 patients experience side effects) = <b>0.01147</b>
P(X > 15 patients experience side effects) = <b>0.0004</b>
And so on.
This gives medical professionals an idea of how likely it is that more than a certain number of patients will experience negative side effects.
<h3>Example 2: Number of Fraudulent Transactions</h3>
Banks use the binomial distribution to model the probability that a certain number of credit card transactions are fraudulent.
For example, suppose it is known that 2% of all credit card transactions in a certain region are fraudulent. If there are 50 transactions per day in a certain region, we can use a  Binomial Distribution Calculator  to find the probability that more than a certain number of fraudulent transactions occur in a given day:
P(X > 1 fraudulent transaction) = <b>0.26423</b>
P(X > 2 fraudulent transactions) = <b>0.07843</b>
P(X > 3 fraudulent transactions) = <b>0.01776</b>
And so on.
This gives banks an idea of how likely it is that more than a certain number of fraudulent transactions will occur in a given day.
<h3>Example 3: Number of Spam Emails per Day</h3>
Email companies use the binomial distribution to model the probability that a certain number of spam emails land in an inbox per day.
For example, suppose it is known that 4% of all emails are spam. If an account receives 20 emails in a given day, we can use a  Binomial Distribution Calculator  to find the probability that a certain number of those emails are spam:
P(X = 0 spam emails) = <b>0.44200</b>
P(X = 1 spam email) = <b>0.36834</b>
P(X = 2 spam emails) = <b>0.14580</b>
And so on.
<h3>Example 4: Number of River Overflows</h3>
Park systems use the binomial distribution to model the probability that rivers overflow a certain number of times each year due to excessive rain.
For example, suppose it is known that a given river overflows during 5% of all storms. If there are 20 storms in a given year, we can use a  Binomial Distribution Calculator  to find the probability that the river overflows a certain number of times:
P(X = 0 overflows) = <b>0.35849</b>
P(X = 1 overflow) = <b>0.37735</b>
P(X = 2 overflows) = <b>0.18868</b>
And so on.
This gives the parks departments an idea of how many times they may need to prepare for overflows throughout the year.
<h3>Example 5: Shopping Returns per Week</h3>
Retail stores use the binomial distribution to model the probability that they receive a certain number of shopping returns each week.
For example, suppose it is known that 10% of all orders get returned at a certain store each week. If there are 50 orders that week, we can use a  Binomial Distribution Calculator  to find the probability that the store receives more than a certain number of returns that week:
P(X > 5 returns) = <b>0.18492</b>
P(X > 10 returns) = <b>0.00935</b>
P(X > 15 returns) = <b>0.00002</b>
And so on.
This gives the store an idea of how many customer service reps they need to have in the store that week to handle returns.
<h2><span class="orange">Understanding the Shape of a Binomial Distribution</span></h2>
The  binomial distribution  describes the probability of obtaining <em>k</em> successes in <em>n</em> binomial experiments.
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
The binomial probability distribution tends to be bell-shaped when one or more of the following two conditions occur:
<b>1.</b> The sample size (n) is large.
<b>2.</b> The probability of success on a given trial (p) is close to 0.5.
However, the binomial probability distribution tends to be skewed when neither of these conditions occur. To illustrate this, consider the following examples:
<h3>Example 1: Sample Size (n) is Large</h3>
The following chart displays the probability distribution for when n = <b>200 </b>and p = <b>0.5</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape1.png">
The x-axis displays the number of successes during 200 trials and the y-axis displays the probability of that number of successes occurring.
Since both <b>(1) </b>the sample size is large and <b>(2) </b>the probability of success on a given trial is close to 0.5, the probability distribution is bell-shaped.
Even when the probability of success on a given trial (p) is not close to 0.5, the probability distribution will still be bell-shaped as long as the sample size (n) is large. To illustrate this, consider the following two scenarios when p = 0.2 and p = 0.8.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape2.png">
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape3.png">
Notice how the probability distribution is bell-shaped in both scenarios.
<h3>Example 2: Probability of Success (p) is Close to 0.5</h3>
The following chart displays the probability distribution for when n = <b>10 </b>and p = <b>0.4</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape4.png">
Although the sample size (n = 10) is small, the probability distribution is still bell-shaped because the probability of success on a given trial (p = 0.4) is close to 0.5
<h3>Example 3: Skewed Binomial Distributions</h3>
When neither<b> (1) </b>the sample size is large nor <b>(2) </b>the probability of success on a given trial is close to 0.5, the binomial probability distribution will be skewed to the left or right.
For example, the following plot shows the probability distribution when n = <b>20 </b>and p = <b>0.1</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape5.png">
Notice how the distribution is skewed to the right.
And the following plot shows the probability distribution when n = <b>20 </b>and p = <b>0.9</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/binomialShape6.png">
Notice how the distribution is skewed to the left.
<h3>Ending Notes</h3>
Each of the charts in this post were created using the statistical programming language R. Learn how to plot your own binomial probability distributions in R using  this tutorial .
<h2><span class="orange">Binomial Distribution Table</span></h2>
  
<h2><span class="orange">An Introduction to the Binomial Distribution</span></h2>
The <b>binomial distribution </b>is one of the most popular distributions in statistics. To understand the binomial distribution, it helps to first understand  binomial experiments .
<h3>Binomial Experiments</h3>
A <b>binomial experiment</b> is an experiment that has the following properties:
The experiment consists of <em>n </em>repeated trials.
Each trial has only two possible outcomes.
The probability of success, denoted <em>p</em>, is the same for each trial.
Each trial is independent.
The most obvious example of a binomial experiment is a coin flip. For example, suppose we flip a coin 10 times. This is a binomial experiment because it has the following four properties:
The experiment consists of <em>n </em>repeated trials – There are 10 trials.
Each trial has only two possible outcomes – heads or tails.
The probability of success, denoted <em>p</em>, is the same for each trial – If we define “success” as landing on heads, then the probability of success is exactly 0.5 for each trial.
Each trial is independent – The outcome of one coin flip does not affect the outcome of any other coin flip.
<h3>The Binomial Distribution</h3>
The <b>binomial distribution</b> describes the probability of obtaining <em>k</em> successes in <em>n</em> binomial experiments.
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0, 1, 2, and 3 heads during these 3 flips:
<b>P(X=0) </b>= <sub>3</sub>C<sub>0</sub> * .5<sup>0</sup> * (1-.5)<sup>3-0</sup> = 1 * 1 * (.5)<sup>3</sup> = <b>0.125</b>
<b>P(X=1) </b>= <sub>3</sub>C<sub>1</sub> * .5<sup>1</sup> * (1-.5)<sup>3-1</sup> = 3 * .5 * (.5)<sup>2</sup> = <b>0.375</b>
<b>P(X=2) </b>= <sub>3</sub>C<sub>2</sub> * .5<sup>2</sup> * (1-.5)<sup>3-2</sup> = 3 * .25 * (.5)<sup>1</sup> = <b>0.375</b>
<b>P(X=3) </b>= <sub>3</sub>C<sub>3</sub> * .5<sup>3</sup> * (1-.5)<sup>3-3</sup> = 1 * .125 * (.5)<sup>0</sup> = <b>0.125</b>
<b><em>Note:</em></b><em> We used this  Combination Calculator  to calculate </em><sub><em>n</em></sub><em>C</em><sub><em>k</em></sub><em> for each example.</em>
We can create a simple histogram to visualize this probability distribution:
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomDist1.png"></figure>
<h3>Calculating Cumulative Binomial Probabilities</h3>
It’s straightforward to calculate a single binomial probability (e.g. the probability of a coin landing on heads 1 time out of 3 flips) using the formula above, but to calculate cumulative binomial probabilities we need to add individual probabilities.
For example, suppose we want to know the probability that a coin lands on heads 1 time or less out of 3 flips. We would use the following formula to calculate this probability:
<b>P(X≤1)</b> = P(X=0) + P(X=1) = 0.125 + 0.375 = <b>0.5</b>.
This is known as a <b>cumulative probability</b> because it involves adding more than one probability. We can calculate the cumulative probability of obtaining <em>k</em> or less heads for each outcome using a similar formula:
<b>P(X≤0)</b> = P(X=0) = <b>0.125</b>.
<b>P(X≤1)</b> = P(X=0) + P(X=1) = 0.125 + 0.375 = <b>0.5</b>.
<b>P(X≤2)</b> = P(X=0) + P(X=1) + P(X=2) = 0.125 + 0.375 + 0.375 = <b>0.875</b>.
<b>P(X≤3)</b> = P(X=0) + P(X=1) + P(X=2) + P(X=3) = 0.125 + 0.375 + 0.375 + 0.125 = <b>1</b>.
We can create a histogram to visualize this cumulative probability distribution:
<figure><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomDist2.png"></figure>
<h3>Binomial Probability Calculator</h3>
When we’re working with small numbers (e.g. 3 coin flips), it’s reasonable to calculate binomial probabilities by hand. However, when we’re working with larger numbers (e.g. 100 coin flips), it can be cumbersome to calculate probabilities by hand. In these cases, it can be helpful to use a <b>binomial probability calculator</b> like the one below.
For example, suppose we flip a coin n = 100 times, the probability that it lands on heads in a given trial is p = 0.5, and we want to know the probability that it will land on heads k = 43 times or less:
<label for="p"><b>p</b> (probability of success on a given trial)</label> <input id="p" min="0" type="number" value="0.5">
<label for="n"><b>n</b> (number of trials)</label> <input id="n" min="0" type="number" value="100">
<label for="k"><b>k</b> (number of successes)</label> <input id="k" min="0" type="number" value="43">
<input id="button" type="button" value="Calculate" onclick="pvalue()">
P(X=43) = 0.03007
P(X&lt;43) = 0.06661
P(X≤43) = 0.09667
P(X>43) = 0.90333
P(X≥43) = 0.93339
<script>
function pvalue() {
//get input values
var p = document.getElementById('p').value*1;
var n = document.getElementById('n').value*1;
var k = document.getElementById('k').value*1;
//assign probabilities to variable names
var exactProb = jStat.binomial.pdf(k,n,p);
var lessProb = jStat.binomial.cdf(k-1,n,p);
var lessEProb = jStat.binomial.cdf(k,n,p);
var greaterProb = 1-jStat.binomial.cdf(k,n,p);
var greaterEProb = 1-jStat.binomial.cdf(k-1,n,p);
//output probabilities
document.getElementById('k1').innerHTML = k;
document.getElementById('k2').innerHTML = k;
document.getElementById('k3').innerHTML = k;
document.getElementById('k4').innerHTML = k;
document.getElementById('k5').innerHTML = k;
document.getElementById('exactProb').innerHTML = exactProb.toFixed(5);
document.getElementById('lessProb').innerHTML = lessProb.toFixed(5);
document.getElementById('lessEProb').innerHTML = lessEProb.toFixed(5);
document.getElementById('greaterProb').innerHTML = greaterProb.toFixed(5);
document.getElementById('greaterEProb').innerHTML = greaterEProb.toFixed(5);
}
</script>Here is how to interpret the output:
The probability that the coin lands on heads exactly 43 times is <b>0.03007</b>.
The probability that the coin lands on heads less than 43 times is <b>0.06661</b>.
The probability that the coin lands on heads 43 times or less is <b>0.09667</b>.
The probability that the coin lands on heads more than 43 times is <b>0.90333</b>.
The probability that the coin lands on heads 43 times or more is <b>0.93339</b>.
<h3>Properties of the Binomial Distribution</h3>
The binomial distribution has the following properties:
The mean of the distribution is<b> μ = np</b>
The variance of the distribution is <b>σ</b><sup><b>2</b></sup><b> = np(1-p)</b>
The standard deviation of the distribution is <b> σ = √np(1-p)</b>
For example, suppose we toss a coin 3 times. Let p = the probability the coin lands on heads.
The mean number of heads we would expect is μ = np = 3*.5 = <b>1.5</b>.
The variance in the number of heads we would expect is σ<sup>2</sup> = np(1-p) = 3*.5*(1-.5) = <b>0.75</b>.
<h3>Binomial Distribution Practice Problems</h3>
Use the following practice problems to test your knowledge of the binomial distribution.
<b>Problem 1</b>
<b>Question: </b>Bob makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?
<b>Answer:</b> Using the Binomial Distribution Calculator above with p = 0.6, n = 12, and k = 10, we find that P(X=10) = <b>0.06385</b>.
<b>Problem 2</b>
<b>Question:</b> Jessica flips a coin 5 times. What is the probability that the coin lands on heads 2 times or fewer?
<b>Answer:</b> Using the Binomial Distribution Calculator above with p = 0.5, n = 5, and k = 2, we find that P(X≤2) = <b>0.5</b>.
<b>Problem 3</b>
<b>Question:</b> The probability that a given student gets accepted to a certain college is 0.2. If 10 students apply, what is the probability that more than 4 get accepted?
<b>Answer:</b> Using the Binomial Distribution Calculator above with p = 0.2, n = 10, and k = 4, we find that P(X>4) = <b>0.03279</b>.
<b>Problem 4</b>
<b>Question: </b>You flip a coin 12 times. What is the mean expected number of heads that will show up?
<b>Answer:</b> Recall that the mean of a binomial distribution is calculated as μ = np. Thus, μ = 12*0.5 = <b>6 heads</b>.
<b>Problem 5</b>
<b>Question:</b> Mark hits a home run during 10% of his attempts. If he has 5 attempts in a given game, what is the variance of the number of home runs he’ll hit?
<b>Answer:</b> Recall that the variance of a binomial distribution is calculated as σ<sup>2</sup> = np(1-p). Thus, σ<sup>2</sup> = 6*.1*(1-.1) = <b>0.54</b>.
The following articles can help you learn how to work with the binomial distribution in different statistical softwares:
 How to calculate binomial probabilities in Excel 
 How to calculate binomial probabilities on a TI-84 calculator 
 How to calculate binomial probabilities in R 
 How to plot a binomial distribution in R 
<h2><span class="orange">Binomial Experiments: An Explanation + Examples</span></h2>
Understanding binomial experiments is the first step to understanding  the binomial distribution .
This tutorial defines a binomial experiment and provides several examples of experiments that <em>are</em> and <em>are not </em>considered to be binomial experiments. 
<h2>Binomial Experiment: Definition</h2>
A <b>binomial experiment </b>is an experiment that has the following four properties:
<b>1. The experiment consists of <em>n </em>repeated trials.</b> The number <em>n </em>can be any amount. For example, if we flip a coin 100 times, then <em>n </em>= 100. 
<b>2. Each trial has only two possible outcomes.</b> We often call outcomes either a “success” or a “failure” but a “success” is just a label for something we’re counting. For example, when we flip a coin we might call a head a “success” and a tail a “failure.”
<b>3. The probability of success, denoted <em>p</em>, is the same for each trial. </b>In order for an experiment to be a true binomial experiment, the probability of “success” must be the same for each trial. For example, when we flip a coin, the probability of getting heads (“success”) is always the same each time we flip the coin.
<b>4. Each trial is independent</b>. This simply means that the outcome of one trial does not affect the outcome of another trial. For example, suppose we flip a coin and it lands on heads. The fact that it landed on heads doesn’t change the probability that it will land on heads on the next flip. Each flip (i.e. each “trial”) is independent.
<h2>Examples of Binomial Experiments</h2>
The following experiments are all examples of binomial experiments.
<h3>Example #1</h3>
<em><b>Flip a coin 10 times. Record the number of times that it lands on tails.</b></em>
This is a binomial experiment because it has the following four properties:
<b>The experiment consists of <em>n </em>repeated trials.</b> In this case, there are 10 trials.
<b>Each trial has only two possible outcomes. </b>The coin can only land on heads or tails.
<b>The probability of success is the same for each trial</b>. If we define “success” as landing on heads, then the probability of success is exactly 0.5 for each trial.
<b>Each trial is independent</b>. The outcome of one coin flip does not affect the outcome of any other coin flip.
<h3>Example #2</h3>
<em><b>Roll a fair 6-sided die 20 times. Record the number of times that a 2 comes up.</b></em>
This is a binomial experiment because it has the following four properties:
<b>The experiment consists of <em>n </em>repeated trials.</b> In this case, there are 20 trials.
<b>Each trial has only two possible outcomes. </b>If we define a 2 as a “success” then each time the die either lands on a 2 (a success) or some other number (a failure).
<b>The probability of success is the same for each trial</b>. For each trial, the probability that the die lands on a 2 is 1/6. This probability does not change from one trial to the next.
<b>Each trial is independent</b>. The outcome of one die roll does not affect the outcome of any other die roll.
<h3>Example #3</h3>
<em><b>Tyler makes 70% of his free-throw attempts. Suppose he makes 15 attempts. Record the number of baskets he makes.</b></em>
This is a binomial experiment because it has the following four properties:
<b>The experiment consists of <em>n </em>repeated trials.</b> In this case, there are 15 trials.
<b>Each trial has only two possible outcomes. </b>For each attempt, Tyler either makes the basket or misses it.
<b>The probability of success is the same for each trial</b>. For each trial, the probability that Tyler makes the basket is 70%. This probability does not change from one trial to the next.
<b>Each trial is independent</b>. The outcome of one free-throw attempt does not affect the outcome of any other free-throw attempt.
<h2>Examples that are <em>not</em> Binomial Experiments</h2>
<h3>Example #1</h3>
<em><b>Ask 100 people how old they are</b></em><b>.</b>
This is <em>not </em>a binomial experiment because there are more than two possible outcomes.
<h3>Example #2</h3>
<b><i>Roll a fair 6-sided die until a 5 comes up.</i></b>
This is <em>not </em>a binomial experiment because there is not a pre-defined <em>n </em>number of trials. We have no idea how many rolls it will take until a 5 comes up.
<h3>Example #3</h3>
<b><i>Pull 5 cards from a deck of cards. </i></b>
This is <em>not </em>a binomial experiment because the outcome of one trial (e.g. pulling a certain card from the deck) affects the outcome of future trials.
<h2>A Binomial Experiment Example & Solution</h2>
The following example shows how to solve a question about a binomial experiment.
<b><em>You flip a coin 10 times. What is the probability that the coin lands on heads exactly 7 times?</em></b>
Whenever we’re interested in finding the probability of <em>n </em>successes in a binomial experiment, we must use the following formula:
P(exactly <em>k </em>successes) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup>
where:
<b>n: </b>the number of trials
<b>k:</b> the number of successes
<b>C: </b>the symbol for “combination”
<b>p: </b>probability of success on a given trial
Plugging these numbers into the formula, we get:
P(7 heads) = <sub>10</sub>C<sub>7</sub> * 0.5<sup>7</sup> * (1-0.5)<sup>10-7</sup> = (120) * (.0078125) * (.125) = <b>0.11719</b>.
Thus, the probability that the coin lands on heads 7 times is <b>0.11719</b>.
<h2><span class="orange">How to Calculate Binomial Probabilities on a TI-84 Calculator</span></h2>
The  binomial distribution  is one of the most commonly used distributions in all of statistics. This tutorial explains how to use the following functions on a TI-84 calculator to find binomial probabilities:
<b>binompdf(n, p, x) </b>returns the probability associated with the binomial pdf.
<b>binomcdf(n, p, x) </b>returns the cumulative probability associated with the binomial cdf.
where:
<b>n </b>= number of trials
<b>p </b>= probability of success on a given trial
<b>x </b>= total number of successes
Both of these functions can be accessed on a TI-84 calculator by pressing 2nd and then pressing vars. This will take you to a <b>DISTR </b>screen where you can then use <b>binompdf() </b>and <b>binomcdf()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomTI1.png">
The following examples illustrate how to use these functions to answer different questions.
<h3>Example 1: Binomial probability of exactly x successes</h3>
<b>Question: </b>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes exactly 10?
<b>Answer: </b>Use the function binomialpdf(n, p, x):
<b>binomialpdf(12, .60, 10) = 0.0639</b>
<h3>Example 2: Binomial probability of less than x successes</h3>
<b>Question: </b>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes less than 10?
<b>Answer: </b>Use the function<b> binomialcdf(n, p, x-1)</b>:
<b>binomialcdf(12, .60, 9) = 0.9166</b>
<h3>Example 3: Binomial probability of at most x successes</h3>
<b>Question: </b>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes at most 10?
<b>Answer: </b>Use the function<b> binomialcdf(n, p, x)</b>:
<b>binomialcdf(12, .60, 10) = 0.9804</b>
<h3>Example 4: Binomial probability of more than x successes</h3>
<b>Question: </b>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes more than 10?
<b>Answer: </b>Use the function<b> 1 – binomialcdf(n, p, x)</b>:
<b>1 – binomialcdf(12, .60, 10) = 0.0196</b>
<h3>Example 5: Binomial probability of at least x successes</h3>
<b>Question: </b>Nathan makes 60% of his free-throw attempts. If he shoots 12 free throws, what is the probability that he makes more than 10?
<b>Answer: </b>Use the function<b> 1 – binomialcdf(n, p, x-1)</b>:
<b>1 – binomialcdf(12, .60, 9) = 0.0834</b>
<h2><span class="orange">Binomial Standard Deviation Calculator</span></h2>
The <b>standard deviation for the binomial distribution</b> is defined as:
σ = √n*p*(1<U+2212>p)
where <i>n</i> is the sample size and <i>p</i> is the population proportion.
To calculate the standard deviation for a given binomial distribution, simply fill in the values below and then click the “Calculate” button.
<label for="p"><b>Population proportion (p)</b></label>
<input type="number" id="p" value="0.43"><label for="n"><b>Sample size (n)</b></label>
<input type="number" id="n" value="40">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
σ = <b>16.56</b>
<b>Explanation:</b>
σ = √n*p*(1<U+2212>p)
σ = √40*0.43*(1<U+2212>0.43)
σ = <b>16.56</b>
<script>
function calc() {
//get input values
var p = document.getElementById('p').value*1;
var n = document.getElementById('n').value*1;
//calculate stuff
var result = Math.sqrt(n*p*(1-p));
//output
document.getElementById('result').innerHTML = result.toFixed(3);
document.getElementById('result2').innerHTML = result.toFixed(3);
document.getElementById('n2').innerHTML = n;
document.getElementById('p2').innerHTML = p;
document.getElementById('p3').innerHTML = p;
}
</script>
<h2><span class="orange">How to Perform a Binomial Test in Excel</span></h2>
A <b>binomial test</b> compares a sample proportion to a hypothesized proportion.
For example, suppose we have a 6-sided die. If we roll it 24 times, we would expect the number “3” to show up 1/6 of the time, e.g. 24 * (1/6) = 4 times.
If the number “3” actually shows up 6 times, is that evidence that the die is biased towards the number “3”? We could perform a binomial test to answer that question.
In Excel, we can use the following function to perform a binomial test:
<b>BINOM.DIST(number_s, trials, probability_s, cumulative)</b>
where:
<b>number_s: </b>number of “successes”
<b>trials: </b>total number of trials
<b>probability_s: </b>the probability of success on each trial
<b>cumulative: </b>If TRUE, then BINOM.DIST returns the cumulative distribution function, which is the probability that there are at most number_s successes; if FALSE, it returns the probability mass function, which is the probability that there are number_s successes. We will almost always use TRUE.
The following examples illustrate how to perform binomial tests in Excel.
<b>Example 1: We roll a 6-sided die 24 times and it lands on the number “3” exactly 6 times. Perform a binomial test to determine if the die is biased towards the number “3.”</b>
The null and alternative hypotheses for our test are as follows:
H<sub>0</sub>: π ≤ 1/6 (the die is not biased towards the number “3”)
H<sub>A</sub>: π > 1/6
<em>*π is the symbol for population proportion.</em>
We will enter the following formula into Excel:
P(x ≥ 6) = 1 – BINOM.DIST(5, 24, 1/6, TRUE) = 1 – 0.80047 = <b>0.19953</b>.
Because this p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say the die is biased towards the number “3.”
<b>Example 2: We flip a coin 30 times and it lands on heads exactly 19 times. Perform a binomial test to determine if the coin is biased towards heads.</b>
The null and alternative hypotheses for our test are as follows:
H<sub>0</sub>: π ≤ 1/2 (the coin is not biased towards heads)
H<sub>A</sub>: π > 1/2
We will enter the following formula into Excel:
P(x ≥ 19) = 1 – BINOM.DIST(18, 30, 1/2, TRUE) = 1 – 0.89976 = <b>0.10024</b>.
Because this p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say the coin is biased towards heads.
<b>Example 3: </b>A shop makes widgets with 80% effectiveness. They implement a new system that they hope will improve the rate of effectiveness. They randomly select 50 widgets from a recent production run and find that 46 of them are effective. Perform a binomial test to determine if the new system leads to higher effectiveness.
The null and alternative hypotheses for our test are as follows:
H<sub>0</sub>: π ≤ 0.80 (the new system does not lead to an increase in effectiveness)
H<sub>A</sub>: π > 0.80
We will enter the following formula into Excel:
P(x ≥ 46) = 1 – BINOM.DIST(45, 50, 0.8, TRUE) = 1 – 0.9815 = <b>0.0185</b>.
Because this p-value is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say the new system leads to an increase in effectiveness.
<b>Example 4: </b>A shop makes gadgets with 60% reliability. They implement a new process that they hope will improve the reliability. They randomly select 40 gadgets from a recent production run. What is the minimum number of gadgets that need to be reliable in order for the shop to say, with 95% confidence, that the new process improves the reliability?
For this example we will need to use the following function:
<b>BINOM.INV(trials, probability_s, alpha)</b>
where:
<b>trials: </b>total number of trials
<b>probability_s: </b>probability of “success” on each trial
<b>alpha:</b> significance level
We will enter the following formula into Excel:
BINOM.INV(40, 0.60, 0.95) = <b>29</b>.
Thus, we would need at least 29 of the gadgets to be reliable in order to say, with 95% confidence, that the new process improves reliability.
<h2><span class="orange">How to Perform a Binomial Test in Python</span></h2>
A <b>binomial test</b> compares a sample proportion to a hypothesized proportion.
For example, suppose we have a 6-sided die. If we roll it 12 times, we would expect the number “3” to show up 1/6 of the time, which would be 12 * (1/6) = 2 times.
If the number “3” actually shows up 4 times, is that evidence that the die is biased towards the number “3”? We could perform a binomial test to answer that question.
In Python, we can perform a binomial test using the  binom_test() function  from the scipy.stats library, which uses the following syntax:
<b>binom_test(x, n=None, p=0.5, alternative=’two-sided’)</b>
where:
<b>x: </b>number of “successes”
<b>n: </b>total number of trials
<b>p: </b>the probability of success on each trial
<b>alternative: </b>the alternative hypothesis. Default is ‘two-sided’ but you can also specify ‘greater’ or ‘less.’
This function returns the p-value of the test. We can load this function by using the following sytax:
<b>from scipy.stats import binom_test
</b>
The following examples illustrate how to perform binomial tests in Python.
<b>Example 1: We roll a 6-sided die 24 times and it lands on the number “3” exactly 6 times. Perform a binomial test to determine if the die is biased towards the number “3.”</b>
The null and alternative hypotheses for our test are as follows:
<b>H<sub>0</sub>:</b> π ≤ 1/6 (the die is not biased towards the number “3”)
<b>H<sub>A</sub>:</b> π > 1/6
<em>*π is the symbol for population proportion.</em>
We will enter the following formula into Python:
<b>binom_test(x=6, n=24, p=1/6, alternative='greater')
0.1995295129479586
</b>
Because this p-value (0.1995) is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say the die is biased towards the number “3.”
<b>Example 2: We flip a coin 30 times and it lands on heads exactly 19 times. Perform a binomial test to determine if the coin is biased towards heads.</b>
The null and alternative hypotheses for our test are as follows:
<b>H<sub>0</sub>:</b> π ≤ 1/2 (the coin is not biased towards heads)
<b>H<sub>A</sub>:</b> π > 1/2
We will enter the following formula into Python:
<b>binom_test(x=19, n=30, p=1/2, alternative='greater')
0.10024421103298661
</b>
Because this p-value (0.10024) is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say the coin is biased towards heads.
<b>Example 3: A shop makes widgets with 80% effectiveness. They implement a new system that they hope will improve the rate of effectiveness. They randomly select 50 widgets from a recent production run and find that 47 of them are effective. Perform a binomial test to determine if the new system leads to higher effectiveness.</b>
The null and alternative hypotheses for our test are as follows:
<b>H<sub>0</sub>:</b> π ≤ 0.80 (the new system does not lead to an increase in effectiveness)
<b>H<sub>A</sub>:</b> π > 0.80
We will enter the following formula into Python:
<b>binom_test(x=47, n=50, p=0.8, alternative='greater')
0.005656361012155314
</b>
Because this p-value (0.00565) is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say the new system leads to an increase in effectiveness.
<h2><span class="orange">How to Perform a Binomial Test in R</span></h2>
A <b>binomial test</b> compares a sample proportion to a hypothesized proportion. The test has the following null and alternative hypotheses:
<b>H</b><sub><b>0</b></sub>: π = p (the population proportion π is equal to some value p)
<b>H<sub>A</sub></b>: π ≠ p (the population proportion π is not equal to some value p)
<em>The test can also be performed with a one-tailed alternative that the true population proportion is greater than or less than some value p.</em>
To perform a binomial test in R, you can use the following function:
<b>binom.test(x, n, p)</b>
where:
<b>x:</b> number of successes
<b>n:</b> number of trials
<b>p:</b> probability of success on a given trial
The following examples illustrate how to use this function in R to perform binomial tests.
<b>Example 1: Two-tailed Binomial Test</b>
You want to determine whether or not a die lands on the number “3” during 1/6 of the rolls so you roll the die 24 times and it lands on “3” a total of 9 times.  Perform a Binomial test to determine if the die actually lands on “3” during 1/6 of rolls.
<b>#perform two-tailed Binomial test
binom.test(9, 24, 1/6)
#output
Exact binomial test
data:  9 and 24
number of successes = 9, number of trials = 24, p-value = 0.01176
alternative hypothesis: true probability of success is not equal to 0.1666667
95 percent confidence interval:
 0.1879929 0.5940636
sample estimates:
probability of success  0.375 
</b>
The p-value of the test is <b>0.01176</b>. Since this is less than 0.05, we can reject the null hypothesis and conclude that there is evidence to say the die does <em>not </em>land on the number “3” during 1/6 of the rolls.
<b>Example 2: Left-tailed Binomial Test</b>
You want to determine whether or not a coin is less likely to land on heads compared to tails so you flip the coin 30 times and find that it lands on heads just 11 times. Perform a Binomial test to determine if the coin is actually less likely to land on heads compared to tails.
<b>#perform left-tailed Binomial test
binom.test(11, 30, 0.5, alternative="less")
#output
Exact binomial test
data:  11 and 30
number of successes = 11, number of trials = 30, p-value = 0.1002
alternative hypothesis: true probability of success is less than 0.5
95 percent confidence interval:
 0.0000000 0.5330863
sample estimates:
probability of success 
             0.3666667 </b>
The p-value of the test is <b>0.1002</b>. Since this is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the coin is less likely to land on heads compared to tails.
<b>Example 3: Right-tailed Binomial Test</b>
A shop makes widgets with 80% effectiveness. They implement a new system that they hope will improve the rate of effectiveness. They randomly select 50 widgets from a recent production run and find that 46 of them are effective. Perform a binomial test to determine if the new system leads to higher effectiveness.
<b>#perform right-tailed Binomial test
binom.test(46, 50, 0.8, alternative="greater")
#output
Exact binomial test
data:  46 and 50
number of successes = 46, number of trials = 50, p-value = 0.0185
alternative hypothesis: true probability of success is greater than 0.8
95 percent confidence interval:
 0.8262088 1.0000000
sample estimates:
probability of success   0.92 
</b>
The p-value of the test is <b>0.0185</b>. Since this is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say that the new system produces effective widgets at a higher rate than 80%.
<h2><span class="orange">Binomial vs. Geometric Distribution: Similarities & Differences</span></h2>
Two commonly used distributions in statistics are the  binomial distribution  and the  geometric distribution .
This tutorial provides a brief explanation of each distribution along with the similarities and differences between the two.
<h3>The Binomial Distribution</h3>
The <b>binomial distribution</b> describes the probability of obtaining <em>k</em> successes in <em>n</em>  binomial experiments .
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0 heads during these 3 flips:
<b>P(X=0) </b>= <sub>3</sub>C<sub>0</sub> * .5<sup>0</sup> * (1-.5)<sup>3-0</sup> = 1 * 1 * (.5)<sup>3</sup> = <b>0.125</b>
<h3>The Geometric Distribution</h3>
The <b>geometric distribution</b> describes the probability of experiencing a certain amount of failures before experiencing the first success in a series of binomial experiments.
If a  random variable  <em>X</em> follows a geometric distribution, then the probability of experiencing <em>k</em> failures before experiencing the first success can be found by the following formula:
<b>P(X=k) = (1-p)<sup>k</sup>p</b>
where:
<b>k: </b>number of failures before first success
<b>p: </b>probability of success on each trial
For example, suppose we want to know how many times we’ll have to flip a fair coin until it lands on heads. We can use the formula above to determine the probability of experiencing 3 “failures” before the coin finally lands on heads:
<b>P(X=3) </b>= (1-.5)<sup>3</sup>(.5) = <b>0.0625</b>
<h3>Similarities & Differences</h3>
The binomial and geometric distribution share the following <b>similarities</b>:
The outcome of the experiments in both distributions can be classified as “success” or “failure.”
The probability of success is the same for each trial.
Each trial is independent.
The distributions share the following key <b>difference</b>:
In a binomial distribution, there is a fixed number of trials (i.e. flip a coin 3 times)
In a geometric distribution, we’re interested in the number of trials required <em>until</em> we obtain a success (i.e. how many flips will we need to make before we see Tails?)
<h3>Practice Problems: When to Use Each Distribution</h3>
In each of the following practice problems, determine whether the random variable follows a binomial distribution or geometric distribution.
<b>Problem 1: Rolling Dice</b>
Jessica plays a game of luck in which she keeps rolling a dice until it lands on the number 4. Let <em>X</em> be the number of rolls until a 4 appears. What type of distribution does the random variable <em>X</em> follow?
Answer: <em>X</em> follows a geometric distribution because we’re interested in estimating the number of rolls required until we finally get a 4. This is not a binomial distribution because there is not a fixed number of trials.
<b>Problem 2: Shooting Free-Throws</b>
Tyler makes 80% of all free-throws he attempts. Suppose he shoots 10 free-throws. Let <em>X</em> be the number of times Tyler makes a basket during the 10 attempts. What type of distribution does the random variable <em>X</em> follow?
Answer: <em>X</em> follows a binomial distribution because there is a fixed number of trials (10 attempts), the probability of “success” on each trial is the same, and each trial is independent.
<h2><span class="orange">Binomial vs. Poisson Distribution: Similarities & Differences</span></h2>
Two distributions that are similar in statistics are the  Binomial distribution  and the  Poisson distribution .
This tutorial provides a brief explanation of each distribution along with the similarities and differences between the two.
<h3>The Binomial Distribution</h3>
The <b>Binomial distribution</b> describes the probability of obtaining <em>k</em> successes in <em>n</em>  binomial experiments .
If a  random variable  <em>X</em> follows a binomial distribution, then the probability that <em>X</em> = <em>k</em> successes can be found by the following formula:
<b>P(X=k) = <sub>n</sub>C<sub>k</sub> * p<sup>k</sup> * (1-p)<sup>n-k</sup></b>
where:
<b>n:</b> number of trials
<b>k: </b>number of successes
<b>p:</b> probability of success on a given trial
<b><sub>n</sub>C<sub>k</sub>: </b>the number of ways to obtain <em>k</em> successes in <em>n</em> trials
For example, suppose we flip a coin 3 times. We can use the formula above to determine the probability of obtaining 0 heads during these 3 flips:
<b>P(X=0) </b>= <sub>3</sub>C<sub>0</sub> * .5<sup>0</sup> * (1-.5)<sup>3-0</sup> = 1 * 1 * (.5)<sup>3</sup> = <b>0.125</b>
<h3>The Poisson Distribution</h3>
The <b>Poisson distribution</b> describes the probability of experiencing <em>k</em> events during a fixed time interval.
If a  random variable  <em>X</em> follows a Poisson distribution, then the probability that <em>X</em> = <em>k</em> events can be found by the following formula:
<b>P(X=k) = λ<sup>k</sup> * e<sup>– λ</sup> / k!</b>
where:
<b>λ: </b>mean number of successes that occur during a specific interval
<b>k: </b>number of successes
<b>e: </b>a constant equal to approximately 2.71828
For example, suppose a particular hospital experiences an average of 2 births per hour. We can use the formula above to determine the probability of experiencing 3 births in a given hour:
<b>P(X=3) </b>= 2<sup>3</sup> * e<sup>– 2</sup> / 3! = <b>0.18045</b>
<h3>Similarities & Differences</h3>
The Binomial and Poisson distribution share the following <b>similarities</b>:
Both distributions can be used to model the number of occurrences of some event.
In both distributions, events are assumed to be independent.
The distributions share the following key <b>difference</b>:
In a Binomial distribution, there is a fixed number of trials (e.g. flip a coin 3 times)
In a Poisson distribution, there could be any number of events that occur during a certain time interval (e.g. how many customers will arrive at a store in a given hour?)
<h3>Practice Problems: When to Use Each Distribution</h3>
In each of the following practice problems, determine whether the random variable follows a Binomial distribution or Poisson distribution.
<b>Problem 1: Network Failures</b>
A tech company wants to model the probability that a certain number of network failures occur in a given week. Suppose it’s known that an average of 4 network failures occur each week. Let <em>X</em> be the number of network failures in a given week. What type of distribution does the random variable <em>X</em> follow?
Answer: <em>X</em> follows a Poisson distribution because we’re interested in modeling the number of network failures in a given week and there is no upper limit on the number of failures that could occur. This is not a Binomial distribution because there is not a fixed number of trials.
<b>Problem 2: Shooting Free-Throws</b>
Tyler makes 70% of all free-throws he attempts. Suppose he shoots 10 free-throws. Let <em>X</em> be the number of times Tyler makes a basket during the 10 attempts. What type of distribution does the random variable <em>X</em> follow?
Answer: <em>X</em> follows a Binomial distribution because there is a fixed number of trials (10 attempts), the probability of “success” on each trial is the same, and each trial is independent.
<h2><span class="orange">BinomPDF vs BinomCDF: The Difference (Plus Examples)</span></h2>
The  binomial distribution  is one of the most commonly used distributions in all of statistics.
On a TI-84 calculator there are two functions you can use to find probabilities related to the binomial distribution:
<b>binompdf(n, p, x)</b>: Finds the probability that <b>exactly <em>x</em> successes occur</b> during <em>n</em> trials where the probability of success on a given trial is equal to <em>p</em>.
<b>binomcdf(n, p, x)</b>: Finds the probability that <b><em>x</em> successes or fewer occur</b> during <em>n</em> trials where the probability of success on a given trial is equal to <em>p</em>.
You can access each of these functions on a TI-84 calculator by pressing 2nd and then pressing VARS. This will take you to a <b>DISTR </b>screen where you can then use <b>binompdf()</b> and <b>binomcdf()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/binomTI1.png">
The following examples show how to use each of these functions in practice.
<h3>Examples: How to Use Binompdf()</h3>
The following examples show how to use the <b>binompdf()</b> function.
<b>Example 1: Free-Throw Attempts</b>
Jessica makes 80% of her free-throw attempts. If she shoots 10 free throws, what is the probability that she makes exactly 7?
To answer this, we can type in the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/binompdf1.png">
The probability that she makes exactly 7 is <b>.2013</b>.
<b>Example 2: Fraudulent Transactions</b>
A bank knows that 3% of all transactions are fraudulent. If 20 transactions occur in a given day, what is the probability that exactly 2 are fraudulent?
To answer this, we can type in the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/binompdf2.png">
The probability that exactly 2 transactions are fraudulent is <b>.0988</b>.
<h3>Examples: How to Use Binomcdf()</h3>
The following examples show how to use the <b>binomcdf()</b> function.
<b>Example 1: Free-Throw Attempts</b>
Jessica makes 50% of her free-throw attempts. If she shoots 10 free throws, what is the probability that she makes 7 or less?
To answer this, we can type in the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/binompdf3.png">
The probability that she makes 7 or less free throws is <b>.9453</b>.
<b>Example 2: Fraudulent Transactions</b>
A bank knows that 3% of all transactions are fraudulent. If 20 transactions occur in a given day, what is the probability that more than 2 transactions are fraudulent?
To answer this, we can type in the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/binompdf4.png">
The probability that more than 2 transactions are fraudulent is <b>.021</b>.
<h2><span class="orange">How to Create a Biplot in R to Visualize PCA Results</span></h2>
<b>Principal components analysis</b> (PCA) is an  unsupervised  machine learning technique that seeks to find principal components that explain a large portion of the variation in a dataset.
To visualize the results of PCA for a given dataset we can create a <b>biplot</b>, which is a plot that displays every observation in a dataset on a plane that is formed by the first two principal components.
We can use the following basic syntax in R to create a biplot:
<b>#perform PCA
results &lt;- princomp(df)
#create biplot to visualize results of PCA
biplot(results)</b>
The following example shows how to use this syntax in practice.
<h3>Example: How to Create a Biplot in R</h3>
For this example we’ll use the built-in R dataset called <b>USArrests</b>:
<b>#view first six rows of <em>USArrests</em> dataset
head(USArrests)
           Murder Assault UrbanPop Rape
Alabama      13.2     236       58 21.2
Alaska       10.0     263       48 44.5
Arizona       8.1     294       80 31.0
Arkansas      8.8     190       50 19.5
California    9.0     276       91 40.6
Colorado      7.9     204       78 38.7
</b>
We can use the following code to perform PCA and visualize the results in a biplot:
<b>#perform PCA
results &lt;- princomp(USArrests)
#visualize results of PCA in biplot
biplot(results)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/biplot1.jpg"448">
The x-axis displays the first principal component, the y-axis displays the second principal component, and the individual  observations  from the dataset are shown inside the plot along with the four variables shown in red.
Note that there are several arguments we can use within the <b>biplot</b> function to modify the appearance of the plot.
For example, we can use the following code to modify the colors, font size, axis limits, plot title, axis titles, and size of the arrows in the plot:
<b>#create biplot with custom appearance
biplot(results,
       col=c('blue', 'red'),
       cex=c(1, 1.3),
       xlim=c(-.4, .4),
       main='PCA Results',
       xlab='First Component',
       ylab='Second Component',
       expand=1.2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/biplot2.jpg">
This biplot is a bit easier to read than the previous one.
You can find a full list of arguments that you can use to modify the appearance of the biplot  here .
<h2><span class="orange">How to Perform Bivariate Analysis in Excel (With Examples)</span></h2>
The term <b>bivariate analysis </b>refers to the analysis of two variables. You can remember this because the prefix “bi” means “two.”
The purpose of bivariate analysis is to understand the relationship between two variables
There are three common ways to perform bivariate analysis:
<b>1.</b> Scatterplots
<b>2.</b> Correlation Coefficients
<b>3.</b> Simple Linear Regression
The following example shows how to perform each of these types of bivariate analysis in Excel using the following dataset that contains information about two variables: <b>(1)</b> Hours spent studying and <b>(2)</b> Exam score received by 20 different students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv2.png">
<h3>1. Scatterplots</h3>
To create a scatterplot of hours vs. score, we can highlight cells <b>A2:B21</b>, then click the <b>Insert</b> tab along the top ribbon, then click <b>Insert Scatter Chart</b> within the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv3.png">
We can also modify the y-axis limits to gain a better view of the data points.
To do so, double click the y-axis. In the <b>Format Axis</b> panel that appears on the right side of the screen, click <b>Axis Options</b> and then change the <b>Minimum</b> and <b>Maximum</b> bounds to 60 and 100, respectively.
The y-axis will automatically update:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv4.png">
The x-axis shows the hours studied and the y-axis shows the exam score received.
From the plot we can see that there is a positive relationship between the two variables. As hours studied increases, exam score tends to increase as well.
<h3>2. Correlation Coefficients</h3>
A Pearson Correlation Coefficient is a way to quantify the linear relationship between two variables.
We can use the following formula in Excel to calculate the correlation coefficient between hours studied and exam score:
<b>=CORREL(A2:A21, B2:B21)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv5.png">
The correlation coefficient turns out to be <b>0.891</b>.
This value is close to 1, which indicates a strong positive correlation between hours studied and exam score received.
<h3>3. Simple Linear Regression</h3>
Simple linear regression is a statistical method we can use to quantify the relationship between two variables.
To fit a simple linear regression model in Excel, click the <b>Data</b> tab along the top ribbon, then click the <b>Data Analysis</b> option in the <b>Analyze</b> group. In the new panel that appears, click <b>Regression</b> and then click <b>OK</b>.
<b>Note</b>: If you don’t see the Data Analysis option, you need to first load the  Excel Analysis ToolPak .
In the panel that appears, enter the following information and then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv6.png">
Once you click <b>OK</b>, the results of the regression model will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv7.png">
The fitted regression equation turns out to be:
Exam Score = 69.0734 + 3.8471*(hours studied)
This tells us that each additional hour studied is associated with an average increase of <b>3.8471</b> in exam score.
We can also use the regression equation to estimate the score that a student will receive based on their total hours studied.
For example, a student who studies for 3 hours is estimated to receive a score of <b>81.6147</b>:
Exam Score = 69.0734 + 3.8471*(hours studied)
Exam Score = 69.0734 + 3.8471*(3)
Exam Score = 81.6147
<h2><span class="orange">How to Perform Bivariate Analysis in Python (With Examples)</span></h2>
The term <b>bivariate analysis </b>refers to the analysis of two variables. You can remember this because the prefix “bi” means “two.”
The purpose of bivariate analysis is to understand the relationship between two variables
There are three common ways to perform bivariate analysis:
<b>1.</b> Scatterplots
<b>2.</b> Correlation Coefficients
<b>3.</b> Simple Linear Regression
The following example shows how to perform each of these types of bivariate analysis in Python using the following pandas DataFrame that contains information about two variables: <b>(1)</b> Hours spent studying and <b>(2)</b> Exam score received by 20 different students:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'hours': [1, 1, 1, 2, 2, 2, 3, 3, 3, 3,             3, 4, 4, 5, 5, 6, 6, 6, 7, 8],   'score': [75, 66, 68, 74, 78, 72, 85, 82, 90, 82,             80, 88, 85, 90, 92, 94, 94, 88, 91, 96]})
#view first five rows of DataFrame
df.head()
hoursscore
0175
1166
2168
3274
4278</b>
<h3>1. Scatterplots</h3>
We can use the following syntax to create a scatterplot of hours studied vs. exam score:
<b>import matplotlib.pyplot as plt
#create scatterplot of hours vs. score
plt.scatter(df.hours, df.score)
plt.title('Hours Studied vs. Exam Score')
plt.xlabel('Hours Studied')
plt.ylabel('Exam Score')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivPython1.png">
The x-axis shows the hours studied and the y-axis shows the exam score received.
From the plot we can see that there is a positive relationship between the two variables: As hours studied increases, exam score tends to increase as well.
<h3>2. Correlation Coefficients</h3>
A Pearson Correlation Coefficient is a way to quantify the linear relationship between two variables.
We can use the <b>corr()</b> function in pandas to create a correlation matrix:
<b>#create correlation matrix
df.corr()
hoursscore
hours1.0000000.891306
score0.8913061.000000</b>
The correlation coefficient turns out to be <b>0.891</b>. This indicates a strong positive correlation between hours studied and exam score received.
<h3>3. Simple Linear Regression</h3>
Simple linear regression is a statistical method we can use to quantify the relationship between two variables.
We can use the <b>OLS()</b> function from the statsmodels package to quickly fit a  simple linear regression model  for hours studied and exam score received:
<b>import statsmodels.api as sm
#define response variable
y = df['score']
#define explanatory variable
x = df[['hours']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.794
Model:                            OLS   Adj. R-squared:                  0.783
Method:                 Least Squares   F-statistic:                     69.56
Date:                Mon, 22 Nov 2021   Prob (F-statistic):           1.35e-07
Time:                        16:15:52   Log-Likelihood:                -55.886
No. Observations:                  20   AIC:                             115.8
Df Residuals:                      18   BIC:                             117.8
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         69.0734      1.965     35.149      0.000      64.945      73.202
hours          3.8471      0.461      8.340      0.000       2.878       4.816
==============================================================================
Omnibus:                        0.171   Durbin-Watson:                   1.404
Prob(Omnibus):                  0.918   Jarque-Bera (JB):                0.177
Skew:                           0.165   Prob(JB):                        0.915
Kurtosis:                       2.679   Cond. No.                         9.37
==============================================================================
</b>
The fitted regression equation turns out to be:
Exam Score = 69.0734 + 3.8471*(hours studied)
This tells us that each additional hour studied is associated with an average increase of <b>3.8471</b> in exam score.
We can also use the fitted regression equation to predict the score that a student will receive based on their total hours studied.
For example, a student who studies for 3 hours is predicted to receive a score of <b>81.6147</b>:
Exam Score = 69.0734 + 3.8471*(hours studied)
Exam Score = 69.0734 + 3.8471*(3)
Exam Score = 81.6147
<h2><span class="orange">How to Perform Bivariate Analysis in R (With Examples)</span></h2>
The term <b>bivariate analysis </b>refers to the analysis of two variables. You can remember this because the prefix “bi” means “two.”
The purpose of bivariate analysis is to understand the relationship between two variables
There are three common ways to perform bivariate analysis:
<b>1.</b> Scatterplots
<b>2.</b> Correlation Coefficients
<b>3.</b> Simple Linear Regression
The following example shows how to perform each of these types of bivariate analysis using the following dataset that contains information about two variables: <b>(1)</b> Hours spent studying and <b>(2)</b> Exam score received by 20 different students:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 1, 1, 2, 2, 2, 3, 3, 3, 3,         3, 4, 4, 5, 5, 6, 6, 6, 7, 8), score=c(75, 66, 68, 74, 78, 72, 85, 82, 90, 82,         80, 88, 85, 90, 92, 94, 94, 88, 91, 96))
#view first six rows of data frame
head(df)
  hours score
1     1    75
2     1    66
3     1    68
4     2    74
5     2    78
6     2    72</b>
<h3>1. Scatterplots</h3>
We can use the following syntax to create a scatterplot of hours studied vs. exam score in R:
<b>#create scatterplot of hours studied vs. exam score
plot(df$hours, df$score, pch=16, col='steelblue',
     main='Hours Studied vs. Exam Score',
     xlab='Hours Studied', ylab='Exam Score')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/biv1.png">
The x-axis shows the hours studied and the y-axis shows the exam score received.
From the plot we can see that there is a positive relationship between the two variables: As hours studied increases, exam score tends to increase as well.
<h3>2. Correlation Coefficients</h3>
A Pearson Correlation Coefficient is a way to quantify the linear relationship between two variables.
We can use the <b>cor()</b> function in R to calculate the Pearson Correlation Coefficient between two variables:
<b>#calculate correlation between hours studied and exam score received
cor(df$hours, df$score)
[1] 0.891306
</b>
The correlation coefficient turns out to be <b>0.891</b>.
This value is close to 1, which indicates a strong positive correlation between hours studied and exam score received.
<h3>3. Simple Linear Regression</h3>
Simple linear regression is a statistical method we can use to find the equation of the line that best “fits” a dataset, which we can then use to understand the exact relationship between two variables.
We can use the <b>lm()</b> function in R to fit a  simple linear regression model  for hours studied and exam score received:
<b>#fit simple linear regression model
fit &lt;- lm(score ~ hours, data=df)
#view summary of model
summary(fit)
Call:
lm(formula = score ~ hours, data = df)
Residuals:
   Min     1Q Median     3Q    Max 
-6.920 -3.927  1.309  1.903  9.385 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  69.0734     1.9651   35.15  &lt; 2e-16 ***
hours         3.8471     0.4613    8.34 1.35e-07 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 4.171 on 18 degrees of freedom
Multiple R-squared:  0.7944,Adjusted R-squared:  0.783 
F-statistic: 69.56 on 1 and 18 DF,  p-value: 1.347e-07</b>
The fitted regression equation turns out to be:
Exam Score = 69.0734 + 3.8471*(hours studied)
This tells us that each additional hour studied is associated with an average increase of <b>3.8471</b> in exam score.
We can also use the fitted regression equation to predict the score that a student will receive based on their total hours studied.
For example, a student who studies for 3 hours is predicted to receive a score of <b>81.6147</b>:
Exam Score = 69.0734 + 3.8471*(hours studied)
Exam Score = 69.0734 + 3.8471*(3)
Exam Score = 81.6147
<h2><span class="orange">A Quick Introduction to Bivariate Analysis</span></h2>
The term <b>bivariate analysis </b>refers to the analysis of two variables. You can remember this because the prefix “bi” means “two.”
The purpose of bivariate analysis is to understand the relationship between two variables. You can contrast this type of analysis with the following:
<b> Univariate Analysis : </b>The analysis of one variable.
<b>Multivariate Analysis: </b>The analysis of two or more variables.
There are three common ways to perform bivariate analysis:
<b>1.</b> Scatterplots.
<b>2.</b> Correlation Coefficients.
<b>3.</b> Simple Linear Regression.
This tutorial provides an example of each of these types of bivariate analysis using the following dataset that contains information about two variables: <b>(1)</b> Hours spent studying and <b>(2)</b> Exam score received by 20 different students:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/bivariate1.png">
<h3>1. Scatterplots</h3>
A  scatterplot  offers a visual way to perform bivariate analysis. It allows us to visualize the relationship between two variables by placing the value of one variable on the x-axis and the value of the other variable on the y-axis.
In the scatterplot below, we place hours studied on the x-axis and exam score on the y-axis:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/bivariate2.png">
We can clearly see that there is a positive relationship between the two variables: As hours studied increases, exam score tends to increase as well.
<h3>2. Correlation Coefficients</h3>
A correlation coefficient offers another way to perform bivariate analysis. The most common type of correlation coefficient is the  Pearson Correlation Coefficient , which is a measure of the linear association between two variables.<em> </em>It has a value between -1 and 1 where:
-1 indicates a perfectly negative linear correlation between two variables
0 indicates no linear correlation between two variables
1 indicates a perfectly positive linear correlation between two variables
This simple metric gives us a good idea of how two variables are related. In practice, we often use scatterplots <em>and </em>correlation coefficients to understand the relationship between two variables so we can visualize <em>and </em>quantify their relationship.
<h3>3. Simple Linear Regression</h3>
A third way to perform bivariate analysis is with  simple linear regression .
Using this method, we choose one variable to be an  explanatory variable  and the other variable to be a  response variable . We then find the line that best “fits” the dataset, which we can then use to understand the exact relationship between the two variables.
For example, the line of best fit for the dataset above is:
<b>Exam score = 69.07 + 3.85*(hours studied)</b>
This means that each additional hour studied is associated with an average exam score increase of 3.85. By fitting this linear regression model, we can quantify the exact relationship between hours studied and exam score received.
<b>Related:</b>  How to Perform Simple Linear Regression in Excel 
<h3>Conclusion</h3>
Bivariate analysis is one of the most common types of analysis used in statistics because we’re often interested in understanding the relationship between two variables.
By using scatterplots, correlation coefficients, and simple linear regression, we can visualize and quantify the relationship between two variables.
Often these three methods are all used together in an analysis to gain a full picture of how two variables are related, so it’s a good idea to familiarize yourself with each method.
<h2><span class="orange">5 Examples of Bivariate Data in Real Life</span></h2>
<b>Bivariate data</b> refers to a dataset that contains exactly two variables.
This type of data occurs all the time in real-world situations and we typically use the following methods to analyze this type of data:
Scatterplots
Correlation Coefficients
Simple Linear Regression
The following examples show different scenarios where bivariate data appears in real life.
<h3>Example 1: Business</h3>
Businesses often collect bivariate data about total money spent on advertising and total revenue.
For example, a business may collect the following data for 12 consecutive sales quarters:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate11.png">
This is an example of bivariate data because it contains information on exactly two variables: advertising spend and total revenue.
The business may decide to fit a  simple linear regression model  to this dataset and find the following fitted model:
Total Revenue = 14,942.75 + 2.70*(Advertising Spend)
This tells the business that for each additional dollar spent on advertising, total revenue increases by an average of $2.70.
<h3>Example 2: Medical</h3>
Medical researchers often collect bivariate data to gain a better understanding of the relationship between variables related to health.
For example, a researcher may collect the following data about age and resting heart rate for 15 individuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate12.png">
The researcher may then decide to calculate the correlation between the two variables and find it to be <b>0.812</b>.
This indicates that there is a strong positive correlation between the two variables. That is, as age increases resting heart rate tends to increase in a predictable manner as well.
<b>Related:</b>  What is Considered to Be a “Strong” Correlation? 
<h3>Example 3: Academics</h3>
Researchers often collect bivariate data to understand what variables affect the performance of university students.
For example, a researcher may collect data on the number of hours studied per week and the corresponding GPA for students in a certain class:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate14.png">
She may then create a simple scatterplot to visualize the relationship between these two variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate13-1.png">
Clearly there is a positive association between the two variables: As the number of hours studied per week increases, the GPA of the student tends to increase as well.
<h3>Example 4: Economics</h3>
Economists often collect bivariate data to understand the relationship between two socioeconomic variables.
For example, an economist may collect data on the total years of schooling and total annual income among individuals in a certain city:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate15.png">
He may then decide to fit the following simple linear regression model:
Annual Income = -45,353 + 7,120*(Years of Schooling)
This tells the economist that for each additional year of schooling, annual income increases by $7,120 on average.
<h3>Example 5: Biology</h3>
Biologists often collect bivariate data to understand how two variables are related among plants or animals.
For example, a biologist may collect data on total rainfall and total number of plants in different regions:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/bivariate16.png">
The biologist may then decide to calculate the correlation between the two variables and find it to be <b>0.926</b>.
This indicates that there is a strong positive correlation between the two variables.
That is, higher rainfall is closely associated with an increased number of plants in a region.
<h2><span class="orange">How to Simulate & Plot a Bivariate Normal Distribution in R</span></h2>
In statistics, two variables follow a <b>bivariate normal distribution</b> if they have a normal distribution when added together.
This tutorial explains how to perform the following tasks in R:
Simulate a bivariate normal distribution
Plot a bivariate normal distribution using a contour plot (2-D plot)
Plot a bivariate normal distribution using a surface plot (3-D plot)
Let’s jump in!
<h3>Example 1: Simulate a Bivariate Normal Distribution in R</h3>
The easiest way to simulate a bivariate normal distribution in R is to use the <b>mvrnorm()</b> function from the <b>MASS</b> package.
The following code shows how to use this function to simulate a bivariate normal distribution in practice:
<b>library(MASS)
#make this example reproducible
set.seed(0)
#simulate bivariate normal distribution
bivariate_data &lt;- as.data.frame(mvrnorm(n=100,                        mu=c(0, 0),                        Sigma=matrix(c(5, 3, 4, 4), ncol=2)))
#view first six rows of bivariate dataset
head(bivariate_data)
           V1         V2
1 -2.03600343 -2.9623059
2  0.07719131  1.2948982
3 -3.26729701 -1.7928069
4 -2.62985132 -2.3015471
5 -1.75126215  0.3056698
6  3.67698436  2.2020238
</b>
Here’s what each argument in the <b>mvrnorm()</b> function does:
<b>n</b>: Defines the sample size
<b>mu</b>: Defines the mean of each variable
<b>Sigma</b>: Defines the covariance matrix of the two variables
The end result is a data frame with two variables that follow a normal distribution when added together.
<h3>
<b>Example 2: Plot a Bivariate Normal Distribution</b>
</h3>
The easiest way to plot a bivariate normal distribution in R is to use functions from the <b>mnormt()</b> package.
For example, we can use the <b>contour()</b> function from this package to create a contour plot, which offers a 2-D visualization of the bivariate normal distribution:
<b>library(mnormt)
#make this example reproducible
set.seed(0)
#create bivariate normal distribution
x     &lt;- seq(-3, 3, 0.1) 
y     &lt;- seq(-3, 3, 0.1)
mu    &lt;- c(0, 0)
sigma &lt;- matrix(c(2, -1, -1, 2), nrow=2)
f     &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     &lt;- outer(x, y, f)
#create contour plot
contour(x, y, z)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/bivar1.jpg">
We can also use the <b>persp()</b> function from to create a surface plot, which offers a 3-D visualization of the bivariate normal distribution:
<b>library(mnormt)
#make this example reproducible
set.seed(0)
#create bivariate normal distribution
x     &lt;- seq(-3, 3, 0.1) 
y     &lt;- seq(-3, 3, 0.1)
mu    &lt;- c(0, 0)
sigma &lt;- matrix(c(2, -1, -1, 2), nrow=2)
f     &lt;- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     &lt;- outer(x, y, f)
#create surface plot
persp(x, y, z, theta=-30, phi=25, expand=0.6, ticktype='detailed')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/bivar2.jpg">
Here’s what each argument in the <b>persp()</b> function does:
<b>theta, phi</b>: Defines the angles of the viewing direction.
<b>expand</b>: Controls the size of the z-axis.
<b>ticktype</b>: Controls the appearance of the ticks on the axes.
The end result is a 3-D surface plot of the bivariate normal distribution.
<h2><span class="orange">How to Create a Bland-Altman Plot in Excel</span></h2>
A  Bland-Altman plot  is used to visualize the differences in measurements between two different instruments or two different measurement techniques.
It’s useful for determining how similar two instruments or techniques are at measuring the same construct.
This tutorial provides a step-by-step example of how to create a Bland-Altman plot in Excel.
<h3>Step 1: Create the Data</h3>
Suppose a biologist uses two different instruments (A and B) to measure the weight of the same set of 20 different frogs, in grams.
The weight of the frogs, as measured by each instrument, is shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel1.png">
<h3>Step 2: Calculate the Difference in Measurements</h3>
Next, we’ll use the following formulas to calculate the average measurement and the difference in measurements for each frog:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel2.png">
We can then copy and paste this formula down to every cell in the two columns:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel3.png">
<h3>Step 3: Calculate the Average Difference & Confidence Interval</h3>
Next, we can use the following formulas to calculate the average difference between the two instruments along with the upper and lower 95% confidence interval limits for the average difference:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel4.png">
The average difference turns out to be <b>0.5</b> and the 95% confidence interval for the average difference is <b>[-1.921, 2.921]</b>.
<h3>Step 4: Create the Bland-Altman Plot</h3>
To create the Bland-Altman plot, highlight the cells in the range <b>C2:D21</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel5.png">
Along the top ribbon, click <b>Insert</b> and then click the first chart in the <b>Insert Scatter (X, Y) or Bubble Chart</b> group within the <b>Charts</b> group. The following scatterplot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel6.png">
The x-axis shows the average measurement of the instruments and the y-axis shows the difference between the measurements from the two instruments.
To add a horizontal line that represents the average difference in measurements, we need to create a data series that shows the minimum and maximum values along the x-axis (0 and 30) along with values that show the average difference:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel7.png">
Next, right click on the chart and click <b>Select Data</b>. In the window that appears, click <b>Add</b> under the <b>Legend Entries (Series)</b> section:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel8.png">
In the new window that appears, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel9.png">
Once you click <b>OK</b>, two orange dots will appear on the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel12.png">
Right click on one of the orange dots and click <b>Format Data Series…</b>
In the window that appears on the right side of the screen, click <b>Solid Line</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel10.png">
 This will turn the two orange points into a solid orange line that represents the average difference between the two instruments:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandExcel11.png">
Repeat this process for the upper and lower confidence interval lines.
Feel free to modify the line styles, axes names, and title of the chart to produce a Bland-Altman plot that looks aesthetically pleasing:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bland1.png">
<h2><span class="orange">How to Create a Bland-Altman Plot in Python</span></h2>
A  Bland-Altman plot  is used to visualize the differences in measurements between two different instruments or two different measurement techniques.
It’s useful for determining how similar two instruments or techniques are at measuring the same construct.
This tutorial provides a step-by-step example of how to create a Bland-Altman plot in Python.
<h3>Step 1: Create the Data</h3>
Suppose a biologist uses two different instruments (A and B) to measure the weight of the same set of 20 different frogs, in grams.
We’ll create the following data frame that represents the weight of each frog, as measured by each instrument:
<b>import pandas as pd
df = pd.DataFrame({'A': [5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9,         10, 11, 13, 14, 14, 15, 18, 22, 25],   'B': [4, 4, 5, 5, 5, 7, 8, 6, 9, 7, 7, 11,         13, 13, 12, 13, 14, 19, 19, 24]})
</b>
<h3>Step 2: Create the Bland-Altman Plot</h3>
Next, we’ll use the <b>mean_diff_plot()</b> function from the statsmodels package to create a Bland-Altman plot:
<b>import statsmodels.api as sm
import matplotlib.pyplot as plt
#create Bland-Altman plot                  
f, ax = plt.subplots(1, figsize = (8,5))
sm.graphics.mean_diff_plot(df.A, df.B, ax = ax)
#display Bland-Altman plot
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandPython1.png">
The x-axis of the plot displays the average measurement of the two instruments and the y-axis displays the difference in measurements between the two instruments.
The black solid line represents the average difference in measurements between the two instruments while the two dashed lines represent the 95% confidence interval limits for the average difference.
The average difference turns out to be <b>0.5</b> and the 95% confidence interval for the average difference is <b>[-1.86, 2.86]</b>.
<h2><span class="orange">How to Create a Bland-Altman Plot in R (Step-by-Step)</span></h2>
A  Bland-Altman plot  is used to visualize the differences in measurements between two different instruments or two different measurement techniques.
It’s useful for determining how similar two instruments or techniques are at measuring the same construct.
This tutorial provides a step-by-step example of how to create a Bland-Altman plot in R.
<h3>Step 1: Create the Data</h3>
Suppose a biologist uses two different instruments (A and B) to measure the weight of the same set of 20 different frogs, in grams.
We’ll create the following data frame in R that represents the weight of each frog, as measured by each instrument:
<b>#create data
df &lt;- data.frame(A=c(5, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9,     10, 11, 13, 14, 14, 15, 18, 22, 25), B=c(4, 4, 5, 5, 5, 7, 8, 6, 9, 7, 7, 11,     13, 13, 12, 13, 14, 19, 19, 24))
#view first six rows of data
head(df)
  A B
1 5 4
2 5 4
3 5 5
4 6 5
5 6 5
6 7 7</b>
<h3>Step 2: Calculate the Difference in Measurements</h3>
Next, we’ll create two new columns in the data frame that contain the average measurement for each frog along with the difference in measurements:
<b>#create new column for average measurement
df$avg &lt;- rowMeans(df) 
#create new column for difference in measurements
df$diff &lt;- df$A - df$B
#view first six rows of data
head(df)
  A B avg diff
1 5 4 4.5    1
2 5 4 4.5    1
3 5 5 5.0    0
4 6 5 5.5    1
5 6 5 5.5    1
6 7 7 7.0    0
</b>
<h3>Step 3: Calculate the Average Difference & Confidence Interval</h3>
Next, we’ll calculate the average difference in measurements between the two instruments along with the upper and lower 95% confidence interval limits for the average difference:
<b>#find average difference
mean_diff &lt;- mean(df$diff)
mean_diff
[1] 0.5
#find lower 95% confidence interval limits
lower &lt;- mean_diff - 1.96*sd(df$diff)
lower
[1] -1.921465
#find upper 95% confidence interval limits
upper &lt;- mean_diff + 1.96*sd(df$diff)
upper
[1] 2.921465
</b>
The average difference turns out to be <b>0.5</b> and the 95% confidence interval for the average difference is <b>[-1.921, 2.921]</b>.
<h3>Step 4: Create the Bland-Altman Plot</h3>
Next, we’ll use the following code to create a Bland-Altman plot using the  ggplot2  data visualization package:
<b><span>#load ggplot2
library(ggplot2)
#create Bland-Altman plot
ggplot(df, aes(x = avg, y = diff)) +
  geom_point(size=2) +
  geom_hline(yintercept = mean_diff) +
  geom_hline(yintercept = lower, color = "red", linetype="dashed") +
  geom_hline(yintercept = upper, color = "red", linetype="dashed") +
  ggtitle("Bland-Altman Plot") +
  ylab("Difference Between Measurements") +
  xlab("Average Measurement")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/blandR1.png">
The x-axis of the plot displays the average measurement of the two instruments and the y-axis displays the difference in measurements between the two instruments.
The black line represents the average difference in measurements between the two instruments while the two red dashed lines represent the 95% confidence interval limits for the average difference.
<h2><span class="orange">What is a Bland-Altman Plot? (Definition & Example)</span></h2>
A <b>Bland-Altman plot</b> is used to visualize the differences in measurements between two different instruments or two different measurement techniques.
It is often used to assess how similar a new instrument or technique is at measuring something compared to the instrument or technique currently being used.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bland1.png">
The x-axis of the plot displays the average measurement of the two instruments and the y-axis displays the difference in measurements between the two instruments.
The following three lines are also shown in the plot:
The average difference in measurements between the two instruments
The upper limit of the 95% confidence interval for the average difference
The lower limit of the 95% confidence interval for the average difference
This type of plot is useful for determining two things:
<b>1. What is the average difference in measurements between the two instruments?</b>
The horizontal line drawn in the middle of the chart shows the average difference in measurements between the two instruments. This value is often referred to as the “bias” between the instruments.
The further this value is from zero, the larger the average difference in measurements between the instruments.
<b>2. What is the typical range of agreement between the two instruments?</b>
The upper and lower confidence interval lines gives us an idea of the typical range of agreement between the two instruments. In general, 95% of the differences between the two instruments fall within these confidence limits. 
The wider the confidence interval, the wider the range of differences in measurements between the two instruments.
The following step-by-step example shows how to create and interpret a Bland-Altman plot from scratch.
<em><b>Note:</b> A Bland-Altman plot is sometimes referred to as a Tukey mean-difference plot. These names are used interchangeably. </em>
<h3>Step 1: Collect the Data</h3>
Suppose a biologist wants to know how similar two different instruments are at measuring the weight of frogs, in grams. He uses two instruments (A and B) to weight the same set of 20 frogs.
The weight of the frogs, as measured by each instrument, is shown in the following table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bland2.png">
<h3>Step 2: Calculate the Average Measurement & Difference in Measurements</h3>
Next, we will calculate the average measurement ( (A+B)/2 ) and the difference in measurements (A-B) for each frog:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bland3.png">
<h3>Step 3: Calculate the Mean Difference & Confidence Interval</h3>
The average of the values in the <em>Difference</em> column turns out to be <b>0.5</b>. 
The standard deviation of values in the <em>Difference</em> column turns out to be <b>1.235</b>.
The upper and lower limits of the confidence interval for the average difference can be calculated as:
<b>Upper Limit:</b> x + 1.96*s = 0.5 + 1.96*1.235 = <b>2.92</b>
<b>Lower Limit:</b> x – 1.96*s = 0.5 – 1.96*1.235 = <b>-1.92</b>
Here’s how to interpret these values:
On average, instrument A weighs frogs to be 0.5 grams heavier than instrument B.
95% of the differences in weight between the two instruments are expected to fall in the range of -1.92 grams and 2.92 grams.
Next, we’ll create a Bland-Altman plot to visualize these values.
<h3>Step 4: Create the Plot</h3>
Next, we can create the following plot that shows the average measurement of the two instruments on the x-axis and the difference between measurements on the y-axis.
We can also add a horizontal line at the mean difference between the measurements (0.5) along with an upper confidence limit (2.92) and a lower confidence limit (-1.92) that we calculated in the previous step:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bland1.png">
<h2><span class="orange">Blocking in Statistics: Definition & Example</span></h2>
Often in experiments, researchers are interested in understanding the relationship between an  explanatory variable  and a  response variable .
Unfortunately <b>nuisance variables </b>often arise in experimental studies, which are variables that effect the relationship between the explanatory and response variable but are of no interest to researchers.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/blocking1.png">
For example, suppose researchers want to understand the effect that a new diet has on weight less. The explanatory variable is the new diet and the response variable is the amount of weight loss. 
However, a nuisance variable that will likely cause variation is <b>gender</b>. It’s likely that the gender of an individual will effect the amount of weight they’ll lose, regardless of whether the new diet works or not.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/blocking2.png">
<h3>Introducing Blocking</h3>
One common way to control for the effect of nuisance variables is through <b>blocking</b>, which involves splitting up individuals in an experiment based on the value of some nuisance variable.
In our previous example, we would place individuals into one of two blocks: 
Male
Female
Then, within each block we would randomly assign individuals to one of two treatments:
A new diet
A standard diet
By doing this, the variation within each block would be much lower compared to the variation among all individuals and we would be able to gain a better understanding of how the new diet affects weight loss while controlling for gender.
To illustrate this, consider the following chart that shows total weight loss for 16 individuals in the study:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/blocking3-1.png">
At first glance, it doesn’t appears that the new diet is associated with more weight loss.
However, once we split the individuals into two blocks based on gender, it becomes apparent that the new diet does seem to be associated with more weight loss:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/blocking4-1.png">
By placing the individuals into blocks, the relationship between the new diet and weight loss became more clear since we were able to control for the nuisance variable of gender.
<h3>More Examples of Blocking</h3>
Gender is a common nuisance variable to use as a blocking factor in experiments since males and females tend to respond differently to a wide variety of treatments.
However, other common nuisance variables that can be used as blocking factors include:
Age group
Income group
Education level
Amount of exercise
Region
Depending on the nature of the experiment, it’s also possible to use several blocking factors at once. However, in practice only one or two are typically used since more blocking factors requires larger sample sizes to derive significant results.
<h3>Nuisance Variables vs. Lurking Variables</h3>
In the previous example, gender was a <em>known</em> nuisance variable that researchers knew affected weight loss. However, often in experiments there are also  lurking variables , which are variables that also affect the relationship between an explanatory and response variable but are either unknown or simply not included in the study because it’s hard to collect data on them.
For example, suppose each individual has a certain amount of innate discipline that they can draw upon to lose more weight. Since discipline is hard to measure, it’s not included as a blocking factor in the study but one way to control for it is to use <b>randomization</b>.
By randomly assigning individuals to either the new diet or the standard diet, researchers can maximize the chances that the overall level of discipline of individuals between the two groups is roughly equal.
Thus, in any experiment that uses blocking it’s also important to randomly assign individuals to treatments to control for the effects of any potential lurking variables.
<h2><span class="orange">How to Use Bold Font in R (With Examples)</span></h2>
You can use the following basic syntax to produce bold font in R plots:
<b>substitute(paste(bold('this text is bold')))
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Bold Font on Axis Labels of Plot</h3>
The following code shows how to create a scatter plot in R using normal font for both axis labels:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatter plot with normal font for axis labels
plot(x, y, xlab='X Label', ylab='Y Label')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/boldfont1.jpg"452">
And the following code shows how to specify bold font for the x-axis and y-axis labels of a plot:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatterplot with axes labels in bold
plot(x, y, xlab = substitute(paste(bold('X Label'))),
           ylab = substitute(paste(bold('Y Label'))))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/boldfont2.jpg"450">
Notice how the labels for both axes are now bold.
<h3>Example 2: Bold Font with Text in Plot</h3>
The following code shows how to include bold font for a text element inside of a plot:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatterplot
plot(x, y)
#add normal text at location x=3, y=14
text(3, 14, 'This is some normal text')
#add bold text at location x=3, y=16 
text(3, 16, substitute(paste(bold('This is some bold text'))))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/boldfont3.jpg"456">
Notice the difference between the bold font and the normal font.
<h2><span class="orange">Bonferroni Correction Calculator</span></h2>
When you conduct a single statistical test to determine if two group means are equal, you typically compare the p-value of the test to some alpha (α) level like 0.05.
If the p-value of the test is less than 0.05, you reject the null hypothesis and conclude that the group means are different.
However, when you conduct multiple tests at once to compare several group means, there is a higher chance of committing a type I error and rejecting the null hypothesis when it’s actually true.
To control for this, you can perform a <b>Bonferroni Correction</b> and adjust the α level to be equal to:
α<sub>adjusted</sub> = α/n
where:
α: The original α level
n: The total number of comparisons
Then, you only reject the null hypothesis of each individual test when the p-value is less than this adjusted α level.
To perform a Bonferroni Correction and calculate the adjusted α level, simply fill in the boxes below and then click the “Calculate” button.
<label for="z"><b>Original α Level</b></label>
<input type="number" id="a" value="0.05">
<label for="E"><b>Number of Comparisons (n)</b></label>
<input type="number" id="n" value="4">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Adjusted α: </b>0.01250
<b>Interpretation: </b>If you conduct 4 comparisons, only reject the null hypothesis of each comparison if it has a p-value less than 0.01250.
<script>
function calc() {
//get input values
var a  = document.getElementById('a').value*1;
var n  = document.getElementById('n').value*1;
//find number of bins
var adj = a/n;
//output
document.getElementById('adj').innerHTML = adj.toFixed(5);
document.getElementById('n_out').innerHTML = n;
document.getElementById('adj_out').innerHTML = adj.toFixed(5);
}
</script>
<h2><span class="orange">How to Perform a Bonferroni Correction in Excel</span></h2>
A <b>Bonferroni Correction</b> refers to the process of adjusting the alpha (α) level for a family of statistical tests so that we control for the probability of committing a type I error.
The formula for a Bonferroni Correction is as follows:
<b>α<sub>new</sub></b> = α<sub>original </sub>/ n
where:
α<sub>original</sub>: The original α level
n: The total number of comparisons or tests being performed
For example, if we perform three statistical tests at once and wish to use α = .05 for each test, the Bonferroni Correction tell us that we should use α<sub>new</sub> =<b> .01667</b>.
α<sub>new</sub> = α<sub>original </sub>/ n = .05 / 3 = .01667
Thus, we should only reject the null hypothesis of each individual test if the p-value of the test is less than .01667.
This type of correction is often made in  post-hoc tests  following an ANOVA when we want to compare several group means at once.
The following step-by-step example shows how to perform a Bonferroni Correction following a one-way ANOVA in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset that shows the exam scores of students who used one of three different studying techniques to prepare for the exam:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel1.png">
<h3>Step 2: Perform the One-Way ANOVA</h3>
Next, let’s perform a one-way ANOVA to determine if the mean exam scores are equal across all three groups.
First, highlight all of the data including the column headers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel2.png">
Next, click the <b>Data</b> tab along the top ribbon and then click <b>Data Analysis</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option available, you need to first  load the Analysis ToolPak .
In the window that appears, click <b>Anova: Single Factor</b> and then click <b>OK</b>.
Fill in the following information, then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel3.png">
The results of the one-way ANOVA will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel4.png">
Recall that a one-way ANOVA has the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null hypothesis):</b> All group means are equal.
<b>H<sub>A </sub>(alternative hypothesis):</b> At least one group mean is different<sub> </sub>from the rest.
Since the p-value in the ANOVA table (0.001652) is less than .05, we have sufficient evidence to reject the null hypothesis. In other words, the mean exam scores between the three groups are not equal.
Next, we can perform multiple comparisons using a Bonferroni correction between the three groups to see exactly which group means are different.
<h3>Step 3: Perform Multiple Comparisons Using a Bonferroni Correction</h3>
Using a Bonferroni correction, we can calculate the adjusted alpha level as follows:
<b>α<sub>new</sub></b> = α<sub>original </sub>/ n
In our example, we’ll be performing the following three comparisons:
Technique 1 vs. Technique 2
Technique 1 vs. Technique 3
Technique 2 vs. Technique 3
Since we want to use α = .05 for each test, the Bonferroni Correction tell us that we should use α<sub>new</sub> =<b> .0167</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel5.png">
Next, we’ll use a t-test to compare the means between each group. In Excel, we can use the following syntax:
<b>=TTEST(Array1, Array2, tails=2, type=2)
</b>
where:
<b>Array1:</b> The first array of data
<b>Array2:</b> The second array of data
<b>tails:</b> The number of tails of the test. We’ll use “2” to indicate a two-tailed test.
<b>type:</b> The type of t-test to perform. We’ll use “2” to indicate a t-test with equal variances.
The following screenshot shows how to perform each t-test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bonfExcel6.png">
The only p-value that is less than the Bonferroni-adjusted alpha level is from the comparison between technique 1 vs. technique 2, which had a p-value of <b>0.001042</b>.
Thus, we would conclude that only statistically significant difference in mean exam scores was between technique 1 and technique 2.
<h2><span class="orange">How to Perform a Bonferroni Correction in R</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups.
If the overall  p-value  from the ANOVA table is less than some significance level, then we have sufficient evidence to say that at least one of the means of the groups is different from the others.
However, this doesn’t tell us <em>which </em>groups are different from each other. It simply tells us that not all of the group means are equal.
In order to find out exactly which groups are different from each other, we must conduct pairwise t-tests between each group while controlling for the  family-wise error rate .
One of the most common ways to do so is to use <b>Bonferroni’s correction</b> when calculating the p-values for each of the pairwise t-tests.
This tutorial explains how to perform Bonferroni’s correction in R.
<h3>Example: Bonferroni’s Correction in R</h3>
Suppose a teacher wants to know whether or not three different studying techniques lead to different exam scores among students.
To test this, she randomly assigns 10 students to use each studying technique. After one week of using their assigned study technique, each student takes the same exam.
We can use the following steps in R to fit a one-way ANOVA and use Bonferroni’s correction to calculate pairwise differences between the exam scores of each group.
<b>Step 1: Create the dataset.</b>
The following code shows how to create a dataset that contains exam scores for all 30 students:
<b>#create data frame
data &lt;- data.frame(technique = rep(c("tech1", "tech2", "tech3"), each = 10),   score = c(76, 77, 77, 81, 82, 82, 83, 84, 85, 89,             81, 82, 83, 83, 83, 84, 87, 90, 92, 93,             77, 78, 79, 88, 89, 90, 91, 95, 95, 98))
#view first six rows of data frame
head(data)
  technique score
1     tech1    76
2     tech1    77
3     tech1    77
4     tech1    81
5     tech1    82
6     tech1    82
</b>
<b>Step 2: Visualize the exam scores for each group.</b>
The following code shows how to produce boxplots to visualize the distribution of exam scores for each group:
<b>boxplot(score ~ technique,
        data = data,
        main = "Exam Scores by Studying Technique",
        xlab = "Studying Technique",
        ylab = "Exam Scores",
        col = "steelblue",
        border = "black")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/bonf1.png">
<b>Step 3: Perform a one-way ANOVA.</b>
The following code shows how to perform a one-way ANOVA to test for differences among mean exam scores in each group:
<b>#fit the one-way ANOVA model
model &lt;- aov(score ~ technique, data = data)
#view model output
summary(model)
            Df Sum Sq Mean Sq F value Pr(>F)  
technique    2  211.5  105.73   3.415 0.0476 *
Residuals   27  836.0   30.96                 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
Since the overall p-value (<b>0.0476</b>) is less than .05, this is an indication that each group does not have the same average exam score.
Next, we will perform pairwise t-tests using Bonferroni’s correction for the p-values to calculate pairwise differences between the exam scores of each group.
<b>Step 4: Perform pairwise t-tests.</b>
To perform pairwise t-tests with Bonferroni’s correction in R we can use the  pairwise.t.test()  function, which uses the following syntax:
<b>pairwise.t.test(x, g, p.adjust.method=”bonferroni”)</b>
where:
<b>x:</b> A numeric vector of response values
<b>g:</b> A vector that specifies the group names (e.g. studying technique)
The following code shows how to use this function for our example:
<b>#perform pairwise t-tests with Bonferroni's correction
pairwise.t.test(data$score, data$technique, p.adjust.method="bonferroni")
Pairwise comparisons using t tests with pooled SD 
data:  data$score and data$technique 
      tech1 tech2
tech2 0.309 -    
tech3 0.048 1.000
P value adjustment method: bonferroni</b>
The way to interpret the output is as follows:
The adjusted p-value for the mean difference in exam scores between technique 1 and technique 2 is <b>.309</b>.
The adjusted p-value for the mean difference in exam scores between technique 1 and technique 3 is <b>.048</b>.
The adjusted p-value for the mean difference in exam scores between technique 2 and technique 3 is <b>1.000</b>.
Based on the output, we can see that the only significant difference is between technique 1 and technique 3.
<h2><span class="orange">The Bonferroni Correction: Definition & Example</span></h2>
Whenever you perform a  hypothesis test , there is always a chance of committing a type I error. This is when you reject the null hypothesis when it is actually true.
We sometimes call this a “false positive” – when we claim there is a statistically significant effect, but there actually isn’t.
When we perform one hypothesis test, the type I error rate is equal to the significance level (α), which is commonly chosen to be 0.01, 0.05, or 0.10. However, when we conduct multiple hypothesis tests at once, the probability of getting a false positive increases.
When we conduct multiple hypothesis tests at once, we have to deal with something known as a <b>family-wise error rate</b>, which is the probability that at least one of the tests produces a false positive. This can be calculated as:
<b>Family-wise error rate = 1 – (1-α)<sup>n</sup></b>
where:
<b>α:</b> The significance level for a single hypothesis test
<b>n:</b> The total number of tests
If we conduct just one hypothesis test using α = .05, the probability that we commit a type I error is just .05.
Family-wise error rate = 1 – (1-α)<sup>c  </sup>= 1 – (1-.05)<sup>1 </sup>= <b>0.05</b>
If we conduct two hypothesis tests at once and use α = .05 for each test, the probability that we commit a type I error increases to 0.0975.
Family-wise error rate = 1 – (1-α)<sup>c  </sup>= 1 – (1-.05)<sup>2 </sup>= <b>0.0975</b>
And if we conduct five hypothesis tests at once using α = .05 for each test, the probability that we commit a type I error increases to 0.2262.
Family-wise error rate = 1 – (1-α)<sup>c  </sup>= 1 – (1-.05)<sup>5 </sup>= <b>0.2262</b>
It’s easy to see that as we increase the number of statistical tests, the probability of commiting a type I error with at least one of the tests quickly increases.
One way to deal with this is by using a Bonferroni Correction.
<h3>What is a Bonferroni Correction?</h3>
A <b>Bonferroni Correction</b> refers to the process of adjusting the alpha (α) level for a family of statistical tests so that we control for the probability of committing a type I error.
The formula for a Bonferroni Correction is as follows:
<b>α<sub>new</sub></b> = α<sub>original </sub>/ n
where:
α<sub>original</sub>: The original α level
n: The total number of comparisons or tests being performed
For example, if we perform three statistical tests at once and wish to use α = .05 for each test, the Bonferroni Correction tell us that we should use α<sub>new</sub> =<b> .01667</b>.
α<sub>new</sub> = α<sub>original </sub>/ n = .05 / 3 = .01667
Thus, we should only reject the null hypothesis of each individual test if the p-value of the test is less than .01667.
<h3>Bonferroni Correction: An Example</h3>
Suppose a professor wants to know whether or not three different studying techniques lead to different exam scores among students.
To test this, she randomly assigns 30 students to use each studying technique. After one week of using their assigned study technique, each student takes the same exam.
She then performs a  one-way ANOVA  and finds that the overall p-value is <b>0.0476</b>. Since this is less than .05, she rejects the null hypothesis of the one-way ANOVA and concludes that not each studying technique produces the same mean exam score.
To find out <em>which</em> studying techniques produce statistically significant scores, she performs the following pairwise t-tests:
Technique 1 vs. Technique 2
Technique 1 vs. Technique 3
Technique 2 vs. Technique 3
She wants to control the probability of committing a type I error at α = .05. Since she’s performing multiple tests at once, she decides to apply a Bonferroni Correction and use α<sub>new</sub> =<b> .01667</b>.
α<sub>new</sub> = α<sub>original </sub>/ n = .05 / 3 = .01667
She then proceeds to perform t-tests for each group and finds the following:
Technique 1 vs. Technique 2 | p-value = .0463
Technique 1 vs. Technique 3 | p-value = .3785
Technique 2 vs. Technique 3  | p-value = .0114
Since the p-value for Technique 2 vs. Technique 3 is the only p-value less than .01667, she concludes that there is only a statistically significant difference between technique 2 and technique 3.
<h2><span class="orange">A Simple Introduction to Boosting in Machine Learning</span></h2>
Most  supervised machine learning algorithms  are based on using a single predictive model like  linear regression ,  logistic regression ,  ridge regression , etc. 
Methods like  bagging  and  random forests , however, build many different models based on repeated bootstrapped samples of the original dataset. Predictions on new data are made by taking the average of the predictions made by the individual models.
These methods tend to offer an improvement in prediction accuracy over methods that only use a single predictive model because they use the following process:
First, build individual models that have  high variance and low bias  (e.g. deeply grown  decision trees ).
Next, take the average of the predictions made by individual models in order to reduce the variance.
Another method that tends to offer even further improvement in predictive accuracy is known as <b>boosting</b>.
<h3>What is Boosting?</h3>
Boosting is a method that can be used with any type of model, but it is most often used with decision trees.
The idea behind boosting is simple:
<b>1. First, build a weak model.</b>
A “weak” model is one whose error rate is only slightly better than random guessing.
In practice, this is typically a decision tree with only one or two splits.
<b>2. Next, build another weak model based on the residuals of the previous model.</b>
In practice, we use the residuals from the previous model (i.e. the errors in our predictions) to fit a new model that slightly improves upon the overall error rate.
<b>3. Continue this process until k-fold cross-validation tells us to stop.</b>
In practice, we use  k-fold cross-validation  to identify when we should stop growing the boosted model.
By using this method, we can start with a weak model and keep “boosting” the performance of it by sequentially building new trees that improve upon the performance of the previous tree until we end up with a final model that has high predictive accuracy.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/boosting1.png">
<h3>Why Does Boosting Work?</h3>
It turns out that boosting is able to produce some of the most powerful models in all of machine learning.
In many industries, boosted models are used as the go-to models in production because they tend to outperform all other models.
The reason boosted models work so well comes down to understanding a simple idea:
<b>1.</b> First, boosted models build a weak decision tree that has low predictive accuracy. This decision tree is said to have low variance and high bias.
<b>2. </b>As boosted models go through the process of sequentially improving previous decision trees, the overall model is able to slowly reduce the bias at each step without increasing the variance by much.
<b>3.</b> The final fitted model tends to have sufficiently low bias <em>and</em> low variance, which leads to a model that is able to produce low test error rates on new data.
<h3>Pros & Cons of Boosting</h3>
The obvious benefit of boosting is that it’s able to produce models that have high predictive accuracy compared to almost all other types of models.
One potential drawback is that a fitted boosted model is very difficult to interpret. While it may offer tremendous ability to predict the response values of new data, it’s difficult to explain the exact process that it uses to do so.
In practice, most data scientists and machine learning practitioners build boosted models because they want to be able to predict the response values of new data accurately. Thus, the fact that boosted models are hard to interpret usually isn’t an issue.
<h3>Boosting in Practice</h3>
In practice there are actually many types of algorithms that are used for boosting, including:
 XGBoost 
 AdaBoost 
 CatBoost 
 LightGBM 
Depending on the size of your dataset and the processing power of your machine, one of these methods may be preferable to the other. 
<h2><span class="orange">How to Calculate a Bootstrap Standard Error in R</span></h2>
<b>Bootstrapping</b> is a method that can be used to estimate the standard error of a mean.
The basic process for calculating a bootstrapped standard error is as follows:
Take <em>k </em>repeated samples  with replacement  from a given dataset.
For each sample, calculate the standard error: s/√n
This results in <em>k</em> different estimates for the standard error. To find the bootstrapped standard error, take the mean of the <em>k</em> standard errors.
The following examples explain two different methods that can be used to calculate a bootstrapped standard error in R.
<h3>Method 1: Use the Boot Package</h3>
One way to calculate a bootstrap standard error in R is to use the <b>boot()</b> function from the <b>boot</b> library.
The following code shows how to calculate a bootstrap standard error for a given dataset in R:
<b>#make this example reproducible
set.seed(10)
#load boot library
library(boot)
#define dataset
x &lt;- c(12, 14, 14, 15, 18, 21, 25, 29, 32, 35)
#define function to calculate mean
meanFunc &lt;- function(x,i){mean(x[i])}
#calculate standard error using 100 bootstrapped samples
boot(x, meanFunc, 100)
Bootstrap Statistics :
    original  bias    std. error
t1*     21.5   0.254    2.379263</b>
The “original” value of <b>21.5</b> shows the mean of the original dataset. The “std. error” value of <b>2.379263</b> shows the bootstrap standard error of the mean.
Note that we used 100 bootstrapped samples to estimate the standard error of the mean in this example, but we could have used 1,000 or 10,000 or any number of bootstrapped samples we’ d like.
<h3>Method 2: Write Your Own Formula</h3>
Another way to calculate a bootstrapped standard error is to write our own function.
The following code shows how to do so:
<b>#make this example reproducible
set.seed(10)
#load boot library
library(boot)
#define dataset
x &lt;- c(12, 14, 14, 15, 18, 21, 25, 29, 32, 35)
mean(replicate(100, sd(sample(x, replace=T))/sqrt(length(x))))
[1] 2.497414
</b>
The bootstrapped standard error turns out to be <b>2.497414</b>.
Notice that this standard error is quite similar to the one calculated in the previous example.
<h2><span class="orange">How to Perform Bootstrapping in Excel (With Example)</span></h2>
<b>Bootstrapping</b> is a method that can be used to construct a confidence interval for a  statistic  when the sample size is small and the underlying distribution is unknown.
The basic process for bootstrapping is as follows:
Take <em>k </em>repeated samples with replacement from a given dataset.
For each sample, calculate the statistic you’re interested in.
This results in <em>k</em> different estimates for a given statistic, which you can then use to calculate a confidence interval for the statistic.
The following step-by-step example shows how to perform bootstrapping in Excel.
<h3>Step 1: Enter the Original Data</h3>
First, we’ll enter the values for some dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/bootstrapexcel1.jpg"414">
<h3>Step 2: Generate Bootstrap Samples</h3>
Next, we’ll use the following formula to generate bootstrap samples:
<b>=INDEX($A$2:$A$16, RANDBETWEEN(1, ROWS($A$2:$A$16)),1)
</b>
We can type this formula into cell <b>D2</b> to randomly select one value from the original dataset.
We can then drag this formula to the right for 10 cells to generate our first bootstrapped sample.
We can then drag this formula down 300 rows to create 300 bootstrapped samples:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/bootstrapexcel2.jpg">
<b>Note</b>: Bootstrapping uses  sampling with replacement , which means that one value from the original dataset may appear multiple times in any given sample.
<h3>Step 3: Calculate Statistic of Interest for Each Sample</h3>
Next, we can calculate the statistic of interest for each sample.
For example, we could calculate the mean, median, standard deviation, interquartile range, etc. for every sample.
For this particular example, we’ll calculate the median value for each sample:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/bootstrapexcel3.jpg"672">
We can see:
The first bootstrapped sample has a median value of <b>14</b>.
The second bootstrapped sample has a median value of <b>16</b>.
The third bootstrapped sample has a median value of <b>13.5</b>.
And so on.
<h3>Step 4: Calculate Bootstrapped Confidence Interval</h3>
Lastly, we can calculate a 95% bootstrapped confidence interval for the median by finding the value located at percentile 2.5% and percentile 97.5% in column N.
We can use the following formulas to do so:
<b>=PERCENTILE(N2:N301, 0.025)
=PERCENTILE(N2:N301, 0.975)
</b>
The following screenshot shows how to use these formulas in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/bootstrapexcel4.jpg">
From the output we can see that the 95% bootstrapped confidence interval for the median value of the original dataset is <b>[10.475, 19.7625]</b>.
Note that in this example we chose to generate 300 bootstrapped samples each with a sample size of n=10, but you can generate as many bootstrapped samples as you’d like.
When using statistical software, it’s common to generate thousands of bootstrapped samples which can then be used to construct a confidence interval.
<h2><span class="orange">How to Perform Bootstrapping in Python (With Example)</span></h2>
<b>Bootstrapping</b> is a method that can be used to construct a confidence interval for a  statistic  when the sample size is small and the underlying distribution is unknown.
The basic process for bootstrapping is as follows:
Take <em>k </em>repeated samples with replacement from a given dataset.
For each sample, calculate the statistic you’re interested in.
This results in <em>k</em> different estimates for a given statistic, which you can then use to calculate a confidence interval for the statistic.
The easiest way to perform bootstrapping in Python is to use the  bootstrap  function from the <b>SciPy</b> library.
The following example shows how to use this function in practice.
<h3>Example: Perform Bootstrapping in Python</h3>
Suppose we create a dataset in Python that contains 15 values:
<b>#define array of data values
data = [7, 9, 10, 10, 12, 14, 15, 16, 16, 17, 19, 20, 21, 21, 23]</b>
We can use the following code to calculate a 95% bootstrapped confidence interval for the median value:
<b>from scipy.stats import bootstrap
import numpy as np
#convert array to sequence
data = (data,)
#calculate 95% bootstrapped confidence interval for median
bootstrap_ci = bootstrap(data, np.median, confidence_level=0.95,         random_state=1, method='percentile')
#view 95% boostrapped confidence interval
print(bootstrap_ci.confidence_interval)
ConfidenceInterval(low=10.0, high=20.0)
</b>
The 95% bootstrapped confidence interval for the median turns out to be <b>[10.0, 20.0]</b>.
Here’s what the<b> boostrap()</b> function actually did under the hood:
The <b>bootstrap()</b> function generated 9,999 samples with replacement. (The default is 9,999 but you can use the <b>n_resamples</b> argument to change this number)
For each bootstrapped sample, the median was calculated.
The median value of each sample was arranged from smallest to largest and the median value at percentile 2.5% and percentile 97.5% were used to construct the lower and upper limits of the 95% confidence interval.
Note that you can calculate a bootstrapped confidence interval for virtually any statistic.
For example, we can change <b>np.median</b> to <b>np.std</b> within the <b>bootstrap()</b> function to instead calculate a 95% confidence interval for the standard deviation:
<b>from scipy.stats import bootstrap
import numpy as np
#convert array to sequence
data = (data,)
#calculate 95% bootstrapped confidence interval for median
bootstrap_ci = bootstrap(data, np.std, confidence_level=0.95,         random_state=1, method='percentile')
#view 95% boostrapped confidence interval
print(bootstrap_ci.confidence_interval)
ConfidenceInterval(low=3.3199732261303283, high=5.66478399066117)
</b>
The 95% bootstrapped confidence interval for the standard deviation turns out to be <b>[3.32, 5.67]</b>.
<b>Note</b>: For these examples we chose to create 95% confidence intervals, but you can change the value in the <b>confidence_level</b> argument to construct a confidence interval of a different size.
<h2><span class="orange">How to Perform Bootstrapping in R (With Examples)</span></h2>
<b>Bootstrapping</b> is a method that can be used to estimate the standard error of any  statistic  and produce a  confidence interval  for the statistic.
The basic process for bootstrapping is as follows:
Take <em>k </em>repeated samples with replacement from a given dataset.
For each sample, calculate the statistic you’re interested in.
This results in <em>k</em> different estimates for a given statistic, which you can then use to calculate the standard error of the statistic and create a confidence interval for the statistic.
We can perform bootstrapping in R by using the following functions from the  boot library :
1. Generate bootstrap samples.
<b>boot(data, statistic, R, …)</b>
where:
<b>data:</b> A vector, matrix, or data frame
<b>statistic:</b> A function that produces the statistic(s) to be bootstrapped
<b>R:</b> Number of bootstrap replicates 
2. Generate a bootstrapped confidence interval.
<b>boot.ci(bootobject, conf, type)</b>
where:
<b>bootobject:</b> An object returned by the boot() function
<b>conf:</b> The confidence interval to calculate. Default is 0.95
<b>type:</b> Type of confidence interval to calculate. Options include “norm”, “basic”, “stud”, “perc”, “bca” and “all” – Default is “all”
The following examples show how to use these functions in practice.
<h3>Example 1: Bootstrap a Single Statistic</h3>
The following code shows how to calculate the standard error for the  R-squared  of a simple linear regression model:
<b>set.seed(0)
library(boot)
#define function to calculate R-squared
rsq_function &lt;- function(formula, data, indices) {
  d &lt;- data[indices,] #allows boot to select sample
  fit &lt;- lm(formula, data=d) #fit regression model
  return(summary(fit)$r.square) #return R-squared of model
}
#perform bootstrapping with 2000 replications
reps &lt;- boot(data=mtcars, statistic=rsq_function, R=2000, formula=mpg~disp)
#view results of boostrapping
reps
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = mtcars, statistic = rsq_function, R = 2000, formula = mpg ~ 
    disp)
Bootstrap Statistics :
     original      bias    std. error
t1* 0.7183433 0.002164339  0.06513426</b>
From the results we can see:
The estimated R-squared for this regression model is <b>0.7183433</b>.
The standard error for this estimate is <b>0.06513426</b>.
We can quickly view the distribution of the bootstrapped samples as well:
<b>plot(reps)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/boot1.png">
We can also use the following code to calculate the 95% confidence interval for the estimated R-squared of the model:
<b>#calculate adjusted bootstrap percentile (BCa) interval
boot.ci(reps, type="bca")
CALL : 
boot.ci(boot.out = reps, type = "bca")
Intervals : 
Level       BCa          
95%   ( 0.5350,  0.8188 )  
Calculations and Intervals on Original Scale</b>
From the output we can see that the 95% bootstrapped confidence interval for the true R-squared values is (.5350, .8188).
<h3>Example 2: Bootstrap Multiple Statistics</h3>
The following code shows how to calculate the standard error for each coefficient in a multiple linear regression model:
<b>set.seed(0)
library(boot)
#define function to calculate fitted regression coefficients
coef_function &lt;- function(formula, data, indices) {
  d &lt;- data[indices,] #allows boot to select sample
  fit &lt;- lm(formula, data=d) #fit regression model
  return(coef(fit)) #return coefficient estimates of model
}
#perform bootstrapping with 2000 replications
reps &lt;- boot(data=mtcars, statistic=coef_function, R=2000, formula=mpg~disp)
#view results of boostrapping
reps
ORDINARY NONPARAMETRIC BOOTSTRAP
Call:
boot(data = mtcars, statistic = coef_function, R = 2000, formula = mpg ~ 
    disp)
Bootstrap Statistics :
       original        bias    std. error
t1* 29.59985476 -5.058601e-02  1.49354577
t2* -0.04121512  6.549384e-05  0.00527082</b>
From the results we can see:
The estimated coefficient for the intercept of the model is <b>29.59985476</b> and the standard error of this estimate is <b>1.49354577</b>.
The estimated coefficient for the predictor variable <em>disp</em> in the model is <b>-0.04121512</b> and the standard error of this estimate is <b>0.00527082</b>.
We can quickly view the distribution of the bootstrapped samples as well:
<b>plot(reps, index=1) #intercept of model
plot(reps, index=2) #disp predictor variable
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/boot2.png">
We can also use the following code to calculate the 95% confidence intervals for each coefficient:
<b>#calculate adjusted bootstrap percentile (BCa) intervals
boot.ci(reps, type="bca", index=1) #intercept of model
boot.ci(reps, type="bca", index=2) #disp predictor variable
CALL : 
boot.ci(boot.out = reps, type = "bca", index = 1)
Intervals : 
Level       BCa          
95%   (26.78, 32.66 )  
Calculations and Intervals on Original Scale
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 2000 bootstrap replicates
CALL : 
boot.ci(boot.out = reps, type = "bca", index = 2)
Intervals : 
Level       BCa          
95%   (-0.0520, -0.0312 )  
Calculations and Intervals on Original Scale</b>
From the output we can see that the 95% bootstrapped confidence intervals for the model coefficients are as follows:
C.I. for intercept: (26.78, 32.66)
C.I. for <em>disp</em>: (-.0520, -.0312)
<h2><span class="orange">Box-Cox Transformation in Excel (Step-by-Step)</span></h2>
A <b>box-cox transformation</b> is a commonly used method for transforming a non-normally distributed dataset into a more  normally distributed  one.
The basic idea is to find some value for λ such that the transformed data is as close to normally distributed as possible, using the following formula:
y(λ) = (y<sup>λ</sup> – 1) / λ  if y ≠ 0
y(λ) = log(y)  if y = 0
The following step-by-step example shows how to perform a box-cox transformation on a dataset in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for a dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel1.png">
<h3>Step 2: Sort the Data</h3>
Next, create an index column and a column of sorted data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel2.png">
<h3>Step 3: Choose an Arbitrary Value for Lambda</h3>
Next, we’ll choose an arbitrary value of 1 for lambda and apply a temporary box-cox transformation to the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel3.png">
<h3>Step 4: Calculate the Z-Scores</h3>
Next, we’ll calculate the z-score for each value in the index:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel4.png">
We’ll then calculate the correlation between the box-cox transformed values and the z-scores:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel5.png">
<h3>Step 5: Find the Optimal Lambda Value</h3>
Next, we’ll use Goal Seek to find the optimal lambda value to use in the box-cox transformation.
To do so, click the <b>Data</b> tab along the top ribbon. Then click <b>What-If-Analysis</b> within the <b>Forecast</b> group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel6.png">
In the dropdown menu, click <b>Goal Seek</b> and fill in the following values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel7.png">
Once you click <b>OK</b>, Goal Seek will automatically find the optimal lambda value to be <b>-0.5225</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel8.png">
<h3>Step 6: Perform the Box-Cox Transformation</h3>
Lastly, we’ll apply the box-cox transformation to the original data, using a lambda value of -0.5225:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/boxExcel9.png">
<b>Bonus:</b> We can confirm that the transformed data is normally distributed by performing a  Jarque-Bera test in Excel .
<h2><span class="orange">How to Perform a Box-Cox Transformation in R (With Examples)</span></h2>
A <b>box-cox transformation</b> is a commonly used method for transforming a non-normally distributed dataset into a more  normally distributed  one.
The basic idea behind this method is to find some value for λ such that the transformed data is as close to normally distributed as possible, using the following formula:
y(λ) = (y<sup>λ</sup> – 1) / λ  if y ≠ 0
y(λ) = log(y)  if y = 0
We can perform a box-cox transformation in R by using the <b>boxcox()</b> function from the <b>MASS()</b> library. The following example shows how to use this function in practice.
<em>Refer to  this paper  from the University of Connecticut for a nice summary of the development of the Box-Cox transformation.</em>
<h3>Example: Box-Cox Transformation in R</h3>
The following code shows how to fit a linear regression model to a dataset, then use the <b>boxcox()</b> function to find an optimal lambda to transform the  response variable  and fit a new model. 
<b>library(MASS)
#create data
y=c(1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 6, 7, 8)
x=c(7, 7, 8, 3, 2, 4, 4, 6, 6, 7, 5, 3, 3, 5, 8)
#fit linear regression model
model &lt;- lm(y~x)
#find optimal lambda for Box-Cox transformation 
bc &lt;- boxcox(y ~ x)
(lambda &lt;- bc$x[which.max(bc$y)])
[1] -0.4242424
#fit new linear regression model using the Box-Cox transformation
new_model &lt;- lm(((y^lambda-1)/lambda) ~ x)
</b>
The optimal lambda was found to be <b>-0.4242424</b>. Thus, the new regression model replaced the original response variable y with the variable y = (y<sup>-0.4242424</sup> – 1) / -0.4242424.
The following code shows how to create two  Q-Q plots  in R to visualize the differences in residuals between the two regression models:
<b>#define plotting area
op &lt;- par(pty = "s", mfrow = c(1, 2))
#Q-Q plot for original model
qqnorm(model$residuals)
qqline(model$residuals)
#Q-Q plot for Box-Cox transformed model
qqnorm(new_model$residuals)
qqline(new_model$residuals)
#display both Q-Q plots
par(op)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxCoxR1.png">
As a rule of thumb, if the data points fall along a straight diagonal line in a Q-Q plot then the dataset likely follows a normal distribution.
Notice how the box-cox transformed model produces a Q-Q plot with a much straighter line than the original regression model.
This is an indication that the residuals of the box-cox transformed model are much more normally distributed, which satisfies one of  the assumptions of linear regression .
<h2><span class="orange">How to Perform a Box-Cox Transformation in Python</span></h2>
A <b>box-cox transformation</b> is a commonly used method for transforming a non-normally distributed dataset into a more  normally distributed  one.
The basic idea behind this method is to find some value for λ such that the transformed data is as close to normally distributed as possible, using the following formula:
y(λ) = (y<sup>λ</sup> – 1) / λ  if y ≠ 0
y(λ) = log(y)  if y = 0
We can perform a box-cox transformation in Python by using the  scipy.stats.boxcox()  function.
The following example shows how to use this function in practice.
<h3>Example: Box-Cox Transformation in Python</h3>
Suppose we generate a random set of 1,000 values that come from an  exponential distribution :
<b>#load necessary packages
import numpy as np 
from scipy.stats import boxcox 
import seaborn as sns 
#make this example reproducible
np.random.seed(0)
#generate dataset
data = np.random.exponential(size=1000)
#plot the distribution of data values
sns.distplot(data, hist=False, kde=True) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/boxCox1.png">
We can see that the distribution does not appear to be normal.
We can use the <b>boxcox()</b> function to find an optimal value of lambda that produces a more normal distribution:
<b>#perform Box-Cox transformation on original data
transformed_data, best_lambda = boxcox(data) 
#plot the distribution of the transformed data values
sns.distplot(transformed_data, hist=False, kde=True) </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/boxCox2.png">
We can see that the transformed data follows much more of a normal distribution.
We can also find the exact lambda value used to perform the Box-Cox transformation:
<b>#display optimal lambda value
print(best_lambda)
0.2420131978174143
</b>
The optimal lambda was found to be roughly <b>0.242</b>.
Thus, each data value was transformed using the following equation:
New = (old<sup>0.242</sup> – 1) / 0.242
We can confirm this by looking at the values from the original data compared to the transformed data:
<b>#view first five values of original dataset
data[0:5]
array([0.79587451, 1.25593076, 0.92322315, 0.78720115, 0.55104849])
#view first five values of transformed dataset
transformed_data[0:5]
array([-0.22212062,  0.23427768, -0.07911706, -0.23247555, -0.55495228])
</b>
The first value in the original dataset was <b>0.79587</b>. Thus, we applied the following formula to transform this value:
New = (.79587<sup>0.242</sup> – 1) / 0.242 = <b>-0.222</b>
We can confirm that the first value in the transformed dataset is indeed <b>-0.222</b>.
<h2><span class="orange">How to Make a Box Plot in Google Sheets</span></h2>
A <b>box plot </b>is a type of plot that we can use to visualize the  five number summary  of a dataset, which includes:
The minimum
The first quartile
The median
The third quartile
The maximum
This tutorial explains how to create a box plot in Google Sheets.
<h3>Example: Box Plots in Google Sheets</h3>
Use the following steps to create a box plot in Google Sheets.
<b>Step 1: Enter the data.</b>
First enter the values of your dataset into one column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/fiveNumSumSheets1.png">
<b>Step 2: Calculate the five number summary</b>
Typically we need to know the median value of a dataset to create a box plot, but as we’ll see in the next step we will instead use a candlestick chart to create something that looks like a boxplot since Google Sheets doesn’t have a box plot chart option.
In order to create a candlestick chart we only need to know the minimum, 1st quartile, 3rd quartile, and maximum value of a dataset. The following image shows the formulas to use to calculate these numbers:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxplotSheets1.png">
<b>Step 3: Create the box plot.</b>
Next, highlight the values in columns <em>A </em>through <em>E</em> in the first row:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxplotSheets2.png">
Click the <b>Insert </b>tab along the top ribbon, then click <b>Chart </b>in the dropdown menu:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxplotSheets3.png">
In the <b>Chart Editor </b>window that appears on the right side of the screen, click the dropdown menu for <b>Chart type </b>and then click the chart type titled <b>Candlestick chart</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxplotSheets4.png">
Once you click this, the following chart will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/boxplotSheets5.png">
The way to interpret the chart is as follows:
The top line extends to the maximum value of the dataset (<b>28</b>)
The top of the box represents the value of the third quartile (<b>22</b>)
The bottom of the box represents the value of the first quartile (<b>7.5</b>)
The bottom line extends to the minimum value of the dataset (<b>4</b>)
Within the <b>Customize </b>subsection of the <b>Chart Editor </b>window on the right side of the screen you can also modify the plot to include titles, adjust gridlines, and modify the axis labels.
<h2><span class="orange">How to Identify Skewness in Box Plots</span></h2>
A <b>box plot</b> is a type of plot that displays the five number summary of a dataset, which includes:
The minimum value
The first quartile (the 25th percentile)
The median value
The third quartile (the 75th percentile)
The maximum value
We use the following process to draw a box plot:
Draw a box from the first quartile (Q1) to the third quartile (Q3)
Then draw a line inside the box at the median
Then draw “whiskers” from the quartiles to the minimum and maximum values
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/boxplot.png">
We can determine whether or not a distribution is skewed based on the location of the median value in the box plot.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew7.png">
When the median is closer to the bottom of the box and the whisker is shorter on the lower end of the box, the distribution is right-skewed (or “positively” skewed).
When the median is closer to the top of the box and the whisker is shorter on the upper end of the box, the distribution is left-skewed (or “negatively” skewed).
When the median is in the middle of the box and the whiskers are roughly equal on each side, the distribution is symmetrical (or “no” skew).
The following examples illustrate how to use box plots to determine if a distribution is right-skewed, left-skewed, or has no skew.
<h3>Example 1: Right-Skewed Distribution</h3>
The distribution of annual household incomes in the United States is right-skewed. Most households earn between $40k and $80k per year but there’s a long right tail on the distribution that represents households that earn much more.
If we created a box plot to visualize the distribution of household incomes, it would look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew1.png">
Notice that the vertical line inside the box that represents the median is much closer to the first quartile than the third quartile, which means the distribution is right-skewed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew2.png">
<h3>Example 2: Left-Skewed Distribution</h3>
The distribution of the age of deaths in most populations is left-skewed. Most people live to be between 70 and 80 years old, with fewer and fewer living less than this age.
If we created a box plot to visualize the distribution of the age of deaths, it would look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew3.png">
Notice that the vertical line inside the box that represents the median is much closer to the third quartile than the first quartile, which means the distribution is left-skewed.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew4.png">
<h3>Example 3: Symmetrical Distribution</h3>
The distribution of the height of males is roughly  symmetrically distributed  and has no skew. For example, the average height of a male in the United States is roughly 69.1 inches. The distribution of heights is roughly symmetrical, with some being shorter and some being taller.
If we created a box plot to visualize the distribution of the height of males in the United States, it would look something like this: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew5.png">
Notice that the vertical line inside the box that represents the median is equally close to the first quartile and the third quartile, which means the distribution is symmetrical and has no skew.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/boxSkew6.png">
<h2><span class="orange">How to Create and Interpret Box Plots in Excel</span></h2>
A <b>box plot </b>is a type of plot that we can use to visualize the five number summary of a dataset, which includes:
The minimum
The first quartile
The median
The third quartile
The maximum 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/boxplot.png">
This tutorial explains how to create and interpret box plots in Excel.
<h3>How to Create a Box Plot in Excel</h3>
Perform the following steps to create a box plot in Excel.
<b>Step 1: Enter the data.</b>
Enter the data in one column.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel1.png">
<b>Step 2: Create the box plot.</b>
Highlight all of the data values.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel3.png">
On the <b>Insert </b>tab, go to the <b>Charts </b>group and click the <b>Statistic Chart </b>symbol.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel2.png">
Click <b>Box and Whisker</b>. A box plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel4.png">
To see the actual values that are summarized in the box plot, click on the plot. Then click the green plus sign that appears in the top right corner. Then check the box next to <b>Data Labels</b>. The following labels will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel5.png">
Here is how to interpret the labels in the box plot:
<b>Min: 3</b>. This is the smallest value in the dataset that is not considered an outlier. For this particular example, there are no outliers.
<b>Q1: 5.25.</b> This is the value of the first quartile in the dataset.
<b>Median: 15. </b>This is the median value in the dataset.
<b>Mean: 14.75.</b> The tiny “x” in the middle of the box represents the mean of the dataset.
<b>Q3: 23.75</b>. This is the value of the third quartile in the dataset.
<b>Max: 29</b>. This is the largest value in the dataset that is not considered an outlier. For this particular example, there are no outliers.
The screenshot below shows the exact formulas you can use in Excel to find each of these values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel6.png">
<h3>A Note on Outliers</h3>
The interquartile range (IQR) is the distance between the third quartile and the first quartile. Excel considers any data value to be an “outlier” if it is 1.5 times the IQR larger than the third quartile or 1.5 times the IQR smaller than the first quartile.
In this example, the IQR is 23.75 – 5.25 = 18.5. Thus, any value smaller than 5.25  – (18.5 * 1.5) = <b>-22.5 </b>or any value larger than 23.75 + (18.5 * 1.5) = <b>51.5 </b>would be considered an outlier.
Since no value in the dataset is smaller than -22.5 or larger than 51.5, there are no dots that appear in the box plot to indicate outliers. However, if our largest value was actually 52 then the box plot would show a dot to indicate the outlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel7.png">
<h3>How to Create Multiple Box Plots in Excel</h3>
You can easily create multiple box plots in Excel by simply entering more than one dataset in separate columns.
For example, suppose we have two datasets. To create a boxplot for each dataset, we would simply highlight both columns of data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel8-1.png">
Then once again on the <b>Insert </b>tab, go to the <b>Charts </b>group and click the <b>Statistic Chart </b>symbol.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel2.png">
Click <b>Box and Whisker</b>. A box plot for each dataset will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotExcel9.png">
This is particularly useful if we want to quickly visualize the distributions of two or more datasets.
<h2><span class="orange">How to Create and Interpret Box Plots in SPSS</span></h2>
A  box plot  is used to visualize the five number summary of a dataset, which includes:
The minimum
The first quartile
The median
The third quartile
The maximum 
This tutorial explains how to create and modify box plots in SPSS.
<h3>How to Create a Single Box Plot in SPSS</h3>
Suppose we have the following dataset that shows the average points scored per game by 16 basketball players on a certain team:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS1.png">
To create a box plot to visualize the distribution of these data values, we can click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Explore</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS2.png">
This will bring up the following window:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS3.png">
To create a box plot, drag the variable <b>points </b>into the box labelled <b>Dependent List</b>. Then make sure <b>Plots </b>is selected under the option that says <b>Display </b>near the bottom of the box.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/stemSPSS4.png"(max-width: 540px) 100vw, 540px">
Once you click <b>OK</b>, the following box plot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS1.png">
Here’s how to interpret this box plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS2.png">
<h3>A Note on Outliers</h3>
The interquartile range (IQR) is the distance between the third quartile and the first quartile. SPSS considers any data value to be an outlier if it is 1.5 times the IQR larger than the third quartile or 1.5 times the IQR smaller than the first quartile.
Outliers are displayed as tiny circles in SPSS. In the previous example there were no outliers, which is why there were no tiny circles shown in the box plot. However, if our largest value in the dataset was actually 50 then the box plot would show a tiny circle to indicate the outlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS6-1.png">
If an outlier is present in your dataset, you have a few options:
<b>Make sure the outlier is not a data entry error.</b> Sometimes data values are simply recorded incorrectly. If an outlier is present, first verify that the value was entered correctly and that it wasn’t an error.
<b>Assign a new value to the outlier</b>. If the outlier turns out to be a result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
<b>Remove the outlier. </b>If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<h3>How to Create Multiple Box Plots in SPSS</h3>
If you have several variables, SPSS can also create multiple side-by-side box plots. For example, suppose we have the following data on average points scored by 16 players on three different teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS3.png">
To create a box plot for each of these variables, we can once again click on the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Explore</b>. We can then drag all three variables into the box labelled <b>Dependent List</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS4.png">
Once we click <b>OK</b>, the following box plots will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/boxSPSS5.png">
This helps us easily visualize the differences in the distributions between these three teams.
We can also observe the following:
The median points scored per game is highest for team B and lowest for team C
The variation in the number of points scored per game is highest for team B, which can be seen by how long their box plot is compared to team A and team C.
The player with the highest points per game is on team B and the player with the lowest points per game is on team C.
Box plots are useful because they can provide us with so much information about the distribution of datasets just from a single plot.
<h2><span class="orange">How to Create and Modify Box Plots in Stata</span></h2>
A <b>box plot </b>is a type of plot that we can use to visualize the five number summary of a dataset, which includes:
The minimum
The first quartile
The median
The third quartile
The maximum 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/boxplot.png">
This tutorial explains how to create and modify box plots in Stata.
<h2>Example: Box Plots in Stata</h2>
We’ll use a dataset called <em>auto </em>to illustrate how to create and modify boxplots in Stata.
First, load the data by typing the following into the Command box and clicking <em>Enter</em>:
<b>use http://www.stata-press.com/data/r13/auto</b>
<h3>Vertical Box Plots</h3>
We can create a vertical box plot for the variable <em>mpg </em>by using the <b>graph box </b>command:
<b>graph box mpg</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata1.png">
<h3>Horizontal Box Plots</h3>
Alternatively, we can create a horizontal box plot by using the <b>graph hbox </b>command:
<b>graph hbox mpg</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata2.png">
<h3>Box Plots by Category</h3>
We can also create several box plots based on a single categorical variable using the <b>over() </b>command. For example, the following command can be used to create box plots that show the distribution of <em>mpg</em>, based on the categorical variable <em>foreign</em>, which indicates whether a car is foreign or domestic.
<b>graph box mpg, over(foreign)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata3.png">
<h3>Multiple Box Plots by Category</h3>
We can also create box plots for more than one variable based on a categorical variable. For example, the following command can be used to create box plots for the variables <em>headroom </em>and <em>gear_ratio</em>, based on the categorical variable <em>foreign</em>:
<b>graph box headroom gear_ratio, over(foreign)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata4.png">
<h3>Modifying the Appearance of Box Plots</h3>
We can use several different commands to modify the appearance of the box plots.
We can add a title to the plot using the <b>title() </b>command:
<b>graph box mpg, title(“Distribution of mpg”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata5.png">
We can also add a subtitle underneath the title using the <b>subtitle() </b>command:
<b>graph box mpg, title(“Distribution of mpg”) subtitle(“(sample size = 74 cars)”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata6.png">
We can also add a note or comment at the bottom of the graph by using the <b>note() </b>command:
<b>graph box mpg, note(“Source: 1978 Automobile Data”)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata7.png">
Lastly, we can change the actual color of the box plot by using the <b>box(variable #, color(color_choice))</b> command:
<b>graph box mpg, box(1, color(green))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/boxPlotStata8.png">
A full list of available colors can be found in the  Stata Documentation .
<h2><span class="orange">Boxplot Generator</span></h2>
td, tr, th {
    border: 1px solid black;
}
table {
    border-collapse: collapse;
}
td, th {
    min-width: 50px;
    height: 21px;
}
    .label_radio {
text-align: center;
}
#text_area_input {
padding-left: 35%;
float: left;
}
svg:not(:root) {
  overflow: visible;
}
</style>
A <b>boxplot</b> (sometimes called a box-and-whisker plot) is a plot that shows the five-number summary of a dataset. The five-number summary is the minimum, first quartile, median, third quartile, and the maximum.
To create a boxplot for a given dataset, enter your comma separated data in the box below:
<textarea id="input_data" name="x" rows="5" cols="40"></textarea>
<input type="button" id="button" onclick="calc()" value="Generate Boxplot">
Minimum: 
First quartile: 
Median: 
Third quartile: 
Maximum: 
<script>
//function to get weird object keys from object that holds boxplot
function resolve(path, obj=self, separator='.') {
    var properties = Array.isArray(path) ? path : path.split(separator)
    return properties.reduce((prev, curr) => prev && prev[curr], obj)
}
function calc() {
var input_data = document.getElementById('input_data').value.match(/\d+/g).map(Number);
var data = [];
for (var i=0; i < input_data.length; i++) {
    data.push({
        Q1: input_data[i]
    });
}
var box = (values) => {
  var sorted = values.slice();
  sorted.sort((a, b) => a - b);
  var max = sorted[sorted.length - 1];
  var min = sorted[0];
  //var upper = d3.quantile(sorted, 0.75);
  //var mid = d3.quantile(sorted, 0.5);
  //var lower = d3.quantile(sorted, 0.25);
// find the median of the sample
    var mid = math.median(sorted);
    // split the data by the median
    var _firstHalf = sorted.filter(function(f){ return f < mid})
    var _secondHalf = sorted.filter(function(f){ return f > mid})
    // find the medians for each split, calculate IQR
    var lower = math.median(_firstHalf);
    var upper = math.median(_secondHalf);
  var lowerLimit = lower - ((upper - lower) * 1.5);
  var lowerWhisker = sorted.find(function(d) { return d > lowerLimit; });
  var upperLimit = upper - (-1 * ((upper - lower) * 1.5));
  var upperWhisker = sorted.reverse().find(function(d) { return d < upperLimit; });
  return {
    upper: upper, mid: mid, lower: lower, lowerWhisker: lowerWhisker,
    upperWhisker: upperWhisker, max: max, min:min,
    outliers: values.filter(function(d) { return d > upperWhisker || d < lowerWhisker; })
  }
}
var flatten = function(arrays) {
  return arrays.reduce(function(a, b) {
    return a.concat(b);
  }, [])
}
  var quarters = Object.keys(data[0]);
  var series = quarters.map(function(q) {
    return {
      quarter: q,
      data: box(data.map(function(d) { return Number(d[q]); }))
    };
  })
  var yFormat = d3.format(',.0f');
  var yExtent = fc.extentLinear()
    .accessors([function(d) { return d.max; }, function(d) { return d.min; }])
    .pad([0, 0.1])
    .include([0])
  var boxplot = fc.seriesSvgBoxPlot()
      .crossValue(function(d) { return d.quarter; })
      .medianValue(function(d) { return d.data.mid; })
      .barWidth(50)
      .upperQuartileValue(function(d) { return d.data.upper; })
      .lowerQuartileValue(function(d) { return d.data.lower; })
      .highValue(function(d) { return d.data.upperWhisker; })
      .lowValue(function(d) { return d.data.lowerWhisker; });
  var point = fc.seriesSvgPoint()
      .crossValue(function(d) { return d[0]; })
      .mainValue(function(d) { return d[1]; });
  var label = fc.seriesSvgPoint()
      .crossValue(function(d) { return d[0]; })
      .mainValue(function(d) { return d[1]; })
      .decorate(function(selection) {
        selection.enter()
            .select('path')
            .attr('display', 'none')
        selection.enter()
            .append('text')
            .style('text-anchor', 'middle')
            .attr('transform', function(d, i) { return  'translate(' + (i % 2 === 0 ? -50 : 50) + ', 5)'; })
            .text(function(d) { return yFormat(d[1]); })
            .attr('stroke', 'transparent')
            .attr('fill', 'black');
      });
  var multi = fc.seriesSvgMulti()
      .series([boxplot, point, label])
      .mapping((data, index, series) => {
        switch(series[index]) {
          case point:
            return flatten(data.map(function(s) {
              return s.data.outliers.map(function(o) { return [s.quarter, o]; })
            }))
          case boxplot:
            return data;
        }
      });
  var chart = fc.chartSvgCartesian(
        d3.scalePoint(),
        d3.scaleLinear()
      )
      .xDomain(quarters)
      .xPadding(0.1)
      .yDomain(yExtent(series.map(function(d) { return d.data; })))
      .yTickFormat(yFormat)
      .plotArea(multi);
  d3.select('#chart')
      .datum(series)
      .call(chart);
      
      //output results
      var min = resolve('0->data->min', series, '->');
      var quartile1 = resolve('0->data->lower', series, '->');
      var mid = resolve('0->data->mid', series, '->');
      var quartile3 = resolve('0->data->upper', series, '->');
      var max = resolve('0->data->max', series, '->');
      var outliers = resolve('0->data->outliers', series, '->');
      
      document.getElementById('min').innerHTML = min;
      document.getElementById('quartile1').innerHTML = quartile1;
      document.getElementById('mid').innerHTML = mid;
      document.getElementById('quartile3').innerHTML = quartile3;
      document.getElementById('max').innerHTML = max;
  
} //end calc function
</script>
<h2><span class="orange">How to Create Boxplot from Pandas DataFrame</span></h2>
You can use the following syntax to create boxplots from a pandas DataFrame:
<b>#create boxplot of one column
df.boxplot(column=['col1'])
#create boxplot of multiple columns
df.boxplot(column=['col1', 'col2'])
#create boxplot grouped by one column
df.boxplot(column=['col1'], by='col2') 
</b>
The following examples show how to use this syntax in practice with the following DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'conference': ['A', 'A', 'A', 'B', 'B', 'B'],   'points': [5, 7, 7, 9, 12, 9],   'assists': [11, 8, 10, 6, 6, 5],   'rebounds': [4, 2, 5, 8, 6, 11],})
#view DataFrame
df</b>
<h3>Example 1: Boxplot of One Column</h3>
The following code shows how to create a boxplot for one column in a pandas DataFrame:
<b>df.boxplot(column=['points'], grid=False, color='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/pandasBox1.png">
<h3>Example 2: Boxplot of Multiple Columns</h3>
The following code shows how to create a boxplot for multiple columns in a pandas DataFrame:
<b>df.boxplot(column=['points', 'assists'], grid=False, color='black')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/pandasBox2.png">
<h3>Example 3: Boxplot Grouped by One Column</h3>
The following code shows how to create a boxplot grouped by one column in a pandas DataFrame:
<b>df.boxplot(column=['points'], by='conference', grid=False, color='black')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/pandasBox3.png">
<h2><span class="orange">How to Draw Boxplots with Mean Values in R (With Examples)</span></h2>
You can use the following methods to draw a boxplot with a mean value in R:
<b>Method 1: Use Base R</b>
<b>#create boxplots
boxplot(df$values~df$group)
#calculate mean value by group
means &lt;- tapply(df$values, df$group, mean)
#add means as circles to each boxplot
points(means, pch=20) </b>
<b>Method 2: Use ggplot2</b>
<b>library(ggplot2)
#create boxplots with mean values shown as circles
ggplot(df, aes(x=group, y=values, fill=group)) +
  geom_boxplot() +
  stat_summary(fun=mean, geom='point', shape=20)
</b>
The following examples show how to use each method in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=5), points=c(4, 4, 5, 6, 8, 7, 6, 8, 9, 12,          11, 12, 13, 16, 18))
#view first six rows of data frame
head(df)
  team points
1    A      4
2    A      4
3    A      5
4    A      6
5    A      8
6    B      7
</b>
<h3>Example 1: Create Boxplots with Mean Values in Base R</h3>
The following code shows how to create boxplots with mean values in base R:
<b>#create boxplots
boxplot(df$points~df$team)
#calculate mean value by group
means &lt;- tapply(df$points, df$team, mean)
#add means as circles to each boxplot
points(means, pch=20, cex=1.5)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/boxplot_mean1.jpg"449">
The black lines inside each boxplot represent the <b>median</b> value and the black circles inside each boxplot represent the <b>mean</b> value.
<b>Note</b>: Change the value for the <b>cex</b> argument to change the circle size.
<h3>Example 2: Create Boxplots with Mean Values in ggplot2</h3>
The following code shows how to create boxplots with mean values in ggplot2:
<b>library(ggplot2)
#create boxplots with mean values
ggplot(df, aes(x=team, y=points, fill=team)) +
  geom_boxplot() +
  stat_summary(fun=mean, geom='point', shape=20, size=8) +
  theme(legend.position='none')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/boxplot_mean2.jpg">
The black lines inside each boxplot represent the <b>median</b> value and the black circles inside each boxplot represent the <b>mean</b> value.
<b>Note</b>: Change the value for the <b>size </b>argument within the <b>stat_summary()</b> function to change the circle size.
<h2><span class="orange">Boxplots</span></h2>
A <b>boxplot</b> (sometimes called a box-and-whisker plot) is a plot that shows the five-number summary of a dataset.
The five-number summary include:
The minimum
The first quartile
The median
The third quartile
The maximum
A boxplot allows us to easily visualize the distribution of values in a dataset using one simple plot.
<h3>How to Make a Boxplot</h3>
To make a boxplot, we draw a box from the first to the third quartile. Then we draw a vertical line at the median. Lastly, we draw “whiskers” from the quartiles to the minimum and maximum value.
  
Suppose we have the following dataset that shows the height of ten plants:
  
To make a boxplot, we need to find the minimum, first quartile, median, third quartile, and maximum.
<b>Step 1: Arrange the data from smallest to largest.</b>
10, 11, 12, 12, 13, 14, 16, 19, 20, 24
<b>Step 2: Find the median.</b>
In this case, it’s the average of the middle two numbers:
10, 11, 12, 12, <b>13, 14</b>, 16, 19, 20, 24
Median = (13 + 14) / 2 = 13.5
<b>Step 3: Find the lower quartile (Q1) and upper quartile (Q3).</b>
The lower quartile is the median of the numbers to the left of the median. In this case, it’s 12.
<b>10, 11, 12, 12, 13</b>, 14, 16, 19, 20, 24
The upper quartile is the median of the numbers to the right of the median. In this case, it’s 19.
10, 11, 12, 12, 13, <b>14, 16, 19, 20, 24</b>
<b>Step 4: Find the minimum and maximum values.</b>
The minimum is 10 and the maximum is 24.
<b>10</b>, 11, 12, 12, 13, 14, 16, 19, 20, <b>24</b>
<b>Step 5: Draw the boxplot using our five number summary.</b>
<b>  </b>
<h3>Why Are Boxplots Useful?</h3>
Boxplots are useful because they help us visualize five important descriptive statistics of a dataset: the minimum, lower quartile, median, upper quartile, and maximum.
Boxplots also help us easily answer questions like:
<b><em>What is the median height of the plants?</em></b>
To answer this, we can look for the vertical line within the box that indicates the median. In this case, it’s 13.5 inches. 
  
<b><em>How tall is the tallest plant?</em></b>
To answer this, we can look for the dot at the end of the right whisker that indicates the maximum. In this case, it’s 24 inches.
  
<b><em>What percent of plants are taller than 19 inches?</em></b>
To answer this, we can see that the the upper quartile (Q3) is equal to 19. Recall that the upper quartile represents the 75th percentile, which means 75% of values are equal to or less than 19.
This means that 25% of values are greater than 19. So, 25% of plants are taller than 19 inches.
<h3>  </h3>
<h3>How to Create Boxplots Using Different Software</h3>
The following tutorials provide step-by-step examples of how to create boxplots using different software:
 How to Make Box Plots in Excel 
 How to Make a Box Plot in Google Sheets 
 How to Make Box Plots in SPSS 
 How to Make Box Plots in Stata 
 Online Box Plot Generator 
 
<h2><span class="orange">How to Calculate Bray-Curtis Dissimilarity in R</span></h2>
The  Bray-Curtis Dissimilarity  is a way to measure the dissimilarity between two different sites.
It’s often used in ecology and biology to quantify how different two sites are in terms of the species found in those sites. 
It is calculated as:
<b>BC<sub>ij</sub> = 1 – (2*C<sub>ij</sub>) / (S<sub>i</sub> + S<sub>j</sub>)</b>
where:
<b>C<sub>ij</sub>:</b> The sum of the lesser values for the species found in each site.
<b>S<sub>i</sub>:</b> The total number of specimens counted at site <em>i</em>
<b>S<sub>j</sub>:</b> The total number of specimens counted at site <em>j</em>
The Bray-Curtis Dissimilarity always ranges between 0 and 1 where:
<b>0</b> indicates that two sites have zero dissimilarity. In other words, they share the exact same number of each type of species.
<b>1</b> indicates that two sites have complete dissimilarity. In other words, they share none of the same type of species.
For example, suppose a botanist goes out and counts the number of five different plant species (A, B, C, D, and E) in two different sites. 
The following table summarizes the data she collected:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bray_curtis.png">
Using this data, she can calculate the Bray-Curtis dissimilarity as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bray_curtis2.png">
Plugging these numbers into the Bray-Curtis dissimilarity formula, we get:
BC<sub>ij</sub> = 1 – (2*C<sub>ij</sub>) / (S<sub>i</sub> + S<sub>j</sub>)
BC<sub>ij</sub> = 1 – (2*15) / (21 + 24)
BC<sub>ij</sub> = 0.33
The Bray-Curtis dissimilarity between these two sites is <b>0.33</b>.
The following example shows how to calculate Bray-Curtis dissimilarity in R.
<h3>Example: Calculating Bray-Curtis Dissimilarity in R</h3>
First, let’s create the following data frame in R to hold our data values:
<b>#create data frame
df &lt;- data.frame(A=c(4, 3), B=c(0, 6), C=c(2, 0), D=c(7, 4), E=c(8, 11))
#view data frame
df
  A B C D  E
1 4 0 2 7  8
2 3 6 0 4 11</b>
We can use the following code to calculate the Bray-Curtis dissimilarity between the two rows of the data frame:
<b>#calculate Bray-Curtis dissimilarity
sum(apply(df, 2, function(x) abs(max(x)-min(x)))) / sum(rowSums(df))
[1] 0.3333333
</b>
The Bray-Curtis dissimilarly turns out to be <b>0.33</b>.
This matches the value that we calculated earlier by hand.
<b>Note</b>: This formula will only work if each row in the data frame represents a distinct site.
<h2><span class="orange">Bray-Curtis Dissimilarity: Definition & Examples</span></h2>
Named after  J. Roger Bray  and  John Thomas Curtis , the <b>Bray-Curtis Dissimilarity</b> is a way to measure the dissimilarity between two different sites.
It’s often used in ecology and biology to quantify how different two sites are in terms of the species found in those sites. 
The Bray-Curtis Dissimilarity is calculated as:
<b>BC<sub>ij</sub> = 1 – (2*C<sub>ij</sub>) / (S<sub>i</sub> + S<sub>j</sub>)</b>
where:
<b>C<sub>ij</sub>:</b> The sum of the lesser values for the species found in each site.
<b>S<sub>i</sub>:</b> The total number of specimens counted at site <em>i</em>
<b>S<sub>j</sub>:</b> The total number of specimens counted at site <em>j</em>
The Bray-Curtis Dissimilarity always ranges between 0 and 1 where:
<b>0</b> indicates that two sites have zero dissimilarity. In other words, they share the exact same number of each type of species.
<b>1</b> indicates that two sites have complete dissimilarity. In other words, they share none of the same type of species.
The following example shows how to calculate the Bray-Curtis Dissimilarity for two sites.
<h3>Example: Calculating the Bray-Curtis Dissimilarity</h3>
Suppose a botanist goes out and counts the number of five different plant species (A, B, C, D, and E) in two different sites. 
The following table summarizes the data she collected:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bray_curtis.png">
Using this data, she can calculate the Bray-Curtis dissimilarity as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bray_curtis2.png">
Plugging these numbers into the Bray-Curtis dissimilarity formula, we get:
BC<sub>ij</sub> = 1 – (2*C<sub>ij</sub>) / (S<sub>i</sub> + S<sub>j</sub>)
BC<sub>ij</sub> = 1 – (2*15) / (21 + 24)
BC<sub>ij</sub> = 0.33
The Bray-Curtis dissimilarity between these two sites is <b>0.33</b>.
<h3>Key Assumption of the Bray-Curtis Dissimilarity</h3>
The Bray-Curtis dissimilarity assumes that the two sites are of equal size.
This is a crucial assumption because if one site is four times larger than the other site, then we’ll naturally count more species in the larger site compared to the smaller site simply because there is so much more area to cover.
To illustrate this, suppose that one of the sites that the botanist collected data for was four times larger than the other site:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/bray_curtis3.png">
We would expect much higher frequencies of the species in Site 1 simply because it’s so much larger than Site 2.
Thus, when we go to calculate the Bray-Curtis Dissimilarity, it would be quite large. However, this would be misleading because the difference between the two sites isn’t in their composition, but rather in their size.
<h2><span class="orange">How to Perform a Breusch-Godfrey Test in R</span></h2>
One of the  key assumptions in linear regression  is that there is no correlation between the residuals, e.g. the residuals are independent.
To test for first-order autocorrelation, we can perform a  Durbin-Watson test . However, if we’d like to test for autocorrelation at higher orders then we need to perform a <b>Breusch-Godfrey test</b>.
This test uses the following  hypotheses :
<b>H<sub>0</sub> (null hypothesis): </b>There is no autocorrelation at any order less than or equal to <em>p</em>.
<b>H<sub>A</sub> (alternative hypothesis): </b>There exists autocorrelation at some order less than or equal to <em>p</em>.
The test statistic follows a Chi-Square distribution with <em>p</em> degrees of freedom.
If the  p-value  that corresponds to this test statistic is less than a certain significance level (e.g. 0.05) then we can reject the null hypothesis and conclude that autocorrelation exists among the residuals at some order less than or equal to <em>p</em>.
To perform a Breusch-Godfrey test in R, we can use the <b>bgtest(y ~ x, order = p)</b> function from the <b>lmtest</b> library.
This tutorial provides an example of how to use this syntax in R.
<h3>Example: Breusch-Godfrey Test in R</h3>
First, let’s create a fake dataset that contains two predictor variables (x1 and x2) and one response variable (y).
<b>#create dataset
df &lt;- data.frame(x1=c(3, 4, 4, 5, 8, 9, 11, 13, 14, 16, 17, 20), x2=c(7, 7, 8, 8, 12, 4, 5, 15, 9, 17, 19, 19),  y=c(24, 25, 25, 27, 29, 31, 34, 34, 39, 30, 40, 49))
#view first six rows of dataset
head(df)
  x1 x2  y
1  3  7 24
2  4  7 25
3  4  8 25
4  5  8 27
5  8 12 29
6  9  4 31</b>
Next, we can perform a Breusch-Godfrey test using the <b>bgtest() </b>function from the <b>lmtest </b>package.
For this example, we’ll test for autocorrelation among the residuals at order p =3:
<b>#load lmtest package
library(lmtest)
#perform Breusch-Godfrey test
bgtest(y ~ x1 + x2, order=3, data=df)
Breusch-Godfrey test for serial correlation of order up to 3
data:  y ~ x1 + x2
LM test = 8.7031, df = 3, p-value = 0.03351
</b>
From the output we can see that the test statistic is X<sup>2</sup> = <b>8.7031 </b>with 3 degrees of freedom. The corresponding p-value is <b>0.03351</b>. 
Since this p-value is less than 0.05, we can reject the null hypothesis and conclude that autocorrelation exists among the residuals at some order less than or equal to 3.
<h3>How to Handle Autocorrelation</h3>
If you reject the null hypothesis and conclude that autocorrelation is present in the residuals, then you have a few different options to correct this problem if you deem it to be serious enough:
For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
For negative serial correlation, check to make sure that none of your variables are <em>overdifferenced</em>.
For seasonal correlation, consider adding seasonal dummy variables to the model.
<h2><span class="orange">How to Perform a Breusch-Godfrey Test in Python</span></h2>
One of the  key assumptions in linear regression  is that there is no correlation between the residuals, e.g. the residuals are independent.
To test for first-order autocorrelation, we can perform a  Durbin-Watson test . However, if we’d like to test for autocorrelation at higher orders then we need to perform a <b>Breusch-Godfrey test</b>.
This test uses the following  hypotheses :
<b>H<sub>0</sub> (null hypothesis): </b>There is no autocorrelation at any order less than or equal to <em>p</em>.
<b>H<sub>A</sub> (alternative hypothesis): </b>There exists autocorrelation at some order less than or equal to <em>p</em>.
The test statistic follows a Chi-Square distribution with <em>p</em> degrees of freedom.
If the  p-value  that corresponds to this test statistic is less than a certain significance level (e.g. 0.05) then we can reject the null hypothesis and conclude that autocorrelation exists among the residuals at some order less than or equal to <em>p</em>.
To perform a Breusch-Godfrey test in Python, we can use the <b>acorr_breusch_godfrey()</b> function from the <b>statsmodels </b>library.
The following step-by-step example explains how to perform the Breusch-Godfrey test in Python.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that contains two predictor variables (x1 and x2) and one response variable (y).
<b>import pandas as pd 
#create dataset
df = pd.DataFrame({'x1': [3, 4, 4, 5, 8, 9, 11, 13, 14, 16, 17, 20],   'x2': [7, 7, 8, 8, 12, 4, 5, 15, 9, 17, 19, 19],    'y': [24, 25, 25, 27, 29, 31, 34, 34, 39, 30, 40, 49]})
#view first five rows of dataset
df.head()
x1x2y
03724
14725
24825
35827
481229
</b>
<h3>Step 2: Fit a Regression Model</h3>
Next, we can fit a  multiple linear regression model  using x1 and x2 as predictor variables and y as the  response variable .
<b>import statsmodels.api as sm
#define response variable
y = df['y']
#define predictor variables
x = df[['x1', 'x2']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
</b>
<h3>Step 3: Perform the Breusch-Godfrey Test</h3>
Next, we’ll perform the Breusch-Godfrey test to test for autocorrelation among the residuals at order <em>p</em>. For this example, we’ll choose <em>p</em> = 3.
<b>import statsmodels.stats.diagnostic as dg
#perform Breusch-Godfrey test at order <em>p</em> = 3
print(dg.acorr_breusch_godfrey(model, nlags=3))
(8.70314827, 0.0335094873, 5.27967224, 0.0403980576)
</b>
The first value in the output represents the test statistic and the second value represents the corresponding p-value.
From the output we can see the following:
Test statistic X<sup>2</sup> = <b>8.7031</b>
P-value = <b>0.0335</b>
Since this p-value is less than 0.05, we can reject the null hypothesis and conclude that autocorrelation exists among the residuals at some order less than or equal to 3.
<h3>How to Handle Autocorrelation</h3>
If you reject the null hypothesis and conclude that autocorrelation is present in the residuals, then you have a few different options to correct this problem if you deem it to be serious enough:
For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
For negative serial correlation, check to make sure that none of your variables are <em>overdifferenced</em>.
For seasonal correlation, consider adding seasonal dummy variables to the model.
<h2><span class="orange">How to Perform a Breusch-Pagan Test in Excel</span></h2>
A  Breusch-Pagan Test  is used to determine if  heteroscedasticity  is present in a regression analysis.
This tutorial explains how to perform a Breusch-Pagan Test in Excel.
<h3>Example: Breusch-Pagan Test in Excel</h3>
For this example we will use the following dataset that describes the attributes of 10 basketball players.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel1.png"(max-width: 423px) 100vw, 423px">
We will fit a multiple linear regression model using rating as the response variable and points, assists, and rebounds as the explanatory variables. Then we will perform a Breusch-Pagan Test to determine if heteroscedasticity is present in the regression. 
<b>Step 1: Perform multiple linear regression.</b>
Along the top ribbon in Excel, go to the Data tab and click on Data Analysis. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png"(max-width: 504px) 100vw, 504px">
Once you click on Data Analysis, a new window will pop up. Select <em>Regression </em>and click OK. Fill in the necessary arrays for the response variables and the explanatory variables, then click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel3.png"(max-width: 453px) 100vw, 453px">
This produces the following output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/VIFExcel4.png"(max-width: 668px) 100vw, 668px">
<b>Step 2: Calculate the squared residuals.</b>
Next, we will calculate the predicted values and the squared residuals for each response value. To calculate the predicted values, we will use the coefficients from the regression output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganExcel1.png">
We will use the same formula to obtain each predicted value:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganExcel2.png">
Next, we will calculate the squared residuals for each prediction:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganExcel3.png">
We will use the same formula to obtain each squared residual:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganExcel4.png">
<b>Step 3: Perform a new multiple linear regression using the squared residuals as the response values.</b>
Next, we will perform the same steps as before to conduct multiple linear regression using points, assists, and rebounds as the explanatory variables, except we will use the squared residuals as the response values this time. Here is the output of that regression:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganExcel5.png">
<b>Step 4: Perform the Breusch-Pagan Test.</b>
Lastly, we will perform the Breusch-Pagan Test to see if heteroscedasticity was present in the original regression.
First we will calculate the Chi-Square test statistic using the formula:
X<sup>2</sup> = n*R<sup>2</sup><sub>new</sub>
where:
n = number of observations
R<sup>2</sup><sub>new </sub>= R Square of the “new” regression in which the squared residuals were used as the response variable.
In our example, X<sup>2</sup> = 10 * 0.600395 = <b>6.00395</b>.
Next, we will find the p-value associated with this test statistic. We can use the following formula in Excel to do so:
=CHISQ.DIST.RT(test statistic, degrees of freedom)
In our case, the degrees of freedom is the number shown for <em>df </em>of regression in the output. In this case, it’s 3. Thus, our formula becomes:
=CHISQ.DIST.RT(6.00395, 3) = <b>0.111418</b>.
Because this p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that heteroscedasticity is present in the original regression model.
<h2><span class="orange">How to Perform a Breusch-Pagan Test in Python</span></h2>
In regression analysis,  heteroscedasticity  refers to the unequal scatter of residuals. Specifically, it refers to the case where there is a systematic change in the spread of the residuals over the range of measured values.
Heteroscedasticity is a problem because ordinary least squares (OLS) regression assumes that the residuals come from a population that has <em>homoscedasticity</em>, which means constant variance.
When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust.
One way to determine if heteroscedasticity is present in a  regression analysis  is to use a  Breusch-Pagan Test <b>.</b>
This tutorial explains how to perform a Breusch-Pagan Test in Python.
<h2>Example: Breusch-Pagan Test in Python</h2>
For this example we’ll use the following dataset that describes the attributes of 10 basketball players:
<b>import numpy as np
import pandas as pd
#create dataset
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view dataset
df
ratingpointsassistsrebounds
09025511
1852078
28214710
3881686
4942756
5902079
6761266
77515910
88714910
9861957</b>
We will fit a multiple linear regression model using rating as the response variable and points, assists, and rebounds as the explanatory variables. Then we will perform a Breusch-Pagan Test to determine if heteroscedasticity is present in the regression.
<b>Step 1: Fit a multiple linear regression model.</b>
First, we’ll fit a multiple linear regression model:
<b>import statsmodels.formula.api as smf
#fit regression model
fit = smf.ols('rating ~ points+assists+rebounds', data=df).fit()
#view model summary
print(fit.summary())
</b>
<b>Step 2: Perform a Breusch-Pagan test.</b>
Next, we’ll perform a Breusch-Pagan test to determine if heteroscedasticity is present.
<b>from statsmodels.compat import lzip
import statsmodels.stats.api as sms
#perform Bresuch-Pagan test
names = ['Lagrange multiplier statistic', 'p-value',
        'f-value', 'f p-value']
test = sms.het_breuschpagan(fit.resid, fit.model.exog)
lzip(names, test)
[('Lagrange multiplier statistic', 6.003951995818433),
 ('p-value', 0.11141811013399583),
 ('f-value', 3.004944880309618),
 ('f p-value', 0.11663863538255281)]</b>
A Breusch-Pagan test uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> Homoscedasticity is present.
<b>The alternative hypothesis: (Ha):</b> Homoscedasticity is <em>not </em>present (i.e. heteroscedasticity exists)
In this example, the Lagrange multiplier statistic for the test is <b>6.004 </b>and the corresponding p-value is <b>0.1114</b>. Because this p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that heteroscedasticity is present in the regression model.
<h2>How to Fix Heteroscedasticity</h2>
In the previous example we saw that heteroscedasticity was not present in the regression model. 
However, when heteroscedasticity actually is present there are three common ways to remedy the situation:
<b>1.</b> <b>Transform the dependent variable. </b>One way to fix heteroscedasticity is to transform the dependent variable in some way. One common transformation is to simply take the log of the dependent variable.
<b>2. Redefine the dependent variable. </b>Another way to fix heteroscedasticity is to redefine the dependent variable. One common way to do so is to use a <em>rate</em> for the dependent variable, rather than the raw value.
<b>3. Use weighted regression. </b>Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
Read more details about each of these three methods in  this post .
<h2><span class="orange">How to Perform a Breusch-Pagan Test in R</span></h2>
A  Breusch-Pagan Test  is used to determine if  heteroscedasticity  is present in a regression analysis.
This tutorial explains how to perform a Breusch-Pagan Test in R.
<h3>Example: Breusch-Pagan Test in R</h3>
In this example we will fit a regression model using the built-in R dataset <b>mtcars </b>and then perform a Breusch-Pagan Test using the <b>bptest </b>function from the <b>lmtest </b>library to determine if heteroscedasticity is present.
<b>Step 1: Fit a regression model.</b>
First, we will fit a regression model using <b>mpg </b>as the response variable and <b>disp </b> and <b>hp </b>as the two explanatory variables.
<b>#load the dataset
data(mtcars)
#fit a regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09
</b>
<b>Step 2: Perform a Breusch-Pagan Test.</b>
Next, we will perform a Breusch-Pagan Test to determine if heteroscedasticity is present.
<b>#load lmtest library
library(lmtest)
#perform Breusch-Pagan Test
bptest(model)
studentized Breusch-Pagan test
data:  model
BP = 4.0861, df = 2, p-value = 0.1296
</b>
The test statistic is <b>4.0861</b> and the corresponding p-value is <b>0.1296</b>. Since the p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that heteroscedasticity is present in the regression model.
<h2>What To Do Next</h2>
If you fail to reject the null hypothesis of the Breusch-Pagan test, then heteroscedasticity is not present and you can proceed to interpret the output of the original regression.
However, if you reject the null hypothesis, this means heteroscedasticity is present in the data. In this case, the standard errors that are shown in the output table of the regression may be unreliable.
There are a couple common ways that you can fix this issue, including:
<b>1. Transform the response variable. </b>You can try performing a transformation on the response variable. For example, you could use the  log of the response variable  instead of the original response variable. Typically taking the log of the response variable is an effective way of making heteroscedasticity go away. Another common transformation is to use the square root of the response variable.
<b>2. Use weighted regression. </b>This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
<h2><span class="orange">How to Perform a Breusch-Pagan Test in Stata</span></h2>
 Multiple linear regression  is a method we can use to understand the relationship between several explanatory variables and a response variable. 
Unfortunately, one problem that often occurs in regression is known as  heteroscedasticity , in which there is a systematic change in the variance of residuals over a range of measured values.
One test that we can use to determine if heteroscedasticity is present is the  Breusch-Pagan Test . This test produces a Chi-Square test statistic and a corresponding p-value.
If the p-value is below a certain threshold (common choices are 0.01, 0.05, and 0.10) then there is sufficient evidence to say that heteroscedasticity is present.
This tutorial explains how to perform a Breusch-Pagan Test in Stata.
<h2>Example: Breusch-Pagan Test in Stata</h2>
We will use the built-in Stata dataset <em>auto </em>to illustrate how to perform the Breusch-Pagan Test.
<b>Step 1: Load and view the data.</b>
First, use the following command to load the data:
<b>sysuse auto</b>
Then, view the raw data by using the following command:
<b>br</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/robustErrorsStata1.png"(max-width: 634px) 100vw, 634px">
<b>Step 2: Perform multiple linear regression.</b>
Next, we will type in the following command to perform a multiple linear regression using <em>price </em>as the response variable and <em>mpg </em>and <em>weight </em>as the explanatory variables:
<b>regress price mpg weight</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegStata1.png">
<b>Step 3: Perform the Breusch-Pagan Test.</b>
Once we fit the regression model, then we can perform the Breusch-Pagan Test using the <b>hettest </b>command, which is short for “heteroscedasticity test”:
<b>hettest</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/breuschPaganStata1.png">
Here is how to interpret the output:
<b>Ho: </b>This is the null hypothesis of the test, which states that there is constant variance among the residuals.
<b>Variables: </b>This tells us the response variable that was used in the regression model. In this case, it was the variable <em>price</em>.
<b>chi2(1): </b>This is the Chi-Square test statistic of the test. In this case, it is 14.78.
<b>Prob > chi2: </b>This is the p-value that corresponds to the Chi-Square test statistic. In this case, it is 0.0001. Since this value is less than 0.05, we can reject the null hypothesis and conclude that heteroscedasticity is present in the data.
<h2>What To Do Next</h2>
If you fail to reject the null hypothesis of the Breusch-Pagan test, then heteroscedasticity is not present and you can proceed to interpret the output of the original regression.
However, if you reject the null hypothesis of the Breusch-Pagan test, this means heteroscedasticity is present in the data. In this case, the standard errors that are shown in the output table of the regression are unreliable. There are several ways that you can fix this issue, including:
<b>1. Transform the response variable. </b>You can try performing a transformation on the response variable. For example, you could use log(price) instead of price as the response variable. Typically taking the log of the response variable is an effective way of making heteroscedasticity go away. Another common transformation is to use the square root of the response variable.
<b>2. Use weighted regression. </b>This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
<b>3. Use robust standard errors. </b>Robust standard errors are more “robust” to the problem of heteroscedasticity and tend to provide a more accurate measure of the true standard error of a regression coefficient. Check out  this tutorial  to learn about how to use robust standard errors in regression in Stata.
<h2><span class="orange">The Breusch-Pagan Test: Definition & Example</span></h2>
One of the key  assumptions of linear regression  is that the  residuals  are distributed with equal variance at each level of the predictor variable. This assumption is known as <b>homoscedasticity</b>.
When this assumption is violated, we say that  heteroscedasticity  is present in the residuals. When this occurs, the results of the regression become unreliable.
One way to visually detect whether heteroscedasticity is present is to create a plot of the residuals against the fitted values of the regression model.
If the residuals become more spread out at higher values in the plot, this is a tell-tale sign that heteroscedasticity is present.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/het2.jpg">
A formal statistical test we can use to determine if heteroscedasticity is present is the <b>Breusch-Pagan test</b>.
This tutorial provides a brief explanation of the Breusch-Pagan test along with an example.
<h3>What is the Breusch-Pagan Test?</h3>
The <b>Breusch-Pagan test</b> is used to determine whether or not heteroscedasticity is present in a regression model.
The test uses the following null and alternative  hypotheses :
<b>Null Hypothesis (H<sub>0</sub>):</b> Homoscedasticity is present (the residuals are distributed with equal variance)
<b>Alternative Hypothesis (H<sub>A</sub>):</b> Heteroscedasticity is present (the residuals are not distributed with equal variance)
If the p-value of the test is less than some  significance level  (i.e. α = .05) then we reject the null hypothesis and conclude that heteroscedasticity is present in the regression model.
We use the following steps to perform a Breusch-Pagan test:
<b>1. </b>Fit the regression model.
<b>2. </b>Calculate the squared residuals of the model.
<b>3.</b> Fit a new regression model, using the squared residuals as the response values.
<b>4. </b>Calculate the Chi-Square test statistic X<sup>2</sup> as n*R<sup>2</sup><sub>new</sub> where:
<b>n:</b> The total number of  observations 
<b>R<sup>2</sup><sub>new</sub>:</b> The R-squared of the new regression model that used the squared residuals as the response values
If the p-value that corresponds to this Chi-Square test statistic with <em>p</em> (the number of predictors) degrees of freedom is less than some significance level (i.e. α = .05) then reject the null hypothesis and conclude that heteroscedasticity is present.
Otherwise, fail to reject the null hypothesis. In this case, it’s assumed that homoscedasticity is present.
Note that most statistical software can easily perform the Breusch-Pagan test so you will likely never have to perform these steps by hand, but it’s useful to know what’s going on behind the scenes.
<h3>An Example of the Breusch-Pagan Test</h3>
Suppose we have the following dataset that contains information for 10 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/bp1.png">
Using statistical software, we fit the following  multiple linear regression model :
rating = 62.47 + 1.12*(points) + 0.88*(assists) – 0.43*(rebounds)
We then use this model to make predictions for the rating of each player and calculated the squared residuals (i.e. the squared difference between the predicted rating and the actual rating):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/bp2.png">
Next, we fit a new regression model using the squared residuals as the response values and the original predictor variables as the predictor variables once again. We find the following:
<b>n:</b> 10
<b>R<sup>2</sup><sub>new</sub>:</b> 0.600395
Thus, our Chi-Square test statistic for the Breusch-Pagan test is n*R<sup>2</sup><sub>new</sub> = 10*.600395 = <b>6.00395</b>. The degrees of freedom is <em>p</em> = 3 predictor variables.
According to the  Chi-Square to P-Value Calculator , the p-value that corresponds to X<sup>2</sup> = 6.00395 with 3 degrees of freedom is <b>0.111418</b>.
Since this p-value is not less than .05, we fail to reject the null hypothesis. Thus, we assume that homoscedasticity is present.
<h3>The Breusch-Pagan Test in Practice</h3>
The following tutorials provide step-by-step examples of how to perform the Breusch-Pagan test in different statistical programs:
 How to Perform a Breusch-Pagan Test in Excel 
 How to Perform a Breusch-Pagan Test in R 
 How to Perform a Breusch-Pagan Test in Python 
 How to Perform a Breusch-Pagan Test in Stata 
<h2><span class="orange">What is a Brier Score?</span></h2>
A <b>Brier Score </b>is a metric we use in statistics to measure the accuracy of probabilistic forecasts. It is typically used when the outcome of a forecast is binary – either the outcome occurs or it does not occur.
For example, suppose a weather forecast says there is a 90% chance of rain and it actually does rain. We can calculate the Brier Score for this forecast using the following formula:
<b>Brier Score</b> = (f – o)<sup>2</sup>
where:
f = forecasted probability
o = outcome (1 if the event occurs, 0 if it doesn’t occur)
In this example, the Brier Score for our forecast would be (0.9 – 1)<sup>2</sup> = -0.1<sup>2</sup> = <b>0.01</b>
A Brier Score for a set of forecasts is simply calculated as the average of the Brier Scores for the individual forecasts:
<b>Brier Score </b>= 1/n * Σ(f<sub>t</sub> – o<sub>t</sub>)<sup>2</sup>
where:
n = sample size (the number of forecasts)
Σ = a fancy symbol that means “sum”
f<sub>t</sub> = forecasted probability at event <em>t</em>
o = outcome at event <em>t</em> (1 if the event occurs, 0 if it doesn’t occur)
A Brier Score can take on any value between 0 and 1, with 0 being the best score achievable and 1 being the worst score achievable. The lower the Brier Score, the more accurate the prediction(s).
<h2>Examples of Calculating Brier Scores</h2>
The following examples illustrate how to calculate Brier Scores.
<b>Example 1: A forecast says there is a 0% chance of rain and it does rain.</b>
Brier Score = (0 – 1)<sup>2</sup> = 1
<b>Example 2: A forecast says there is a 100% chance of rain and it does rain.</b>
Brier Score = (1 – 1)<sup>2</sup> = 0
<b>Example 3: A forecast says there is a 27% chance of rain and it does rain.</b>
Brier Score = (.27 – 1)<sup>2</sup> = 0.5329
<b>Example 4: A forecast says there is a 97% chance of rain and it does not rain.</b>
Brier Score = (.97 – 0)<sup>2</sup> = 0.9409
<b>Example 5: A weather forecaster makes the following predictions:</b>
<table><tbody>
<tr>
<th style="text-align: center;"><b>Chance of Rain</b></th>
<th style="text-align: center;"><b>Outcome</b></th>
</tr>
<tr>
<td style="text-align: center;">27%</td>
<td style="text-align: center;">Rain</td>
</tr>
<tr>
<td style="text-align: center;">67%</td>
<td style="text-align: center;">Rain</td>
</tr>
<tr>
<td style="text-align: center;">83%</td>
<td style="text-align: center;">No Rain</td>
</tr>
<tr>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">Rain</td>
</tr>
</tbody></table>
We can calculate the Brier Score for this set of predictions using the following formulas:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Chance of Rain</b></th>
<th style="text-align: center;"><b>Outcome</b></th>
<th style="text-align: center;"><b>Brier Score</b></th>
</tr>
<tr>
<td style="text-align: center;">27%</td>
<td style="text-align: center;">Rain</td>
<td style="text-align: center;">(.27-1)<sup>2</sup> = .5329</td>
</tr>
<tr>
<td style="text-align: center;">67%</td>
<td style="text-align: center;">Rain</td>
<td style="text-align: center;">(.67-1)<sup>2</sup> = .1089</td>
</tr>
<tr>
<td style="text-align: center;">83%</td>
<td style="text-align: center;">No Rain</td>
<td style="text-align: center;">(.83-0)<sup>2</sup> = .6889</td>
</tr>
<tr>
<td style="text-align: center;">90%</td>
<td style="text-align: center;">Rain</td>
<td style="text-align: center;">(.90-1)<sup>2</sup> = .01</td>
</tr>
</tbody></table>
Brier Score = (.5329 + .1089 + .6889 + .01) / 4 = <b>0.3352</b>.
<h2>Brier Skill Scores</h2>
A <b>Brier Skill Score </b>is a metric that tells us how well the Brier Score of a new forecasting model compares to an existing forecasting model. It is calculated as:
<b>Brier Skill Score</b> = (BS<sub>E</sub> – BS<sub>N</sub>) / BS<sub>E</sub>
where:
BS<sub>E</sub> = Brier Score of existing model
BS<sub>N</sub> = Brier Score of new model
If a Brier Skill Score is positive, then the new model makes more accurate predictions. If the Brier Skill Score is negative, then the new model makes worse predictions. And if the Brier Skill Score is equal to zero, then the new model offers no improvement over the existing model.
For example, suppose our existing model has a Brier Score of BS<sub>E</sub> = 0.4221 and our new model has a Brier Score of BS<sub>N</sub> = 0.3352. The Brier Skill Score of our new model can be calculated as:
<b>Brier Skill Score </b> = (0.4421 – 0.3352) / (0.4421) = <b>0.2418</b>.
Since this number is positive, it’s an indication that our new model provides more accurate forecasts relative to the existing model.
The higher the Brier Skill Score, the bigger the improvement is in the new model compared to the existing model.
<h2><span class="orange">How to Perform a Brown–Forsythe Test in Python</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a significant difference between the means of three or more independent groups. 
One of the  assumptions  of a one-way ANOVA is that the variances of the populations that the  samples  come from are equal.
One of the most common ways to test for this is by using a <b>Brown-Forsythe test</b>, which is a statistical test that uses the following  hypotheses :
<b>H<sub>0</sub>:</b> The variances among the populations are equal.
<b>H<sub>A</sub></b>: The variances among the populations are not equal.
If the  p-value  of the test is less than some significance level (e.g. α = .05) then we reject the null hypothesis and conclude that the variances are not equal among the different populations.
This tutorial provides a step-by-step example of how to perform a Brown-Forsythe test in Python.
<h3>Step 1: Enter the Data</h3>
Suppose researchers want to know if three different fertilizers lead to different levels of plant growth. 
They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
The following arrays show the height of plants in each of the three groups:
<b>group1 = [7, 14, 14, 13, 12, 9, 6, 14, 12, 8]
group2 = [15, 17, 13, 15, 15, 13, 9, 12, 10, 8]
group3 = [6, 8, 8, 9, 5, 14, 13, 8, 10, 9]
</b>
<h3>Step 2: Summarize the Data</h3>
Before we perform a Brown-Forsythe test, we can calculate the variance of the plant measurements in each group:
<b>#import numpy
import numpy as np
#calculate variance of plant measurements in each group
print(np.var(group1), np.var(group2), np.var(group3))
8.69 7.81 7.0</b>
We can see that the variances between the groups differ, but to determine if these differences are  statistically significant  we can perform the Brown-Forsythe test.
<h3>Step 3: Perform the Brown-Forsythe Test</h3>
To perform a Brown-Forsythe test in Python, we can use the  scipy.stats.levene()  function and specify the center to be <b>median</b>:
<b>import scipy.stats as stats
stats.levene(group1, group2, group3, center='median')
LeveneResult(statistic=0.17981072555205047, pvalue=0.8364205218185946)
</b>
From the output we can observe the following:
Test statistic: <b>0.1798</b>
p-value: <b>0.8364</b>
The p-value of the test turns out to be greater than .05, so we fail to reject the null hypothesis of the test.
The differences in the variances between the groups is not statistically significant.
<h3>Next Steps</h3>
If we fail to reject the null hypothesis of the Brown-Forsythe Test, then we can proceed to perform a one-way ANOVA on the data.
However, if we reject the null hypothesis then this indicates that the assumption of equal variances is violated. In this case, we have two options:
<b>1. Proceed with a One-Way ANOVA anyway.</b>
It turns out that a one-way ANOVA is actually robust to unequal variances as long as the largest variance is no larger than 4 times the smallest variance.
In step 2 from the example above, we found that the smallest variance was 7.0 and the largest variance was 8.69. Thus, the ratio of the largest to smallest variance is 8.69 / 7.0 = <b>1.24</b>.
Since this value is less than 4, we could simply proceed with the one-way ANOVA even if the Brown-Forsythe test indicated that the variances were not equal.
<b>2. Perform a Kruskal-Wallis Test</b>
If the ratio of the largest variance to the smallest variance is greater than 4, we may instead choose to perform a  Kruskal-Wallis test . This is considered the non-parametric equivalent to the one-way ANOVA.
You can find a step-by-step example of a Kruskal-Wallis test in Python  here .
<h2><span class="orange">Brown–Forsythe Test in R: Step-by-Step Example</span></h2>
A  one-way ANOVA  is used to determine whether or not there is a significant difference between the means of three or more independent groups. 
One of the  assumptions  of a one-way ANOVA is that the variances of the populations that the  samples  come from are equal.
One of the most common ways to test for this is by using a <b>Brown-Forsythe test</b>, which is a statistical test that uses the following  hypotheses :
<b>H<sub>0</sub>:</b> The variances among the populations are equal.
<b>H<sub>A</sub></b>: The variances among the populations are not equal.
If the  p-value  of the test is less than some significance level (e.g. α = .05) then we reject the null hypothesis and conclude that the variances are not equal among the different populations.
This tutorial provides a step-by-step example of how to perform a Brown-Forsythe test in R.
<h3>Step 1: Enter the Data</h3>
Suppose we’d like to know whether or not three different workout programs lead to different levels of weight loss.
To test this, we recruit 90 people and randomly assign 30 to use each program. We then measure the weight loss of each person after one month.
The following dataset contains information on how much weight people lost on each program:
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = as.factor(rep(c("A", "B", "C"), each = 30)),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#view first six rows of data frame
head(data)
#  program weight_loss
#1       A   2.6900916
#2       A   0.7965260
#3       A   1.1163717
#4       A   1.7185601
#5       A   2.7246234
#6       A   0.6050458</b>
<h3>Step 2: Summarize & Visualize the Data</h3>
Before we perform a Brown-Forsythe test, we can create boxplots to visualize the variance of weight loss for each group:
<b>boxplot(weight_loss ~ program, data = data)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/bftest1.png">
We can also calculate the variance of weight loss in each group:
<b>#load dplyr package
library(dplyr)
#calculate variance of weight loss by group
data %>%
  group_by(program) %>%
  summarize(var=var(weight_loss))
# A tibble: 3 x 2
  program   var
     
1 A       0.819
2 B       1.53 
3 C       2.46 </b>
We can see that the variances between the groups differ, but to determine if these differences are  statistically significant  we can perform the Brown-Forsythe test.
<h3>Step 3: Perform the Brown-Forsythe Test</h3>
To perform a Brown-Forsythe test in R, we can use the <b>bf.test()</b> function from the  onewaytests  package:
<b>#load onewaytests package
library(onewaytests)
#perform Brown-Forsythe test
bf.test(weight_loss ~ program, data = data)
  Brown-Forsythe Test (alpha = 0.05) 
------------------------------------------------------------- 
  data : weight_loss and program 
  statistic  : 30.83304 
  num df     : 2 
  denom df   : 74.0272 
  p.value    : 1.816529e-10 
  Result     : Difference is statistically significant. 
------------------------------------------------------------- </b>
The p-value of the test turns out to be less than 0.000 and, as the output declares, the differences in variances between the three groups is statistically significant.
<h3>Next Steps</h3>
If you fail to reject the null hypothesis of the Brown-Forsythe Test, then you can proceed to perform a one-way ANOVA on the data.
However, if you reject the null hypothesis then this means the assumption of equal variances is violated. In this case, you have two options:
<b>1. Proceed with a One-Way ANOVA anyway.</b>
It turns out that a one-way ANOVA is actually robust to unequal variances as long as the largest variance is no larger than 4 times the smallest variance.
In step 2 from the example above, we found that the smallest variance was 0.819 and the largest variance was 2.46. Thus, the ratio of the largest to smallest variance is 2.46 / .819 = <b>3.003</b>.
Since this value is less than 4, we could simply proceed with the one-way ANOVA.
<b>2. Perform a Kruskal-Wallis Test</b>
If the ratio of the largest variance to the smallest variance is greater than 4, we may instead choose to perform a  Kruskal-Wallis test . This is considered the non-parametric equivalent to the one-way ANOVA.
You can find a step-by-step example of a Kruskal-Wallis test in R  here .

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
