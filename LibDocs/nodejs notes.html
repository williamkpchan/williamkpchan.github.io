<base target="_blank"><html><head><title>nodejs notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "nodejs notes"
  var markerName = "h2"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:24px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>nodejs notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<a href="Zombie.js.html" class="whitebut ">Zombie.js</a>
<a href="Puppeteer.html" class="whitebut ">Puppeteer</a>
<a href="https://nodejs.org/en/knowledge/" class="whitebut ">nodejs Knowledge Base</a>


</div>
<pre><br><br>
<a href="https://blog.bitsrc.io/a-beginners-guide-to-server-side-web-development-with-node-js-17385da09f93" class="whitebut ">Beginner’s Guide to Server-Side Web Development with Node.js</a>
<a href="https://stackoverflow.com/questions/30028127/what-is-the-best-way-to-communicate-between-two-servers/30038822" class="whitebut ">nodejs communicate between two servers</a>

<h2>nodejs tutorials</h2>
<a href="nodejs.html">&diams;nodejs</a>
<a href="Node入门.html">&diams;Node入门</a>
<a href="NodejsSQL.html" class="gold embossts">&diams;NodejsSQL</a>
<a class="goldword" href="http://www.tutorialspoint.com/nodejs/">Node.js Tutorial</a>
<a href="http://www.tutorialspoint.com/nodejs_terminal_online.php">Online Node Terminal</a>

<a href="Node命令行程序.html"><u class="redb">&diams;Node命令行程序</u></a>

<a href="nodejsList.html"><u class="redb">&diams;nodejsList</u></a>
<a href="https://medium.com/@kieranmaher13/nodejs-in-three-ish-minutes-4c4401b43b2c"><span class="redinsha">nodejs in 3 minutes</span></a>
<a href="http://www.youtube.com/watch?v=U8XF6AFGqlc">Node.js Tutorial For Absolute Beginners</a>
<a href="Getting Started with Node.js.html"><b class="redword">&diams;Getting Started with Node.js</b></a>

<a href="https://packagecontrol.io/packages/Nodejs"><span class="whiteOnblacksha">Sublime Text Nodejs</span></a>

<a href="nodejs_mongodb.html"><span class="orangeb">&diams;nodejs_mongodb</span></a>
Usage
node [options] [V8 options] [script.js | -e "script" | - ] [arguments]
node [options] [V8 options] [script.js | -e "script" | -] [--] [arguments]
node inspect [script.js | -e "script" | &lt;host>:&lt;port>] …
node --v8-options

Next, create a new source file in the projects folder and call it hello-world.js.

Open hello-world.js and paste in the following content:

const http = require('http');
const hostname = '127.0.0.1';
const port = 3000;
const server = http.createServer((req, res) => {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello, World!\n');
});

server.listen(port, hostname, () => {
  console.log(`Server running at http://${hostname}:${port}/`);
});

Save the file, go back to the terminal window, and enter the following command:

$ node hello-world.js

Now, open any preferred web browser and visit http://127.0.0.1:3000.

<h2><span class="orange">XMLHttpRequest</span></h2>
Example Using the XMLHttpRequest Object
function loadXMLDoc() {
  var xhttp = new XMLHttpRequest();
  xhttp.onreadystatechange = function() {
    if (this.readyState == 4 && this.status == 200) {
      document.getElementById("demo").innerHTML =
      this.responseText;
    }
  };
  xhttp.open("GET", "xmlhttp_info.txt", true);
  xhttp.send();
}

XMLHttpRequest is a built-in browser object that allows to make HTTP requests in JavaScript.

Despite of having the word “XML” in its name, it can operate on any data, not only in XML format. 
We can upload/download files, track progress and much more.

Right now, there’s another, more modern method <code>fetch</code>, that somewhat deprecates <code>XMLHttpRequest</code>.
<b>A Fetch API Example</b>
let file = "fetch_info.txt"
fetch (file)
.then(x => x.text())
.then(y => document.getElementById("demo").innerHTML = y);

Since Fetch is based on async and await, the example above might be easier to understand like this:

getText("fetch_info.txt");
async function getText(file) {
  let x = await fetch(file);
  let y = await x.text();
  document.getElementById("demo").innerHTML = y;
}
In modern web-development <code>XMLHttpRequest</code> is used for three reasons:
Historical reasons: we need to support existing scripts with <code>XMLHttpRequest</code>.
We need to support old browsers, and don’t want polyfills (e.g. to keep scripts tiny).
We need something that <code>fetch</code> can’t do yet, e.g. to track upload progress.

Does that sound familiar? If yes, then all right, go on with <code>XMLHttpRequest</code>. 
Otherwise, please head on to <a href="/fetch">Fetch</a>.

<h2>The basics</h2>XMLHttpRequest has two modes of operation: synchronous and asynchronous.

Let’s see the asynchronous first, as it’s used in the majority of cases.

To do the request, we need 3 steps:

Create <code>XMLHttpRequest</code>:

<code>let xhr = new XMLHttpRequest();</code>

The constructor has no arguments.

Initialize it, usually right after <code>new XMLHttpRequest</code>:

<code>xhr.open(method, URL, [async, user, password])</code>

This method specifies the main parameters of the request:

<code>method</code> – HTTP-method. 
Usually <code>"GET"</code> or <code>"POST"</code>.
<code>URL</code> – the URL to request, a string, can be <a href="/url">URL</a> object.
<code>async</code> – if explicitly set to <code>false</code>, then the request is synchronous, we’ll cover that a bit later.
<code>user</code>, <code>password</code> – login and password for basic HTTP auth (if required).

Please note that <code>open</code> call, contrary to its name, does not open the connection. 
It only configures the request, but the network activity only starts with the call of <code>send</code>.

Send it out.

<code>xhr.send([body])</code>

This method opens the connection and sends the request to server. 
The optional <code>body</code> parameter contains the request body.

Some request methods like <code>GET</code> do not have a body. 
And some of them like <code>POST</code> use <code>body</code> to send the data to the server. 
We’ll see examples of that later.

Listen to <code>xhr</code> events for response.

These three events are the most widely used:

<code>load</code> – when the request is complete (even if HTTP status is like 400 or 500), and the response is fully downloaded.
<code>error</code> – when the request couldn’t be made, e.g. network down or invalid URL.
<code>progress</code> – triggers periodically while the response is being downloaded, reports how much has been downloaded.

<code>xhr.onload = function() {
  alert(`Loaded: ${xhr.status} ${xhr.response}`);
};

xhr.onerror = function() {
  // only triggers if the request couldn't be made at all
  alert(`Network Error`);
};

xhr.onprogress = function(event) {
  // triggers periodically
 // event.loaded - how many bytes downloaded
 // event.lengthComputable = true if the server sent Content-Length header
 // event.total - total number of bytes (if lengthComputable)
 alert(`Received ${event.loaded} of ${event.total}`);
};</code>

Here’s a full example. 
The code below loads the URL at <code>/article/xmlhttprequest/example/load</code> from the server and prints the progress:

<code>// 1. Create a new XMLHttpRequest object
let xhr = new XMLHttpRequest();

// 2. Configure it: GET-request for the URL /article/.../load
xhr.open('GET', '/article/xmlhttprequest/example/load');

// 3. Send the request over the network
xhr.send();

// 4. This will be called after the response is received
xhr.onload = function() {
  if (xhr.status != 200) {
    // analyze HTTP status of the response
    alert(`Error ${xhr.status}: ${xhr.statusText}`); // e.g. 404: Not Found
  } else { // show the result
    alert(`Done, got ${xhr.response.length} bytes`); // response is the server response
  }
};

xhr.onprogress = function(event) {
  if (event.lengthComputable) {
    alert(`Received ${event.loaded} of ${event.total} bytes`);
  } else {
    alert(`Received ${event.loaded} bytes`); // no Content-Length
  }
};

xhr.onerror = function() {
  alert("Request failed");
};</code>

Once the server has responded, we can receive the result in the following <code>xhr</code> properties:

<dt><code>status</code></dt>
<dd>HTTP status code (a number): <code>200</code>, <code>404</code>, <code>403</code> and so on, can be <code>0</code> in case of a non-HTTP failure.</dd>
<dt><code>statusText</code></dt>
<dd>HTTP status message (a string): usually <code>OK</code> for <code>200</code>, <code>Not Found</code> for <code>404</code>, <code>Forbidden</code> for <code>403</code> and so on.</dd>
<dt><code>response</code> (old scripts may use <code>responseText</code>)</dt>
<dd>The server response body.</dd>

We can also specify a timeout using the corresponding property:

<code>xhr.timeout = 10000; // timeout in ms, 10 seconds</code>

If the request does not succeed within the given time, it gets canceled and <code>timeout</code> event triggers.

URL search parameters
To add parameters to URL, like <code>?name=value</code>, and ensure the proper encoding, we can use <a href="/url">URL</a> object:

<code>let url = new URL('https://google.com/search');
url.searchParams.set('q', 'test me!');

// the parameter 'q' is encoded
xhr.open('GET', url); // https://google.com/search?q=test+me%21</code>

<h2>Response Type</h2>We can use <code>xhr.responseType</code> property to set the response format:

<code>"</code> (default) – get as string,
<code>"text"</code> – get as string,
<code>"arraybuffer"</code> – get as <code>ArrayBuffer</code> (for binary data, see chapter <a href="/arraybuffer-binary-arrays">ArrayBuffer, binary arrays</a>),
<code>"blob"</code> – get as <code>Blob</code> (for binary data, see chapter <a href="/blob">Blob</a>),
<code>"document"</code> – get as XML document (can use XPath and other XML methods) or HTML document (based on the MIME type of the received data),
<code>"json"</code> – get as JSON (parsed automatically).

For example, let’s get the response as JSON:
<code>let xhr = new XMLHttpRequest();

xhr.open('GET', '/article/xmlhttprequest/example/json');
xhr.responseType = 'json';
xhr.send();
// the response is {"message": "Hello, world!"}
xhr.onload = function() {
  let responseObj = xhr.response;
  alert(responseObj.message); // Hello, world!
};</code>

Please note:
In the old scripts you may also find <code>xhr.responseText</code> and even <code>xhr.responseXML</code> properties.

They exist for historical reasons, to get either a string or XML document. 
Nowadays, we should set the format in <code>xhr.responseType</code> and get <code>xhr.response</code> as demonstrated above.

<h2>Ready states</h2><code>XMLHttpRequest</code> changes between states as it progresses. 
The current state is accessible as  <code>xhr.readyState</code>.

All states, as in <a href="https://xhr.spec.whatwg.org/#states">the specification</a>:

<code>UNSENT = 0; // initial state
OPENED = 1; // open called
HEADERS_RECEIVED = 2; // response headers received
LOADING = 3; // response is loading (a data packet is received)
DONE = 4; // request complete</code>

An <code>XMLHttpRequest</code> object travels them in the order <code>0</code> → <code>1</code> → <code>2</code> → <code>3</code> → … → <code>3</code> → <code>4</code>. 
State <code>3</code> repeats every time a data packet is received over the network.

We can track them using <code>readystatechange</code> event:

<code>xhr.onreadystatechange = function() {
  if (xhr.readyState == 3) {
    // loading
  }
  if (xhr.readyState == 4) {
    // request finished
  }
};</code>

You can find <code>readystatechange</code> listeners in really old code, it’s there for historical reasons, as there was a time when there were no <code>load</code> and other events. 
Nowadays, <code>load/error/progress</code> handlers deprecate it.

<h2>Aborting request</h2>We can terminate the request at any time. 
The call to <code>xhr.abort()</code> does that:

<code>xhr.abort(); // terminate the request</code>

That triggers <code>abort</code> event, and <code>xhr.status</code> becomes <code>0</code>.

<h2>Synchronous requests</h2>If in the <code>open</code> method the third parameter <code>async</code> is set to <code>false</code>, the request is made synchronously.

In other words, JavaScript execution pauses at <code>send()</code> and resumes when the response is received. 
Somewhat like <code>alert</code> or <code>prompt</code> commands.

Here’s the rewritten example, the 3rd parameter of <code>open</code> is <code>false</code>:

<code>let xhr = new XMLHttpRequest();

xhr.open('GET', '/article/xmlhttprequest/hello.txt', false);

try {
xhr.send();
if (xhr.status != 200) {
alert(`Error ${xhr.status}: ${xhr.statusText}`);
} else {
alert(xhr.response);
}
} catch(err) { // instead of onerror
alert("Request failed");
}</code>

It might look good, but synchronous calls are used rarely, because they block in-page JavaScript till the loading is complete. 
In some browsers it becomes impossible to scroll. 
If a synchronous call takes too much time, the browser may suggest to close the “hanging” webpage.

Many advanced capabilities of <code>XMLHttpRequest</code>, like requesting from another domain or specifying a timeout, are unavailable for synchronous requests. 
Also, as you can see, no progress indication.

Because of all that, synchronous requests are used very sparingly, almost never. 
We won’t talk about them any more.

<h2>HTTP-headers</h2><code>XMLHttpRequest</code> allows both to send custom headers and read headers from the response.

There are 3 methods for HTTP-headers:

<dl>
<dt><code>setRequestHeader(name, value)</code></dt>
<dd>
Sets the request header with the given <code>name</code> and <code>value</code>.

For instance:

<code>xhr.setRequestHeader('Content-Type', 'application/json');</code>

Headers limitations
Several headers are managed exclusively by the browser, e.g. <code>Referer</code> and <code>Host</code>.
The full list is <a href="https://xhr.spec.whatwg.org/#the-setrequestheader()-method">in the specification</a>.

<code>XMLHttpRequest</code> is not allowed to change them, for the sake of user safety and correctness of the request.

Can’t remove a header
Another peculiarity of <code>XMLHttpRequest</code> is that one can’t undo <code>setRequestHeader</code>.

Once the header is set, it’s set. 
Additional calls add information to the header, don’t overwrite it.

For instance:

<code>xhr.setRequestHeader('X-Auth', '123');
xhr.setRequestHeader('X-Auth', '456');

// the header will be:
// X-Auth: 123, 456</code>

</dd>
<dt><code>getResponseHeader(name)</code></dt>
<dd>
Gets the response header with the given <code>name</code> (except <code>Set-Cookie</code> and <code>Set-Cookie2</code>).

For instance:

<code>xhr.getResponseHeader('Content-Type')</code>

</dd>
<dt><code>getAllResponseHeaders()</code></dt>
<dd>
Returns all response headers, except <code>Set-Cookie</code> and <code>Set-Cookie2</code>.

Headers are returned as a single line, e.g.:

<code>Cache-Control: max-age=31536000
Content-Length: 4260
Content-Type: image/png
Date: Sat, 08 Sep 2012 16:53:16 GMT</code>

The line break between headers is always <code>"\r\n"</code> (doesn’t depend on OS), so we can easily split it into individual headers. 
The separator between the name and the value is always a colon followed by a space <code>": "</code>. 
That’s fixed in the specification.

So, if we want to get an object with name/value pairs, we need to throw in a bit JS.

Like this (assuming that if two headers have the same name, then the latter one overwrites the former one):

<code>let headers = xhr
.getAllResponseHeaders()
.split('\r\n')
.reduce((result, current) =&gt; {
let [name, value] = current.split(': ');
result[name] = value;
return result;
}, {});

// headers['Content-Type'] = 'image/png'</code>

</dd>
</dl>
<h2>POST, FormData</h2>To make a POST request, we can use the built-in <a href="https://developer.mozilla.org/en-US/docs/Web/API/FormData">FormData</a> object.

The syntax:

<code>let formData = new FormData([form]); // creates an object, optionally fill from &lt;form&gt;
formData.append(name, value); // appends a field</code>

We create it, optionally fill from a form, <code>append</code> more fields if needed, and then:

<code>xhr.open('POST', ...)</code> – use <code>POST</code> method.
<code>xhr.send(formData)</code> to submit the form to the server.

For instance:





<code>&lt;form name="person"&gt;
&lt;input name="name" value="John"&gt;
&lt;input name="surname" value="Smith"&gt;
&lt;/form&gt;

&lt;script&gt;
// pre-fill FormData from the form
let formData = new FormData(document.forms.person);

// add one more field
formData.append("middle", "Lee");

// send it out
let xhr = new XMLHttpRequest();
xhr.open("POST", "/article/xmlhttprequest/post/user");
xhr.send(formData);

xhr.onload = () =&gt; alert(xhr.response);
&lt;/script&gt;</code>

The form is sent with <code>multipart/form-data</code> encoding.

Or, if we like JSON more, then <code>JSON.stringify</code> and send as a string.

Just don’t forget to set the header <code>Content-Type: application/json</code>, many server-side frameworks automatically decode JSON with it:

<code>let xhr = new XMLHttpRequest();

let json = JSON.stringify({
name: "John",
surname: "Smith"
});

xhr.open("POST", '/submit')
xhr.setRequestHeader('Content-type', 'application/json; charset=utf-8');

xhr.send(json);</code>

The <code>.send(body)</code> method is pretty omnivore. 
It can send almost any <code>body</code>, including <code>Blob</code> and <code>BufferSource</code> objects.

<h2>Upload progress</h2>The <code>progress</code> event triggers only on the downloading stage.

That is: if we <code>POST</code> something, <code>XMLHttpRequest</code> first uploads our data (the request body), then downloads the response.

If we’re uploading something big, then we’re surely more interested in tracking the upload progress. 
But <code>xhr.onprogress</code> doesn’t help here.

There’s another object, without methods, exclusively to track upload events: <code>xhr.upload</code>.

It generates events, similar to <code>xhr</code>, but <code>xhr.upload</code> triggers them solely on uploading:

<code>loadstart</code> – upload started.
<code>progress</code> – triggers periodically during the upload.
<code>abort</code> – upload aborted.
<code>error</code> – non-HTTP error.
<code>load</code> – upload finished successfully.
<code>timeout</code> – upload timed out (if <code>timeout</code> property is set).
<code>loadend</code> – upload finished with either success or error.

Example of handlers:

<code>xhr.upload.onprogress = function(event) {
alert(`Uploaded ${event.loaded} of ${event.total} bytes`);
};

xhr.upload.onload = function() {
alert(`Upload finished successfully.`);
};

xhr.upload.onerror = function() {
alert(`Error during the upload: ${xhr.status}`);
};</code>

Here’s a real-life example: file upload with progress indication:





<code>&lt;input type="file" onchange="upload(this.files[0])"&gt;

&lt;script&gt;
function upload(file) {
let xhr = new XMLHttpRequest();

// track upload progress
xhr.upload.onprogress = function(event) {
console.log(`Uploaded ${event.loaded} of ${event.total}`);
};

// track completion: both successful or not
xhr.onloadend = function() {
if (xhr.status == 200) {
console.log("success");
} else {
console.log("error " + this.status);
}
};

xhr.open("POST", "/article/xmlhttprequest/post/upload");
xhr.send(file);
}
&lt;/script&gt;</code>

<h2>Cross-origin requests</h2>
<a href="https://www.npmjs.com/package/cors" class="whitebut ">node.js package CORS</a>

<code>XMLHttpRequest</code> can make cross-origin requests, using the same CORS policy as <a href="/fetch-crossorigin">fetch</a>.

Just like <code>fetch</code>, it doesn’t send cookies and HTTP-authorization to another origin by default. 
To enable them, set <code>xhr.withCredentials</code> to <code>true</code>:

<code>let xhr = new XMLHttpRequest();
xhr.withCredentials = true;

xhr.open('POST', 'http://anywhere.com/request');
...</code>

See the chapter <a href="/fetch-crossorigin">Fetch: Cross-Origin Requests</a> for details about cross-origin headers.

<h2>Summary</h2>Typical code of the GET-request with <code>XMLHttpRequest</code>:

<code>let xhr = new XMLHttpRequest();

xhr.open('GET', '/my/url');

xhr.send();

xhr.onload = function() {
if (xhr.status != 200) { // HTTP error?
// handle error
alert( 'Error: ' + xhr.status);
return;
}

// get the response from xhr.response
};

xhr.onprogress = function(event) {
// report progress
alert(`Loaded ${event.loaded} of ${event.total}`);
};

xhr.onerror = function() {
// handle non-HTTP error (e.g. network down)
};</code>

There are actually more events, the <a href="https://xhr.spec.whatwg.org/#events">modern specification</a> lists them (in the lifecycle order):

<code>loadstart</code> – the request has started.
<code>progress</code> – a data packet of the response has arrived, the whole response body at the moment is in <code>response</code>.
<code>abort</code> – the request was canceled by the call <code>xhr.abort()</code>.
<code>error</code> – connection error has occurred, e.g. wrong domain name. 
Doesn’t happen for HTTP-errors like 404.
<code>load</code> – the request has finished successfully.
<code>timeout</code> – the request was canceled due to timeout (only happens if it was set).
<code>loadend</code> – triggers after <code>load</code>, <code>error</code>, <code>timeout</code> or <code>abort</code>.

The <code>error</code>, <code>abort</code>, <code>timeout</code>, and <code>load</code> events are mutually exclusive. 
Only one of them may happen.

The most used events are load completion (<code>load</code>), load failure (<code>error</code>), or we can use a single <code>loadend</code> handler and check the properties of the request object <code>xhr</code> to see what happened.

We’ve already seen another event: <code>readystatechange</code>. 
Historically, it appeared long ago, before the specification settled. 
Nowadays, there’s no need to use it, we can replace it with newer events, but it can often be found in older scripts.

If we need to track uploading specifically, then we should listen to same events on <code>xhr.upload</code> object.

<h2><span class="orange">Chat with WebSockets, Server and Client both in NodeJs</span></h2>
Usually when somebody thinks chat, they think WebSockets. 
Normally there are tutorials how you make a server in NodeJs for example and a client in HTML. 
But in this case the client will also be a NodeJs app. 
It will display messages from the server and listen to users input with NodeJs <code>readline</code>.

<h2>Extra</h2>
There will be no persistence here. 
A client wil<span>l</span> see only messages that server sends him from the point he connects to it. 
Also whenever a client is connected, the server will generate a random color and a random name, and the client will use that. 
It’s not necessary, it’s just a few extra stuff.

Utils — this file contains the generate random color and generate random name. 
You can check the contents of that file in full on Github. 
Link at the end of story.
<h2>Server</h2>
<code>myClient</code> is just a class called Client (not the actual client we are gonna define that connects to the server). 
It contains 2 fields, name &amp; color.

<code>handleReceviedMsg</code> accepts the message that was send to server, and returns that it’s JSON stringified.

For WebSockets we are using the ws package. 
And chalk is used to make the console log a little colourful.

First we create the WebSocketServer, running on port 8080, with option for client tracking. 
This tracking enables that the WebSocket server has clients property.

When the server detects a connection, we create a client instance. 
It’s just a placeholder for name and color. 
Then we send the init or welcome message to the client, with that information.

Whenever a new message is received, we will send that message to every client back. 
Even to the one who has send it. 
So they know that the message was received and transmitted.
<h2>Client</h2>
The client connects to the WebSocket server. 
On initial or welcome message, the server will return the name and the color. 
The client will use that to display message in color with the help of chalk.

For client input we use nodes readline. 
Readline listents on the line event. 
This event is triggered whenever the client presses the Enter key. 
On that event we capture the text that was inputed and together with the clients name and color we send that to the server.

Everything else should be pretty much self explanatory.

On open is when we have a connection established. 
On close is when the connection gets terminated. 
On message is whenever server send us a message.

<img class="lazy" data-src="https://miro.medium.com/max/972/1*cjeTJvXhzR1xiAgA3f-YmQ.png">
client.js

<img class="lazy" data-src="https://miro.medium.com/max/932/1*H7WqjR0xG_fjBWE99b9ZHQ.png">
server.js

Whole structure and code: <a href="https://github.com/zprima/wschat" rel="noopener nofollow">https://github.com/zprima/wschat</a>

<h2><span class="orange">Creating a chat with Node.js</span></h2>
https://itnext.io/creating-a-chat-with-node-js-from-the-scratch-707896d64593
<h2><span class="red goldbs borRad20 bordred1">About Socket.IO</span></h2>
<a href="socket.io.html" class="whitebut red goldts bluebs">socket.io</a>
This Node.js module brings a way to connect directly from the client browser to the application server. 
The library works through events, that is, the server or client will trigger events so that there are responses from one of the parties

In a way, let’s use two very basic methods, which are <code>emit</code> and <code>on</code>. 
One serves to make the emission of the event and the other to receive the response of it. 
Each side of the application will therefore have the Socket.IO library added.

In addition to allowing the direct exchange of messages between two devices, Socket.IO also allows the broadcast of messages, sending an event to all other connected users. 
The broadcast can be both from client and server.

When the user accesses the page, a socket is created with the server and it is through this socket that the exchange of messages between a client and a server takes place. 
This, in turn, can either issue an event to a single Socket or to all the sockets connected to it, what we call a message broadcast.

<h2>The project</h2>
Let’s create now a directory called <code>\ChatJs</code> and, inside of it, we will create a file called app.js, which will be the main file of our server. 
As a first part we will create a fairly simple server that will only present a successful message on the browser screen

Creating a single application.
var app = require('http').createServer(response);
app.listen(3000);
console.log("App running…");
function response(req, res) {
 res.writeHead(200);
 res.end("Hi, your server is working!");
}

The script creates an HTTP server (which will be listening on port 3000) which has as main method to be requested the <code>response()</code> function, which, in turn, has two parameters: <code>req</code> (request) and <code>res</code> (response). 
Into the function, we define a success code (200) and end it with a string warning that the server is ok.

Soon after, just run the following command, which will run our application at the prompt:
node app.js

Note that when you run this code at the prompt, the terminal presents the content of the <code>console.log</code> function warning that the application is running. 
However, it will not print any other lines, indicating that our application is currently running.

At this point, we have only our Node.js server running. 
If you access the browser at <a href="http://localhost:3000/">http://localhost:3000/</a> you’ll see the message we passed in the <code>end</code> method

Next, we will make our server present an HTML response that will be the main page of our chat. 
For this, we will have to load the <code>FileSystem</code> module, since we will navigate the project directory and open a file. 
So, let’s change our <code>app.js</code> just like we see at <i>Listing 4</i>. 
Before making the changes, go to the prompt and press <code>Ctrl + C</code> (or <code>command + C</code>) to stop our application on the server.

<i>Listing 4.</i> Introducing an HTML page.
var app = require('http').createServer(response);
var fs = require('fs');
app.listen(3000);
console.log("App running…");
function response(req, res) {
 fs.readFile(__dirname + '/index.html',
 function (err, data) {
 if (err) {
   res.writeHead(500);
   return res.end('Failed to load file index.html');
 } res.writeHead(200);
   res.end(data);
 });
}

After these changes we will again execute the command <code>node app.js</code> and, when accessing again the address <a href="http://localhost:3000/">http://localhost:3000/</a>, you’ll come across the message “<em>Error loading the index.html file”</em>, just because we don’t have an index.html file inside our project yet.

It is also important to remember that the server we created so far does not differentiate the path, ie you can put anything after <a href="http://localhost:3000/">http://localhost:3000/</a> and it will always respond in the same way because we have not implemented how it’d treat these paths. 
Soon, you can very well call up addresses like <a href="http://localhost:3000/chat">http://localhost:3000/chat</a>, <a href="http://localhost:3000/error">http://localhost:3000/error</a>, <a href="http://localhost:3000/potato">http://localhost:3000/potato</a>, etc.

Let’s create a simple interface for our chat. 
Create an <code>index.html</code> file inside the project root directory. 
In this file enter a code equal to that shown in <i>Listing 5</i>.

<i>Listing 5.</i> Chat HTML code.
&lt;!DOCTYPE html>
&lt;html>
&lt;head>
 &lt;title>ChatJS&lt;/title>
 &lt;link rel="stylesheet" type="text/css" href="/css/style.css" />
&lt;/head>
&lt;body>
 &lt;div id="history">&lt;/div>
 &lt;form id="chat">
 &lt;input type="text" id="msg_text" name="msg_text" />
 &lt;input type="submit" value="Send!" /> 
 &lt;/form>
&lt;/body>
&lt;/html>

Our index, for now, will only deals with a div called <code>history</code> that is where all the messages exchanged in the chat will be arranged. 
Then, we have soon after a form with a text box and the button of message sending. 
A very simple chat structure so far.

However, if you now try to access the address <a href="http://localhost:3000/">http://localhost:3000/</a> you will receive the same error message. 
This is because we do not restart our server application, then we go to the prompt again, press <code>Ctrl + C</code> and then reexecute the app.

As you may have noticed, we already left a link tag in the <code>&lt;head></code> of our application to load our CSS. 
Within the directory of our project create another directory called <code>css</code> and, inside it, the <code>style.css</code> file with the same content as shown in <i>Listing 6</i>.

<i>Listing 6.</i> style.css file.
html, body, input { font-family: Georgia, Tahoma, Arial, sans-serif; margin: 0; padding: 0;}
body { background: #302F31; padding: 10px;}

form { margin: 15px 0;}
form input[type='text'] { border: 2px solid #eb5424; border-radius: 5px; padding: 5px; width: 75%;}
form input[type='submit'] { background: #eb5424; border: none; border-radius: 5px; color: #FFF; cursor: pointer; font-weight: bold; padding: 7px 5px; width: 19%;}

#history { background: #FFF; border: 2px solid #eb5424; height: 550px;}


If we restart the application, the style is not yet applied to the index page. 
The reason is that our app.js only deals with a request path so far. 
To solve this we will change our app.js file so that it loads the files that are passed in the request URL, instead of placing each of the URLs manually. 
Let’s take a closer look at the changes listed in <i>Listing 7</i>.

<i>Listing 7.</i> Path changes in app.js.
var app = require('http').createServer(response);
var fs = require('fs');
app.listen(3000);
console.log("App running...");
function response(req, res) {
    var file = "";
    if (req.url == "/") {
        file = __dirname + '/index.html';
    } else {
        file = __dirname + req.url;
    }
    fs.readFile(file, function(err, data) {
        if (err) {
            res.writeHead(404);
            return res.end('Page or file not found');
        }
        res.writeHead(200);
        res.end(data);
    });
}

After restarting the app

<h2>Sending messages</h2>
We will now work on the messaging mechanism. 
Our application will work by communicating with the Node.js server through the client-side library of Socket.IO while jQuery takes place in the interaction with the page.

For this, we will change the app.js file as shown in <i>Listing 8</i>, and include a line of a command at the beginning of the file stating that we are including Socket.IO in the application.

<i>Listing 8.</i> Including Socket.IO module.
var app = require('http').createServer(response);
var fs = require('fs');
var io = require('socket.io')(app);
…

In order to use the <code>require</code> function in a module we need first to install it for our application. 
So, stop the application and run the following command to get this done:
npm install socket.io

Once finished, go to your index.html page and add the code snippet shown in <i>Listing 9</i>, at the end of the file.

<i>Listing 9.</i> Message sending event.
…
 &lt;script type="text/javascript" src="<a href="https://code.jquery.com/jquery-3.3.1.min.js">https://code.jquery.com/jquery-3.3.1.min.js</a>">&lt;/script>
 &lt;script type="text/javascript" src="/socket.io/socket.io.js">&lt;/script>
 &lt;script type="text/javascript">
 var socket = io.connect();
 $("form#chat").submit(function(e) {
   e.preventDefault();
 
   <span class="orange">socket.emit</span>("send message", $(this).find("#msg_text").val(), function() {
     $("form#chat #msg_text").val(");
   });
 });
 &lt;/script>
&lt;/body>
&lt;/html>

We are declaring a <code>socket</code> variable that refers to the Socket.IO library, which will be responsible for all socket functionalities. 
Next, we declare a <code>submit</code> event of our form in jQuery and pass a <code>preventDefault</code> so that the form does not proceed to its <code>action</code>, since we are the ones who are going to take care of the form response.

Note that the <code>emit</code> method of the library is invoked, in which we pass as parameters three things: the event name (this will be useful on the server), the data we are sending (in this case we are only sending the contents of the <code>message</code> field) and finally the <code>callback</code>, a function that will be executed once the event is issued. 
The latter, in particular, will only serve to clear the message field, so the user does not have to delete the message after sending it.

If we now test our application the message sending will not work, not even the callback to clear the message field, because we have not yet put the functionality of what the server have to do as soon as it receives this event. 
To do this, edit the app.js file and put the code shown in <i>Listing 11</i> at the end of it.

<i>Listing 11.</i> Receiving messages from the client.
io.on("connection", function(socket) {
    <span class="orange">socket.on</span>("send message", function(sent_msg, callback) {
        sent_msg = "repeat: " + sent_msg;
        <span class="orange">io.sockets.emit</span>("update messages", sent_msg);
        callback();
    });
});

We’ve created a method that will work in response to the client’s connection to the server. 
When the client accesses the page it triggers this method on the server and when this socket receives a <code>send message</code> we trigger a method that has as parameters the sent data (the <code>message</code> field) and the callback that we created on the client side.

Within this method we put the second part of the functionality: the module will send to the sockets connected to the server (all users) the <code>update messages</code> event and will also pass which new message was sent, with a specific datetime format. 
To provide the date and time we create a separate function because we will still use this method a few more times throughout the development. 
Right away, we call the callback that we created on the client side, which is the method for clearing the fields.

Finally, also edit the index.html file and create the method that will update the messages for the users. 
The idea is quite simple: let’s give an <code>append</code> in the <code>history</code> div (the changes are in <i>Listing 12</i>). 
The following lines should be entered shortly after submitting the form.

<i>Listing 12.</i> Updating message history.
<span class="orange">socket.on</span>("update messages", function(msg){
var final_message = $("&lt;p />").text(msg);
   $("#history").append(final_message);
});

Basically, the conversation between the server and the client is the same on both sides, that is, the two have an <code>emit</code> and <code>on</code> functions for issuing and receiving events, respectively.

So, restart and access the application in two tabs and just send a message to see the power of Socket.IO in action. 
The application should display the message
full source code <a href="https://github.com/diogosouza/ChatJS">here</a>.

<h2>multiple clients</h2>
<a href="https://stackoverflow.com/questions/35306822/nodejs-server-cant-handle-multiple-users" class="whitebut ">phantomJS headless NodeJS server handle multiple users </a>

There are multiple clients C1, C2, ..., Cn
Clients emit request to the server R1,...,Rn
Server receives request, does data processing
When data-processing is complete, Server emits response to clients Rs1, .., Rs2
When the server has finished data processing it emits the response in the following way:

// server listens for request from client
socket.on('request_from_client', function(data){
    // user data and request_type is stored in the data variable
    var user = data.user.id
    var action = data.action

    // server does data processing 
    do_some_action(..., function(rData){
        // when the processing is completed, the response data is emitted as a response_event
        // The problem is here, how to make sure that the response data goes to the right client
        socket.emit('response_to_client', rData)
    })
})

The instance of the socket object corresponds to a client connection. 
So every message you emit from that instance is send to the client that opened that socket connection. 
Remember that upon the connection event you get (through the onDone callback) the socket connection object. 
This event triggers everytime a client connects to the socket.io server.

If you want to send a message to all clients you can use
io.sockets.emit("message-to-all-clients")

and if you want to send an event to every client apart the one that emits the event socket.broadcast.emit("message-to-all-other-clients");

On every connection, a "new channel" is created.

<h2>multiple clients connecting to same server</h2>
Server-

  var dgram = require('dgram');
  var client = dgram.createSocket('udp4');

  /** @requires socket.io */
  var io = require('socket.io')(http);

  /** Array of clients created to keep track of who is listening to what*/
      var clients = [];

      io.sockets.on('connection', function(socket, username){

        /** printing out the client who joined */
        console.log('New client connected (id=' + socket.id + ').');

        /** pushing new client to client array*/
        clients.push(socket);

      /** listening for acknowledgement message */
      client.on('message', function( message, rinfo ){

        /** creating temp array to put data in */
        var temp = [];

        /**converting data bit to bytes */
        var number= req.body.size * 2

        /** acknowledgement message is converted to a string from buffer */
        var message = message.toString();

        /** cutting hex string to correspong to requested data size*/
        var data = message.substring(0, number);

        /** converting that data to decimal */
        var data = parseInt(data, 16);

        /** adding data to data array */
        temp[0] = data

        /** emitting message to html page */
        socket.emit('temp', temp);
      });

      /** listening if client has disconnected */
      socket.on('disconnect', function() {
          clients.splice(clients.indexOf(client), 1);
          console.log('client disconnected (id=' + socket.id + ').');
          clearInterval(loop);
      });
    });
  }
});

Client-
var socket = io.connect('192.168.0.136:3000');
  socket.on(temp', function(temp){
    var temp= temp.toString();
    var message= temp.split(',').join("<br>");
    $('#output').html('<output>' + message + '</output>');
  });

When a client connects, a random number called temp is emitted to the client. 
The above code works when one client connects to the server. 
Now how can you set a new connection each time? So that if one tab is opened, it gets its own random message back, while when another tab opens, it gets its own random message back.

You could send an id back to the client and save it to localStorage (or a variable or anywhere else).
Then have the client listen to a 'room' that is just for this client.
e.g.
   var uuidSocket = io(serverHost + "/" + uuid);
   uuidSocket.on("Info", (data:any)=> {
     // do something on data
   });
This id / room will be used by the server to inform the specific client. And so on.

Server side:

// you have your socket ready and inside the on('connect'...) you handle a register event where the client passes an id if one exists else you create one.

socket.on('register', function(clientUuid){ // a client requests registration
      var id = clientUuid == null? uuid.v4() : clientUuid; // create an id if client doesn't already have one
      var nsp;
      var ns = "/" + id;

      socket.join(id);
      var nsp = app.io.of(ns); // create a room using this id that is only for this client
      clientToRooms[ns] = nsp; // save it to a dictionary for future use

      // set up what to do on connection
      nsp.on('connection', function(nsSocket){
        console.log('someone connected');

        nsSocket.on('Info', function(data){
          // just an example
        });
      });
Client side:

// you already have declared uuid, uuidSocket and have connected to the socket previously so you define what to do on register:
    socket.on("register", function(data){
      if (uuid == undefined || uuidSocket == undefined) {// first time we get id from server
        //save id to a variable
        uuid = data.uuid;

        // save to localstorage for further usage (optional - only if you want one client per browser e.g.)
        localStorage.setItem('socketUUID', uuid);

        uuidSocket = io(serverHost + "/" + uuid); // set up the room --> will trigger nsp.on('connect',... ) on the server

        uuidSocket.on("Info", function(data){
          //handle on Info
        });

// initiate the register from the client
 socket.emit("register", uuid);

Broadcasting means sending a message to everyone else except for the socket that starts it.

Server:

var io = require('socket.io')(80);

io.on('connection', function (socket) {
  socket.broadcast.emit('user connected');
});

<h2><span class="orange">Communication</span></h2>
<a href="Complete Guide To Node Client-Server Communication.html"><span class="orangeb">&diams;Complete Guide To Node Client-Server Communication</span></a>

<a href="https://stackoverflow.com/questions/11115508/in-node-js-how-do-i-communicate-with-client-side-javascript">node js communicate with client side javascript</a>
<a href="https://flaviocopes.com/node-websockets/">Using WebSockets with Node.js</a>

The easiest way is to set up <a href="http://expressjs.com/" class="whitebut ">Express</a> and have your client side code communicate via Ajax (for example, using jQuery).

(function() {
  var app, express;
  express = require("express");
  app = express.createServer();

  app.configure(function() {
    app.use(express.bodyParser());
    return app.use(app.router);
  });

  app.configure("development", function() {
    return app.use(express.errorHandler({
      dumpExceptions: true,
      showStack: true
    }));
  });

  app.post("/locations", function(request, response) {
    var latitude, longitude;
    latitude = request.body.latitude;
    longitude = request.body.longitude;
    return response.json({}, 200);
  });
  app.listen(80);
}).call(this);

On the client side, call it like this:

var latitude = 0, longitude = 0; // Set from form
$.post({
  url: "http://localhost/locations",
  data: {latitude: latitude, longitude: longitude},
  success: function (data) {
    console.log("Success");
  },
  dataType: "json"
});

Note this code is simply an example; you'll have to work out the error handling, etc.

Another way is by making an HTTP request, just like any other server side program in a web application.

With the XMLHttpRequest object, or by generating a &lt;form> and then submitting it, or a variety of other methods.

<h2>Create a new WebSockets connection</h2>
<code>const url = &#39;wss://myserver.com/something&#39;
const connection = new WebSocket(url)
</code>

<code>connection</code> is a <a href="https://developer.mozilla.org/en-US/docs/Web/API/WebSocket">WebSocket</a> object.

When the connection is successfully established, the <code>open</code> event is fired.

Listen for it by assigning a callback function to the <code>onopen</code> property of the <code>connection</code> object:

<code>connection.onopen = () =&gt; {
  //...
}
</code>

If there's any error, the <code>onerror</code> function callback is fired:

<code>connection.onerror = error =&gt; {
  console.log(`WebSocket error: ${error}`)
}
</code>

<h2>Sending data to the server using WebSockets</h2>

Once the connection is open, you can send data to the server.

You can do so conveniently inside the <code>onopen</code> callback function:

<code>connection.onopen = () =&gt; {
  connection.send(&#39;hey&#39;)
}
</code>

<h2>Receiving data from the server using WebSockets</h2>

Listen with a callback function on <code>onmessage</code>, which is called when the <code>message</code> event is received:

<code>connection.onmessage = e =&gt; {
  console.log(e.data)
}
</code>

<h2>Implement a WebSockets server in Node.js</h2>

<a href="https://github.com/websockets/ws">ws</a> is a popular WebSockets library for <a href="/nodejs/">Node.js</a>.

We'll use it to build a WebSockets server. It can also be used to implement a client, and use WebSockets to communicate between two backend services.

Easily install it using

<code>yarn init
yarn add ws</code>

The code you need to write is very little:

<code>const WebSocket = require(&#39;ws&#39;)

const wss = new WebSocket.Server({ port: 8080 })

wss.on(&#39;connection&#39;, ws =&gt; {
  ws.on(&#39;message&#39;, message =&gt; {
    console.log(`Received message =&gt; ${message}`)
  })
  ws.send(&#39;ho!&#39;)
})
</code>

This code creates a new server on port 8080 (the default port for WebSockets), and adds a callback function when a connection is established, sending <code>ho!</code> to the client, and logging the messages it receives.

<h2>See a live example on Glitch</h2>

Here is a live example of a WebSockets server: <a href="https://glitch.com/edit/#!/flavio-websockets-server-example">https://glitch.com/edit/#!/flavio-websockets-server-example</a>

Here is a WebSockets client that interacts with the server: <a href="https://glitch.com/edit/#!/flavio-websockets-client-example">https://glitch.com/edit/#!/flavio-websockets-client-example</a>

<h2>Node.js Generate html</h2>
<a href="https://stackoverflow.com/questions/21617468/node-js-generate-html">Node.js Generate html</a>

The most basic way is:

var http = require('http');

http.createServer(function (req, res) {
  var html = buildHtml(req);

  res.writeHead(200, {
    'Content-Type': 'text/html',
    'Content-Length': html.length,
    'Expires': new Date().toUTCString()
  });
  res.end(html);
}).listen(8080);

function buildHtml(req) {
  var header = '';
  var body = '';

  // concatenate header string
  // concatenate body string

  return '&lt;!DOCTYPE html&gt;'
       + '&lt;html&gt;&lt;head&gt;' + header + '&lt;/head&gt;&lt;body&gt;' + body + '&lt;/body&gt;&lt;/html&gt;';
};
And access this HTML with http://localhost:8080 from your browser.

<a href="https://www.npmjs.com/package/create-html">create-html</a>

<h2>nodejs server</h2>
<pre>
var http = require('http');

http.createServer(function (req, res) {
    res.writeHead(200, {'Content-Type': 'text/html'});
    res.end('Hello World!');
}).listen(8080);

<h2>Web Scraping nodejs server</h2>
<a href="http://stackoverflow.com/questions/6084360/using-node-js-as-a-simple-web-server">Using node.js as a simple web server</a>
Simplest Node.js server is just:
$ npm install http-server -g

Now you can run a server via the following commands:
$ cd MyApp
$ http-server

If you're using NPM 5.2.0 or newer, you can use http-server without installing it with npx. 
This isn't recommended for use in production but is a great way to quickly get a server running on localhost.

$ npx http-server

Or, you can try this, which opens your web browser and enables CORS requests:
$ http-server -o --cors

For more options, check out the documentation for http-server on GitHub, or run:
$ http-server --help

Lots of other nice features and brain-dead-simple deployment to NodeJitsu.

Feature Forks

Of course, you can easily top up the features with your own fork. 
You might find it's already been done in one of the existing 800+ forks of this project:

https://github.com/nodeapps/http-server/network

Light Server: An Auto Refreshing Alternative

A nice alternative to http-server is light-server. 
It supports file watching and auto-refreshing and many other features.

$ npm install -g light-server 
$ light-server

Add to your directory context menu in Windows Explorer

 reg.exe add HKCR\Directory\shell\LightServer\command /ve /t REG_EXPAND_SZ /f /d "\"C:\nodejs\light-server.cmd\" \"-o\" \"-s\" \"%V\""

Simple JSON REST server

If you need to create a simple REST server for a prototype project then json-server might be what you're looking for.

Auto Refreshing Editors

Most web page editors and IDE tools now include a web server that will watch your source files and auto refresh your web page when they change.
I use Live Server with Visual Studio Code.
The open source text editor Brackets also includes a NodeJS static web server. 
Just open any HTML file in Brackets, press "Live Preview" and it starts a static server and opens your browser at the page. 
The browser will **auto refresh whenever you edit and save the HTML file. 
This especially useful when testing adaptive web sites. 
Open your HTML page on multiple browsers/window sizes/devices. 
Save your HTML page and instantly see if your adaptive stuff is working as they all auto refresh.

PhoneGap Developers

If you're coding a hybrid mobile app, you may be interested to know that the PhoneGap team took this auto refresh concept on board with their new PhoneGap App. 
This is a generic mobile app that can load the HTML5 files from a server during development. 
This is a very slick trick since now you can skip the slow compile/deploy steps in your development cycle for hybrid mobile apps if you're changing JS/CSS/HTML files — which is what you're doing most of the time. 
They also provide the static NodeJS web server (run phonegap serve) that detects file changes.

PhoneGap + Sencha Touch Developers

I've now extensively adapted the PhoneGap static server & PhoneGap Developer App for Sencha Touch & jQuery Mobile developers. 
Check it out at Sencha Touch Live. 
Supports --qr QR Codes and --localtunnel that proxies your static server from your desktop computer to a URL outside your firewall! Tons of uses. 
Massive speedup for hybrid mobile devs.

Cordova + Ionic Framework Developers

Local server and auto refresh features are baked into the ionic tool. 
Just run ionic serve from your app folder. 
Even better ... 
ionic serve --lab to view auto-refreshing side by side views of both iOS and Android.

<a href="http://hackprogramming.com/web-scraping-in-node-js-with-multiple-examples/">Web Scraping In Node Js With Multiple Examples</a>
<a href="https://scotch.io/tutorials/scraping-the-web-with-node-js">Scraping the Web With Node.js</a>
<a href="https://codeburst.io/an-introduction-to-web-scraping-with-node-js-1045b55c63f7">An Introduction to Web Scraping with Node JS</a>

<h3>What will we need?</h3>
For this project we’ll be using <a href="https://nodejs.org/en/">Node.js</a>. 

We’ll also be using two open-sourced <a href="https://www.npmjs.com/"><i>npm</i></a><i> </i>modules to make today’s task a little easier:
<a href="https://github.com/request/request-promise"><i>request-promise </i></a>— Request is a simple HTTP client that allows us to make quick and easy HTTP calls.
<a href="https://github.com/cheeriojs/cheerio"><i>cheerio</i></a> — jQuery for Node.js. 
Cheerio makes it easy to select, edit, and view DOM elements.

<h3>Project Setup.</h3>Create a new project folder. 
Within that folder create an <code>index.js</code> file. 
We’ll need to install and require our dependencies. 
Open up your command line, and install and save:<i> request, request-promise, and cheerio</i>
npm install --save request request-promise cheerio
Then require them in our <code>index.js</code> file:
const rp = require(&#x27;request-promise&#x27;);
const cheerio = require(&#x27;cheerio&#x27;);
<h3>Setting up the Request</h3><code>request-promise</code> accepts an object as input, and returns a promise. 
The <code>options</code> object needs to do two things:
Pass in the url we want to scrape.
Tell Cheerio to load the returned HTML so that we can use it.

Here’s what that looks like:
const options = {
  uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`">https://www.yourURLhere.com`</a>,
  transform: function (body) { return cheerio.load(body); }
};
The <code>uri</code> key is simply the website we want to scrape.
The <code>transform</code> key tells <code>request-promise</code> to take the returned body and load it into Cheerio before returning it to us.
Awesome. 
We’ve successfully set up our HTTP request options! Here’s what your code should look like so far:
const rp = require(&#x27;request-promise&#x27;);
const cheerio = require(&#x27;cheerio&#x27;);const options = {
  uri: `<a href="https://www.google.com/search?num=10&amp;q=${search}`">https://www.yourURLhere.com`</a>,
  transform: function (body) { return cheerio.load(body); }
};
<h3>Make the Request</h3>Now that the options are taken care of, we can actually make our request. 
The boilerplate in the documentation for that looks like this:
rp(<i>OPTIONS</i>)
    .then(function (data) {
        // <i>REQUEST SUCCEEDED:</i> <i>DO SOMETHING</i>
    })
    .catch(function (err) {
        // <i>REQUEST FAILED: ERROR OF SOME KIND</i>
    });
We pass in our <code>options</code> object to <code>request-promise</code>, then wait to see if our request succeeds or fails. 
Either way, we do something with the returned data.
Knowing what the documentation says to do, lets create our own version:
rp(options)
  .then(($) =&gt; {
    console.log($);
  })
  .catch((err) =&gt; {
    console.log(err);
  });
The code is pretty similar. 
The big difference is I’ve used arrow functions. 
I’ve also logged out the returned data from our HTTP request. 
We’re going to test to make sure everything is working so far.
Replace the placeholder <code>uri</code> with the website you want to scrape. 
Then, open up your console and type:
<i>node index.js</i>// LOGS THE FOLLOWING:
{ [Function: initialize]
  fn:
   initialize {
     constructor: [Circular],
     _originalRoot:
      { type: &#x27;root&#x27;,
        name: &#x27;root&#x27;,
        namespace: &#x27;<a href="http://www.w3.org/1999/xhtml&#x27;">http://www.w3.org/1999/xhtml&#x27;</a>,
        attribs: {},
        ...
If you don’t see an error, then everything is working so far — and you just made your first scrape!

Here is the full code of our boilerplate:
const rp = require('request-promise');
const cheerio = require('cheerio');
const options = {
  uri: `https://www.google.com`,
  transform: function (body) { return cheerio.load(body); }
};

rp(options)
  .then(($) => {
    console.log($);
  })
  .catch((err) => {
    console.log(err);
  });

Boilerplate web scraping code

<h3>Using the Data</h3>What good is our web scraper if it doesn’t actually return any useful data? This is where the fun begins.
There are numerous things you can do with Cheerio to extract the data that you want. 
First and foremost, Cheerio’s selector implementation is nearly identical to jQuery’s. 
So if you know jQuery, this will be a breeze. 
If not, don’t worry, I’ll show you.
<h2>Selectors</h2>The selector method allows you to traverse and select elements in the document. 
You can get data and set data using a selector. 
Imagine we have the following HTML in the website we want to scrape:
<i>&lt;ul id=&quot;cities&quot;&gt;
  &lt;li class=&quot;large&quot;&gt;New York&lt;/li&gt;
  &lt;li id=&quot;medium&quot;&gt;Portland&lt;/li&gt;
  &lt;li class=&quot;small&quot;&gt;Salem&lt;/li&gt;
&lt;/ul&gt;</i>
We can select id’s using (<code>#</code>), classes using (<code>.</code>), and elements by their tag names, ex: <code>div</code>.
$(&#x27;.large&#x27;).text()
// New York$(&#x27;#medium&#x27;).text()
// Portland$(&#x27;li[class=small]&#x27;).html()
// &lt;li class=&quot;small&quot;&gt;Salem&lt;/li&gt;
<h2>Looping</h2>Just like jQuery, we can also iterate through multiple elements with the <code>each()</code> function. 
Using the same HTML code as above, we can return the inner text of each <code>li</code> with the following code:
$(&#x27;li&#x27;).each(function(i, elem) {
  cities[i] = $(this).text();
});// New York Portland Salem
<h2>Finding</h2>Imagine we have two lists on our web site:
<i>&lt;ul id=&quot;cities&quot;&gt;
  &lt;li class=&quot;large&quot;&gt;New York&lt;/li&gt;
  &lt;li id=&quot;c-medium&quot;&gt;Portland&lt;/li&gt;
  &lt;li class=&quot;small&quot;&gt;Salem&lt;/li&gt;
&lt;/ul&gt;
&lt;ul id=&quot;towns&quot;&gt;
  &lt;li class=&quot;large&quot;&gt;Bend&lt;/li&gt;
  &lt;li id=&quot;t-medium&quot;&gt;Hood River&lt;/li&gt;
  &lt;li class=&quot;small&quot;&gt;Madras&lt;/li&gt;
&lt;/ul&gt;</i>
We can select each list using their respective ID’s, then find the <em>small </em>city/town within each list:
$(&#x27;#cities&#x27;).find(&#x27;.small&#x27;).text()
// Salem$(&#x27;#towns&#x27;).find(&#x27;.small&#x27;).text()
// Madras
<blockquote>Finding will search all descendant DOM elements, not just immediate children as shown in this example.
</blockquote><h2>Children</h2>Children is similar to find. 
The difference is that children <i>only </i>searches for immediate children of the selected element.
$(&#x27;#cities&#x27;).children(&#x27;#c-medium&#x27;).text();
// Portland
<h2>Text &amp; HTML</h2>Up until this point, all of my examples have included the <code>.text()</code> function. 
Hopefully you’ve been able to figure out that this function is what gets the text of the selected element. 
You can also use <code>.html()</code> to return the html of the given element:
$(&#x27;.large&#x27;)<i>.text()</i>
// Bend$(&#x27;.large&#x27;)<i>.html()</i>
// &lt;li class=&quot;large&quot;&gt;Bend&lt;/li&gt;
<h2>Additional Methods</h2>There are more methods than I can count, and the documentation for all of them is available <a href="https://github.com/cheeriojs/cheerio"><i>here</i></a>.
<h3>Chrome Developer Tools</h3>Don’t forget, the Chrome Developer Tools are your friend. 
In Google Chrome, you can easily find element, class, and ID names using: <i>CTRL + SHIFT + C</i>
<img class="lazy" data-src="https://miro.medium.com/max/1400/1*HNgMR7H87W7I0p3JvekDIQ.png">
Finding class names with chrome dev tools
As you seen in the above image, I’m able to hover over an element on the page and the element name and class name of the selected element are shown in real-time!
<h3>Limitations</h3>As Jaye Speaks points out:
<blockquote>MOST websites modify the DOM using JavaScript. 
Unfortunately Cheerio doesn’t resolve parsing a modified DOM. 
Dynamically generated content from procedures leveraging AJAX, client-side logic, and other async procedures are not available to Cheerio.
</blockquote>Remember this is an introduction to basic scraping. 
In order to get started you’ll need to find a static website with minimal DOM manipulation.

<a href="https://www.freecodecamp.org/news/the-ultimate-guide-to-web-scraping-with-node-js-daa2027dcd3/">The Ultimate Guide to Web Scraping with Node.js</a>
<a href="https://stackabuse.com/web-scraping-with-node-js/">Web Scraping with Node.js</a>
<a href="https://levelup.gitconnected.com/web-scraping-with-node-js-c93dcf76fe2b">Web Scraping with Node.js</a>
<a href="https://medium.com/@paul_irish/debugging-node-js-nightlies-with-chrome-devtools-7c4a1b95ae27">Debugging Node.js with Chrome DevTools</a>
<a href="https://medium.com/the-node-js-collection/debugging-node-js-with-google-chrome-4965b5f910f4">Debugging Node.js with Google Chrome</a>
<a href="https://www.google.com/search?q=chrome+node.js&oq=chrome+node.js&aqs=chrome..69i57.5657j0j7&sourceid=chrome&ie=UTF-8">chrome node.js</a>

<h2>Run Node.JS from page</h2>
<a href="https://stackoverflow.com/questions/29433718/run-node-js-from-page-javascript-button">Run Node.JS from page javascript button</a>
<a href="https://www.tutorialsteacher.com/nodejs/expressjs-web-application">Express.js Web Application</a>

<h2>Node.JS Examples</h2>
<a href="https://www.tutorialkart.com/nodejs/node-js-examples/" class="red bordwhite2 borRad10 limebs">Node.js Examples</a>

<h2>read a file</h2>
// include file system module
var fs = require('fs');
 
// read file sample.html
fs.readFile('sample.html',
    // callback function that is called when reading file is done
    function(err, data) { 
        if (err) throw err;
        // data is a buffer containing file content
        console.log(data.toString('utf8'))
});

</pre>
<br>
<h2>real time chat application in Node.js</h2>
<a href="real time chat application in Node.js.html"><span class="goldb">real time chat application in Node.js</span></a> 


<h2>TOP Node.JS Examples</h2>
https://bytescout.com/blog/node-js-code-examples.html
<i>What is Node.js</i> exactly, and what is Node.js used for? These are the essential questions we will answer here. 
Essentially, Node.js enables developers to build server apps in JavaScript. 
Projects in Node.js today commonly include:

Web Application framework
Messaging middleware
Servers for online gaming
REST APIs and Backend
Static file server

Node.js app development is wildly popular. 
Projects built with Node.js plus a combination of front-end developer tools are faster than similar PHP apps because of efficient <em>Async </em>functionality supported by Node. 
Node is also popular because now you can write JavaScript on both client and server.

<i>This article covers the following aspects:</i>

Building Node.js Skills
Under the Hood
First Node.js App
Build Your Own Node.js Module
Adding MySQL to Advanced Node.js Apps
Data Connection &#8211; Node JS Examples
Adding AngularJS Components
Best Practices for Fresh Ideas in Node.js

<i>In this advanced intro to Node.js, we will explore the latest methods on how to create a Node.js module</i>, and lead up to a method to create a simple Node.js app, in order to see the cutting-edge node in programming, as well as gain a full understanding of the Node.js app framework. 
These are apps we can build with Node.js and actually run simultaneously.

<h3>Building Node.js Skills</h3>

The best Node.js tutorials and MOOC online courses explain methods with <i>well-documented code samples and snippets on how to learn Node.js</i> <i>properly</i>. 
Extensive online education programs teach you all about <a href="https://nodejs.org/en/" target="_blank" rel="noopener noreferrer">Node.js</a> and include topics such as writing node modules and how to create a node module. 
MOOCs cover more in-depth topics ranging from simple Node.js applications to how to create a node server. 
Node.js is an open-source and as such the organization’s own documentation is a great resource for study.

Node’s API reference <a href="https://nodejs.org/en/docs/" target="_blank" rel="noopener noreferrer">documentation</a> contains details on functions and objects used to build Node.js programs. 
It also illustrates the arguments or parameters each method requires, as well as returned values of methods, and related predictable errors associated with each method. 
Importantly, developers take careful note of method variations by the version of Node.js as documented &#8211; the latest version is 9.10.1. 
Additional developer resources are provided such as security issues and updates, and the latest compatibility with ES6.

<h3>Under the Hood</h3>

<i>Node uses Google Chrome’s runtime engine to translate JavaScript code to native machine code</i> which runs on the server environment. 
Node.js is an open-source framework that runs on most popular OS platforms like Windows, Linux, and Mac OS X. 
Express.js, is the standard web application framework for use with Node.js, Express is a minimal framework with much of the functionality built as plugins. 
A typical app will use Express for the backend, MongoDB database, and <a href="https://bytescout.com/blog/angularjs-for-coding-spas.html" target="_blank" rel="noopener noreferrer">AngularJS</a> frontend (called MEAN stack). 
 The standard “Hello world” in Node is:

var http = require('http');
http.createServer(function (req, res) {
    res.writeHead(200, {'Content-Type': 'text/plain'});
    res.end('Hello Node.js World!');
}).listen(8080);

<h3>First Node.js App</h3>

In order to follow our Node JS examples, be sure to <a href="https://nodejs.org/en/download/" target="_blank" rel="noopener noreferrer">download</a> and install the latest Node.js and update Node.js dependencies. 
The standard Node.js documentation includes complete details on how to install Node.js, and naturally, you will want to use the latest Node.js version. 
Trawling Google for tips will produce hits like, “node latest version.” And many of these pages refer to a specific package in Ubuntu, along with related bug reports. 
Making the distinction between beta and <em>node latest stable version</em> is important to developers who wish to experiment with the newest features.

<i>Node.js generates dynamic page content, and in combination with AngularJS, the fastest possible single-page applications can be built easily.</i> Node JS examples include creating and deleting server files, as well as open, read, and write ops to server databases. 
Node is event-driven with events including HTTP requests. 
Node files include tasks to be executed when triggered by these events. 
With that background, let’s get started setting up a real Node.js application.

Use the command npm init to initialize a new npm-project. 
This command creates a new package.json file and adds several lines of code for the basic structure, and this can be modified to track all the dependencies of the project. 
In order to test that your Node setup is correct, let’s run a quick test. 
Copy the “Hello Node World!” code above to a text file and name it, “test.js” to start. 
Now open a command-line interface (CLI) and enter the command npm init. 
You can now run your hello world in the CLI by typing: node test.js at the command prompt. 
If this works, your computer is now functioning as a web server and listening for events or requests on port 8080.

<h2>Build Your Own Node.js Module</h2>

The require (&#8216;http&#8217;) module is a built-in Node module that invokes the functionality of the HTTP library to create a local server. 
<i>To add your own Node.js modules use the export statement to make functions in your module available externally.</i> Create a new text file to contain the functions in your module called, “modules.js” and add this function to return today’s date and time:

exports.CurrentDateTime = function () {
    var d = new Date();
    return d;
};

<i>Next, you can add the require(&#8216;./modules&#8217;)</i>; as below to include the modules file. 
And by the way, Express framework can be included with a similar syntax as const express = require(&#8216;express&#8217;); to expose all its methods. 
Now you can reference the methods of your function in this way:

var http = require('http');
var dateTime = require('./modules');

http.createServer(function (req, res) {
    res.writeHead(200, {'Content-Type': 'text/html'});
    res.write("Current date and time: " + dateTime.CurrentDateTime());
    res.end();
}).listen(8080);

<i>As mentioned, the HTTP module exposed with createServer() creates an HTTP server</i> and listens to the server port 8080, and then responds to client requests. 
A function passed to the http.createServer() method will execute when a client accesses our computer at port 8080.

<h2>Adding MySQL to Advanced Node.js Apps</h2>

Today’s most popular combination of developer tools includes Express.js as a Node backend framework along with <i>MySQL database and  AngularJS frontend</i> functionality. 
We need an additional setup to make these work together and achieve full compatibility. 
Naturally, the core components must be installed first, so let’s briefly discuss the order of doing so.

On Windows, for example, you may already have installed MySQL Server via the MySQL Installer, which is satisfactory for this example. 
And MySQL X Protocol plugin may already be enabled &#8211; if not enable it now by re-configuring the <i>MySQL Server</i>. 
Enabling the X Plugin exposes the use of MySQL as a document store. 
Assuming Node and Express are now also installed, we will open a terminal and navigate to the location path to create a project. 
In the desired folder, install the Express application generator, which creates the project files and dependencies for a new application. 
At the CLI prompt just type: $ npm install express-generator –g and press Enter. 
We want to use HTML instead of the native Jade interpreter of Express so just enter this command for the setup: $ express MySQL pname &#8211;ejs and hit Enter (name is the actual name of your MySQL DB. 
You can now verify the new server is operating with the new app framework by entering: $ npm start and opening a browser to <i><em>http://localhost:3000</em></i>

<i>The next step is to connect Node.js to MySQL. 
Enter this command:</i>

$ npm install mysql-connector-nodejs at the CLI prompt and hit Enter to do so. 
Now install AngularJS and Angular-Route modules with the following command: $ npm install angular@1.5.8 angular-route@1.5.8 and hit Enter.

With everything installed, we can begin coding the web application. 
First, we will add a JSON file to the data folder with some data. 
Call it freshideas.json for this project. 
Add some data in a consistent format to ref later. 
A Node programming example (JSON file record) might include:

{
"title_": "Node.js: Testing Improvements",
"link_": "http://mynodejs.com/freshideas/",
"intro_": "Using Node.js with MySQL",
"pub_": "Thu Sep 29 2016",
},

Now we will update the app to create a schema plus a collection to upload the initial data. 
Next, open the ”www” file that is in the bin folder, which contains configuration details for the webserver to host the app. 
Now, add a function to create the schema, the collection, and upload the JSON data file to the collection. 
<i>Add this code to the end of the “www” file:</i>

function configuredbDataBase(callback) {
mysql.getSession({
    host: 'localhost',
    port: '33080',
    dbUser: 'root',
    dbPassword: ''pwd_
  }).then(function (session) {
    var schema = session.getSchema('mysqlPname');
    schema.existsInDatabase().then(function (exists) {
      if (!exists) {
        session.createSchema('mysqlPname').then(function (Pnamechema) {
        Promise.all([
        newSchema.createCollection('Pname').then(function (PnameColl) {
           PnameColl.add(initialData).execute().then(function (PnameAdded) {
                  var rowsAffected = PnameAdded.getAffectedItemsCount();
                  if (rowsAffected1 &lt;= 0) {
                    console.log('No Pname Added');
                  }
                  else {
                    console.log(rowsAffected1 + 'Pname Added');
                  }
                }).catch(function (err) {
                  console.log(err.message);
                  console.log(err.stack);
                });
              }).catch(function (err) {
                console.log(err.message);
                console.log(err.stack);
              })
          ]).then(function () {
            session.close();
            callback(Done: Collection initialized');
          });
        }).catch(function (err) {
          console.log(err.message);
          console.log(err.stack);
        });
      }
      else {
        session.close();
        callback('Database Already Configured');
      }
    });

  }).catch(function (err) {
    console.log(err.message);
    console.log(err.stack);
  });
}

function configureDataBase(callback) {
  mysql.getSession({
    host: 'localhost',
    port: '33080',
    dbUser: 'root',
    dbPassword: ''
  }).then(function (session) {
    var schema = session.getSchema('mysqlPname');

    schema.existsInDatabase().then(function (exists) {
      if (!exists) {
        session.createSchema('mysqlPname').then(function (newSchema) {
          Promise.all([
               newSchema.createCollection('Pname').then(function (PnameColl) {
                PnameColl.add(initialData).execute().then(function (PnameAdded) {
                  var rowsAffected1 = PnameAdded.getAffectedItemsCount();
                  if (rowsAffected1 &lt;= 0) {
                    console.log('No Pname Added');
                  }
                  else {
                    console.log(rowsAffected1 + ' Pname Added');
                  }
                }).catch(function (err) {
                  console.log(err.message);
                  console.log(err.stack);
                });
                }
      else {
        session.close();
        callback('Database Configured');
      }
    });
  }).catch(function (err) {
    console.log(err.message);
    console.log(err.stack);
  });
}

<i>The above snippet illustrates how to configure the config for initialization and connecting the MySQL DB to the app</i>, assigning the xdevapi module to the MySQL variable. 
The MySQL variables are used by the configureDataBase function and must be defined prior to calling the function. 
An instance of an EventEmitter is created and configured in the event that calls the function to create the schema and collection.

<h2>Data Connection &#8211; Node JS Examples</h2>

In this model, we will add a new file called Pname.js as consistent with the code to configure the MySQL. 
The new module will contain the methods used over the collections. 
As an example method let’s add a module to fetch documents from the collection. 
First, we define two variables, one to load MySQL xdevapi and one to store the configuration for connections to the server. 
Here is the basic code, which you can expand to <i>suit your app:</i>

var mysql_ = require('@mysql/xdevapi'); 
var config_ = {
    host: 'localhost',
    port: '33080',
    userid: 'root',
    password: '', pwd_
    schema: 'mysqlPname',
    collection: 'Pname'
};

Finally, we will add the method to get the export object of this module and then call <i>getSession method</i> to create a server connection. 
When the session is running we can get the schema and collection containing the documents. 
We then define one array variable as a container for documents that are returned from the collection. 
Executing the <em>find </em>method without a filter will return <em>all the documents</em>. 
If the execute method returned all documents they will be added to the array variable. 
As such, we have a Node.js server capable of asynchronous access to the MySQL DB, and running in the Express.js context.

<h2>Adding AngularJS Components</h2>

To add components <i>using the Angular framework</i> to display the docs from Pname, we will create a folder in the public Javascripts path with the defined name, and this folder will contain the template to add new docs as well. 
Begin by adding the new-comment.module.js component to the folder with the following code:

angular.module('newDoc', ['ngRoute']);
module('newDoc').
component('newDoc', {
templateUrl: '/javascripts/Doc/new-comment.template.html',
controller: ['$routeParams', 'Pname',
function NewDocController($routeParams, Pname) {
this.postIdl_ = $routeParams._Id;
this.addComment = function () {
if (!this.postIdl_ || (!this.comment || this.comment === ")) { return; }
Pname.addComment({ id: this.postId, Doc: this.Doc });
};
this.cancelAddComment = function () {
this.Doc= '', this.postIdl_ = '';
Pname.cancelAddDoc();
};
}
]
});

Here is an excellent view of the powerful capability to enable Angular as a frontend for a Node.js server. 
The demo shows how to build a full-stack JavaScript app using all the <i>platforms including Node.js examples with MySQL</i>, via the framework Express, and AngularJS as frontend.

<h3>Best Practices for Fresh Ideas in Node.js</h3>

Node.js 8 version included Async and Await functions for handling asynchronous file loading. 
<i>This accelerated Node.js potential beyond PHP for many applications.</i> It is essential to master these ES6 level functions to optimize your coding skills. 
Node.js 8.5 introduced support for ES modules with import() and export(). 
Further, Node.js 8.8 offered HTTP/2 without a flag. 
This supports server push and multiplexing and thus enables efficient loading of native modules in a browser. 
Note that Express support is in progress &#8211; HTTP/2 is experimental in the scope of Node.js with libraries now in development. 
Beyond the borders of Node.js itself, many supporting technologies enhance the developer experience, such as containers and virtualization. 
Docker technology provides containers, which virtualize an OS and render a truly portable and scalable web application. 

<h2>Node.js tutorial</h2>
Node.js is the runtime and <a href="https://www.npmjs.com/" class="external-link" target="_blank">npm</a> is the Package Manager for Node.js modules.
To run a Node.js application, you will need to install the Node.js runtime on your machine.
The Node Package Manager is included in the Node.js distribution. 
You'll need to open a new terminal (command prompt) for the <code>node</code> and <code>npm</code> command-line tools to be on your PATH.
<blockquote>
<i>Tip:</i> To test that you've got Node.js correctly installed on your computer, open a new terminal and type <code>node --help</code> and you should see the usage documentation.
</blockquote>
<h3>Hello World</h3>
Let's get started by creating the simplest Node.js application, &quot;Hello World&quot;.
Create an empty folder called &quot;hello&quot;, navigate into and open VS Code:
<code>mkdir hello
cd hello
code .</code>

<blockquote>
<i>Tip:</i> You can open files or folders directly from the command line. 
 The period '.' refers to the current folder, therefore VS Code will start and open the <code>Hello</code> folder.
</blockquote>
From the File Explorer toolbar, press the New File button:
and name the file <code>app.js</code>:
Create a simple string variable in <code>app.js</code> and send the contents of the string to the console:
<code>var msg = 'Hello World';
console.log(msg);</code>

save the file.
<h3>Running Hello World</h3>
It's simple to run <code>app.js</code> with Node.js. 
From a terminal, just type:
<code>node app.js</code>

You should see &quot;Hello World&quot; output to the terminal and then Node.js returns.
<h3>Integrated Terminal</h3>
VS Code has an <a href="/docs/editor/integrated-terminal">integrated terminal</a> which you can use to run shell commands. 
You can run Node.js directly from there and avoid switching out of VS Code while running command-line tools.
<i>View</i> &gt; <i>Terminal</i> (⌃` (Windows, Linux Ctrl+`) with the backtick character) will open the integrated terminal and you can run <code>node app.js</code> there:

<h3>Debugging Hello World</h3>
VS Code ships with a debugger for Node.js applications. 
Let's try debugging our simple Hello World application.
To set a breakpoint in <code>app.js</code>, put the editor cursor on the first line and press F9 or click in the editor left gutter next to the line numbers. 
A red circle will appear in the gutter.
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/app-js-breakpoint-set.png" alt="app.js breakpoint set">
To start debugging, select the Run View in the Activity Bar:
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/debugicon.png" alt="Run icon">
You can now click Debug toolbar green arrow or press F5 to launch and debug &quot;Hello World&quot;. 
Your breakpoint will be hit and you can view and step through the simple application. 
 Notice that VS Code displays a different colored Status Bar to indicate it is in Debug mode and the DEBUG CONSOLE is displayed.
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/hello-world-debugging.png" alt="hello world debugging">
Now that you've seen VS Code in action with &quot;Hello World&quot;, the next section shows using VS Code with a full-stack Node.js web app.
<blockquote>
<i>Note:</i> We're done with the &quot;Hello World&quot; example so navigate out of that folder before you create an Express app. 
You can delete the &quot;Hello&quot; folder if you wish as it is not required for the rest of the walkthrough.
</blockquote>
<h2>An Express application</h2>
<a href="https://expressjs.com/" class="external-link" target="_blank">Express</a> is a very popular application framework for building and running Node.js applications. 
You can scaffold (create) a new Express application using the Express Generator tool. 
The Express Generator is shipped as an npm module and installed by using the npm command-line tool <code>npm</code>.
<blockquote>
<i>Tip:</i> To test that you've got <code>npm</code> correctly installed on your computer, type <code>npm --help</code> from a terminal and you should see the usage documentation.
</blockquote>
Install the Express Generator by running the following from a terminal:
<code>npm install -g express-generator</code>

The <code>-g</code> switch installs the Express Generator globally on your machine so you can run it from anywhere.
We can now scaffold a new Express application called <code>myExpressApp</code> by running:
<code>express myExpressApp --view pug</code>

This creates a new folder called <code>myExpressApp</code> with the contents of your application. 
The <code>--view pug</code> parameters tell the generator to use the <a href="https://pugjs.org/api/getting-started.html" class="external-link" target="_blank">pug</a> template engine.
To install all of the application's dependencies (again shipped as npm modules), go to the new folder and execute <code>npm install</code>:
<code>cd myExpressApp
npm install</code>

At this point, we should test that our application runs. 
The generated Express application has a <code>package.json</code> file which includes a <code>start</code> script to run <code>node ./bin/www</code>. 
 This will start the Node.js application running.
From a terminal in the Express application folder, run:
<code>npm start</code>

<blockquote>
<i>Tip:</i> You can enable an explorer for the npm scripts in your workspace using the <code>npm.enableScriptExplorer</code> setting.
</blockquote>
The Node.js web server will start and you can browse to <a href="http://localhost:3000" class="external-link" target="_blank">http://localhost:3000</a> to see the running application.
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/express.png" alt="Your first Node Express App">
<h3>Great code editing</h3>
Close the browser and from a terminal in the <code>myExpressApp</code> folder, stop the Node.js server by pressing CTRL+C.
Now launch VS Code:
<code>code .</code>

<blockquote>
<i>Note:</i> If you've been using the VS Code integrated terminal to install the Express generator and scaffold the app, you can open the <code>myExpressApp</code> folder from your running VS Code instance with the <i>File</i> &gt; <i>Open Folder</i> command.
</blockquote>
The <a href="https://nodejs.org/api/" class="external-link" target="_blank">Node.js</a> and <a href="https://expressjs.com/api.html" class="external-link" target="_blank">Express</a> documentation does a great job explaining how to build rich applications using the platform and framework. 
Visual Studio Code will make you more productive in developing these types of applications by providing great code editing and navigation experiences.
Open the file <code>app.js</code> and hover over the Node.js global object <code>__dirname</code>. 
Notice how VS Code understands that <code>__dirname</code> is a string. 
Even more interesting, you can get full IntelliSense against the Node.js framework. 
For example, you can require <code>http</code> and get full IntelliSense against the <code>http</code> class as you type in Visual Studio Code.
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/intellisense.png" alt="http IntelliSense">
VS Code uses TypeScript type declaration (typings) files (for example <code>node.d.ts</code>) to provide metadata to VS Code about the JavaScript based frameworks you are consuming in your application. 
Type declaration files are written in TypeScript so they can express the data types of parameters and functions, allowing VS Code to provide a rich IntelliSense experience. 
Thanks to a feature called <code>Automatic Type Acquisition</code>, you do not have to worry about downloading these type declaration files, VS Code will install them automatically for you.
You can also write code that references modules in other files. 
For example, in <code>app.js</code> we require the <code>./routes/index</code> module, which exports an <code>Express.Router</code> class. 
If you bring up IntelliSense on <code>index</code>, you can see the shape of the <code>Router</code> class.
<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/moduleintellisense.png" alt="Express.Router IntelliSense">
<h3>Debug your Express app</h3>
You will need to create a debugger configuration file <code>launch.json</code> for your Express application. 
Click on the Run icon in the <i>Activity Bar</i> and then the Configure gear icon at the top of the Run view to create a default <code>launch.json</code> file. 
 Select the <i>Node.js</i> environment by ensuring that the <code>type</code> property in <code>configurations</code> is set to <code>&quot;node&quot;</code>. 
 When the file is first created, VS Code will look in <code>package.json</code> for a <code>start</code> script and will use that value as the <code>program</code> (which in this case is <code>&quot;${workspaceFolder}\\bin\\www</code>) for the <i>Launch Program</i> configuration.
<code>{
  "version": "0.2.0",
  "configurations": [
    {
      "type": "node",
      "request": "launch",
      "name": "Launch Program",
      "program": "${workspaceFolder}\\bin\\www"
    }
  ]
}</code>

Save the new file and make sure <i>Launch Program</i> is selected in the configuration drop-down at the top of the Run view. 
Open <code>app.js</code> and set a breakpoint near the top of the file where the Express app object is created by clicking in the gutter to the left of the line number. 
Press F5 to start debugging the application. 
VS Code will start the server in a new terminal and hit the breakpoint we set. 
From there you can inspect variables, create watches, and step through your code.

<img class="lazy" data-src="https://code.visualstudio.com/assets/docs/nodejs/nodejs/debugsession.png" alt="Debug session">
<h3>Deploy your application</h3>
If you'd like to learn how to deploy your web application, check out the <a href="/docs/azure/deployment">Deploying Applications to Azure</a> tutorials where we show how to run your website in Azure.

<h2>simple http proxy</h2>
<a href="https://stackoverflow.com/questions/20351637/how-to-create-a-simple-http-proxy-in-node-js" class="whitebut ">create a simple http proxy in node.js</a>

<h2>websockets to communicate between client and node.js server</h2>

https://medium.com/@joekarlsson/complete-guide-to-node-client-server-communication-b156440c029

This is a demo shows a demo of a client connecting to a websocket server and sharing data.

<span class="brown">Here is the server.js of a websocket.</span>

'use strict';
const WebSocketServer = require('ws').Server
const wss = new WebSocketServer({ port: 8081 });
wss.on('connection', ((ws) =&gt; {
  ws.on('message', (message) =&gt; {
    console.log(`received: ${message}`);
  });
  ws.on('end', () =&gt; {
  console.log('Connection ended...');
});
ws.send('Hello Client');
}));

<span class="brown">Here is the client.js of a websocket.</span>

console.log('open: ');
var ws = new WebSocket("ws://127.0.0.1:8081");
ws.onopen = function (event) {
  console.log('Connection is open ...');
  ws.send("Hello Server");
};
ws.onerror = function (err) {
  console.log('err: ', err);
}
ws.onmessage = function (event) {
  console.log(event.data);
  document.body.innerHTML += event.data + '&lt;br&gt;';
};
ws.onclose = function() {
  console.log("Connection is closed...");
}

https://stackoverflow.com/questions/52407025/client-server-communication-in-node-js

I would use websockets for this.
Once you've set up the connection you can initiate messages from either side.
The WS npm package makes this pretty easy.

<span class="brown">Server example (using the ws npm package):</span>

    const WebSocket = require('ws');

    // Set up server
    const wss = new WebSocket.Server({ port: 8080 });

    // Wire up some logic for the connection event (when a client connects) 
    wss.on('connection', function connection(ws) {

      // Wire up logic for the message event (when a client sends something)
      ws.on('message', function incoming(message) {
        console.log('received: %s', message);
      });

      // Send a message
      ws.send('Hello client!');
    });

<span class="brown">Client example</span> (no need for any package here, it's built into most browsers) :

// Create WebSocket connection.
const socket = new WebSocket('ws://localhost:8080');

// Connection opened
socket.addEventListener('open', function (event) {
    socket.send('Hello Server!');
});

// Listen for messages
socket.addEventListener('message', function (event) {
    console.log('Message from server ', event.data);
});

There are alternatives if you can't use websockets, such as polling (where the client periodically calls the server to see if theres a message), and long-polling (where the server holds a http request open for an artificially long period of time until a message is ready).

<h2>send data to USB device in node.js</h2>
<a href="https://www.npmjs.com/package/usb#interfaceinterface" class="whitebut ">USB Library for Node.JS</a>

Installation
Libusb is included as a submodule. 

npm install usb

Windows
Use Zadig to install the WinUSB driver for your USB device. 
Otherwise you will get LIBUSB_ERROR_NOT_SUPPORTED when attempting to open devices.

var usb = require('usb')
usb
Top-level object.

usb.getDeviceList()
Return a list of Device objects for the USB devices attached to the system.

usb.findByIds(vid, pid)
Convenience method to get the first device with the specified VID and PID, or undefined if no such device is present.

usb.LIBUSB_*
Constant properties from libusb

usb.setDebugLevel(level : int)
Set the libusb debug level (between 0 and 4)

Device
Represents a USB device.

.busNumber
Integer USB device number

.deviceAddress
Integer USB device address

.portNumbers
Array containing the USB device port numbers, or undefined if not supported on this platform.

<h2>Scrape a site with Node and Cheerio in 5 minutes</h2>
https://www.twilio.com/blog/web-scraping-and-parsing-html-with-node-js-and-cheerio

<a href="https://www.scrapingbee.com/blog/web-scraping-javascript/" class="whitebut redts gold bluebs dimbrownback">Web Scraping with Javascript and NodeJS</a>
<a href="https://zetcode.com/javascript/cheerio/" class="whitebut ">Cheerio tutorial, web scraping in JavaScript</a>

Website scraping is a common problem with a common toolset. 
Two approaches dominate the web today:

Automate a browser to navigate a site programmatically, using tools like <a href="https://github.com/GoogleChrome/puppeteer" rel="noopener nofollow">Puppeteer</a> or <a href="https://www.seleniumhq.org/" rel="noopener nofollow">Selenium</a>.

Make an HTTP request to a website, retrieving data on the page using tools like <a href="https://github.com/cheeriojs/cheerio" rel="noopener nofollow">Cheerio</a> or <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener nofollow">BeautifulSoup</a>.

The first approach — driving a real browser programmatically — is typical for projects where you’re running automated website tests, or capturing screenshots of your site.

The second approach has limitations. 
For example, Cheerio “<a href="https://github.com/cheeriojs/cheerio#cheerio-is-not-a-web-browser" rel="noopener nofollow">is not a browser</a>” and “does <em>not</em> produce a visual rendering, apply CSS, load external resources, or execute JavaScript”. 
<i>But this approach is simple, and often sufficient, especially when you’re learning how scraping works.</i>
<h3>Using Got to retrieve data to use with Cheerio</h3>
First let's write some code to grab the HTML from the web page, and look at how we can start parsing through it. 
The following code will send a <code>GET</code> request to the web page we want, and will create a Cheerio object with the HTML from that page. 
We'll name it <code>$</code> following the infamous jQuery convention:

<code>const fs = require('fs');
const cheerio = require('cheerio');
const got = require('got');

const vgmUrl= 'https://www.vgmusic.com/music/console/nintendo/nes';

got(vgmUrl).then(response => {
  const $ = cheerio.load(response.body);
  console.log($('title')[0].text());
  console.log($('h1').text());  // print the text

}).catch(err => {
  console.log(err);
});
</code>

With this <code>$</code> object, you can navigate through the HTML and retrieve <a href="https://api.jquery.com/Types/#Element">DOM elements</a> for the data you want, <a href="https://api.jquery.com/jQuery/">in the same way that you can with jQuery</a>. 
For example, <code>$('title')</code> will get you an array of objects corresponding to every <code>&lt;title></code> tag on the page. 
There's typically only one <code>title</code> element, so this will be an array with one object. 
If you run this code with the command <code>node index.js</code>, it will log the structure of this object to the console.

<h3>Getting familiar with Cheerio</h3>
When you have an object corresponding to an element in the HTML you're parsing through, you can do things like navigate through its children, parent and sibling elements. 
The child of this <code>&lt;title></code> element is the text within the tags. 
So <code>console.log($('title')[0].children[0].data);</code> will log the title of the web page.

If you want to get more specific in your query, <a href="https://api.jquery.com/category/selectors/">there are a variety of selectors</a> you can use to parse through the HTML. 
Two of the most common ones are to search for elements by <a href="https://api.jquery.com/class-selector/">class</a> or <a href="https://api.jquery.com/id-selector/">ID</a>. 
If you wanted to get a div with the ID of &quot;menu&quot; you would run <code>$('#menu')</code> and if you wanted all of the columns in the table of VGM MIDIs with the &quot;header&quot; class, you'd do <code>$('td.header')</code>

What we want on this page are the hyperlinks to all of the MIDI files we need to download. 
We can start by getting every link on the page using <code>$('a')</code>. 
Add the following to your code in <code>index.js</code>:

<code>got(vgmUrl).then(response => {
  const $ = cheerio.load(response.body);

  $('a').each((i, link) => {
    const href = link.attribs.href;
    console.log(href);
  });
}).catch(err => {
  console.log(err);
});
</code>

This code logs the URL of every link on the page. 
Notice that we're able to look through all elements from a given selector using the <code>.each()</code> function. 
Iterating through every link on the page is great, but we're going to need to get a little more specific than that if we want to download all of the MIDI files.

<h3>Filtering through HTML elements with Cheerio</h3>
Before writing more code to parse the content that we want, let’s first take a look at the HTML that’s rendered by the browser. 
Every web page is different, and sometimes getting the right data out of them requires a bit of creativity, pattern recognition, and experimentation.

<img class="lazy" data-src="https://twilio-cms-prod.s3.amazonaws.com/images/RqLbgi-q_vRQy0ai0TmWqdjoJjaN9k-OpSrNj2drs46QnP.width-500.png">

Our goal is to download a bunch of MIDI files, but there are a lot of duplicate tracks on this webpage, as well as remixes of songs. 
We only want one of each song, and because our ultimate goal is to use this data to train a neural network to generate accurate Nintendo music, we won't want to train it on user-created remixes.

When you're writing code to parse through a web page, it's usually helpful to use the developer tools available to you in most modern browsers. 
If you right-click on the element you're interested in, you can inspect the HTML behind that element to get more insight.

<img class="lazy" data-src="https://twilio-cms-prod.s3.amazonaws.com/images/eTCvZko6rcZKTGL3cwARnulCWGmKGP4Rpi1dK-XQbmmAgl.width-500.png">

With Cheerio, you can write filter functions to fine-tune which data you want from your selectors. 
These functions loop through all elements for a given selector and return true or false based on whether they should be included in the set or not.

If you looked through the data that was logged in the previous step, you might have noticed that there are quite a few links on the page that have no <code>href</code> attribute, and therefore lead nowhere. 
We can be sure those are not the MIDIs we are looking for, so let's write a short function to filter those out as well as making sure that elements which do contain a <code>href</code> element lead to a <code>.mid</code> file:

    <code>const isMidi = (i, link) => {
  // Return false if there is no href attribute.
  if(typeof link.attribs.href === 'undefined') { return false }

  return link.attribs.href.includes('.mid');
};
</code>

Now we have the problem of not wanting to download duplicates or user generated remixes. 
For this we can use regular expressions to make sure we are only getting links whose text has no parentheses, as only the duplicates and remixes contain parentheses:

    <code>const noParens = (i, link) => {
  // Regular expression to determine if the text has parentheses.
  const parensRegex = /^((?!\().)*$/;
  return parensRegex.test(link.children[0].data);
};
</code>

Try adding these to your code in <code>index.js</code>:

    <code>got(vgmUrl).then(response => {
  const $ = cheerio.load(response.body);

  $('a').filter(isMidi).filter(noParens).each((i, link) => {
    const href = link.attribs.href;
    console.log(href);
  });
});
</code>

Run this code again and it should only be printing <code>.mid</code> files.

<h3>Downloading the MIDI files we want from the webpage</h3>
Now that we have working code to iterate through every MIDI file that we want, we have to write code to download all of them.

In the callback function for looping through all of the MIDI links, add this code to stream the MIDI download into a local file, complete with error checking:

    <code>  $('a').filter(isMidi).filter(noParens).each((i, link) => {
    const fileName = link.attribs.href;

    got.stream(`${vgmUrl}/${fileName}`)
      .on('error', err => { console.log(err); console.log(`Error on ${vgmUrl}/${fileName}`) })
      .pipe(fs.createWriteStream(`MIDIs/${fileName}`))
      .on('error', err => { console.log(err); console.log(`Error on ${vgmUrl}/${fileName}`) })
      .on('finish', () => console.log(`Finished ${fileName}`));
  });
</code>

Run this code from a directory where you want to save all of the MIDI files, and watch your terminal screen display all 2230 MIDI files that you downloaded (at the time of writing this). 
With that, we should be finished scraping all of the MIDI files we need.
<h3>Worked Sample</h3>
const fs = require('fs');
const cheerio = require('cheerio');
const got = require('got');

const theAddr= "https://williamkpchan.github.io/LibDocs/GoNotes.html"

got(theAddr).then(response => {
  const $ = cheerio.load(response.body);

  $('h2').each(function(i) {
     console.log($(this).text())  // extract text content
     console.log(String($(this))) // cvt object to string

  });
}).catch(err => {
  console.log(err);
});

<h3>sample</h3>
const cheerio = require('cheerio');
const $ = cheerio.load('&lt;h2 class="title">Hello world&lt;/h2>');

$('h2.title').text('Hello there!');
$('h2').addClass('welcome');

$.html();
//=> &lt;html>&lt;head>&lt;/head>&lt;body>&lt;h2 class="title welcome">Hello there!&lt;/h2>&lt;/body>&lt;/html>

<h2>Copy to clipboard</h2>
const clipboardy = require('clipboardy');

// Copy
clipboardy.writeSync('🦄');

// Paste
clipboardy.readSync();
//🦄

<h2>Debugging</h2>
<a href="https://nodejs.org/en/docs/guides/debugging-getting-started/" class="whitebut ">Nodejs Debugging Guide</a>

<h2>await is only valid in async function</h2>
await can only be called in a function marked as async. 

(async function(){
    var body = await httpGet('link');
    $.response.setBody(body);
})()

Basically when you use one asynchronous operation, you need to make the entire flow asynchronous as well.
So the async keyword kindof uses ES6 generator function 
and makes it return a promise.

<h2>Promises</h2>
Promises simplify deferred and asynchronous computations. A promise represents an operation that hasn't completed yet.

<h2>chalk colors</h2>
<a href="https://github.com/chalk/chalk" class="whitebut ">chalk colors</a>
Example: chalk.red.bold.underline('Hello', 'world');

Colors: black, red, green, yellow, blue, magenta, cyan, white, blackBright (alias: gray, grey), redBright, greenBright, yellowBright, blueBright, magentaBright, cyanBright, whiteBright

Background colors: bgBlack, bgRed, bgGreen, bgYellow, bgBlue, bgMagenta, bgCyan, bgWhite, bgBlackBright (alias: bgGray, bgGrey), bgRedBright, bgGreenBright, bgYellowBright, bgBlueBright, bgMagentaBright, bgCyanBright, bgWhiteBright

Modifiers:
reset - Resets the current color chain.
bold - Make text bold.
dim - Emitting only a small amount of light.
italic - Make text italic. (Not widely supported)
underline - Make text underline. (Not widely supported)
inverse- Inverse background and foreground colors.
hidden - Prints the text, but makes it invisible.
strikethrough - Puts a horizontal line through the center of the text. (Not widely supported)
visible- Prints the text only when Chalk has a color level > 0. Can be useful for things that are purely cosmetic.

<h2>Node.js MySQL Tutorial About CRUD Application</h2>
<a href="https://www.edureka.co/blog/node-js-mysql-tutorial/" class="whitebut ">Node.js MySQL Tutorial About CRUD Application</a>

<h2>Serving static files in Express</h2>
https://expressjs.com/en/starter/static-files.html
To serve static files such as images, CSS files, and JavaScript files, use the express.static built-in middleware function in Express.

The function signature is:

express.static(root, [options])
The root argument specifies the root directory from which to serve static assets.

For example:
app.use(express.static('public'))
Now, you can load the files that are in the public directory:

http://localhost:3000/images/kitten.jpg
http://localhost:3000/css/style.css
http://localhost:3000/js/app.js
http://localhost:3000/images/bg.png
http://localhost:3000/hello.html

Express looks up the files relative to the static directory, so the name of the static directory is not part of the URL.

To use multiple static assets directories, call the express.static middleware function multiple times:

app.use(express.static('public'))
app.use(express.static('files'))

Express looks up the files in the order in which you set the static directories with the express.static middleware function.

NOTE: For best results, use a reverse proxy cache to improve performance of serving static assets.

To create a virtual path prefix (where the path does not actually exist in the file system) for files that are served by the express.static function, specify a mount path for the static directory, as shown below:

app.use('/static', express.static('public'))
Now, you can load the files that are in the public directory from the /static path prefix.

http://localhost:3000/static/images/kitten.jpg
http://localhost:3000/static/css/style.css
http://localhost:3000/static/js/app.js
http://localhost:3000/static/images/bg.png
http://localhost:3000/static/hello.html

However, the path that you provide to the express.static function is relative to the directory from where you launch your node process. If you run the express app from another directory, it’s safer to use the absolute path of the directory that you want to serve:

app.use('/static', express.static(path.join(__dirname, 'public')))

<h2>express dynamic content</h2>
<a href="Node.js dynamic content.html" class="whitebut ">Node.js dynamic content</a>

<h2>Socket.io with multiple clients</h2>
<a href="https://stackoverflow.com/questions/38999157/using-socket-io-with-multiple-clients-connecting-to-same-server/38999513" class="whitebut ">Socket.io with multiple clients connecting to same server</a>

Server side:
// you have your socket ready and inside the on('connect'...) you handle a register event where the client passes an id if one exists else you create one.

// a client requests registration
socket.on('register', function(clientUuid){
      // create an id if client doesn't already have one
      var id = clientUuid == null? uuid.v4() : clientUuid; 
      var nsp;
      var ns = "/" + id;
      socket.join(id);

      // create a room using this id only for this client
      var nsp = app.io.of(ns);
      // save it to a dictionary for future use
      clientToRooms[ns] = nsp;

      // set up what to do on connection
      nsp.on('connection', function(nsSocket){
        console.log('someone connected');

        nsSocket.on('Info', function(data){
          // just an example
        });
      });

Client side:
// you already have declared uuid, uuidSocket and have connected to the socket previously so you define what to do on register:
    socket.on("register", function(data){
      if (uuid == undefined || uuidSocket == undefined) {// first time we get id from server
        //save id to a variable
        uuid = data.uuid;

        // save to localstorage for further usage (optional - only if you want one client per browser e.g.)
        localStorage.setItem('socketUUID', uuid);

        uuidSocket = io(serverHost + "/" + uuid); // set up the room --> will trigger nsp.on('connect',... ) on the server

        uuidSocket.on("Info", function(data){
          //handle on Info
        });

// initiate the register from the client
 socket.emit("register", uuid);

<h2>Send broadcast to all connected client in node js</h2>
var WebSocketServer = require("ws").Server;
var wss = new WebSocketServer({port:8100});

wss.on('connection', function connection(ws) {
    ws.on('message', function(message) {
       wss.broadcast(message);
    }
}

wss.broadcast = function broadcast(msg) {
   console.log(msg);
   wss.clients.forEach(function each(client) {
       client.send(msg);
    });
};

<h2>socket.io broadcast to all connected sockets</h2>
server:
//emit only to the socket that the sender is connected to.
socket.on('target', function(index){
    //Hard coded answers
    var solution = "43526978";
    console.log('index: ' + solution[index]);
    socket.emit('targetResult', solution[index]);
});

In order to emit to everyone, use the following syntax:
socket.on('target', function(index){
    //Hard coded answers
    var solution = "43526978";
    console.log('index: ' + solution[index]);
    io.sockets.emit('targetResult', solution[index]);
});

Notice that, changed socket.emit to io.sockets.emit.
This makes socket.io broadcast to all connected sockets.

<h2>Real Time Applications with Socket.io</h2>
https://www.rithmschool.com/courses/intermediate-node-express/real-time-applications
io.on('connection', function(socket){
  console.log("connection!");
  io.sockets.emit('from server', 'HELLO!');
  socket.on('from client', function(data){
    console.log(data);
  });
});

http.listen(3000, function(){
  console.log('listening on localhost:3000');
});

client

&lt;!DOCTYPE html>
&lt;html lang="en">
&lt;head>
  &lt;meta charset="UTF-8">
  &lt;title>First Socket.io application&lt;/title>
&lt;/head>
&lt;body>
  &lt;script src="/socket.io/socket.io.js">&lt;/script>
  &lt;script>
    var socket = io();

    socket.on('from server', function (data) {
      console.log(data);
      socket.emit('from client', 'WORLD!');
    });
  &lt;/script>
&lt;/body>
&lt;/html>

The client creates a websocket connection with the server (var socket = io()).

Whenever a connection is created, the server receives a 'conncection' event. 

In our application, this causes the server to send to all connected websockets a 'from server' event, with a message of 'HELLO!'.

The client is set up to receive 'from server' events. 

When it receives such a request, it console lots the data, then emits its own event, called 'from client', with data of 'WORLD!'.

One the server receives a 'from client' event, it logs the corresponding data to the terminal.
One thing to note is that when a client connects, the 'from server' event gets emitted to all websocket connections (we emit on io.sockets, not socket). 

You can verify that all clients receive the event by going to localhost:3000 on two separate tabs. 

The first tab should have 'HELLO!' logged twice: once when it connected to the server, and once when the other tab connected!

<h3>Different kinds of messages and rooms</h3>
When using Socket.io, there are different types of messages you may want to send to different users. 

For managing chat rooms, socket.io has the idea of a room which has its own name and each socket has its own id to ensure private messages can work.

Here are the helpful methods for sending certain types of messages to certain users:

io.emit('name of event');
or
io.sockets.emit('name of event'); - sends to everyone in every room, including the sender

io.to('name of room').emit('name of event'); - sends to everyone including the sender, in a room (the first parameter to to)

socket.broadcast.to('name of room').emit('name of event'); - sends to everyone except the sender in a room (the first parameter to to)

socket.emit('name of event') - sends to the sender and no one else

socket.broadcast.to(someOtherSocket.id).emit(); - Send to specific socket only (used for private chat)

<h3>Define what a mailer is</h3>
Use nodemailer to send users emails
mailers
Another very common task when building backend applications is sending email to users. 
This requires setting up an email server and configuring it with your transactional mail provider (Mandrill, SendGrid, Gmail etc.). 
To get started sending mail to your users, check out Nodemailer.

A sample application to send emails
Since Gmail is not the easiest to configure and Mandrill and SendGrid do not have a free tier, we will be using mailgun to set up transactional email. 
You can create a free account here.

Let's now imagine that we want to send some information to a user when a form is submitted. 
Here is what that configuration might look like:

require('dotenv').load();

var express = require("express");
var app = express();
var bodyParser = require("body-parser");
var nodemailer = require('nodemailer');
var mg = require('nodemailer-mailgun-transport');

app.set("view engine", "pug");
app.use(bodyParser.urlencoded({extended:true}));

var auth = {
  auth: {
    api_key: process.env.SECRET_KEY,
    domain: process.env.DOMAIN
  }
}

var nodemailerMailgun = nodemailer.createTransport(mg(auth));

app.get("/", function(req, res, next){
  res.render("index");
});

app.get("/new", function(req, res, next){
  res.render("new");
});

app.post('/', function(req, res, next){
    var mailOpts = {
        from: 'elie@yourdomain.com',
        to: req.body.to,
        subject: req.body.subject,
        text : 'test message form mailgun',
        html : '&lt;b>test message form mailgun&lt;/b>'
    };

    nodemailerMailgun.sendMail(mailOpts, function (err, response) {
        if (err) res.send(err);
        else {
          res.send('email sent!');
        }
    });
});

app.listen(3000, function(){
  console.log("Server is listening on port 3000");
});
As an exercise, try to work with this code to create an application that sends email!

Define what web scraping is
Use cheerio to scrape data from a website
Web Scraping
Web scraping is the process of downloading and extracting data from a website. 
There are 3 main steps in scraping:

Downloading the HTML document from a website (we will be doing this with the request module)
Extracting data from the downloaded HTML (we will be doing this with cheerio)
Doing something with the data (usually saving it somehow, e.g. 
by writing to a file with fs or saving to a database)
Typically, you would want to access the data using a website's API, but often websites don't provide this programmatic access. 
When a website doesn't provide a programmatic way to download data, web scraping is a great way to solve the problem!

Robots.txt
Before you begin web scraping, it is a best practice to understand and honor a site's robots.txt file. 
The file may exist on any website that you visit and its role is to tell programs (like our web scraper) about rules on what it should and should not download on the site. 
Here is Rithm School's robots.txt file. 
As you can see, it doesn't provide any restrictions. 
Compare that file to Craigslist's robots.txt file which is much more restrictive on what can be downloaded by a program.

You can find out more information about the robots.txt file here.

Using cheerio
Cheerio is one of the many modules Node has for web scraping, but it is by far the easiest to get up and running with especially if you know jQuery! The library is based off of jQuery and has identical functions for finding, traversing and manipulating the DOM. 
However, cheerio expects you to have an HTML page which it will load for you to work with. 
In order to retrieve the page, we need to make an HTTP request to get the HTML and we will be using the request module to do that. 
Let's start with a simple application:

mkdir scraping_example && cd scraping_example
touch app.js
npm init -y
npm install --save cheerio request
Now in our app.js, let's scrape the first page of Craigslist:

var cheerio = require("cheerio");
var request = require("request");

request('https://sfbay.craigslist.org/search/apa?bedrooms=1&bathrooms=1&availabilityMode=0', function(err, response, body){
    var $ = cheerio.load(body);
    // let's see the average price of 1 bedroom and bathroom in san francisco (based on 1 page of craigslist...)
    var avg = Array.from($(".result-price")).reduce(function(acc,next){
        return acc + parseInt($(next).text().substr(1));
    }, 0) / $(".result-price").length;
    console.log(`Average 1 bedroom price: \$${avg.toFixed(2)}`)
});
In the terminal, if you run node app.js, it should tell you what the average price of a one-bedroom apartment is in the Bay Area!

Define what a background job is
Explain what redis is and its use in background jobs
Use kue to run background jobs
Background jobs
Another common issue when building applications is ensuring that long processes or tasks are not blocking or slowing down the entire application. 
This could happen when many emails are being sent, large files are being uploaded, or when you want to execute a process and you know there will be less traffic. 
Background job library often involve using another data store (usually a queue) to handle the order and management of jobs being processed. 
Kue is a very common tool (written by the same people who made Mongoose!) for handling background jobs. 
You can read more about it here

Getting started with kue
To get started with kue we need to npm install --save kue and require the kue module and create a queue (which is backed by an in memory data store called redis)

var kue = require('kue');
var queue = kue.createQueue();
Once you have created the queue - it's time to queue up some tasks! These tasks can be time consuming web scraping, gathering analytics, making bulk database writes, uploading files or sending emails.

function sendEmail(title,to,subject,message, done){
    var email = queue.create('email', {title, to, subject, message});
    done();
}

router.post('/', function(req, res, next) {
  queue.process('email', function(job, done){
    const {title,to,subject,message} = req.body;
    // use nodemailer or another tool to send an email
    sendEmail(title,to,subject,message);
  });
});
Kue UI
Kue also ships with a nice package called kue-dashboard which provides an interface for you to see jobs running, stalled, failed, completed and much more. 
You can access it by starting a new server with node_modules/kue/bin/kue-dashboard -p 3001

<h2>Node.js Websocket Examples with Socket.io</h2>
<h3>What are Websockets?</h3>
Over the past few years, a new type of communication started to emerge on the web and in mobile apps, called <a target="_blank" href="https://en.wikipedia.org/wiki/WebSocket">websockets</a>. 

This new protocol opens up a much faster and more efficient line of communication to the client. 
Like HTTP, websockets run on top of a TCP connection, but they're much faster because we don't have to open a new connection for each time we want to send a message since the connection is kept alive for as long as the server or client wants.

Even better, since the connection never dies we finally have full-duplex communication available to us, meaning we can <em>push data to the client instead of having to wait for them to ask for data from the server</em>. 
This allows for data to be communicated back and forth, which is ideal for things like real-time chat applications, or even games.

<h3>Some Websocket Examples</h3>
Of the many different websocket libraries for Node.js available to us, I chose to use <a target="_blank" href="https://github.com/socketio/socket.io">socket.io</a> throughout this article because it seems to be the most popular and is, in my opinion, the easiest to use. 
While each library has its own unique API, they also have many similarities since they're all built on top of the same protocol, so hopefully you'll be able to translate the code below to any library you want to use.

For the HTTP server, I'll be using <a target="_blank" href="https://expressjs.com/">Express</a>, which is the most popular Node server out there. 
Keep in mind that you can also just use the plain <a target="_blank" href="https://nodejs.org/api/http.html">http</a> module if you don't need all of the features of Express. 
Although, since most applications will use Express, that's what we'll be using as well.

<i>Note</i>: Throughout these examples I have removed much of the boilerplate code, so some of this code won't work out of the box. 
In most cases you can refer to the first example to get the boilerplate code.

<h4>Establishing the Connection</h4>
In order for a connection to be established between the client and server, the server must do two things:


<li>Hook in to the HTTP server to handle websocket connections</li>
<li>Serve up the <code>socket.io.js</code> client library as a static resource</li>

In the code below, you can see item (1) being done on the 3rd line. 
Item (2) is done for you (by default) by the <code>socket.io</code> library and is served on the path <code>/socket.io/socket.io.js</code>. 
By default, all websocket connections and resources are served within the <code>/socket.io</code> path.

<i>Server</i>

<code>var app = require('express')();
var server = require('http').Server(app);
var io = require('socket.io')(server);

app.get('/', function(req, res) {
    res.sendFile(__dirname + '/index.html');
});

server.listen(8080);
</code>

The client needs to do two things as well:


<li>Load the library from the server</li>
<li>Call <code>.connect()</code> to the server address and websocket path</li>

<i>Client</i>

<code>&lt;script src=&quot;/socket.io/socket.io.js&quot;>&lt;/script>
&lt;script>
    var socket = io.connect('/');
&lt;/script>
</code>

If you navigate your browser to <code>http://localhost:8080</code> and inspect the HTTP requests behind the scenes using your browser's developer tools, you should be able to see the handshake being executed, including the GET requests and resulting HTTP 101 Switching Protocols response.

<h4>Sending Data from Server to Client</h4>
Okay, now on to some of the more interesting parts. 
In this example we'll be showing you the most common way to send data from the server to the client. 
In this case, we'll be sending a message to a channel, which can be subscribed to and received by the client. 
So, for example, a client application might be listening on the 'announcements' channel, which would contain notifications about system-wide events, like when a user joins a chat room.

On the server this is done by waiting for the new connection to be established, then by calling the <code>socket.emit()</code> method to send a message to all connected clients.

<i>Server</i>

<code>io.on('connection', function(socket) {
    socket.emit('announcements', { message: 'A new user has joined!' });
});
</code>

<i>Client</i>

<code>&lt;script src=&quot;/socket.io/socket.io.js&quot;>&lt;/script>
&lt;script>
    var socket = io.connect('/');
    socket.on('announcements', function(data) {
        console.log('Got announcement:', data.message);
    });
&lt;/script>
</code>

<h4>Sending Data from Client to Server</h4>
But what would we do when we want to send data the other way, from client to server? It is very similar to the last example, using both the <code>socket.emit()</code> and <code>socket.on()</code> methods.

<i>Server</i>

<code>io.on('connection', function(socket) {
    socket.on('event', function(data) {
        console.log('A client sent us this dumb message:', data.message);
    });
});
</code>

<i>Client</i>

<code>&lt;script src=&quot;/socket.io/socket.io.js&quot;>&lt;/script>
&lt;script>
    var socket = io.connect('/');
    socket.emit('event', { message: 'Hey, I have an important message!' });
&lt;/script>
</code>

<h4>Counting Connected Users</h4>
This is a nice example to learn since it shows a few more features of <code>socket.io</code> (like the <code>disconnect</code> event), it's easy to implement, and it is applicable to many webapps. 
We'll be using the <code>connection</code> and <code>disconnect</code> events to count the number of active users on our site, and we'll update all users with the current count.

<i>Server</i>

<code>var numClients = 0;

io.on('connection', function(socket) {
    numClients++;
    io.emit('stats', { numClients: numClients });

    console.log('Connected clients:', numClients);

    socket.on('disconnect', function() {
        numClients--;
        io.emit('stats', { numClients: numClients });

        console.log('Connected clients:', numClients);
    });
});
</code>

<i>Client</i>

<code>&lt;script src=&quot;/socket.io/socket.io.js&quot;>&lt;/script>
&lt;script>
    var socket = io.connect('/');
    socket.on('stats', function(data) {
        console.log('Connected clients:', data.numClients);
    });
&lt;/script>
</code>

A much simpler way to track the user count on the server would be to just use this:

<code>var numClients = io.sockets.clients().length;
</code>

But apparently there are <a target="_blank" href="https://github.com/socketio/socket.io/issues/463">some issues</a> surrounding this, so you might have to keep track of the client count yourself.

<h4>Rooms and Namespaces</h4>
Chances are as your application grows in complexity, you'll need more customization with your websockets, like sending messages to a specific user or set of users. 
Or maybe you want need strict separation of logic between different parts of your app. 
This is where rooms and namespaces come in to play.

<i>Note</i>: These features are not part of the websocket protocol, but added on top by <code>socket.io</code>.

By default, <code>socket.io</code> uses the root namespace (<code>/</code>) to send and receive data. 
Programmatically, you can access this namespace via <code>io.sockets</code>, although many of its methods have shortcuts on <code>io</code>. 
So these two calls are equivalent:

<code>io.sockets.emit('stats', { data: 'some data' });
io.emit('stats', { data: 'some data' });
</code>

To create your own namespace, all you have to do is the following:

<code>var iosa = io.of('/stackabuse');
iosa.on('connection', function(socket){
    console.log('Connected to Stack Abuse namespace'):
});
iosa.emit('stats', { data: 'some data' });
</code>

Also, the client must connect to your namespace explicitly:

<code>&lt;script src=&quot;/socket.io/socket.io.js&quot;>&lt;/script>
&lt;script>
    var socket = io('/stackabuse');
&lt;/script>
</code>

Now any data sent within this namespace will be separate from the default <code>/</code> namespace, regardless of which channel is used.

Going even further, within each namespace you can join and leave 'rooms'. 
These rooms provide another layer of separation on top of namespaces, and since a client can <em>only be added to a room on the server side</em>, they also provide some extra security. 
So if you want to make sure users aren't snooping on certain data, you can use a room to hide it.

To be added to a room, you must <code>.join()</code> it:

<code>io.on('connection', function(socket){
    socket.join('private-message-room');
});
</code>

Then from there you can send messages to everyone belonging to the given room:

<code>io.to('private-message-room').emit('some event');
</code>

And finally, call <code>.leave()</code> to stop getting event messages from a room:

<code>socket.leave('private-message-room');
</code>

<h3>Conclusion</h3>
This is just one library that implements the websockets protocol, and there are many more out there, all with their own unique features and strengths. 
I'd advise trying out some of the others (like <a target="_blank" href="https://www.npmjs.com/package/websocket">node-websockets</a>) so you get a feel for what's out there.

Within just a few lines, you can create some pretty powerful applications, so I'm curious to see what you can come up with!

<h2><span class="orange">Built-in HTTP Module</span></h2>
<h3>Read the Query String</h3>
The req argument has a property called "url" which holds the part of the url that comes after the domain name:

var http = require('http');
http.createServer(function (req, res) {
  res.writeHead(200, {'Content-Type': 'text/html'});
  res.write(req.url);
  res.end();
}).listen(8080);

when opening two addresses should see two different results:

http://localhost:8080/summer
Will produce this result:
/summer

http://localhost:8080/winter
Will produce this result:
/winter

<h2><span class="orange">Split the Query String</span></h2>
built-in modules to split the query string into readable parts, such as the URL module.

var http = require('http');
var url = require('url');

http.createServer(function (req, res) {
  res.writeHead(200, {'Content-Type': 'text/html'});
  var q = url.parse(req.url, true).query;
  var txt = q.year + " " + q.month;
  res.end(txt);
}).listen(8080);

The address:
http://localhost:8080/?year=2017&month=July
Will produce this result:
2017 July

<h2><span class="orange">Single Web Socket Connection per User</span></h2>
Recently, I have been working on a real-time multi-player browser game and ran into the “single-session” problem. 
Essentially, I wanted to prevent a user from connecting more than once via web sockets. 
This is important because being logged on to the same account multiple times could create unfair scenarios and makes the server logic more complex. 
Since web socket connections are long lived, I needed to find a way to prevent this.
<h2><span class="brown">Wish list</span></h2>
A user can only be connected once, no matter how many browser tabs they have open. 
A user can be identified via their authentication token.
The system must work in a clustered environment. 
Individual server nodes should be able to go down without affecting the rest of the system.
Authorization tokens should not be passed via query parameters, instead via a dedicated authentication event after the connection is established.

For this project we will use Node.js, Socket.IO, and Redis.
<h2><span class="brown">Humble Beginnings</span></h2>Let’s set up our project and get this show on the road. 
You can check out the full GitHub <a href="https://github.com/mariotacke/blog-single-user-websocket?ref=hackernoon.com" target="_blank" rel="noopener noreferrer ugc">repo here</a>. 
First, we will set up our Socket.IO server to accept connections from the front-end.
<code>
<span class="purple">const</span> http = <span class="orange">require</span>(<span class="lime">'http'</span>);
<span class="purple">const</span> io = <span class="orange">require</span>(<span class="lime">'socket.io'</span>)();

<span class="purple">const</span> PORT = process.env.PORT || <span class="orange">9000</span>;
<span class="purple">const</span> server = http.createServer();

io.attach(server);

io.on(<span class="lime">'connection'</span>, <span class="hljs-function">(<span class="orange">socket</span>) =></span> {
  <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> connected.`</span>);

  socket.on(<span class="lime">'disconnect'</span>, <span class="hljs-function"><span class="orange">()</span> =></span> {
    <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> disconnected.`</span>);
  });
});

server.listen(PORT);
</code>

(<em>A Socket.IO server in its simplest form</em>)
By default, the server will listen on port 9000 and echo the connection status of each client to the console. 
Socket.IO provides a built-in mechanism to generate a unique socket id which we will use to identify our client’s socket connection.
Next, we create a sample page to connect to our server. 
This page consists of a status display, an input box for our secret token (we will use it for authentication down the road) and buttons to connect and disconnect.
<code><span class="orange">&lt;!DOCTYPE html></span>
<span class="pink">&lt;<span class="pink">html</span>></span>
<span class="pink">&lt;<span class="pink">head</span>></span>
  <span class="pink">&lt;<span class="pink">meta</span> <span class="hljs-attr">charset</span>=<span class="lime">"utf-8"</span> /></span>
  <span class="pink">&lt;<span class="pink">title</span>></span>Single User Websocket<span class="pink">&lt;/<span class="pink">title</span>></span>
  <span class="pink">&lt;<span class="pink">meta</span> <span class="hljs-attr">name</span>=<span class="lime">"viewport"</span> <span class="hljs-attr">content</span>=<span class="lime">"width=device-width, initial-scale=1"</span>></span>
  <span class="pink">&lt;<span class="pink">script</span> <span class="hljs-attr">src</span>=<span class="lime">"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.1.1/socket.io.js"</span>></span><span class="null"></span><span class="pink">&lt;/<span class="pink">script</span>></span>
  <span class="pink">&lt;<span class="pink">script</span> <span class="hljs-attr">src</span>=<span class="lime">"index.js"</span>></span><span class="null"></span><span class="pink">&lt;/<span class="pink">script</span>></span>
<span class="pink">&lt;/<span class="pink">head</span>></span>
<span class="pink">&lt;<span class="pink">body</span>></span>
  <span class="pink">&lt;<span class="pink">h1</span>></span>Single User Websocket Demo<span class="pink">&lt;/<span class="pink">h1</span>></span>
  <span class="pink">&lt;<span class="pink">p</span>></span>
    <span class="pink">&lt;<span class="pink">label</span> <span class="hljs-attr">for</span>=<span class="lime">"status"</span>></span>Status: <span class="pink">&lt;/<span class="pink">label</span>></span>
    <span class="pink">&lt;<span class="pink">input</span> <span class="hljs-attr">type</span>=<span class="lime">"text"</span> <span class="hljs-attr">id</span>=<span class="lime">"status"</span>
      <span class="hljs-attr">name</span>=<span class="lime">"status"</span> <span class="hljs-attr">value</span>=<span class="lime">"Disconnected"</span>
      <span class="hljs-attr">readonly</span>=<span class="lime">"readonly"</span> <span class="hljs-attr">style</span>=<span class="lime">"width: 300px;"</span>
    /></span>
  <span class="pink">&lt;/<span class="pink">p</span>></span>
  <span class="pink">&lt;<span class="pink">p</span>></span>
    <span class="pink">&lt;<span class="pink">label</span> <span class="hljs-attr">for</span>=<span class="lime">"token"</span>></span>My Token: <span class="pink">&lt;/<span class="pink">label</span>></span>
    <span class="pink">&lt;<span class="pink">input</span> <span class="hljs-attr">type</span>=<span class="lime">"text"</span> <span class="hljs-attr">id</span>=<span class="lime">"token"</span> <span class="hljs-attr">name</span>=<span class="lime">"token"</span> <span class="hljs-attr">value</span>=<span class="lime">"secret token"</span> /></span>
  <span class="pink">&lt;/<span class="pink">p</span>></span>
  <span class="pink">&lt;<span class="pink">p</span>></span>
    <span class="pink">&lt;<span class="pink">button</span> <span class="hljs-attr">id</span>=<span class="lime">"connect"</span> <span class="hljs-attr">onclick</span>=<span class="lime">"connect()"</span>></span>
      Connect
    <span class="pink">&lt;/<span class="pink">button</span>></span>
    <span class="pink">&lt;<span class="pink">button</span> <span class="hljs-attr">id</span>=<span class="lime">"disconnect"</span> <span class="hljs-attr">onclick</span>=<span class="lime">"disconnect()"</span> <span class="hljs-attr">disabled</span>></span>
      Disconnect
    <span class="pink">&lt;/<span class="pink">button</span>></span>
  <span class="pink">&lt;/<span class="pink">p</span>></span>
<span class="pink">&lt;/<span class="pink">body</span>></span>
<span class="pink">&lt;/<span class="pink">html</span>></span></code>

<em>(Sample front-end mark-up with inputs and buttons to connect and disconnect)</em>
Also, we need to set up some very rudimentary logic to perform the connect/disconnect and hook up our status and token inputs.
<code><span class="purple">const</span> socketUrl = <span class="lime">'http://localhost:9000'</span>;

<span class="purple">let</span> connectButton;
<span class="purple">let</span> disconnectButton;
<span class="purple">let</span> socket;
<span class="purple">let</span> statusInput;
<span class="purple">let</span> tokenInput;

<span class="purple">const</span> connect = <span class="hljs-function"><span class="orange">()</span> =></span> {
  socket = io(socketUrl, {
    autoConnect: <span class="orange">false</span>,
  });

  socket.on(<span class="lime">'connect'</span>, <span class="hljs-function"><span class="orange">()</span> =></span> {
    <span class="orange">console</span>.log(<span class="lime">'Connected'</span>);
    statusInput.value = <span class="lime">'Connected'</span>;
    connectButton.disabled = <span class="orange">true</span>;
    disconnectButton.disabled = <span class="orange">false</span>;
  });

  socket.on(<span class="lime">'disconnect'</span>, <span class="hljs-function">(<span class="orange">reason</span>) =></span> {
    <span class="orange">console</span>.log(<span class="lime">`Disconnected: <span class="hljs-subst">${reason}</span>`</span>);
    statusInput.value = <span class="lime">`Disconnected: <span class="hljs-subst">${reason}</span>`</span>;
    connectButton.disabled = <span class="orange">false</span>;
    disconnectButton.disabled = <span class="orange">true</span>;
  })

  socket.open();
};

<span class="purple">const</span> disconnect = <span class="hljs-function"><span class="orange">()</span> =></span> {
  socket.disconnect();
}

<span class="orange">document</span>.addEventListener(<span class="lime">'DOMContentLoaded'</span>, <span class="hljs-function"><span class="orange">()</span> =></span> {
  connectButton = <span class="orange">document</span>.getElementById(<span class="lime">'connect'</span>);
  disconnectButton = <span class="orange">document</span>.getElementById(<span class="lime">'disconnect'</span>);
  statusInput = <span class="orange">document</span>.getElementById(<span class="lime">'status'</span>);
  tokenInput = <span class="orange">document</span>.getElementById(<span class="lime">'token'</span>);
});</code>

<em>(Our basic front-end logic… for now)</em>
This is everything you need to set up a basic web socket client and server. 
At this moment, we can connect, disconnect, and log the connection status to the user. 
And all of this in vanilla JavaScript too! 🍻 Next up: authenticating users.
<h2><span class="brown">Authentication</span></h2>Letting users connect without knowing who they are is of little use to us. 
Let’s add basic token authentication to the connection. 
We assume that the connection uses SSL/TLS once deployed. 
Never use an unencrypted connection. 
Ever. 
😶
At this point we have a few options: a) append a user’s token to the query string when they are connecting, or b) let any user connect and require them to send an authentication message after they connect. 
The Web Socket protocol specification (<a href="https://tools.ietf.org/html/rfc6455?ref=hackernoon.com#section-10.5" target="_blank" rel="noopener noreferrer ugc">RFC 6455</a>) does not prescribe a particular way for authentication and it does not allow for custom headers, and since query parameters could be logged by the server, I chose option b) for this example.
We will implement the authentication with <code>socketio-auth</code>
 by <a href="https://medium.com/@facundo.olano?ref=hackernoon.com" target="_blank" rel="noopener noreferrer ugc">Facundo Olano</a>, an Auth module for Socket.IO which allows us to prompt the client for a token <i>after </i>they connect. 
Should the user not provide it within a certain amount of time, we will close the connection from the server.
<code><span class="purple">const</span> http = <span class="orange">require</span>(<span class="lime">'http'</span>);
<span class="purple">const</span> io = <span class="orange">require</span>(<span class="lime">'socket.io'</span>)();
<span class="purple">const</span> socketAuth = <span class="orange">require</span>(<span class="lime">'socketio-auth'</span>);

<span class="purple">const</span> PORT = process.env.PORT || <span class="orange">9000</span>;
<span class="purple">const</span> server = http.createServer();

io.attach(server);

<span style="color:#969896">// dummy user verification</span>
<span class="purple">async</span> <span class="hljs-function"><span class="purple">function</span> <span style="color:#7aa6da">verifyUser</span> (<span class="orange">token</span>) </span>{
  <span class="purple">return</span> <span class="purple">new</span> <span class="orange">Promise</span>(<span class="hljs-function">(<span class="orange">resolve, reject</span>) =></span> {
    <span style="color:#969896">// setTimeout to mock a cache or database call</span>
    setTimeout(<span class="hljs-function"><span class="orange">()</span> =></span> {
      <span style="color:#969896">// this information should come from your cache or database</span>
      <span class="purple">const</span> users = [
        {
          <span class="hljs-attr">id</span>: <span class="orange">1</span>,
          <span class="hljs-attr">name</span>: <span class="lime">'mariotacke'</span>,
          <span class="hljs-attr">token</span>: <span class="lime">'secret token'</span>,
        },
      ];

      <span class="purple">const</span> user = users.find(<span class="hljs-function">(<span class="orange">user</span>) =></span> user.token === token);

      <span class="purple">if</span> (!user) {
        <span class="purple">return</span> reject(<span class="lime">'USER_NOT_FOUND'</span>);
      }

      <span class="purple">return</span> resolve(user);
    }, <span class="orange">200</span>);
  });
}

socketAuth(io, {
  <span class="hljs-attr">authenticate</span>: <span class="purple">async</span> (socket, data, callback) => {
    <span class="purple">const</span> { token } = data;

    <span class="purple">try</span> {
      <span class="purple">const</span> user = <span class="purple">await</span> verifyUser(token);

      socket.user = user;

      <span class="purple">return</span> callback(<span class="orange">null</span>, <span class="orange">true</span>);
    } <span class="purple">catch</span> (e) {
      <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> unauthorized.`</span>);
      <span class="purple">return</span> callback({ <span class="hljs-attr">message</span>: <span class="lime">'UNAUTHORIZED'</span> });
    }
  },
  <span class="hljs-attr">postAuthenticate</span>: <span class="hljs-function">(<span class="orange">socket</span>) =></span> {
    <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> authenticated.`</span>);
  },
  <span class="hljs-attr">disconnect</span>: <span class="hljs-function">(<span class="orange">socket</span>) =></span> {
    <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> disconnected.`</span>);
  },
})

server.listen(PORT);</code>

<em>(Hooking up socketio-auth with a dummy user lookup)</em>
We hook up <code>socketAuth</code>
 by passing it our <code>io</code>
 instance and configurations options in the form of three events: <code>authenticate</code>
, <code>postAuthenticate</code>
, and <code>disconnect</code>
. 
First, our <code>authenticate</code>
 event is triggered after a client connected and emits a subsequent authentication event with a user token payload. 
Should the client not send this <code>authentication</code>
 event within a configurable amount of time, <code>socketio-auth</code>
 will terminate the connection.
Once the user has sent their token, we verify it against our known users in a database. 
For example purposes, I created an async <code>verifyUser</code>
 method that mimics a real database or cache lookup. 
If the user is found, it will be returned, otherwise the promise is rejected with reason <code>USER_NOT_FOUND</code>
.
If all goes well, we invoke the callback and mark the socket as authenticated or return <code>UNAUTHORIZED</code>
 if the token is invalid.
We have to adapt our front-end code to send us the user’s token upon connection. 
We modify our <code>connect</code>
 function as follows:
<code>const connect = <span class="hljs-function"><span class="orange">()</span> =></span> {
  let error = <span class="orange">null</span>;

  socket = io(socketUrl, {
    autoConnect: <span class="orange">false</span>,
  });

  socket.<span class="orange">on</span>(<span class="lime">'connect'</span>, <span class="hljs-function"><span class="orange">()</span> =></span> {
    <span class="orange">console</span>.log(<span class="lime">'Connected'</span>);
    statusInput.value = <span class="lime">'Connected'</span>;
    connectButton.disabled = <span class="orange">true</span>;
    disconnectButton.disabled = <span class="orange">false</span>;

    socket.emit(<span class="lime">'authentication'</span>, {
      token: tokenInput.value,
    });
  });

  socket.<span class="orange">on</span>(<span class="lime">'unauthorized'</span>, <span class="hljs-function"><span class="orange">(reason)</span> =></span> {
    <span class="orange">console</span>.log(<span class="lime">'Unauthorized:'</span>, reason);

    error = reason.message;

    socket.disconnect();
  });

  socket.<span class="orange">on</span>(<span class="lime">'disconnect'</span>, <span class="hljs-function"><span class="orange">(reason)</span> =></span> {
    <span class="orange">console</span>.log(`<span class="javascript">Disconnected: ${error || reason}</span>`);
    statusInput.value = `<span class="javascript">Disconnected: ${error || reason}</span>`;
    connectButton.disabled = <span class="orange">false</span>;
    disconnectButton.disabled = <span class="orange">true</span>;
    error = <span class="orange">null</span>;
  });

  socket.open();
};</code>

<em>(Modified front-end code to emit the user authentication token upon connection)</em>
We added two things: soc<code>ket.emit('authentication', { token })</code>
 to tell the server who we are and an event listener <code>socket.on('unauthorized')</code>
 to react to rejections from our server.
Now we have a system in place that let’s us authenticate users and optionally kick them out should they not provide us a token after they initially connect.
This however still does not prevent a user from connecting twice with the same token. 
Open a separate window and try it out. 
To force a single session, our server has to smarten up. 
💡
<h2><span class="brown">Preventing Multiple Connections</span></h2>Making sure that a user is only connected once is simple enough on a single server since all connections sit in memory. 
We can simply iterate through all connected clients and compare their ids with the new client. 
This approach breaks down when we talk about clusters however. 
There is no easy way to determine if a particular user is connected or not without issuing a query across all nodes. 
With many users connecting, this creates a bottleneck. 
Surely there has to be a better way.
Enter distributed locks with Redis.
We will use Redis to lock and unlock resources, in our case: user sessions. 
Distributed locks are hard and you can read all about them <a href="https://redis.io/topics/distlock?ref=hackernoon.com" target="_blank" rel="noopener noreferrer ugc">here</a>. 
For our use case, we will implement a resource lock on a single Redis node. 
Let’s get started.
The first thing we will do is connect Socket.IO to Redis to enable pub/sub across multiple Socket.IO servers. 
We will use the <code>socket.io-redis</code>
 adapter provided by Socket.IO.
<code><span class="purple">const</span> http = require(<span class="lime">'http'</span>);
<span class="purple">const</span> io = require(<span class="lime">'socket.io'</span>)();
<span class="purple">const</span> socketAuth = require(<span class="lime">'socketio-auth'</span>);
<span class="purple">const</span> adapter = require(<span class="lime">'socket.io-redis'</span>);

<span class="purple">const</span> PORT = <span class="orange">process</span>.env.PORT || <span class="orange">9000</span>;
<span class="purple">const</span> server = http.createServer();

<span class="purple">const</span> redisAdapter = adapter({
  host: <span class="orange">process</span>.env.REDIS_HOST || <span class="lime">'localhost'</span>,
  port: <span class="orange">process</span>.env.REDIS_PORT || <span class="orange">6379</span>,
  password: <span class="orange">process</span>.env.REDIS_PASS || <span class="lime">'password'</span>,
});

io.<span class="orange">attach</span>(server);
io.adapter(redisAdapter);

<span style="color:#969896">// dummy user verification</span>
...</code>

<em>(We use the Socket.IO Redis adapter to enable pub/sub)</em>
This Redis server is used for its pub/sub functionality to coordinate events across multiple Socket.IO instances such as new sockets joining, exchanging messages, or disconnects. 
In our example, we will reuse the same server for our resource locks, though it could use a different Redis server as well.
Let’s create our Redis client as a separate module and promisify the methods so we can use <code>async</code>
 / <code>await</code>
 .
<code><span class="purple">const</span> bluebird = require(<span class="lime">'bluebird'</span>);
<span class="purple">const</span> redis = require(<span class="lime">'redis'</span>);

bluebird.promisifyAll(redis);

<span class="purple">const</span> client = redis.createClient({
  host: <span class="orange">process</span>.env.REDIS_HOST || <span class="lime">'localhost'</span>,
  port: <span class="orange">process</span>.env.REDIS_PORT || <span class="orange">6379</span>,
  password: <span class="orange">process</span>.env.REDIS_PASS || <span class="lime">'password'</span>,
});

<span class="purple">module</span>.exports = client;</code>

<em>(A sample Redis client module)</em>
Let’s talk theory for a moment. 
What is it <em>exactly</em> we are trying to achieve? We want to prevent users from having more than one concurrent web socket connection to us at any given time. 
For an online game this is important because we want to avoid users using their account for multiple games at the same time. 
Also, if we can guarantee that only a single user session per user exists, our server logic is simplified.
To make this work, we must keep track of each connection, acquire a lock, and terminate other connections should the same user try to connect again. 
To acquire a lock, we use Redis’ <code>SET</code>
 method with <code>NX</code>
 and an expiration (more on the expiration later). 
<code>NX</code>
 will make sure that we only set the key if it does not already exist. 
If it does, the command returns <code>null</code>
 . 
We can use this setup to determine if a session already exists and abort if it does.
We modify our <code>authenticate</code>
 function as follows:
<code>authenticate: <span class="purple">async</span> (socket, data, callback) => {
  <span class="purple">const</span> { token } = data;

  <span class="purple">try</span> {
    <span class="purple">const</span> user = <span class="purple">await</span> verifyUser(token);
    <span class="purple">const</span> canConnect = <span class="purple">await</span> redis
      .setAsync(<span class="lime">`users:<span class="hljs-subst">${user.id}</span>`</span>, socket.id, <span class="lime">'NX'</span>, <span class="lime">'EX'</span>, <span class="orange">30</span>);

    <span class="purple">if</span> (!canConnect) {
      <span class="purple">return</span> callback({ <span class="hljs-attr">message</span>: <span class="lime">'ALREADY_LOGGED_IN'</span> });
    }

    socket.user = user;

    <span class="purple">return</span> callback(<span class="orange">null</span>, <span class="orange">true</span>);
  } <span class="purple">catch</span> (e) {
    <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> unauthorized.`</span>);
    <span class="purple">return</span> callback({ <span class="hljs-attr">message</span>: <span class="lime">'UNAUTHORIZED'</span> });
  }
},</code>

<em>(Modified authenticate event handler with Redis lock)</em>
Once we have verified that a user has a valid token, we attempt to acquire a lock for their session (line 6). 
If Redis can <code>SET</code>
 the key, it means that it did not previously exist. 
We also added <code>EX 30</code>
 to the command to auto-expire the lock after 30 seconds. 
This is important because our server or Redis might crash and we don’t want to lock out our users forever. 
The reason I chose 30 seconds is because Socket.IO has a default ping of 25 seconds, that is, every 25 seconds it will probe connected users to see if they are still connected. 
In the next section, we will make use of this to renew the lock.
To renew the lock, we’re going to hook into the <code>packet</code>
 event of our socket connection to intercept <code>ping</code>
 packages. 
These are received every 25 seconds by default. 
If a package is not received by then, Socket.IO will terminate the connection.
<code>postAuthenticate: <span class="purple">async</span> (socket) => {
  <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> authenticated.`</span>);

  socket.conn.on(<span class="lime">'packet'</span>, <span class="purple">async</span> (packet) => {
    <span class="purple">if</span> (socket.auth &amp;&amp; packet.type === <span class="lime">'ping'</span>) {
      <span class="purple">await</span> redis.setAsync(<span class="lime">`users:<span class="hljs-subst">${socket.user.id}</span>`</span>, socket.id, <span class="lime">'XX'</span>, <span class="lime">'EX'</span>, <span class="orange">30</span>);
    }
  });
},</code>

<em>(Hooking into the internal “packet” event of Socket.IO)</em>
We’re using the <code>postAuthenticate</code>
 event to register our <code>packet</code>
 event handler. 
Our handler then checks if the socket is authenticated via <code>socket.auth</code>
 and if the packet is of type <code>ping</code>
. 
To renew the lock, we will again use Redis’ <code>SET</code>
 command, this time with <code>XX</code>
 instead of <code>NX</code>
. 
<code>XX</code>
 states that it will only be set if it already exists. 
We use this mechanism to refresh the expiration time on the key every 25 seconds.
We can now authenticate users, acquire a lock per user id, and prevent multiple sessions from being created. 
Our locks will remain in effect as long as the clients report back to our servers every 25 seconds.
Yet, there is one use case we have overlooked: if a user closes their browser with an active connection and attempts to reconnect, they will erroneously receive an <code>ALREADY_LOGGED_IN</code>
 message. 
This is because the previous lock is still in effect. 
To properly release the lock when a user intentionally leaves our site, we must remove the lock from Redis upon disconnect.
<code>disconnect: <span class="purple">async</span> (socket) => {
  <span class="orange">console</span>.log(<span class="lime">`Socket <span class="hljs-subst">${socket.id}</span> disconnected.`</span>);

  <span class="purple">if</span> (socket.user) {
    <span class="purple">await</span> redis.delAsync(<span class="lime">`users:<span class="hljs-subst">${socket.user.id}</span>`</span>);
  }
},</code>

<em>(Removing the session lock when a user disconnects)</em>
In our <code>disconnect</code>
 event, we check whether or not the socket was authenticated and then remove the lock from Redis via the <code>DEL</code>
 command. 
This cleans up the user session lock and prepares it for the next connection.
That’s all there is to it! To see our connection flow in action, open two browser windows and click Connect in each of them with the same token; you will receive a status of <code>Disconnected: ALREADY_LOGGED_IN</code>
 on the latter. 
Exactly what we wanted. 
Time to sit back and relax. 
😅
<h2><span class="brown">Conclusion</span></h2>In this article I described a way to authenticate web socket connections and prevent multiple user sessions through the use of Node.js, Socket.IO, and Redis. 
This mechanism is stateless and works in a clustered server environment.
To get even better session control and fail over, I suggest delving deeper into distributed locks with Redis and reading about the redlock algorithm.

<h2>make HTTP requests with Axios</h2>
https://blog.logrocket.com/how-to-make-http-requests-like-a-pro-with-axios/
// a client HTTP API based on the XMLHttpRequest interface provided by browsers. 

The most common way for frontend programs to communicate with servers is through the HTTP protocol. 

The Fetch API and the XMLHttpRequest interface allows you to fetch resources and make HTTP requests.

jQuery’s $.ajax() function is a client HTTP API. 

As with Fetch, Axios is promise-based. 
It provides a more powerful and flexible feature set.

<h2>require('child_process')</h2>
https://www.freecodecamp.org/news/node-js-child-processes-everything-you-need-to-know-e69498fe970a/
Node.js Child Processes, using multiple processes

Four different ways to create a child process in Node: spawn(), fork(), exec(), and execFile().

The spawn function launches a command in a new process  that will execute the pwd command.
const { spawn } = require('child_process');
const child = spawn('pwd');

By default, the spawn function does not create a shell to execute the command we pass into it.

The exec function has one other major difference.
It buffers the command’s generated output and passes the whole output value to a callback function (instead of using streams, which is what spawn does).

example implemented with an exec function:
const { exec } = require('child_process');
exec('find . -type f | wc -l', (err, stdout, stderr) => {
  if (err) {
    console.error(`exec error: ${err}`);
    return;
  }
  console.log(`Number of files ${stdout}`);
});
Since the exec function uses a shell to execute the command, we can use the shell syntax directly here making use of the shell pipe feature.

The execFile function execute a file without using a shell.
It behaves exactly like the exec function, but does not use a shell.
On Windows, some files cannot be executed on their own, like .bat or .cmd files.
Those files cannot be executed with execFile and either exec or spawn with shell set to true is required to execute them.
If you need to execute a file without using a shell, 

The *Sync function
The functions spawn, exec, and execFile from the child_process module also have synchronous blocking versions that will wait until the child process exits.

const { 
  spawnSync, 
  execSync, 
  execFileSync,
} = require('child_process');

The fork function is a variation of the spawn function for spawning node processes.
The biggest difference between spawn and fork is that a communication channel is established to the child process when using fork, so we can use the send function on the forked process along with the global process object itself to exchange messages between the parent and forked processes.
We do this through the EventEmitter module interface.

The parent file, parent.js:

const { fork } = require('child_process');

const forked = fork('child.js');

forked.on('message', (msg) => {
  console.log('Message from child', msg);
});

forked.send({ hello: 'world' });
The child file, child.js:

process.on('message', (msg) => {
  console.log('Message from parent:', msg);
});

let counter = 0;

setInterval(() => {
  process.send({ counter: counter++ });
}, 1000);

<h2>string_decoder</h2>
decoding Buffer objects into strings.
end()	Returns what remains of the input stored in the internal buffer
write()	Returns the specified buffer as a string

Example
const { StringDecoder } = require('string_decoder');
const decoder = new StringDecoder('utf8');
const euro = Buffer.from([0xE2, 0x82, 0xAC]);
console.log(decoder.write(euro));

Decode a stream of binary data (a buffer object) into a string:
var StringDecoder = require('string_decoder').StringDecoder;
var d = new StringDecoder('utf8');
var b = Buffer('abc');
console.log(b); //write buffer
console.log(d.write(b)); // write decoded buffer;

<h2>Writing Simple Module</h2>
Write simple logging module which logs the information, warning or error to the console.

In Node.js, module should be placed in a separate JavaScript file. 
So, create a Log.js file and write the following code in it.

Log.js
var log = {
            info: function (info) { 
                console.log('Info: ' + info);
            },
            warning:function (warning) { 
                console.log('Warning: ' + warning);
            },
            error:function (error) { 
                console.log('Error: ' + error);
            }
    };

module.exports = log

In the above example of logging module, we have created an object with three functions - info(), warning() and error(). 
At the end, we have assigned this object to <b>module.exports</b>. 
The module.exports in the above example exposes a log object as a module.

The module.exports is a special object which is included in every JS file in the Node.js application by default. 
Use <b>module.exports</b> or <b>exports</b> to expose a function, object or variable as a module in Node.js.

Now, let's see how to use the above logging module in our application.

Loading Local Module
To use local modules in your application, you need to load it using require() function in the same way as core module. 
However, you need to specify the path of JavaScript file of the module.

The following example demonstrates how to use the above logging module contained in Log.js.

app.js
var myLogModule = require('./Log.js');

myLogModule.info('Node.js started');
In the above example, app.js is using log module. 
First, it loads the logging module using require() function and specified path where logging module is stored. 
Logging module is contained in Log.js file in the root folder. 
So, we have specified the path './Log.js' in the require() function. 
The '.' denotes a root folder.

The require() function returns a log object because logging module exposes an object in Log.js using module.exports. 
So now you can use logging module as an object and call any of its function using dot notation e.g myLogModule.info() or myLogModule.warning() or myLogModule.error()

Run the above example using command prompt (in Windows) as shown below.

C:\> node app.js
Info: Node.js started
Thus, you can create a local module using module.exports and use it in your application.

Let's see how to expose different types as a node module using module.exports in the next section.

<h2>Working with images</h2>
<h4>Manipulate images</h4>
<a href="https://github.com/aheckmann/gm">gm</a>
GraphicsMagick and ImageMagick are two popular tools for creating, editing, composing and converting images. 

<h4>Process images</h4>
<a href="https://github.com/lovell/sharp">Sharp</a>
Sharp claims to be four to five times faster than ImageMagick.

<h4>Generate sprite sheets</h4>
<a href="https://github.com/Ensighten/spritesmith">spritesmith</a>
Sprite sheets are bitmap files that contain many different small images (for example icons), and they are often used to reduce the overhead of downloading images and speed up overall page load. 
Generating sprite sheets manually is very cumbersome, but with spritesmith you can automate the process. 
This module takes a folder as input and combines all the images in it into one sprite sheet. 
It also generates a JSON file that contains all the coordinates for each of the images in the resulting image, which you can directly copy in your CSS code.

<h2>Dates, strings, colours</h2>
<h4>Format dates</h4>
<a href="https://github.com/moment/moment">Moment</a>
Moment.js is a great alternative to JavaScript's Date object

The standard JavaScript API already comes with the Date object for working with dates and times. 
However, this object is not very user-friendly when it comes to printing and formatting dates. 
On the other hand, Moment.js offers a clean and fluid API, and the resulting code is very readable and easy to understand. 

<code>moment()
  .add(7, 'days')
  .subtract(1, 'months')
  .year(2009)
  .hours(0)
  .minutes(0)
.seconds(0);</code>
In addition, there is an add-on available for parsing and formatting dates in different time zones.

<h4>Validate strings</h4>
<a href="https://github.com/chriso/validator.js">validator</a>
When providing forms on a web page, you always should validate the values the user inputs – not only on the client-side, but also on the server-side to prevent malicious data. 
A module that can help you here is validator.js. 
It provides several methods for validating strings, from <i>isEmail()</i> and <i>isURL()</i> to <i>isMobilePhone(</i>) or <i>isCreditCard()</i>, plus you can use it on the server- and the client-side.

<h4>colour values</h4>
<a href="https://www.github.com/bgrins/TinyColor">TinyColor</a>
Converting colour values from one format into another is one of the tasks every frontend developer needs to do once in a while. 
TinyColor2 takes care of this programmatically, and it's available for Node.js as well as for browsers. 
It provides a set of conversion methods (e.g. <i>toHexString()</i>, <i>toRGBString()</i>), as well as methods for all sorts of colour operations (e.g. <i>lighten()</i>, <i>saturate()</i>, <i>complement()</i>).

<h2>Working with different formats</h2>

<h4>Generate PDF files</h4>
<a href="https://github.com/devongovett/pdfkit">pdfkit</a>
You want to dynamically generate PDF files? Then <i>PDFKit</i> is the module you are looking for. 
It supports embedding font types, embedding images and the definition of vector graphics, either programmatically (using a Canvas-like API) or by specifying SVG paths. 
Furthermore, you can define links, include notes, highlight text and more. 
The best way to start is the interactive browser demo, which is available <a href="http://pdfkit.org/demo/browser.html">here</a>.

<h4>Process HTML files</h4>
<a href="https://github.com/cheeriojs/cheerio">cheerio</a>

Cheerio makes processing HTML on the server side much easier

Ever wanted to process HTML code on the server side and missed the jQuery utility methods? ThenCheerio is the answer. 
Although it implements only a subset of the core jQuery library, it makes processing HTML on the server side much easier. 
It is built on top of the <a href="https://github.com/fb55/htmlparser2">htmlparser2 module</a>, an HTML, XML and RSS parser. 
Plus, according to benchmarks, it's eight times faster than <a href="https://github.com/tmpvar/jsdom">jsdom</a>, another module for working with the DOM on the server side.

<h4>Process CSV files</h4>
<a href="https://github.com/wdavidw/node-csv">node-csv</a>
Node-cvg simplifies the process of working with CSV data

The CSV (comma-separated values) format is often used when interchanging table-based data. 
For example, Microsoft Excel allows you to export or import your data in that format. 
node-cvg simplifies the process of working with CSV data in JavaScript, and provides functionalities for generating, parsing, transforming and stringifying CSV. 
It comes with a callback API, a stream API and a synchronous API, so you can choose the style you prefer.

<h4>Process markdown files</h4>
<a href="https://github.com/chjj/marked">marked</a>
Markdown is a popular format when creating content for the web. 
If you ever wanted to process markdown content programmatically (i.e. 
write your own markdown editor), marked is worth a look. 
It takes a string of markdown code as input and outputs the appropriate HTML code. 
It is even possible to further customise that HTML output by providing custom renderers.

<h2>Minification</h2>
<h4>Minify images</h4>
<a href="https://github.com/imagemin/imagemin">imagemin</a>Imagemin is a brilliant module for minifying and optimising images 
A very good module for minifying and optimising images is imagemin, which can be used programmatically (via the command line), as a gulp or Grunt plugin, or through imagemin-app (a graphical application available for all of the three big OSs). 
Its plugin-based architecture means it is also very flexible, and can be extended to support new image formats.

<h4>Minify HTML</h4>
<a href="https://github.com/kangax/html-minifier">html-minifier</a>
This claims to be the best HTML minifier available 
After minifying images you should consider minifying your web app's HTML. 
The module HTMLMinifier can be used via the command line, but is also available for gulp and Grunt. 
On top of that, there are middleware solutions for integrating it into web frameworks like Koa and Express, so you can minify the HTML directly at runtime before serving it to the client via HTTP. 
According to benchmarks on the module's homepage, it is the best HTML minifier available.

<h4>Minify CSS</h4>
<a href="https://github.com/jakubpawlowicz/clean-css">clean-css</a>
As well as images and HTML, you should consider minifying the CSS you send the user. 
A very fast module in this regard is <i>clean-css</i>, which can be used both from the command line and programmatically. 
It comes with support for source maps and also provides different compatibility modes to ensure the minified CSS is compatible with older versions of IE.

<h4>Minify JavaScript</h4>
<a href="https://github.com/mishoo/UglifyJS2">UglifyJS2</a>
UglifyJS2 isn't just for minifying code, but it's very good at it 
The popular module UglifyJS2 is often used for minifying JavaScript code, but because of its parsing features, in principle you can use it to do anything related to processing JavaScript code. 
UglifyJS2 parses JavaScript code into an abstract syntax tree (an object model that represents the code) and provides a tree walker component that can be used to traverse that tree. 
Ever wanted to write your own JavaScript optimiser? Then UglifyJS2 is for you.

<h4>Minify SVG</h4>
<a href="https://github.com/svg/svgo">svgo</a>
Last but not least when it comes to minification, don't forget to minify the SVG content. 
This format has made a great comeback in the past few years, thanks to its great browser and tool support. 
Unfortunately, the SVG content that is generated by editors often contains redundant and useless information like comments and metadata.&nbsp;

With SVGO you can easily remove such information and create a minified version of your SVG content. 
The&nbsp;module has a&nbsp;plugin-based architecture, with (almost) every optimisation implemented as a separate plugin. 
As with all the other modules regarding minification, SVGO can&nbsp;be used either via the command line or programmatically.

<h2>Utilities</h2>
<h4>Log application output</h4>
<a href="https://github.com/winstonjs/winston">winston</a>
When you are dealing with complex web applications a proper logging library can be very useful to help you find runtime problems, both during development and in production. 
A very popular module in this regard is the winston library. 
It supports multiple transports, meaning you can tell winston to simply log to the console, but also to store logs in files or in databases (like CouchDB, MongoDB or Redis) or even stream them to an HTTP endpoint for further processing.

<h4>Generate fake data</h4>
<a href="https://github.com/Marak/Faker.js">Faker</a>When implementing or testing user interfaces you often need dummy data such as email addresses, user names, street addresses and phone numbers. 
That&nbsp;is where faker.js comes into play. 
This can be used either on the server side (as a module for Node.js) or on the client side, and provides a set of methods for generating fake data. 
Need a user name? Just call <i>faker.internet.userName()</i> and you get a random one. 
Need a fake company name? Call <i>faker.company.companyName()</i> and you get one. 
And there are a lot more methods for all types of data.

<h4>Send emails</h4>
<a href="https://github.com/nodemailer/nodemailer">nodemailer</a>
Nodemailer supports text and HTML content, embedded images and SSL/STARTTLS 
Programmatically sending emails is one of the features you need often when implementing websites. 
From registration confirmation, to notifying users of special events or sending newsletters, there are a lot of use cases that require you to get in touch with users.&nbsp;

The standard Node.js API does not offer such a feature, but fortunately the module Nodemailer fills this gap. 
It supports both text and HTML content, embedded images and &ndash; most importantly &ndash; it uses the secure SSL/STARTTLS protocol.

<h4>Create REST APIs</h4>
<a href="https://github.com/restify/node-restify">node-restify</a>
REST is the de facto standard when implementing web applications that make&nbsp;use of web services. 
Frameworks like Express facilitate the creation of such web services, but often come with a lot of features such as templating and rendering that &ndash; depending on the use case &ndash; you may not need. 
On the other hand, the Node.js module restify focuses on the creation and the debugging of REST APIs. 
It has a very similar API to the Connect middleware (which is the base for Express) but gives you more control over HTTP interactions and also supports DTrace for troubleshooting applications in real time.&nbsp;

<h4>Create CLI applications</h4>
<a href="https://github.com/tj/commander.js/">commander</a>
There are already tons of command line applications (CLI applications) written in Node.js to address different use cases (see, for example, the aforementioned modules for minification). 
If you want to write your own CLI application, the module Commander.js is a very good starting point. 
It provides a fluent API for defining various aspects of CLI applications like the commands, options, aliases, help and many more, and it really simplifies the process of creating applications for the command line.

<h4>Conclusion</h4>
We've only scratched the surface of the huge number of Node.js modules out there. 
JavaScript is more popular than ever before and there are new modules popping up every week. 
A good place to stay up to date is the 'most starred packages' section of the <a href="https://www.npmjs.com/">npm homepage</a> or Github's list of <a href="https://www.creativebloq.com/features/20-nodejs-modules-you-need-to-know" data-original-url="github.com/trending/javascript">trending repositories</a>.

<h2>Machine Learning</h2>
<a href="https://www.youtube.com/watch?v=9Hz3P1VgLz4" class="whitebut ">Machine Learning USING JAVASCRIPT</a>

<a href="https://24ways.org/2010/calculating-color-contrast" class="whitebut ">calculating color contrast</a>

<a href="https://codepen.io/anon/pen/NYRRQm?editors=1111" class="whitebut ">Machine Learning USING JAVASCRIPT src code</a>

<a href="https://www.youtube.com/watch?v=lvzekeBQsSo" class="whitebut ">Machine Learning recommendation engine IN THE BROWSER</a>

<a href="https://www.youtube.com/watch?v=RVMHhtTqUxc" class="whitebut ">Simple Machine Learning With JavaScript - Brain.js</a>

<a href="https://www.youtube.com/watch?v=60c4rMq-aH0" class="goldbut red bluebs">Build Your First Machine Learning AI With Neural Networks</a>

<a href="JavaScript machine learning.html" class="whitebut blue limebs whitets">JavaScript machine learning libraries</a>


<h2>node.js access localStorage<</h2>
LocalStorage is never accessible by the server.
Ever. It would be a huge security issue.

node-localstorage

var LocalStorage = require('node-localstorage').LocalStorage

localStorage = new LocalStorage('./scratch');

<h2>load json from node</h2>
https://stackabuse.com/reading-and-writing-json-files-with-node-js/

const fs = require('fs');

let rawdata = fs.readFileSync('student.json');
let student = JSON.parse(rawdata);
console.log(student);


using get-json
very simple to use:

$ npm install get-json --save

var getJSON = require('get-json')

getJSON('http://api.listenparadise.org', function(error, response){
    console.log(response);
})

<h2>Reading a tab separated data from text file</h2>
use the JavaScript split function

var r = [];
var t = "sam	tory	22;raj	kumar	24";
var v = t.split(";");

for (var i = 0; i < v.length; i++) {
  var w = v[i].split("\t");
  r.push({
    Fname: w[0], lastname: w[1], Age: w[2]
  });
}
console.log(r);

<h2>Remove empty elements from an array</h2>
var array = [0, 1, null, 2, "", 3, undefined, 3,,,,,, 4,, 4,, 5,, 6,,,,];

var result = array.filter(function (item) {
  return item != null;
});

console.log(result);

<h2>Web Scraping with a Headless Browser: Puppeteer</h2>
Headless just means there's no graphical user interface (GUI). 
Instead of interacting with visual elements the way you normally would—for example with a mouse or touch device—you automate use cases with a <a href="https://www.toptal.com/software/best-command-line-tools">command-line interface</a> (CLI).

<h3>Headless Chrome and Puppeteer</h3>

There are many web scraping tools that can be used for headless browsing, like <a href="https://zombie.js.org/">Zombie.js</a> or <a href="https://intoli.com/blog/running-selenium-with-headless-firefox/">headless Firefox using Selenium</a>. 
But today we'll be exploring headless Chrome via <a href="https://github.com/GoogleChrome/puppeteer">Puppeteer</a>, as it's a relatively newer player, released at the start of 2018. 
Editor's note: It's worth mentioning <a href="https://intoli.com/tour/1/">Intoli's Remote Browser</a>, another new player.

Puppeteer is a Node.js library which provides a high-level API to control headless Chrome or Chromium or to interact with the DevTools protocol. 
It's maintained by the Chrome DevTools team and an awesome open-source community.
<a href="https://www.toptal.com/puppeteer">Puppeteer</a>

Setup Headless Chrome and Puppeteer
npm i puppeteer --save

<h3>Using Puppeteer API for Automated Web Scraping</h3>
const puppeteer = require('puppeteer');
const url = process.argv[2];
if (!url) { throw "Please provide a URL as the first argument"; }

keep in mind that Puppeteer is a promise-based library: It performs asynchronous calls to the headless Chrome instance under the hood. 
Let's keep the code clean <a href="https://www.toptal.com/javascript/asynchronous-javascript-async-await-tutorial">by using async/await</a>. 
For that, we need to define an async function first and put all the Puppeteer code in there:

async function run () {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.goto(url);
    await page.screenshot({path: 'screenshot.png'});
    browser.close();
}
run();

Altogether, the final code looks like this:

const puppeteer = require('puppeteer');
const url = process.argv[2];
if (!url) {
    throw "Please provide URL as a first argument";
}
async function run () {
    const browser = await puppeteer.launch();
    const page = await browser.newPage();
    await page.goto(url);
    await page.screenshot({path: 'screenshot.png'});
    browser.close();
}
run();

node screenshot.js https://github.com

Explore what happens in our run() function above.

First, we launch a new headless browser instance, then we open a new page (tab) and navigate to the URL provided in the command-line argument. 
Lastly, we use Puppeteer's built-in method for taking a screenshot, and we only need to provide the path where it should be saved. 
We also need to make sure to close the headless browser after we are done with our automation.

Now that we've covered the basics, let's move on to something a bit more complex.

<h3>A Second Puppeteer Scraping Example</h3>
For the next part of our Puppeteer tutorial, let's say we want to scrape down the newest articles from Hacker News.

Create a new file named ycombinator-scraper.js and paste in the following code snippet:

const puppeteer = require('puppeteer');
function run () {
    return new Promise(async (resolve, reject) =&gt; {
        try {
            const browser = await puppeteer.launch();
            const page = await browser.newPage();
            await page.goto("https://news.ycombinator.com/");
            let urls = await page.evaluate(() =&gt; {
                let results = [];
                let items = document.querySelectorAll('a.storylink');
                items.forEach((item) =&gt; {
                    results.push({
                        url:  item.getAttribute('href'),
                        text: item.innerText,
                    });
                });
                return results;
            })
            browser.close();
            return resolve(urls);
        } catch (e) {
            return reject(e);
        }
    })
}
run().then(console.log).catch(console.error);

Okay, there's a bit more going on here compared with the previous example.

The first thing you might notice is that the run() function now returns a promise so the async prefix has moved to the promise function's definition.

We've also wrapped all of our code in a try-catch block so that we can handle any errors that cause our promise to be rejected.

And finally, we're using Puppeteer's built-in method called evaluate(). 
This method lets us run custom JavaScript code as if we were executing it in the DevTools console. 
Anything returned from that function gets resolved by the promise. 
This method is very handy when it comes to scraping information or performing custom actions.

The code passed to the evaluate() method is pretty basic JavaScript that builds an array of objects, each having url and text fields that represent the story URLs we see on <a href="https://news.ycombinator.com/">https://news.ycombinator.com/</a>.

The output of the script looks something like this (but with 30 entries, originally):

[ { url: 'https://www.nature.com/articles/d41586-018-05469-3',
    text: 'Bias detectives: the researchers striving to make algorithms fair' },
  { url: 'https://mino-games.workable.com/jobs/415887',
    text: 'Mino Games Is Hiring Programmers in Montreal' },
  { url: 'http://srobb.net/pf.html',
    text: 'A Beginner\'s Guide to Firewalling with pf' },
  // ...
  { url: 'https://tools.ietf.org/html/rfc8439',
    text: 'ChaCha20 and Poly1305 for IETF Protocols' } ]

Pretty neat, I'd say!

Okay, let's move forward. 
We only had 30 items returned, while there are many more available—they are just on other pages. 
We need to click on the “More” button to load the next page of results.

Let's modify our script a bit to add a support for pagination:

const puppeteer = require('puppeteer');
function run (pagesToScrape) {
    return new Promise(async (resolve, reject) =&gt; {
        try {
            if (!pagesToScrape) {
                pagesToScrape = 1;
            }
            const browser = await puppeteer.launch();
            const page = await browser.newPage();
            await page.goto("https://news.ycombinator.com/");
            let currentPage = 1;
            let urls = [];
            while (currentPage &lt;= pagesToScrape) {
                let newUrls = await page.evaluate(() =&gt; {
                    let results = [];
                    let items = document.querySelectorAll('a.storylink');
                    items.forEach((item) =&gt; {
                        results.push({
                            url:  item.getAttribute('href'),
                            text: item.innerText,
                        });
                    });
                    return results;
                });
                urls = urls.concat(newUrls);
                if (currentPage &lt; pagesToScrape) {
                    await Promise.all([
                        await page.click('a.morelink'),
                        await page.waitForSelector('a.storylink')
                    ])
                }
                currentPage++;
            }
            browser.close();
            return resolve(urls);
        } catch (e) {
            return reject(e);
        }
    })
}
run(5).then(console.log).catch(console.error);

Let's review what we did here:

We added a single argument called pagesToScrape to our main run() function. 
We'll use this to limit how many pages our script will scrape.

There is one more new variable named currentPage which represents the number of the page of results are we looking at currently. 
It's set to 1 initially. 
We also wrapped our evaluate() function in a while loop, so that it keeps running as long as currentPage is less than or equal to pagesToScrape.

We added the block for moving to a new page and waiting for the page to load before restarting the while loop.

You'll notice that we used the page.click() method to have the headless browser click on the “More” button. 
We also used the waitForSelector() method to make sure our logic is paused until the page contents are loaded.

Both of those are high-level Puppeteer API methods ready to use out-of-the-box.

One of the problems you'll probably encounter during scraping with Puppeteer is waiting for a page to load. 
Hacker News has a relatively simple structure and it was fairly easy to wait for its page load completion. 
For more complex use cases, Puppeteer offers a wide range of built-in functionality, which you can explore in the <a href="https://github.com/GoogleChrome/puppeteer/blob/master/docs/api.md">API documentation on GitHub</a>.

This is all pretty cool, but our Puppeteer tutorial hasn't covered optimization yet. 
Let's see how can we make Puppeteer run faster.

<h3>Optimizing Our Puppeteer Script</h3>

The general idea is to not let the headless browser do any extra work. 
This might include loading images, applying CSS rules, firing XHR requests, etc.

As with other tools, optimization of Puppeteer depends on the exact use case, so keep in mind that some of these ideas might not be suitable for your project. 
For instance, if we had avoided loading images in our first example, our screenshot might not have looked how we wanted.

Anyway, these optimizations can be accomplished either by caching the assets on the first request, or canceling the HTTP requests outright as they are initiated by the website.

Let's see how caching works first.

You should be aware that when you launch a new headless browser instance, Puppeteer creates a temporary directory for its profile. 
It is removed when the browser is closed and is not available for use when you fire up a new instance—thus all the images, CSS, cookies, and other objects stored will not be accessible anymore.

We can force Puppeteer to use a custom path for storing data like cookies and cache, which will be reused every time we run it again—until they expire or are manually deleted.

const browser = await puppeteer.launch({
    userDataDir: './data',
});

This should give us a nice bump in performance, as lots of CSS and images will be cached in the data directory upon the first request, and Chrome won't need to download them again and again.

However, those assets will still be used when rendering the page. 
In our scraping needs of Y Combinator news articles, we don't really need to worry about any visuals, including the images. 
We only care about bare HTML output, so let's try to block every request.

Luckily, Puppeteer is pretty cool to work with, in this case, because it comes with support for custom hooks. 
We can provide an interceptor on every request and cancel the ones we don't really need.

The interceptor can be defined in the following way:

await page.setRequestInterception(true);
page.on('request', (request) =&gt; {
    if (request.resourceType() === 'document') {
        request.continue();
    } else {
        request.abort();
    }
});

As you can see, we have full control over the requests that get initiated. 
We can write custom logic to allow or abort specific requests based on their resourceType. 
We also have access to lots of other data like request.url so we can block only specific URLs if we want.

In the above example, we only allow requests with the resource type of "document" to get through our filter, meaning that we will block all images, CSS, and everything else besides the original HTML response.

Here's our final code:

const puppeteer = require('puppeteer');
function run (pagesToScrape) {
    return new Promise(async (resolve, reject) =&gt; {
        try {
            if (!pagesToScrape) {
                pagesToScrape = 1;
            }
            const browser = await puppeteer.launch();
            const page = await browser.newPage();
            await page.setRequestInterception(true);
            page.on('request', (request) =&gt; {
                if (request.resourceType() === 'document') {
                    request.continue();
                } else {
                    request.abort();
                }
            });
            await page.goto("https://news.ycombinator.com/");
            let currentPage = 1;
            let urls = [];
            while (currentPage &lt;= pagesToScrape) {
                await page.waitForSelector('a.storylink');
                let newUrls = await page.evaluate(() =&gt; {
                    let results = [];
                    let items = document.querySelectorAll('a.storylink');
                    items.forEach((item) =&gt; {
                        results.push({
                            url:  item.getAttribute('href'),
                            text: item.innerText,
                        });
                    });
                    return results;
                });
                urls = urls.concat(newUrls);
                if (currentPage &lt; pagesToScrape) {
                    await Promise.all([
                        await page.waitForSelector('a.morelink'),
                        await page.click('a.morelink'),
                        await page.waitForSelector('a.storylink')
                    ])
                }
                currentPage++;
            }
            browser.close();
            return resolve(urls);
        } catch (e) {
            return reject(e);
        }
    })
}
run(5).then(console.log).catch(console.error);

<h3>Stay Safe with Rate Limits</h3>

Headless browsers are very powerful tools. 
They're able to perform almost any kind of web automation task, and Puppeteer makes this even easier. 
Despite all the possibilities, we must comply with a website's terms of service to make sure we don't abuse the system.

Since this aspect is more architecture-related, I won't cover this in depth in this Puppeteer tutorial. 
That said, the most basic way to slow down a Puppeteer script is to add a sleep command to it:

js
await page.waitFor(5000);

This statement will force your script to sleep for five seconds (5000 ms). 
You can put this anywhere before browser.close().

Just like limiting your use of third-party services, there are lots of other more robust ways to control your usage of Puppeteer. 
One example would be building a queue system with a limited number of workers. 
Every time you want to use Puppeteer, you'd push a new task into the queue, but there would only be a limited number of workers able to work on the tasks in it. 
This is a fairly common practice when dealing with third-party API rate limits and can be applied to Puppeteer web data scraping as well.

<h3>Puppeteer's Place in the Fast-moving Web</h3>

In this Puppeteer tutorial, I've demonstrated its basic functionality as a web-scraping tool. 
However, it has much wider use cases, including headless browser testing, PDF generation, and performance monitoring, among many others.

Web technologies are moving forward fast. 
Some websites are so dependent on JavaScript rendering that it's become nearly impossible to execute simple HTTP requests to scrape them or perform some sort of automation. 
Luckily, headless browsers are becoming more and more accessible to handle all of our automation needs, thanks to projects like Puppeteer and the awesome teams behind them!

<h3>Understanding the basics</h3><h3>What do you mean by &quot;headless browser&quot;?</h4>A headless browser is a web browser with no user interface (UI) whatsoever. 
Instead, it follows instructions defined by software developers in different programming languages. 
Headless browsers are mostly used for running automated quality assurance tests, or to scrape websites.
<h3>Is it legal to scrape a website?</h4>Websites often allow other software to scrape their content. 
Please refer to the robots exclusion standard (robots.txt file) of the website that you intend to scrape, as it usually describes which pages you are allowed to scrape. 
You should also check the terms of service to see if you are allowed to scrape.
<h3>What is a headless environment?</h4>Headless means that the given device or software has no user interface or input mechanism such as a keyboard or mouse. 
The term "headless environment" is more often used to describe computer software designed to provide services to other computers or servers.
<h3>What is headless Chrome?</h4>Headless Chrome is essentially the Google Chrome web browser without its graphical user interface (GUI), based on the same underlying technology. 
Headless Chrome is instead controlled by scripts written by software developers.
<h3>What is Google Puppeteer?</h4>Puppeteer is a Node.js library maintained by Chrome's development team from Google. 
Puppeteer provides a high-level API to control headless Chrome or Chromium or interact with the DevTools protocol.
<h3>Is Selenium a framework?</h4>Yes, but not a front-end web framework like Angular or React; Selenium is a software testing framework for web applications. 
Its primary use-case is to automating quality assurance tests on headless browsers, but it's often used to automate administration tasks on websites too.

Tags
<a href="https://www.toptal.com/developers/blog/tags/javascript">JavaScript</a><a href="https://www.toptal.com/developers/blog/tags/nodejs">Node.js</a><a href="https://www.toptal.com/developers/blog/tags/automation">Automation</a><a href="https://www.toptal.com/developers/blog/tags/puppeteer">Puppeteer</a><a href="https://www.toptal.com/resume/nick-chikovani">View full profile </a>Nick Chikovani
Freelance JavaScript Developer


<h2>execute an external program from within Node.js</h2>
var exec = require('child_process').exec;
exec('pwd', function callback(error, stdout, stderr) {
  // result
});

exec has memory limitation of buffer size of 512k.
In this case it is better to use spawn.
With spawn one has access to stdout of executed command at run time

var spawn = require('child_process').spawn;
var prc = spawn('java',  ['-jar', '-Xmx512M', '-Dfile.encoding=utf8', 'script/importlistings.jar']);

//noinspection JSUnresolvedFunction
prc.stdout.setEncoding('utf8');
prc.stdout.on('data', function (data) {
    var str = data.toString()
    var lines = str.split(/(\r?\n)/g);
    console.log(lines.join(""));
});

prc.on('close', function (code) {
    console.log('process exit code ' + code);
});

output may have been in stderr rather than stdout.

The simplest way is:
const { exec } = require("child_process")
exec('yourApp').unref()
unref is necessary to end your process without waiting for "yourApp"

<h2>execute an external program from within Node.js</h2>
var exec = require('child_process').exec;
exec('pwd', function callback(error, stdout, stderr) {
  // result
});

exec has memory limitation of buffer size of 512k.
In this case it is better to use spawn.
With spawn one has access to stdout of executed command at run time

var spawn = require('child_process').spawn;
var prc = spawn('java',  ['-jar', '-Xmx512M', '-Dfile.encoding=utf8', 'script/importlistings.jar']);

//noinspection JSUnresolvedFunction
prc.stdout.setEncoding('utf8');
prc.stdout.on('data', function (data) {
    var str = data.toString()
    var lines = str.split(/(\r?\n)/g);
    console.log(lines.join(""));
});

prc.on('close', function (code) {
    console.log('process exit code ' + code);
});

output may have been in stderr rather than stdout.

The simplest way is:
const { exec } = require("child_process")
exec('yourApp').unref()
unref is necessary to end your process without waiting for "yourApp"



<h2>Node.js MySQL Create Database</h2>
Creating a Database
To create a database in MySQL, use the "CREATE DATABASE" statement:

Example
Create a database named "mydb":

var mysql = require('mysql');

var con = mysql.createConnection({
  host: "localhost",
  user: "yourusername",
  password: "yourpassword"
});

con.connect(function(err) {
  if (err) throw err;
  console.log("Connected!");
  con.query("CREATE DATABASE mydb", function (err, result) {
    if (err) throw err;
    console.log("Database created");
  });
});

Save the code above in a file called "demo_create_db.js" and run the file:

Run "demo_create_db.js"

C:\Users\Your Name>node demo_create_db.js
Which will give you this result:

Connected!
Database created


<h2>scrape with Playwright</h2>
<div id="Playwrighttoc" class="toc"><a href="#Playwrighttopic-0" target="_self">The Project</a><br><a href="#Playwrighttopic-1" target="_self">Installation</a><br><a href="#Playwrighttopic-2" target="_self">Building a scraper</a><br><a href="#Playwrighttopic-3" target="_self">Loading more repositories</a><br><a href="#Playwrighttopic-4" target="_self">Extracting data</a><br><a href="#Playwrighttopic-5" target="_self">Conclusion</a><br></div></center>

Playwright is a rising star in the web scraping and automation space. 
If you thought Puppeteer was powerful, Playwright will blow your mind.

<a href="https://github.com/microsoft/playwright">Playwright</a> is a browser automation library very similar to <a href="https://github.com/puppeteer/puppeteer">Puppeteer</a>. 
Both allow you to control a web browser with only a few lines of code. 
The possibilities are endless. 
From automating mundane tasks and testing web applications to data mining.

With Playwright you can run Firefox and Safari (WebKit), not only Chromium based browsers. 
It will also save you time, because Playwright automates away repetitive code, such as waiting for buttons to appear in the page.
<blockquote>You don’t need to be familiar with Playwright, Puppeteer or web scraping to enjoy this tutorial, but knowledge of HTML, CSS and JavaScript is expected.</blockquote>
In this tutorial you’ll learn how to:

<i>Start a browser with Playwright</i>
<i>Click buttons and wait for actions</i>
<i>Extract data from a website</i>
<h3 id="Playwrighttopic-0">The Project</h3>
To showcase the basics of Playwright, we will create a simple scraper that extracts data about <a href="https://github.com/Playwrighttopics">GitHub PlaywrightTopics</a>. 
You’ll be able to select a Playwrighttopic and the scraper will return information about repositories tagged with this Playwrighttopic.
<img src="https://blog.apify.com/content/images/max/2000/1-gNQkSVS-2f4tk11yVIW4Cw.png">
The page for JavaScript GitHub PlaywrightTopic

We will use Playwright to start a browser, open the GitHub Playwrighttopic page, click the <em>Load more</em> button to display more repositories, and then extract the following information:

Owner
Name
URL
Number of stars
Description
List of repository Playwrighttopics
<h3 id="Playwrighttopic-1">Installation</h3>
To use Playwright you’ll need <a href="https://nodejs.org/">Node.js</a> version higher than 10 and a package manager. 
We’ll use <code>npm</code>, which comes preinstalled with Node.js. 
You can confirm their existence on your machine by running:

<code>node -v &amp;&amp; npm -v</code>

If you’re missing either Node.js or NPM, visit the <a href="https://docs.npmjs.com/downloading-and-installing-node-js-and-npm">installation tutorial</a> to get started.

Now that we know our environment checks out, let’s create a new project and install Playwright.

<code>mkdir playwright-scraper &amp;&amp; cd playwright-scraper
npm init -y
npm i playwright</code>
<blockquote>The first time you install Playwright, it will download browser binaries, so the installation may take a bit longer.</blockquote><h3 id="Playwrighttopic-2">Building a scraper</h3>
Creating a scraper with Playwright is surprisingly easy, even if you have no previous scraping experience. 
If you understand JavaScript and CSS, it will be a piece of cake.

In your project folder, create a file called <code>scraper.js</code> (or choose any other name) and open it in your favorite code editor. 
First, we will confirm that Playwright is correctly installed and working by running a simple script.

      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>

Show hidden characters
// Import the playwright library into our scraper.

const playwright = require('playwright');
async function main() {    // Open a Chromium browser. We use headless: false
    // to be able to watch what's going on.
    const browser = await playwright.chromium.launch({ const page = await browser.newPage({              bypassCSP: true
, // This is needed to enable JavaScript execution on GitHub.          await page.goto('https://github.com/Playwrighttopics/javascript');          await page.waitForTimeout(
10000);          await browser.close();          
}          
main();

<a href="https://gist.github.com/mnmkng/b517605fe5b4d4982cc5f1a52f56bd56#file-playwright-js">playwright.js
</a>


Now run it using your code editor or by executing the following command in your project folder.

<code>node scraper.js</code>

If you saw a Chromium window open and the GitHub PlaywrightTopics page successfully loaded, congratulations, you just robotized your web browser with Playwright!
<img src="https://blog.apify.com/content/images/max/2000/1--g0d8utMzc4sGG7ACHOt7Q.png">
JavaScript GitHub Playwrighttopic
<h3 id="Playwrighttopic-3">Loading more repositories</h3>
When you first open the Playwrighttopic page, the number of displayed repositories is limited to 30. 
You can load more by clicking the <em>Load more…</em> button at the bottom of the page.
<img src="https://blog.apify.com/content/images/max/2000/1-XgVRjVfZ7f-H4GGgAclBaQ.png">

There are two things we need to tell Playwright to load more repositories:

<i>Click</i> the <em>Load more…</em> button.
<i>Wait</i> for the repositories to load.

Clicking buttons is extremely easy with Playwright. 
By prefixing <code>text=</code> to a string you’re looking for, Playwright will find the element that includes this string and click it. 
It will also wait for the element to appear if it’s not rendered on the page yet.
<img src="https://blog.apify.com/content/images/max/2000/1-3c8mpVdheQFEGZDRhew-0A.png">
Clicking a button

This is a huge improvement over Puppeteer and it makes Playwright lovely to work with.

After clicking, we need to wait for the repositories to load. 
If we didn’t, the scraper could finish before the new repositories show up on the page and we would miss that data. 
<code><a href="https://playwright.dev/#version=v1.2.1&amp;path=docs%2Fapi.md&amp;q=pagewaitforfunctionpagefunction-arg-options">page.waitForFunction()</a></code> allows you to execute a function inside the browser and wait until the function returns <code>true</code> .
<img src="https://blog.apify.com/content/images/max/2000/1-qX1qjuy6E4mpFQDVIjHq9g.png">
Waiting for

To find that <code>article.border</code> selector, we used browser Dev Tools, which you can open in most browsers by right-clicking anywhere on the page and selecting <i>Inspect</i>. 
It means: Select the <code>&lt;article&gt;</code> tag with the <code>border</code> class.
<img src="https://blog.apify.com/content/images/2021/10/1-T4qVh-PhMS3TVLYvpT-J-Q-min.png">
Chrome Dev Tools

Let’s plug this into our code and do a test run.

      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>

    // Import the playwright library into our scraper.

    const playwright = require('playwright');

    async function main(){
        // Open a Chromium browser. We use headless: false
        // to be able to watch what's going on.

        const browser = await playwright.chromium.launch({
            headless: false
        });
        // Open a new page / tab in the browser.
        const page = await browser.newPage({
            bypassCSP: true, 
        // This is needed to enable JavaScript execution on GitHub.
        });
        // Tell the tab to navigate to the GitHub PlaywrightTopics page.

        await page.goto('https://github.com/Playwrighttopics/javascript');
        // Click and tell Playwright to keep watching for more than
        // 30 repository cards to appear in the page.

        await page.click('text=Load more');
        await page.waitForFunction(() =>{
            const repoCards = document.querySelectorAll('article.border');
            return repoCards.length > 30 ;
        });
        // Pause for 10 seconds, to see what's going on.
        await page.waitForTimeout(10000);
        // Turn off the browser to clean up after ourselves.
        await browser.close() ;
    }
    main();
        <a href="https://gist.github.com/mnmkng/0ace1f4e1035f92d8da4f4ae1080f026#file-playwright-example-2-js">
          playwright-example-2.js
        </a>

If you watch the run, you’ll see that the browser first scrolls down and clicks the <em>Load more…</em> button, which changes the text into <em>Loading more</em>. 
After a second or two, you’ll see the next batch of 30 repositories appear. 
Great job!
<h3 id="Playwrighttopic-4">Extracting data</h3>
Now that we know how to load more repositories, we will extract the data we want. 
To do this, we’ll use the <code><a href="https://playwright.dev/#version=v1.2.1&amp;path=docs%2Fapi.md&amp;q=pageevalselector-pagefunction-arg-1">page.$$eval</a></code> function. 
It tells the browser to find certain elements and then execute a JavaScript function with those elements.
<img src="https://blog.apify.com/content/images/2021/09/carbon-12-.png">
Extracting data from page

It works like this: <code>page.$$eval</code>finds our repositories and executes the provided function in the browser. 
We get <code>repoCards</code> which is an <code>Array</code> of all the repo elements. 
The return value of the function becomes the return value of the <code>page.$$eval</code> call. 
Thanks to Playwright, you can pull data out of the browser and save them to a variable in Node.js. 
Magic!

If you’re struggling to understand the extraction code itself, be sure to check out <a href="https://developer.mozilla.org/en-US/docs/Learn/CSS/Building_blocks/Selectors">this guide on working with CSS selectors</a> and <a href="https://javascript.info/searching-elements-dom#querySelectorAll">this tutorial on using those selectors to find HTML elements</a>.

And here’s the code with extraction included. 
When you run it, you’ll see 60 repositories with their information printed to the console.

      This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters.
      <a href="https://github.co/hiddenchars" target="_blank">Learn more about bidirectional Unicode characters</a>

    // Import the playwright library into our scraper.
    const playwright = require('playwright');
    async function main(){
        // Open a Chromium browser. We use headless: false
        // to be able to watch what's going on.
        const browser = awaitplaywright.chromium.launch({
            headless: false,
            devtools: true,
        });
        // Open a new page / tab in the browser.
        const page = awaitbrowser.newPage({
            bypassCSP: true, 
        // This is needed to enable JavaScript execution on GitHub.
        });
        // Tell the tab to navigate to the GitHub PlaywrightTopics page.
        awaitpage.goto('https://github.com/Playwrighttopics/javascript');
        // Click and tell Playwright to keep watching for more than
        // 30 repository cards to appear in the page.
        awaitpage.click('text=Load more');
        awaitpage.waitForFunction(() =>{
            const repoCards = document.querySelectorAll('article.border');
            return repoCards.length > 30;
        });
        // Extract data from the page. Selecting all 'article' elements
        // will return all the repository cards we're looking for.
        const repos = awaitpage.$$eval('article.border', (repoCards) =>{
            return repoCards.map(card =>{
                const [ user, repo] = card.querySelectorAll('h3 a');
                const stars = card.querySelector('a.social-count');
                const description = card.querySelector('div.px-3 > p + div');
                const Playwrighttopics = card.querySelectorAll('a.Playwrighttopic-tag');
    
                const toText = ( element) => element && element.innerText.trim();
    
                return{
                    user: toText(user),
                    repo: toText(repo),
                    url: repo.href,
                    stars: toText(stars),
                    description: toText(description),
                    Playwrighttopics: Array.from(Playwrighttopics).map((t) => toText(t)),
                };
            });
        });
        // Print the results. Nice!
        console.log(`We extracted ${ repos.length } repositories.`);
        console.dir(repos);
        // Turn off the browser to clean up after ourselves.
        awaitbrowser.close();
    }
    
    main();


        <a href="https://gist.github.com/mnmkng/3390066cd11c68a35d8998d2e4497c1c#file-playwright-example-3-js">
          playwright-example-3.js
        </a>

Conclusion
In this tutorial we learned how to start a browser with <a href="https://playwright.dev/">Playwright</a>, and control its actions with some of Playwright’s most useful functions: <code>page.click()</code> to emulate mouse clicks, <code>page.waitForFunction()</code> to wait for things to happen and <code>page.$$eval()</code> to extract data from a browser page.

But we’ve only scratched the surface of what’s possible with Playwright. 
You can log into websites, fill forms, intercept network communication, and most importantly, use almost any browser in existence. 
Where will you take this project next? How about turning it into a command-line interface (CLI) tool that takes a Playwrighttopic and number of repositories on input and outputs a file with the repositories? You can do it now.


<h2>Nodejs to parse command line arguments</h2>
Passing in arguments via the command line is an extremely basic programming task, and a necessity for anyone trying to write a simple Command-Line Interface (CLI). 
In Node.js, as in C and many related environments, all command-line arguments received by the shell are given to the process in an array called <code>argv</code> (short for 'argument values').

Node.js exposes this array for every running process in the form of <code>process.argv</code> - let's take a look at an example. 
Make a file called <code>argv.js</code> and add this line:

<code>console.log(process.argv);</code>

Now save it, and try the following in your shell:

<code>$ node argv.js one two three four five
[ 'node',
  '/home/avian/argvdemo/argv.js',
  'one',
  'two',
  'three',
  'four',
  'five' ]</code>

There you have it - an array containing any arguments you passed in. 
Notice the first two elements - <code>node</code> and the path to your script. 
These will always be present - even if your program takes no arguments of its own, your script's interpreter and path are still considered arguments to the shell you're using.

Where everyday CLI arguments are concerned, you'll want to skip the first two. 
Now try this in <code>argv.js</code>:

<code>const myArgs = process.argv.slice(2);
console.log('myArgs: ', myArgs);</code>

This yields:

<code>$ node argv.js one two three four five
myArgs:  [ 'one', 'two', 'three', 'four', 'five' ]</code>

Now let's actually do something with the args:

<code>const myArgs = process.argv.slice(2);
console.log('myArgs: ', myArgs);

switch (myArgs[0]) {
  case 'insult':
    console.log(myArgs[1], 'smells quite badly.');
    break;
  case 'compliment':
    console.log(myArgs[1], 'is really cool.');
    break;
  default:
    console.log('Sorry, that is not something I know how to do.');
}</code>

JS PRO TIP: Remember to <code>break</code> after each <code>case</code> - otherwise you'll run the next case too!

Referring to your command-line arguments by array index isn't very clean, and can quickly turn into a nightmare when you start working with flags and the like - imagine you made a server, and it needed a lot of arguments. 
Imagine having to deal with something like <code>myapp -h host -p port -r -v -b --quiet -x -o outfile</code> - some flags need to know about what comes next, some don't, and most CLIs let users specify arguments in any order they want. 
Sound like a fun string to parse?

Luckily, there are many third party modules that makes all of this trivial - one of which is <a href="https://www.npmjs.com/package/yargs">yargs</a>. 
It's available via <code>npm</code>. 
Use this command from your app's base path:

<code>npm i yargs</code>

Once you have it, give it a try - it can really be a life-saver. 
Lets test it with little fun Leap Year checker and Current Time teller

<code>const yargs = require('yargs');

const argv = yargs
  .command('lyr', 'Tells whether an year is leap year or not', {
    year: {
      description: 'the year to check for',
      alias: 'y',
      type: 'number'
    }
  })
  .option('time', {
    alias: 't',
    description: 'Tell the present Time',
    type: 'boolean'
  })
  .help()
  .alias('help', 'h').argv;

if (argv.time) {
  console.log('The current time is: ', new Date().toLocaleTimeString());
}

if (argv._.includes('lyr')) {
  const year = argv.year || new Date().getFullYear();
  if ((year % 4 == 0 &amp;&amp; year % 100 != 0) || year % 400 == 0) {
    console.log(`${year} is a Leap Year`);
  } else {
    console.log(`${year} is NOT a Leap Year`);
  }
}

console.log(argv);</code>

The last line was included to let you see how <code>yargs</code> handles your arguments. 
Here's a quick reference:


<li><code>argv.$0</code> contains the name of the script file which is executed like: <code>'$0': 'myapp.js'</code>.</li>
<li><code>argv._</code> is an array containing each element not attached to an option(or flag) these elements are referred as <code>commands</code> in yargs.</li>
<li>Individual options(flags) become properties of <code>argv</code>, such as with <code>argv.h</code> and <code>argv.time</code>. 
Note that non-single-letter flags must be passed in as <code>--flag</code> like: <code>node myapp.js --time</code>.</li>

A summary of elements used in the program:


<li><i>argv</i>: This is the modified <code>process.argv</code> which we have configured with yargs.</li>
<li><i>command()</i>: This method is used to add commands, their description and options which are specific to these commands only, like in the above code <code>lyr</code> is the command and <code>-y</code> is lyr specific option: <code>node myapp.js lyr -y 2016</code></li>
<li><i>option()</i>: This method is used to add global options(flags) which can be accessed by all commands or without any command.</li>
<li><i>help()</i>: This method is used to display a help dialogue when <code>--help</code> option is encountered which contains description of all the <code>commands</code> and <code>options</code> available.</li>
<li><i>alias()</i>: This method provides an alias name to an option, like in the above code both <code>--help</code> and <code>-h</code> triggers the help dialogue.</li>

For more information on yargs and the many, many other things it can do for your command-line arguments, please visit <a href="http://yargs.js.org/docs/">http://yargs.js.org/docs/</a>

<h2>Puppeteer Web Scraping in Node.js</h2>
While there are a few different libraries for scraping the web with Node.js, in this tutorial, i'll be using the puppeteer library.
Puppeteer is a popular and easy to use npm package used for web automation and web scraping purposes.
<i>Some of puppeteer's most useful features include:</i>

Being able to extract a scraped element's text content.
Being able to interact with a webpage by filling out forms, clicking on buttons or running searches inside a search bar.
Being able to scrape and download images from the web.
Being able to see the web scraping in progress using headless mode.

Installation

For this tutorial, I will suppose you already have <i>npm</i> and <i>node_modules</i> installed, as well as a <i>package.json</i> and <i>package-lock.json</i> file.

If you don't, here's a great guide on how to do so: <a href="https://www.digitalocean.com/community/tutorials/how-to-use-node-js-modules-with-npm-and-package-json">Setup</a>

To install puppeteer, run one of the following commands in your project's terminal:
<code>npm i puppeteer</code>
Or
<code>yarn add puppeteer</code>
Once puppeteer is installed, it will appear as a directory inside your node_modules.
<h3>make a simple web scraping script in Node.js</h3>
The web scraping script will get the first synonym of "smart" from the web thesaurus by:

Getting the HTML contents of the web thesaurus' webpage.
Finding the element that we want to scrape through it's selector.
Displaying the text contents of the scraped element.
<img src="https://res.cloudinary.com/practicaldev/image/fetch/s--uYsNi1UQ--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/g1yuk24tdujckpswg8en.gif">

Let's get started!

Before scraping, and then extracting this element's text through it's selector in Node.js, we need to setup a few things first:
Create or open an empty javascript file, you can name it whatever you want, but I'll name mine "<i>index.js</i>" for this tutorial. 
Then, require puppeteer on the first line and create the async function inside which we will be writing our web scraping code:
<i>index.js</i>
<code>const puppeteer = require('puppeteer')
async function scrape() {
}
scrape()
</code>

Next, initiate a new browser instance and define the "page" variable, which is going to be used for navigating to webpages and scraping elements within a webpage's HTML contents:
<i>index.js</i>
<code>const puppeteer = require('puppeteer')
async function scrape() {
const browser = await puppeteer.launch({})
const page = await browser.newPage()
}
scrape()
</code>

Scraping the first synonym of "smart"
To locate and copy the selector of the first synonym of "smart", which is what we're going to use to locate the synonym inside of the web thesaurus' webpage, first go to the web thesaurus' synonyms of "smart", right click on the first synonym and click on "inspect". 
This will make this webpage's DOM pop-up at the right of your screen:
<img src="https://res.cloudinary.com/practicaldev/image/fetch/s--LUgj4uNu--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/b0g5w8duzsrv9clx2fff.png">
Next, right click on the highlighted HTML element containing the first synonym and click on "copy selector":
<img src="https://res.cloudinary.com/practicaldev/image/fetch/s--WpX-teFU--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/3tzgpz7rqxfirnfrjz9a.png">
Finally, to navigate to the web thesaurus, scrape and display the first synonym of "smart" through the selector we copied earlier:

First, make the "page" variable navigate to https://www.thesaurus.com/browse/smart inside the newly created browser instance.
Next, we define the "element" variable by making the page wait for our desired element's selector to appear in the webpage's DOM.
The text content of the element is then extracted using the evaluate() function, and displayed inside the "text" variable.
Finally, we close the browser instance.

<i>index.js</i>
<code>const puppeteer = require('puppeteer')
async function scrape() {
const browser = await puppeteer.launch({})
const page = await browser.newPage()
await page.goto('https://www.thesaurus.com/browse/smart')
var element = await page.waitForSelector("#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(1) > a")
var text = await page.evaluate(element => element.textContent, element)
console.log(text)
browser.close()
}
scrape()
</code>

Time to test

Now if you run your index.js script using "node index.js", you will see that it has displayed the first synonym of the word "smart":
<img src="https://res.cloudinary.com/practicaldev/image/fetch/s--VAWi5h1W--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/t65do4rq7bu4w962bvtw.gif">
Scraping the top 5 synonyms of smart
We can implement the same code to scrape the top 5 synonyms of smart instead of 1:
<i>index.js</i>
<code>const puppeteer = require('puppeteer')
async function scrape() {
const browser = await puppeteer.launch({})
const page = await browser.newPage()
await page.goto('https://www.thesaurus.com/browse/smart')
for(i = 1; i &lt; 6; i++){
var element = await page.waitForSelector("#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(" + i + ") > a")
var text = await page.evaluate(element => element.textContent, element)
console.log(text)
}
browser.close()
}
scrape()
</code>

<img src="https://res.cloudinary.com/practicaldev/image/fetch/s--vzWmk0VK--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_66%2Cw_880/https://dev-to-uploads.s3.amazonaws.com/uploads/articles/44rfjhp90eu7k9163j0p.gif">
The "<i>element</i>" variable will be: "<i>#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(1) > a</i>" on the first iteration, "<i>#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(2) > a</i>" on the second, and so on until it reaches the last iteration where the "<i>element</i>" variable will be "<i>#meanings > div.css-ixatld.e15rdun50 > ul > li:nth-child(5) > a</i>".
As you can see, the only thing that is altered in the "element" variable throughout the iterations is the "li:nth-child()" value.
This is because in our case, the elements that we are trying to scrape are all "li" elements inside a "ul" element,
so we can easily scrape them in order by increasing the value inside "li:nth-child()":

li:nth-child(1) for the first synonym.
li:nth-child(2) for the second synonym.
li:nth-child(3) for the third synonym.
li:nth-child(4) for the fourth synonym.
And li:nth-child(5) for the fifth synonym.

Final notes
While web scraping has many advantages like:

Saving time on manually collecting data.
Being able to programmatically aggregate pieces of data scraped from the web.
Creating a dataset of data that might be useful for machine learning, data visualization or data analytics purposes.

It also has 2 disadvantages:

Some websites don't allow for scraping their data, one popular example is craigslist.
Some people consider it to be a gray area since some use cases of web scraping practice user or entity data collection and storage.

<h2>Node.js Modules with npm and package.json</h2>
<div id="Nodejstoc" class="toc"><a href="#Nodejstopic-0" target="_self"> Introduction</a><br><a href="#Nodejstopic-1" target="_self"><span class="orange">Prerequisites</span></a><br><a href="#Nodejstopic-2" target="_self"><span class="orange">Step 1 — Creating a <code>package.json</code> File</span></a><br><a href="#Nodejstopic-3" target="_self"> Using the <code>init</code> Command</a><br><a href="#Nodejstopic-4" target="_self"><span class="orange">Step 2 — Installing Modules</span></a><br><a href="#Nodejstopic-5" target="_self"> Development Dependencies</a><br><a href="#Nodejstopic-6" target="_self"> Automatically Generated Files: <code>node_modules</code> and <code>package-lock.json</code></a><br><a href="#Nodejstopic-7" target="_self"> Installing from package.json</a><br><a href="#Nodejstopic-8" target="_self"> Global Installations</a><br><a href="#Nodejstopic-9" target="_self"><span class="orange">Step 3 — Managing Modules</span></a><br><a href="#Nodejstopic-10" target="_self"> Listing Modules</a><br><a href="#Nodejstopic-11" target="_self"> Updating Modules</a><br><a href="#Nodejstopic-12" target="_self"> Uninstalling Modules</a><br><a href="#Nodejstopic-13" target="_self"> Auditing Modules</a><br><a href="#Nodejstopic-14" target="_self"><span class="orange">Conclusion</span></a><br></div></center><br><br>
<h4 id="Nodejstopic-0"> Introduction</h4>
Because of such features as its speedy Input/Output (I/O) performance and its basis in the well-known JavaScript language, <a href="https://nodejs.org/en/">Node.js</a> has quickly become a popular runtime environment for back-end web development. 

But as interest grows, larger applications are built, and managing the complexity of the codebase and its dependencies becomes more difficult. 
Node.js organizes this complexity using <em>modules</em>, which are any single JavaScript files containing functions or objects that can be used by other programs or modules. 
A collection of one or more modules is commonly referred to as a <em>package</em>, and these packages are themselves organized by package managers.

The <a href="https://www.npmjs.com/">Node.js Package Manager (npm)</a> is the default and most popular package manager in the Node.js ecosystem, and is primarily used to install and manage external modules in a Node.js project. 
It is also commonly used to install a wide range of CLI tools and run project scripts. 
npm tracks the modules installed in a project with the <code>package.json</code> file, which resides in a project’s directory and contains:

All the modules needed for a project and their installed versions
All the metadata for a project, such as the author, the license, etc.
Scripts that can be run to automate tasks within the project

As you create more complex Node.js projects, managing your metadata and dependencies with the <code>package.json</code> file will provide you with more predictable builds, since all external dependencies are kept the same. 

The file will keep track of this information automatically; while you may change the file directly to update your project’s metadata, you will seldom need to interact with it directly to manage modules.
In this tutorial, you will manage packages with npm. 
The first step will be to create and understand the <code>package.json</code> file. 

You will then use it to keep track of all the modules you install in your project. 
Finally, you will list your package dependencies, update your packages, uninstall your packages, and perform an audit to find security flaws in your packages.

<h3 id="Nodejstopic-1"><span class="orange">Prerequisites</span></h3>
To complete this tutorial, you will need:
Node.js installed on your development machine. 

This tutorial uses version 18.3.0. 
To install this on macOS, follow the steps in <a href="https://www.digitalocean.com/community/tutorials/how-to-install-node-js-and-create-a-local-development-environment-on-macos">How to Install Node.js and Create a Local Development Environment on macOS</a>; to install this on Ubuntu 20.04, follow the <i>Installing Using a PPA</i> or <i>Installing using the Node Version Manager</i> section of <a href="https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-ubuntu-20-04">How To Install Node.js on Ubuntu 20.04</a>. 
By having Node.js installed you will also have npm installed; this tutorial uses version 8.11.0.

<h3 id="Nodejstopic-2"><span class="orange">Step 1 — Creating a <code>package.json</code> File</span></h3>
We begin this tutorial by setting up the example project—a fictional Node.js <code>locator</code> module that gets the user’s IP address and returns the country of origin. 

You will not be coding the module in this tutorial. 
However, the packages you manage would be relevant if you were developing it.
First, you will create a <code>package.json</code> file to store useful metadata about the project and help you manage the project’s dependent Node.js modules. 

As the suffix suggests, this is a JSON (JavaScript Object Notation) file. 
JSON is a standard format used for sharing, based on <a href="https://www.digitalocean.com/community/tutorials/understanding-objects-in-javascript">JavaScript objects</a> and consisting of data stored as key-value pairs. 
If you would like to learn more about JSON, read our <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-json">Introduction to JSON</a> article.

Since a <code>package.json</code> file contains numerous properties, it can be cumbersome to create manually, without copy and pasting a template from somewhere else. 
To make things easier, npm provides the <code>init</code> command. 
This is an interactive command that asks you a series of questions and creates a <code>package.json</code> file based on your answers.

<h4 id="Nodejstopic-3"> Using the <code>init</code> Command</h4>
First, set up a project so you can practice managing modules. 
In your shell, create a new folder called <code>locator</code>:

<code>mkdirlocator</code>

Then move into the new folder:
<code>cdlocator</code>
Now, initialize the interactive prompt by entering:
<code>npm init</code>
<i>Note</i>: If your code will use Git for version control, create the Git repository first and then run <code>npm init</code>. 

The command automatically understands that it is in a Git-enabled folder. 
If a Git remote is set, it automatically fills out the <code>repository</code>, <code>bugs</code>, and <code>homepage</code> fields for your <code>package.json</code> file. 
If you initialized the repo after creating the <code>package.json</code> file, you will have to add this information in yourself. 

For more on Git version control, see our <a href="https://www.digitalocean.com/community/tutorial_series/introduction-to-git-installation-usage-and-branches">Introduction to Git: Installation, Usage, and Branches</a> series.
You will receive the following output:
<code>OutputThis utility will walk you through creating a package.json file.

It only covers the most common items, and tries to guess sensible defaults.
See `npm help init` for definitive documentation on these fields
and exactly what they do.

Use `npm install &lt;pkg&gt;` afterwards to install a package and
save it as a dependency in the package.json file.
Press ^C at any time to quit.

package name: (locator)
</code>
You will first be prompted for the <code>name</code> of your new project. 

By default, the command assumes it’s the name of the folder you’re in. 
Default values for each property are shown in parentheses <code>()</code>. 
Since the default value for <code>name</code> will work for this tutorial, press <code>ENTER</code> to accept it.

The next value to enter is <code>version</code>. 
Along with the <code>name</code>, this field is required if your project will be shared with others in the npm package repository.
<i>Note:</i> Node.js packages are expected to follow the <a href="https://semver.org/">Semantic Versioning</a> (semver) guide. 

Therefore, the first number will be the <code>MAJOR</code> version number that only changes when the API changes. 
The second number will be the <code>MINOR</code> version that changes when features are added. 
The last number will be the <code>PATCH</code> version that changes when bugs are fixed.

Press <code>ENTER</code> so the default version of <code>1.0.0</code> is accepted.
The next field is <code>description</code>—a useful string to explain what your Node.js module does. 
Our fictional <code>locator</code> project would get the user’s IP address and return the country of origin. 

A fitting <code>description</code> would be <code>Finds the country of origin of the incoming request</code>, so type in something like this and press <code>ENTER</code>. 
The <code>description</code> is very useful when people are searching for your module.
The following prompt will ask you for the <code>entry point</code>. 

If someone installs and <code>requires</code> your module, what you set in the <code>entry point</code> will be the first part of your program that is loaded. 
The value needs to be the relative location of a JavaScript file, and will be added to the <code>main</code> property of the <code>package.json</code>. 
Press <code>ENTER</code> to keep the default value of <code>index.js</code>.

<i>Note</i>: Most modules have an <code>index.js</code> file as the main point of entry. 
This is the default value for a <code>package.json</code>’s <code>main</code> property, which is the point of entry for npm modules. 
If there is no <code>package.json</code>, Node.js will try to load <code>index.js</code> by default.

Next, you’ll be asked for a <code>test command</code>, an executable script or command to run your project tests. 
In many popular Node.js modules, tests are written and executed with <a href="https://mochajs.org/">Mocha</a>, <a href="https://jestjs.io/">Jest</a>, <a href="https://jasmine.github.io/">Jasmine</a>, or other test frameworks. 
Since testing is beyond the scope of this article, leave this option empty for now, and press <code>ENTER</code> to move on.

The <code>init</code> command will then ask for the project’s git repository, which may live on a service such as GitHub (for more information, see <a href="https://help.github.com/en/github/creating-cloning-and-archiving-repositories/about-repositories">GitHub’s Repository documentation</a>). 
You won’t use this in this example, so leave it empty as well.
After the repository prompt, the command asks for <code>keywords</code>. 

This property is an array of strings with useful terms that people can use to find your repository. 
It’s best to have a small set of words that are really relevant to your project, so that searching can be more targeted. 
List these keywords as a string with commas separating each value. 

For this sample project, type <code>ip,geo,country</code> at the prompt. 
The finished <code>package.json</code> will have three items in the array for <code>keywords</code>.
The next field in the prompt is <code>author</code>. 

This is useful for users of your module who want to get in contact with you. 
For example, if someone discovers an exploit in your module, they can use this to report the problem so that you can fix it. 
The <code>author</code> field is a string in the following format: <code>"<b>Name</b> \&lt;<b>Email</b>\&gt; (<b>Website</b>)"</code>. 

For example, <code>"Sammy \&lt;sammy@your_domain\&gt; (https://your_domain)"</code> is a valid author. 
The email and website data are optional—a valid author could just be a name. 
Add your contact details as an author and confirm with <code>ENTER</code>.

Finally, you’ll be prompted for the <code>license</code>. 
This determines the legal permissions and limitations users will have while using your module. 
Many Node.js modules are open source, so npm sets the default to <a href="https://www.npmjs.com/package/isc-license">ISC</a>.

At this point, you would review your licensing options and decide what’s best for your project. 
For more information on different types of open source licenses, see this <a href="https://opensource.org/licenses">license list from the Open Source Initiative</a>. 
If you do not want to provide a license for a private repository, you can type <code>UNLICENSED</code> at the prompt. 

For this sample, use the default ISC license, and press <code>ENTER</code> to finish this process.
The <code>init</code> command will now display the <code>package.json</code> file it’s going to create. 
It will look similar to this:

<code>OutputAbout to write to /home/<b>sammy</b>/locator/package.json:
{
"name": "locator",
"version": "1.0.0",
"description": "Finds the country of origin of the incoming request",
"main": "index.js",
"scripts": {
"test": "echo \"Error: no test specified\" &amp;&amp; exit 1"
},

"keywords": ["ip", "geo", "country" ],
"author": "<b>Sammy</b> &lt;<b>sammy</b>@<b>your_domain</b>&gt; (https://<b>your_domain</b>)",

"license": "ISC"
}
Is this OK? (yes) </code>

Once the information matches what you see here, press <code>ENTER</code> to complete this process and create the <code>package.json</code> file. 
With this file, you can keep a record of modules you install for your project.

Now that you have your <code>package.json</code> file, you can test out installing modules in the next step.
<h3 id="Nodejstopic-4"><span class="orange">Step 2 — Installing Modules</span></h3>
It is common in software development to use external libraries to perform ancillary tasks in projects. 

This allows the developer to focus on the business logic and create the application more quickly and efficiently by utilizing tools and code that others have written that accomplish tasks one needs.
For example, if our sample <code>locator</code> module has to make an external API request to get geographical data, we could use an HTTP library to make that task easier. 
Since our main goal is to return pertinent geographical data to the user, we could install a package that makes HTTP requests easier for us instead of rewriting this code for ourselves, a task that is beyond the scope of our project.

Let’s run through this example. 
In your <code>locator</code> application, you will use the <a href="https://github.com/axios/axios">axios</a> library, which will help you make HTTP requests. 
Install it by entering the following in your shell:

<code>npm install axios --save </code>

You begin this command with <code>npm install</code>, which will install the package (for brevity you can also use <code>npm i</code>). 
You then list the packages that you want installed, separated by a space. 
In this case, this is <code>axios</code>. 

Finally, you end the command with the optional <code>--save</code> parameter, which specifies that <code>axios</code> will be saved as a project dependency.
When the library is installed, you will see output similar to the following:
<code>Output...
+ axios@0.27.2
added 5 packages from 8 contributors and audited 5 packages in 0.764s
found 0 vulnerabilities
</code>
Now, open the <code>package.json</code> file, using a text editor of your choice. 
This tutorial will use <code>nano</code>:

<code>nano package.json </code>
You’ll see a new property, as highlighted in the following:
locator/package.json
<code>{
"name": "locator",
"version": "1.0.0",
"description": "Finds the country of origin of the incoming request",
"main": "index.js",

"scripts": {
"test": "echo \"Error: no test specified\" &amp;&amp; exit 1"
},
"keywords": [ "ip", "geo", "country" ],
"author": "Sammy sammy@your_domain (https://your_domain)",
"license": "ISC",
<b>"dependencies": { "axios": "^0.27.2" }</b>
}
</code>
The <code>--save</code> option told <code>npm</code> to update the <code>package.json</code> with the module and version that was just installed. 
This is great, as other developers working on your projects can easily see what external dependencies are needed.

<i>Note</i>: You may have noticed the <code>^</code> before the version number for the <code>axios</code> dependency. 
Recall that semantic versioning consists of three digits: <i>MAJOR</i>, <i>MINOR</i>, and <i>PATCH</i>. 
The <code>^</code> symbol signifies that any higher MINOR or PATCH version would satisfy this version constraint. 

If you see <code>~</code> at the beginning of a version number, then only higher PATCH versions satisfy the constraint.
When you are finished reviewing <code>package.json</code>, close the file. 
If you used nano to edit the file, you can do so by pressing <code>CTRL + X</code> and then <code>ENTER</code>.

<h4 id="Nodejstopic-5"> Development Dependencies</h4>
Packages that are used for the development of a project but not for building or running it in production are called <em>development dependencies</em>. 
They are not necessary for your module or application to work in production, but may be helpful while writing the code.

For example, it’s common for developers to use <a href="https://en.wikipedia.org/wiki/Lint_(software)"><em>code linters</em></a> to ensure their code follows best practices and to keep the style consistent. 
While this is useful for development, this only adds to the size of the distributable without providing a tangible benefit when deployed in production.
Install a linter as a development dependency for your project. 

Try this out in your shell:
<code>npm i eslint@8.0.0 --save-dev </code>
In this command, you used the <code>--save-dev</code> flag. 
This will save <code>eslint</code> as a dependency that is only needed for development. 

Notice also that you added <code>@8.0.0</code> to your dependency name. 
When modules are updated, they are tagged with a version. 
The <code>@</code> tells npm to look for a specific tag of the module you are installing. 

Without a specified tag, npm installs the latest tagged version. 
Open <code>package.json</code> again:
<code>nano package.json </code>
This will show the following:

locator/package.json
<code>{
"name": "locator",
"version": "1.0.0",
"description": "Finds the country of origin of the incoming request",
"main": "index.js",

"scripts": {
"test": "echo \"Error: no test specified\" &amp;&amp; exit 1"
},

"keywords": [ "ip", "geo", "country" ],
"author": "Sammy sammy@your_domain (https://your_domain)",
"license": "ISC",
"dependencies": { "axios": "^0.19.0" },
<b>"devDependencies": { "eslint": "^8.0.0" }</b>
}
</code>

<code>eslint</code> has been saved as a <code>devDependencies</code>, along with the version number you specified earlier. 
Exit <code>package.json</code>.
<h4 id="Nodejstopic-6"> Automatically Generated Files: <code>node_modules</code> and <code>package-lock.json</code></h4>

When you first install a package to a Node.js project, <code>npm</code> automatically creates the <code>node_modules</code> folder to store the modules needed for your project and the <code>package-lock.json</code> file that you examined earlier.
Confirm these are in your working directory. 
In your shell, type <code>ls</code> and press <code>ENTER</code>. 

You will observe the following output:
<code>Outputnode_modules    package.json    package-lock.json
</code>

The <code>node_modules</code> folder contains every installed dependency for your project. 
In most cases, you should <i>not</i> commit this folder into your version controlled repository. 
As you install more dependencies, the size of this folder will quickly grow. 

Furthermore, the <code>package-lock.json</code> file keeps a record of the exact versions installed in a more succinct way, so including <code>node_modules</code> is not necessary.
While the <code>package.json</code> file lists dependencies that tell us the suitable versions that should be installed for the project, the <code>package-lock.json</code> file keeps track of all changes in <code>package.json</code> or <code>node_modules</code> and tells us the exact version of the package installed. 
You usually commit this to your version controlled repository instead of <code>node_modules</code>, as it’s a cleaner representation of all your dependencies.

<h4 id="Nodejstopic-7"> Installing from package.json</h4>
With your <code>package.json</code> and <code>package-lock.json</code> files, you can quickly set up the same project dependencies before you start development on a new project. 
To demonstrate this, move up a level in your directory tree and create a new folder named <code>cloned_locator</code> in the same directory level as <code>locator</code>:

<code>cd ..
mkdir cloned_locator
</code>
Move into your new directory:
<code>cd cloned_locator
</code>
Now copy the <code>package.json</code> and <code>package-lock.json</code> files from <code>locator</code> to <code>cloned_locator</code>:

<code>cp ../locator/package.json ../locator/package-lock.json .
</code>

To install the required modules for this project, type:
<code>npm i
</code>
npm will check for a <code>package-lock.json</code> file to install the modules. 
If no lock file is available, it would read from the <code>package.json</code> file to determine the installations. 

It is usually quicker to install from <code>package-lock.json</code>, since the lock file contains the exact version of modules and their dependencies, meaning npm does not have to spend time figuring out a suitable version to install.
When deploying to production, you may want to skip the development dependencies. 
Recall that development dependencies are stored in the <code>devDependencies</code> section of <code>package.json</code>, and have no impact on the running of your app. 

When installing modules as part of the deployment process to deploy your application, omit the dev dependencies by running:
<code>npm i --production
</code>
The <code>--production</code> flag ignores the <code>devDependencies</code> section during installation. 
For now, stick with your development build.

Before moving to the next section, return to the <code>locator</code> folder:
<code>cd ../locator
</code>
<h4 id="Nodejstopic-8"> Global Installations</h4>
So far, you have been installing npm modules for the <code>locator</code> project. 

npm also allows you to install packages <em>globally</em>. 
This means that the package is available to your user in the wider system, like any other shell command. 
This ability is useful for the many Node.js modules that are CLI tools.

For example, you may want to blog about the <code>locator</code> project that you’re currently working on. 
To do so, you can use a library like <a href="https://hexo.io">Hexo</a> to create and manage your static website blog. 
Install the Hexo CLI globally like this:

<code>npm i hexo-cli -g
</code>

To install a package globally, you append the <code>-g</code> flag to the command.
<i>Note</i>: If you get a permission error trying to install this package globally, your system may require super user privileges to run the command. 
Try again with <code>sudo npm i hexo-cli -g</code>.

Test that the package was successfully installed by typing:
<code>hexo --version
</code>
You will see output similar to:
<code>Outputhexo-cli: 4.3.0
os: linux 5.15.0-35-generic Ubuntu 22.04 LTS 22.04 LTS (Jammy Jellyfish)
node: 18.3.0
v8: 10.2.154.4-node.8
uv: 1.43.0
zlib: 1.2.11
brotli: 1.0.9

ares: 1.18.1
modules: 108
nghttp2: 1.47.0
napi: 8
llhttp: 6.0.6
openssl: 3.0.3+quic

cldr: 41.0
icu: 71.1
tz: 2022a
unicode: 14.0
ngtcp2: 0.1.0-DEV
nghttp3: 0.1.0-DEV
</code>
So far, you have learned how to install modules with npm. 
You can install packages to a project locally, either as a production or development dependency. 

You can also install packages based on pre-existing <code>package.json</code> or <code>package-lock.json</code> files, allowing you to develop with the same dependencies as your peers. 
Finally, you can use the <code>-g</code> flag to install packages globally, so you can access them regardless of whether you’re in a Node.js project or not.
Now that you can install modules, in the next section you will practice techniques to administer your dependencies.

<h3 id="Nodejstopic-9"><span class="orange">Step 3 — Managing Modules</span></h3>
A complete package manager can do a lot more than install modules. 
npm has over 20 commands relating to dependency management available. 

In this step, you will:

List modules you have installed.

Update modules to a more recent version.
Uninstall modules you no longer need.
Perform a security audit on your modules to find and fix security flaws.


While these examples will be done in your <code>locator</code> folder, all of these commands can be run globally by appending the <code>-g</code> flag at the end of them, exactly like you did when installing globally.
<h4 id="Nodejstopic-10"> Listing Modules</h4>
If you would like to know which modules are installed in a project, it would be easier to use the <code>list</code> or <code>ls</code> command instead of reading the <code>package.json</code> directly. 
To do this, enter:
<code>npm ls


</code>
You will see output like this:

<code>Output├── axios@0.27.2
└── eslint@8.0.0
</code>

The <code>--depth</code> option allows you to specify what level of the dependency tree you want to see. 
When it’s <code>0</code>, you only see your top level dependencies. 
If you want to see the entire dependency tree, use the <code>--all</code> argument:

<code>npm ls --all

</code>

You will see output like the following:
<code>Output├─┬ axios@0.27.2
│ ├── follow-redirects@1.15.1

│ └─┬ form-data@4.0.0
│   ├── asynckit@0.4.0
│   ├─┬ combined-stream@1.0.8

│   │ └── delayed-stream@1.0.0
│   └─┬ mime-types@2.1.35
│     └── mime-db@1.52.0

└─┬ eslint@8.0.0
├─┬ @eslint/eslintrc@1.3.0
│ ├── ajv@6.12.6 deduped

│ ├── debug@4.3.4 deduped
│ ├── espree@9.3.2 deduped
│ ├── globals@13.15.0 deduped

│ ├── ignore@5.2.0
│ ├── import-fresh@3.3.0 deduped
│ ├── js-yaml@4.1.0 deduped

│ ├── minimatch@3.1.2 deduped
│ └── strip-json-comments@3.1.1 deduped
. 

. 
.
</code>

<h4 id="Nodejstopic-11"> Updating Modules</h4>
It is a good practice to keep your npm modules up to date. 
This improves your likelihood of getting the latest security fixes for a module. 

Use the <code>outdated</code> command to check if any modules can be updated:
<code>npm outdated


</code>
You will get output like the following:
<code>OutputPackage  Current  Wanted  Latest  Location             Depended by

eslint     8.0.0  8.17.0  8.17.0  node_modules/eslint  locator
</code>
This command first lists the <code>Package</code> that’s installed and the <code>Current</code> version. 

The <code>Wanted</code> column shows which version satisfies your version requirement in <code>package.json</code>. 
The <code>Latest</code> column shows the most recent version of the module that was published.
The <code>Location</code> column states where in the dependency tree the package is located. 

The <code>outdated</code> command has the <code>--depth</code> flag like <code>ls</code>. 
By default, the depth is 0.
It seems that you can update <code>eslint</code> to a more recent version. 

Use the <code>update</code> or <code>up</code> command like this:
<code>npm up eslint


</code>
The output of the command will contain the version installed:
<code>Output

removed 7 packages, changed 4 packages, and audited 91 packages in 1s
14 packages are looking for funding
run `npm fund` for details

found 0 vulnerabilities
</code>
To see which version of <code>eslint</code> that you are using now, you can use <code>npm ls</code> using the package name as an argument:

<code>npm ls eslint

</code>

The output will resemble the <code>npm ls</code> command you used before, but include only the <code>eslint</code> package’s versions:
<code>Output└─┬ eslint@8.17.0
└─┬ eslint-utils@3.0.0

└── eslint@8.17.0 deduped
</code>
If you wanted to update all modules at once, then you would enter:

<code>npm up

</code>

<h4 id="Nodejstopic-12"> Uninstalling Modules</h4>
The npm <code>uninstall</code> command can remove modules from your projects. 
This means the module will no longer be installed in the <code>node_modules</code> folder, nor will it be seen in your <code>package.json</code> and <code>package-lock.json</code> files.

Removing dependencies from a project is a normal activity in the software development lifecycle. 
A dependency may not solve the problem as advertised, or may not provide a satisfactory development experience. 
In these cases, it may better to uninstall the dependency and build your own module.

Imagine that <code>axios</code> does not provide the development experience you would have liked for making HTTP requests. 
Uninstall <code>axios</code> with the <code>uninstall</code> or <code>un</code> command by entering:
<code>npm un axios


</code>
Your output will be similar to:

<code>Outputremoved 8 packages, and audited 83 packages in 542ms
13 packages are looking for funding
run `npm fund` for details

found 0 vulnerabilities
</code>
It doesn’t explicitly say that <code>axios</code> was removed. 

To verify that it was uninstalled, list the dependencies once again:
<code>npm ls


</code>
Now, we only see that <code>eslint</code> is installed:
<code>Outputlocator@1.0.0 /home/ubuntu/locator

└── eslint@8.17.0
</code>
This shows that you have successfully uninstalled the <code>axios</code> package.

<h4 id="Nodejstopic-13"> Auditing Modules</h4>
npm provides an <code>audit</code> command to highlight potential security risks in your dependencies. 
To see the audit in action, install an outdated version of the <a href="https://github.com/request/request">request</a> module by running the following:

<code>npm i request@2.60.0

</code>

When you install this outdated version of <code>request</code>, you’ll notice output similar to the following:
<code>Outputnpm WARN deprecated cryptiles@2.0.5: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). 
Please upgrade to the latest version to get the best features, bug fixes, and security patches. 

If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated sntp@1.0.9: This module moved to @hapi/sntp. 
Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.

npm WARN deprecated boom@2.10.1: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). 
Please upgrade to the latest version to get the best features, bug fixes, and security patches. 
If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).

npm WARN deprecated node-uuid@1.4.8: Use uuid module instead
npm WARN deprecated har-validator@1.8.0: this library is no longer supported
npm WARN deprecated hoek@2.16.3: This version has been deprecated in accordance with the hapi support policy (hapi.im/support). 

Please upgrade to the latest version to get the best features, bug fixes, and security patches. 
If you are unable to upgrade at this time, paid support is available for older versions (hapi.im/commercial).
npm WARN deprecated request@2.60.0: request has been deprecated, see https://github.com/request/request/issues/3142

npm WARN deprecated hawk@3.1.3: This module moved to @hapi/hawk. 
Please make sure to switch over as this distribution is no longer supported and may contain bugs and critical security issues.
added 56 packages, and audited 139 packages in 4s

13 packages are looking for funding
run `npm fund` for details
9 vulnerabilities (5 moderate, 2 high, 2 critical)

To address all issues, run:
npm audit fix --force
Run `npm audit` for details.

</code>
npm is telling you that you have deprecated packages and vulnerabilities in your dependencies. 
To get more details, audit your entire project with:

<code>npm audit

</code>

The <code>audit</code> command shows tables of output highlighting security flaws:
<code>Output# npm audit report
bl  &lt;1.2.3

Severity: moderate
Remote Memory Exposure in bl - https://github.com/advisories/GHSA-pp7h-53gx-mx7r
fix available via `npm audit fix`

node_modules/bl
request  2.16.0 - 2.86.0
Depends on vulnerable versions of bl

Depends on vulnerable versions of hawk
Depends on vulnerable versions of qs
Depends on vulnerable versions of tunnel-agent

node_modules/request
cryptiles  &lt;=4.1.1
Severity: critical

Insufficient Entropy in cryptiles - https://github.com/advisories/GHSA-rq8g-5pc5-wrhr
Depends on vulnerable versions of boom
fix available via `npm audit fix`

node_modules/cryptiles
hawk  &lt;=9.0.0
Depends on vulnerable versions of boom

Depends on vulnerable versions of cryptiles
Depends on vulnerable versions of hoek
Depends on vulnerable versions of sntp

node_modules/hawk
. 
. 

.
9 vulnerabilities (5 moderate, 2 high, 2 critical)
To address all issues, run:

npm audit fix
</code>
You can see the path of the vulnerability, and sometimes npm offers ways for you to fix it. 

You can run the update command as suggested, or you can run the <code>fix</code> subcommand of <code>audit</code>. 
In your shell, enter:
<code>npm audit fix


</code>
You will see similar output to:

<code>Outputnpm WARN deprecated har-validator@5.1.5: this library is no longer supported
npm WARN deprecated uuid@3.4.0: Please upgrade  to version 7 or higher. 
Older versions may use Math.random() in certain circumstances, which is known to be problematic. 

See https://v8.dev/blog/math-random for details.
npm WARN deprecated request@2.88.2: request has been deprecated, see https://github.com/request/request/issues/3142
added 19 packages, removed 34 packages, changed 13 packages, and audited 124 packages in 3s

14 packages are looking for funding
run `npm fund` for details
found 0 vulnerabilities

</code>
npm was able to safely update two of the packages, decreasing your vulnerabilities by the same amount. 
However, you still have three deprecated packages in your dependencies. 

The <code>audit fix</code> command does not always fix every problem. 
Although a version of a module may have a security vulnerability, if you update it to a version with a different API then it could break code higher up in the dependency tree.
You can use the <code>--force</code> parameter to ensure the vulnerabilities are gone, like this:

<code>npm audit fix --force

</code>

As mentioned before, this is not recommended unless you are sure that it won’t break functionality.
<h3 id="Nodejstopic-14"><span class="orange">Conclusion</span></h3>
In this tutorial, you went through various exercises to demonstrate how Node.js modules are organized into packages, and how these packages are managed by npm. 

In a Node.js project, you used npm packages as dependencies by creating and maintaining a <code>package.json</code> file—a record of your project’s metadata, including what modules you installed. 
You also used the npm CLI tool to install, update, and remove modules, in addition to listing the dependency tree for your projects and checking and updating modules that are outdated.
In the future, leveraging existing code by using modules will speed up development time, as you don’t have to repeat functionality. 

You will also be able to create your own npm modules, and these will in turn will be managed by others via npm commands. 
As for next steps, experiment with what you learned in this tutorial by installing and testing the variety of packages out there. 
See what the ecosystem provides to make problem solving easier. 

For example, you could try out <a href="https://www.typescriptlang.org/">TypeScript</a>, a superset of JavaScript, or turn your website into mobile apps with <a href="https://cordova.apache.org/">Cordova</a>. 
If you’d like to learn more about Node.js, see our <a href="https://www.digitalocean.com/community/tags/node-js?type=tutorials">other Node.js tutorials</a>.

<h2>nodejs-web-scraper</h2>
<div id="nodejs-web-scrapertoc" class="toc"><a href="#nodejs-web-scrapertopic-0" target="_self">Installation</a><br><a href="#nodejs-web-scrapertopic-1" target="_self">Basic examples</a><br><a href="#nodejs-web-scrapertopic-2" target="_self"> Collect articles from a news site</a><br><a href="#nodejs-web-scrapertopic-3" target="_self"> Get data of every page as a dictionary</a><br><a href="#nodejs-web-scrapertopic-4" target="_self"> Download all images from a page</a><br><a href="#nodejs-web-scrapertopic-5" target="_self"> Use multiple selectors</a><br><a href="#nodejs-web-scrapertopic-6" target="_self">Advanced Examples</a><br><a href="#nodejs-web-scrapertopic-7" target="_self"> Pagination</a><br><a href="#nodejs-web-scrapertopic-8" target="_self"> Get an entire HTML file</a><br><a href="#nodejs-web-scrapertopic-9" target="_self"> Downloading a file that is not an image</a><br><a href="#nodejs-web-scrapertopic-10" target="_self"> getElementContent and getPageResponse hooks</a><br><a href="#nodejs-web-scrapertopic-11" target="_self"> Add additional conditions</a><br><a href="#nodejs-web-scrapertopic-12" target="_self"> Scraping an auth protected site</a><br><a href="#nodejs-web-scrapertopic-13" target="_self">API</a><br><a href="#nodejs-web-scrapertopic-14" target="_self"> <span class="brown">class Scraper(config)</span></a><br><a href="#nodejs-web-scrapertopic-15" target="_self"> <span class="brown">class Root([config])</span></a><br><a href="#nodejs-web-scrapertopic-16" target="_self"> <span class="brown">class OpenLinks(querySelector,[config])</span></a><br><a href="#nodejs-web-scrapertopic-17" target="_self"> <span class="brown">class CollectContent(querySelector,[config])</span></a><br><a href="#nodejs-web-scrapertopic-18" target="_self"> <span class="brown">class DownloadContent(querySelector,[config])</span></a><br><a href="#nodejs-web-scrapertopic-19" target="_self">Pagination explained</a><br><a href="#nodejs-web-scrapertopic-20" target="_self">Error Handling</a><br><a href="#nodejs-web-scrapertopic-21" target="_self">Automatic logs</a><br><a href="#nodejs-web-scrapertopic-22" target="_self">Concurrency</a><br></div></center><br><br>

nodejs-web-scraper is a simple tool for scraping/crawling server-side rendered pages.
It supports features like recursive scraping(pages that "open" other pages), file download and handling, automatic retries of failed requests, concurrency limitation, pagination, request delay, etc. Tested on Node 10 - 16(Windows 7, Linux Mint).

The API uses Cheerio selectors. <a href="https://www.npmjs.com/package/cheerio">Click here for reference</a>

For any questions or suggestions, please open a Github issue.

<h3 id="nodejs-web-scrapertopic-0">Installation</h3>
$ npm install nodejs-web-scraper

<h3 id="nodejs-web-scrapertopic-1">Basic examples</h3>
<h4 id="nodejs-web-scrapertopic-2"> Collect articles from a news site</h4>
Let's say we want to get every article(from every category), from a news site. We want each item to contain the title,
story and image link(or links).

const { Scraper, Root, DownloadContent, OpenLinks, CollectContent } = require('nodejs-web-scraper');
const fs = require('fs');

(async () =&gt; {

    const config = {
        baseSiteUrl: `https://www.some-news-site.com/`,
        startUrl: `https://www.some-news-site.com/`,
        filePath: './images/',
        concurrency: 10,//Maximum concurrent jobs. More than 10 is not recommended.Default is 3.
        maxRetries: 3,//The scraper will try to repeat a failed request few times(excluding 404). Default is 5.       
        logPath: './logs/'//Highly recommended: Creates a friendly JSON for each operation object, with all the relevant data. 
    }
    const scraper = new Scraper(config);//Create a new Scraper instance, and pass config to it.

    //Now we create the "operations" we need:

    const root = new Root();//The root object fetches the startUrl, and starts the process.  
 
    //Any valid cheerio selector can be passed. For further reference: https://cheerio.js.org/
    const category = new OpenLinks('.category',{name:'category'});//Opens each category page.

    const article = new OpenLinks('article a', {name:'article' });//Opens each article page.

    const image = new DownloadContent('img', { name: 'image' });//Downloads images.

    const title = new CollectContent('h1', { name: 'title' });//"Collects" the text from each H1 element.

    const story = new CollectContent('section.content', { name: 'story' });//"Collects" the the article body.

    root.addOperation(category);//Then we create a scraping "tree":
      category.addOperation(article);
       article.addOperation(image);
       article.addOperation(title);
       article.addOperation(story);

    await scraper.scrape(root);

    const articles = article.getData()//Will return an array of all article objects(from all categories), each
    //containing its "children"(titles,stories and the downloaded image urls) 

    //If you just want to get the stories, do the same with the "story" variable:
    const stories = story.getData();

    fs.writeFile('./articles.json', JSON.stringify(articles), () =&gt; { })//Will produce a formatted JSON containing all article pages and their selected data.

    fs.writeFile('./stories.json', JSON.stringify(stories), () =&gt; { })
})();

This basically means: "go to <a href="https://www.some-news-site.com">https://www.some-news-site.com</a>; Open every category; Then open every article in each category page; Then collect the title, story and image href, and download all images on that page".
<h4 id="nodejs-web-scrapertopic-3"> Get data of every page as a dictionary</h4>
An alternative, perhaps more firendly way to collect the data from a page, would be to use the "getPageObject" hook.

const { Scraper, Root, OpenLinks, CollectContent, DownloadContent } = require('nodejs-web-scraper');
const fs = require('fs');

(async () =&gt; {

    const pages = [];//All ad pages.

    //pageObject will be formatted as {title,phone,images}, becuase these are the names we chose for the scraping operations below.
    //Note that each key is an array, because there might be multiple elements fitting the querySelector.
    //This hook is called after every page finished scraping.
    //It will also get an address argument. 
    const getPageObject = (pageObject,address) =&gt; {
                          pages.push(pageObject) }

    const config = {
        baseSiteUrl: `https://www.profesia.sk`,
        startUrl: `https://www.profesia.sk/praca/`,
        filePath: './images/',
        logPath: './logs/'
    }

    const scraper = new Scraper(config);

    const root = new Root();//Open pages 1-10. You need to supply the querystring that the site uses(more details in the API docs).

    const jobAds = new OpenLinks('.list-row h2 a', { name: 'Ad page', getPageObject });//Opens every job ad, and calls the getPageObject, passing the formatted dictionary.

    const phones = new CollectContent('.details-desc a.tel', { name: 'phone' })//Important to choose a name, for the getPageObject to produce the expected results.

    const titles = new CollectContent('h1', { name: 'title' });

    root.addOperation(jobAds);
     jobAds.addOperation(titles);
     jobAds.addOperation(phones);

    await scraper.scrape(root);
    
    fs.writeFile('./pages.json', JSON.stringify(pages), () =&gt; { });//Produces a formatted JSON with all job ads.
})()

Let's describe again in words, what's going on here: "Go to <a href="https://www.profesia.sk/praca/">https://www.profesia.sk/praca/</a>; Then paginate the root page, from 1 to 10; Then, on each pagination page, open every job ad; Then, collect the title, phone and images of each ad."
<h4 id="nodejs-web-scrapertopic-4"> Download all images from a page</h4>
A simple task to download all images in a page(including base64)

const { Scraper, Root, DownloadContent } = require('nodejs-web-scraper');

(async () =&gt; {

   const config = {
        baseSiteUrl: `https://spectator.sme.sk`,//Important to provide the base url, which is the same as the starting url, in this example.
        startUrl: `https://spectator.sme.sk/`,
        filePath: './images/',
        cloneFiles: true,//Will create a new image file with an appended name, if the name already exists. Default is false. 
       }

    const scraper = new Scraper(config);

    const root = new Root();//Root corresponds to the config.startUrl. This object starts the entire process

    const images = new DownloadContent('img')//Create an operation that downloads all image tags in a given page(any Cheerio selector can be passed).

    root.addOperation(images);//We want to download the images from the root page, we need to Pass the "images" operation to the root.

    await scraper.scrape(root);//Pass the Root to the Scraper.scrape() and you're done.

})();

When done, you will have an "images" folder with all downloaded files.
<h4 id="nodejs-web-scrapertopic-5"> Use multiple selectors</h4>
If you need to select elements from different possible classes("or" operator), just pass comma separated classes.
This is part of the Jquery specification(which Cheerio implemets), and has nothing to do with the scraper.

const { Scraper, Root, CollectContent } = require('nodejs-web-scraper');

(async () =&gt; {

   const config = {
        baseSiteUrl: `https://spectator.sme.sk`,
        startUrl: `https://spectator.sme.sk/`,           
       }

    function getElementContent(element){
        // Do something...
    }   

    const scraper = new Scraper(config);

    const root = new Root();

    const title = new CollectContent('.first_class, .second_class',{getElementContent});//Any of these will fit.

    root.addOperation(title);

    await scraper.scrape(root);

})();
<h3 id="nodejs-web-scrapertopic-6">Advanced Examples</h3>
<h4 id="nodejs-web-scrapertopic-7"> Pagination</h4>
Get every job ad from a job-offering site. Each job object will contain a title, a phone and image hrefs. Being that the site is paginated, use the pagination feature.

const { Scraper, Root, OpenLinks, CollectContent, DownloadContent } = require('nodejs-web-scraper');
const fs = require('fs');

(async () =&gt; {

    const pages = [];//All ad pages.

    //pageObject will be formatted as {title,phone,images}, becuase these are the names we chose for the scraping operations below.
    const getPageObject = (pageObject,address) =&gt; {                  
        pages.push(pageObject)
    }

    const config = {
        baseSiteUrl: `https://www.profesia.sk`,
        startUrl: `https://www.profesia.sk/praca/`,
        filePath: './images/',
        logPath: './logs/'
    }

    const scraper = new Scraper(config);

    const root = new Root({ pagination: { queryString: 'page_num', begin: 1, end: 10 } });//Open pages 1-10.
    // YOU NEED TO SUPPLY THE QUERYSTRING that the site uses(more details in the API docs). "page_num" is just the string used on this example site.

    const jobAds = new OpenLinks('.list-row h2 a', { name: 'Ad page', getPageObject });//Opens every job ad, and calls the getPageObject, passing the formatted object.

    const phones = new CollectContent('.details-desc a.tel', { name: 'phone' })//Important to choose a name, for the getPageObject to produce the expected results.

    const images = new DownloadContent('img', { name: 'images' })

    const titles = new CollectContent('h1', { name: 'title' });

    root.addOperation(jobAds);
     jobAds.addOperation(titles);
     jobAds.addOperation(phones);
     jobAds.addOperation(images);

    await scraper.scrape(root);
    
    fs.writeFile('./pages.json', JSON.stringify(pages), () =&gt; { });//Produces a formatted JSON with all job ads.
})()

Let's describe again in words, what's going on here: "Go to <a href="https://www.profesia.sk/praca/">https://www.profesia.sk/praca/</a>; Then paginate the root page, from 1 to 10; Then, on each pagination page, open every job ad; Then, collect the title, phone and images of each ad."
<h4 id="nodejs-web-scrapertopic-8"> Get an entire HTML file</h4>
const sanitize = require('sanitize-filename');//Using this npm module to sanitize file names.
const fs = require('fs');
const { Scraper, Root, OpenLinks } = require('nodejs-web-scraper');

(async () =&gt; {
    const config = {
        baseSiteUrl: `https://www.profesia.sk`,
        startUrl: `https://www.profesia.sk/praca/`,
        removeStyleAndScriptTags: false//Telling the scraper NOT to remove style and script tags, cause i want it in my html files, for this example.        
    }

    let directoryExists;

    const getPageHtml = (html, pageAddress) =&gt; {//Saving the HTML file, using the page address as a name.

        if(!directoryExists){
            fs.mkdirSync('./html');
            directoryExists = true;
        }
        const name = sanitize(pageAddress)
        fs.writeFile(`./html/${name}.html`, html, () =&gt; { })
    }

    const scraper = new Scraper(config);

    const root = new Root({ pagination: { queryString: 'page_num', begin: 1, end: 100 } });

    const jobAds = new OpenLinks('.list-row h2 a', { getPageHtml });//Opens every job ad, and calls a hook after every page is done.

    root.addOperation(jobAds);

    await scraper.scrape(root);
})() 

Description: "Go to <a href="https://www.profesia.sk/praca/">https://www.profesia.sk/praca/</a>; Paginate 100 pages from the root; Open every job ad; Save every job ad page as an html file;
<h4 id="nodejs-web-scrapertopic-9"> Downloading a file that is not an image</h4>
  const config = {        
        baseSiteUrl: `https://www.some-content-site.com`,
        startUrl: `https://www.some-content-site.com/videos`,
        filePath: './videos/',
        logPath: './logs/'
    }
    const scraper = new Scraper(config);   
    const root = new Root();
    const video = new DownloadContent('a.video',{ contentType: 'file' });//The "contentType" makes it clear for the scraper that this is NOT an image(therefore the "href is used instead of "src"). 

    const description = new CollectContent('h1').       

    root.addOperation(video);      
    root.addOperation(description);

   await scraper.scrape(root);

    console.log(description.getData())//You can call the "getData" method on every operation object, giving you the aggregated data collected by it.

Description: "Go to <a href="https://www.some-content-site.com">https://www.some-content-site.com</a>; Download every video; Collect each h1; At the end, get the entire data from the "description" object;
<h4 id="nodejs-web-scrapertopic-10"> getElementContent and getPageResponse hooks</h4>
  const getPageResponse = async (response) =&gt; {
        //Do something with response.data(the HTML content). No need to return anything.
    }

    const myDivs=[];
    const getElementContent = (content, pageAddress) =&gt; {
               
        myDivs.push(`myDiv content from page ${pageAddress} is ${content}...`)
    }

    const config = {        
        baseSiteUrl: `https://www.nice-site`,
        startUrl: `https://www.nice-site/some-section`,       
       }

    const scraper = new Scraper(config);

    const root = new Root();

    const articles = new OpenLinks('article a');

    const posts = new OpenLinks('.post a'{getPageResponse});//Is called after the HTML of a link was fetched, but before the children have been scraped. Is passed the response object of the page.    

    const myDiv = new CollectContent('.myDiv',{getElementContent});//Will be called after every "myDiv" element is collected.

    root.addOperation(articles);      
        articles.addOperation(myDiv);
    root.addOperation(posts);
        posts.addOperation(myDiv)   

   await scraper.scrape(root);

Description: "Go to <a href="https://www.nice-site/some-section">https://www.nice-site/some-section</a>; Open every article link; Collect each .myDiv; Call getElementContent()".

"Also, from <a href="https://www.nice-site/some-section">https://www.nice-site/some-section</a>, open every post; Before scraping the children(myDiv object), call getPageResponse(); CollCollect each .myDiv".
<h4 id="nodejs-web-scrapertopic-11"> Add additional conditions</h4>
In some cases, using the cheerio selectors isn't enough to properly filter the DOM nodes. This is where the "condition" hook comes in. Both OpenLinks and DownloadContent can register a function with this hook, allowing you to decide if this DOM node should be scraped, by returning true or false.

    const config = {        
        baseSiteUrl: `https://www.nice-site`,
        startUrl: `https://www.nice-site/some-section`,       
       }

    /**
     * Will be called for each node collected by cheerio, in the given operation(OpenLinks or DownloadContent)      
     */
   const condition = (cheerioNode) =&gt; {      
         //Note that cheerioNode contains other useful methods, like html(), hasClass(), parent(), attr() and more.           
        const text = cheerioNode.text().trim();//Get the innerText of the &lt;a&gt; tag.
        if(text === 'some text i am looking for'){//Even though many links might fit the querySelector, Only those that have this innerText,
        // will be "opened".
            return true
        }
    }   

    const scraper = new Scraper(config);
    const root = new Root();

    //Let's assume this page has many links with the same CSS class, but not all are what we need.
    const linksToOpen = new OpenLinks('some-css-class-that-is-just-not-enough',{condition});    

    root.addOperation(linksToOpen);      

    await scraper.scrape(root);
<h4 id="nodejs-web-scrapertopic-12"> Scraping an auth protected site</h4>
Please refer to this guide: <a href="https://nodejs-web-scraper.ibrod83.com/blog/2020/05/23/crawling-subscription-sites/">https://nodejs-web-scraper.ibrod83.com/blog/2020/05/23/crawling-subscription-sites/</a>
<h3 id="nodejs-web-scrapertopic-13">API</h3>
<h4 id="nodejs-web-scrapertopic-14"> <span class="brown">class Scraper(config)</span></h4>
The main nodejs-web-scraper object. Starts the entire scraping process via Scraper.scrape(Root). Holds the configuration and global state.

These are the available options for the scraper, with their default values:

const config ={
            baseSiteUrl: '',//Mandatory.If your site sits in a subfolder, provide the path WITHOUT it.
            startUrl: '',//Mandatory. The page from which the process begins.   
            logPath:null,//Highly recommended.Will create a log for each scraping operation(object).               
            cloneFiles: true,//If an image with the same name exists, a new file with a number appended to it is created. Otherwise. it's overwritten.
            removeStyleAndScriptTags: true,// Removes any &lt;style&gt; and &lt;script&gt; tags found on the page, in order to serve Cheerio with a light-weight string. change this ONLY if you have to.           
            concurrency: 3,//Maximum concurrent requests.Highly recommended to keep it at 10 at most. 
            maxRetries: 5,//Maximum number of retries of a failed request.      
            delay: 200,
            timeout: 6000,
            filePath: null,//Needs to be provided only if a "downloadContent" operation is created.
            auth: null,//Can provide basic auth credentials(no clue what sites actually use it)
            headers: null,//Provide custom headers for the requests.           
            proxy:null,//Use a proxy. Pass a full proxy URL, including the protocol and the port.           
            showConsoleLogs:true,//Set to false, if you want to disable the messages
            onError:null//callback function that is called whenever an error occurs - signature is: onError(errorString) =&gt; {}
        }

Public methods:
<table>
<thead><tr><th>Name</th><th>Description</th></tr></thead>
<tbody>
<tr><td>async scrape(Root)</td><td>After all objects have been created and assembled, you begin the process by calling this method, passing the root object</td></tr>
</tbody>
</table>

<h4 id="nodejs-web-scrapertopic-15"> <span class="brown">class Root([config])</span></h4>
Root is responsible for fetching the first page, and then scrape the children. It can also be paginated, hence the optional config. For instance:

const root= new Root({ pagination: { queryString: 'page', begin: 1, end: 100 }})

The optional config takes these properties:

{    
    pagination:{},//In case your root page is paginated.    
    getPageObject:(pageObject,address)=&gt;{},//Gets a formatted page object with all the data we choose in our scraping setup. Also gets an address argument.
    getPageHtml:(htmlString,pageAddress)=&gt;{}//Get the entire html page, and also the page address. Called with each link opened by this OpenLinks object.  
    getPageData:(cleanData)=&gt;{}//Called after all data was collected by the root and its children.
    getPageResponse:(response)=&gt;{}//Will be called after a link's html was fetched, but BEFORE the child operations are performed on it(like, collecting some data from it). Is passed the response object(a custom response object, that also contains the original node-fetch response). Notice that any modification to this object, might result in an unexpected behavior with the child operations of that page.
    getException:(error)=&gt;{}//Get every exception thrown by Root.  
    
}

Public methods:

<table>
<thead><tr><th>Name</th><th>Description</th></tr></thead>
<tbody>
<tr><td>addOperation(Operation)</td><td>(OpenLinks,DownloadContent,CollectContent)</td></tr>
<tr><td>getData()</td><td>Gets all data collected by this operation. In the case of root, it will just be the entire scraping tree.</td></tr>
<tr><td>getErrors()</td><td>In the case of root, it will show all errors in every operation.</td></tr>
</tbody>
</table>

<h4 id="nodejs-web-scrapertopic-16"> <span class="brown">class OpenLinks(querySelector,[config])</span></h4>
Responsible for "opening links" in a given page. Basically it just creates a nodelist of anchor elements, fetches their html, and continues the process of scraping, in those pages - according to the user-defined scraping tree.

The optional config can have these properties:

{
    name:'some name',//Like every operation object, you can specify a name, for better clarity in the logs.
    pagination:{},//Look at the pagination API for more details.
    condition:(cheerioNode)=&gt;{},//Use this hook to add additional filter to the nodes that were received by the querySelector. Return true to include, falsy to exclude.
    getPageObject:(pageObject,address)=&gt;{},//Gets a formatted page object with all the data we choose in our scraping setup. Also gets an address argument.
    getPageHtml:(htmlString,pageAddress)=&gt;{}//Get the entire html page, and also the page address. Called with each link opened by this OpenLinks object.
    getElementList:(elementList)=&gt;{},//Is called each time an element list is created. In the case of OpenLinks, will happen with each list of anchor tags that it collects. Those elements all have Cheerio methods available to them.
    getPageData:(cleanData)=&gt;{}//Called after all data was collected from a link, opened by this object.(if a given page has 10 links, it will be called 10 times, with the child data).
    getPageResponse:(response)=&gt;{}//Will be called after a link's html was fetched, but BEFORE the child operations are performed on it(like, collecting some data from it). Is passed the response object(a custom response object, that also contains the original node-fetch response). Notice that any modification to this object, might result in an unexpected behavior with the child operations of that page.
    getException:(error)=&gt;{}//Get every exception throw by this openLinks operation, even if this was later repeated successfully.
    slice:[start,end]//You can define a certain range of elements from the node list.Also possible to pass just a number, instead of an array, if you only want to specify the start. This uses the Cheerio/Jquery slice method.
}

Public methods:

<table>
<thead><tr><th>Name</th><th>Description</th></tr></thead>
<tbody>
<tr><td>addOperation(Operation)</td><td>Add a scraping "operation"(OpenLinks,DownloadContent,CollectContent)</td></tr>
<tr><td>getData()</td><td>Will get the data from all pages processed by this operation</td></tr>
<tr><td>getErrors()</td><td>Gets all errors encountered by this operation.</td></tr>
</tbody>
</table>
 
<h4 id="nodejs-web-scrapertopic-17"> <span class="brown">class CollectContent(querySelector,[config])</span></h4>
Responsible for simply collecting text/html from a given page.
The optional config can receive these properties:

{
    name:'some name',
    contentType:'text',//Either 'text' or 'html'. Default is text.   
    shouldTrim:true,//Default is true. Applies JS String.trim() method.
    getElementList:(elementList,pageAddress)=&gt;{},  
    getElementContent:(elementContentString,pageAddress)=&gt;{}//Called with each element collected.
    getAllItems: (items, address)=&gt;{}//Called after an entire page has its elements collected.  
    slice:[start,end]
}

Public methods:

<table>
<thead><tr><th>Name</th><th>Description</th></tr></thead>
<tbody>
<tr><td>getData()</td><td>Gets all data collected by this operation.</td></tr>
</tbody>
</table>
 
<h4 id="nodejs-web-scrapertopic-18"> <span class="brown">class DownloadContent(querySelector,[config])</span></h4>
Responsible downloading files/images from a given page.
The optional config can receive these properties:

{
    name:'some name',
    contentType:'image',//Either 'image' or 'file'. Default is image.
    alternativeSrc:['first-alternative','second-alternative']//Provide alternative attributes to be used as the src. Will only be invoked,
    //If the "src" attribute is undefined or is a dataUrl. If no matching alternative is found, the dataUrl is used. 
    condition:(cheerioNode)=&gt;{},//Use this hook to add additional filter to the nodes that were received by the querySelector. Return true to include, falsy to exclude.
    getElementList:(elementList)=&gt;{},
    getException:(error)=&gt;{}//Get every exception throw by this downloadContent operation, even if this was later repeated successfully.
    filePath:'./somePath',//Overrides the global filePath passed to the Scraper config.  
    slice:[start,end]
}

Public methods:

<table>
<thead><tr><th>Name</th><th>Description</th></tr></thead>
<tbody>
<tr><td>getData()</td><td>Gets all file names that were downloaded, and their relevant data</td></tr>
<tr><td>getErrors()</td><td>Gets all errors encountered by this operation.</td></tr>
</tbody>
</table>

<h3 id="nodejs-web-scrapertopic-19">Pagination explained</h3>
nodejs-web-scraper covers most scenarios of pagination(assuming it's server-side rendered of course).

    //If a site uses a queryString for pagination, this is how it's done:

    const productPages = new openLinks('a.product'{ pagination: { queryString: 'page_num', begin: 1, end: 1000 } });//You need to specify the query string that the site uses for pagination, and the page range you're interested in.

    //If the site uses some kind of offset(like Google search results), instead of just incrementing by one, you can do it this way:
    
    { pagination: { queryString: 'page_num', begin: 1, end: 100,offset:10 } }

    //If the site uses routing-based pagination:

    { pagination: { routingString: '/', begin: 1, end: 100 } }

<h3 id="nodejs-web-scrapertopic-20">Error Handling</h3>
nodejs-web-scraper will automatically repeat every failed request(except 404,400,403 and invalid images). Number of repetitions depends on the global config option "maxRetries", which you pass to the Scraper. If a request fails "indefinitely", it will be skipped. After the entire scraping process is complete, all "final" errors will be printed as a JSON into a file called <i>"finalErrors.json"</i>(assuming you provided a logPath).

Alternatively, use the <code>onError</code> callback function in the scraper's global config.

<h3 id="nodejs-web-scrapertopic-21">Automatic logs</h3>
If a logPath was provided, the scraper will create a log for each operation object you create, and also the following ones: "log.json"(summary of the entire scraping tree), and "finalErrors.json"(an array of all FINAL errors encountered). I really recommend using this feature, along side your own hooks and data handling.

<h3 id="nodejs-web-scrapertopic-22">Concurrency</h3>
The program uses a rather complex concurrency management. Being that the memory consumption can get very high in certain scenarios, I've force-limited the concurrency of pagination and "nested" OpenLinks operations. It should still be very quick. As a general note, i recommend to limit the concurrency to 10 at most. Also the config.delay is a key a factor.

<h2>Web Scraping with JavaScript and NodeJS</h2>
<div id="WebScrapingtoc" class="toc"><a href="#WebScrapingtopic-0" target="_self"> Prerequisites</a><br><a href="#WebScrapingtopic-1" target="_self"> Outcomes</a><br><a href="#WebScrapingtopic-2" target="_self"><span class="orange">Understanding NodeJS: A brief introduction</span></a><br><a href="#WebScrapingtopic-3" target="_self"> The JavaScript Event Loop</a><br><a href="#WebScrapingtopic-4" target="_self"><span class="orange">HTTP clients: querying the web</span></a><br><a href="#WebScrapingtopic-5" target="_self"> 1. Built-In HTTP Client</a><br><a href="#WebScrapingtopic-6" target="_self"> 2. Fetch API</a><br><a href="#WebScrapingtopic-7" target="_self"> 3. Axios</a><br><a href="#WebScrapingtopic-8" target="_self"> 4. SuperAgent</a><br><a href="#WebScrapingtopic-9" target="_self"> SuperAgent plugins</a><br><a href="#WebScrapingtopic-10" target="_self"> 5. Request</a><br><a href="#WebScrapingtopic-11" target="_self"><span class="orange">Data Extraction in JavaScript</span></a><br><a href="#WebScrapingtopic-12" target="_self"> Regular expressions: the hard way</a><br><a href="#WebScrapingtopic-13" target="_self"> Cheerio: Core jQuery for traversing the DOM</a><br><a href="#WebScrapingtopic-14" target="_self"> jsdom: the DOM for Node</a><br><a href="#WebScrapingtopic-15" target="_self"><span class="orange">Headless Browsers in JavaScript</span></a><br><a href="#WebScrapingtopic-16" target="_self"> 1. Puppeteer: the headless browser</a><br><a href="#WebScrapingtopic-17" target="_self"> 2. Nightmare: an alternative to Puppeteer</a><br><a href="#WebScrapingtopic-18" target="_self"> 3. Playwright, the new web scraping framework</a><br><a href="#WebScrapingtopic-19" target="_self"><span class="orange">Summary</span></a><br><a href="#WebScrapingtopic-20" target="_self"><span class="orange">Resources</span></a><br></div></center><br><br>

JavaScript has become one of the most popular and widely used languages due to the massive improvements it has seen and the introduction of the runtime known as NodeJS. Whether it's a web or mobile application, JavaScript now has the right tools. This article will explain how the vibrant ecosystem of NodeJS allows you to efficiently scrape the web to meet most of your requirements.

<h4 id="WebScrapingtopic-0"> Prerequisites</h4>
This post is primarily aimed at developers who have some level of experience with JavaScript. However, if you have a firm understanding of web scraping but have no experience with JavaScript, it may still serve as light introduction to JavaScript. Still, having experience in the following fields will certainly help:
✅ Experience with JavaScript
✅ Experience using the browser's DevTools to extract selectors of elements
✅ Some experience with ES6 JavaScript (Optional)

⭐ Make sure to check out the resources at the end of this article for more details on the subject!

<h4 id="WebScrapingtopic-1"> Outcomes</h4>
After reading this post will be able to:
Have a functional understanding of NodeJS
Use multiple HTTP clients to assist in the web scraping process
Use multiple modern and battle-tested libraries to scrape the web

<h3 id="WebScrapingtopic-2"><span class="orange">Understanding NodeJS: A brief introduction</span></h3>
JavaScript was originally meant to add rudimentary scripting abilities to browsers, in order to allow websites to support more custom ways of interactivity with the user, like showing a dialog box or creating additional HTML content on-the-fly.

For this purpose, browsers are providing a runtime environment (with global objects such as <code>document</code> and <code>window</code>) to enable your code to interact with the browser instance and the page itself. And for more than a decade, JavaScript was really mostly confined to that use case and to the browser. However that changed when <i>Ryan Dahl introduced NodeJS in 2009</i>.

NodeJS took Chrome's JavaScript engine and brought it to the server (or better the command line). Contrary to the browser environment, it did not have any more access to a <em>browser window</em> or <em>cookie storage</em>, but what it got instead, was full access to the system resources. Now, it could easily open network connections, store records in databases, or even just read and write files on your hard drive.

Essentially, Node.js introduced JavaScript as a server-side language and provides a regular JavaScript engine, freed from the usual browser sandbox shackles and, instead, pumped up with a standard system library for networking and file access.

<h4 id="WebScrapingtopic-3"> The JavaScript Event Loop</h4>
What it kept, was the <a href="https://nodejs.org/fa/docs/guides/event-loop-timers-and-nexttick/">Event Loop</a>. As opposed to how many languages handle concurrency, with multi-threading, JavaScript has always only used a single thread and performed blocking operations in an asynchronous fashion, relying primarily on <a href="https://developer.mozilla.org/docs/Glossary/Callback_function">callback functions</a> (or function pointers, as C developers may call them).

Let's check that quickly out with a simple web server example:

<code>const http = require('http');
const PORT = 3000;

const server = http.createServer((req, res) =&gt; {
  res.statusCode = 200;
  res.setHeader('Content-Type', 'text/plain');
  res.end('Hello World');
});

server.listen(port, () =&gt; {
  console.log(`Server running at PORT:${port}/`);
});</code>

Here, we import the HTTP standard library with <code>require</code>, then create a server object with <code>createServer</code> and pass it an anonymous handler function, which the library will invoke for each incoming HTTP request. Finally, we <code>listen</code> on the specified port - and that's actually it.

There are two interesting bits here and both already hint at our event loop and JavaScript's asynchronicity:
The handler function we pass to <code>createServer</code>
The fact that <code>listen</code> is not a blocking call, but returns immediately

In most other languages, we'd usually have an <code>accept</code> function/method, which would block our thread and return the connection socket of the connecting client. At this point, the latest, we'd have to switch to multi-threading, as otherwise we could handle exactly one connection at a time. In this case, however, we don't have to deal with thread management and we always stay with one thread, thanks to callbacks and the event loop.

As mentioned, <code>listen</code> will return immediately, but - although there's no code following our <code>listen</code> call - the application won't exit immediately. That is because we still have a callback registered via <code>createServer</code> (the function we passed).

Whenever a client sends a request, Node.js will parse it in the background and call our anonymous function and pass the request object. The only thing we have to pay attention to here is to return swiftly and not block the function itself, but it's hard to do that, as almost all standard calls are asynchronous (either via callbacks or Promises) - just make sure you don't run <code>while (true);</code> 😀

<i>But enough of theory, let's check it out, shall we?</i>

If you have Node.js installed, all you need to do is save the code to the file <code>MyServer.js</code> and run it in your shell with <code>node MyServer.js</code>. Now, just open your browser and load <a href="http://localhost:3000">http://localhost:3000</a> - voilà, you should get a lovely "Hello World" greeting. That was easy, wasn't it?

One could assume the single-threaded approach may come with performance issues, because it only has one thread, but it's actually quite the opposite and that's the beauty of asynchronous programming. Single-threaded, asynchronous programming can have, especially for I/O intensive work, quite a few performance advantages, because one does not need to pre-allocate resources (e.g. threads).

All right, that was a very nice example of how we easily create a web server in Node.js, but we are in the business of scraping, aren't we? So let's take a look at the JavaScript HTTP client libraries.

<h3 id="WebScrapingtopic-4"><span class="orange">HTTP clients: querying the web</span></h3>
HTTP clients are tools capable of sending a request to a server and then receiving a response from it. Almost every tool that will be discussed in this article uses an HTTP client under the hood to query the server of the website that you will attempt to scrape.

<h4 id="WebScrapingtopic-5"> 1. Built-In HTTP Client</h4>
As mentioned in your server example, Node.js does ship by default with an HTTP library. That library also has a <a href="https://nodejs.org/api/http.html">built-in HTTP client</a>.

<code>const http = require('http');

const req = http.request('http://example.com', res =&gt; {
	const data = [];

	res.on('data', _ =&gt; data.push(_))
	res.on('end', () =&gt; console.log(data.join()))
});

req.end();</code>

It's rather easy to get started, as there are zero third-party dependencies to install or manage, however - as you can notice from our example - the library does require a bit of boilerplate, as it provides the response only in chunks and you eventually need to stitch them together manually. You'll also need to use a <a href="https://nodejs.org/api/https.html">separate library for HTTPS URLs</a>.

In short, it's convenient because it comes out-of-the-box, but it may require you to write more code than you may want. Hence, let's take a look at the other HTTP libraries. Shall we?

<h4 id="WebScrapingtopic-6"> 2. Fetch API</h4>
Another built-in method would be the <a href="https://developer.mozilla.org/docs/Web/API/Fetch_API">Fetch API</a>.

While browsers have supported it for a while already, it took Node.js a bit longer, but as of <a href="https://github.com/nodejs/node/blob/main/doc/changelogs/CHANGELOG_V18.md#2022-04-19-version-1800-current-bethgriggs">version 18</a>, Node.js does support <code>fetch()</code>. To be fair, for the time being, it still is considered an experimental feature, so if you prefer to play it safe, you can also opt for the polyfill/wrapper library <a href="https://github.com/node-fetch/node-fetch">node-fetch</a>, which provides the same functionality.

While at it, also check out our dedicated <a href="/blog/node-fetch/">article on node-fetch</a>.

The Fetch API heavily uses <a href="https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/Promise">Promises</a> and coupled with <a href="https://developer.mozilla.org/docs/Web/JavaScript/Reference/Operators/await">await</a>, that can really provide you with lean and legible code.

<code>async function fetch_demo()
{
	const resp = await fetch('https://www.reddit.com/r/programming.json');

	console.log(await resp.json());
}

fetch_demo();</code>

The only workaround we had to employ, was to wrap our code into a function, as <code>await</code> is not supported on the top-level yet. Apart from that we really just called <code>fetch()</code> with our URL, awaited the response (Promise-magic happening in the background, of course), and used the <a href="https://developer.mozilla.org/docs/Web/API/Response/json"><code>json()</code> function</a> of our <a href="https://developer.mozilla.org/docs/Web/API/Response">Response object</a> (awaiting again) to get the response. Mind you, an already JSON-parsed response 😲.

<i>Not bad, two lines of code, no manual handling of data, no distinction between HTTP and HTTPS, and a native JSON object.</i>

<code>fetch</code> optionally accepts an additional <a href="https://developer.mozilla.org/docs/Web/API/fetch#options">options argument</a>, where you can fine-tune your request with a specific request method (e.g. <code>POST</code>), additional HTTP headers, or pass authentication credentials.

<h4 id="WebScrapingtopic-7"> 3. Axios</h4>
Axios is pretty similar to Fetch. It's also a Promise-based HTTP client and it runs in both, browsers and Node.js. Users of TypeScript will also love its built-in type support.

One drawback, however, contrary to the libraries we mentioned so far, we do have to install it first.

<code>npm install axios</code>

Perfect, let's check out a first plain-Promise example:

<code>const axios = require('axios')

axios
	.get('https://www.reddit.com/r/programming.json')
	.then((response) =&gt; {
		console.log(response)
	})
	.catch((error) =&gt; {
		console.error(error)
	});</code>

Pretty straightforward. Relying on Promises, we can certainly also use <code>await</code> again and make the whole thing a bit less verbose. So let's wrap it into a function one more time:

<code>async function getForum() {
	try {
		const response = await axios.get(
			'https://www.reddit.com/r/programming.json'
		)
		console.log(response)
	} catch (error) {
		console.error(error)
	}
}</code>

All you have to do is call <code>getForum</code>! You can find the Axios library at <a href="https://github.com/axios/axios">Github</a>.

<h4 id="WebScrapingtopic-8"> 4. SuperAgent</h4>
Much like Axios, SuperAgent is another robust HTTP client that has support for promises and the async/await syntax sugar. It has a fairly straightforward API like Axios, but SuperAgent has more dependencies and is less popular.

Regardless, making an HTTP request with SuperAgent using promises, async/await, and callbacks looks like this:

<code>const superagent = require("superagent")
const forumURL = "https://www.reddit.com/r/programming.json"

// callbacks
superagent
	.get(forumURL)
	.end((error, response) =&gt; {
		console.log(response)
	})

// promises
superagent
	.get(forumURL)
	.then((response) =&gt; {
		console.log(response)
	})
	.catch((error) =&gt; {
		console.error(error)
	})

// promises with async/await
async function getForum() {
	try {
		const response = await superagent.get(forumURL)
		console.log(response)
	} catch (error) {
		console.error(error)
	}
}</code>

You can find the SuperAgent library at <a href="https://github.com/visionmedia/superagent">GitHub</a> and installing SuperAgent is as simple as <code>npm install superagent</code>.

<h4 id="WebScrapingtopic-9"> SuperAgent plugins</h4>
One feature, that sets SuperAgent apart from the other libraries here, is its extensibility. It features quite a list of <a href="https://github.com/visionmedia/superagent#plugins">plugins</a> which allow for the tweaking of a request or response. For example, the <a href="https://github.com/leviwheatcroft/superagent-throttle">superagent-throttle</a> plugin would allow you to define throttling rules for your requests.

<h4 id="WebScrapingtopic-10"> 5. Request</h4>
Even though it is not actively maintained any more, <a href="https://www.npmjs.com/package/request">Request</a> still is a popular and widely used HTTP client in the JavaScript ecosystem.

It is fairly simple to make an HTTP request with Request:

<code>const request = require('request')
request('https://www.reddit.com/r/programming.json', function (
  error,
  response,
  body
) {
  console.error('error:', error)
  console.log('body:', body)
})</code>

What you will definitely have noticed here, is that we were neither using plain Promises nor <code>await</code>. That is because Request still employs the traditional callback approach, however there are a couple of <a href="https://github.com/request/request#promises--asyncawait">wrapper libraries</a> to support await as well.

You can find the Request library at <a href="https://github.com/request/request">GitHub</a>, and installing it is as simple as running <code>npm install request</code>.

<i>Should you use Request?</i> We included Request in this list because it still is a popular choice. Nonetheless, development has officially stopped and it is not being actively maintained any more. Of course, that does not mean it is unusable, and there are still lots of libraries using it, but the fact itself, may still make us think twice before we use it for a brand-new project, especially with quite a list of viable alternatives and native <code>fetch</code> support.

<h3 id="WebScrapingtopic-11"><span class="orange">Data Extraction in JavaScript</span></h3>
Fetching the content of a site is, undoubtedly, an important step in any scraping project, but it's only the first step and we actually need to locate and extract the data as well. This is what we are going to check out next, how we can handle an HTML document in JavaScript and how to locate and select information for data extraction.

First off, regular expressions 🙂

<h4 id="WebScrapingtopic-12"> Regular expressions: the hard way</h4>
The simplest way to get started with web scraping without any dependencies, is to use a bunch of regular expressions on the HTML content you received from your HTTP client. But there is a big tradeoff.

While absolutely great in their domain, regular expressions are not ideal for parsing document structures like HTML. Plus, newcomers often struggle with getting them right ("do I need a look-ahead or a look-behind?"). For complex web scraping, regular expressions can also get out of hand. With that said, let's give it a go nonethless.

Say there's a label with some username in it and we want the username. This is similar to what you'd have to do if you relied on regular expressions:

<code>const htmlString = '&lt;label&gt;Username: John Doe&lt;/label&gt;'
const result = htmlString.match(/&lt;label&gt;Username: (.+)&lt;\/label&gt;/)

console.log(result[1])
// John Doe</code>

We are using <a href="https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/String/match"><code>String.match()</code></a> here, which will provide us with an array containing the data of the evaluation of our regular expression. As we used a capturing group (<code>(.+)</code>), the second array element (<code>result[1]</code>) will contain whatever that group managed to capture.

While this certainly worked in our example, anything more complex will either not work or will require a way more complex expression. Just imagine you have a couple of <code>&lt;label&gt;</code> elements in your HTML document.

Don't get us wrong, regular expressions are an unimaginable great tool, just not for HTML 😊 - so let us introduce you to the world of CSS selectors and the DOM.

<h4 id="WebScrapingtopic-13"> Cheerio: Core jQuery for traversing the DOM</h4>
<a href="https://www.scrapingbee.com/blog/cheerio-npm/">Cheerio</a> is an efficient and light library that allows you to use the rich and powerful API of jQuery on the server-side. If you have used jQuery before, you will feel right at home with Cheerio. It provides you with an incredibly easy way to parse an HTML string into a DOM tree, which you can then access via the elegant interface you may be familiar with from jQuery (including function-chaining).

<code>const cheerio = require('cheerio')
const $ = cheerio.load('&lt;h2 class="title"&gt;Hello world&lt;/h3&gt;')

$('h2.title').text('Hello there!')
$('h2').addClass('welcome')

$.html()
// &lt;h2 class="title welcome"&gt;Hello there!&lt;/h3&gt;</code>

As you can see, using Cheerio really is almost identical to how you'd use jQuery.

Keep in mind, Cheerio really focuses on DOM-manipulation and you won't be able to directly "port" jQuery functionality, such as XHR/AJAX requests or mouse handling (e.g. <code>onClick</code>), one-to-one in Cheerio.

Cheerio is a great tool for most use cases when you need to handle the DOM yourself. Of course, if you want to crawl a JavaScript-heavy site (e.g. typical <a href="https://en.wikipedia.org/wiki/Single-page_application">Single-page applications</a>) you may need something closer to a full browser engine. We'll be talking about that in just second, under <a href="#headless-browsers-in-javascript">Headless Browsers in JavaScript</a>.

Time for a quick Cheerio example, wouldn't you agree? To demonstrate the power of Cheerio, we will attempt to crawl the <a href="https://www.reddit.com/r/programming/">r/programming</a> forum in Reddit and get a list of post names.

First, install Cheerio and Axios by running the following command: <code>npm install cheerio axios</code>.

Then create a new file called <code>crawler.js</code> and copy/paste the following code:

<code>const axios = require('axios');
const cheerio = require('cheerio');

const getPostTitles = async () =&gt; {
	try {
		const { data } = await axios.get(
			'https://old.reddit.com/r/programming/'
		);
		const $ = cheerio.load(data);
		const postTitles = [];

		$('div &gt; p.title &gt; a').each((_idx, el) =&gt; {
			const postTitle = $(el).text()
			postTitles.push(postTitle)
		});

		return postTitles;
	} catch (error) {
		throw error;
	}
};

getPostTitles()
    .then((postTitles) =&gt; console.log(postTitles));</code>

<code>getPostTitles()</code> is an asynchronous function that will crawl the subreddit r/programming forum. First, the HTML of the website is obtained using a simple HTTP GET request with the Axios HTTP client library. Then, the HTML data is fed into Cheerio using the <code>cheerio.load()</code> function.

Wonderful, we now have fully parsed HTML document as DOM tree in, good old-fashioned jQuery-manner, in <code>$</code>. What's next? Well, might not be a bad idea to know where to get our posting titles from. So, let's right click one of the titles and pick <code>Inspect</code>. That should get us right to the right element in the browser's developer tools.








    <svg width="1163" height="728" aria-hidden="true" style="background-color:white"></svg>
    <img data-sizes="auto" data-srcset="
    
      , /blog/web-scraping-javascript/reddit_inspect_huad9a253d5ec1846fd6bfeb329aea3a1d_235175_825x0_resize_catmullrom_3.png 825w
    
    
    " data-src="/blog/web-scraping-javascript/reddit_inspect.png" width="1163" height="728" alt="Inspecting Reddit DOM">
    <noscript>
        <img
                loading="lazy"
                
        srcset='
        
        , /blog/web-scraping-javascript/reddit_inspect_huad9a253d5ec1846fd6bfeb329aea3a1d_235175_825x0_resize_catmullrom_3.png 825w
        
        
        '
        
        src="/blog/web-scraping-javascript/reddit_inspect.png"
        
        width="1163" height="728"
        alt='Inspecting Reddit DOM'>
    </noscript>

<br>
Excellent, equipped with our <a href="/blog/practical-xpath-for-web-scraping/">knowledge on XPath</a> or CSS selectors, we can now easily compose the expression we need for that element. For our example, we chose CSS selectors and following one just works beautifully.

<code>div &gt; p.title &gt; a</code>
If you used jQuery, you probably know what we are up to, right? 😏

<code>$('div &gt; p.title &gt; a')</code>

You were absolutely right. The Cheerio call is identical to jQuery (there was a reason why we used <code>$</code> for our DOM variable before) and using Cheerio with our CSS selector will give us the very list of elements matching our selector.

Now, we just need to iterate with <code>each()</code> over all elements and call their <code>text()</code> function to get their text content. 💯 jQuery, isn't it?

So much about the explanation. Time to run our code.

Open up your shell and run <code>node crawler.js</code>. You'll then see an array of about 25 or 26 different post titles (it'll be quite long). While this is a simple use case, it demonstrates the simple nature of the API provided by Cheerio.

If your use case requires the execution of JavaScript and loading of external sources, the following few options will be helpful.

Do not forget to check out our <a href="/blog/nodejs-axios-proxy/">NodeJS Axios proxy</a> tutorial if you want to learn more about using proxies for web scraping!

<h4 id="WebScrapingtopic-14"> jsdom: the DOM for Node</h4>
Similarly to how Cheerio replicates jQuery on the server-side, <a href="https://github.com/jsdom/jsdom">jsdom</a> does the same for the browser's native DOM functionality.

Unlike Cheerio, however, jsdom does not only parse HTML into a DOM tree, it can also handle embedded JavaScript code and it allows you to "interact" with page elements.

Instantiating a jsdom object is rather easy:

<code>const { JSDOM } = require('jsdom')
const { document } = new JSDOM(
	'&lt;h2 class="title"&gt;Hello world&lt;/h3&gt;'
).window

const heading = document.querySelector('.title')
heading.textContent = 'Hello there!'
heading.classList.add('welcome')

heading.innerHTML
// &lt;h2 class="title welcome"&gt;Hello there!&lt;/h3&gt;</code>

Here, we imported the library with <code>require</code> and created a new jsdom instance using the constructor and passed our HTML snippet. Then, we simply used <a href="https://developer.mozilla.org/docs/Web/API/Document/querySelector"><code>querySelector()</code></a> (as we know it from front-end development) to select our element and tweaked its attributes a bit. Fairly standard and we could have done that with Cheerio as well, of course.

What sets jsdom, however, apart is aforementioned support for embedded JavaScript code and, that, we are going to check out now.

The following example uses a simple local HTML page, with one button adding a <code>&lt;div&gt;</code> with an ID.

<code>const { JSDOM } = require("jsdom")

const HTML = `
	&lt;html&gt;
		&lt;body&gt;
			&lt;button onclick="const e = document.createElement('div'); e.id = 'myid'; this.parentNode.appendChild(e);"&gt;Click me&lt;/button&gt;
		&lt;/body&gt;
	&lt;/html&gt;`;

const dom = new JSDOM(HTML, {
	runScripts: "dangerously",
	resources: "usable"
});

const document = dom.window.document;

const button = document.querySelector('button');

console.log("Element before click: " + document.querySelector('div#myid'));
button.click();
console.log("Element after click: " + document.querySelector('div#myid'));</code>

Nothing too complicated here:
we <code>require()</code> jsdom
set up our <code>HTML</code> document
pass <code>HTML</code> to our jsdom constructor (important, we need to enable <code>runScripts</code>)
select the button with a <code>querySelector()</code> call
and <code>click()</code> it

Voilà, that should give us this output

<code>Element before click: null
Element after click: [object HTMLDivElement]</code>

Fairly straightforward and the example showcased how we can use jsdom to actually execute the page's JavaScript code. When we loaded the document, there was initially no <code>&lt;div&gt;</code>. Only once we clicked the button, it was added by the site's code, not our crawler's code.

In this context, the important details are <code>runScripts</code> and <code>resources</code>. These flags instruct jsdom to run the page's code, as well as fetch any relevant JavaScript files. As <a href="https://github.com/jsdom/jsdom#executing-scripts">jsdom's documentation</a> points out, that could potentially allow any site to escape the sandbox and get access to your local system, just by crawling it. <i>Proceed with caution please.</i>

jsdom is a great library to handle most of typical browser tasks within your local Node.js instance, but it still has some limitations and that's where headless browsers really come to shine.

💡 We released a new feature that makes this whole process way simpler. You can now extract data from HTML with one simple API call. Feel free to check the documentation <a href="/documentation/data-extraction/">here</a>.

<h3 id="WebScrapingtopic-15"><span class="orange">Headless Browsers in JavaScript</span></h3>
Sites become more and more complex and often regular HTTP crawling won't suffice any more, but one actually needs a full-fledged browser engine, to get the necessary information from a site.

This is particularly true for <a href="https://en.wikipedia.org/wiki/Single-page_application">SPAs</a> which heavily rely on JavaScript and dynamic and asynchronous resources.

Browser automation and headless browsers come to the rescue here. Let's check out how they can help us to easily crawl Single-page Applications and other sites making use of JavaScript.

<h4 id="WebScrapingtopic-16"> 1. Puppeteer: the headless browser</h4>
<a href="https://pptr.dev">Puppeteer</a>, as the name implies, allows you to manipulate the browser programmatically, just like how a puppet would be manipulated by its puppeteer. It achieves this by providing a developer with a high-level API to control a headless version of Chrome by default and can be configured to run non-headless.

<img src="https://user-images.githubusercontent.com/746130/40333229-5df5480c-5d0c-11e8-83cb-c3e371de7374.png" alt="puppeteer-hierachy"> <em>Taken from the Puppeteer Docs (<a href="https://github.com/puppeteer/puppeteer/blob/v3.0.2/docs/api.md">Source</a>)</em>

Puppeteer is particularly more useful than the aforementioned tools because it allows you to crawl the web as if a real person were interacting with a browser. This opens up a few possibilities that weren't there before:
You can get screenshots or generate PDFs of pages.
You can crawl a Single Page Application and generate pre-rendered content.
You can automate many different user interactions, like keyboard inputs, form submissions, navigation, etc.

It could also play a big role in many other tasks outside the scope of web crawling like UI testing, assist performance optimization, etc.

Quite often, you will probably want to take screenshots of websites or, get to know about a competitor's product catalog. Puppeteer can be used to do this. To start, install Puppeteer by running the following command: <code>npm install puppeteer</code>

This will download a bundled version of Chromium which takes up about 180 to 300 MB, depending on your operating system. You can avoid that step, and use an already installed setup, by specifying a couple of <a href="https://pptr.dev/#environment-variables">Puppeteer environment variables</a>, such as <code>PUPPETEER_SKIP_CHROMIUM_DOWNLOAD</code>. Generally, though, Puppeteer does recommended to use the bundled version and does not support custom setups.

Let's attempt to get a screenshot and PDF of the r/programming forum in Reddit, create a new file called <code>crawler.js</code>, and copy/paste the following code:

<code>const puppeteer = require('puppeteer')

async function getVisual() {
	try {
		const URL = 'https://www.reddit.com/r/programming/'
		const browser = await puppeteer.launch()

		const page = await browser.newPage()
		await page.goto(URL)

		await page.screenshot({ path: 'screenshot.png' })
		await page.pdf({ path: 'page.pdf' })

		await browser.close()
	} catch (error) {
		console.error(error)
	}
}

getVisual()</code>

<code>getVisual()</code> is an asynchronous function that will take a screenshot of our page, as well as export it as PDF document.

To start, an instance of the browser is created by running <code>puppeteer.launch()</code>. Next, we create a new browser tab/page with <code>newPage()</code>. Now, we just need to call <code>goto()</code> on our page instance and pass it our URL.

All these functions are of asynchronous nature and will return immediately, but as they are returning a JavaScript Promise, and we are using <code>await</code>, the flow still appears to be synchronous and, hence, once <code>goto</code> "returned", our website should have loaded.

Excellent, we are ready to get pretty pictures. Let's just call <code>screenshot()</code> on our page instance and pass it a path to our image file. We do the same with <code>pdf()</code> and voilà, we should have at the specified locations two new files. Because we are responsible netizens, we also call <code>close()</code> on our browser object, to clean up behind ourselves. That's it.

Once thing to keep in mind, when <code>goto()</code> returns, the page has loaded but it might not be done with all its asynchronous loading. So depending on your site, you may want to add additional logic in a production crawler, to wait for certain JavaScript events or DOM elements.

But let's run the code. Pop up a shell window, type <code>node crawler.js</code>, and after a few moments, you should have exactly the two mentioned files in your directory.

It's a great tool and if you are really keen on it now, please also check out our other guides on Puppeteer.
<a href="/blog/download-file-puppeteer/">How to download a file with Puppeteer</a>
<a href="/blog/submit-form-puppeteer/">Handling and submitting HTML forms with Puppeteer</a>
<a href="/blog/pyppeteer/">Using Puppeteer with Python and Pyppeteer</a>

<h4 id="WebScrapingtopic-17"> 2. Nightmare: an alternative to Puppeteer</h4>
<a href="https://github.com/segmentio/nightmare">Nightmare</a> is another a high-level browser automation library like Puppeteer. It uses Electron and web and scraping benchmarks indicate it shows a significantly better performance than its predecessor PhantomJS. If Puppeteer is too complex for your use case or there are issues with the default Chromium bundle, Nightmare - despite its name 😨 - may just be the right thing for you.

As so often, our journey starts with NPM: <code>npm install nightmare</code>

Once Nightmare is available on your system, we will use it to find ScrapingBee's website through a Brave search. To do so, create a file called <code>crawler.js</code> and copy/paste the following code into it:

<code>const Nightmare = require('nightmare')
const nightmare = Nightmare()

nightmare
	.goto('https://search.brave.com/')
	.type('#searchbox', 'ScrapingBee')
	.click('#submit-button')
	.wait('#results a')
	.evaluate(
		() =&gt; document.querySelector('#results a').href
	)
	.end()
	.then((link) =&gt; {
		console.log('ScrapingBee Web Link:', link)
	})
	.catch((error) =&gt; {
		console.error('Search failed:', error)
	})</code>

After the usual library import with <code>require</code>, we first create a new instance of Nightmare and save that in <code>nightmare</code>. After that, we are going to have lots of fun with function-chaining and Promises 🥳
We use <code>goto()</code> to load Brave from <a href="https://search.brave.com">https://search.brave.com</a>
We <a href="https://github.com/segmentio/nightmare#typeselector-text"><code>type</code></a> our search term "ScrapingBee" in Brave's search input, with the CSS selector <code>#searchbox</code> (Brave's quite straightforward with its naming, isn't it?)
We <a href="https://github.com/segmentio/nightmare#clickselector"><code>click</code></a> the submit button to start our search. Again, that's with the CSS selector <code>#submit-button</code> (Brave's <i>really</i> straightforward, we love that❣️)
Let's take a quick break, until Brave returns the search list. <a href="https://github.com/segmentio/nightmare#waitselector"><code>wait</code></a>, with the right selector works wonders here. <code>wait</code> also accepts time value, if you need to wait for a specific period of time.
Once Nightmare got the link list from Brave, we simply use <a href="https://github.com/segmentio/nightmare#evaluatefn-arg1-arg2"><code>evaluate()</code></a> to run our custom code on the page (in this case <code>querySelector()</code>) and get the first <code>&lt;a&gt;</code> element matching our selector, and return its <code>href</code> attribute.
Last but not least, we call <a href="https://github.com/segmentio/nightmare#end"><code>end()</code></a> to run and complete our task queue.

That's it, folks. <code>end()</code> returns a standard Promise with the value from our call to <code>evaluate()</code>. Of course, you could also use <code>await</code> here.

That was pretty easy, wasn'it? And if everything went all right 🤞, we should have now got the link to ScrapingBee's website at <a href="https://www.scrapingbee.com">https://www.scrapingbee.com</a>

<code>ScrapingBee Web Link: https://www.scrapingbee.com/</code>

Wanna try it yourself? Just run <code>node crawler.js</code> in your shell 👍

<h4 id="WebScrapingtopic-18"> 3. Playwright, the new web scraping framework</h4>
Playwright is the new cross-language, cross-platform headless framework supported by Microsoft.<br><br>Its main advantage over Puppeteer is that it is cross platform and very easy to use.<br><br>Here is how to simply scrape a page with it:

<code>const playwright = require('playwright');
async function main() {
    const browser = await playwright.chromium.launch({
        headless: false // setting this to true will not run the UI
    });

    const page = await browser.newPage();
    await page.goto('https://finance.yahoo.com/world-indices');
    await page.waitForTimeout(5000); // wait for 5 seconds
    await browser.close();
}

main();</code>

Feel free to check out our <a href="/blog/playwright-web-scraping/">Playwright tutorial</a> if you want to learn more.

<h3 id="WebScrapingtopic-19"><span class="orange">Summary</span></h3>
Phew, that was a long read! But we hope, our examples managed to give you a first glimpse into the world of web scraping with JavaScript and which libraries you can use to crawl the web and scrape the information you need.

Let's give it a quick recap, what we learned today was:
✅ <i>NodeJS</i> is a JavaScript <em>runtime</em> that allow JavaScript to be run <em>server-side</em>. It has a <i>non-blocking</i> nature thanks to the <a href="https://developer.mozilla.org/docs/Web/JavaScript/EventLoop">Event Loop</a>.
✅ <i>HTTP clients</i>, such as the native libaries and <em>fetch</em>, as well as <em>Axios</em>, <em>SuperAgent</em>, <a href="https://www.scrapingbee.com/blog/node-fetch/">node-fetch</a>, and <em>Request</em>, are used to send HTTP requests to a <em>server</em> and receive a response.
✅ <i>Cheerio</i> abstracts the best out of <em>jQuery</em> for the sole purpose of running it <em>server-side</em> for web crawling but <em>does not execute JavaScript</em> code.
✅ <i>JSDOM</i> creates a DOM per the standard <em>JavaScript specification</em> out of an HTML string and allows you to perform DOM manipulations on it.
✅ <i>Puppeteer</i> and <i>Nightmare</i> are <em>high-level browser automation</em> libraries, that allow you to <em>programmatically manipulate</em> web applications as if a real person were interacting with them.

This article focused on JavaScript's scraping ecosystem and its tools. However, there are certainly also other apsects to scraping, which we could not cover in this context.

For example, sites often employ techniques to recognize and block crawlers. You'll want to avoid these and blend in as <em>normal visitor</em>. On this subject, and more, we have an excellent, dedicated <a href="/blog/web-scraping-without-getting-blocked/">guide on how not to get blocked as a crawler</a>. <i>Check it out please.</i>

💡 Should you love scraping, but the usual time-constraints for your project don't allow you to tweak your crawlers to perfection, then please have a look at our <a href="/">scraping API platform</a>. ScrapingBee was built with all these things in mind and has got your back in all crawling tasks.

Happy Scraping!

<h3 id="WebScrapingtopic-20"><span class="orange">Resources</span></h3>
Would you like to read more? Check these links out:
<a href="https://nodejs.org/en/about/">NodeJS Website</a> - The main site of NodeJS with its official documentation.
<a href="https://developer.chrome.com/docs/puppeteer/">Puppeteer's Docs</a> - Google's documentation of Puppeteer, with getting started guides and the API reference.
<a href="https://www.scrapingbee.com/blog/playwright-web-scraping/">Playright</a> - An alternative to Puppeteer, backed by Microsoft.
<a href="https://www.scrapingbee.com/blog/">ScrapingBee's Blog</a> - Contains a lot of information about Web Scraping goodies on multiple platforms.
<a href="https://www.scrapingbee.com/blog/infinite-scroll-puppeteer/">Handling infinite scroll with Puppeteer</a>
<a href="https://www.scrapingbee.com/blog/node-unblocker/">Node-unblocker</a> - a Node.js package to facilitate web scraping through proxies.+











<br><br>
<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({ elements_selector: ".lazy" });
</script>
</pre>
</body>
</html>
