<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width"/>
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script>
$(document).ready(function(){
    $('h1, h2, h3, h4, h5, .goldword, strong,  div.title').click(function(){
    parent.history.back();
    return false;
    });
});
</script>
<style>
body{width:80%;margin-left: 10%}
strong, h1, h2 {color: gold;}
</style>
</head><body>
<center><h1>7 Libraries for Python & R</h1>
<div id="toc"></div></center>
<br>
<br>
<br>


<h2>Introduction</h2>
<p>
If you follow me, you know that this year I started a series called <em>
Weekly Digest for Data Science and AI: Python &amp; R</em>
, where I highlighted the best libraries, repos, packages, and tools that help us be better data scientists for all kinds of tasks.</p>
<p>
The great folks at <a href="http://heartbeat.fritz.ai" data-href="http://heartbeat.fritz.ai" target="_blank">
Heartbeat</a>
 sponsored a lot of these digests, and they asked me to create a list of the best of the best‚Äîthose libraries that really changed or improved the way we worked this year (and beyond).</p>
 <p>
If you want to read the past digests, take a look here:</p>
<div>
<a href="https://www.getrevue.co/profile/favio" data-href="https://www.getrevue.co/profile/favio" title="https://www.getrevue.co/profile/favio">
<strong>
Weekly Digest for Data Science and AI - Revue</strong>
<br>
<em>
Weekly Digest for Data Science and AI - Personal newsletter of Favio V√°zquez...</em>
www.getrevue.co</a>
<a href="https://www.getrevue.co/profile/favio" data-media-id="94e85e4ac1e7884484ac80daf44f403f" data-thumbnail-img-id="0*GDzyl8-N1Ld-LQ93" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*GDzyl8-N1Ld-LQ93);">
</a>
</div>
<blockquote>
Disclaimer: This list is based on the libraries and packages I reviewed in my personal newsletter. All of them were trending in one way or another among programmers, data scientists, and AI enthusiasts. Some of them were created before 2018, but if they were trending, they could be considered.</blockquote>
<h3>Top 7 for Python</h3>
<h4>
7. AdaNet‚Ää‚Äî‚ÄäFast and flexible AutoML with learning guarantees.</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*ZPLMGasPVMFyCyiG.png">
<a href="https://github.com/tensorflow/adanet" data-href="https://github.com/tensorflow/adanet" rel="nofollow noopener noopener" target="_blank">
https://github.com/tensorflow/adanet</a>
<p>
AdaNet is a lightweight and scalable TensorFlow AutoML framework for training and deploying adaptive neural networks using the <em>
AdaNet</em>
 algorithm [<a href="https://arxiv.org/abs/1607.01097" data-href="https://arxiv.org/abs/1607.01097" rel="nofollow noopener noopener" target="_blank">
 Cortes et al. ICML 2017</a>
]. <em>
AdaNet</em>
 combines several learned subnetworks in order to mitigate the complexity inherent in designing effective neural networks.</p>
 <p>
This package will help you selecting optimal neural network architectures, implementing an adaptive algorithm for learning a neural architecture as an ensemble of subnetworks.</p>
<p>
You will need to know TensorFlow to use the package because it implements a TensorFlow Estimator, but this will help you simplify your machine learning programming by encapsulating training and also evaluation, prediction and export for serving.</p>
<p>
You can build an ensemble of neural networks, and the library will help you optimize an objective that balances the trade-offs between the ensemble‚Äôs performance on the training set and its ability to generalize to unseen data.</p>
<h4>
Installation</h4>
<p>
<code>
adanet</code>
 depends on bug fixes and enhancements not present in TensorFlow releases prior to 1.7. You must install or upgrade your TensorFlow package to at least 1.7:</p>
 <pre>
$ pip install &quot;tensorflow&gt;=1.7.0&quot;</pre>
<h4>
Installing from source</h4>
<p>
To install from source, you‚Äôll first need to install <code>
bazel</code>
 following their <a href="https://docs.bazel.build/versions/master/install.html" data-href="https://docs.bazel.build/versions/master/install.html" rel="nofollow noopener noopener" target="_blank">
 installation instructions</a>
.</p>
<p>
Next clone <code>
adanet</code>
 and <code>
 cd</code>
 into its root directory:</p>
 <pre>
$ git clone <a href="https://github.com/tensorflow/adanet" data-href="https://github.com/tensorflow/adanet" rel="nofollow noopener noopener" target="_blank">
https://github.com/tensorflow/adanet</a>
 &amp;&amp; cd adanet</pre>
 <p>
From the <code>
adanet</code>
 root directory run the tests:</p>
 <pre>
$ cd adanet<br>
$ bazel test -c opt //...</pre>
<p>
Once you have verified that everything works well, install <code>
adanet</code>
 as a <a href="https://github.com/tensorflow/adanet/blob/master/adanet/pip_package/PIP.md" data-href="https://github.com/tensorflow/adanet/blob/master/adanet/pip_package/PIP.md" rel="nofollow noopener noopener" target="_blank">
 pip package </a>
.</p>
<p>
You‚Äôre now ready to experiment with <code>
adanet</code>
.</p>
<pre>
import adanet</pre>
<h4>Usage</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*YcvUdmZdv4NWjKF9.gif">
<p>
Here you can find two examples on the usage of the package:</p>
<div>
<a href="https://github.com/tensorflow/adanet/tree/master/adanet/examples/tutorials" data-href="https://github.com/tensorflow/adanet/tree/master/adanet/examples/tutorials" title="https://github.com/tensorflow/adanet/tree/master/adanet/examples/tutorials" rel="nofollow">
<strong>
tensorflow/adanet</strong>
<br>
<em>
Fast and flexible AutoML with learning guarantees.‚Ää‚Äî‚Äätensorflow/adanet</em>
github.com</a>
<a href="https://github.com/tensorflow/adanet/tree/master/adanet/examples/tutorials" data-media-id="e85004f65fc9d39c802b2035ea99e5de" data-thumbnail-img-id="0*JL5HZSi-zuLF2DAs" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*JL5HZSi-zuLF2DAs);">
</a>
</div>
<p>
You can read more about it in the original blog post:</p>
<div>
<a href="https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html?m=1" data-href="https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html?m=1" title="https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html?m=1" rel="nofollow">
<strong>
Introducing AdaNet: Fast and Flexible AutoML with Learning Guarantees</strong>
<br>
<em>
Posted by Charles Weill, Software Engineer, Google AI, NYC Ensemble learning , the art of combining different machine‚Ä¶</em>
ai.googleblog.com</a>
<a href="https://ai.googleblog.com/2018/10/introducing-adanet-fast-and-flexible.html?m=1" data-media-id="9a91431f213398dbdceb9f4a91de8ec9" data-thumbnail-img-id="0*FINa4k-7SYzVP0o6" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*FINa4k-7SYzVP0o6);">
</a>
</div>
</div>
</div>
</section>
<section name="2bed">
<div>
<hr>
</div>
<div>
<div>
<h4>
6. TPOT‚Äî An automated Python machine learning tool that optimizes machine learning pipelines using genetic programming.</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*C5teFLKPLkW48BP5.jpg">
<a href="https://github.com/EpistasisLab/tpot" data-href="https://github.com/EpistasisLab/tpot" rel="nofollow noopener noopener" target="_blank">
https://github.com/EpistasisLab/tpot</a>
<p>
<a href="https://heartbeat.fritz.ai/weekly-digest-for-data-science-and-ai-python-and-r-volume-6-830ed997cf07" data-href="https://heartbeat.fritz.ai/weekly-digest-for-data-science-and-ai-python-and-r-volume-6-830ed997cf07" target="_blank">
Previously</a>
 I talked about Auto-Keras, a great library for AutoML in the Pythonic world. Well, I have another very interesting tool for that.</p>
 <p>
The name is <strong>
TPOT</strong>
 (Tree-based Pipeline Optimization Tool), and it‚Äôs an amazing library. It‚Äôs basically a Python automated machine learning tool that optimizes machine learning pipelines using <strong>
 genetic programming</strong>
.</p>
</div>
<img src="https://cdn-images-1.medium.com/max/1000/0*nDQABCJuEPhds4el.png">

<p>
TPOT can automate a lot of stuff life feature selection, model selection, feature construction, and much more. Luckily, if you‚Äôre a Python machine learner, TPOT is built on top of Scikit-learn, so all of the code it generates should look familiar.</p>
<p>
What it does is automate the most tedious parts of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data, and then it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.</p>
<p>
This is how it works:</p>
</div>
<img src="https://cdn-images-1.medium.com/max/1000/0*Y1E0P1CbeV0yQoUL.png">
<p>
For more details you can read theses great article by <a href="https://medium.com/@mattmayo13" data-href="https://medium.com/@mattmayo13" data-anchor-type="2" data-user-id="a0bc63d95eb0" data-action-value="a0bc63d95eb0" data-action="show-user-card" data-action-type="hover" target="_blank">
Matthew Mayo</a>
:</p>
<div>
<a href="https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html" data-href="https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html" title="https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html">
<strong>
Using AutoML to Generate Machine Learning Pipelines with TPOT</strong>
<br>
<em>
Thus far in this series of posts we have: This post will take a different approach to constructing pipelines. Certainly‚Ä¶</em>
www.kdnuggets.com</a>
<a href="https://www.kdnuggets.com/2018/01/managing-machine-learning-workflows-scikit-learn-pipelines-part-4.html" data-media-id="417258584474deda85ca47a2e9ed1ddb" data-thumbnail-img-id="0*eu84p7k4q_wwYKH8" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*eu84p7k4q_wwYKH8);">
</a>
</div>
<p>
and <a href="https://medium.com/@randal_olson" data-href="https://medium.com/@randal_olson" data-anchor-type="2" data-user-id="79a76605c902" data-action-value="79a76605c902" data-action="show-user-card" data-action-type="hover" target="_blank">
Randy Olson</a>
:</p>
<div>
<a href="https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html" data-href="https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html" title="https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html">
<strong>
TPOT: A Python Tool for Automating Data Science</strong>
<br>
<em>
By Randy Olson, University of Pennsylvania. Machine learning is often touted as: A field of study that gives computers‚Ä¶</em>
www.kdnuggets.com</a>
<a href="https://www.kdnuggets.com/2016/05/tpot-python-automating-data-science.html" data-media-id="ffb05abd04e3ca3e08f3595320781443" data-thumbnail-img-id="0*1cSLy1-59imdPJVQ" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*1cSLy1-59imdPJVQ);">
</a>
</div>
<h4>
Installation</h4>
<p>
You actually need to follow some instructions before installing TPOT. Here they are:</p>
<div>
<a href="http://epistasislab.github.io/tpot/installing/" data-href="http://epistasislab.github.io/tpot/installing/" title="http://epistasislab.github.io/tpot/installing/">
<strong>
Installation‚Ää‚Äî‚ÄäTPOT</strong>
<br>
<em>
Optionally, you can install XGBoost if you would like TPOT to use the eXtreme Gradient Boosting models. XGBoost is‚Ä¶</em>
epistasislab.github.io</a>
<a href="http://epistasislab.github.io/tpot/installing/" data-media-id="a323b2d408b6d70a7e92b974c6780b2e">
</a>
</div>
<p>
After that you can just run:</p>
<pre>
<code>
pip install tpot</code>
</pre>
<h4>
Examples:</h4>
<p>
First let‚Äôs start with the basic Iris dataset:</p>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/6f7bf95f158299c2ac7e853700d1f368?postId=6b7cca2bf000" data-media-id="6f7bf95f158299c2ac7e853700d1f368" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>

<p>
So here we built a very basic TPOT pipeline that will try to look for the best ML pipeline to predict the <code>
iris.target</code>
<em>
. </em>
And then we save that pipeline. After that, what we have to do is very simple‚Ää‚Äî‚Ääload the <code>
.py</code>
 file you generated and you‚Äôll see:</p>
 <pre><strong>import</strong> <strong> numpy</strong> <strong> as</strong> <strong> np</strong></pre>
<pre><strong>from</strong> <strong> sklearn.kernel_approximation</strong> <strong> import</strong> RBFSampler<br> <strong>from</strong> <strong> sklearn.model_selection</strong> <strong> import</strong> train_test_split<br> <strong>from</strong> <strong> sklearn.pipeline</strong> <strong> import</strong> make_pipeline<br> <strong>from</strong> <strong> sklearn.tree</strong> <strong> import</strong> DecisionTreeClassifier</pre>
 <pre>
<em>
# NOTE: Make sure that the class is labeled &#39;class&#39; in the data file</em>
<br>
tpot_data = np.recfromcsv(&#39;PATH/TO/DATA/FILE&#39;, delimiter=&#39;COLUMN_SEPARATOR&#39;, dtype=np.float64)<br>
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index(&#39;class&#39;), axis=1)<br>
training_features, testing_features, training_classes, testing_classes = \<br>
    train_test_split(features, tpot_data[&#39;class&#39;], random_state=42)</pre>
    <pre>
exported_pipeline = make_pipeline(<br>
    RBFSampler(gamma=0.8500000000000001),<br>
    DecisionTreeClassifier(criterion=&quot;entropy&quot;, max_depth=3, min_samples_leaf=4, min_samples_split=9)<br>
)</pre>
<pre>
exported_pipeline.fit(training_features, training_classes)<br>
results = exported_pipeline.predict(testing_features)</pre>
<p>
And that‚Äôs it. You built a classifier for the Iris dataset in a simple but powerful way.</p>
<p>
Let‚Äôs go the MNIST dataset now:</p>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/53af583dd8d599c7194d048ed5a2bb40?postId=6b7cca2bf000" data-media-id="53af583dd8d599c7194d048ed5a2bb40" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>

<p>
As you can see, we did the same! Let‚Äôs load the <code>
.py</code>
 file you generated again and you‚Äôll see:</p>
 <pre>
<strong>
import</strong>
 <strong>
 numpy</strong>
 <strong>
 as</strong>
 <strong>
 np</strong>
</pre>
<pre>
<strong>
from</strong>
 <strong>
 sklearn.model_selection</strong>
 <strong>
 import</strong>
 train_test_split<br>
 <strong>
from</strong>
 <strong>
 sklearn.neighbors</strong>
 <strong>
 import</strong>
 KNeighborsClassifier</pre>
 <pre>
<em>
# NOTE: Make sure that the class is labeled &#39;class&#39; in the data file</em>
<br>
tpot_data = np.recfromcsv(&#39;PATH/TO/DATA/FILE&#39;, delimiter=&#39;COLUMN_SEPARATOR&#39;, dtype=np.float64)<br>
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index(&#39;class&#39;), axis=1)<br>
training_features, testing_features, training_classes, testing_classes = \<br>
    train_test_split(features, tpot_data[&#39;class&#39;], random_state=42)</pre>
    <pre>
exported_pipeline = KNeighborsClassifier(n_neighbors=4, p=2, weights=&quot;distance&quot;)</pre>
<pre>
exported_pipeline.fit(training_features, training_classes)<br>
results = exported_pipeline.predict(testing_features)</pre>
<p>
Super easy and fun. Check them out! Try it and please give them a star!</p>
</div>
</div>
</section>
<section name="0fb4">
<div>
<hr>
</div>
<div>
<div>
<h4>
5. SHAP‚Ää‚Äî‚ÄäA unified approach to explain the output of any machine learning model</h4>
<figure>
<div style="max-width: 700px; max-height: 249px;">
<div style="padding-bottom: 35.6%;">
</div>
<div data-image-id="0*ngrNi7J-wpcwXXyO.png" data-width="800" data-height="285" data-action="zoom" data-action-value="0*ngrNi7J-wpcwXXyO.png">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*ngrNi7J-wpcwXXyO.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/0*ngrNi7J-wpcwXXyO.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/0*ngrNi7J-wpcwXXyO.png">
</noscript>
</div>
</div>
<figcaption>
<a href="https://github.com/slundberg/shap" data-href="https://github.com/slundberg/shap" rel="nofollow noopener noopener" target="_blank">
https://github.com/slundberg/shap</a>
</figcaption>
</figure>
<p>
Explaining machine learning models isn‚Äôt always easy. Yet it‚Äôs so important for a range of business applications. Luckily, there are some great libraries that help us with this task. In many applications, we need to know, understand, or prove how input variables are used in the model, and how they impact final model predictions.</p>
<p>
<strong>
SHAP</strong>
 (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods and representing the only possible consistent and locally accurate additive feature attribution method based on expectations.</p>
 <h4>
Installation</h4>
<p>
SHAP can be installed from <a href="https://pypi.org/project/shap" data-href="https://pypi.org/project/shap" rel="nofollow noopener noopener" target="_blank">
PyPI</a>
</p>
<pre>
<code>
pip install shap</code>
</pre>
<p>
or <a href="https://anaconda.org/conda-forge/shap" data-href="https://anaconda.org/conda-forge/shap" rel="nofollow noopener noopener" target="_blank">
conda-forge</a>
</p>
<pre>
<code>
conda install -c conda-forge shap</code>
</pre>
<h4>
Usage</h4>
<p>
There are tons of different models and ways to use the package. Here, I‚Äôll take one example from the DeepExplainer.</p>
<p>
Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with <a href="https://arxiv.org/abs/1704.02685" data-href="https://arxiv.org/abs/1704.02685" rel="nofollow noopener noopener" target="_blank">
DeepLIFT</a>
, as described in the SHAP NIPS paper that you can read here:</p>
<div>
<a href="https://arxiv.org/abs/1802.03888" data-href="https://arxiv.org/abs/1802.03888" title="https://arxiv.org/abs/1802.03888">
<strong>
[1802.03888] Consistent Individualized Feature Attribution for Tree Ensembles</strong>
<br>
<em>
Abstract: Interpreting predictions from tree ensemble methods such as gradient boosting machines and random forests is‚Ä¶</em>
arxiv.org</a>
<a href="https://arxiv.org/abs/1802.03888" data-media-id="41f80c621ae542df980187a09e06e2dd">
</a>
</div>
<p>
Here you can see how SHAP can be used to explain the result of a Keras model for the MNIST dataset:</p>
<figure>
<div>
<div style="padding-bottom: 35.699999999999996%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME width="700" height="250" data-src="https://heartbeat.fritz.ai//media/ad6c097f87442a1c0f685be1184368b9?postId=6b7cca2bf000" data-media-id="ad6c097f87442a1c0f685be1184368b9" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<noscript>
<div>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/ad6c097f87442a1c0f685be1184368b9?postId=6b7cca2bf000" data-media-id="ad6c097f87442a1c0f685be1184368b9" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<p>
You can find more examples here:</p>
<div>
<a href="https://github.com/slundberg/shap#sample-notebooks" data-href="https://github.com/slundberg/shap#sample-notebooks" title="https://github.com/slundberg/shap#sample-notebooks">
<strong>
slundberg/shap</strong>
<br>
<em>
A unified approach to explain the output of any machine learning model.‚Ää‚Äî‚Ääslundberg/shap</em>
github.com</a>
<a href="https://github.com/slundberg/shap#sample-notebooks" data-media-id="671e7f017fbd9f100026f8f22dbf0ae7" data-thumbnail-img-id="0*OTt8atmTEEuOaLJv" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*OTt8atmTEEuOaLJv);">
</a>
</div>
<p>
Take a look. You‚Äôll be surprised :)</p>
</div>
</div>
</section>
<section name="c611">
<div>
<hr>
</div>
<div>
<div>
<h4>
4. Optimus‚Ää‚Äî‚Ääüöö Agile Data Science Workflows made easy with Python and Spark.</h4>
<figure>
<div style="max-width: 600px; max-height: 92px;">
<div style="padding-bottom: 15.299999999999999%;">
</div>
<img data-image-id="0*fyeLdR3ChxjU2wli.png" data-width="600" data-height="92" src="https://cdn-images-1.medium.com/max/800/0*fyeLdR3ChxjU2wli.png">
</div>
<figcaption>
<a href="https://github.com/ironmussa/Optimus" data-href="https://github.com/ironmussa/Optimus" rel="nofollow noopener noopener" target="_blank">
https://github.com/ironmussa/Optimus</a>
</figcaption>
</figure>
<p>
Ok, so full disclosure, this library is like my baby. I‚Äôve been working on it for a long time now, and I‚Äôm very happy to show you version 2.</p>
<p>
Optimus V2 was created to make data cleaning a breeze. The API was designed to be super easy for newcomers and very familiar for people that come from working with pandas. Optimus expands the Spark DataFrame functionality, adding <code>
.rows</code>
 and <code>
 .cols</code>
 attributes.</p>
 <p>
With Optimus you can clean your data, prepare it, analyze it, create profilers and plots, and perform machine learning and deep learning, all in a distributed fashion, because on the back-end we have Spark, TensorFlow, and Keras.</p>
<p>
It‚Äôs super easy to us. It‚Äôs like the evolution of pandas, with a piece of dplyr, joined by Keras and Spark. The code you create with Optimus will work on your local machine, and with a simple change of masters, it can run on your local cluster or in the cloud.</p>
<p>
You will see a lot of interesting functions created to help with every step of the data science cycle.</p>
<p>
Optimus is perfect as a companion for an agile methodology for data science because it can help you in almost all the steps of the process, and it can easily connect to other libraries and tools.</p>
<p>
If you want to read more about an Agile DS Methodology check this out:</p>
<div>
<a href="http://www.business-science.io/business/2018/08/21/agile-business-science-problem-framework.html" data-href="http://www.business-science.io/business/2018/08/21/agile-business-science-problem-framework.html" title="http://www.business-science.io/business/2018/08/21/agile-business-science-problem-framework.html">
<strong>
Agile Framework For Creating An ROI-Driven Data Science Practice</strong>
<br>
<em>
Data Science is an amazing field of research that is under active development both from the academia and the industry‚Ä¶</em>
www.business-science.io</a>
<a href="http://www.business-science.io/business/2018/08/21/agile-business-science-problem-framework.html" data-media-id="8459d8a6fd1b19d08395494a2d03c99d" data-thumbnail-img-id="0*Xig6hEaxC_lNspmI" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Xig6hEaxC_lNspmI);">
</a>
</div>
<h4>
Installation (pip):</h4>
<pre>
<code>
pip install optimuspyspark</code>
</pre>
<h4>
Usage:</h4>
<p>
As one example, you can load data from a url, transform it, and apply some predefined cleaning functions:</p>
<pre>
from optimus import Optimus<br>
op = Optimus()</pre>
<pre>
# This is a custom function<br>
def func(value, arg):<br>
    return &quot;this was a number&quot;<br>
    <br>
df =op.load.url(&quot;https://raw.githubusercontent.com/ironmussa/Optimus/master/examples/foo.csv&quot;)</pre>
<pre>
df\<br>
    .rows.sort(&quot;product&quot;,&quot;desc&quot;)\<br>
    .cols.lower([&quot;firstName&quot;,&quot;lastName&quot;])\<br>
    .cols.date_transform(&quot;birth&quot;, &quot;new_date&quot;, &quot;yyyy/MM/dd&quot;, &quot;dd-MM-YYYY&quot;)\<br>
    .cols.years_between(&quot;birth&quot;, &quot;years_between&quot;, &quot;yyyy/MM/dd&quot;)\<br>
    .cols.remove_accents(&quot;lastName&quot;)\<br>
    .cols.remove_special_chars(&quot;lastName&quot;)\<br>
    .cols.replace(&quot;product&quot;,&quot;taaaccoo&quot;,&quot;taco&quot;)\<br>
    .cols.replace(&quot;product&quot;,[&quot;piza&quot;,&quot;pizzza&quot;],&quot;pizza&quot;)\<br>
    .rows.drop(df[&quot;id&quot;]&lt;7)\<br>
    .cols.drop(&quot;dummyCol&quot;)\<br>
    .cols.rename(str.lower)\<br>
    .cols.apply_by_dtypes(&quot;product&quot;,func,&quot;string&quot;, data_type=&quot;integer&quot;)\<br>
    .cols.trim(&quot;*&quot;)\<br>
    .show()</pre>
    <p>
You can transform this:</p>
<figure>
<div>
<div style="padding-bottom: 35.699999999999996%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME width="700" height="250" data-src="https://heartbeat.fritz.ai//media/b6e1a97b5c55b86142e94e0b03c78888?postId=6b7cca2bf000" data-media-id="b6e1a97b5c55b86142e94e0b03c78888" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<noscript>
<div>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/b6e1a97b5c55b86142e94e0b03c78888?postId=6b7cca2bf000" data-media-id="b6e1a97b5c55b86142e94e0b03c78888" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<p>
into this:</p>
<figure>
<div>
<div style="padding-bottom: 35.699999999999996%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME width="700" height="250" data-src="https://heartbeat.fritz.ai//media/b5b75077e237a257683500e4e5b041e0?postId=6b7cca2bf000" data-media-id="b5b75077e237a257683500e4e5b041e0" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<noscript>
<div>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/b5b75077e237a257683500e4e5b041e0?postId=6b7cca2bf000" data-media-id="b5b75077e237a257683500e4e5b041e0" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<p>
Pretty cool, right?</p>
<p>
You can do a thousand more things with the library, so please check it out:</p>
<div>
<a href="https://www.hioptimus.com/" data-href="https://www.hioptimus.com/" title="https://www.hioptimus.com/">
<strong>
Optimus‚Ää‚Äî‚ÄäData cleansing and exploration made simple</strong>
<br>
<em>
Prepare, process and explore your Big Data with the fastest open source library on the planet using Apache Spark and‚Ä¶</em>
www.hioptimus.com</a>
<a href="https://www.hioptimus.com/" data-media-id="a0d33e31f0aed5c84ac716bab9b43e5d">
</a>
</div>
</div>
</div>
</section>
<section name="641d">
<div>
<hr>
</div>
<div>
<div>
<h4>
3. spacy‚Ää‚Äî‚ÄäIndustrial-strength Natural Language Processing (NLP) with Python and Cython</h4>
<figure>
<div style="max-width: 700px; max-height: 231px;">
<div style="padding-bottom: 33%;">
</div>
<div data-image-id="0*U7FbmTp8AJP0QNih.jpg" data-width="800" data-height="264" data-action="zoom" data-action-value="0*U7FbmTp8AJP0QNih.jpg">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*U7FbmTp8AJP0QNih.jpg?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/0*U7FbmTp8AJP0QNih.jpg">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/0*U7FbmTp8AJP0QNih.jpg">
</noscript>
</div>
</div>
<figcaption>
<a href="https://spacy.io/" data-href="https://spacy.io/" rel="nofollow noopener noopener" target="_blank">
https://spacy.io/</a>
</figcaption>
</figure>
<p>
<a href="https://spacy.io/" data-href="https://spacy.io/" rel="noopener" target="_blank">
From the creators</a>
:</p>
<blockquote>
<em>
spaCy is designed to help you do real work‚Ää‚Äî‚Ääto build real products, or gather real insights. The library respects your time, and tries to avoid wasting it. It‚Äôs easy to install, and its API is simple and productive. We like to think of spaCy as the Ruby on Rails of Natural Language Processing.</em>
</blockquote>
<p>
spaCy is the best way to prepare text for deep learning. It interoperates seamlessly with TensorFlow, PyTorch, Scikit-learn, Gensim, and the rest of Python‚Äôs awesome AI ecosystem. With spaCy, you can easily construct linguistically sophisticated statistical models for a variety of NLP problems.</p>
<h4>
Installation:</h4>
<pre>
pip3 install spacy<br>
$ python3 -m spacy download en</pre>
<p>
Here, we‚Äôre also downloading the English language model. You can find models for German, Spanish, Italian, Portuguese, French, and more here:</p>
<div>
<a href="https://spacy.io/models/" data-href="https://spacy.io/models/" title="https://spacy.io/models/">
<strong>
Models Overview ¬∑ spaCy Models Documentation</strong>
<br>
<em>
spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency‚Ä¶</em>
spacy.io</a>
<a href="https://spacy.io/models/" data-media-id="74f03e0cfceaaaff2208a3d1888aeed5" data-thumbnail-img-id="0*MbtBOjpTVHwmNUoB" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*MbtBOjpTVHwmNUoB);">
</a>
</div>
<p>
Here‚Äôs an example from the main webpage:</p>
<pre>
# python -m spacy download en_core_web_sm</pre>
<pre>
import spacy</pre>
<pre>
# Load English tokenizer, tagger, parser, NER and word vectors<br>
nlp = spacy.load(&#39;en_core_web_sm&#39;)</pre>
<pre>
# Process whole documents<br>
text = (u&quot;When Sebastian Thrun started working on self-driving cars at &quot;<br>
        u&quot;Google in 2007, few people outside of the company took him &quot;<br>
        u&quot;seriously. ‚ÄúI can tell you very senior CEOs of major American &quot;<br>
        u&quot;car companies would shake my hand and turn away because I wasn‚Äôt &quot;<br>
        u&quot;worth talking to,‚Äù said Thrun, now the co-founder and CEO of &quot;<br>
        u&quot;online higher education startup Udacity, in an interview with &quot;<br>
        u&quot;Recode earlier this week.&quot;)<br>
    doc = nlp(text)</pre>
<pre>
# Find named entities, phrases and concepts<br>
for entity in doc.ents:<br>
    print(entity.text, entity.label_)</pre>
    <pre>
# Determine semantic similarities<br>
doc1 = nlp(u&quot;my fries were super gross&quot;)<br>
doc2 = nlp(u&quot;such disgusting fries&quot;)<br>
similarity = doc1.similarity(doc2)<br>
print(doc1.text, doc2.text, similarity)</pre>
<p>
In this example, we first download the English tokenizer, tagger, parser, NER, and word vectors. Then we create some text, and finally we print the entities, phrases, and concepts found, and then we determine the semantic similarity of the two phrases. If you run this code you get this:</p>
<pre>
Sebastian Thrun PERSON<br>
Google ORG<br>
2007 DATE<br>
American NORP<br>
Thrun PERSON<br>
Recode ORG<br>
earlier this week DATE<br>
my fries were super gross such disgusting fries 0.7139701635071919</pre>
<p>
Very simple and super useful. There is also a spaCy Universe, where you can find great resources developed with or for spaCy. It includes standalone packages, plugins, extensions, educational materials, operational utilities, and bindings for other languages:</p>
<div>
<a href="https://spacy.io/universe/" data-href="https://spacy.io/universe/" title="https://spacy.io/universe/">
<strong>
Universe ¬∑ spaCy</strong>
<br>
<em>
This section collects the many great resources developed with or for spaCy. It includes standalone packages, plugins‚Ä¶</em>
spacy.io</a>
<a href="https://spacy.io/universe/" data-media-id="1147c30ffd868e1a09594823d48fbb17" data-thumbnail-img-id="0*c80SvI2k2GHafAEX" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*c80SvI2k2GHafAEX);">
</a>
</div>
<p>
By the way, the usage page is great, with very good explanations and code:</p>
<div>
<a href="https://spacy.io/usage/" data-href="https://spacy.io/usage/" title="https://spacy.io/usage/">
<strong>
Install spaCy ¬∑ spaCy Usage Documentation</strong>
<br>
<em>
spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency‚Ä¶</em>
spacy.io</a>
<a href="https://spacy.io/usage/" data-media-id="47a02286d2d7ebce7e74e57dd804071e" data-thumbnail-img-id="0*YY9mAul0xv4osbLo" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*YY9mAul0xv4osbLo);">
</a>
</div>
<p>
Take a look at the visualizers page. Awesome features, here:</p>
<div>
<a href="https://spacy.io/usage/visualizers" data-href="https://spacy.io/usage/visualizers" title="https://spacy.io/usage/visualizers">
<strong>
Visualizers ¬∑ spaCy Usage Documentation</strong>
<br>
<em>
spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency‚Ä¶</em>
spacy.io</a>
<a href="https://spacy.io/usage/visualizers" data-media-id="8f80feb49325ebb12c7f5f46abe3e4fe" data-thumbnail-img-id="0*r9BMgQcUhWGX7xWG" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*r9BMgQcUhWGX7xWG);">
</a>
</div>
</div>
</div>
</section>
<section name="627b">
<div>
<hr>
</div>
<div>
<div>
<h4>
2. jupytext‚Ää‚Äî‚ÄäJupyter notebooks as Markdown Documents, Julia, Python or R scripts</h4>
<figure>
<div style="max-width: 700px; max-height: 467px;">
<div style="padding-bottom: 66.7%;">
</div>
<div data-image-id="0*XnCNk_zbycIRa9Bk" data-width="940" data-height="627" data-action="zoom" data-action-value="0*XnCNk_zbycIRa9Bk">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*XnCNk_zbycIRa9Bk?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/0*XnCNk_zbycIRa9Bk">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/0*XnCNk_zbycIRa9Bk">
</noscript>
</div>
</div>
</figure>
<p>
For me, this is one of the packages of the year. It‚Äôs such an important part of what we do as data scientists. Almost all of us work in notebooks like Jupyter, but we also use IDEs like PyCharm for more hardcore parts of our projects.</p>
<p>
The good news is that plain scripts, which you can draft and test in your favorite IDE, open transparently as notebooks in Jupyter when using Jupytext. Run the notebook in Jupyter to generate the outputs, <a href="https://github.com/mwouts/jupytext#paired-notebooks" data-href="https://github.com/mwouts/jupytext#paired-notebooks" rel="noopener" target="_blank">
associate</a>
 an <code>
 .ipynb </code>
representation, and save and share your research as either a plain script or as a traditional Jupyter notebook with outputs.</p>
<p>
You can see a workflow of what you can do with the package in the gif below:</p>
</div>
<img src="https://cdn-images-1.medium.com/max/2000/0*ibKGix2NMWaTLpPR.gif">

<div>
<h2>Installation</h2>
<p>
Install Jupytext with:</p>
<pre>
pip install jupytext --upgrade</pre>
<p>
Then, configure Jupyter to use Jupytext:</p>
<ul>
<li>
generate a Jupyter config, if you don‚Äôt have one yet, with <code>
jupyter notebook --generate-config</code>
</li>
<li>
edit <code>
.jupyter/jupyter_notebook_config.py</code>
 and append the following:</li>
</ul>
<pre>
c.NotebookApp.contents_manager_class = &quot;jupytext.TextFileContentsManager&quot;</pre>
<ul>
<li>
and restart Jupyter, i.e. run:</li>
</ul>
<pre>
jupyter notebook</pre>
<p>
You can give it a try here:</p>
<div>
<a href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" data-href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" title="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo">
<strong>
Binder (beta)</strong>
<br>
</a>
<a href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" data-href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" rel="nofollow">
https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo</a>
<a href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" data-href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" title="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo">
mybinder.org</a>
<a href="https://mybinder.org/v2/gh/mwouts/jupytext/master?filepath=demo" data-media-id="0bbec893229e74c7ed508b8eef03c8ff">
</a>
</div>
</div>
</div>
</section>
<section name="558c">
<div>
<hr>
</div>
<div>
<div>
<h4>
1. Chartify‚Ää‚Äî‚ÄäPython library that makes it easy for data scientists to create charts.</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*mlxHDfxOQtvHJw36.png">
<a href="https://xkcd.com/1945/" data-href="https://xkcd.com/1945/" rel="nofollow noopener noopener" target="_blank">
https://xkcd.com/1945/</a>
<p>
This, for me, is the winner of the year, for Python. If you are in the Python world, most likely you waste a lot of your time trying to create a decent plot. Luckily, we have libraries like Seaborn that make our life easier. But the issue is that their plots are not dynamic.</p>
<p>
Then you have Bokeh‚Äîan amazing library‚Äîbut creating interactive plots with it can be a pain in the a**. If you want to know more about Bokeh and interactive plots for Data Science, take a look at these great articles by <a href="https://medium.com/@williamkoehrsen" data-href="https://medium.com/@williamkoehrsen" data-anchor-type="2" data-user-id="e2f299e30cb9" data-action-value="e2f299e30cb9" data-action="show-user-card" data-action-type="hover" target="_blank">
William Koehrsen</a>
:</p>
<div>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4" data-href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4" title="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4" rel="nofollow">
<strong>
Data Visualization with Bokeh in Python, Part I: Getting Started</strong>
<br>
<em>
Elevate your visualization game</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-one-getting-started-a11655a467d4" data-media-id="752a6102c992a34b2c918f266cdde201" data-thumbnail-img-id="1*oZ77rKfBGgaIfqqyBLSyew.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*oZ77rKfBGgaIfqqyBLSyew.jpeg);">
</a>
</div>
<div>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512" data-href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512" title="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512" rel="nofollow">
<strong>
Data Visualization with Bokeh in Python, Part II: Interactions</strong>
<br>
<em>
Moving beyond static plots</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-ii-interactions-a4cf994e2512" data-media-id="88ebe6361700d9a5d70c49585e65a596" data-thumbnail-img-id="1*6ROhPANEBYJb8om0kVDVyQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*6ROhPANEBYJb8om0kVDVyQ.jpeg);">
</a>
</div>
<div>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23" data-href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23" title="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23" rel="nofollow">
<strong>
Data Visualization with Bokeh in Python, Part III: Making a Complete Dashboard</strong>
<br>
<em>
Creating an interactive visualization application in Bokeh</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/data-visualization-with-bokeh-in-python-part-iii-a-complete-dashboard-dc6a86aa6e23" data-media-id="f78f41775807662f4ca81859601bc70b" data-thumbnail-img-id="1*wWPUyFSC0LlX960L3FTeDQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*wWPUyFSC0LlX960L3FTeDQ.jpeg);">
</a>
</div>
<p>
Chartify is built in top of Bokeh. But it‚Äôs also so much simpler.</p>
<p>
From the authors:</p>
<h4>
Why use Chartify?</h4>
<ul>
<li>
Consistent input data format: Spend less time transforming data to get your charts to work. All plotting functions use a consistent tidy input data format.</li>
<li>
Smart default styles: Create pretty charts with very little customization required.</li>
<li>
Simple API: We‚Äôve attempted to make to the API as intuitive and easy to learn as possible.</li>
<li>
Flexibility: Chartify is built on top of <a href="http://bokeh.pydata.org/en/latest/" data-href="http://bokeh.pydata.org/en/latest/" rel="nofollow noopener noopener" target="_blank">
Bokeh</a>
, so if you do need more control you can always fall back on Bokeh‚Äôs API.</li>
</ul>
<h4>
Installation</h4>
<ol>
<li>
Chartify can be installed via pip:</li>
</ol>
<pre>
<code>
pip3 install chartify</code>
</pre>
<p>
2. Install chromedriver requirement (Optional. Needed for PNG output):</p>
<ul>
<li>
Install Google Chrome.</li>
<li>
Download the appropriate version of chromedriver for your OS <a href="https://sites.google.com/a/chromium.org/chromedriver/downloads" data-href="https://sites.google.com/a/chromium.org/chromedriver/downloads" rel="nofollow noopener noopener" target="_blank">
here</a>
.</li>
<li>
Copy the executable file to a directory within your PATH.</li>
<li>
View directories in your PATH variable: <code>
echo $PATH</code>
</li>
<li>
Copy chromedriver to the appropriate directory, e.g.: <code>
cp chromedriver /usr/local/bin</code>
</li>
</ul>
<h4>
Usage</h4>
<p>
Let‚Äôs say we want to create this chart:</p>
<img src="https://cdn-images-1.medium.com/max/800/1*fZaVJz_a57v1KofWElXMtw.png">

<pre>
import pandas as pd<br>
import chartify</pre>
<pre>
# Generate example data<br>
data = chartify.examples.example_data()</pre>
<p>
Now that we have some example data loaded let‚Äôs do some transformations:</p>
<pre>
total_quantity_by_month_and_fruit = (data.groupby(<br>
        [data[&#39;date&#39;] + pd.offsets.MonthBegin(-1), &#39;fruit&#39;])[&#39;quantity&#39;].sum()<br>
        .reset_index().rename(columns={&#39;date&#39;: &#39;month&#39;})<br>
        .sort_values(&#39;month&#39;))<br>
    print(total_quantity_by_month_and_fruit.head())</pre>
<pre>
month          fruit     quantity<br>
0 2017-01-01   Apple         7<br>
1 2017-01-01  Banana         6<br>
2 2017-01-01   Grape         1<br>
3 2017-01-01  Orange         2<br>
4 2017-02-01   Apple         8</pre>
<p>
And now we can plot it:</p>
<pre>
# Plot the data<br>
ch = chartify.Chart(blank_labels=True, x_axis_type=&#39;datetime&#39;)<br>
ch.set_title(&quot;Stacked area&quot;)<br>
ch.set_subtitle(&quot;Represent changes in distribution.&quot;)<br>
ch.plot.area(<br>
        data_frame=total_quantity_by_month_and_fruit,<br>
        x_column=&#39;month&#39;,<br>
        y_column=&#39;quantity&#39;,<br>
        color_column=&#39;fruit&#39;,<br>
        stacked=True)<br>
    ch.show(&#39;png&#39;)</pre>
<p>
Super easy to create a plot, and it‚Äôs interactive. If you want more examples to create stuff like this:</p>
<img src="https://cdn-images-1.medium.com/max/800/0*TnHdpOgjtGR1uRJ-.png">

<img src="https://cdn-images-1.medium.com/max/800/0*MIWlOMxw0AdGqPEj.png">

<img src="https://cdn-images-1.medium.com/max/800/0*DqpaqVdpWgnxXKYL.png">
<p>
And more, check the original repo:</p>
<div>
<a href="https://github.com/spotify/chartify" data-href="https://github.com/spotify/chartify" title="https://github.com/spotify/chartify" rel="nofollow">
<strong>
spotify/chartify</strong>
<br>
<em>
Python library that makes it easy for data scientists to create charts.‚Ää‚Äî‚Ääspotify/chartify</em>
github.com</a>
<a href="https://github.com/spotify/chartify" data-media-id="5df83eefff316449a8bd037f3d863f3c" data-thumbnail-img-id="0*avRm1FWrHtgpgWgn" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*avRm1FWrHtgpgWgn);">
</a>
</div>
</div>
</div>
</section>
<section name="bee1">
<div>
<hr>
</div>
<div>
<div>
<h3>Top 7 for R</h3>
<figure>
<div>
<img data-image-id="1*dYE_cme1yJfb5_vNDQitqg.gif" src="https://cdn-images-1.medium.com/max/800/1*dYE_cme1yJfb5_vNDQitqg.gif">
</div>
</figure>
<h4>
7. infer‚Ää‚Äî‚ÄäAn R package for tidyverse-friendly statistical inference</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*l7j5V5phw3xSluQl.png">
<a href="https://github.com/tidymodels/infer" data-href="https://github.com/tidymodels/infer" rel="nofollow noopener noopener" target="_blank">
https://github.com/tidymodels/infer</a>
<p>
Inference, or statistical inference, is the process of using data analysis to deduce properties of an underlying probability distribution.</p>
<p>
The objective of this package is to perform statistical inference using an expressive statistical grammar that coheres with the <code>
tidyverse</code>
 design framework.</p>
 <h4>Installation</h4>
<p>
To install the current stable version of <code>
infer</code>
 from CRAN:</p>
 <pre>
install.packages(&quot;infer&quot;)</pre>
<h4>Usage</h4>
<p>
Let‚Äôs try a simple example on the <code>
mtcars</code>
 dataset to see what the library can do for us.</p>
 <p>
First, let‚Äôs overwrite <code>
mtcars</code>
 so that the variables <code>
 cyl</code>
, <code>
vs</code>
, <code>
am</code>
, <code>
gear</code>
, and <code>
carb</code>
 are <code>
 factor</code>
s.</p>
<pre>
<code>
library(infer)<br>
library(dplyr)<br>
mtcars &lt;- mtcars %&gt;%<br>
  mutate(cyl = factor(cyl),<br>
         vs = factor(vs),<br>
         am = factor(am),<br>
         gear = factor(gear),<br>
         carb = factor(carb))<br>
         # For reproducibility         <br>
set.seed(2018)</code>
</pre>
<p>
We‚Äôll try hypothesis testing. Here, a hypothesis is proposed so that it‚Äôs testable on the basis of observing a process that‚Äôs modeled via a set of random variables. Normally, two statistical data sets are compared, or a data set obtained by sampling is compared against a synthetic data set from an idealized model.</p>
<pre>
<code>
mtcars %&gt;%<br>
  <a href="http://infer.netlify.com/reference/specify.html" data-href="http://infer.netlify.com/reference/specify.html" rel="noopener" target="_blank">
  specify</a>
(response = mpg) %&gt;% # formula alt: mpg ~ NULL<br>
  <a href="http://infer.netlify.com/reference/hypothesize.html" data-href="http://infer.netlify.com/reference/hypothesize.html" rel="noopener" target="_blank">
  hypothesize</a>
(null = &quot;point&quot;, med = 26) %&gt;% <br>
  <a href="http://infer.netlify.com/reference/generate.html" data-href="http://infer.netlify.com/reference/generate.html" rel="noopener" target="_blank">
  generate</a>
(reps = 100, type = &quot;bootstrap&quot;) %&gt;% <br>
  <a href="http://infer.netlify.com/reference/calculate.html" data-href="http://infer.netlify.com/reference/calculate.html" rel="noopener" target="_blank">
  calculate</a>
(stat = &quot;median&quot;)</code>
</pre>
<p>
Here, we first specify the response and explanatory variables, then we declare a null hypothesis. After that, we generate resamples using bootstrap and finally calculate the median. The result of that code is:</p>
<pre>
<code>
## # A tibble: 100 x 2<br>
##    replicate  stat<br>
##        &lt;int&gt; &lt;dbl&gt;<br>
##  1         1  26.6<br>
##  2         2  25.1<br>
##  3         3  25.2<br>
##  4         4  24.7<br>
##  5         5  24.6<br>
##  6         6  25.8<br>
##  7         7  24.7<br>
##  8         8  25.6<br>
##  9         9  25.0<br>
## 10        10  25.1<br>
## # ... with 90 more rows</code>
</pre>
<p>
One of the greatest parts of this library is the <code>
visualize</code>
 function. This will allow you to visualize the distribution of the simulation-based inferential statistics or the theoretical distribution (or both). For an example, let‚Äôs use the flights data set. First, let‚Äôs do some data preparation:</p>
 <pre>
<code>
library(nycflights13)
library(dplyr)
library(ggplot2)
library(stringr)
library(infer)
set.seed(2017)
fli_small &lt;- flights %&gt;% 
  na.omit() %&gt;% 
  <a href="http://dplyr.tidyverse.org/reference/sample.html" data-href="http://dplyr.tidyverse.org/reference/sample.html" rel="noopener" target="_blank">
  sample_n</a>
(size = 500) %&gt;% 
  <a href="http://dplyr.tidyverse.org/reference/mutate.html" data-href="http://dplyr.tidyverse.org/reference/mutate.html" rel="noopener" target="_blank">
  mutate</a>
(season = <a href="http://dplyr.tidyverse.org/reference/case_when.html" data-href="http://dplyr.tidyverse.org/reference/case_when.html" rel="noopener" target="_blank">
case_when</a>
(
    month %in% c(10:12, 1:3) ~ &quot;winter&quot;,
    month %in% c(4:9) ~ &quot;summer&quot;
  )) %&gt;% 
  <a href="http://dplyr.tidyverse.org/reference/mutate.html" data-href="http://dplyr.tidyverse.org/reference/mutate.html" rel="noopener" target="_blank">
  mutate</a>
(day_hour = <a href="http://dplyr.tidyverse.org/reference/case_when.html" data-href="http://dplyr.tidyverse.org/reference/case_when.html" rel="noopener" target="_blank">
case_when</a>
(
    <a href="http://dplyr.tidyverse.org/reference/between.html" data-href="http://dplyr.tidyverse.org/reference/between.html" rel="noopener" target="_blank">
    between</a>
(hour, 1, 12) ~ &quot;morning&quot;,
    <a href="http://dplyr.tidyverse.org/reference/between.html" data-href="http://dplyr.tidyverse.org/reference/between.html" rel="noopener" target="_blank">
    between</a>
(hour, 13, 24) ~ &quot;not morning&quot;
  )) %&gt;% 
  <a href="http://dplyr.tidyverse.org/reference/select.html" data-href="http://dplyr.tidyverse.org/reference/select.html" rel="noopener" target="_blank">
  select</a>
(arr_delay, dep_delay, season, 
         day_hour, origin, carrier)</code>
     </pre>
<p>
And now we can run a randomization approach to <code>
œá2-statistic</code>
:</p>
<pre>
<code>
chisq_null_distn &lt;- fli_small %&gt;%<br>
  <a href="http://infer.netlify.com/reference/specify.html" data-href="http://infer.netlify.com/reference/specify.html" rel="noopener" target="_blank">
  specify</a>
(origin ~ season) %&gt;% # alt: response = origin, explanatory = season<br>
  <a href="http://infer.netlify.com/reference/hypothesize.html" data-href="http://infer.netlify.com/reference/hypothesize.html" rel="noopener" target="_blank">
  hypothesize</a>
(null = &quot;independence&quot;) %&gt;%<br>
  <a href="http://infer.netlify.com/reference/generate.html" data-href="http://infer.netlify.com/reference/generate.html" rel="noopener" target="_blank">
  generate</a>
(reps = 1000, type = &quot;permute&quot;) %&gt;%<br>
  <a href="http://infer.netlify.com/reference/calculate.html" data-href="http://infer.netlify.com/reference/calculate.html" rel="noopener" target="_blank">
  calculate</a>
(stat = &quot;Chisq&quot;)<br>
chisq_null_distn %&gt;% <a href="http://infer.netlify.com/reference/visualize.html" data-href="http://infer.netlify.com/reference/visualize.html" rel="noopener" target="_blank">
visualize</a>
(obs_stat = obs_chisq, direction = &quot;greater&quot;)</code>
</pre>
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/0*kPwXn02LclmBEhEL.png">

<p>
or see the theoretical distribution:</p>
<pre>
<code>
fli_small %&gt;%<br>
  <a href="http://infer.netlify.com/reference/specify.html" data-href="http://infer.netlify.com/reference/specify.html" rel="noopener" target="_blank">
  specify</a>
(origin ~ season) %&gt;% <br>
  <a href="http://infer.netlify.com/reference/hypothesize.html" data-href="http://infer.netlify.com/reference/hypothesize.html" rel="noopener" target="_blank">
  hypothesize</a>
(null = &quot;independence&quot;) %&gt;%<br>
  # generate() ## Not used for theoretical<br>
  <a href="http://infer.netlify.com/reference/calculate.html" data-href="http://infer.netlify.com/reference/calculate.html" rel="noopener" target="_blank">
  calculate</a>
(stat = &quot;Chisq&quot;) %&gt;%<br>
  <a href="http://infer.netlify.com/reference/visualize.html" data-href="http://infer.netlify.com/reference/visualize.html" rel="noopener" target="_blank">
  visualize</a>
(method = &quot;theoretical&quot;, obs_stat = obs_chisq, direction = &quot;right&quot;)</code>
</pre>
<figure>
<div>
<img data-image-id="0*K_YC5NQYbQZNJwT-.png" src="https://cdn-images-1.medium.com/max/800/0*K_YC5NQYbQZNJwT-.png">
</div>
</figure>
<p>
For more on this package visit:</p>
<div>
<a href="http://infer.netlify.com/index.html" data-href="http://infer.netlify.com/index.html" title="http://infer.netlify.com/index.html">
<strong>
Tidy Statistical Inference</strong>
<br>
<em>
The objective of this package is to perform inference using an expressive statistical grammar that coheres with the‚Ä¶</em>
infer.netlify.com</a>
<a href="http://infer.netlify.com/index.html" data-media-id="829a95cd40736675ab0b2f4dfc6d59f6">
</a>
</div>
</div>
</div>
</section>
<section name="ce20">
<div>
<hr>
</div>
<div>
<div>
<h4>6. janitor‚Ää‚Äî‚Ääsimple tools for data cleaning in R</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*R4f3znrYLI7mkN7h.png">
<a href="https://github.com/sfirke/janitor" data-href="https://github.com/sfirke/janitor" rel="nofollow noopener noopener" target="_blank">
https://github.com/sfirke/janitor</a>

<p>
Data cleansing is a topic very close to me. I‚Äôve been working with my team at <a href="https://iron-ai.com/" data-href="https://iron-ai.com/" rel="noopener" target="_blank">
Iron-AI</a>
 to create a tool for Python called Optimus. You can see more about it here:</p>
 <div>
<a href="http://hioptimus.com/" data-href="http://hioptimus.com/" title="http://hioptimus.com/">
<strong>
Data cleansing and exploration with Python and Apache Spark‚Ää‚Äî‚ÄäBig Data and Data Science‚Ää‚Äî‚ÄäOptimus</strong>
<br>
<em>
The group of BBVA Data &amp; Analytics in Mexico has been using Optimus for the past months and we have boosted our‚Ä¶</em>
hioptimus.com</a>
<a href="http://hioptimus.com/" data-media-id="7e1f8113a55b59e1e6d2aa03bd20658a" data-thumbnail-img-id="0*s336Prf0qH1QbXBr" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*s336Prf0qH1QbXBr);">
</a>
</div>
<p>
But this tool I‚Äôm showing you is a very cool package with simple functions for data cleaning.</p>
<p>
It has three main functions:</p>
<ul>
<li>
perfectly format <code>
data.frame</code>
 column names;</li>
 <li>
create and format frequency tables of one, two, or three variables (think an improved <code>
table()</code>
; and</li>
<li>
isolate partially-duplicate records.</li>
</ul>
<p>
Oh, and it‚Äôs a <a href="https://github.com/hadley/tidyverse/blob/master/vignettes/manifesto.Rmd" data-href="https://github.com/hadley/tidyverse/blob/master/vignettes/manifesto.Rmd" rel="noopener" target="_blank">
tidyverse</a>
-oriented package. Specifically, it works nicely with the <code>
%&gt;%</code>
 pipe and is optimized for cleaning data brought in with the <a href="https://github.com/tidyverse/readr" data-href="https://github.com/tidyverse/readr" rel="noopener" target="_blank">
 readr</a>
 and <a href="https://github.com/tidyverse/readxl" data-href="https://github.com/tidyverse/readxl" rel="noopener" target="_blank">
 readxl</a>
 packages.</p>
 <h4>Installation</h4>
<pre>
install.packages(&quot;janitor&quot;)</pre>
<h4>Usage</h4>
<p>
I‚Äôm using the example from the repo, and the data <code>
dirty_data.xlsx</code>
.</p>
<pre>
library(pacman) # for loading packages<br>
p_load(readxl, janitor, dplyr, here)</pre>
<pre>
roster_raw &lt;- read_excel(here(&quot;dirty_data.xlsx&quot;)) # available at <a href="http://github.com/sfirke/janitor" data-href="http://github.com/sfirke/janitor" rel="noopener" target="_blank">
http://github.com/sfirke/janitor</a>
<br>
glimpse(roster_raw)<br>
#&gt; Observations: 13<br>
#&gt; Variables: 11<br>
#&gt; $ `First Name`        &lt;chr&gt; &quot;Jason&quot;, &quot;Jason&quot;, &quot;Alicia&quot;, &quot;Ada&quot;, &quot;Desus&quot;, &quot;Chien-Shiung&quot;, &quot;Chien-Shiung&quot;, N...<br>
#&gt; $ `Last Name`         &lt;chr&gt; &quot;Bourne&quot;, &quot;Bourne&quot;, &quot;Keys&quot;, &quot;Lovelace&quot;, &quot;Nice&quot;, &quot;Wu&quot;, &quot;Wu&quot;, NA, &quot;Joyce&quot;, &quot;Lam...<br>
#&gt; $ `Employee Status`   &lt;chr&gt; &quot;Teacher&quot;, &quot;Teacher&quot;, &quot;Teacher&quot;, &quot;Teacher&quot;, &quot;Administration&quot;, &quot;Teacher&quot;, &quot;Tea...<br>
#&gt; $ Subject             &lt;chr&gt; &quot;PE&quot;, &quot;Drafting&quot;, &quot;Music&quot;, NA, &quot;Dean&quot;, &quot;Physics&quot;, &quot;Chemistry&quot;, NA, &quot;English&quot;,...<br>
#&gt; $ `Hire Date`         &lt;dbl&gt; 39690, 39690, 37118, 27515, 41431, 11037, 11037, NA, 32994, 27919, 42221, 347...<br>
#&gt; $ `% Allocated`       &lt;dbl&gt; 0.75, 0.25, 1.00, 1.00, 1.00, 0.50, 0.50, NA, 0.50, 0.50, NA, NA, 0.80<br>
#&gt; $ `Full time?`        &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, NA, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, ...<br>
#&gt; $ `do not edit! ---&gt;` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA<br>
#&gt; $ Certification       &lt;chr&gt; &quot;Physical ed&quot;, &quot;Physical ed&quot;, &quot;Instr. music&quot;, &quot;PENDING&quot;, &quot;PENDING&quot;, &quot;Science ...<br>
#&gt; $ Certification__1    &lt;chr&gt; &quot;Theater&quot;, &quot;Theater&quot;, &quot;Vocal music&quot;, &quot;Computers&quot;, NA, &quot;Physics&quot;, &quot;Physics&quot;, N...<br>
#&gt; $ Certification__2    &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA</pre>
<p>
With this:</p>
<pre>
roster &lt;- roster_raw %&gt;%<br>
  clean_names() %&gt;%<br>
  remove_empty(c(&quot;rows&quot;, &quot;cols&quot;)) %&gt;%<br>
  mutate(hire_date = excel_numeric_to_date(hire_date),<br>
         cert = coalesce(certification, certification_1)) %&gt;% # from dplyr<br>
  select(-certification, -certification_1) # drop unwanted columns</pre>
  <p>
With the <code>
clean_names()</code>
 function, we‚Äôre telling R that we‚Äôre about to use janitor. Then we clean the empty rows and columns, and then using dplyr we change the format for the dates, create a new column with the information of <code>
 certification</code>
<em>
 </em>
 and <code>
certification_1</code>
, and then drop them.</p>
<p>
And with this piece of code‚Ä¶</p>
<pre>
roster %&gt;% get_dupes(first_name, last_name)</pre>
<p>
we can find duplicated records that have the same name and last name.</p>
<p>
The package also introduces the <em>
tabyl </em>
function that tabulates the data, like <em>
table </em>
but pipe-able, data.frame-based, and fully featured. For example:</p>
<pre>
roster %&gt;%<br>
  tabyl(subject)<br>
  #&gt;     subject n    percent valid_percent<br>
#&gt;  Basketball 1 0.08333333           0.1<br>
#&gt;   Chemistry 1 0.08333333           0.1<br>
#&gt;        Dean 1 0.08333333           0.1<br>
#&gt;    Drafting 1 0.08333333           0.1<br>
#&gt;     English 2 0.16666667           0.2<br>
#&gt;       Music 1 0.08333333           0.1<br>
#&gt;          PE 1 0.08333333           0.1<br>
#&gt;     Physics 1 0.08333333           0.1<br>
#&gt;     Science 1 0.08333333           0.1<br>
#&gt;        &lt;NA&gt; 2 0.16666667            NA</pre>
<p>
You can do a lot more things with the package, so visit their site and give them some love :)</p>
</div>
</div>
</section>
<section name="a63e">
<div>
<hr>
</div>
<div>
<div>
<h4>5. Esquisse‚Ää‚Äî‚ÄäRStudio add-in to make plots with ggplot2</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*VpP28-xHHFTjJG9S.png">
<a href="https://github.com/dreamRs/esquisse" data-href="https://github.com/dreamRs/esquisse" rel="nofollow noopener noopener" target="_blank">
https://github.com/dreamRs/esquisse</a>
<p>
This add-in allows you to interactively explore your data by visualizing it with the ggplot2 package. It allows you to draw bar graphs, curves, scatter plots, and histograms, and then export the graph or retrieve the code generating the graph.</p>
<h4>Installation</h4>
<p>
Install from CRAN with :</p>
<pre>
# From CRAN<br>
install.packages(&quot;esquisse&quot;)</pre>
<h4>Usage</h4>
<p>
Then launch the add-in via the RStudio menu. If you don‚Äôt have <code>
data.frame</code>
in your environment, datasets in <code>
ggplot2</code>
 are used.</p>
 <p>
<code>
<strong>
ggplot2</strong>
</code>
<strong>
 builder addin</strong>
</p>
<img src="https://cdn-images-1.medium.com/max/800/0*c2Vy8L1cU9l9iGSS.gif">
<p>
Launch the add-in via the RStudio menu or with:</p>
<pre>
esquisse::esquisser()</pre>
<p>
The first step is to choose a <code>
data.frame</code>
:</p>
<img src="https://cdn-images-1.medium.com/max/800/0*DFY7Oju6WJi5ZNcF.png">
<p>
Or you can use a dataset directly with:</p>
<pre>
esquisse::esquisser(data = iris)</pre>
<p>
After that, you can drag and drop variables to create a plot:</p>
<img src="https://cdn-images-1.medium.com/max/800/0*BVW_U69wAzoNKAZG.png">
<p>
You can find information about the package and sub-menus in the original repo:</p>
<div>
<a href="https://github.com/dreamRs/esquisse" data-href="https://github.com/dreamRs/esquisse" title="https://github.com/dreamRs/esquisse">
<strong>
dreamRs/esquisse</strong>
<br>
<em>
RStudio add-in to make plots with ggplot2. Contribute to dreamRs/esquisse development by creating an account on GitHub.</em>
github.com</a>
<a href="https://github.com/dreamRs/esquisse" data-media-id="096e529428334b3689e0de03454f8063" data-thumbnail-img-id="0*fTAuf_A43x1ACUah" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*fTAuf_A43x1ACUah);">
</a>
</div>
</div>
</div>
</section>
<section name="8b17">
<div>
<hr>
</div>
<div>
<div>
<h4>4. DataExplorer‚Ää‚Äî‚ÄäAutomate data exploration and treatment</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*Cx355tN49un9vQTR.png">
<a href="https://github.com/boxuancui/DataExplorer" data-href="https://github.com/boxuancui/DataExplorer" rel="nofollow noopener noopener" target="_blank">
https://github.com/boxuancui/DataExplorer</a>
<p>
<a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" data-href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="nofollow noopener noopener" target="_blank">
Exploratory Data Analysis (EDA)</a>
 is an initial and important phase of data analysis/predictive modeling. During this process, analysts/modelers will have a first look of the data, and thus generate relevant hypotheses and decide next steps. However, the EDA process can be a hassle at times. This <a href="https://cran.r-project.org/" data-href="https://cran.r-project.org/" rel="nofollow noopener noopener" target="_blank">
 R</a>
 package aims to automate most of data handling and visualization, so that users could focus on studying the data and extracting insights.</p>
 <h4>Installation</h4>
<p>
The package can be installed directly from CRAN.</p>
<pre>
install.packages(&quot;DataExplorer&quot;)</pre>
<h4>Usage</h4>
<p>
With the package you can create reports, plots, and tables like this:</p>
<pre>
<code>
## Plot basic description for airquality data<br>
<a href="https://boxuancui.github.io/DataExplorer/reference/plot_intro.html" data-href="https://boxuancui.github.io/DataExplorer/reference/plot_intro.html" rel="nofollow noopener noopener" target="_blank">
plot_intro</a>
(airquality)</code>
</pre>
<img src="https://cdn-images-1.medium.com/max/800/0*SNvrDuAjfUD9nh5o.png">
<pre>
<code>
## View missing value distribution for airquality data<br>
<a href="https://boxuancui.github.io/DataExplorer/reference/plot_missing.html" data-href="https://boxuancui.github.io/DataExplorer/reference/plot_missing.html" rel="nofollow noopener noopener" target="_blank">
plot_missing</a>
(airquality)</code>
</pre>
<img src="https://cdn-images-1.medium.com/max/800/0*j5ya6c5LORPCiWfI.png">

<pre>
<code>
## Left: frequency distribution of all discrete variables<br>
<a href="https://boxuancui.github.io/DataExplorer/reference/plot_bar.html" data-href="https://boxuancui.github.io/DataExplorer/reference/plot_bar.html" rel="nofollow noopener noopener" target="_blank">
plot_bar</a>
(diamonds)<br>
## Right: `price` distribution of all discrete variables<br>
<a href="https://boxuancui.github.io/DataExplorer/reference/plot_bar.html" data-href="https://boxuancui.github.io/DataExplorer/reference/plot_bar.html" rel="nofollow noopener noopener" target="_blank">
plot_bar</a>
(diamonds, with = &quot;price&quot;)</code>
</pre>
<img src="https://cdn-images-1.medium.com/max/800/0*70Lw4jV20fBb9FuT.png">

<pre>
<code>
## View histogram of all continuous variables<br>
<a href="https://boxuancui.github.io/DataExplorer/reference/plot_histogram.html" data-href="https://boxuancui.github.io/DataExplorer/reference/plot_histogram.html" rel="nofollow noopener noopener" target="_blank">
plot_histogram</a>
(diamonds)</code>
</pre>
<img src="https://cdn-images-1.medium.com/max/800/0*y8Ch6yX6y-D1MZ10.png">

<p>
You can find much more like this on the package‚Äôs official webpage:</p>
<div>
<a href="https://boxuancui.github.io/DataExplorer/" data-href="https://boxuancui.github.io/DataExplorer/" title="https://boxuancui.github.io/DataExplorer/" rel="nofollow">
<strong>
Automate data exploration and treatment</strong>
<br>
<em>
Automated data exploration process for analytical tasks and predictive modeling, so that users could focus on‚Ä¶</em>
boxuancui.github.io</a>
<a href="https://boxuancui.github.io/DataExplorer/" data-media-id="d634e6cfac4a66a46393dc1ba688eb3f">
</a>
</div>
<p>
And in this vignette:</p>
<div>
<a href="https://boxuancui.github.io/DataExplorer/articles/dataexplorer-intro.html" data-href="https://boxuancui.github.io/DataExplorer/articles/dataexplorer-intro.html" title="https://boxuancui.github.io/DataExplorer/articles/dataexplorer-intro.html" rel="nofollow">
<strong>
Introduction to DataExplorer</strong>
<br>
<em>
This document introduces the package DataExplorer, and shows how it can help you with different tasks throughout your‚Ä¶</em>
boxuancui.github.io</a>
<a href="https://boxuancui.github.io/DataExplorer/articles/dataexplorer-intro.html" data-media-id="b722f42455f57937bae5a256c572f3db">
</a>
</div>
</div>
</div>
</section>
<section name="0237">
<div>
<hr>
</div>
<div>
<div>
<h4>3. Sparklyr‚Ää‚Äî‚ÄäR interface for Apache Spark</h4>
<img src="https://cdn-images-1.medium.com/max/800/0*JjZNkStqLFrOeV_x.png">
<a href="https://github.com/rstudio/sparklyr" data-href="https://github.com/rstudio/sparklyr" rel="nofollow noopener noopener" target="_blank">
https://github.com/rstudio/sparklyr</a>
<p>
Sparklyr will allow you to:</p>
<ul>
<li>
Connect to <a href="http://spark.apache.org/" data-href="http://spark.apache.org/" rel="noopener" target="_blank">
Spark</a>
 from R. The sparklyr package provides a <br>
 complete <a href="https://github.com/hadley/dplyr" data-href="https://github.com/hadley/dplyr" rel="noopener" target="_blank">
dplyr</a>
 backend.</li>
 <li>
Filter and aggregate Spark datasets, and then bring them into R for <br>
analysis and visualization.</li>
<li>
Use Spark‚Äôs distributed <a href="http://spark.apache.org/docs/latest/mllib-guide.html" data-href="http://spark.apache.org/docs/latest/mllib-guide.html" rel="noopener" target="_blank">
machine learning</a>
 library from R.</li>
 <li>
Create <a href="http://spark.rstudio.com/extensions.html" data-href="http://spark.rstudio.com/extensions.html" rel="noopener" target="_blank">
extensions</a>
 that call the full Spark API and provide <br>
interfaces to Spark packages.</li>
</ul>
<h4>Installation</h4>
<p>
You can install the Sparklyr package from CRAN as follows:</p>
<pre>
<code>
install.packages(&quot;sparklyr&quot;)</code>
</pre>
<p>
You should also install a local version of Spark for development purposes:</p>
<pre>
<code>
<strong>
library</strong>
(sparklyr)<br>
spark_install(version = &quot;2.3.1&quot;)</code>
</pre>
<h4>Usage</h4>
<p>
The first part of using Spark is always creating a context and connecting to a local or remote cluster.</p>
<p>
Here we‚Äôll connect to a local instance of Spark via the <a href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html" data-href="http://spark.rstudio.com/reference/sparklyr/latest/spark_connect.html" rel="nofollow noopener noopener" target="_blank">
spark_connect</a>
 function:</p>
 <pre>
library(sparklyr)<br>
sc &lt;- spark_connect(master = &quot;local&quot;)</pre>
<h4>Using sparklyr with dplyr and ggplot2</h4>
<p>
We‚Äôll start by copying some datasets from R into the Spark cluster (note that you may need to install the nycflights13 and Lahman packages in order to execute this code):</p>
<pre>
install.packages(c(&quot;nycflights13&quot;, &quot;Lahman&quot;))</pre>
<pre>
library(dplyr)<br>
iris_tbl &lt;- copy_to(sc, iris)<br>
flights_tbl &lt;- copy_to(sc, nycflights13::flights, &quot;flights&quot;)<br>
batting_tbl &lt;- copy_to(sc, Lahman::Batting, &quot;batting&quot;)<br>
src_tbls(sc)</pre>
<pre>
<code>
## [1] &quot;batting&quot; &quot;flights&quot; &quot;iris&quot;</code>
</pre>
<p>
To start with, here‚Äôs a simple filtering example:</p>
<pre>
# filter by departure delay and print the first few records<br>
flights_tbl %&gt;% filter(dep_delay == 2)</pre>
<pre>
<code>
## # Source:   lazy query [?? x 19]<br>
## # Database: spark_connection<br>
##     year month   day dep_time sched_dep_time dep_delay arr_time<br>
##    &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;<br>
##  1  2013     1     1      517            515         2      830<br>
##  2  2013     1     1      542            540         2      923<br>
##  3  2013     1     1      702            700         2     1058<br>
##  4  2013     1     1      715            713         2      911<br>
##  5  2013     1     1      752            750         2     1025<br>
##  6  2013     1     1      917            915         2     1206<br>
##  7  2013     1     1      932            930         2     1219<br>
##  8  2013     1     1     1028           1026         2     1350<br>
##  9  2013     1     1     1042           1040         2     1325<br>
## 10  2013     1     1     1231           1229         2     1523<br>
## # ... with more rows, and 12 more variables: sched_arr_time &lt;int&gt;,<br>
## #   arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, tailnum &lt;chr&gt;,<br>
## #   origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, hour &lt;dbl&gt;,<br>
## #   minute &lt;dbl&gt;, time_hour &lt;dttm&gt;</code>
</pre>
<p>
Let‚Äôs plot the data on flight delays:</p>
<pre>
delay &lt;- flights_tbl %&gt;% <br>
  group_by(tailnum) %&gt;%<br>
  summarise(count = n(), dist = mean(distance), delay = mean(arr_delay)) %&gt;%<br>
  filter(count &gt; 20, dist &lt; 2000, !is.na(delay)) %&gt;%<br>
  collect</pre>
  <pre>
# plot delays<br>
library(ggplot2)<br>
ggplot(delay, aes(dist, delay)) +<br>
  geom_point(aes(size = count), alpha = 1/2) +<br>
  geom_smooth() +<br>
  scale_size_area(max_size = 2)</pre>
  <pre>
<code>
## `geom_smooth()` using method = &#39;gam&#39;</code>
</pre>
<img src="https://cdn-images-1.medium.com/max/800/0*GCW1TSFK8DFLGg03.png">

<h4>Machine Learning with Sparklyr</h4>
<p>
You can orchestrate machine learning algorithms in a Spark cluster via the <a href="http://spark.apache.org/docs/latest/mllib-guide.html" data-href="http://spark.apache.org/docs/latest/mllib-guide.html" rel="noopener" target="_blank">
machine learning</a>
 functions within Sparklyr. These functions connect to a set of high-level APIs built on top of DataFrames that help you create and tune machine learning workflows.</p>
 <p>
Here‚Äôs an example where we use <a href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html" data-href="http://spark.rstudio.com/reference/sparklyr/latest/ml_linear_regression.html" rel="noopener" target="_blank">
ml_linear_regression</a>
 to fit a linear regression model. We‚Äôll use the built-in <code>
 mtcars</code>
 dataset to see if we can predict a car‚Äôs fuel consumption (<code>
 mpg</code>
) based on its weight (<code>
wt</code>
), and the number of cylinders the engine contains (<code>
cyl</code>
). We‚Äôll assume in each case that the relationship between <code>
mpg</code>
 and each of our features is linear.</p>
 <pre>
<code>
<em>
# copy mtcars into spark</em>
<br>
mtcars_tbl &lt;- copy_to(sc, mtcars)</code>
</pre>
<pre>
<code>
<em>
# transform our data set, and then partition into &#39;training&#39;, &#39;test&#39;</em>
<br>
partitions &lt;- mtcars_tbl %&gt;%<br>
  filter(hp &gt;= 100) %&gt;%<br>
  mutate(cyl8 = cyl == 8) %&gt;%<br>
  sdf_partition(training = 0.5, test = 0.5, seed = 1099)</code>
</pre>
<pre>
<code>
<em>
# fit a linear model to the training dataset</em>
<br>
fit &lt;- partitions$training %&gt;%<br>
  ml_linear_regression(response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))<br>
fit</code>
</pre>
<pre>
<code>
## Call: ml_linear_regression.tbl_spark(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))  <br>
## <br>
## Formula: mpg ~ wt + cyl<br>
## <br>
## Coefficients:<br>
## (Intercept)          wt         cyl <br>
##   33.499452   -2.818463   -0.923187</code>
</pre>
<p>
For linear regression models produced by Spark, we can use <code>
summary()</code>
 to learn a bit more about the quality of our fit and the statistical significance of each of our predictors.</p>
 <pre>
<code>
summary(fit)</code>
</pre>
<pre>
<code>
## Call: ml_linear_regression.tbl_spark(., response = &quot;mpg&quot;, features = c(&quot;wt&quot;, &quot;cyl&quot;))  <br>
## <br>
## Deviance Residuals:<br>
##    Min     1Q Median     3Q    Max <br>
## -1.752 -1.134 -0.499  1.296  2.282 <br>
## <br>
## Coefficients:<br>
## (Intercept)          wt         cyl <br>
##   33.499452   -2.818463   -0.923187 <br>
## <br>
## R-Squared: 0.8274<br>
## Root Mean Squared Error: 1.422</code>
</pre>
<p>
Spark machine learning supports a wide array of algorithms and feature transformations, and as illustrated above, it‚Äôs easy to chain these functions together with dplyr pipelines.</p>
<p>
Check out more about machine learning with sparklyr here:</p>
<div>
<a href="http://spark.rstudio.com/mlib/" data-href="http://spark.rstudio.com/mlib/" title="http://spark.rstudio.com/mlib/">
<strong>
sparklyr</strong>
<br>
<em>
An R interface to Spark</em>
spark.rstudio.com</a>
<a href="http://spark.rstudio.com/mlib/" data-media-id="52480f680d708055411e198e0c18a719" data-thumbnail-img-id="0*lPLyg7U9_KrQ3WK8" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*lPLyg7U9_KrQ3WK8);">
</a>
</div>
<p>
And more information in general about the package and examples here:</p>
<div>
<a href="http://spark.rstudio.com/" data-href="http://spark.rstudio.com/" title="http://spark.rstudio.com/">
<strong>
sparklyr</strong>
<br>
<em>
An R interface to Spark</em>
spark.rstudio.com</a>
<a href="http://spark.rstudio.com/" data-media-id="54da7b8b65bad431ddb8b28f5387b5ec" data-thumbnail-img-id="0*67w0OPg09_TEyyiq" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*67w0OPg09_TEyyiq);">
</a>
</div>
</div>
</div>
</section>
<section name="e463">
<div>
<hr>
</div>
<div>
<div>
<h4>2. Drake‚Ää‚Äî‚ÄäAn R-focused pipeline toolkit for reproducibility and high-performance computing</h4>
<figure>
<div>
<div style="padding-bottom: 20.8%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000124697749-3o1diq-t500x500.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME data-width="800" data-height="166" width="700" height="145" data-src="https://heartbeat.fritz.ai//media/ead6de547117dadbc1d366308db7f528?postId=6b7cca2bf000" data-media-id="ead6de547117dadbc1d366308db7f528" data-thumbnail="https://i.embed.ly/1/image?url=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000124697749-3o1diq-t500x500.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<div>
<IFRAME data-width="800" data-height="166" width="700" height="145" src="https://heartbeat.fritz.ai//media/ead6de547117dadbc1d366308db7f528?postId=6b7cca2bf000" data-media-id="ead6de547117dadbc1d366308db7f528" data-thumbnail="https://i.embed.ly/1/image?url=http%3A%2F%2Fi1.sndcdn.com%2Fartworks-000124697749-3o1diq-t500x500.jpg&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<figure>
<div style="max-width: 700px; max-height: 601px;">
<div style="padding-bottom: 85.9%;">
</div>
<div data-image-id="0*sEmFtk2JwYBANqO4.jpg" data-width="735" data-height="631" data-action="zoom" data-action-value="0*sEmFtk2JwYBANqO4.jpg">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*sEmFtk2JwYBANqO4.jpg?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/0*sEmFtk2JwYBANqO4.jpg">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/0*sEmFtk2JwYBANqO4.jpg">
</noscript>
</div>
</div>
<figcaption>
Drake programming</figcaption>
</figure>
<p>
Nope, just kidding. But the name of the package is <code>
drake</code>
!</p>
<figure>
<div style="max-width: 457px; max-height: 528px;">
<div style="padding-bottom: 115.5%;">
</div>
<div data-image-id="1*JVAo8q2UQdP5aEd74HIgRw.png" data-width="457" data-height="528">
<img src="https://cdn-images-1.medium.com/freeze/max/30/1*JVAo8q2UQdP5aEd74HIgRw.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/1*JVAo8q2UQdP5aEd74HIgRw.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/1*JVAo8q2UQdP5aEd74HIgRw.png">
</noscript>
</div>
</div>
<figcaption>
<a href="https://github.com/ropensci/drake" data-href="https://github.com/ropensci/drake" rel="nofollow noopener noopener" target="_blank">
https://github.com/ropensci/drake</a>
</figcaption>
</figure>
<p>
This is such an amazing package. I‚Äôll create a separate post with more details about it, so wait for that!</p>
<figure>
<div style="max-width: 700px; max-height: 222px;">
<div style="padding-bottom: 31.7%;">
</div>
<div data-image-id="1*TGGxxAXnyISi4UbAAyR0RQ.png" data-width="856" data-height="271" data-action="zoom" data-action-value="1*TGGxxAXnyISi4UbAAyR0RQ.png">
<img src="https://cdn-images-1.medium.com/freeze/max/30/1*TGGxxAXnyISi4UbAAyR0RQ.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/1*TGGxxAXnyISi4UbAAyR0RQ.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/1*TGGxxAXnyISi4UbAAyR0RQ.png">
</noscript>
</div>
</div>
</figure>
<p>
<code>
Drake</code>
 is a package created as a general-purpose workflow manager for data-driven tasks. It rebuilds intermediate data objects when their dependencies change, and it skips work when the results are already up to date.</p>
 <p>
Also, not every run-through starts from scratch, and completed workflows have tangible evidence of reproducibility.</p>
<p>
Reproducibility, good management, and tracking experiments are all necessary for easily testing others‚Äô work and analysis. It‚Äôs a huge deal in Data Science, and you can read more about it here:</p>
<p>
From <a href="https://medium.com/@zbohannan" data-href="https://medium.com/@zbohannan" data-anchor-type="2" data-user-id="e5168a12ebf7" data-action-value="e5168a12ebf7" data-action="show-user-card" data-action-type="hover" target="_blank">
Zach Scott</a>
:</p>
<div>
<a href="https://towardsdatascience.com/data-sciences-reproducibility-crisis-b87792d88513" data-href="https://towardsdatascience.com/data-sciences-reproducibility-crisis-b87792d88513" title="https://towardsdatascience.com/data-sciences-reproducibility-crisis-b87792d88513">
<strong>
Data Science‚Äôs Reproducibility Crisis</strong>
<br>
<em>
What is Reproducibility in Data Science and Why Should We Care?</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/data-sciences-reproducibility-crisis-b87792d88513" data-media-id="e799270aa108a422995cade29d3f4277" data-thumbnail-img-id="1*bVodZRPliooT5GBDDe0crw.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*bVodZRPliooT5GBDDe0crw.jpeg);">
</a>
</div>
<div>
<a href="https://towardsdatascience.com/toward-reproducibility-balancing-privacy-and-publication-77fee2366eee" data-href="https://towardsdatascience.com/toward-reproducibility-balancing-privacy-and-publication-77fee2366eee" title="https://towardsdatascience.com/toward-reproducibility-balancing-privacy-and-publication-77fee2366eee">
<strong>
Toward Reproducibility: Balancing Privacy and Publication</strong>
<br>
<em>
Can there ever be a Goldilocks option in the conflict between data security and research disclosure?</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/toward-reproducibility-balancing-privacy-and-publication-77fee2366eee" data-media-id="ef2986ef23ec8311b2669efbad762cc8" data-thumbnail-img-id="1*7nqF1qUTtx3b52I2Vyk0tg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*7nqF1qUTtx3b52I2Vyk0tg.jpeg);">
</a>
</div>
<p>
And in an article by me :)</p>
<div>
<a href="https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72" data-href="https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72" title="https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72">
<strong>
Manage your Machine Learning Lifecycle with MLflow‚Ää‚Äî‚ÄäPart 1.</strong>
<br>
<em>
Reproducibility, good management and tracking experiments is necessary for making easy to test other‚Äôs work and‚Ä¶</em>
towardsdatascience.com</a>
<a href="https://towardsdatascience.com/manage-your-machine-learning-lifecycle-with-mlflow-part-1-a7252c859f72" data-media-id="b9bc81095d9c3cb011a1d150f5a96086" data-thumbnail-img-id="1*dslIrTa9vtUvAUv5oixsEQ.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*dslIrTa9vtUvAUv5oixsEQ.jpeg);">
</a>
</div>
<p>
With <code>
drake</code>
, you can automatically</p>
<ol>
<li>
Launch the parts that changed since last time.</li>
<li>
Skip the rest.</li>
</ol>
<h4>Installation</h4>
<pre>
<code>
<em>
# Install the latest stable release from CRAN.</em>
<br>
<strong>
install.packages</strong>
(&quot;drake&quot;)</code>
</pre>
<pre>
<code>
<em>
# Alternatively, install the development version from GitHub.</em>
<br>
<strong>
install.packages</strong>
(&quot;devtools&quot;)<br>
<strong>
library</strong>
(devtools)<br>
<strong>
install_github</strong>
(&quot;ropensci/drake&quot;)</code>
</pre>
<p>
There are some known errors when installing from CRAN. For more on these errors, visit:</p>
<div>
<a href="https://ropenscilabs.github.io/drake-manual/index.html#installation" data-href="https://ropenscilabs.github.io/drake-manual/index.html#installation" title="https://ropenscilabs.github.io/drake-manual/index.html#installation">
<strong>
<em>
The drake R Package User Manual</em>
</strong>
<br>
The drake R Package User Manualropenscilabs.github.io</a>
<a href="https://ropenscilabs.github.io/drake-manual/index.html#installation" data-media-id="ef1fefbc625ee2e5baf90ca87f7ec017">
</a>
</div>
<p>
I encountered a mistake, so I recommend that for now you install the package from GitHub.</p>
<p>
Ok, so let‚Äôs reproduce a simple example with a twist:</p>
<figure>
<div>
<div style="padding-bottom: 35.699999999999996%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME width="700" height="250" data-src="https://heartbeat.fritz.ai//media/a475ced6050d717a36798a896bde4085?postId=6b7cca2bf000" data-media-id="a475ced6050d717a36798a896bde4085" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<noscript>
<div>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/a475ced6050d717a36798a896bde4085?postId=6b7cca2bf000" data-media-id="a475ced6050d717a36798a896bde4085" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<p>
I added a simple plot to see the linear model within <code>
drake</code>
‚Äôs main example. With this code, you‚Äôre creating a plan for executing your whole project.</p>
<p>
First, we read the data. Then we prepare it for analysis, create a simple hist, calculate the correlation, fit the model, plot the linear model, and finally create a <code>
rmarkdown</code>
 report.</p>
 <p>
The code I used for the final report is here:</p>
<figure>
<div>
<div style="padding-bottom: 35.699999999999996%;">
</div>
<div>
<img src="https://i.embed.ly/1/display/resize?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;width=40" crossorigin="anonymous">
<canvas>
</canvas>
<div>
<IFRAME width="700" height="250" data-src="https://heartbeat.fritz.ai//media/2a709761357f0b7c70bb615f020ed50e?postId=6b7cca2bf000" data-media-id="2a709761357f0b7c70bb615f020ed50e" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
<noscript>
<div>
<IFRAME width="700" height="250" src="https://heartbeat.fritz.ai//media/2a709761357f0b7c70bb615f020ed50e?postId=6b7cca2bf000" data-media-id="2a709761357f0b7c70bb615f020ed50e" data-thumbnail="https://i.embed.ly/1/image?url=https%3A%2F%2Favatars0.githubusercontent.com%2Fu%2F10162068%3Fs%3D400%26v%3D4&amp;key=a19fcc184b9711e1b4764040d3dc5c07" allowfullscreen frameborder="0">
</IFRAME>
</div>
</noscript>
</div>
</div>
</figure>
<p>
If we change some of our functions or analysis, when we execute the plan, <code>
drake</code>
 will know what has changed and will only run those changes. It creates a graph so you can see what‚Äôs happening:</p>
</div>
<div>
<figure>
<div>
<div style="padding-bottom: 55.7%;">
</div>
<div data-image-id="1*b5HGY5soFkbcjQ_nL9wcVA.png" data-width="1366" data-height="761">
<img src="https://cdn-images-1.medium.com/freeze/max/30/1*b5HGY5soFkbcjQ_nL9wcVA.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/2000/1*b5HGY5soFkbcjQ_nL9wcVA.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/2000/1*b5HGY5soFkbcjQ_nL9wcVA.png">
</noscript>
</div>
</div>
<figcaption>
Graph for analysis</figcaption>
</figure>
</div>
<div>
<p>
In Rstudio, this graph is interactive, and you can save it to HTML for later analysis.</p>
<p>
There are more awesome things that you can do with <code>
drake</code>
 that I‚Äôll show in a future post :)</p>
</div>
</div>
</section>
<section name="1374">
<div>
<hr>
</div>
<div>
<div>
<h4>1. DALEX‚Ää‚Äî‚ÄäDescriptive mAchine Learning EXplanations</h4>
<figure>
<div style="max-width: 200px; max-height: 173px;">
<div style="padding-bottom: 86.5%;">
</div>
<div data-image-id="1*PFGroqRgJcYAGHreiivwOw.png" data-width="200" data-height="173">
<img src="https://cdn-images-1.medium.com/freeze/max/30/1*PFGroqRgJcYAGHreiivwOw.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/800/1*PFGroqRgJcYAGHreiivwOw.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/800/1*PFGroqRgJcYAGHreiivwOw.png">
</noscript>
</div>
</div>
<figcaption>
<a href="https://github.com/pbiecek/DALEX" data-href="https://github.com/pbiecek/DALEX" rel="nofollow noopener noopener noopener" target="_blank">
https://github.com/pbiecek/DALEX</a>
</figcaption>
</figure>
<p>
Explaining machine learning models isn‚Äôt always easy. Yet it‚Äôs so important for a range of business applications. Luckily, there are some great libraries that help us with this task. For example:</p>
<div>
<a href="https://github.com/thomasp85/lime" data-href="https://github.com/thomasp85/lime" title="https://github.com/thomasp85/lime">
<strong>
thomasp85/lime</strong>
<br>
<em>
lime‚Ää‚Äî‚ÄäLocal Interpretable Model-Agnostic Explanations (R port of original Python package)</em>
github.com</a>
<a href="https://github.com/thomasp85/lime" data-media-id="4d651312e0e674bfe84c91c788f2acce" data-thumbnail-img-id="0*GbYTRluIMGg_BNC3" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*GbYTRluIMGg_BNC3);">
</a>
</div>
<p>
(By the way, sometimes a simple visualization with <code>
ggplot</code>
 can help you explain a model. For more on this check the awesome article below by <a href="https://medium.com/@mattmayo13" data-href="https://medium.com/@mattmayo13" target="_blank">
 Matthew Mayo</a>
)</p>
<div>
<a href="https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html" data-href="https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html" title="https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html">
<strong>
Interpreting Machine Learning Models: An Overview</strong>
<br>
<em>
An article on machine learning interpretation appeared on O‚ÄôReilly‚Äôs blog back in March, written by Patrick Hall, Wen‚Ä¶</em>
www.kdnuggets.com</a>
<a href="https://www.kdnuggets.com/2017/11/interpreting-machine-learning-models-overview.html" data-media-id="f3b3498d883c04c352cb46f7ca38b1eb" data-thumbnail-img-id="0*myNhMXOz6SNm7sLL" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*myNhMXOz6SNm7sLL);">
</a>
</div>
<p>
In many applications, we need to know, understand, or prove how input variables are used in the model, and how they impact final model predictions.<code>
DALEX</code>
 is a set of tools that helps explain how complex models are working.</p>
 <p>
To install from CRAN, just run:</p>
<pre>
<code>
install.packages(&quot;DALEX&quot;)</code>
</pre>
<p>
They have amazing documentation on how to use DALEX with different ML packages:</p>
<ul>
<li>
<a href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html" data-href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_caret.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX with caret</a>
</li>
<li>
<a href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html" data-href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_mlr.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX with mlr</a>
</li>
<li>
<a href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html" data-href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/DALEX_h2o.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX with H2O</a>
</li>
<li>
<a href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html" data-href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_and_xgboost.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX with xgboost package</a>
</li>
<li>
<a href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html" data-href="https://rawgithub.com/pbiecek/DALEX_docs/master/vignettes/DALEX_teaching.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX for teaching. Part 1</a>
</li>
<li>
<a href="https://rawgit.com/pbiecek/DALEX_docs/master/examples/What%20they%20have%20learned%20-%20part%202.html" data-href="https://rawgit.com/pbiecek/DALEX_docs/master/examples/What%20they%20have%20learned%20-%20part%202.html" rel="nofollow noopener noopener noopener" target="_blank">
How to use DALEX for teaching. Part 2</a>
</li>
<li>
<a href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/Comparison_between_breakdown%2C_lime%2C_shapley.html" data-href="https://rawgit.com/pbiecek/DALEX_docs/master/vignettes/Comparison_between_breakdown%2C_lime%2C_shapley.html" rel="nofollow noopener noopener noopener" target="_blank">
breakDown vs lime vs shapleyR</a>
</li>
</ul>
<p>
Great cheat sheets:</p>
</div>
<div>
<figure>
<div style="max-width: 1000px; max-height: 614px;">
<div style="padding-bottom: 61.4%;">
</div>
<div data-image-id="0*VcQnC5Qqmq3J8KyH.png" data-width="2000" data-height="1228" data-action="zoom" data-action-value="0*VcQnC5Qqmq3J8KyH.png">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*VcQnC5Qqmq3J8KyH.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/1000/0*VcQnC5Qqmq3J8KyH.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/1000/0*VcQnC5Qqmq3J8KyH.png">
</noscript>
</div>
</div>
<figcaption>
<a href="https://github.com/pbiecek/DALEX" data-href="https://github.com/pbiecek/DALEX" rel="nofollow noopener noopener noopener" target="_blank">
https://github.com/pbiecek/DALEX</a>
</figcaption>
</figure>
<figure>
<div style="max-width: 1000px; max-height: 774px;">
<div style="padding-bottom: 77.4%;">
</div>
<div data-image-id="0*AAZruOQhEx4_iXhD.png" data-width="2000" data-height="1548" data-action="zoom" data-action-value="0*AAZruOQhEx4_iXhD.png">
<img src="https://cdn-images-1.medium.com/freeze/max/30/0*AAZruOQhEx4_iXhD.png?q=20" crossorigin="anonymous">
<canvas>
</canvas>
<img data-src="https://cdn-images-1.medium.com/max/1000/0*AAZruOQhEx4_iXhD.png">
<noscript>
<img src="https://cdn-images-1.medium.com/max/1000/0*AAZruOQhEx4_iXhD.png">
</noscript>
</div>
</div>
<figcaption>
<a href="https://github.com/pbiecek/DALEX" data-href="https://github.com/pbiecek/DALEX" rel="nofollow noopener noopener noopener" target="_blank">
https://github.com/pbiecek/DALEX</a>
</figcaption>
</figure>
</div>
<div>
<p>
Here‚Äôs an interactive notebook where you can learn more about the package:</p>
<div>
<a href="https://mybinder.org/v2/gh/pbiecek/DALEX_docs/master?filepath=jupyter-notebooks%2FDALEX.ipynb" data-href="https://mybinder.org/v2/gh/pbiecek/DALEX_docs/master?filepath=jupyter-notebooks%2FDALEX.ipynb" title="https://mybinder.org/v2/gh/pbiecek/DALEX_docs/master?filepath=jupyter-notebooks%2FDALEX.ipynb">
<strong>
Binder (beta)</strong>
<br>
<em>
Edit description</em>
mybinder.org</a>
<a href="https://mybinder.org/v2/gh/pbiecek/DALEX_docs/master?filepath=jupyter-notebooks%2FDALEX.ipynb" data-media-id="ef1a409e85f58059a860ab04d34a7ec2">
</a>
</div>
<p>
And finally, some book-style documentation on <code>
DALEX</code>
, machine learning, and explainability:</p>
<div>
<a href="https://pbiecek.github.io/DALEX_docs/" data-href="https://pbiecek.github.io/DALEX_docs/" title="https://pbiecek.github.io/DALEX_docs/">
<strong>
DALEX: Descriptive mAchine Learning EXplanations</strong>
<br>
<em>
Do not trust a black-box model. Unless it explains itself.</em>
pbiecek.github.io</a>
<a href="https://pbiecek.github.io/DALEX_docs/" data-media-id="866f02bd58f2897617b14bd75fee2303">
</a>
</div>
<p>
Check it out in the original repository:</p>
<div>
<a href="https://github.com/pbiecek/DALEX" data-href="https://github.com/pbiecek/DALEX" title="https://github.com/pbiecek/DALEX">
<strong>
pbiecek/DALEX</strong>
<br>
<em>
DALEX‚Ää‚Äî‚ÄäDescriptive mAchine Learning EXplanations</em>
github.com</a>
<a href="https://github.com/pbiecek/DALEX" data-media-id="d00fbdc120ed5c9ce40c7aa67010eb09" data-thumbnail-img-id="0*-Rz2yqdJ141uP4Ei" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*-Rz2yqdJ141uP4Ei);">
</a>
</div>
<br>
<br>
<br>
<br>

<script>
	var toc = $('#toc');
	$('h3').each(function(i) {
		var topic = $(this), topicNumber = i + 1;
		toc.append('<a href="#topic-'+topicNumber+'" target="_self">'+topic.text()+'</a><br>');
		topic.attr('id', 'topic-' + topicNumber);
	});
</script>
</body>
</html>
