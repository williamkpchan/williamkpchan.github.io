<base target="_blank"><html><head><title>statologyContents 9</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 9"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 9</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Calculate the Interquartile Range in Google Sheets</span></h2>
The <b>interquartile</b> <b>range</b>, often denoted IQR, is a way to measure the spread of the middle 50% of a dataset. It is calculated as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset.
Note that <em>quartiles</em> are simply values that split up a dataset into four equal parts.
The IQR is often used to measure the spread of values in a dataset because it’s known to be  resistant to outliers . Since it only tells us the spread of the middle 50% of the dataset, it isn’t affect by unusually small or unusually large outliers.
This makes it a preferable way to measure  dispersion  compared to a metric like the range, which simply tells us the difference between the largest and the smallest values in a dataset.
This tutorial explains how to calculate the IQR for a given dataset in Google Sheets.
<h3>Example: How to Calculate IQR in Google Sheets</h3>
Use the following steps to calculate the interquartile range (IQR) of a dataset in Google Sheets.
<b>Step 1: Enter the data.</b>
First, enter all of the values of a dataset into one column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/iqrSheets1.png">
<b>Step 2: Calculate the first and third quartiles.</b>
Next, we’ll use the <b>QUARTILE() </b>function to calculate the first (Q1) and third (Q3) quartiles of the dataset.
Note that this function uses the following syntax:
<b>QUARTILE(data, quartile_number)</b>
where:
<b>data: </b>An array of data values
<b>quartile_number:</b> The quartile to calculate
The following image shows the formulas to use to calculate Q1 and Q3 for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/iqrSheets2.png">
<b>Step 3: Calculate the IQR.</b>
Lastly, we can subtract the first quartile (Q1) from the third quartile (Q3) to obtain the interquartile range:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/iqrSheets3.png">
The interquartile range turns out to be <b>16</b>. This tells us the spread of the middle 50% of values in our dataset.
<h2><span class="orange">How to Calculate Interquartile Range in R (With Examples)</span></h2>
The <b>interquartile range</b> represents the difference between the first quartile (the 25th percentile) and the third quartile (the 75th percentile) of a dataset.
In simple terms, it measures the spread of the middle 50% of values.
IQR = Q3 – Q1
We can use the built-in <b>IQR()</b> function to calculate the interquartile range of a set of values in R:
<b>IQR(x)
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Interquartile Range of a Vector</h3>
The following code shows how to calculate the interquartile range of values in a vector:
<b>#define vector
x &lt;- c(4, 6, 6, 7, 8, 12, 15, 17, 20, 21, 21, 23, 24, 27, 28)
#calculate interquartile range of values in vector
IQR(x)
[1] 14.5
</b>
<h3>Example 2: Interquartile Range of a Vector with Missing Values</h3>
If your vector has missing values, be sure to specify <b>na.rm=TRUE</b> to ignore missing values when calculating the interquartile range:
<b>#define vector with some missing values
x &lt;- c(4, 6, NA, 7, NA, NA, 15, 17, 20, 21, 21, 23, 24, 27, 28)
#calculate interquartile range of values in vector
IQR(x, na.rm=TRUE)
[1] 10.25
</b>
<h3>Example 3: Interquartile Range of Column in Data Frame</h3>
The following code shows how to calculate the interquartile range of a specific column in a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#calculate interquartile range of 'var1' column
IQR(df$var1)
[1] 1</b>
<h3>Example 4: Interquartile Range of Several Columns in Data Frame</h3>
The following code shows how to calculate the interquartile range of several columns in a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#calculate interquartile range of 'var1', 'var2', and 'var4' columns
sapply(df[ , c('var1', 'var2', 'var4')], IQR)
var1 var2 var4 
   1    4    7</b>
<h2><span class="orange">Is the Interquartile Range (IQR) Affected By Outliers?</span></h2>
In statistics, we’re often interested in knowing how “spread out” the values are in a distribution.
One popular way to measure spread is <b>the interquartile range</b>, which is calculated as the difference between the first quartile and the third quartile in a dataset. Quartiles are simply values that split up a dataset into four equal parts.
<h2>Example: Calculating the Interquartile Range</h2>
The following example shows how to calculate the interquartile range for a given dataset:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/exam_scores1.jpg" sizes="(max-width: 148px) 100vw, 148px"> 
<b>Step 1: Arrange the values from smallest to largest.</b>
58, 66, 71, 73, 74, 77, 78, 82, 84, 85, 88, 88, 88, 90, 90, 92, 92, 94, 96, 98
<b>2. Find the median.</b>
58, 66, 71, 73, 74, 77, 78, 82, 84, <b>85, 88</b>, 88, 88, 90, 90, 92, 92, 94, 96, 98
In this case, the median is between 85 and 88.
<b>3. The median splits the dataset into two halves. The median of the lower half is the lower quartile and the median of the upper half is the upper quartile:</b>
58, 66, 71, 73, <b>74, 77</b>, 78, 82, 84, 85, 88, 88, 88, 90, <b>90, 92</b>, 92, 94, 96, 98
<b>4. Calculate the interquartile range.</b>
In this case, the first quartile is the average of the middle two values in the lower half of the data set (75.5) and the third quartile is the average of the middle two values in the upper half of the data set (91).
Thus, the  interquartile range is 91 – 75.5 = <b>15.5</b>
<h2>The Interquartile Range is Not Affected By Outliers</h2>
One reason that people prefer to use the interquartile range (IQR) when calculating the “spread” of a dataset is because it’s resistant to outliers. Since the IQR is simply the range of the middle 50% of data values, it’s not affected by  extreme outliers .
To demonstrate this, consider the following dataset:
[1, 4, 8, 11, 13, 17, 17, 20]
Here are the various measures of spread for this dataset:
Interquartile range: 11
Range: 19
Standard deviation: 6.26
Variance: 39.23
Now, consider the same dataset but with an extreme outlier added to it:
[1, 4, 8, 11, 13, 17, 17, 20, <b>150</b>]
Here are the various measures of spread for this dataset:
Interquartile range: 12.5
Range: 149
Standard deviation: 43.96
Variance: 1,932.84
Notice how the interquartile range changes only slightly, from 11 to 12.5. However, all of the other measures of dispersion change drastically.
This demonstrates that the interquartile range is not affected by outliers like the other measures of dispersion. For this reason, it’s a reliable way to measure the spread of the middle 50% of values in any distribution.
<b>Further Reading:</b>
 Measures of Dispersion 
 Interquartile Range Calculator 
 
<h2><span class="orange">How to Calculate The Interquartile Range in Python</span></h2>
The <b>interquartile</b> <b>range</b>, often denoted “IQR”, is a way to measure the spread of the middle 50% of a dataset. It is calculated as the difference between the first quartile* (the 25th percentile) and the third quartile (the 75th percentile) of a dataset. 
Fortunately it’s easy to calculate the interquartile range of a dataset in Python using the  numpy.percentile()  function.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Interquartile Range of One Array</h3>
The following code shows how to calculate the interquartile range of values in a single array:
<b>import numpy as np
#define array of data
data = np.array([14, 19, 20, 22, 24, 26, 27, 30, 30, 31, 36, 38, 44, 47])
#calculate interquartile range 
q3, q1 = np.percentile(data, [75 ,25])
iqr = q3 - q1
#display interquartile range 
iqr
12.25</b>
The interquartile range of this dataset turns out to be <b>12.25</b>. This is the spread of the middle 50% of values in this dataset.
<h3>Example 2: Interquartile Range of a Data Frame Column</h3>
The following code shows how to calculate the interquartile range of a single column in a data frame:
<b>import numpy as np
import pandas as pd
#create data frame
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#calculate interquartile range of values in the 'points' column
q75, q25 = np.percentile(df['points'], [75 ,25])
iqr = q75 - q25
#display interquartile range 
iqr
5.75</b>
The interquartile range of values in the points column turns out to be <b>5.75</b>.
<h3>Example 3: Interquartile Range of Multiple Data Frame Columns</h3>
The following code shows how to calculate the interquartile range of multiple columns in a data frame at once:
<b>import numpy as np
import pandas as pd
#create data frame
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#define function to calculate interquartile range
def find_iqr(x):
  return np.subtract(*np.percentile(x, [75, 25]))
#calculate IQR for 'rating' and 'points' columns
df[['rating', 'points']].apply(find_iqr)
rating    6.75
points    5.75
dtype: float64
#calculate IQR for all columns
df.apply(find_iqr)
rating      6.75
points      5.75
assists     2.50
rebounds    3.75
dtype: float64
</b>
<b>Note: </b>We use the  pandas.DataFrame.apply()  function to calculate the IQR for multiple columns in the data frame above.
<h2><span class="orange">How to Find Interquartile Range on a TI-84 Calculator</span></h2>
The <b>interquartile</b> <b>range</b>, often denoted IQR, is a way to measure the spread of the middle 50% of a dataset.
It is calculated as the difference between the first quartile (Q1) and the third quartile (Q3) of a dataset.
Note that <em>quartiles</em> are simply values that split up a dataset into four equal parts.
The IQR is often used to measure the spread of values in a dataset because it’s known to be  resistant to outliers . Since it only tells us the spread of the middle 50% of the dataset, it isn’t affect by unusually small or unusually large values.
This makes it a preferable way to measure  dispersion  compared to a metric like the range, which simply tells us the difference between the largest and the smallest values in a dataset.
The following step-by-step example shows how to calculate the IQR for the following dataset on a TI-84 calculator:
<b>Dataset:</b> 4, 6, 6, 7, 8, 12, 15, 17, 20, 21, 21, 23, 24, 27, 28
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values.
Press Stat, then press EDIT. Then enter the values of the dataset in column L1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum1.png">
<h3>Step 2: Find the Interquartile Range</h3>
Next, press Stat and then scroll over to the right and press CALC.
Then press 1-Var Stats.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum2.png">
In the new screen that appears, press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum3.png">
Once you press Enter, a list of summary statistics will appear. Scroll down to the very bottom of the list:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum4.png">
From this screen we can observe the values for the first quartile (Q1) and the third quartile (Q3) of the dataset:
First quartile (Q1): <b>7</b>
Third quartile (Q3): <b>23</b>
The interquartile range is calculated as Q3 – Q1, which would be 23 – 7 = <b>16</b>.
This tells us that the spread of the middle 50% of values in the dataset is 16.
<h2><span class="orange">Interquartile Range vs. Standard Deviation: What’s the Difference?</span></h2>
The <b>interquartile range</b> and the <b>standard deviation</b> are two ways to measure the spread of values in a dataset.
This tutorial provides a brief explanation of each metric along with the similarities and differences between the two.
<h3>Interquartile Range</h3>
The <b>interquartile range</b> (IQR) of a dataset is the difference between the first quartile (the 25th percentile) and the third quartile (the 75th percentile). It measures the spread of the middle 50% of values.
<b>IQR = Q3 – Q1</b>
For example, suppose we have the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
According to the  Interquartile Range Calculator , the interquartile range (IQR) for this dataset is calculated as:
<b>Q1:</b> 12
<b>Q3:</b> 26.5
<b>IQR</b> = Q3 – Q1 = 14.5
This tells us that the middle 50% of values in the dataset have a spread of <b>14.5</b>.
Standard Deviation</b>
The <b>standard deviation</b> of a dataset is a way to measure the typical deviation of individual values from the mean value. It is calculated as:
<b>s = √(Σ(x<sub>i</sub> – x)<sup>2</sup> / (n-1))</b>
For example, suppose we have the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
We can use a calculator to find that the sample standard deviation of this dataset is <b>9.25</b>. This gives us an idea of how far the typical value lies from the mean.
<h3>Similarities & Differences</h3>
The interquartile range and standard deviation share the following <b>similarity:</b>
Both metrics measure the spread of values in a dataset.
However, the interquartile range and standard deviation have the following key <b>difference:</b>
The interquartile range (IQR) is not affected by extreme outliers. For example, an extremely small or extremely large value in a dataset will not affect the calculation of the IQR because the IQR only uses the values at the 25th percentile and 75th percentile of the dataset.
The standard deviation <em>is</em> affected by extreme outliers. For example, an extremely large value in a dataset will cause the standard deviation to be much larger since the standard deviation uses every single value in a dataset in its formula.
<h3>When to Use Each</h3>
You should use the interquartile range to measure the spread of values in a dataset when there are extreme outliers present.
Conversely, you should use the standard deviation to measure the spread of values when there are no extreme outliers present.
To illustrate why, consider the following dataset:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32
Earlier in the article we calculated the following metrics for this dataset:
<b>IQR:</b> 14.5
<b>Standard Deviation:</b> 9.25 
However, consider if the dataset had one extreme outlier:
<b>Dataset:</b> 1, 4, 8, 11, 13, 17, 19, 19, 20, 23, 24, 24, 25, 28, 29, 31, 32, <b>378</b>
We could use a calculator to find the following metrics for this dataset:
<b>IQR:</b> 15
<b>Standard Deviation:</b> 85.02
Notice that the interquartile range barely changes when an outlier is present, while the standard deviation increase from 9.25 all the way to 85.02.
<h2><span class="orange">How to Use the intersect() Function in R (With Examples)</span></h2>
You can use the <b>intersect()</b> function in base R to find the intersection of two objects.
The “intersection” simply represents the elements that the two objects have in common.
This function uses the following basic syntax:
<b>intersect(object1, object2)</b>
The following examples show how to use the <b>intersect()</b> function with vectors and data frames.
<h3>Example 1: Use intersect() with Vectors</h3>
The following code shows how to use the <b>intersect()</b> function to find the intersection between two vectors in R:
<b>#define two vectors
x &lt;- c(1, 4, 5, 5, 9, 12, 19)
y &lt;- c(1, 2, 5, 5, 10, 14, 19)
#find intersection between two vectors
intersect(x, y)
[1]  1  5 19
</b>
From the output we can see that vectors x and y have three values in common: <b>1</b>, <b>5</b>, and <b>19</b>.
Note that the <b>intersect()</b> function also works with character vectors:
<b>#define two vectors
x &lt;- c('A', 'B', 'C', 'D', 'E')
y &lt;- c('C', 'D', 'E', 'F')
#find intersection between two vectors
intersect(x, y)
[1] "C" "D" "E"
</b>
From the output we can see that vectors x and y have three strings in common: <b>C</b>, <b>D</b>, and <b>E</b>.
Note that the two vectors do not have to be the same length for the <b>intersect()</b> function to work.
<h3>Example 2: Use intersect() with Data Frames</h3>
In order to find the rows that two data frames have in common, we must use the <b>intersect()</b> function from the <b>dplyr</b> package.
The following code shows how to use this function to find the rows that two data frames have in common:
<b>library(dplyr) 
#define two data frames
df1 &lt;- data.frame(team=c('A', 'A', 'B', 'B'),  points=c(12, 20, 25, 19))
df1
  team points
1    A     12
2    A     20
3    B     25
4    B     19
df2 &lt;- data.frame(team=c('A', 'A', 'B', 'C'),  points=c(12, 22, 25, 32))
df2
  team points
1    A     12
2    A     22
3    B     25
4    C     32
#find intersection between two data frames
dplyr::intersect(df1, df2)
  team points
1    A     12
2    B     25</b>
From the output we can see that the data frames have two rows in common.
Note that this <b>intersect()</b> function will only return the rows that have the same values in <em>every</em> column between the two data frames.
Also note that we could use the <b>length()</b> function with the <b>intersect()</b> function to simply find the number of rows the two data frames have in common:
<b>#find number of rows in common between the two data frames
length(dplyr::intersect(df1, df2))
[1] 2
</b>
From the output we can see that the two data frames have <b>2</b> rows in common.
<h2><span class="orange">What is an Intervening Variable?</span></h2>
An <b>intervening variable </b>is a variable that affects the relationship between an  independent variable  and a  dependent variable .
Often this type of variable can appear when researchers are studying the relationship between two variables and don’t realize that another variable is actually <em>intervening </em>in the relationship.
<h3><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/intervening1.png"></h3>
Intervening variables pop up in many different research situations. Here are a few examples.
<h3>Example 1: Education & Spending</h3>
Researchers may be interested in the relationship between education (the independent variable) and yearly spending (the dependent variable).
After collecting data on education level and yearly spending for 1,000 individuals, they find that there is a  strong positive correlation  between the two variables. In particular, they find that individuals who have more education tend to spend more.
However, without realizing it the researchers have failed to take note of the <b>intervening variable</b> <em>income</em>. It turns out that individuals who have higher levels of education tend to hold higher-paying jobs, which means they naturally have more money to spend.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/intervening2-1.png">
<h3>Example 2: Poverty and Life Expectancy</h3>
Researchers may be interested in the relationship between poverty (the independent variable) and life expectancy (the dependent variable).
After collecting data on poverty and life expectancy for 10,000 individuals, they find that there is a strong correlation between the two variables. In particular, they find that more impoverished individuals tend to have lower life expectancies.
However, without realizing it the researchers have failed to take note of the <b>intervening variable</b> <em>healthcare</em>. It turns out that individuals who are more impoverished have less reliable access to healthcare, which naturally means that they have lower life expectancies.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/intervening3.png">
<h3>Example 3: Hours Spent Practicing & Points per Game</h3>
A sports researcher may be interested in the relationship between hours spent practicing by players (the independent variable) and their average points per game (the dependent variable).
After collecting data on hours spent practicing and points per game for 100 players, they find that there is a strong correlation between the two variables. In particular, they find that players who practice more tend to average more points per game.
However, without realizing it the researcher has failed to take note of the <b>intervening variable</b> <em>minutes played</em>. It turns out that individuals who practice for more hours are around the coach more who then gets to know the player better and tends to put them in the game more, which gives then more opportunities to score more points.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/intervening4.png">
<h3>The Importance of Identifying Intervening Variables</h3>
Understanding intervening variables can often help researchers clarify the relationship between an independent and dependent variable because the intervening variables are often the true variable that explains variations in the dependent variable.
In many cases, the independent variable causes changes in some intervening variable, which then causes changes in the dependent variable under study.
By identifying the intervening variable, it becomes easier to understand the actual relationship between the independent and dependent variable.
<em><b>Technical Note: </b>Intervening variables are sometimes also referred to as mediating variables or intermediary variables.</em>
<h2><span class="orange">How to Calculate Intraclass Correlation Coefficient in Excel</span></h2>
An  intraclass correlation coefficient  (ICC) is used to determine if items (or subjects) can be rated reliably by different raters.
The value of an ICC can range from 0 to 1, with 0 indicating no  reliability  among raters and 1 indicating perfect reliability.
This tutorial provides a step-by-step example of how to calculate ICC in Excel.
<h3>Step 1: Create the Data</h3>
Suppose four different judges were asked to rate the quality of 10 different college entrance exams. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/iccExcel1.png">
<h3>Step 2: Fit an ANOVA</h3>
In order to calculate the ICC for these ratings, we first need to fit an <b>Anova: Two-Factor Without Replication</b>.
To do so, highlight cells A1:E11 as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/iccExcel2.png">
To do so, click the <b>Data</b> tab along the top ribbon and then click the <b>Data Analysis</b> option under the <b>Analysis</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
If you don’t see this option available, you need to first  load the Analysis ToolPak .
In the dropdown menu that appears, click <b>Anova: Two-Factor Without Replication</b> and then click <b>OK</b>. In the new window that appears, fill in the following information and then click <b>OK</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/iccExcel3.png">
The following results will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/iccExcel4.png">
<h3>Step 3: Calculate the Intraclass Correlation Coefficient</h3>
We can use the following formula to calculate the ICC among the raters:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/iccExcel5.png">
The intraclass correlation coefficient (ICC) turns out to be <b>0.782</b>.
Here is how to interpret the value of an intraclass correlation coefficient, according to  Koo & Li :
<b>Less than 0.50:</b> Poor reliability
<b>Between 0.5 and 0.75:</b> Moderate reliability
<b>Between 0.75 and 0.9:</b> Good reliability
<b>Greater than 0.9:</b> Excellent reliability
Thus, we would conclude that an ICC of <b>0.782</b> indicates that the exams can be rated with “good” reliability by different raters.
<h3>A Note on Calculating ICC</h3>
There are several different versions of an ICC that can be calculated, depending on the following three factors:
<b>Model:</b> One-Way Random Effects, Two-Way Random Effects, or Two-Way Mixed Effects
<b>Type of Relationship:</b> Consistency or Absolute Agreement
<b>Unit:</b> Single rater or the mean of raters
In the previous example, the ICC that we calculated used the following assumptions:
<b>Model:</b> Two-Way Random Effects
<b>Type of Relationship:</b> Absolute Agreement
<b>Unit:</b> Single rater
For a detailed explanation of these assumptions, please refer to  this article .
<h2><span class="orange">How to Calculate Intraclass Correlation Coefficient in Python</span></h2>
An  intraclass correlation coefficient  (ICC) is used to determine if items or subjects can be rated reliably by different raters.
The value of an ICC can range from 0 to 1, with 0 indicating no  reliability  among raters and 1 indicating perfect reliability.
The easiest way to calculate ICC in Python is to use the  pingouin.intraclass_corr()  function from the  pingouin statistical package , which uses the following syntax:
<b>pingouin.intraclass_corr(data, targets, raters, ratings)</b>
where:
<b>data:</b> The name of the dataframe
<b>targets:</b> Name of column containing the “targets” (the things being rated)
<b>raters:</b> Name of column containing the raters
<b>ratings:</b> Name of column containing the ratings
This tutorial provides an example of how to use this function in practice.
<h3>Step 1: Install Pingouin</h3>
First, we must install Pingouin:
<b>pip install pingouin</b>
<h3>Step 2: Create the Data</h3>
Suppose four different judges were asked to rate the quality of six different college entrance exams. We can create the following dataframe to hold the ratings of the judges:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'exam': [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6,            1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6],   'judge': ['A', 'A', 'A', 'A', 'A', 'A',             'B', 'B', 'B', 'B', 'B', 'B',             'C', 'C', 'C', 'C', 'C', 'C',             'D', 'D', 'D', 'D', 'D', 'D'],   'rating': [1, 1, 3, 6, 6, 7, 2, 3, 8, 4, 5, 5,              0, 4, 1, 5, 5, 6, 1, 2, 3, 3, 6, 4]})
#view first five rows of DataFrame
df.head()
examjudgerating
01A1
12A1
23A3
34A6
45A6
</b>
<h3>Step 3: Calculate the Intraclass Correlation Coefficient</h3>
Next, we’ll use the following code to calculate the intraclass correlation coefficient:
<b>import pingouin as pg
icc = pg.intraclass_corr(data=df, targets='exam', raters='judge', ratings='rating')
icc.set_index('Type')
        Description        ICC  F    df1 df2 pvalCI95%
Type
ICC1Single raters absolute0.505252  5.084916  5 18  0.004430  [0.11, 0.89]
ICC2Single random raters0.503054  4.909385  5 15  0.007352  [0.1, 0.89]
ICC3Single fixed raters0.494272  4.909385  5 15  0.007352  [0.09, 0.88]
ICC1kAverage raters absolute0.803340  5.084916  5 18  0.004430  [0.33, 0.97]
ICC2kAverage random raters0.801947  4.909385  5 15  0.007352  [0.31, 0.97]
ICC3kAverage fixed raters0.796309  4.909385  5 15  0.007352  [0.27, 0.97]
</b>
This function returns the following results:
<b>Description:</b> The type of ICC calculated
<b>ICC:</b> The intraclass correlation coefficient (ICC)
<b>F:</b> The F-value of the ICC
<b>df1, df2:</b> The degrees of freedom associated with the F-value
<b>pval:</b> The p-value associated with the F-value
<b>CI95%:</b> The 95% confidence interval for the ICC
Notice that there are six different ICC’s calculated here. This is because there are multiple ways to calculate the ICC depending on the following assumptions:
<b>Model:</b> One-Way Random Effects, Two-Way Random Effects, or Two-Way Mixed Effects
<b>Type of Relationship:</b> Consistency or Absolute Agreement
<b>Unit:</b> Single rater or the mean of raters
For a detailed explanation of these assumptions, please refer to  this article .
<h2><span class="orange">How to Calculate Intraclass Correlation Coefficient in R</span></h2>
An  intraclass correlation coefficient  (ICC) is used to determine if items or subjects can be rated reliably by different raters.
The value of an ICC can range from 0 to 1, with 0 indicating no  reliability  among raters and 1 indicating perfect reliability.
The easiest way to calculate ICC in R is to use the <b>icc()</b> function from the <b>irr</b> package, which uses the following syntax:
<b>icc(ratings, model, type, unit)</b>
where:
<b>ratings:</b> A dataframe or matrix of ratings
<b>model:</b> The type of model to use. Options include “oneway” or “twoway”
<b>type:</b> The type of relationship to calculate between raters. Options include “consistency” or “agreement”
<b>unit:</b> The unit of analysis. Options include “single” or “average”
This tutorial provides an example of how to use this function in practice.
<h3>Step 1: Create the Data</h3>
Suppose four different judges were asked to rate the quality of 10 different college entrance exams. We can create the following dataframe to holding the ratings of the judges:
<b>#create data
data &lt;- data.frame(A=c(1, 1, 3, 6, 6, 7, 8, 9, 8, 7),   B=c(2, 3, 8, 4, 5, 5, 7, 9, 8, 8),   C=c(0, 4, 1, 5, 5, 6, 6, 9, 8, 8),   D=c(1, 2, 3, 3, 6, 4, 6, 8, 8, 9))
</b>
<h3>Step 2: Calculate the Intraclass Correlation Coefficient</h3>
Suppose the four judges were randomly selected from a population of qualified entrance exam judges and that we’d like to measure the absolute agreement among judges and that we’re interested in using the ratings from a single rater perspective as the basis for our measurement.
We can use the following code in R to fit a <b>two-way model</b>, using <b>absolute agreement</b> as the relationship among raters, and using <b>single</b> as our unit of interest:
<b>#load the interrater reliability package
library(irr)
#define data
data &lt;- data.frame(A=c(1, 1, 3, 6, 6, 7, 8, 9, 8, 7),   B=c(2, 3, 8, 4, 5, 5, 7, 9, 8, 8),   C=c(0, 4, 1, 5, 5, 6, 6, 9, 8, 8),   D=c(1, 2, 3, 3, 6, 4, 6, 8, 8, 9))
#calculate ICC
icc(data, model = "twoway", type = "agreement", unit = "single")
   Model: twoway 
   Type : agreement 
   Subjects = 10 
     Raters = 4 
   ICC(A,1) = 0.782
 F-Test, H0: r0 = 0 ; H1: r0 > 0 
    F(9,30) = 15.3 , p = 5.93e-09 
 95%-Confidence Interval for ICC Population Values:
  0.554 &lt; ICC &lt; 0.931</b>
The intraclass correlation coefficient (ICC) turns out to be <b>0.782</b>.
Here is how to interpret the value of an intraclass correlation coefficient, according to  Koo & Li :
<b>Less than 0.50:</b> Poor reliability
<b>Between 0.5 and 0.75:</b> Moderate reliability
<b>Between 0.75 and 0.9:</b> Good reliability
<b>Greater than 0.9:</b> Excellent reliability
Thus, we would conclude that an ICC of <b>0.782</b> indicates that the exams can be rated with “good” reliability by different raters.
<h3>A Note on Calculating ICC</h3>
There are several different versions of an ICC that can be calculated, depending on the following three factors:
<b>Model:</b> One-Way Random Effects, Two-Way Random Effects, or Two-Way Mixed Effects
<b>Type of Relationship:</b> Consistency or Absolute Agreement
<b>Unit:</b> Single rater or the mean of raters
In the previous example, the ICC that we calculated used the following assumptions:
<b>Model:</b> Two-Way Random Effects
<b>Type of Relationship:</b> Absolute Agreement
<b>Unit:</b> Single rater
For a detailed explanation of these assumptions, please refer to  this article .
<h2><span class="orange">Intraclass Correlation Coefficient: Definition + Example</span></h2>
An <b>intraclass correlation coefficient</b> (ICC) is used to measure the  reliability  of ratings in studies where there are two or more raters.
The value of an ICC can range from 0 to 1, with 0 indicating no reliability among raters and 1 indicating perfect reliability among raters.
In simple terms, an ICC is used to determine if items (or subjects) can be rated reliably by different raters.
There are several different versions of an ICC that can be calculated, depending on the following three factors:
<b>Model:</b> One-Way Random Effects, Two-Way Random Effects, or Two-Way Mixed Effects
<b>Type of Relationship:</b> Consistency or Absolute Agreement
<b>Unit:</b> Single rater or the mean of raters
Here’s a brief description of the three different <b>models</b>:
<b>1. One-way random effects model:</b> This model assumes that each subject is rated by a different group of randomly chosen raters. Using this model, the raters are considered the source of random effects. This model is rarely used in practice because the same group of raters is usually used to rate each subject.
<b>2. Two-way random effects model:</b> This model assumes that a group of <em>k</em> raters is randomly selected from a population and then used to rate subjects. Using this model, both the raters and the subjects are considered sources of random effects. This model is often used when we’d like to generalize our findings to any raters who are similar to the raters used in the study.
<b>3. Two-way mixed effects model:</b> This model also assumes that a group of <em>k</em> raters is randomly selected from a population and then used to rate subjects. However, this model assumes that the group of raters we chose are the <em>only</em> raters of interest, which means we aren’t interested in generalizing our findings to any other raters who might also share similar characteristics as the raters used in the study.
Here’s a brief description of the two different <b>types of relationships</b> we might be interested in measuring:
<b>1. Consistency:</b> We are interested in the systematic differences between the ratings of judges (e.g. did the judges rate similar subjects low and high?)
<b>2. Absolute Agreement:</b> We are interested in the absolute differences between the ratings of judges (e.g. what is the absolute difference in ratings between judge A and judge B?)
Here’s a brief description of the two different <b>units</b> we might be interested in measuring:
<b>1. Single rater:</b> We are only interested in using the ratings from a single rater as the basis for measurement.
<b>2. Mean of raters:</b> We are interested in using the mean of ratings from all judges as the basis for measurement. 
<em><b>Note:</b> If you want to measure the level of agreement between two raters who each rate items on a  dichotomous outcome , you should instead use  Cohen’s Kappa .</em>
<h3>How to Interpret Intraclass Correlation Coefficient</h3>
Here is how to interpret the value of an intraclass correlation coefficient, according to  Koo & Li :
<b>Less than 0.50:</b> Poor reliability
<b>Between 0.5 and 0.75:</b> Moderate reliability
<b>Between 0.75 and 0.9:</b> Good reliability
<b>Greater than 0.9:</b> Excellent reliability
The following example shows how to calculate an intraclass correlation coefficient in practice.
<h3>Example: Calculating Intraclass Correlation Coefficient</h3>
Suppose four different judges were asked to rate the quality of 10 different college entrance exams. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/icc1.png">
Suppose the four judges were randomly selected from a population of qualified entrance exam judges and that we’d like to measure the absolute agreement among judges and that we’re interested in using the ratings from a single rater perspective as the basis for our measurement.
We can use the following code in R to fit a <b>two-way random effects model</b>, using <b>absolute agreement</b> as the relationship among raters, and using <b>single</b> as our unit of interest:
<b>#load the interrater reliability package
library(irr)
#define data
data &lt;- data.frame(A=c(1, 1, 3, 6, 6, 7, 8, 9, 8, 7),   B=c(2, 3, 8, 4, 5, 5, 7, 9, 8, 8),   C=c(0, 4, 1, 5, 5, 6, 6, 9, 8, 8),   D=c(1, 2, 3, 3, 6, 4, 6, 8, 8, 9))
#calculate ICC
icc(data, model = "twoway", type = "agreement", unit = "single")
   Model: twoway 
   Type : agreement 
   Subjects = 10 
     Raters = 4 
   ICC(A,1) = 0.782
 F-Test, H0: r0 = 0 ; H1: r0 > 0 
    F(9,30) = 15.3 , p = 5.93e-09 
 95%-Confidence Interval for ICC Population Values:
  0.554 &lt; ICC &lt; 0.931</b>
The intraclass correlation coefficient (ICC) turns out to be <b>0.782</b>.
Based on the rules of thumb for interpreting ICC, we would conclude that an ICC of <b>0.782</b> indicates that the exams can be rated with “good” reliability by different raters.
<h2><span class="orange">How to Fix: invalid value encountered in true_divide</span></h2>
One warning you may encounter when using NumPy is:
<b>RuntimeWarning: invalid value encountered in true_divide</b>
This warning occurs when you attempt to divide by some invalid value (such as NaN, Inf, etc.) in a NumPy array.
It’s worth noting that this is only a <b>warning</b> and NumPy will simply return a nan value when you attempt to divide by an invalid value.
The following example shows how to address this warning in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to divide the values in one NumPy array by the values in another NumPy array:
<b>import numpy as np
#define NumPy arrays
x = np.array([4, 5, 5, 7, 0])
y = np.array([2, 4, 6, 7, 0])
#divide the values in <em>x</em> by the values in <em>y</em>
np.divide(x, y)
array([2.    , 1.25  , 0.8333, 1.    ,    nan])
RuntimeWarning: invalid value encountered in true_divide
</b>
Notice that NumPy divides each value in x by the corresponding value in y, but a <b>RuntimeWarning</b> is produced.
This is because the last division operation performed was zero divided by zero, which resulted in a <b>nan</b> value.
<h3>How to Address this Warning</h3>
As mentioned earlier, this RuntimeWarning is only a <b>warning</b> and it didn’t prevent the code from being run. 
However, if you’d like to suppress this type of warning then you can use the following syntax:
<b>np.seterr(invalid='ignore')
</b>
This tells NumPy to hide any warning with some “invalid” message in it.
So, if we run the code again then we won’t receive any warning:
<b>import numpy as np
#define NumPy arrays
x = np.array([4, 5, 5, 7, 0])
y = np.array([2, 4, 6, 7, 0])
#divide the values in <em>x</em> by the values in <em>y</em>
np.divide(x, y)
array([2.    , 1.25  , 0.8333, 1.    ,    nan])</b>
A <b>nan</b> value is still returned for the last value in the output, but no warning message is displayed this time.
<h2><span class="orange">Inverse Normal Distribution: Definition & Example</span></h2>
The term <b>inverse normal distribution</b> refers to the method of using a known probability to find the corresponding z-critical value in a  normal distribution .
This is not to be confused with the  Inverse Gaussian distribution , which is a continuous probability distribution.
This tutorial provides several examples of how to use the inverse normal distribution in different statistical softwares.
<h3>Inverse Normal Distribution on a TI-83 or TI-84 Calculator</h3>
You’re most likely to encounter the term “inverse normal distribution” on a TI-83 or TI-84 calculator, which uses the following function to find the z-critical value that corresponds to a certain probability:
<b>invNorm(probability, μ, σ)</b>
where:
<b>probability:</b> the significance level
<b>μ:</b> population mean
<b>σ:</b> population standard deviation
You can access this function on a TI-84 calculator by pressing 2nd and then pressing vars. This will take you to a <b>DISTR </b>screen where you can then use <b>invNorm()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/zcritTI1.png">
For example, we can use this function to find the z-critical value that corresponds to a probability value of 0.05:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/zcritTI3.png">
The z-critical value that corresponds to a probability value of 0.05 is <b>-1.64485</b>.
<b>Related:</b>  How to Use invNorm on a TI-84 Calculator (With Examples) 
<h3>Inverse Normal Distribution in Excel</h3>
To find the z-critical value associated with a certain probability value in Excel, we can use the <b>INVNORM()</b> function, which uses the following syntax:
<b>INVNORM(p, mean, sd)</b>
where:
<b>p:</b> the significance level
<b>mean:</b> population mean
<b>sd:</b> population standard deviation
For example, we can use this function to find the z-critical value that corresponds to a probability value of 0.05:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/invNormExcel1.png">
The z-critical value that corresponds to a probability value of 0.05 is <b>-1.64485</b>.
<h3>Inverse Normal Distribution in R</h3>
To find the z-critical value associated with a certain probability value in R, we can use the  qnorm()  function, which uses the following syntax:
<b>qnorm(p, mean, sd)</b>
where:
<b>p:</b> the significance level
<b>mean:</b> population mean
<b>sd:</b> population standard deviation
For example, we can use this function to find the z-critical value that corresponds to a probability value of 0.05:
<b>qnorm(p=.05, mean=0, sd=1)
[1] -1.644854
</b>
Once again, the z-critical value that corresponds to a probability value of 0.05 is <b>-1.64485</b>.
<h2><span class="orange">Inverse t Distribution Calculator</span></h2>
This calculator finds the t-Score associated with a given degrees of freedom and confidence level.
Enter the degrees of freedom and the confidence level in the boxes below and then click the “Calculate” button to find the corresponding one-sided and two-sided t-Score.
<label for="dof"><b>Degrees of freedom</b></label>
<input type="number" id="dof" value="10"><label for="conf"><b>Confidence level</b></label>
<input type="number" id="conf" max="1" value="0.95">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
One-sided t-Score: <b>1.8125</b>
Two-sided t-Score: <b>2.2281</b>
<script>
function calc() {
//get input values and calculate WHIP
var dof = document.getElementById('dof').value*1;
var conf1 = document.getElementById('conf').value*1;
var conf2 = conf1 - (-(1-conf1)/2);
var t1 = jStat.studentt.inv(conf1, dof);
var t2 = jStat.studentt.inv(conf2, dof);
//output
document.getElementById('t1').innerHTML = t1.toFixed(4);
document.getElementById('t2').innerHTML = t2.toFixed(4);
}
</script>
<h2><span class="orange">How to Use invNorm on a TI-84 Calculator (With Examples)</span></h2>
You can use the <b>invNorm()</b> function on a TI-84 calculator to find z critical values associated with the  normal distribution .
This function uses the following syntax:
<b>invNorm(probability, μ, σ)</b>
where:
<b>probability:</b> the significance level
<b>μ:</b> population mean
<b>σ:</b> population standard deviation
You can access this function on a TI-84 calculator by pressing 2nd and then pressing VARS. This will take you to a <b>DISTR </b>screen where you can then use <b>invNorm()</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/zcritTI1.png">
The following examples show how to use this function in practice.
<h3>Example 1: Z-Critical Value for One-Tailed Tests</h3>
Suppose a researcher is conducting a left-tailed hypothesis test using α = .05. What is the z-critical value that corresponds to this alpha level?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/inv1.png">
The answer is z = <b>-1.64485</b>.
Suppose a researcher is conducting a right-tailed hypothesis test using α = .05. What is the z-critical value that corresponds to this alpha level?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/inv2.png">
The answer is z = <b>1.64485</b>.
<h3>Example 2: Z-Critical Value for Two-Tailed Tests</h3>
Suppose a researcher is conduct a two-tailed hypothesis test using α = .05. What is the z-critical value that corresponds to this alpha level?
To find this critical value, we can use the formula 1 – α/2. In this case, we will use 1 – .05/2 = .975 for the probability:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/inv3.png">
The answer is z = <b>1.96</b>.
<h3>Example 3: Z-Critical Value for Cut-Off Scores</h3>
Suppose the scores on a particular exam are normally distributed with a mean of 70 and a standard deviation of 8. What score separates the top 10% from the rest?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/inv4.png">
The answer is <b>80.25</b>.
Suppose the heights of males in a particular city are normally distributed with a mean of 68 inches and a standard deviation of 4 inches. What height separates the bottom 25% from the rest?
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/inv5.png">
The answer is <b>65.3 </b>inches.
<h2><span class="orange">A Complete Guide to the Iris Dataset in R</span></h2>
The <b>iris</b> dataset is a built-in dataset in R that contains measurements on 4 different attributes (in centimeters) for 50 flowers from 3 different species.
This tutorial explains how to explore and summarize a dataset in R, using the iris dataset as an example.
<b>Related:</b>  A Complete Guide to the mtcars Dataset in R 
<h3>Load the Iris Dataset</h3>
Since the iris dataset is a built-in dataset in R, we can load it by using the following command:
<b>data(iris)</b>
We can take a look at the first six rows of the dataset by using the <b>head()</b> function:
<b>#view first six rows of iris dataset
head(iris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2  setosa
2          4.9         3.0          1.4         0.2  setosa
3          4.7         3.2          1.3         0.2  setosa
4          4.6         3.1          1.5         0.2  setosa
5          5.0         3.6          1.4         0.2  setosa
6          5.4         3.9          1.7         0.4  setosa
</b>
<h3>Summarize the Iris Dataset</h3>
We can use the <b>summary()</b> function to quickly summarize each variable in the dataset:
<b>#summarize iris dataset
summary(iris)
  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   
 Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  
 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  
 Median :5.800   Median :3.000   Median :4.350   Median :1.300  
 Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  
 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  
 Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  
       Species  
 setosa    :50  
 versicolor:50  
 virginica :50  </b>
For each of the numeric variables we can see the following information:
<b>Min</b>: The minimum value.
<b>1st Qu</b>: The value of the first quartile (25th percentile).
<b>Median</b>: The median value.
<b>Mean</b>: The mean value.
<b>3rd Qu</b>: The value of the third quartile (75th percentile).
<b>Max</b>: The maximum value.
For the only categorical variable in the dataset (Species) we see a frequency count of each value:
<b>setosa</b>: This species occurs 50 times.
<b>versicolor</b>: This species occurs 50 times.
<b>virginica</b>: This species occurs 50 times.
We can use the <b>dim()</b> function to get the dimensions of the dataset in terms of number of rows and number of columns:
<b>#display rows and columns
dim(iris)
[1] 150   5
</b>
We can see that the dataset has <b>150</b> rows and <b>5</b> columns.
We can also use the <b>names()</b> function to display the column names of the data frame:
<b>#display column names
names(iris)
[1] "Sepal.Length" "Sepal.Width"  "Petal.Length" "Petal.Width"  "Species"     
</b>
<h3>Visualize the Iris Dataset</h3>
We can also create some plots to visualize the values in the dataset.
For example, we can use the <b>hist()</b> function to create a histogram of the values for a certain variable:
<b>#create histogram of values for sepal length
hist(iris$Sepal.Length,
     col='steelblue',
     main='Histogram',
     xlab='Length',
     ylab='Frequency')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/iris1.png">
We can also use the <b>plot()</b> function to create a scatterplot of any pairwise combination of variables:
<b>#create scatterplot of sepal width vs. sepal length
plot(iris$Sepal.Width, iris$Sepal.Length,
     col='steelblue',
     main='Scatterplot',
     xlab='Sepal Width',
     ylab='Sepal Length',
     pch=19)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/iris2.png">
We can also use the <b>boxplot()</b> function to create a boxplot by group:
<b>#create scatterplot of sepal width vs. sepal length
boxplot(Sepal.Length~Species,
        data=iris,
        main='Sepal Length by Species',
        xlab='Species',
        ylab='Sepal Length',
        col='steelblue',
        border='black')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/iris3.png">
The x-axis displays the three species and the y-axis displays the distribution of values for sepal length for each species.
This type of plot allows us to quickly see that the sepal length tends to be largest for the virginica species and smallest for the setosa species.
<h2><span class="orange">Is Age a Discrete or Continuous Variable?</span></h2>
In statistics, numerical variables can be classified as either discrete or continuous:
<b>Discrete:</b> Variables that can only take on whole numbers. For example:
Number of pets owned by a family (1, 2, 5, etc.)
Number of people in a stadium (100, 500, 900, etc.)
Number of cookies in a jar (3, 11, 22, etc.)
<b>Continuous:</b> Variables that can take on <em>any</em> number, including numbers with several values after the decimal point. For example:
Height (70.3434277 inches)
Weight (189.5 pounds)
Time (14.226 seconds)
<b>Rule of Thumb:</b>
 
If you can <i>count </i>the items, then you are working with a discrete variable – e.g. counting the number of people in a stadium.
 
But if you can <i>measure </i>the items, you are working with a continuous variable – e.g. measuring height, weight, time, etc.
Using this rule of thumb, you can easily classify most variables as discrete or continuous.
However, one variable that can be tricky to classify is <b>age</b>. On one hand, you can <em>count</em> the age of a person in years (e.g. 40 years old) but you could also <em>measure</em> someone’s age to an exact number (e.g. 40.225 years old).
<b>So, is age a discrete or continuous variable?</b>
<h3>Is Age Discrete or Continuous?</h3>
<b>Technically speaking, age is a continuous variable because it can take on <em>any</em> value with any number of decimal places.</b>
If you know someone’s birth date, you can calculate their exact age including years, months, weeks, days, hours, seconds, etc. so it’s possible to say that someone is 6.225549 years old.
You couldn’t do the same thing with a discrete variable like “number of pets owned” by a family. For example, you couldn’t say that a family owns 6.225549 pets. They either own 6 or 7 pets.
<b>However, when conducting some type of statistical analysis, age is almost always treated as a discrete variable.</b>
Consider the following examples to illustrate this.
<b>Example 1: Using Age in Medical Studies</b>
Suppose a medical professional is conducting a study in which she wants to know how age, diet, and exercise affect blood pressure.
When collecting data on the individuals in the study, she will record their age using whole numbers like 27 years old, 30 years old, 45 years old, etc.
Although age is technically a continuous variable, she will treat it as a discrete variable and only collect data using whole numbers. 
<b>Example 2: Using Age in Biological Studies</b>
Suppose a biologist wants to understand the correlation between plant height and plant age.
When calculating data on individual plants, she will measure their height in centimeters and measure their age in either days, weeks, or months. For example, she might measure their age as 22 days, 29 days, 34 days, etc. 
Although she could measure age as 22.4543 days, 29.8868 days, 34.0001 days, etc. she will likely measure it using whole numbers as this is easier to do.
<h3>Summary</h3>
If you’re asked whether age is a continuous or discrete variable in an Introductory Statistics class, the correct answer is technically continuous.
However, in the real world age is often treated as a discrete variable because it makes more sense when collecting data and when reporting the results of a study.
<h2><span class="orange">Is Age An Interval or Ratio Variable? (Explanation & Example)</span></h2>
In statistics, all variables are measured on one of four  measurement scales :
<b>Nominal</b>: Variables that have no quantitative values.
<b>Ordinal</b>: Variables that have a natural order, but no quantifiable difference between values.
<b>Interval</b>: Variables that have a natural order and a quantifiable difference between values, but no “true zero” value.
<b>Ratio</b>: Variables that have a natural order, a quantifiable difference between values, and a “true zero” value.
The following graphic summarizes these different levels of measurement:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/levels_measurement.jpg"659">
One question students often have is:
<em><b>Is “age” considered an interval or ratio variable?</b></em>
The short answer:
<b>Age is considered a ratio variable because it has a “true zero” value.</b>
It’s possible for an individual to be zero years old (a newborn) and we can say that the difference between 0 years and 10 years is the same as the difference between 10 years and 20 years.
Since age is a ratio variable, we can also say that someone who is 10 years old is twice as old as someone who is 5 years old.
Contrast this with an interval variable like temperature: We cannot say that 10 degrees Celsius is twice as warm as 5 degrees Celsius because there is no “true zero” when it comes to temperature since degrees can be negative.
<h3>When is Age Not a Ratio Variable?</h3>
The only time that age would not be considered a ratio variable is if the data we collect on age is in categories.
For example, we may send out a survey and ask people to report which age bracket they belong in from the following choices:
0-19 years old
20-39 years old
40-59 years old
60+ years old
In this scenario, age would be treated as an <b>ordinal</b> variable because a natural order exists among the potential values.
We would say 0-19 years old is younger than 20-39 years old, which is younger than 40-50 years old, which is younger than 60+ years old.
We would not classify age as a ratio variable in this scenario because we can’t say with certainty that someone in the 20-39 years old group is twice as old as someone in the 0-19 years age group since we don’t know exact ages.
This represents a rare scenario where we would not classify age as a ratio variable.
<h2><span class="orange">Is Age Considered a Qualitative or Quantitative Variable?</span></h2>
In statistics, there are two types of variables:
<b>Quantitative Variables</b>: Variables that represent a measurable quantity. Examples include:
Square footage
Height
Weight
Population size
<b>Qualitative Variables</b>: Variables that take on names or labels and fit into categories. Examples include:
Eye color
Gender
Marital Status
Dog breed
One question that students often have is:
<em><b>Is age considered a qualitative or quantitative variable?</b></em>
The short answer:
<b>Age is a quantitative variable because it represents a measurable quantity.</b>
For example, if someone is 35 years old then we know they are 5 years older than a 30 year-old but 5 years younger than a 40-year old.
We couldn’t say the same thing about a qualitative variable like “eye color” because it doesn’t make sense to compare “blue” vs “green” vs “brown” eyes in numerical terms.
Also, since age is a quantitative variable this means we can calculate summary statistics for it, such as:
 Measures of central tendency  like the mean, median, and mode.
 Measures of dispersion  like the range, interquartile range, and standard deviation.
For example, if we have a dataset that contains the ages of 100 individuals, we could calculate the mean age, median age, the range of ages, and so on.
We could not do the same for qualitative variables.
<h3>When Is Age Not a Quantitative Variable?</h3>
The only scenario where age would not be considered a quantitative variable is when we use age brackets.
In this scenario, age would be a qualitative variable because ages could be grouped into categories.
For example, suppose an economist wants to study the relationship between annual income and age so he sends out a survey to 1,000 individuals and asks them to indicate their age bracket as one of the following:
Under 18
18 to 35
35 to 52
53 to 70 
Over 70
In this scenario, age would be considered a qualitative variable because each individual would fall into a certain age category as opposed to a specific numerical age value.
In this scenario, we couldn’t calculate summary statistics like the mean and median because we don’t actually know the specific age of each individual.
<h2><span class="orange">How to Use is.na in R (With Examples)</span></h2>
You can use the <b>is.na()</b> function in R to check for missing values in vectors and data frames.
<b>#check if each individual value is NA
is.na(x)
#count total NA values
sum(is.na(x))
#identify positions of NA values
which(is.na(x))
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Use is.na() with Vectors</h3>
The following code shows how to use the <b>is.na()</b> function to check for missing values in a vector:
<b>#define vector with some missing values
x &lt;- c(3, 5, 5, NA, 7, NA, 12, 16)
#check if each individual value is NA
is.na(x)
[1] FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE
#count total NA values
sum(is.na(x))
[1] 2
#identify positions of NA values
which(is.na(x))
[1] 4 6
</b>
From the output we can see:
There are 2 missing values in the vector.
The missing values are located in position 4 and 6.
<h3>
<b>Example 2: Use is.na() with Data Frames</b>
</h3>
The following code shows how to use the is.na() function to check for missing values in a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, NA, NA, 3, 2), var3=c(3, 3, 6, NA, 8), var4=c(NA, 1, 2, 8, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3   NA
2    3   NA    3    1
3    3   NA    6    2
4    4    3   NA    8
5    5    2    8    9
#find total NA values in data frame
sum(is.na(df))
[1] 4
#find total NA values by column
sapply(df, function(x) sum(is.na(x)))
var1 var2 var3 var4 
   0    2    1    1 </b>
From the output we can see that there are <b>4</b> total NA values in the data frame.
We can also see:
There are <b>0</b> NA values in the ‘var1’ column.
There are <b>2</b> NA values in the ‘var2’ column.
There are <b>1</b> NA values in the ‘var3’ column.
There are <b>1</b> NA values in the ‘var4’ column.
<h2><span class="orange">How to Use is.null in R (With Examples)</span></h2>
You can use the <b>is.null</b> function in R to test whether a data object is NULL.
This function uses the following basic syntax:
<b>is.null(x)</b>
where:
<b>x</b>: An R object to be tested
The following examples show how to use this function in different scenarios.
<h3>Example 1: Use is.null to Check if Object is NULL</h3>
The following code shows how to use <b>is.null</b> to test whether two different vectors are equal to NULL:
<b>#create non-null vector
x &lt;- c(1, 4, 15, 6, 7)
#test if x is NULL
is.null(x)
[1] FALSE
#create null vector
y &lt;- NULL
#test if y is NULL
is.null(y)
[1] TRUE
</b>
The <b>is.null</b> function returns <b>FALSE</b> for the first vector and <b>TRUE</b> for the second vector.
Also note that <b>is.null</b> will return <b>TRUE</b> if a vector exists but is empty:
<b>#create empty vector
x &lt;- c()
#test if x is NULL
is.null(x)
[1] TRUE
</b>
<h3>Example 2: Use !is.null to Check if Object is Not NULL</h3>
The following code shows how to use <b>!is.null</b> to test whether two different vectors are not equal to NULL:
<b>#create non-null vector
x &lt;- c(1, 4, 15, 6, 7)
#test if x is not NULL
!is.null(x)
[1] TRUE
#create non-null vector
y &lt;- NULL
#test if y is not NULL
!is.null(y)
[1] FALSE
</b>
The <b>!is.null</b> function returns <b>TRUE</b> for the first vector and <b>FALSE</b> for the second vector.
<h2><span class="orange">Is Time An Interval or Ratio Variable? (Explanation & Example)</span></h2>
In statistics, all variables are measured on one of four  measurement scales :
<b>Nominal</b>: Variables that have no quantitative values.
<b>Ordinal</b>: Variables that have a natural order, but no quantifiable difference between values.
<b>Interval</b>: Variables that have a natural order and a quantifiable difference between values, but no “true zero” value.
<b>Ratio</b>: Variables that have a natural order, a quantifiable difference between values, and a “true zero” value.
The following graphic summarizes these different levels of measurement:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/levels_measurement.jpg"659">
One question students often have is:
<em><b>Is “time” considered an interval or ratio variable?</b></em>
The short answer:
<b>Time is considered an interval variable because differences between all time points are equal but there is no “true zero” value for time.</b>
For example, the difference between 1 PM and 2 PM is the same as the difference between 2 PM and 3 PM, which is the same as the difference between 3 PM and 4 PM, and so on.
However, there is not “true zero” value of time. For example, we can’t say that 2 PM is twice as old of a time as 1 PM.
Contrast this with a ratio variable like weight: We can say that 100 pounds is twice as much as 50 pounds. The same cannot be said for time.
<h3>When is Time Not an Interval Variable?</h3>
The only scenario where time would not be considered an interval variable is if we’re talking about a <b>duration of time</b>.
Consider the following scenarios:
<b>Scenario 1: Marathon Times</b>
Suppose we keep track of how long it takes people to run a marathon. In this scenario, the duration of time would be considered a ratio variable because there is a “true zero” value – zero seconds.
We could also say that someone who runs the marathon in 2 hours ran it in half the amount of time as someone who ran it in 4 hours.
<b>Scenario 2: Cooking Times</b>
Suppose we compare two recipes for cooking a certain meal. One recipe has a total cooking time of 40 minutes and the other has a cooking time of 20 minutes.
In this scenario, the duration of cooking time would be considered a ratio variable because there is a true zero value – zero minutes.
We could also say that one recipe has a cooking time that is twice as long as the other.
These represent scenarios where we would classify time as a ratio variable instead  of an interval variable.
<h2><span class="orange">How to Use ISERROR in Google Sheets (With Examples)</span></h2>
The <b>ISERROR </b>function in Google Sheets can be used to check whether a value in a specific cell is an error.
This function uses the following basic syntax:
<b>=ISERROR(A1)</b>
This function returns <b>TRUE</b> if the value in cell <b>A1</b> is an error, otherwise it returns <b>FALSE</b>.
The following examples show to use this function in two different scenarios in Google Sheets.
<h3>Example 1: Use ISERROR to Return New Value</h3>
Suppose we attempt to divide the values in column A by the values in column B in the following Google Sheets spreadsheet:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/iserror1.jpg"497">
When dividing by zero, we get <b>#DIV/0!</b> as a result in column C.
We could use the following formula to instead return a value of “Invalid Division” as a result:
<b>=IF(ISERROR(A2/B2), "Invalid Division", A2/B2)
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/iserror2.jpg"546">
If the function <b>ISERROR(A2/B2)</b> is <b>TRUE</b>, then “Invalid Division” is returned.
Otherwise, if the function <b>ISERROR(A2/B2)</b> is <b>FALSE</b>, then the result of A2/B2 is returned.
<h3>Example 2: Use ISERROR with VLOOKUP</h3>
Suppose we attempt to perform a <b>VLOOKUP</b> in the following spreadsheet to find the value in the <b>Points</b> column that corresponds to a name of “Mag” in the <b>Team</b> column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/iserror3-1.jpg"527">
Since the name “Mag” does not exist in the <b>Team</b> column, we receive <b>#N/A</b> as a result.
We could instead use the following formula to return a value of “Team Does Not Exist” if the <b>VLOOKUP</b> formula is unable to find the team name:
<b>=IF(ISERROR(VLOOKUP("Mag", A1:B11, 2, FALSE)), "Team Does Not Exist", VLOOKUP("Mag", A1:B11, 2, FALSE)) 
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/iserror4.jpg"645">
Since the name “Mag” does not exist in the <b>Team</b> column, the formula returns “Team Does Not Exist” as a result instead of <b>#N/A</b>.
<b>Note</b>: You can find the complete online documentation for the <b>ISERROR </b>function  here .
<h2><span class="orange">How to Use Italic Font in R (With Examples)</span></h2>
You can use the following basic syntax to produce italic font in R plots:
<b>substitute(paste(italic('this text is italic')))
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Italic Font in Title of Plot</h3>
The following code shows how to use italic font in the title of a plot in R:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatterplot with title in italics
plot(x, y, main = substitute(paste(italic('Scatterplot of x vs. y'))))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/italic1.png">
Note that we can also specify italic font for only some of the words in the title:
<b>#create scatterplot with only some of title in italics
plot(x, y, main = substitute(paste(italic('Scatterplot of'), ' x vs. y')))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/italic2.png">
<h3>Example 2: Italic Font on Axis Labels of Plot</h3>
The following code shows how to specify italic font for the x-axis and y-axis labels of a plot:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatterplot with axes labels in italics
plot(x, y, xlab = substitute(paste(italic('X Label'))),
           ylab = substitute(paste(italic('Y Label'))))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/italic3.png">
<h3>Example 3: Italic Font with Text in Plot</h3>
The following code shows how to include italic font for a text element inside of a plot:
<b>#define data
x &lt;- c(1, 2, 3, 4, 4, 5, 6, 6, 7, 9)
y &lt;- c(8, 8, 9, 10, 13, 12, 10, 11, 14, 17)
#create scatterplot
plot(x, y)
#add italic text at location x=3, y=16
text(3, 16, substitute(paste(italic('This is some italic text'))))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/italic4.png">
<h2><span class="orange">How to Calculate Jaccard Similarity in R</span></h2>
The  Jaccard similarity index  measures the similarity between two sets of data. It can range from 0 to 1. The higher the number, the more similar the two sets of data.
The Jaccard similarity index is calculated as:
<b>Jaccard Similarity</b> = (number of observations in both sets) / (number in either set)
Or, written in notation form:
<b>J(A, B) = </b>|A∩B| / |A∪B|
This tutorial explains how to calculate Jaccard Similarity for two sets of data in R.
<h3>Example: Jaccard Similarity in R</h3>
Suppose we have the following two sets of data:
<b>a &lt;- c(0, 1, 2, 5, 6, 8, 9)</b>
<b>b &lt;- c(0, 2, 3, 4, 5, 7, 9)</b>
We can define the following function to calculate the Jaccard Similarity between the two sets:
<b>#define Jaccard Similarity function
jaccard &lt;- function(a, b) {
    intersection = length(intersect(a, b))
    union = length(a) + length(b) - intersection
    return (intersection/union)
}
#find Jaccard Similarity between the two sets 
jaccard(a, b)
0.4</b>
The Jaccard Similarity between the two lists is <b>0.4</b>.
Note that the function will return <b>0 </b>if the two sets don’t share any values:
<b>c &lt;- c(0, 1, 2, 3, 4, 5)
d &lt;- c(6, 7, 8, 9, 10)
jaccard(c, d)
[1] 0</b>
And the function will return <b>1 </b>if the two sets are identical:
<b>e &lt;- c(0, 1, 2, 3, 4, 5)
f &lt;- c(0, 1, 2, 3, 4, 5)
jaccard(e, f)
[1] 1</b>
The function also works for sets that contain strings:
<b>g &lt;- c('cat', 'dog', 'hippo', 'monkey')
h &lt;- c('monkey', 'rhino', 'ostrich', 'salmon')
jaccard(g, h)
0.142857</b>
You can also use this function to find the <b>Jaccard distance </b>between two sets, which is the <em>dissimilarity</em> between two sets and is calculated as 1 – Jaccard Similarity.
<b>a &lt;- c(0, 1, 2, 5, 6, 8, 9)
b &lt;- c(0, 2, 3, 4, 5, 7, 9)
#find Jaccard distance between sets <em>a </em>and <em>b</em>
1 - jaccard(a, b)
[1] 0.6</b>
<em>Refer to  this Wikipedia page  to learn more details about the Jaccard Similarity Index.</em>
<h2><span class="orange">How to Calculate Jaccard Similarity in Python</span></h2>
The  Jaccard similarity index  measures the similarity between two sets of data. It can range from 0 to 1. The higher the number, the more similar the two sets of data.
The Jaccard similarity index is calculated as:
<b>Jaccard Similarity</b> = (number of observations in both sets) / (number in either set)
Or, written in notation form:
<b>J(A, B) = </b>|A∩B| / |A∪B|
This tutorial explains how to calculate Jaccard Similarity for two sets of data in Python.
<h3>Example: Jaccard Similarity in Python</h3>
Suppose we have the following two sets of data:
<b>import numpy as np
a = [0, 1, 2, 5, 6, 8, 9]
b = [0, 2, 3, 4, 5, 7, 9]</b>
We can define the following function to calculate the Jaccard Similarity between the two sets:
<b>#define Jaccard Similarity function
def jaccard(list1, list2):
    intersection = len(list(set(list1).intersection(list2)))
    union = (len(list1) + len(list2)) - intersection
    return float(intersection) / union
#find Jaccard Similarity between the two sets 
jaccard(a, b)
0.4</b>
The Jaccard Similarity between the two lists is <b>0.4</b>.
Note that the function will return <b>0 </b>if the two sets don’t share any values:
<b>c = [0, 1, 2, 3, 4, 5]
d = [6, 7, 8, 9, 10]
jaccard(c, d)
0.0</b>
And the function will return <b>1 </b>if the two sets are identical:
<b>e = [0, 1, 2, 3, 4, 5]
f = [0, 1, 2, 3, 4, 5]
jaccard(e, f)
1.0</b>
The function also works for sets that contain strings:
<b>g = ['cat', 'dog', 'hippo', 'monkey']
h = ['monkey', 'rhino', 'ostrich', 'salmon']
jaccard(g, h)
0.142857</b>
You can also use this function to find the <b>Jaccard distance </b>between two sets, which is the <em>dissimilarity</em> between two sets and is calculated as 1 – Jaccard Similarity.
<b>a = [0, 1, 2, 5, 6, 8, 9]
b = [0, 2, 3, 4, 5, 7, 9]
#find Jaccard distance between sets <em>a </em>and <em>b</em>
1 - jaccard(a, b)
0.6</b>
<b>Related: </b> How to Calculate Jaccard Similarity in R 
<em>Refer to  this Wikipedia page  to learn more details about the Jaccard Similarity Index.</em>
<h2><span class="orange">A Simple Explanation of the Jaccard Similarity Index</span></h2>
The <b>Jaccard Similarity Index</b> is a measure of the similarity between two sets of data.
Developed by  Paul Jaccard , the index ranges from 0 to 1. The closer to 1, the more similar the two sets of data.
The Jaccard similarity index is calculated as:
<b>Jaccard Similarity</b> = (number of observations in both sets) / (number in either set)
Or, written in notation form:
<b>J(A, B) = </b>|A∩B| / |A∪B|
If two datasets share the exact same members, their Jaccard Similarity Index will be 1. Conversely, if they have no members in common then their similarity will be 0.
The following examples show how to calculate the Jaccard Similarity Index for a few different datasets.
<h3>Example 1: Jaccard Similarity </h3>
Suppose we have the following two sets of data:
<b>A = [0, 1, 2, 5, 6, 8, 9]
B = [0, 2, 3, 4, 5, 7, 9]</b>
To calculate the Jaccard Similarity between them, we first find the total number of observations in both sets, then divide by the total number of observations in either set:
<b>Number of observations in both:</b> {0, 2, 5, 9} = 4
<b>Number of observations in either:</b> {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} = 10
<b>Jaccard Similarity:</b> 4 / 10 = 0.4
The Jaccard Similarity Index turns out to be <b>0.4</b>.
<h3>Example 2: Jaccard Similarity Continued</h3>
Suppose we have the following two sets of data:
<b>C = [0, 1, 2, 3, 4, 5]
D = [6, 7, 8, 9, 10]</b>
To calculate the Jaccard Similarity between them, we first find the total number of observations in both sets, then divide by the total number of observations in either set:
<b>Number of observations in both:</b> {} = 0
<b>Number of observations in either:</b> {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10} = 11
<b>Jaccard Similarity:</b> 0 / 11 = 0
The Jaccard Similarity Index turns out to be <b>0</b>. This indicates that the two datasets share no common members.
<h3>Example 3: Jaccard Similarity for Characters</h3>
Note that we can also use the Jaccard Similarity index for datasets that contain characters as opposed to numbers.
For example, suppose we have the following two sets of data:
<b>E = ['cat', 'dog', 'hippo', 'monkey']
F = ['monkey', 'rhino', 'ostrich', 'salmon']</b>
To calculate the Jaccard Similarity between them, we first find the total number of observations in both sets, then divide by the total number of observations in either set:
<b>Number of observations in both:</b> {‘monkey’} = 1
<b>Number of observations in either:</b> {‘cat’, ‘dog’, hippo’, ‘monkey’, ‘rhino’, ‘ostrich’, ‘salmon’} = 7
<b>Jaccard Similarity: </b>1 / 7= 0.142857
The Jaccard Similarity Index turns out to be <b>0.142857</b>. Since this number is fairly low, it indicates that the two sets are quite dissimilar. 
<h3>The Jaccard Distance</h3>
The <b>Jaccard distance</b> measures the <i>dissimilarity</i> between two datasets and is calculated as:
Jaccard distance = 1 – Jaccard Similarity
This measure gives us an idea of the difference between two datasets or the <em>difference</em> between them.
For example, if two datasets have a Jaccard Similarity of 80% then they would have a Jaccard distance of 1 – 0.8 = 0.2 or 20%.
<h2><span class="orange">An Introduction to Jaro-Winkler Similarity (Definition & Example)</span></h2>
In statistics, the <b>Jaro-Winkler similarity </b>is a way to measure the similarity between two strings.
The <b>Jaro similarity</b> (sim<sub>j</sub>) between two strings is defined as:
sim<sub>j</sub> = 1/3 * ( m /|s<sub>1</sub>| + m/|s<sub>2</sub>| + (m-t)/m )
where:
<b>m</b>: Number of matching characters
Two characters from s<sub>1</sub> and s<sub>2</sub> are considered matching if they are the same and not farther than [max(|s<sub>1</sub>|, |s<sub>2</sub>|) / 2] – 1 characters apart.
<b>|s<sub>1</sub>|</b>, <b>|s<sub>2</sub>|</b>: The length of the first and second strings, respectively
<b>t</b>: Number of transpositions
Calculated as the number of matching (but different sequence order) characters divided by 2.
The <b>Jaro-Winkler similarity</b> (sim<sub>w</sub>) is defined as:
sim<sub>w</sub> = sim<sub>j</sub> + lp(1 – sim<sub>j</sub>)
where:
<b>sim<sub>j</sub></b>: The Jaro similarity between two strings, s<sub>1</sub> and s<sub>2</sub>
<b>l</b>: Length of the common prefix at the start of the string (max of 4 characters)
<b>p</b>: Scaling factor for how much the score is adjusted upwards for having common prefixes. Typically this is defined as p = 0.1 and should not exceed p = 0.25.
The Jaro-Winkler similarity between two strings is always between 0 and 1 where:
<b>0</b> indicates no similarity between the strings
<b>1</b> indicates that the strings are an exact match
<b>Note</b>: The Jaro-Winkler <em>distance</em> would be defined as 1 – sim<sub>w</sub>.
The following example shows how to calculate the Jaro-Winkler similarity between two strings in practice.
<h3>Example: Calculating Jaro-Winkler Similarity Between Two Strings</h3>
Suppose we have the following two strings:
String 1 (s<sub>1</sub>): <b>mouse</b>
String 2 (s<sub>2</sub>): <b>mute</b>
First, let’s calculate the Jaro Similarity between these two strings:
sim<sub>j</sub> = 1/3 * ( m /|s<sub>1</sub>| + m/|s<sub>2</sub>| + (m-t)/m )
where:
<b>m</b>: Number of matching characters
Two characters from s<sub>1</sub> and s<sub>2</sub> are considered matching if they are the same and not farther than [max(|s<sub>1</sub>|, |s<sub>2</sub>|) / 2] – 1 characters apart.
In this case, [max(|s<sub>1</sub>|, |s<sub>2</sub>|) / 2] – 1 is calculated as 5/2 – 1 = 1.5. We would define three letters as matching: m, u, e. Thus,<b> m = 3</b>.
<b>|s<sub>1</sub>|</b>, <b>|s<sub>2</sub>|</b>: The length of the first and second strings, respectively
In this case, <b>|s<sub>1</sub>| = 5 </b>and <b>|s<sub>1</sub>| = 4</b>.
<b>t</b>: Number of transpositions
Calculated as the number of matching (but different sequence order) characters divided by 2.
In this case, there are three matching characters but they’re already in the same sequence order, so <b>t = 0</b>.
Thus, we would calculate the Jaro Similarity as:
sim<sub>j</sub> = 1/3 * ( 3/5 + 3/4 + (3-0)/3 ) = 0.78333.
Next, let’s calculate the Jaro-Winkler similarity (sim<sub>w</sub>) as:
sim<sub>w</sub> = sim<sub>j</sub> + lp(1 – sim<sub>j</sub>)
In this case, we would calculate:
sim<sub>w</sub> = 0.78333 + (1)*(0.1)(1 – 0.78333) = 0.805.
The Jaro-Winkler similarity between the two strings is <b>0.805</b>.
Since this value is close to 1, it tells us that the two strings are very similar.
We can confirm this is correct by calculating the Jaro-Winkler similarity between the two strings in R:
<b>library(stringdist)
#calculate Jaro-Winkler similarity between 'mouse' and 'mute'
1 - stringdist("mouse", "mute", method = "jw", p=0.1)
[1] 0.805
</b>
This matches the value that we calculated by hand.
<h2><span class="orange">How to Perform a Jarque-Bera Test in Excel</span></h2>
The <b>Jarque-Bera test</b> is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a  normal distribution .
The test statistic of the Jarque-Bera test is always a positive number and if it’s far from zero, it indicates that the sample data do not have a normal distribution.
The test statistic <em><b>JB</b> </em>is defined as:
<em><b>JB</b> </em> =(n/6) * (S<sup>2</sup> + (C<sup>2</sup>/4))
where:
<b>n:</b> the number of observations in the sample
<b>S: </b>the sample skewness
<b>C:</b> the sample kurtosis
Under the null hypothesis of normality, <em>JB ~ </em>X<sup>2</sup>(2)
This tutorial explains how to conduct a Jarque-Bera test in Excel.
<h2>Jarque-Bera test in Excel</h2>
Use the following steps to perform a Jarque-Bera test for a given dataset in Excel.
<b>Step 1: Input the data.</b>
First, input the dataset into one column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/jarqueBeraExcel1.png">
<b>Step 2: Calculate the Jarque-Bera Test Statistic.</b>
Next, calculate the JB test statistic. Column F shows the formulas used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/jarqueBeraExcel2.png">
<b>Step 3: Calculate the p-value of the test.</b>
Recall that under the null hypothesis of normality, the test statistic JB follows a Chi-Square distribution with 2 degrees of freedom. Thus, to find the p-value for the test we will use the following function in Excel: <b>=CHISQ.DIST.RT(JB test statistic, 2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/jarqueBeraExcel3.png">
The p-value of the test is 0.5921</b>. Since this p-value is not less than 0.05, we fail to reject the null hypothesis. We don’t have sufficient evidence to say that the dataset is not normally distributed.
<h2><span class="orange">How to Perform a Jarque-Bera Test in Python</span></h2>
The <b>Jarque-Bera test</b> is a goodness-of-fit test that determines whether or not sample data have skewness and kurtosis that matches a normal distribution.
The test statistic of the Jarque-Bera test is always a positive number and the further it is from zero, the more evidence that the sample data does not follow a normal distribution.
This tutorial explains how to conduct a Jarque-Bera test in Python.
<h2>How to Perform a Jarque-Bera test in Python</h2>
To conduct a Jarque-Bera test in Python we can use the  jarque_bera function  from the Scipy library, which uses the following syntax:
<b>jarque_bera(x)</b>
where:
<b>x:</b> an array of observations
This function returns a test statistic and a corresponding p-value.
<h3>Example 1</h3>
Suppose we perform a Jarque-Bera test on a list of 5,000 values that follow a normal distribution:
<b>import numpy as np
import scipy.stats as stats
#generate array of 5000 values that follow a standard normal distribution
np.random.seed(0)
data = np.random.normal(0, 1, 5000)
#perform Jarque-Bera test
stats.jarque_bera(data)
(statistic=1.2287, pvalue=0.54098)
</b>
The test statistic is <b>1.2287 </b>and the corresponding p-value is <b>0.54098</b>. Since this p-value is not less than .05, we fail to reject the null hypothesis. We don’t have sufficient evidence to say that this data has skewness and kurtosis that is significantly different from a normal distribution.
This result shouldn’t be surprising since the data that we generated is composed of 5000 random variables that follow a normal distribution.
<h3>Example 2</h3>
Now suppose we perform a Jarque-Bera test on a list of 5,000 values that follow a uniform distribution:
<b>import numpy as np
import scipy.stats as stats
#generate array of 5000 values that follow a uniform distribution
np.random.seed(0)
data = np.random.uniform(0, 1, 5000)
#perform Jarque-Bera test
stats.jarque_bera(data)
(statistic=300.1043, pvalue=0.0)</b>
The test statistic is <b>300.1043</b> and the corresponding p-value is <b>0.0</b>. Since this p-value is less than .05, we reject the null hypothesis. Thus, we have sufficient evidence to say that this data has skewness and kurtosis that is significantly different from a normal distribution.
This result also shouldn’t be surprising since the data that we generated is composed of 5000 random variables that follow a uniform distribution, which should have skewness and kurtosis that are much different than a normal distribution.
<h3>When to Use the Jarque-Bera Test</h3>
The Jarque-Bera Test is typically used for large datasets (n > 2000) in which other normality tests (like the Shapiro-Wilk test) are unreliable.
This is an appropriate test to use before you perform some analysis in which it’s assumed that the dataset follows a normal distribution. A Jarque-Bera test can tell you whether or not this assumption is satisfied.
<h2><span class="orange">How to Use the Jitter Function in R for Scatterplots</span></h2>
This tutorial explains when and how to use the <b>jitter</b> function in R for scatterplots.
<h3>When to Use Jitter</h3>
 <b>Scatterplots</b>  are excellent for visualizing the relationship between two continuous variables. For example, the following scatterplot helps us visualize the relationship between height and weight for 100 athletes:
<b>#define vectors of heights and weights
weights &lt;- runif(100, 160, 240) 
heights &lt;- (weights/3) + rnorm(100)
#create data frame of heights and weights
data &lt;- as.data.frame(cbind(weights, heights))
#view first six rows of data frame
head(data)
#   weights  heights
#1 170.8859 57.20745
#2 183.2481 62.01162
#3 235.6884 77.93126
#4 231.9864 77.12520
#5 200.8562 67.93486
#6 169.6987 57.54977
#create scatterplot of heights vs weights
plot(data$weights, data$heights, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter1.jpg">
However, on some occasions we may want to visualize the relationship between one continuous variable and another variable that is <em>almost </em>continuous. 
For example, suppose we have the following dataset that shows the number of games a basketball player has started out of the first 10 games in a season as well as their average points per game:
<b>#create data frame
games_started &lt;- sample(1:10, 300, TRUE)
points_per_game &lt;- 3*games_started + rnorm(300)
data &lt;- as.data.frame(cbind(games_started, points_per_game))
#view first six rows of data frame
head(data)
#  games_started points_per_game
#1             9       25.831554
#2             9       26.673983
#3            10       29.850948
#4             4       12.024353
#5             4       11.534192
#6             1        4.383127</b>
<em>Points per game</em> is a continuous variable, but <em>games started </em>is a discrete variable. If we attempt to create a scatterplot of these two variables, here is what it would look like:
<b>#create scatterplot of games started vs average points per game
plot(data$games_started, data$points_per_game, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter2.jpg">
From this scatterplot, we can tell that <em>games started </em>and <em>average points per game </em>has a positive relationship, but it’s a bit hard to see the individual points in the plot because so many of them overlap with each other.
By using the <b>jitter </b>function, we can add a bit of “noise” to the x-axis variable <em>games started </em>so that we can see the individual points on the plot more clearly:
<b>#add jitter to <em>games started
</em>plot(jitter(data$games_started), data$points_per_game, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter3.jpg">
We can optionally add a numeric argument to jitter to add even more noise to the data:
<b>#add jitter to <em>games started
</em>plot(jitter(data$games_started, 2), data$points_per_game, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter4.jpg">
We should be careful not to add too much jitter, though, as this can distort the original data too much:
<b>plot(jitter(data$games_started, 20), data$points_per_game, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter5.jpg">
<h3>Jittering Provides a Better View of the Data</h3>
Jittering is particularly useful when one of the levels of the discrete variable has far more values than the other levels.
For example, in the following dataset there are three hundred basketball players who started 2 out of the first 5 games in the season, but just one hundred players who started 1, 3, 4, or 5 games:
<b>games_started &lt;- sample(1:5, 100, TRUE)
points_per_game &lt;- 3*games_started + rnorm(100)
data &lt;- as.data.frame(cbind(games_started, points_per_game))
games_twos &lt;- rep(2, 200)
points_twos &lt;- 3*games_twos + rnorm(200)
data_twos &lt;- as.data.frame(cbind(games_twos, points_twos))
names(data_twos) &lt;- c('games_started', 'points_per_game')
all_data &lt;- rbind(data, data_twos)</b>
When we visualize the number of games played vs average points per game, we can tell that there are more players who have played 2 games, but it’s hard to tell exactly <em>how many more </em>have played 2 games:
<b>plot(all_data$games_started, all_data$points_per_game, pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter6.jpg">
Once we add jitter to the <em>games started </em>variable, though, we can see just how many more players there are who have started 2 games:
<b>plot(jitter(all_data$games_started), all_data$points_per_game,
     pch = 16, col = 'steelblue')</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter7.jpg">
Increasing the amount of jitter by a little bit reveals this difference even more:
<b>plot(jitter(all_data$games_started, 1.5), all_data$points_per_game,
     pch = 16, col = 'steelblue')</b>
<h3>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/jitter8.jpg">
<b>Jittering for Visualizations Only</b>
</h3>
As mentioned before, jittering adds some random noise to data, which can be beneficial when we want to visualize data in a scatterplot. By using the jitter function, we can get a better picture of the true underlying relationship between two variables in a dataset.
However, when using a statistical analysis like regression, it doesn’t make sense to add random noise to variables in a dataset since this would impact the results of an analysis.
Thus, jitter is only meant to be used for data visualization, not for data analysis.
<h2><span class="orange">How to Join Multiple Data Frames Using dplyr</span></h2>
Often you may be interested in joining multiple data frames in R. Fortunately this is easy to do using the  left_join()  function from the  dplyr  package.
<b>library(dplyr)</b>
For example, suppose we have the following three data frames:
<b>#create data frame
df1 &lt;- data.frame(a = c('a', 'b', 'c', 'd', 'e', 'f'),  b = c(12, 14, 14, 18, 22, 23))
df2 &lt;- data.frame(a = c('a', 'a', 'a', 'b', 'b', 'b'),  c = c(23, 24, 33, 34, 37, 41))
df3 &lt;- data.frame(a = c('d', 'e', 'f', 'g', 'h', 'i'),  d = c(23, 24, 33, 34, 37, 41))
</b>
To join all three data frames together, we can simply perform two left joins, one after the other:
<b>#join the three data frames
df1 %>%
    left_join(df2, by='a') %>%
    left_join(df3, by='a')
   a  b  c  d
1  a 12 23 NA
2  a 12 24 NA
3  a 12 33 NA
4  b 14 34 NA
5  b 14 37 NA
6  b 14 41 NA
7  c 14 NA NA
8  d 18 NA 23
9  e 22 NA 24
10 f 23 NA 33
</b>
Note that you can also save the result of this join as a data frame:
<b>#join the three data frames and save result as new data frame named all_data
all_data &lt;- df1 %>%
              left_join(df2, by='a') %>%
              left_join(df3, by='a')
#view summary of resulting data frame
glimpse(all_data)
Observations: 10
Variables: 4
$ a &lt;chr> "a", "a", "a", "b", "b", "b", "c", "d", "e", "f"
$ b &lt;dbl> 12, 12, 12, 14, 14, 14, 14, 18, 22, 23
$ c &lt;dbl> 23, 24, 33, 34, 37, 41, NA, NA, NA, NA
$ d &lt;dbl> NA, NA, NA, NA, NA, NA, NA, 23, 24, 33
</b>
<h2><span class="orange">Joint Frequency: Definition & Examples</span></h2>
A <b>two-way frequency table</b> is a table that displays the frequencies for two categorical variables.
For example, the following two-way table shows the results of a survey that asked 100 people which sport they liked best: baseball, basketball, or football.
The rows display the gender of the respondent and the columns show which sport they chose:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq1.png">
The <b>marginal frequencies</b> are the frequencies shown in the margins of the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq2.png">
These values tell us the total values for each variable. For example:
<b>36</b> total respondents chose baseball as their favorite sport
<b>31</b> total respondents chose basketball as their favorite sport
<b>33</b> total respondents chose football as their favorite sport
We can also see:
<b>48</b> total respondents were male
<b>52</b> total respondents were female
The <b>joint frequencies</b> are the frequencies shown in the cells of the table:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq3.png">
These values are known as “joint” frequencies because they tell us the frequency of two values that occur jointly.
For example, we can see:
There were a total of <b>13</b> respondents who were male <em>and</em> preferred baseball.
There were a total of <b>15</b> respondents who were male <em>and</em> preferred basketball.
There were a total of <b>20</b> respondents who were male <em>and</em> preferred football.
There were a total of <b>23</b> respondents who were female <em>and</em> preferred baseball.
There were a total of <b>16</b> respondents who were female <em>and</em> preferred basketball.
There were a total of <b>13</b> respondents who were female <em>and</em> preferred football.
Notice that the sum of all the joint frequencies adds up to the total number of survey respondents:
Total survey respondents = 13 + 15 + 20 + 23 + 16 + 13 = <b>100</b>.
<h3>What Are Joint Relative Frequencies?</h3>
A <b>joint relative frequency</b> tells us the frequency of one variable <em>relative</em> to another variable. 
For example, consider our two-way table from earlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq1.png">
<b>Question 1: </b>What is the joint relative frequency that a survey respondent prefers baseball, given that the respondent is a female?
To answer this, we will only consider the row that contains female responses. We’ll then take the number of females who prefer baseball and divide by the total number of females.
This turns out to be 23/52 = 0.4423 = <b>44.23%</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq4.png">
In other words, 44.23% of all female survey respondents prefer baseball as their favorite sport.
<b>Question 2: </b>What is the joint relative frequency that a survey respondent is male, given that they prefer football as their favorite sport?
To answer this, we will only consider the column that contains football as the favorite sport. We’ll then take the number of males who prefer football and divide by the total number of respondents who preferred football.
This turns out to be 20/33 = 0.606 = <b>60.6%</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/jointFreq5.png">
In other words, 60.6% of all survey respondents who prefer football are male.
<h2><span class="orange">What is a Joint Probability Distribution?</span></h2>
A <b>two-way frequency table</b> is a table that displays the frequencies (or “counts”) for two categorical variables.
For example, the following two-way table shows the results of a survey that asked 100 people which sport they liked best: baseball, basketball, or football.
The rows display the gender of the respondent and the columns show which sport they chose:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/marginal1.png">
In this example, there are two variables: Sports and Gender.
A <b>joint probability distribution</b> simply describes the probability that a given individual takes on two specific values for the variables.
The word “joint” comes from the fact that we’re interested in the probability of two things happening at once.
For example, out of the 100 total individuals there were 13 who were male <em>and</em> chose baseball as their favorite sport.
Thus, we would say the joint probability that a given individual is male <em>and</em> chooses baseball as their favorite sport is 13/100 = <b>0.13</b> or <b>13%</b>.
Written in mathematical notation:
P(Gender = Male, Sport = Baseball) = 13/100 = <b>0.13</b>.
We can use this process to calculate the entire joint probability distribution:
P(Gender = Male, Sport = Baseball) = 13/100 = <b>0.13</b>
P(Gender = Male, Sport = Basketball) = 15/100 = <b>0.15</b>
P(Gender = Male, Sport = Football) = 20/100 = <b>0.20</b>
P(Gender = Female, Sport = Baseball) = 23/100 = <b>0.23</b>
P(Gender = Female, Sport = Basketball) = 16/100 = <b>0.16</b>
P(Gender = Female, Sport = Football) = 13/100 = <b>0.13</b>
Notice that the sum of the probabilities is equal to <b>1</b>, or <b>100%</b>.
<h3>Why Use a Joint Probability Distribution?</h3>
Joint probability distributions are useful because we often collect data for two variables (like Sports and Gender) and we’re interested in answering questions related to <em>both</em> variables.
For example, we might want to understand how likely it is that a given individual in a  population  is male <em>and</em> prefers baseball as their favorite sport.
Or we may be interested in understanding how likely it is that a given individual is female <em>and</em> prefers football as their favorite sport.
A joint probability distribution can help us answer these questions.
Use the following examples as practice for gaining a better understanding of joint probability distributions.
<h3>Example 1</h3>
The following two-way table shows the results of a survey that asked 238 people which movie genre they liked best:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/marginal3.png">
<b>Question:</b> What is the probability that a given individual is female and prefers Drama as their favorite movie genre?
<b>Answer:</b> P(Gender = Female, Genre = Drama) = 58/238 = <b>0.244</b> = <b>24.4%</b>
<h3>Example 2</h3>
The following two-way table shows the exam scores of 64 students in a class based on how many hours they spent studying:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/marginal4.png">
<b>Question:</b> What is the probability that a given individual studies for 2 hours and receives a score between 91 and 100?
<b>Answer:</b> P(Study = 2 hours, Score = 91-100) = 3/64 = <b>0.047</b> = <b>4.7%</b>
<h2><span class="orange">How to Convert a JSON File to a Pandas DataFrame</span></h2>
Occasionally you may want to convert a JSON file into a pandas DataFrame. Fortunately this is easy to do using the pandas  read_json()  function, which uses the following syntax:
<b>read_json(‘path’, orient=’index’)</b>
where:
<b>path: </b>the path to your JSON file.
<b>orient: </b>the orientation of the JSON file. Default is ‘index’ but you can specify ‘split’, ‘records’, ‘columns’, or ‘values’ instead.
The following examples show how to use this function for a variety of different JSON strings.
<h3>Example 1: Converting a JSON File with a “Records” Format</h3>
Suppose we have a JSON file called <b>my_file.json </b>in the following format:
<b>[
   {
      "points": 25,
      "assists": 5
   },
   {
      "points": 12,
      "assists": 7
   },
   {
      "points": 15,
      "assists": 7
   },
   {
      "points": 19,
      "assists": 12
   }
]</b> 
We can load this JSON file into a pandas DataFrame by simply specifying the path to it along with orient=’<b>records</b>‘ as follows:
<b>#load JSON file into pandas DataFrame
df = pd.read_json('C:/Users/Zach/Desktop/json_file.json', orient='records')
#view DataFrame
df
        assistspoints
0525
1712
2715
31219
</b>
<h3>Example 2: Converting a JSON File with an “Index” Format</h3>
Suppose we have a JSON file called <b>my_file.json </b>in the following format:
<b>{
   "0": {
      "points": 25,
      "assists": 5
   },
   "1": {
      "points": 12,
      "assists": 7
   },
   "2": {
      "points": 15,
      "assists": 7
   },
   "3": {
      "points": 19,
      "assists": 12
   }
} </b>
We can load this JSON file into a pandas DataFrame by simply specifying the path to it along with orient=’<b>index</b>‘ as follows:
<b>#load JSON file into pandas DataFrame
df = pd.read_json('C:/Users/Zach/Desktop/json_file.json', orient='index')
#view DataFrame
df
        assistspoints
0525
1712
2715
31219
</b>
<h3>Example 3: Converting a JSON File with a “Columns” Format</h3>
Suppose we have a JSON file called <b>my_file.json </b>in the following format:
<b>{
   "points": {
      "0": 25,
      "1": 12,
      "2": 15,
      "3": 19
   },
   "assists": {
      "0": 5,
      "1": 7,
      "2": 7,
      "3": 12
   }
} </b>
We can load this JSON file into a pandas DataFrame by simply specifying the path to it along with orient=’<b>columns</b>‘ as follows:
<b>#load JSON file into pandas DataFrame
df = pd.read_json('C:/Users/Zach/Desktop/json_file.json', orient='columns')
#view DataFrame
df
        assistspoints
0525
1712
2715
31219</b>
<h3>Example 4: Converting a JSON File with a “Values” Format</h3>
Suppose we have a JSON file called <b>my_file.json </b>in the following format:
<b>[
   [
      25,
      5
   ],
   [
      12,
      7
   ],
   [
      15,
      7
   ],
   [
      19,
      12
   ]
] </b>
We can load this JSON file into a pandas DataFrame by simply specifying the path to it along with orient=’<b>values</b>‘ as follows:
<b>#load JSON file into pandas DataFrame
df = pd.read_json('C:/Users/Zach/Desktop/json_file.json', orient='values')
#view DataFrame
df
        01
0255
1127
2157
31912
31219</b>
<em>You can find the complete documentation for the read_json() function  here .</em>
<h2><span class="orange">K-Fold Cross Validation in Python (Step-by-Step)</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.
One commonly used method for doing this is known as  k-fold cross-validation , which uses the following approach:
<b>1.</b> Randomly divide a dataset into <em>k</em> groups, or “folds”, of roughly equal size.
<b>2.</b> Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds. Calculate the test MSE on the observations in the fold that was held out.
<b>3.</b> Repeat this process <em>k</em> times, using a different set each time as the holdout set.
<b>4.</b> Calculate the overall test MSE to be the average of the <em>k</em> test MSE’s.
This tutorial provides a step-by-step example of how to perform k-fold cross validation for a given model in Python.
<h3>Step 1: Load Necessary Libraries</h3>
First, we’ll load the necessary functions and libraries for this example:
<b>from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from numpy import mean
from numpy import absolute
from numpy import sqrt
import pandas as pd
</b>
<h3>Step 2: Create the Data</h3>
Next, we’ll create a pandas DataFrame that contains two predictor variables, x<sub>1</sub> and x<sub>2</sub>, and a single response variable y.
<b>df = pd.DataFrame({'y': [6, 8, 12, 14, 14, 15, 17, 22, 24, 23],   'x1': [2, 5, 4, 3, 4, 6, 7, 5, 8, 9],   'x2': [14, 12, 12, 13, 7, 8, 7, 4, 6, 5]})
</b>
<h3>Step 3: Perform K-Fold Cross-Validation</h3>
Next, we’ll then fit a  multiple linear regression model  to the dataset and perform LOOCV to evaluate the model performance.
<b>#define predictor and response variables
X = df[['x1', 'x2']]
y = df['y']
#define cross-validation method to use
<span>cv <span>= <span>KFold<span>(<span>n_splits<span>=10<span>, <span>random_state<span>=1<span>, <span>shuffle<span>=True<span>)
#build multiple linear regression model
model = LinearRegression()
#use k-fold CV to evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error',         cv=cv, n_jobs=-1)
#view mean absolute error
mean(absolute(scores))
3.6141267491803646
</b>
From the output we can see that the mean absolute error (MAE) was <b>3.614</b>. That is, the average absolute error between the model prediction and the actual observed data is 3.614.
In general, the lower the MAE, the more closely a model is able to predict the actual observations.
Another commonly used metric to evaluate model performance is the root mean squared error (RMSE). The following code shows how to calculate this metric using LOOCV:
<b>#define predictor and response variables
X = df[['x1', 'x2']]
y = df['y']
#define cross-validation method to use
<span>cv <span>= <span>KFold<span>(<span>n_splits<span>=5<span>, <span>random_state<span>=1<span>, <span>shuffle<span>=True<span>) 
#build multiple linear regression model
model = LinearRegression()
#use LOOCV to evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error',         cv=cv, n_jobs=-1)
#view RMSE
sqrt(mean(absolute(scores)))
4.284373111711816</b>
From the output we can see that the root mean squared error (RMSE) was <b>4.284</b>.
The lower the RMSE, the more closely a model is able to predict the actual observations.
In practice we typically fit several different models and compare the RMSE or MAE of each model to decide which model produces the lowest test error rates and is therefore the best model to use.
Also note that in this example we chose to use k=5 folds, but you can choose however many folds you’d like.
In practice, we typically choose between 5 and 10 folds because this turns out to be the optimal number of folds that produce reliable test error rates.
<em>You can find the complete documentation for the KFold() function from sklearn  here .</em>
<h2><span class="orange">K-Fold Cross Validation in R (Step-by-Step)</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.
One commonly used method for doing this is known as  k-fold cross-validation , which uses the following approach:
<b>1.</b> Randomly divide a dataset into <em>k</em> groups, or “folds”, of roughly equal size.
<b>2.</b> Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds. Calculate the test MSE on the observations in the fold that was held out.
<b>3.</b> Repeat this process <em>k</em> times, using a different set each time as the holdout set.
<b>4.</b> Calculate the overall test MSE to be the average of the <em>k</em> test MSE’s.
The easiest way to perform k-fold cross-validation in R is by using the  trainControl()  function from the <b>caret</b> library in R.
This tutorial provides a quick example of how to use this function to perform k-fold cross-validation for a given model in R.
<h3>Example: K-Fold Cross-Validation in R</h3>
Suppose we have the following dataset in R:
<b>#create data frame
df &lt;- data.frame(y=c(6, 8, 12, 14, 14, 15, 17, 22, 24, 23), x1=c(2, 5, 4, 3, 4, 6, 7, 5, 8, 9), x2=c(14, 12, 12, 13, 7, 8, 7, 4, 6, 5))
#view data frame
df
yx1x2
6214
8512
12412
14313
1447
1568
1777
2254
2486
2395
</b>
The following code shows how to fit a  multiple linear regression model  to this dataset in R and perform k-fold cross validation with k = 5 folds to evaluate the model performance:
<b>library(caret)
#specify the cross-validation method
ctrl &lt;- trainControl(method = "cv", number = 5)
#fit a regression model and use k-fold CV to evaluate performance
model &lt;- train(y ~ x1 + x2, data = df, method = "lm", trControl = ctrl)
#view summary of k-fold CV               
print(model)
Linear Regression 
10 samples
 2 predictor
No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 8, 8, 8, 8, 8 
Resampling results:
  RMSE      Rsquared  MAE     
  3.018979  1         2.882348
Tuning parameter 'intercept' was held constant at a value of TRUE
</b>
Here is how to interpret the output:
No pre-processing occured. That is, we didn’t  scale the data  in any way before fitting the models.
The resampling method we used to evaluate the model was cross-validation with 5 folds.
The sample size for each training set was 8.
<b>RMSE:</b> The root mean squared error. This measures the average difference between the predictions made by the model and the actual observations. The lower the RMSE, the more closely a model can predict the actual observations.
<b>Rsquared:</b> This is a measure of the correlation between the predictions made by the model and the actual observations. The higher the R-squared, the more closely a model can predict the actual observations.
<b>MAE:</b> The mean absolute error. This is the average absolute difference between the predictions made by the model and the actual observations. The lower the MAE, the more closely a model can predict the actual observations.
Each of the three metrics provided in the output (RMSE, R-squared, and MAE) give us an idea of how well the model performed on previously unseen data.
In practice we typically fit several different models and compare the three metrics provided by the output seen here to decide which model produces the lowest test error rates and is therefore the best model to use.
We can use the following code to examine the final model fit:
<b>#view final model
model$finalModel
Call:
lm(formula = .outcome ~ ., data = dat)
Coefficients:
(Intercept)           x1           x2  
    21.2672       0.7803      -1.1253  
</b>
The final model turns out to be:
<b>y = 21.2672 + 0.7803*(x<sub>1</sub>) – 1.12538(x<sub>2</sub>)</b>
We can use the following code to view the model predictions made for each fold:
<b>#view predictions for each fold
model$resample
      RMSE Rsquared      MAE Resample
1 4.808773        1 3.544494    Fold1
2 3.464675        1 3.366812    Fold2
3 6.281255        1 6.280702    Fold3
4 3.759222        1 3.573883    Fold4
5 1.741127        1 1.679767    Fold5  </b>
Note that in this example we chose to use k=5 folds, but you can choose however many folds you’d like. In practice, we typically choose between 5 and 10 folds because this turns out to be the optimal number of folds that produce reliable test error rates.
<h2><span class="orange">An Easy Guide to K-Fold Cross-Validation</span></h2>
To evaluate the performance of some model on a dataset, we need to measure how well the predictions made by the model match the observed data.
The most common way to measure this is by using the mean squared error (MSE), which is calculated as:
MSE = (1/n)*Σ(y<sub>i</sub> – f(x<sub>i</sub>))<sup>2</sup>
where:
<b>n:</b> Total number of observations
<b>y<sub>i</sub>:</b> The response value of the i<sup>th</sup> observation
<b>f(x<sub>i</sub>):</b> The predicted response value of the i<sup>th</sup> observation
The closer the model predictions are to the observations, the smaller the MSE will be.
In practice, we use the following process to calculate the MSE of a given model:
<b>1.</b> Split a dataset into a training set and a testing set.
<b>2.</b> Build the model using only data from the training set.
<b>3.</b> Use the model to make predictions on the testing set and measure the test MSE.
The test MSE gives us an idea of how well a model will perform on data it hasn’t previously seen. However, the drawback of using only one testing set is that the test MSE can vary greatly depending on which observations were used in the training and testing sets.
One way to avoid this problem is to fit a model several times using a different training and testing set each time, then calculating the test MSE to be the average of all of the test MSE’s.
This general method is known as cross-validation and a specific form of it is known as <b>k-fold cross-validation</b>.
<h3>K-Fold Cross-Validation</h3>
<b>K-fold cross-validation</b> uses the following approach to evaluate a model:
<b>Step 1: Randomly divide a dataset into <em>k</em> groups, or “folds”, of roughly equal size.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold1.png">
<b>Step 2: Choose one of the folds to be the holdout set. Fit the model on the remaining k-1 folds. Calculate the test MSE on the observations in the fold that was held out.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold2.png">
<b>Step 3: Repeat this process <em>k</em> times, using a different set each time as the holdout set.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/kfold3.png">
<b>Step 4: Calculate the overall test MSE to be the average of the <em>k</em> test MSE’s.</b>
<b>Test MSE = (1/k)*ΣMSE<sub>i</sub></b>
where:
<b>k: </b>Number of folds
<b>MSE<sub>i</sub></b>: Test MSE on the i<sup>th</sup> iteration
<h3>How to Choose K</h3>
In general, the more folds we use in k-fold cross-validation the lower the bias of the test MSE but the higher the variance. Conversely, the fewer folds we use the higher the bias but the lower the variance. This is a classic example of  the bias-variance tradeoff  in machine learning.
In practice, we typically choose to use between 5 and 10 folds. As noted in  <em>An Introduction to Statistical Learning</em>,  this number of folds has been shown to offer an optimal balance between bias and variance and thus provide reliable estimates of test MSE:
To summarize, there is a bias-variance trade-off associated with the choice of k in k-fold cross-validation.
 
Typically, given these considerations, one performs k-fold cross-validation using k = 5 or k = 10, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.
 
-Page 184, <em>An Introduction to Statistical Learning</em>
<h3>Advantages of K-Fold Cross-Validation</h3>
When we split a dataset into just one training set and one testing set, the test MSE calculated on the observations in the testing set can vary greatly depending on which observations were used in the training and testing sets.
By using k-fold cross-validation, we’re able to use calculate the test MSE using several different variations of training and testing sets. This makes it much more likely for us to obtain an unbiased estimate of the test MSE.
K-fold cross-validation also offers a computational advantage over  leave-one-out cross-validation (LOOCV)  because it only has to fit a model <em>k</em> times as opposed to <em>n</em> times.
For models that take a long time to fit, k-fold cross-validation can compute the test MSE much quicker than LOOCV and in many cases the test MSE calculated by each approach will be quite similar if you use a sufficient number of folds.
<h3>Extensions of K-Fold Cross-Validation</h3>
There are several extensions of k-fold cross-validation, including:
<b>Repeated K-fold Cross-Validation:</b> This is where k-fold cross-validation is simply repeated <em>n</em> times. Each time the training and testing sets are shuffled, so this further reduces the bias in the estimate of test MSE although this takes longer to perform than ordinary k-fold cross-validation.
<b>Leave-One-Out Cross-Validation:</b> This is a special case of k-fold cross-validation in which <em>k</em>=<em>n</em>. You can read more about this method  here .
<b>Stratified K-Fold Cross-Validation:</b> This is a version of k-fold cross-validation in which the dataset is rearranged in such a way that each fold is representative of the whole. As noted by  Kohavi , this method tends to offer a better tradeoff between bias and variance compared to ordinary k-fold cross-validation.
<b>Nested Cross-Validation:</b> This is where k-fold cross validation is performed within each fold of cross-validation. This is often used to perform hyperparameter tuning during model evaluation.
<h2><span class="orange">K-Means Clustering in Python: Step-by-Step Example</span></h2>
One of the most common clustering algorithms in  machine learning  is known as <b>k-means clustering</b>.
K-means clustering is a technique in which we place each observation in a dataset into one of <em>K</em> clusters.
The end goal is to have <em>K </em>clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.
In practice, we use the following steps to perform K-means clustering:
<b>1. Choose a value for <em>K</em>.</b>
First, we must decide how many clusters we’d like to identify in the data. Often we have to simply test several different values for <em>K</em> and analyze the results to see which number of clusters seems to make the most sense for a given problem.
<b>2. Randomly assign each observation to an initial cluster, from 1 to <em>K.</em></b>
<b>3. Perform the following procedure until the cluster assignments stop changing.</b>
For each of the <em>K </em>clusters, compute the cluster <em>centroid.</em> This is simply the vector of the <em>p</em> feature means for the observations in the <em>k</em>th cluster.
Assign each observation to the cluster whose centroid is closest. Here, <em>closest</em> is defined using  Euclidean distance .
The following step-by-step example shows how to perform k-means clustering in Python by using the <b>KMeans</b> function from the <b>sklearn</b> module.
<h2>Step 1: Import Necessary Modules</h2>
First, we’ll import all of the modules that we will need to perform k-means clustering:
<b>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler</b>
<h2>Step 2: Create the DataFrame</h2>
Next, we’ll create a DataFrame that contains the following three variables for 20 different basketball players:
points
assists
rebounds
The following code shows how to create this pandas DataFrame:
<b>#create DataFrame
df = pd.DataFrame({'points': [18, np.nan, 19, 14, 14, 11, 20, 28, 30, 31,              35, 33, 29, 25, 25, 27, 29, 30, 19, 23],   'assists': [3, 3, 4, 5, 4, 7, 8, 7, 6, 9, 12, 14,               np.nan, 9, 4, 3, 4, 12, 15, 11],   'rebounds': [15, 14, 14, 10, 8, 14, 13, 9, 5, 4,                11, 6, 5, 5, 3, 8, 12, 7, 6, 5]})
#view first five rows of DataFrame
print(df.head())
   points  assists  rebounds
0    18.0      3.0        15
1     NaN      3.0        14
2    19.0      4.0        14
3    14.0      5.0        10
4    14.0      4.0         8
</b>
We will use k-means clustering to group together players that are similar based on these three metrics.
<h2>Step 3: Clean & Prep the DataFrame</h2>
Next, we’ll perform the following steps:
Use<b> dropna()</b> to drop rows with NaN values in any column
Use<b> StandardScaler()</b> to scale each variable to have a mean of 0 and a standard deviation of 1
The following code shows how to do so:
<b>#drop rows with NA values in any columns
df = df.dropna()
#create scaled DataFrame where each variable has mean of 0 and standard dev of 1
scaled_df = StandardScaler().fit_transform(df)
#view first five rows of scaled DataFrame
print(scaled_df[:5])
[[-0.86660275 -1.22683918  1.72722524]
 [-0.72081911 -0.96077767  1.45687694]
 [-1.44973731 -0.69471616  0.37548375]
 [-1.44973731 -0.96077767 -0.16521285]
 [-1.88708823 -0.16259314  1.45687694]]</b>
<b>Note</b>: We use scaling so that each variable has equal importance when fitting the k-means algorithm. Otherwise, the variables with the widest ranges would have too much influence.
<h2>Step 4: Find the Optimal Number of Clusters</h2>
To perform k-means clustering in Python, we can use the <b>KMeans</b> function from the <b>sklearn</b> module.
This function uses the following basic syntax:
<b>KMeans(init=’random’, n_clusters=8, n_init=10, random_state=None)</b>
where:
<b>init</b>: Controls the initialization technique.
<b>n_clusters</b>: The number of clusters to place observations in.
<b>n_init</b>: The number of initializations to perform. The default is to run the k-means algorithm 10 times and return the one with the lowest SSE.
<b>random_state</b>: An integer value you can pick to make the results of the algorithm reproducible. 
The most important argument in this function is n_clusters, which specifies how many clusters to place the observations in.
However, we don’t know beforehand how many clusters is optimal so we must create a plot that displays the number of clusters along with the SSE (sum of squared errors) of the model.
Typically when we create this type of plot we look for an “elbow” where the sum of squares begins to “bend” or level off. This is typically the optimal number of clusters.
The following code shows how to create this type of plot that displays the number of clusters on the x-axis and the SSE on the y-axis:
<b>#initialize kmeans parameters
kmeans_kwargs = {
"init": "random",
"n_init": 10,
"random_state": 1,
}
#create list to hold SSE values for each k
sse = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)
    kmeans.fit(scaled_df)
    sse.append(kmeans.inertia_)
#visualize results
plt.plot(range(1, 11), sse)
plt.xticks(range(1, 11))
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/kmean1.jpg"531">
In this plot it appears that there is an elbow or “bend” at k = <b>3 clusters</b>.
Thus, we will use 3 clusters when fitting our k-means clustering model in the next step.
<b>Note</b>: In the real-world, it’s recommended to use a combination of this plot along with domain expertise to pick how many clusters to use.
<h2>Step 5: Perform K-Means Clustering with Optimal <em>K</em></h2>
The following code shows how to perform k-means clustering on the dataset using the optimal value for <em>k</em> of 3:
<b>#instantiate the k-means class, using optimal number of clusters
kmeans = KMeans(init="random", n_clusters=3, n_init=10, random_state=1)
#fit k-means algorithm to data
kmeans.fit(scaled_df)
#view cluster assignments for each observation
kmeans.labels_
array([1, 1, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0]) 
</b>
The resulting array shows the cluster assignments for each observation in the DataFrame.
To make these results easier to interpret, we can add a column to the DataFrame that shows the cluster assignment of each player:
<b>#append cluster assingments to original DataFrame
df['cluster'] = kmeans.labels_
#view updated DataFrame
print(df)
    points  assists  rebounds  cluster
0     18.0      3.0        15        1
2     19.0      4.0        14        1
3     14.0      5.0        10        1
4     14.0      4.0         8        1
5     11.0      7.0        14        1
6     20.0      8.0        13        1
7     28.0      7.0         9        2
8     30.0      6.0         5        2
9     31.0      9.0         4        0
10    35.0     12.0        11        0
11    33.0     14.0         6        0
13    25.0      9.0         5        0
14    25.0      4.0         3        2
15    27.0      3.0         8        2
16    29.0      4.0        12        2
17    30.0     12.0         7        0
18    19.0     15.0         6        0
19    23.0     11.0         5        0
</b>
The <b>cluster</b> column contains a cluster number (0, 1, or 2) that each player was assigned to.
Players that belong to the same cluster have roughly similar values for the <b>points</b>, <b>assists</b>, and <b>rebounds</b> columns.
<b>Note</b>: You can find the complete documentation for the <b>KMeans</b> function from <b>sklearn</b>  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Perform Linear Regression in Python 
 How to Perform Logistic Regression in Python 
 How to Perform K-Fold Cross Validation in Python 
<h2><span class="orange">K-Means Clustering in R: Step-by-Step Example</span></h2>
Clustering is a technique in machine learning that attempts to find <em>clusters</em> of  observations  within a dataset.
The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.
Clustering is a form of  unsupervised learning  because we’re simply attempting to find structure within a dataset rather than predicting the value of some  response variable .
Clustering is often used in marketing when companies have access to information like:
Household income
Household size
Head of household Occupation
Distance from nearest urban area
When this information is available, clustering can be used to identify households that are similar and may be more likely to purchase certain products or respond better to a certain type of advertising.
One of the most common forms of clustering is known as <b>k-means clustering</b>.
<h3>What is K-Means Clustering?</h3>
K-means clustering is a technique in which we place each observation in a dataset into one of <em>K</em> clusters.
The end goal is to have <em>K </em>clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.
In practice, we use the following steps to perform K-means clustering:
<b>1. Choose a value for <em>K</em>.</b>
First, we must decide how many clusters we’d like to identify in the data. Often we have to simply test several different values for <em>K</em> and analyze the results to see which number of clusters seems to make the most sense for a given problem.
<b>2. Randomly assign each observation to an initial cluster, from 1 to <em>K.</em></b>
<b>3. Perform the following procedure until the cluster assignments stop changing.</b>
For each of the <em>K </em>clusters, compute the cluster <em>centroid.</em> This is simply the vector of the <em>p</em> feature means for the observations in the <em>k</em>th cluster.
Assign each observation to the cluster whose centroid is closest. Here, <em>closest</em> is defined using  Euclidean distance .
<h3>K-Means Clustering in R</h3>
The following tutorial provides a step-by-step example of how to perform k-means clustering in R.
<h3>Step 1: Load the Necessary Packages</h3>
First, we’ll load two packages that contain several useful functions for k-means clustering in R.
<b>library(factoextra)
library(cluster)</b>
<h3>Step 2: Load and Prep the Data</h3>
For this example we’ll use the <em>USArrests</em> dataset built into R, which contains the number of arrests per 100,000 residents in each U.S. state in 1973 for <em>Murder</em>, <em>Assault</em>, and <em>Rape </em>along with the percentage of the population in each state living in urban areas, <em>UrbanPop</em>.
The following code shows how to do the following:
Load the <em>USArrests</em> dataset
Remove any rows with missing values
Scale each variable in the dataset to have a mean of 0 and a standard deviation of 1
<b>#load data
df &lt;- USArrests
#remove rows with missing values</b>
<b>df &lt;- na.omit(df)
#scale each variable to have a mean of 0 and sd of 1</b>
<b>df &lt;- scale(df)
#view first six rows of dataset
head(df)
               Murder   Assault   UrbanPop         Rape
Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
Arizona    0.07163341 1.4788032  0.9989801  1.042878388
Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
California 0.27826823 1.2628144  1.7589234  2.067820292
Colorado   0.02571456 0.3988593  0.8608085  1.864967207
</b>
<h3>Step 3: Find the Optimal Number of Clusters</h3>
To perform k-means clustering in R we can use the built-in <b>kmeans()</b> function, which uses the following syntax:
<b>kmeans(data, centers, nstart)</b>
where:
<b>data:</b> Name of the dataset.
<b>centers:</b> The number of clusters, denoted <em>k</em>.
<b>nstart:</b> The number of initial configurations. Because it’s possible that different initial starting clusters can lead to different results, it’s recommended to use several different initial configurations. The k-means algorithm will find the initial configurations that lead to the smallest within-cluster variation.
Since we don’t know beforehand how many clusters is optimal, we’ll create two different plots that can help us decide:
<b>1. Number of Clusters vs. the Total Within Sum of Squares</b>
First, we’ll use the <b>fviz_nbclust()</b> function to create a plot of the number of clusters vs. the total within sum of squares:
<b>fviz_nbclust(df, kmeans, method = "wss")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean1.png">
Typically when we create this type of plot we look for an “elbow” where the sum of squares begins to “bend” or level off. This is typically the optimal number of clusters.
For this plot it appears that there is a bit of an elbow or “bend” at k = 4 clusters.
<b>2. Number of Clusters vs. Gap Statistic</b>
Another way to determine the optimal number of clusters is to use a metric known as the  gap statistic , which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.
We can calculate the gap statistic for each number of clusters using the <b>clusGap()</b> function from the <em>cluster</em> package along with a plot of clusters vs. gap statistic using the <b>fviz_gap_stat()</b> function:
<b>#calculate gap statistic based on number of clusters
gap_stat &lt;- clusGap(df,    FUN = kmeans,    nstart = 25,    K.max = 10,    B = 50)
#plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean2.png">
From the plot we can see that gap statistic is highest at k = 4 clusters, which matches the elbow method we used earlier.
<h3>Step 4: Perform K-Means Clustering with Optimal <em>K</em></h3>
Lastly, we can perform k-means clustering on the dataset using the optimal value for <em>k</em> of 4:
<b>#make this example reproducible
set.seed(1)
#perform k-means clustering with k = 4 clusters
km &lt;- kmeans(df, centers = 4, nstart = 25)
#view results
km
K-means clustering with 4 clusters of sizes 16, 13, 13, 8
Cluster means:
      Murder    Assault   UrbanPop        Rape
1 -0.4894375 -0.3826001  0.5758298 -0.26165379
2 -0.9615407 -1.1066010 -0.9301069 -0.96676331
3  0.6950701  1.0394414  0.7226370  1.27693964
4  1.4118898  0.8743346 -0.8145211  0.01927104
Clustering vector:
       Alabama         Alaska        Arizona       Arkansas     California       Colorado 
             4              3              3              4              3              3 
   Connecticut       Delaware        Florida        Georgia         Hawaii          Idaho 
             1              1              3              4              1              2 
      Illinois        Indiana           Iowa         Kansas       Kentucky      Louisiana 
             3              1              2              1              2              4 
         Maine       Maryland  Massachusetts       Michigan      Minnesota    Mississippi 
             2              3              1              3              2              4 
      Missouri        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
             3              2              2              3              2              1 
    New Mexico       New York North Carolina   North Dakota           Ohio       Oklahoma 
             3              3              4              2              1              1 
        Oregon   Pennsylvania   Rhode Island South Carolina   South Dakota      Tennessee 
             1              1              1              4              2              4 
         Texas           Utah        Vermont       Virginia     Washington  West Virginia 
             3              1              2              1              1              2 
     Wisconsin        Wyoming 
             2              1 
Within cluster sum of squares by cluster:
[1] 16.212213 11.952463 19.922437  8.316061
 (between_SS / total_SS =  71.2 %)
Available components:
[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss" "betweenss"   
[7] "size"         "iter"         "ifault"         
</b>
From the results we can see that:
<b>16 </b>states were assigned to the first cluster
<b>13</b> states were assigned to the second cluster
<b>13</b> states were assigned to the third cluster
<b>8 </b>states were assigned to the fourth cluster
We can visualize the clusters on a scatterplot that displays the first two principal components on the axes using the <b>fivz_cluster()</b> function:
<b>#plot results of final k-means model
fviz_cluster(km, data = df)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmean4.png">
We can also use the <b>aggregate()</b> function to find the mean of the variables in each cluster:
<b>#find means of each cluster
aggregate(USArrests, by=list(cluster=km$cluster), mean)
cluster  Murder   AssaultUrbanPop    Rape
13.60000  78.5384652.0769212.17692
210.81538 257.3846276.0000033.19231
35.65625 138.8750073.8750018.78125
413.93750 243.6250053.7500021.41250
</b>
We interpret this output is as follows:
The mean number of murders per 100,000 citizens among the states in cluster 1 is <b>3.6</b>.
The mean number of assaults per 100,000 citizens among the states in cluster 1 is <b>78.5</b>.
The mean percentage of residents living in an urban area among the states in cluster 1 is <b>52.1%</b>.
The mean number of rapes per 100,000 citizens among the states in cluster 1 is <b>12.2.</b>
And so on.
We can also append the cluster assignments of each state back to the original dataset:
<b>#add cluster assigment to original data
final_data &lt;- cbind(USArrests, cluster = km$cluster)
#view final data
head(final_data)
    MurderAssaultUrbanPop  Rape cluster
Alabama    13.223658  21.2 4
Alaska    10.026348  44.5 2
Arizona     8.129480  31.0 2
Arkansas     8.819050  19.5 4
California   9.027691  40.6 2
Colorado     7.920478  38.7 2
</b>
<h3>Pros & Cons of K-Means Clustering</h3>
K-means clustering offers the following benefits:
It is a fast algorithm.
It can handle large datasets well.
However, it comes with the following potential drawbacks:
It requires us to specify the number of clusters before performing the algorithm.
It’s sensitive to outliers.
Two alternatives to k-means clustering are  k-medoids clustering  and  hierarchical clustering .
You can find the complete R code used in this example  here .
<h2><span class="orange">K-Medoids in R: Step-by-Step Example</span></h2>
Clustering is a technique in machine learning that attempts to find groups or <em>clusters</em> of  observations  within a dataset.
The goal is to find clusters such that the observations within each cluster are quite similar to each other, while observations in different clusters are quite different from each other.
Clustering is a form of  unsupervised learning  because we’re simply attempting to find structure within a dataset rather than predicting the value of some  response variable .
Clustering is often used in marketing when companies have access to information like:
Household income
Household size
Head of household Occupation
Distance from nearest urban area
When this information is available, clustering can be used to identify households that are similar and may be more likely to purchase certain products or respond better to a certain type of advertising.
One of the most common forms of clustering is known as  k-means clustering .
Unfortunately, this method can be influenced by outliers so an alternative that is often used is <b>k-medoids clustering</b>.
<h3>What is K-Medoids Clustering?</h3>
K-medoids clustering is a technique in which we place each observation in a dataset into one of <em>K</em> clusters.
The end goal is to have <em>K </em>clusters in which the observations within each cluster are quite similar to each other while the observations in different clusters are quite different from each other.
In practice, we use the following steps to perform K-means clustering:
<b>1. Choose a value for <em>K</em>.</b>
First, we must decide how many clusters we’d like to identify in the data. Often we have to simply test several different values for <em>K</em> and analyze the results to see which number of clusters seems to make the most sense for a given problem.
<b>2. Randomly assign each observation to an initial cluster, from 1 to <em>K.</em></b>
<b>3. Perform the following procedure until the cluster assignments stop changing.</b>
For each of the <em>K </em>clusters, compute the cluster <em>centroid.</em> This is the vector of the <em>p</em> feature <b>medians</b> for the observations in the <em>k</em>th cluster.
Assign each observation to the cluster whose centroid is closest. Here, <em>closest</em> is defined using  Euclidean distance .
<b>Technical Note:</b>
 
Because k-medoids computes cluster centroids using medians instead of means, it tends to be more robust to outliers compared to k-means.
 
In practice, if there are no extreme outliers in the dataset then k-means and k-medoids will produce similar results.
<h3>K-Medoids Clustering in R</h3>
The following tutorial provides a step-by-step example of how to perform k-medoids clustering in R.
<h3>Step 1: Load the Necessary Packages</h3>
First, we’ll load two packages that contain several useful functions for k-medoids clustering in R.
<b>library(factoextra)
library(cluster)</b>
<h3>Step 2: Load and Prep the Data</h3>
For this example we’ll use the <em>USArrests</em> dataset built into R, which contains the number of arrests per 100,000 residents in each U.S. state in 1973 for <em>Murder</em>, <em>Assault</em>, and <em>Rape </em>along with the percentage of the population in each state living in urban areas, <em>UrbanPop</em>.
The following code shows how to do the following:
Load the <em>USArrests</em> dataset
Remove any rows with missing values
Scale each variable in the dataset to have a mean of 0 and a standard deviation of 1
<b>#load data
df &lt;- USArrests
#remove rows with missing values</b>
<b>df &lt;- na.omit(df)
#scale each variable to have a mean of 0 and sd of 1</b>
<b>df &lt;- scale(df)
#view first six rows of dataset
head(df)
               Murder   Assault   UrbanPop         Rape
Alabama    1.24256408 0.7828393 -0.5209066 -0.003416473
Alaska     0.50786248 1.1068225 -1.2117642  2.484202941
Arizona    0.07163341 1.4788032  0.9989801  1.042878388
Arkansas   0.23234938 0.2308680 -1.0735927 -0.184916602
California 0.27826823 1.2628144  1.7589234  2.067820292
Colorado   0.02571456 0.3988593  0.8608085  1.864967207
</b>
<h3>Step 3: Find the Optimal Number of Clusters</h3>
To perform k-medoids clustering in R we can use the <b>pam()</b> function, which stands for “partitioning around medians” and uses the following syntax:
<b>pam(data, k, metric = “euclidean”, stand = FALSE)</b>
where:
<b>data:</b> Name of the dataset.
<b>k:</b> The number of clusters.
<b>metric:</b> The metric to use to calculate distance. Default is <em>euclidean</em> but you could also specify <em>manhattan</em>.
<b>stand: </b>Whether or not to standardize each variable in the dataset. Default is FALSE.
Since we don’t know beforehand how many clusters is optimal, we’ll create two different plots that can help us decide:
<b>1. Number of Clusters vs. the Total Within Sum of Squares</b>
First, we’ll use the <b>fviz_nbclust()</b> function to create a plot of the number of clusters vs. the total within sum of squares:
<b>fviz_nbclust(df, pam, method = "wss")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmedoid1.png">
The total within sum of squares will typically always increase as we increase the number of clusters, so when we create this type of plot we look for an “elbow” where the sum of squares begins to “bend” or level off.
The point where the plot bends is typically the optimal number of clusters. Beyond this number,  overfitting  is likely to occur.
For this plot it appear that there is a bit of an elbow or “bend” at k = 4 clusters.
<b>2. Number of Clusters vs. Gap Statistic</b>
Another way to determine the optimal number of clusters is to use a metric known as the  gap statistic , which compares the total intra-cluster variation for different values of k with their expected values for a distribution with no clustering.
We can calculate the gap statistic for each number of clusters using the <b>clusGap()</b> function from the <em>cluster</em> package along with a plot of clusters vs. gap statistic using the <b>fviz_gap_stat()</b> function:
<b>#calculate gap statistic based on number of clusters
gap_stat &lt;- clusGap(df,    FUN = pam,    K.max = 10, #max clusters to consider    B = 50) #total bootstrapped iterations
#plot number of clusters vs. gap statistic
fviz_gap_stat(gap_stat)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmedoid2.png">
From the plot we can see that gap statistic is highest at k = 4 clusters, which matches the elbow method we used earlier.
<h3>Step 4: Perform K-Medoids Clustering with Optimal <em>K</em></h3>
Lastly, we can perform k-medoids clustering on the dataset using the optimal value for <em>k</em> of 4:
<b>#make this example reproducible
set.seed(1)
#perform k-medoids clustering with k = 4 clusters
kmed &lt;- pam(df, k = 4)
#view results
kmed
              ID     Murder    Assault   UrbanPop         Rape
Alabama        1  1.2425641  0.7828393 -0.5209066 -0.003416473
Michigan      22  0.9900104  1.0108275  0.5844655  1.480613993
Oklahoma      36 -0.2727580 -0.2371077  0.1699510 -0.131534211
New Hampshire 29 -1.3059321 -1.3650491 -0.6590781 -1.252564419
Clustering vector:
       Alabama         Alaska        Arizona       Arkansas     California 
             1              2              2              1              2 
      Colorado    Connecticut       Delaware        Florida        Georgia 
             2              3              3              2              1 
        Hawaii          Idaho       Illinois        Indiana           Iowa 
             3              4              2              3              4 
        Kansas       Kentucky      Louisiana          Maine       Maryland 
             3              3              1              4              2 
 Massachusetts       Michigan      Minnesota    Mississippi       Missouri 
             3              2              4              1              3 
       Montana       Nebraska         Nevada  New Hampshire     New Jersey 
             3              3              2              4              3 
    New Mexico       New York North Carolina   North Dakota           Ohio 
             2              2              1              4              3 
      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina 
             3              3              3              3              1 
  South Dakota      Tennessee          Texas           Utah        Vermont 
             4              1              2              3              4 
      Virginia     Washington  West Virginia      Wisconsin        Wyoming 
             3              3              4              4              3 
Objective function:
   build     swap 
1.035116 1.027102 
Available components:
 [1] "medoids"    "id.med"     "clustering" "objective"  "isolation" 
 [6] "clusinfo"   "silinfo"    "diss"       "call"       "data"          
</b>
Note that the four cluster centroids are actual observations in the dataset. Near the top of the output we can see that the four centroids are the following states:
Alabama
Michigan
Oklahoma
New Hampshire
We can visualize the clusters on a scatterplot that displays the first two principal components on the axes using the <b>fivz_cluster()</b> function:
<b>#plot results of final k-medoids model
fviz_cluster(kmed, data = df)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/kmedoid3.png">
We can also append the cluster assignments of each state back to the original dataset:
<b>#add cluster assignment to original data
final_data &lt;- cbind(USArrests, cluster = kmed$cluster)
#view final data
head(final_data)
           Murder Assault UrbanPop Rape cluster
Alabama      13.2     236       58 21.2       1
Alaska       10.0     263       48 44.5       2
Arizona       8.1     294       80 31.0       2
Arkansas      8.8     190       50 19.5       1
California    9.0     276       91 40.6       2
Colorado      7.9     204       78 38.7       2
</b>
You can find the complete R code used in this example  here .
<h2><span class="orange">KDA Calculator</span></h2>
<b>KDA ratio</b> is a measure of the total number of kills and assists relative to the number of deaths that a player accrues in a video game. The formula to find KDA ratio is:
<b>KDA</b> = (kills + assists) / deaths
To find the KDA ratio, simply fill in the values below and then click the “Calculate” button.
<label for="kill"><b>Total Kills</b></label>
<input type="number" id="kill" value="17"><label for="assist"><b>Total Assists</b></label>
<input type="number" id="assist" value="4"><label for="death"><b>Total Deaths</b></label>
<input type="number" id="death" value="8">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
KDA Ratio = <b>2.625</b>
<script>
function calc() {
//get input values and calculate WHIP
var kill = document.getElementById('kill').value*1;
var assist = document.getElementById('assist').value*1;
var death = document.getElementById('death').value*1;
var KDA = (kill-(-assist)) / death;
//output ros
document.getElementById('KDA').innerHTML = KDA.toFixed(3);
}
</script>
<h2><span class="orange">Kendall’s Tau: Definition + Example</span></h2>
In statistics, <b>correlation </b>refers to the strength and direction of a relationship between two variables. The value of a correlation coefficient can range from -1 to 1, with -1 indicating a perfect negative relationship, 0 indicating no relationship, and 1 indicating a perfect positive relationship.
The most commonly used correlation coefficient is the  Pearson Correlation Coefficient , which measures the linear association between two numerical variables.
One less commonly used correlation coefficient is <b>Kendall’s Tau</b>, which measures the relationship between two columns of ranked data.
The formula to calculate Kendall’s Tau, often abbreviated τ, is as follows:
τ = (C-D) / (C+D)
where:
C = the number of concordant pairs
D = the number of discordant pairs
The following example illustrates how to use this formula to calculate Kendall’s Tau rank correlation coefficient for two columns of ranked data.
<h2>Example of Calculating Kendall’s Tau</h2>
Suppose two basketball coaches rank 12 of their players from worst to best. The following table shows the rankings that each coach assigned to the players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall1.png">
Because we are working with two columns of ranked data, it’s appropriate to use Kendall’s Tau to calculate the correlation between the two coaches’ rankings. Use the following steps to calculate Kendall’s Tau:
<b>Step 1: Count the number of concordant pairs.</b>
Look only at the ranks for Coach #2. Starting with the first player, count how many ranks below him are <em>larger</em>. For example, there are 11 numbers below “1” that are larger, so we’ll write 11:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall2.png">
Move to the next player and repeat the process. There are 10 numbers below “2” that are larger, so we’ll write 10:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall3.png">
Once we reach a player whose rank is <em>less </em>than the player before him, we simply assign it the same value as the player before him. For example, Elliot has a rank of “4” which is less than the previous player’s rank of “5” so we simply assign him the same value as the player before him:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall4.png">
Repeat this process for all of the players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall5.png">
<b>Step 2: Count the number of discordant pairs.</b>
Again, look only at the ranks for Coach #2. For each player, count how many ranks below him are <em>smaller</em>. For example, Coach #2 assigned AJ a rank of “1” and there are no players below him with a smaller rank. Thus, we assign him a value of 0:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall6.png">
Repeat this process for each player:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall7.png">
<b>Step 3: Calculate the sum of each column and find Kendall’s Tau.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/kendall8.png">
Kendall’s Tau = (C-D) / (C+D) = (63-3) / (63+3) = (60/66) = <b>0.909</b>.
<h2>Statistical Significance of Kendall’s Tau</h2>
When you have more than n= 10 pairs, Kendall’s Tau generally follows a normal distribution. You can use the following formula to calculate a z-score for Kendall’s Tau:
z = 3τ*√n(n-1) / √2(2n+5)
where:
τ = value you calculated for Kendall’s Tau
n = number of pairs
Here’s how to calculate <em>z </em>for the previous example:
z = 3(.909)*√12(12-1) / √2(2*12+5) = <b>4.11</b>.
Using the  Z Score to P Value Calculator , we see that the p-value for this z-score is <b>0.00004</b>, which is statistically significant at alpha level 0.05. Thus, there is a statistically significant correlation between the ranks that the two coaches assigned to the players.
<h2>Bonus: How to Calculate Kendall’s Tau in R</h2>
In the statistical software R, you can use the <b>kendall.tau() </b>function from the VGAM library to calculate Kendall’s Tau for two vectors, which uses the following syntax:
<b>kendall.tau(x, y)</b>
where <em>x </em>and <em>y </em>are two numerical vectors of equal lenghth.
The following code illustrates how to calculate Kendall’s Tau for the exact data that we used in the previous example:
<b>#load <em>VGAM
</em>library(VGAM)
#create vector for each coach's rankings
coach_1 &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)
coach_2 &lt;- c(1, 2, 3, 5, 4, 7, 6, 8, 10, 9, 11, 12)
#calculate Kendall's Tau
kendall.tau(coach_1, coach_2)
#[1] 0.9090909</b>
Notice how the value for Kendall’s Tau matches the value that we calculated by hand.
<h2><span class="orange">How to Create Kernel Density Plots in R (With Examples)</span></h2>
A <b>kernel density plot</b> is a type of plot that displays the distribution of values in a dataset using one continuous curve.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/kde2.png">
A kernel density plot is similar to a  histogram , but it’s even better at displaying the shape of a distribution since it isn’t affected by the number of bins used in the histogram.
We can use the following methods to create a kernel density plot in R:
<b>Method 1: Create One Kernel Density Plot</b>
<b>#define kernel density
kd &lt;- density(data)
#create kernel density plot
plot(kd)
</b>
<b>Method 2: Create a Filled-In Kernel Density Plot</b>
<b>#define kernel density
kd &lt;- density(data)
#create kernel density plot
plot(kd)
#fill in kernel density plot with specific color
polygon(kd, col='blue', border='black')
</b>
<b>Method 3: Create Multiple Kernel Density Plots</b>
<b>#plot first kernel density plot
kd1 &lt;- density(data1)
plot(kd1, col='blue')
#plot second kernel density plot
kd2 &lt;- density(data2)
lines(kd2, col='red')
#plot third kernel density plot
kd3 &lt;- density(data3)
lines(kd3, col='purple')
...</b>
The following examples show how to use each method in practice.
<h3>Method 1: Create One Kernel Density Plot</h3>
The following code shows how to create a kernel density plot for one dataset in R:
<b>#create data
data &lt;- c(3, 3, 4, 4, 5, 6, 7, 7, 7, 8, 12, 13, 14, 17, 19, 19)
#define kernel density
kd &lt;- density(data)
#create kernel density plot
plot(kd, main='Kernel Density Plot of Data')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/kde1.png">
The x-axis shows the values of the dataset and the y-axis shows the relative frequency of each value. The highest points in the plot show where the values occur most often.
<h3>Method 2: Create a Filled-In Kernel Density Plot</h3>
The following code shows how to create a kernel density plot with a specific border color and filled-in color:
<b>#create data
data &lt;- c(3, 3, 4, 4, 5, 6, 7, 7, 7, 8, 12, 13, 14, 17, 19, 19)
#define kernel density
kd &lt;- density(data)
#create kernel density plot
plot(kd)
#add color
polygon(kd, col='steelblue', border='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/kde2.png">
<h3>Method 3: Create Multiple Kernel Density Plots</h3>
The following code shows how to create multiple kernel density plots in one plot in R:
<b>#create datasets
data1 &lt;- c(3, 3, 4, 4, 5, 6, 7, 7, 7, 8, 12, 13, 14, 17, 19, 19)
data2 &lt;- c(12, 3, 14, 14, 4, 5, 6, 10, 14, 7, 7, 8, 10, 12, 17, 20)
#plot first kernel density plot
kd1 &lt;- density(data1)
plot(kd1, col='blue', lwd=2)
#plot second kernel density plot
kd2 &lt;- density(data2)
lines(kd2, col='red', lwd=2)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/kde3.png">
Note that we can use similar syntax to create as many kernel density plots in one chart as we’d like.
<h2><span class="orange">How to Calculate KL Divergence in R (With Example)</span></h2>
In statistics, the <b>Kullback–Leibler (KL) divergence</b> is a distance metric that quantifies the difference between two probability distributions.
If we have two probability distributions, P and Q, we typically write the KL divergence using the notation KL(P || Q), which means “P’s divergence from Q.”
We calculate it using the following formula:
KL(P || Q) = ΣP(x) <em>ln</em>(P(x) / Q(x))
If the KL divergence between two distributions is zero, then it indicates that the distributions are identical.
The easiest way to calculate the KL divergence between two probability distributions in R is to use the <b>KL()</b> function from the <b>philentropy</b> package.
The following example shows how to use this function in practice.
<h2>Example: Calculating KL Divergence in R</h2>
Suppose we have the following two probability distributions in R:
<b>#define two probability distributions
P &lt;- c(.05, .1, .2, .05, .15, .25, .08, .12)
Q &lt;- c(.3, .1, .2, .1, .1, .02, .08, .1)
</b>
<b>Note</b>: It’s important that the probabilities for each distribution sum to one.
We can use the following code to calculate the KL divergence between the two distributions:
<b>library(philentropy)
#rbind distributions into one matrix
x &lt;- rbind(P,Q)
#calculate KL divergence
KL(x, unit='log')
Metric: 'kullback-leibler' using unit: 'log'; comparing: 2 vectors.
kullback-leibler 
       0.5898852 
</b>
The KL divergence of distribution P from distribution Q is about <b>0.589</b>.
Note that the units used in this calculation are known as  nats , which is short for <em>natural unit of information</em>.
Thus, we would say that the KL divergence is <b>0.589 nats</b>.
Also note that the KL divergence is not a symmetric metric. This means that if we calculate the KL divergence of distribution Q from distribution P, we will likely get a different value:
<b>library(philentropy)
#rbind distributions into one matrix
x &lt;- rbind(Q,P)
#calculate KL divergence
KL(x, unit='log')
Metric: 'kullback-leibler' using unit: 'log'; comparing: 2 vectors.
kullback-leibler 
       0.4975493 
</b>
The KL divergence of distribution Q from distribution P is about <b>0.497 nats</b>.
Also note that some formulas use log base-2 to calculate the KL divergence. In this case, we refer to the divergence in terms of  bits  instead of nats.
To calculate the KL divergence in terms of bits, you can instead use log2 in the <b>unit</b> argument:
<b>library(philentropy)
#rbind distributions into one matrix
x &lt;- rbind(P,Q)
#calculate KL divergence (in bits)
KL(x, unit='log2')
Metric: 'kullback-leibler' using unit: 'log2'; comparing: 2 vectors.
kullback-leibler 
       0.7178119
</b>
The KL divergence of distribution P from distribution Q is about <b>0.7178 bits</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Generate a Normal Distribution in R 
 How to Plot a Normal Distribution in R 
<h2><span class="orange">How to Calculate KL Divergence in Python (Including Example)</span></h2>
In statistics, the <b>Kullback–Leibler (KL) divergence</b> is a distance metric that quantifies the difference between two probability distributions.
If we have two probability distributions, P and Q, we typically write the KL divergence using the notation KL(P || Q), which means “P’s divergence from Q.”
We calculate it using the following formula:
KL(P || Q) = ΣP(x) <em>ln</em>(P(x) / Q(x))
If the KL divergence between two distributions is zero, then it indicates that the distributions are identical.
We can use the  scipy.special.rel_entr()  function to calculate the KL divergence between two probability distributions in Python.
The following example shows how to use this function in practice.
<h3>Example: Calculating KL Divergence in Python</h3>
Suppose we have the following two probability distributions in Python:
<b>Note</b>: It’s important that the probabilities for each distribution sum to one.
<b>#define two probability distributions
P = [.05, .1, .2, .05, .15, .25, .08, .12]
Q = [.3, .1, .2, .1, .1, .02, .08, .1]
</b>
We can use the following code to calculate the KL divergence between the two distributions:
<b>from scipy.special import rel_entr
#calculate (P || Q)
sum(rel_entr(P, Q))
0.589885181619163</b>
The KL divergence of distribution P from distribution Q is about <b>0.589</b>.
Note that the units used in this calculation are known as  nats , which is short for <em>natural unit of information</em>.
Thus, we would say that the KL divergence is <b>0.589 nats</b>.
Also note that the KL divergence is not a symmetric metric. This means that if we calculate the KL divergence of distribution Q from distribution P, we will likely get a different value:
<b>from scipy.special import rel_entr
#calculate (Q || P)
sum(rel_entr(Q, P))
0.497549319448034</b>
The KL divergence of distribution Q from distribution P is about <b>0.497 nats</b>.
<b>Note</b>: Some formulas use log base-2 to calculate the KL divergence. In this case, we refer to the divergence in terms of  bits  instead of nats.
<h2><span class="orange">How to Perform a Kolmogorov-Smirnov Test in Excel</span></h2>
The <b>Kolmogorov-Smirnov test</b> is used to determine whether or not or not a sample is  normally distributed .
This test is widely used because many statistical tests and procedures make the  assumption  that the data is normally distributed.
The following step-by-step example shows how to perform a Kolmogorov-Smirnov test on a sample dataset in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the values for a dataset with a sample size of n = 20:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/ks1.jpg"414">
<h3>Step 2: Calculate Actual vs. Expected Values from Normal Distribution</h3>
Next, we’ll calculate the actual values vs. the expected values from the normal distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/ks2.jpg">
Here is the formula we used in various cells:
<b>B2</b>: =ROW() – 1
<b>C2</b>: =B2/COUNT($A$2:$A$21)
<b>D2</b>: =(B2-1)/COUNT($A$2:$A$21)
<b>E2</b>: =IF(C2&lt;1, NORM.S.INV(C2),””)
<b>F2</b>: =NORM.DIST(A2, $J$1, $J$2, TRUE)
<b>G2</b>: =ABS(F2–D2)
<b>J1</b>: =AVERAGE(A2:A21)
<b>J2</b>: =STDEV.S(A2:A21)
<b>J4</b>: =MAX(G2:G21)
<h3>Step 3: Interpret the Results</h3>
A Kolmogorov-Smirnov test uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: The data is normally distributed.
<b>H<sub>A</sub></b>: The data is not normally distributed.
To determine if we should reject or fail to reject the null hypothesis we must refer to the <b>Maximum</b> value in the output, which turns out to be <b>0.10983</b>.
This represents the maximum absolute difference between the actual values of our sample and the expected values from a normal distribution.
To determine if this maximum value is statistically significant, we must refer to a Kolmogorov-Smirnov Table of critical values and find the number equal to n = 20 and α = .05.
The critical value turns out to be <b>0.190</b>.
Since our maximum value is not greater than this critical value, we fail to reject the null hypothesis.
This means we can assume that our sample data is normally distributed.
<h2><span class="orange">How to Perform a Kolmogorov-Smirnov Test in SAS</span></h2>
The <b>Kolmogorov-Smirnov test</b> is used to determine whether or not or not a sample is  normally distributed .
This test is widely used because many statistical tests and procedures make the  assumption  that the data is normally distributed.
The following step-by-step example shows how to perform a Kolmogorov-Smirnov test on a sample dataset in SAS.
<h3>Example: Kolmogorov-Smirnov Test in SAS</h3>
First, let’s create a dataset in SAS with a sample size of n = 20:
<b>/*create dataset*/
data my_data;
    input Values;
    datalines;
5.57
8.32
8.35
8.74
8.75
9.38
9.91
9.96
10.36
10.65
10.77
10.97
11.15
11.18
11.47
11.64
11.88
12.24
13.02
13.19
;
run;
</b>
Next, we’ll use <b>proc univariate</b> to perform a Kolmogorov-Smirnov test to determine if the sample is normally distributed:
<b>/*perform Kolmogorov-Smirnov test*/
proc univariate data=my_data;
   histogram Values / normal(mu=est sigma=est);
run;</b>
At the bottom of the output we can see the test statistic and corresponding p-value of the Kolmogorov-Smirnov test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/ksSAS1.jpg">
The test statistic is <b>0.1098</b> and the corresponding p-value is <b>>0.150</b>.
Recall that a Kolmogorov-Smirnov test uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: The data is normally distributed.
<b>H<sub>A</sub></b>: The data is not normally distributed.
Since the p-value from the test is not less than .05, we fail to reject the null hypothesis.
This means we can assume that the dataset is normally distributed.
<h2><span class="orange">How to Perform a Kolmogorov-Smirnov Test in Python</span></h2>
The <b>Kolmogorov-Smirnov test </b>is used to test whether or not or not a sample comes from a certain distribution.
To perform a Kolmogorov-Smirnov test in Python we can use the  scipy.stats.kstest()  for a one-sample test or  scipy.stats.ks_2samp()  for a two-sample test.
This tutorial shows an example of how to use each function in practice.
<h3>Example 1: One Sample Kolmogorov-Smirnov Test</h3>
Suppose we have the following sample data:
<b>from numpy.random import seed
from numpy.random import poisson
#set seed (e.g. make this example reproducible)
seed(0)
#generate dataset of 100 values that follow a Poisson distribution with mean=5
data = poisson(5, 100)</b>
The following code shows how to perform a Kolmogorov-Smirnov test on this sample of 100 data values to determine if it came from a normal distribution:
<b>from scipy.stats import kstest
#perform Kolmogorov-Smirnov test
kstest(data, 'norm')
KstestResult(statistic=0.9072498680518208, pvalue=1.0908062873170218e-103)
</b>
From the output we can see that the test statistic is <b>0.9072 </b>and the corresponding p-value is <b>1.0908e-103</b>. Since the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the sample data does not come from a normal distribution.
This result also shouldn’t be surprising since we generated the sample data using the <b>poisson() </b>function, which generates random values that follow a  Poisson distribution .
<h3>Example 2: Two Sample Kolmogorov-Smirnov Test</h3>
Suppose we have the following two sample datasets:
<b>from numpy.random import seed
from numpy.random import randn
from numpy.random import lognormal
#set seed (e.g. make this example reproducible)
seed(0)
#generate two datasets
data1 = randn(100)
data2 = lognormal(3, 1, 100)</b>
The following code shows how to perform a Kolmogorov-Smirnov test on these two samples to determine if they came from the same distribution:
<b>from scipy.stats import ks_2samp
#perform Kolmogorov-Smirnov test
ks_2samp(data1, data2)
KstestResult(statistic=0.99, pvalue=4.417521386399011e-57)
</b>
From the output we can see that the test statistic is <b>0.99 </b>and the corresponding p-value is <b>4.4175e-57</b>. Since the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the two sample datasets do not come from the same distribution.
This result also shouldn’t be surprising since we generated values for the first sample using the standard normal distribution and values for the second sample using the lognormal distribution.
<h2><span class="orange">Kolmogorov-Smirnov Test in R (With Examples)</span></h2>
The <b>Kolmogorov-Smirnov test </b>is used to test whether or not or not a sample comes from a certain distribution.
To perform a one-sample or two-sample Kolmogorov-Smirnov test in R we can use the  ks.test()  function.
This tutorial shows example of how to use this function in practice.
<h3>Example 1: One Sample Kolmogorov-Smirnov Test</h3>
Suppose we have the following sample data:
<b>#make this example reproducible
seed(0)
#generate dataset of 100 values that follow a Poisson distribution with mean=5
data &lt;- rpois(n=20, lambda=5)</b>
<b>Related: </b> A Guide to dpois, ppois, qpois, and rpois in R 
The following code shows how to perform a Kolmogorov-Smirnov test on this sample of 100 data values to determine if it came from a normal distribution:
<b>#perform Kolmogorov-Smirnov test
ks.test(data, "pnorm")
One-sample Kolmogorov-Smirnov test
data:  data
D = 0.97725, p-value &lt; 2.2e-16
alternative hypothesis: two-sided
</b>
From the output we can see that the test statistic is <b>0.97725 </b>and the corresponding p-value is <b>2.2e-16</b>. Since the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the sample data does not come from a normal distribution.
This result shouldn’t be surprising since we generated the sample data using the <b>rpois() </b>function, which generates random values that follow a  Poisson distribution .
<h3>Example 2: Two Sample Kolmogorov-Smirnov Test</h3>
Suppose we have the following two sample datasets:
<b>#make this example reproducible
seed(0)
#generate two datasets
data1 &lt;- rpois(n=20, lambda=5)
data2 &lt;- rnorm(100)</b>
The following code shows how to perform a Kolmogorov-Smirnov test on these two samples to determine if they came from the same distribution:
<b>#perform Kolmogorov-Smirnov test
ks.test(data1, data2)
Two-sample Kolmogorov-Smirnov test
data:  data1 and data2
D = 0.99, p-value = 1.299e-14
alternative hypothesis: two-sided
</b>
From the output we can see that the test statistic is <b>0.99 </b>and the corresponding p-value is <b>1.299e-14</b>. Since the p-value is less than .05, we reject the null hypothesis. We have sufficient evidence to say that the two sample datasets do not come from the same distribution.
This result also shouldn’t be surprising since we generated values for the first sample using the Poisson distribution and values for the second sample using the  normal distribution .
<h2><span class="orange">How to Perform a KPSS Test in Python</span></h2>
A <b>KPSS test</b> can be used to determine if a time series is trend stationary.
This test uses the following null and alternative hypothesis:
<b>H<sub>0</sub></b>: The time series is trend stationary.
<b>H<sub>A</sub></b>: The time series is <em>not</em> trend stationary.
If the  p-value  of the test is less than some significance level (e.g. α = .05) then we reject the null hypothesis and conclude that the time series is not trend stationary.
Otherwise, we fail to reject the null hypothesis.
The following examples show how to perform a KPSS test in Python.
<h3>Example 1: KPSS Test in Python (With Stationary Data)</h3>
First, let’s create some fake data in Python to work with:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#create time series data
data = np.random.normal(size=100)
#create line plot of time series data
plt.plot(data)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kpss3.jpg"524">
We can use the<b> kpss()</b> function from the <b>statsmodels </b>package to perform a KPSS test on this time series data:
<b>import statsmodels.api as sm
#perform KPSS test
sm.tsa.stattools.kpss(data, regression='ct')
(0.0477617848370993,
 0.1,
 1,
 {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216})
InterpolationWarning: The test statistic is outside of the range of p-values available
in the look-up table. The actual p-value is greater than the p-value returned.
</b>
Here’s how to interpret the output:
The KPSS test statistic: <b>0.04776</b>
The p-value: <b>0.1</b>
The truncation lag parameter: <b>1</b>
The critical values at <b>10%</b>, <b>5%</b>, <b>2.5%</b>, and <b>1%</b>
The p-value is <b>0.1</b>. Since this value is not less than .05, we fail to reject the null hypothesis of the KPSS test.
This means we can assume that the time series is trend stationary.
<b>Note 1</b>: The p-value is actually even greater than 0.1, but the lowest value that the kpss() function will output is 0.1.
<b>Note 2</b>: We must use the argument <b>regression=’ct’</b> to specify that the null hypothesis of the test is that the data is trend stationary.
<h3>Example 2: KPSS Test in Python (With Non-Stationary Data)</h3>
First, let’s create some fake data in Python to work with:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#create time series data
data =np.array([0, 3, 4, 3, 6, 7, 5, 8, 15, 13, 19, 12, 29, 15, 45, 23, 67, 45])
#create line plot of time series data
plt.plot(data)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kpss4.jpg"525">
Once again, we can use the<b> kpss()</b> function from the <b>statsmodels </b>package to perform a KPSS test on this time series data:
<b>import statsmodels.api as sm
#perform KPSS test
sm.tsa.stattools.kpss(data, regression='ct')
(0.15096358910843685,
 0.04586367574296928,
 3,
 {'10%': 0.119, '5%': 0.146, '2.5%': 0.176, '1%': 0.216})
</b>
Here’s how to interpret the output:
The KPSS test statistic: <b>0.1509</b>
The p-value: <b>0.0458</b>
The truncation lag parameter: <b>3</b>
The critical values at <b>10%</b>, <b>5%</b>, <b>2.5%</b>, and <b>1%</b>
The p-value is <b>0.0458</b>. Since this value is less than .05, we reject the null hypothesis of the KPSS test.
This means the time series is <em>not</em> trend stationary.
<b>Note</b>: You can find the complete documentation for the kpss() function from the statsmodels package  here .
<h3>
<b>Additional Resources</b>
</h3>
The following tutorials provide additional information on how to work with time series data in Python:
 How to Perform an Augmented Dickey-Fuller Test in Python 
 How to Perform a Mann-Kendall Trend Test in Python 
 How to Plot a Time Series in Matplotlib 
<h2><span class="orange">How to Perform a KPSS Test in R (Including Example)</span></h2>
A <b>KPSS test</b> can be used to determine if a time series is trend stationary.
This test uses the following null and alternative hypothesis:
<b>H<sub>0</sub></b>: The time series is trend stationary.
<b>H<sub>A</sub></b>: The time series is <em>not</em> trend stationary.
If the  p-value  of the test is less than some significance level (e.g. α = .05) then we reject the null hypothesis and conclude that the time series is not trend stationary.
Otherwise, we fail to reject the null hypothesis.
The following examples show how to perform a KPSS test in R.
<h3>Example 1: KPSS Test in R (With Stationary Data)</h3>
First, let’s create some fake data in R to work with:
<b>#make this example reproducible
set.seed(100)
#create time series data
data&lt;-rnorm(100)
#plot time series data as line plot
plot(data, type='l')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kpss1.jpg"441">
We can use the<b> kpss.test()</b> function from the <b>tseries</b> package to perform a KPSS test on this time series data:
<b>library(tseries)
#perform KPSS test
kpss.test(data, null="Trend")
KPSS Test for Trend Stationarity
data:  data
KPSS Trend = 0.034563, Truncation lag parameter = 4, p-value = 0.1
Warning message:
In kpss.test(data, null = "Trend") : p-value greater than printed p-value
</b>
The p-value is <b>0.1</b>. Since this value is not less than .05, we fail to reject the null hypothesis of the KPSS test.
This means we can assume that the time series is trend stationary.
<b>Note</b>: The p-value is actually even greater than 0.1, but the lowest value that the kpss.test() function will output is 0.1.
<h3>Example 2: KPSS Test in R (With Non-Stationary Data)</h3>
First, let’s create some fake data in R to work with:
<b>#make this example reproducible
#create time series data
data &lt;-c(0, 3, 4, 3, 6, 7, 5, 8, 15, 13, 19, 12, 29, 15, 45, 23, 67, 45)
#plot time series data as line plot
plot(data, type='l')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kpss2.jpg"408">
Once again, we can use the<b> kpss.test()</b> function from the <b>tseries</b> package to perform a KPSS test on this time series data:
<b>library(tseries)
#perform KPSS test
kpss.test(data, null="Trend")
KPSS Test for Trend Stationarity
data:  data
KPSS Trend = 0.149, Truncation lag parameter = 2, p-value = 0.04751</b>
The p-value is <b>0.04751</b>. Since this value is less than .05, we reject the null hypothesis of the KPSS test.
This means the time series is <em>not</em> trend stationary.
<h3>
<b>Additional Resources</b>
</h3>
The following tutorials provide additional information on how to work with time series data in R:
 How to Plot a Time Series in R 
 How to Perform an Augmented Dickey-Fuller Test in R 
<h2><span class="orange">Kruskal-Wallis Test Calculator</span></h2>
This Kruskal-Wallis Test calculator compares the medians of three or more independent samples. It is the nonparametric version of the One-Way ANOVA.
Simply enter the values for up to five samples into the cells below, then press the “Calculate” button.
<b>Sample 1</b>
<textarea id="a" rows="3" cols="40">11, 12, 12, 14, 16, 19, 19, 20, 21, 21</textarea>
<b>Sample 2</b>
<textarea id="b" rows="3" cols="40">16, 16, 16, 17, 18, 19, 22, 24, 25, 32</textarea>
<b>Sample 3</b>
<textarea id="c" rows="3" cols="40">18, 18, 19, 21, 21, 22, 22, 23, 24, 26</textarea>
<b>Sample 4</b>
<textarea id="d" rows="3" cols="40"></textarea>
<b>Sample 5</b>
<textarea id="e" rows="3" cols="40"></textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<div>
H Statistic: 6.59806
<div>
p-value: 0.03692
<script>
//create function that performs calculations
function calc() {
//get raw data for each group
var group_a  = document.getElementById('a').value.split(',').map(Number);
var group_b  = document.getElementById('b').value.split(',').map(Number);
var group_c  = document.getElementById('c').value.split(',').map(Number);
var group_d  = document.getElementById('d').value.split(',').map(Number);
var group_e  = document.getElementById('e').value.split(',').map(Number);
//define addition function
function add(a, b) {
    return a + b;
}
    //define function to perform kruskal-wallis test
    function kruskal() {
var ngroups = arguments.length,
groupRankSums = new Array( ngroups ),
groupsIndicators = [],
i, j,
arg,
n = [], N,
x = [],
ranks, ties, tieSumTerm,
key, s,
stat_placeholder, stat, param, pval;
if ( ngroups < 2 ) {
throw new Error( 'kruskal()::invalid number of input arguments. Must provide at least two array-like arguments. Value: `' + arg + '`.' );
}
for ( i = 0; i < ngroups; i++ ) {
arg = arguments[ i ];
n[ i ] = arg.length;
groupRankSums[ i ] = 0;
for ( j = 0; j < n[ i ]; j++ ) {
groupsIndicators.push( i );
x.push( arg[ j ] );
}
}
N = x.length;
var sorted = x.slice().sort(function(a,b){return a-b});
var reversed = sorted.slice(0).reverse();
ranks = x.slice().map(function(v){ return ((sorted.indexOf(v) + 1) + (reversed.length - reversed.indexOf(v))) / 2;});
// calculate # ties for each value &#038; rank sums per group
ties = {};
for ( i = 0; i < N; i++ ) {
groupRankSums[ groupsIndicators[ i ] ] += ranks[ i ];
if ( x[ i ] in ties ) {
ties[ x[ i ] ] += 1;
} else {
ties[ x[ i ] ] = 1;
}
}
// calculate test statistic using short-cut formula
rank_squares = groupRankSums.map(function(x) { return Math.pow(x, 2); });
rank_squares_sum = (rank_squares.map(function(x, i) { return x / n[i]; })).reduce(add, 0);
stat = (12/(N*(N+1))) * rank_squares_sum - 3*(N+1);
param = ngroups - 1;
pval = 1 - jStat.chisquare.cdf(stat, param);
return {
'H': stat,
'df': param,
'pValue': pval,
};
} // end FUNCTION kruskal()
if (group_a.length > 1) { var flag_group_a = 1; } else { var flag_group_a = 0;};
if (group_b.length > 1) { var flag_group_b = 1; } else { var flag_group_b = 0;};
if (group_c.length > 1) { var flag_group_c = 1; } else { var flag_group_c = 0;};
if (group_d.length > 1) { var flag_group_d = 1; } else { var flag_group_d = 0;};
if (group_e.length > 1) { var flag_group_e = 1; } else { var flag_group_e = 0;};
var total_treatments = [flag_group_a, flag_group_b, flag_group_c, flag_group_d, flag_group_e].reduce(add, 0);
var out;
if (total_treatments == 2) {
out = kruskal(group_a, group_b);
} else if (total_treatments == 3) {
out = kruskal(group_a, group_b, group_c);
} else if (total_treatments == 4) {
out = kruskal(group_a, group_b, group_c, group_d);
} else if (total_treatments == 5){
out = kruskal(group_a, group_b, group_c, group_d, group_e);
}
var H = out[Object.keys(out)[0]];
var p = out[Object.keys(out)[2]];
        
//--------------OUTPUT RESULTS-----------//
document.getElementById('H').innerHTML = H.toFixed(5);
document.getElementById('p').innerHTML = p.toFixed(5);
}
</script>
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in Excel</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the One-Way ANOVA.
This tutorial explains how to conduct a Kruskal-Wallis Test in Excel.
<h3>Example: Kruskal-Wallis Test in Excel</h3>
Researchers want to know if three different fertilizers lead to different levels of plant growth. They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
Use the following steps to perform a Kruskal-Wallis Test to determine if the median growth is the same across the three groups.
<b>Step 1: Enter the data.</b>
Enter the following data, which shows the total growth (in inches) for each of the 10 plants in each group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/kruskalWallisExcel1.png">
<b>Step 2: Rank the data.</b>
Next, we will use the <b>RANK.AVG() </b>function to assign a rank to the growth of each plant out of all 30 plants. The following formula shows how to calculate the rank for the first plant in the first group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/kruskalWallisExcel2.png">
Copy this formula to the rest of the cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/kruskalWallisExcel3.png">
Then, calculate the sum of the ranks for each column along with the sample size and the squared sum of ranks divided by the sample size:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/kruskalWallisExcel4.png">
<b>Step 3: Calculate the test statistic and the corresponding p-value.</b>
The test statistic is defined as:
H = 12/(n(n+1)) * ΣR<sub>j</sub><sup>2</sup>/n<sub>j</sub> – 3(n+1)
where:
n = total sample size
R<sub>j</sub><sup>2</sup> =sum of ranks for the j<sup>th</sup> group
n<sub>j</sub> =sample size of j<sup>th</sup> group
Under the null hypothesis, H follows a Chi-square distribution with k-1 degrees of freedom.
The following screenshot shows the formulas used to calculate the test statistic, H, and the corresponding p-value:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/kruskalWallisExcel5.png">
The test statistic is H = <b>6.204 </b>and the corresponding p-value is p = <b>0.045</b>. Since this p-value is less than 0.05, we can reject the null hypothesis that the median plant growth is the same for all three fertilizers. We have sufficient evidence to conclude that the type of fertilizer used leads to statistically significant differences in plant growth.
<b>Step 4: Report the results.</b>
Lastly, we want to report the results of the Kruskal-Wallis Test. Here is an example of how to do so:
A Kruskal-Wallist Test was performed to determine if median plant growth was the same for three different plant fertilizers. A total of 30 plants were used in the analysis. Each fertilizer was applied to 10 different plants.
 
The test revealed that the median plant growth was not the same (H = 6.204, p = 0.045) among the three fertilizers. That is, there was a statistically significant difference in median plant growth among two or more of the fertilizers.
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in R</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups.
It is considered to be the non-parametric equivalent of the  One-Way ANOVA .
This tutorial explains how to perform a Kruskal-Wallis Test in R.
<h3>Example: Kruskal-Wallis Test in R</h3>
Suppose researchers want to know if three different fertilizers lead to different levels of plant growth. They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
Use the following steps to perform a Kruskal-Wallis Test to determine if the median growth is the same across the three groups.
<b>Step 1: Enter the data.</b>
First, we’ll create the following data frame that contains the growth of all 30 plants along with their fertilizer group:
<b>#create data frame
df &lt;- data.frame(group=rep(c('A', 'B', 'C'), each=10), height=c(7, 14, 14, 13, 12, 9, 6, 14, 12, 8,          15, 17, 13, 15, 15, 13, 9, 12, 10, 8,          6, 8, 8, 9, 5, 14, 13, 8, 10, 9))
#view first six rows of data frame
head(df)
  group height
1     A      7
2     A     14
3     A     14
4     A     13
5     A     12
6     A      9</b>
<b>Step 2: Perform the Kruskal-Wallis Test.</b>
Next, we’ll perform a Kruskal-Wallis Test using the built-in <b>kruskal.test()</b> function from base R:
<b>#perform Kruskal-Wallis Test 
kruskal.test(height ~ group, data = df) 
Kruskal-Wallis rank sum test
data:  height by group
Kruskal-Wallis chi-squared = 6.2878, df = 2, p-value = 0.04311
</b>
<b>Step 3: Interpret the results.</b>
The Kruskal-Wallis Test uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> The median is equal across all groups.
<b>The alternative hypothesis: (H<sub>A</sub>):</b> The median is <em>not </em>equal across all groups.
In this case, the test statistic is <b>6.2878 </b>and the corresponding p-value is <b>0.0431</b>.
Since this  p-value  is less than 0.05, we can reject the null hypothesis that the median plant growth is the same for all three fertilizers.
This means we have sufficient evidence to conclude that the type of fertilizer used leads to statistically significant differences in plant growth.
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in Python</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups.
It is considered to be the non-parametric equivalent of the  One-Way ANOVA .
This tutorial explains how to conduct a Kruskal-Wallis Test in Python.
<h3>Example: Kruskal-Wallis Test in Python</h3>
Researchers want to know if three different fertilizers lead to different levels of plant growth. They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
Use the following steps to perform a Kruskal-Wallis Test to determine if the median growth is the same across the three groups.
<b>Step 1: Enter the data.</b>
First, we’ll create three arrays to hold our our plant measurements for each of the three groups:
<b>group1 = [7, 14, 14, 13, 12, 9, 6, 14, 12, 8]
group2 = [15, 17, 13, 15, 15, 13, 9, 12, 10, 8]
group3 = [6, 8, 8, 9, 5, 14, 13, 8, 10, 9]
</b>
<b>Step 2: Perform the Kruskal-Wallis Test.</b>
Next, we’ll perform a Kruskal-Wallis Test using the  kruskal() function  from the scipy.stats library:
<b>from scipy import stats
#perform Kruskal-Wallis Test 
stats.kruskal(group1, group2, group3)
(statistic=6.2878, pvalue=0.0431)
</b>
<b>Step 3: Interpret the results.</b>
The Kruskal-Wallis Test uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> The median is equal across all groups.
<b>The alternative hypothesis: (Ha):</b> The median is <em>not </em>equal across all groups.
In this case, the test statistic is <b>6.2878 </b>and the corresponding p-value is <b>0.0431</b>. Since this p-value is less than 0.05, we can reject the null hypothesis that the median plant growth is the same for all three fertilizers. We have sufficient evidence to conclude that the type of fertilizer used leads to statistically significant differences in plant growth.
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in SAS</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups.
It is considered to be the non-parametric equivalent of the  One-Way ANOVA .
This tutorial provides a step-by-step example of how to conduct a Kruskal-Wallis Test in SAS.
<h3>Step 1: Enter the Data</h3>
Suppose researchers want to know if three different fertilizers lead to different levels of plant growth. 
They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
We’ll enter the following data in SAS, which shows the total growth (in inches) for each of the 10 plants in each group:
<b>/*create dataset*/
data fertilizer_data;
    input fertilizer $ growth;
    datalines;
fert1 7
fert1 14
fert1 14
fert1 13
fert1 12
fert1 9
fert1 6
fert1 14
fert1 12
fert1 8
fert2 15
fert2 17
fert2 13
fert2 15
fert2 15
fert2 13
fert2 9
fert2 12
fert2 10
fert2 8
fert3 6
fert3 8
fert3 8
fert3 9
fert3 5
fert3 14
fert3 13
fert3 8
fert3 10
fert3 9
;
run;
</b>
<h3>Step 2: Perform the Kruskal-Wallis Test</h3>
Next, we will use the <b>proc npar1way</b> statement to perform a Kruskal-Wallis test to compare the median growth of the plants between the three fertilizer groups:
<b>/*perform Kruskal-Wallis test*/
proc npar1way data=fertilizer_data wilcoxon dscf;
    class fertilizer;
    var growth;
run;</b>
<h3>Step 3: Interpret the Results</h3>
The first table in the output shows the overall Chi-Squared test statistic and the corresponding p-value for the Kruskal-Wallis test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/kruskalsas1.jpg"386">
The p-value of the test is <b>0.0431</b>. Since this value is less than .05, we reject the null hypothesis that the median plant growth is the same for all three fertilizers.
This means we have sufficient evidence to conclude that the type of fertilizer used leads to statistically significant differences in plant growth.
The last table in the output shows the p-values for the pairwise comparisons between each of the three groups:
 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/kruskalsas2.jpg"357">
From this table we can see that the only p-value less than .05 is the comparison between fertilizer 2 and fertilizer 3, which has a p-value of <b>0.0390</b>.
This means there is a statistically significant difference in plant growth between fertilizer 2 and fertilizer 3, but not between any other pairwise comparisons.
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in SPSS</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the  One-Way ANOVA .
This tutorial explains how to conduct a Kruskal-Wallis Test in SPSS.
<h3>Example: Kruskal-Wallis Test in SPSS</h3>
A researcher wants to know whether or not three drugs have different effects on knee pain, so he recruits 30 individuals who all experience similar knee pain and randomly splits them up into three groups to receive either Drug 1, Drug 2, or Drug 3.
After one month of taking the drug, the researcher asks each individual to rate their knee pain on a scale of 1 to 100, with 100 indicating the most severe pain. The ratings for all 30 individuals are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/kruskallspss1.png">
Use the following steps to perform a Kruskal-Wallis Test to determine whether or not there is a difference between the reported levels of knee pain between the three groups:
<b>Step 1: Perform a Kruskal-Wallis Test.</b>
Click the <b>Analyze </b>tab, then <b>Nonparametric Tests</b>, then <b>Legacy Dialogs</b>, then <b>K Independent Samples</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/kruskallspss2.png">
In the window that pops up, drag the variable <b>pain </b>into the box labelled Test Variable List and <b>drug </b>into the box labelled Grouping Variable. Then click <b>Define Range </b>and set the Minimum value to 1 and the Maximum value to 3. Then click <b>Continue</b>. Make sure the box is checked next to <b>Kruskal-Wallis H </b>and then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/kruskallspss3.png">
<b>Step 2: Interpret the results.</b>
Once you click <b>OK</b>, the results of the Kruskal-Wallis test will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/kruskallspss4.png">
The second table in the output displays the results of the test:
<b>Kruskal-Wallis H: </b>This is the X<sup>2</sup> test statistic.
<b>df: </b>This is the degrees of freedom, calculated as #groups-1 = 3-1 = 2.
<b>Asymp. Sig: </b>This is the p-value associated with a X<sup>2</sup> test statistic of 3.097 with 2 degrees of freedom. This can also be found by using the  Chi-Square Score to P Value Calculator .
Since the p-value (.213) is not less than .05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that there is a statistically significant difference between the knee pain ratings across these three groups.
<h2><span class="orange">How to Perform a Kruskal-Wallis Test in Stata</span></h2>
A  Kruskal-Wallis Test  is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups. It is considered to be the non-parametric equivalent of the  One-Way ANOVA .
This tutorial explains how to conduct a Kruskal-Wallis Test in Stata.
<h3>How to Perform a Kruskal-Wallis Test in Stata</h3>
For this example we will use the <em>census </em>dataset, which contains 1980 census data for all fifty states in the U.S. Within the dataset, the states are classified into four different regions:
Northeast
North Central
South
West
We will perform a Kruskal-Wallis Test to determine if the median age is equal across these four regions.
<b>Step 1: Load and view the data.</b>
First, load the dataset by typing the following command into the Command box:
<b>use http://www.stata-press.com/data/r13/census</b>
Get a quick summary of the dataset by using the following command:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/kruskallStata1.png">
We can see that there are 13 different variables in this dataset, but the only two we will be working with are <em>medage </em>(median age) and <em>region</em>.
<b>Step 2: Visualize the data.</b>
Before we perform the Kruskal-Wallis Test, let’s first create some  box plots  to visualize the distribution of median age for each of the four regions:
<b>graph box medage, over(region)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/kruskallStata2.png">
Just from looking at the box plots we can see that the distributions seem to vary between regions. Next, we’ll perform a Kruskal-Wallis Test to see if these differences are statistically significant.
<b>Step 3: Perform a Kruskal-Wallis Test.</b>
Use the following syntax to perform a Kruskal-Wallis Test:
<b>kwallis measurement_variable, by(grouping_variable)</b>
In our case, we will use the following syntax:
<b>kwallis medage, by(region)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/kruskallStata3.png">
Here is how to interpret the output:
<b>Summary table: </b>This table shows the number of observations per region and the rank sums for each region.
<b>Chi-squared with ties: </b>This is the value of the test statistic, which turns out to be 17.062.
<b>probability:</b> This is the p-value that corresponds to the test statistic, which turns out to be 0.0007. Since this value is less than .05, we can reject the null hypothesis and conclude that the median age is not equal across the four regions.
<b>Step 4: Report the results.</b>
Lastly, we want to report the results of the Kruskal-Wallis Test. Here is an example of how to do so:
A Kruskal-Wallist Test was performed to determine if the median age of individuals was the same across the following four regions in the United States:
 
Northeast (n = 9)
North Central (n = 12)
South (n = 16)
West (n = 13)
The test revealed that the median age of individuals was not the same (X<sup>2</sup> =17.062, p = 0.0007) across the four regions. That is, there was a statistically significant difference in median age between two or more of the regions.
<h2><span class="orange">Kruskal-Wallis Test: Definition, Formula, and Example</span></h2>
A <b>Kruskal-Wallis test</b> is used to determine whether or not there is a statistically significant difference between the medians of three or more independent groups.
This test is the nonparametric equivalent of the  one-way ANOVA  and is typically used when the normality assumption is violated.
The Kruskal-Wallis test does not assume  normality  in the data and is much less sensitive to outliers than the one-way ANOVA.
Here are a couple examples of when you might conduct a Kruskal-Wallis test:
<b>Example 1: Comparing Study Techinques</b>
You randomly split up a class of 90 students into three groups of 30. Each group uses a different studying technique for one month to prepare for an exam.
At the end of the month, all of the students take the same exam. You want to know whether or not the studying technique has an impact on exam scores.
From previous studies you know that the distributions of exam scores for these three studying techniques are not normally distributed so you conduct a Kruskal-Wallis test to determine if there is a statistically significant difference between the median scores of the three groups.
<b>Example 2: Comparing Sunlight Exposure</b>
You want to know whether or not sunlight impacts the growth of a certain plant, so you plant groups of seeds in four different locations that experience either high sunlight, medium sunlight, low sunlight or no sunlight.
After one month you measure the height of each group of plants. It is known that the distribution of heights for this certain plant is not normally distributed and is prone to outliers.
To determine if sunlight impacts growth, you conduct a Kruskal-Wallis test to determine if there is a statistically significant difference between the median height of the four groups.
<h2>Kruskal-Wallis Test Assumptions</h2>
Before we can conduct a Kruskal-Wallis test, we need to make sure the following assumptions are met:
<b>1. Ordinal or Continuous Response Variable </b>– the response variable should be an ordinal or continuous variable. An example of an ordinal variable is a survey response question measured on a Likert Scale (e.g. a 5-point scale from “strongly disagree” to “strongly agree”) and an example of a continuous variable is weight (e.g. measured in pounds).
<b>2. Independence</b> – the observations in each group need to be independent of each other. Usually a randomized design will take care of this.
<b>3. Distributions have similar shapes</b> – the distributions in each group need to have a similar shape.
If these assumptions are met, then we can proceed with conducting a Kruskal-Wallis test.
<h2>Example of a Kruskal-Wallis Test</h2>
A researcher wants to know whether or not three drugs have different effects on knee pain, so he recruits 30 individuals who all experience similar knee pain and randomly splits them up into three groups to receive either Drug 1, Drug 2, or Drug 3.
After one month of taking the drug, the researcher asks each individual to rate their knee pain on a scale of 1 to 100, with 100 indicating the most severe pain.
The ratings for all 30 individuals are shown below:
<table>
<thead><tr>
<th style="text-align: center;">Drug 1</th>
<th style="text-align: center;">Drug 2</th>
<th style="text-align: center;">Drug 3</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">71</td>
<td style="text-align: center;">57</td>
</tr>
<tr>
<td style="text-align: center;">65</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;">63</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">58</td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">78</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">65</td>
</tr>
<tr>
<td style="text-align: center;">78</td>
<td style="text-align: center;">31</td>
<td style="text-align: center;">61</td>
</tr>
<tr>
<td style="text-align: center;">70</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">62</td>
</tr>
<tr>
<td style="text-align: center;">61</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">44</td>
</tr>
<tr>
<td style="text-align: center;">50</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">44</td>
<td style="text-align: center;">42</td>
<td style="text-align: center;">77</td>
</tr>
</tbody>
</table>
The researcher wants to know whether or not the three drugs have different effects on knee pain, so he conducts a Kruskal-Wallis Test using a .05 significance level to determine if there is a statistically significant difference between the median knee pain ratings across these three groups.
We can use the following steps to perform the Kruskal-Wallis Test:
<b>Step 1. State the hypotheses. </b>
<b>The null hypothesis (H<sub>0</sub>):</b> The median knee-pain ratings across the three groups are equal.
<b>The alternative hypothesis: (Ha):</b> At least one of the median knee-pain ratings is different from the others.
<b>Step 2. Perform the Kruskal-Wallis Test.</b>
To conduct a Kruskal-Wallis Test, we can simply enter the values shown above into the  Kruskal-Wallis Test Calculator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/kruskalCalc1.png">
Then click the “Calculate” button:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/01/kruskalCalc2.png">
<b>Step 3. Interpret the results. </b>
Since the  p-value  of the test (<b>0.21342</b>) is not less than 0.05, we fail to reject the null hypothesis.
We do not have sufficient evidence to say that there is a statistically significant difference between the median knee pain ratings across these three groups.
<h2><span class="orange">Kuder-Richardson Formula 20 (Definition & Example)</span></h2>
The <b>Kuder-Richardson Formula 20</b>, often abbreviated KR-20, is used to measure the  internal consistency  reliability of a test in which each question only has two answers: right or wrong.
The Kuder-Richardson Formula 20 is as follows:
<b>KR-20 = (k / (k-1)) * (1 – Σp<sub>j</sub>q<sub>j</sub> / σ<sup>2</sup>)</b>
where:
<b>k</b>: Total number of questions
<b>p<sub>j</sub></b>: Proportion of individuals who answered question j correctly
<b>q<sub>j</sub></b>: Proportion of individuals who answered question j incorrectly
<b>σ<sup>2</sup></b>: Variance of scores for all individuals who took the test
The value for KR-20 ranges from 0 to 1, with higher values indicating higher reliability.
The following example shows how to calculate the value for KR-20 in practice.
<h3>Example: Calculating Kuder-Richardson Formula 20</h3>
Suppose we administer a test with 7 questions to 10 students.
The results of the test are listed below in Excel, with 1 indicating a correct answer and 0 indicating an incorrect answer:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kr1-1.jpg"629">
The following screenshot shows how to calculate the KR-20 value for this test:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/kr2-1.jpg"596">
Here are the formulas used in various cells:
<b>B13</b>: =SUM(B2:B11) / 10
<b>B14</b>: =1-B13
<b>B15</b>: =B13*B14
<b>B17</b>: =COUNTA(B1:H1)
<b>B18</b>: =SUM(B15:H15)
<b>B19</b>: =VAR.S(I2:I11)
<b>B20</b>: =(B17/(B17-1))*(1-B18/B19)
The KR-20 value turns out to be <b>0.0603</b>.
Since this value is extremely low, this indicates that the test has low reliability.
This means the questions may need to be re-written or re-phrased in such a way that the reliability of the test can be increased.
<h2><span class="orange">How to Perform Label Encoding in Python (With Example)</span></h2>
Often in machine learning, we want to convert  categorical variables  into some type of numeric format that can be readily used by algorithms.
One way to do this is through <b>label encoding</b>, which assigns each categorical value an integer value based on alphabetical order.
For example, the following screenshot shows how to convert each unique value in a categorical variable called <b>Team</b> into an integer value based on alphabetical order:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/labelencode2-1.jpg"467">
You can use the following syntax to perform label encoding in Python:
<b>from sklearn.preprocessing import LabelEncoder
#create instance of label encoder
lab = LabelEncoder()
#perform label encoding on 'team' column
df['my_column'] = lab.fit_transform(df['my_column'])
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Label Encoding in Python</h2>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'],   'points': [25, 12, 15, 14, 19, 23, 25, 29]})
#view DataFrame
print(df)
  team  points
0    A      25
1    A      12
2    B      15
3    B      14
4    B      19
5    B      23
6    C      25
7    C      29</b>
We can use the following code to perform label encoding to convert each categorical value in the <b>team</b> column into an integer value:
<b>from sklearn.preprocessing import LabelEncoder
#create instance of label encoder
lab = LabelEncoder()
#perform label encoding on 'team' column
df['team'] = lab.fit_transform(df['team'])
#view updated DataFrame
print(df)
   team  points
0     0      25
1     0      12
2     1      15
3     1      14
4     1      19
5     1      23
6     2      25
7     2      29
</b>
From the output we can see:
Each “A” value has been converted to <b>0</b>.
Each “B” value has been converted to <b>1</b>.
Each “C” value has been converted to <b>2</b>.
Note that you can also use the <b>inverse_transform()</b> function to obtain the original values from the <b>team</b> column:
<b>#display original team labels
lab.inverse_transform(df['team'])
array(['A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'], dtype=object)
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Python:
 How to Convert Categorical Variable to Numeric in Pandas 
 How to Convert Boolean Values to Integer Values in Pandas 
 How to Use factorize() to Encode Strings as Numbers in Pandas 
<h2><span class="orange">How to Perform Label Encoding in R (With Examples)</span></h2>
Often in machine learning, we want to convert  categorical variables  into some type of numeric format that can be readily used by algorithms.
One way to do this is through <b>label encoding</b>, which assigns each categorical value an integer value based on alphabetical order.
For example, the following screenshot shows how to convert each unique value in a categorical variable called <b>Team</b> into an integer value based on alphabetical order:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/labelencode2-1.jpg"467">
There are two common ways to perform label encoding in R:
<b>Method 1: Use Base R</b>
<b>df$my_var &lt;- as.numeric(factor(df$my_var))
</b>
<b>Method 2: Use CatEncoders Package</b>
<b>library(CatEncoders)
#define original categorical labels
labs = LabelEncoder.fit(df$my_var)
#convert labels to numeric values
df$team = transform(labs, df$my_var)</b>
The following examples show how to use each method in practice.
<h2>Example 1: Label Encoding Using Base R</h2>
The following code shows how to use the <b>factor()</b> function from base R to convert a categorical variable called <b>team</b> into a numeric variable:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'), points=c(25, 12, 15, 14, 19, 23, 25, 29))
#view data frame
df
  team points
1    A     25
2    A     12
3    B     15
4    B     14
5    B     19
6    B     23
7    C     25
8    C     29
#perform label encoding on team variable
df$team &lt;- as.numeric(factor(df$team))
#view updated data frame
df
  team points
1    1     25
2    1     12
3    2     15
4    2     14
5    2     19
6    2     23
7    3     25
8    3     29
</b>
Notice the new values in the <b>team</b> column:
“A” has become <b>1</b>.
“B” has become <b>2</b>.
“C” has become <b>3</b>.
We have successfully converted the <b>team</b> column from a categorical variable into a numeric variable.
<h2>Example 2: Label Encoding Using CatEncoders Package</h2>
The following code shows how to use functions from the <b>CatEncoders()</b> package to convert a categorical variable called <b>team</b> into a numeric variable:
<b>library(CatEncoders)
#create data frame
df &lt;- data.frame(team=c('A', 'A', 'B', 'B', 'B', 'B', 'C', 'C'), points=c(25, 12, 15, 14, 19, 23, 25, 29))
#define original categorical labels
labs = LabelEncoder.fit(df$team)
#convert labels to numeric values
df$team = transform(labs, df$team)
#view updated data frame
df
  team points
1    1     25
2    1     12
3    2     15
4    2     14
5    2     19
6    2     23
7    3     25
8    3     29</b>
Once again, we have generated the following new values in the <b>team</b> column:
“A” has become <b>1</b>.
“B” has become <b>2</b>.
“C” has become <b>3</b>.
This matches the results from the previous example.
Note that using this method, you can also use <b>inverse.transform()</b> to obtain the original values from the <b>team</b> column:
<b>#display original team labels
inverse.transform(labs, df$team)
[1] "A" "A" "B" "B" "B" "B" "C" "C"
</b>
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Create Categorical Variable from Continuous in R 
 How to Create Categorical Variables in R 
 How to Convert Categorical Variables to Numeric in R 
<h2><span class="orange">Label Encoding vs. One Hot Encoding: What’s the Difference?</span></h2>
Often in machine learning, we want to convert  categorical variables  into some type of numeric format that can be readily used by algorithms.
There are two common ways to convert categorical variables into numeric variables:
<b>1. Label Encoding:</b> Assign each categorical value an integer value based on alphabetical order.
<b>2. One Hot Encoding:</b> Create new variables that take on values 0 and 1 to represent the original categorical values.
For example, suppose we have the following dataset with two variables and we would like to convert the <b>Team</b> variable from a categorical variable into a numeric one:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/labelencode1.jpg"164">
The following examples show how to use both <b>label encoding</b> and <b>one hot encoding</b> to do so.
<h2>Example: Using Label Encoding</h2>
Using <b>label encoding</b>, we would convert each unique value in the <b>Team</b> column into an integer value based on alphabetical order:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/labelencode2-1.jpg"467">
In this example, we can see:
Each “A” value has been converted to <b>0</b>.
Each “B” value has been converted to <b>1</b>.
Each “C” value has been converted to <b>2</b>.
We have successfully converted the <b>Team</b> column from a categorical variable into a numeric variable.
<h2>Example: Using One Hot Encoding</h2>
Using <b>one hot encoding</b>, we would convert the <b>Team</b> column into new variables that contain only 0 and 1 values:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/oneHot1.png">
When using this approach, we create one new column for each unique value in the original categorical variable.
For example, the categorical variable <b>Team</b> had <b>three unique values</b> so we created <b>three new columns</b> in the dataset that all contain 0 or 1 values.
Here’s how to interpret the values in the new columns:
The value in the new <b>Team_A</b> column is 1 if the original value in the <b>Team</b> column was A. Otherwise, the value is 0.
The value in the new <b>Team_B</b> column is 1 if the original value in the <b>Team</b> column was B. Otherwise, the value is 0.
The value in the new <b>Team_C</b> column is 1 if the original value in the <b>Team</b> column was C. Otherwise, the value is 0.
We have successfully converted the <b>Team</b> column from a categorical variable into three numeric variables – sometimes referred to as “dummy” variables.
<b>Note</b>: When using these “dummy” variables in a regression model or other machine learning algorithm, be sure to avoid the  dummy variable trap .
<h2>When to Use Label Encoding vs. One Hot Encoding</h2>
In most scenarios, <b>one hot encoding</b> is the preferred way to convert a categorical variable into a numeric variable because <b>label encoding</b> makes it seem that there is a ranking between values.
For example, consider when we used label encoding to convert team into a numeric variable:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/labelencode2-1.jpg"467">
The label encoded data makes it seem like team C is somehow greater or larger than teams B and A since it has a higher numeric value.
This isn’t an issue if the original categorical variable actually is an  ordinal variable  with a natural ordering or ranking, but in many scenarios this isn’t the case.
However, one drawback of <b>one hot encoding</b> is that it requires you to make as many new variables as there are unique values in the original categorical variable.
This means that if your categorical variable has 100 unique values, you’ll have to create 100 new variables when using one hot encoding.
Depending on the size of your dataset and the type of variables you’re working with, you may prefer one hot encoding or label encoding.
<h2>Additional Resources</h2>
The following tutorials explain how to perform <b>label encoding</b> in practice:
 How to Perform Label Encoding in R 
 How to Perform Label Encoding in Python 
The following tutorials explain how to perform <b>one hot encoding</b> in practice:
 How to Perform One-Hot Encoding in R 
 How to Perform One-Hot Encoding in Python 
<h2><span class="orange">How to Label Outliers in Boxplots in ggplot2</span></h2>
This tutorial provides a step-by-step example of how to label  outliers  in boxplots in ggplot2.
<h2>Step 1: Create the Data Frame</h2>
First, let’s create the following data frame that contains information on points scored by 60 different basketball players on three different teams:
<b>#make this example reproducible
set.seed(1)
#create data frame
df &lt;- data.frame(team=rep(c('A', 'B', 'C'), each=20), player=rep(LETTERS[1:20], times=3), points=round(rnorm(n=60, mean=30, sd=10), 2))
#view head of data frame
head(df)
  team player points
1    A      A  23.74
2    A      B  31.84
3    A      C  21.64
4    A      D  45.95
5    A      E  33.30
6    A      F  21.80
</b>
<b>Note</b>: We used the  set.seed()  function to ensure that this example is reproducible.
<h2>Step 2: Define a Function to Identify Outliers</h2>
In ggplot2, an observation is defined as an outlier if it meets one of the following two requirements:
The observation is 1.5 times the interquartile range less than the first quartile (Q1)
The observation is 1.5 times the interquartile range greater than the third quartile (Q3).
We can create the following function in R to label observations as outliers if they meet one of these two requirements:
<b>find_outlier &lt;- function(x) {
  return(x &lt; quantile(x, .25) - 1.5*IQR(x) | x > quantile(x, .75) + 1.5*IQR(x))
}
</b>
<b>Related:</b>  How to Interpret Interquartile Range 
<h2>Step 3: Label Outliers in Boxplots in ggplot2</h2>
Next, we can use the following code to label outliers in boxplots in ggplot2:
<b>library(ggplot2)
library(dplyr)
#add new column to data frame that indicates if each observation is an outlier
df &lt;- df %>%
        group_by(team) %>%
        mutate(outlier = ifelse(find_outlier(points), points, NA))
#create box plot of points by team and label outliers
ggplot(df, aes(x=team, y=points)) +
  geom_boxplot() +
  geom_text(aes(label=outlier), na.rm=TRUE, hjust=-.5)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/outlier1.jpg">
Notice that two outliers are labeled in the plot.
The first outlier is a player on team A who scored <b>7.85</b> points and the other outlier is a player on team B who scored <b>10.11</b> points.
Note that we could also use a different variable to label these outliers.
For example, we could swap out <b>points</b> for <b>player</b> in the <b>mutate()</b> function to instead label the outliers based on the player name:
<b>library(ggplot2)
library(dplyr)
#add new column to data frame that indicates if each observation is an outlier
df &lt;- df %>%
        group_by(team) %>%
        mutate(outlier = ifelse(find_outlier(points), player, NA))
#create box plot of points by team and label outliers
ggplot(df, aes(x=team, y=points)) +
  geom_boxplot() +
  geom_text(aes(label=outlier), na.rm=TRUE, hjust=-.5)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/outlier2.jpg"479">
The outlier on team A now has a label of <b>N</b> and the outlier on team B now has a label of <b>D</b>, since these represent the player names who have outlier values for points.
<b>Note</b>: The  hjust  argument in <b>geom_text()</b> is used to push the label horizontally to the right so that it doesn’t overlap the dot in the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Change Font Size in ggplot2 
 How to Remove a Legend in ggplot2 
 How to Rotate Axis Labels in ggplot2 
<h2><span class="orange">How to Label Points on a Scatterplot in R (With Examples)</span></h2>
This tutorial provides an example of how to label the points on a scatterplot in both base R and ggplot2.
<h3>Example 1: Label Scatterplot Points in Base R</h3>
To add labels to scatterplot points in base R you can use the <b>text()</b> function, which uses the following syntax:
<b>text(x, y, labels, …)</b>
<b>x:</b> The x-coordinate of the labels
<b>y:</b> The y-coordinate of the labels
<b>labels:</b> The text to use for the labels
The following code shows how to label a single point on a scatterplot in base R:
<b>#create data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6), y=c(7, 9, 14, 19, 12, 15), z=c('A', 'B', 'C', 'D', 'E', 'F'))
#create scatterplot
plot(df$x, df$y)
#add label to third point in dataset
text(df$x[3], df$y[3]-1, labels=df$z[3])
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/labelscatter1.png">
The following code shows how to label every point on a scatterplot in base R:
<b>#create data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6), y=c(7, 9, 14, 19, 12, 15), z=c('A', 'B', 'C', 'D', 'E', 'F'))
#create scatterplot
plot(df$x, df$y)
#add labels to every point
text(df$x, df$y-1, labels=df$z)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/labelscatter2.png">
<h3>Example 2: Label Scatterplot Points in ggplot2</h3>
The following code shows how to label a single point on a scatterplot in ggplot2:
<b>#load ggplot2
library(ggplot2)
#create data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6), y=c(7, 9, 14, 19, 12, 15), z=c('A', 'B', 'C', 'D', 'E', 'F'))
#create scatterplot with a label on the third point in dataset
ggplot(df, aes(x,y)) +
  geom_point() +
  annotate('text', x = 3, y = 13.5, label = 'C')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/labelscatter3.png">
The following code shows how to label every point on a scatterplot in ggplot2:
<b>#load ggplot2 & ggrepel for easy annotations
library(ggplot2)
library(ggrepel)
#create data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6), y=c(7, 9, 14, 19, 12, 15), z=c('A', 'B', 'C', 'D', 'E', 'F'))
#create scatterplot with a label on every point
ggplot(df, aes(x,y)) +
  geom_point() +
  geom_text_repel(aes(label = z))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/labelscatter4.png">
<h2><span class="orange">How to Perform a Lack of Fit Test in R (Step-by-Step)</span></h2>
A <b>lack of fit test</b> is used to determine whether or not a full  regression model  offers a significantly better fit to a dataset than some reduced version of the model.
For example, suppose we would like to use <em>number of hours studied</em> to predict <em>exam score</em> for students at a certain college. We may decide to fit the following two regression models:
<b>Full Model:</b> Score = β<sub>0</sub> + B<sub>1</sub>(hours) + B<sub>2</sub>(hours)<sup>2</sup>
<b>Reduced Model:</b> Score = β<sub>0</sub> + B<sub>1</sub>(hours)
The following step-by-step example shows how to perform a lack of fit test in R to determine if the full model offers a significantly better fit than the reduced model.
<h3>Step 1: Create & Visualize a Dataset</h3>
First, we’ll use the following code to create a dataset that contains the number of hours studied and exam score received for 50 students:
<b>#make this example reproducible
set.seed(1)
#create dataset
df &lt;- data.frame(hours = runif(50, 5, 15), score=50)
df$score = df$score + df$hours^3/150 + df$hours*runif(50, 1, 2)
#view first six rows of data
head(df)
      hours    score
1  7.655087 64.30191
2  8.721239 70.65430
3 10.728534 73.66114
4 14.082078 86.14630
5  7.016819 59.81595
6 13.983897 83.60510</b>
Next, we’ll create a scatterplot to visualize the relationship between hours and score:
<b>#load ggplot2 visualization package
library(ggplot2)
#create scatterplot
ggplot(df, aes(x=hours, y=score)) +
  geom_point()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lackFit1.png">
<h3>Step 2: Fit Two Different Models to the Dataset</h3>
Next, we’ll fit two different regression models to the dataset:
<b>#fit full model
full &lt;- lm(score ~ poly(hours,2), data=df)
#fit reduced model
reduced &lt;- lm(score ~ hours, data=df) 
</b>
<h3>Step 3: Perform a Lack of Fit Test</h3>
Next, we’ll use the <b>anova()</b> command to perform a lack of fit test between the two models:
<b>#lack of fit test
anova(full, reduced)
Analysis of Variance Table
Model 1: score ~ poly(hours, 2)
Model 2: score ~ hours
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1     47 368.48                                
2     48 451.22 -1   -82.744 10.554 0.002144 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
</b>
The F test-statistic turns out to be <b>10.554</b> and the corresponding p-value is <b>0.002144</b>. Since this p-value is less than .05, we can reject the null hypothesis of the test and conclude that the full model offers a statistically significantly better fit than the reduced model.
<h3>Step 4: Visualize the Final Model</h3>
Lastly, we can visualize the final model (the full model) relative to the original dataset:
<b>ggplot(df, aes(x=hours, y=score)) + 
          geom_point() +
          stat_smooth(method='lm', formula = y ~ poly(x,2), size = 1) + 
          xlab('Hours Studied') +
          ylab('Score')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lackFit2.png">
We can see that the curve of the model fits the data quite well.
<h2><span class="orange">lapply vs. sapply in R: What’s the Difference?</span></h2>
The <b>lapply()</b> function in R can be used to apply a function to each element of a list, vector, or data frame and obtain a <b>list</b> as a result.
The <b>sapply()</b> function can also be used to apply a function to each element of a list, vector, or data frame but it returns a <b>vector</b> as a result.
The following examples show how to use each of these functions in R.
<h3>Example: How to Use lapply() in R</h3>
The following code shows how to use the <b>lapply()</b> function to multiply each value in each column of a data frame by 2:
<b>#create data frame
df &lt;- data.frame(x=c(1, 2, 2, 3, 5), y=c(4, 4, 6, 7, 8), z=c(7, 7, 9, 9, 9))
#view data frame
df
  x y z
1 1 4 7
2 2 4 7
3 2 6 9
4 3 7 9
5 5 8 9
#multiply each value in each column by 2
lapply(df, function(df) df*2)
$x
[1]  2  4  4  6 10
$y
[1]  8  8 12 14 16
$z
[1] 14 14 18 18 18</b>
Notice that the result is a list.
<h3>Example: How to Use sapply() in R</h3>
The following code shows how to use the <b>sapply()</b> function to multiply each value in each column of a data frame by 2:
<b>#create data frame
df &lt;- data.frame(x=c(1, 2, 2, 3, 5), y=c(4, 4, 6, 7, 8), z=c(7, 7, 9, 9, 9))
#view data frame
df
  x y z
1 1 4 7
2 2 4 7
3 2 6 9
4 3 7 9
5 5 8 9
#multiply each value in each column by 2
sapply(df, function(df) df*2)
      x  y  z
[1,]  2  8 14
[2,]  4  8 14
[3,]  4 12 18
[4,]  6 14 18
[5,] 10 16 18</b>
Notice that the result is a matrix of vectors.
Note that you can use <b>as.data.frame()</b> to return a data frame as a result instead of a matrix:
<b>#multiply each value in each column by 2 and return a data frame
as.data.frame(sapply(df, function(df) df*2))
   x  y  z
1  2  8 14
2  4  8 14
3  4 12 18
4  6 14 18
5 10 16 18</b>
<h3>When to Use lapply() vs. sapply()</h3>
In 99% of cases, you’ll use <b>sapply()</b> because it makes the most sense to return a vector or matrix as a result.
However, in some rare circumstances you may need to use <b>lapply()</b> instead if you need the result to be a list.
Note that <b>sapply()</b> and <b>lappy()</b> perform the same operations on a vector, matrix, or data frame. The only difference is the class of the object that is returned.
<h2><span class="orange">How to Use LARGE IF Function in Excel (With Examples)</span></h2>
You can use the following formulas to perform a LARGE IF function in Excel:
<b>Formula 1: LARGE IF with One Criteria</b>
<b>=LARGE(IF(A2:A16="A",C2:C16),2)
</b>
This formula finds the 2nd largest value in C2:C16 where the value in A2:A16 is equal to “A”.
<b>Formula 2: LARGE IF with Multiple Criteria</b>
<b>=LARGE(IF((A2:A16="A")*(B2:B16="Guard"),C2:C16),2)
</b>
This formula finds the 2nd largest value in C2:C16 where the value in A2:A16 is equal to “A” <b>and</b> the value in B2:B16 is equal to “Guard”.
The following examples show how to use each formula in practice with the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/largeifexcel1.jpg"456">
<h3>Example 1: LARGE IF with One Criteria</h3>
We can use the following formula to find the 2nd largest value in C2:C16 where the value in A2:A16 is equal to “A”:
<b>=LARGE(IF(A2:A16="A",C2:C16),2)
</b>
The following screenshot shows how to use this formula: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/largeifexcel2.jpg">
This tells us that the 2nd largest points value among all players on team A is <b>14</b>.
<h3>Example 2: LARGE IF with Multiple Criteria</h3>
We can use the following formula to find the 2nd largest value in C2:C16 where the value in A2:A16 is equal to “A” <b>and</b> the value in B2:B16 is equal to “Guard”:
<b>=LARGE(IF((A2:A16="A")*(B2:B16="Guard"),C2:C16),2)</b>
The following screenshot shows how to use this formula: 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/largeifexcel3.jpg">
This tells us that the 2nd largest points value among all Guards on team A is <b>7</b>.
<h2><span class="orange">How to Use a LARGE IF Formula in Google Sheets</span></h2>
You can use the following methods to create a <b>LARGE IF</b> formula in Google Sheets:
<b>Method 1: LARGE IF with One Criteria</b>
<b>=ArrayFormula(LARGE(IF(A2:A11="value",C2:C11),2))
</b>
This formula finds the second largest value in column <b>C</b> where the value in column <b>A</b> is equal to “value.”
<b>Method 2: LARGE IF with Multiple Criteria</b>
<b>=ArrayFormula(LARGE(IF((A2:A11="value1")*(B2:B11="value2")=1,C2:C11),5))
</b>
This formula finds the fifth largest value in column <b>C</b> where the value in column <b>A</b> is equal to “value1” and the value in column B is equal to “value2.”
The following examples show how to use each method with the following dataset in in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/largeif1.jpg"553">
<h3>Example 1: LARGE IF with One Criteria</h3>
We can use the following formula to calculate the second largest value in the <b>Points</b> column only for the rows where the <b>Team </b>column is equal to “Spurs”:
<b>=ArrayFormula(LARGE(IF(A2:A11="Spurs",C2:C11),2))</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/largeif2.jpg">
We can see that the second largest <b>Points</b> value among rows where <b>Team</b> is equal to “Spurs” is <b>26</b>.
<h3>Example 2: LARGE IF with Multiple Criteria</h3>
We can use the following formula to calculate the second largest value in the <b>Points</b> column only for the rows where the <b>Team </b>column is equal to “Spurs” and the <b>Position</b> column is equal to “Guard”:
<b>=ArrayFormula(LARGE(IF((A2:A11="Spurs")*(B2:B11="Guard")=1,C2:C11),2))</b>
The following screenshot shows how to use this syntax in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/largeif3.jpg"666">
We can see that the second largest <b>Points</b> value among rows where <b>Team</b> is equal to “Spurs” and <b>Position</b> is equal to “Guard” is <b>24</b>.
<b>Note</b>: You can find the complete documentation for the <b>LARGE</b> function in Google Sheets  here .
<h2><span class="orange">The Large Sample Condition: Definition & Example</span></h2>
In statistics, we’re often interested in using  samples  to draw inferences about populations through  hypothesis tests  or  confidence intervals .
Most of the formulas that we use in hypothesis tests and confidence intervals make the assumption that a given sample roughly follows a  normal distribution .
However, in order to safely make this assumption we need to make sure our sample size is large enough. Specifically, we need to make sure that the <b>Large Sample Condition </b>is met.
<b>The Large Sample Condition:</b> The sample size is at least 30.
<em><b>Note: </b>In some textbooks, a “large enough” sample size is defined as at least 40 but the number 30 is more commonly used.</em>
When this condition is met, it can be assumed that  the sampling distribution of the sample mean  is approximately normal. This assumption allows us to use samples to draw inferences about the populations from which they came from.
The reason why the number 30 is used is based upon the Central Limit Theorem. You can read more about that in this  blog post .
<h3>Example: Verifying the Large Sample Condition</h3>
Suppose a certain machine creates crackers. The distribution of the weight of these cookies is skewed to the right with a mean of 10 ounces and a standard deviation of 2 ounces. If we take a simple random sample of 100 cookies produced by this machine, what is the probability that the mean weight of the cookies in this sample is less than 9.8 ounces?
In order to answer this question, we can use the  Normal CDF Calculator , but we first need to verify that the sample size is large enough in order to assume that the distribution of the sampling mean is normal.
In this example, our sample size is <b>n = 100</b>, which is much larger than 30. Despite the fact that the true distribution of the weight of the cookies is skewed to the right, since our sample size is “large enough” we can assume that the distribution of the sampling mean is normal. Thus, we would be safe to use the Normal CDF Calculator to solve this problem.
<h3>Modifications to the Large Sample Condition</h3>
Often a sample size is considered “large enough” if it’s greater than or equal to 30, but this number can vary a bit based on the underlying shape of the population distribution.
In particular:
If the population distribution is symmetric, sometimes a sample size as small as 15 is sufficient.
If the population distribution is skewed, generally a sample size of at least 30 is needed.
If the population distribution is extremely skewed, then a sample size of 40 or higher may be necessary.
Depending on the shape of the population distribution, you may require more or less than a sample size of 30 in order for the Central Limit Theorem to apply.
<h2><span class="orange">Lasso Regression in Python (Step-by-Step)</span></h2>
 Lasso regression  is a method we can use to fit a regression model when  multicollinearity  is present in the data.
In a nutshell, least squares regression tries to find coefficient estimates that minimize the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)2</b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
Conversely, lasso regression seeks to minimize the following:
<b>RSS + λΣ|β<sub>j</sub>|</b>
where <em>j</em> ranges from 1 to <em>p</em> predictor variables and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>. In lasso regression, we select a value for λ that produces the lowest possible test MSE (mean squared error).
This tutorial provides a step-by-step example of how to perform lasso regression in Python.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import the necessary packages to perform lasso regression in Python:
<b>import pandas as pd
from numpy import arange
from sklearn.linear_model import LassoCV
from sklearn.model_selection import RepeatedKFold</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use a dataset called <b>mtcars</b>, which contains information about 33 different cars. We’ll use <b>hp</b> as the response variable and the following variables as the predictors:
mpg
wt
drat
qsec
The following code shows how to load and view this dataset:
<b>#define URL where data is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data_full = pd.read_csv(url)
#select subset of data
data = data_full[["mpg", "wt", "drat", "qsec", "hp"]]
#view first six rows of data
data[0:6]
mpgwtdratqsechp
021.02.6203.9016.46110
121.02.8753.9017.02110
222.82.3203.8518.6193
321.43.2153.0819.44110
418.73.4403.1517.02175
518.13.4602.7620.22105</b>
<h3>Step 3: Fit the Lasso Regression Model</h3>
Next, we’ll use the  LassoCV()  function from sklearn to fit the lasso regression model and we’ll use the  RepeatedKFold()  function to perform k-fold cross-validation to find the optimal alpha value to use for the penalty term.
<em><b>Note:</b> The term “alpha” is used instead of “lambda” in Python.</em>
For this example we’ll choose k = 10 folds and repeat the cross-validation process 3 times.
Also note that LassoCV() only tests alpha values 0.1, 1, and 10 by default. However, we can define our own alpha range from 0 to 1 by increments of 0.01:
<b>#define predictor and response variables
X = data[["mpg", "wt", "drat", "qsec"]]
y = data["hp"]
#define cross-validation method to evaluate model
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
#define model
model = LassoCV(alphas=arange(0, 1, 0.01), cv=cv, n_jobs=-1)
#fit model
model.fit(X, y)
#display lambda that produced the lowest test MSE
print(model.alpha_)
0.99</b>
The lambda value that minimizes the test MSE turns out to be <b>0.99</b>.
<h3>Step 4: Use the Model to Make Predictions</h3>
Lastly, we can use the final lasso regression model to make predictions on new observations. For example, the following code shows how to define a new car with the following attributes:
mpg: 24
wt: 2.5
drat: 3.5
qsec: 18.5
The following code shows how to use the fitted lasso regression model to predict the value for <em>hp</em> of this new observation:
<b>#define new observation
new = [24, 2.5, 3.5, 18.5]
#predict hp value using lasso regression model
model.predict([new])
array([105.63442071])
</b>
Based on the input values, the model predicts this car to have an <em>hp</em> value of <b>105.63442071</b>.
You can find the complete Python code used in this example  here .
<h2><span class="orange">Lasso Regression in R (Step-by-Step)</span></h2>
 Lasso regression  is a method we can use to fit a regression model when  multicollinearity  is present in the data.
In a nutshell, least squares regression tries to find coefficient estimates that minimize the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)2</b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
Conversely, lasso regression seeks to minimize the following:
<b>RSS + λΣ|β<sub>j</sub>|</b>
where <em>j</em> ranges from 1 to <em>p</em> predictor variables and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>. In lasso regression, we select a value for λ that produces the lowest possible test MSE (mean squared error).
This tutorial provides a step-by-step example of how to perform lasso regression in R.
<h3>Step 1: Load the Data</h3>
For this example, we’ll use the R built-in dataset called <b>mtcars</b>. We’ll use <b>hp</b> as the response variable and the following variables as the predictors:
mpg
wt
drat
qsec
To perform lasso regression, we’ll use functions from the <b>glmnet</b> package. This package requires the  response variable  to be a vector and the set of predictor variables to be of the class <b>data.matrix</b>.
The following code shows how to define our data:
<b>#define response variable
y &lt;- mtcars$hp
#define matrix of predictor variables
x &lt;- data.matrix(mtcars[, c('mpg', 'wt', 'drat', 'qsec')])
</b>
<h3>Step 2: Fit the Lasso Regression Model</h3>
Next, we’ll use the <b>glmnet()</b> function to fit the lasso regression model and specify <b>alpha=1</b>.
Note that setting alpha equal to 0 is equivalent to using  ridge regression  and setting alpha to some value between 0 and 1 is equivalent to using an elastic net.<b> </b>
To determine what value to use for lambda, we’ll perform  k-fold cross-validation  and identify the lambda value that produces the lowest test mean squared error (MSE).
Note that the function <b>cv.glmnet()</b> automatically performs k-fold cross validation using k = 10 folds.
<b>library(glmnet)
#perform k-fold cross-validation to find optimal lambda value
cv_model &lt;- cv.glmnet(x, y, alpha = 1)
#find optimal lambda value that minimizes test MSE
best_lambda &lt;- cv_model$lambda.min
best_lambda
[1] 5.616345
#produce plot of test MSE by lambda value
plot(cv_model) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/lassoR1.png">
The lambda value that minimizes the test MSE turns out to be <b>5.616345</b>.
<h3>Step 3: Analyze Final Model</h3>
Lastly, we can analyze the final model produced by the optimal lambda value.
We can use the following code to obtain the coefficient estimates for this model:
<b>#find coefficients of best model
best_model &lt;- glmnet(x, y, alpha = 1, lambda = best_lambda)
coef(best_model)
5 x 1 sparse Matrix of class "dgCMatrix"   s0
(Intercept) 484.20742
mpg          -2.95796
wt           21.37988
drat          .      
qsec        -19.43425</b>
No coefficient is shown for the predictor <b>drat</b> because the lasso regression shrunk the coefficient all the way to zero. This means it was completely dropped from the model because it wasn’t influential enough.
Note that this is a key difference between  ridge regression  and  lasso regression . Ridge regression shrinks all coefficients <em>towards</em> zero, but lasso regression has the potential to remove predictors from the model by shrinking the coefficients <em>completely</em> to zero.
We can also use the final lasso regression model to make predictions on new observations. For example, suppose we have a new car with the following attributes:
mpg: 24
wt: 2.5
drat: 3.5
qsec: 18.5
The following code shows how to use the fitted lasso regression model to predict the value for <em>hp</em> of this new observation:
<b>#define new observation
new = matrix(c(24, 2.5, 3.5, 18.5), nrow=1, ncol=4) 
#use lasso regression model to predict response value
predict(best_model, s = best_lambda, newx = new)
[1,] 109.0842</b>
Based on the input values, the model predicts this car to have an <em>hp</em> value of <b>109.0842</b>.
Lastly, we can calculate the  R-squared of the model  on the training data:
<b>#use fitted best model to make predictions
y_predicted &lt;- predict(best_model, s = best_lambda, newx = x)
#find SST and SSE
sst &lt;- sum((y - mean(y))^2)
sse &lt;- sum((y_predicted - y)^2)
#find R-Squared
rsq &lt;- 1 - sse/sst
rsq
[1] 0.8047064
</b>
The R-squared turns out to be <b>0.8047064</b>. That is, the best model was able to explain <b>80.47%</b> of the variation in the response values of the training data.
You can find the complete R code used in this example  here .
<h2><span class="orange">Introduction to Lasso Regression</span></h2>
In ordinary  multiple linear regression , we use a set of <em>p</em> predictor variables and a  response variable  to fit a model of the form:
<b>Y = β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub> + ε</b>
where:
<b>Y</b>: The response variable
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The average effect on Y of a one unit increase in X<sub>j</sub>, holding all other predictors fixed
<b>ε</b>: The error term
The values for β<sub>0</sub>, β<sub>1</sub>, B<sub>2</sub>, … , β<sub>p</sub> are chosen using <b>the least square method</b>, which minimizes the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)<sup>2</sup></b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
However, when the predictor variables are highly correlated then  multicollinearity  can become a problem. This can cause the coefficient estimates of the model to be unreliable and have high variance. That is, when the model is applied to a new set of data it hasn’t seen before, it’s likely to perform poorly.
One way to get around this issue is to use a method known as <b>lasso regression</b>, which instead seeks to minimize the following:
<b>RSS + λΣ|β<sub>j</sub>|</b>
where <em>j</em> ranges from 1 to <em>p</em> and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>.
When λ = 0, this penalty term has no effect and lasso regression produces the same coefficient estimates as least squares.
However, as λ approaches infinity the shrinkage penalty becomes more influential and the predictor variables that aren’t importable in the model get shrunk towards zero and some even get dropped from the model.
<h3>Why Use Lasso Regression?</h3>
The advantage of lasso regression compared to least squares regression lies in the  bias-variance tradeoff .
Recall that mean squared error (MSE) is a metric we can use to measure the accuracy of a given model and it is calculated as:
MSE = Var(<em>f<U+0302>(</em>x<sub>0</sub>)) + [Bias(<em>f<U+0302>(</em>x<sub>0</sub>))]<sup>2</sup> + Var(ε)
MSE = Variance + Bias<sup>2</sup> + Irreducible error
The basic idea of lasso regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall MSE.
To illustrate this, consider the following chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridge1.png">
Notice that as λ increases, variance drops substantially with very little increase in bias. Beyond a certain point, though, variance decreases less rapidly and the shrinkage in the coefficients causes them to be significantly underestimated which results in a large increase in bias.
We can see from the chart that the test MSE is lowest when we choose a value for λ that produces an optimal tradeoff between bias and variance.
When λ = 0, the penalty term in lasso regression has no effect and thus it produces the same coefficient estimates as least squares. However, by increasing λ to a certain point we can reduce the overall test MSE.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/lasso1.png">
This means the model fit by lasso regression will produce smaller test errors than the model fit by least squares regression.
<h3>Lasso Regression vs. Ridge Regression</h3>
Lasso regression and  ridge regression  are both known as <em>regularization methods</em> because they both attempt to minimize the sum of squared residuals (RSS) along with some penalty term.
In other words, they constrain or <em>regularize</em> the coefficient estimates of the model.
However, the penalty terms they use are a bit different:
Lasso regression attempts to minimize <b>RSS + λΣ|β<sub>j</sub>|</b>
Ridge regression attempts to minimize <b>RSS + λΣβ<sub>j</sub><sup>2</sup></b>
When we use ridge regression, the coefficients of each predictor are shrunken towards zero but none of them can go <em>completely to zero</em>.
Conversely, when we use lasso regression it’s possible that some of the coefficients could go <em>completely to zero</em> when λ gets sufficiently large.
In technical terms, lasso regression is capable of producing “sparse” models – models that only include a subset of the predictor variables.
This begs the question: <b>Is ridge regression or lasso regression better?</b>
The answer: It depends!
In cases where only a small number of predictor variables are significant, lasso regression tends to perform better because it’s able to shrink insignificant variables completely to zero and remove them from the model.
However, when many predictor variables are significant in the model and their coefficients are roughly equal then ridge regression tends to perform better because it keeps all of the predictors in the model.
To determine which model is better at making predictions, we perform  k-fold cross-validation . Whichever model produces the lowest test mean squared error (MSE) is the preferred model to use.
<h3>Steps to Perform Lasso Regression in Practice</h3>
The following steps can be used to perform lasso regression:
<b>Step 1: Calculate the correlation matrix and VIF values for the predictor variables.</b>
First, we should produce a  correlation matrix  and calculate the  VIF (variance inflation factor) values  for each predictor variable.
If we detect high correlation between predictor variables and high VIF values (some texts define a “high” VIF value as 5 while others use 10) then lasso regression is likely appropriate to use.
However, if there is no multicollinearity present in the data then there may be no need to perform lasso regression in the first place. Instead, we can perform ordinary least squares regression.
<b>Step 2: Fit the lasso regression model and choose a value for λ.</b>
Once we determine that lasso regression is appropriate to use, we can fit the model (using popular programming languages like R or Python) using the optimal value for λ.
To determine the optimal value for λ, we can fit several models using different values for λ and choose λ to be the value that produces the lowest test MSE.
<b>Step 3: Compare lasso regression to ridge regression and ordinary least squares regression.</b>
Lastly, we can compare our lasso regression model to a ridge regression model and least squares regression model to determine which model produces the lowest test MSE by using k-fold cross-validation.
Depending on the relationship between the predictor variables and the response variable, it’s entirely possible for one of these three models to outperform the others in different scenarios.
<h3>Lasso Regression in R & Python</h3>
The following tutorials explain how to perform lasso regression in R and Python:
 Lasso Regression in R (Step-by-Step) 
 Lasso Regression in Python (Step-by-Step) 
<h2><span class="orange">What is Latin Hypercube Sampling?</span></h2>
<b>Latin hypercube sampling</b> is a method that can be used to sample random numbers in which samples are distributed evenly over a sample space.
It is widely used to generate samples that are known as <em>controlled random samples</em> and is often applied in Monte Carlo analysis because it can dramatically reduce the number of simulations needed to achieve accurate results.
<h3>Introductory Example</h3>
To wrap your head around the idea of latin hypercube sampling, consider the following simple example:
Suppose we’d like to obtain a sample of 2 values from a dataset that is normally distributed with a mean of 0 and a standard deviation of 1.
If we used a true random number generator to obtain this sample, it’s possible that both values could be greater than 0 or that both values could be less than 0.
However, if we used latin hypercube sampling to obtain this sample then it would be guaranteed that one value would be above 0 and one would be below 0 because we could specifically partition the  sample space  into one region with values above 0 and one region with values below 0, then select a random sample from each region.
<h3>One-Dimensional Latin Hypercube Sampling</h3>
The idea behind one-dimensional latin hypercube sampling is simple: Divide a given  CDF  into <em>n</em> different regions and randomly choose one value from each region to obtain a sample of size <em>n</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/latin1.png">
The benefit of this approach is that it ensures that at least one value from each region is included in the sample.
<h3>Two-Dimensional Latin Hypercube Sampling</h3>
We can easily extend the idea of one-dimensional latin hypercube sampling into two dimensions as well.
For two variables, x and y, we can divide the sample space of each variable into <em> n </em>evenly spaced regions and pick a random sample from each sample space to obtain random values across two dimensions.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/latin2.png">
It’s important to note that the two variables must be independent for this sampling technique to achieve the desired results.
<h3>N-Dimensional Latin Hypercube Sampling</h3>
To perform latin hypercube sampling in greater dimensions, we can simply extend the idea of two-dimensional latin hypercube sampling into even more dimensions.
Each variable is simply split into evenly spaced regions and random samples are then chosen from each region to obtain a controlled random sample.
<b>Related:</b>  What is High Dimensional Data? 
<h3>Why Use Latin Hypercube Sampling?</h3>
The main advantage of latin hypercube sampling is that it produces samples that reflect the true underlying distribution and it tends to require much smaller sample sizes than  simple random sampling . 
This method of sampling can be particularly advantageous if you’re working with data that has a high number of dimensions and you need to obtain random samples that are sure to reflect the true underlying distribution of the data.
<h2><span class="orange">Law of Large Numbers: Definition + Examples</span></h2>
The <b>law of large numbers </b>states that as a sample size becomes larger, the  sample mean  gets closer to the expected value.
The most basic example of this involves flipping a coin. Each time we flip a coin, the probability that it lands on heads is 1/2. Thus, the <em>expected </em>proportion of heads that will appear over an infinite number of flips is 1/2 or <b>0.5</b>. 
However, if we flip a coin 10 times we might find that it only lands on heads 3 times. Since 10 flips is a small sample size, there’s no guarantee that the proportion of heads will be close to <b>0.5</b>.
If we continue flipping the coin another 10 times, we might find that it lands on heads a total of 9 times out of 20. If we flip it 10 more times, we might find that it lands on heads 22 times out of 30. 
As we flip the coin more and more, the proportion of times that it lands on heads will converge to the expected proportion of <b>0.5</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/lln1.png">
This simple idea of the law of large numbers is applied by many types of businesses and industries in real life.
<h3>The Law of Large Numbers in Casinos</h3>
Casinos rely on the law of large numbers to reliably produce profits. For most games, the casino wins about 51-55% of the time. This means that individuals can get lucky and win a decent amount from time to time, but over the course of tens of thousands of individual players, the casino will win the expected 51-55% of the time.
For example, Jessica might play a few games at the casino and win $50. 
Mike might play a few games as well and lose $70.
John might play a few games and win $25.
Susan might play a few games and lose $40.
Some players will win money and some will lose money, but because of the way the games are designed the casinos can be sure that they’ll win over the course of thousands of individuals.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/lln2.png">
<h3>The Law of Large Numbers in Insurance</h3>
Insurance companies also rely on the law of large numbers to remain profitable.
The basic idea is that insurance companies can provide insurance to thousands of individuals who pay a certain premium each month and only a small percentage of the individuals they ensure will actually need to use the insurance to pay for large unexpected expenses.
For example, 1,000 people might each pay $1,000 per year for insurance, which results in a profit of $1,000,000 for an insurance company.
However, 90 people might each need to receive $10,000 from the insurance company to cover unexpected expenses from various accidents, which results in a $900,000 loss for the insurance company.
In the end, the insurance company earns a profit of $1,000,000 – $900,000 = <b>$100,000</b>.
This means that the insurance company can expect to earn a fairly predictable profit, on average, across thousands of individuals.
Note that this business model works because an insurance company ensures a <b>large number of people</b>. If they only ensured 10 people it would be far too risky because a large unexpected expense could wipe out the business.
Thus, insurance companies rely on the law of large numbers to predictably forecast their profits.
<h3>The Law of Large Numbers in Renewable Energy</h3>
The law of large numbers is also used by renewable energy companies.
The basic idea is that wind turbines and solar panels can power generators to produce electricity in different parts of the company. However, it’s not also windy or sunny outside, which means the wind turbines and solar panels aren’t always able to produce reliable energy for the power generators.
The way renewable energy companies get around this problem is by hooking up tens of thousands of wind turbines or solar panels to a single power grid, which makes it much more likely that these energy sources will produce a reliable amount of power for the grid.
It also makes it much easier to predict just how much energy will be produced by these power sources, since engineers can simply take the expected average across tens of thousands of wind turbines or solar panels.
<em>An in-depth explanation of this phenomenon can be found in  this article by Scientific American .</em>
<h2><span class="orange">Law of Total Probability: Definition & Examples</span></h2>
In probability theory, <b>the law of total probability</b> is a useful way to find the probability of some event <em>A</em> when we don’t directly know the probability of <em>A</em> but we do know that events <em>B</em><sub>1</sub>, <em>B</em><sub>2</sub>, <em>B</em><sub>3</sub>… form a partition of the  sample space  <em>S</em>.
This law states the following:
<b>The Law of Total Probability</b>
 
If <em>B</em><sub>1</sub>, <em>B</em><sub>2</sub>, <em>B</em><sub>3</sub>… form a partition of the sample space <em>S</em>, then we can calculate the probability of event <em>A</em> as:
 
P(<em>A</em>) = ΣP(<em>A</em>|<em>B</em><sub>i</sub>)*P(<em>B</em><sub>i</sub>)
The easiest way to understand this law is with a simple example.
Suppose there are two bags in a box, which contain the following marbles:
<b>Bag 1:</b> 7 red marbles and 3 green marbles
<b>Bag 2:</b> 2 red marbles and 8 green marbles
If we randomly select one of the bags and then randomly select one marble from that bag, what is the probability that it’s a green marble?
In this example, let P(<em>G</em>) = probability of choosing a green marble. This is the probability that we’re interested in, but we can’t compute it directly.
Instead we need to use the conditional probability of <em>G</em>, given some events <em>B</em> where the <em>B</em><sub>i</sub>‘s form a partition of the sample space <em>S</em>. In this example, we have the following conditional probabilities:
P(G|B<sub>1</sub>) = 3/10 = 0.3
P(G|B<sub>2</sub>) = 8/10 = 0.8
Thus, using the law of total probability we can calculate the probability of choosing a green marble as:
P(G) = ΣP(G|B<sub>i</sub>)*P(B<sub>i</sub>)
P(G) = P(G|B<sub>1</sub>)*P(B<sub>1</sub>) + P(G|B<sub>2</sub>)*P(B<sub>2</sub>)
P(G) = (0.3)*(0.5) + (0.8)*(0.5)
P(G) = <b>0.55</b>
If we randomly select one of the bags and then randomly select one marble from that bag, the probability we choose a green marble is <b>0.55</b>.
Read through the next two examples to solidify your understanding of the law of total probability.
<h3>Example 1: Widgets</h3>
Company A supplies 80% of widgets for a car shop and only 1% of their widgets turn out to be defective. Company B supplies the remaining 20% of widgets for the car shop and 3% of their widgets turn out to be defective.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lawtot1.png">
If a customer randomly purchases a widget from the car shop, what is the probability that it will be defective?
If we let P(<em>D</em>) = the probability of a widget being defective and <em>P(B</em><sub>i</sub>) be the probability that the widget came from one of the companies, then we can compute the probability of buying a defective widget as:
P(D) = ΣP(D|B<sub>i</sub>)*P(B<sub>i</sub>)
P(D) = P(D|B<sub>1</sub>)*P(B<sub>1</sub>) + P(D|B<sub>2</sub>)*P(B<sub>2</sub>)
P(D) = (0.01)*(0.80) + (0.03)*(0.20)
P(D) = <b>0.014</b>
If we randomly buy a widget from this car shop, the probability that it will be defective is <b>0.014</b>.
<h3>Example 2: Forests</h3>
Forest A occupies 50% of the total land in a certain park and 20% of the plants in this forest are poisonous. Forest B occupies 30% of the total land and 40% of the plants in it are poisonous. Forest C occupies the remaining 20% of the land and 70% of the plants in it are poisonous.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lawtot2.png">
If we randomly enter this park and pick a plant from the ground, what is the probability that it will be poisonous?
If we let P(<em>P</em>) = the probability of the plant being poisonous, and <em>P(B<sub>i</sub>)</em> be the probability that we’ve entered one of the three forests, then we can compute the probability of a randomly chosen plant being poisonous as:
P(P) = ΣP(P|B<sub>i</sub>)*P(B<sub>i</sub>)
P(P) = P(P|B<sub>1</sub>)*P(B<sub>1</sub>) + P(P|B<sub>2</sub>)*P(B<sub>2</sub>) + P(P|B<sub>3</sub>)*P(B<sub>3</sub>)
P(P) = (0.20)*(0.50) + (0.40)*(0.30) + (0.70)*(0.20)
P(P) = <b>0.36</b>
If we randomly pick a plant from the ground, the probability that it will be poisonous is <b>0.36</b>.
<h2><span class="orange">Leave-One-Out Cross-Validation in Python (With Examples)</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.
One commonly used method for doing this is known as  leave-one-out cross-validation (LOOCV) , which uses the following approach:
<b>1.</b> Split a dataset into a training set and a testing set, using all but one observation as part of the training set.
<b>2.</b> Build a model using only data from the training set.
<b>3.</b> Use the model to predict the response value of the one observation left out of the model and calculate the mean squared error (MSE).
<b>4.</b> Repeat this process <em>n</em> times. Calculate the test MSE to be the average of all of the test MSE’s.
This tutorial provides a step-by-step example of how to perform LOOCV for a given model in Python.
<h3>Step 1: Load Necessary Libraries</h3>
First, we’ll load the necessary functions and libraries for this example:
<b>from sklearn.model_selection import train_test_split
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from numpy import mean
from numpy import absolute
from numpy import sqrt
import pandas as pd
</b>
<h3>Step 2: Create the Data</h3>
Next, we’ll create a pandas DataFrame that contains two predictor variables, x<sub>1</sub> and x<sub>2</sub>, and a single response variable y.
<b>df = pd.DataFrame({'y': [6, 8, 12, 14, 14, 15, 17, 22, 24, 23],   'x1': [2, 5, 4, 3, 4, 6, 7, 5, 8, 9],   'x2': [14, 12, 12, 13, 7, 8, 7, 4, 6, 5]})
</b>
<h3>Step 3: Perform Leave-One-Out Cross-Validation</h3>
Next, we’ll then fit a  multiple linear regression model  to the dataset and perform LOOCV to evaluate the model performance.
<b>#define predictor and response variables
X = df[['x1', 'x2']]
y = df['y']
#define cross-validation method to use
cv = LeaveOneOut()
#build multiple linear regression model
model = LinearRegression()
#use LOOCV to evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error',         cv=cv, n_jobs=-1)
#view mean absolute error
mean(absolute(scores))
3.1461548083469726
</b>
From the output we can see that the mean absolute error (MAE) was <b>3.146</b>. That is, the average absolute error between the model prediction and the actual observed data is 3.146.
In general, the lower the MAE, the more closely a model is able to predict the actual observations.
Another commonly used metric to evaluate model performance is the root mean squared error (RMSE). The following code shows how to calculate this metric using LOOCV:
<b>#define predictor and response variables
X = df[['x1', 'x2']]
y = df['y']
#define cross-validation method to use
cv = LeaveOneOut()
#build multiple linear regression model
model = LinearRegression()
#use LOOCV to evaluate model
scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error',         cv=cv, n_jobs=-1)
#view RMSE
sqrt(mean(absolute(scores)))
3.619456476385567</b>
From the output we can see that the root mean squared error (RMSE) was <b>3.619</b>. The lower the RMSE, the more closely a model is able to predict the actual observations.
In practice we typically fit several different models and compare the RMSE or MAE of each model to decide which model produces the lowest test error rates and is therefore the best model to use.
<h2><span class="orange">Leave-One-Out Cross-Validation in R (With Examples)</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.
One commonly used method for doing this is known as  leave-one-out cross-validation (LOOCV) , which uses the following approach:
<b>1.</b> Split a dataset into a training set and a testing set, using all but one observation as part of the training set.
<b>2.</b> Build a model using only data from the training set.
<b>3.</b> Use the model to predict the response value of the one observation left out of the model and calculate the mean squared error (MSE).
<b>4.</b> Repeat this process <em>n</em> times. Calculate the test MSE to be the average of all of the test MSE’s.
The easiest way to perform LOOCV in R is by using the  trainControl()  function from the <b>caret</b> library in R.
This tutorial provides a quick example of how to use this function to perform LOOCV for a given model in R.
<h3>Example: Leave-One-Out Cross-Validation in R</h3>
Suppose we have the following dataset in R:
<b>#create data frame
df &lt;- data.frame(y=c(6, 8, 12, 14, 14, 15, 17, 22, 24, 23), x1=c(2, 5, 4, 3, 4, 6, 7, 5, 8, 9), x2=c(14, 12, 12, 13, 7, 8, 7, 4, 6, 5))
#view data frame
df
yx1x2
6214
8512
12412
14313
1447
1568
1777
2254
2486
2395
</b>
The following code shows how to fit a  multiple linear regression model  to this dataset in R and perform LOOCV to evaluate the model performance:
<b>library(caret)
#specify the cross-validation method
ctrl &lt;- trainControl(method = "LOOCV")
#fit a regression model and use LOOCV to evaluate performance
model &lt;- train(y ~ x1 + x2, data = df, method = "lm", trControl = ctrl)
#view summary of LOOCV               
print(model)
Linear Regression 
10 samples
 2 predictor
No pre-processing
Resampling: Leave-One-Out Cross-Validation 
Summary of sample sizes: 9, 9, 9, 9, 9, 9, ... 
Resampling results:
  RMSE      Rsquared   MAE     
  3.619456  0.6186766  3.146155
Tuning parameter 'intercept' was held constant at a value of TRUE
</b>
Here is how to interpret the output:
10 different samples were used to build 10 models. Each model used 2 predictor variables.
No pre-processing occured. That is, we didn’t  scale the data  in any way before fitting the models.
The resampling method we used to generate the 10 samples was Leave-One-Out Cross Validation.
The sample size for each training set was 9.
<b>RMSE:</b> The root mean squared error. This measures the average difference between the predictions made by the model and the actual observations. The lower the RMSE, the more closely a model can predict the actual observations.
<b>Rsquared:</b> This is a measure of the correlation between the predictions made by the model and the actual observations. The higher the R-squared, the more closely a model can predict the actual observations.
<b>MAE:</b> The mean absolute error. This is the average absolute difference between the predictions made by the model and the actual observations. The lower the MAE, the more closely a model can predict the actual observations.
Each of the three metrics provided in the output (RMSE, R-squared, and MAE) give us an idea of how well the model performed on previously unseen data.
In practice we typically fit several different models and compare the three metrics provided by the output seen here to decide which model produces the lowest test error rates and is therefore the best model to use.
<h2><span class="orange">A Quick Intro to Leave-One-Out Cross-Validation (LOOCV)</span></h2>
To evaluate the performance of a model on a dataset, we need to measure how well the predictions made by the model match the observed data.
The most common way to measure this is by using the mean squared error (MSE), which is calculated as:
MSE = (1/n)*Σ(y<sub>i</sub> – f(x<sub>i</sub>))<sup>2</sup>
where:
<b>n:</b> Total number of observations
<b>y<sub>i</sub>:</b> The response value of the i<sup>th</sup> observation
<b>f(x<sub>i</sub>):</b> The predicted response value of the i<sup>th</sup> observation
The closer the model predictions are to the observations, the smaller the MSE will be.
In practice, we use the following process to calculate the MSE of a given model:
<b>1.</b> Split a dataset into a training set and a testing set.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv1.png">
<b>2.</b> Build the model using only data from the training set.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv2.png">
<b>3.</b> Use the model to make predictions on the testing set and measure the MSE – this is know as the <b>test MSE</b>.<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv3.png">
The test MSE gives us an idea of how well a model will perform on data it hasn’t previously seen, i.e. data that wasn’t used to “train” the model.
However, the drawback of using only one testing set is that the test MSE can vary greatly depending on which observations were used in the training and testing sets.
It’s possible that if we use a different set of observations for the training set and the testing set that our test MSE could turn out to be much larger or smaller.
One way to avoid this problem is to fit a model several times using a different training and testing set each time, then calculating the test MSE to be the average of all of the test MSE’s.
This general method is known as cross-validation and a specific form of it is known as <b>leave-one-out cross-validation</b>.
<h3>Leave-One-Out Cross Validation</h3>
<b>Leave-one-out cross-validation</b> uses the following approach to evaluate a model:
<b>1. </b>Split a dataset into a training set and a testing set, using all but one observation as part of the training set:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv4.png">
Note that we only leave one observation “out” from the training set. This is where the method gets the name “leave-one-out” cross-validation.
<b>2.</b> Build the model using only data from the training set.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv5.png">
<b>3.</b> Use the model to predict the response value of the one observation left out of the model and calculate the MSE.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/loocv6.png">
<b>4. </b>Repeat the process <em>n</em> times.
Lastly, we repeat this process <em>n</em> times (where <em>n</em> is the total number of observations in the dataset), leaving out a different observation from the training set each time.
We then calculate the test MSE to be the average of all of the test MSE’s:
<b>Test MSE = (1/n)*ΣMSE<sub>i</sub></b>
where:
<b>n:</b> The total number of observations in the dataset
<b>MSEi: </b>The test MSE during the i<sup>th</sup> time of fitting the model.
<h3>Pros & Cons of LOOCV</h3>
Leave-one-out cross-validation offers the following <b>pros</b>:
It provides a much less biased measure of test MSE compared to using a single test set because we repeatedly fit a model to a dataset that contains <em>n-1</em> observations.
It tends not to overestimate the test MSE compared to using a single test set.
However, leave-one-out cross-validation comes with the following <b>cons:</b>
It can be a time-consuming process to use when <em>n</em> is large.
It can also be time-consuming if a model is particularly complex and takes a long time to fit to a dataset.
It can be computationally expensive.
Fortunately, modern computing has become so efficient in most fields that LOOCV is a much more reasonable method to use compared to many years ago.
Note that LOOCV can be used in both  regression and classification  settings as well. For regression problems, it calculates the test MSE to be the mean squared difference between predictions and observations while in classification problems it calculates the test MSE to be the percentage of observations correctly classified during the <em>n</em> repeated model fittings.
<h3>How to Perform LOOCV in R & Python</h3>
The following tutorials provide step-by-step examples of how to perform LOOCV for a given model in R and Python:
 Leave-One-Out Cross-Validation in R 
 Leave-One-Out Cross-Validation in Python 
<h2><span class="orange">How to Perform a Left Join in Excel (With Example)</span></h2>
A <b>left join</b> allows you to join together two tables in which every row in the left table is kept and only the rows who have a matching value in a particular column of the right table are kept.
The following step-by-step example shows how to use the <b>VLOOKUP</b> function to perform a left join in Excel.
<h2>Step 1: Enter the Values for Each Table</h2>
First, let’s enter the following values for two tables in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/left1.jpg"560">
We will perform a left join in which we keep all rows from the left table and only join in the rows from the right table that have matching values in the <b>Team</b> column.
<h2>Step 2: Create a Copy of the First Table</h2>
Next, let’s copy and paste the values of the first table into a new cell range:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/left2.jpg"600">
<h2>Step 3: Perform Left Join Using VLOOKUP</h2>
Next, we’ll type the following formula into cell C13:
<b>=VLOOKUP(A2, $E$2:$G$9, {2,3}, FALSE)
</b>
We’ll then drag and fill this formula down to each remaining cell in column C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/left3.jpg">
Feel free to add the column headers <b>Assists</b> and <b>Rebounds</b> as well:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/07/left4.jpg"525">
The left join is now complete.
Each row from the left table is kept and only the rows with matching team names in the right table are kept.
If a given team didn’t have a match in the right table, then a <b>#N/A</b> value is shown for the Assists and Rebounds columns.
For example, the Mavs did exist in the right table, so they have corresponding values for Assists and Rebounds.
However, the Warriors did not exist in the right table, so they have <b>#N/A</b> for Assists and Rebounds.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Excel:
 Excel: How to Filter Cells that Contain Multiple Words 
 Excel: How to Filter a Column by Multiple Values 
 Excel Advanced Filter: Display Rows with Non-Blank Values 
<h2><span class="orange">How to Do a Left Join in R (With Examples)</span></h2>
You can use the <b>merge()</b> function to perform a left join in base R:
<b>#left join using base R
merge(df1,df2, all.x=TRUE)
</b>
You can also use the <b>left_join()</b> function from the  dplyr  package to perform a left join:
<b>#left join using dplyr
dplyr::left_join(df2, df1)</b>
<b>Note:</b> If you’re working with extremely large datasets, the <b>left_join()</b> function will tend to be faster than the <b>merge()</b> function.
The following examples show how to use each of these functions in practice with the following data frames:
<b>#define first data frame
df1 &lt;- data.frame(team=c('Mavs', 'Hawks', 'Spurs', 'Nets'),  points=c(99, 93, 96, 104))
df1
   team points
1  Mavs     99
2 Hawks     93
3 Spurs     96
4  Nets    104
#define second data frame
df2 &lt;- data.frame(team=c('Mavs', 'Hawks', 'Spurs', 'Nets'),  rebounds=c(25, 32, 38, 30),  assists=c(19, 18, 22, 25))
df2
   team rebounds assists
1  Mavs       25      19
2 Hawks       32      18
3 Spurs       38      22
4  Nets       30      25</b>
<h2>Example 1: Left Join Using Base R</h2>
We can use the <b>merge()</b> function in base R to perform a left join, using the ‘team’ column as the column to join on:
<b>#perform left join using base R
merge(df1, df2, by='team', all.x=TRUE)
   team points rebounds assists
1 Hawks     93       32      18
2  Mavs     99       25      19
3  Nets    104       30      25
4 Spurs     96       38      22
</b>
<h2>Example 2: Left Join Using dplyr</h2>
We can use the <b>left_join()</b> function from the dplyr package to perform a left join, using the ‘team’ column as the column to join on:
<b>library(dplyr)
#perform left join using dplyr 
left_join(df1, df2, by='team')
   team points rebounds assists
1  Mavs     99       25      19
2 Hawks     93       32      18
3 Spurs     96       38      22
4  Nets    104       30      25</b>
One difference you’ll notice between these two functions is that the <b>merge()</b> function automatically sorts the rows alphabetically based on the column you used to perform the join.
Conversely, the <b>left_join()</b> function preserves the original order of the rows from the first data frame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in R:
 How to Do an Inner Join in R 
 How to Perform Fuzzy Matching in R 
 How to Add a Column to Data Frame in R 
 How to Drop Columns from Data Frame in R 
<h2><span class="orange">How to Do a Left Join in Pandas (With Example)</span></h2>
You can use the following basic syntax to perform a left join in pandas:
<b>import pandas as pd
df1.merge(df2, on='column_name', how='left')
</b>
The following example shows how to use this syntax in practice.
<h2>Example: How to Do Left Join in Pandas</h2>
Suppose we have the following two pandas DataFrames that contains information about various basketball teams:
<b>import pandas as pd
#create DataFrame
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'],    'points': [18, 22, 19, 14, 14, 11, 20, 28]})
df2 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'G', 'H'],    'assists': [4, 9, 14, 13, 10, 8]})
#view DataFrames
print(df1)
  team  points
0    A      18
1    B      22
2    C      19
3    D      14
4    E      14
5    F      11
6    G      20
7    H      28
print(df2)
  team  assists
0    A        4
1    B        9
2    C       14
3    D       13
4    G       10
5    H        8</b>
We can use the following code to perform a left join, keeping all of the rows from the first DataFrame and adding any columns that match based on the <b>team</b> column in the second DataFrame:
<b>#perform left join
df1.merge(df2, on='team', how='left')
        teampointsassists
0A184.0
1B229.0
2C1914.0
3D1413.0
4E14NaN
5F11NaN
6G2010.0
7H288.0
</b>
Every team from the left DataFrame (<b>df1</b>) is returned in the merged DataFrame and only the rows in the right DataFrame (<b>df2</b>) that match a team name in the left DataFrame are returned.
Notice that the two teams in <b>df2</b> (teams E and F) that do not match a team name in <b>df1</b> simply return a <b>NaN</b> value in the assists column of the merged DataFrame.
Note that you can also use <b>pd.merge()</b> with the following syntax to return the exact same result:
<b>#perform left join
pd.merge(df1, df2, on='team', how='left')
        teampointsassists
0A184.0
1B229.0
2C1914.0
3D1413.0
4E14NaN
5F11NaN
6G2010.0
7H288.0</b>
Notice that this merged DataFrame matches the one from the previous example.
<b>Note</b>: You can find the complete documentation for the <b>merge</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Perform an Anti-Join in Pandas 
 How to Perform an Inner Join in Pandas 
 How to Perform a Cross Join in Pandas 
<h2><span class="orange">Left Skewed vs. Right Skewed Distributions</span></h2>
<b>Skewness</b> is a way to describe the symmetry of a distribution.
A distribution is <b>left skewed</b> if it has a “tail” on the left side of the distribution:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew2.png">
A distribution is <b>right skewed</b> if it has a “tail” on the right side of the distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew1.png">
And a distribution has <b>no skew</b> if it’s symmetrical on both sides:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew3.png">
Note that left skewed distributions are sometimes called “negatively-skewed” distributions and right skewed distributions are sometimes called “positively-skewed” distributions.
<h3>Properties of Skewed Distributions</h3>
The following diagrams show where the mean, median and mode are typically located in different distributions.
<b>Left Skewed Distribution:</b> Mean &lt; Median &lt; Mode
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew4.png">
In a left skewed distribution, the mean is less than the median.
<b>Right Skewed Distribution:</b> Mode &lt; Median &lt; Mean
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew6.png">
In a right skewed distribution, the mean is greater than the median.
<b>No Skew:</b> Mean = Median = Mode
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew5.png">
In a  symmetrical distribution , the mean, median, and mode are all equal.
<h3>Using Box Plots to Visualize Skewness</h3>
A <b>box plot</b> is a type of plot that displays the five number summary of a dataset, which includes:
The minimum value
The first quartile (the 25th percentile)
The median value
The third quartile (the 75th percentile)
The maximum value
To make a box plot, we draw a box from the first to the third quartile. Then we draw a vertical line at the median. Lastly, we draw “whiskers” from the quartiles to the minimum and maximum value.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/boxplot.png">
Depending on the location of the median value in the boxplot, we can tell whether or not a distribution is left skewed, right skewed, or symmetrical.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew7.png">
When the median is closer to the bottom of the box and the whisker is shorter on the lower end of the box, the distribution is right skewed.
When the median is closer to the top of the box and the whisker is shorter on the upper end of the box, the distribution is left skewed.
When the median is in the middle of the box and the whiskers are roughly equal on each side, the distribution is symmetrical.
<h3>Examples of Skewed Distributions</h3>
Here are some real-life examples of skewed distributions.
<b>Left-Skewed Distribution:</b> The distribution of age of deaths.
The distribution of the age of deaths in most populations is left-skewed. Most people live to be between 70 and 80 years old, with fewer and fewer living less than this age.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew9.png">
<b>Right-Skewed Distribution:</b> The distribution of household incomes.
The distribution of household incomes in the U.S. is right-skewed, with most households earning between $40k and $80k per year but with a long right tail of households that earn much more.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew10.png">
<b>No Skew:</b> The distribution of male heights.
It’s well-known that the height of males is roughly normally distributed and has no skew. For example, the average height of a male in the U.S. is roughly 69.1 inches. The distribution of heights is roughly symmetrical, with some being shorter and some being taller.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/skew8.png">
<h2><span class="orange">How to Identify a Left Tailed Test vs. a Right Tailed Test</span></h2>
In statistics, we use  hypothesis tests  to determine whether some claim about a  population parameter  is true or not.
Whenever we perform a hypothesis test, we always write a <b>null hypothesis</b> and an <b>alternative hypothesis</b>, which take the following forms:
<b>H<sub>0</sub></b> (Null Hypothesis): Population parameter = ≤, ≥ some value
<b>H<sub>A</sub></b> (Alternative Hypothesis): Population parameter &lt;, >, ≠ some value
There are three different types of hypothesis tests:
Two-tailed test: The alternative hypothesis contains the “≠” sign
Left-tailed test: The alternative hypothesis contains the “&lt;” sign
Right-tailed test: The alternative hypothesis contains the “>” sign
Notice that we only have to look at the sign in the alternative hypothesis to determine the type of hypothesis test.
<b>Left-tailed test:</b> The alternative hypothesis contains the “&lt;” sign
 
<b>Right-tailed test:</b> The alternative hypothesis contains the “>” sign
The following examples show how to identify left-tailed and right-tailed tests in practice.
<h2>Example: Left-Tailed Test</h2>
Suppose it’s assumed that the average weight of a certain widget produced at a factory is 20 grams. However, one inspector believes the true average weight is less than 20 grams.
To test this, he weighs a  simple random sample  of 20 widgets and obtains the following information:
n = <b>20</b> widgets
x = <b>19.8</b> grams
s = <b>3.1</b> grams
He then performs a hypothesis test using the following null and alternative hypotheses:
<b>H<sub>0</sub></b> (Null Hypothesis): μ ≥ 20 grams
<b>H<sub>A</sub></b> (Alternative Hypothesis): μ &lt; 20 grams
The test statistic is calculated as:
<em>t </em> = (x – μ) / (s/√n)
<em>t</em> = (19.8-20) / (3.1/√20)
<em>t</em> = -.2885
According to the  t-Distribution table , the t critical value at α = .05 and n-1 = 19 degrees of freedom is –<b>1.729</b>.
Since the test statistic is not less than this value, the inspector fails to reject the null hypothesis. He does not have sufficient evidence to say that the true mean weight of widgets produced at this factory is less than 20 grams.
<h2>Example: Right-Tailed Test</h2>
Suppose it’s assumed that the average height of a certain species of plant is 10 inches tall. However, one botanist claims the true average height is greater than 10 inches.
To test this claim, she goes out and measures the height of a  simple random sample  of 15 plants and obtains the following information:
n = <b>15</b> plants
x = <b>11.4</b> inches
s = <b>2.5</b> inches
She then performs a hypothesis test using the following null and alternative hypotheses:
<b>H<sub>0</sub></b> (Null Hypothesis): μ ≤ 10 inches
<b>H<sub>A</sub></b> (Alternative Hypothesis): μ > 10 inches
The test statistic is calculated as:
<em>t </em> = (x – μ) / (s/√n)
<em>t</em> = (11.4-10) / (2.5/√15)
<em>t</em> = 2.1689
According to the  t-Distribution table , the t critical value at α = .05 and n-1 = 14 degrees of freedom is <b>1.761</b>.
Since the test statistic is greater than this value, the botanist can reject the null hypothesis. She has sufficient evidence to say that the true mean height for this species of plant is greater than 10 inches.
<h2>Additional Resources</h2>
 How to Read the t-Distribution Table 
 One Sample t-test Calculator 
 Two Sample t-test Calculator 
<h2><span class="orange">How to Draw a Legend Outside of a Plot in R</span></h2>
The easiest way to draw a legend outside of a plot in base R is to add extra space to the right of the plot by using the following syntax:
<b>par(mar=c(5, 4, 4, 8), xpd=TRUE)
</b>
The following step-by-step example shows how to use this syntax in practice.
<h3>Step 1: Create the Data</h3>
First, let’s create some data to work with:
<b>#create data frames
df1 &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7),  y=c(2, 7, 19, 26, 24, 29, 31))
df2 &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7),  y=c(4, 4, 7, 9, 12, 13, 8))</b>
<h3>Step 2: Create a Plot with a Legend Outside of Plot</h3>
Next, let’s create a plot and add a legend outside of the plot in the top right corner:
<b>#add extra space to the right of the plot
par(mar=c(5, 4, 4, 8), xpd=TRUE)
#plot both data frames
plot(y ~ x, df1, pch=1, main="Scatterplot with multiple groups")
points(y ~ x, df2, pch=3)
#add legend outside of plot
legend("topright", inset=c(-0.2, 0), legend=c("df1","df2"), pch=c(1,3), title="Data")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/legendOutside1.png">
<h3>Step 3: Modify the Legend Location</h3>
The <b>inset(x, y)</b> argument can be used to control the location of the legend to the right of the plot. For example, we can make the <em>x</em> argument more negative to push the legend even further to the right:
<b>#add extra space to the right of the plot
par(mar=c(5, 4, 4, 8), xpd=TRUE)
#plot both data frames
plot(y ~ x, df1, pch=1, main="Scatterplot with multiple groups")
points(y ~ x, df2, pch=3)
#add legend outside of plot
legend("topright", inset=c(-0.3, 0), legend=c("df1","df2"), pch=c(1,3), title="Data")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/legendOutside2.png">
We can also make the <em>y</em> argument more positive to push the legend lower: 
<b>#add extra space to the right of the plot
par(mar=c(5, 4, 4, 8), xpd=TRUE)
#plot both data frames
plot(y ~ x, df1, pch=1, main="Scatterplot with multiple groups")
points(y ~ x, df2, pch=3)
#add legend outside of plot
legend("topright", inset=c(-0.3, .5), legend=c("df1","df2"), pch=c(1,3), title="Data")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/legendOutside3.png">
Feel free to modify the values in the <b>inset(x, y)</b> argument to modify the location of the legend.
You can find more R tutorials on  this page .
<h2><span class="orange">How to Use length() Function in R (4 Examples)</span></h2>
You can use the <b>length() </b>function in R to calculate the length of vectors, lists, and other objects.
This function uses the following basic syntax:
<b>length(x)
</b>
where:
<b>x</b>: The name of the object to calculate length for
The following examples show how to use this function in different scenarios.
<h3>Example 1: Use length() with Vector</h3>
The following code shows how to use the <b>length()</b> function to calculate the number of elements in a vector:
<b>#create vector
my_vector &lt;- c(2, 7, 6, 6, 9, 10, 14, 13, 4, 20, NA)
#calculate length of vector
length(my_vector)
[1] 11</b>
We can see that the vector has 11 total elements.
Note that <b>length()</b> also counts NA values.
To exclude NA values when calculating the length of a vector, we can use the following syntax:
<b>#create vector
my_vector &lt;- c(2, 7, 6, 6, 9, 10, 14, 13, 4, 20, NA)
#calculate length of vector, excluding NA values
sum(!is.na(my_vector))
[1] 10</b>
We can see that the vector has 10 elements that are non-NA values.
<h3>
<b>Example 2: Use length() with List</b>
</h3>
The following code shows how to use the <b>length() </b>function to calculate the length of an entire list along with the length of a specific element in a list:
<b>#create list
my_list &lt;- list(A=1:5, B=c('hey', 'hi'), C=c(3, 5, 7))
#calculate length of entire list
length(my_list)
[1] 3
#calculate length of first element in list
length(my_list[[1]])
[1] 5</b>
From the output we can see that the list has <b>3</b> total elements and we can see that the first element in the list has a length of <b>5</b>.
<h3>
<b>Example 3: Use length() with Data Frame</b>
</h3>
If we use the <b>length() </b>function with a data frame in R, it will return the number of columns in the data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'B', 'B', 'C', 'D'), points=c(10, 15, 29, 24, 30, 31))
#view data frame
df
  team points
1    A     10
2    B     15
3    B     29
4    B     24
5    C     30
6    D     31
#calculate length of data frame (returns number of columns)
length(df)
[1] 2 
</b>
If we would like to calculate the number of rows instead, we can use the <b>nrow()</b> function:
<b>#calculate number of rows in data frame
nrow(df)
[1] 6
</b>
This tells us that there are <b>6</b> total rows in the data frame.
<h3>
<b>Example 4: Use length() with String</b>
</h3>
If we use the <b>length() </b>function with a string in R, it will typically just return a value of one:
<b>#define string
my_string &lt;- "hey there"
#calculate length of string
length(my_string)
[1] 1</b>
To actually count the number of characters in a string, we can use the <b>nchar()</b> function instead:
<b>#define string
my_string &lt;- "hey there"
#calculate total characters in string
nchar(my_string)
[1] 9
</b>
This tells us that there are <b>9</b> total characters in the string, including spaces.
<h2><span class="orange">How to Fix: Length of values does not match length of index</span></h2>
One error you may encounter when using pandas is:
<b>ValueError: Length of values does not match length of index
</b>
This error occurs when you attempt to assign a NumPy array of values to a new column in a pandas DataFrame, yet the length of the array does not match the current length of the index.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#define DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14],   'assists': [5, 7, 13, 12]})
#view DataFrame
print(df)
   points  assists
0      25        5
1      12        7
2      15       13
3      14       12
</b>
Now suppose we attempt to add a new column called ‘rebounds’ as a NumPy array:
<b>import numpy as np
#attempt to add 'rebounds' column
df['rebounds'] = np.array([3, 3, 7])
ValueError: Length of values (3) does not match length of index (4)
</b>
We receive a <b>ValueError</b> because we attempt to add a NumPy array with a length of <b>3</b> to a DataFrame that has an index with a length of <b>4</b>.
<h3>How to Fix the Error</h3>
The easiest way to fix this error is to simply create a new column using a pandas Series as opposed to a NumPy array.
By default, if the length of the pandas Series does not match the length of the index of the DataFrame then NaN values will be filled in:
<b>#create 'rebounds' column
df['rebounds'] = pd.Series([3, 3, 7])
#view updated DataFrame
df
pointsassistsrebounds
02553.0
11273.0
215137.0
31412NaN</b>
Using a pandas Series, we’re able to successfully add the ‘rebounds’ column and the missing values are simply filled in with NaN.
Note that we can quickly convert the NaN values to some other value (such as zero) using the <b>fillna()</b> method as follows:
<b>#fill in NaN values with zero
df = df.fillna(0)
#view updated DataFrame
df
pointsassistsrebounds
02553.0
11273.0
215137.0
314120.0</b>
Notice that the NaN value has been converted to a zero.
<h2><span class="orange">How to Use LETTERS in R (With Examples)</span></h2>
You can use the <b>LETTERS</b> constant in R to access letters from the alphabet.
The following examples show the most common ways to use the <b>LETTERS</b> constant in practice.
<h2>Example 1: Generate Uppercase Letters</h2>
If you simply type <b>LETTERS</b>, every letter in the alphabet will be displayed in uppercase:
<b>#display every letter in alphabet in uppercase
LETTERS
 [1] "A" "B" "C" "D" "E" "F" "G" "H" "I" "J" "K" "L" "M" "N" "O" "P" "Q" "R" "S"
[20] "T" "U" "V" "W" "X" "Y" "Z"
</b>
To access a specific subset of letters in the alphabet, you can use the following syntax:
<b>#display letters in positions 4 through 8 in uppercase
LETTERS[4:8]
[1] "D" "E" "F" "G" "H"
</b>
Notice that only the letters in positions 4 through 8 are returned.
<h2>Example 2: Generate Lowercase Letters</h2>
If you type <b>letters</b>, every letter in the alphabet will be displayed in lowercase:
<b>#display every letter in alphabet in lowercase
letters
 [1] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" "k" "l" "m" "n" "o" "p" "q" "r" "s"
[20] "t" "u" "v" "w" "x" "y" "z"
</b>
To access a specific subset of letters in the alphabet, you can use the following syntax:
<b>#display letters in positions 4 through 8 in lowercase
letters[4:8]
[1] "d" "e" "f" "g" "h"
</b>
Notice that only the letters in positions 4 through 8 are returned.
<h2>Example 3: Generate Random Letters</h2>
You can randomly select one letter from the alphabet by using the <b>sample()</b> function:
<b>#select random uppercase letter from alphabet
sample(LETTERS, 1)
[1] "K"
</b>
You can also generate a random sequence of letters by using the <b>paste()</b> function along with the <b>sample()</b> function:
<b>#generate random sequence of 10 letters in uppercase
paste(sample(LETTERS, 10, replace=TRUE), collapse="")
[1] "BPTISQSOJI"
</b>
<h2>Example 4: Concatenate Letters with Other Strings</h2>
You can also use the <b>paste()</b> function to concatenate each letter in the alphabet with another string:
<b>#display each letter with "letter_" in front
paste("letter_", letters, sep="")
 [1] "letter_a" "letter_b" "letter_c" "letter_d" "letter_e" "letter_f"
 [7] "letter_g" "letter_h" "letter_i" "letter_j" "letter_k" "letter_l"
[13] "letter_m" "letter_n" "letter_o" "letter_p" "letter_q" "letter_r"
[19] "letter_s" "letter_t" "letter_u" "letter_v" "letter_w" "letter_x"
[25] "letter_y" "letter_z"
</b>
Notice that “letter_” has been concatenated to the beginning of each letter.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Remove Last Character from String in R 
 How to Find Location of Character in a String in R 
 How to Select Columns Containing a Specific String in R 
<h2><span class="orange">What Are Levels of an Independent Variable?</span></h2>
In an experiment, there are two types of variables:
<b>The independent variable: </b>The variable that an experimenter changes or controls so that they can observe the effects on the dependent variable.
<b>The dependent variable: </b>The variable being measured in an experiment that is “dependent” on the independent variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/ind_dep1.png"(max-width: 669px) 100vw, 669px">
In an experiment, a researcher wants to understand how changes in an independent variable affect a dependent variable.
When an independent variable has multiple experimental conditions, we say that there are <b>levels of the independent variable</b>.
For example, suppose a teacher wants to know how three different studying techniques affect exam scores. She randomly assigns 30 students each to use one of the three studying techniques for a week, then each student takes the exact same exam.
In this example, the independent variable is Studying Technique and it has <b>three levels</b>:
Technique 1
Technique 2
Technique 3
In other words, there are the three experimental conditions that the students can potentially be exposed to.
The dependent variable in this example is Exam Score, which is “dependent” on the studying technique used by the student.
The following examples illustrate a few more experiments that use independent variables with multiple levels.
<h3>Example 1: Advertising Spend</h3>
Suppose a marketer conducts an experiment in which he spends three different amounts of money (low, medium, high) on TV advertising to see how it affects the sales of a certain product.
In this experiment, we have the following variables:
<b>Independent Variable:</b> Advertising Spend
<b>3 Levels:</b>
Low
Medium
High
<b>Dependent Variable:</b> Total sales of the product
<h3>Example 2: Placebo vs. Medication</h3>
Suppose a doctor wants to know if a certain medication reduces blood pressure in patients. He recruits a  simple random sample  of 100 patients and randomly assigns 50 to use a pill that contains the real medication and 50 to use a pill that is actually just a placebo.
In this experiment, we have the following variables:
<b>Independent Variable:</b> Type of Medication
<b>2 Levels:</b>
True medication pill
Placebo pill
<b>Dependent Variable:</b> Overall change in blood pressure
<h3>Example 3: Plant Growth</h3>
Suppose a botanist uses five different fertilizers (We’ll call them A, B, C, D, E) in a field to determine if they have different effects on plant growth.
In this experiment, we have the following variables:
<b>Independent Variable:</b> Type of fertilizer 
<b>5 Levels:</b>
Fertilizer A
Fertilizer B
Fertilizer C
Fertilizer D
Fertilizer E
<b>Dependent Variable:</b> Plant growth
<h3>How to Analyze Levels of an Independent Variable</h3>
Typically we use a  one-way ANOVA  to determine if the levels of an independent variable cause different outcomes in a dependent variable.
A one-way ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0 </sub>(null):</b> All group means are equal
<b>H<sub>1 </sub>(alternative):</b> At least one group mean is different<sub> </sub>from the rest
For example, we could use a one-way ANOVA to determine if the five different types of fertilizer in the previous example lead to different mean growth rates for the plants.
If the p-value of the ANOVA is less than some significance level (e.g. α = .05), then we can reject the null hypothesis. This means we have sufficient evidence to say that the mean plant growth is not equal at all five levels of the fertilizer.
We could then proceed to conduct  post-hoc tests  to determine exactly which fertilizers lead to different mean growth rates.
<h2><span class="orange">Levels of Measurement: Nominal, Ordinal, Interval and Ratio</span></h2>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/levels_measurement.jpg">
In statistics, we use data to answer interesting questions. But not all data is created equal. There are actually four different <b>data measurement scales</b> that are used to categorize different types of data:
<b>1.</b> Nominal
<b>2.</b> Ordinal
<b>3.</b> Interval
<b>4.</b> Ratio
In this post, we define each measurement scale and provide examples of variables that can be used with each scale.
<h2>Nominal</h2>
The simplest measurement scale we can use to label variables is a <b>nominal scale</b>.
<b>Nominal scale:</b> A scale used to label variables that have no quantitative values.
Some examples of variables that can be measured on a nominal scale include:
<b>Gender:</b> Male, female
<b>Eye color:</b> Blue, green, brown
<b>Hair color:</b> Blonde, black, brown, grey, other
<b>Blood type:</b> O-, O+, A-, A+, B-, B+, AB-, AB+
<b>Political Preference: </b>Republican, Democrat, Independent
<b>Place you live:</b> City, suburbs, rural
Variables that can be measured on a nominal scale have the following properties:
<b>They have no natural order.</b> For example, we can’t arrange eye colors in order of worst to best or lowest to highest.
<b>Categories are mutually exclusive.</b> For example, an individual can’t have <em>both </em>blue and brown eyes. Similarly, an individual can’t live <em>both </em>in the city and in a rural area.
<b>The only number we can calculate for these variables are <em>counts</em>.</b> For example, we can count how many individuals have blonde hair, how many have black hair, how many have brown hair, etc.
<b>The only  measure of central tendency  we can calculate for these variables is <em>the mode</em>.</b> The mode tells us which category had the most counts. For example, we could find which eye color occurred most frequently.
The most common way that nominal scale data is collected is through a survey. For example, a researcher might survey 100 people and ask each of them what type of place they live in.
<b>Question:</b> What type of area do you live in?
<b>Possible Answers:</b> City, Suburbs, Rural.
Using this data, the researcher can find out how many people live in each area, as well as which area is the most common to live in.
<h2>Ordinal</h2>
The next type of measurement scale that we can use to label variables is an <b>ordinal scale</b>.
<b>Ordinal scale:</b> A scale used to label variables that have a natural <em>order</em>, but no quantifiable difference between values.
Some examples of variables that can be measured on an ordinal scale include:
<b>Satisfaction:</b> Very unsatisfied, unsatisfied, neutral, satisfied, very satisfied
<b>Socioeconomic status:</b> Low income, medium income, high income
<b>Workplace status:</b> Entry Analyst, Analyst I, Analyst II, Lead Analyst
<b>Degree of pain:</b> Small amount of pain, medium amount of pain, high amount of pain
Variables that can be measured on an ordinal scale have the following properties:
<b>They have a natural order.</b> For example, “very satisfied” is better than “satisfied,” which is better than “neutral,” etc.
<b>The difference between values can’t be evaluated. </b>For example, we can’t exactly say that the difference between “very satisfied and “satisfied” is the same as the difference between “satisfied” and “neutral.”
<b>The two  measures of central tendency  we can calculate for these variables are <em>the mode </em>and <em>the median</em>.</b> The mode tells us which category had the most counts and the median tells us the “middle” value.
Ordinal scale data is often collected by companies through surveys who are looking for feedback about their product or service. For example, a grocery store might survey 100 recent customers and ask them about their overall experience.
<b>Question:</b> How satisfied were you with your most recent visit to our store?
<b>Possible Answers:</b> Very unsatisfied, unsatisfied, neutral, satisfied, very satisfied.
Using this data, the grocery store can analyze the total number of responses for each category, identify which response was most common, and identify the median response.
<h2>Interval</h2>
The next type of measurement scale that we can use to label variables is an <b>interval scale</b>.
<b>Interval scale:</b> A scale used to label variables that have a natural order and a quantifiable difference between values, <em>but no “true zero” value</em>.
Some examples of variables that can be measured on an interval scale include:
<b>Temperature:</b> Measured in Fahrenheit or Celsius
<b>Credit Scores:</b> Measured from 300 to 850
<b>SAT Scores:</b> Measured from 400 to 1,600
Variables that can be measured on an interval scale have the following properties:
<b>These variables have a natural order.</b>
<b>We can measure the mean, median, mode, and standard deviation of these variables.</b>
<b>These variables have an exact difference between values. </b>Recall that ordinal variables have no exact difference between variables – we don’t know if the difference between “very satisfied” and “satisfied” is the same as the difference between “satisfied” and “neutral.” For variables on an interval scale, though, we know that the difference between a credit score of 850 and 800 is the exact same as the difference between 800 and 750.
<b>These variables have no “true zero” value. </b>For example, it’s impossible to have a credit score of zero. It’s also impossible to have an SAT score of zero. And for temperatures, it’s possible to have negative values (e.g. -10° F) which means there isn’t a true zero value that values can’t go below.
The nice thing about interval scale data is that it can be analyzed in more ways than nominal or ordinal data. For example, researchers could gather data on the credit scores of residents in a certain county and calculate the following metrics:
Median credit score (the “middle” credit score value)
Mean credit score (the average credit score)
Mode credit score (the credit score that occurs most often)
Standard deviation of credit scores (a way to measure how spread out credit scores are)
<h2>Ratio</h2>
The last type of measurement scale that we can use to label variables is a <b>ratio scale</b>.
<b>Ratio scale:</b> A scale used to label variables that have a natural order, a quantifiable difference between values, and a “true zero” value.
Some examples of variables that can be measured on a ratio scale include:
<b>Height:</b> Can be measured in centimeters, inches, feet, etc. and cannot have a value below zero.
<b>Weight: </b>Can be measured in kilograms, pounds, etc. and cannot have a value below zero.
<b>Length: </b>Can be measured in centimeters, inches, feet, etc. and cannot have a value below zero.
Variables that can be measured on a ratio scale have the following properties:
<b>These variables have a natural order.</b>
<b>We can calculate the mean, median, mode, standard deviation, and a variety of other descriptive statistics for these variables.</b>
<b>These variables have an exact difference between values.</b>
<b>These variables have a “true zero” value. </b>For example, length, weight, and height all have a minimum value (zero) that can’t be exceeded. It’s not possible for ratio variables to take on negative values. For this reason, the <em>ratio </em>between values can be calculated. For example, someone who weighs 200 lbs. can be said to weigh <em>two times </em>as much as someone who weights 100 lbs. Likewise someone who is 6 feet tall is<em> 1.5 times</em> taller than someone who is 4 feet tall.
Data that can be measured on a ratio scale can be analyzed in a variety of ways. For example, researchers could gather data about the height of individuals in a certain school and calculate the following metrics:
Median height
Mean height
Mode height
Standard deviation of heights
Ratio of tallest height to smallest height
<h2>Summary</h2>
The following table provides a summary of the variables in each measurement scale:
<table><tbody>
<tr>
<th><b>Property</b></th>
<th><b>Nominal</b></th>
<th><b>Ordinal</b></th>
<th><b>Interval</b></th>
<th><b>Ratio</b></th>
</tr>
<tr>
<td><b>Has a natural “order”</b></td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td><b>Mode can be calculated</b></td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td><b>Median can be calculated</b></td>
<td> </td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td><b>Mean can be calculated</b></td>
<td> </td>
<td> </td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td><b>Exact difference between values</b></td>
<td> </td>
<td> </td>
<td style="text-align: center;">YES</td>
<td style="text-align: center;">YES</td>
</tr>
<tr>
<td><b>Has a “true zero” value</b></td>
<td> </td>
<td> </td>
<td> </td>
<td style="text-align: center;">YES</td>
</tr>
</tbody></table>
<h2><span class="orange">How to Perform Levene’s Test in Excel</span></h2>
<b>Levene’s Test </b>is used to determine whether two or more groups have equal variances. This is a widely used test in statistics because many statistical tests use the assumption that groups have equal variances.
This tutorial explains how to perform Levene’s Test in Excel.
<h2>Example: Levene’s Test in Excel</h2>
Researchers want to know if three different fertilizers lead to different levels of plant growth. They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
Before they conduct a statistical test to determine if there is a difference in plant growth between the groups, they first want to perform <b>Levene’s Test </b>to determine whether or not the three groups have equal variances.
Use the following steps to perform Levene’s Test in Excel.
<b>Step 1: Enter the data.</b>
Enter the following data, which shows the total growth (in inches) for each of the 10 plants in each group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel1.png">
<b>Step 2: Calculate the mean of each group.</b>
Next, calculate the mean of each group using the <b>AVERAGE() </b>function:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel2.png">
<b>Step 3: Calculate the absolute residuals.</b>
Next, calculate the absolute residuals for each group. The following screenshot shows the formula to use to calculate the residual of the first observation in the first group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel3.png">
Copy this formula to all remaining cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel4.png">
<b>Step 4: Perform a One-Way ANOVA.</b>
Excel doesn’t have a built-in function to perform Levene’s Test, but a workaround is to perform a one-way ANOVA on the absolute residuals. If the p-value from the ANOVA is less than some significance level (.e.g 0.05), this indicates that the three groups do not have equal variances.
To perform a one-way ANOVA, go to the <b>Data</b> tab and click on <b>Data Analysis</b>. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
Once you click on <b>Data Analysis,</b> a new window will pop up. Select <b>Anova: Single Factor </b>and click OK.
For <b>Input Range</b>, choose the cells where the absolute residuals are located. For <b>Output Range</b>, choose a cell where you would like the results of the one-way ANOVA to appear. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel6.png">
The results of the one-way ANOVA will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/leveneExcel7.png">
We can see that the p-value of the one-way ANOVA is <b>0.591251</b>. Because this value is not less than 0.05, we fail to reject the null hypothesis. In other words, we don’t have sufficient evidence to say that the variance between the three groups is different.
<h2><span class="orange">How to Perform Levene’s Test in Python</span></h2>
<b>Levene’s Test </b>is used to determine whether two or more groups have equal variances. It is commonly used because many statistical tests make the assumption that groups have equal variances and Levene’s Test allows you to determine if this assumption is satisified.
This tutorial explains how to perform Levene’s Test in Python.
<h3>Example: Levene’s Test in Python</h3>
Researchers want to know if three different fertilizers lead to different levels of plant growth. They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
Use the following steps to perform Levene’s Test in Python to determine whether or not the three groups have equal variances.
<b>Step 1: Input the data.</b>
First, we’ll create three arrays to hold the data values:
<b>group1 = [7, 14, 14, 13, 12, 9, 6, 14, 12, 8]
group2 = [15, 17, 13, 15, 15, 13, 9, 12, 10, 8]
group3 = [6, 8, 8, 9, 5, 14, 13, 8, 10, 9]
</b>
<b>Step 2: Perform Levene’s Test.</b>
Next, we’ll perform Levene’s Test using the  levene() function  from the SciPy library, which uses the following syntax:
<b>levene(sample1, sample2, …, center=’median’)</b>
where:
<b>sample1, sample2, etc:</b> Names of the samples.
<b>center:</b> Method to use for Levene’s test. The default is ‘median’, but other choices include ‘mean’ and ‘trimmed.’
As mentioned in  the documentation , there are actually three different variations of Levene’s test you can use. The recommended usages are as follows:
<b>‘median’: </b>recommended for skewed distributions.
<b>‘mean’:</b> recommended for symmetric, moderate-tailed distributions.
<b>‘trimmed’: </b>recommended for heavy-tailed distributions.
The following code illustrates how to perform Levene’s test using both the <b>mean </b>and the <b>median </b>as the center:
<b>import scipy.stats as stats
#Levene's test centered at the median
stats.levene(group1, group2, group3, center='median')
(statistic=0.1798, pvalue=0.8364)
#Levene's test centered at the mean
stats.levene(group1, group2, group3, center='mean')
(statistic=0.5357, pvalue=0.5914)</b>
In both methods, the p-value is not less than .05. This means in both cases we would fail to reject the null hypothesis. This means we do not have sufficient evidence to say that the variance in plant growth between the three fertilizers is significantly different.
In other words, the three groups have equal variances. If we were to perform some statistical test (like a  one-way ANOVA ) that assumes each group has equal variance, then this assumption would be met.
<h2><span class="orange">How to Conduct Levene’s Test for Equality of Variances in R</span></h2>
Many statistical tests (like a  one-way ANOVA  or  two-way ANOVA ) make the assumption that the variance among several groups is equal. 
One way to formally test this assumption is to use <b>Levene’s Test</b>, which tests whether or not the variance among two or more groups is equal.
This test has the following hypotheses:
<b>Null hypothesis (H<sub>0</sub>)</b>: The variance among the groups is equal.
<b>Alternative hypothesis (H<sub>A</sub>)</b>: The variance among the groups is <em>not </em>equal.
If the p-value from the test is less than our chosen significance level, we can reject the null hypothesis and conclude that we have enough evidence to state that the variance among the groups is not equal.
<h2>How to Conduct Levene’s Test in R</h2>
To conduct Levene’s test in R, we can use the <b>leveneTest()</b> function from the <b>car </b>library, which uses the following syntax:
leveneTest(response variable ~ group variable, data = data)
As an example, consider the following data frame that shows how much weight people lost on three different weight loss programs:
<b>#make this example reproducible
set.seed(0)
#create data frame
data &lt;- data.frame(program = rep(c("A", "B", "C"), each = 30),   weight_loss = c(runif(30, 0, 3),                   runif(30, 0, 5),                   runif(30, 1, 7)))
#view first six rows of data frame
head(data)
#  program weight_loss
#1       A   2.6900916
#2       A   0.7965260
#3       A   1.1163717
#4       A   1.7185601
#5       A   2.7246234
#6       A   0.6050458</b>
To check if the variance in weight loss is equal among these three programs, we can use the <b>leveneTest()</b> function and use 0.05 as our significance level:
<b>#load car package
library(car)
#conduct Levene's Test for equality of variances
leveneTest(weight_loss ~ program, data = data)
#Levene's Test for Homogeneity of Variance (center = median)
#      Df F value  Pr(>F)  
#group  2  4.1716 0.01862 *
#      87                  
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
The p-value of the test is <b>0.01862</b>, which is less than our significance level of 0.05.
Thus, we reject the null hypothesis and conclude that the variance among the three groups is <em>not </em>equal.
<h3>Visualizing the Differences in Variances</h3>
From conducting Levene’s test, we know that the variances among the three groups are not equal.
In addition to conducting this test, we can create boxplots that display the distribution of weight loss for each of the three groups so that we can gain a visual understanding of why Levene’s test rejected the null hypothesis of equal variances.
<b>boxplot(weight_loss ~ program,
  data = data,
  main = "Weight Loss Distribution by Program",
  xlab = "Program",
  ylab = "Weight Loss",
  col = "steelblue",
  border = "black")</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/oneWayAnova1.jpg">
We can see that the variance for weight loss is noticeably higher for participants in Program C compared to the other two programs.
Thus, it makes sense that Levene’s test rejected the null hypothesis that the variances are equal among the three groups.
<h2><span class="orange">How to Perform Levene’s Test in SPSS</span></h2>
<b>Levene’s Test </b>is used to determine whether two or more groups have equal variances.
It is widely used because many statistical tests use the assumption that groups have  equal variances .
This tutorial explains how to perform Levene’s Test in SPSS.
<h2>Example: Levene’s Test in SPSS</h2>
Researchers want to know if three different fertilizers lead to different levels of plant growth.
They randomly select 30 different plants and split them into three groups of 10, applying a different fertilizer to each group. At the end of one month they measure the height of each plant.
The following screenshot shows the amount of growth (in inches) for each individual plant, along with the fertilizer (1, 2, or 3) that was applied to the plant:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/leveneSPSS1.png">
Use the following steps to perform Levene’s Test in SPSS to determine whether or not the three groups have equal variances.
<b>Step 1: Choose the Explore option.</b>
Click the <b>Analyze </b>tab, then <b>Descriptive Statistics</b>, then <b>Explore</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/leveneSPSS2.png">
<b>Step 2: Fill in the necessary values to perform the test.</b>
Drag <b>growth </b>into the box under Dependent List and drag <b>fertilize </b>into the box under Factor List.
Then click <b>Plots</b> and make sure <b>Power estimation </b>is selected. Then click <b>continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/leveneSPSS3.png">
<b>Step 3: Interpret the results.</b>
Once you click <b>OK</b>, the results of Levene’s test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/leveneSPSS4.png">
This table displays the test statistic for four different versions of Levene’s Test. The numbers we’re interested in are in the first row, which displays the results of Levene’s Test based on the mean.
The test statistic is <b>.536 </b>and the corresponding p-value is <b>.591*</b>.
Since this p-value is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the variance in plant growth between the three fertilizers is significantly different.
In other words, the three groups have equal variances. If we were to perform some statistical test (like a  one-way ANOVA ) that assumes each group has equal variance, then this assumption would be met.
<em><b>*</b>This p-value correspond to an F statistic of .536 with numerator df = 2 and denominator df = 27. This p-value can also be calculated using the  F Distribution Calculator .</em>
<h2><span class="orange">How to Perform Levene’s Test in Stata</span></h2>
<b>Levene’s Test </b>is used to determine whether two or more groups have equal variances. This is a widely used test in statistics because many statistical tests use the assumption that groups have equal variances.
This tutorial explains how to perform Levene’s Test in Stata.
<h2>Example: Levene’s Test in Stata</h2>
For this example we will use the dataset <em>stay</em>, which contains information about the length of stay for 1,778 different patients hospitalized for a given medical procedure differs by gender. The dataset contains 884 males and 894 females.
Use the following steps to perform Levene’s Test to determine if the variances in length of stay is equal for males and females.
<b>Step 1: Load and view the data.</b>
Use the following command to load the dataset in Stata.
<b>use http://www.stata-press.com/data/r13/stay</b>
View the first ten rows of data using the following command:
<b>list in 1/10</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/leveneStata1.png">
The first column displays the length of stay (in days) for an individual and the second column displays the sex of the individual.
<b>Step 2: Perform Levene’s Test.</b>
We will use the following syntax to perform Levene’s Test:
<b>robvar measurement_variable, by(grouping_variable)</b>
In our case we will use the following syntax:
<b>robvar lengthstay, by(sex)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/leveneStata2.png">
Here is how to interpret the output:
<b>Summary table: </b>This table shows the mean length of stay, standard deviation in length of stay, and total observations for both males and females. We can see that the standard deviation in length of stay is higher for males (9.7884747) compared to females (9.1081478), but Levene’s Test will tell us whether or not this difference is statistically significant.
<b>W0: 0.55505315</b>. This is the test statistic for Levene’s Test centered at the mean. The corresponding p-value is <b>0.45625888</b>.
<b>W50: 0.42714734</b>. This is the test statistic for Levene’s Test centered at the median. The corresponding p-value is <b>0.51347664</b>.
<b>W10: 0.44577674</b>. This is the test statistic for Levene’s Test centered using the 10% trimmed mean – i.e. the top 5% and bottom 5% of values are trimmed out so they don’t overly influence the test. The corresponding p-value is <b>0.50443411</b>.
No matter which version of Levene’s Test you use, the p-value for each version is not less than 0.05. This indicates that there is not a statistically significant difference in the variance of length of stay between males and females.
<em><b>Note: </b>Conover, Johnson, and Johnson (1981) recommend using the median test for assymetric data because it tends to provide more accurate results. For symmetric data, the median test and the mean test will produce similar results.</em>
<h2><span class="orange">How to Calculate Levenshtein Distance in Python</span></h2>
The <b>Levenshtein distance</b> between two strings is the minimum number of single-character edits required to turn one word into the other.
The word “edits” includes substitutions, insertions, and deletions.
For example, suppose we have the following two words:
PARTY
PARK
The Levenshtein distance between the two words (i.e. the number of edits we have to make to turn one word into the other) would be <b>2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/levenshtein1.png">
In practice, the Levenshtein distance is used in many different applications including approximate string matching, spell-checking, and natural language processing.
This tutorial explains how to calculate the Levenshtein distance between strings in Python by using the python-Levenshtein module.
You can use the following syntax to install this module:
<b>pip install python-Levenshtein</b>
You can then load the function to calculate the Levenshtein distance:
<b>from Levenshtein import distance as lev
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Levenshtein Distance Between Two Strings</h3>
The following code shows how to calculate the Levenshtein distance between the two strings “party” and “park”:
<b>#calculate Levenshtein distance
lev('party', 'park')
2</b>
The Levenshtein distance turns out to be <b>2</b>.
<h3>Example 2: Levenshtein Distance Between Two Arrays</h3>
The following code shows how to calculate the Levenshtein distance between every pairwise combination of strings in two different arrays:
<b>#define arrays
a = ['Mavs', 'Spurs', 'Lakers', 'Cavs']
b &lt;- ['Rockets', 'Pacers', 'Warriors', 'Celtics']
#calculate Levenshtein distance between two arrays
for i,k in zip(a, b):
  print(lev(i, k))
6
4
5
5
</b>
The way to interpret the output is as follows:
The Levenshtein distance between ‘Mavs’ and ‘Rockets’ is <b>6</b>.
The Levenshtein distance between ‘Spurs’ and ‘Pacers’ is <b>4</b>.
The Levenshtein distance between ‘Lakers’ and ‘Warriors’ is <b>5</b>.
The Levenshtein distance between ‘Cavs’ and ‘Celtics’ is <b>5</b>.
<h2><span class="orange">How to Calculate Levenshtein Distance in R (With Examples)</span></h2>
The <b>Levenshtein distance</b> between two strings is the minimum number of single-character edits required to turn one word into the other.
The word “edits” includes substitutions, insertions, and deletions.
For example, suppose we have the following two words:
PARTY
PARK
The Levenshtein distance between the two words (i.e. the number of edits we have to make to turn one word into the other) would be <b>2</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/levenshtein1.png">
In practice, the Levenshtein distance is used in many different applications including approximate string matching, spell-checking, and natural language processing.
This tutorial explains how to calculate the Levenshtein distance between strings in R by using the  stringdist()  function from the <b>stringdist</b> package in R.
This function uses the following basic syntax:
<b>#load stringdist package
library(stringdist)
#calculate Levenshtein distance between two strings
stringdist("string1", "string2", method = "lv")
</b>
Note that this function can calculate many different distance metrics. By specifying method = “lv”, we tell the function to calculate the Levenshtein distance.
<h3>Example 1: Levenshtein Distance Between Two Strings</h3>
The following code shows how to calculate the Levenshtein distance between the two strings “party” and “park” using the <b>stringdist()</b> function:
<b>#load stringdist package
library(stringdist)
#calculate Levenshtein distance between two strings
stringdist('party', 'park', method = 'lv')
[1] 2
</b>
The Levenshtein distance turns out to be <b>2</b>.
<h3>Example 2: Levenshtein Distance Between Two Vectors</h3>
The following code shows how to calculate the Levenshtein distance between every pairwise combination of strings in two different vectors:
<b>#load stringdist package
library(stringdist)
#define vectors
a &lt;- c('Mavs', 'Spurs', 'Lakers', 'Cavs')
b &lt;- c('Rockets', 'Pacers', 'Warriors', 'Celtics')
#calculate Levenshtein distance between two vectors
stringdist(a, b, method='lv')
[1] 6 4 5 5
</b>
The way to interpret the output is as follows:
The Levenshtein distance between ‘Mavs’ and ‘Rockets’ is <b>6</b>.
The Levenshtein distance between ‘Spurs’ and ‘Pacers’ is <b>4</b>.
The Levenshtein distance between ‘Lakers’ and ‘Warriors’ is <b>5</b>.
The Levenshtein distance between ‘Cavs’ and ‘Celtics’ is <b>5</b>.
<h3>Example 3: Levenshtein Distance Between Data Frame Columns</h3>
The following code shows how to calculate the Levenshtein distance between every pairwise combination of strings in two different columns of a data frame:
<b>#load stringdist package
library(stringdist)
#define data
data &lt;- data.frame(a = c('Mavs', 'Spurs', 'Lakers', 'Cavs'),   b = c('Rockets', 'Pacers', 'Warriors', 'Celtics'))
#calculate Levenshtein distance
stringdist(data$a, data$b, method='lv')
[1] 6 4 5 5
</b>
We could then append the Levenshtein distance as a new column in the data frame if we’d like:
<b>#save Levenshtein distance as vector
lev &lt;- stringdist(data$a, data$b, method='lv')
#append Levenshtein distance as new column 
data$lev &lt;- lev
#view data frame
data
       a        b lev
1   Mavs  Rockets   6
2  Spurs   Pacers   4
3 Lakers Warriors   5
4   Cavs  Celtics   5
</b>
<h2><span class="orange">How to Calculate Leverage Statistics in R</span></h2>
In statistics, an  observation  is considered an <b>outlier</b> if it has a value for the response variable that is much larger than the rest of the observations in the dataset.
Similarly, an observation is considered to have high <b>leverage</b> if it has a value (or values) for the predictor variables that are much more extreme compared to the rest of the observations in the dataset.
One of the first steps in any type of analysis is to take a closer look at the observations that have high leverage since they could have a large impact on the results of a given model.
This tutorial shows a step-by-step example of how to calculate and visualize the leverage for each observation in a model in R.
<h3>Step 1: Build a Regression Model</h3>
First, we’ll build a  multiple linear regression model  using the built-in <b>mtcars</b> dataset in R:
<b>#load the dataset
data(mtcars)
#fit a regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09
</b>
<h3>Step 2: Calculate the Leverage for each Observation</h3>
Next, we’ll use the <b>hatvalues()</b> function to calculate the leverage for each observation in the model:
<b>#calculate leverage for each observation in the model
hats &lt;- as.data.frame(hatvalues(model))
#display leverage stats for each observation
hats
    hatvalues(model)
Mazda RX4                 0.04235795
Mazda RX4 Wag             0.04235795
Datsun 710                0.06287776
Hornet 4 Drive            0.07614472
Hornet Sportabout         0.08097817
Valiant                   0.05945972
Duster 360                0.09828955
Merc 240D                 0.08816960
Merc 230                  0.05102253
Merc 280                  0.03990060
Merc 280C                 0.03990060
Merc 450SE                0.03890159
Merc 450SL                0.03890159
Merc 450SLC               0.03890159
Cadillac Fleetwood        0.19443875
Lincoln Continental       0.16042361
Chrysler Imperial         0.12447530
Fiat 128                  0.08346304
Honda Civic               0.09493784
Toyota Corolla            0.08732818
Toyota Corona             0.05697867
Dodge Challenger          0.06954069
AMC Javelin               0.05767659
Camaro Z28                0.10011654
Pontiac Firebird          0.12979822
Fiat X1-9                 0.08334018
Porsche 914-2             0.05785170
Lotus Europa              0.08193899
Ford Pantera L            0.13831817
Ferrari Dino              0.12608583
Maserati Bora             0.49663919
Volvo 142E                0.05848459
</b>
Typically we take a closer look at observations that have a leverage value greater than 2.
An easy way to do this is to sort the observations based on their leverage value, descending:
<b>#sort observations by leverage, descending
hats[order(-hats['hatvalues(model)']), ]
 [1] 0.49663919 0.19443875 0.16042361 0.13831817 0.12979822 0.12608583
 [7] 0.12447530 0.10011654 0.09828955 0.09493784 0.08816960 0.08732818
[13] 0.08346304 0.08334018 0.08193899 0.08097817 0.07614472 0.06954069
[19] 0.06287776 0.05945972 0.05848459 0.05785170 0.05767659 0.05697867
[25] 0.05102253 0.04235795 0.04235795 0.03990060 0.03990060 0.03890159
[31] 0.03890159 0.03890159
</b>
We can see that the largest leverage value is <b>0.4966</b>. Since this isn’t greater than 2, we know that none of the observations in our dataset have high leverage.
<h3>Step 3: Visualize the Leverage for each Observation</h3>
Lastly, we can create a quick plot to visualize the leverage for each observation:
<b>#plot leverage values for each observation
plot(hatvalues(model), type = 'h')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/leverage1.png">
The x-axis displays the index of each observation in the dataset and the y-value displays the corresponding leverage statistic for each observation.
<h2><span class="orange">How to Perform a Likelihood Ratio Test in Python</span></h2>
A <b>likelihood ratio test</b> compares the goodness of fit of two nested  regression models .
A  nested model  is simply one that contains a subset of the predictor variables in the overall regression model.
For example, suppose we have the following regression model with four predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub> + β<sub>4</sub>x<sub>4</sub> + ε
One example of a nested model would be the following model with only two of the original predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + ε
To determine if these two models are significantly different, we can perform a likelihood ratio test which uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> The full model and the nested model fit the data equally well. Thus, you should <b>use the nested model</b>.
<b>H<sub>A</sub>:</b> The full model fits the data significantly better than the nested model. Thus, you should <b>use the full model</b>.
If the  p-value  of the test is below a certain significance level (e.g. 0.05), then we can reject the null hypothesis and conclude that the full model offers a significantly better fit.
The following step-by-step example shows how to perform a likelihood ratio test in Python.
<h3>Step 1: Load the Data</h3>
In this example, we’ll show how to fit the following two regression models in Python using data from the <b>mtcars</b> dataset:
<b>Full model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb + β<sub>3</sub>hp + β<sub>4</sub>cyl
<b>Reduced model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb
First, we’ll load the dataset:
<b>from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
import pandas as pd
import scipy
#define URL where dataset is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data = pd.read_csv(url)
</b>
<b>Related:</b>  How to Read CSV Files with Pandas 
<h3>Step 2: Fit the Regression Models</h3>
First, we’ll fit the full model and calculate the log-likelihood of the model:
<b>#define response variable
y1 = data['mpg']
#define predictor variables
x1 = data[['disp', 'carb', 'hp', 'cyl']]
#add constant to predictor variables
x1 = sm.add_constant(x1)
#fit regression model
full_model = sm.OLS(y1, x1).fit()
#calculate log-likelihood of model
full_ll = full_model.llf
print(full_ll)
-77.55789711787898
</b>
Then, we’ll fit the reduced model and calculate the log-likelihood of the model:
<b>#define response variable
y2 = data['mpg']
#define predictor variables
x2 = data[['disp', 'carb']]
#add constant to predictor variables
x2 = sm.add_constant(x2)
#fit regression model
reduced_model = sm.OLS(y2, x2).fit()
#calculate log-likelihood of model
reduced_ll = reduced_model.llf
print(reduced_ll)
-78.60301334355185
</b>
<h3>Step 3: Perform the Log-Likelihood Test</h3>
Next, we’ll use the following code to perform the log-likelihood test:
<b>#calculate likelihood ratio Chi-Squared test statistic
LR_statistic = -2*(reduced_ll-full_ll)
print(LR_statistic)
2.0902324513457415
#calculate p-value of test statistic using 2 degrees of freedom
p_val = scipy.stats.chi2.sf(LR_statistic, 2)
print(p_val)
0.35165094613502257
</b>
From the output we can see that the Chi-Squared test-statistic is <b>2.0902</b> and the corresponding p-value is <b>0.3517</b>.
Since this p-value is not less than .05, we will fail to reject the null hypothesis.
This means the full model and the nested model fit the data equally well. Thus, we should use the nested model because the additional predictor variables in the full model don’t offer a significant improvement in fit.
Thus, our final model would be:
mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb
<b>Note</b>: We used 2 degrees of freedom when calculating the p-value because this represented the difference between the total predictor variables used between the two models.
<h2><span class="orange">How to Perform a Likelihood Ratio Test in R</span></h2>
A <b>likelihood ratio test</b> compares the goodness of fit of two nested regression models.
A  nested model  is simply one that contains a subset of the predictor variables in the overall regression model.
For example, suppose we have the following regression model with four predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + β<sub>3</sub>x<sub>3</sub> + β<sub>4</sub>x<sub>4</sub> + ε
One example of a nested model would be the following model with only two of the original predictor variables:
Y = β<sub>0</sub> + β<sub>1</sub>x<sub>1</sub> + β<sub>2</sub>x<sub>2</sub> + ε
To determine if these two models are significantly different, we can perform a likelihood ratio test which uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> The full model and the nested model fit the data equally well. Thus, you should <b>use the nested model</b>.
<b>H<sub>A</sub>:</b> The full model fits the data significantly better than the nested model. Thus, you should <b>use the full model</b>.
If the p-value of the test is below a certain significance level (e.g. 0.05), then we can reject the null hypothesis and conclude that the full model offers a significantly better fit.
The following example shows how to perform a likelihood ratio test in R.
<h3>Example: Likelihood Ratio Test in R</h3>
The following code shows how to fit the following two regression models in R using data from the built-in <b>mtcars</b> dataset:
<b>Full model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb + β<sub>3</sub>hp + β<sub>4</sub>cyl
<b>Reduced model:</b> mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb
We will use the <b>lrtest()</b> function from the <b>lmtest</b> package to perform a likelihood ratio test on these two models:
<b>library(lmtest)
#fit full model
model_full &lt;- lm(mpg ~ disp + carb + hp + cyl, data = mtcars)
#fit reduced model
model_reduced &lt;- lm(mpg ~ disp + carb, data = mtcars)
#perform likelihood ratio test for differences in models
lrtest(model_full, model_reduced)
Likelihood ratio test
Model 1: mpg ~ disp + carb + hp + cyl
Model 2: mpg ~ disp + carb
  #Df  LogLik Df  Chisq Pr(>Chisq)
1   6 -77.558                     
2   4 -78.603 -2 2.0902     0.3517</b>
From the output we can see that the Chi-Squared test-statistic is <b>2.0902</b> and the corresponding p-value is <b>0.3517</b>.
Since this p-value is not less than .05, we will fail to reject the null hypothesis.
This means the full model and the nested model fit the data equally well. Thus, we should use the nested model because the additional predictor variables in the full model don’t offer a significant improvement in fit.
We could then carry out another likelihood ratio test to determine if a model with only one predictor variable is significantly different from a model with the two predictors:
<b>library(lmtest)
#fit full model
model_full &lt;- lm(mpg ~ disp + carb, data = mtcars)
#fit reduced model
model_reduced &lt;- lm(mpg ~ disp, data = mtcars)
#perform likelihood ratio test for differences in models
lrtest(model_full, model_reduced)
Likelihood ratio test
Model 1: mpg ~ disp + carb
Model 2: mpg ~ disp
  #Df  LogLik Df  Chisq Pr(>Chisq)   
1   4 -78.603                        
2   3 -82.105 -1 7.0034   0.008136 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</b>
From the output we can see that the p-value of the likelihood ratio test is <b>0.008136</b>. Since this is less than .05, we would reject the null hypothesis.
Thus, we would conclude that the model with two predictors offers a significant improvement in fit over the model with just one predictor.
Thus, our final model would be:
mpg = β<sub>0</sub> + β<sub>1</sub>disp + β<sub>2</sub>carb
<h2><span class="orange">Likelihood vs. Probability: What’s the Difference?</span></h2>
Two terms that students often confuse in statistics are <b>likelihood</b> and <b>probability</b>.
Here’s the difference in a nutshell:
<b>Probability</b> refers to the chance that a particular outcome occurs based on the values of parameters in a model.
<b>Likelihood</b> refers to how well a sample provides support for particular values of a parameter in a model.
When calculating the probability of some outcome, we assume the parameters in a model are trustworthy.
However, when we calculate likelihood we’re trying to determine if we can trust the parameters in a model based on the sample data that we’ve observed.
The following examples illustrate the difference between probability and likelihood in various scenarios.
<h3>Example 1: Likelihood vs. Probability in Coin Tosses</h3>
Suppose we have a coin that is assumed to be fair. If we flip the coin one time, the <b>probability</b> that it will land on heads is 0.5.
Now suppose we flip the coin 100 times and it only lands on heads 17 times. We would say that the <b>likelihood</b> that the coin is fair is quite low. If the coin was actually fair, we would expect it to land on heads much more often.
When calculating the probability of a coin landing on heads, we simply assume that P(heads) = 0.5 on a given toss.
However, when calculating the likelihood we’re trying to determine if the model parameter (p = 0.5) is actually correctly specified.
In the example above, a coin landing on heads only 17 out of 100 times makes us highly suspicious that the truly probability of the coin landing on heads on a given toss is actually p = 0.5.
<h3>Example 2: Likelihood vs. Probability in Spinners</h3>
Suppose we have a spinner split into thirds with three colors on it: red, green, and blue. Suppose we assume that it’s equally likely for the spinner to land on any of the three colors.
If we spin it one time, the <b>probability</b> that it lands on red is 1/3.
Now suppose we spin it 100 times and it lands on red 2 times, green 90 times, and blue 8 times. We would say that the <b>likelihood</b> that the spinner is actually equally likely to land on each color is very low.
When calculating the probability of the spinner landing on red, we simply assume that P(red) = 1/3 on a given spin.
However, when calculating the likelihood we’re trying to determine if the model parameters (P(red) = 1/3, P(green) = 1/3, P(blue) = 1/3) are actually correctly specified.
In the example above, the results of the 100 spins make us highly suspicious that each color is equally likely to occur.
<h3>Example 3: Likelihood vs. Probability in Gambling</h3>
Suppose a casino claims that the probability of winning money on a certain slot machine is 40% for each turn.
If we take one turn , the <b>probability</b> that we will win money is 0.40.
Now suppose we take 100 turns and we win 42 times. We would conclude that the <b>likelihood</b> that the probability of winning in 40% of turns seems to be fair.
When calculating the probability of winning on a given turn, we simply assume that P(winning) =0.40 on a given turn.
However, when calculating the likelihood we’re trying to determine if the model parameter P(winning) = 0.40 is actually correctly specified.
In the example above, winning 42 times out of 100 makes us believe that a probability of winning 40% of the time seems reasonable.
<h2><span class="orange">How to Find A Line of Best Fit in Google Sheets</span></h2>
A <b>line of best fit</b> is a line that best “fits” the trend in a given dataset.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets0.png">
This tutorial provides a step-by-step example of how to create a line of best fit in Google Sheets.
<h3>Step 1: Create the Dataset</h3>
First, let’s create a fake dataset to work with:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets1.png">
<h3>Step 2: Create a Scatterplot</h3>
Next, we’ll create a  scatterplot  to visualize the data.
First, highlight cells <b>A2:B11</b> as follows:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets2.png">
Next, click the <b>Insert</b> tab and then click <b>Chart</b> from the dropdown menu:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets3.png">
Google Sheets will automatically insert a scatterplot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets4.png">
<h3>Step 3: Add the Line of Best Fit</h3>
Next, double click anywhere on the scatterplot to bring up the <b>Chart Editor</b> window on the right:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/polyRegSheets5.png">
Next, click <b>Series</b>. Then, scroll down and check the box next to <b>Trendline</b> and change the line color to whatever you’d like. We’ll choose Red. For Label, choose <b>Use Equation</b> and then check the box next to <b>Show R<sup>2</sup></b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets5.png">
The following trendline will automatically be added to the chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/lineBestSheets6.png">
We can see that the line of best fit seems to capture the trend in the data quite well.
Above the scatterplot, we see that the equation for this line of best fit is as follows:
<b>y = 2.8*x + 4.44</b>
The  R-squared  for this line turns out to be <b>.938</b>. This indicates that 93.8% of the variation in the response variable, <em>y</em>, can be explained by the predictor variable, <em>x</em>.
We can also use the equation for the line of best fit to find the estimated value of <em>y</em> based on the value of <em>x</em>. For example, if <em>x</em> = 3 then <em>y</em> is estimated to be <b>12.84</b>:
y = 2.8*(3) + 4.44 = 12.84
<h2><span class="orange">How to Plot Line of Best Fit in R (With Examples)</span></h2>
You can use one of the following methods to plot a line of best fit in R:
<b>Method 1: Plot Line of Best Fit in Base R</b>
<b>#create scatter plot of x vs. y
plot(x, y)
#add line of best fit to scatter plot
abline(lm(y ~ x))
</b>
<b>Method 2: Plot Line of Best Fit in ggplot2</b>
<b>library(ggplot2)
#create scatter plot with line of best fit
ggplot(df, aes(x=x, y=y)) +
    geom_point() +
    geom_smooth(method=lm, se=FALSE)</b>
The following examples show how to use each method in practice.
<h3>Example 1: Plot Line of Best Fit in Base R</h3>
The following code shows how to plot a line of best fit for a simple linear regression model using base R:
<b>#define data
x &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)
y &lt;- c(2, 5, 6, 7, 9, 12, 16, 19)
#create scatter plot of x vs. y
plot(x, y)
#add line of best fit to scatter plot
abline(lm(y ~ x))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/lineOf1.png">
Feel free to modify the style of the points and the line as well:
<b>#define data
x &lt;- c(1, 2, 3, 4, 5, 6, 7, 8)
y &lt;- c(2, 5, 6, 7, 9, 12, 16, 19)
#create scatter plot of x vs. y
plot(x, y, pch=16, col='red', cex=1.2)
#add line of best fit to scatter plot
abline(lm(y ~ x), col='blue' , lty='dashed')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/lineOf2.png">
We can also use the following code to quickly calculate the line of best fit:
<b>#find regression model coefficients
summary(lm(y ~ x))$coefficients
              Estimate Std. Error   t value     Pr(>|t|)
(Intercept) -0.8928571  1.0047365 -0.888648 4.084029e-01
x            2.3095238  0.1989675 11.607544 2.461303e-05</b>
The line of best fit turns out to be: <b>y = -0.89 + 2.32x</b>.
<h3>Example 2: Plot Line of Best Fit in ggplot2</h3>
The following code shows how to plot a line of best fit for a simple linear regression model using the  ggplot2  data visualization package:
<b>library(ggplot2)
#define data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=c(2, 5, 6, 7, 9, 12, 16, 19))
#create scatter plot with line of best fit
ggplot(df, aes(x=x, y=y)) +
    geom_point() +
    geom_smooth(method=lm, se=FALSE)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/lineOf3.png">
Feel free to modify the aesthetics of the plot as well:
<b>library(ggplot2)
#define data
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=c(2, 5, 6, 7, 9, 12, 16, 19))
#create scatter plot with line of best fit
ggplot(df, aes(x=x, y=y)) +
    geom_point(col='red', size=2) +
    geom_smooth(method=lm, se=FALSE, col='purple', linetype='dashed') +
    theme_bw()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/lineOf4.png">
<h2><span class="orange">How to Plot Line of Best Fit in Python (With Examples)</span></h2>
You can use the following basic syntax to plot a line of best fit in Python:
<b>#find line of best fit
a, b = np.polyfit(x, y, 1)
#add points to plot
plt.scatter(x, y)
#add line of best fit to plot
plt.plot(x, a*x+b)
</b>
The following example shows how to use this syntax in practice.
<h3>Example 1: Plot Basic Line of Best Fit in Python</h3>
The following code shows how to plot a basic line of best fit in Python:
<b>import numpy as np
import matplotlib.pyplot as plt
#define data
x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([2, 5, 6, 7, 9, 12, 16, 19])
#find line of best fit
a, b = np.polyfit(x, y, 1)
#add points to plot
plt.scatter(x, y)
#add line of best fit to plot
plt.plot(x, a*x+b)        </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/lineBest1.png">
<h3>Example 2: Plot Custom Line of Best Fit in Python</h3>
The following code shows how to create the same line of best fit as the previous example except with the following additions:
Customized colors for the points and the line of best fit
Customized style and width for the line of best fit
The equation of the fitted regression line displayed on the plot
<b>import numpy as np
import matplotlib.pyplot as plt
#define data
x = np.array([1, 2, 3, 4, 5, 6, 7, 8])
y = np.array([2, 5, 6, 7, 9, 12, 16, 19])
#find line of best fit
a, b = np.polyfit(x, y, 1)
#add points to plot
plt.scatter(x, y, color='purple')
#add line of best fit to plot
plt.plot(x, a*x+b, color='steelblue', linestyle='--', linewidth=2)
#add fitted regression equation to plot
plt.text(1, 17, 'y = ' + '{:.2f}'.format(b) + ' + {:.2f}'.format(a) + 'x', size=14)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/lineBest2.png">
Feel free to place the fitted regression equation in whatever <b>(x, y)</b> coordinates you would like on the plot.
For this particular example, we chose (x, y) = (1, 17).
<h2><span class="orange">How to Find Line of Best Fit on TI-84 Calculator</span></h2>
A <b>line of best fit</b> is the line that best “fits” the trend of a dataset.
This tutorial provides a step-by-step example of how to calculate the line of best fit for the following dataset on a TI-84 calculator:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line1.png">
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values. Press STAT, then press EDIT. Then enter the x-values of the dataset in column L1 and the y-values in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line2.png">
<h3>Step 2: Find the Line of Best Fit</h3>
Next, we will find the line of best fit.
Press Stat, then scroll over to CALC. Then scroll down to LinReg(ax+b) and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line3.png">
Then press 2nd and 1 and comma , then press 2nd and 2 and comma , then press VARS and scroll over to to Y-VARS and press ENTER.
Your screen should look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line7.png">
Press ENTER once more and the following line of best fit will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line4.png">
The line of best fit is: <b>y = 5.493 + 1.14x</b>
<h3>Step 3: Plot the Line of Best Fit</h3>
Lastly, we will plot the line of best fit.
Press ZOOM and then scroll down to ZOOMSTAT and press ENTER.
The following scatterplot with the line of best fit will be shown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/line8.png">
<h2><span class="orange">How to Create Line Plots in SAS (With Examples)</span></h2>
You can use <b>proc sgplot</b> to create line plots in SAS.
This procedure uses the following basic syntax:
<b>/*create dataset*/
proc sgplot data=my_data;
    series x=x_variable y=y_variable;
run;
</b>
The following examples show how to use this procedure to create line plots in SAS.
<h3>Example 1: Create Line Plot with One Line</h3>
Suppose we have the following dataset in SAS that shows the total sales made by a store during 10 consecutive days:
<b>/*create dataset*/
data my_data;
    input day $ sales;
    datalines;
1 7
2 12
3 15
4 14
5 13
6 11
7 10
8 16
9 18
10 24
;
run;
/*view dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot1.jpg"132">
We can use<b> proc sgplot</b> to create a line plot that displays the day on the x-axis and sales on the y-axis:
<b>/*create line plot that displays sales by day*/
proc sgplot data=my_data;
    series x=day y=sales;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot2.jpg"564">
We can use the following code to modify the appearance of the chart, including the title, labels, colors, line pattern, and line thickness:
<b>/*create custom line plot*/
title "Sales by Day";
proc sgplot data=my_data;
    series x=day y=sales / lineattrs=(color=red pattern=dash thickness=4);
    xaxis display=(nolabel noline noticks);
run;
title; </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot3.jpg"569">
<h3>Example 2: Create Line Plot with Multiple Lines</h3>
Suppose we have the following dataset in SAS that shows the total sales made by three different stores during five consecutive days:
<b>/*create dataset*/
data my_data;
    input store $ day $ sales;
    datalines;
A 1 13
A 2 18
A 3 20
A 4 25
A 5 26
B 1 3
B 2 7
B 3 12
B 4 12
B 5 11
C 1 6
C 2 12
C 3 19
C 4 20
C 5 21
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot4.jpg"184">
We can use<b> proc sgplot</b> with the <b>group</b> argument to create a line plot that displays the sales made by each of the three stores:
<b>/*create line plot that displays sales by day for each store*/
title "Sales by Day by Store";
proc sgplot data=my_data;
    styleattrs datacontrastcolors=(red green blue);
    series x=day y=sales / group=store;
run;
title;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot5.jpg"597">
The x-axis displays the day and the y-axis displays the sales made by the stores.
The three individual lines show the sales made by each of the three stores during each day.
<h2><span class="orange">Linear Discriminant Analysis in Python (Step-by-Step)</span></h2>
 Linear discriminant analysis  is a method you can use when you have a set of predictor variables and you’d like to classify a  response variable  into two or more classes.
This tutorial provides a step-by-step example of how to perform linear discriminant analysis in Python.
<h3>Step 1: Load Necessary Libraries</h3>
First, we’ll load the necessary functions and libraries for this example:
<b>from sklearn.model_selection import train_test_split
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis 
from sklearn import datasets
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use the <b>iris</b> dataset from the sklearn library. The following code shows how to load this dataset and convert it to a pandas DataFrame to make it easy to work with:
<b>#load <em>iris </em>dataset
iris = datasets.load_iris()
#convert dataset to pandas DataFrame
df = pd.DataFrame(data = np.c_[iris['data'], iris['target']], columns = iris['feature_names'] + ['target'])
df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)
df.columns = ['s_length', 's_width', 'p_length', 'p_width', 'target', 'species']
#view first six rows of DataFrame
df.head()
   s_length  s_width  p_length  p_width  target species
0       5.1      3.5       1.4      0.2     0.0  setosa
1       4.9      3.0       1.4      0.2     0.0  setosa
2       4.7      3.2       1.3      0.2     0.0  setosa
3       4.6      3.1       1.5      0.2     0.0  setosa
4       5.0      3.6       1.4      0.2     0.0  setosa
#find how many total observations are in dataset
len(df.index)
150</b>
We can see that the dataset contains 150 total observations.
For this example we’ll build a linear discriminant analysis model to classify which species a given flower belongs to.
We’ll use the following predictor variables in the model:
Sepal length
Sepal width
Petal length
Petal width
And we’ll use them to predict the response variable <em>Species</em>, which takes on the following three potential classes:
setosa
versicolor
virginica
<h3>Step 3: Fit the LDA Model</h3>
Next, we’ll fit the LDA model to our data using the  LinearDiscriminantAnalsyis  function from sklearn:
<b>#define predictor and response variables
X = df[['s_length', 's_width', 'p_length', 'p_width']]
y = df['species']
#Fit the LDA model
model = LinearDiscriminantAnalysis()
model.fit(X, y)
</b>
<h3>Step 4: Use the Model to Make Predictions</h3>
Once we’ve fit the model using our data, we can evaluate how well the model performed by using repeated stratified k-fold cross validation.
For this example, we’ll use 10 folds and 3 repeats:
<b>#Define method to evaluate model
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
#evaluate model
scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
print(np.mean(scores))   
0.9777777777777779</b>
We can see that the model performed a mean accuracy of <b>97.78%</b>.
We can also use the model to predict which class a new flower belongs to, based on input values:
<b>#define new observation
new = [5, 3, 1, .4]
#predict which class the new observation belongs to
model.predict([new])
array(['setosa'], dtype='&lt;U10')
</b>
We can see that the model predicts this new observation to belong to the species called <em>setosa</em>.
<h3>Step 5: Visualize the Results</h3>
Lastly, we can create an LDA plot to view the linear discriminants of the model and visualize how well it separated the three different species in our dataset:
<b>#define data to plot
X = iris.data
y = iris.target
model = LinearDiscriminantAnalysis()
data_plot = model.fit(X, y).transform(X)
target_names = iris.target_names
#create LDA plot
plt.figure()
colors = ['red', 'green', 'blue']
lw = 2
for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    plt.scatter(data_plot[y == i, 0], data_plot[y == i, 1], alpha=.8, color=color,
                label=target_name)
#add legend to plot
plt.legend(loc='best', shadow=False, scatterpoints=1)
#display LDA plot
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/LDApython1.png">
You can find the complete Python code used in this tutorial  here .
<h2><span class="orange">Linear Discriminant Analysis in R (Step-by-Step)</span></h2>
 Linear discriminant analysis  is a method you can use when you have a set of predictor variables and you’d like to classify a  response variable  into two or more classes.
This tutorial provides a step-by-step example of how to perform linear discriminant analysis in R.
<h3>Step 1: Load Necessary Libraries</h3>
First, we’ll load the necessary libraries for this example:
<b>library(MASS)
library(ggplot2)</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use the built-in <b>iris</b> dataset in R. The following code shows how to load and view this dataset:
<b>#attach <em>iris</em> dataset to make it easy to work with
attach(iris)
#view structure of dataset
str(iris)
'data.frame':150 obs. of  5 variables:
 $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
 $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
 $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
 $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
 $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 ...
</b>
We can see that the dataset contains 5 variables and 150 total observations.
For this example we’ll build a linear discriminant analysis model to classify which species a given flower belongs to.
We’ll use the following predictor variables in the model:
Sepal.length
Sepal.Width
Petal.Length
Petal.Width
And we’ll use them to predict the response variable <em>Species</em>, which takes on the following three potential classes:
setosa
versicolor
virginica
<h3>Step 3: Scale the Data</h3>
One of the key assumptions of linear discriminant analysis is that each of the predictor variables have the same variance. An easy way to assure that this assumption is met is to scale each variable such that it has a mean of 0 and a standard deviation of 1.
We can quickly do so in R by using the <b>scale()</b> function:
<b>#scale each predictor variable (i.e. first 4 columns)
iris[1:4] &lt;- scale(iris[1:4])
</b>
We can use the  apply() function  to verify that each predictor variable now has a mean of 0 and a  standard deviation  of 1:
<b>#find mean of each predictor variable
apply(iris[1:4], 2, mean)
 Sepal.Length   Sepal.Width  Petal.Length   Petal.Width 
-4.484318e-16  2.034094e-16 -2.895326e-17 -3.663049e-17 
#find standard deviation of each predictor variable
apply(iris[1:4], 2, sd) 
Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
           1            1            1            1
</b>
<h3>Step 4: Create Training and Test Samples</h3>
Next, we’ll split the dataset into a training set to train the model on and a testing set to test the model on:
<b>#make this example reproducible
set.seed(1)
#Use 70% of dataset as training set and remaining 30% as testing set
sample &lt;- sample(c(TRUE, FALSE), nrow(iris), replace=TRUE, prob=c(0.7,0.3))
train &lt;- iris[sample, ]
test &lt;- iris[!sample, ] 
</b>
<h3>Step 5: Fit the LDA Model</h3>
Next, we’ll use the  lda() function  from the <b>MASS</b> package to fit the LDA model to our data:
<b>#fit LDA model
model &lt;- lda(Species~., data=train)
#view model output
model
Call:
lda(Species ~ ., data = train)
Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3207547  0.3207547  0.3584906 
Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa       -1.0397484   0.8131654   -1.2891006  -1.2570316
versicolor    0.1820921  -0.6038909    0.3403524   0.2208153
virginica     0.9582674  -0.1919146    1.0389776   1.1229172
Coefficients of linear discriminants:    LD1        LD2
Sepal.Length  0.7922820  0.5294210
Sepal.Width   0.5710586  0.7130743
Petal.Length -4.0762061 -2.7305131
Petal.Width  -2.0602181  2.6326229
Proportion of trace:
   LD1    LD2 
0.9921 0.0079 
</b>
Here is how to interpret the output of the model:
<b>Prior probabilities of group: </b>These represent the proportions of each Species in the training set. For example, 35.8% of all observations in the training set were of species <em>virginica</em>.
<b>Group means:</b> These display the mean values for each predictor variable for each species.
<b>Coefficients of linear discriminants:</b> These display the linear combination of predictor variables that are used to form the decision rule of the LDA model. For example:
<b>LD1:</b> .792*Sepal.Length + .571*Sepal.Width – 4.076*Petal.Length – 2.06*Petal.Width
<b>LD2:</b> .529*Sepal.Length + .713*Sepal.Width – 2.731*Petal.Length + 2.63*Petal.Width
<b>Proportion of trace:</b> These display the percentage separation achieved by each linear discriminant function.
<h3>Step 6: Use the Model to Make Predictions</h3>
Once we’ve fit the model using our training data, we can use it to make predictions on our test data:
<b>#use LDA model to make predictions on test data
predicted &lt;- predict(model, test)
names(predicted)
[1] "class"     "posterior" "x"   
</b>
This returns a list with three variables:
<b>class:</b> The predicted class
<b>posterior:</b> The  posterior probability  that an observation belongs to each class
<b>x:</b> The linear discriminants
We can quickly view each of these results for the first six observations in our test dataset:
<b>#view predicted class for first six observations in test set
head(predicted$class)
[1] setosa setosa setosa setosa setosa setosa
Levels: setosa versicolor virginica
#view posterior probabilities for first six observations in test set
head(predicted$posterior)
   setosa   versicolor    virginica
4       1 2.425563e-17 1.341984e-35
6       1 1.400976e-21 4.482684e-40
7       1 3.345770e-19 1.511748e-37
15      1 6.389105e-31 7.361660e-53
17      1 1.193282e-25 2.238696e-45
18      1 6.445594e-22 4.894053e-41
#view linear discriminants for first six observations in test set
head(predicted$x)
         LD1        LD2
4   7.150360 -0.7177382
6   7.961538  1.4839408
7   7.504033  0.2731178
15 10.170378  1.9859027
17  8.885168  2.1026494
18  8.113443  0.7563902
</b>
We can use the following code to see what percentage of observations the LDA model correctly predicted the Species for:
<b>#find accuracy of model
mean(predicted$class==test$Species)
[1] 1</b>
It turns out that the model correctly predicted the Species for <b>100%</b> of the observations in our test dataset.
In the real-world an LDA model will rarely predict every class outcome correctly, but this iris dataset is simply built in a way that machine learning algorithms tend to perform very well on it.
<h3>Step 7: Visualize the Results</h3>
Lastly, we can create an LDA plot to view the linear discriminants of the model and visualize how well it separated the three different species in our dataset:
<b>#define data to plot
lda_plot &lt;- cbind(train, predict(model)$x)
#create plot
ggplot(lda_plot, aes(LD1, LD2)) +
  geom_point(aes(color = Species))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/LDA_R1.png">
You can find the complete R code used in this tutorial  here .
<h2><span class="orange">Introduction to Linear Discriminant Analysis</span></h2>
When we have a set of predictor variables and we’d like to classify a  response variable  into one of two classes, we typically use  logistic regression .
For example, we may use logistic regression in the following scenario:
We want to use <em>credit score</em> and <em>bank balance</em> to predict whether or not a given customer will default on a loan. (Response variable = “Default” or “No default”)
However, when a response variable has more than two possible classes then we typically prefer to use a method known as <b>linear discriminant analysis</b>, often referred to as LDA.
For example, we may use LDA in the following scenario:
We want to use <em>points per game</em> and <em>rebounds per game</em> to predict whether a given high school basketball player will get accepted into one of three schools: Division 1, Division 2, or Division 3.
Although LDA and logistic regression models are both used for  classification , it turns out that LDA is far more stable than logistic regression when it comes to making predictions for multiple classes and is therefore the preferred algorithm to use when the response variable can take on more than two classes.
LDA also performs better when sample sizes are small compared to logistic regression, which makes it a preferred method to use when you’re unable to gather large samples.
<h3>How to Build LDA Models</h3>
LDA makes the following assumptions about a given dataset:
<b>(1)</b> The values of each predictor variable are  normally distributed . That is, if we made a histogram to visualize the distribution of values for a given predictor, it would roughly have a “bell shape.”
<b>(2)</b> Each predictor variable has the same  variance . This is almost never the case in real-world data, so we typically scale each variable to have the same mean and variance before actually fitting a LDA model.
Once these assumptions are met, LDA then estimates the following values:
<b>μ<sub>k</sub></b>: The mean of all training observations from the k<sup>th</sup> class.
<b>σ<sup>2</sup></b>: The weighted average of the sample variances for each of the <em>k</em> classes.
<b>π<sub>k</sub></b>: The proportion of the training observations that belong to the k<sup>th</sup> class.
LDA then plugs these numbers into the following formula and assigns each observation X = x to the class for which the formula produces the largest value:
<b>D<sub>k</sub>(x) = x * (μ<sub>k</sub>/σ<sup>2</sup>) – (μ<sub>k</sub><sup>2</sup>/2σ<sup>2</sup>) + log(π<sub>k</sub>)</b>
Note that LDA has <em>linear</em> in its name because the value produced by the function above comes from a result of <em>linear functions</em> of x.
<h3>How to Prepare Data for LDA</h3>
Make sure your data meets the following requirements before applying a LDA model to it:
<b>1. The response variable is categorical</b>. LDA models are designed to be used for classification problems, i.e. when the response variable can be placed into classes or categories.
<b>2. The predictor variables follow a normal distribution</b>. First, check that each predictor variable is roughly normally distributed. If this is not the case, you may choose to first  transform the data  to make the distribution more normal.
<b>3. Each predictor variable has the same variance</b>. As mentioned earlier, LDA assumes that each predictor variable has the same variance. Since this is rarely the case in practice, it’s a good idea to scale each variable in the dataset such that it has a mean of 0 and a standard deviation of 1.
<b>4. Account for extreme outliers.</b> Be sure to check for extreme outliers in the dataset before applying LDA. Typically you can check for outliers visually by simply using  boxplots  or  scatterplots .
<h3>Examples of Using Linear Discriminant Analysis</h3>
LDA models are applied in a wide variety of fields in real life. Some examples include:
<b>1. Marketing</b>. Retail companies often use LDA to classify shoppers into one of several categories. For example, they may build an LDA model to predict whether or not a given shopper will be a low spender, medium spender, or high spender using predictor variables like <em>income</em>, <em>total annual spending</em>, and <i>household size</i>.
<b>2. Medical</b>. Hospitals and medical research teams often use LDA to predict whether or not a given group of abnormal cells is likely to lead to a mild, moderate, or severe illness.
<b>3. Product development</b>. Companies may build LDA models to predict whether a certain consumer will use their product daily, weekly, monthly, or yearly based on a variety of predictor variables like <em>gender</em>, <em>annual income</em>, and <em>frequency of similar product usage</em>.
<b>4. Ecology.</b> Researchers may build LDA models to predict whether or not a given coral reef will have an overall health of good, moderate, bad, or endangered based on a variety of predictor variables like <em>size</em>, <em>yearly contamination</em>, and <em>age</em>.
<h3>LDA in R & Python</h3>
The following tutorials provide step-by-step examples of how to perform linear discriminant analysis in R and Python:
 Linear Discriminant Analysis in R (Step-by-Step) 
 Linear Discriminant Analysis in Python (Step-by-Step) 
<h2><span class="orange">Linear Interpolation in Excel: Step-by-Step Example</span></h2>
<b>Interpolation</b> is the process of estimating an unknown value of a function between two known values.
Given two known values (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>), we can estimate the y-value for some point x by using the following formula:
y = y<sub>1</sub> + (x-x<sub>1</sub>)(y<sub>2</sub>-y<sub>1</sub>)/(x<sub>2</sub>-x<sub>1</sub>)
This tutorial explains how to use linear interpolation to find some unknown y-value based on an x-value in Excel.
<h3>Example: Linear Interpolation in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/linearInt1.png">
If we create a quick plot of the data, here’s what it would look like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/linearInt2.png">
Now suppose that we’d like to find the y-value associated with a new x-value of <b>13</b>. We can see that we have <em>measured</em> y-values for x-values of 12 and 14, but not for an x-value of 13.
We can use the following formula to perform linear interpolation in Excel to find the estimated y-value:
<b>=FORECAST(NewX,OFFSET(KnownY,MATCH(NewX,KnownX,1)-1,0,2), OFFSET(KnownX,MATCH(NewX,KnownX,1)-1,0,2))
</b>
Here’s how to use this function to estimate the y-values associated with an x-value of 13:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/linearInt3.png">
The estimated y-value turns out to be <b>33.5</b>.
If we add the point (13, 33.5) to our plot, it appears to match the function quite well:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/linearInt4.png">
We can use this formula to estimate the y-value of any x-value by simply replacing the <b>NewX</b> in the formula with any new x-value.
Note that in order for this function to work, the new x-value should fall within the range of the existing x-values.
<em>You can find more Excel tutorials  here .</em>
<h2><span class="orange">How to Perform Linear Interpolation in R (With Example)</span></h2>
<b>Linear interpolation</b> is the process of estimating an unknown value of a function between two known values.
Given two known values (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>), we can estimate the y-value for some point x by using the following formula:
y = y<sub>1</sub> + (x-x<sub>1</sub>)(y<sub>2</sub>-y<sub>1</sub>)/(x<sub>2</sub>-x<sub>1</sub>)
The following example shows how perform linear interpolation in R.
<h3>Example: Linear Interpolation in R</h3>
Suppose we have the following data frame with x and y values in R:
<b>#define data frame
df &lt;- data.frame(x=c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20), y=c(4, 7, 11, 16, 22, 29, 38, 49, 63, 80))
#view data frame
df
    x  y
1   2  4
2   4  7
3   6 11
4   8 16
5  10 22
6  12 29
7  14 38
8  16 49
9  18 63
10 20 80</b>
We can use the following code to create a scatterplot to visualize the (x, y) values in the data frame:
<b>#create scatterplot
plot(df$x, df$y, col='blue', pch=19)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/linearR1.jpg"430">
Now suppose that we’d like to find the y-value associated with a new x-value of <b>13</b>.
We can use the <b>approx()</b> function in R to do so:
<b>#fit linear regression model using data frame
model &lt;- lm(y ~ x, data = df)
#interpolate y value based on x value of 13
y_new = approx(df$x, df$y, xout=13)
#view interpolated y value
y_new
$x
[1] 13
$y
[1] 33.5
</b>
The estimated y-value turns out to be <b>33.5</b>.
If we add the point (13, 33.5) to our plot, it appears to match the function quite well:
<b>#create scatterplot
plot(df$x, df$y, col='blue', pch=19)
#add the predicted point to the scatterplot
points(13, y_new$y, col='red', pch=19)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/linearR2.jpg">
We can use this exact formula to perform linear interpolation for any new x-value.
<h2><span class="orange">How to Perform Linear Interpolation in Python (With Example)</span></h2>
<b>Linear interpolation</b> is the process of estimating an unknown value of a function between two known values.
Given two known values (x<sub>1</sub>, y<sub>1</sub>) and (x<sub>2</sub>, y<sub>2</sub>), we can estimate the y-value for some point x by using the following formula:
y = y<sub>1</sub> + (x-x<sub>1</sub>)(y<sub>2</sub>-y<sub>1</sub>)/(x<sub>2</sub>-x<sub>1</sub>)
We can use the following basic syntax to perform linear interpolation in Python:
<b>import scipy.interpolate
y_interp = scipy.interpolate.interp1d(x, y)
#find y-value associated with x-value of 13
print(y_interp(13))
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Linear Interpolation in Python</h3>
Suppose we have the following two lists of values in Python:
<b>x = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]
y = [4, 7, 11, 16, 22, 29, 38, 49, 63, 80]
</b>
We can create a quick plot x vs. y:
<b>import matplotlib.pyplot as plt
#create plot of x vs. y
plt.plot(x, y, '-ob')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/interpo1.png">
Now suppose that we’d like to find the y-value associated with a new x-value of <b>13</b>.
We can use the following code to do so:
<b>import scipy.interpolate
y_interp = scipy.interpolate.interp1d(x, y)
#find y-value associated with x-value of 13 
print(y_interp(13))
33.5</b>
The estimated y-value turns out to be <b>33.5</b>.
If we add the point (13, 33.5) to our plot, it appears to match the function quite well:
<b>import matplotlib.pyplot as plt
#create plot of x vs. y
plt.plot(x, y, '-ob')
#add estimated y-value to plot
plt.plot(13, 33.5, 'ro')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/interpo2.png">
We can use this exact formula to perform linear interpolation for any new x-value.
<h2><span class="orange">The Four Assumptions of Linear Regression</span></h2>
 <b>Linear regression</b>  is a useful statistical method we can use to understand the relationship between two variables, x and y. However, before we conduct linear regression, we must first make sure that four assumptions are met:
<b>1. Linear relationship:</b> There exists a linear relationship between the independent variable, x, and the dependent variable, y.
<b>2. Independence: </b>The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.
<b>3. Homoscedasticity: </b>The residuals have constant variance at every level of x.
<b>4. Normality: </b>The residuals of the model are normally distributed.
If one or more of these assumptions are violated, then the results of our linear regression may be unreliable or even misleading.
In this post, we provide an explanation for each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2>Assumption 1: Linear Relationship</h2>
<h3>Explanation</h3>
The first assumption of linear regression is that there is a linear relationship between the independent variable, x, and the independent variable, y.
<h3>How to determine if this assumption is met</h3>
The easiest way to detect if this assumption is met is to create a scatter plot of x vs. y. This allows you to visually see if there is a linear relationship between the two variables. If it looks like the points in the plot could fall along a straight line, then there exists some type of linear relationship between the two variables and this assumption is met.
For example, the points in the plot below look like they fall on roughly a straight line, which indicates that there is a linear relationship between x and y:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/assumptionsLinReg1.jpg"491">
However, there doesn’t appear to be a linear relationship between x and y in the plot below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/assumptionsLinReg1-1.jpg"491">
And in this plot there appears to be a clear relationship between x and y, <em>but not a linear relationship</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/01/assumptionsLinReg1-2.jpg"487">
<h3>What to do if this assumption is violated</h3>
If you create a scatter plot of values for x and y and see that there is <em>not </em>a linear relationship between the two variables, then you have a couple options:
<b>1.</b> Apply a nonlinear transformation to the independent and/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and/or dependent variable.
<b>2. </b>Add another independent variable to the model. For example, if the plot of x vs. y has a parabolic shape then it might make sense to add X<sup>2</sup> as an additional independent variable in the model.
<h2>Assumption 2: Independence</h2>
<h3>Explanation</h3>
The next assumption of linear regression is that the residuals are independent. This is mostly relevant when working with time series data. Ideally, we don’t want there to be a pattern among consecutive residuals. For example, residuals shouldn’t steadily grow larger as time goes on.
<h3>How to determine if this assumption is met</h3>
The simplest way to test if this assumption is met is to look at a residual time series plot, which is a plot of residuals vs. time. Ideally, most of the residual autocorrelations should fall within the 95% confidence bands around zero, which are located at about +/- 2-over the square root of <em>n</em>, where <em>n</em> is the sample size. You can also formally test if this assumption is met using the  Durbin-Watson test .
<h3>What to do if this assumption is violated</h3>
Depending on the nature of the way this assumption is violated, you have a few options:
For positive serial correlation, consider adding lags of the dependent and/or independent variable to the model.
For negative serial correlation, check to make sure that none of your variables are <em>overdifferenced</em>.
For seasonal correlation, consider adding seasonal dummy variables to the model.
<h2>Assumption 3: Homoscedasticity</h2>
<h3>Explanation</h3>
The next assumption of linear regression is that the residuals have constant variance at every level of x. This is known as <em>homoscedasticity</em>.  When this is not the case, the residuals are said to suffer from  <em>heteroscedasticity</em> .
When heteroscedasticity is present in a regression analysis, the results of the analysis become hard to trust. Specifically, heteroscedasticity increases the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.
<h3>How to determine if this assumption is met</h3>
The simplest way to detect heteroscedasticity is by creating a <em>fitted value vs. residual plot</em>. 
Once you fit a regression line to a set of data, you can then create a scatterplot that shows the fitted values of the model vs. the residuals of those fitted values. The scatterplot below shows a typical <em>fitted value vs. residual plot</em> in which heteroscedasticity is present.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/het1.jpg">
Notice how the residuals become much more spread out as the fitted values get larger. This “cone” shape is a classic sign of heteroscedasticity:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/02/het2.jpg">
<h3>What to do if this assumption is violated</h3>
There are three common ways to fix heteroscedasticity:
<b>1. Transform the dependent variable. </b>One common transformation is to simply take the log of the dependent variable. For example, if we are using population size (independent variable) to predict the number of flower shops in a city (dependent variable), we may instead try to use population size to predict the log of the number of flower shops in a city. Using the log of the dependent variable, rather than the original dependent variable, often causes heteroskedasticity to go away.
<b>2. Redefine the dependent variable. </b> One common way to redefine the dependent variable is to use a <em>rate</em>, rather than the raw value. For example, instead of using the population size to predict the number of flower shops in a city, we may instead use population size to predict the number of flower shops per capita. In most cases, this reduces the variability that naturally occurs among larger populations since we’re measuring the number of flower shops per person, rather than the sheer amount of flower shops.
<b>3. Use weighted regression. </b>Another way to fix heteroscedasticity is to use weighted regression. This type of regression assigns a weight to each data point based on the variance of its fitted value. Essentially, this gives small weights to data points that have higher variances, which shrinks their squared residuals. When the proper weights are used, this can eliminate the problem of heteroscedasticity.
<h2>Assumption 4: Normality</h2>
<h3>Explanation</h3>
The next assumption of linear regression is that the residuals are normally distributed. 
<h3>How to determine if this assumption is met</h3>
There are two common ways to check if this assumption is met:
<b>1.</b> Check the assumption visually using  Q-Q plots .
A Q-Q plot, short for quantile-quantile plot, is a type of plot that we can use to determine whether or not the residuals of a model follow a normal distribution. If the points on the plot roughly form a straight diagonal line, then the normality assumption is met.
The following Q-Q plot shows an example of residuals that roughly follow a normal distribution:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/qqplot3.jpg">
However, the Q-Q plot below shows an example of when the residuals clearly depart from a straight diagonal line, which indicates that they do not follow  normal distribution:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/qqplot4.jpg">
<b>2. </b>You can also check the normality assumption using formal statistical tests like Shapiro-Wilk, Kolmogorov-Smironov, Jarque-Barre, or D’Agostino-Pearson. However, keep in mind that these tests are sensitive to large sample sizes – that is, they often conclude that the residuals are not normal when your sample size is large. This is why it’s often easier to just use graphical methods like a Q-Q plot to check this assumption.
<h3>What to do if this assumption is violated</h3>
If the normality assumption is violated, you have a few options:
First, verify that any outliers aren’t having a huge impact on the distribution. If there are outliers present, make sure that they are real values and that they aren’t data entry errors.
Next, you can apply a nonlinear transformation to the independent and/or dependent variable. Common examples include taking the log, the square root, or the reciprocal of the independent and/or dependent variable.
<b>Further Reading:</b>
 Introduction to Simple Linear Regression 
 Understanding Heteroscedasticity in Regression Analysis 
 How to Create & Interpret a Q-Q Plot in R 
<h2><span class="orange">How to Perform Linear Regression by Hand</span></h2>
 Simple linear regression  is a statistical method you can use to quantify the relationship between a predictor variable and a  response variable .
This tutorial explains how to perform simple linear regression by hand.
<h3>Example: Simple Linear Regression by Hand</h3>
Suppose we have the following dataset that shows the weight and height of seven individuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/linHand1-1.png">
Use the following steps to fit a linear regression model to this dataset, using weight as the predictor variable and height as the response variable.
<b>Step 1: Calculate X*Y, X<sup>2</sup>, and Y<sup>2</sup></b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/linHand2.png">
<b>Step 2: Calculate ΣX, ΣY, ΣX*Y, ΣX<sup>2</sup>, and ΣY<sup>2</sup></b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/linHand3.png">
<b>Step 3: Calculate b<sub>0</sub></b>
The formula to calculate b<sub>0 </sub>is: [(ΣY)(ΣX<sup>2</sup>) – (ΣX)(ΣXY)]  /  [n(ΣX<sup>2</sup>) – (ΣX)<sup>2</sup>]
In this example, b<sub>0 </sub>= [(477)(222755) – (1237)(85125)]  /  [7(222755) – (1237)<sup>2</sup>] = <b>32.783</b>
<b>Step 4: Calculate b<sub>1</sub></b>
The formula to calculate b<sub>1 </sub>is: [n(ΣXY) – (ΣX)(ΣY)]  /  [n(ΣX<sup>2</sup>) – (ΣX)<sup>2</sup>]
In this example, b<sub>1 </sub>= [7(85125) – (1237)(477)]  /  [7(222755) – (1237)<sup>2</sup>] = <b>0.2001</b>
<b>Step 5: Place b<sub>0 </sub>and b<sub>1</sub> in the estimated linear regression equation.</b>
The estimated linear regression equation is: <U+0177> = b<sub>0</sub> + b<sub>1</sub>*x
In our example, it is <b><U+0177> = 0.32783 + (0.2001)*x</b>
<h3>How to Interpret a Simple Linear Regression Equation</h3>
Here is how to interpret this estimated linear regression equation: <U+0177> = 32.783 + 0.2001x
<b>b<sub>0</sub> = 32.7830</b>. When weight is zero pounds, the predicted height is 32.783 inches. Sometimes the value for b<sub>0</sub> can be useful to know, but in this example it doesn’t actually make sense to interpret b<sub>0</sub> since a person can’t weigh zero pounds.
<b>b<sub>1 </sub>= 0.2001</b>. A one pound increase in weight is associated with a 0.2001 inch increase in height.
<h3>Simple Linear Regression Calculator</h3>
We can double check our results by inputting our data into the  simple linear regression calculator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/linHand0.png">
This equation matches the one that we calculated by hand.
<h2><span class="orange">Linear Regression Calculator</span></h2>
This calculator produces a linear regression equation based on values for a predictor variable and a response variable.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">6, 7, 7, 8, 12, 14, 15, 16, 16, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">14, 15, 15, 17, 18, 18, 19, 24, 25, 29</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<b>Linear Regression Equation:</b>
<U+0177> = 0.9694 + (7.7673)*x
<b>Goodness of Fit:</b>
R Square: 0.8282
<b>Interpretation:</b>
When the predictor variable is equal to 0, the average value for the response variable is 0.9694.
Each one unit increase in the predictor variable is associated with an average change of (7.7673) in the response variable.
82.82% of the variation in the response variable can be explained by the predictor variable.
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        lr['r2'] = Math.pow((n*sum_xy - sum_x*sum_y)/Math.sqrt((n*sum_xx-sum_x*sum_x)*(n*sum_yy-sum_y*sum_y)),2);
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope;
var b = lr.intercept;
var r2 = lr.r2;
var r2p = r2*100;
document.getElementById('a').innerHTML = a.toFixed(4);
document.getElementById('b').innerHTML = b.toFixed(4);
document.getElementById('r2').innerHTML = r2.toFixed(4);
document.getElementById('interceptOut').innerHTML = b.toFixed(4);
document.getElementById('slopeOut').innerHTML = a.toFixed(4);
document.getElementById('r2Out').innerHTML = r2p.toFixed(2);
}
//output error message if boths lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Perform Linear Regression in Google Sheets</span></h2>
<b>Linear regression</b> is a method that can be used to quantify the relationship between one or more  explanatory variables  and a  response variable .
We use simple linear regression when there is only one explanatory variable and multiple linear regression when there are two or more explanatory variables.
It’s possible to perform both types of regressions using the  LINEST()  function in Google Sheets, which uses the following syntax:
<b>LINEST(known_data_y, known_data_x, calculate_b, verbose)</b>
where:
<b>known_data_y: </b>Array of response values
<b>known_data_x: </b>Array of explanatory values
<b>calculate_b: </b>Indicates whether or not to calculate the y-intercept. This is TRUE by default and we leave it this way for linear regression.
<b>verbose: </b>Indicates whether or not to provide additional regression statistics beyond just the slope and intercept. This is FALSE by default, but we will specify this to be TRUE in our examples.
The following examples show how to use this function in practice.
<h3>Simple Linear Regression in Google Sheets</h3>
Suppose we are interested in understanding the relationship between <em>hours studied</em> and <em>exam score.</em> studies for an exam and the exam score they receive.
To explore this relationship, we can perform simple linear regression using <em>hours studied</em> as an explanatory variable and <em>exam score </em>as a response variable.
The following screenshot shows how to perform simple linear regression using a dataset of 20 students with the following formula used in cell D2:
<b>=LINEST(B2:B21, A2:A21, TRUE, TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/regressionGoogleSheets.png">
The following screenshot provide annotations for the output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/regressionGoogleSheets2.png">
Here is how to interpret the most relevant numbers in the output:
<b>R Square: 0.72725</b>. This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variable. In this example, roughly 72.73% of the variation in the exam scores can be explained by the number of hours studied.
<b>Standard error:</b> <b>5.2805</b>. This is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of 5.2805 units from the regression line.
<b>Coefficients: </b>The coefficients give us the numbers necessary to write the estimated regression equation. In this example the estimated regression equation is:
<b>Exam score = 67.16 + 5.2503*(hours)</b>
We interpret the coefficient for hours to mean that for each additional hour studied, the exam score is expected to increase by <b>5.2503</b>, on average. We interpret the coefficient for the intercept to mean that the expected exam score for a student who studies zero hours is <b>67.16</b>.
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study. For example, a student who studies for three hours is expected to receive an exam score of <b>82.91</b>:
Exam score = 67.16 + 5.2503*(3) = 82.91
<h3>Multiple Linear Regression in Google Sheets</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain college entrance exam.
To explore this relationship, we can perform multiple linear regression using <em>hours studied</em> and <em>prep exams taken </em>as explanatory variables and <em>exam score </em>as a response variable.
The following screenshot shows how to perform multiple linear regression using a dataset of 20 students with the following formula used in cell E2:
<b>=LINEST(C2:C21, A2:B21, TRUE, TRUE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/regressionGoogleSheets3.png">
Here is how to interpret the most relevant numbers in the output:
<b>R Square: 0.734</b>. This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variables. In this example, 73.4% of the variation in the exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>Standard error:</b> <b>5.3657</b>. This is the average distance that the observed values fall from the regression line. In this example, the observed values fall an average of 5.3657 units from the regression line.
<b>Estimated regression equation: </b>We can use the coefficients from the output of the model to create the following estimated regression equation:
<b>Exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams)</b>
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study and the number of prep exams they take. For example, a student who studies for three hours and takes one prep exam is expected to receive a score of <b>83.75</b>:
Exam score = 67.67 + 5.56*(3) – 0.60*(1) = 83.75
<h2><span class="orange">How to Interpret P-Values in Linear Regression (With Example)</span></h2>
In statistics, linear regression models are used to quantify the relationship between one or more predictor variables and a  response variable .
Whenever you perform regression analysis using some statistical software, you will receive a regression table that summarizes the results of the model.
Two of the most important values in a regression table are the regression coefficients and their corresponding <b>p-values</b>.
The p-values tell you whether or not there is a statistically significant relationship between each predictor variable and the response variable.
The following example shows how to interpret the p-values of a  multiple linear regression  model in practice.
<h3>Example: Interpreting P-Values in Regression Model</h3>
Suppose we want to fit a regression model using the following variables:
<b>Predictor Variables</b>
Total number of hours studied (between 0 and 20)
Whether or not a student used a tutor (yes or no)
<b>Response Variable</b>
Exam score ( between  0 and 100)
We want to examine the relationship between the predictor variables and the response variable to find out if hours studied and tutoring actually have a meaningful impact on exam score.
Suppose we run a regression analysis and get the following output:
<table><tbody>
<tr>
<th style="text-align: left;">Term</th>
<th style="text-align: left;">Coefficient</th>
<th style="text-align: left;">Standard Error</th>
<th style="text-align: left;">t Stat</th>
<th style="text-align: left;">P-value</th>
</tr>
<tr>
<td><b>Intercept</b></td>
<td>48.56</td>
<td>14.32</td>
<td>3.39</td>
<td>0.002</td>
</tr>
<tr>
<td><b>Hours studied</b></td>
<td>2.03</td>
<td>0.67</td>
<td>3.03</td>
<td>0.009</td>
</tr>
<tr>
<td><b>Tutor</b></td>
<td>8.34</td>
<td>5.68</td>
<td>1.47</td>
<td>0.138</td>
</tr>
</tbody></table>
Here’s how to interpret the output for each term in the model:
<h3>Interpreting the P-value for Intercept</h3>
The <b>intercept </b>term in a regression table tells us the average expected value for the response variable when all of the predictor variables are equal to zero.
In this example, the regression coefficient for the intercept is equal to <b>48.56</b>. This means that for a student who studied for zero hours<em>, </em>the average expected exam score is 48.56.
The p-value is <b>0.002</b>, which tells us that the intercept term is statistically different than zero.
In practice, we don’t usually care about the p-value for the intercept term. Even if the p-value isn’t less than some significance level (e.g. 0.05), we would still keep the intercept term in the model.
<h3>Interpreting the P-value for a Continuous Predictor Variable</h3>
In this example, <b>Hours studied </b>is a continuous predictor variable that ranges from 0 to 20 hours.
From the regression output, we can see that the regression coefficient for Hours studied is <b>2.03</b>. This means that, on average, each additional hour studied is associated with an increase of 2.03 points on the final exam, assuming the predictor variable <b>Tutor </b>is held constant.
For example, consider student A who studies for 10 hours and uses a tutor. Also consider student B who studies for 11 hours and also uses a tutor. According to our regression output, student B is expected to receive an exam score that is <b>2.03</b> points higher than student A.
The corresponding p-value is <b>0.009</b>, which is statistically significant at an alpha level of 0.05.
This tells us that that the average change in exam score for each additional hour studied is <b>statistically significantly different than zero</b>.
Another way to put this: <b>Hours studied</b> has a statistically significant relationship with the response variable <b>exam score</b>.
<h3>Interpreting the P-value for a Categorical Predictor Variable</h3>
In this example, <b>Tutor </b>is a categorical predictor variable that can take on two different values:
1 = the student used a tutor to prepare for the exam
0 = the student did not used a tutor to prepare for the exam
From the regression output, we can see that the regression coefficient for Tutor is <b>8.34</b>. This means that, on average, a student who used a tutor scored 8.34 points higher on the exam compared to a student who did not used a tutor, assuming the predictor variable Hours studied is held constant.
For example, consider student A who studies for 10 hours and uses a tutor. Also consider student B who studies for 10 hours and does not use a tutor. According to our regression output, student A is expected to receive an exam score that is 8.34 points higher than student B.
The corresponding p-value is <b>0.138</b>, which is not statistically significant at an alpha level of 0.05.
This tells us that that the average change in exam score for each additional hour studied is <b>not statistically significantly different than zero</b>.
Another way to put this: The predictor variable <b>Tutor</b> does not have a statistically significant relationship with the response variable <b>exam score</b>.
This indicates that although students who used a tutor scored higher on the exam, this difference could have been due to random chance.
<h2><span class="orange">A Complete Guide to Linear Regression in Python</span></h2>
<b>Linear regression</b> is a method we can use to understand the relationship between one or more predictor variables and a response variable.
This tutorial explains how to perform linear regression in Python.
<h3>Example: Linear Regression in Python</h3>
Suppose we want to know if the number of hours spent studying and the number of prep exams taken affects the score that a student receives on a certain exam.
To explore this relationship, we can perform the following steps in Python to conduct a multiple linear regression.
<b>Step 1: Enter the data.</b>
First, we’ll create a pandas DataFrame to hold our dataset:
<b>import pandas as pd
#create data
df = pd.DataFrame({'hours': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6, 5, 3, 4, 6, 2, 1, 2],   'exams': [1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2, 4, 4, 4, 5, 1, 0, 1],   'score': [76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96, 90, 82, 85, 99, 83, 62, 76]})
#view data 
df
        hoursexams   score
01176
12378
22385
34588
42272
51269
65194
74194
82088
94392
104490
113375
126296
135490
143482
154485
166599
172183
181062
192176
</b>
<b>Step 2: Perform linear regression.</b>
Next, we’ll use the  OLS() function  from the statsmodels library to perform ordinary least squares regression, using “hours” and “exams” as the predictor variables and “score” as the response variable:
<b>import statsmodels.api as sm
#define response variable
y = df['score']
#define predictor variables
x = df[['hours', 'exams']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.734
Model:                            OLS   Adj. R-squared:                  0.703
Method:                 Least Squares   F-statistic:                     23.46
Date:                Fri, 24 Jul 2020   Prob (F-statistic):           1.29e-05
Time:                        13:20:31   Log-Likelihood:                -60.354
No. Observations:                  20   AIC:                             126.7
Df Residuals:                      17   BIC:                             129.7
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         67.6735      2.816     24.033      0.000      61.733      73.614
hours          5.5557      0.899      6.179      0.000       3.659       7.453
exams         -0.6017      0.914     -0.658      0.519      -2.531       1.327
==============================================================================
Omnibus:                        0.341   Durbin-Watson:                   1.506
Prob(Omnibus):                  0.843   Jarque-Bera (JB):                0.196
Skew:                          -0.216   Prob(JB):                        0.907
Kurtosis:                       2.782   Cond. No.                         10.8
==============================================================================
</b>
<b>Step 3: Interpret the results.</b>
Here is how to interpret the most relevant numbers in the output:
<b>R-squared: 0.734</b>. This is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the predictor variables. In this example, 73.4% of the variation in the exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>F-statistic: 23.46</b>. This is the overall F-statistic for the regression model.
<b>Prob (F-statistic): 1.29e-05. </b>This is the p-value associated with the overall F-statistic.  It tells us whether or not the regression model as a whole is statistically significant. In other words, it tells us if the two predictor variables combined have a statistically significant association with the response variable. In this case the p-value is less than 0.05, which indicates that the predictor variables “hours studied” and “prep exams taken” combined have a statistically significant association with exam score.
<b>coef: </b>The coefficients for each predictor variable tell us the average expected change in the response variable, assuming the other predictor variable remains constant. For example, for each additional hour spent studying, the average exam score is expected to increase by <b>5.56</b>, assuming that prep exams taken remains constant.
Here’s another way to think about this: If student A and student B both take the same amount of prep exams but student A studies for one hour more, then student A is expected to earn a score that is <b>5.56</b> points higher than student B.
We interpret the coefficient for the intercept to mean that the expected exam score for a student who studies zero hours and takes zero prep exams is <b>67.67</b>.
<b>P>|t|. </b>The individual p-values tell us whether or not each predictor variable is statistically significant. We can see that “hours” is statistically significant (p = 0.00) while “exams”<b> </b>(p = 0.52) is not statistically significant at α = 0.05. Since “exams” is not statistically significant, we may end up deciding to remove it from the model.
<b>Estimated regression equation: </b>We can use the coefficients from the output of the model to create the following estimated regression equation:
<b>exam score = 67.67 + 5.56*(hours) – 0.60*(prep exams)</b>
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study and the number of prep exams they take. For example, a student who studies for three hours and takes one prep exam is expected to receive a score of <b>83.75</b>:
Keep in mind that because prep exams taken was not statistically significant (p = 0.52), we may decide to remove it because it doesn’t add any improvement to the overall model. In this case, we could perform simple linear regression using only hours studied as the predictor variable.
<b>Step 4: Check model assumptions.</b>
Once you perform linear regression, there are several assumptions you may want to check to ensure that the results of the regression model are reliable. These assumptions include:
<b>Assumption #1: </b>There exists a linear relationship between the predictor variables and the response variable.
<li data-slot-rendered-dynamic="true">Check this assumption by generating a  residual plot  that displays the fitted values against the residual values for a regression model.
<b>Assumption #2: </b>Independence of residuals.
Check this assumption by performing a  Durbin-Watson Test .
<b>Assumption #3: </b>Homoscedasticity of residuals.
Check this assumption by performing a  Breusch-Pagan Test .
<b>Assumption #4: </b>Normality of residuals.
Check this assumption visually using a  Q-Q plot .
Check this assumption with formal tests like a  Jarque-Bera Test  or an  Anderson-Darling Test .
<b>Assumption #5: </b>Verify that multicollinearity doesn’t exist among predictor variables.
Check this assumption by calculating the  VIF value  of each predictor variable.
If these assumptions are met, you can be confident that the results of your multiple linear regression model are reliable.
<em>You can find the complete Python code used in this tutorial  here .</em>
<h2><span class="orange">4 Examples of Using Linear Regression in Real Life</span></h2>
<b>Linear regression </b>is one of the most commonly used techniques in statistics. It is used to quantify the relationship between one or more predictor variables and a response variable.
The most basic form of linear is regression is known as  simple linear regression , which is used to quantify the relationship between one predictor variable and one response variable.
If we have more than one predictor variable then we can use multiple linear regression, which is used to quantify the relationship between several predictor variables and a response variable.
This tutorial shares four different examples of when linear regression is used in real life.
<h3>Linear Regression Real Life Example #1</h3>
Businesses often use linear regression to understand the relationship between advertising spending and revenue.
For example, they might fit a simple linear regression model using advertising spending as the predictor variable and revenue as the response variable. The regression model would take the following form:
<b>revenue = β<sub>0</sub> + β<sub>1</sub>(ad spending)</b>
The coefficient <b>β<sub>0</sub></b> would represent total expected revenue when ad spending is zero.
The coefficient <b>β<sub>1</sub></b> would represent the average change in  total revenue when ad spending is increased by one unit (e.g. one dollar).
If β<sub>1</sub> is negative, it would mean that more ad spending is associated with less revenue.
If β<sub>1</sub> is close to zero, it would mean that ad spending has little effect on revenue.
And if β<sub>1</sub> is positive, it would mean more ad spending is associated with more revenue.
Depending on the value of β<sub>1</sub>, a company may decide to either decrease or increase their ad spending.
<h3>Linear Regression Real Life Example #2</h3>
Medical researchers often use linear regression to understand the relationship between drug dosage and blood pressure of patients.
For example, researchers might administer various dosages of a certain drug to patients and observe how their blood pressure responds. They might fit a simple linear regression model using dosage as the predictor variable and blood pressure as the response variable. The regression model would take the following form:
<b>blood pressure = β<sub>0</sub> + β<sub>1</sub>(dosage)</b>
The coefficient <b>β<sub>0</sub></b> would represent the expected blood pressure when dosage is zero.
The coefficient <b>β<sub>1</sub></b> would represent the average change in  blood pressure when dosage is increased by one unit.
If β<sub>1</sub> is negative, it would mean that an increase in dosage is associated with a decrease in blood pressure.
If β<sub>1</sub> is close to zero, it would mean that an increase in dosage is associated with no change in blood pressure.
If β<sub>1</sub> is positive, it would mean that an increase in dosage is associated with an increase in blood pressure.
Depending on the value of β<sub>1</sub>, researchers may decide to change the dosage given to a patient.
<h3>Linear Regression Real Life Example #3</h3>
Agricultural scientists often use linear regression to measure the effect of fertilizer and water on crop yields.
For example, scientists might use different amounts of fertilizer and water on different fields and see how it affects crop yield. They might fit a multiple linear regression model using fertilizer and water as the predictor variables and crop yield as the response variable. The regression model would take the following form:
<b>crop yield = β<sub>0</sub> + β<sub>1</sub>(amount of fertilizer) + β<sub>2</sub>(amount of water)</b>
The coefficient <b>β<sub>0</sub></b> would represent the expected crop yield with no fertilizer or water.
The coefficient <b>β<sub>1</sub></b> would represent the average change in crop yield when fertilizer is increased by one unit, <em>assuming the amount of water remains unchanged.</em>
The coefficient <b>β<sub>2</sub></b> would represent the average change in crop yield when water is increased by one unit, <em>assuming the amount of fertilizer remains unchanged.</em>
Depending on the values of β<sub>1</sub> and β<sub>2</sub>, the scientists may change the amount of fertilizer and water used to maximize the crop yield.
<h3>Linear Regression Real Life Example #4</h3>
Data scientists for professional sports teams often use linear regression to measure the effect that different training regimens have on player performance.
For example, data scientists in the NBA might analyze how different amounts of weekly yoga sessions and weightlifting sessions affect the number of points a player scores. They might fit a multiple linear regression model using yoga sessions and weightlifting sessions as the predictor variables and total points scored as the response variable. The regression model would take the following form:
<b>points scored = β<sub>0</sub> + β<sub>1</sub>(yoga sessions) + β<sub>2</sub>(weightlifting sessions)</b>
The coefficient <b>β<sub>0</sub></b> would represent the expected points scored for a player who participates in zero yoga sessions and zero weightlifting sessions.
The coefficient <b>β<sub>1</sub></b> would represent the average change in points scored when weekly yoga sessions is increased by one, <em>assuming the number of weekly weightlifting sessions remains unchanged.</em>
The coefficient <b>β<sub>2</sub></b> would represent the average change in points scored when weekly weightlifting sessions is increased by one, <em>assuming the number of weekly yoga sessions remains unchanged.</em>
Depending on the values of β<sub>1</sub> and β<sub>2</sub>, the data scientists may recommend that a player participates in more or less weekly yoga and weightlifting sessions in order to maximize their points scored.
<h2>Conclusion</h2>
Linear regression is used in a wide variety of real-life situations across many different types of industries. Fortunately, statistical software makes it easy to perform linear regression.
Feel free to explore the following tutorials to learn how to perform linear regression using different softwares:
 How to Perform Simple Linear Regression in Excel 
 How to Perform Multiple Linear Regression in Excel 
 How to Perform Multiple Linear Regression in R 
 How to Perform Multiple Linear Regression in Stata 
 How to Perform Linear Regression on a TI-84 Calculator 
<h2><span class="orange">How to Perform Linear Regression on a TI-84 Calculator</span></h2>
<b>Linear regression </b>is a method we can use to understand the relationship between an explanatory variable, x, and a response variable, y.
This tutorial explains how to perform linear regression on a TI-84 calculator.
<h3>Example: Linear Regression on a TI-84 Calculator</h3>
Suppose we are interested in understanding the relationship between the number of hours a student studies for an exam and the exam score they receive.
To explore this relationship, we can perform the following steps on a TI-84 calculator to conduct a simple linear regression using hours studied as an explanatory variable and exam score as a response variable.
<b>Step 1: Input the data.</b>
First, we will input the data values for both the explanatory and the response variable. Press  Stat  and then press  EDIT . Enter the following values for the explanatory variable (hours studied) in column L1 and the values for the response variable (exam score) in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/linRegTI1.png">
<b>Step 2: Perform linear regression.</b>
Next, we will perform linear regression. Press Stat and then scroll over to <b>CALC</b>. Then scroll down to <b>8: Linreg(a+bx)</b> and press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/correlationTI5.png">
For Xlist and Ylist, make sure L1 and L2 are selected since these are the columns we used to input our data. Leave <b>FreqList </b>blank. Scroll down to <b>Calculate </b>and press Enter. The following output will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/linRegTI2.png">
<b>Step 3: Interpret the results.</b>
From the results, we can see that the estimated regression equation is as follows:
<b>exam score = 68.7127 + 5.5138*(hours)</b>
We interpret the coefficient for hours to mean that for each additional hour studied, the exam score is expected to increase by <b>5.5138</b>, on average. We interpret the coefficient for the intercept to mean that the expected exam score for a student who studies zero hours is <b>68.7127</b>.
We can use this estimated regression equation to calculate the expected exam score for a student, based on the number of hours they study.
For example, a student who studies for three hours is expected to receive an exam score of <b>85.25</b>:
exam score = 68.7127 + 5.5138*(3) = 85.25
We can also see that the r-squared for the regression model is r<sup>2</sup> = <b>0.7199</b>.
This value is known as the coefficient of determination. It is the proportion of the variance in the response variable that can be explained by the explanatory variable. In this example, 71.99% of the variation in the exam scores can be explained by the number of hours studied.
<h2><span class="orange">Introduction to Simple Linear Regression</span></h2>
<b>Simple linear regression</b> is a statistical method you can use to understand the relationship between two variables, x and y.
One variable, <b>x</b>, is known as the <b>predictor variable</b>.
The other variable, <b>y</b>, is known as the <b>response variable</b>.
For example, suppose we have the following dataset with the weight and height of seven individuals:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height1.jpg"> 
Let <em>weight </em>be the predictor variable and let <em>height </em>be the response variable.
If we graph these two variables using a  scatterplot , with weight on the x-axis and height on the y-axis, here’s what it would look like:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height.jpg"> 
Suppose we’re interested in understanding the relationship between weight and height. From the scatterplot we can clearly see that as weight increases, height tends to increase as well, but to actually <em>quantify </em>this relationship between weight and height, we need to use linear regression.
Using linear regression, we can find the line that best “fits” our data. This line is known as the <b>least squares regression line </b>and it can be used to help us understand the relationships between weight and height.
Usually you would use software like Microsoft Excel, SPSS, or a graphing calculator to actually find the equation for this line.
The formula for the line of best fit is written as:
<U+0177> = b<sub>0</sub> + b<sub>1</sub>x
where <U+0177> is the predicted value of the response variable, b<sub>0</sub> is the y-intercept, b<sub>1</sub> is the regression coefficient, and x is the value of the predictor variable.
<b>Related:</b>  4 Examples of Using Linear Regression in Real Life 
<h2>Finding the “Line of Best Fit”</h2>
For this example, we can simply plug our data into the  Statology Linear Regression Calculator  and hit <em>Calculate</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/linRegCalc1.png">
The calculator automatically finds the <b>least squares regression line</b>:
<U+0177> = 32.7830 + 0.2001x
If we zoom out on our scatterplot from earlier and added this line to the chart, here’s what it would look like:
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/weight_height4.jpg"543"> 
Notice how our data points are scattered closely around this line. That’s because this least squares regression lines is the best fitting line for our data out of all the possible lines we could draw.
<h2>How to Interpret a Least Squares Regression Line</h2>
Here is how to interpret this least squares regression line: <U+0177> = 32.7830 + 0.2001x
<b>b<sub>0</sub> = 32.7830</b>. This means when the predictor variable <em>weight</em> is zero pounds, the predicted height is 32.7830 inches. Sometimes the value for b<sub>0</sub> can be useful to know, but in this specific example it doesn’t actually make sense to interpret b<sub>0</sub> since a person can’t weight zero pounds.
<b>b<sub>1 </sub>= 0.2001</b>. This means that a one unit increase in <em>x</em> is associated with a 0.2001 unit increase in <em>y</em>. In this case, a one pound increase in weight is associated with a 0.2001 inch increase in height.
<h2>How to Use the Least Squares Regression Line</h2>
Using this least squares regression line, we can answer questions like:
<em>For a person who weighs 170 pounds, how tall would we expect them to be?</em>
To answer this, we can simply plug in 170 into our regression line for x and solve for y:
<U+0177> = 32.7830 + 0.2001(170) = <b>66.8 inches</b>
<em>For a person who weighs 150 pounds, how tall would we expect them to be?</em>
To answer this, we can plug in 150 into our regression line for x and solve for y:
<U+0177> = 32.7830 + 0.2001(150) = <b>62.798 inches</b>
<b>Caution:</b> <em>When using a regression equation to answer questions like these, make sure you only use values for the predictor variable that are within the range of the predictor variable in the original dataset we used to generate the least squares regression line. For example, the weights in our dataset ranged from 140 lbs to 212 lbs, so it only makes sense to answer questions about predicted height when the weight is between 140 lbs and 212 lbs.</em>
<h2>The Coefficient of Determination</h2>
One way to measure how well the least squares regression line “fits” the data is using the <b>coefficient of determination</b>, denoted as R<sup>2</sup>.
The coefficient of determination is the proportion of the variance in the response variable that can be explained by the predictor variable.
The coefficient of determination can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all. A value of 1 indicates that the response variable can be perfectly explained without error by the predictor variable.
An R<sup>2 </sup>between 0 and 1 indicates just how well the response variable can be explained by the predictor variable. For example, an R<sup>2 </sup>of 0.2 indicates that 20% of the variance in the response variable can be explained by the predictor variable; an R<sup>2 </sup>of 0.77 indicates that 77% of the variance in the response variable can be explained by the predictor variable.
Notice in our output from earlier we got an R<sup>2 </sup>of 0.9311, which indicates that 93.11% of the variability in height can be explained by the predictor variable of weight:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/linRegCalc2.png">
This tells us that weight is a very good predictor of height.
<h2>Assumptions of Linear Regression</h2>
For the results of a linear regression model to be valid and reliable, we need to check that the following four assumptions are met:
<b>1. Linear relationship:</b> There exists a linear relationship between the independent variable, x, and the dependent variable, y.
<b>2. Independence: </b>The residuals are independent. In particular, there is no correlation between consecutive residuals in time series data.
<b>3. Homoscedasticity: </b>The residuals have constant variance at every level of x.
<b>4. Normality: </b>The residuals of the model are normally distributed.
If one or more of these assumptions are violated, then the results of our linear regression may be unreliable or even misleading.
Refer to  this post  for an explanation for each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2><span class="orange">How to Use the linearHypothesis() Function in R</span></h2>
You can use the <b>linearHypothesis()</b> function from the <b>car</b> package in R to test linear hypotheses in a specific regression model.
This function uses the following basic syntax:
<b>linearHypothesis(fit, c("var1=0", "var2=0"))</b>
This particular example tests if the regression coefficients <b>var1</b> and <b>var2</b> in the model called <b>fit</b> are jointly equal to zero.
The following example shows how to use this function in practice.
<h2>Example: How to Use linearHypothesis() Function in R</h2>
Suppose we have the following data frame in R that shows the number of hours spent studying, number of practice exams taken, and final exam score for 10 students in some class:
<b>#create data frame
df &lt;- data.frame(score=c(77, 79, 84, 85, 88, 99, 95, 90, 92, 94), hours=c(1, 1, 2, 3, 2, 4, 4, 2, 3, 3), prac_exams=c(2, 4, 4, 2, 4, 5, 4, 3, 2, 1))
#view data frame
df
   score hours prac_exams
1     77     1          2
2     79     1          4
3     84     2          4
4     85     3          2
5     88     2          4
6     99     4          5
7     95     4          4
8     90     2          3
9     92     3          2
10    94     3          1
</b>
Now suppose we would like to fit the following multiple linear regression model in R:
Exam score = β<sub>0</sub> + β<sub>1</sub>(hours) + β<sub>2</sub>(practice exams)
We can use the  lm()  function to fit this model:
<b>#fit multiple linear regression model
fit &lt;- lm(score ~ hours + prac_exams, data=df)
#view summary of model
summary(fit)
Call:
lm(formula = score ~ hours + prac_exams, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-5.8366 -2.0875  0.1381  2.0652  4.6381 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  72.7393     3.9455  18.436 3.42e-07 ***
hours         5.8093     1.1161   5.205  0.00125 ** 
prac_exams    0.3346     0.9369   0.357  0.73150    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.59 on 7 degrees of freedom
Multiple R-squared:  0.8004,Adjusted R-squared:  0.7434 
F-statistic: 14.03 on 2 and 7 DF,  p-value: 0.003553
</b>
Now suppose we would like to test if the coefficient for <b>hours</b> and <b>prac_exams</b> are both equal to zero.
We can use the <b>linearHypothesis()</b> function to do so:
<b>library(car)
#perform hypothesis test for hours=0 and prac_exams=0
linearHypothesis(fit, c("hours=0", "prac_exams=0"))
Linear hypothesis test
Hypothesis:
hours = 0
prac_exams = 0
Model 1: restricted model
Model 2: score ~ hours + prac_exams
  Res.Df    RSS Df Sum of Sq      F   Pr(>F)   
1      9 452.10                                
2      7  90.24  2    361.86 14.035 0.003553 **
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
The hypothesis test returns the following values:
<b>F test statistic</b>: 14.035
<b>p-value</b>: .003553
This particular hypothesis test uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: Both regression coefficients are equal to zero.
<b>H<sub>A</sub></b>: At least one regression coefficient is not equal to zero.
Since the p-value of the test (.003553) is less than .05, we reject the null hypothesis.
In other words, we don’t have sufficient evidence to say that the regression coefficients for <b>hours</b> and <b>prac_exams</b> are both equal to zero.
<h2>Additional Resources</h2>
The following tutorials provide additional information about linear regression in R:
 How to Interpret Regression Output in R 
 How to Perform Simple Linear Regression in R 
 How to Perform Multiple Linear Regression in R 
 How to Perform Logistic Regression in R 
<h2><span class="orange">How to Convert a List to a DataFrame in Python</span></h2>
Often you may want to convert a list to a DataFrame in Python.
Fortunately this is easy to do using the  pandas.DataFrame  function, which uses the following syntax:
<b>pandas.DataFrame(data=None, index=None, columns=None, …)</b>
where:
<b>data:</b> The data to convert into a DataFrame
<b>index:</b> Index to use for the resulting DataFrame
<b>columns:</b> Column labels to use for the resulting DataFrame
This tutorial provides several examples of how to use this function in practice.
<h3>Example 1: Convert One List to a DataFrame</h3>
The following code shows how to convert one list into a pandas DataFrame:
<b>import pandas as pd
#create list that contains points scored by 10 basketball players
data = [4, 14, 17, 22, 26, 29, 33, 35, 35, 38]
#convert list to DataFrame
df = pd.DataFrame(data, columns=['points'])
#view resulting DataFrame
print(df)
       points
0       4
1      14
2      17
3      22
4      26
5      29
6      33
7      35</b>
<h3>Example 2: Convert Several Lists to a DataFrame</h3>
The following code shows how to convert several lists into a pandas DataFrame:
<b>import pandas as pd
#define lists
points = [4, 14, 17, 22, 26, 29, 33, 35, 35, 38]
rebounds = [1, 4, 4, 5, 8, 7, 5, 6, 9, 11]
#convert lists into a single list
data = []
data.append(points)
data.append(rebounds)
#view new list
data
[[4, 14, 17, 22, 26, 29, 33, 35, 35, 38], [1, 4, 4, 5, 8, 7, 5, 6, 9, 11]]
#convert list into DataFrame
df = pd.DataFrame(data).transpose()
df.columns=['points', 'rebounds']
#view resulting DataFrame
df
       points     rebounds
0       4         1
1      14         4
2      17         4
3      22         5
4      26         8
5      29         7
6      33         5
7      35         6
8      35         9
9      38         11
</b>
<h3>Example 3: Convert List of Lists to a DataFrame</h3>
The following code shows how to convert a list of lists into a pandas DataFrame:
<b>import pandas as pd
#define list of lists
data = [[4, 1], [14, 4], [17, 4], [22, 5], [26, 8],
        [29, 7], [33, 5], [35, 6], [35, 9], [38,11]]
#convert list into DataFrame
df = pd.DataFrame(data, columns=['points', 'rebounds'])
#view resulting DataFrame
df
       points     rebounds
0       4         1
1      14         4
2      17         4
3      22         5
4      26         8
5      29         7
6      33         5
7      35         6
8      35         9
9      38         11</b>
You can use the following code to quickly check how many rows and columns are in the resulting DataFrame:
<b>#display number of rows and columns in DataFrame
df.shape
(10, 2)
</b>
We can see that the resulting DataFrame has <b>10</b> rows and <b>2</b> columns.
And we can use the following code to retrieve the names of the columns in the resulting DataFrame:
<b>#display column names of DataFrame
list(df)
['points', 'rebounds']
</b>
<h2><span class="orange">How to Perform Listwise Deletion in R (With Example)</span></h2>
<b>Listwise deletion</b> is a method that deletes all rows from a data frame that have a missing value in <em>any</em> column.
The easiest way to perform listwise deletion in R is to use the following syntax:
<b>complete_df &lt;- df[complete.cases(df), ]
</b>
This syntax uses the  complete.cases()  function to create a new data frame that only contains the rows from an original data frame that have no missing values in any column.
The following example shows how to use this function in practice.
<h3>Example: Perform Listwise Deletion in R</h3>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>#create data frame
df &lt;- data.frame(rating=c(70, 75, 75, 78, 81, 85, 89, 91, 94, 97), points=c(12, 15, 14, 13, NA, 29, 24, 18, 20, 25), assists=c(9, 5, NA, 5, 7, 8, 11, 12, 13, 11))
#view data frame
df
   rating points assists
1      70     12       9
2      75     15       5
3      75     14      NA
4      78     13       5
5      81     NA       7
6      85     29       8
7      89     24      11
8      91     18      12
9      94     20      13
10     97     25      11
</b>
Notice that two rows contain NA values in certain columns.
We can use the following syntax to perform <b>listwise deletion</b> and only keep the rows that have no missing values in any column:
<b>#create new data frame that only contains rows with no missing values
complete_df &lt;- df[complete.cases(df), ]
#view new data frame
complete_df
   rating points assists
1      70     12       9
2      75     15       5
4      78     13       5
6      85     29       8
7      89     24      11
8      91     18      12
9      94     20      13
10     97     25      11
</b>
Notice that none of the rows in this new data frame have empty values in any column.
Also note that we could use the  nrow()  function to find how many rows in the original data frame had missing values in any column:
<b>#count how many rows have missing values in any column
nrow(df[!complete.cases(df), ])
[1] 2
</b>
This tells us that <b>2</b> rows in the original data frame had missing values in at least one column.
And we can just as easily count how many rows did not have missing values in any column:
<b>#count how many rows do not have missing values in any column
nrow(df[complete.cases(df), ])
[1] 8</b>
This tells us that <b>8</b> rows in the original data frame did not have missing values in any column.
<h2><span class="orange">How to Perform a Ljung-Box Test in Python</span></h2>
The <b>Ljung-Box test </b>is a statistical test that checks if autocorrelation exists in a time series.
It uses the following hypotheses:
<b>H<sub>0</sub>:</b> The residuals are independently distributed.
<b>H<sub>A</sub>:</b> The residuals are not independently distributed; they exhibit serial correlation.
Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.
This tutorial explains how to perform a Ljung-Box test in Python.
<h3>Example: Ljung-Box Test in Python</h3>
To perform the Ljung-Box test on a data series in Python, we can use the  acorr_ljungbox()  function from the <b>statsmodels </b>library which uses the following syntax:
<b>acorr_ljungbox(x, lags=None)</b>
where:
<b>x: </b>The data series
<b>lags: </b>Number of lags to test
This function returns a test statistic and a corresponding p-value. If the p-value is less than some threshold (e.g. α = .05), you can reject the null hypothesis and conclude that the residuals are not independently distributed.
The following code shows how to use this function to perform the Ljung-Box test on the built-in statsmodels dataset called “SUNACTIVITY”:
<b>import statsmodels.api as sm
#load data series
data = sm.datasets.sunspots.load_pandas().data
#view first ten rows of data series 
data[:5]
YEARSUNACTIVITY
01700.05.0
11701.011.0
21702.016.0
31703.023.0
41704.036.0
#fit ARMA model to dataset
res = sm.tsa.ARMA(data["SUNACTIVITY"], (1,1)).fit(disp=-1)
#perform Ljung-Box test on residuals with lag=5
sm.stats.acorr_ljungbox(res.resid, lags=[5], return_df=True)
          lb_statlb_pvalue
5107.864881.157710e-21</b>
The test statistic of the test is <b>107.86488</b> and the p-value of the test is <b>1.157710e-21</b>, which is much less than 0.05. Thus, we reject the null hypothesis of the test and conclude that the residuals are not independent.
Note that we chose to use a lag value of 5 in this example, but you can choose any value that you would like to use for the lag. For example, we could instead use a value of 20:
<b>#perform Ljung-Box test on residuals with lag=20
sm.stats.acorr_ljungbox(res.resid, lags=[20], return_df=True)
           lb_statlb_pvalue
20343.6340169.117477e-61</b>
The test statistic of the test is <b>343.634016 </b>and the p-value of the test is <b>9.117477e-61</b>, which is much less than 0.05. Thus, we reject the null hypothesis of the test once again and conclude that the residuals are not independent.
Depending on your particular situation you may choose a lower or higher value to use for the lag. 
<h2><span class="orange">Ljung-Box Test: Definition + Example</span></h2>
The <b>Ljung-Box test</b>, named after statisticians  Greta M. Ljung  and  George E.P. Box , is a statistical test that checks if autocorrelation exists in a time series.
The Ljung-Box test is used widely in econometrics and in other fields in which time series data is common.
<h2>The Basics of the Ljung-Box Test</h2>
Here are the basics of the Ljung-Box test:
<h3>Hypotheses</h3>
The Ljung-Box test uses the following hypotheses:
<b>H<sub>0</sub>:</b> The residuals are independently distributed.
<b>H<sub>A</sub>:</b> The residuals are not independently distributed; they exhibit serial correlation.
Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model.
<h3>Test Statistic</h3>
The test statistic for the Ljung-Box test is as follows:
<b>Q</b> = n(n+2) Σp<sub>k</sub><sup>2</sup> / (n-k)
where:
<b>n</b> = sample size
Σ = a fancy symbol that means “sum” and is taken as the sum of 1 to <em>h</em>, where <em>h </em>is the number of lags being tested.
<b>p<sub>k</sub></b> = sample autocorrelation at lag <em>k</em>
<h3>Rejection Region</h3>
The test statistic <em>Q</em> follows a chi-square distribution with <em>h </em>degrees of freedom; that is, Q ~ X<sup>2</sup>(h).
We reject the null hypothesis and say that the residuals of the model are not independently distributed if Q > X<sup>2</sup><sub>1-α, h</sub>
<h2>Example: How to Conduct a Ljung-Box Test in R</h2>
To conduct a Ljung-Box test in R for a given time series, we can use the <b>Box.test()</b> function, which uses the following notation:
<b>Box.test</b>(x, lag =1, type=c(“Box-Pierce”, “Ljung-Box”), fitdf = 0)
where:
<b>x:</b> A numeric vector or univariate time series
<b>lag: </b>Specified number of lags
<b>type: </b>Test to be performed; options include Box-Pierce and Ljung-Box
<b>fitdf: bD</b>egrees of freedom to be subtracted if x is a series of residuals
The following example illustrates how to perform the Ljung-Box test for an arbitrary vector of 100 values that follow a normal distribution with mean = 0 and variance = 1:
<b>#make this example reproducible
set.seed(1)
#generate a list of 100 normally distributed random variables
data &lt;- rnorm(100, 0, 1)
#conduct Ljung-Box test
Box.test(data, lag = 10, type = "Ljung")
</b>
This generates the following output:
<b>Box-Ljung test
data:  data
X-squared = 6.0721, df = 10, p-value = 0.8092
</b>
The test statistic of the test is Q = <b>6.0721</b> and the p-value of the test is <b>0.8092</b>, which is much larger than 0.05. Thus, we fail to reject the null hypothesis of the test and conclude that the data values are independent.
Note that we used a lag value of 10 in this example, but you can choose any value that you would like to use for the lag, depending on your particular situation.
<b>Related: </b> How to Perform a Ljung-Box Test in Python 
<h2><span class="orange">How to Use lm() Function in R to Fit Linear Models</span></h2>
The <b>lm()</b> function in R is used to fit linear regression models.
This function uses the following basic syntax:
<b>lm(formula, data, …)</b>
where:
<b>formula:</b> The formula for the linear model (e.g. y ~ x1 + x2)
<b>data:</b> The name of the data frame that contains the data
The following example shows how to use this function in R to do the following:
Fit a regression model
View the summary of the regression model fit
View the diagnostic plots for the model
Plot the fitted regression model
Make predictions using the regression model
<h3>Fit Regression Model</h3>
The following code shows how to use the <b>lm()</b> function to fit a linear regression model in R:
<b>#define data
df = data.frame(x=c(1, 3, 3, 4, 5, 5, 6, 8, 9, 12),
                y=c(12, 14, 14, 13, 17, 19, 22, 26, 24, 22))
#fit linear regression model using 'x' as predictor and 'y' as response variable
model &lt;- lm(y ~ x, data=df)
</b>
<h3>View Summary of Regression Model</h3>
We can then use the <b>summary()</b> function to view the summary of the regression model fit:
<b>#view summary of regression model
summary(model)
Call:
lm(formula = y ~ x, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-4.4793 -0.9772 -0.4772  1.4388  4.6328 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  11.1432     1.9104   5.833  0.00039 ***
x             1.2780     0.2984   4.284  0.00267 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 2.929 on 8 degrees of freedom
Multiple R-squared:  0.6964,Adjusted R-squared:  0.6584 
F-statistic: 18.35 on 1 and 8 DF,  p-value: 0.002675
</b>
Here’s how to interpret the most important values in the model:
<b>F-statistic</b> = 18.35, corresponding <b>p-value</b> = .002675. Since this p-value is less than .05, the model as a whole is statistically significant.
<b>Multiple R-squared</b> = .6964. This tells us that 69.64% of the variation in the response variable, y, can be explained by the predictor variable, x.
<b>Coefficient estimate of x</b>: 1.2780. This tells us that each additional one unit increase in x is associated with an average increase of 1.2780 in y.
We can then use the coefficient estimates from the output to write the estimated regression equation:
y = 11.1432 + 1.2780*(x)
<b>Bonus</b>: You can find a complete guide to interpreting every value in the regression output in R  here .
<h3>View Diagnostic Plots of Model</h3>
We can then use the <b>plot()</b> function to plot the diagnostic plots for the regression model:
<b>#create diagnostic plots
plot(model)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/lm2.png">
These plots allow us to analyze the  residuals  of the regression model to determine if the model is appropriate to use for the data.
Refer to  this tutorial  for a complete explanation of how to interpret the diagnostic plots for a model in R.
<h3>Plot the Fitted Regression Model</h3>
We can use the <b>abline()</b> function to plot the fitted regression model:
<b>#create scatterplot of raw data
plot(df$x, df$y, col='red', main='Summary of Regression Model', xlab='x', ylab='y')
#add fitted regression line
abline(model)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/lm3.png">
<h3>Use the Regression Model to Make Predictions</h3>
We can use the <b>predict()</b> function to predict the response value for a new observation:
<b>#define new observation
new &lt;- data.frame(x=c(5))
#use the fitted model to predict the value for the new observation
predict(model, newdata = new)
      1 
17.5332 
</b>
The model predicts that this new observation will have a response value of <b>17.5332</b>.
<h2><span class="orange">How to Load Multiple Packages in R (With Example)</span></h2>
You can use the following basic syntax to load multiple packages in R at once:
<b>lapply(some_packages, library, character.only=TRUE)
</b>
In this example, <b>some_packages</b> represents a vector of package names you’d like to load.
The following example shows how to use this syntax in practice.
<h2>Example: Load Multiple Packages in R</h2>
The following code shows how to summarize a dataset in R and create a plot using three different packages:
<b>dplyr</b>
<b>ggplot2</b>
<b>ggthemes</b>
In this example, we load each package individually using three different <b>library()</b> functions:
<b>library(dplyr)
library(ggplot2)
library(ggthemes)
#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(category=rep(c('A', 'B', 'C', 'D', 'E'), each=10), value=runif(50, 10, 20))
#create summary data frame
df_summary &lt;- df %>%
  group_by(category) %>%
  summarize(mean=mean(value),
            sd=sd(value))
#plot mean value of each category with error bars
ggplot(df_summary) +
    geom_bar(aes(x=category, y=mean), stat='identity') +
    geom_errorbar(aes(x=category, ymin=mean-sd, ymax=mean+sd), width=0.3) +
    theme_tufte()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/multiple11.jpg"472">
Using this code, we’re able to load all three packages and produce a plot that summarizes the values in a dataset.
However, we could achieve the same outcome using the<b> lapply()</b> function to load all three packages using just one line of code:
<b>#define vector of packages to load
some_packages &lt;- c('ggplot2', 'dplyr', 'ggthemes')
#load all packages at once
lapply(some_packages, library, character.only=TRUE)
#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(category=rep(c('A', 'B', 'C', 'D', 'E'), each=10), value=runif(50, 10, 20))
#create summary data frame
df_summary &lt;- df %>%
  group_by(category) %>%
  summarize(mean=mean(value),
            sd=sd(value))
#plot mean value of each category with error bars
ggplot(df_summary) +
    geom_bar(aes(x=category, y=mean), stat='identity') +
    geom_errorbar(aes(x=category, ymin=mean-sd, ymax=mean+sd), width=0.3) +
    theme_tufte()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/multiple11.jpg"472">
Once again, we’re able to load all three packages and produce the same plot as before.
The difference is that we’re able to load all three packages using just one line of code this time.
This <b>lapply()</b> function is particularly useful when you want to load a long list of packages without typing out the<b> library()</b> function each time.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Clear the Environment in R 
 How to Clear All Plots in RStudio 
 How to Interpret in R: The following objects are masked 
<h2><span class="orange">How to Perform LOESS Regression in R (With Example)</span></h2>
<b>LOESS regression</b>, sometimes called local regression, is a method that uses local fitting to fit a regression model to a dataset.
The following step-by-step example shows how to perform LOESS regression in R.
<h3>Step 1: Create the Data</h3>
First, let’s create the following data frame in R:
<b>#view DataFrame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14), y=c(1, 4, 7, 13, 19, 24, 20, 15, 13, 11, 15, 18, 22, 27))
#view first six rows of data frame
head(df)
  x  y
1 1  1
2 2  4
3 3  7
4 4 13
5 5 19
6 6 24
</b>
<h3>Step 2: Fit Several LOESS Regression Models</h3>
We can use the <b>loess()</b> function to fit several LOESS regression models to this dataset, using various values for the <b>span</b> parameter:
<b>#fit several LOESS regression models to dataset
loess50 &lt;- loess(y ~ x, data=df, span=.5)
smooth50 &lt;- predict(loess50) 
loess75 &lt;- loess(y ~ x, data=df, span=.75)
smooth75 &lt;- predict(loess75) 
loess90 &lt;- loess(y ~ x, data=df, span=.9)
smooth90 &lt;- predict(loess90) 
#create scatterplot with each regression line overlaid
plot(df$x, df$y, pch=19, main='Loess Regression Models')
lines(smooth50, x=df$x, col='red')
lines(smooth75, x=df$x, col='purple')
lines(smooth90, x=df$x, col='blue')
legend('bottomright', legend=c('.5', '.75', '.9'),
        col=c('red', 'purple', 'blue'), pch=19, title='Smoothing Span')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/loess1.jpg">
Notice that the lower the value that we use for <b>span</b>, the less “smooth” the regression model will be and the more the model will attempt to hug the data points.
<h3>Step 3: Use K-Fold Cross Validation to Find the Best Model</h3>
To find the optimal <b>span</b> value to use, we can perform  k-fold cross validation  using functions from the <b>caret</b> package:
<b>library(caret)
#define k-fold cross validation method
ctrl &lt;- trainControl(method = "cv", number = 5)
grid &lt;- expand.grid(span = seq(0.5, 0.9, len = 5), degree = 1)
#perform cross-validation using smoothing spans ranginf from 0.5 to 0.9
model &lt;- train(y ~ x, data = df, method = "gamLoess", tuneGrid=grid, trControl = ctrl)
#print results of k-fold cross-validation
print(model)
14 samples
 1 predictor
No pre-processing
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 12, 11, 11, 11, 11 
Resampling results across tuning parameters:
  span  RMSE       Rsquared   MAE      
  0.5   10.148315  0.9570137   6.467066
  0.6    7.854113  0.9350278   5.343473
  0.7    6.113610  0.8150066   4.769545
  0.8   17.814105  0.8202561  11.875943
  0.9   26.705626  0.7384931  17.304833
Tuning parameter 'degree' was held constant at a value of 1
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were span = 0.7 and degree = 1.</b>
We can see that the value for <b>span</b> that produced the lowest value for the  root mean squared error  (RMSE) is <b>0.7</b>.
Thus, for our final LOESS regression model we would choose to use a value of <b>0.7</b> for the <b>span</b> argument within the <b>loess()</b> function.
<h2><span class="orange">How to Calculate Log in R (With Examples)</span></h2>
You can use the <b>log()</b> function in R to calculate the log of some value with a specified base:
<b>#calculate log of 9 with base 3
log(9, base=3)
</b>
If you don’t specify a base, R will use the default base value of <em>e</em>.
<b>#calculate log of 9 with base <em>e</em>
log(9)
[1] 2.197225
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Calculate Log of Single Value</h3>
The following code shows how to calculate the log of individual values in R using different bases:
<b>#calculate log of 100 with base <em>e</em>
log(100)
[1] 4.60517
#calculate log of 100 with base 10
log(100, base=10)
[1] 2
#calculate log of 100 with base 3
log(100, base=3)
[1] 4.191807
</b>
<h3>Example 2: Calculate Log of Values in Vector</h3>
The following code shows how to calculate the log of every value in a vector in R:
<b>#define vector
x &lt;- c(3, 6, 12, 16, 28, 45)
#calculate log of each value in vector with base <em>e</em>
log(x)
[1] 1.098612 1.791759 2.484907 2.772589 3.332205 3.806662
</b>
<h3>Example 3: Calculate Log of Values in Data Frame</h3>
The following code shows how to calculate the log of values in a specific column of a data frame in R:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#calculate log of each value in 'var1' column
log(df$var1, base=10)
[1] 0.0000000 0.4771213 0.4771213 0.6020600 0.6989700
</b>
And the following code shows how to use the <b>sapply()</b> function calculate the log of values in every column of a data frame:
<b>#define data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 7, 8, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 8, 9))
#calculate log of values in every column
sapply(df, function(x) log(x, base=10))
          var1      var2      var3      var4
[1,] 0.0000000 0.8450980 0.4771213 0.0000000
[2,] 0.4771213 0.8450980 0.4771213 0.0000000
[3,] 0.4771213 0.9030900 0.7781513 0.3010300
[4,] 0.6020600 0.4771213 0.7781513 0.9030900
[5,] 0.6989700 0.3010300 0.9030900 0.9542425
</b>
<h2><span class="orange">How to Create a Log-Log Plot in Excel</span></h2>
A <b>log-log plot </b>is a  scatterplot  that uses logarithmic scales on both the x-axis and the y-axis.
This type of plot is useful for visualizing two variables when the true relationship between them follows a power law. This phenomenon occurs in many fields in real life including astronomy, biology, chemistry, and physics.
This tutorial shows how to create a log-log plot for two variables in Excel.
<h3>Example: Log-Log Plot in Excel</h3>
Suppose we have the following dataset in Excel that shows the values for two variables, x and y:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel1.png">
Use the following steps to create a log-log plot for this dataset:
<b>Step 1: Create a scatterplot.</b>
Highlight the data in the range <b>A2:B11</b>. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel2.png">
Along the top ribbon, click the <b>Insert </b>tab. Within the <b>Charts </b>group, click on <b>Scatter</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel3.png">
The following scatterplot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel4.png">
<b>Step 2: Change the x-axis scale to logarithmic.</b>
Right click on the values along the x-axis and click <b>Format Axis</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel5.png">
In the new window that pops up, check the box next to <b>Logarithmic scale </b>to change the x-axis scale.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel6.png">
<b>Step 3: Change the y-axis scale to logarithmic.</b>
Next, click on the y-axis and repeat the same step to change the y-axis scale to logarithmic. The resulting plot will look like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/loglogexcel7.png">
Notice that the x-axis now spans from 1 to 10 while the y-axis spans from 1 to 1,000. Also notice how the relationship between the variables x and y now appears more linear. This is an indication that the two variables do indeed have a power law relationship.
<em><b>You can find more Excel tutorials  here .</b></em>
<h2><span class="orange">How to Create a Log-Log Plot in R</span></h2>
A <b>log-log plot </b>is a plot that uses logarithmic scales on both the x-axis and the y-axis.
This type of plot is useful for visualizing two variables when the true relationship between them follows some type of power law.
This tutorial explains how to create a log-log plot in R using both base R and the data visualization package  ggplot2 .
<h3>Method 1: Create a Log-Log Plot in Base R</h3>
Suppose we have the following dataset in R:
<b>#create data
df &lt;- data.frame(x=3:22, y=c(3, 4, 5, 7, 9, 13, 15, 19, 23, 24, 29,     38, 40, 50, 56, 59, 70, 89, 104, 130))
#create scatterplot of x vs. y
plot(df$x, df$y, main='Raw Data')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogR1.png">
Clearly the relationship between variables <em>x</em> and <em>y</em> follows a power law.
The following code shows how to create a log-log plot for these two variables in base R:
<b>#create log-log plot of x vs. y
plot(log(df$x), log(df$y), main='Log-Log Plot')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogR2.png">
Notice how the relationship between log(x) and log(y) is much more linear compared to the previous plot.
<h3>Method 2: Create a Log-Log Plot in ggplot2</h3>
The following code shows how to create a log-log plot for the exact same dataset using ggplot2:
<b>library(ggplot2) 
#create data
df &lt;- data.frame(x=3:22, y=c(3, 4, 5, 7, 9, 13, 15, 19, 23, 24, 29,     38, 40, 50, 56, 59, 70, 89, 104, 130))
#define new data frame
df_log &lt;- data.frame(x=log(df$x),     y=log(df$y))
#create scatterplot using ggplot2
ggplot(df_log, aes(x=x, y=y)) +
  geom_point()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogR3.png">
Feel free to customize the title, axis labels, and theme to make the plot more aesthetically pleasing:
<b>ggplot(df_log, aes(x=x, y=y)) +
  geom_point() +
  labs(title='Log-Log Plot', x='Log(x)', y='Log(y)') +
  theme_minimal()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogR4.png">
<h2><span class="orange">How to Create a Log-Log Plot in Python</span></h2>
A <b>log-log plot </b>is a plot that uses logarithmic scales on both the x-axis and the y-axis.
This type of plot is useful for visualizing two variables when the true relationship between them follows some type of power law.
This tutorial explains how to create a log-log plot in Python.
<h3>How to Create a Log-Log Plot in Python</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
import matplotlib.pyplot as plt
#create DataFrame
df = pd.DataFrame({'x': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,         14, 15, 16, 17, 18, 19, 20, 21, 22],   'y': [3, 4, 5, 7, 9, 13, 15, 19, 23, 24, 29,         38, 40, 50, 56, 59, 70, 89, 104, 130]})
#create scatterplot
plt.scatter(df.x, df.y)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogPython1.png">
Clearly the relationship between <em>x</em> and <em>y</em> follows a power law.
The following code shows how to use <b>numpy.log()</b> to perform a log transformation on both variables and create a log-log plot to visualize the relationship bewteen them:
<b>import numpy as np
#perform log transformation on both x and y
xlog = np.log(df.x)
ylog = np.log(df.y)
#create log-log plot
plt.scatter(xlog, ylog)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogPython2.png">
The x-axis displays the log of x and the y-axis displays the log of y.
Notice how the relationship between log(x) and log(y) is much more linear compared to the previous plot.
Feel free to add a title and axis labels to make the plot easier to interpret:
<b>#create log-log plot with labels
plt.scatter(xlog, ylog, color='purple')
plt.xlabel('Log(x)')
plt.ylabel('Log(y)')
plt.title('Log-Log Plot')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogPython3.png">
Also note that you can create a line plot instead of a scatterplot by simply using <b>plt.plot()</b> as follows:
<b>#create log-log line plot
plt.plot(xlog, ylog, color='purple')
plt.xlabel('Log(x)')
plt.ylabel('Log(y)')
plt.title('Log-Log Plot')</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/loglogPython4.png">
<h2><span class="orange">How to Use the Log-Normal Distribution in Python</span></h2>
You can use the  lognorm()  function from the <b>SciPy</b> library in Python to generate a random variable that follows a log-normal distribution.
The following examples show how to use this function in practice.
<h3>How to Generate a Log-Normal Distribution</h3>
You can use the following code to generate a random variable that follows a log-normal distribution with μ = 1 and σ = 1:
<b>import math
import numpy as np
from scipy.stats import lognorm
#make this example reproducible
np.random.seed(1)
#generate log-normal distributed random variable with 1000 values
lognorm_values = lognorm.rvs(s=1, scale=math.exp(1), size=1000)
#view first five values
lognorm_values[:5]
array([13.79554017,  1.47438888,  1.60292205,  0.92963   ,  6.45856805])
</b>
Note that within the <b>lognorm.rvs()</b> function, <b>s</b> is the standard deviation and the value inside <b>math.exp()</b> is the mean for the log-normal distribution that you’d like to generate.
In this example, we defined the mean to be <b>1</b> and the standard deviation to also be <b>1</b>.
<h3>How to Plot a Log-Normal Distribution</h3>
We can use the following code to create a histogram of the values for the log-normally distributed random variable we created in the previous example:
<b>import matplotlib.pyplot as plt
#create histogram
plt.hist(lognorm_values, density=True, edgecolor='black')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormPython1.jpg"575">
Matplotlib uses 10 bins in histograms by default, but we can easily increase this number using the <b>bins</b> argument.
For example, we can increase the number of bins to 20:
<b>import matplotlib.pyplot as plt
#create histogram
plt.hist(lognorm_values, density=True, edgecolor='black', bins=20)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lognormPython2.jpg">
The greater the number of bins, the more narrow the bars will be in the histogram.
<b>Related:</b>  Three Ways to Adjust Bin Size in Matplotlib Histograms 
<h2><span class="orange">How to Perform a Log Rank Test in R</span></h2>
 A <b>log rank test</b> is the most common way to compare survival curves between two groups.
This test uses the following  hypotheses :
<b>H<sub>0</sub>:</b> There is no difference in survival between the two groups.
<b>H<sub>A</sub>:</b> There <em>is</em> a difference in survival between the two groups.
If the  p-value  of the test is less than some significance level (e.g. α = .05), then we can reject the null hypothesis and conclude that there is sufficient evidence to say there is a difference in survival between the two groups.
To perform a log rank test in R, we can use the <b>survdiff()</b> function from the <b>survival</b> package, which uses the following syntax:
<b>survdiff(Surv(time, status) ~ predictors, data)</b>
This function returns a Chi-Squared test statistic and a corresponding p-value.
The following example shows how to use this function to perform a log rank test in R.
<h3>Example: Log Rank Test in R</h3>
For this example, we’ll use the <b>ovarian</b> dataset from the <b>survival</b> package. This dataset contains the following information about 26 patients:
Survival time (in months) after being diagnosed with ovarian cancer
Whether or not survival time was censored
Type of treatment received (rx =1 or rx = 2)
The following code shows how to view the first six rows of this dataset:
<b>library(survival)
#view first six rows of dataset
head(ovarian)
  futime fustat     age resid.ds rx ecog.ps
1     59      1 72.3315        2  1       1
2    115      1 74.4932        2  1       1
3    156      1 66.4658        2  1       2
4    421      0 53.3644        2  2       1
5    431      1 50.3397        2  1       1
6    448      0 56.4301        1  1       2</b>
The following code shows how to perform a log rank test to determine if there is a difference in survival between patients who received different treatments:
<b>#perform log rank test
survdiff(Surv(futime, fustat) ~ rx, data=ovarian)
Call:
survdiff(formula = Surv(futime, fustat) ~ rx, data = ovarian)
      N Observed Expected (O-E)^2/E (O-E)^2/V
rx=1 13        7     5.23     0.596      1.06
rx=2 13        5     6.77     0.461      1.06
 Chisq= 1.1  on 1 degrees of freedom, p= 0.3 
</b>
The Chi-Squared test statistic is <b>1.1</b> with 1 degree of freedom and the corresponding p-value is <b>0.3</b>. Since this p-value is not less than .05, we fail to reject the null hypothesis.
In other words, we don’t have sufficient evidence to say that there is a statistically significant difference in survival between the two treatments.
We can also plot the survival curves for each group using the following syntax:
<b>#plot survival curves for each treatment group
plot(survfit(Surv(futime, fustat) ~ rx, data = ovarian), 
     xlab = "Time", 
     ylab = "Overall survival probability")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/logranktest1.png">
We can see that the survival curves are slightly different, but the log rank test told us that the difference is not statistically significant.
<h2><span class="orange">Logarithmic Regression Calculator</span></h2>
This calculator produces a logarithmic regression equation based on values for a predictor variable and a response variable.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">6, 7, 7, 8, 12, 14, 15, 16, 16, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">14, 15, 15, 17, 18, 18, 19, 24, 25, 29</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<b>Logarithmic Regression Equation:</b>
<U+0177> = -5.1656 + 10.1997 * ln(x)
<script>
function calc() {
//get input data
var x_hold = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
var x = [];
for(var i=0; i<x_hold.length; i++) {
    x[i] = Math.log(x_hold[i]);
}
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        lr['r2'] = Math.pow((n*sum_xy - sum_x*sum_y)/Math.sqrt((n*sum_xx-sum_x*sum_x)*(n*sum_yy-sum_y*sum_y)),2);
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope;
var b = lr.intercept;
var first = Math.pow(10, b);
var second = Math.pow(10, a);
document.getElementById('a').innerHTML = a.toFixed(4);
document.getElementById('b').innerHTML = b.toFixed(4);
}
//output error message if boths lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">Logarithmic Regression in Excel (Step-by-Step)</span></h2>
<b>Logarithmic regression</b> is a type of regression used to model situations where growth or decay accelerates rapidly at first and then slows over time.
For example, the following plot demonstrates an example of logarithmic decay:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/expRegExcel2.png">
For this type of situation, the relationship between a predictor variable and a response variable could be modeled well using logarithmic regression.
The equation of a logarithmic regression model takes the following form:
<b>y = a + b*ln(x)</b>
where:
<b>y:</b> The response variable
<b>x:</b> The predictor variable
<b>a, b:</b> The regression coefficients that describe the relationship between <em>x</em> and <em>y</em>
The following step-by-step example shows how to perform logarithmic regression in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data for two variables: <em>x</em> and <em>y</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegExcel1.png">
<h3>Step 2: Take the Natural Log of the Predictor Variable</h3>
Next, we need to create a new column that represents the natural log of the predictor variable <em>x</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegExcel2.png">
<h3>Step 3: Fit the Logarithmic Regression Model</h3>
Next, we’ll fit the logarithmic regression model. To do so, click the <b>Data</b> tab along the top ribbon, then click <b>Data Analysis</b> within the <b>Analysis</b> group.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
<em>If you don’t see Data Analysis as an option, you need to first  load the Analysis ToolPak .</em>
In the window that pops up, click <b>Regression</b>. In the new window that pops up, fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegExcel3.png">
Once you click <b>OK</b>, the output of the logarithmic regression model will be shown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegExcel4.png">
The  overall F-value  of the model is 828.18 and the corresponding p-value is extremely small (3.70174E-13), which indicates that the model as a whole is useful.
Using the coefficients from the output table, we can see that the fitted logarithmic regression equation is:
<b>y = 63.0686 – 20.1987 * ln(x)</b>
We can use this equation to predict the response variable, <em>y</em>, based on the value of the predictor variable, <em>x</em>. For example, if <em>x</em> = 12, then we would predict that <em>y</em> would be <b>12.87</b>:
y = 63.0686 – 20.1987 * ln(12) = <b>12.87</b>
<b>Bonus:</b> Feel free to use this online  Logarithmic Regression Calculator  to automatically compute the logarithmic regression equation for a given predictor and response variable.
<h2><span class="orange">How to Perform Logarithmic Regression in Google Sheets</span></h2>
<b>Logarithmic regression</b> is a type of regression used to model situations where growth or decay accelerates rapidly at first and then slows over time.
For example, the following plot demonstrates an example of logarithmic decay:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/expRegExcel2.png">
For this type of situation, the relationship between a predictor variable and a  response variable  could be modeled well using logarithmic regression.
The equation of a logarithmic regression model takes the following form:
<b>y = a + b*ln(x)</b>
where:
<b>y:</b> The response variable
<b>x:</b> The predictor variable
<b>a, b:</b> The regression coefficients that describe the relationship between <em>x</em> and <em>y</em>
The following step-by-step example shows how to perform logarithmic regression in Google Sheets.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data for two variables: <em>x</em> and <em>y</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/logreg11.jpg"480">
<h3>Step 2: Take the Natural Log of the Predictor Variable</h3>
Next, we need to create a new column that represents the natural log of the predictor variable <em>x</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/logreg12.jpg"490">
<h3>Step 3: Fit the Logarithmic Regression Model</h3>
Next, we’ll fit the logarithmic regression model.
To do so, type the following formula into cell <b>E2</b>:
<b>=LINEST(B2:B16, C2:C16)
</b>
Once you press <b>Enter</b>, the coefficients of the logarithmic regression model will be shown:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/06/logreg13.jpg">
Using the coefficients from the output, we can see that the fitted logarithmic regression equation is:
<b>y = 63.0686 – 20.1987 * ln(x)</b>
We can use this equation to predict the response variable, <em>y</em>, based on the value of the predictor variable, <em>x</em>.
For example, if <em>x</em> = 12, then we would predict that <em>y</em> would be <b>12.87</b>:
y = 63.0686 – 20.1987 * ln(12) = <b>12.87</b>
<b>Bonus:</b> Feel free to use this online  Logarithmic Regression Calculator  to automatically compute the logarithmic regression equation for a given predictor and response variable.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Perform Linear Regression in Google Sheets 
 How to Perform Polynomial Regression in Google Sheets 
 How to Create a Residual Plot in Google Sheets 
<h2><span class="orange">Logarithmic Regression in R (Step-by-Step)</span></h2>
<b>Logarithmic regression</b> is a type of regression used to model situations where growth or decay accelerates rapidly at first and then slows over time.
For example, the following plot demonstrates an example of logarithmic decay:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/expRegExcel2.png">
For this type of situation, the relationship between a predictor variable and a response variable could be modeled well using logarithmic regression.
The equation of a logarithmic regression model takes the following form:
<b>y = a + b*ln(x)</b>
where:
<b>y:</b> The response variable
<b>x:</b> The predictor variable
<b>a, b:</b> The regression coefficients that describe the relationship between <em>x</em> and <em>y</em>
The following step-by-step example shows how to perform logarithmic regression in R.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data for two variables: <em>x</em> and <em>y</em>:
<b>x=1:15
y=c(59, 50, 44, 38, 33, 28, 23, 20, 17, 15, 13, 12, 11, 10, 9.5)
</b>
<h3>Step 2: Visualize the Data</h3>
Next, let’s create a quick  scatterplot  to visualize the relationship between <em>x</em> and <em>y</em>:
<b>plot(x, y)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegR1.png">
From the plot we can see that there exists a clear logarithmic decay pattern between the two variables. The value of the response variable, <em>y</em>, decreases rapidly at first and then slows over time.
Thus, it seems like a good idea to fit a logarithmic regression equation to describe the relationship between the variables.
<h3>Step 3: Fit the Logarithmic Regression Model</h3>
Next, we’ll use the <b>lm()</b> function to fit a logarithmic regression model, using the natural log of <em>x</em> as the predictor variable and <em>y</em> as the response variable
<b>#fit the model
model &lt;- lm(y ~ log(x))</b>
<b>
#view the output of the model
summary(model)
Call:
lm(formula = y ~ log(x))
Residuals:
   Min     1Q Median     3Q    Max 
-4.069 -1.313 -0.260  1.127  3.122 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)  63.0686     1.4090   44.76 1.25e-15 ***
log(x)      -20.1987     0.7019  -28.78 3.70e-13 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 2.054 on 13 degrees of freedom
Multiple R-squared:  0.9845,Adjusted R-squared:  0.9834 
F-statistic: 828.2 on 1 and 13 DF,  p-value: 3.702e-13</b>
The  overall F-value  of the model is 828.2 and the corresponding p-value is extremely small (3.702e-13), which indicates that the model as a whole is useful.
Using the coefficients from the output table, we can see that the fitted logarithmic regression equation is:
<b>y = 63.0686 – 20.1987 * ln(x)</b>
We can use this equation to predict the response variable, <em>y</em>, based on the value of the predictor variable, <em>x</em>. For example, if <em>x</em> = 12, then we would predict that <em>y</em> would be <b>12.87</b>:
y = 63.0686 – 20.1987 * ln(12) = <b>12.87</b>
<b>Bonus:</b> Feel free to use this online  Logarithmic Regression Calculator  to automatically compute the logarithmic regression equation for a given predictor and response variable.
<h3>Step 4: Visualize the Logarithmic Regression Model</h3>
Lastly, we can create a quick plot to visualize how well the logarithmic regression model fits the data:
<b>#plot x vs. y
plot(x, y)
#define x-values to use for regression line
x=seq(from=1,to=15,length.out=1000)
#use the model to predict the y-values based on the x-values
y=predict(model,newdata=list(x=seq(from=1,to=15,length.out=1000)),
          interval="confidence")
#add the fitted regression line to the plot (lwd specifies the width of the line)
matlines(x,y, lwd=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/logRegR2.png">
We can see that the logarithmic regression model does a good job of fitting this particular dataset.
<h2><span class="orange">Logarithmic Regression in Python (Step-by-Step)</span></h2>
<b>Logarithmic regression</b> is a type of regression used to model situations where growth or decay accelerates rapidly at first and then slows over time.
For example, the following plot demonstrates an example of logarithmic decay:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/expRegExcel2.png">
For this type of situation, the relationship between a predictor variable and a  response variable  could be modeled well using logarithmic regression.
The equation of a logarithmic regression model takes the following form:
<b>y = a + b*ln(x)</b>
where:
<b>y:</b> The response variable
<b>x:</b> The predictor variable
<b>a, b:</b> The regression coefficients that describe the relationship between <em>x</em> and <em>y</em>
The following step-by-step example shows how to perform logarithmic regression in Python.
<h3>Step 1: Create the Data</h3>
First, let’s create some fake data for two variables: <em>x</em> and <em>y</em>:
<b>import numpy as np
x = np.arange(1, 16, 1)
y = np.array([59, 50, 44, 38, 33, 28, 23, 20, 17, 15, 13, 12, 11, 10, 9.5])
</b>
<h3>Step 2: Visualize the Data</h3>
Next, let’s create a quick  scatterplot  to visualize the relationship between <em>x</em> and <em>y</em>:
<b>import matplotlib.pyplot as plt
plt.scatter(x, y)
plt.show()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/logPython1.png">
From the plot we can see that there exists a logarithmic decay pattern between the two variables. The value of the response variable, <em>y</em>, decreases rapidly at first and then slows over time.
Thus, it seems like a good idea to fit a logarithmic regression equation to describe the relationship between the variables.
<h3>Step 3: Fit the Logarithmic Regression Model</h3>
Next, we’ll use the <b>polyfit()</b> function to fit a logarithmic regression model, using the natural log of <em>x</em> as the predictor variable and <em>y</em> as the response variable:
<b>#fit the model
fit = np.polyfit(np.log(x), y, 1)
#view the output of the model
print(fit)
[-20.19869943  63.06859979]
</b>
We can use the coefficients in the output to write the following fitted logarithmic regression equation:
<b>y = 63.0686 – 20.1987 * ln(x)</b>
We can use this equation to predict the response variable, <em>y</em>, based on the value of the predictor variable, <em>x</em>. For example, if <em>x</em> = 12, then we would predict that <em>y</em> would be <b>12.87</b>:
y = 63.0686 – 20.1987 * ln(12) = <b>12.87</b>
<b>Bonus:</b> Feel free to use this online  Logarithmic Regression Calculator  to automatically compute the logarithmic regression equation for a given predictor and response variable.
<h2><span class="orange">How to Perform Logarithmic Regression on a TI-84 Calculator</span></h2>
<b>Logarithmic regression</b> is a type of regression used to model situations where growth or decay accelerates rapidly at first and then slows over time.
For example, the following plot demonstrates an example of logarithmic decay:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/expRegExcel2.png">
For this type of situation, the relationship between a predictor variable and a  response variable  could be modeled well using logarithmic regression.
The equation of a logarithmic regression model takes the following form:
<b>y = a + b*ln(x)</b>
where:
<b>y:</b> The response variable
<b>x:</b> The predictor variable
<b>a, b:</b> The regression coefficients that describe the relationship between <em>x</em> and <em>y</em>
The following step-by-step example shows how to perform logarithmic regression on a TI-84 calculator for the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/logreg1.png">
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values. Press STAT, then press EDIT. Then enter the x-values of the dataset in column L1 and the y-values in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/logreg2.png">
<h3>Step 2: Fit the Logarithmic Regression Model</h3>
Next, we fill fit the logarithmic regression model.
Press Stat, then scroll over to CALC. Then scroll down to LnReg and press ENTER twice.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/logreg3.png">
The following results will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/logreg4.png">
<h3>Step 3: Interpret the Results</h3>
We can use the coefficients in the output to write the following fitted logarithmic regression equation:
<b>y = 76.21296 – 29.8634 * ln(x)</b>
We can use this equation to predict the response variable, <em>y</em>, based on the value of the predictor variable, <em>x</em>. For example, if <em>x</em> = 8, then we would predict that <em>y</em> would be <b>14.11</b>:
y = 76.21296 – 29.8634 * ln(8) = <b>14.11</b>
<b>Bonus:</b> Feel free to use this online  Logarithmic Regression Calculator  to automatically compute the logarithmic regression equation for a given predictor and response variable.
<h2><span class="orange">How to Use LOGEST Function in Excel (With Example)</span></h2>
You can use the <b>LOGEST </b>function in Excel to calculate the formula of an exponential curve that fits your data.
The equation of the curve will take on the following form:
y = b * m<sup>x</sup>
This function uses the following basic syntax:
<b>=LOGEST(known_y's, [known_x's], [const], [stats])</b>
where:
<b>known_y’s</b>: An array of known y-values
<b>known_x’s</b>: An array of known x-values
<b>const</b>: Optional argument. If TRUE, the constant b is treated normally. If FALSE, the constant b is set to 1.
<b>stats</b>: Optional argument. If TRUE, additional regression statistics are returned. If FALSE, additional regression statistics are not returned.
The following step-by-step example shows how to use this function in practice.
<h2>Step 1: Enter the Data</h2>
First, let’s enter the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logest1.jpg"420">
<h2>Step 2: Visualize the Data</h2>
Next, let’s create a quick scatter plot of x vs. y to verify that the data actually follow an exponential curve:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logest2.jpg"505">
We can see that the data do indeed follow an exponential curve.
<h2>Step 3: Use LOGEST to Find the Exponential Curve Formula</h2>
Next, we can type the following formula into any cell to calculate the exponential curve formula:
<b>=LOGEST(B2:B11, A2:A11)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logest3.jpg">
The first value in the output represents the value for <b>m</b> and the second value in the output represents the value for <b>b</b> in the equation:
<b>y = b * m<sup>x</sup></b>
Thus, we would write this exponential curve formula as:
<b>y = 1.909483 * 1.489702<sup>x</sup></b>
We could then use this formula to predict the values of y based on the value of x.
For example, if x has a value of 8 then we would predict that y has a value of <b>46.31</b>:
y = 1.909483 * 1.489702<sup>8</sup> = 46.31
<h2>Step 4 (Optional): Display Additional Regression Statistics</h2>
We can set the value for the <b>stats</b> argument in the <b>LOGEST</b> function equal to <b>TRUE</b> to display additional regression statistics for the fitted regression equation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logest4.jpg"533">
Here’s how to interpret each value in the output:
The standard error for m is <b>.02206</b>.
The standard error for b is <b>.136879</b>.
The R<sup>2</sup> for the model is <b>.97608</b>.
The standard error for y is <b>.200371</b>.
The F-statistic is <b>326.4436</b>.
The degrees of freedom is <b>8</b>.
The regression sum of squares is <b>13.10617</b>.
The residual sum of squares is <b>.321187</b>.
In general, the most interesting metric in these additional statistics is the R<sup>2</sup> value, which represents the proportion of the variance in the response variable that can be explained the predictor variable.
The value for R<sup>2</sup> can range from 0 to 1.
Since the R<sup>2</sup> for this particular model is close to 1, it tells us that the predictor variable x does a good job of predicting the value of the response variable y.
<b>Related:</b>  What is a Good R-squared Value? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Excel:
 How to Use DEVSQ in Excel 
 How to Use SUMSQ in Excel 
 How to Perform Nonlinear Regression in Excel 
<h2><span class="orange">How to Use LOGEST Function in Google Sheets (With Example)</span></h2>
You can use the <b>LOGEST </b>function in Google Sheets to calculate the formula of an exponential curve that fits your data.
The equation of the curve will take on the following form:
y = b * m<sup>x</sup>
This function uses the following basic syntax:
<b>=LOGEST(known_data_y, [known_data_x], [b], [verbose])</b>
where:
<b>known_data_y</b>: An array of known y-values
<b>known_data_x</b>: An array of known x-values
<b>b</b>: Optional argument. If TRUE, the constant b is treated normally. If FALSE, the constant b is set to 1.
<b>verbose</b>: Optional argument. If TRUE, additional regression statistics are returned. If FALSE, additional regression statistics are not returned.
The following step-by-step example shows how to use this function in practice.
<h2>Step 1: Enter the Data</h2>
First, let’s enter the following dataset in Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logestsheets1.jpg"490">
<h2>Step 2: Visualize the Data</h2>
Next, let’s create a quick scatter plot of x vs. y to verify that the data actually follow an exponential curve:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logestsheets2.jpg"569">
We can see that the data do indeed follow an exponential curve.
<h2>Step 3: Use LOGEST to Find the Exponential Curve Formula</h2>
Next, we can type the following formula into any cell to calculate the exponential curve formula:
<b>=LOGEST(B2:B11, A2:A11)</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logestsheets3-1.jpg">
The first value in the output represents the value for <b>m</b> and the second value in the output represents the value for <b>b</b> in the equation:
<b>y = b * m<sup>x</sup></b>
Thus, we would write this exponential curve formula as:
<b>y = 1.909483 * 1.489702<sup>x</sup></b>
We could then use this formula to predict the values of y based on the value of x.
For example, if x has a value of 8 then we would predict that y has a value of <b>46.31</b>:
y = 1.909483 * 1.489702<sup>8</sup> = 46.31
<h2>Step 4 (Optional): Display Additional Regression Statistics</h2>
We can set the value for the <b>verbose </b>argument in the <b>LOGEST</b> function equal to <b>TRUE</b> to display additional regression statistics for the fitted regression equation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/logestsheets4.jpg"610">
Here’s how to interpret each value in the output:
The standard error for m is <b>.02206</b>.
The standard error for b is <b>.136879</b>.
The R<sup>2</sup> for the model is <b>.97608</b>.
The standard error for y is <b>.200371</b>.
The F-statistic is <b>326.4436</b>.
The degrees of freedom is <b>8</b>.
The regression sum of squares is <b>13.106169</b>.
The residual sum of squares is <b>.321187</b>.
In general, the most interesting metric in these additional statistics is the R<sup>2</sup> value, which represents the proportion of the variance in the response variable that can be explained the predictor variable.
The value for R<sup>2</sup> can range from 0 to 1.
Since the R<sup>2</sup> for this particular model is close to 1, it tells us that the predictor variable x does a good job of predicting the value of the response variable y.
<b>Related:</b>  What is a Good R-squared Value? 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Google Sheets:
 How to Perform Linear Regression in Google Sheets 
 How to Perform Polynomial Regression in Google Sheets 
 How to Calculate R-Squared in Google Sheets 
<h2><span class="orange">How to Perform Logistic Regression in Excel</span></h2>
<b>Logistic regression</b> is a method that we use to fit a regression model when the response variable is binary.
This tutorial explains how to perform logistic regression in Excel.
<h3>Example: Logistic Regression in Excel</h3>
Use the following steps to perform logistic regression in Excel for a dataset that shows whether or not college basketball players got drafted into the NBA (draft: 0 = no, 1 = yes) based on their average points, rebounds, and assists in the previous season.
<b>Step 1: Input the data.</b>
First, input the following data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel1.png">
<b>Step 2: Enter cells for regression coefficients.</b>
Since we have three explanatory variables in the model (pts, rebs, ast), we will create cells for three regression coefficients plus one for the intercept in the model. We will set the values for each of these to 0.001, but we will optimize for them later.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel2.png">
Next, we will have to create a few new columns that we will use to optimize for these regression coefficients including the logit, e<sup>logit</sup>,  probability, and log likelihood.
<b>Step 3: Create values for the logit.</b>
Next, we will create the logit column by using the the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel3.png">
<b>Step 4: Create values for e<sup>logit</sup>.</b>
Next, we will create values for e<sup>logit </sup>by using the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel4.png">
<b>Step 5: Create values for probability.</b>
Next, we will create values for probability by using the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel5.png">
<b>Step 6: Create values for log likelihood.</b>
Next, we will create values for log likelihood by using the following formula:
<b>Log likelihood = LN(Probability)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel7.png">
<b>Step 7: Find the sum of the log likelihoods.</b>
Lastly, we will find the sum of the log likelihoods, which is the number we will attempt to maximize to solve for the regression coefficients.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel6.png">
<b>Step 8: Use the Solver to solve for the regression coefficients.</b>
If you haven’t already install the Solver in Excel, use the following steps to do so:
Click <b>File</b>.
Click <b>Options</b>.
Click <b>Add-Ins</b>.
Click <b>Solver Add-In</b>, then click <b>Go</b>.
In the new window that pops up, check the box next to <b>Solver Add-In</b>, then click <b>Go</b>.
Once the Solver is installed, go to the <b>Analysis </b>group on the <b>Data </b>tab and click <b>Solver</b>. Enter the following information:
<b>Set Objective: </b>Choose cell H14 that contains the sum of the log likelihoods.
<b>By Changing Variable Cells: </b>Choose the cell range B15:B18 that contains the regression coefficients.
<b>Make Unconstrained Variables Non-Negative: </b>Uncheck this box.
<b>Select a Solving Method: </b>Choose GRG Nonlinear.
Then click <b>Solve</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel8.png">
The Solver automatically calculates the regression coefficient estimates:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel9.png">
By default, the regression coefficients can be used to find the probability that draft = 0. However, typically in logistic regression we’re interested in the probability that the response variable = 1. So, we can simply reverse the signs on each of the regression coefficients:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/logisticExcel10.png">
Now these regression coefficients can be used to find the probability that draft = 1.
For example, suppose a player averages 14 points per game, 4 rebounds per game, and 5 assists per game. The probability that this player will get drafted into the NBA can be calculated as:
P(draft = 1) = e<sup>3.681193 + 0.112827*(14) -0.39568*(4) – 0.67954*(5)</sup> / (1+e<sup>3.681193 + 0.112827*(14) -0.39568*(4) – 0.67954*(5)</sup>) = <b>0.57</b>.
Since this probability is greater than 0.5, we predict that this player would<em> </em>get drafted into the NBA.
<b>Related:</b>  How to Create a ROC Curve in Excel (Step-by-Step) 
<h2><span class="orange">How to Perform Logistic Regression in Google Sheets</span></h2>
 Logistic regression  is a method we can use to fit a regression model when the  response variable  is binary.
The following step-by-step example shows how to perform logistic regression in Google Sheets.
<h2>Step 1: Install the XLMiner Analysis ToolPak</h2>
To perform logistic regression in Google Sheets, we need to first install the free <b>XLMiner Analysis Toolpak</b>.
To do so, click <b>Add-ons > Get add-ons</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets0.png">
Next, type <b>XLMiner Analysis ToolPak</b> in the search bar and click the icon that appears:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets1.png">
Lastly, click the blue <b>Install</b> button.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets2.png">
<h2>Step 2: Enter the Data</h2>
Next, we’ll enter the following data into Google Sheets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/log1.jpg"548">
We will fit a logistic regression model that uses points and assists to predict whether a basketball player gets drafted into the NBA (0 =No, 1 = Yes).
<h2>Step 3: Perform Logistic Regression</h2>
To fit the logistic regression model, click the <b>Extensions</b> tab, then click <b>XL Miner Analysis ToolPak</b>, then click <b>Start</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/log2.jpg"627">
In the panel that appears on the right side of the screen, click the dropdown arrow next to <b>Logistic Regression</b> and type in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/log3.jpg">
Once you click <b>OK</b>, the summary of the logistic regression model will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/log4.jpg">
The coefficients in the output indicate the average change in log odds of getting drafted.
For example, a one unit increase in <b>points </b>is associated with an average increase of <b>0.212 </b>in the log odds of getting drafted.
The sign on the coefficients tells us whether there is a positive or negative associated between each predictor variable and the response variable.
For example, since points has a positive sign for the coefficient it means that increasing the value for points increases the chances that a player gets drafted (assuming assists is held constant).
Conversely, since assists has a negative sign for the coefficient it means that increasing the value for assists decreases the chances that a player gets drafted (assuming points is held constant).
The p-values in the output also give us an idea of how effective each predictor variable is at predicting the probability of getting drafted:
P-value for points: <b>0.02</b>
P-value for assists: <b>0.35</b>
We can see that points seems to be a statistically significant predictor variable since it has a p-value less than .05, but assists does not seem to be statistically significant since it does not have a p-value less than .05.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Google Sheets:
 How to Perform Polynomial Regression in Google Sheets 
 How to Perform Linear Regression in Google Sheets 
 How to Calculate R-Squared in Google Sheets 
<h2><span class="orange">How to Perform Logistic Regression in R (Step-by-Step)</span></h2>
 Logistic regression  is a method we can use to fit a regression model when the  response variable  is binary.
Logistic regression uses a method known as <em>maximum likelihood estimation</em> to find an equation of the following form:
<b>log[p(X) / (1-p(X))]  =  β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></b>
where:
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The coefficient estimate for the j<sup>th</sup> predictor variable
The formula on the right side of the equation predicts the <b>log odds</b> of the response variable taking on a value of 1.
Thus, when we fit a logistic regression model we can use the following equation to calculate the probability that a given observation takes on a value of 1:
p(X) = e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup> / (1 + e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup>)
We then use some probability threshold to classify the observation as either 1 or 0.
For example, we might say that observations with a probability greater than or equal to 0.5 will be classified as “1” and all other observations will be classified as “0.”
This tutorial provides a step-by-step example of how to perform logistic regression in R.
<h3>Step 1: Load the Data</h3>
For this example, we’ll use the <b>Default</b> dataset from the ISLR package. We can use the following code to load and view a summary of the dataset:
<b>#load dataset
data &lt;- ISLR::Default
#view summary of dataset
summary(data)
 default    student       balance           income     
 No :9667   No :7056   Min.   :   0.0   Min.   :  772  
 Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340         Median : 823.6   Median :34553         Mean   : 835.4   Mean   :33517         3rd Qu.:1166.3   3rd Qu.:43808         Max.   :2654.3   Max.   :73554  
#find total observations in dataset
nrow(data)
[1] 10000
</b>
This dataset contains the following information about 10,000 individuals:
<b>default:</b> Indicates whether or not an individual defaulted.
<b>student:</b> Indicates whether or not an individual is a student.
<b>balance:</b> Average balance carried by an individual.
<b>income:</b> Income of the individual.
We will use student status, bank balance, and income to build a logistic regression model that predicts the probability that a given individual defaults.
<h3>Step 2: Create Training and Test Samples</h3>
Next, we’ll split the dataset into a training set to <em>train</em> the model on and a testing set to <em>test</em> the model on.
<b>#make this example reproducible
set.seed(1)
#Use 70% of dataset as training set and remaining 30% as testing set
sample &lt;- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train &lt;- data[sample, ]
test &lt;- data[!sample, ]  </b>
<h3>Step 3: Fit the Logistic Regression Model</h3>
Next, we’ll use the <b>glm</b> (general linear model) function and specify family=”binomial” so that R fits a logistic regression model to the dataset:
<b>#fit logistic regression model
model &lt;- glm(default~student+balance+income, family="binomial", data=train)
#disable scientific notation for model summary
options(scipen=999)
#view model summary
summary(model)
Call:
glm(formula = default ~ student + balance + income, family = "binomial", 
    data = train)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5586  -0.1353  -0.0519  -0.0177   3.7973  
Coefficients: Estimate    Std. Error z value            Pr(>|z|)    
(Intercept) -11.478101194   0.623409555 -18.412 &lt;0.0000000000000002 ***
studentYes   -0.493292438   0.285735949  -1.726              0.0843 .  
balance       0.005988059   0.000293765  20.384 &lt;0.0000000000000002 ***
income        0.000007857   0.000009965   0.788              0.4304    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 2021.1  on 6963  degrees of freedom
Residual deviance: 1065.4  on 6960  degrees of freedom
AIC: 1073.4
Number of Fisher Scoring iterations: 8
</b>
The coefficients in the output indicate the average change in log odds of defaulting. For example, a one unit increase in <b>balance</b> is associated with an average increase of <b>0.005988</b> in the log odds of defaulting.
The p-values in the output also give us an idea of how effective each predictor variable is at predicting the probability of default:
P-value of student status: <b>0.0843</b>
P-value of balance: <b>&lt;0.0000</b>
P-value of income: <b>0.4304</b>
We can see that balance and student status seem to be important predictors since they have low p-values while income is not nearly as important.
<b>Assessing Model Fit:</b>
In typical  linear regression , we use R<sup>2</sup> as a way to assess how well a model fits the data. This number ranges from 0 to 1, with higher values indicating better model fit.
However, there is no such R<sup>2</sup> value for logistic regression. Instead, we can compute a metric known as  McFadden’s R<sup>2</sup> , which ranges from 0 to just under 1. Values close to 0 indicate that the model has no predictive power. In practice, values over 0.40 indicate that a model fits the data very well.
We can compute McFadden’s R<sup>2</sup> for our model using the <b>pR2</b> function from the pscl package:
<b>pscl::pR2(model)["McFadden"]
 McFadden 
0.4728807 
</b>
A value of <b>0.4728807</b> is quite high for McFadden’s R<sup>2</sup>, which indicates that our model fits the data very well and has high predictive power.
<b>Variable Importance:</b>
We can also compute the importance of each predictor variable in the model by using the <b>varImp</b> function from the caret package:
<b>caret::varImp(model)
             Overall
studentYes  1.726393
balance    20.383812
income      0.788449</b>
Higher values indicate more importance. These results match up nicely with the p-values from the model. Balance is by far the most important predictor variable, followed by student status and then income.
<b>VIF Values:</b>
We can also calculate the  VIF values  of each variable in the model to see if  multicollinearity  is a problem:
<b>#calculate VIF values for each predictor variable in our model
car::vif(model)
 student  balance   income 
2.754926 1.073785 2.694039
</b>
As a rule of thumb, VIF values above 5 indicate severe multicollinearity. Since none of the  predictor variables in our models have a VIF over 5, we can assume that multicollinearity is not an issue in our model.
<h3>Step 4: Use the Model to Make Predictions</h3>
Once we’ve fit the logistic regression model, we can then use it to make predictions about whether or not an individual will default based on their student status, balance, and income:
<b>#define two individuals
new &lt;- data.frame(balance = 1400, income = 2000, student = c("Yes", "No"))
#predict probability of defaulting
predict(model, new, type="response")
         1          2 
0.02732106 0.04397747
</b>
The probability of an individual with a balance of $1,400, an income of $2,000, and a student status of “Yes” has a probability of defaulting of <b>.0273</b>. Conversely, an individual with the same balance and income but with a student status of “No” has a probability of defaulting of <b>0.0439</b>. 
We can use the following code to calculate the probability of default for every individual in our test dataset:
<b>#calculate probability of default for each individual in test dataset
predicted &lt;- predict(model, test, type="response")</b>
<h3>Step 5: Model Diagnostics</h3>
Lastly, we can analyze how well our model performs on the test dataset.
By default, any individual in the test dataset with a probability of default greater than 0.5 will be predicted to default. However, we can find the optimal probability to use to maximize the accuracy of our model by using the <b>optimalCutoff()</b> function from the InformationValue package:
<b>library(InformationValue)
#convert defaults from "Yes" and "No" to 1's and 0's
test$default &lt;- ifelse(test$default=="Yes", 1, 0)
#find optimal cutoff probability to use to maximize accuracy
optimal &lt;- optimalCutoff(test$default, predicted)[1]
optimal
[1] 0.5451712
</b>
This tells us that the optimal probability cutoff to use is <b>0.5451712</b>. Thus, any individual with a probability of defaulting of 0.5451712 or higher will be predicted to default, while any individual with a probability less than this number will be predicted to not default.
Using this threshold, we can create a confusion matrix which shows our predictions compared to the actual defaults:
<b>confusionMatrix(test$default, predicted)
     0  1
0 2912 64
1   21 39</b>
We can also calculate the sensitivity (also known as the “true positive rate”) and specificity (also known as the “true negative rate”) along with the total misclassification error (which tells us the percentage of total incorrect classifications):
<b>#calculate sensitivity
sensitivity(test$default, predicted)
[1] 0.3786408
#calculate specificity
specificity(test$default, predicted)
[1] 0.9928401
#calculate total misclassification error rate
misClassError(test$default, predicted, threshold=optimal)
[1] 0.027
</b>
The total misclassification error rate is <b>2.7%</b> for this model. In general, the lower this rate the better the model is able to predict outcomes, so this particular model turns out to be very good at predicting whether an individual will default or not.
Lastly, we can plot the ROC (Receiver Operating Characteristic) Curve which displays the percentage of true positives predicted by the model as the prediction probability cutoff is lowered from 1 to 0. The higher the AUC (area under the curve), the more accurately our model is able to predict outcomes:
<b>#plot the ROC curve
plotROC(test$default, predicted)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/roc.png">
We can see that the AUC is <b>0.9131</b>, which is quite high. This indicates that our model does a good job of predicting whether or not an individual will default.
<em>The complete R code used in this tutorial can be found  here .</em>
<h2><span class="orange">How to Perform Logistic Regression in SAS</span></h2>
 Logistic regression  is a method we can use to fit a regression model when the response variable is binary.
Logistic regression uses a method known as <em>maximum likelihood estimation</em> to find an equation of the following form:
<b>log[p(X) / (1-p(X))]  =  β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></b>
where:
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The coefficient estimate for the j<sup>th</sup> predictor variable
The formula on the right side of the equation predicts the <b>log odds</b> of the response variable taking on a value of 1.
The following step-by-step example shows how to fit a logistic regression model in SAS.
<h3>Step 1: Create the Dataset</h3>
First, we’ll create a dataset that contains information on the following three variables for 18 students:
Acceptance into a certain college (1 = yes, 0 = no)
GPA (scale of 1 to 4)
ACT score (scale of 1 to 36)
<b>/*create dataset*/
data my_data;
    input acceptance gpa act;
    datalines;
1 3 30
0 1 21
0 2 26
0 1 24
1 3 29
1 3 34
0 3 31
1 2 29
0 1 21
1 2 21
0 1 15
1 3 32
1 4 31
1 4 29
0 1 24
1 4 29
1 3 21
1 4 34
;
run;
/*view dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/logSAS1.jpg"196">
<h3>Step 2: Fit the Logistic Regression Model</h3>
Next, we’ll use <b>proc logistic</b> to fit the logistic regression model, using “acceptance” as the response variable and “gpa” and “act” as the predictor variables.
<b>Note</b>: We must specify <b>descending</b> so SAS knows to predict the probability that the response variable will take on a value of 1. By default, SAS predicts the probability that the response variable will take on a value of 0.
<b>/*fit logistic regression model*/
proc logistic data=my_data descending;
  model acceptance = gpa act;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/logsas2.jpg"434">
The first table of interest is titled <b>Model Fit Statistics</b>.
From this table we can see the AIC value of the model, which turns out to be <b>16.595</b>. The lower the AIC value, the better a model is able to fit the data.
However, there is no threshold for what is considered a  “good” AIC value . Rather, we use AIC to compare the fit of several models fit to the same dataset. The model with the lowest AIC value is generally considered the best.
The next table of interest is titled <b>Testing Global Null Hypothesis: BETA=0</b>.
From this table we can see the Likelihood Ratio Chi-square value of <b>13.4620</b> with a corresponding p-value of <b>0.0012</b>.
Since this p-value is less than .05, this tells us that the logistic regression model as a whole is statistically significant.
Next, we can analyze the coefficient estimates in the table titled Analysis of <b>Maximum Likelihood Estimates</b>.
From this table we can see the coefficients for gpa and act, which indicate the average change in log odds of getting accepted into the university for a one unit increase in each variable.
For example:
A one-unit increase in GPA value is associated with an average increase of <b>2.9665</b> in the log odds of getting accepted into the university.
A one-unit increase in ACT score is associated with an average <em>decrease</em> of <b>0.1145</b> in the log odds of getting accepted into the university.
The corresponding p-values in the output also give us an idea of how effective each predictor variable is at predicting the probability of getting accepted:
P-value of GPA: <b>0.0679</b>
P-value of ACT: <b>0.6289</b>
This tells us that GPA seems to be a statistically significant predictor of university acceptance while ACT score seems to not be statistically significant.
<h2><span class="orange">How to Perform Logistic Regression in Python (Step-by-Step)</span></h2>
 Logistic regression  is a method we can use to fit a regression model when the  response variable  is binary.
Logistic regression uses a method known as <em>maximum likelihood estimation</em> to find an equation of the following form:
<b>log[p(X) / (1-p(X))]  =  β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></b>
where:
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The coefficient estimate for the j<sup>th</sup> predictor variable
The formula on the right side of the equation predicts the <b>log odds</b> of the response variable taking on a value of 1.
Thus, when we fit a logistic regression model we can use the following equation to calculate the probability that a given observation takes on a value of 1:
p(X) = e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup> / (1 + e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup>)
We then use some probability threshold to classify the observation as either 1 or 0.
For example, we might say that observations with a probability greater than or equal to 0.5 will be classified as “1” and all other observations will be classified as “0.”
This tutorial provides a step-by-step example of how to perform logistic regression in R.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import the necessary packages to perform logistic regression in Python:
<b>import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import matplotlib.pyplot as plt
</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use the <b>Default</b> dataset from the  Introduction to Statistical Learning book . We can use the following code to load and view a summary of the dataset:
<b>#import dataset from CSV file on Github
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/default.csv"
data = pd.read_csv(url)
#view first six rows of dataset
data[0:6]
        defaultstudentbalance        income
000729.52649544361.625074
101817.18040712106.134700
2001073.54916431767.138947
300529.25060535704.493935
400785.65588338463.495879
501919.5885307491.558572  
#find total observations in dataset
len(data.index)
10000
</b>
This dataset contains the following information about 10,000 individuals:
<b>default:</b> Indicates whether or not an individual defaulted.
<b>student:</b> Indicates whether or not an individual is a student.
<b>balance:</b> Average balance carried by an individual.
<b>income:</b> Income of the individual.
We will use student status, bank balance, and income to build a logistic regression model that predicts the probability that a given individual defaults.
<h3>Step 3: Create Training and Test Samples</h3>
Next, we’ll split the dataset into a training set to <em>train</em> the model on and a testing set to <em>test</em> the model on.
<b>#define the predictor variables and the response variable
X = data[['student', 'balance', 'income']]
y = data['default']
#split the dataset into training (70%) and testing (30%) sets
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)  </b>
<h3>Step 4: Fit the Logistic Regression Model</h3>
Next, we’ll use the <b>LogisticRegression()</b> function to fit a logistic regression model to the dataset:
<b>#instantiate the model
log_regression = LogisticRegression()
#fit the model using the training data
log_regression.fit(X_train,y_train)
#use model to make predictions on test data
y_pred = log_regression.predict(X_test)
</b>
<h3>Step 5: Model Diagnostics</h3>
Once we fit the regression model, we can then analyze how well our model performs on the test dataset.
First, we’ll create the confusion matrix for the model:
<b>cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix
array([[2886,    1],
       [ 113,    0]])
</b>
From the confusion matrix we can see that:
#True positive predictions: 2886
#True negative predictions: 0
#False positive predictions: 113
#False negative predictions: 1
We can also obtain the accuracy of the model, which tells us the percentage of correction predictions the model made:
<b>print("Accuracy:",metrics.accuracy_score(y_test, y_pred))l
Accuracy: 0.962
</b>
This tells us that the model made the correct prediction for whether or not an individual would default <b>96.2%</b> of the time.
Lastly, we can plot the ROC (Receiver Operating Characteristic) Curve which displays the percentage of true positives predicted by the model as the prediction probability cutoff is lowered from 1 to 0.
The higher the AUC (area under the curve), the more accurately our model is able to predict outcomes:
<b>#define metrics
y_pred_proba = log_regression.predict_proba(X_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.legend(loc=4)
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/auc1.png">
<em>The complete Python code used in this tutorial can be found  here .</em>
<h2><span class="orange">4 Examples of Using Logistic Regression in Real Life</span></h2>
<b> Logistic regression  </b>is a statistical method that we use to fit a regression model when the response variable is binary.
This tutorial shares four different examples of when logistic regression is used in real life.
<h3>Logistic Regression Real Life Example #1</h3>
Medical researchers want to know how exercise and weight impact the probability of having a heart attack. To understand the relationship between the predictor variables and the probability of having a heart attack, researchers can perform logistic regression.
The response variable in the model will be heart attack and it has two potential outcomes:
A heart attack occurs.
A heart attack does not occur.
The results of the model will tell researchers exactly how changes in exercise and weight affect the probability that a given individual has a heart attack. The researchers can also use the fitted logistic regression model to predict the probability that a given individual has a heart attacked, based on their weight and their time spent exercising.
<h3>Logistic Regression Real Life Example #2</h3>
Researchers want to know how GPA, ACT score, and number of AP classes taken impact the probability of getting accepted into a particular university. To understand the relationship between the predictor variables and the probability of getting accepted, researchers can perform logistic regression.
The response variable in the model will be “acceptance” and it has two potential outcomes:
A student gets accepted.
A student does not get accepted.
The results of the model will tell researchers exactly how changes in GPA, ACT score, and number of AP classes taken affect the probability that a given individual gets accepted into the university. The researchers can also use the fitted logistic regression model to predict the probability that a given individual gets accepted, based on their GPA, ACT score, and number of AP classes taken.
<h3>Logistic Regression Real Life Example #3</h3>
A business wants to know whether word count and country of origin impact the probability that an email is spam. To understand the relationship between these two predictor variables and the probability of an email being spam, researchers can perform logistic regression.
The response variable in the model will be “spam” and it has two potential outcomes:
The email is spam.
The email is not spam.
The results of the model will tell the business exactly how changes in word count and country of origin affect the probability of a given email being spam. The business can also use the fitted logistic regression model to predict the probability that a given email is spam, based on its word count and country of origin.
<h3>Logistic Regression Real Life Example #4</h3>
A credit card company wants to know whether transaction amount and credit score impact the probability of a given transaction being fraudulent. To understand the relationship between these two predictor variables and the probability of a transaction being fraudulent, the company can perform logistic regression.
The response variable in the model will be “fraudulent” and it has two potential outcomes:
The transaction is fraudulent.
The transaction is not fraudulent.
The results of the model will tell the company exactly how changes in transaction amount and credit score affect the probability of a given transaction being fraudulent. The company can also use the fitted logistic regression model to predict the probability that a given transaction is fraudulent, based on the transaction amount and the credit score of the individual who made the transaction.
<h2><span class="orange">How to Perform Logistic Regression in SPSS</span></h2>
<b>Logistic regression</b> is a method that we use to fit a  regression model  when the response variable is binary.
This tutorial explains how to perform logistic regression in SPSS.
<h3>Example: Logistic Regression in SPSS</h3>
Use the following steps to perform logistic regression in SPSS for a dataset that shows whether or not college basketball players got drafted into the NBA (draft: 0 = no, 1 = yes) based on their average points per game and division level.
<b>Step 1: Input the data.</b>
First, input the following data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/logspss1.png">
<b>Step 2: Perform logistic regression.</b>
Click the <b>Analyze </b>tab, then <b>Regression</b>, then <b>Binary Logistic Regression</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/logspss2.png">
In the new window that pops up, drag the binary response variable <b>draft </b>into the box labelled Dependent. Then drag the two predictor variables <b>points </b>and <b>division </b>into the box labelled Block 1 of 1. Leave the <b>Method </b>set to Enter. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/logspss3.png">
<b>Step 3. Interpret the output.</b>
Once you click <b>OK</b>, the output of the logistic regression will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/logspss4.png">
Here is how to interpret the output:
<b>Model Summary: </b>The most useful metric in this table is the Nagelkerke R Square, which tells us the percentage of the variation in the  response variable  that can be explained by the predictor variables. In this case, points and division are able to explain <b>72.5%</b> of the variability in draft.
<b>Classification Table: </b>The most useful metric in this table is the Overall Percentage, which tells us the percentage of observations that the model was able to classify correctly. In this case, the logistic regression model was able to correctly predict the draft result of <b>85.7%</b> of players.
<b>Variables in the Equation: </b>This last table provides us with several useful metrics, including:
<b>Wald: </b>The Wald test statistic for each predictor variable, which is used to determine whether or not each predictor variable is statistically significant.
<b>Sig: </b>The p-value that corresponds to the Wald test statistic for each predictor variable. We see that the p-value for <b>points </b>is .039 and the p-value for <b>division </b>is .557.
<b>Exp(B): </b>The odds ratio for each predictor variable. This tells us the change in the odds of a player getting drafted associated with a one unit increase in a given predictor variable. For example, the odds of a player in division 2 getting drafted are just .339 of the odds of a player in division 1 getting drafted. Similarly, each additional unit increase in points per game is associated with an increase of 1.319 in the odds of a player getting drafted.
We can then use the coefficients (the values in the column labeled B) to predict the probability that a given player will get drafted, using the following formula:
Probability = e<sup>-3.152 + .277(points) – 1.082(division)</sup> / (1+e<sup>-3.152 + .277(points) – 1.082(division)</sup>)
For example, the probability that a player who averages 20 points per game and plays in division 1 gets drafted can be calculated as:
Probability = e<sup>-3.152 + .277(20) – 1.082(1)</sup> / (1+e<sup>-3.152 + .277(20) – 1.082(1)</sup>) = <b>.787</b>.
Since this probability is greater than 0.5, we would predict that this player would get drafted.
<b>Step 4. Report the results.</b>
Lastly, we want to report the results of our logistic regression. Here is an example of how to do so:
Logistic regression was performed to determine how points per game and division level affect a basketball player’s probability of getting drafted. A total of 14 players were used in the analysis.
 
The model explained 72.5% of the variation in draft result and correctly classified 85.7% of cases.
 
The odds of a player in division 2 getting drafted were just .339 of the odds of a player in division 1 getting drafted.
 
Each additional unit increase in points per game was associated with an increase of 1.319 in the odds of a player getting drafted.
<h2><span class="orange">How to Perform Logistic Regression in Stata</span></h2>
<b>Logistic Regression</b> is a method that we use to fit a regression model when the response variable is binary. Here are some examples of when we may use logistic regression:
We want to know how exercise, diet, and weight impact the probability of having a heart attack. The response variable is <em>heart attack</em> and it has two potential outcomes: a heart attack occurs or does not occur.
We want to know how GPA, ACT score, and number of AP classes taken impact the probability of getting accepted into a particular university. The response variable is <em>acceptance </em>and it has two potential outcomes: accepted or not accepted.
We want to know whether word count and email title impact the probability that an email is spam. The response variable is <em>spam </em>and it has two potential outcomes: spam or not spam.
This tutorial explains how to perform logistic regression in Stata.
<h2>Example: Logistic Regression in Stata</h2>
Suppose we are interested in understanding whether a mother’s age and her smoking habits affect the probability of having a baby with a low birthweight.
To explore this, we can perform logistic regression using age and smoking (either yes or no) as explanatory variables and low birthweight (either yes or no) as a response variable. Since the response variable is binary – there are only two possible outcomes – it is appropriate to use logistic regression.
Perform the following steps in Stata to conduct a logistic regression using the dataset called <em>lbw</em>, which contains data on 189 different mothers.
<b>Step 1: Load the data.</b>
Load the data by typing the following into the Command box:
<b>use http://www.stata-press.com/data/r13/lbw</b>
<b>Step 2: Get a summary of the data.</b>
Gain a quick understanding of the data you’re working with by typing the following into the Command box:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/logisticRegStata1.png">
We can see that there are 11 different variables in the dataset, but the only three that we care about are the following:
<b>low</b> – whether or not the baby had a low birthweight. 1 = yes, 0 = no.
<b>age</b> – age of the mother.
<b>smoke</b> – whether or not the mother smoked during pregnancy. 1 = yes, 0 = no.
<b>Step 3: Perform logistic regression.</b>
Type the following into the Command box to perform logistic regression using <em>age </em>and <em>smoke </em>as explanatory variables and <em>low </em>as the response variable.
<b>logit low age smoke</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/logisticRegStata3.png">
Here is how to interpret the most interesting numbers in the output:
<b>Coef (age):</b> -.0497792. Holding <em>smoke</em> constant, each one year increase in age is associated with a exp(-.0497792) = .951 increase in the odds of a baby having low birthweight. Because this number is less than 1, it means that an increase in age is actually associated with a decrease in the odds of having a baby with low birthweight.
For example, suppose mother A and mother B are both smokers. If mother A is one year older than mother B, then the odds that mother A has a low birthweight baby are just 95.1% of the odds that mother B has a low birthweight baby.
<b>P>|z| (age): </b>0.119. This is the p-value associated with the test statistic for <em>age</em>. Since this value is not less than 0.05, age is not a statistically significant predictor of low birthweight.
<b>Odds Ratio (smoke):</b> .6918486. Holding <em>age</em> constant, a mother who smokes during pregnancy has exp(.6918486) = 1.997 higher odds of having a baby with low birthweight compared to a mother who does not smoke during pregnancy.
For example, suppose mother A and mother B are both 30 years old. If mother A smokes during pregnancy and mother B does not, then the odds that mother A has a low birthweight baby are 99.7% higher than the odds that mother B has a low birthweight baby.
<b>P>|z| (smoke): </b>0.032. This is the p-value associated with the test statistic for <em>smoke</em>. Since this value is less than 0.05, <em>smoke</em> is a statistically significant predictor of low birthweight.
<b>Step 4: Report the results.</b>
Lastly, we want to report the results of our logistic regression. Here is an example of how to do so:
A logistic regression was performed to determine whether a mother’s age and her smoking habits affect the probability of having a baby with a low birthweight. A sample of 189 mothers was used in the analysis.
 
Results showed that there was a statistically significant relationship between smoking and probability of low birthweight (z = 2.15, p = .032) while there was not a statistically significant relationship between age and probability of low birthweight (z = -1.56, p = .119).
<h2><span class="orange">Logistic Regression vs. Linear Regression: The Key Differences</span></h2>
Two of the most commonly used regression models are <b>linear regression</b> and <b>logistic regression</b>.
Both types of regression models are used to quantify the relationship between one or more predictor variables and a  response variable , but there are some key differences between the two models:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/linear_vs_logistic1-1.png">
Here’s a summary of the differences:
<b>Difference #1: Type of Response Variable</b>
A linear regression model is used when the response variable takes on a continuous value such as:
Price
Height
Age
Distance
Conversely, a logistic regression model is used when the response variable takes on a categorical value such as:
Yes or No
Male or Female
Win or Not Win
<b>Difference #2: Equation Used</b>
Linear regression uses the following equation to summarize the relationship between the predictor variable(s) and the response variable:
Y = β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub>
where:
Y: The response variable
X<sub>j</sub>: The j<sup>th</sup> predictor variable
β<sub>j</sub>: The average effect on Y of a one unit increase in X<sub>j</sub>, holding all other predictors fixed
Conversely, logistic regression uses the following equation:
p(X) = e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup> / (1 + e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup>)
This equation is used to predict the probability that an individual observation falls into a certain category.
<b>Difference #3: Method Used to Fit Equation</b>
Linear regression uses a method known as <b>ordinary least squares</b> to find the best fitting regression equation.
Conversely, logistic regression uses a method known as <b>maximum likelihood estimation</b> to find the best fitting regression equation.
<b>Difference #4: Output to Predict</b>
Linear regression predicts a continuous value as the output. For example:
Price ($150, $199, $400, etc.)
Height (14 inches, 2 feet, 94.32 centimeters, etc.)
Age (2 months, 6 years, 41.5 years, etc.)
Distance (1.23 miles, 4.5 kilometers, etc.)
Conversely, logistic regression predicts probabilities as the output. For example:
40.3% chance of getting accepted to a university.
93.2% chance of winning a game.
34.2% chance of a law getting passed.
<h3>When to Use Logistic vs. Linear Regression</h3>
The following practice problems can help you gain a better understanding of when to use logistic regression or linear regression.
<b>Problem #1: Annual Income</b>
Suppose an economist wants to use predictor variables (1) weekly hours worked and (2) years of education to predict the annual income of individuals.
In this scenario, he would use <b>linear regression</b> because the response variable (annual income) is continuous.
<b>Problem #2: University Acceptance</b>
Suppose a college admissions officer wants to use the predictor variables (1) GPA and (2) ACT score to predict the probability that a student will get accepted into a certain university.
In this scenario, she would use <b>logistic regression</b> because the response variable is categorial and can only take on two values – accepted or not accepted.
<b>Problem #3: Home Price</b>
Suppose a real estate agent wants to use the predictor variables (1) square footage, (2) number of bedrooms, and (3) number of bathrooms to predict the selling house of prices.
In this scenario, she would use <b>linear regression</b> because the response variable (price) is continuous.
<b>Problem #4: Spam Detection</b>
Suppose a computer programmer wants to use the predictor variables (1) number of words and (2) country of origin to predict the probability that a given email is spam.
In this scenario, he would use <b>logistic regression</b> because the response variable is categorical and can only take on two values – spam or not spam.
 Introduction to Simple Linear Regression 
 Introduction to Multiple Linear Regression 
 4 Examples of Using Linear Regression in Real Life 
The following tutorials offer more details on logistic regression:
 Introduction to Logistic Regression 
 4 Examples of Using Logistic Regression in Real Life 
<h2><span class="orange">Introduction to Logistic Regression</span></h2>
When we want to understand the relationship between one or more predictor variables and a continuous response variable, we often use  linear regression .
However, when the response variable is categorical we can instead use <b>logistic regression</b>.
Logistic regression is a type of  classification algorithm  because it attempts to “classify” observations from a dataset into distinct categories.
Here are a few examples of when we might use logistic regression:
We want to use <em>credit score</em> and <em>bank balance</em> to predict whether or not a given customer will default on a loan. (Response variable = “Default” or “No default”)
We want to use <em>average rebounds per game</em> and <em>average points per game</em> to predict whether or not a given basketball player will get drafted into the NBA (Response variable = “Drafted” or “Not Drafted”)
We want to use <em>square footage</em> and <em>number of bathrooms</em> to predict whether or not a house in a certain city will be listed at a selling price of $200k or more. (Response variable = “Yes” or “No”)
Notice that the response variable in each of these examples can only take on one of two values. Contrast this with linear regression in which the response variable takes on some continuous value.
<h3>The Logistic Regression Equation</h3>
Logistic regression uses a method known as maximum likelihood estimation (details will not be covered here) to find an equation of the following form:
<b>log[p(X) / (1-p(X))]  =  β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></b>
where:
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The coefficient estimate for the j<sup>th</sup> predictor variable
The formula on the right side of the equation predicts the <b>log odds</b> of the response variable taking on a value of 1.
Thus, when we fit a logistic regression model we can use the following equation to calculate the probability that a given observation takes on a value of 1:
p(X) = e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup> / (1 + e<sup>β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub></sup>)
We then use some probability threshold to classify the observation as either 1 or 0.
For example, we might say that observations with a probability greater than or equal to 0.5 will be classified as “1” and all other observations will be classified as “0.”
<h3>How to Interpret Logistic Regression Output</h3>
Suppose we use a logistic regression model to predict whether or not a given basketball player will get drafted into the NBA based on their average rebounds per game and average points per game.
Here is the output for the logistic regression model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/logRegOutput1.png">
Using the coefficients, we can compute the probability that any given player will get drafted into the NBA based on their average rebounds and points per game using the following formula:
P(Drafted) = e<sup>-2.8690 + 0.0698*(rebs) + 0.1694*(points)</sup> / (1+e<sup>-2.8690 + 0.0698*(rebs) + 0.1694*(points)</sup>)
For example, suppose a given player averages 8 rebounds per game and 15 points per game. According to the model, the probability that this player will get drafted into the NBA is <b>0.557</b>.
P(Drafted) = e<sup>-2.8690 + 0.0698*(8) + 0.1694*(15)</sup> / (1+e<sup>-2.8690 + 0.0698*(8) + 0.1694*(15)</sup>) = <b>0.557</b>
Since this probability is greater than 0.5, we would predict that this player will get drafted.
Contrast this with a player who only averages 3 rebounds and 7 points per game. The probability that this player will get drafted into the NBA is <b>0.186</b>.
P(Drafted) = e<sup>-2.8690 + 0.0698*(3) + 0.1694*(7)</sup> / (1+e<sup>-2.8690 + 0.0698*(3) + 0.1694*(7)</sup>) = <b>0.186</b>
Since this probability is less than 0.5, we would predict that this player will not get drafted.
<h3>Assumptions of Logistic Regression</h3>
Logistic regression uses the following assumptions:
<b>1. The response variable is binary.</b> It is assumed that the response variable can only take on two possible outcomes.
<b>2. The observations are independent.</b> It is assumed that the observations in the dataset are independent of each other. That is, the observations should not come from repeated measurements of the same individual or be related to each other in any way.
<b>3. There is no severe multicollinearity among predictor variables</b>. It is assumed that none of the predictor variables are  highly correlated  with each other.
<b>4. There are no extreme outliers.</b> It is assumed that there are no extreme outliers or influential observations in the dataset.
<b>5. There is a linear relationship between the predictor variables and the logit of the response variable</b>. This assumption can be tested using a Box-Tidwell test.
<b>6. The sample size is sufficiently large.</b> As a rule of thumb, you should have a minimum of 10 cases with the least frequent outcome for each explanatory variable. For example, if you have 3 explanatory variables and the expected probability of the least frequent outcome is 0.20, then you should have a sample size of at least (10*3) / 0.20 = 150.
Check out  this post  for a detailed explanation of how to check these assumptions.
<h2><span class="orange">How to Create a Lollipop Chart in R</span></h2>
Similar to a bar chart, a <b>lollipop chart</b> is useful for comparing the quantitative values of a categorical variable. Instead of using bars, though, a lollipop chart uses lines with circles on the end to represent the quantitative values.
A lollipop chart is an excellent way to compare multiple categories while keeping the amount of color on the chart minimal and drawing the attention of the reader to the actual values on the chart as opposed to the lines or other graphics on the chart. Many people also consider the lollipop chart to be aesthetically pleasing.
In this tutorial we will walk through the necessary steps to create the following lollipop chart:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli6.jpg">
<h2>Example: Lollipop Chart in R</h2>
For this example, we will use the built-in R dataset <b>mtcars</b>:
<b>#view first six rows of <em>mtcars
</em>head(mtcars)
</b>
<h3><img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli0.jpg"></h3>
<h2>A Basic Lollipop Chart</h2>
The following code illustrates how<b> </b>to create a lollipop chart to compare the <em>mpg </em>(miles per gallon) for each of the 32 cars in the dataset.
The names of the cars are defined in the row names of the dataset, so first we create a new column in the dataset that contains these row names.
Next, we load the library <b>ggplot2</b>, which we will use to actually create the lollipop chart.
With ggplot2, we use <b>geom_segment </b>to create the lines on the plot. We define the starting and ending x-values as <em>0 </em>and <em>mpg</em>, respectively. We define the starting and ending y-values as <em>car</em>:
<b>#create new column for car names
mtcars$car &lt;- row.names(mtcars)
#load <em>ggplot2 </em>library
library(ggplot2)
#create lollipop chart
ggplot(mtcars, aes(x = mpg, y = car)) +
        geom_segment(aes(x = 0, y = car, xend = mpg, yend = car)) +
        geom_point()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli1_1.jpg">
<h2>Adding Labels</h2>
We can also add labels to the chart by using the <b>label </b>and <b>geom_text </b>arguments:
<b>ggplot(mtcars, aes(x = mpg, y = car, label = mpg)) +
        geom_segment(aes(x = 0, y = car, xend = mpg, yend = car)) +
        geom_point() +
        geom_text(nudge_x = 1.5)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli2_1.jpg">
Or instead of placing the labels at the end of each line, we could place them inside of the circles themselves by enlarging the circles and changing the label font color to white:
<b>ggplot(mtcars, aes(x = mpg, y = car, label = mpg)) +
        geom_segment(aes(x = 0, y = car, xend = mpg, yend = car)) +
        geom_point(size = 7) +
        geom_text(color = 'white', size = 2)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli3.jpg">
<h2>Comparing Values to an Average</h2>
We can also use a lollipop chart to compare values to a specific number. For example, we can find the average value for <em>mpg </em>in the dataset and then compare the <em>mpg </em>of each car to the average. 
The following code uses the library <b>dplyr </b>to find the average value for <em>mpg </em>and then arrange the cars in order by <em>mpg </em>ascending:
<b>#load library <em>dplyr
</em>library(dplyr)
#find mean value of<em> mpg </em>and arrange cars in order<em> by mpg </em>descending
mtcars_new &lt;- mtcars %>%
                arrange(mpg) %>%
                mutate(mean_mpg = mean(mpg),       flag = ifelse(mpg - mean_mpg > 0, TRUE, FALSE),       car = factor(car, levels = .$car))
#view first six rows of <em>mtcars_new
</em>head(mtcars_new)
</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli4.jpg">
Next, the following code creates the lollipop chart by defining the color of the circle to be equal to the value of <em>flag </em>(in this case, TRUE or FALSE) and the starting x-value for each car is equal to the average value of <em>mpg</em>.
<b>ggplot(mtcars_new, aes(x = mpg, y = car, color = flag)) +
        geom_segment(aes(x = mean_mpg, y = car, xend = mpg, yend = car)) +
        geom_point()</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli5.jpg">
By using this type of color scheme, we can easily tell which cars have an <em>mpg </em>less than and above the average for the dataset.
By default, R uses blue and red as the colors for the chart. However, we can use whatever colors we’d like by using the <b>scale_colour_manual</b> argument:
<b>ggplot(mtcars_new, aes(x = mpg, y = car, color = flag)) +
        geom_segment(aes(x = mean_mpg, y = car, xend = mpg, yend = car)) +
        geom_point() +
        scale_colour_manual(values = c("purple", "blue"))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli7.jpg">
<h2>Altering the Aesthetics of the Chart</h2>
Lastly, we can use the wide capabilities of <b>ggplot2 </b>to alter the aesthetics of the chart further and create a professional-looking final product:
<b>ggplot(mtcars_new, aes(x = mpg, y = car, color = flag)) +
        geom_segment(aes(x = mean_mpg, y = car, xend = mpg, yend = car), color = "grey") +
        geom_point() +
        annotate("text", x = 27, y = 20, label = "Above Average", color = "#00BFC4", size = 3, hjust = -0.1, vjust = .75) +
        annotate("text", x = 27, y = 17, label = "Below Average", color = "#F8766D", size = 3, hjust = -0.1, vjust = -.1) +
        geom_segment(aes(x = 26.5, xend = 26.5, y = 19, yend = 23),     arrow = arrow(length = unit(0.2,"cm")), color = "#00BFC4") +
        geom_segment(aes(x = 26.5, xend = 26.5 , y = 18, yend = 14),     arrow = arrow(length = unit(0.2,"cm")), color = "#F8766D") +
        labs(title = "Miles per Gallon by Car") +
        theme_minimal() +
        theme(axis.title = element_blank(),
              panel.grid.minor = element_blank(),
              legend.position = "none",
              text = element_text(family = "Georgia"),
              axis.text.y = element_text(size = 8),
              plot.title = element_text(size = 20, margin = margin(b = 10), hjust = 0),
              plot.subtitle = element_text(size = 12, color = "darkslategrey", margin = margin(b = 25, l = -25)),
              plot.caption = element_text(size = 8, margin = margin(t = 10), color = "grey70", hjust = 0))</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/lolli6.jpg">
<h2><span class="orange">What is a Long Tail Distribution? (Definition & Example)</span></h2>
In statistics, a <b>long tail distribution</b> is a distribution that has a long “tail” that slowly tapers off toward the end of the distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/longtail1-1.jpg">
It turns out that this type of distribution appears all the time in different real-world domains and it has interesting implications.
This article provides several examples of long-tail distributions in the real world and shares why long-tail distributions are important.
<h3>Example 1: Long-Tail Distributions in Book Sales</h3>
One of the most well-known examples of a long-tail distribution is book sales.
There are a few books that have sold hundreds of millions of copies (Harry Potter, The Lord of the Rings, The Da Vinci Code, etc.) but most books sell less than one hundred copies total.
If we created a bar chart to visualize the total sales of every book ever published, we would find that the chart exhibits a long-tailed distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/longtail2.jpg">
A few well-known books have sold hundreds of millions of copies and then there’s a long tail of thousands upon thousands of books that have sold very few copies.
<h3>Example 2: Long-Tail Distributions in YouTube Channels</h3>
Another well-known example of a long-tail distribution is the number of subscribers for YouTube channels.
There are a handful of channels that have tens of millions of subscribers, but then there is a long tail of thousands upon thousands of channels that have less than 1,000 followers.
If we created a bar chart to visualize the total subscribers for each YouTube channel in existence, we would find that the chart exhibits a long-tailed distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/longtail3.jpg">
<h3>Example 3: Long-Tail Distributions in City Populations</h3>
Another example of a long-tail distribution is the population size of cities in the United States.
There are a handful of cities that have millions of residents, but then there is a long tail of thousands upon thousands of cities that have fewer than 10,000 total residents.
If we created a bar chart to visualize the population size for all cities in the United States, we would find that the chart exhibits a long-tailed distribution:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/longtail4.jpg"533">
<h2>Why Long-Tail Distributions Matter</h2>
Long-tail distributions are particularly important to understand in business because they present an opportunity for a business model that revolves around making a few sales for <b>a lot</b> of products.
For example, consider our example from earlier about book sales:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/longtail2.jpg">
All of those “books you’ve never heard of” actually had customers interested in reading them, but most big retailers didn’t carry them in store because they didn’t sell many copies.
However, when <b>Amazon</b> suddenly made it possible to buy these niches books online they were able to sell a few copies of tens of thousands of different books from the long tail of the distribution.
The combined sales of all of these books in the long tail actually amounted to significant profits.
Other businesses have also capitalized on this model. For example, <b>eBay</b> has made it possible for hundreds of thousands of people to make a few sales of niche items.
Since eBay takes a small cut of each sale, all of these sales from long-tail sellers who only sell a couple items per year actually adds up to significant profits for eBay.
Yet another example of a business who has capitalized on the long-tail model is <b>Netflix</b>.
Most TV shows and movies that are released only have small audiences, but by creating a platform where all of these niche long-tail shows could be viewed a few times each, Netflix was able to build a massive base of subscribers who each subscribed to watch particular shows.
In fact, there is an entire book written about this business model called  The Long Tail  by Chris Anderson.
The whole book is about how making a few sales on <em>lots</em> of products can actually be a great business model.
<h2><span class="orange">Long vs. Wide Data: What’s the Difference?</span></h2>
A dataset can be written in two different formats: <b>wide</b> and <b>long</b>.
A <b>wide</b> format contains values that <em>do not</em> repeat in the first column.
A <b>long</b> format contains values that <em>do </em>repeat in the first column.
For example, consider the following two datasets that contain the exact same data expressed in different formats:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/wideLong1-1.png">
Notice that in the <b>wide</b> dataset, each value in the first column is unique.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/wideLong2.png">
By contrast, in the <b>long</b> dataset the values in the first column repeat.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/wideLong3-1.png">
Both datasets contain the exact same information about the teams, but they’re simply expressed in different formats.
<h2>When to Use Wide vs. Long Data</h2>
Depending on what you want to do with your data, it may make more sense to have it in a wide or long format.
<h3>When to Use Wide Format</h3>
As a rule of thumb, if you’re analyzing data then you typically will use a <b>wide</b> data format.
For example, if you want to find the average points, assists, and rebounds scored per team then it’s often easier to have the data in a wide format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/wideLong4.png">
Most datasets that you encounter in the real world will also be recorded in a wide format because it’s easier for our brains to interpret.
For example, in the format above it’s easy to read the points, assists, and rebounds values for each team on the same row.
<h3>When to Use Long Format</h3>
As a rule of thumb, if you’re visualizing multiple variables in a plot using statistical software such as  R  you typically must convert your data to a <b>long</b> format in order for the software to create the plot.
For actual examples of this, check out these tutorials in R in which the data must be in a <b>long</b> format to create certain types of plots:
 How to Plot Multiple Density Plots in R 
 How to Plot Multiple Columns in R 
 How to Create a Heatmap in R 
Occasionally you may need to reshape your data into a different format if you’re using  Python  as well.
The following tutorials explain how to reshape data frames in Python:
 How to Reshape Data from Long to Wide in Python 
 How to Reshape Data from Wide to Long in Python 
<h2><span class="orange">How to Create a Lorenz Curve in Excel (With Example)</span></h2>
Named after American economist  Max Lorenz , the <b>Lorenz curve</b> is a way to visualize the income distribution of a population.
This tutorial provides a step-by-step example of how to create the following Lorenz curve in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz8.jpg">
Let’s jump in!
<h3>Step 1: Enter the Data</h3>
First, we must enter values for two columns: the cumulative population and cumulative income of individuals in a certain country:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/giniExcel1.jpg"470">
Here’s how to interpret the values:
The bottom 20% of individuals in this country account for <b>10%</b> of the total income.
The bottom 50% of individuals in this country account for <b>31%</b> of the total income.
The bottom 60% of individuals in this country account for <b>40%</b> of the total income.
100% of individuals in this country account for<b> 100%</b> of the total income.
Next, we’ll enter the (x, y) values for the line of equality that we will eventually add to the graph:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenze1.jpg"506">
<h3>Step 2: Create Basic Lorenz Curve</h3>
Next, highlight the values in cells <b>A2:B6</b> and then click the <b>Insert</b> tab along the top ribbon, then click the <b>Scatter with Smooth Lines</b> option in the <b>Charts</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz2.jpg"498">
The following chart will automatically be created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz3.jpg"508">
Next, right click anywhere on the chart and click the option that says <b>Select Data</b>.
In the window that appears, click <b>Add</b> under <b>Legend Entries</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz4.jpg"579">
Click the <b>Add</b> button, then enter the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz5.jpg"365">
Once you click <b>OK</b>, the diagonal line of equality will automatically be added to the plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz6.jpg"477">
The basic Lorenz curve is complete. The blue line represents the Lorenz curve and the orange line represents the line of equality.
<h3>Step 3: Customize Lorenz Curve</h3>
Lastly, we can customize the appearance of the chart.
Click on the green plus (<b>+</b>) sign in the top right corner of the chart, then click <b>Legend</b>, then click <b>Bottom</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz7.jpg"454">
A legend will be added to the bottom of the chart.
Next, click on the gridlines in the chart and delete them.
Then, click on each individual line and change the color to whatever you’d like.
Then, add an axis title to both the x-axis and y-axis.
The end result will look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/05/lorenz8.jpg">
<b>Related:</b>  How to Calculate Gini Coefficient in Excel 
<h2><span class="orange">How to Convert Strings to Lowercase in R (With Examples)</span></h2>
You can use the built-in <b>tolower()</b> function in R to convert strings to lowercase.
<b>#convert string to lowercase
tolower(string_name)
</b>
The following examples show how to use this function in practice.
<h3>Example 1: Convert a Single String to Lowercase</h3>
The following code shows how to convert a single string to lowercase in R:
<b>#create string
my_string &lt;- 'THIS IS A SENTENCE WITH WORDS.'
#convert string to all lowercase
tolower(my_string)
[1] "this is a sentence with words."
</b>
Note that the <b>tolower()</b> function converts <em>all</em> characters in a string to lowercase
<h3>Example 2: Convert Each String in Column to Lowercase</h3>
The following code shows how to convert every string in a column of a data frame to lowercase:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Nets', 'Spurs'), points=c(99, 94, 85), rebounds=c(31, 22, 29))
#view data frame
df
   team points rebounds
1  Mavs     99       31
2  Nets     94       22
3 Spurs     85       29
#convert team names to lowercase
df$team &lt;- tolower(df$team)
#view updated data frame
df
   team points rebounds
1  mavs     99       31
2  nets     94       22
3 spurs     85       29</b>
<h3>Example 3: Convert Strings in Multiple Columns to Lowercase</h3>
The following code shows how to convert strings in multiple columns of a data frame to lowercase:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Nets', 'Spurs'), conf=c('WEST', 'EAST', 'WEST'), points=c(99, 94, 85))
#view data frame
df
   team conf points
1  Mavs WEST     99
2  Nets EAST     94
3 Spurs WEST     85
#convert team and conference to lowercase
df[c('team', 'conf')] &lt;- sapply(df[c('team', 'conf')], function(x) tolower(x))
#view updated data frame
df
   team conf points
1  mavs west     99
2  nets east     94
3 spurs west     85
</b>
<h2><span class="orange">How to Perform Lowess Smoothing in R (Step-by-Step)</span></h2>
In statistics, the term <b>lowess</b> refers to “locally weighted scatterplot smoothing” – the process of producing a smooth curve that fits the data points in a scatterplot.
To perform lowess smoothing in R we can use the <b>lowess()</b> function, which uses the following syntax:
<b>lowess(x, y, f = 2/3)</b>
where:
<b>x:</b> A numerical vector of x values.
<b>y:</b> A numerical vector of y values.
<b>f:</b> The value for the smoother span. This gives the proportion of points in the plot which influence the smooth at each value. Larger values result in more smoothness.
The following step-by-step example shows how to perform lowess smoothing for a given dataset in R.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset:
<b>df &lt;- data.frame(x=c(1, 1, 2, 2, 3, 4, 6, 6, 7, 8, 10, 11, 11, 12, 13, 14),</b>
<b>                 y=c(4, 7, 9, 10, 14, 15, 19, 16, 17, 21, 22, 34, 44, 40, 43, 45))</b>
<h3>Step 2: Plot the Data</h3>
Next, let’s plot the x and y values from the dataset:
<b>plot(df$x, df$y)</b>
<h3><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/lowess1.png"></h3>
<h3>Step 3: Plot the Lowess Curve</h3>
Next, let’s plot the lowess smoothing curve over the points in the scatterplot:
<b>#create scatterplot
plot(df$x, df$y)
#add lowess smoothing curve to plot
lines(lowess(df$x, df$y), col='red')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/lowess2.png">
<h3>Step 4: Adjust the Smoother Span (Optional)</h3>
We can also adjust the <b>f</b> argument in the lowess() function to increase or decrease the value used for the smoother span.
Note that the larger the value we provide, the smoother the lowess curve will be.
<b>#create scatterplot
plot(df$x, df$y)
#add lowess smoothing curves
lines(lowess(df$x, df$y), col='red')
lines(lowess(df$x, df$y, f=0.3), col='purple')
lines(lowess(df$x, df$y, f=3), col='steelblue')
#add legend to plot
legend('topleft',
       col = c('red', 'purple', 'steelblue'),
       lwd = 2,
       c('Smoother = 1', 'Smoother = 0.3', 'Smoother = 3'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/lowess3.png">
<h2><span class="orange">R: How to Convert Character to Date Using Lubridate</span></h2>
You can use various functions from the <b>lubridate</b> package in R to convert a character column to a date format.
Two of the most common functions include:
<b>ymd()</b> – Convert character in year-month-date format to date
<b>mdy()</b> – Convert character in month-day-year format to date
The following examples show how to use the <b>ymd()</b> and <b>mdy()</b> functions in practice.
<b>Note</b>: Refer to the  lubridate documentation  for a complete list of functions you can use to convert characters to dates depending on the format your dates are in.
<h2>Example 1: Convert Character to Date Using ymd()</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(date=c('2022-01-05', '2022-02-18', '2022-03-21',        '2022-09-15', '2022-10-30', '2022-12-25'), sales=c(14, 29, 25, 23, 39, 46))
#view data frame
df
        date sales
1 2022-01-05    14
2 2022-02-18    29
3 2022-03-21    25
4 2022-09-15    23
5 2022-10-30    39
6 2022-12-25    46
#view class of date column
class(df$date)
[1] "character"
</b>
Currently the values in the <b>date</b> column are characters, but we can use the <b>ymd()</b> function from the lubridate package to convert them to dates:
<b>library(lubridate)
#convert character to date format
df$date &lt;- ymd(df$date)
#view updated data frame
df
        date sales
1 2022-01-05    14
2 2022-02-18    29
3 2022-03-21    25
4 2022-09-15    23
5 2022-10-30    39
6 2022-12-25    46
#view updated class of date column
class(df$date)
[1] "Date"
</b>
We can see that the <b>date</b> column now has a class of Date instead of character.
<h2>Example 2: Convert Character to Date Using mdy()</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(date=c('March 4, 2022', 'April 9, 2022', 'May 6, 2022',        'May 29, 2022', 'June 1, 2022', 'July 2, 2022'), sales=c(14, 29, 25, 23, 39, 46))
#view data frame
df
           date sales
1 March 4, 2022    14
2 April 9, 2022    29
3   May 6, 2022    25
4  May 29, 2022    23
5  June 1, 2022    39
6  July 2, 2022    46
#view class of date column
class(df$date)
[1] "character"
</b>
Currently the values in the <b>date</b> column are characters, but we can use the <b>mdy()</b> function from the lubridate package to convert them to dates:
<b>library(lubridate)
#convert character to date format
df$date &lt;- mdy(df$date)
#view updated data frame
df
        date sales
1 2022-03-04    14
2 2022-04-09    29
3 2022-05-06    25
4 2022-05-29    23
5 2022-06-01    39
6 2022-07-02    46
#view updated class of date column
class(df$date)
[1] "Date"
</b>
We can see that the <b>date</b> column now has a class of Date instead of character.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Convert Date to Numeric in R 
 How to Extract Month from Date in R 
 How to Add and Subtract Months from a Date in R 
<h2><span class="orange">R: How to Get First or Last Day of Month Using Lubridate</span></h2>
You can use the following methods to get the first or last day of the month for some date in R using functions from the  lubridate  package:
<b>Method 1: Get First Day of Month</b>
<b>library(lubridate)</b>
<b>
df$first_day &lt;- floor_date(ymd(df$date), 'month')
</b>
<b>Method 2: Get Last Day of Month</b>
<b>library(lubridate)
df$last_day &lt;- ceiling_date(ymd(df$date), 'month') - days(1)
</b>
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(date=c('2022-01-05', '2022-02-18', '2022-03-21',        '2022-09-15', '2022-10-30', '2022-12-25'), sales=c(14, 29, 25, 23, 39, 46))
#view data frame
df
        date sales
1 2022-01-05    14
2 2022-02-18    29
3 2022-03-21    25
4 2022-09-15    23
5 2022-10-30    39
6 2022-12-25    46
</b>
<h2>Example 1: Get First Day of Month Using lubridate</h2>
The following code shows how to use the <b>floor_date()</b> function from the lubridate package to get the first day of the month for each value in the <b>date</b> column:
<b>#add new column that contains first day of month
df$first_day &lt;- floor_date(ymd(df$date), 'month')
#view updated data frame
df
        date sales  first_day
1 2022-01-05    14 2022-01-01
2 2022-02-18    29 2022-02-01
3 2022-03-21    25 2022-03-01
4 2022-09-15    23 2022-09-01
5 2022-10-30    39 2022-10-01
6 2022-12-25    46 2022-12-01
</b>
Notice that the values in the new <b>first_day </b>column contain the first day of the month for each value in the <b>date</b> column.
<b>Note</b>: We used the <b>ymd()</b> function to first convert the strings in the date column to a recognizable date format.
<h2>Example 2: Get Last Day of Month Using lubridate</h2>
The following code shows how to use the <b>ceiling_date()</b> function from the lubridate package to get the last day of the month for each value in the <b>date</b> column:
<b>#add new column that contains last day of month
df$last_day &lt;- ceiling_date(ymd(df$date), 'month') - days(1)
#view updated data frame
df
        date sales   last_day
1 2022-01-05    14 2022-01-31
2 2022-02-18    29 2022-02-28
3 2022-03-21    25 2022-03-31
4 2022-09-15    23 2022-09-30
5 2022-10-30    39 2022-10-31
6 2022-12-25    46 2022-12-3122-12-01
</b>
Notice that the values in the new <b>last_day </b>column contain the last day of the month for each value in the <b>date</b> column.
Refer to the lubridate  documentation page  for more date formatting options.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Convert Date to Numeric in R 
 How to Extract Month from Date in R 
 How to Add and Subtract Months from a Date in R 
<h2><span class="orange">Lurking Variables: Definition & Examples</span></h2>
A <b>lurking variable</b> is a variable that is not included in a statistical analysis, yet impacts the relationship between two variables within the analysis.
A lurking variable can hide the true relationship between variables or it can falsely cause a relationship to appear to be present between variables. Essentially, lurking variables can cause the results of a study to be misleading.
In observational studies, it’s important to be aware of the fact that lurking variables could cause unusual interpretations of data and the relationships between variables. In experimental studies, it’s important to design the experiment in such a way that (as much as possible) eliminates the risk of lurking variables.
<h2>Examples of Lurking Variables</h2>
The following examples illustrate several cases in which lurking variables could be present in a study:
<h3>Example 1</h3>
A researcher finds that ice cream sales and shark attacks are highly positively correlated. Does this mean that increased ice cream sales is causing more shark attacks?
That’s unlikely. The more likely cause is the lurking variable <em>weather</em>. When it is warmer outside, more people buy ice cream and more people go in the ocean.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/lurkingVariable1.jpg">
<h3>Example 2</h3>
A researcher finds that popcorn consumption and the amount of traffic accidents over the years is highly correlated. Does this mean that higher popcorn consumption is causing more traffic accidents?
That’s unlikely. The more likely cause is the lurking variable <em>population</em>. As the population increases, both the amount of popcorn consumed and the amount of traffic accidents increases.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/lurkingVariable2.jpg">
<h3>Example 3</h3>
A study finds that the more volunteers that show up after a natural disaster, the greater the damage. Does this mean that volunteers are causing more damage to occur?
That’s unlikely. The more likely cause is the lurking variable <em>size of the natural disaster</em>. A larger natural disaster causes more volunteers to show up as well as an increase in the amount of damage done by the natural disaster.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/lurkingVariable3.jpg">
<h3>Example 4</h3>
A study finds that glove sales and snowboarding accidents are highly correlated. Does this mean that gloves are causing more snowboard accidents to occur?
That’s unlikely. The more likely cause is the lurking variable <em>temperature</em>. As temperature decreases, more people buy gloves and more people go snowboarding.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/05/lurkingVariable4.jpg">
<h2>How to Identify Lurking Variables</h2>
To discover lurking variables, it helps to have domain expertise in the area under study. By knowing what potential variables could be affecting the relationship between the variables in the study that aren’t included explicitly in the study, you may be able to uncover potential lurking variables.
Another way to identify potential lurking variables is through examining residual plots. If there is a trend (either linear or non-linear) in the residuals, this could mean that a lurking variable not included in the study is impacting the variables within the study in some way.
<h2>How to Eliminate the Risk of Lurking Variables</h2>
In observational studies, it can be very difficult to eliminate the risk of lurking variables. In most cases, the best you can do is simply identify, rather then prevent, potential lurking variables that may be impacting the study.
In experimental studies, however, the impact of lurking variables can mostly be eliminated with good experimental design.
For example, suppose we want to know whether two pills have a different impact on blood pressure. We know that lurking variables such as <em>diet</em> and <em>smoking habits </em>also impact blood pressure, so we can attempt to control for these lurking variables by using a  randomized design . This means we randomly assign patients to take either the first or second pill.
Since we randomly assign patients to groups, we can assume that the lurking variables will affect both groups roughly equally. This means any differences in blood pressure can be attributed to the pill, rather than the effect of a lurking variable.
<h2><span class="orange">MAE vs. RMSE: Which Metric Should You Use?</span></h2>
Regression models are used to quantify the relationship between one or more predictor variables and a  response variable .
Whenever we fit a regression model, we want to understand how well the model is able to use the values of the predictor variables to predict the value of the response variable.
Two metrics we often use to quantify how well a model fits a dataset are the mean absolute error (MAE) and the root mean squared error (RMSE), which are calculated as follows:
<b>MAE</b>: A metric that tells us the mean absolute difference between the predicted values and the actual values in a dataset. The lower the MAE, the better a model fits a dataset.
MAE = 1/n * Σ|y<sub>i</sub> – <U+0177><sub>i</sub>|
where:
Σ is a symbol that means “sum”
y<sub>i</sub> is the observed value for the i<sup>th</sup> observation
<U+0177><sub>i</sub> is the predicted value for the i<sup>th</sup> observation
n is the sample size
<b>RMSE</b>: A metric that tells us the square root of the average squared difference between the predicted values and the actual values in a dataset. The lower the RMSE, the better a model fits a dataset.
It is calculated as:
RMSE = √Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)<sup>2</sup> / n
where:
Σ is a symbol that means “sum”
<U+0177><sub>i</sub> is the predicted value for the i<sup>th</sup> observation
y<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
<h3>Example: Calculating RMSE & MAE</h3>
Suppose we use a regression model to predict the number of points that 10 players will score in a basketball game.
The following table shows the predicted points from the model vs. the actual points the players scored:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/mse_1.png">
Using the  MAE Calculator , we can calculate the MAE to be <b>3.2</b>
This tells us that the mean absolute difference between the predicted values made by the model and the actual values is 3.2.
Using the  RMSE Calculator , we can calculate the RMSE to be <b>4</b>.
This tells us that the square root of the average squared differences between the predicted points scored and the actual points scored is 4.
Notice that each metric gives us an idea of the typical difference between the predicted value made by the model and the actual value in the dataset, but the interpretation of each metric is slightly different.
<h3>RMSE vs. MAE: Which Metric Should You Use?</h3>
If you would like to give more weights to observations that are further from the mean (i.e. if being “off” by 20 is more than twice as bad as being off by 10″) then it’s better to use the RMSE to measure error because the RMSE is more sensitive to observations that are further from the mean.
However, if being “off” by 20 is twice as bad as being off by 10 then it’s better to use the MAE.
To illustrate this, suppose we have one player who is a clear outlier in their number of points scored:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/10/out2.png">
Using the online calculators mentioned earlier, we can calculate the MAE and RMSE to be:
<b>MAE</b>: 8
<b>RMSE</b>: 16.4356
Notice that the RMSE increases much more than the MAE.
This is because RMSE uses squared differences in its formula and the squared difference between the observed value of 76 and the predicted value of 22 is quite large. This causes the value for RMSE to increase significantly.
In practice, we typically fit several regression models to a dataset and calculate just one of these metrics for each model.
For example, we might fit three different regression models and calculate the RMSE for each model. We would then select the model with the lowest RMSE value as the “best” model because it is the one that makes predictions that are closest to the actual values from the dataset.
In either case, just make sure to calculate the same metric for each model. For example, don’t calculate MAE for one model and RMSE for another model and then compare those two metrics. 
<h2><span class="orange">How to Calculate Mahalanobis Distance in Python</span></h2>
The <b>Mahalanobis distance </b>is the distance between two points in a multivariate space. It’s often used to find outliers in statistical analyses that involve several variables.
This tutorial explains how to calculate the Mahalanobis distance in Python.
<h3>Example: Mahalanobis Distance in Python</h3>
Use the following steps to calculate the Mahalanobis distance for every observation in a dataset in Python.
<b>Step 1: Create the dataset.</b>
First, we’ll create a dataset that displays the exam score of 20 students along with the number of hours they spent studying, the number of prep exams they took, and their current grade in the course:
<b>import numpy as np
import pandas as pd 
import scipy as stats
data = {'score': [91, 93, 72, 87, 86, 73, 68, 87, 78, 99, 95, 76, 84, 96, 76, 80, 83, 84, 73, 74],
        'hours': [16, 6, 3, 1, 2, 3, 2, 5, 2, 5, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4],
        'prep': [3, 4, 0, 3, 4, 0, 1, 2, 1, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2],
        'grade': [70, 88, 80, 83, 88, 84, 78, 94, 90, 93, 89, 82, 95, 94, 81, 93, 93, 90, 89, 89]
        }
df = pd.DataFrame(data,columns=['score', 'hours', 'prep','grade'])
df.head()
 score hours prep grade
0   91    16    3    70
1   93     6    4    88
2   72     3    0    80
3   87     1    3    83
4   86     2    4    88
</b>
<b>Step 2: Calculate the Mahalanobis distance for each observation.</b>
Next, we will write a short function to calculate the Mahalanobis distance.
<b>#create function to calculate Mahalanobis distance
def mahalanobis(x=None, data=None, cov=None):
    x_mu = x - np.mean(data)
    if not cov:
        cov = np.cov(data.values.T)
    inv_covmat = np.linalg.inv(cov)
    left = np.dot(x_mu, inv_covmat)
    mahal = np.dot(left, x_mu.T)
    return mahal.diagonal()
#create new column in dataframe that contains Mahalanobis distance for each row
df['mahalanobis'] = mahalanobis(x=df, data=df[['score', 'hours', 'prep', 'grade']])
#display first five rows of dataframe
df.head()
 score hours prep grade mahalanobis
0   91    16    3    70   16.501963
1   93     6    4    88    2.639286
2   72     3    0    80    4.850797
3   87     1    3    83    5.201261
4   86     2    4    88    3.828734
</b>
<b>Step 3: Calculate the p-value for each Mahalanobis distance.</b>
We can see that some of the Mahalanobis distances are much larger than others. To determine if any of the distances are statistically significant, we need to calculate their p-values.
The p-value for each distance is calculated as the p-value that corresponds to the Chi-Square statistic of the Mahalanobis distance with k-1 degrees of freedom, where k = number of variables. So, in this case we’ll use a degrees of freedom of 4-1 = 3.
<b>from scipy.stats import chi2
#calculate p-value for each mahalanobis distance 
df['p'] = 1 - chi2.cdf(df['mahalanobis'], 3)
#display p-values for first five rows in dataframe
df.head()
 score hours prep grade mahalanobis         p
0   91    16    3    70   16.501963  0.000895
1   93     6    4    88    2.639286  0.450644
2   72     3    0    80    4.850797  0.183054
3   87     1    3    83    5.201261  0.157639
4   86     2    4    88    3.828734  0.280562
</b>
Typically a p-value that is <b>less than .001 </b>is considered to be an outlier. We can see that the first observation is an outlier in the dataset because it has a p-value less than .001.
Depending on the context of the problem, you may decide to remove this observation from the dataset since it’s an outlier and could affect the results of the analysis.
<h2><span class="orange">How to Calculate Mahalanobis Distance in R</span></h2>
The <b>Mahalanobis distance </b>is the distance between two points in a multivariate space.
It is often used to find outliers in statistical analyses that involve several variables.
This tutorial explains how to calculate the Mahalanobis distance in R.
<h2>Example: Mahalanobis Distance in R</h2>
Use the following steps to calculate the Mahalanobis distance for every  observation  in a dataset in R.
<b>Step 1: Create the dataset.</b>
First, we’ll create a dataset that displays the exam score of 20 students along with the number of hours they spent studying, the number of prep exams they took, and their current grade in the course:
<b>#create data
df = data.frame(score = c(91, 93, 72, 87, 86, 73, 68, 87, 78, 99, 95, 76, 84, 96, 76, 80, 83, 84, 73, 74),
        hours = c(16, 6, 3, 1, 2, 3, 2, 5, 2, 5, 2, 3, 4, 3, 3, 3, 4, 3, 4, 4),
        prep = c(3, 4, 0, 3, 4, 0, 1, 2, 1, 2, 3, 3, 3, 2, 2, 2, 3, 3, 2, 2),
        grade = c(70, 88, 80, 83, 88, 84, 78, 94, 90, 93, 89, 82, 95, 94, 81, 93, 93, 90, 89, 89))
#view first six rows of data
head(df)
  score hours prep grade
1    91    16    3    70
2    93     6    4    88
3    72     3    0    80
4    87     1    3    83
5    86     2    4    88
6    73     3    0    84
</b>
<b>Step 2: Calculate the Mahalanobis distance for each observation.</b>
Next, we’ll use the built-in  mahalanobis()  function in R to calculate the Mahalanobis distance for each observation, which uses the following syntax:
<b>mahalanobis(x, center, cov)</b>
where:
<b>x: </b>matrix of data
<b>center: </b>mean vector of the distribution
<b>cov: </b>covariance matrix of the distribution
The following code shows how to implement this function for our dataset:
<b>#calculate Mahalanobis distance for each observation
mahalanobis(df, colMeans(df), cov(df))
 [1] 16.5019630  2.6392864  4.8507973  5.2012612  3.8287341  4.0905633
 [7]  4.2836303  2.4198736  1.6519576  5.6578253  3.9658770  2.9350178
[13]  2.8102109  4.3682945  1.5610165  1.4595069  2.0245748  0.7502536
[19]  2.7351292  2.2642268
</b>
<b>Step 3: Calculate the p-value for each Mahalanobis distance.</b>
We can see that some of the Mahalanobis distances are much larger than others.
To determine if any of the distances are statistically significant, we need to calculate their  p-values .
The p-value for each distance is calculated as the p-value that corresponds to the Chi-Square statistic of the Mahalanobis distance with k-1 degrees of freedom, where k = number of variables.
So, in this case we’ll use a degrees of freedom of 4-1 = 3.
<b>#create new column in data frame to hold Mahalanobis distances
df$mahal &lt;- mahalanobis(df, colMeans(df), cov(df))
#create new column in data frame to hold p-value for each Mahalanobis distance
df$p &lt;- pchisq(df$mahal, df=3, lower.tail=FALSE)
#view data frame
df
   score hours prep grade      mahal            p
1     91    16    3    70 16.5019630 0.0008945642
2     93     6    4    88  2.6392864 0.4506437265
3     72     3    0    80  4.8507973 0.1830542407
4     87     1    3    83  5.2012612 0.1576392526
5     86     2    4    88  3.8287341 0.2805615121
6     73     3    0    84  4.0905633 0.2518495222
7     68     2    1    78  4.2836303 0.2324211504
8     87     5    2    94  2.4198736 0.4899458807
9     78     2    1    90  1.6519576 0.6476670033
10    99     5    2    93  5.6578253 0.1294978092
11    95     2    3    89  3.9658770 0.2651724541
12    76     3    3    82  2.9350178 0.4017530495
13    84     4    3    95  2.8102109 0.4218217836
14    96     3    2    94  4.3682945 0.2243432904
15    76     3    2    81  1.5610165 0.6682610031
16    80     3    2    93  1.4595069 0.6916471506
17    83     4    3    93  2.0245748 0.5673218169
18    84     3    3    90  0.7502536 0.8613248635
19    73     4    2    89  2.7351292 0.4342904353
20    74     4    2    89  2.2642268 0.5194087143
</b>
Typically a p-value that is <b>less than .001 </b>is considered to be an outlier.
We can see that the first observation is an outlier in the dataset because it has a p-value less than .001.
Depending on the context of the problem, you may decide to remove this observation from the dataset since it’s an outlier and could affect the results of the analysis.
<b>Related: </b> How to Perform Multivariate Normality Tests in R 
<h2><span class="orange">How to Calculate Mahalanobis Distance in SPSS</span></h2>
The <b>Mahalanobis distance </b>is the distance between two points in a multivariate space. It’s often used to find outliers in statistical analyses that involve several variables.
This tutorial explains how to calculate the Mahalanobis distance in SPSS.
<h3>Example: Mahalanobis Distance in SPSS</h3>
Suppose we have the following dataset that displays the exam score of 20 students along with the number of hours they spent studying, the number of prep exams they took, and their current grade in the course:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal1.png">
We can use the following steps to calculate the Mahalanobis distance for each observation in the dataset to determine if there are any multivariate outliers.
<b>Step 1: Select the linear regression option.</b>
Click the <b>Analyze </b>tab, then <b>Regression</b>, then <b>Linear</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal2.png">
<b>Step 2: Select the Mahalanobis option.</b>
Drag the response variable <em>score </em>into the box labelled Dependent. Drag the other three predictor variables into the box labelled Independent(s). Then click the <b>Save </b>button. In the new window that pops up, make sure the box next to <b>Mahalanobis </b>is checked. Then click <b>Continue</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal3.png">
Once you click <b>OK</b>, the Mahalanobis distance for each observation in the dataset will appear in a new column titled <b>MAH_1</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal4.png">
We can see that some of the distances are much larger than others. To determine if any of the distances are statistically significant, we need to calculate their p-values.
<b>Step 3: Calculate the p-values of each Mahalanobis distance.</b>
Click the <b>Transform </b>tab, then <b>Compute Variable</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal5.png">
In the <b>Target Variable </b>box, choose a new name for the variable you’re creating. We chose “pvalue.” In the <b>Numeric Expression </b>box, type the following:
<b>1 – CDF.CHISQ(MAH_1, 3)</b>
Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal6.png">
This will produce a p-value that corresponds to the Chi-Square value with 3 degrees of freedom. We use <b>3 </b>degrees of freedom because there are 3 predictor variables in our regression model.
<b>Step 4: Interpret the p-values.</b>
Once you click <b>OK</b>, the p-value for each Mahalanobis distance will be displayed in a new column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal7.png">
By default, SPSS only displays the p-values to two decimal places. You can increase the number of decimal places by clicking <b>Variable View </b>at the bottom of SPSS and increasing the number in the <b>Decimals </b>column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal8.png">
Once you return to the <b>Data View</b>, you can see each p-value shown to five decimal places. Any p-value that is <b>less than .001 </b>is considered to be an outlier.
We can see that the first observation is the only outlier in the dataset because it has a p-value less than .001:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/mahal9.png">
<h3>How to Handle Outliers</h3>
If an outlier is present in your data, you have a couple options:
<b>1. Make sure the outlier is not the result of a data entry error.</b>
Sometimes an individual simply enters the wrong data value when recording data. If an outlier is present, first verify that the data value was entered correctly and that it wasn’t an error.
<b>2. Remove the outlier.</b>
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<h2><span class="orange">How to Use make.names Function in R (With Examples)</span></h2>
You can use the <b>make.names </b>function in R to create syntactically valid names out of character vectors.
This function uses the following basic syntax:
<b>make.names(names, unique = FALSE)
</b>
where:
<b>names</b>: Character vector to be coerced to syntactically valid names.
<b>unique</b>: Whether or not to create unique names. Default is FALSE.
The following examples show how to use this function in different scenarios.
<h3>Example 1: Create Valid Names for Vector</h3>
Suppose we have the following vector of numeric values:
<b>#create vector of numeric values
numeric_values &lt;- c(1, 1, 4, 7, 8)
#create syntactically valid names from numeric values
make.names(numeric_values)
[1] "X1" "X1" "X4" "X7" "X8"
</b>
R defines “valid names” as names that start with a character or dot.
Thus, to convert each of the numeric values in the vector to a valid name, R simply adds an “X” in front of each value.
Note that two of the names (“X1”) are the exact same.
To force the names to be unique, we can specify <b>unique=TRUE</b>:
<b>#create vector of numeric values
numeric_values &lt;- c(1, 1, 4, 7, 8)
#create syntactically valid names from numeric values
make.names(numeric_values, unique=TRUE)
[1] "X1"   "X1.1" "X4"   "X7"   "X8"
</b>
Notice that each name is now unique.
<h3>Example 2: Create Valid Names for Matrix</h3>
Suppose we have the following matrix in R:
<b>#create matrix
mat &lt;- matrix(c(1, 2, 3, 7, 2, 4, 4, 6, 0, 1), ncol=2)
#view matrix
mat
     [,1] [,2]
[1,]    1    4
[2,]    2    4
[3,]    3    6
[4,]    7    0
[5,]    2    1
#view column names of matrix
colnames(mat)
NULL</b>
Notice that the matrix currently doesn’t have any column names.
However, we can use the <b>make.names()</b> function to quickly create column names:
<b>#create column names for matrix
colnames(mat) &lt;- make.names(1:ncol(mat))
#view updated matrix
mat
     X1 X2
[1,]  1  4
[2,]  2  4
[3,]  3  6
[4,]  7  0
[5,]  2  1</b>
Notice that the matrix now has “X1” and “X2” as the column names.
If we’d like, we can now extract values from a specific column in the matrix using the column name:
<b>#view values in "X1" column of matrix
mat[, 'X1']
[1] 1 2 3 7 2
</b>
Also note that you can type the following into R to read the complete documentation on how to create syntactically valid names:
<b>?make.names</b>
<h2><span class="orange">What is Mallows’ Cp? (Defintion & Example)</span></h2>
<b>Mallows’ Cp</b> is a metric that is used to pick the best  regression model  among several different models.
It is calculated as:
Cp = RSS<sub>p</sub>/S<sup>2</sup> – N + 2(P+1)
where:
<b>RSS<sub>p</sub></b>: The residual sum of squares for a model with <em>p</em> predictor variables
<b>S<sup>2</sup></b>: The residual mean square for the model (estimated by MSE)
<b>N:</b> The sample size
<b>P:</b> The number of predictor variables
Mallows’ Cp is used when we have several potential predictor variables that we’d like to use in a regression model and we’d like to identify the best model that uses a subset of these predictor variables.
We can identify the “best” regression model by identifying the model with the lowest Cp value that is less than P+1, where P is the number of predictor variables in the model.
The following example shows how to use Mallows’ Cp to pick the best regression model among several potential models.
<h3>Example: Using Mallows’ Cp to Pick the Best Model</h3>
Suppose a professor would like to use hours studied, prep exams taken, and current GPA as predictor variables in a regression model to predict the score that a student will receive on the final exam.
He fits seven different regression models and calculates the value for Mallows’ Cp for each model:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/mallow1.png">
If the value of Mallows’ Cp is less than the number of coefficients in the model (P+1) then the model is said to be unbiased.
We can see that there are two models that are unbiased:
The model with Hours and GPA as the predictor variables (Mallows’ Cp = 2.9, P+1 = 3)
The model with Prep Exams and GPA as the predictor variables (Mallows’ Cp = 2.7, P+1 = 3)
Among these two models, the model that uses Prep Exams and GPA as the predictor variables has the lowest value for Mallows’ Cp, which tells us that it’s the best model that leads to the least amount of bias.
<h3>Notes on Mallows’ Cp</h3>
Here are few things to keep in mind with regards to Mallows’ Cp:
Models that have a Mallows’ Cp value near P+1 are said to have low bias.
If every potential model has a high value for Mallows’ Cp, this is an indication that some important predictor variables are likely missing from each model.
If several potential models have low values for Mallow’s Cp, choose the model with the lowest value as the best model to use.
Also keep in mind that Mallows’ Cp is only one way to measure the quality of fit of a regression model. 
Another commonly used metric is adjusted R-squared, which tells us the proportion of variance in the  response variable  that can be explained by the predictor variables in the model, adjusted for the number of predictor variables used.
When deciding which regression model is best among a list of several different models, it’s a good idea to look at both Mallows’ Cp and adjusted R-squared.
<h2><span class="orange">How to Calculate Manhattan Distance in Excel</span></h2>
The <b>Manhattan distance</b> between two vectors, <em>A</em> and <em>B</em>, is calculated as:
Σ|A<sub>i</sub> – B<sub>i</sub>|
where <em>i</em> is the i<sup>th</sup> element in each vector.
This distance is used to measure the dissimilarity between two vectors and is commonly used in many  machine learning algorithms .
The following example shows how to calculate the Manhattan distance between two vectors in Excel.
<h3>Example: Calculating Manhattan Distance in Excel</h3>
Suppose we have the following two vectors, A and B, in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/manhattanExcel1.png">
To calculate the Manhattan distance between these two vectors, we need to first use the <b>ABS()</b> function to calculate the absolute difference between each corresponding element in the vectors:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/manhattanExcel2.png">
Next, we need to use the <b>SUM()</b> function to sum each of the absolute differences:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/manhattanExcel3.png">
The Manhattan distance between the two vectors turns out to be <b>51</b>.
<h2><span class="orange">How to Calculate Manhattan Distance in R (With Examples)</span></h2>
The <b>Manhattan distance</b> between two vectors, <em>A</em> and <em>B</em>, is calculated as:
Σ|a<sub>i</sub> – b<sub>i</sub>|
where <em>i</em> is the i<sup>th</sup> element in each vector.
This distance is used to measure the dissimilarity between any two vectors and is commonly used in many different  machine learning algorithms .
This tutorial provides a couple examples of how to calculate Manhattan distance in R.
<h3>Example 1: Manhattan Distance Between Two Vectors</h3>
The following code shows how to create a custom function to calculate the Manhattan distance between two vectors in R:
<b>#create function to calculate Manhattan distance
manhattan_dist &lt;- function(a, b){
     dist &lt;- abs(a-b)
     dist &lt;- sum(dist)
     return(dist)
}
#define two vectors
a &lt;- c(2, 4, 4, 6)
b &lt;- c(5, 5, 7, 8)
#calculate Manhattan distance between vectors
manhattan_dist(a, b)
[1] 9</b>
The Manhattan distance between these two vectors turns out to be <b>9</b>.
We can confirm this is correct by quickly calculating the Manhattan distance by hand:
Σ|a<sub>i</sub> – b<sub>i</sub>| = |2-5| + |4-5| + |4-7| + |6-8| = 3 + 1 + 3 + 2 = <b>9</b>.
<h3>Example 2: Manhattan Distance Between Vectors in a Matrix</h3>
To calculate the Manhattan distance between several vectors in a matrix, we can use the built-in <b>dist()</b> function in R:
<b>#create four vectors
a &lt;- c(2, 4, 4, 6)
b &lt;- c(5, 5, 7, 8)
c &lt;- c(9, 9, 9, 8)
d &lt;- c(1, 2, 3, 3)
#bind vectors into one matrix
mat &lt;- rbind(a, b, c, d)
#calculate Manhattan distance between each vector in the matrix
dist(mat, method = "manhattan")
   a  b  c
b  9      
c 19 10   
d  7 16 26
</b>
The way to interpret this output is as follows:
The Manhattan distance between vector <em>a</em> and <em>b</em> is <b>9</b>.
The Manhattan distance between vector <em>a</em> and <em>c</em> is <b>19</b>.
The Manhattan distance between vector <em>a</em> and <em>d</em> is <b>7</b>.
The Manhattan distance between vector <em>b</em> and <em>c</em> is <b>10</b>.
The Manhattan distance between vector <em>b</em> and <em>d</em> is <b>16</b>.
The Manhattan distance between vector <em>c</em> and <em>d</em> is <b>26</b>.
Note that each vector in the matrix should be the same length.
<h2><span class="orange">How to Calculate Manhattan Distance in Python (With Examples)</span></h2>
The <b>Manhattan distance</b> between two vectors, <em>A</em> and <em>B</em>, is calculated as:
Σ|A<sub>i</sub> – B<sub>i</sub>|
where <em>i</em> is the i<sup>th</sup> element in each vector.
This distance is used to measure the dissimilarity between two vectors and is commonly used in many  machine learning algorithms .
This tutorial shows two ways to calculate the Manhattan distance between two vectors in Python.
<h3>Method 1: Write a Custom Function</h3>
The following code shows how to create a custom function to calculate the Manhattan distance between two vectors in Python:
<b>from math import sqrt
#create function to calculate Manhattan distance 
def manhattan(a, b):
    return sum(abs(val1-val2) for val1, val2 in zip(a,b))
 
#define vectors
A = [2, 4, 4, 6]
B = [5, 5, 7, 8]
#calculate Manhattan distance between vectors
manhattan(A, B)
9</b>
The Manhattan distance between these two vectors turns out to be <b>9</b>.
We can confirm this is correct by quickly calculating the Manhattan distance by hand:
Σ|A<sub>i</sub> – B<sub>i</sub>| = |2-5| + |4-5| + |4-7| + |6-8| = 3 + 1 + 3 + 2 = <b>9</b>.
<h3>Method 2: Use the cityblock() function</h3>
Another way to calculate the Manhattan distance between two vectors is to use the  cityblock()  function from the SciPy package:
<b>from scipy.spatial.distance import cityblock
#define vectors
A = [2, 4, 4, 6]
B = [5, 5, 7, 8]
#calculate Manhattan distance between vectors
cityblock(A, B)
9</b>
Once again the Manhattan distance between these two vectors turns out to be <b>9</b>.
Note that we can also use this function to find the Manhattan distance between two columns in a pandas DataFrame:
<b>from scipy.spatial.distance import cityblock
import pandas as pd
#define DataFrame
df = pd.DataFrame({'A': [2, 4, 4, 6],   'B': [5, 5, 7, 8],   'C': [9, 12, 12, 13]})
#calculate Manhattan distance between columns A and B 
cityblock(df.A, df.B)
9</b>
<h2><span class="orange">What is a Manipulated Variable? (Definition & Example)</span></h2>
An <b>experiment</b> is a controlled scientific study. In statistics, we often conduct experiments to understand how changing one variable affects another variable.
A <b>manipulated variable</b> is a variable that we change or “manipulate” to see how that change affects some other variable. A manipulated variable is also sometimes called an  independent variable .
A <b>response variable</b> is the variable that changes as a result of the manipulated variable being changed. A response variable is sometimes called a  dependent variable  because its value often depends on the value of the manipulated variable.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/manipulated1.png">
Often in experiments there are also <b>controlled variables</b>, which are variables that are intentionally kept constant.
The goal of an experiment is to keep all variables constant <em>except</em> for the manipulated variable so that we can attribute any change in the response variable to the changes made in the manipulated variable.
Let’s check out a couple examples of different experiments to gain a better understanding of manipulated variables.
<h3>Example 1: Free-Throw Shooting</h3>
Suppose a basketball coach wants to conduct an experiment to determine if three different shooting techniques affect the free-throw percentage of his players.
He divides his team into three groups and has each group use a different technique to shoot 100 free-throws. He then records the average free-throw percentage for each group.
In this experiment, we would have the following variables:
<b>Manipulated variable:</b> The shooting technique. This is the variable that we manipulate to see how it affects free-throw percentage.
<b>Response variable:</b> The free-throw percentage. This is the variable that changes as a result of the manipulated variable being changed.
<b>Controlled variables:</b> We would want to make sure that each of the three groups shoot free-throws under the same conditions. So, variables that we might control include (1) gym lighting, (2) time of day, and (3) gym temperature.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/manipulated2.png">
<h3>Example 2: Exam Scores</h3>
Suppose a teacher wants to understand how the number of hours spent studying affects exam scores. She intentionally has groups of students study for 1, 2, 3, 4, or 5 hours prior to an exam. She then has each group take the same exam and records the average scores for each group.
In this experiment, we would have the following variables:
<b>Manipulated variable:</b> The number of hours spent studying. This is the variable that the teacher manipulates to see how it affects exam scores.
<b>Response variable:</b> The exam scores. This is the variable that changes as a result of the manipulated variable being changed.
<b>Controlled variables:</b> We would want to make sure that each of the groups of students take the exam under the same conditions. So, variables that we might control include (1) time available to complete exam, (2) number of breaks given during exam, and (3) time of day when exam is administered.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/manipulated3.png">
<h3>Additional Reading</h3>
 What is an Antecedent Variable? 
 What is an Extraneous Variable? 
 What is an Intervening Variable? 
 What is a Confounding Variable? 
<h2><span class="orange">How to Perform a Mann-Kendall Trend Test in Python</span></h2>
A <b>Mann-Kendall Trend Test </b>is used to determine whether or not a trend exists in time series data. It is a non-parametric test, meaning there is no underlying assumption made about the normality of the data.
The hypotheses for the test are as follows:
<b>H<sub>0</sub> (null hypothesis): </b>There is no trend present in the data.
<b>H<sub>A</sub> (alternative hypothesis): </b>A trend is present in the data. (This could be a positive or negative trend)
If the p-value of the test is lower than some significance level (common choices are 0.10, 0.05, and 0.01), then there is statistically significant evidence that a trend is present in the time series data.
This tutorial explains how to perform a Mann-Kendall Trend Test in Python.
<h3>Example: Mann-Kendall Trend Test in Python</h3>
To perform a Mann-Kendall Trend Test in Python, we will first install the  pymannkendall  package:
<b>pip install pymannkendall</b>
Once we’ve installed this package, we can perform the Mann-Kendall Trend Test on a set of time series data:
<b>#create dataset
data = [31, 29, 28, 28, 27, 26, 26, 27, 27, 27, 28, 29, 30, 29, 30, 29, 28]
#perform Mann-Kendall Trend Test
import pymannkendall as mk
mk.original_test(data)
Mann_Kendall_Test(trend='no trend', h=False, p=0.422586268671707,  z=0.80194241623, Tau=0.147058823529, s=20.0,  var_s=561.33333333, slope=0.0384615384615, intercept=27.692307692)</b>
Here is how to interpret the output of the test:
<b>trend</b>: This tells the trend. Possible output includes increasing, decreasing, or no trend.
<b>h:</b> True if trend is present. False if no trend is present.
<b>p:</b> The p-value of the test.
<b>z:</b> The normalize test statistic.
<b>Tau:</b> Kendall Tau.
<b>s:</b> Mann-Kendal’s score
<b>var_s:</b> Variance S
<b>slope:</b> Theil-Sen estimator/slope
<b>intercept:</b> Intercept of Kendall-Theil Robust Line
The main value we’re interested in is the p-value, which tells us whether or not there is a statistically significant trend in the data.
In this example, the p-value is <b>0.4226</b> which is not less than .05. Thus, there is no significant trend in the time series data.
Along with performing the Mann-Kendall Trend test, we can create a quick line plot using Matplotlib to visualize the actual time series data:
<b>import matplotlib.pyplot as plt
plt.plot(data)
</b>
timese<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/mannKendallPython1.png">
From the plot we can see that the data is a bit all over the place, which confirms that there is no clear trend in the data.
<h2><span class="orange">How to Perform a Mann-Kendall Trend Test in R</span></h2>
A <b>Mann-Kendall Trend Test </b>is used to determine whether or not a trend exists in time series data. It is a non-parametric test, meaning there is no underlying assumption made about the normality of the data.
The hypotheses for the test are as follows:
<b>H<sub>0</sub> (null hypothesis): </b>There is no trend present in the data.
<b>H<sub>A</sub> (alternative hypothesis): </b>A trend is present in the data. (This could be a positive or negative trend)
If the p-value of the test is lower than some significance level (common choices are 0.10, 0.05, and 0.01), then there is statistically significant evidence that a trend is present in the time series data.
This tutorial explains how to perform a Mann-Kendall Trend Test in R.
<h3>Example: Mann-Kendall Trend Test in R</h3>
To perform a Mann-Kendall Trend Test in R, we will use the <b>MannKendall()</b> function from the <b>Kendall</b> library, which uses the following syntax:
<b>MannKendall(x)</b>
where:
<b>x</b> = a vector of data, often a time series
To illustrate how to perform the test, we will use the built-in <b>PrecipGL </b>dataset from the <b>Kendall </b>library, which contains information about the annual precipitation for all of the Great Lakes from the years 1900 to 1986:
<b>#load Kendall library and PrecipGL dataset
library(Kendall)
data(PrecipGL)
#view dataset
PrecipGL
Time Series:
Start = 1900 
End = 1986 
Frequency = 1 
[1] 31.69 29.77 31.70 33.06 31.31 32.72 31.18 29.90 29.17 31.48 28.11 32.61
[13] 31.31 30.96 28.40 30.68 33.67 28.65 30.62 30.21 28.79 30.92 30.92 28.13
[25] 30.51 27.63 34.80 32.10 33.86 32.33 25.69 30.60 32.85 30.31 27.71 30.34
[37] 29.14 33.41 33.51 29.90 32.69 32.34 35.01 33.05 31.15 36.36 29.83 33.70
[49] 29.81 32.41 35.90 37.45 30.39 31.15 35.75 31.14 30.06 32.40 28.44 36.38
[61] 31.73 31.27 28.51 26.01 31.27 35.57 30.85 33.35 35.82 31.78 34.25 31.43
[73] 35.97 33.87 28.94 34.62 31.06 38.84 32.25 35.86 32.93 32.69 34.39 33.97
[85] 32.15 40.16 36.32
attr(,"title")
[1] Annual precipitation, 1900-1986, Entire Great Lakes
</b>
To see if there is a trend in the data, we can perform the Mann-Kendall Trend Test:
<b>#Perform the Mann-Kendall Trend Test
MannKendall(PrecipGL)
tau = 0.265, 2-sided pvalue =0.00029206
</b>
The test statistic is <b>0.265 </b>and the corresponding two-sided p-value is <b>0.00029206</b>. Because this p-value is less than 0.05, we will reject the null hypothesis of the test and conclude that a trend is present in the data.
To visualize the trend, we can create a time series plot of the annual precipitation by year and add a smooth line to depict the trend:
<b>#Plot the time series data
plot(PrecipGL)
#Add a smooth line to visualize the trend 
lines(lowess(time(PrecipGL),PrecipGL), col='blue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/mannKendall1.png">
Note that we can also perform a seasonally-adjusted Mann-Kendall Trend Test to account for any seasonality in the data by using the<b> SeasonalMannKendall(x)</b> command:
<b>#Perform a seasonally-adjusted Mann-Kendall Trend Test
SeasonalMannKendall(PrecipGL)
tau = 0.265, 2-sided pvalue =0.00028797
</b>
The test statistic is <b>0.265 </b>and the corresponding two-sided p-value is <b>0.00028797</b>. Once again this p-value is less than 0.05, so we will reject the null hypothesis of the test and conclude that a trend is present in the data.
<h2><span class="orange">Mann-Whitney U Table</span></h2>
The following tables provide critical values for two tailed Mann-Whitney U tests for various levels of alpha. 
For one-tailed tests, simply double the value of alpha and use the appropriate two-tailed table.
Read the Mann-Whitney U Test tutorial  here .
<b>Alpha = .01 (two-tailed)</b>
  
<b>Alpha = .05 (two-tailed)</b>
  
<b>Alpha = .10 (two-tailed)</b>
  
<h2><span class="orange">Mann-Whitney U Test Calculator</span></h2>
A Mann-Whitney U test is used to compare the differences between two independent samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30). It is considered to be the nonparametric equivalent to the two-sample independent t-test.
<b>Tutorial: </b>  Mann-Whitney U Test 
To conduct a Mann-Whitney U test for two independent samples, simply enter the data values below and click the “Calculate” button.
Sample 1 || Sample 2
<textarea id="x" name="x" rows="15" cols="4"></textarea><textarea id="y" name="y" rows="15" cols="4"></textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<div>
<div>
<div>
<script>
//create function that performs t test calculations
function calc() {
var x = document.getElementById('x').value.match(/\d+/g).map(Number);
var y = document.getElementById('y').value.match(/\d+/g).map(Number);
//calculate critical value U and p-value using script from https://gist.githubusercontent.com/gungorbudak/1c3989cc26b9567c6e50/raw/f8f2918a366798793f5fb7e92de0df9142feb737/mannwhitneyu.js
var t = mannwhitneyu.test(x, y, alternative = 'two-sided');
        var critical_value_U = t[Object.keys(t)[0]];
        var p_value_one_tail = (t[Object.keys(t)[1]]) / 2;
        var p_value_two_tail = t[Object.keys(t)[1]];
//--------------OUTPUT RESULTS-----------//
document.getElementById('U').innerHTML = "Test statistic U: " + critical_value_U.toFixed(1);
document.getElementById('p_value_one_tail').innerHTML = "One-tailed P value: " +  p_value_one_tail.toFixed(5);
document.getElementById('p_value_two_tail').innerHTML = "Two-tailed P value: " +  p_value_two_tail.toFixed(5);
}
</script>
<h2><span class="orange">How to Perform a Mann-Whitney U Test in SAS</span></h2>
A <b>Mann-Whitney U test</b> (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30).
It is considered to be the nonparametric equivalent to the  two sample t-test .
This tutorial explains how to perform a Mann-Whitney U test in SAS.
<h3>Example: Mann-Whitney U Test in SAS</h3>
Suppose researchers want to know if a fuel treatment leads to a change in the average mpg of a car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with the fuel treatment and 12 cars without it.
The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/mannSAS1.jpg"151">
Because the sample sizes are small and they suspect that the sample distributions are not  normally distributed , they decide to perform a Mann-Whitney U test to determine if there is a statistically significant difference in mpg between the two groups.
Use the following steps to perform a Mann-Whitney U test in SAS.
<b>Step 1: Create the Dataset</b>
First, we’ll use the following code to create the dataset in SAS:
<b>/*create dataset*/
data mpg_data;
    input group $ mpg;
    datalines;
treated 24
treated 25
treated 21
treated 22
treated 23
treated 18
treated 17
treated 28
treated 24
treated 27
treated 21
treated 23
untreated 20
untreated 23
untreated 21
untreated 25
untreated 18
untreated 17
untreated 18
untreated 24
untreated 20
untreated 24
untreated 23
untreated 19
;
run;
</b>
<b>Step 2: Perform the Mann Whitney U Test</b>
Next, we’ll use <b>proc npar1way</b> to perform the Mann Whitney U test:
<b>/*perform Mann Whitney U test*/
proc npar1way data=mpg_data wilcoxon;
    class group;
    var mpg;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/mann1.jpg"374">
From the Wilcoxon Two-Sample Test table, we see that the two-sided p-value of the test turns out to be <b>0.2114</b>.
Recall that the Mann Whitney U test uses the following null and alternative hypotheses:
<b>H<sub>0</sub></b>: The two populations have the same median.
<b>H<sub>A</sub></b>: The two populations have different medians.
Since the p-value of the test (<b>.2114</b>) is not less than .05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the mpg is different between the cars that receive fuel treatment and those that don’t.
SAS also provides boxplots to visualize the distribution of mpg values for each group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/mann2.jpg"575">
From the plot we can see the cars that received the fuel treatment tended to have higher mpg values, but from the results of the Mann Whitney U test we know that the differences between the two groups was not statistically significant.
<h2><span class="orange">How to Conduct a Mann-Whitney U Test in Python</span></h2>
A  <b>Mann-Whitney U test</b>  is used to compare the differences between two samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30).
It is considered to be the nonparametric equivalent to the  <b>two sample t-test</b> .
This tutorial explains how to conduct a Mann-Whitney U test in Python.
<h3>Example: Mann-Whitney U Test in Python</h3>
Researchers want to know if a fuel treatment leads to a change in the average mpg of a car. To test this, they measure the mpg of 12 cars with the fuel treatment and 12 cars without it.
Since the sample sizes are small and the researchers suspect that the sample distributions are not normally distributed, they decided to perform a Mann-Whitney U test to determine if there is a statistically significant difference in mpg between the two groups.
Perform the following steps to conduct a Mann-Whitney U test in Python.
<b>Step 1: Create the data.</b>
First, we’ll create two arrays to hold the mpg values for each group of cars:
<b>group1 = [20, 23, 21, 25, 18, 17, 18, 24, 20, 24, 23, 19]</b>
<b>group2 = [24, 25, 21, 22, 23, 18, 17, 28, 24, 27, 21, 23]</b>
<b>Step 2: Conduct a Mann-Whitney U Test.</b>
Next, we’ll use the  mannwhitneyu() function  from the scipy.stats library to conduct a Mann-Whitney U test, which uses the following syntax:
<b>mannwhitneyu(x, y, use_continuity=True, alternative=None)</b>
where:
<b>x: </b>an array of sample observations from group 1
<b>y: </b>an array of sample observations from group 2
<b>use_continuity: </b>whether a continuity correction (1/2) should be taken into account. Default is True.
<b>alternative: </b>defines the alternative hypothesis. Default is ‘None’ which computes a p-value half the size of the ‘two-sided’ p-value. Other options include ‘two-sided’, ‘less’, and ‘greater.’
Here’s how to use this function in our specific example:
<b>import scipy.stats as stats
#perform the Mann-Whitney U test
stats.mannwhitneyu(group1, group2, alternative='two-sided')
(statistic=50.0, pvalue=0.2114)
</b>
The test statistic is <b>50.0 </b>and the corresponding two-sided p-value is <b>0.2114</b>.
<b>Step 3: Interpret the results.</b>
In this example, the Mann-Whitney U Test uses the following null and alternative hypotheses:
<b>H<sub>0</sub>: </b>The mpg is equal between the two groups
<b>H<sub>A</sub>: </b>The mpg is <em>not </em>equal between the two groups
Since the p-value (<b>0.2114</b>) is not less than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the true mean mpg is different between the two groups.
<h2><span class="orange">How to Perform a Mann-Whitney U Test in R</span></h2>
A  Mann-Whitney U test  (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two independent samples when the sample distributions are not  normally distributed  and the sample sizes are small (n &lt;30).
It is considered to be the nonparametric equivalent to the  two-sample independent t-test .
This tutorial explains how to perform a Mann-Whitney U test in R.
<h2>Example: Mann-Whitney U Test in R</h2>
Researchers want to know whether or not a new drug is effective at preventing panic attacks. A total of 12 patients are randomly split into two groups of 6 and assigned to receive the new drug or the placebo. The patients then record how many panic attacks they have over the course of one month.
The results are shown below:
<div>
<table>
<thead><tr>
<th style="text-align: center;"><b>NEW DRUG</b></th>
<th style="text-align: center;"><b>PLACEBO</b></th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
Conduct a Mann-Whitney U Test to determine if there is a difference in the number of panic attacks for the patients in the placebo group compared to the new drug group. Use a .05 level of significance. 
There are two different ways to perform the Mann-Whitney U test, but both methods use the <b>wilcox.test() </b>function and both lead to the same outcome.
<b>Option 1: Enter the data as two separate vectors.</b>
<b>#create a vector for each group
new &lt;- c(3, 5, 1, 4, 3, 5)
placebo &lt;- c(4, 8, 6, 2, 1, 9)
#perform the Mann Whitney U test
wilcox.test(new, placebo)
#output
Wilcoxon rank sum test with continuity correction
data:  new and placebo
W = 13, p-value = 0.468
alternative hypothesis: true location shift is not equal to 0
</b>
<b>Option 2: Enter the data into one data frame with two columns. One column contains the number of panic attacks and the other contains the group.</b>
<b>#create a data frame with two columns, one for each group
drug_data &lt;- data.frame(attacks = c(3, 5, 1, 4, 3, 5, 4, 8, 6, 2, 1, 9),        drug_group = c(rep("old", 6), rep("placebo", 6)))
#perform the Mann Whitney U test
wilcox.test(attacks~drug_group, data = drug_data)
#output
data:  attacks by drug_group
W = 13, p-value = 0.468
alternative hypothesis: true location shift is not equal to 0
</b>
Notice that both methods lead to the exact same result. Namely, the test statistic is W = 13 and the corresponding p-value is <b>0.468</b>.
Since the p-value is greater than 0.05, we fail to reject the null hypothesis.
This means we do not have sufficient evidence to say that the number of panic attacks experienced by patients in the placebo group is different from the new drug group.
<h3>Notes on Using Wilcox.test()</h3>
By default, <b>wilcox.test() </b>assumes you want to run a two-tailed hypothesis test. However, you can specify <b>alternative=”less” </b>or <b>alternative=”more” </b>if you’d instead like to run a one-tailed test.
For example, suppose we’d like to test the hypothesis that the new drug leads to <em>less </em>panic attacks than the placebo. In this case, we could specify <b>alternative=”less” </b>in our wilcox.test() function:
<b>#create a vector for each group
new &lt;- c(3, 5, 1, 4, 3, 5)
placebo &lt;- c(4, 8, 6, 2, 1, 9)
#perform the Mann Whitney U test, specify alternative="less"
wilcox.test(new, placebo, alternative="less")
#output
Wilcoxon rank sum test with continuity correction
data:  new and placebo
W = 13, p-value = 0.234
alternative hypothesis: true location shift is less than 0
</b>
Notice that the test statistic is still W = 13, but the p-value is now <b>0.234</b>, which is exactly half as large as the previous p-value for the two-sided test.
Since the p-value is still greater than 0.05, we would still fail to reject the null hypothesis.
We do not have sufficient evidence to say that the number of panic attacks experienced by patients in the new drug group was less than that of the patients in the placebo group.
<h2><span class="orange">How to Perform a Mann-Whitney U Test in SPSS</span></h2>
A  <b>Mann-Whitney U test</b>  (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30). It is considered to be the nonparametric equivalent to the  <b>two sample t-test</b> .
This tutorial explains how to perform a Mann-Whitney U test in SPSS.
<h3>Example: Mann-Whitney U Test in SPSS</h3>
Researchers want to know if a fuel treatment leads to a change in the average mpg of a car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with the fuel treatment and 12 cars without it.
The following screenshot shows the mpg for each car along with the group they belong to (0 = no fuel treatment, 1 = fuel treatment):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/twoSampSPSS1.png">
Since the sample sizes are small and the researchers suspect that the sample distributions are not normally distributed, they decided to perform a Mann-Whitney U test to determine if there is a statistically significant difference in mpg between the two groups.
Perform the following steps to conduct a Mann-Whitney U test in SPSS.
<b>Step 1: Select the Mann-Whitney U Test option.</b>
Click the <b>Analyze </b>tab, then <b>Nonparametric Tests</b>, then <b>Legacy Dialogs</b>, then <b>2 Independent Samples</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/mannSPSS1.png">
<b>Step 2: Fill in the necessary values to perform the test.</b>
Drag <b>mpg </b>into the box labelled <b>Test Variable List</b> and <b>group</b> into the box labelled <b>Grouping Variable</b>. Click on <b>Define Groups </b>and  define Group 1 as the rows with value 0 and define Group 2 as the rows with value 1.
Make sure the box next to <b>Mann-Whitney U </b>is checked. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/mannSPSS2.png">
<b>Step 3: Interpret the results.</b>
Once you click <b>OK</b>, the results of the Mann-Whitney U Test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/mannSPSS3.png">
The most important numbers in the output are the <b>Z test statistic </b>and the <b>Asymptotic 2-tailed p-value</b>:
<li data-slot-rendered-dynamic="true"><b>Z test statistic: </b>-1.279
<li data-slot-rendered-dynamic="true"><b>p-value: </b>.201
Since the p-value is not less than 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the true mean mpg is different between the two groups.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our Mann-Whitney U test. Here is an example of how to do so:
A Mann-Whitney U test was conducted on 24 cars to determine if a new fuel treatment lead to a difference in mean miles per gallon. Each group had 12 cars.
 
Results showed that the mean mpg was not statistically significantly different between the two groups (z = -1.279, p = .2010) at a significance level of 0.05.
 
Based on these results, the new fuel treatment does not have a significant impact on the miles per gallon of cars.
<h2><span class="orange">How to Perform a Mann-Whitney U Test in Stata</span></h2>
A  <b>Mann-Whitney U test</b>  (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30). It is considered to be the nonparametric equivalent to the  <b>two sample t-test</b> .
This tutorial explains how to perform a Mann-Whitney U test in Stata.
<h2>Example: Mann-Whitney U Test in Stata</h2>
Researchers want to know if a fuel treatment leads to a change in the average mpg of a car. To test this, they conduct an experiment in which they measure the mpg of 12 cars with the fuel treatment and 12 cars without it.
Because the sample sizes are small and they suspect that the sample distributions are not normally distributed, they decided to perform a Mann-Whitney U test to determine if there is a statistically significant difference in mpg between the two groups.
Perform the following steps to conduct a Mann-Whitney U test in Stata.
<b>Step 1: Load the data.</b>
First, load the data by typing <b>use http://www.stata-press.com/data/r13/fuel2 </b>in the command box and clicking Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyStata1.png">
<b>Step 2: View the raw data.</b>
Before we perform the Mann Whitney U test, let’s first view the raw data. Along the top menu bar, go to <b>Data > Data Editor > Data Editor (Browse)</b>. The first column, <em>mpg</em>, shows the mpg for a given car while the second column, <em>treat</em>, shows whether or not the car received fuel treatment.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyStata2.png">
<b>Step 3: Perform a Mann-Whitney U test.</b>
Along the top menu bar, go to <b>Statistics > Summaries, tables, and tests > Nonparametric tests of hypotheses > Wilcoxon rank-sum test</b>.
For Variable, choose <em>mpg</em>. For Grouping variable, choose <em>treat</em>. Check both of the boxes below to display a p-value for an exact test and display an estimate of the probability that the variable for the first group is larger than the variable for the second group. Lastly, click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyStata3.png">
The results of the test will be displayed:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/mannWhitneyStata4.png">
The values that we are primarily interested in are <b>z </b>= -1.279 and <b>Prob > |z| </b>= 0.2010.
Since the p-value of the test (0.2010) is not smaller than our significance level of 0.05, we fail to reject the null hypothesis. We do not have sufficient evidence to say that the true mean mpg is different between the two groups.
<b>Step 5: Report the results.</b>
Lastly, we will report the results of our Mann-Whitney U test. Here is an example of how to do so:
A Mann-Whitney U test was conducted on 24 cars to determine if a new fuel treatment lead to a difference in mean miles per gallon. Each group had 12 cars.
 
Results showed that the mean mpg was not statistically significantly different between the two groups (z = -1.279, p = .2010) at a significance level of 0.05.
 
Based on these results, the new fuel treatment does not have a significant impact on the miles per gallon of cars.
<h2><span class="orange">Mann-Whitney U Test</span></h2>
A <b>Mann-Whitney U test</b> (sometimes called the Wilcoxon rank-sum test) is used to compare the differences between two independent samples when the sample distributions are not normally distributed and the sample sizes are small (n &lt;30).
It is considered to be the nonparametric equivalent to the  two-sample independent t-test .
Here are some examples of when you might use a Mann-Whitney U test:
You want to compare the salaries of five graduates from university A vs. the salaries of five graduates from university B. The salaries are not normally distributed.
You want to know if weight loss varies for two groups: 12 people using diet A and 10 people using diet B. The weight loss is not normally distributed. 
You want to know if the scores of 8 students in class A differ from those of 7 students in class B. The scores are not normally distributed.
In each example you have two groups that you want to compare, the sampling distributions are not normal, and the sample sizes are small.
Thus, a Mann-Whitney U test is appropriate as long as the following assumptions are met.
<h2>Assumptions of the Mann-Whitney U Test</h2>
Before you conduct a Mann-Whitney U test, you need to make sure the following four assumptions are met:
<b>Ordinal or Continuous:</b> The variable you’re analyzing is ordinal or continuous. Examples of ordinal variables include Likert items (e.g., a 5-point scale from “strongly disagree” to “strongly agree”). Examples of continuous variables include height (measured in inches), weight (measured in pounds), or exam scores (measured from 0 to 100).
<b>Independence:</b> All of the observations from both groups are independent of each other.
<b>Shape:</b> The shapes of the distributions for the two groups are roughly the same.
If these assumptions are met, then you can proceed to conduct a Mann-Whitney U test.
<h2>How to Conduct a Mann-Whitney U Test</h2>
To conduct a Mann-Whitney U test, we follow the standard  five-step hypothesis testing procedure :
<b>1. State the hypotheses. </b>
In most cases, a Mann-Whitney U test is performed as a two-sided test. The null and alternative hypotheses are written as:
<b>H<sub>0</sub>:</b> The two populations are equal
<b>H<sub>a</sub>:</b> The two populations are not equal
<b>2. Determine a significance level to use for the hypothesis.</b>
Decide on a significance level. Common choices are .01, .05, and .1.
<b>3. Find the test statistic.</b>
The test statistic is denoted as U and is the smaller of U<sub>1</sub> and U<sub>2</sub>, as defined below:
U<sub>1</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>1</sub>(n<sub>1</sub>+1)/2  – R<sub>1</sub>
U<sub>2</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>2</sub>(n<sub>2</sub>+1)/2  – R<sub>2</sub>
where n<sub>1</sub> and n<sub>2</sub> are the sample sizes for sample 1 and 2 respectively, and R<sub>1</sub> and R<sub>2</sub> are the sum of the ranks for sample 1 and 2 respectively.
<em>The examples below will show how to find this test statistic in detail.</em>
<b>4. Reject or fail to reject the null hypothesis.</b>
Using the test statistic, determine if you can reject or fail to reject the null hypothesis based on the significance level and critical value found in the  Mann-Whitney U Table .
<b>5. Interpret the results. </b>
Interpret the results of the test in the context of the question being asked.
<h2>Examples of Conducting a Mann-Whitney U Test</h2>
The follow examples show how to conduct a Mann-Whitney U test.
<h3>Example 1</h3>
We want to know whether or not a new drug is effective at preventing panic attacks. A total of 12 patients are randomly split into two groups of 6 and assigned to receive the new drug or the placebo. The patients then record how many panic attacks they have over the course of one month.
The results are shown below:
<table>
<thead><tr>
<th style="text-align: center;">NEW DRUG</th>
<th style="text-align: center;">PLACEBO</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
</tr>
<tr>
<td style="text-align: center;">4</td>
<td style="text-align: center;">2</td>
</tr>
<tr>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
</tr>
<tr>
<td style="text-align: center;">5</td>
<td style="text-align: center;">9</td>
</tr>
</tbody>
</table>
<em><b>Conduct a Mann-Whitney U test to see if there is a difference in the number of panic attacks for the patients in the placebo group compared to the new drug group. Use a .05 level of significance. </b></em>
<b>1. State the hypotheses. </b>
<b>H<sub>0</sub>:</b> The two populations are equal
<b>H<sub>a</sub>:</b> The two populations are not equal
<b>2. Determine a significance level to use for the hypothesis.</b>
The problem tells us that we are to use a .05 level of significance.
<b>3. Find the test statistic.</b>
Recall that test statistic is denoted as U and is the smaller of U<sub>1</sub> and U<sub>2</sub>, as defined below:
U<sub>1</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>1</sub>(n<sub>1</sub>+1)/2  – R<sub>1</sub>
U<sub>2</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>2</sub>(n<sub>2</sub>+1)/2  – R<sub>2</sub>
where n<sub>1</sub> and n<sub>2</sub> are the sample sizes for sample 1 and 2 respectively, and R<sub>1</sub> and R<sub>2</sub> are the sum of the ranks for sample 1 and 2 respectively.
In order to find R<sub>1</sub> and R<sub>2</sub>, we need to combine the observations from both groups and rank them in order from least to greatest:
<table>
<thead><tr>
<th style="text-align: center;">NEW DRUG</th>
<th style="text-align: center;">PLACEBO</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;"><b>3</b></td>
<td style="text-align: center;"><b>4</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>5</b></td>
<td style="text-align: center;"><b>8</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>1</b></td>
<td style="text-align: center;"><b>6</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>4</b></td>
<td style="text-align: center;"><b>2</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>3</b></td>
<td style="text-align: center;"><b>1</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>5</b></td>
<td style="text-align: center;"><b>9</b></td>
</tr>
</tbody>
</table>
<b>Total Sample: 1, 1, 2, 3, 3, 4, 4, 5, 5, 6, 8, 9</b>
<div>
<b>Ranks: 1.5, 1.5, 3, 4.5, 4.5, 6.5, 6.5, 8.5, 8.5, 10, 11, 12</b>
<b>R<sub>1</sub></b> = sum of ranks for sample 1 = 1.5+4.5+4.5+6.5+8.5+8.5 =  34
<b>R<sub>2</sub></b> = sum of ranks for sample 2 = 1.5+3+6.5+10+11+12 =  44
Next, we use our sample sizes n<sub>1</sub> and n<sub>2</sub> along with our sum of ranks R<sub>1</sub> and R<sub>2</sub> to find U<sub>1</sub> and U<sub>2</sub>.
U<sub>1</sub> = 6(6)  +  6(6+1)/2  – 34 = 23
U<sub>2</sub> = 6(6) +  6(6+1)/2  – 44 = 13
Our test statistics is the smaller of U<sub>1</sub> and U<sub>2</sub>, which happens to be U = 13.
<em>Note: We could also use the  Mann-Whitney U Test Calculator  to find that U = 13.</em>
<b>4. Reject or fail to reject the null hypothesis.</b>
Using n<sub>1</sub> = 6 and n<sub>2</sub> = 6 with a significance level of .05, the  Mann-Whitney U Table  tells us that the critical value is 5:
  
Since our test statistic (13) is greater than our critical value (5), we fail to reject the null hypothesis.
<b>5. Interpret the results. </b>
Since we failed to reject the null hypothesis, we do not have sufficient evidence to say that the number of panic attacks experienced by patients in the placebo group is different from the new drug group.
<h3>Example 2</h3>
We want to know whether or not studying for 30 minutes per day for one week helps students score better on a test. A total of 15 patients are randomly assigned to a study or no-study group. After one week, all of the students take the same test.
The test scores for the two groups are shown below:
<table>
<thead><tr>
<th style="text-align: center;">STUDY</th>
<th style="text-align: center;">NO-STUDY</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;">89</td>
<td style="text-align: center;">88</td>
</tr>
<tr>
<td style="text-align: center;">92</td>
<td style="text-align: center;">93</td>
</tr>
<tr>
<td style="text-align: center;">94</td>
<td style="text-align: center;">95</td>
</tr>
<tr>
<td style="text-align: center;">96</td>
<td style="text-align: center;">75</td>
</tr>
<tr>
<td style="text-align: center;">91</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: center;">99</td>
<td style="text-align: center;">80</td>
</tr>
<tr>
<td style="text-align: center;">84</td>
<td style="text-align: center;">81</td>
</tr>
<tr>
<td style="text-align: center;">90</td>
</tr>
</tbody>
</table>
<em><b>Conduct a Mann-Whitney U test to see if there is a difference in the test scores for the study vs. no-study group. Use a .01 level of significance. </b></em>
<b>1. State the hypotheses. </b>
<b>H<sub>0</sub>:</b> The two populations are equal
<b>H<sub>a</sub>:</b> The two populations are not equal
<b>2. Determine a significance level to use for the hypothesis.</b>
The problem tells us that we are to use a .01 level of significance.
<b>3. Find the test statistic.</b>
Recall that test statistic is denoted as U and is the smaller of U<sub>1</sub> and U<sub>2</sub>, as defined below:
U<sub>1</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>1</sub>(n<sub>1</sub>+1)/2  – R<sub>1</sub>
U<sub>2</sub> = n<sub>1</sub>n<sub>2</sub>  +  n<sub>2</sub>(n<sub>2</sub>+1)/2  – R<sub>2</sub>
where n<sub>1</sub> and n<sub>2</sub> are the sample sizes for sample 1 and 2 respectively, and R<sub>1</sub> and R<sub>2</sub> are the sum of the ranks for sample 1 and 2 respectively.
In order to find R<sub>1</sub> and R<sub>2</sub>, we need to combine the observations from both groups and rank them in order from least to greatest:
<table>
<thead><tr>
<th style="text-align: center;"><b>STUDY</b></th>
<th style="text-align: center;">NO-STUDY</th>
</tr></thead>
<tbody>
<tr>
<td style="text-align: center;"><b>89</b></td>
<td style="text-align: center;"><b>88</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>92</b></td>
<td style="text-align: center;"><b>93</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>94</b></td>
<td style="text-align: center;"><b>95</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>96</b></td>
<td style="text-align: center;"><b>75</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>91</b></td>
<td style="text-align: center;"><b>72</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>99</b></td>
<td style="text-align: center;"><b>80</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>84</b></td>
<td style="text-align: center;"><b>81</b></td>
</tr>
<tr>
<td style="text-align: center;"><b>90</b></td>
</tr>
</tbody>
</table>
<b>Total Sample: 72, 75, 80, 81, 84, 88, 89, 90, 91, 92, 93, 94, 95, 96, 99</b>
<b>Ranks: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15</b>
<b>R<sub>1</sub></b> = sum of ranks for sample 1 = 5+7+8+9+10+12+14+15 = 80
<b>R<sub>2</sub></b> = sum of ranks for sample 2 = 1+2+3+4+6+11+13 = 40
Next, we use our sample sizes n<sub>1</sub> and n<sub>2</sub> along with our sum of ranks R<sub>1</sub> and R<sub>2</sub> to find U<sub>1</sub> and U<sub>2</sub>.
U<sub>1</sub> = 8(7)  +  8(8+1)/2  – 80 = 12
U<sub>2</sub> = 8(7) +  7(7+1)/2  – 40 = 44
Our test statistics is the smaller of U<sub>1</sub> and U<sub>2</sub>, which happens to be U = 12.
<em>Note: We could also use the  Mann-Whitney U Test Calculator  to find that U = 12.</em>
<b>4. Reject or fail to reject the null hypothesis.</b>
Using n<sub>1</sub> = 8 and n<sub>2</sub> = 7 with a significance level of .01, the  Mann-Whitney U Table  tells us that the critical value is 6:
  
Since our test statistic (12) is greater than our critical value (6), we fail to reject the null hypothesis.
<b>5. Interpret the results. </b>
Since we failed to reject the null hypothesis, we do not have sufficient evidence to say that the test scores of the students who studied is different than the test scores of the students who did not study.
<h2>Additional Resources</h2>
 Mann-Whitney U Test Calculator 
 Mann-Whitney U Table 
 How to Perform a Mann-Whitney U Test in Excel 
 How to Perform a Mann-Whitney U Test in R 
 How to Perform a Mann-Whitney U Test in Python 
 How to Perform a Mann-Whitney U Test in SPSS 
 How to Perform a Mann-Whitney U Test in Stata 
<h2><span class="orange">The Complete Guide: How to Check MANOVA Assumptions</span></h2>
A <b>MANOVA</b> (multivariate analysis of variance) is used to analyze how one or more factor variables affects multiple response variables.
For example, we might use a MANOVA to analyze how level of education (High school degree, Associate’s degree, Bachelor’s degree, Master’s degree) affects both annual income and total student loan debt.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/manova.png">
<b>Related:</b>  The Differences Between ANOVA, ANCOVA, MANOVA, and MANCOVA 
Whenever we perform a MANOVA, we should check to make sure that the following assumptions are met:
<b>1. Multivariate Normality</b> – Response variables are multivariate normally distributed within each group of the factor variable(s).
<b>2. Independence</b> – Each observation is randomly and independently sampled from the population.
<b>3. Equal Variance</b> – The population covariance matrices of each group are equal.
<b>4. No Multivariate Outliers</b> – There are no extreme multivariate outliers.
In this post, we provide an explanation for each assumption along with how to determine if the assumption is met.
<h3>Assumption 1: Multivariate Normality</h3>
A MANOVA assumes that the response variables are multivariate normally distributed within each group of the factor variable.
If there are at least 20 observations for each combination of factor * response variable, then we can assume that the multivariate normality assumption is met.
If there are less than 20 observations for each combination of factor * response variable, we can create a scatterplot matrix to visualize the residuals and visually check whether this assumption is met.
Fortunately, it’s well-known that MANOVA is robust against departures from multivariate normality so small to moderate departures typically don’t cause any problems.
<h3>Assumption 2: Independence</h3>
A MANOVA assumes that each observation is randomly and independently sampled from the population.
As long as a  probability sampling method  (every member in a population has an equal probability of being selected to be in the sample) is used to collect the data, we can assume that each observation has been randomly and independently sampled.
Examples of probability sampling methods include:
Simple random sampling
Stratified random sampling
Cluster random sampling
Systematic random sampling
<h3>Assumption 3: Equal Variance</h3>
A MANOVA assumes that the population covariance matrices of each group are equal.
The most common way to check this assumption is to use Box’s M test. This test is known to be quite strict, so we usually use a significance level of .001 to determine whether or not the population covariance matrices are equal.
If the p-value for Box’s M test is greater than .001, we can assume that this assumption is met.
Fortunately, even if the p-value for the test is less than .001 a MANOVA tends to be robust against departures from this assumption.
In order for non-equal covariance matrices to be a problem, the differences between the covariance matrices needs to be quite extreme.
<h3>Assumption 4: No Multivariate Outliers</h3>
A MANOVA assumes that there are no extreme multivariate outliers present in the data that could significantly influence the results.
The most common way to check this assumption is to calculate the Mahalanobis distance for each observation, which represents the distance between two points in a multivariate space.
If the corresponding p-value for a Mahalanobis distance of any observation is less than .001, we typically declare that observation to be an extreme outlier.
Refer to the following tutorials to see how to calculate Mahalanobis distance in various statistical software:
 How to Calculate Mahalanobis Distance in R 
 How to Calculate Mahalanobis Distance in SPSS 
 How to Calculate Mahalanobis Distance in Python 
<h2><span class="orange">How to Conduct a MANOVA in R</span></h2>
To understand the MANOVA, it first helps to understand the ANOVA.
An  ANOVA  (analysis of variance) is used to determine whether or not there is a statistically significant difference between the means of three or more independent groups. 
For example, suppose we want to know whether or not studying technique has an impact on exam scores for a class of students. We randomly split the class into three groups. Each group uses a different studying technique for one month to prepare for an exam. At the end of the month, all of the students take the same exam.
To find out if studying technique impacts exam scores, we can conduct a one-way ANOVA, which will tell us if if there is a statistically significant difference between the mean scores of the three groups.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA1.jpg">
In an ANOVA, we have one response variable. However, in a <b>MANOVA</b> (multivariate analysis of variance) we have multiple response variables.
For example, suppose we want to know how level of education (i.e. high school, associates degree, bachelors degrees, masters degree, etc.) impacts both annual income and amount of student loan debt. In this case, we have one factor (level of education) and two response variables (annual income and student loan debt), so we could conduct a one-way MANOVA.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/NOVA4.jpg">
<b>Related:</b>  Understanding the Differences Between ANOVA, ANCOVA, MANOVA, and MANCOVA 
<h2>How to Conduct a MANOVA in R</h2>
In the following example, we’ll illustrate how to conduct a one-way MANOVA in R using the built-in dataset <b>iris</b>, which contains information about the length and width of different measurements of flowers for three different species (“setosa”, “virginica”, “versicolor”):
<b>#view first six rows of <em>iris </em>dataset
head(iris)
#  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
#1          5.1         3.5          1.4         0.2  setosa
#2          4.9         3.0          1.4         0.2  setosa
#3          4.7         3.2          1.3         0.2  setosa
#4          4.6         3.1          1.5         0.2  setosa
#5          5.0         3.6          1.4         0.2  setosa
#6          5.4         3.9          1.7         0.4  setosa
</b>
Suppose we want to know if species has any effect on sepal length and sepal width. Using <em>species</em> as the independent variable, and <em>sepal length</em> and <em>sepal width</em> as the response variables, we can conduct a one-way MANOVA using the <b>manova() </b>function in R.
The <b>manova()</b> function uses the following syntax:
<b>manova(cbind(rv1, rv2, …) ~ iv, data)</b>
where:
<b>rv1, rv2</b>: response variable 1, response variable 2, etc.
<b>iv</b>: independent variable
<b>data</b>: name of the data frame
In our example with the iris dataset, we can fit a MANOVA and view the results using the following syntax:
<b>#fit the MANOVA model
model &lt;- manova(cbind(Sepal.Length, Sepal.Width) ~ Species, data = iris)
#view the results
summary(model)
#           Df  Pillai approx F num Df den Df    Pr(>F)    
#Species     2 0.94531   65.878      4    294 &lt; 2.2e-16 ***
#Residuals 147                                             
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</b>
From the output we can see that the F-statistic is 65.878 and the corresponding p-value is extremely small. This indicates that there is a statistically significant difference in sepal measurements based on species. 
<b>Technical Note:</b> By default, manova() uses the <em>Pillai </em>test statistic. Since the distribution of this test statistic is complex, an approximate F value is also provided for easier interpretation.
 
In addition, it’s possible to specify “Roy”, “Hotelling-Lawley”, or “Wilks” as the test statistic to be used by using the following syntax: summary(model, test = ‘Wilks’)
To find out exactly how both <em>sepal length </em>and <em>sepal width </em>are affected by <em>species</em>, we can perform univariate ANOVAs using<b> summary.aov() </b>as shown in the following code:
<b>summary.aov(model)
# Response Sepal.Length :
#             Df Sum Sq Mean Sq F value    Pr(>F)    
#Species       2 63.212  31.606  119.26 &lt; 2.2e-16 ***
#Residuals   147 38.956   0.265                      
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
# Response Sepal.Width :
#             Df Sum Sq Mean Sq F value    Pr(>F)    
#Species       2 11.345  5.6725   49.16 &lt; 2.2e-16 ***
#Residuals   147 16.962  0.1154                      
#---
#Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
We can see from the output that the p-values for both univariate ANOVAs are extremely small (&lt;2.2e-16), which indicates that <em>species </em>has a statistically significant effect on both <em>sepal width </em>and <em>sepal length</em>.
<h2>Visualizing Group Means</h2>
It can also be helpful to visualize the group means for each level of our independent variable <em>species</em> to gain a better understanding of our results.
For example, we can use the <b>gplots </b>library and the <b>plotmeans() </b>function to visualize the mean <em>sepal length</em> by <em>species</em>:
<b>#load <em>gplots </em>library
library(gplots)
#visualize mean <em>sepal length</em> by <em>species</em>
plotmeans(iris$Sepal.Length ~ iris$Species)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/manova_R1.jpg">
From the plot we can see that the mean sepal length varies quite a bit by species. This matches the results from our MANOVA, which told us that there was a statistically significant difference in sepal measurements based on species.
We can also visualize the mean <em>sepal width </em>by <em>species</em>:
<b>plotmeans(iris$Sepal.Width ~ iris$Species)</b>
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/04/manova_R2.jpg">
View the full RDocumentation for the <b>manova()</b> function  here .
<h2><span class="orange">How to Perform a MANOVA in SPSS</span></h2>
A  one-way ANOVA  is used to determine whether or not different levels of an explanatory variable lead to statistically different results in some response variable.
For example, we might be interested in understanding whether or not three levels of education (Associate’s degree, Bachelor’s degree, Master’s degree) lead to statistically different annual incomes. In this case, we have one explanatory variable and one response variable.
<b>Explanatory variable:</b> level of education
<b>Response variable:</b> annual income
A <b>MANOVA</b> is an extension of the one-way ANOVA in which there is more than one response variable. For example, we might be interested in understanding whether or not level of education leads to different annual incomes <em>and </em>different amounts of student loan debt. In this case, we have one explanatory variable and two response variables:
<b>Explanatory variable:</b> level of education
<b>Response variables:</b> annual income, student loan debt
Because we have more than one response variable, it would be appropriate to use a MANOVA in this case.
In this tutorial, we’ll explain how to perform a MANOVA in SPSS.
<h3>Example: MANOVA in SPSS</h3>
To illustrate how to perform a MANOVA in SPSS, we’ll use the following dataset that contains the following three variables for 24 individuals:
<b>educ: </b>level of education (0 = Associate, 1 = Bachelor, 2 = Master)
<b>income: </b>annual income
<b>debt: </b>total student loan debt
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS1.png">
Use the following steps to perform a MANOVA in SPSS:
<b>Step 1: Perform a MANOVA.</b>
Click the <b>Analyze </b>tab, then <b>General Linear Model</b>, then <b>Multivariate</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS2.png">
In the new window that pops up, drag the variables <b>income </b>and <b>debt </b>into the box labelled Dependent Variables. Then drag the factor variable <b>education </b>into the box labelled Fixed Factors:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS3.png">
Next, click the button <b>Post Hoc</b>. Drag the factor <b>education </b>into the box labelled <b>Post Hoc Tests for</b>. Then check the box next to <b>Tukey</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS5.png">
Lastly, click <b>OK</b>.
<b>Step 2: Interpret the results.</b>
Once you click <b>OK</b>, the results of the MANOVA will appear. Here is how to interpret the output:
<b>Multivariate Tests</b>
This table tells you whether or not  level of education leads to statistically significant differences in annual income and total student debt. We will look at the numbers in the row titled <b>Wilks’ Lambda</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS6.png">
The overall F statistic is <b>6.138 </b>and the corresponding p-value is <b>.001</b>. Since this value is less than .05, this indicates that level of education does have a significant effect on annual income and total student debt.
<b>Tests of Between-Subjects Effects</b>
This table shows the individual p-values for both <b>income </b>and <b>debt</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS7.png">
The p-value for income is <b>.003 </b>and the p-value for debt is <b>.000</b>. Since both of these values are less than .05, it means that level of education has a statistically significant effect on both income and debt.
<b>Post Hoc Tests</b>
This table displays the Tukey post hoc comparisons for each level of education.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/manovaSPSS8.png">
From the table we can observe the following:
The amount of income for individuals with an Associate’s degree (education=0) is significantly different than the amount of income for individuals with a Master’s degree (education=1) | <b>p-value = .003</b>.
The amount of income for individuals with a Bachelor’s degree (education=1) is significantly different than the amount of income for individuals with a Master’s degree (education=2) | <b>p-value = .029</b>.
The amount of income for individuals with an Associate’s degree (education=0) is significantly different than the amount of income for individuals with a Bachelor’s degree (education=1) | <b>p-value = .018</b>.
The amount of income for individuals with an Associate’s degree (education=0) is significantly different than the amount of income for individuals with a Master’s degree (education=2) | <b>p-value = .000</b>.
<b>Further Reading: </b> The Differences Between ANOVA, ANCOVA, MANOVA, and MANCOVA 
<h2><span class="orange">How to Perform a MANOVA in Stata</span></h2>
A  one-way ANOVA  is used to determine whether or not different levels of an explanatory variable lead to statistically different results in some response variable.
For example, we might be interested in understanding whether or not three levels of education (Associate’s degree, Bachelor’s degree, Master’s degree) lead to statistically different annual incomes. In this case, we have one explanatory variable and one response variable.
<b>Explanatory variable:</b> level of education
<b>Response variable:</b> annual income
A <b>MANOVA</b> is an extension of the one-way ANOVA in which there is more than one response variable. For example, we might be interested in understanding whether or not level of education leads to different annual incomes <em>and </em>different amounts of student loan debt. In this case, we have one explanatory variable and two response variables:
<b>Explanatory variable:</b> level of education
<b>Response variables:</b> annual income, student loan debt
Because we have more than one response variable, it would be appropriate to use a MANOVA in this case.
Next, we’ll explain how to perform a MANOVA in Stata.
<h2>Example: MANOVA in Stata</h2>
To illustrate how to perform a MANOVA in Stata, we’ll use the following dataset that contains the following three variables for 24 individuals:
<b>educ: </b>level of education (0 = Associate, 1 = Bachelor, 2 = Master)
<b>income: </b>annual income
<b>debt: </b>total student loan debt
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/manovaStata2.png">
You can replicate this example by manually entering the data yourself by going to <b>Data > Data Editor > Data Editor (Edit) </b>along the top menu bar.
To perform the MANOVA using education as the explanatory variable and income and debt as the response variables, we can use the following command:
<b>manova income debt = educ</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/manovaStata3.png">
Stata produces four unique test statistics along with their corresponding p-values:
<b>Wilks’ lambda: </b>F-Statistic = 5.02, P-value = 0.0023.
<b>Pillai’s trace: </b>F-Statistic = 4.07, P-value = 0.0071.
<b>Lawley-Hotelling trace: </b>F-Statistic = 5.94, P-value = 0.0008.
<b>Roy’s largest root: </b>F-Statistic = 13.10, P-value = 0.0002.
<em>For an in-depth explanation of how each test statistic is calculated, refer to  this article  from the Penn State Eberly College of Science.</em>
The p-value for each test statistic is less than 0.05, so the null hypothesis will be rejected no matter which one you use. This means we have sufficient evidence to say that level of education leads to statistically significant differences in annual income and total student debt.
<b>Note on p-values: </b>The letter next to the p-value in the output table indicates how the F-statistic was calculated (e = exact calculation, a = approximate calculation, u = an upper bound).
<h2><span class="orange">How to Use the map() Function in R (With Examples)</span></h2>
The <b>map()</b> function from the <b>purrr</b> package in R can be used to apply some function to each element in a vector or list and return a list as a result.
This function uses the following basic syntax:
<b>map(.x, .f)</b>
where:
<b>.x</b>: A vector or list
<b>.f</b>: A function
The following examples show how to use this function in different scenarios.
<h2>Example 1: Use map() to Generate Random Variables</h2>
The following code shows how to use the <b>map()</b> function to generate three random variables that each contain five values that follow a standard normal distribution:
<b>library(purrr)
#define vector
data &lt;- 1:3
#apply rnorm() function to each value in vector
data %>%
  map(function(x) rnorm(5, x))
[[1]]
[1] 0.0556774 1.8053082 2.6489861 2.2640136 1.1062672
[[2]]
[1] 1.450175 1.123048 3.413677 3.055304 2.713801
[[3]]
[1] 2.936732 2.157129 3.693738 2.994391 2.567040
</b>
For each element in the original vector, the <b>map()</b> function applied the rnorm() function to generate five random values that come from a  standard normal distribution .
<h2>Example 2: Use map() to Transform Each Value in a Vector</h2>
The following code shows how to use the <b>map()</b> function to calculate the square of each value in a vector:
<b>library(purrr)
#define vector
data &lt;- c(2, 4, 10, 15, 20)
#calculate square of each value in the vector
data %>%
  map(function(x) x^2)
[[1]]
[1] 4
[[2]]
[1] 16
[[3]]
[1] 100
[[4]]
[1] 225
[[5]]
[1] 400</b>
For each element in the original vector, the <b>map()</b> function applied a function that calculated the square of each value.
<h2>Example 3: Use map() to Calculate Mean of Each Vector in List</h2>
The following code shows how to use the <b>map()</b> function to calculate the mean value of each vector in a list:
<b>library(purrr)
#define list of vectors
data &lt;- list(c(1, 2, 3),
             c(4, 5, 6),
             c(7, 8, NA))
#calculate mean value of each vector in list
data %>%
  map(mean, na.rm=TRUE)
[[1]]
[1] 2
[[2]]
[1] 5
[[3]]
[1] 7.5</b>
For each vector in the list, the <b>map()</b> function calculated the mean value.
From the output we can see:
The mean value of the first vector in the list is <b>2</b>.
The mean value of the second vector in the list is <b>5</b>.
The mean value of the third vector in the list is <b>7.5</b>.
<b>Note</b>: The argument <b>na.rm=TRUE</b> tells R to ignore NA values when calculating the mean.
<h2>Additional Resources</h2>
The following tutorials explain how to use other common functions in R:
 How to Use the tapply() Function in R 
 How to Use the dim() Function in R 
 How to Use the table() Function in R 
 How to Use sign() Function in R 
<h2><span class="orange">MAPE Calculator</span></h2>
The <b>mean absolute percentage error (MAPE)</b> is a metric that tells us how far apart our predicted values are from our observed values in a regression analysis, on average. It is calculated as:
<b>MAPE</b> = (1/n) * Σ(|O<sub>i</sub> – P<sub>i</sub>|/O<sub>i</sub> * 100 
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
To find the MAPE for a regression, simply enter a list of observed values and predicted values in the two boxes below, then click the “Calculate” button:
<b>Observed values:</b>
<textarea id="input_data_obs" name="x" rows="5" cols="40">34, 37, 44, 47, 48, 48, 46, 43, 32, 27, 26, 24</textarea>
<b>Predicted values:</b>
<textarea id="input_data_pred" name="x" rows="5" cols="40">37, 40, 46, 44, 46, 50, 45, 44, 34, 30, 22, 23</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<b>MAPE = 2.43242%</b>
<script>
function calc() {
var obs = document.getElementById('input_data_obs').value.split(',').map(Number);
var pred = document.getElementById('input_data_pred').value.split(',').map(Number);
//check that both lists are equal length
if (obs.length - pred.length == 0) {
document.getElementById('error_msg').innerHTML = '';
//calculate RMSE
let error = 0
for (let i = 0; i < obs.length; i++) {
error += Math.abs(obs[i] - pred[i]) / obs[i]
}
var RMSE = error / obs.length * 100;
document.getElementById('RMSE').innerHTML = RMSE.toFixed(5);
}
else {
 document.getElementById('RMSE').innerHTML = '';
 document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
