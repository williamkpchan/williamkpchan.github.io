<base target="_blank"><html><head><title>statologyContents 17</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="https://williamkpchan.github.io/lazyload.min.js"></script>
<script src='https://williamkpchan.github.io/mainscript.js'></script>
<script src="https://williamkpchan.github.io/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var topicEnd = "<br>";
  var bookid = "statologyContents 17"
  var markerName = "h2, h3"
</script>
<style>
body{width:70%;margin-left: 15%; font-size:20px;}
h1, h2 {color: gold;}
strong {color: orange;}
b {color: brown;}
img {max-width:60%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px;}
</style></head><body onkeypress="chkKey()"><center>
<h1>statologyContents 17</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a><br><br>
<div id="toc"></div></center><br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br></div>
<pre><br><br>
<h2><span class="orange">How to Use rbind in Python (Equivalent to R)</span></h2>
The <b>rbind</b> function in R, short for <em>row-bind</em>, can be used to combine data frames together by their rows.
We can use the  concat()  function from pandas to perform the equivalent function in Python:
<b>df3 = pd.concat([df1, df2])
</b>
The following examples shows how to use this function in practice.
<h3>Example 1: Use rbind in Python with Equal Columns</h3>
Suppose we have the following two pandas DataFrames:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [99, 91, 104, 88, 108]})
print(df1)
  team  points
0    A      99
1    B      91
2    C     104
3    D      88
4    E     108
df2 = pd.DataFrame({'assists': ['F', 'G', 'H', 'I', 'J'],    'rebounds': [91, 88, 85, 87, 95]})
print(df2)
  team  points
0    F      91
1    G      88
2    H      85
3    I      87
4    J      95
</b>
We can use the <b>concat()</b> function to quickly bind these two DataFrames together by their rows:
<b>#row-bind two DataFrames
df3 = pd.concat([df1, df2])
#view resulting DataFrame
df3
teampoints
0A99
1B91
2C104
3D88
4E108
0F91
1G88
2H85
3I87
4J95
</b>
Note that we can also use <b>reset_index()</b> to reset the index values of the new DataFrame:
<b>#row-bind two DataFrames and reset index values
df3 = pd.concat([df1, df2]).reset_index(drop=True)
#view resulting DataFrame
df3
teampoints
0A99
1B91
2C104
3D88
4E108
5F91
6G88
7H85
8I87
9J95</b>
<h3>Example 2: Use rbind in Python with Unequal Columns</h3>
We can also use the <b>concat()</b> function to row-bind two DataFrames together that have an unequal number of columns and any missing values will simply be filled with NaN:
<b>import pandas as pd
#define DataFrames
df1 = pd.DataFrame({'team': ['A', 'B', 'C', 'D', 'E'],    'points': [99, 91, 104, 88, 108]})
df2 = pd.DataFrame({'team': ['F', 'G', 'H', 'I', 'J'],    'points': [91, 88, 85, 87, 95],    'rebounds': [24, 27, 27, 30, 35]})
#row-bind two DataFrames
df3 = pd.concat([df1, df2]).reset_index(drop=True)
#view resulting DataFrame
df3
teampointsrebounds
0A99NaN
1B91NaN
2C104NaN
3D88NaN
4E108NaN
5F9124.0
6G8827.0
7H8527.0
8I8730.0
9J9535.0</b>
<h2><span class="orange">How to Use rbind in R (With Examples)</span></h2>
The <b>rbind</b> function in R, short for <em>row-bind</em>, can be used to combine vectors, matrices and data frames by rows. 
The following examples show how to use this function in practice.
<h3>Example 1: Rbind Vectors into a Matrix</h3>
The following code shows how to use rbind to row-bind two vectors into a single matrix:
<b>#create two vectors
a &lt;- c(1, 3, 3, 4, 5)
b &lt;- c(7, 7, 8, 3, 2)
#rbind the two vectors into a matrix
new_matrix &lt;- rbind(a, b)
#view matrix
new_matrix
  [,1] [,2] [,3] [,4] [,5]
a    1    3    3    4    5
b    7    7    8    3    2
</b>
<h3>Example 2: Rbind Vector to a Data Frame</h3>
The following code shows how to use rbind to row-bind a vector to an existing data frame:
<b>#create data frame
df &lt;- data.frame(a=c(1, 3, 3, 4, 5), b=c(7, 7, 8, 3, 2), c=c(3, 3, 6, 6, 8))
#define vector
d &lt;- c(11, 14, 16)
#rbind vector to data frame
df_new &lt;- rbind(df, d)
#view data frame
df_new
   a  b  c
1  1  7  3
2  3  7  3
3  3  8  6
4  4  3  6
5  5  2  8
6 11 14 16
</b>
<h3>Example 3: Rbind Multiple Vectors to a Data Frame</h3>
The following code shows how to use rbind to row-bind multiple vectors to an existing data frame:
<b>#create data frame
df &lt;- data.frame(a=c(1, 3, 3, 4, 5), b=c(7, 7, 8, 3, 2), c=c(3, 3, 6, 6, 8))
#define vectors
d &lt;- c(11, 14, 16)
e &lt;- c(34, 35, 36) 
#rbind vectors to data frame
df_new &lt;- rbind(df, d, e)
#view data frame
df_new
   a  b  c
1  1  7  3
2  3  7  3
3  3  8  6
4  4  3  6
5  5  2  8
6 11 14 16
7 34 35 36
</b>
<h3>Example 4: Rbind Two Data Frames</h3>
The following code shows how to use rbind to row-bind two data frames into one data frame:
<b>#create two data frames
df1 &lt;- data.frame(a=c(1, 3, 3, 4, 5),  b=c(7, 7, 8, 3, 2),  c=c(3, 3, 6, 6, 8))
df2 &lt;- data.frame(a=c(11, 14, 16, 17, 22),  b=c(34, 35, 36, 36, 40),  c=c(2, 2, 5, 7, 8))
#rbind two data frames into one data frame
df_new &lt;- rbind(df1, df2)
#view data frame
df_new
    a  b c
1   1  7 3
2   3  7 3
3   3  8 6
4   4  3 6
5   5  2 8
6  11 34 2
7  14 35 2
8  16 36 5
9  17 36 7
10 22 40 8
</b>
Note that R will throw an error in either of the following scenarios:
The data frames don’t have the same number of columns.
The data frames don’t have the same column names.
<b>Bonus:</b> If you want to bind together vectors, matrices, or data frames by columns, you can used the  cbind  function instead.
<h2><span class="orange">How to Use rbindlist in R to Make One Data Table from Many</span></h2>
The <b>rbindlist()</b> function in R can be used to create one data.table from a list of many data.table or data.frame objects.
This function uses the following basic syntax:
<b>rbindlist(l, use.names="check", fill=FALSE, idcol=NULL)
</b>
where:
<b>l</b>: List containing data.table, data.frame, or list objects.
<b>use.names</b>: TRUE binds by column names. FALSE binds by position.
<b>fill</b>: TRUE fills missing values with NA.
<b>idcol</b>: Creates columnshowing which list item those rows came from.
The following example shows how to use this function in practice.
<h3>Example: Use rbindlist to Make One Data Table</h3>
Suppose we have the following list of data.table and data.frame objects in R:
<b>library(data.table)
#create data frames and data tables
data1 &lt;- data.table(team=c('A', 'B', 'C'),    points=c(22, 27, 38))
data2 &lt;- data.table(team=c('D', 'E', 'F'),    points=c(22, 14, 20))
data3 &lt;- data.frame(team=c('G', 'H', 'I'),    points=c(11, 15, 18))
#view data frames and data tables
print(data1)
print(data2)
print(data3)
   team points
1:    A     22
2:    B     27
3:    C     38
   team points
1:    D     22
2:    E     14
3:    F     20
  team points
1    G     11
2    H     15
3    I     18</b>
We can use the following <b>rbindlist()</b> function to bind together the list of data.table and data.frame objects into one data.table:
<b>#define list of objects to bind together
data_list &lt;- list(data1, data2, data3)
#bind together list of objects
big_data &lt;- rbindlist(data_list)
#view result
big_data
   team points
1:    A     22
2:    B     27
3:    C     38
4:    D     22
5:    E     14
6:    F     20
7:    G     11
8:    H     15
9:    I     18</b>
The result is a data.table object with nine rows that are composed of the rows from the list of objects we provided.
We can also use the <b>class()</b> function to verify that the result is indeed a data.table object:
<b>#view class of resulting object
class(big_data)
[1] "data.table" "data.frame"
</b>
We can see that the result is indeed a data.table object.
<h3>The Benefit of Using rbindlist</h3>
The alternative to using <b>rbindlist</b> would be to use<b> do.call</b> with the  rbind  function in base R:
<b>#use rbind to bind together list of objects
do.call("rbind", data_list)
   team points
1:    A     22
2:    B     27
3:    C     38
4:    D     22
5:    E     14
6:    F     20
7:    G     11
8:    H     15
9:    I     18
</b>
This code produces the same result but it turns out that <b>rbindlist</b> is significantly faster, especially for extremely large data.table or data.frame objects.
<h2><span class="orange">How to Save and Load RDA Files in R (With Examples)</span></h2>
Files that end with an <b>.rda</b> extension represent Rdata files.
You can use the <b>save()</b> function to save these types of files in R:
<b>save(df, file='my_data.rda')
</b>
And you can use the <b>load()</b> function to load these types of files in R:
<b>load(file='my_data.rda')</b>
The following example shows how to use each of these functions in practice.
<h3>Example: Save and Load RDA Files in R</h3>
Suppose we create the following data frame in R:
<b>#make this example reproducible
set.seed(0)
#create data frame
df &lt;- data.frame(x=rnorm(100), y=rnorm(100), z=rnorm(100))
#view data frame
head(df)
           x          y          z
1  1.2629543  0.7818592 -1.0457177
2 -0.3262334 -0.7767766 -0.8962113
3  1.3297993 -0.6159899  1.2693872
4  1.2724293  0.0465803  0.5938409
5  0.4146414 -1.1303858  0.7756343
6 -1.5399500  0.5767188  1.5573704
</b>
We can use the <b>save()</b> function to save this data frame to an .rda file:
This file will automatically be saved in the current working directory. You can find the working directory by using the <b>getwd()</b> function:
<b>#display working directory
getwd()
"C:/Users/Bob/Documents"</b>
Now suppose we use the <b>rm()</b> function to remove the data frame from the current R environment:
<b>#remove data frame from current R environment
rm(df)</b>
If we look at our current environment in RStudio, we’ll see that it doesn’t contain any objects:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/env1.png">
We can then use the <b>load()</b> function to load the .rda file into the current R environment:
<b>load(file='my_data.rda')</b>
If we look at the current environment again in RStudio, we’ll see that it now contains the data frame:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/env2.png">
<h2><span class="orange">How to Use read.delim Function in R</span></h2>
You can use the <b>read.delim()</b> function to read delimited text files into R.
This function uses the following basic syntax:
<b>read.delim(file, header=TRUE, sep=’\t’)</b>
where:
<b>file</b>: The file location.
<b>header</b>: Whether the first line represents the header of the table. Default is TRUE.
<b>sep</b>: The table delimiter. Default is tab (\t).
The following example shows how to use this function in practice.
<h3>Example: How to Use read.delim in R</h3>
First, let’s create a data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('Mavs', 'Mavs', 'Spurs', 'Nets'), points=c(99, 90, 84, 96), assists=c(22, 19, 16, 20), rebounds=c(30, 39, 42, 26))
#view data frame
df
   team points assists rebounds
1  Mavs     99      22       30
2  Mavs     90      19       39
3 Spurs     84      16       42
4  Nets     96      20       26</b>
Next, let’s use the <b>write.table()</b> function to export the data frame to a tab-delimited text file:
<b>#export to tab-delimited text file
write.table(df, 'my_data.txt', quote=FALSE, sep='\t', row.names=FALSE)
</b>
I can then navigate to the location where I exported the data and view the text file:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/read_delim1.png">
 I can then use the <b>read.delim()</b> function to read in the text file:
<b>#read in tab-delimited text file
my_df &lt;- read.delim('my_data.txt')
#view data
my_df
   team points assists rebounds
1  Mavs     99      22       30
2  Mavs     90      19       39
3 Spurs     84      16       42
4  Nets     96      20       26</b>
The data frame matches the data frame that we created earlier.
Note that the default table delimiter for the <b>read.delim()</b> function is a tab (\t).
Thus, the following code produces the same results:
<b>#read in tab-delimited text file
my_df &lt;- read.delim('my_data.txt', sep='\t')
#view data
my_df
   team points assists rebounds
1  Mavs     99      22       30
2  Mavs     90      19       39
3 Spurs     84      16       42
4  Nets     96      20       26</b>
<h3>Notes on Using read.delim()</h3>
Note that you can use the <b>getwd()</b> function to get the current working directory to find where the first data frame was exported to.
You can also use the <b>setwd()</b> function if you’d like to change the location of the current working directory.
<h2><span class="orange">How to Read and Interpret a Regression Table</span></h2>
In statistics,  regression  is a technique that can be used to analyze the relationship between predictor variables and a response variable.
When you use software (like R, SAS, SPSS, etc.) to perform a regression analysis, you will receive a regression table as output that summarize the results of the regression. It’s important to know how to read this table so that you can understand the results of the regression analysis.
This tutorial walks through an example of a regression analysis and provides an in-depth explanation of how to read and interpret the output of a regression table.
<h2>A Regression Example</h2>
Suppose we have the following dataset that shows the total number of hours studied, total prep exams taken, and final exam score received for 12 different students:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable1.jpg">
To analyze the relationship between hours studied and prep exams taken with the final exam score that a student receives, we run a multiple linear regression using <em>hours studied </em>and <em>prep </em><em>exams taken </em>as the predictor variables and <em>final exam score </em>as the response variable.
We receive the following output:
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable2.jpg">
<h2>Examining the Fit of the Model</h2>
The first section shows several different numbers that measure the fit of the regression model, i.e. how well the regression model is able to “fit” the dataset.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable3.jpg" alt="">
Here is how to interpret each of the numbers in this section:
<h3>Multiple R</h3>
This is the  correlation coefficient . It measures the strength of the linear relationship between the predictor variables and the response variable. A multiple R of 1 indicates a perfect linear relationship while a multiple R of 0 indicates no linear relationship whatsoever. Multiple R is the square root of R-squared (see below).
In this example, <b>the multiple R is 0.72855</b>, which indicates a fairly strong linear relationship between the predictors <em>study hours </em>and <em>prep exams </em>and the response variable <em>final exam score</em>.
<h3>R-Squared</h3>
This is often written as r<sup>2</sup>, and is also known as the <em>coefficient of determination</em>. It is the proportion of the variance in the response variable that can be explained by the predictor variable.
The value for R-squared can range from 0 to 1. A value of 0 indicates that the response variable cannot be explained by the predictor variable at all. A value of 1 indicates that the response variable can be perfectly explained without error by the predictor variable.
In this example,<b> the R-squared is 0.5307</b>, which indicates that 53.07% of the variance in the final exam scores can be explained by the number of hours studied and the number of prep exams taken.
<b>Related:</b>  What is a Good R-squared Value? 
<h3>Adjusted R-Squared</h3>
This is a modified version of R-squared that has been adjusted for the number of predictors in the model. It is always lower than the R-squared. The adjusted R-squared can be useful for comparing the fit of different regression models to one another.
In this example, <b>the Adjusted R-squared is 0.4265.</b>
<h3>Standard Error of the Regression</h3>
The standard error of the regression is the average distance that the observed values fall from the regression line. In this example, <b>the observed values fall an average of 7.3267 units from the regression line.</b>
<b>Related:</b>  Understanding the Standard Error of the Regression 
<h3>Observations</h3>
This is simply the number of  observations  our dataset. In this example, <b>the total observations is 12</b>.
<h2>Testing the Overall Significance of the Regression Model</h2>
The next section shows the degrees of freedom, the sum of squares, mean squares, F statistic, and overall significance of the regression model.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable4.jpg">
Here is how to interpret each of the numbers in this section:
<h3>Regression degrees of freedom</h3>
This number is equal to: the number of regression coefficients – 1. In this example, we have an intercept term and two predictor variables, so we have three regression coefficients total, which means <b>the regression degrees of freedom is 3 – 1 = 2</b>.
<h3>Total degrees of freedom</h3>
This number is equal to: the number of observations – 1. In this example, we have 12 observations, so <b>the total degrees of freedom is 12 – 1 = 11</b>.
<h3>Residual degrees of freedom</h3>
This number is equal to: total df – regression df. In this example, the residual degrees of freedom is <b>11 – 2 = 9</b>.
<h3>Mean Squares</h3>
The regression mean squares is calculated by regression SS / regression df. In this example, <b>regression MS = 546.53308 / 2 = 273.2665</b>.
The residual mean squares is calculated by residual SS / residual df. In this example, <b>residual MS = 483.1335 / 9 = 53.68151</b>.
<h3>F Statistic</h3>
The f statistic is calculated as regression MS / residual MS. This statistic indicates whether the regression model provides a better fit to the data than a model that contains no independent variables.
In essence, it tests if the regression model as a whole is useful. Generally if none of the predictor variables in the model are statistically significant, the overall F statistic is also not statistically significant.
In this example, <b>the F statistic is 273.2665 / 53.68151 = 5.09</b>.
<h3>Significance of F (P-value)</h3>
The last value in the table is the p-value associated with the F statistic. To see if the overall regression model is significant, you can compare the p-value to a significance level; common choices are .01, .05, and .10.
If the p-value is less than the significance level, there is sufficient evidence to conclude that the regression model fits the data better than the model with no predictor variables. This finding is good because it means that the predictor variables in the model actually improve the fit of the model.
In this example, <b>the p-value is 0.033</b>, which is less than the common significance level of 0.05. This indicates that the regression model as a whole is statistically significant, i.e. the model fits the data better than the model with no predictor variables.
<h2>Testing the Overall Significance of the Regression Model</h2>
The last section shows the coefficient estimates, the standard error of the estimates, the t-stat, p-values, and confidence intervals for each term in the regression model.
<img class="lazy" data-src="https://fourpillarfreedom.com/wp-content/uploads/2019/03/regressionTable5.jpg">
Here is how to interpret each of the numbers in this section:
<h3>Coefficients</h3>
The coefficients give us the numbers necessary to write the estimated regression equation:
<b>y<sub>hat</sub> = b<sub>0</sub> + b<sub>1</sub>x<sub>1</sub> + b<sub>2</sub>x<sub>2</sub>.</b>
In this example, the estimated regression equation is:
<b>final exam score = 66.99 + 1.299(Study Hours) + 1.117(Prep Exams)</b>
Each individual coefficient is interpreted as the average increase in the response variable for each one unit increase in a given predictor variable, assuming that all other predictor variables are held constant. For example, for each additional hour studied, the average expected increase in final exam score is 1.299 points, <em>assuming that the number of prep exams taken is held constant. </em>
The intercept is interpreted as the expected average final exam score for a student who studies for zero hours and takes zero prep exams. In this example, a student is expected to score a 66.99 if they study for zero hours and take zero prep exams. Be careful when interpreting the intercept of a regression output, though, because it doesn’t always make sense to do so.
For example, in some cases, the intercept may turn out to be a negative number, which often doesn’t have an obvious interpretation. This doesn’t mean the model is wrong, it simply means that the intercept by itself should not be interpreted to mean anything.
<h3>Standard Error, t-stats, and p-values</h3>
The standard error is a measure of the uncertainty around the estimate of the coefficient for each variable.
The t-stat is simply the coefficient divided by the standard error. For example, the t-stat for <em>Study Hours </em>is 1.299 / 0.417 = 3.117.
The next column shows the p-value associated with the t-stat. This number tells us if a given response variable is significant in the model. In this example, we see that the p-value for <em>Study Hours </em>is 0.012 and the p-value for <em>Prep Exams </em>is 0.304. This indicates that <em>Study Hours </em>is a significant predictor of final exam score, while <em>Prep Exams </em>is not. 
<h3>Confidence Interval for Coefficient Estimates</h3>
The last two columns in the table provide the lower and upper bounds for a 95% confidence interval for the coefficient estimates. 
For example, the coefficient estimate for <em>Study Hours </em>is 1.299, but there is some uncertainty around this estimate. We can never know for sure if this is the exact coefficient. Thus, a 95% confidence interval gives us a range of likely values for the true coefficient.
In this case, the 95% confidence interval for <em>Study Hours</em> is (0.356, 2.24). Notice that this confidence interval does not contain the number “0”, which means we’re quite confident that the true value for the coefficient of <em>Study Hours </em>is non-zero, i.e. a positive number.
By contrast, the 95% confidence interval for <em>Prep Exams </em>is (-1.201, 3.436). Notice that this confidence interval <em>does</em> contain the number “0”, which means that the true value for the coefficient of <em>Prep Exams </em>could be zero, i.e. non-significant in predicting final exam scores.
<h2><span class="orange">How to Read Zip Files in R (With Example)</span></h2>
You can use the following basic syntax to read a ZIP file into R:
<b>library(readr)
#import data1.csv located within my_data.zip
df &lt;- read_csv(unzip("my_data.zip", "data1.csv"))
</b>
The following example shows how to use this syntax in practice.
<h3>Example: How to Read Zip Files in R</h3>
Suppose I have a ZIP file called <b>my_data.zip</b> that contains the following three CSV files:
data1.csv
data2.csv
data3.csv
Assuming my  working directory  contains this ZIP file, I can use the following syntax to display all files located within <b>my_data.zip</b>:
<b>#display all files in my_data.zip
unzip("my_data.zip", list = TRUE)
       Name Length                Date
1 data1.csv     37 2022-03-10 09:48:00
2 data2.csv     36 2022-03-10 09:49:00
3 data3.csv     34 2022-03-10 10:54:00 
</b>
We can see the names of each file located within <b>my_data.zip</b> along with their length and the date they were created.
Next, I can use the following syntax to import the dataset called <b>data1.csv</b> into a data frame in R:
<b>library(readr)
#read data1.csv into data frame
df1 &lt;- read_csv(unzip("my_data.zip", "data1.csv"))
#view data frame
df1
# A tibble: 4 x 2
  team  points
    
1 A         12
2 B         31
3 C         27
4 D         30
</b>
We can see that R successfully imported this CSV file into a data frame.
<b>Note</b>: You can find the complete documentation for the <b>read_csv()</b> function  here .
<h2><span class="orange">How to Use readLines() Function in R (With Examples)</span></h2>
The <b>readLines()</b> function in R can be used to read some or all text lines from a connection object.
This function uses the following syntax:
<b>readLines(con, n=-1L)</b>
where:
<b>con:</b> A connection object or character string
<b>n:</b> The maximum number of lines to read. Default is to read all lines.
The following examples show how to use this function in practice with the following text file called <b>some_data.txt</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/readlines1.jpg">
<h3>Example 1: Use readLines() to Read All Lines from Text File</h3>
Suppose the text file is saved in my <b>Documents</b> folder on my computer.
I can use the following <b>readLines()</b> function to read each line from this text file:
<b>#read every line from some_data.txt
readLines("C:/Users/Bob/Documents/some_data.txt")
[1] "The first line of the file"  "The second line of the file"
[3] "The third line of the file"  "The fourth line of the file"
[5] "The fifth line of the file"  "The sixth line of the file"  
</b>
The text file contains 6 lines, so the <b>readLines()</b> function produces a character vector of length 6.
If I’d like, I can save the lines from the text file in a data frame instead:
<b>#read every line from some_data.txt
my_data &lt;- readLines("C:/Users/Bob/Documents/some_data.txt")
#create data frame
df = data.frame(values=my_data)
#view data frame
df
       values
1  The first line of the file
2 The second line of the file
3  The third line of the file
4 The fourth line of the file
5  The fifth line of the file
6  The sixth line of the file</b>
The result is a data frame with one column and six rows.
<h3>Example 2: Use readLines() to Read First N Lines from Text File</h3>
Once again suppose the text file is saved in my <b>Documents</b> folder on my computer.
I can use the following <b>readLines()</b> function with the <b>n</b> argument to read only the first n lines from this text file:
<b>#read first 4 lines from some_data.txt
readLines("C:/Users/Bob/Documents/some_data.txt", n=4)
[1] "The first line of the file"  "The second line of the file"
[3] "The third line of the file"  "The fourth line of the file"
</b>
The <b>readLines()</b> function produces a character vector of length 4.
I can also use brackets to access a specific line from this text file.
For example, I can use the following code to access just the second line from the character vector:
<b>#read first 4 lines from some_data.txt
my_data &lt;- readLines("C:/Users/Bob/Documents/some_data.txt", n=4)
#display second line only
my_data[2]
[1] "The second line of the file"
</b>
<h2><span class="orange">How to Recode Values Using dplyr</span></h2>
Occasionally you may be interested in recoding certain values in a dataframe in R. Fortunately this can easily be done using the  recode()  function from the dplyr package.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Recode a Single Column in a Dataframe</h3>
The following code shows how to recode a single column in a dataframe:
<b>library(dplyr)
#create dataframe 
df &lt;- data.frame(player = c('A', 'B', 'C', 'D'), points = c(24, 29, 13, 15), result = c('Win', 'Loss', 'Win', 'Loss'))
#view dataframe 
df
#change 'Win' and 'Loss' to '1' and '0'
df %>% mutate(result=recode(result, 'Win'='1', 'Loss'='0'))
       player points result
1      A     24      1
2      B     29      0
3      C     13      1
4      D     15      0
</b>
<h3>Example 2: Recode a Single Column in a Dataframe and Provide NA Values</h3>
The following code shows how to recode a single column in a dataframe and give a value of <b>NA </b>to any value that is not explicitly given a new value:
<b>library(dplyr)
#create dataframe 
df &lt;- data.frame(player = c('A', 'B', 'C', 'D'), points = c(24, 29, 13, 15), result = c('Win', 'Loss', 'Win', 'Loss'))
#view dataframe 
df
#change 'Win' to '1' and give all other values a value of NA
df %>% mutate(result=recode(result, 'Win'='1', .default=NA_character_))
       player points result
1      A     24      1
2      B     29      &lt;NA>
3      C     13      1
4      D     15      &lt;NA></b>
<h3>Example 3: Recode Multiple Columns in a Dataframe</h3>
The following code shows how to recode multiple columns at once in a dataframe:
<b>library(dplyr)
#create dataframe 
df &lt;- data.frame(player = c('A', 'B', 'C', 'D'), points = c(24, 29, 13, 15), result = c('Win', 'Loss', 'Win', 'Loss'))
#recode 'player' and 'result' columns
df %>% mutate(player=recode(player, 'A'='Z'),
              result=recode(result, 'Win'='1', 'Loss'='0'))
       player points result
1      Z     24      1
2      B     29      0
3      C     13      1
4      D     15      0</b>
<em>You can find the complete documentation for the recode() function  here .</em>
<h2><span class="orange">What is Referral Bias?</span></h2>
<b>Referral bias</b> is a type of bias that occurs when the types of individuals included in a study are not  representative  of the individuals in the overall population.
This type of bias occurs most often when researchers study the outcomes of patients in tertiary-care centers. A <em>tertiary-care center</em> is a center where patients in hospitals are referred to say they can receive specialized care for a certain medical condition or disease.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/referral1.png">
In most cases, only patients who are healthy enough to survive the relocation to a tertiary-care center are actually transferred which means the patients that are treated in these centers tend to be healthier, on average, compared to the overall population of patients who suffer from the condition.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/referral2.png">
Thus, when researchers analyze the recovery rates of these patients in tertiary-care centers it’s likely that the recovery rates will be much higher compared to the overall population of patients admitted into the hospital since the individuals were healthier to start with.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/referral3.png">
As a result, researchers may inaccurately conclude that a certain disease or medical condition is less severe than it actually is because they’re analyzing a sample of patients that is not representative of the population.
Also, researchers may inaccurately conclude that a certain facility is more capable of treating patients with a certain disease effectively because the recovery rate or survival rate is extremely high.
However, these rates may only be high because the sample of patients that receive treatment in these centers are already quite healthy.
<h3>Factors that Lead to Referral Bias</h3>
In the previous example we saw that <em>health </em>was a factor that could cause referral bias.
Other factors that could also affect the likelihood of a patient to be transferred to a tertiary-care center (and thus lead to referral bias) include:
Ability of patient to pay for a specialized treatment
Distance between hospital and tertiary-care center
Combination of underlying health conditions
Means to be transferred from hospital to tertiary-care center.
Each of these factors could cause the sample of patients that receive care at a tertiary-care center to be quite different from the population of patients that actually come to the hospital, which could result in referral bias.
 Nonresponse Bias 
 Undercoverage Bias 
 Self-Selection Bias 
 Omitted Variable Bias 
 Neyman Bias 
Refer to  this tutorial  for a background on the importance of representative samples in research.
<h2><span class="orange">Regression Sum of Squares (SSR) Calculator</span></h2>
This calculator finds the regression sum of squares of a regression equation based on values for a predictor variable and a response variable.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">6, 7, 7, 8, 12, 14, 15, 16, 16, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">14, 15, 15, 17, 18, 18, 16, 14, 11, 8</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Regression Sum of Squares (SSR): 19.6122</b>
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
var xbar = math.mean(x);
var ybar = math.mean(y);
let xbar2_hold = 0
for (let i = 0; i < x.length; i++) {
xbar2_hold += Math.pow(x[i], 2);
}
var xbar2 = xbar2_hold / x.length;
let sxx = 0
for (let i = 0; i < x.length; i++) {
sxx += Math.pow(x[i] - xbar, 2);
}
let syy = 0
for (let i = 0; i < y.length; i++) {
syy += Math.pow(y[i] - ybar, 2);
}
let sxy = 0
for (let i = 0; i < x.length; i++) {
sxy += (x[i] - xbar)*(y[i]-ybar);
}
let sxx2 = 0
for (let i = 0; i < x.length; i++) {
sxx2 += (x[i] - xbar)*(Math.pow(x[i], 2)-xbar2);
}
let sx2x2 = 0
for (let i = 0; i < x.length; i++) {
sx2x2 += Math.pow((Math.pow(x[i], 2)-xbar2), 2);
}
let sx2y = 0
for (let i = 0; i < x.length; i++) {
sx2y += (Math.pow(x[i], 2)-xbar2)*(y[i]-ybar);
}
var b = ((sxy*sx2x2)-(sx2y*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var c = ((sx2y*sxx)-(sxy*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var a = ybar - (b*xbar) - (c*xbar2);
var sst = syy;
var ssr = (sxy/sxx)*sxy;
var sse = sst-ssr;
document.getElementById('ssr').innerHTML = ssr.toFixed(4);
}
//output error message if boths lists are not equal
else {
 //document.getElementById('out').innerHTML = '';
 document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">Regression Through the Origin: Definition & Example</span></h2>
Simple linear regression is a method that can be used to quantify the relationship between one or more predictor variables and a  response variable .
A simple linear regression model takes on the following form:
<b>y = β<sub>0</sub> + β<sub>1</sub>x</b>
where:
<b>y</b>: The value of the response variable
<b>β<sub>0</sub></b>: The value of the response variable when x = 0 (known as the “intercept” term)
<b>β<sub>1</sub></b>: The average increase in the response variable associated with a one unit increase in x
<b>x</b>: The value of the predictor variable
A modified version of this model is known as <b>regression through the origin</b>, which forces y to be equal to 0 when x is equal to 0.
This type of model takes on the following form:
<b>y = β<sub>1</sub>x</b>
Notice that the intercept term has been completely dropped from the model.
This model is sometimes used when researchers know that the response variable must be equal to zero when the predictor variable is equal to zero.
In the real world, this type of model is used most commonly in forestry or  ecology studies .
For example, researchers may use tree circumference to predict tree height. If a given tree has a circumference of zero, it must have a height of zero.
Thus, when fitting a regression model to this data it wouldn’t make sense for the intercept term to be non-zero.
The following example shows the difference between fitting an ordinary simple linear regression model compared to a model that implements regression through the origin.
<h3>Example: Regression Through the Origin</h3>
Suppose a biologist wants to fit a regression model using tree circumference to predict tree height. She goes out and collects the following measurements for a sample of 15 trees:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/origin1-1.jpg"305">
We can use the following code in R to fit a simple linear regression model along with a regression model that uses no intercept and plot both regression lines:
<b>#create data frame
df &lt;- data.frame(circ=c(15, 19, 25, 39, 44, 46, 49, 54, 67, 79, 81, 84, 88, 90, 99), height=c(200, 234, 285, 375, 440, 470, 564, 544, 639, 750, 830, 854,          901, 912, 989))
#fit a simple linear regression model
model &lt;- lm(height ~ circ, data = df)
#fit regression through the origin
model_origin &lt;- lm(height ~ 0 + ., data = df)
#create scatterplot
plot(df$circ, df$height, xlab='Circumference', ylab='Height',
     cex=1.5, pch=16, ylim=c(0,1000), xlim=c(0,100))
#add the fitted regression lines to the scatterplot
abline(model, col='blue', lwd=2)
abline(model_origin, lty='dashed', col='red', lwd=2)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/origin2.jpg">
The red dashed line represents the regression model that goes through the origin and the blue solid line represents the ordinary simple linear regression model.
We can use the following code in R to get the coefficient estimates for each model:
<b>#display coefficients for simple linear regression model
coef(model)
(Intercept)        circ 
  40.696971    9.529631 
#display coefficients for regression model through the origin
coef(model_origin)
    circ 
10.10574 
</b>
The fitted equation for the simple linear regression model is:
Height = 40.6969 + 9.5296(Circumference)
And the fitted equation for the regression model through the origin is:
Height = 10.1057(Circumference)
Notice that the coefficient estimates for the circumference variable are slightly different.
<h3>Cautions on Using Regression Through the Origin</h3>
Before using regression through the origin, you must be absolutely sure that a value of 0 for the predictor variable implies a value of 0 for the response variable. In many scenarios, it’s almost impossible to know this for sure.
And if you’re using regression through the origin to save one degree of freedom from estimating the intercept, this rarely makes a substantial difference if your sample size is large enough.
If you do choose to use regression through the origin, be sure to state your reasoning in your final analysis or report.
<h2><span class="orange">Regression vs. Classification: What’s the Difference?</span></h2>
Machine learning algorithms can be broken down into two distinct types:  supervised and unsupervised learning algorithms .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/supervised4.png">
Supervised learning algorithms can be further classified into two types:
<b>1. Regression:</b> <b>The response variable is continuous.</b>
For example, the  response variable  could be:
Weight
Height
Price
Time
Total units
In each case, a regression model seeks to predict a continuous quantity.
<b>Regression Example:</b>
 
Suppose we have a dataset that contains three variables for 100 different houses: square footage, number of bathrooms, and selling price.
 
We could fit a regression model that uses square footage and number of bathrooms as explanatory variables and selling price as the response variable.
 
We could then use this model to predict the selling price of a house, based on its square footage and number of bathrooms.
 
This is an example of a regression model because the response variable (selling price) is continuous.
The most common way to measure the accuracy of a regression model is by calculating the root mean square error (RMSE), a metric that tells us how far apart our predicted values are from our observed values in a model, on average. It is calculated as:
<b>RMSE</b> = √Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
The smaller the RMSE, the better a regression model is able to fit the data.
<b>2. Classification:</b> <b>The response variable is categorical.</b>
For example, the response variable could take on the following values:
Male or female
Pass or fail
Low, medium, or high
In each case, a classification model seeks to predict some class label.
<b>Classification Example:</b>
 
Suppose we have a dataset that contains three variables for 100 different college basketball players: average points per game, division level, and whether or not they got drafted into the NBA.
 
We could fit a classification model that uses average points per game and division level as explanatory variables and “drafted” as the response variable.
 
We could then use this model to predict whether or not a given player will get drafted into the NBA based on their average points per game and division level.
 
This is an example of a classification model because the response variable (“drafted”) is categorical. That is, it can only take on values in two different categories: “Drafted” or “Not drafted.”
The most common way to measure the accuracy of a classification model is by simply calculating the percentage of correct classifications the model makes:
<b>Accuracy = correction classifications / total attempted classifications * 100%</b>
For example, if a model correctly identifies whether or not a player will get drafted into the NBA 88 times out of 100 possible times then the accuracy of the model is:
<b>Accuracy = (88/100) * 100% = 88%</b>
The higher the accuracy, the better a classification model is able to predict outcomes.
<h3>Similarities Between Regression and Classification</h3>
Regression and classification algorithms are similar in the following ways:
Both are supervised learning algorithms, i.e. they both involve a response variable.
Both use one or more  explanatory variables  to build models to predict some response.
Both can be used to understand how changes in the values of explanatory variables affect the values of a response variable.
<h3>Differences Between Regression and Classification</h3>
Regression and classification algorithms are different in the following ways:
Regression algorithms seek to predict a continuous quantity and classification algorithms seek to predict a class label.
The way we measure the accuracy of regression and classification models differs.
<h3>Converting Regression into Classification</h3>
It’s worth noting that a regression problem can be converted into a classification problem by simply <b>discretizing</b> the response variable into buckets.
For example, suppose we have a dataset that contains three variables: square footage, number of bathrooms, and selling price.
We could build a regression model using square footage and number of bathrooms to predict selling price.
However, we could discretize selling price into three different classes:
$80k – $160k: “Low selling price”
$161k – $240k: “Medium selling price”
$241k – $320k: “High selling price”
We could then use square footage and number of bathrooms as explanatory variables to predict which class (low, medium or high) that a given house selling price will fall in.
This would be an example of a classification model since we’re attempting to place each house in a class.
<h3>Summary</h3>
The following table summarizes the similarities and differences between regression and classification algorithms:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/regression_vs_classification1.png">
<h2><span class="orange">What is a Regressor? (Definition & Examples)</span></h2>
In statistics, a <b>regressor</b> is the name given to any variable in a regression model that is used to predict a response variable.
A regressor is also referred to as:
An  explanatory variable 
An  independent variable 
A  manipulated variable 
A predictor variable
A feature
All of these terms are used interchangeably depending on the type of field you’re working in: statistics, machine learning, econometrics, biology, etc.
<b>Note:</b> Sometimes a response variable is called a “regressand.”
<h3>Regressors in Regression Models</h3>
Most regression models take the following form:
<b>Y = β<sub>0</sub> + B<sub>1</sub>x<sub>1</sub>+ B<sub>2</sub>x<sub>2</sub> + B<sub>3</sub>x<sub>3</sub> + ε</b>
where:
<b>Y:</b> The response variable
<b>β<sub>i</sub>:</b> The coefficients for the regressors
<b>x<sub>i</sub>:</b> The regressors
<b>ε:</b> The error term
The whole point of building a regression model is to understand how changes in a regressor lead to changes in a response variable (or “regressand”).
Note that regression models can have one or more regressors.
When there is only one regressor, the model is referred to as a  simple linear regression model  and when there are multiple regressors, the model is referred to as a  multiple linear regression model  to indicate that there are <em>multiple</em> regressors.
The following examples illustrate how to interpret regressors in different regression models.
<h3>Example 1: Crop Yield</h3>
Suppose a farmer is interested in understanding the factors that affect total crop yield (in pounds). He collects data and builds the following regression model:
Crop Yield = 154.34 + 3.56*(Pounds of Fertilizer) + 1.89*(Pounds of Soil)
This model has two regressors: Fertilizer and Soil.
Here’s how to interpret these two regressors:
<b>Fertilizer:</b> For each additional pound of fertilizer used, crop yield increases by an average of 3.56 pounds, assuming the amount of soil is held constant.
<b>Soil:</b> For each additional pound of soil used, crop yield increases by an average of 1.89 pounds, assuming the amount of fertilizer is held constant.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/regressor1.png">
<h3>Example 2: Exam Scores</h3>
Suppose a professor is interested in understanding how the amount of hours studied affects exam scores. He collects data and builds the following regression model:
Exam Score = 68.34 + 3.44*(Hours Studied)
This model has one regressor: Hours studied. We interpret the coefficient for this regressor to mean that for each additional hour studied, exam score increases by an average of 3.44 points.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/03/regressor2.png">
<h2><span class="orange">The Relationship Between Mean & Standard Deviation (With Example)</span></h2>
The <b>mean </b>represents the average value in a dataset.
It is calculated as:
Sample mean = Σx<sub>i</sub> / n
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> observation in a dataset
<b>n:</b> The total number of observations in the dataset
The<b> standard deviation</b> represents how spread out the values are in a dataset relative to the mean.
It is calculated as:
Sample standard deviation = √Σ(x<sub>i</sub> – x<sub>bar</sub>)<sup>2</sup> / (n-1)
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The i<sup>th</sup> value in the sample
<b>x<sub>bar</sub>:</b> The mean of the sample
<b>n:</b> The sample size
Notice the relationship between the mean and standard deviation: <b>The mean is used in the formula to calculate the standard deviation</b>.
<b>In fact, we can’t calculate the standard deviation of a sample unless we know the sample mean.</b>
The following example shows how to calculate the sample mean and sample standard deviation for a dataset in practice.
<h2>Example: Calculating the Mean & Standard Deviation for a Dataset</h2>
Suppose we have the following dataset that shows the points scored by 10 different basketball players:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/rel1.jpg"166">We can calculate the sample mean of points scored by using the following formula:
Sample mean = Σx<sub>i</sub> / n
Sample mean = (22+14+15+18+19+8+9+34+30+7) / 10
Sample mean = 17.6
The sample mean of points scored is <b>17.6</b>. This represents the average number of points scored among all players.
Once we know the sample mean, we can the plug it into the formula to calculate the sample standard deviation:
Sample standard deviation = √Σ(x<sub>i</sub> – x<sub>bar</sub>)<sup>2</sup> / (n-1)
Sample standard deviation = √((22-17.6)<sup>2</sup> + (14-17.6)<sup>2</sup> + (15-17.6)<sup>2</sup> + (18-17.6)<sup>2</sup> + (19-17.6)<sup>2 </sup>+ (8-17.6)<sup>2</sup> + (9-17.6)<sup>2</sup> + (34-17.6)<sup>2</sup> + (30-17.6)<sup>2</sup> + (7-17.6)<sup>2</sup>) / (10-1)
Sample standard deviation = 9.08
The sample standard deviation is <b>9.08</b>. This represents the average distance between each points value and the sample mean of points.
It’s helpful to know both the mean and the standard deviation of a dataset because each metric tells us something different.
The <b>mean</b> gives us an idea of where the “center” value of a dataset is located.
The <b>standard deviation</b> gives us an idea of how spread out the values are around the mean in a dataset. The higher the value for the standard deviation, the more spread out the values are in a sample.
By knowing both of these values, we can know a great deal about the distribution of values in a dataset.
<h2>Additional Resources</h2>
The following tutorials provide additional information about the mean and standard deviation:
 Why is the Mean Important in Statistics? 
 Why is Standard Deviation Important in Statistics? 
 How to Calculate the Mean and Standard Deviation in Excel 
<h2><span class="orange">How to Calculate Relative Frequencies Using dplyr</span></h2>
Often you may want to calculate the relative frequencies/proportions of values in one or more columns of a data frame in R.
Fortunately this is easy to do using functions from the  dplyr  package. This tutorial demonstrates how to use these functions to calculate relative frequencies on the following data frame:
<b>#create data frame
df &lt;- data.frame(team = c('A', 'A', 'A', 'B', 'B', 'B', 'B'), position = c('G', 'F', 'F', 'G', 'G', 'G', 'F'), points = c(12, 15, 19, 22, 32, 34, 39))
#view data frame
df
  team position points
1    A        G     12
2    A        F     15
3    A        F     19
4    B        G     22
5    B        G     32
6    B        G     34
7    B        F     39
</b>
<h3>Example 1: Relative Frequency of One Variable</h3>
The following code shows how to calculate the relative frequency of each team in the data frame:
<b>library(dplyr)
df %>%
  group_by(team) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
# A tibble: 2 x 3
  team       n  freq
    
1 A          3 0.429
2 B          4 0.571
</b>
This tells us that team A accounts for 42.9% of all rows in the data frame while team B accounts for the remaining 57.1% of rows. Notice that together they add up to 100%.
<b>Related: </b> The Complete Guide: How to Group & Summarize Data in R 
<h3>Example 2: Relative Frequency of Multiple Variables</h3>
The following code shows how to calculate the relative frequency of positions <em>by</em> team:
<b>library(dplyr)
df %>%
  group_by(team, position) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n))
# A tibble: 4 x 4
# Groups:   team [2]
  team   position     n  freq
       
1 A      F            2 0.667
2 A      G            1 0.333
3 B      F            1 0.250
4 B      G            3 0.750
</b>
This tells us that:
66.7% of players on team A are in position F.
33.3% of players on team A are in position G.
25.0% of players on team A are in position F.
75.0% of players on team B are in position G.
<b>Related: </b> How to Use Mutate to Create New Variables in R 
<h3>Example 3: Display Relative Frequencies as Percentages</h3>
The following code shows how to calculate the relative frequency of positions by team and how to display these relative frequencies as percentages:
<b>library(dplyr)
df %>%
  group_by(team, position) %>%
  summarise(n = n()) %>%
  mutate(freq = paste0(round(100 * n/sum(n), 0), '%'))
# A tibble: 4 x 4
# Groups:   team [2]
  team   position     n freq 
       
1 A      F            2 67%  
2 A      G            1 33%  
3 B      F            1 25%  
4 B      G            3 75%</b>
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">Relative Frequency Calculator</span></h2>
A <b>frequency table</b> is a table that shows how many times certain values occur in a dataset.
A <b>relative frequency table</b> is a table that shows how many times certain values occur relative to all the observations in a dataset.
To create a relative frequency table for a given dataset, simply enter the comma-separated values in the box below and then click the “Calculate” button.
<textarea id="input_data" name="x" rows="5" cols="40">4, 14, 16, 22, 24, 25, 37, 38, 38, 40, 41, 41, 43, 44</textarea>
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<script>
function calc() {
//remove current table if one exists
var element = document.getElementsByTagName('table')[0];
    if(element) {element.parentNode.removeChild(element)}
//get input values
var input_data = document.getElementById('input_data').value.split(',').map(Number);
//calculate stuff
var occurrence = function (array) {
    "use strict";
    var result = {};
    if (array instanceof Array) { // Check if input is array.
        array.forEach(function (v, i) {
            if (!result[v]) { // Initial object property creation.
                result[v] = [i]; // Create an array for that property.
            } else { // Same occurrences found.
                result[v].push(i); // Fill the array.
            }
        });
    }
    return result;
};
var size = Object.keys(occurrence(input_data)).length;
//create array of relative and cumulative values
var relFreqArray = [];
for (var i = 0; i < size; i++){
    relFreqArray[i] = (occurrence(input_data)[Object.keys(occurrence(input_data))[i]].length / input_data.length)
}
for (var cumsum = [relFreqArray[0]], i = 0, l = relFreqArray.length-1; i<l; i++) {
    cumsum[i+1] = cumsum[i] + relFreqArray[i+1]; 
}
//generate table of frequencies
var table = document.createElement('table');
    function boldHTML(text) {
  var element = document.createElement("b");
  element.innerHTML = text;
  return element;
}
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    var td3 = document.createElement('td');
    var td4 = document.createElement('td');
    td1.appendChild(boldHTML('Value'));
    td2.appendChild(boldHTML('Frequency'));
    td3.appendChild(boldHTML('Relative Frequency'));
    td4.appendChild(boldHTML('Cumulative Relative Frequency'));
    tr.appendChild(td1);
    tr.appendChild(td2);
    tr.appendChild(td3);
    tr.appendChild(td4);
    table.appendChild(tr);
for (var i = 0; i < size; i++){
    var tr = document.createElement('tr');   
    var td1 = document.createElement('td');
    var td2 = document.createElement('td');
    var td3 = document.createElement('td');
    var td4 = document.createElement('td');
    var text1 = document.createTextNode(Object.keys(occurrence(input_data))[i]);
    var text2 = document.createTextNode(occurrence(input_data)[Object.keys(occurrence(input_data))[i]].length);
    var text3 = document.createTextNode(relFreqArray[i].toFixed(4));
    var text4 = document.createTextNode(cumsum[i].toFixed(4));
    td1.appendChild(text1);
    td2.appendChild(text2);
    td3.appendChild(text3);
    td4.appendChild(text4);
    tr.appendChild(td1);
    tr.appendChild(td2);
    tr.appendChild(td3);
    tr.appendChild(td4);
    table.appendChild(tr);
}
document.getElementById('table_output').appendChild(table);
//output results
//document.getElementById('ss').innerHTML = ss.toFixed(2);
}
</script>
<h2><span class="orange">What is a Relative Frequency Distribution?</span></h2>
A <b>frequency distribution</b> describes how often different values occur in a dataset.
For example, suppose we gather a  simple random sample  of 400 households in a city and record the number of pets in each household. The following table shows the results:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/relFreq1.png">
This table represents a frequency distribution.
A related distribution is known as a <b>relative frequency distribution</b>, which shows the relative frequency of each value in a dataset as a percentage of all frequencies.
For example, in the previous table we saw that there were 400 total households. To find the relative frequency of each value in the distribution, we simply divide each individual frequency by 400:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/relFreq2.png">
Note that relative frequency distributions have the following properties:
Each individual relative frequency is between 0% and 100%.
The sum of all individual relative frequencies adds up to 100%.
If these conditions are not met, then the relative frequency distribution is not valid.
<h3>Why Relative Frequency Distributions Are Useful</h3>
Relative frequency distributions are useful because they allow us to understand how common a value is in a dataset relative to all other values.
In the previous example we saw that 150 households had just one pet. But this number by itself isn’t particularly useful.
Instead, knowing that <b>37.5%</b> of all households in the sample had just one pet is more useful to know. This helps us understand that a little more than 1 in 3 households had just one pet, which gives us some perspective on how “common” it is to own just one pet.
<h3>Visualizing a Relative Frequency Distribution</h3>
The most common way to visualize a relative frequency distribution is to create a  relative frequency histogram , which displays the individual data values along the x-axis of a graph and uses bars to represent the relative frequencies of each class along the y-axis.
For example, here’s what a relative frequency histogram would look like for the data in our previous example:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/relFreq3.png">
The x-axis displays the number of pets in the household and the y-axis displays the relative frequency of households that have that number of pets.
This histogram is a useful way for us to visualize the distribution of relative frequencies.
<h2><span class="orange">How to Calculate Relative Frequency in Excel</span></h2>
A <b>frequency table </b>is a table that displays information about frequencies. Frequencies simply tell us how many times a certain event has occurred. 
For example, the following table shows how many items a shop sold in different price ranges in a given week:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
</tr>
</tbody></table>
The first column displays the price class and the second column displays the frequency of that class.
It’s also possible to calculate the <b>relative frequency </b>for each class, which is simply the frequency of each class as a percentage of the whole.
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Relative Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.303</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.318</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.121</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.061</td>
</tr>
</tbody></table>
In total, there were 66 items sold. Thus, we found the relative frequency of each class by taking the frequency of each class and dividing by the total items sold.
For example, there were 20 items sold in the price range of $1 – $10. Thus, the relative frequency of the class $1 – $10 is 20 / 66 = <b>0.303</b>.
Next, there were 21 items sold in the price range of $11 – $20. Thus, the relative frequency of the class $11 – $20 is 21 / 66 = <b>0.318</b>.
The following example illustrates how to find relative frequencies in Excel.
<h3>Example: Relative Frequencies in Excel</h3>
First, we will enter the class and the frequency in columns A and B:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel1.png">
Next, we will calculate the relative frequency of each class in column C. Column D shows the formulas we used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel2.png">
We can verify that our calculations are correct by making sure the sum of the relative frequencies adds up to 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel3.png">
We can also create a relative frequency histogram to visualize the relative frequencies.
Simply highlight the relative frequencies:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel4.png">
Then go to the <b>Charts </b>group in the <b>Insert </b>tab and click the first chart type in <b>Insert Column or Bar Chart</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel5.png">
A relative frequency histogram will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel6.png">
Modify the x-axis labels by right-clicking on the chart and clicking <b>Select Data</b>. Under <b>Horizontal (Category) Axis Labels </b>click <b>Edit </b>and type in the cell range that contains the item prices. Click <b>OK </b>and the new axis labels will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqExcel7.png">
<h2><span class="orange">How to Create a Relative Frequency Histogram in R</span></h2>
A  <b>relative frequency histogram</b>  is a graph that displays the relative frequencies of values in a dataset.
This tutorial explains how to create a relative frequency histogram in R by using the <b>histogram()</b> function from the <b>lattice</b>, which uses the following syntax:
<b>histogram(x, type)</b>
where:
<b>x: </b>data
<b>type:</b> type of relative frequency histogram you’d like to create; options include percent, count, and density.
<h3>Default Histogram</h3>
First, load the <b>lattice </b>package:
<b>library(lattice)</b>
By default, this package creates a relative frequency histogram with <b>percent </b>along the y-axis:
<b>#create data
data &lt;- c(0, 0, 2, 3, 4, 4, 5, 6, 7, 12, 12, 14)
#create relative frequency histogram
histogram(data)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqR1.png">
<h3>Modifying the Histogram</h3>
We can modify the histogram to include a title, different axes labels, and a different color using the following arguments:
<b>main: </b>the title
<b>xlab: </b>the x-axis label
<b>ylab: </b>the y-axis label
<b>col: </b>the fill color to use in the histogram
For example:
<b>#modify the histogram
histogram(data,
          main='Points per Game by Player',
          xlab='Points per Game',
          col='steelblue')
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqR2.png">
<h3>Modifying the Numbers of Bins</h3>
We can specify the number of bins to use in the histogram using the <b>breaks </b>argument:
<b>#modify the number of bins
histogram(data,
          main='Points per Game by Player',
          xlab='Points per Game',
          col='steelblue',
          breaks=15)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqR3.png">
The more bins you specify, the more you will be able to get a granular look at your data. Conversely, the fewer number of bins you specify, the more aggregated the data will become:
<b>#modify the number of bins
histogram(data,
          main='Points per Game by Player',
          xlab='Points per Game',
          col='steelblue',
          breaks=3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/relFreqR4.png">
<b>Related:</b> Use  Sturges’ Rule  to identify the optimal number of bins to use in a histogram.
<h2><span class="orange">Relative Frequency Histogram: Definition + Example</span></h2>
Often in statistics you will encounter tables that display information about frequencies. Frequencies simply tell us how many times a certain event has occurred. 
For example, the following table shows how many items a particular shop sold in a week based on the price of the item:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
</tr>
</tbody></table>
This type of table is known as a <b>frequency table</b>. In one column we have the “class” and in the other column we have the frequency of the class.
Often we use<b> frequency histograms </b>to visualize the values in a frequency table since it’s typically easier to gain an understanding of data when we can visualize the numbers.
A histogram lists the classes along the x-axis of a graph and uses bars to represent the frequency of each class along the y-axis. The following frequency histogram provides a visual representation of the frequency table above:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/freqHist0.png">
A close cousin of a frequency table is a <b>relative frequency table</b>, which simply lists the frequencies of each class as a percentage of the whole.
The following table shows the relative frequencies of the same dataset we saw earlier:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Relative Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.303</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.318</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.121</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.061</td>
</tr>
</tbody></table>
In total, there were 66 items sold. Thus, we found the relative frequency of each class by taking the frequency of each class and dividing by the total items sold.
For example, there were 20 items sold in the price range of $1 – $10. Thus, the relative frequency of the class $1 – $10 is 20 / 66 = <b>0.303</b>.
Next, there were 21 items sold in the price range of $11 – $20. Thus, the relative frequency of the class $11 – $20 is 21 / 66 = <b>0.318</b>.
We perform the same calculation for each class to get the relative frequencies.
Once we have the relative frequency of each class, we can then create a <b>relative frequency histogram </b>to visualize these relative frequencies.
Similar to a frequency histogram, this type of histogram displays the classes along the x-axis of the graph and uses bars to represent the relative frequencies of each class along the y-axis.
<b>T</b>he only difference is the labels used on the y-axis. <b>Instead of displaying raw frequencies, a relative frequency histogram displays percentages.</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/freqHist1-1.png">
<h2>When to Use a Relative Frequency Histogram</h2>
A frequency histogram can be useful when you’re interested in raw data values. For example, a shop might have a goal to sell at least 10 items each week in the $41 – $50 range.
By creating a frequency histogram of their data, they can easily see that they’re not meeting their goal of selling 10 items per week in this price range:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/freqHist0.png">
Conversely, a relative frequency histogram is useful when you’re interested in percentage values. For example, a shop might have a goal of selling 5% of their total items in the $41 – $50 price range.
By creating a relative frequency histogram of their data, they can see that they are meeting this goal:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/freqHist1-1.png">
Note that a <b>frequency histogram </b>and a <b>relative frequency histogram </b>will both look the exact same. The only difference is the values displayed on the y-axis.
<h2><span class="orange">How to Calculate Relative Frequency in Python</span></h2>
<b>Relative frequency</b> measures how frequently a certain value occurs in a dataset <em>relative </em>to the total number of values in a dataset.
You can use the following function in Python to calculate relative frequencies:
<b>def rel_freq(x):</b>
<b>    freqs = [(value, x.count(value) / len(x)) for value in set(x)] </b>
<b>    return freqs</b>
The following examples show how to use this function in practice.
<h3>Example 1: Relative Frequencies for a List of Numbers</h3>
The following code shows how to use this function to calculate relative frequencies for a list of numbers:
<b>#define data
data = [1, 1, 1, 2, 3, 4, 4]
#calculate relative frequencies for each value in list
rel_freq(data)
[(1, 0.42857142857142855),
 (2, 0.14285714285714285),
 (3, 0.14285714285714285),
 (4, 0.2857142857142857)]
</b>
The way to interpret this output is as follows:
The value “1” has a relative frequency of <b>0.42857 </b>in the dataset.
The value “2” has a relative frequency of <b>0.142857 </b>in the dataset.
The value “3” has a relative frequency of <b>0.142857 </b>in the dataset.
The value “4” has a relative frequency of <b>0.28571 </b>in the dataset.
You’ll notice that all of the relative frequencies add up to 1.
<h3>Example 2: Relative Frequencies for a List of Characters</h3>
The following code shows how to use this function to calculate relative frequencies for a list of characters:
<b>#define data
data = ['a', 'a', 'b', 'b', 'c']
#calculate relative frequencies for each value in list
rel_freq(data)
[('a', 0.4), ('b', 0.4), ('c', 0.2)]
</b>
The way to interpret this output is as follows:
The value “a” has a relative frequency of <b>0.4 </b>in the dataset.
The value “b” has a relative frequency of <b>0.4 </b>in the dataset.
The value “c” has a relative frequency of <b>0.2 </b>in the dataset.
Once again, all of the relative frequencies add up to 1.
<h3>Example 3: Relative Frequencies for a Column in a pandas DataFrame</h3>
The following code shows how to use this function to calculate relative frequencies for a specific column in a pandas DataFrame:
<b>import pandas as pd
#define data
data = pd.DataFrame({'A': [25, 15, 15, 14, 19],     'B': [5, 7, 7, 9, 12],     'C': [11, 8, 10, 6, 6]})
#calculate relative frequencies of values in column 'A'
rel_freq(list(data['A']))
[(25, 0.2), (19, 0.2), (14, 0.2), (15, 0.4)]
</b>
The way to interpret this output is as follows:
The value “25” has a relative frequency of <b>0.2 </b>in the column.
The value “19” has a relative frequency of <b>0.2 </b>in the column.
The value “14” has a relative frequency of <b>0.2 </b>in the column.
The value “15” has a relative frequency of <b>0.4 </b>in the column.
Once again, all of the relative frequencies add up to 1.
<h3>Additional Resources
</h3>
 Relative Frequency Calculator 
 Relative Frequency Histogram: Definition + Example 
 How to Calculate Relative Frequency in Excel 
<h2><span class="orange">How to Create Relative Frequency Tables in R</span></h2>
A <b>relative frequency table</b> tells you how often certain values in a dataset occur <em>relative</em> to the total number of values in the dataset.
You can use the following basic syntax to create a frequency table in R:
<b>table(data)/length(data)
</b>
The <b>table()</b> function calculates the frequency of each individual data value and the <b>length()</b> function calculates the total number of values in the dataset.
Thus, dividing each individual frequency by the length of the dataset gives us the relative frequency of each value in the dataset.
The following examples show how to use this syntax in practice.
<h3>Example 1: Relative Frequency Table for One Vector</h3>
The following code shows how to create a relative frequency table for a single vector in R:
<b>#define data
data &lt;- c('A', 'A', 'B', 'B', 'B', 'C', 'C', 'C', 'C', 'C')
#create relative frequency table
table(data)/length(data)
  A   B   C 
0.2 0.3 0.5 </b>
Here’s how to interpret the table:
<b>20%</b> of all values in the dataset are the letter A
<b>30%</b> of all values in the dataset are the letter B
<b>50%</b> of all values in the dataset are the letter C
<h3>Example 2: Relative Frequency Table for One Data Frame Column</h3>
The following code shows how to create a relative frequency table for one column of a data frame in R:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'A', 'A', 'B', 'B', 'C'), wins=c(2, 9, 11, 12, 15, 17, 18, 19), points=c(1, 2, 2, 2, 3, 3, 3, 3))
#view first few rows of data frame
head(df)
  team wins points
1    A    2      1
2    A    9      2
3    A   11      2
4    A   12      2
5    A   15      3
6    B   17      3
#calculate relative frequency table for 'team' column
table(df$team)/length(df$team)
 
    A      B      C 
0.625  0.250  0.125
</b>
<h3>Example 3: Relative Frequency Table for All Data Frame Columns</h3>
The following code shows how to create a relative frequency table for every column of a data frame in R:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'A', 'A', 'B', 'B', 'C'), wins=c(2, 9, 11, 12, 15, 17, 18, 19), points=c(1, 2, 2, 2, 3, 3, 3, 3))
#calculate relative frequency table for each column
sapply(df, function(x) table(x)/nrow(df))
$team
x
    A     B     C 
0.625 0.250 0.125 
$wins
x
    2     9    11    12    15    17    18    19 
0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 
$points
x
    1     2     3 
0.125 0.375 0.500 
</b>
<h2><span class="orange">How to Calculate Relative Frequency on a TI-84 Calculator</span></h2>
<b>Relative frequencie</b>s tell us how often certain events occur, <em>relative</em> to the total number of events.
For example, the following table shows how many items a shop sold in different price ranges in a given week:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Item Price</b></th>
<th style="text-align: center;"><b>Frequency</b></th>
<th style="text-align: center;"><b>Relative Frequency</b></th>
</tr>
<tr>
<td style="text-align: center;">$1 – $10</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">0.303</td>
</tr>
<tr>
<td style="text-align: center;">$11 – $20</td>
<td style="text-align: center;">21</td>
<td style="text-align: center;">0.318</td>
</tr>
<tr>
<td style="text-align: center;">$21 – $30</td>
<td style="text-align: center;">13</td>
<td style="text-align: center;">0.197</td>
</tr>
<tr>
<td style="text-align: center;">$31 – $40</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.121</td>
</tr>
<tr>
<td style="text-align: center;">$41 – $50</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.061</td>
</tr>
</tbody></table>
There were 66 items sold in total. Thus, we found the relative frequency of each class by taking the frequency of each class and dividing by the total items sold.
For example, there were 20 items sold in the price range of $1 – $10. Thus, the relative frequency of the class $1 – $10 is 20 / 66 = <b>0.303</b>.
Next, there were 21 items sold in the price range of $11 – $20. Thus, the relative frequency of the class $11 – $20 is 21 / 66 = <b>0.318</b>.
And so on.
The following step-by-step example shows how to calculate relative frequencies on a TI-84 calculator.
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values.
Press Stat, then press EDIT. Then enter the values in column L1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/relFreqTI1.png">
<h3>Step 2: Calculate the Relative Frequencies</h3>
Next, highlight the top of column L2 and type in the following formula:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/relFreqTI2.png">
Here’s how to actually type in this formula:
Press 2nd, then press 1. This will input “L1” in the formula.
Press ÷. This will input “/” in the formula.
Press 2nd, then press STAT. Scroll over to “MATH” and then press 5. This will input “sum(” in the formula.
Press 2nd, then press 1. This will input “L1” inside the sum() in the formula.
Press ). This will input the second “)” at the end of the formula.
Once you press Enter, the relative frequencies will appear in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/relFreqTI3.png">
Here’s how to interpret the output:
The relative frequency of the first class is <b>.30303</b>.
The relative frequency of the first class is <b>.31818</b>.
The relative frequency of the first class is <b>.19697</b>.
The relative frequency of the first class is <b>.12121</b>.
The relative frequency of the first class is <b>.06061</b>.
Note that the sum of all of the relative frequencies is 1.
<h2><span class="orange">How to Calculate a Confidence Interval for Relative Risk</span></h2>
We often calculate <b>relative risk </b>when analyzing a 2×2 table, which takes on the following format:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/oddsRatioExcel0.png">
The <b>relative risk</b> tells us the probability of an event occurring in a treatment group compared to the probability of an event occurring in a control group.
It is calculated as:
<b>Relative risk</b> = [A/(A+B)] / [C/(C+D)]
We can then use the following formula to calculate a confidence interval for the relative risk (RR):
<b>Lower 95% CI</b> = e<sup>ln(RR) – 1.96√1/a + 1/c – 1/(a+b) – 1/(c+d)</sup>
<b>Upper 95% CI</b> = e<sup>ln(RR) + 1.96√1/a + 1/c – 1/(a+b) – 1/(c+d)</sup>
The following example shows how to calculate a relative risk and a corresponding confidence interval in practice.
<h3>Example: Calculating a Confidence Interval for Relative Risk</h3>
Suppose a basketball coach uses a new training program to see if it increases the number of players who are able to pass a certain skills test, compared to an old training program.
The coach recruits 50 players to use each program. The following table shows the number of players who passed and failed the skills test, based on the program they used:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/odds_CI1.png">
We can calculate the relative risk as:
Relative Risk = [A/(A+B)] / [C/(C+D)]
Relative Risk = [34/(34+16)] / [39/(39+11)]
Relative Risk = <b>0.8718</b>
We would interpret this to mean that the probability that a player passes the test by using the new program are just 0.8718 times the probability that a player passes the test by using the old program.
In other words, the probability that a player passes the test are actually lowered by using the new program.
We can then use the following formulas to calculate the 95% confidence interval for the relative risk:
Lower 95% CI = e<sup>ln(.8718) – 1.96√(1/34 + 1/39 – 1/(34+16) – 1/(39+11)</sup> = <b>0.686</b>
Upper 95% CI = e<sup>ln(.8718) + 1.96√(1/34 + 1/39 + 1/(34+16) – 1/(39+11)</sup> = <b>1.109</b>
Thus, the 95% confidence interval for the relative risk is <b>[0.686, 1.109]</b>.
We are 95% confident that the true relative risk between the new and old training program is contained in this interval.
Since this confidence interval contains the value 1, it is not statistically significant.
This should make sense if we consider the following:
A relative risk greater than 1 would mean that the probability that a player passes the test by using the new program is <em>higher</em> than the probability that a player passes the test by using the old program.
A relative risk less than 1 would mean that the probability that a player passes the test by using the new program is <em>lower</em> than the probability that a player passes the test by using the old program.
So, since our 95% confidence interval for the relative risk contains the value 1, it means the probability of a player passing the skills test using the new program may or may not be higher than the probability of the same player passing the test using the old program.
<h2><span class="orange">How to Calculate Relative Standard Deviation in Excel</span></h2>
The <b>relative standard deviation</b> is a measure of the sample standard deviation relative to the  sample mean  for a given dataset.
It is calculated as:
<b>Relative standard deviation = s / x * 100%</b>
where:
<b>s:</b> sample standard deviation
<b>x:</b> sample mean
This metric gives us an idea of how closely  observations  are clustered around the mean.
For example, suppose the standard deviation of a dataset is 4. If the mean is 400, then the relative standard deviation is 4/400 * 100% = 1%. This means the observations are clustered tightly around the mean.
However, a dataset that has a standard deviation of 40 and a mean of 400 will have a relative standard deviation of 10%. This means the observations are much more spread out around the mean relative to the previous dataset.
This tutorial provides an example of how to calculate relative standard deviation in Excel.
<h3>Example: Relative Standard Deviation in Excel</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/rsd1.png">
The following formulas show how to calculate the sample mean, sample standard deviation, and relative sample standard deviation of the dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/rsd2.png">
The relative standard deviation turns out to be <b>0.59</b>.
This tells us that the standard deviation of the dataset is 59% of the size of the mean of the dataset. This number is quite large, which indicates that the values are spread out quite a lot around the sample mean.
If we have multiple datasets, we can use the same formula to calculate the relative standard deviation (RSD) for each dataset and compare the RSD’s across the datasets:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/rsd3.png">
We can see that Dataset 3 has the largest relative standard deviation, which indicates that the values in that dataset are the most spread out relative to the mean of the dataset.
Conversely, we can see that Dataset 2 has the smallest relative standard deviation, which indicates that the values in that dataset are the least spread out relative to the mean of that particular dataset.
<em>You can find more Excel tutorials  here .</em>
<h2><span class="orange">What is Reliability Analysis? (Definition & Example)</span></h2>
In statistics, the term <b>reliability</b> refers to the consistency of a measure.
If we measure something like intelligence, knowledge, productivity, efficiency, etc. in individuals multiple times, are the measurements consistent?
Ideally, researchers want a test to have high reliability because that means it provides consistent measurements over time which means the results of the test can be trusted.
It turns out that there are four ways to measure reliability:
<b>1.  Split-Half Reliability Method </b> – Determines how much error in the test results is due to poor test construction -e.g. poorly worded questions or confusing instructions.
This method uses the following process:
Split a test into two halves. For example, one half may be composed of even-numbered questions while the other half is composed of odd-numbered questions.
Administer each half to the same individual.
Repeat for a large group of individuals.
Calculate the  correlation  between the scores for both halves.
The higher the correlation between the two halves, the higher the  internal consistency  of the test or survey. Ideally you would like the correlation between the halves to be high because this indicates that all parts of the test are contributing equally to what is being measured.
<b>2.  Test-Retest Reliability Method </b> – Determines how much error in the test results is due to administration problems – e.g. loud environment, poor lighting, insufficient time to complete test.
This method uses the following process:
Administer a test to a group of individuals.
Wait some amount of time (days, weeks, or months) and administer the same test to the same group of individuals.
Calculate the correlation between the scores of the two tests.
Generally a test-retest reliability correlation of at least 0.80 or higher indicates good reliability.
<b>3.  Parallel Forms Reliability Method </b> – Determines how much error in the test results is due to outside effects – e.g. students getting access to questions ahead of time or students getting better scores by simply practicing more.
This method uses the following process:
Administer one version of a test to a group of individuals.
Administer an alternate but equally difficult version of the test to the same group of individuals.
Calculate the correlation between the scores of the two tests.
<b>4.  Inter-rater Reliability Method </b> – Determines how consistently each item on a test measures the true construct being measured – e.g. are all questions clearly communicated and relevant to the construct being measured?
This method involves having multiple qualified raters or judges rate each item on a test and then calculating the overall percent agreement between raters or judges.
The higher the percent agreement between judges, the higher the reliability of the test.
<h3>Reliability vs. Validity</h3>
<b>Reliability</b> refers to the consistency of a measure and <b>validity</b> refers to the extent to which a test or scale measures the construct it sets out to measure.
A good test or scale is one that has both high reliability and high validity. However, it’s possible for a test or scale to have reliability without having validity.
For example, suppose a given scale that weighs boxes consistently weighs the boxes as 10 pounds over the true weight. This scale is reliable because it’s consistent in its measurements, but it’s not valid because it doesn’t measure the true value of the weight.
<h3>Reliability & Standard Error of Measurement</h3>
A reliability coefficient can also be used to calculate a <b>standard error of measurement</b>, which estimates the variation around a “true” score for an individual when repeated measures are taken.
It is calculated as:
<b>SE<sub>m</sub></b> = s√1-R
where:
<b>s:</b> The standard deviation of measurements
<b>R:</b> The reliability coefficient of a test
Refer to  this article  for an in-depth explanation of the standard error of measurement.
<h2><span class="orange">How to Get Remainder in Google Sheets Using MOD Function</span></h2>
You can use the <b>MOD</b> function in Google Sheets to calculate the remainder after a division operation.
This function uses the following basic syntax:
<b>=MOD(dividend, divisor)</b>
where:
<b>dividend</b>: The number to be divided
<b>divisor</b>: The number to divide by
The following examples show to use this function in different scenarios in Google Sheets.
<h3>Example 1: Using MOD Function with No Remainder</h3>
The following screenshot shows how to use the <b>MOD</b> function in a situation where there is no remainder:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/mod1.jpg"482">
From the output we can see that the remainder of 36 divided by 6 is <b>zero</b>.
This is because 6 divides into 36 exactly 6 times and no remainder is left.
<h3>Example 2: Using MOD Function with a Remainder</h3>
The following screenshot shows how to use the <b>MOD</b> function in a situation where there is a remainder:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/mod2.jpg"499">
From the output we can see that the remainder of 36 divided by 7 is <b>one</b>.
This is because 7 divides into 36 a total of 5 times and a remainder of 1 is left.
<h3>Example 3: Using MOD Function when Dividing by Zero</h3>
The following screenshot shows the result of using the <b>MOD</b> function when we attempt to divide by zero:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/mod3.jpg"504">
We receive a <b>#DIV/0!</b> error because it’s not possible to divide by zero.
If we’d like to suppress this error, we can instead use the following formula:
<b>=IFERROR(MOD(A2, B2), "")
</b>
The following screenshot shows how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/mod4.jpg"506">
Since we attempted to divide by zero, the formula simply returned a blank value as a result.
<b>Note</b>: You can find the complete online documentation for the <b>MOD </b>function  here .
<h2><span class="orange">How to Remove Axis Labels in ggplot2 (With Examples)</span></h2>
You can use the following basic syntax to remove axis labels in ggplot2:
<b>ggplot(df, aes(x=x, y=y))+
  geom_point() +
  theme(axis.text.x=element_blank(), #remove x axis labels
        axis.ticks.x=element_blank(), #remove x axis ticks
        axis.text.y=element_blank(),  #remove y axis labels
        axis.ticks.y=element_blank()  #remove y axis ticks
        )
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Remove X-Axis Labels</h3>
The following code shows how to remove x-axis labels from a scatterplot in ggplot2:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y=c(11, 13, 15, 14, 19, 22, 28, 25, 30, 29))
#create scatterplot
ggplot(df, aes(x=x, y=y))+
  geom_point() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank() 
        )</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/remove1.png">
Notice that only the x-axis labels (and tick marks) have been removed.
<h3>Example 2: Remove Y-Axis Labels</h3>
The following code shows how to remove y-axis labels from a scatterplot in ggplot2:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y=c(11, 13, 15, 14, 19, 22, 28, 25, 30, 29))
#create scatterplot
ggplot(df, aes(x=x, y=y))+
  geom_point() +
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank() 
        )</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/remove2.png">
Notice that just the y-axis labels (and tick marks) have been removed.
<h3>Example 3: Remove Both Axis Labels</h3>
The following code shows how to remove labels from both axes in a scatterplot in ggplot2:
<b>library(ggplot2)
#create data frame
df &lt;- data.frame(x=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), y=c(11, 13, 15, 14, 19, 22, 28, 25, 30, 29))
#create scatterplot
ggplot(df, aes(x=x, y=y))+
  geom_point() +
  theme(axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank() 
        )</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/remove3.png">
The labels and tick marks on both axes have been removed.
<h2><span class="orange">How to Remove Columns in R (With Examples)</span></h2>
Often you may want to remove one or more columns from a data frame in R. Fortunately this is easy to do using the  select()  function from the  dplyr  package.
<b>library(dplyr)</b>
This tutorial shows several examples of how to use this function in practice using the following data frame:
<b>#create data frame
df &lt;- data.frame(player = c('a', 'b', 'c', 'd', 'e'), position = c('G', 'F', 'F', 'G', 'G'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#view data frame
df
  player position points rebounds
1      a        G     12        5
2      b        F     15        7
3      c        F     19        7
4      d        G     22       12
5      e        G     32       11</b>
<h3>Example 1: Remove Columns by Name</h3>
The following code shows how to remove columns from a data frame by name:
<b>#remove column named 'points'
df %>% select(-points)
  player position rebounds
1      a        G        5
2      b        F        7
3      c        F        7
4      d        G       12
5      e        G       11
</b>
<h3>Example 2: Remove Columns in List</h3>
The following code shows how to remove columns from a data frame that are in a specific list:
<b>#remove columns named 'points' or 'rebounds'
df %>% select(-one_of('points', 'rebounds')) 
  player position
1      a        G
2      b        F
3      c        F
4      d        G
5      e        G</b>
<h3>Example 3: Remove Columns in Range</h3>
The following code shows how to remove all columns in the range from ‘position’ to ‘rebounds’:
<b>#remove columns in range from 'position' to 'rebounds'
df %>% select(-(position:rebounds)) 
  player
1      a
2      b
3      c
4      d
5      e</b>
<h3>Example 4: Remove Columns that Contain a Phrase</h3>
The following code shows how to remove all columns that contain the word ‘points’
<b>#remove columns that contain the word 'points'
df %>% select(-contains('points')) 
  player position rebounds
1      a        G        5
2      b        F        7
3      c        F        7
4      d        G       12
5      e        G       11</b>
<h3>Example 5: Remove Columns that Start with Certain Letters</h3>
The following code shows how to remove all columns that start with the letters ‘po’:
<b>#remove columns that start with 'po'
df %>% select(-starts_with('po')) 
  player rebounds
1      a        5
2      b        7
3      c        7
4      d       12
5      e       11</b>
<h3>Example 6: Remove Columns that End with Certain Letters</h3>
The following code shows how to remove all columns that end with the letter ‘s’:
<b>#remove columns that end with 's'
df %>% select(-ends_with('s')) 
  player position
1      a        G
2      b        F
3      c        F
4      d        G
5      e        G</b>
<h3>Example 7: Remove Columns by Position</h3>
The following code shows how to remove columns in specific positions:
<b>#remove columns in position 1 and 4
df %>% select(-1, -4) 
  position points
1        G     12
2        F     15
3        F     19
4        G     22
5        G     32</b>
<b>Note</b>: You can find the complete documentation for the <b>select()</b> function  here .
<h2><span class="orange">How to Remove Columns with NA Values in R</span></h2>
You can use one of the following two methods to remove columns from a data frame in R that contain NA values:
<b>Method 1: Use Base R</b>
<b>df[ , colSums(is.na(df))==0]
</b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
df %>% select_if(~ !any(is.na(.)))</b>
Both methods produce the same result.
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, NA, NA, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, NA))
#view data frame
df
  team points assists rebounds
1    A     99      33       30
2    B     NA      28       28
3    C     NA      31       24
4    D     88      39       24
5    E     95      34       NA
</b>
<h2>Example 1: Remove Columns with NA Values Using Base R</h2>
The following code shows how to remove columns with NA values using functions from base R:
<b>#define new data frame
new_df &lt;- df[ , colSums(is.na(df))==0]
#view new data frame
new_df
  team assists
1    A      33
2    B      28
3    C      31
4    D      39
5    E      34</b>
Notice that the two columns with NA values (points and rebounds) have both been removed from the data frame.
<h2>Example 2: Remove Columns with NA Values Using dplyr</h2>
The following code shows how to remove columns with NA values using functions from the  dplyr  package:
<b>library(dplyr)
#define new data frame
new_df &lt;- df %>% select_if(~ !any(is.na(.)))
#view new data frame
new_df
  team assists
1    A      33
2    B      28
3    C      31
4    D      39
5    E      34
</b>
Once again, the two columns with NA values (points and rebounds) have both been removed from the data frame.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Add a Column to a Data Frame in R 
 How to Rename Data Frame Columns in R 
 How to Sort a Data Frame by Column in R 
<h2><span class="orange">How to Remove Dollar Signs in R (With Examples)</span></h2>
You can easily remove dollar signs and commas from data frame columns in R by using <b>gsub() </b>function. This tutorial shows three examples of using this function in practice.
<h3>Remove Dollar Signs in R</h3>
The following code shows how to remove dollar signs from a particular column in a data frame in R:
<b>#create data frame 
df1 &lt;- data.frame(ID=1:5, sales=c('$14.45', '$13.39', '$17.89', '$18.99', '$20.88'), stringsAsFactors=FALSE)
df1
  ID  sales
1  1 $14.45
2  2 $13.39
3  3 $17.89
4  4 $18.99
5  5 $20.88
#remove dollar signs from <em>sales </em>column
df1$sales = as.numeric(gsub("\\$", "", df1$sales))
df1
  ID sales
1  1 14.45
2  2 13.39
3  3 17.89
4  4 18.99
5  5 20.88</b>
<h3>Remove Dollar Signs & Commas in R</h3>
The following code shows how to remove both dollar signs and columns from a particular column in a data frame in R:
<b>#create data frame 
df2 &lt;- data.frame(ID=1:3, sales=c('$14,000', '$13,300', '$17,890'), stringsAsFactors=FALSE)
df2
  ID   sales
1  1 $14,000
2  2 $13,300
3  3 $17,890
#remove dollar signs and commas from <em>sales </em>column
df2$sales = as.numeric(gsub("[\\$,]", "", df2$sales))
df2
  ID sales
1  1 14000
2  2 13300
3  3 17890</b>
Note that you can now perform calculations on the sales column since the dollar signs and commas are removed.
For example, we can now calculate the sum of the sales column:
<b>#calculate sum of sales
sum(df2$sales)
[1] 45190
</b>
<h2><span class="orange">How to Remove Duplicate Rows in R (With Examples)</span></h2>
You can use one of the following two methods to remove duplicate rows from a data frame in R:
<b>Method 1: Use Base R</b>
<b>#remove duplicate rows across entire data frame
df[!duplicated(df), ]
#remove duplicate rows across specific columns of data frame
df[!duplicated(df[c('var1')]), ]
</b>
<b>Method 2: Use dplyr</b>
<b>#remove duplicate rows across entire data frame 
df %>%
  distinct(.keep_all = TRUE)
#remove duplicate rows across specific columns of data frame
df %>%
  distinct(var1, .keep_all = TRUE)
</b>
The following examples show how to use this syntax in practice with the following data frame:
<b>#define data frame
df &lt;- data.frame(team=c('A', 'A', 'A', 'B', 'B', 'B'), position=c('Guard', 'Guard', 'Forward', 'Guard', 'Center', 'Center'))
#view data frame
df
  team position
1    A    Guard
2    A    Guard
3    A  Forward
4    B    Guard
5    B   Center
6    B   Center
</b>
<h3>Example 1: Remove Duplicate Rows Using Base R</h3>
The following code shows how to remove duplicate rows from a data frame using functions from base R:
<b>#remove duplicate rows from data frame
df[!duplicated(df), ]
  team position
1    A    Guard
3    A  Forward
4    B    Guard
5    B   Center
</b>
The following code shows how to remove duplicate rows from specific columns of a data frame using base R:
<b>#remove rows where there are duplicates in the 'team' column
df[!duplicated(df[c('team')]), ]
  team position
1    A    Guard
4    B    Guard
</b>
<h3>Example 2: Remove Duplicate Rows Using dplyr</h3>
The following code shows how to remove duplicate rows from a data frame using the <b>distinct()</b> function from the  dplyr  package:
<b>library(dplyr)
#remove duplicate rows from data frame
df %>%
  distinct(.keep_all = TRUE)
  team position
1    A    Guard
2    A  Forward
3    B    Guard
4    B   Center</b>
Note that the <b>.keep_all</b> argument tells R to keep all of the columns from the original data frame.
The following code shows how to use the <b>distinct()</b> function to remove duplicate rows from specific columns of a data frame:
<b>library(dplyr)
#remove duplicate rows from data frame
df %>%
  distinct(team, .keep_all = TRUE)
  team position
1    A    Guard
2    B    Guard
</b>
<h2><span class="orange">How to Remove Specific Elements from Vector in R</span></h2>
You can use the following basic syntax to remove specific elements from a vector in R:
<b>#remove 'a', 'b', 'c' from my_vector
my_vector[! my_vector %in% c('a', 'b, 'c')]
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Remove Elements from Character Vector </h3>
The following code shows how to remove elements from a character vector in R:
<b>#define vector
x &lt;- c('Mavs', 'Nets', 'Hawks', 'Bucks', 'Spurs', 'Suns')
#remove 'Mavs' and 'Spurs' from vector
x &lt;- x[! x %in% c('Mavs', 'Spurs')]
#view updated vector
x
[1] "Nets"  "Hawks" "Bucks" "Suns" 
</b>
Notice that both ‘Mavs’ and ‘Spurs’ were removed from the vector.
<h3>Example 2: Remove Elements from Numeric Vector </h3>
The following code shows how to remove elements from a numeric vector in R:
<b>#define numeric vector
x &lt;- c(1, 2, 2, 2, 3, 4, 5, 5, 7, 7, 8, 9, 12, 12, 13)
#remove 1, 4, and 5
x &lt;- x[! x %in% c(1, 4, 5)]
#view updated vector
x
[1]  2  2  2  3  7  7  8  9 12 12 13
</b>
Notice that every occurrence of the values 1, 4, and 5 were removed from the vector.
We can also specify a range of values that we’d like to remove from the numeric vector:
<b>#define numeric vector
x &lt;- c(1, 2, 2, 2, 3, 4, 5, 5, 7, 7, 8, 9, 12, 12, 13)
#remove values between 2 and 10
x &lt;- x[! x %in% 2:10]
#view updated vector
x
[1]  1 12 12 13</b>
Notice that every value between 2 and 10 was removed from the vector.
We can also remove values greater or less than a specific number:
<b>#define numeric vector
x &lt;- c(1, 2, 2, 2, 3, 4, 5, 5, 7, 7, 8, 9, 12, 12, 13)
#remove values less than 3 <em>or </em>greater than 10
x &lt;- x[!(x &lt; 3 | x > 10)]
#view updated vector
x
[1] 3 4 5 5 7 7 8 9
</b>
<h2><span class="orange">How to Remove Empty Rows from Data Frame in R</span></h2>
You can use the following methods to remove empty rows from a data frame in R:
<b>Method 1: Remove Rows with NA in All Columns</b>
<b>df[rowSums(is.na(df)) != ncol(df), ]
</b>
<b>Method 2: Remove Rows with NA in At Least One Column</b>
<b>df[complete.cases(df), ]</b>
The following examples show how to use each method in practice.
<h2>Example 1: Remove Rows with NA in All Columns</h2>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(x=c(3, 4, NA, 6, 8, NA), y=c(NA, 5, NA, 2, 2, 5), z=c(1, 2, NA, 6, 8, NA))
#view data frame
df
   x  y  z
1  3 NA  1
2  4  5  2
3 NA NA NA
4  6  2  6
5  8  2  8
6 NA  5 NA</b>
We can use the following code to remove rows from the data frame that have NA values in every column:
<b>#remove rows with NA in all columns
df[rowSums(is.na(df)) != ncol(df), ]
   x  y  z
1  3 NA  1
2  4  5  2
4  6  2  6
5  8  2  8
6 NA  5 NA
</b>
Notice that the one row with NA values in every column has been removed.
<h2>Example 2: Remove Rows with NA in At Least One Column</h2>
Once again suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(x=c(3, 4, NA, 6, 8, NA), y=c(NA, 5, NA, 2, 2, 5), z=c(1, 2, NA, 6, 8, NA))
#view data frame
df
   x  y  z
1  3 NA  1
2  4  5  2
3 NA NA NA
4  6  2  6
5  8  2  8
6 NA  5 NA</b>
We can use the following code to remove rows from the data frame that have NA values in at least one column:
<b>#remove rows with NA in at least one column
df[complete.cases(df), ]
  x y z
2 4 5 2
4 6 2 6
5 8 2 8
</b>
Notice that all rows with an NA value in at least one column have been removed.
<b>Related:</b>  How to Use complete.cases in R (With Examples) 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Create an Empty Data Frame in R 
 How to Create an Empty List in R 
 How to Create an Empty Vector in R 
<h2><span class="orange">How to Remove First Row from Data Frame in R (2 Examples)</span></h2>
You can use one of the following methods to remove the first row from a data frame in R:
<b>Method 1: Use Base R</b>
<b>df &lt;- df[-1, ]
</b>
<b>Method 2: Use dplyr package</b>
<b>library(dplyr)
df &lt;- df %>% slice(-1)
</b>
The following examples show how to use each method in practice.
<h3>Example 1: Remove First Row Using Base R</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c(NA, 'A', 'B', 'C', 'D', 'E'), points=c(NA, 99, 90, 86, 88, 95), assists=c(NA, 33, 28, 31, 39, 34), rebounds=c(NA, 30, 28, 24, 24, 28))
#view data frame
df
  team points assists rebounds
1 &lt;NA>     NA      NA       NA
2    A     99      33       30
3    B     90      28       28
4    C     86      31       24
5    D     88      39       24
6    E     95      34       28</b>
We can use the following code to remove the first row from the data frame:
<b>#remove first row
df &lt;- df[-1, ]
#view updated data frame
df
  team points assists rebounds
2    A     99      33       30
3    B     90      28       28
4    C     86      31       24
5    D     88      39       24
6    E     95      34       28
</b>
Notice that the first row has been removed.
Also notice that the row names now start at 2.
To reset the row names to start at 1, simply use the following code:
<b>#reset row names
rownames(df) &lt;- NULL
#view updated data frame
df
  team points assists rebounds
1    A     99      33       30
2    B     90      28       28
3    C     86      31       24
4    D     88      39       24
5    E     95      34       28</b>
<h3>Example 2: Remove First Row Using dplyr Package</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c(NA, 'A', 'B', 'C', 'D', 'E'), points=c(NA, 99, 90, 86, 88, 95), assists=c(NA, 33, 28, 31, 39, 34), rebounds=c(NA, 30, 28, 24, 24, 28))
#view data frame
df
  team points assists rebounds
1 &lt;NA>     NA      NA       NA
2    A     99      33       30
3    B     90      28       28
4    C     86      31       24
5    D     88      39       24
6    E     95      34       28</b>
We can use the  slice()  function from the <b>dplyr</b> package to remove the first row from the data frame:
<b>library(dplyr)
#remove first row from data frame
df &lt;- df %>% slice(-1)
#view updated data frame
df
  team points assists rebounds
1    A     99      33       30
2    B     90      28       28
3    C     86      31       24
4    D     88      39       24
5    E     95      34       28
</b>
Notice that the first row has been removed.
The nice thing about using this approach is that the row numbers are automatically reset after removing the first row.
<h2><span class="orange">How to Remove a Legend in ggplot2 (With Examples)</span></h2>
You can use the following syntax to remove a legend from a plot in ggplot2:
<b>ggplot(df, aes(x=x, y=y, color=z)) +
  geom_point() +
  theme(legend.position="none")
</b>
By specifying <b>legend.position=”none”</b> you’re telling ggplot2 to remove all legends from the plot.
The following step-by-step example shows how to use this syntax in practice.
<h3>Step 1: Create the Data Frame</h3>
First, let’s create a data frame:
<b>#create data frame
df &lt;- data.frame(assists=c(3, 4, 4, 3, 1, 5, 6, 7, 9), points=c(14, 8, 8, 16, 3, 7, 17, 22, 26), position=rep(c('Guard', 'Forward', 'Center'), times=3))
#view data frame
df
  assists points position
1       3     14    Guard
2       4      8  Forward
3       4      8   Center
4       3     16    Guard
5       1      3  Forward
6       5      7   Center
7       6     17    Guard
8       7     22  Forward
9       9     26   Center
</b>
<h3>Step 2: Create a Plot Using ggplot2</h3>
Next, let’s use ggplot2 to create a simple scatterplot:
<b>library(ggplot2)
#create scatterplot
ggplot(df, aes(x=assists, y=points, color=position)) +
  geom_point(size=3)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/removeLegend1.png">
By default, ggplot2 includes a legend so that it’s easier to interpret the colors in the scatterplot.
<h3>Step 3: Remove the Legend from the Plot</h3>
Next, let’s use <b>legend.position=”none”</b> to remove the legend from the plot:
<b>library(ggplot2)
#create scatterplot with no legend
ggplot(df, aes(x=assists, y=points, color=position)) +
  geom_point(size=3) +
  theme(legend.position="none")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/removeLegend2.png">
The legend has been completely removed from the plot.
<b>Related:</b>  How to Change Legend Labels in ggplot2 
<h2><span class="orange">How to Remove a Legend in Matplotlib (With Examples)</span></h2>
You can use the following basic syntax to remove a legend from a plot in Matplotlib:
<b>import matplotlib.pyplot as plt
plt.legend('', frameon=False)</b>
The quotation marks <b>‘ ‘</b> tell Matplotlib to place no variables in the legend and the <b>frameon</b> argument tells Matplotlib to remove the frame around the legend.
The following example shows how to use this syntax in practice.
<h2>Example: Remove a Legend in Matplotlib</h2>
Suppose we have the following pandas DataFrame that contains information about various basketball players:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'team': ['A', 'A', 'A', 'A', 'B', 'B', 'B', 'B'],   'position': ['G', 'G', 'F', 'F', 'G', 'F', 'F', 'F'],   'points': [5, 7, 7, 9, 12, 9, 9, 4]})
#view DataFrame
print(df)
  team position  points
0    A        G       5
1    A        G       7
2    A        F       7
3    A        F       9
4    B        G      12
5    B        F       9
6    B        F       9
7    B        F       4</b>
We can use the following code to create a stacked bar chart to visualize the total points scored by players on each team and each position:
<b>import matplotlib.pyplot as plt
#create stacked bar chart
df.groupby(['team', 'position']).size().unstack().plot(kind='bar', stacked=True)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/nolegend1.jpg"534">
Notice that Matplotlib places a legend in the top right corner by default.
To create this same plot without the legend, we can use the following code:
<b>import matplotlib.pyplot as plt
#create stacked bar chart
df.groupby(['team', 'position']).size().unstack().plot(kind='bar', stacked=True)
#remove legend
plt.legend('', frameon=False)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/nolegend2.jpg">
This stacked bar chart matches the previous chart, but the legend has been removed.
Also note that we can use the same syntax to remove a legend from any plot in Matplotlib.
For example, the following code shows how to create a pie chart in Matplotlib and remove the legend:
<b>import matplotlib.pyplot as plt
#create pie chart that shows total points scored by team
df.groupby(['team']).sum().plot(kind='pie', y='points')
#remove legend
plt.legend('', frameon=False)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/08/nolegend3.jpg"345">
The result is a pie chart with no legend.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in Matplotlib:
 How to Remove Ticks from Matplotlib Plots 
 How to Change Font Sizes on a Matplotlib Plot 
 How to Add an Average Line to Plot in Matplotlib 
<h2><span class="orange">How to Remove a Legend Title in ggplot2</span></h2>
You can use the following syntax to remove a legend title from a plot in ggplot2:
<b>ggplot(df, aes(x=x_var, y=y_var, color=group_var)) +
  geom_point() +
  labs(color=NULL)
</b>
The argument <b>color=NULL</b> in the <b>labs()</b> function tells ggplot2 not to display any legend title.
The following example shows how to use this syntax in practice.
<h2>Example: Remove Legend Title from Plot in ggplot2</h2>
Suppose we have the following data frame in R that contains information about various basketball players:
<b>df &lt;- data.frame(assists=c(3, 4, 4, 3, 1, 5, 6, 7, 9), points=c(14, 8, 8, 16, 3, 7, 17, 22, 26), position=rep(c('Guard', 'Forward', 'Center'), times=3))
#view data frame
df
  assists points position
1       3     14    Guard
2       4      8  Forward
3       4      8   Center
4       3     16    Guard
5       1      3  Forward
6       5      7   Center
7       6     17    Guard
8       7     22  Forward
9       9     26   Center
</b>
If we use <b>geom_point()</b> to create a scatterplot in ggplot2, a legend will be shown with a title by default:
<b>library(ggplot2)
#create scatter plot of assists vs. points, grouped by position
ggplot(df, aes(x=assists, y=points, color=position)) +
  geom_point(size=3)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/ggtitle1.jpg"543">
Notice that the legend currently has the text “position” shown as the legend title.
To remove this title from the legend, we can use the<b> labs(color=NULL)</b> argument:
<b>library(ggplot2)
#create scatter plot and remove legend title
ggplot(df, aes(x=assists, y=points, color=position)) +
  geom_point(size=3) +
  labs(color=NULL)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/12/ggtitle2.jpg"564">
Notice that the legend title has been removed.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Change the Legend Title in ggplot2 
 How to Change Legend Size in ggplot2 
 How to Change Legend Position in ggplot2 
<h2><span class="orange">How to Remove Multiple Rows in R (With Examples)</span></h2>
You can use one of the following methods to remove multiple rows from a data frame in R:
<b>Method 1: Remove Specific Rows</b>
<b>#remove rows 2, 3, and 4
new_df &lt;- df[-c(2, 3, 4), ]
</b>
<b>Method 2: Remove Range of Rows</b>
<b>#remove rows 2 through 5
new_df &lt;- df[-c(2:5), ]</b>
<b>Method 3: Remove Last N Rows</b>
<b>#remove rows 4 through last row
new_df &lt;- df[-c(4:nrow(df)), ]</b>
The following examples show how to use each of these methods in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F'), points=c(99, 90, 86, 88, 95, 99), assists=c(33, 28, 31, 39, 34, 24))
#view data frame
df
  team points assists
1    A     99      33
2    B     90      28
3    C     86      31
4    D     88      39
5    E     95      34
6    F     99      24
</b>
<h3>Example 1: Remove Specific Rows</h3>
The following code shows how to remove rows 2, 3, and 4 from the data frame:
<b>#define new data frame with rows 2, 3, 4 removed
new_df &lt;- df[-c(2, 3, 4),]
#view new data frame
new_df
  team points assists
1    A     99      33
5    E     95      34
6    F     99      24</b>
Notice that rows 2, 3, and 4 have all been removed from the data frame.
<h3>Example 2: Remove Range of Rows</h3>
The following code shows how to remove rows in the range of 2 through 5:
<b>#define new data frame with rows 2 through 5 removed
new_df &lt;- df[-c(2:5),]
#view new data frame
new_df
  team points assists
1    A     99      33
6    F     99      24
</b>
Notice that rows 2, 3, 4, and 5 have been removed.
<h3>Example 3: Remove Last N Rows</h3>
The following code shows how to remove rows 4 through the last row:
<b>#remove rows 4 through last row
new_df &lt;- df[-c(4:nrow(df)), ]
#view new data frame
new_df
  team points assists
1    A     99      33
2    B     90      28
3    C     86      31
</b>
Notice that row 4 and all rows after it have been removed.
<h2><span class="orange">How to Remove NA from Matrix in R (With Example)</span></h2>
You can use the following methods to remove NA values from a matrix in R:
<b>Method 1: Remove Rows with NA Values</b>
<b>new_matrix &lt;- my_matrix[!rowSums(is.na(my_matrix)),]
</b>
<b>Method 2: Remove Columns with NA Values</b>
<b>new_matrix &lt;- my_matrix[, !colSums(is.na(my_matrix))]</b>
The following examples show how to use each method in practice with the following matrix in R:
<b>#create matrix
my_matrix &lt;- matrix(c(NA, 0, NA, 5, 7, 4, 1, 3, 9, 5, 5, 8), nrow=4)
#view matrix
my_matrix
     [,1] [,2] [,3]
[1,]   NA    7    9
[2,]    0    4    5
[3,]   NA    1    5
[4,]    5    3    8
</b>
<h2>Method 1: Remove Rows with NA Values</h2>
The following code shows how to remove all rows from the matrix that contain NA values:
<b>#remove all rows with NA values
new_matrix &lt;- my_matrix[!rowSums(is.na(my_matrix)),]
#view updated matrix
new_matrix
     [,1] [,2] [,3]
[1,]    0    4    5
[2,]    5    3    8</b>
Notice that all rows with NA values have been removed from the matrix.
<b>Related:</b>  How to Use rowSums() Function in R 
<h2>Method 2: Remove Columns with NA Values</h2>
The following code shows how to remove all columns from the matrix that contain NA values:
<b>#remove all columns with NA values
new_matrix &lt;- my_matrix[, !colSums(is.na(my_matrix))]
#view updated matrix
new_matrix
     [,1] [,2]
[1,]    7    9
[2,]    4    5
[3,]    1    5
[4,]    3    8</b>
Notice that all columns with NA values have been removed from the matrix.
<b>Related:</b>  How to Use colSums() Function in R 
<h2>Bonus: Convert NA Values to Zero in Matrix</h2>
If you simply want to convert all NA values to zero in a matrix, you can use the following syntax:
<b>#remove all columns with NA values
my_matrix[is.na(my_matrix)] &lt;- 0
#view updated matrix
my_matrix
     [,1] [,2] [,3]
[1,]    0    7    9
[2,]    0    4    5
[3,]    0    1    5
[4,]    5    3    8</b>
Notice that all NA values have been converted to zero.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations with missing values in R:
 How to Remove NA Values from Vector in R 
 How to Find and Count Missing Values in R 
 How to Impute Missing Values in R 
<h2><span class="orange">How to Remove NA Values from Vector in R (3 Methods)</span></h2>
You can use one of the following methods to remove NA values from a vector in R:
<b>Method 1: Remove NA Values from Vector</b>
<b>data &lt;- data[!is.na(data)]
</b>
<b>Method 2: Remove NA Values When Performing Calculation Using na.rm</b>
<b>max(data, na.rm=T)
mean(data, na.rm=T)
...</b>
<b>Method 3: Remove NA Values When Performing Calculation Using na.omit</b>
<b>max(na.omit(data))</b>
<b>mean(na.omit(data))
...</b>
The following example shows how to use each of these methods in practice.
<h3>Method 1: Remove NA Values from Vector</h3>
The following code shows how to remove NA values from a vector in R:
<b>#create vector with some NA values
data &lt;- c(1, 4, NA, 5, NA, 7, 14, 19)
#remove NA values from vector
data &lt;- data[!is.na(data)]
#view updated vector
data
[1]  1  4  5  7 14 19
</b>
Notice that each of the NA values in the original vector have been removed.
<h3>Method 2: Remove NA Values When Performing Calculation Using na.rm</h3>
The following code shows how to use the <b>na.rm</b> argument to remove NA values from a vector when performing some calculation:
<b>#create vector with some NA values
data &lt;- c(1, 4, NA, 5, NA, 7, 14, 19)
#calculate max value and remove NA values
max(data, na.rm=T)
[1] 19
#calculate mean and remove NA values
mean(data, na.rm=T)
[1] 8.333333
#calculate median and remove NA values
median(data, na.rm=T)
[1] 6</b>
<h3>Method 3: Remove NA Values When Performing Calculation Using na.omit</h3>
The following code shows how to use the <b>na.omit</b> argument to omit NA values from a vector when performing some calculation:
<b>#create vector with some NA values
data &lt;- c(1, 4, NA, 5, NA, 7, 14, 19)
#calculate max value and omit NA values
max(na.omit(data))
[1] 19
#calculate mean and omit NA values
mean(na.omit(data))
[1] 8.333333
#calculate median and omit NA values
median(na.omit(data))
[1] 6</b>
<h2><span class="orange">How to Remove Rows with Some or All NAs in R</span></h2>
Often you may want to remove rows with all or some NAs (missing values) in a data frame in R.
This tutorial explains how to remove these rows using base R and the  tidyr  package. We’ll use the following data frame for each of the following examples:
<b>#create data frame with some missing values
df &lt;- data.frame(points = c(12, NA, 19, 22, 32), assists = c(4, NA, 3, NA, 5), rebounds = c(5, NA, 7, 12, NA))
#view data frame
df
  points assists rebounds
1     12       4        5
2     NA      NA       NA
3     19       3        7
4     22      NA       12
5     32       5       NA
</b>
<h3>Remove NAs Using Base R</h3>
The following code shows how to use <b>complete.cases() </b>to remove all rows in a data frame that have a missing value in <em>any </em>column:
<b>#remove all rows with a missing value in <em>any </em>column
df[complete.cases(df), ]
  points assists rebounds
1     12       4        5
3     19       3        7
</b>
The following code shows how to use <b>complete.cases() </b>to remove all rows in a data frame that have a missing value in <em>specific </em>columns:
<b>#remove all rows with a missing value in the <em>third </em>column
df[complete.cases(df[ , 3]),]
  points assists rebounds
1     12       4        5
3     19       3        7
4     22      NA       12
#remove all rows with a missing value in either the <em>first </em>or <em>third </em>column
df[complete.cases(df[ , c(1,3)]),]
  points assists rebounds
1     12       4        5
3     19       3        7
4     22      NA       12
</b>
<h3>Remove NAs Using Tidyr</h3>
The following code shows how to use <b>drop_na() </b>from the tidyr package to remove all rows in a data frame that have a missing value in <em>any </em>column:
<b>#load tidyr package
library(tidyr)
#remove all rows with a missing value in <em>any </em>column
df %>% drop_na()
  points assists rebounds
1     12       4        5
3     19       3        7</b>
The following code shows how to use <b>drop_na() </b>from the tidyr package to remove all rows in a data frame that have a missing value in <em>specific </em>columns:
<b>#load tidyr package
library(tidyr)
#remove all rows with a missing value in the <em>third </em>column
df %>% drop_na(rebounds)
  points assists rebounds
1     12       4        5
3     19       3        7
4     22      NA       12</b>
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">How to Remove Outliers from Multiple Columns in R</span></h2>
Often you may want to remove outliers from multiple columns at once in R.
One common way to define an observation as an outlier is if it is 1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).
Using this definition, we can use the following steps to create a simple function to identify outliers and then apply this function across multiple columns in an R data frame.
<b>Step 1: Create data frame.</b>
First, let’s create a data frame in R:
<b>df &lt;- data.frame(index=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), var1=c(4, 4, 5, 4, 3, 2, 8, 9, 4, 5), var2=c(1, 2, 4, 4, 6, 9, 7, 8, 5, 29), var3=c(9, 9, 9, 5, 5, 3, 4, 5, 11, 34))
</b>
<b>Step 2: Define outlier function.</b>
Next, let’s define a function that can identify outliers and a function that can then remove outliers:
<b>outliers &lt;- function(x) {
  Q1 &lt;- quantile(x, probs=.25)
  Q3 &lt;- quantile(x, probs=.75)
  iqr = Q3-Q1
 upper_limit = Q3 + (iqr*1.5)
 lower_limit = Q1 - (iqr*1.5)
 x > upper_limit | x &lt; lower_limit
}
remove_outliers &lt;- function(df, cols = names(df)) {
  for (col in cols) {
    df &lt;- df[!outliers(df[[col]]),]
  }
  df
}</b>
<b>Step 3: Apply outlier function to data frame.</b>
Lastly, let’s apply this function across multiple columns of the data frame to remove outliers:
<b>remove_outliers(df, c('var1', 'var2', 'var3'))
  index var1 var2 var3
1     1    4    1    9
2     2    4    2    9
3     3    5    4    9
4     4    4    4    5
5     5    3    6    5
9     9    4    5   11
</b>
<em>You can find more R tutorials  here .</em>
<h2><span class="orange">How to Remove Outliers in Boxplots in R</span></h2>
Occasionally you may want to remove outliers from boxplots in R.
This tutorial explains how to do so using both base R and  ggplot2 .
<h2>Remove Outliers in Boxplots in Base R</h2>
Suppose we have the following dataset:
<b>data &lt;- c(5, 8, 8, 12, 14, 15, 16, 19, 20, 22, 24, 25, 25, 26, 30, 48)</b>
The following code shows how to create a boxplot for this dataset in base R:
<b>boxplot(data)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/boxplotR1.png">
To remove the outliers, you can use the argument <b>outline=FALSE</b>:
<b>boxplot(data, outline=FALSE)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/boxplotR2.png">
<h2>Remove Outliers in Boxplots in ggplot2</h2>
Suppose we have the following dataset:
<b>data &lt;- data.frame(y=c(5, 8, 8, 12, 14, 15, 16, 19, 20, 22, 24, 25, 25, 26, 30, 48))</b>
The following code shows how to create a boxplot using the ggplot2 visualization library:
<b>library(ggplot2)
ggplot(data, aes(y=y)) +
  geom_boxplot()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/boxplotR3.png">
To remove the outliers, you can use the argument <b>outlier.shape=NA</b>:
<b>ggplot(data, aes(y=y)) +
  geom_boxplot(outlier.shape = NA)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/boxplotR4.png">
Notice that ggplot2 does not automatically adjust the y-axis.
To adjust the y-axis, you can use <b>coord_cartesian</b>:
<b>ggplot(data, aes(y=y)) +
  geom_boxplot(outlier.shape = NA) +
  coord_cartesian(ylim=c(5, 30))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/boxplotR5.png">
The y-axis now ranges from 5 to 30, just as we specified using the <b>ylim()</b> argument.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in ggplot2:
 How to Set Axis Limits in ggplot2 
 How to Create Side-by-Side Plots in ggplot2 
 How to Label Outliers in Boxplots in ggplot2 
<h2><span class="orange">How to Remove Outliers in Python</span></h2>
An <b>outlier </b>is an observation that lies abnormally far away from other values in a dataset. Outliers can be problematic because they can affect the results of an analysis.
This tutorial explains how to identify and remove outliers in Python.
<h3>How to Identify Outliers in Python</h3>
Before you can remove outliers, you must first decide on what you consider to be an outlier. There are two common ways to do so:
<b>1. Use the interquartile range.</b>
The interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) in a dataset. It measures the spread of the middle 50% of values.
You could define an observation to be an outlier if it is 1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).
<b>Outliers = Observations > Q3 + 1.5*IQR  or  Q1 – 1.5*IQR</b>
<b>2. Use z-scores.</b>
A  z-score  tells you how many standard deviations a given value is from the mean. We use the following formula to calculate a z-score:
<b>z</b> = (X – μ) / σ
where:
X is a single raw data value
μ is the population mean
σ is the population standard deviation
You could define an observation to be an outlier if it has a z-score less than -3 or greater than 3.
<b>Outliers = Observations with z-scores > 3 or &lt; -3</b>
<h3>How to Remove Outliers in Python</h3>
Once you decide on what you consider to be an outlier, you can then identify and remove them from a dataset. To illustrate how to do so, we’ll use the following pandas DataFrame:
<b>import numpy as np
import pandas as pd 
import scipy.stats as stats
#create dataframe with three columns 'A', 'B', 'C'
np.random.seed(10)
data = pd.DataFrame(np.random.randint(0, 10, size=(100, 3)), columns=['A', 'B', 'C'])
#view first 10 rows 
data[:10]
           A          B          C
0  13.315865   7.152790 -15.454003
1  -0.083838   6.213360  -7.200856
2   2.655116   1.085485   0.042914
3  -1.746002   4.330262  12.030374
4  -9.650657  10.282741   2.286301
5   4.451376 -11.366022   1.351369
6  14.845370 -10.798049 -19.777283
7 -17.433723   2.660702  23.849673
8  11.236913  16.726222   0.991492
9  13.979964  -2.712480   6.132042
</b>
We can then define and remove outliers using the z-score method or the interquartile range method:
<b>Z-score method:</b>
<b>#find absolute value of z-score for each observation
z = np.abs(stats.zscore(data))
#only keep rows in dataframe with all z-scores less than absolute value of 3 
data_clean = data[(z&lt;3).all(axis=1)]
#find how many rows are left in the dataframe 
data_clean.shape
(99,3)
</b>
<b>Interquartile range method:</b>
<b>#find Q1, Q3, and interquartile range for each column
Q1 = data.quantile(q=.25)
Q3 = data.quantile(q=.75)
IQR = data.apply(stats.iqr)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
data_clean = data[~((data &lt; (Q1-1.5*IQR)) | (data > (Q3+1.5*IQR))).any(axis=1)]
#find how many rows are left in the dataframe 
data_clean.shape
(89,3)</b>
We can see that the z-score method identified and removed one observation as an outlier, while the interquartile range method identified and removed 11 total observations as outliers.
<h3>When to Remove Outliers</h3>
If one or more outliers are present in your data, you should first make sure that they’re not a result of data entry error. Sometimes an individual simply enters the wrong data value when recording data.
If the outlier turns out to be a result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<h2><span class="orange">How to Remove Outliers in R</span></h2>
An <b>outlier </b>is an observation that lies abnormally far away from other values in a dataset. Outliers can be problematic because they can affect the results of an analysis.
This tutorial explains how to identify and remove outliers in R.
<h3>How to Identify Outliers in R</h3>
Before you can remove outliers, you must first decide on what you consider to be an outlier. There are two common ways to do so:
<b>1. Use the interquartile range.</b>
The interquartile range (IQR) is the difference between the 75th percentile (Q3) and the 25th percentile (Q1) in a dataset. It measures the spread of the middle 50% of values.
You could define an observation to be an outlier if it is 1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).
<b>Outliers = Observations > Q3 + 1.5*IQR  or &lt; Q1 – 1.5*IQR</b>
<b>2. Use z-scores.</b>
A  z-score  tells you how many standard deviations a given value is from the mean. We use the following formula to calculate a z-score:
<b>z</b> = (X – μ) / σ
where:
X is a single raw data value
μ is the population mean
σ is the population standard deviation
You could define an observation to be an outlier if it has a z-score less than -3 or greater than 3.
<b>Outliers = Observations with z-scores > 3 or &lt; -3</b>
<h3>How to Remove Outliers in R</h3>
Once you decide on what you consider to be an outlier, you can then identify and remove them from a dataset. To illustrate how to do so, we’ll use the following data frame:
<b>#make this example reproducible 
set.seed(0)
#create data frame with three columns A', 'B', 'C' 
df &lt;- data.frame(A=rnorm(1000, mean=10, sd=3), B=rnorm(1000, mean=20, sd=3), C=rnorm(1000, mean=30, sd=3))
#view first six rows of data frame
head(df)
         A        B        C
1 13.78886 19.13945 31.33304
2  9.02130 25.52332 30.03579
3 13.98940 19.52971 29.97216
4 13.81729 15.83059 29.09287
5 11.24392 15.58069 31.47707
6  5.38015 19.79144 28.19184
</b>
We can then define and remove outliers using the z-score method or the interquartile range method:
<b>Z-score method:</b>
The following code shows how to calculate the z-score of each value in each column in the data frame, then remove rows that have at least one z-score with an absolute value greater than 3:
<b>#find absolute value of z-score for each value in each column
z_scores &lt;- as.data.frame(sapply(df, function(df) (abs(df-mean(df))/sd(df))))
#view first six rows of z_scores data frame
head(z_scores)
          A          B          C
1 1.2813403 0.25350805 0.39419878
2 0.3110243 1.80496734 0.05890232
3 1.3483190 0.12766847 0.08112630
4 1.2908343 1.32044506 0.38824414
5 0.4313316 1.40102642 0.44450451
6 1.5271674 0.04327186 0.70295309
#only keep rows in dataframe with all z-scores less than absolute value of 3 
no_outliers &lt;- z_scores[!rowSums(z_scores>3), ]
#view row and column count of new data frame
dim(no_outliers)
[1] 994    3
</b>
The original data frame had 1,000 rows and 3 columns. The new data frame has 994 rows and 3 columns, which tells us that 6 rows were removed because they had at least one z-score with an absolute value greater than 3 in one of their columns.
<b>Interquartile range method:</b>
In some cases we may only be interested in identifying outliers in one column of a data frame. For example, suppose we only want to remove rows that have an outlier in column ‘A’ of our data frame.
The following code shows how to remove rows from the data frame that have a value in column ‘A’ that is 1.5 times the interquartile range greater than the third quartile (Q3) or 1.5 times the interquartile range less than the first quartile (Q1).
<b>#find Q1, Q3, and interquartile range for values in column A
Q1 &lt;- quantile(df$A, .25)
Q3 &lt;- quantile(df$A, .75)
IQR &lt;- IQR(df$A)
#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
no_outliers &lt;- subset(df, df$A> (Q1 - 1.5*IQR) & df$A&lt; (Q3 + 1.5*IQR))
#view row and column count of new data frame
dim(no_outliers) 
[1] 994   3</b>
The original data frame had 1,000 rows and 3 columns. The new data frame has 994 rows and 3 columns, which tells us that 6 rows were removed because they had at least one outlier in column A.
<h3>When to Remove Outliers</h3>
If one or more outliers are present, you should first verify that they’re not a result of a data entry error. Sometimes an individual simply enters the wrong data value when recording data.
If the outlier turns out to be a result of a data entry error, you may decide to assign a new value to it such as  the mean or the median  of the dataset.
If the value is a true outlier, you may choose to remove it if it will have a significant impact on your overall analysis. Just make sure to mention in your final report or analysis that you removed an outlier.
<h2><span class="orange">The Complete Guide: When to Remove Outliers in Data</span></h2>
An <b>outlier</b> is an  observation  that lies abnormally far away from other values in a dataset.
Outliers can be problematic because they can affect the results of an analysis.
However, they can also be informative about the data you’re studying because they can reveal abnormal cases or individuals that have rare traits.
In any analysis, you must decide to remove or keep outliers.
Fortunately, you can use the following flow chart to help you decide:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/removeOutlier1.png">
Let’s take a look a closer look at each question in the flow chart.
<h3>Is the Outlier a Result of Data Entry Error?</h3>
Sometimes outliers in a dataset are simply a result of data entry error.
For example, suppose a biologist is collecting data on the height of a certain species of plants and records the following data:
6.83 inches
7.51 inches
5.21 inches
5.84 inches
7.83 inches
<b>755 inches</b>
6.53 inches
6.31 inches
5.91 inches
Clearly the entry for 755 inches is an outlier and is likely a result of data entry error. More than likely, the height should have been 7.55 inches but was simply entered incorrectly.
If the biologist kept this observation and calculated a  descriptive statistic  like the mean height of the plants in the sample, this observation would greatly skew the results and give an inaccurate picture of the true mean height of the plants.
In this scenario (and in scenarios similar to this one) it makes sense to remove this outlier from the dataset because it’s an error and is not a legitimate data point to include in the analysis.
<h3>Does the Outlier Significantly Affect the Results of the Analysis?</h3>
If an observation is a true outlier and not just a result of a data entry error, then we need to examine whether or not the outlier affects the results of the analysis.
For example, suppose a biologist is studying the relationship between fertilizer and plant height. She wants to fit a  simple linear regression  model using fertilizer as the predictor variable and plant height as the  response variable .
She collects the following data for 12 different plants:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/removeOutlier2.png">
Clearly the last observation is an outlier.
However, if we create a scatterplot to visualize this dataset we can see that the regression line wouldn’t change much whether we included the outlier or not:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/removeOutlier3.png">
In this scenario, the outlier doesn’t actually violate any of the  assumptions of a linear regression model , so we could keep it in the dataset.
However, suppose we had the following outlier in the data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/removeOutlier4.png">
Clearly this outlier significantly affects the regression line so we could fit one regression model with the outlier and one without, then report the results of both regression models.
<h3>Does the Outlier Affect the Assumptions Made in the Analysis?</h3>
If an outlier is not a result of a data entry error and it does not significantly affect the results of an analysis, then we need to ask whether or not the outlier affects the assumptions made in an analysis.
If it does not affect the assumptions, then we can simply keep it in the data.
However, if it does affect the assumptions then we have a couple options:
<b>1. Remove it.</b> We can simply remove it from the data and make a note of this when reporting the results.
<b>2. Perform a transformation on the data.</b> Instead of removing the outlier, we could try performing a  transformation  on the data such as taking the square root or the log of all of the data values. This has been shown to shrink outlier values and often makes the data more  normally distributed .
No matter how you decide to handle outliers in your data, you should make a note of your decision in the output of your analysis along with your reasoning.
<h2><span class="orange">How to Remove Rows in R (With Examples)</span></h2>
You can use the following syntax to remove specific row numbers in R:
<b>#remove 4th row
new_df &lt;- df[-c(4), ]
#remove 2nd through 4th row
new_df &lt;- df[-c(2:4), ]
#remove 1st, 2nd, and 4th row
new_df &lt;- df[-c(1, 2, 4), ]
</b>
You can use the following syntax to remove rows that don’t meet specific conditions:
<b>#only keep rows where col1 value is less than 10 and col2 value is less than 6
new_df &lt;- subset(df, col1&lt;10 & col2&lt;6)
</b>
And you can use the following syntax to remove rows with an NA value in any column:
<b>#remove rows with NA value in any column
new_df &lt;- na.omit(df)</b>
The following examples show how to use each of these functions in practice.
<h3>Example 1: Remove Rows by Number</h3>
The following code shows how to remove rows by specific row numbers in R:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D', 'E'), pts=c(17, 12, 8, 9, 25), rebs=c(3, 3, 6, 5, 8), blocks=c(1, 1, 2, 4, NA))
#view data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   8    6      2
4      D   9    5      4
5      E  25    8     NA
#remove 4th row
df[-c(4), ]
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   8    6      2
5      E  25    8     NA
#remove 2nd through 4th row
df[-c(2:4), ]
  player pts rebs blocks
1      A  17    3      1
5      E  25    8     NA
#remove 1st, 2nd, and 4th row
df[-c(1, 2, 4), ]
  player pts rebs blocks
3      C   8    6      2
5      E  25    8     NA
</b>
<h3>Example 2: Remove Rows by Condition</h3>
The following code shows how to remove rows that don’t meet a specific condition:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D', 'E'), pts=c(17, 12, 8, 9, 25), rebs=c(3, 3, 6, 5, 8), blocks=c(1, 1, 2, 4, NA))
#view data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   8    6      2
4      D   9    5      4
5      E  25    8     NA
#only keep rows where pts is less than 10 and rebs is less than 6
subset(df, pts&lt;10 & rebs&lt;6)
  player pts rebs blocks
4      D   9    5      4
</b>
<h3>Example 3: Remove Rows with NA Values</h3>
The following code shows how to remove rows with a NA value in any row:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D', 'E'), pts=c(17, 12, 8, 9, 25), rebs=c(3, 3, 6, 5, 8), blocks=c(1, 1, 2, 4, NA))
#view data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   8    6      2
4      D   9    5      4
5      E  25    8     NA
#remove rows with NA value in any row:
na.omit(df)
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   8    6      2
4      D   9    5      4
</b>
<h2><span class="orange">How to Rename Factor Levels in R (With Examples)</span></h2>
There are two methods you can use to rename factor levels in R:
<b>Method 1: Use levels() from Base R</b>
<b>levels(df$col_name) &lt;- c('new_name1', 'new_name2', 'new_name3')</b>
<b>Method 2: Use recode() from dplyr package</b>
<b>library(dplyr)
data$col_name &lt;- recode(data$col_name, name1 = 'new_name1',                        name2 = 'new_name2',                       name3 = 'new_name3')</b>
The following examples show how to use each of these methods in practice.
<h3>Method 1: Use levels() Function</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df &lt;- data.frame(conf = factor(c('North', 'East', 'South', 'West')), points = c(34, 55, 41, 28))
#view data frame
df
   conf points
1 North     34
2  East     55
3 South     41
4  West     28
#view levels of 'conf' variable
levels(df$conf)
[1] "East"  "North" "South" "West" </b>
The following code shows how to rename one factor level by name using the <b>levels()</b> function:
<b>#rename just 'North' factor level
levels(df$conf)[levels(df$conf)=='North'] &lt;- 'N'
#view levels of 'conf' variable
levels(df$conf)
[1] "East"  "N"     "South" "West"</b>
And the following code shows how to rename every factor level:
<b>#rename every factor level
levels(df$conf) &lt;- c('N', 'E', 'S', 'W')
#view levels of 'conf' variable
levels(df$conf)
[1] "N" "E" "S" "W"</b>
<h3>Example 2: Use recode() Function</h3>
The following code shows how to use the <b>recode()</b> function from the dplyr package to rename factor levels:
<b>library(dplyr)
#create data frame
df &lt;- data.frame(conf = factor(c('North', 'East', 'South', 'West')), points = c(34, 55, 41, 28))
#recode factor levels
df$conf &lt;- recode(df$conf, North = 'N',           East  = 'E',           South = 'S',           West  = 'W')
levels(df$conf)
[1] "E" "N" "S" "W"
</b>
<b>Note</b>: You can find the complete documentation for the recode() function  here .
<h2><span class="orange">How to Rename Files in R (With Examples)</span></h2>
You can use the following methods to rename files in R:
<b>Method 1: Rename One File</b>
<b>file.rename(from='old_name.csv', to='new_name.csv')
</b>
<b>Method 2: Replace Pattern in Multiple Files</b>
<b>file.rename(list.files(pattern ='old'),
            str_replace(list.files(pattern='old'), pattern='old', 'new'))</b>
The following examples show how to use each method in practice.
<h3>Example: Rename One File</h3>
Suppose we have a folder with four CSV files in R:
<b>#display all files in current working directory
list.files()
"data1.csv"  "data2_good.csv"  "data3_good.csv"  "data4_good.csv"
</b>
We can use the following code to rename the file called <b>data1.csv</b> to <b>data1_good.csv</b>:
<b>#rename one file
file.rename(from='data1.csv', to='data1_good.csv')
#display all files in current working directory
list.files()
"data1_good.csv"  "data2_good.csv"  "data3_good.csv"  "data4_good.csv"</b>
Notice that the file has been successfully renamed.
<h3>Example: Replace Pattern in Multiple Files</h3>
Suppose we have a folder with four CSV files in R:
<b>#display all files in current working directory
list.files()
"data1_good.csv"  "data2_good.csv"  "data3_good.csv"  "data4_good.csv"
</b>
We can use the following code to replace “good” with “bad” in the name of every single file:
<b>library(stringr)
file.rename(list.files(pattern ='good'),
            str_replace(list.files(pattern='good'), pattern='good', 'bad'))
#display all files in current working directory
list.files()
"data1_bad.csv"  "data2_bad.csv"  "data3_bad.csv"  "data4_bad.csv"</b>
Notice that “good” has been replaced with “bad” in the name of every CSV file.
<b>Related:</b>  How to Use str_replace in R 
<h2><span class="orange">How to Rename Index in Pandas DataFrame</span></h2>
You can use the following syntax to rename the index column of a pandas DataFrame:
<b>df.index.rename('new_index_name', inplace=True)
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Rename Index in Pandas DataFrame</h3>
Suppose we have the following pandas DataFrame:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'points': [25, 12, 15, 14, 19, 23, 25, 29],   'assists': [5, 7, 7, 9, 12, 9, 9, 4],   'rebounds': [11, 8, 10, 6, 6, 5, 9, 12]})
#view DataFrame
df
   points  assists  rebounds
0      25        5        11
1      12        7         8
2      15        7        10
3      14        9         6
4      19       12         6
5      23        9         5
6      25        9         9
7      29        4        12
</b>
Currently the DataFrame has no index name:
<b>#display index name
print(df.index.name)
None</b>
We can use <b>df.index.rename()</b> to rename the index:
<b>#rename index
df.index.rename('new_index', inplace=True)
#view updated DataFrame
df
   points assists rebounds
new_index
0   25  5  11
1   12  7  8
2   15  7  10
3   14  9  6
4   19  12  6
5   23  9  5
6   25  9  9
7   29  4  12
</b>
Note that <b>inplace=True</b> tells pandas to retain all of the original DataFrame properties.
We can verify that the DataFrame now has an index name:
<b>#display index name
print(df.index.name)
new_index</b>
<h2><span class="orange">How to Rename an Object in R (With Examples)</span></h2>
To rename an object in R, we can use the assignment operator as follows:
<b>new_name &lt;- old_name
</b>
This syntax can be used to rename vectors, data frames, matrices, lists, and any other type of data object in R.
The following example shows how to use this syntax in practice. 
<h3>Example: Rename Object in R</h3>
Suppose we have the following data frame called <b>my_data</b> in R:
<b>#create data frame
some_data &lt;- data.frame(x=c(3, 4, 4, 5, 9),        y=c(3, 8, 7, 10, 4),        z=c(1, 2, 2, 6, 7))
#view data frame
some_data
  x  y z
1 3  3 1
2 4  8 2
3 4  7 2
4 5 10 6
5 9  4 7</b>
We can use the assignment operator to rename this data frame to <b>new_data</b>:
<b>#rename data frame
new_data &lt;- some_data
#view data frame
new_data
  x  y z
1 3  3 1
2 4  8 2
3 4  7 2
4 5 10 6
5 9  4 7</b>
Notice that we’re able to type <b>new_data</b> to view this data frame now.
However, it’s important to note that the old name <b>some_data</b> can still be used to reference this data frame:
<b>#view data frame
some_data
  x  y z
1 3  3 1
2 4  8 2
3 4  7 2
4 5 10 6
5 9  4 7
</b>
To remove this name from our R environment, we can use the <b>rm()</b> function:
<b>#remove old name of data frame
rm(some_data)
</b>
Now if we attempt to use the old name, the object will no longer be in an our environment:
<b>#attempt to use old name to view data frame
some_data
Error: object 'some_data' not found 
</b>
<h2><span class="orange">How to Rename a Single Column in R (With Examples)</span></h2>
You can use one of the following methods to rename a single column in a data frame in R:
<b>Method 1: Rename a Single Column Using Base R</b>
<b>#rename column by name
colnames(df)[colnames(df) == 'old_name'] &lt;- 'new_name'
#rename column by position
#colnames(df)[2] &lt;- 'new_name'</b>
<b>Method 2: Rename a Single Column Using dplyr</b>
<b>library(dplyr)
#rename column by name
df &lt;- df %>% rename_at('old_name', ~'new_name')
#rename column by position
df &lt;- df %>% rename_at(2, ~'new_name')
</b>
The following examples show how to use each method in practice with the following data frame in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B', 'C', 'D', 'E'), points=c(99, 90, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28))
#view data frame
df
  team points assists rebounds
1    A     99      33       30
2    B     90      28       28
3    C     86      31       24
4    D     88      39       24
5    E     95      34       28
</b>
<h2>Example 1: Rename a Single Column Using Base R</h2>
The following code shows how to rename the <b>points</b> column to <b>total_points</b> by using column names:
<b>#rename 'points' column to 'total_points'
colnames(df)[colnames(df) == 'points'] &lt;- 'total_points'
#view updated data frame
df
  team total_points assists rebounds
1    A           99      33       30
2    B           90      28       28
3    C           86      31       24
4    D           88      39       24
5    E           95      34       28
</b>
The following code shows how to rename the <b>points</b> column to <b>total_points</b> by using column position:
<b>#rename column in position 2 to 'total_points'
colnames(df)[2] &lt;- 'total_points'
#view updated data frame
df
  team total_points assists rebounds
1    A           99      33       30
2    B           90      28       28
3    C           86      31       24
4    D           88      39       24
5    E           95      34       28</b>
Notice that both methods produce the same result. 
<h2>Example 2: Rename a Single Column Using dplyr</h2>
The following code shows how to rename the <b>points</b> column to <b>total_points</b> by name using the <b>rename_at()</b> function in  dplyr :
<b>library(dplyr)
#rename 'points' column to 'total_points' by name
df &lt;- df %>% rename_at('points', ~'total_points')
#view updated data frame
df
  team total_points assists rebounds
1    A           99      33       30
2    B           90      28       28
3    C           86      31       24
4    D           88      39       24
5    E           95      34       28
</b>
The following code shows how to rename the <b>points</b> column to <b>total_points</b> by column position using the <b>rename_at()</b> function in  dplyr :
<b>library(dplyr)
#rename column in position 2 to 'total_points'
df &lt;- df %>% rename_at(2, ~'total_points')
#view updated data frame
df
  team total_points assists rebounds
1    A           99      33       30
2    B           90      28       28
3    C           86      31       24
4    D           88      39       24
5    E           95      34       28</b>
Notice that both methods produce the same result.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Select Specific Columns in R 
 How to Keep Certain Columns in R 
 How to Sort by Multiple Columns in R 
<h2><span class="orange">How to Reorder Boxplots in R (With Examples)</span></h2>
Often you may want to reorder boxplots in R.
The following examples show how to do so using two different methods:
<b>Method 1:</b> Reorder Based on Specific Order
<b>Method 2:</b> Reorder Based on Median Value of Boxplot
Each example will use the built-in <b>airquality</b> dataset in R:
<b>#view first six lines of <em>airquality</em> data
head(airquality)
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA      NA 14.3   56     5   5
6    28      NA 14.9   66     5   6
</b>
Here’s what a plot of multiple boxplots will look like for this dataset without specifying an order:
<b>#create boxplot that shows distribution of temperature by month
boxplot(Temp~Month, data=airquality, col="lightblue", border="black")      
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/boxorder1.jpg"440">
<h3>Example 1: Reorder Boxplots Based on Specific Order</h3>
The following code shows how to order the boxplots based on the following order for the <b>Month</b> variable: 5, 8, 6, 9, 7.
<b>#reorder Month values
airquality$Month &lt;- factor(airquality$Month , levels=c(5, 8, 6, 9, 7))
#create boxplot of temperatures by month using the order we specified
boxplot(Temp~Month, data=airquality, col="lightblue", border="black")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/boxorder2.jpg"440">
Notice that the boxplots now appear in the order that we specified using the <b>levels</b> argument.
<b>Related:</b>  How to Reorder Factor Levels in R 
<h3>Example 2: Reorder Boxplots Based on Median Value</h3>
The following code shows how to order the boxplots in <b>ascending order</b> based on the median temperature value for each month:
<b>#reorder Month values in ascending order based on median value of Temp
airquality$Month &lt;- with(airquality, reorder(Month , Temp, median , na.rm=T))
#create boxplot of temperatures by month
boxplot(Temp~Month, data=airquality, col="lightblue", border="black")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/boxorder3.jpg"448">
The boxplots now appear in ascending order based on the median value for each month.
<b>Note</b>: The median value for each boxplot is the horizontal black line that runs through the middle of each box.
We can also order the boxplots in <b>descending order</b> by using a negative sign in front of Temp in the <b>reorder</b> function:
<b>#reorder Month values in descending order based on median value of Temp
airquality$Month &lt;- with(airquality, reorder(Month , -Temp, median , na.rm=T))
#create boxplot of temperatures by month
boxplot(Temp~Month, data=airquality, col="lightblue", border="black")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/boxorder4.jpg"446">
The boxplots now appear in descending order based on the median value for each month.
<h2><span class="orange">How to Reorder Columns in R</span></h2>
Often you may want to reorder columns in a data frame in R.
Fortunately this is easy to do using the  select()  function from the  dplyr  package.
<b>library(dplyr)</b>
This tutorial shows several examples of how to use this function in practice using the following data frame:
<b>#create data frame
df &lt;- data.frame(player = c('a', 'b', 'c', 'd', 'e'), position = c('G', 'F', 'F', 'G', 'G'), points = c(12, 15, 19, 22, 32), rebounds = c(5, 7, 7, 12, 11))
#view data frame
df
  player position points rebounds
1      a        G     12        5
2      b        F     15        7
3      c        F     19        7
4      d        G     22       12
5      e        G     32       11</b>
<h2>Example 1: Move a Column to the First Position</h2>
The following code shows how to move a specific column in a data frame to the first position:
<b>#move column 'points' to first position
df %>% select(points, everything())
  points player position rebounds
1     12      a        G        5
2     15      b        F        7
3     19      c        F        7
4     22      d        G       12
5     32      e        G       11
</b>
This code tells dplyr to select the points column first, then include every other column after points.
<h2>Example 2: Move a Column to the Last Position</h2>
The following code shows how to move a specific column in a data frame to the last position:
<b>#move column 'points' to last position
df %>% select(-points, points)
  player position rebounds points
1      a        G        5     12
2      b        F        7     15
3      c        F        7     19
4      d        G       12     22
5      e        G       11     32</b>
This code tells dplyr to select all columns <em>except </em>the points column, then to select the points column again. This has the effect of moving the points column to the last position in the data frame.
<h2>Example 3: Reorder Multiple Columns</h2>
The following code shows how to reorder several columns at once in a specific order:
<b>#change all column names to uppercase
df %>% select(rebounds, position, points, player)
  rebounds position points player
1        5        G     12      a
2        7        F     15      b
3        7        F     19      c
4       12        G     22      d
5       11        G     32      e
</b>
<h2>Example 4: Reorder Columns Alphabetically</h2>
The following code shows how to order the columns in alphabetical order:
<b>#order columns alphabetically
df %>% select(order(colnames(.)))
  player points position rebounds
1      a     12        G        5
2      b     15        F        7
3      c     19        F        7
4      d     22        G       12
5      e     32        G       11</b>
<h2>Example 5: Reverse Column Order</h2>
The following code shows how to reverse the column order in a data frame:
<b>#reverse column order
df %>% select(rebounds:player, everything())
  rebounds points position player
1        5     12        G      a
2        7     15        F      b
3        7     19        F      c
4       12     22        G      d
5       11     32        G      e</b>
<b>Note</b>: You can find the complete documentation for the <b>select()</b> function  here .
<h2><span class="orange">How to Reorder Factor Levels in R (With Examples)</span></h2>
Occasionally you may want to re-order the levels of some factor variable in R. Fortunately this is easy to do using the following syntax:
<b>factor_variable &lt;- factor(factor_variable, levels=c('this', 'that', 'those', ...))
</b>
The following example show how to use this function in practice.
<h3>Example: Reorder Factor Levels in R</h3>
First, let’s create a data frame with one factor variable and one numeric variable:
<b>#create data frame
df &lt;- data.frame(region=factor(c('A', 'B', 'C', 'D', 'E')), sales=c(12, 18, 21, 14, 34))
#view data frame
df
  region sales
1      A    12
2      B    18
3      C    21
4      D    14
5      E    34
</b>
We can use the <b>levels()</b> argument to get the current levels of the factor variable <em>region</em>:
<b>#display factor levels for <em>region</em>
levels(df$region)
[1] "A" "B" "C" "D" "E"
</b>
And we can use the following syntax to re-order the factor levels:
<b>#re-order factor levels for <em>region
</em>df$region &lt;- factor(df$region, levels=c('A', 'E', 'D', 'C', 'B'))
#display factor levels for <em>region</em>
levels(df$region)
[1] "A" "E" "D" "C" "B"
</b>
The factor levels are now in the order that we specified using the <b>levels</b> argument.
If we then want to create a barplot in R and order the bars based on the factor levels of <em>region</em>, we can use the following syntax:
<b>#re-order data frame based on factor levels for <em>region
</em>df &lt;- df[order(levels(df$region)),]
#create barplot and place bars in order based on factor levels for <em>region</em>
barplot(df$sales, names=df$region)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/reorderFactor1.png">
Notice how the bars are in the order of the factor levels that we specified for <em>region</em>.
You can find more R tutorials on  this page .
<h2><span class="orange">How to Use rep() Function in R to Replicate Elements</span></h2>
You can use the <b>rep()</b> function in R  to replicate elements of vectors or lists a certain number of times.
This function uses the following basic syntax:
<b>rep(x, times = 1, length.out = NA, each = 1)</b>
where:
<b>x</b>: The object to replicate
<b>times</b>: The number of times to replicate object
<b>length.out</b>: Repeated x as many times as necessary to create vector of this length
<b>each</b>: Number of times to replicate individual elements in object
The following examples show how to use this function in practice.
<b>Note</b>: The rep() function is different than the  replicate()  function.
<h3>Example 1: Replicate a Vector Multiple Times</h3>
The following code shows how to use the <b>rep()</b> function to replicate a vector three times:
<b>#define vector
x &lt;- c(1, 10, 50)
#replicate the vector three times
rep(x, times=3)
[1]  1 10 50 1 10 50 1 10 50
</b>
The entire vector was replicated three times.
<h3>Example 2: Replicate Each Value in Vector the Same Number of Times</h3>
The following code shows how to use the <b>rep()</b> function to replicate <b>each</b> value in the vector five times:
<b>#define vector
x &lt;- c(1, 10, 50)
#replicate each value in vector five times
rep(x, each=5)
[1] 1 1 1 1 1 10 10 10 10 10 50 50 50 50 50
</b>
Each individual value in the vector was replicated five times.
<h3>Example 3: Replicate Each Value in Vector a Different Number of Times</h3>
The following code shows how to use the <b>rep()</b> function to replicate each value in the vector a specific number of <b>times</b>:
<b>#define vector
x &lt;- c(1, 10, 50)
#replicate each value in vector a specific number of times
rep(x, times=c(2, 5, 3))
[1]  1  1 10 10 10 10 10 50 50 50
</b>
From the output we can see:
The value 1 was replicated <b>2</b> times.
The value 10 was replicated <b>5</b> times.
The value 50 was replicated <b>3</b> times.
<h3>Example 4: Replicate Each Value in Vector the Same Number of Times, Multiple Times</h3>
The following code shows how to use the <b>rep()</b> function to replicate <b>each</b> value in the vector four times and to repeat this process two <b>times</b>:
<b>#define vector
x &lt;- c('A', 'B')
#replicate each value in vector four times and do this process two times
rep(x, each=4, times=2)
[1] "A" "A" "A" "A" "B" "B" "B" "B" "A" "A" "A" "A" "B" "B" "B" "B"
</b>
Each value in the vector was replicated four times and we repeated this process two times.
<h2><span class="orange">The Three Assumptions of the Repeated Measures ANOVA</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
However, before we perform a repeated measures ANOVA we must make sure the following assumptions are met:
<b>1. Independence:</b> Each of the observations should be independent.
<b>2. Normality:</b> The distribution of the response variable is normally distributed.
<b>3. Sphericity:</b> The variances of the differences between all combinations of related groups must be equal.
If one or more of these assumptions are violated, then the results of the repeated measures ANOVA may be unreliable.
In this article we provide an explanation for each assumption, how to determine if the assumption is met, and what to do if the assumption is violated.
<h2>Assumption 1: Independence</h2>
A repeated measures ANOVA assumes that each  observation  in your dataset is independent of every other observation.
<h3>How to Determine if this Assumption is Met</h3>
The easiest way to check this assumption is to verify that each individual in the dataset was randomly sampled from the  population  using a  random sampling method .
If a random sampling method was used, it’s safe to assume that each observation is independent.
<h3>What to Do if this Assumption is Violated</h3>
If this assumption is violated, then this is a serious issue because the values of each individual may be related to each other in some way.
Often the only remedy in this scenario is to recruit individuals for a new study using a random sampling method.
<h2>Assumption 2: Normality</h2>
A repeated measures ANOVA assumes that the distribution of the  response variable  is  normally distributed .
<h3>How to Determine if this Assumption is Met</h3>
There are two ways to check if this assumption is met:
<b>1. Create a Histogram or Q-Q Plot</b>
You can visually check if the distribution of the response variable is roughly normally distributed by creating a histogram or Q-Q plot.
If you create a <b>histogram</b>, simply check that the distribution of the response variable roughly follows a “bell” shape. If it does, you can often assume that the normality assumption is met:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/rep11.png">
If you create a <b>Q-Q plot</b>, check if the data points fall along a straight diagonal line. If they do, then you can typically assume that the normality assumption is met:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/11/rep12.png">
<b>Related:</b>  How to Use Q-Q Plots to Check Normality 
<b>2. Perform a Formal Statistical Test</b>
 You can also conduct a  Shapiro-Wilk Test  for normality. If the  p-value  of the test is less than .05, this suggests that the data is not normally distributed.
However, be aware that when working with extremely large sample sizes then statistical tests like the Shapiro-Wilk test will almost always tell you that your data is non-normal.
For this reason, it’s often best to inspect your data visually using graphs like histograms and Q-Q plots. By simply looking at the graphs, you can get a pretty good idea of whether or not the data is normally distributed.
<h3>What to Do if this Assumption is Violated</h3>
In general, a repeated measures ANOVA is considered to be fairly robust against violations of the normality assumption as long as the sample sizes are sufficiently large.
If the normality assumption is severely violated, you have two choices:
<b>1.</b>  Transform the response values  of your data so that the distributions are more normally distributed.
<b>2.</b> Perform an equivalent non-parametric test such as the  Friedman Test  that doesn’t require the assumption of normality. 
<h2>Assumption 3: Sphericity</h2>
A repeated measures ANOVA assumes <b>sphericity</b> – that variances of the differences between all combinations of related groups must be equal.
If this assumption is violated, then the F-ratio becomes inflated and the results of the repeated measures ANOVA become unreliable.
<h3>How to Determine if this Assumption is Met</h3>
To test if this assumption is met, we can perform  Mauchly’s Test of Sphericity .
This test uses the following null and alternative hypothesis:
<b>H<sub>0</sub></b>: The variances of the differences are equal
<b>H<sub>A</sub></b>: The variances of the differences are <em>not</em> equal
If the p-value of the test is less than some  significance level  (e.g. α = .05) then we reject the null hypothesis and conclude that the variances of the differences are not equal.
Otherwise, if the p-value is not less than some significance level (e.g. α = .05) then we fail to reject the null hypothesis and conclude that the assumption of sphericity is met.
Depending on which statistical software you use, the results of this test will look something like this:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/mauchly2.png">
Since the p-value is not less than .05, we would fail to reject the null hypothesis and conclude that the assumption of sphericity is met in this particular example.
<h3>What to Do if this Assumption is Violated</h3>
If we reject the null hypothesis of Mauchly’s test of sphericity, then we typically apply a correction to the degrees of freedom used to calculate the F-value in the repeated measures ANOVA table.
There are three corrections we can apply:
Huynh-Feldt (least conservative)
Greenhouse–Geisser
Lower-bound (most conservative)
Each of these corrections tend to increase the p-values in the output table of the repeated measures ANOVA to account for the fact that the assumption of sphericity is violated.
We can then use these p-values to determine if we should reject or fail to reject the null hypothesis of the repeated measures ANOVA.
<h2><span class="orange">How to Perform a Repeated Measures ANOVA By Hand</span></h2>
A  <b>repeated measures ANOVA</b>  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to perform a one-way repeated measures ANOVA by hand.
<h3>Example: One-Way Repeated Measures ANOVA by Hand</h3>
Researchers want to know if three different drugs lead to different reaction times. To test this, they measure the reaction time (in seconds) of five patients on each drug. The results are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/05/repANOVAhand1.png">
Since each patient is measured on each of the three drugs, we will use a one-way repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Use the following steps to perform the repeated measures ANOVA by hand:
<b>Step 1: Calculate SST.</b>
First, we will calculate the total sum of squares (SST), which can be found using the following formula:
SST = s<sup>2</sup><sub>total</sub>(n<sub>total</sub>-1)
where:
<b>s<sup>2</sup><sub>total</sub></b>: the variance for the entire dataset
<b>n<sub>total</sub></b>: the total number of observations in the entire dataset
In this example we calculate SST to be: (64.2667)(15-1) = <b>899.7</b>
<b>Step 2: Calculate SSB</b>
Next, we will calculate the between sum of squares (SSB), which can be found using the following formula:
SSB = Σn<sub>j</sub>(x<sub>j </sub> – x<sub>total</sub>)<sup>2</sup>
where:
<b>Σ</b>: a greek symbol that means “sum”
<b>n<sub>j</sub></b>: the total number of observations in the j<sup>th</sup> group
<b>x<sub>j</sub></b>: the mean of the j<sup>th</sup> group
<b>x<sub>total</sub></b>: the mean of the entire dataset
In this example we calculate SSB to be: (5)(26.4-22.533)<sup>2</sup> +(5)(25.6-22.533)<sup>2</sup> + (5)(15.6-22.533)<sup>2</sup> = <b>362.1</b>
<b>Step 3: Calculate SSS.</b>
Next, we will calculate the subject sum of squares (SSS), which can be found using the following formula:
SSS =(Σr<sup>2</sup><sub>k</sub>/c) – (N<sup>2</sup>/rc)
where:
<b>Σ</b>: a greek symbol that means “sum”
<b>r<sup>2</sup></b><sub>k</sub>: squared sum of the k<sup>th</sup> patient
<b>N: </b>the grand total of the entire dataset
<b>r: </b>total number of patients
<b>c: </b>total number of groups
In this example we calculate SSS to be: ((74<sup>2 </sup>+ 42<sup>2</sup> + 62<sup>2 </sup>+ 92<sup>2</sup> + 68<sup>2</sup>)/3) – (338<sup>2</sup>/(5)(3)) = <b>441.1</b>
<b>Step 4: Calculate SSE.</b>
Next, we will calculate the error sum of squares (SSE), which can be found using the following formula:
SSE = SST – SSB – SSS
In this example we calculate SSE to be: 899.7 – 362.1 – 441.1 = <b>96.5</b>
<b>Step 5: Fill in the Repeated measures ANOVA table.</b>
Now that we have SSB, SSS, and SSE, we can fill in the repeated measures ANOVA table:
<table><tbody>
<tr>
<th style="text-align: center;"><b>Source</b></th>
<th style="text-align: center;"><b>Sum of Squares (SS)</b></th>
<th style="text-align: center;"><b>df</b></th>
<th style="text-align: center;"><b>Mean Squares (MS)</b></th>
<th style="text-align: center;"><b>F</b></th>
</tr>
<tr>
<td><b>Between</b></td>
<td style="text-align: center;">362.1</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">181.1</td>
<td style="text-align: center;">15.006</td>
</tr>
<tr>
<td><b>Subject</b></td>
<td style="text-align: center;">441.1</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">110.3</td>
<td></td>
</tr>
<tr>
<td><b>Error</b></td>
<td style="text-align: center;">96.5</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">12.1</td>
<td></td>
</tr>
</tbody></table>
Here is how we calculated the various numbers in the table:
<b>df between: </b>#groups – 1 = 3 – 1 = 2
<b>df subject: </b>#participants – 1 = 5 – 1 = 4
<b>df error: </b>df between * df subject = 2*4 = 8 
<b>MS between: </b>SSB / df between = 362.1 / 2 = 181.1
<b>MS subject: </b>SSS / df subject = 441.1 / 4 = 110.3
<b>MS error: </b>SSE / df error = 96.5 / 8 = 12.1
<b>F: </b>MS between / MS error = 181.1 / 12.1 = 15.006
<b>Step 6: Interpret the results.</b>
The F test statistic for this one-way repeated measures ANOVA is <b>15.006</b>. To determine if this is a statistically significant result, we must compare this to the F critical value found in the  F distribution table  with the following values:
α (significance level) = 0.05
DF1 (numerator degrees of freedom) = df between = 2
DF2 (denominator degrees of freedom) = df error = 8
We find that the F critical value is <b>4.459</b>.
Since the F test statistic in the ANOVA table is greater than the F critical value in the F distribution table, we reject the null hypothesis. This means we have sufficient evidence to say that there is a statistically significant difference between the mean response times of the drugs.
<footer>
<imghttps://secure.gravatar.com/avatar/02b70638736d6f84bdd4015e398b3630?s=68&d=mm&r=g"><b>Bjorn</b> <span>says:
<!-- .comment-author -->
 <time datetime="2020-09-29T13:25:53-04:00">September 29, 2020 at 1:25 pm</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
Hello
There seems to be an error in the formula  step 3
SSS =(Σ rk^2/c) – (N^2/rc) :  N should be Grand Total (as in the calculation),  not number of obs. as explained underneath the formula
and in the following calculation:
 ((742 + 422 + 622 + 922 + 682)/3) – (3382/(6)(3)) = 441.1:
should be:
 ((742 + 422 + 622 + 922 + 682)/3) – (3382/(5)(3)) = 441.1
<!-- .comment-content -->
 Reply </article>
<footer>
<imghttps://secure.gravatar.com/avatar/958f66fbe89e80753a8c8a0492bf5cc8?s=68&d=mm&r=g"><b>Zach</b> <span>says:
<!-- .comment-author -->
 <time datetime="2020-09-29T13:33:41-04:00">September 29, 2020 at 1:33 pm</time> 
<!-- .comment-metadata -->
</footer><!-- .comment-meta -->
Good catch, just fixed it!
<!-- .comment-content -->
 Reply </article>
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in Excel</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to conduct a one-way repeated measures ANOVA in Excel.
<h3>Example: Repeated Measures ANOVA in Excel</h3>
Researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs. Since each patient is measured on each of the four drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Perform the following steps to conduct the repeated measures ANOVA in Excel.
<b>Step 1: Enter the data.</b>
Enter the following data, which shows the response time (in seconds) of five patients on the four drugs:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/repeatedANOVAExcel1.png">
<b>Step 2: Perform the repeated measures ANOVA.</b>
To perform the repeated measures ANOVA, go to the <b>Data</b> tab and click on <b>Data Analysis</b>. If you don’t see this option, then you need to first  install the free Analysis ToolPak .
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/twoSampExcel3.png">
Once you click on <b>Data Analysis,</b> a new window will pop up. Select <b>Anova: Two-Factor Without Replication </b>and click OK.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/repeatedANOVAExcel2.png">
<b>Note:</b>
 
The Analysis Toolpak doesn’t have an explicit function to perform a repeated measures ANOVA, but the <b>Anova: Two-Factor Without Replication </b>will produce the results we’re looking for, as we’ll see in the output.
For <b>Input Range</b>, type in the cell range that contains the response times for the patients. Feel free to leave <b>Alpha </b>at 0.05, unless you wish to use a different significance level. For <b>Output Range</b>, choose a cell where you would like the results to appear. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/repeatedANOVAExcel3.png">
The results will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/repeatedANOVAExcel4.png">
In this case we are not interested in the results for the Rows, only for the Columns, which tell us the variation in response time based on the drug.
The F test-statistic is <b>24.75887 </b>and the corresponding p-value is <b>0.0000199</b>. Since this p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs.
<b>Step 3: Report the results.</b>
Lastly, we will report the results of our repeated measures ANOVA. Here is an example of how to do so:
A one-way repeated measures ANOVA was conducted on 5 individuals to examine the effect that four different drugs had on response time.
 
Results showed that the type of drug used lead to statistically significant differences in response time (F(3, 12) = 24.75887, p &lt; 0.001).
<h2><span class="orange">Repeated Measures ANOVA in Google Sheets (Step-by-Step)</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial provides a step-by-step example of how to perform a repeated measures ANOVA in Google Sheets.
<h3>Step 1: Install the XLMiner Analysis ToolPak</h3>
To perform a one-way ANOVA in Google Sheets, we need to first install the free XLMiner Analysis Toolpak.
To do so, click <b>Add-ons > Get add-ons</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets0.png">
Next, type <b>XLMiner Analysis ToolPak</b> in the search bar and click the icon that appears:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets1.png">
Lastly, click the <b>Install</b> button.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/onewayanovaSheets2.png">
<h3>Step 2: Enter the Data</h3>
Next, we need to enter the data to use for the repeated measures ANOVA.
For this example, suppose researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs.
The reaction times are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/repeated1.png">
<h3>Step 3: Perform the Repeated Measures ANOVA</h3>
To perform a repeated measures ANOVA on this dataset, click <b>Add-ons > XLMiner Analysis ToolPak > Start</b>. The Analysis ToolPak will appear on the right side of the screen.
Click <b>Anova: Two-Factor Without Replication</b> and fill in the following information:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/repeated2.png">
<h3>Step 4: Interpret the Results</h3>
Once you click <b>OK</b>, the results of the repeated measures ANOVA will appear starting in the cell you specified in <b>Output Range</b>. In our case, we chose to display the results starting in cell A8:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/repeated3.png">
In this case we are not interested in the results for the Rows, only for the Columns, which tell us the variation in response time based on the drug.
From the output we can see that the F test-statistic is <b>24.75887 </b>and the corresponding p-value is <b>0.0000199</b>.
Since this p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs.
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in R</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to conduct a one-way repeated measures ANOVA in R.
<h3>Example: Repeated Measures ANOVA in R</h3>
Researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs. Since each patient is measured on each of the four drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Use the following steps to perform the repeated measures ANOVA in R.
<b>Step 1: Enter the data.</b>
First, we’ll create a data frame to hold our data:
<b>#create data
df &lt;- data.frame(patient=rep(1:5, each=4), drug=rep(1:4, times=5), response=c(30, 28, 16, 34,            14, 18, 10, 22,            24, 20, 18, 30,            38, 34, 20, 44,            26, 28, 14, 30))
#view data
df
   patient drug response
1        1    1       30
2        1    2       28
3        1    3       16
4        1    4       34
5        2    1       14
6        2    2       18
7        2    3       10
8        2    4       22
9        3    1       24
10       3    2       20
11       3    3       18
12       3    4       30
13       4    1       38
14       4    2       34
15       4    3       20
16       4    4       44
17       5    1       26
18       5    2       28
19       5    3       14
20       5    4       30   </b>
<b>Step 2: Perform the repeated measures ANOVA.</b>
Next, we will perform the repeated measures ANOVA using the <b>aov() </b>function:
<b>#fit repeated measures ANOVA model
model &lt;- aov(response~factor(drug)+Error(factor(patient)), data = df)
#view model summary
summary(model)
Error: factor(patient)
          Df Sum Sq Mean Sq F value Pr(>F)
Residuals  4  680.8   170.2               
Error: Within
             Df Sum Sq Mean Sq F value   Pr(>F)    
factor(drug)  3  698.2   232.7   24.76 1.99e-05 ***
Residuals    12  112.8     9.4                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
<b>Step 3: Interpret the results.</b>
A repeated measures ANOVA uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub> (the population means are all equal)
<b>The alternative hypothesis: (Ha):</b> at least one population mean is different from the rest
In this example, the F test-statistic is <b>24.76 </b>and the corresponding p-value is <b>1.99e-05</b>. Since this p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our repeated measures ANOVA.
Here is an example of how to do so:
A one-way repeated measures ANOVA was conducted on five individuals to examine the effect that four different drugs had on response time.
 
Results showed that the type of drug used lead to statistically significant differences in response time (F(3, 12) = 24.76, p &lt; 0.001).
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in SAS</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial provides a step-by-step example of how to perform a repeated measures ANOVA in SAS.
<h3>Step 1: Create the Data</h3>
Suppose a researcher want to know if four different drugs lead to different reaction times. To test this, he measures the reaction time of five patients on the four different drugs.
The reaction times are shown below:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/repeatSAS1.jpg"345">
We can use the following code to create this dataset in SAS:
<b>/*create dataset*/
data my_data;
    input Subject Drug Value;
    datalines;
1 1 30
1 2 28
1 3 16
1 4 34
2 1 14
2 2 18
2 3 10
2 4 22
3 1 24
3 2 20
3 3 18
3 4 30
4 1 38
4 2 34
4 3 20
4 4 44
5 1 26
5 2 28
5 3 14
5 4 30
;
run;
</b>
<h3>Step 2: Perform the Repeated Measures ANOVA</h3>
Next, we’ll use <b>proc glm </b>to perform the repeated measures ANOVA:
<b>/*perform repeated measures ANOVA*/
proc glm data=my_data;
class Subject Drug;
model Value = Subject Drug;
run;</b>
<h3>Step 3: Interpret the Results</h3>
We can analyze the ANOVA table in the output:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/repeatSAS2.jpg"450">
The only value we’re interested in is the F value and corresponding p-value for Drug since we want to know if the four different drugs lead to different reaction times.
From the output we can see:
The F Value for Drug: <b>24.76</b>
The p-value for Drug: <b>&lt;.0001</b>
Recall that a repeated measures ANOVA uses the following null and alternative hypotheses:
<b>H<sub>0</sub>:</b> All group means are equal.
<b>H<sub>A</sub>:</b> At least one group mean is different<sub> </sub>from the rest.
Since the p-value for Drug (&lt;.0001) is less than α = .05, we reject the null hypothesis.
This means we have sufficient evidence to say that the mean response time is not equal among the four different drugs.
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in Python</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to conduct a one-way repeated measures ANOVA in Python.
<h3>Example: Repeated Measures ANOVA in Python</h3>
Researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs. Since each patient is measured on each of the four drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Use the following steps to perform the repeated measures ANOVA in Python.
<b>Step 1: Enter the data.</b>
First, we’ll create a pandas DataFrame to hold our data:
<b>import numpy as np
import pandas as pd
#create data
df = pd.DataFrame({'patient': np.repeat([1, 2, 3, 4, 5], 4),   'drug': np.tile([1, 2, 3, 4], 5),   'response': [30, 28, 16, 34,                14, 18, 10, 22,                24, 20, 18, 30,                38, 34, 20, 44,                 26, 28, 14, 30]})
#view first ten rows of data 
df.head[:10]
patientdrugresponse
01130
11228
21316
31434
42114
52218
62310
72422
83124
93220   </b>
<b>Step 2: Perform the repeated measures ANOVA.</b>
Next, we will perform the repeated measures ANOVA using the  AnovaRM() function  from the statsmodels library:
<b>from statsmodels.stats.anova import AnovaRM
#perform the repeated measures ANOVA
print(AnovaRM(data=df, depvar='response', subject='patient', within=['drug']).fit())
              Anova
==================================
     F Value Num DF  Den DF Pr > F
----------------------------------
drug 24.7589 3.0000 12.0000 0.0000
==================================
</b>
<b>Step 3: Interpret the results.</b>
A repeated measures ANOVA uses the following null and alternative hypotheses:
<b>The null hypothesis (H<sub>0</sub>):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub> (the population means are all equal)
<b>The alternative hypothesis: (Ha):</b> at least one population mean is different from the rest
In this example, the F test-statistic is <b>24.7589 </b>and the corresponding p-value is <b>0.0000</b>. Since this p-value is less than 0.05, we reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our repeated measures ANOVA. Here is an example of how to do so:
A one-way repeated measures ANOVA was conducted on 5 individuals to examine the effect that four different drugs had on response time.
 
Results showed that the type of drug used lead to statistically significant differences in response time (F(3, 12) = 24.75887, p &lt; 0.001).
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in SPSS</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
This tutorial explains how to conduct a one-way repeated measures ANOVA in SPSS.
<h3>Example: Repeated Measures ANOVA in SPSS</h3>
Researchers want to know if four different drugs lead to different reaction times. To test this, they measure the reaction time of five patients on the four different drugs. Since each patient is measured on each of the four drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Perform the following steps to conduct the repeated measures ANOVA in SPSS.
<b>Step 1: Enter the data.</b>
Enter the following data, which shows the response time (in seconds) of five patients on the four drugs:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS1.png">
<b>Step 2: Perform a repeated measures ANOVA.</b>
Click the <b>Analyze </b>tab, then <b>General Linear Model</b>, then <b>Repeated Measures</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS2.png">
In the new window that pops up, type in the <b>drug </b>for the Within-Subject Factor Name. Type in <b>4 </b>for the Number of Levels (since each subject in the study tested 4 different drugs), then click <b>Add</b>. Type in <b>responseTime </b>for the <b>Measure Name</b>, then click <b>Add</b>. Lastly, click <b>Define</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS3.png">
In the new window that pops up, drag each of the four drug variables into the box labelled <b>Within-Subjects Variables</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS4.png">
Next, click <b>Plots</b>. Drag the variable <b>drug </b>into the box labelled <b>Horizontal Axis</b>. Then click <b>Add</b>. Then click <b>Continue</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS5.png">
Next, click <b>EM Means</b>. Drag the variable <b>drug </b>into the box labelled <b>Display Means for</b>. Then check the box next to <b>Compare main effects </b>and select <b>Bonferroni</b> from the dropdown menu. Then click <b>Continue</b>. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS6-1.png">
Lastly, click <b>OK</b>.
<b>Step 2: Interpret the results.</b>
Once you click <b>OK</b>, the results of the repeated measures ANOVA will appear. Here is how to interpret the output:
<b>Tests of Within-Subjects Effects</b>
This table displays the overall F-statistic and the corresponding p-value of the repeated measures ANOVA. Typically we use the values in the row titled <b>Greenhouse-Geisser</b>.
According to this row, the F-statistic is <b>24.759 </b>and the corresponding p-value is <b>.001</b>. Since this p-value is less than .05, we can reject the null hypothesis and conclude that there is a statistically significant difference in mean response times between the four drugs.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS7.png">
<b>Pairwise Comparisons</b>
Since we rejected the null hypothesis, it means that at least two of the group means are different. To determine which group means are different, we can use this table that displays the pairwise comparisons between each drug. 
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS8.png">
From the table we can see the p-values for the following comparisons:
drug 1 vs. drug 2 | p-value = <b>1.000</b>
drug 1 vs. drug 3 | p-value = <b>.083</b>
drug 1 vs. drug 4 | p-value = <b>.010</b>
drug 2 vs. drug 3 | p-value = <b>.071</b>
drug 2 vs. drug 4 | p-value = <b>.097</b>
drug 3 vs. drug 4 | p-value = <b>.011</b>
The only p-values below .05 are for drug 1 vs. drug 4 and drug 3 vs. drug 4. All of the other comparisons have p-values greater than .05.
<b>Plot of Estimated Marginal Means</b>
This plot displays the estimated mean response times for each drug. From the plot we can clearly see that response times varied noticeably between the four different drugs:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/repeatedANOVASPSS9.png">
<b>Step 3: Report the results.</b>
Lastly, we can report the results of the repeated measures ANOVA. Here is an example of how to do so:
A one-way repeated measures ANOVA was performed to determine if the mean reaction time in patients differed between four different drugs.
 
A one-way repeated measures ANOVA revealed that the type of drug used lead to statistically significant differences in response time (F = 24.75887, p = 0.001).
 
Bonferroni’s test for multiple comparisons found that there was a statistically significant difference in response times between patients on drug 1 vs. drug 4 along with drug 3 vs. drug 4.
<h2><span class="orange">How to Perform a Repeated Measures ANOVA in Stata</span></h2>
A  repeated measures ANOVA  is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
We use a one-way repeated measures ANOVA in two specific situations:
<b>1. Measuring the mean scores of subjects during three or more time points.</b> For example, you might want to measure the resting heart rate of subjects one month before they start a training program, during the middle of the training program, and one month after the training program to see if there is a significant difference in mean resting heart rate across these three time points.
  
Notice how the same subjects show up at each time point. We <em>repeatedly </em>measured the same subjects, hence the reason why we used a one-way repeated measures ANOVA.
<b>2. Measuring the mean scores of subjects under three different conditions.</b> For example, you might have subjects watch three different movies and rate each one based on how much they enjoyed it.
  
Again, the same subjects show up in each group, so we need to use a one-way repeated measures ANOVA to test for the difference in means across these three conditions. 
This tutorial explains how to conduct a one-way repeated measures ANOVA in Stata.
<h2 data-slot-rendered-dynamic="true"><b>Example: Repeated Measures ANOVA in Stata</h2>
Researchers measure the reaction time of five patients on four different drugs. Since each patient is measured on each of the four drugs, we will use a repeated measures ANOVA to determine if the mean reaction time differs between drugs.
Perform the following steps to conduct the repeated measures ANOVA in Stata.
<b>Step 1: Load the data.</b>
First, load the data by typing <b>use http://www.stata-press.com/data/r14/t43 </b>in the command box and clicking Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayRepeatedStata1.png">
<b>Step 2: View the raw data.</b>
Before we perform a repeated measures ANOVA, let’s first view the raw data. Along the top menu bar, go to <b>Data > Data Editor > Data Editor (Browse)</b>. This will show us the response times for each of the 5 patients on each of the four drugs:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayRepeatedStata2.png">
<b>Step 3: Perform a repeated measures ANOVA.</b>
Along the top menu bar, go to <b>Statistics > Linear models and related > ANOVA/MANOVA > Analysis of variance and covariance</b>.
For Dependent variable, choose <em>score</em>. For Model, choose <em>person </em>and <em>drug </em>as the two explanatory variables. Check the box that says Repeated-measures variables and choose <em>drug </em>as the variable that repeats. Leave everything else as it is and click <em>OK</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayRepeatedStata3.png">
This will automatically produce the following two tables that show the results for the repeated measures ANOVA:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/oneWayRepeatedStata4.png">
In the first table we are interested in the <b>F value</b> and the <b>p-value</b> (displayed as Prob>F) for the <em>drug </em>variable. Notice that F = 24.76 and the p-value is 0.000. This indicates there is a statistically significant difference among the mean scores for the four drugs.
The second table should only be used if we suspect that the sphericity assumption has been violated. This is the assumption that the variances of the differences between all pairwise combinations of groups must be equal. If we believe this assumption has been violated then we can use one of the three correction factors – Hunyh-Feldt epsilon, Greenhouse-Geisser epsilon, or Box’s conservative epsilon.
The p-value for the variable <em>drug </em>is shown for each of these three correction factors:
Hunyh-Feldt (H-F) p-value = 0.000
Greenhouse-Geisser (G-G) p-value = 0.0006
Box’s conservative (Box) p-value = 0.0076
Notice that each of the p-values is less than 0.05, so there is still a statistically significant difference among the mean scores for the four drugs no matter which correction factor we use.
<b>Step 4: Report the results.</b>
Lastly, we will report the results of our repeated measures ANOVA. Here is an example of how to do so:
A one-way repeated measures ANOVA was conducted on 5 individuals to examine the effect that four different drugs had on response time.
 
Results showed that the type of drug used lead to statistically significant differences in response time (F(3, 12) = 24.75, p &lt; 0.001).
<h2><span class="orange">Repeated Measures ANOVA: Definition, Formula, and Example</span></h2>
A<b> repeated measures ANOVA</b> is used to determine whether or not there is a statistically significant difference between the means of three or more groups in which the same subjects show up in each group.
A repeated measures ANOVA is typically used in two specific situations:
<b>1. Measuring the mean scores of subjects during three or more time points.</b> For example, you might want to measure the resting heart rate of subjects one month before they start a training program, during the middle of the training program, and one month after the training program to see if there is a significant difference in mean resting heart rate across these three time points.
  
Notice how the same subjects show up at each time point. We <em>repeatedly </em>measured the same subjects, hence the reason why we used a repeated measures ANOVA.
<b>2. Measuring the mean scores of subjects under three different conditions.</b> For example, you might have subjects watch three different movies and rate each one based on how much they enjoyed it. 
  
Again, the same subjects show up in each group, so we need to use a repeated measures ANOVA to test for the difference in means across these three conditions. 
<h2>One-Way ANOVA vs. Repeated Measures ANOVA</h2>
In a typical  one-way ANOVA , different subjects are used in each group. For example, we might ask subjects to rate three movies, just like in the example above, but we use different subjects to rate each movie:
  
In this case, we would conduct a typical one-way ANOVA to test for the difference between the mean ratings of the three movies. 
In real life there are two benefits of using the same subjects across multiple treatment conditions:
<b>1.</b> It’s cheaper and faster for researchers to recruit and pay a smaller number of people to carry out an experiment since they can just obtain data from the same people multiple times. 
<b>2. </b>We are able to attribute some of the variance in the data to the subjects themselves, which makes it easier to obtain a smaller p-value.
One potential drawback of this type of design is that subjects might get bored or tired if an experiment lasts too long, which could skew the results. For example, subjects might give lower movie ratings to the third movie they watch because they’re tired and ready to go home.
<h2>Repeated Measures ANOVA: Example</h2>
Suppose we recruit five subjects to participate in a training program. We measure their resting heart rate before participating in a training program, after participating for 4 months, and after participating for 8 months. 
The following table shows the results:
  
We want to know whether there is a difference in mean resting heart rate at these three time points so we conduct a repeated measures ANOVA at the .05 significance level using the following steps:
<b>Step 1. State the hypotheses. </b>
<b>The null hypothesis (H<sub>0</sub>):</b> μ<sub>1</sub> = μ<sub>2</sub> = μ<sub>3</sub> (the population means are all equal)
<b>The alternative hypothesis: (Ha):</b> at least one population mean is different from the rest
<b>Step 2. Perform the repeated measures ANOVA.</b>
We will use the  Repeated Measures ANOVA Calculator  using the following input:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/anovarep1.png">
Once we click “Calculate” then the following output will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/12/anovarep2.png">
<b>Step 3. Interpret the results. </b>
From the output table we see that the F test statistic is <b>9.598 </b>and the corresponding p-value is <b>0.00749</b>.
Since this p-value is less than 0.05, we reject the null hypothesis. This means we have sufficient evidence to say that there is a statistically significant difference between the mean resting heart rate at the three different points in time.
<h2><span class="orange">How to Use the replace() Function in R</span></h2>
The <b>replace()</b> function in R can be used to replace specific elements in a vector with new values.
This function uses the following syntax:
<b>replace(x, list, values)</b>
where:
<b>x</b>: Name of vector
<b>list</b>: Elements to replace
<b>values</b>: Replacement values
The following examples show how to use this function in practice.
<h3>Example 1: Replace One Value in Vector</h3>
The following code shows how to replace the element in position 2 of a vector with a new value of 50:
<b>#define vector of values
data &lt;- c(3, 6, 8, 12, 14, 15, 16, 19, 22)
#define new vector with a different value in position 2
data_new &lt;- replace(data, 2, 50)
#view new vector
data_new
[1]  3 50  8 12 14 15 16 19 22
</b>
Notice that the element in position 2 has changed, but every other value in the original vector remained the same.
<h3>
<b>Example 2: Replace Multiple Values in Vector</b>
</h3>
The following code shows how to replace the values of multiple elements in a vector with new values:
<b>#define vector of values
data &lt;- c(2, 4, 6, 8, 10, 12, 14, 16)
#define new vector with different values in position 1, 2, and 8
data_new &lt;- replace(data, c(1, 2, 8), c(50, 100, 200))
#view new vector
data_new
[1]  50 100   6   8  10  12  14 200
</b>
Notice that the elements in position 1, 2, and 8 all changed.
<h3>
<b>Example 3: Replace Values in Data Frame</b>
</h3>
The following code shows how to replace the values in a certain column of a data frame that meet a specific condition:
<b>#define data frame
df &lt;- data.frame(x=c(1, 2, 4, 4, 5, 7), y=c(6, 6, 8, 8, 10, 11))
#view data frame
df
  x  y
1 1  6
2 2  6
3 4  8
4 4  8
5 5 10
6 7 11
#replace values in column 'x' greater than 4 with a new value of 50
df$x &lt;- replace(df$x, df$x > 4, 50)
#view updated data frame
df
   x  y
1  1  6
2  2  6
3  4  8
4  4  8
5 50 10
6 50 11
</b>
Each value in column ‘x’ that was greater than 4 was replaced with a value of 50.
All other values in the data frame remained the same.
<h2><span class="orange">How to Replace NAs with Strings in R (With Examples)</span></h2>
You can use the <b>replace_na()</b> function from the <b>tidyr</b> package to replace NAs with specific strings in  a column of a data frame in R:
<b>#replace NA values in column x with "missing"
df$x %>% replace_na('none')
</b>
You can also use this function to replace NAs with specific strings in multiple columns of a data frame:
<b>#replace NA values in column x with "missing" and NA values in column y with "none"
df %>% replace_na(list(x = 'missing', y = 'none')) </b>
The following examples show how to use this function in practice.
<h3>Example 1: Replace NAs with Strings in One Column</h3>
The following code shows how to replace NAs with a specific string in one column of a data frame:
<b>library(tidyr)
df &lt;- data.frame(status=c('single', 'married', 'married', NA), education=c('Assoc', 'Bach', NA, 'Master'), income=c(34, 88, 92, 90))
#view data frame
df
   status education income
1  single     Assoc     34
2 married      Bach     88
3 married      &lt;NA>     92
4    &lt;NA>    Master     90
#replace missing values with 'single' in status column
df$status &lt;- df$status %>% replace_na('single')
#view updated data frame
df 
   status education income
1  single     Assoc     34
2 married      Bach     88
3 married      &lt;NA>     92
4  single    Master     90
</b>
<h3>Example 2: Replace NAs with Strings in Multiple Columns</h3>
The following code shows how to replace NAs with a specific string in multiple columns of a data frame:
<b>library(tidyr)
df &lt;- data.frame(status=c('single', 'married', 'married', NA), education=c('Assoc', 'Bach', NA, 'Master'), income=c(34, 88, 92, 90))
#view data frame
df
   status education income
1  single     Assoc     34
2 married      Bach     88
3 married      &lt;NA>     92
4    &lt;NA>    Master     90
#replace missing values with 'single' in status column
df &lt;- df %>% replace_na(list(status = 'single', education = 'none'))
#view updated data frame
df 
   status education income
1  single     Assoc     34
2 married      Bach     88
3 married      none     92
4  single    Master     90</b>
<h2><span class="orange">How to Replace Values in Data Frame in R (With Examples)</span></h2>
You can use the following syntax to replace a particular value in a data frame in R with a new value:
<b>df[df == 'Old Value'] &lt;- 'New value'
</b>
You can use the following syntax to replace one of several values in a data frame with a new value:
<b>df[df == 'Old Value 1' | df == 'Old Value 2'] &lt;- 'New value'</b>
And you can use the following syntax to replace a particular value in a specific column of a data frame with a new value:
<b>df['column1'][df['column1'] == 'Old Value'] &lt;- 'New value'
</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Replace Particular Value Across Entire Data Frame</h3>
The following code shows how to replace one particular value with a new value across an entire data frame:
<b>#create data frame
df &lt;- data.frame(a = as.factor(c(1, 5, 7, 8)), b = c('A', 'B', 'C', 'D'), c = c(14, 14, 19, 22), d = c(3, 7, 14, 11))
#view data frame
df
  a b  c  d
1 1 A 14  3
2 5 B 14  7
3 7 C 19 14
4 8 D 22 11
#replace '14' with '24' across entire data frame
df[df == 14] &lt;- 24
#view updated data frame
df 
  a b  c  d
1 1 A 24  3
2 5 B 24  7
3 7 C 19 24
4 8 D 22 11
</b>
<h3>Example 2: Replace One of Several Values Across Entire Data Frame</h3>
The following code shows how to replace one of several values with a new value across an entire data frame:
<b>#create data frame
df &lt;- data.frame(a = as.factor(c(1, 5, 7, 8)), b = c('A', 'B', 'C', 'D'), c = c(14, 14, 19, 22), d = c(3, 7, 14, 11))
#view data frame
df
  a b  c  d
1 1 A 14  3
2 5 B 14  7
3 7 C 19 14
4 8 D 22 11
#replace '14' and '19' with '24' across entire data frame
df[df == 14 | df == 19] &lt;- 24
#view updated data frame
df
  a b  c  d
1 1 A 24  3
2 5 B 24  7
3 7 C 24 24
4 8 D 22 11
</b>
<h3>Example 3: Replace Value in Specific Column of Data Frame</h3>
The following code shows how to replace one particular value with a new value in a specific column of a data frame:
<b>#create data frame
df &lt;- data.frame(a = as.factor(c(1, 5, 7, 8)), b = c('A', 'B', 'C', 'D'), c = c(14, 14, 19, 22), d = c(3, 7, 14, 11))
#view data frame
df
  a b  c  d
1 1 A 14  3
2 5 B 14  7
3 7 C 19 14
4 8 D 22 11
#replace '14' in column <em>c</em> with '24'
df['c'][df['c'] == 14] &lt;- 24
#view updated data frame
df 
  a b  c  d
1 1 A 24  3
2 5 B 24  7
3 7 C 19 14
4 8 D 22 11</b>
<h3>Example 4: Replace Values of a Factor Variable in Data Frame</h3>
If you attempt to replace a particular value of a factor variable, you will encounter the following warning message:
<b>#create data frame
df &lt;- data.frame(a = as.factor(c(1, 5, 7, 8)), b = c('A', 'B', 'C', 'D'), c = c(14, 14, 19, 22), d = c(3, 7, 14, 11))
#attempt to replace '1' with '24' in column <em>a</em>
df['a'][df['a'] == 1] &lt;- 24
Warning message:
In `[&lt;-.factor`(`*tmp*`, thisvar, value = 24) :
  invalid factor level, NA generated
     a b  c  d
1 &lt;NA> A 14  3
2    5 B 14  7
3    7 C 19 14
4    8 D 22 11
</b>
To avoid this warning, you need to first convert the factor variable to a numeric variable:
<b>#convert column <em>a</em> to numeric
df$a &lt;- as.numeric(as.character(df$a))
#replace '1' with '24' in column <em>a</em>
df['a'][df['a'] == 1] &lt;- 24
#view updated data frame
df
   a b  c  d
1 24 A 14  3
2  5 B 14  7
3  7 C 19 14
4  8 D 22 11
</b>
<h2><span class="orange">How to Replace Values in a List in Python</span></h2>
Often you may be interested in replacing one or more values in a list in Python.
Fortunately this is easy to do in Python and this tutorial explains several different examples of doing so.
<h3>Example 1: Replace a Single Value in a List</h3>
The following syntax shows how to replace a single value in a list in Python:
<b>#create list of 4 items</b>
<b>x = ['a', 'b', 'c', 'd']</b>
<b>#replace first item in list</b>
<b>x[0] = 'z'</b>
<b>#view updated list</b>
<b>x
['z', 'b', 'c', 'd']
</b>
<h3>Example 2: Replace Multiple Values in a List</h3>
The following syntax shows how to replace multiple values in a list in Python:
<b>#create list of 4 items</b>
<b>x = ['a', 'b', 'c', 'd']</b>
<b>#replace first three items in list</b>
<b>x[0:3] = ['x', 'y', 'z']</b>
<b>#view updated list</b>
<b>x
['x', 'y', 'z', 'd']</b>
<h3>Example 3: Replace Specific Values in a List</h3>
The following syntax shows how to replace specific values in a list in Python:
<b>#create list of 6 items
y = [1, 1, 1, 2, 3, 7]
#replace 1's with 0's
y = [0 if x==1 else x for x in y]
#view updated list
y
[0, 0, 0, 2, 3, 7]
</b>
You can also use the following syntax to replace values that are greater than a certain threshold:
<b>#create list of 6 items
y = [1, 1, 1, 2, 3, 7]
#replace all values above 1 with a '0'
y = [0 if x>1 else x for x in y]
#view updated list
y
[1, 1, 1, 0, 0, 0]
</b>
Similarly you could replace values that are less than or equal to some threshold:
<b>#create list of 6 items
y = [1, 1, 1, 2, 3, 7]
#replace all values less than or equal to 2 a '0'
y = [0 if x&lt;=2 else x for x in y]
#view updated list
y
[0, 0, 0, 0, 3, 7]</b>
<em>Find more Python tutorials  here .</em>
<h2><span class="orange">How to Replace Zero with NA in R (With Examples)</span></h2>
You can use the following methods to replace zero with NA values in R:
<b>Method 1: Replace Zero with NA in All Columns</b>
<b>df[df == 0] &lt;- NA</b>
<b>Method 2: Replace Zero with NA in One Column</b>
<b>df$col1[df$col1 == 0] &lt;- NA</b>
<b>Method 3: Replace Zero with NA in Several Specific Columns</b>
<b>df[, c('col1', 'col2')][df[, c('col1', 'col2')] == 0] &lt;- NA</b>
The following examples show how to use each method in practice with the following data frame:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D', 'E'), pts=c(17, 12, NA, 9, 25), rebs=c(3, 3, NA, NA, 8), blocks=c(1, 1, 2, 4, NA))
#view data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C  NA   NA      2
4      D   9   NA      4
5      E  25    8     NA
</b>
<h2>Example 1: Replace Zero with NA in All Columns</h2>
The following code shows how to replace zeros with NA values in all columns of a data frame:
<b>#replace zero with NA in all columns
df[df == 0] &lt;- NA
#view updated data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C  NA   NA      2
4      D   9   NA      4
5      E  25    8     NA
</b>
Notice that the zeros have been replaced with NA values in every column of the data frame.
<h2>Example 2: Replace Zero with NA in One Column</h2>
The following code shows how to replace zero with NA values in one column of a data frame:
<b>#replace zero with NA in 'rebs' column only
df$rebs[df$rebs == 0] &lt;- NA
#view data frame
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C   0   NA      2
4      D   9   NA      4
5      E  25    8      0</b>
Notice that each zero has been replaced with NA in the ‘rebs’ column while all other columns have remained unchanged.
<h2>Example 3: Replace Zero with NA in Several Specific Columns</h2>
The following code shows how to replace zero with NA values in several specific columns of a data frame:
<b>#replace zero with NA values in 'pts' and 'rebs' columns only
df[, c('pts', 'rebs')][df[, c('pts', 'rebs')] == 0] &lt;- NA
#view data frame
df
  player pts rebs blocks
1      A  17    3      1
2      B  12    3      1
3      C  NA   NA      2
4      D   9   NA      4
5      E  25    8      0</b>
Notice that each zero has been replaced with NA in the ‘pts’ and ‘rebs’ columns while the ‘blocks’ column has remained unchanged.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Replace Blanks with NA in R 
 How to Replace NAs with Strings in R 
 How to Replace NA with Mean in R 
 How to Replace NA with Median in R 
<h2><span class="orange">How to Use the replicate() Function in R (With Examples)</span></h2>
You can use the <b>replicate()</b> function to repeatedly evaluate some expression in R a certain number of times.
This function uses the following basic syntax:
<b>replicate(n, expr)</b>
where:
<b>n</b>: The number of times to repeatedly evaluate some expression.
<b>expr</b>: The expression to evaluate.
The following examples show how to use this function in practice.
<h3>Example 1: Replicate a Value Multiple Times</h3>
The following code shows how to use the <b>replicate()</b> function to repeatedly evaluate a single value multiple times:
<b>#replicate the value 3 exactly 10 times
replicate(n=10, 3)
[1] 3 3 3 3 3 3 3 3 3 3
#replicate the letter 'A' exactly 7 times
replicate(n=7, 'A')
[1] "A" "A" "A" "A" "A" "A" "A"
#replicate FALSE exactly 5 times
replicate(n=5, FALSE)
[1] FALSE FALSE FALSE FALSE FALSE
</b>
<h3>Example 2: Replicate a Function Multiple Times</h3>
Now suppose we’d like to repeatedly evaluate some function.
For example, suppose we use the <b>rnorm()</b> function to produce three values for a  random variable  that follows a normal distribution with a mean of 0 and a standard deviation of 1:
<b>#make this example reproducible
set.seed(1)
#generate 3 values that follow normal distribution
rnorm(3, mean=0, sd=1) 
[1] -0.6264538  0.1836433 -0.8356286
</b>
Using the <b>replicate()</b> function, we can repeatedly evaluate this rnorm() function a certain number of times.
For example, we can evaluate this function 5 times:
<b>#make this example reproducible
set.seed(1)
#generate 3 values that follow normal distribution (replicate this 4 times)
replicate(n=4, rnorm(3, mean=0, sd=1))
           [,1]      [,2]       [,3]       [,4]
[1,]  1.5952808 0.4874291 -0.3053884 -0.6212406
[2,]  0.3295078 0.7383247  1.5117812 -2.2146999
[3,] -0.8204684 0.5757814  0.3898432  1.1249309</b>
The result is a matrix with 3 rows and 4 columns.
Or perhaps we’d like to evaluate this function 6 times:
<b>#make this example reproducible
set.seed(1)
#generate 3 values that follow normal distribution (replicate this 6 times)
replicate(n=6, rnorm(3, mean=0, sd=1))
           [,1]      [,2]       [,3]       [,4]        [,5]      [,6]
[1,]  1.5952808 0.4874291 -0.3053884 -0.6212406 -0.04493361 0.8212212
[2,]  0.3295078 0.7383247  1.5117812 -2.2146999 -0.01619026 0.5939013
[3,] -0.8204684 0.5757814  0.3898432  1.1249309  0.94383621 0.9189774</b>
The result is a matrix with 6 rows and 3 columns.
<h3>Using replicate() to Simulate Data</h3>
The <b>replicate()</b> function is particularly useful for running simulations.
For example, suppose we’d like to generate 5 samples of size n = 10 that each follow a normal distribution.
We can use the <b>replicate()</b> function to produce 5 different samples and we can then use the <b>colMeans()</b> function to find the mean value of each sample:
<b>#make this example reproducible
set.seed(1)
#create 5 samples each of size n=10
data &lt;- replicate(n=5, rnorm(10, mean=0, sd=1))
#view samples
data
            [,1]        [,2]        [,3]        [,4]       [,5]
 [1,] -0.6264538  1.51178117  0.91897737  1.35867955 -0.1645236
 [2,]  0.1836433  0.38984324  0.78213630 -0.10278773 -0.2533617
 [3,] -0.8356286 -0.62124058  0.07456498  0.38767161  0.6969634
 [4,]  1.5952808 -2.21469989 -1.98935170 -0.05380504  0.5566632
 [5,]  0.3295078  1.12493092  0.61982575 -1.37705956 -0.6887557
 [6,] -0.8204684 -0.04493361 -0.05612874 -0.41499456 -0.7074952
 [7,]  0.4874291 -0.01619026 -0.15579551 -0.39428995  0.3645820
 [8,]  0.7383247  0.94383621 -1.47075238 -0.05931340  0.7685329
 [9,]  0.5757814  0.82122120 -0.47815006  1.10002537 -0.1123462
[10,] -0.3053884  0.59390132  0.41794156  0.76317575  0.8811077
#calculate mean of each sample
colMeans(data)
[1]  0.1322028  0.2488450 -0.1336732  0.1207302  0.1341367
</b>
From the output we can see:
The mean of the first sample is <b>0.1322</b>.
The mean of the second sample is <b>0.2488</b>.
The mean of the third sample is <b>-0.1337</b>.
And so on.
<h2><span class="orange">How to Replicate Rows in Data Frame in R</span></h2>
You can use the following methods to replicate rows in a data frame in R using functions from the  dplyr  package:
<b>Method 1: Replicate Each Row the Same Number of Times </b>
<b>library(dplyr)
#replicate each row 3 times
df %>% slice(rep(1:n(), each = 3))</b>
<b>Method 2: Replicate Each Row a Different Number of Times</b>
<b>library(dplyr)
#replicate the first row 3 times and the second row 5 times
df %>% slice(rep(1:n(), times = c(3, 5)))</b>
The following examples show how to use each method in practice.
<h2>Example 1: Replicate Each Row the Same Number of Times</h2>
Suppose we have the following data frame with two rows in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B'), points=c(10, 15), rebounds=c(4, 8), assists=c(2, 5))
#view data frame
df
  team points rebounds assists
1    A     10        4       2
2    B     15        8       5
</b>
We can use the following syntax to repeat each row in the data frame three times:
<b>library(dplyr)
#create new data frame that repeats each row in original data frame 3 times
new_df &lt;- df %>% slice(rep(1:n(), each = 3))
#view new data frame
new_df
  team points rebounds assists
1    A     10        4       2
2    A     10        4       2
3    A     10        4       2
4    B     15        8       5
5    B     15        8       5
6    B     15        8       5</b>
Notice that each of the rows from the original data frame have been repeated three times.
<h2>Example 2: Replicate Each Row a Different Number of Times</h2>
Suppose we have the following data frame with two rows in R:
<b>#create data frame
df &lt;- data.frame(team=c('A', 'B'), points=c(10, 15), rebounds=c(4, 8), assists=c(2, 5))
#view data frame
df
  team points rebounds assists
1    A     10        4       2
2    B     15        8       5
</b>
We can use the following syntax to repeat the first row three times and the second row five times:
<b>library(dplyr)
#create new data frame that repeats first row 3 times and second row 5 times
new_df &lt;- df %>% slice(rep(1:n(), times = c(3, 5)))
#view new data frame
new_df
  team points rebounds assists
1    A     10        4       2
2    A     10        4       2
3    A     10        4       2
4    B     15        8       5
5    B     15        8       5
6    B     15        8       5
7    B     15        8       5
8    B     15        8       5</b>
Notice that the first row in the original data frame has been repeated three times and the second row has been repeated five times.
<b>Related:</b>  How to Use the slice() Function in dplyr 
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in dplyr:
 How to Select Columns by Index Using dplyr 
 How to Select the First Row by Group Using dplyr 
 How to Filter by Multiple Conditions Using dplyr 
 How to Filter Rows that Contain a Certain String Using dplyr 
<h2><span class="orange">What is a Representative Sample and Why is it Important?</span></h2>
In statistics, we’re often interested in studying characteristics of specific populations. For example, we might be interested in studying:
The overall job satisfaction of mechanical engineers in a certain city.
Political preferences of individuals in a certain county.
The age distribution of individuals in a certain country.
Movie preferences of students in a certain school.
In each of these examples, we want to gain an understanding of a certain  population .
<b>Population: </b>The entire group of individuals you are interested in studying.
Unfortunately, it can be expensive and time-consuming to gather data for every individual in a population, which is why researchers typically gather data for a <b>sample </b>from a population and then generalize the findings from the sample to the larger population.
<b>Sample: </b>A subset of the population.
For example, suppose we want to understand the movie preferences of students in a certain school that has 1,000 total students. Since it would take too long to survey every individual student, we might instead take a random sample of 100 students and ask them about their preferences.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/sample_population.jpg"581">
The 1,000 students represent the population, while the 100 randomly selected students represent the sample. Once we collect data for the sample of 100 students, we can then generalize those findings to the overall population of 1,000 students, <em>but only if our sample is representative of our population</em>.
<b>Representative sample:</b> A sample in which the characteristics of the individuals closely match the characteristics of the overall population.
Ideally, we want our sample to be like a “mini version” of our population. So, if the overall student population is composed of 50% girls and 50% boys, our sample would not be representative if it included 90% boys and only 10% girls.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/rep_sample1.jpg">
Or, if the overall population is composed of equal parts freshman, sophomores, juniors, and seniors, then our sample would not be representative if it only included freshman.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2019/12/rep_sample2.jpg">
<h2>The Importance of Obtaining a Representative Sample</h2>
The reason we want a representative sample is so that we can confidently generalize the findings from the sample to the population.
For example, suppose we want to know what percentage of students at a certain school prefer “drama” as their favorite movie genre. If the total student population is a mix of 50% boys and 50% girls, then a sample with a mix of 90% boys and 10% girls might lead to biased results if far fewer boys prefer drama as their favorite genre.
Or, if the total population is a mix of equal parts freshman, sophomores, juniors, and seniors, then a sample with only freshman might lead to biased results as well if younger students (e.g. freshman) tend to prefer drama at much higher rates than older students.
If the characteristics of individuals in our sample do not closely match the characteristics of the individuals in the overall population, then we cannot generalize the findings from the sample to the overall population with any confidence.
<h2>How to Obtain a Representative Sample</h2>
To maximize the chances that we obtain a representative sample, we need to focus on two things when obtaining our sample:
<h3>1. Use an appropriate sampling method.</h3>
There are  many ways to obtain a sample from a population , but here are three methods that are likely to obtain a representative sample:
<b>Simple random sample: </b>Randomly select individuals through the use of a random number generator or some means of random selection.
<b>Example:</b> Assign a number to all 1,000 students. Then, use a random number generator to select 100 random numbers and use the corresponding students as members in the sample.
<b>Benefit: </b>Simple random samples are usually representative of the population we’re interested in since every member has an equal chance of being included in the sample.
<b>Systematic random sample: </b>Put every member of a population into some order. Choose a random starting point and select every n<sup>th</sup> member to be in the sample.
<b>Example:</b> Create a list in alphabetical order based on the last name of all 1,000 students, randomly choose a starting point, and pick every 10th student to be in the sample.
<b>Benefit: </b>Systematic random samples are usually representative of the population we’re interested in since every member has an equal chance of being included in the sample.
<b>Stratified random sample:</b> Split a population into groups. Randomly select some members from each group to be in the sample.
<b>Example:</b> Split up all students according to their grade – freshman, sophomores, juniors, and seniors. Randomly select 25 students from each grade to be in the sample.
<b>Benefit: </b>Stratified random samples ensure that an equal number of students from each grade are included in the sample.
<h3>2. Make sure the sample is large enough.</h3>
Along with using an appropriate sampling method, it’s important to ensure that the sample is large enough so that we have enough data to generalize to the larger population.
For example, a sample of eight students – a boy and a girl from each grade – might represent a mini version of the larger population, but it’s probably not large enough to capture all of the variability that naturally exists in the responses of the students.
So, how large does your sample need to be?
That depends on the following factors:
<b>Population size:</b> In general, the larger the population size, the larger the sample needs to be. For example, you’ll need a much larger sample if you want to generalize your findings to an entire country compared to a single city.
<b>Confidence level:</b> How confident you want to be that the true population value you’re interested in falls within your confidence interval. Common confidence levels include 90%, 95%, and 99%. The higher the confidence level, the larger your sample needs to be.
<b>Margin of error:</b> How much error you’re willing to tolerate. No sample will be perfect, so you must be willing to accept at least some amount of error. Most research studies will report their findings with a margin of error, for example “40% of students reported that <em>drama </em>was their favorite movie genre, with a margin of error of +/- 5%.” The lower the margin of error, the smaller your sample needs to be.
There are plenty of sample size calculators online to help you determine how large your sample needs to be based on these factors.  This calculator from Survey Monkey  is particularly easy to use.
<h2>Things to Keep in Mind</h2>
Even if you use an appropriate sampling method and ensure that your sample is large enough, keep in mind the following things:
There will always be <em>some </em>sampling error. The sample will never be perfectly representative of the larger population.
In general, the larger the sample, the more likely it will be representative of the population.
You need to strike a balance between sample size and real-world variables like time and cost. A larger sample might have a higher chance of representing the overall population, but it might be more expensive and time-consuming to obtain.
<h2><span class="orange">How to Create a Residual Plot by Hand</span></h2>
A <b>residual plot</b> is a type of plot that displays the values of a predictor variable in a regression model along the x-axis and the values of the  residuals  along the y-axis.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand2.png">
This plot is used to assess whether or not the residuals in a regression model are  normally distributed  and whether or not they exhibit  heteroscedasticity .
The following step-by-step example shows how to create a residual plot for a regression model by hand.
<h3>Step 1: Find the Predicted Values</h3>
Suppose we want to fit a regression model to the following dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand1.png">
Using statistical software (like Excel, R, Python, SPSS, etc.) we can find that the fitted regression model is:
y = 10.4486 + 1.3037(x)
We can then use this model to predict the value of y, based on the value of x. For example, if x = 3, then we would predict y to be:
y = 10.4486 + 1.3037(3) = 14.359
We can repeat this process for every  observation  in our dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand3.png">
<h3>Step 2: Find the Residuals</h3>
A residual for a given observation in our dataset is calculated as:
Residual = observed value – predicted value
For example, the residual of the first observation would be calculated as:
Residual = 15 – 14.359 = 0.641
We can repeat this process for every observation in our dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand4.png">
<h3>Step 3: Create the Residual Plot</h3>
Lastly, we can create a residual plot by placing the x values along the x-axis and the residual values along the y-axis.
For example, the first point we’ll place in our plot is (3, 0.641)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand5.png">
The next point we’ll place in our plot is (5, 0.033)
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand6.png">
We’ll continue until we’ve placed all 10 pairwise combinations of x values and residual values in the plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand2.png">
Any point above zero in the plot represents a positive residual. This means the observed value for y is greater than the value predicted by the regression model.
Any point below zero represents a negative residual. This means the observed value for y is less than the value predicted by the regression model.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/05/residHand7.png">
Since the points in the plot are randomly scattered around a residual value of 0 with no clear pattern, this indicates that the relationship between x and y is linear and a linear regression model is appropriate to use.
And since the residuals don’t systematically increases or decrease as the predictor variable gets larger, this means  heteroskedasticity  is not a problem with this regression model.
<h2><span class="orange">How to Create a Residual Plot in Google Sheets</span></h2>
A <b>residual plot</b> is a type of plot that displays the fitted values against the residual values for a regression model.
This type of plot is often used to assess whether or not a linear regression model is appropriate for a given dataset and to check for  heteroscedasticity  of residuals.
The following step-by-step example explains how to create a residual plot for a simple linear regression model in Google Sheets.
<h3>Step 1: Enter the Data</h3>
First, let’s enter the following values for a dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle1.jpg"447">
<h3>Step 2: Calculate the Equation of the Regression Model</h3>
Next, we’ll use the <b>SLOPE</b> and <b>INTERCEPT</b> functions to calculate the equation of the simple linear regression model for this dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle2.jpg"478">
Using these values, we can write the following simple linear regression equation:
y = 29.631 + 0.755x
<h3>Step 3: Calculate the Predicted Values</h3>
Next, we can use the regression equation to calculate the predicted values for each observation.
We’ll type the following formula into cell <b>C2</b>:
<b>=$B$16+$B$15*A2
</b>
We can then copy and paste this formula down to every remaining cell in column <b>C</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle3.jpg"471">
<h3>Step 4: Calculate the Residuals</h3>
A  residual  is the difference between an observed value and a predicted value in a regression model.
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
To calculate the residual for each observation in our dataset, we can type the following formula into cell <b>D2</b>:
<b>=B2-C2
</b>
We can then copy and paste this formula down to every remaining cell in column <b>D</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle4.jpg"546">
<h3>Step 5: Create the Residual Plot</h3>
To create the residual plot, we can highlight the values in the range <b>A2:A13</b>, then hold the “Ctrl” key and highlight the values in the range <b>D2:D13</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle5.jpg"512">
Then click the <b>Insert</b> tab, then click <b>Chart</b> in the dropdown menu.
In the <b>Chart editor</b> panel that appears on the right side of the screen, choose <b>Scatter chart</b> as the Chart type:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle6.jpg"279">
The following residual plot will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/residgoogle7.jpg">
The x-axis displays the values for the predictor variable in our regression model and the y-axis displays the residuals.
One  key assumption  of linear regression is that the residuals have constant variance at every level of x, so we often use a residual plot to determine if this assumption is met.
If the residuals are roughly evenly scattered around zero in the plot with no clear pattern, then we typically say the assumption of constant variance is met.
In our residual plot above we can see that the points in the plot seem to be randomly scattered around zero with no clear pattern, thus we would conclude that the constant variance assumption is met for this particular regression model.
<h2><span class="orange">How to Create a Residual Plot in Python</span></h2>
A <b>residual plot</b> is a type of plot that displays the fitted values against the residual values for a  regression model .
This type of plot is often used to assess whether or not a linear regression model is appropriate for a given dataset and to check for  heteroscedasticity  of residuals.
This tutorial explains how to create a residual plot for a linear regression model in Python.
<h3>Example: Residual Plot in Python</h3>
For this example we’ll use a dataset that describes the attributes of 10 basketball players:
<b>import numpy as np
import pandas as pd
#create dataset
df = pd.DataFrame({'rating': [90, 85, 82, 88, 94, 90, 76, 75, 87, 86],   'points': [25, 20, 14, 16, 27, 20, 12, 15, 14, 19],   'assists': [5, 7, 7, 8, 5, 7, 6, 9, 9, 5],   'rebounds': [11, 8, 10, 6, 6, 9, 6, 10, 10, 7]})
#view dataset
df
ratingpointsassistsrebounds
09025511
1852078
28214710
3881686
4942756
5902079
6761266
77515910
88714910
9861957</b>
<h3>Residual Plot for Simple Linear Regression</h3>
Suppose we fit a simple linear regression model using <em>points </em>as the predictor variable and <em>rating </em>as the response variable:
<b>#import necessary libraries 
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.formula.api import ols
#fit simple linear regression model
model = ols('rating ~ points', data=df).fit()
#view model summary
print(model.summary())
</b>
We can create a residual vs. fitted plot by using the  plot_regress_exog() function  from the statsmodels library:
<b>#define figure size
fig = plt.figure(figsize=(12,8))
#produce regression plots
fig = sm.graphics.plot_regress_exog(model, 'points', fig=fig)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/residplotpython1.png">
Four plots are produced. The one in the top right corner is the residual vs. fitted plot. The x-axis on this plot shows the actual values for the predictor variable <em>points</em> and the y-axis shows the residual for that value.
Since the residuals appear to be randomly scattered around zero, this is an indication that heteroscedasticity is not a problem with the predictor variable.
<h3>Residual Plots for Multiple Linear Regression</h3>
Suppose we instead fit a multiple linear regression model using <em>assists </em>and <em>rebounds </em>as the predictor variable and <em>rating </em>as the response variable:
<b>#fit multiple linear regression model
model = ols('rating ~ assists + rebounds', data=df).fit()
#view model summary
print(model.summary())
</b>
Once again we can create a residual vs. predictor plot for each of the individual predictors using the  plot_regress_exog() function  from the statsmodels library.
For example, here’s what the residual vs. predictor plot looks like for the predictor variable <em>assists</em>:
<b>#create residual vs. predictor plot for 'assists'
fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_regress_exog(model, 'assists', fig=fig)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/residplotpython2.png">
And here’s what the residual vs. predictor plot looks like for the predictor variable <em>rebounds</em>:
<b>#create residual vs. predictor plot for 'assists'
fig = plt.figure(figsize=(12,8))
fig = sm.graphics.plot_regress_exog(model, 'rebounds', fig=fig)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/residplotpython3.png">
 In both plots the residuals appear to be randomly scattered around zero, which is an indication that heteroscedasticity is not a problem with either predictor variable in the model.
<h2><span class="orange">How to Create a Residual Plot in R</span></h2>
<b>Residual plots</b> are often used to assess whether or not the  residuals  in a regression analysis are normally distributed and whether or not they exhibit  heteroscedasticity .
This tutorial explains how to create residual plots for a regression model in R.
<h3>Example: Residual Plots in R</h3>
In this example we will fit a regression model using the built-in R dataset <b>mtcars </b>and then produce three different residual plots to analyze the residuals.
<b>Step 1: Fit regression model.</b>
First, we will fit a regression model using <b>mpg </b>as the response variable and <b>disp</b> and <b>hp</b> as explanatory variables:
<b>#load the dataset
data(mtcars)
#fit a regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#get list of residuals 
res &lt;- resid(model)
</b>
<b>Step 2: Produce residual vs. fitted plot.</b>
Next, we will produce a residual vs. fitted plot, which is helpful for visually detecting heteroscedasticity – e.g. a systematic change in the spread of residuals over a range of values. 
<b>#produce residual vs. fitted plot
plot(fitted(model), res)
#add a horizontal line at 0 
abline(0,0)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/residPlotR1.png">
The x-axis displays the fitted values and the y-axis displays the residuals. From the plot we can see that the spread of the residuals tends to be higher for higher fitted values, but it doesn’t look serious enough that we would need to make any changes to the model.
<b>Step 3: Produce a Q-Q plot.</b>
We can also produce a Q-Q plot, which is useful for determining if the residuals follow a normal distribution. If the data values in the plot fall along a roughly straight line at a 45-degree angle, then the data is normally distributed.
<b>#create Q-Q plot for residuals
qqnorm(res)
#add a straight diagonal line to the plot
qqline(res) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/residPlotR2.png">
We can see that the residuals tend to stray from the line quite a bit near the tails, which could indicate that they’re not normally distributed.
<b>Step 4: Produce a density plot.</b>
We can also produce a density plot, which is also useful for visually checking whether or not the residuals are normally distributed. If the plot is roughly bell-shaped, then the residuals likely follow a normal distribution.
<b>#Create density plot of residuals
plot(density(res))
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/residPlotR3.png">
We can see that the density plot roughly follows a bell shape, although it is slightly skewed to the right. Depending on the type of study, a researcher may or may not decide to perform a transformation on the data to ensure that the residuals are more normally distributed.
<h2><span class="orange">How to Create a Residual Plot on a TI-84 Calculator</span></h2>
 A <b>residual plot</b> is used to assess whether or not the  residuals  in a regression analysis are normally distributed and whether or not they exhibit  heteroscedasticity .
This tutorial provides a step-by-step example of how to create a residual plot for the following dataset on a TI-84 calculator:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI1.png">
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values.
Press Stat, then press EDIT. Then enter the x-values of the dataset in column L1 and the y-values in column L2:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI2.png">
<h3>Step 2: Perform Linear Regression</h3>
Next, we will fit a linear regression model to the dataset.
Press Stat, then scroll over to CALC. Then scroll down to LinReg(ax+b) and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI3.png">
Press ENTER once again to perform linear regression:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI4.png">
The fitted regression model is: y = 7.397 + 1.389x
<h3>Step 3: Create the Residual Plot</h3>
Next, press 2nd and then press Y=. In the new screen that appears, press ENTER on the first plot option.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI5.png">
Hover over the “On” option and press press ENTER. Then scroll down to YList and press 2nd and then press STAT. Then press “7” to choose the residuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI6.png">
The term “RESID” will then appear next to Ylist:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI7.png">
Lastly, press ZOOM and then scroll down to ZoomStat and press ENTER.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI8.png">
The residual plot will appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI9.png">
The x-axis displays the x values from the dataset and the y-axis displays the residuals from the regression model.
To see the actual values of the residuals, press 2nd and then press STAT. Then press “7” to choose the residuals:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI6.png">
Press ENTER once more to display the residuals.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residTI10.png">
Scroll to the right to see the values for each of the residuals.
<h2><span class="orange">How to Calculate Residual Standard Error in R</span></h2>
Whenever we fit a linear regression model in R, the model takes on the following form:
Y = β<sub>0</sub> + β<sub>1</sub>X + … + β<sub>i</sub>X +<U+03F5>
where <U+03F5> is an error term that is independent of X.
No matter how well X can be used to predict the values of Y, there will always be some random error in the model. One way to measure the dispersion of this random error is to use the <b>residual standard error</b>, which is a way to measure the standard deviation of the residuals <U+03F5>.
The residual standard error of a regression model is calculated as:
<b>Residual standard error = √SS<sub>residuals</sub> / df<sub>residuals</sub></b>
where:
<b>SS<sub>residuals</sub></b>: The residual sum of squares.
<b>df<sub>residuals</sub></b>: The residual degrees of freedom, calculated as n – k – 1 where n = total observations and k = total model parameters.
There are three methods we can use to calculate the residual standard error of a regression model in R.
<h3>Method 1: Analyze the Model Summary</h3>
The first way to obtain the residual standard error is to simply fit a linear regression model and then use the <b>summary() </b>command to obtain the model results. Then, just look for “residual standard error” near the bottom of the output:
<b>#load built-in <em>mtcars </em>dataset
data(mtcars)
#fit regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#view model summary
summary(model)
Call:
lm(formula = mpg ~ disp + hp, data = mtcars)
Residuals:
    Min      1Q  Median      3Q     Max 
-4.7945 -2.3036 -0.8246  1.8582  6.9363 
Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
disp        -0.030346   0.007405  -4.098 0.000306 ***
hp          -0.024840   0.013385  -1.856 0.073679 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 3.127 on 29 degrees of freedom
Multiple R-squared:  0.7482,Adjusted R-squared:  0.7309 
F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09</b>
We can see that the residual standard error is <b>3.127</b>.
<h3>Method 2: Use a Simple Formula</h3>
Another way to obtain the residual standard error (RSE) is to fit a linear regression model and then use the following formula to calculate RSE:
<b>sqrt(deviance(model)/df.residual(model))
</b>
Here is how to implement this formula in R:
<b>#load built-in <em>mtcars </em>dataset
data(mtcars)
#fit regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#calculate residual standard error
sqrt(deviance(model)/df.residual(model))
[1] 3.126601
</b>
We can see that the residual standard error is <b>3.126601</b>.
<h3>Method 3: Use a Step-By-Step Formula</h3>
Another way to obtain the residual standard error is to fit a linear regression model and then use a step-by-step approach to calculate each individual component of the formula for RSE:
<b>#load built-in <em>mtcars </em>dataset
data(mtcars)
#fit regression model
model &lt;- lm(mpg~disp+hp, data=mtcars)
#calculate the number of model parameters - 1
k=length(model$coefficients)-1
#calculate sum of squared residuals
SSE=sum(model$residuals**2)
#calculate total observations in dataset
n=length(model$residuals)
#calculate residual standard error
sqrt(SSE/(n-(1+k)))
[1] 3.126601
</b>
We can see that the residual standard error is <b>3.126601</b>.
<h3>How to Interpret the Residual Standard Error</h3>
As mentioned before, the residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model.
The lower the value for RSE, the more closely a model is able to fit the data (but be careful of  overfitting ). This can be a useful metric to use when comparing two or more models to determine which model best fits the data.
<h2><span class="orange">Residual Sum of Squares Calculator</span></h2>
This calculator finds the residual sum of squares of a regression equation based on values for a predictor variable and a response variable.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">6, 7, 7, 8, 12, 14, 15, 16, 16, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">14, 15, 15, 17, 18, 18, 16, 14, 11, 8</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Residual Sum of Squares (SSE): 68.7878</b>
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
var xbar = math.mean(x);
var ybar = math.mean(y);
let xbar2_hold = 0
for (let i = 0; i < x.length; i++) {
xbar2_hold += Math.pow(x[i], 2);
}
var xbar2 = xbar2_hold / x.length;
let sxx = 0
for (let i = 0; i < x.length; i++) {
sxx += Math.pow(x[i] - xbar, 2);
}
let syy = 0
for (let i = 0; i < y.length; i++) {
syy += Math.pow(y[i] - ybar, 2);
}
let sxy = 0
for (let i = 0; i < x.length; i++) {
sxy += (x[i] - xbar)*(y[i]-ybar);
}
let sxx2 = 0
for (let i = 0; i < x.length; i++) {
sxx2 += (x[i] - xbar)*(Math.pow(x[i], 2)-xbar2);
}
let sx2x2 = 0
for (let i = 0; i < x.length; i++) {
sx2x2 += Math.pow((Math.pow(x[i], 2)-xbar2), 2);
}
let sx2y = 0
for (let i = 0; i < x.length; i++) {
sx2y += (Math.pow(x[i], 2)-xbar2)*(y[i]-ybar);
}
var b = ((sxy*sx2x2)-(sx2y*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var c = ((sx2y*sxx)-(sxy*sxx2)) / ((sxx*sx2x2)-Math.pow(sxx2, 2));
var a = ybar - (b*xbar) - (c*xbar2);
var sst = syy;
var ssr = (sxy/sxx)*sxy;
var sse = sst-ssr;
document.getElementById('sse').innerHTML = sse.toFixed(4);
}
//output error message if boths lists are not equal
else {
 //document.getElementById('out').innerHTML = '';
 document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate Residual Sum of Squares in Excel</span></h2>
A  residual  is the difference between an observed value and a predicted value in a regression model.
It is calculated as:
Residual = Observed value – Predicted value
One way to understand how well a regression model fits a dataset is to calculate the <b>residual sum of squares</b>, which is calculated as:
Residual sum of squares = Σ(e<sub>i</sub>)<sup>2</sup>
where:
<b>Σ</b>: A Greek symbol that means “sum”
<b>e<sub>i</sub></b>: The i<sup>th</sup> residual
The lower the value, the better a model fits a dataset.
This tutorial provides examples of how to calculate the residual sum of squares for a simple linear regression model and a multiple linear regression model in Excel.
<h3>Example 1: Residual Sum of Squares for Simple Linear Regression</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/rssExcel2.png">
To calculate the residual sum of squares for a simple linear regression model using x as the predictor variable and y as the response variable we can use the  LINEST()  function, which uses the following syntax:
<b>LINEST(known_ys, [known_xs], [const], [stats])</b>
where:
<b>known_ys:</b> The range of y-values
<b>known_sx:</b> The range of x-values
<b>const:</b> Whether to force the constant b to be zero. We will leave this blank.
<b>stats:</b> A list of regression statistics. We will specify this to be TRUE.
The following screenshot shows how to use this function in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/rssExcel1.png">
The residual sum of squares for the regression model is displayed in the last cell of the second column of the output. In this example, the residual sum of squares turns out to be <b>50.75</b>.
<h3>Example 2: Residual Sum of Squares for Multiple Linear Regression</h3>
Suppose we have the following dataset in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/rssExcel3.png">
Once again we can use the LINEST() function to calculate the residual sum of squares for the model.
The only difference is that we’ll specify two columns of values for the <b>known_xs</b> argument:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/01/rssExcel4.png">
The residual sum of squares for this multiple linear regression model turns out to be <b>49.83</b>.
<h2><span class="orange">How to Calculate Residual Sum of Squares in Python</span></h2>
A  residual  is the difference between an observed value and a predicted value in a regression model.
It is calculated as:
Residual = Observed value – Predicted value
One way to understand how well a regression model fits a dataset is to calculate the <b>residual sum of squares</b>, which is calculated as:
Residual sum of squares = Σ(e<sub>i</sub>)<sup>2</sup>
where:
<b>Σ</b>: A Greek symbol that means “sum”
<b>e<sub>i</sub></b>: The i<sup>th</sup> residual
The lower the value, the better a model fits a dataset.
This tutorial provides a step-by-step example of how to calculate the residual sum of squares for a regression model in Python.
<h3>Step 1: Enter the Data</h3>
For this example we’ll enter data for the number of hours spent studying, total prep exams taken, and exam score received by 14 different students:
<b>import pandas as pd
#create DataFrame
df = pd.DataFrame({'hours': [1, 2, 2, 4, 2, 1, 5, 4, 2, 4, 4, 3, 6, 5],   'exams': [1, 3, 3, 5, 2, 2, 1, 1, 0, 3, 4, 3, 2, 4],   'score': [76, 78, 85, 88, 72, 69, 94, 94, 88, 92, 90, 75, 96, 90]})
</b>
<h3>Step 2: Fit the Regression Model</h3>
Next, we’ll use the  OLS() function  from the statsmodels library to perform ordinary least squares regression, using “hours” and “exams” as the predictor variables and “score” as the response variable:
<b>import statsmodels.api as sm
#define response variable
y = df['score']
#define predictor variables
x = df[['hours', 'exams']]
#add constant to predictor variables
x = sm.add_constant(x)
#fit linear regression model
model = sm.OLS(y, x).fit()
#view model summary
print(model.summary())
            OLS Regression Results                            
==============================================================================
Dep. Variable:                  score   R-squared:                       0.722
Model:                            OLS   Adj. R-squared:                  0.671
Method:                 Least Squares   F-statistic:                     14.27
Date:                Sat, 02 Jan 2021   Prob (F-statistic):           0.000878
Time:                        15:58:35   Log-Likelihood:                -41.159
No. Observations:                  14   AIC:                             88.32
Df Residuals:                      11   BIC:                             90.24
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
============================================================================== coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         71.8144      3.680     19.517      0.000      63.716      79.913
hours          5.0318      0.942      5.339      0.000       2.958       7.106
exams         -1.3186      1.063     -1.240      0.241      -3.658       1.021
==============================================================================
Omnibus:                        0.976   Durbin-Watson:                   1.270
Prob(Omnibus):                  0.614   Jarque-Bera (JB):                0.757
Skew:                          -0.245   Prob(JB):                        0.685
Kurtosis:                       1.971   Cond. No.                         12.1
==============================================================================
</b>
<h3>Step 3: Calculate the Residual Sum of Squares</h3>
We can use the following code to calculate the residual sum of squares for the model:
<b>print(model.ssr)
293.25612951525414
</b>
The residual sum of squares turns out to be <b>293.256</b>.
<h2><span class="orange">How to Calculate Residual Sum of Squares in R</span></h2>
A  residual  is the difference between an observed value and a predicted value in a regression model.
It is calculated as:
Residual = Observed value – Predicted value
One way to understand how well a regression model fits a dataset is to calculate the <b>residual sum of squares</b>, which is calculated as:
Residual sum of squares = Σ(e<sub>i</sub>)<sup>2</sup>
where:
<b>Σ</b>: A Greek symbol that means “sum”
<b>e<sub>i</sub></b>: The i<sup>th</sup> residual
The lower the value, the better a model fits a dataset.
We can easily calculate the residual sum of squares for a regression model in R by using one of the following two methods:
<b>#build regression model
model &lt;- lm(y ~ x1 + x2 + ..., data = df)
#calculate residual sum of squares (method 1)
deviance(model)
#calculate residual sum of squares (method 2)
sum(resid(model)^2)
</b>
Both methods will produce the exact same results.
The following example shows how to use these functions in practice.
<h3>Example: Calculating Residual Sum of Squares in R</h3>
For this example, we’ll use the built-in <b>mtcars</b> dataset in R:
<b>#view first six rows of mtcars dataset
head(mtcars)
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
</b>
The following code shows how to fit a multiple linear regression model for this dataset and calculate the residual sum of squares of the model:
<b>#build multiple linear regression model
model &lt;- lm(mpg ~ wt + hp, data = mtcars)
#calculate residual sum of squares (method 1)
deviance(model)
[1] 195.0478
#calculate residual sum of squares (method 2)
sum(resid(model)^2)
[1] 195.0478</b>
We can see that the residual sum of squares turns out to be <b>195.0478</b>.
If we have two competing models, we can calculate the residual sum of squares for both to determine which one fits the data better:
<b>#build two different models
model1 &lt;- lm(mpg ~ wt + hp, data = mtcars)
model2 &lt;- lm(mpg ~ wt + disp, data = mtcars)
#calculate residual sum of squares for both models
deviance(model1)
[1] 195.0478
deviance(model2)
[1] 246.6825 
</b>
We can see that the residual sum of squares for model 1 is lower, which indicates that it fits the data better than model 2.
We can confirm this by calculating the  R-squared  of each model:
<b>#build two different models
model1 &lt;- lm(mpg ~ wt + hp, data = mtcars)
model2 &lt;- lm(mpg ~ wt + disp, data = mtcars)
#calculate R-squared for both models
summary(model1)$r.squared
[1] 0.8267855
summary(model2)$r.squared
[1] 0.7809306</b>
The R-squared for model 1 turns out to be higher, which indicates that it’s able to explain more of the variance in the response values compared to model 2.
<h2><span class="orange">What is Residual Variance? (Definition & Example)</span></h2>
<b>Residual variance</b> (sometimes called “unexplained variance”) refers to the variance in a model that cannot be explained by the variables in the model.
The higher the residual variance of a model, the less the model is able to explain the variation in the data.
Residual variance appears in the output of two different statistical models:
<b>1. ANOVA:</b> Used to compare the means of three or more independent groups.
<b>2. Regression:</b> Used to quantify the relationship between one or more predictor variables and a  response variable .
The following examples show how to interpret residual variance in each of these methods.
<h3>Residual Variance in ANOVA Models</h3>
Whenever we fit an ANOVA (“analysis of variance”) model, we end up with an ANOVA table that looks like the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residualVar1.png">
The value for the residual variance of the ANOVA model can be found in the SS (“sum of squares”) column for the <b>Within Groups</b> variation.
This value is also referred to as “sum of squared errors” and is calculated using the following formula:
Σ(X<sub>ij</sub> – X<sub>j</sub>)<sup>2</sup> 
where:
<b>Σ</b>: a greek symbol that means “sum”
<b>X<sub>ij</sub></b>: the i<sup>th</sup> observation in group j
<b>X<sub>j</sub></b>: the mean of group j
In the ANOVA model above we see that the residual variance is 1,100.6.
To determine if this residual variance is “high” we can calculate the mean sum of squared for within groups and mean sum of squared for between groups and find the ratio between the two, which results in the overall F-value in the ANOVA table.
F = MS<sub>between</sub> / MS<sub>within</sub>
F = 96.1 / 40.76296
F = 2.357
The F-value in the ANOVA table above is 2.357 and the corresponding p-value is 0.113848. Since this p-value is not less than α = .05, we do not have sufficient evidence to reject the null hypothesis.
This means we don’t have sufficient evidence to say that the mean difference between the groups we’re comparing is significantly different.
This tells us that the residual  variance in the ANOVA model is high relative to the variation that the model actually can explain.
<h3>Residual Variance in Regression Models</h3>
In a regression model, the residual variance is defined as the sum of squared differences between predicted data points and observed data points.
It is calculated as:
Σ(<U+0177><sub>i</sub> – y<sub>i</sub>)<sup>2</sup>
where:
<b>Σ</b>: a greek symbol that means “sum”
<b><U+0177><sub>i</sub>:</b> The predicted data points
<b>y<sub>i</sub>:</b> The observed data points
When we fit a regression model, we typically end up with output that looks like the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/residualVar2-1.png">
The value for the residual variance of the ANOVA model can be found in the SS (“sum of squares”) column for the Residual variation.
The ratio of the residual variation relative to the total variation in the model tells us the percentage of variation in the response variable that can’t be explained by the predictor variables in the model.
For example, in the table above we would calculate this percentage as:
Unexplained variation = SS Residual / SS Total
Unexplained variation = 5.9024 / 174.5
Unexplained variation = .0338
We can also calculate this value using the following formula:
Unexplained variation = 1 – R<sup>2</sup>
Unexplained variation = 1 – 0.96617
Unexplained variation = .0338
The R-squared value for the model tells us the percentage of variation in the response variable that can be explained by the predictor variable.
Thus, the lower the unexplained variation, the better a model is able to use the predictor variables to explain the variation in the response variable.
<h2><span class="orange">Residuals Calculator</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in a regression model. It is calculated as:
Residual = Observed value – Predicted value
This calculator finds the residuals for each observation in a simple linear regression model.
Simply enter a list of values for a predictor variable and a response variable in the boxes below, then click the “Calculate” button:
<b>Predictor values:</b>
<textarea id="x" rows="5" cols="40">1, 3, 3, 5, 7, 13, 15, 19</textarea>
<b>Response values:</b>
<textarea id="y" rows="5" cols="40">7, 7, 12, 13, 18, 24, 29, 33</textarea>
<input type="button" id="buttonCalc" onclick="calc()" value="Calculate">
<b>Linear Regression Equation:</b>
<U+0177> = 5.6631 + (1.4802)*x
<b>List of Residuals:</b>
-0.143
-3.104
1.896
-0.064
1.975
-0.906
1.133
-0.787
<script>
function calc() {
//get input data
var x = document.getElementById('x').value.split(',').map(Number);
var y = document.getElementById('y').value.split(',').map(Number);
//check that both lists are equal length
if (x.length - y.length == 0) {
document.getElementById('error_msg').innerHTML = '';
function linearRegression(y,x){
        var lr = {};
        var n = y.length;
        var sum_x = 0;
        var sum_y = 0;
        var sum_xy = 0;
        var sum_xx = 0;
        var sum_yy = 0;
        for (var i = 0; i < y.length; i++) {
            sum_x += x[i];
            sum_y += y[i];
            sum_xy += (x[i]*y[i]);
            sum_xx += (x[i]*x[i]);
            sum_yy += (y[i]*y[i]);
        } 
        lr['slope'] = (n * sum_xy - sum_x * sum_y) / (n*sum_xx - sum_x * sum_x);
        lr['intercept'] = (sum_y - lr.slope * sum_x)/n;
        return lr;
}
var lr = linearRegression(y, x);
var a = lr.slope;
var b = lr.intercept;
//calculate residuals
residuals = [];
for (var obs = 0; obs < y.length; obs++) {
this_resid = (y[obs] - (b - (-1*a*x[obs]))).toFixed(3);
residuals.push(this_resid);
}
document.getElementById('a').innerHTML = a.toFixed(4);
document.getElementById('b').innerHTML = b.toFixed(4);
document.getElementById('resids_out').innerHTML = residuals.toString().split(',').join("<br />");
}
//output error message if both lists are not equal
else {
document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">What is a Residuals vs. Leverage Plot? (Definition & Example)</span></h2>
A <b>residuals vs. leverage plot</b> is a type of  diagnostic plot  that allows us to identify influential observations in a regression model.
Here is how this type of plot appears in the statistical programming language R:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic2.png">
Each observation from the dataset is shown as a single point within the plot. The x-axis shows the leverage of each point and the y-axis shows the standardized residual of each point.
<b>Leverage</b> refers to the extent to which the coefficients in the regression model would change if a particular observation was removed from the dataset.
Observations with high leverage have a strong influence on the coefficients in the regression model. If we remove these observations, the coefficients of the model would change noticeably.
<b>Standardized residuals</b> refer to the standardized difference between a predicted value for an observation and the actual value of the observation.
It’s worth noting that an observation can have a high absolute value for a standardized residual, yet have a low value for leverage.
<h3>How to Interpret a Residuals vs. Leverage Plot</h3>
If any point in this plot falls outside of Cook’s distance (the red dashed lines) then it is considered to be an influential observation.
Let’s refer to the residuals vs. leverage plot from earlier:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/07/diagnostic2.png">
In the example above, we can see that observation #10 lies closest to the border of Cook’s distance, but it doesn’t fall outside of the dashed line. This means <b>there are not any influential points</b> in our regression model.
However, suppose we had the following residuals vs. leverage plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/lev1.png">
We can see that observation #1 in the top right corner falls outside of the red dashed lines. This indicates that <b>it is an influential point</b>.
This means that if we removed this observation from our dataset and fit the regression model again, the coefficients of the model would change significantly.
<h3>How to Handle Influential Observations</h3>
If you create a residuals vs. leverage plot for a model and you find that one or more observations are identified as influential, there are a few things you can do:
<b>1. Verify that the observation is not an error.</b>
Before you take any action, you should first verify that the influential observation(s) are not a result of a data entry error or some other odd occurrence.
<b>2. Attempt to fit another regression model.</b>
Influential observations could indicate that the model you specified does not provide a good fit to the data. In this case, you may try a  polynomial regression model  or a nonlinear model.
<b>3. Remove the influential observations.</b>
Lastly, you may decide to simply remove the influential observations if the model you specified seems to fit the data well except for the one or two influential observations.
<h2><span class="orange">What Are Residuals in Statistics?</span></h2>
A <b>residual</b> is the difference between an observed value and a predicted value in  regression analysis .
It is calculated as:
<b>Residual = Observed value – Predicted value</b>
Recall that the goal of linear regression is to quantify the relationship between one or more predictor variables and a  response variable . To do this, linear regression finds the line that best “fits” the data, known as the <em>least squares regression line</em>.
This line produces a prediction for each  observation  in the dataset, but it’s unlikely that the prediction made by the regression line will <em>exactly</em> match the observed value.
The difference between the prediction and the observed value is the residual. If we plot the observed values and overlay the fitted regression line, the residuals for each observation would be the vertical distance between the observation and the regression line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals1-1.png">
An observation has a <b>positive residual</b> if its value is greater than the predicted value made by the regression line. 
Conversely, an observation has a <b>negative residual</b> if its value is less than the predicted value made by the regression line.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals2.png">
Some observations will have positive residuals while others will have negative residuals, but all of the residuals will add up to <b>zero</b>.
<h3>Example of Calculating Residuals</h3>
Suppose we have the following dataset with 12 total observations:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals3.png">
If we use some statistical software (like  R ,  Excel ,  Python ,  Stata , etc.) to fit a linear regression line to this dataset, we’ll find that the line of best fit turns out to be:
<b>y = 29.63 + 0.7553x</b>
Using this line, we can calculate the predicted value for each Y value based on the value of X. For example, the predicted value of the first observation would be:
y = 29.63 + 0.7553*(8) = <b>35.67</b>
We can then calculate the residual for this observation as:
Residual = Observed value – Predicted value = 41 – 35.67 = <b>5.33</b>
We can repeat this process to find the residual for every single observation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals4.png">
If we create a  scatterplot  to visualize the observations along with the fitted regression line, we’ll see that some of the observations lie above the line while some fall below the line:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals5.png">
<h3>Properties of Residuals</h3>
Residuals have the following properties:
Each observation in a dataset has a corresponding residual. So, if a dataset has 100 total observations then the model will produce 100 predicted values, which results in 100 total residuals.
The sum of all residuals adds up to zero.
The mean value of the residuals is zero.
<h3>How Are Residuals Used in Practice?</h3>
In practice, residuals are used for three different reasons in regression:
<b>1. Assess model fit.</b>
Once we produce a fitted regression line, we can calculate the <em>residuals sum of squares (RSS)</em>, which is the sum of all of the squared residuals. The lower the RSS, the better the regression model fits the data.
<b>2. Check the assumption of normality.</b>
One of the key  assumptions of linear regression  is that the residuals are normally distributed.
To check this assumption, we can create a Q-Q plot, which is a type of plot that we can use to determine whether or not the residuals of a model follow a normal distribution.
If the points on the plot roughly form a straight diagonal line, then the normality assumption is met.
<figure id="attachment_9410" aria-describedby="caption-attachment-9410" style="width: 493px"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/07/qqplotpython1.png"><figcaption id="caption-attachment-9410"><b>Example of a Q-Q plot</b></figcaption></figure><b>3. Check the assumption of homoscedasticity.</b>
Another key assumption of linear regression is that the residuals have constant variance at every level of x. This is known as homoscedasticity. When this is not the case, the residuals are said to suffer from  heteroscedasticity .
To check if this assumption is met, we can create a  residual plot , which is a scatterplot that shows the residuals vs. the predicted values of the model.
<figure id="attachment_12427" aria-describedby="caption-attachment-12427" style="width: 496px"><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/residuals6.png"caption-attachment-12427"><b>Example of residual vs. fitted values plot</b></figcaption></figure>If the residuals are roughly evenly scattered around zero in the plot with no clear pattern, then we typically say the assumption of homoscedasticity is met.
<h2><span class="orange">What is Restriction of Range?</span></h2>
Often in statistics we’re interested in measuring the  correlation  between two variables. This helps us understand the following:
The <b>direction</b> of the relationship between two variables. <em>As one variable increases, does the other variable tend to increase or decrease?</em>
The <b>strength</b> of the relationship between two variables. <em>How closely do the two variables change in value?</em>
Unfortunately, one problem that can occur when measuring the correlation between two variables is known as <b>restriction of range</b>. This occurs when the <em>range</em> of values measured for one of the variables is restricted for some reason.
For example, suppose we’d like to measure the correlation between <em>hours studied</em> and <em>exam score</em> for students at a particular school.
If we collect data on these two variables for all 1,000 students in the school, we may find that the correlation between <em>hours studied</em> and <em>exam score</em> is <b>0.73</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/restrictedRange0.png">
This correlation is quite high, which indicates a strong positive relationship between the two variables. As students study more, they tend to earn higher exam scores.
However, suppose we only collected data for students in Honors courses. It might turn out that all of these students studied for at least 6 hours.
Thus, if we calculate the correlation between <em>hours studied</em> and <em>exam score</em> for these students, we would be using a restricted range for the variable <em>hours studied</em>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/restrictedRange1.png">
If we zoom in on the scatterplot for the range where <em>Hours</em> is greater than 6, here’s what the plot looks like:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/restrictedRange2.png">
The correlation between the two variables on this plot turns out to be <b>0.37</b>, which is significantly lower than <b>0.73</b>.
Thus, if we only collected data on <em>hours studied</em> and <em>exam score</em> for students in Honors courses then we might assume that there is a weak relationship between hours studied and exam score.
However, this result would be misleading because we used a <b>restricted range</b> for one of the variables.
<h3>Real-World Examples of Restricted Range</h3>
The problem of a restricted range can occur in many different research studies in practice. Here are a couple examples:
<b>1. Studies of high-performance athletes</b>. Researchers may be interested in studying whether or not a certain workout regimen produces more muscle mass than some standard regimen.
If the researchers only collect data for high-performance athletes, it’s likely that these athletes all have a high amount of muscle mass already so there will be a narrow range of values available to calculate the correlation between the workout regimen and the muscle mass produced.
<b>2. Studies of high-performance students.</b> Researchers may be interested in studying whether or not a certain tutoring program has a positive effect on grades. By nature, students who are eager to improve their grades and participate in the tutoring program may already be high-performance students.
Thus, there may not be much room for improvement in grades among these students. When researchers calculate the correlation between the hours spent in the tutoring program and the resulting increase in grades, the true correlation may be understated because the range for improvement in grades has been restricted.
<h3>How to Account for Restricted Ranges</h3>
One popular way to account for restricted ranges is known as <b>Thorndike’s Case 2</b>, a formula developed by psychometrician Robert L. Thorndike.
This formula provides an estimate of the true correlation between two variables and uses the following calculation:
True correlation = √(1-(SD<sup>2</sup><sub>y restricted</sub>-SD<sup>2</sup><sub>y unrestricted</sub>)) * (1-r<sup>2</sup><sub>restricted</sub>)
where:
SD<sup>2</sup><sub>y restricted</sub>: The squared standard deviation of the available data on the response variable y.
SD<sup>2</sup><sub>y unrestricted</sub>: The known squared standard deviation of the response variable for the population.
r<sup>2</sup><sub>restricted</sub>: The squared correlation on the available restricted data.
This formula has been shown to be effective at producing unbiased estimates of the true correlation between two variables when one of the variables suffers from having a restricted range.
Note that in order to use this formula, you need to have an estimate of the true population standard deviation for the response variable.
<h2><span class="orange">How to Retrieve Row Numbers in R (With Examples)</span></h2>
Often you may want to get the row numbers in a data frame in R that contain a certain value. Fortunately this is easy to do using the <b>which()</b> function.
This tutorial shows several examples of how to use this function in practice.
<h3>Example 1: Get Row Numbers that Match a Certain Value</h3>
Suppose we have the following data frame in R:
<b>#create data frame
df = data.frame(points=c(25, 12, 15, 14, 19),
                assists=c(5, 7, 7, 9, 12),
                team=c('Mavs', 'Mavs', 'Spurs', 'Celtics', 'Warriors'))
#view data frame
df
  points assists      team
1     25       5      Mavs
2     12       7      Mavs
3     15       7     Spurs
4     14       9   Celtics
5     19      12  Warriors
</b>
We can use the following syntax to get the row numbers where ‘team’ is equal to Mavs:
<b>#get row numbers where 'team' is equal to Mavs
which(df$team == 'Mavs')
[1] 1 2
</b>
We can see that the team name is equal to ‘Mavs’ at row numbers <b>0 </b>and <b>1</b>.
We can also use the  %in% operator  to get the row numbers where the team name is in a certain list of team names:
<b>#get row numbers where 'team' is equal to Mavs or Spurs
which(df$team %in% c('Mavs', 'Spurs'))
[1] 1 2 3
</b>
We can see that the team name is equal to ‘Mavs’ or ‘Spurs’ at rows numbers <b>1</b>, <b>2</b>, and <b>3</b>.
<h3>Example 2: Get Sum of Row Numbers</h3>
If we want to know the total number of rows where a column is equal to a certain value, we can use the following syntax:
<b>#find total number of rows where team is equal to Mavs
length(which(df$team == 'Mavs'))
[1] 2</b>
We can see that team is equal to ‘Mavs’ in a total of <b>2 </b>rows.
<h3>Example 3: Return Data Frame with Certain Rows</h3>
And if we’d like to return a data frame where the rows in one column are equal to a certain value, we can use the following syntax:
<b>#return data frame containing rows that have team equal to 'Mavs'
df[which(df$team == 'Mavs'), ]
  points assists team
1     25       5 Mavs
2     12       7 Mavs</b>
Notice that only the two rows where team is equal to ‘Mavs’ are returned.
<h2><span class="orange">Reverse Causation: Definition & Examples</span></h2>
<b>Reverse causation</b> occurs when you believe that X causes Y, but in reality Y actually causes X.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/reverseCausation1.png">
This is a common error that many people make when they look at two phenomenon and wrongly assume that one is the cause while the other is the effect.
<h3>Example 1: Smoking & Depression</h3>
One common error of reverse causation involves smoking and depression.
In an observational study, researchers may observe that people who smoke more tend to be more depressed. Thus, they may naively assume that smoking <em>causes </em>depression.
However, it’s possible that researchers are getting this backwards and in reality depression actually causes people to smoke because they view it as a way to alleviate their negative emotions and blow off some stream.
<h3>Example 2: Income & Happiness</h3>
Another common error of reverse causation involves annual income and reported happiness levels.
In an observational study, researchers may observe that people who earn higher annual incomes may also report being happier overall in life. Thus, they may simply assume that higher income leads to more happiness.
However, in reality it may be true that people who are naturally happier tend to be better workers and thus earn higher incomes. Thus, researchers may actually get the relationship backwards. Higher income might not cause more happiness. More happiness might be the cause for higher income.
<h3>Example 3: Drug Use & Mental Wellbeing</h3>
Another example of reverse causation involves drug use and mental wellbeing.
In an observational study, researchers may observe that people who use drugs may also have lower levels of reported mental wellbeing. Researchers may then naively assume that drug use <em>causes </em>lower mental wellbeing.
In reality, it may be that people who naturally have lower levels of wellbeing are more likely to use drugs, which means that the true relationship between drug use and mental wellbeing is reversed.
<h3>Judging Causality</h3>
One way to assess the causality between phenomenon is to use the  Bradford Hill Criteria , a set of nine criteria proposed by English statistician Sir Austin Bradford Hill in 1965 which are designed to provide evidence of a causal relationship between two variables.
The nine criteria are as follows:
<b>1.</b> <b>Strength: </b> The larger the association between two variables, the more likely that it is causal.
<b>2. Consistency:</b> Consistent findings observed by different researchers in different locations and with different samples increases the chances that an association is causal.
<b>3. Specificity:</b> Causation is likely if there is a very specific population at a specific site and disease with no other likely explanation.
<b>4. Temporality:</b> The effect has to occur after the cause.
<b>5. Biological gradient:</b> Greater exposure should generally lead to greater incidence of the effect.
<b>6. Plausibility:</b> A plausible mechanism between cause and effect is helpful.
<b>7. Coherence:</b> Coherence between epidemiological and laboratory findings increases the likelihood of an effect.
<b>8. Experiment:</b> Experimental evidence increases the chances that a relationship is causal since other variables can be controlled for during experiments.
<b>9. Analogy:</b> The use of analogies or similarities between the observed association and any other associations can increase the chances of a causal relationship being present.
By using these nine criteria, you can increase the chances that you’re able to correctly identify a cause-and-effect relationship between two variables.
<h2><span class="orange">How to Perform Reverse Coding in Excel (With Example)</span></h2>
When creating surveys, researchers sometimes rephrase “positive” questions in a “negative” way to make sure that individuals are giving consistent responses.
We say that these types of questions are <b>reverse-coded</b>.
When using a survey to assign a composite score to individuals, it’s important to make sure the reverse-coded questions are reverse-scored as well.
The following example shows how to reverse the scores on reverse-coded questions in Excel.
<h3>Example: Reverse Coding in Excel</h3>
Suppose researchers administer a survey with 5 questions to 10 individuals in which the possible responses to each questions are:
Strongly Agree
Agree
Neither Agree Nor Disagree
Disagree
Strongly Disagree
The following screenshot shows the results of the survey in which “Strongly Agree” is assigned a value of 5, “Agree” is assigned a value of 4, and so on:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/reverseExcel1.jpg"491">
Suppose questions 2 and 5 are reverse coded, so we must reverse their scores.
That is:
1 should become 5.
2 should become 4.
3 should become 3.
4 should become 2.
5 should become 1.
The easiest way to do this is to take the max possible score (5) and add 1 to get 6. Then subtract the original scores from 6 to get the reverse scored value.
For example:
5 becomes: 6 – 5 = <b>1</b>.
4 becomes: 6 – 4 = <b>2</b>.
3 becomes: 6 – 3 = <b>3</b>.
2 becomes: 6 – 2 = <b>4</b>.
1 becomes: 6 – 1 = <b>5</b>.
To do this in Excel, simply copy and paste all of the original answers into a new area on the spreadsheet:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/reverseExcel2.jpg"490">
In cell <b>B17</b> type: <b>=6-B2</b>. Then copy and paste this formula down to all other cells in column B.
In cell <b>E17</b> type: <b>=6-E2</b>. Then copy and paste this formula down to all other cells in column E.
The scores in column B and column E will now both be reverse coded:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/reverseExcel3.jpg"490">
<h2><span class="orange">How to Perform Reverse Coding in R (With Example)</span></h2>
When creating surveys, researchers sometimes rephrase “positive” questions in a “negative” way to make sure that individuals are giving consistent responses.
We say that these types of questions are <b>reverse-coded</b>.
When using a survey to assign a composite score to individuals, it’s important to make sure the reverse-coded questions are reverse-scored as well.
The following example shows how to reverse the scores on reverse-coded questions in R.
<h2>Example: Reverse Coding in R</h2>
Suppose researchers administer a survey with 5 questions to 10 individuals in which the possible responses to each questions are:
Strongly Agree
Agree
Neither Agree Nor Disagree
Disagree
Strongly Disagree
The following data frame contains the results of the survey in which “Strongly Agree” is assigned a value of 5, “Agree” is assigned a value of 4, and so on:
<b>#create data frame that contains survey results
df &lt;- data.frame(Q1=c(5, 4, 4, 5, 4, 3, 2, 1, 2, 1), Q2=c(1, 2, 2, 1, 2, 3, 4, 5, 4, 5), Q3=c(4, 4, 4, 5, 4, 3, 2, 4, 3, 1), Q4=c(3, 4, 2, 2, 1, 2, 5, 4, 3, 2), Q5=c(2, 2, 3, 2, 3, 1, 4, 5, 3, 4))
#view data frame
df
   Q1 Q2 Q3 Q4 Q5
1   5  1  4  3  2
2   4  2  4  4  2
3   4  2  4  2  3
4   5  1  5  2  2
5   4  2  4  1  3
6   3  3  3  2  1
7   2  4  2  5  4
8   1  5  4  4  5
9   2  4  3  3  3
10  1  5  1  2  4</b>
Suppose questions 2 and 5 are reverse coded, so we must reverse their scores.
That is:
1 should become 5.
2 should become 4.
3 should become 3.
4 should become 2.
5 should become 1.
The easiest way to do this is to take the max possible score (5) and add 1 to get 6. Then subtract the original scores from 6 to get the reverse scored value.
For example:
5 becomes: 6 – 5 = <b>1</b>.
4 becomes: 6 – 4 = <b>2</b>.
3 becomes: 6 – 3 = <b>3</b>.
2 becomes: 6 – 2 = <b>4</b>.
1 becomes: 6 – 1 = <b>5</b>.
We can use the following code to do this in R:
<b>#define columns to reverse code
reverse_cols = c("Q2", "Q5")
#reverse code Q2 and Q5 columns
df[ , reverse_cols] = 6 - df[ , reverse_cols]
#view updated data frame
df
   Q1 Q2 Q3 Q4 Q5
1   5  5  4  3  4
2   4  4  4  4  4
3   4  4  4  2  3
4   5  5  5  2  4
5   4  4  4  1  3
6   3  3  3  2  5
7   2  2  2  5  2
8   1  1  4  4  1
9   2  2  3  3  3
10  1  1  1  2  2
</b>
Notice that all of the values in the Q2 and Q5 columns have been reverse coded.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Convert Factor to Numeric in R 
 How to Rename Factor Levels in R 
 How to Transform Data in R (Log, Square Root, Cube Root) 
<h2><span class="orange">What is Reverse Coding? (Definition & Example)</span></h2>
When creating surveys or questionnaires, researchers sometimes rephrase “positive” questions in a “negative” way to make sure that individuals are giving consistent responses.
For example, consider the following two questions:
<b>1.</b> When working on new projects, I prefer to work alone rather than in a small group.
Strongly Agree
Agree
Neither Agree Nor Disagree
Disagree
Strongly Disagree
<b>2.</b> Given the choice, I prefer to work with a small group rather than by myself on new projects.
Strongly Agree
Agree
Neither Agree Nor Disagree
Disagree
Strongly Disagree
For question 1, “Strongly Agree” corresponds to introversion. However, in question 2, “Strongly Agree” corresponds to extroversion.
We say that question 2 is <b>reverse-coded</b>.
Both questions seek to measure the level of introversion and extroversion of individuals, but they use opposite wording.
When assigning a composite score to individuals to determine their level of introversion or extroversion, it’s important to make sure the reverse-coded questions are reverse-scored as well.
The following example shows how to reverse the scores on reverse-coded questions.
<h3>Example: How to Reverse Code Questions</h3>
Suppose researchers use the previous two questions to assign an “introversion” score to individuals. Higher scores indicate higher levels of introversion.
Suppose researchers assign a value of 5 to “Strongly Agree”, 4 to “Agree”, 3 to “Neither Agree Nor Disagree”, “2 to “Disagree”, and 1 to “Strongly Disagree.”
Then consider the overall average score of someone who answered “Strongly Agree” to the first question and “Strongly Disagree” to the second question:
<b>1.</b> When working on new projects, I prefer to work alone rather than in a small group.
<b>Strongly Agree (5)</b>
Agree (4)
Neither Agree Nor Disagree (3)
Disagree (2)
Strongly Disagree (1)
<b>2.</b> Given the choice, I prefer to work with a small group rather than by myself on new projects.
Strongly Agree (5)
Agree (4)
Neither Agree Nor Disagree (3)
Disagree (2)
<b>Strongly Disagree (1)</b>
Their average score would be calculated as: (5 + 1) / 2 = <b>3</b>. This would make them seem perfectly in the middle of being introverted and extroverted.
However, if you read the individual questions you can see that they prefer to work alone in both scenarios. They should receive a much higher score for introversion.
<b>We must reverse score the second question since it’s reverse-coded</b>.
The easiest way to do this is to take the max possible score (5) and add one. Then subtract the original scores to get the reverse scored value.
For example:
“Strongly Agree” becomes 6 – 5 = 1.
“Agree” becomes 6 – 4 = 2.
“Neither Agree Nor Disagree” becomes 6 – 3 = 3.
“Disagree” becomes 6 – 2 = 4.
“Strongly Disagree” becomes 6 – 1 = 5.
Then consider the overall average score of someone who answered “Strongly Agree” to the first question and “Strongly Disagree” to the second question:
<b>1.</b> When working on new projects, I prefer to work alone rather than in a small group.
<b>Strongly Agree (5)</b>
Agree (4)
Neither Agree Nor Disagree (3)
Disagree (2)
Strongly Disagree (1)
<b>2.</b> Given the choice, I prefer to work with a small group rather than by myself on new projects.
Strongly Agree (1)
Agree (2)
Neither Agree Nor Disagree (3)
Disagree (4)
<b>Strongly Disagree (5)</b>
Their average score would be calculated as: (5 + 5) / 2 = <b>5</b>. This means they received the maximum introversion score. This makes sense given their responses to the questions.
<b>Note 1</b>: In practice, most surveys will have far more than two questions but for simplicity sake we only used two questions in this example.
<b>Note 2</b>: In this example we manually reverse scored the questions, but most statistical software has the ability to reverse code questions for you.
<h2><span class="orange">How to Perform a Reverse VLOOKUP in Google Sheets</span></h2>
This tutorial explains how to perform a reverse VLOOKUP in Google Sheets.
<h3>Example: Reverse VLOOKUP in Google Sheets</h3>
Suppose we have the following dataset that shows the number of points scored by various basketball teams:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/reversev1.jpg"485">
We can use the following <b>VLOOKUP</b> formula to find the number of points associated with the “Knicks” team:
<b>=VLOOKUP("Knicks", A1:B9, 2)
</b>
Here’s what this formula does:
It identifies <b>Knicks</b> as the value to find.
It specifies <b>A1:B9</b> as the range to analyze.
It specifies that we’d like to return the value in the <b>2nd</b> column from the left.
Here’s how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/reversev2.jpg"514">
This formula correctly returns the value of <b>99</b>.
However, suppose we instead want to know which team is associated with a points value of <b>99</b>.
We can use the following <b>reverse VLOOKUP</b> formula:
<b>=VLOOKUP(99, {B2:B9, A2:A9}, 2)
</b>
Here’s what this formula does:
It identifies <b>99</b> as the value to find.
It specifies <b>B2:B9</b> and <b>A2:A9</b> as the ranges to analyze.
It specifies that we’d like to return the value in the <b>2nd</b> column from the right.
Here’s how to use this formula in practice:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/reversev3.jpg">
The formula correctly identifies the <b>Knicks</b> as the team with a points value of <b>99</b>.
<h2><span class="orange">Ridge Regression in Python (Step-by-Step)</span></h2>
 Ridge regression  is a method we can use to fit a regression model when  multicollinearity  is present in the data.
In a nutshell, least squares regression tries to find coefficient estimates that minimize the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)2</b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
Conversely, ridge regression seeks to minimize the following:
<b>RSS + λΣβ<sub>j</sub><sup>2</sup></b>
where <em>j</em> ranges from 1 to <em>p</em> predictor variables and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>. In ridge regression, we select a value for λ that produces the lowest possible test MSE (mean squared error).
This tutorial provides a step-by-step example of how to perform ridge regression in Python.
<h3>Step 1: Import Necessary Packages</h3>
First, we’ll import the necessary packages to perform ridge regression in Python:
<b>import pandas as pd
from numpy import arange
from sklearn.linear_model import Ridge
from sklearn.linear_model import RidgeCV
from sklearn.model_selection import RepeatedKFold</b>
<h3>Step 2: Load the Data</h3>
For this example, we’ll use a dataset called <b>mtcars</b>, which contains information about 33 different cars. We’ll use <b>hp</b> as the response variable and the following variables as the predictors:
mpg
wt
drat
qsec
The following code shows how to load and view this dataset:
<b>#define URL where data is located
url = "https://raw.githubusercontent.com/Statology/Python-Guides/main/mtcars.csv"
#read in data
data_full = pd.read_csv(url)
#select subset of data
data = data_full[["mpg", "wt", "drat", "qsec", "hp"]]
#view first six rows of data
data[0:6]
mpgwtdratqsechp
021.02.6203.9016.46110
121.02.8753.9017.02110
222.82.3203.8518.6193
321.43.2153.0819.44110
418.73.4403.1517.02175
518.13.4602.7620.22105</b>
<h3>Step 3: Fit the Ridge Regression Model</h3>
Next, we’ll use the  RidgeCV()  function from sklearn to fit the ridge regression model and we’ll use the  RepeatedKFold()  function to perform k-fold cross-validation to find the optimal alpha value to use for the penalty term.
<em><b>Note:</b> The term “alpha” is used instead of “lambda” in Python.</em>
For this example we’ll choose k = 10 folds and repeat the cross-validation process 3 times.
Also note that RidgeCV() only tests alpha values .1, 1, and 10 by default. However, we can define our own alpha range from 0 to 1 by increments of 0.01:
<b>#define predictor and response variables
X = data[["mpg", "wt", "drat", "qsec"]]
y = data["hp"]
#define cross-validation method to evaluate model
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
#define model
model = RidgeCV(alphas=arange(0, 1, 0.01), cv=cv, scoring='neg_mean_absolute_error')
#fit model
model.fit(X, y)
#display lambda that produced the lowest test MSE
print(model.alpha_)
0.99</b>
The lambda value that minimizes the test MSE turns out to be <b>0.99</b>.
<h3>Step 4: Use the Model to Make Predictions</h3>
Lastly, we can use the final ridge regression model to make predictions on new observations. For example, the following code shows how to define a new car with the following attributes:
mpg: 24
wt: 2.5
drat: 3.5
qsec: 18.5
The following code shows how to use the fitted ridge regression model to predict the value for <em>hp</em> of this new observation:
<b>#define new observation
new = [24, 2.5, 3.5, 18.5]
#predict hp value using ridge regression model
model.predict([new])
array([104.16398018])
</b>
Based on the input values, the model predicts this car to have an <em>hp</em> value of <b>104.16398018</b>.
You can find the complete Python code used in this example  here .
<h2><span class="orange">Ridge Regression in R (Step-by-Step)</span></h2>
 Ridge regression  is a method we can use to fit a regression model when  multicollinearity  is present in the data.
In a nutshell, least squares regression tries to find coefficient estimates that minimize the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)2</b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
Conversely, ridge regression seeks to minimize the following:
<b>RSS + λΣβ<sub>j</sub><sup>2</sup></b>
where <em>j</em> ranges from 1 to <em>p</em> predictor variables and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>. In ridge regression, we select a value for λ that produces the lowest possible test MSE (mean squared error).
This tutorial provides a step-by-step example of how to perform ridge regression in R.
<h3>Step 1: Load the Data</h3>
For this example, we’ll use the R built-in dataset called <b>mtcars</b>. We’ll use <b>hp</b> as the response variable and the following variables as the predictors:
mpg
wt
drat
qsec
To perform ridge regression, we’ll use functions from the <b>glmnet</b> package. This package requires the  response variable  to be a vector and the set of predictor variables to be of the class <b>data.matrix</b>.
The following code shows how to define our data:
<b>#define response variable
y &lt;- mtcars$hp
#define matrix of predictor variables
x &lt;- data.matrix(mtcars[, c('mpg', 'wt', 'drat', 'qsec')])
</b>
<h3>Step 2: Fit the Ridge Regression Model</h3>
Next, we’ll use the <b>glmnet()</b> function to fit the ridge regression model and specify <b>alpha=0</b>.
Note that setting alpha equal to 1 is equivalent to using Lasso Regression and setting alpha to some value between 0 and 1 is equivalent to using an elastic net.
Also note that ridge regression requires the data to be standardized such that each predictor variable has a mean of 0 and a standard deviation of 1.
Fortunately <b>glmnet()</b> automatically performs this standardization for you. If you happened to already standardize the variables, you can specify <b>standardize=False</b>.
<b>library(glmnet)
#fit ridge regression model
model &lt;- glmnet(x, y, alpha = 0)
#view summary of model
summary(model)
          Length Class     Mode   
a0        100    -none-    numeric
beta      400    dgCMatrix S4     
df        100    -none-    numeric
dim         2    -none-    numeric
lambda    100    -none-    numeric
dev.ratio 100    -none-    numeric
nulldev     1    -none-    numeric
npasses     1    -none-    numeric
jerr        1    -none-    numeric
offset      1    -none-    logical
call        4    -none-    call   
nobs        1    -none-    numeric
</b>
<h3>Step 3: Choose an Optimal Value for Lambda</h3>
Next, we’ll identify the lambda value that produces the lowest test mean squared error (MSE) by using  k-fold cross-validation .
Fortunately, <b>glmnet</b> has the function <b>cv.glmnet()</b> that automatically performs k-fold cross validation using k = 10 folds.
<b>#perform k-fold cross-validation to find optimal lambda value
cv_model &lt;- cv.glmnet(x, y, alpha = 0)
#find optimal lambda value that minimizes test MSE
best_lambda &lt;- cv_model$lambda.min
best_lambda
[1] 10.04567
#produce plot of test MSE by lambda value
plot(cv_model) 
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridgeR1.png">
The lambda value that minimizes the test MSE turns out to be <b>10.04567</b>.
<h3>Step 4: Analyze Final Model</h3>
Lastly, we can analyze the final model produced by the optimal lambda value.
We can use the following code to obtain the coefficient estimates for this model:
<b>#find coefficients of best model
best_model &lt;- glmnet(x, y, alpha = 0, lambda = best_lambda)
coef(best_model)
5 x 1 sparse Matrix of class "dgCMatrix"    s0
(Intercept) 475.242646
mpg          -3.299732
wt           19.431238
drat         -1.222429
qsec        -17.949721</b>
We can also produce a Trace plot to visualize how the coefficient estimates changed as a result of increasing lambda:
<b>#produce Ridge trace plot
plot(model, xvar = "lambda")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridgeR2.png">
Lastly, we can calculate the  R-squared of the model  on the training data:
<b>#use fitted best model to make predictions
y_predicted &lt;- predict(model, s = best_lambda, newx = x)
#find SST and SSE
sst &lt;- sum((y - mean(y))^2)
sse &lt;- sum((y_predicted - y)^2)
#find R-Squared
rsq &lt;- 1 - sse/sst
rsq
[1] 0.7999513
</b>
The R-squared turns out to be <b>0.7999513</b>. That is, the best model was able to explain <b>79.99%</b> of the variation in the response values of the training data.
You can find the complete R code used in this example  here .
<h2><span class="orange">Introduction to Ridge Regression</span></h2>
In ordinary  multiple linear regression , we use a set of <em>p</em> predictor variables and a  response variable  to fit a model of the form:
<b>Y = β<sub>0</sub> + β<sub>1</sub>X<sub>1</sub> + β<sub>2</sub>X<sub>2</sub> + … + β<sub>p</sub>X<sub>p</sub> + ε</b>
where:
<b>Y</b>: The response variable
<b>X<sub>j</sub></b>: The j<sup>th</sup> predictor variable
<b>β<sub>j</sub></b>: The average effect on Y of a one unit increase in X<sub>j</sub>, holding all other predictors fixed
<b>ε</b>: The error term
The values for β<sub>0</sub>, β<sub>1</sub>, B<sub>2</sub>, … , β<sub>p</sub> are chosen using <b>the least square method</b>, which minimizes the sum of squared residuals (RSS):
<b>RSS = Σ(y<sub>i</sub> – <U+0177><sub>i</sub>)<sup>2</sup></b>
where:
<b>Σ</b>: A greek symbol that means <em>sum</em>
<b>y<sub>i</sub></b>: The actual response value for the i<sup>th</sup> observation
<b><U+0177><sub>i</sub></b>: The predicted response value based on the multiple linear regression model
However, when the predictor variables are highly correlated then  multicollinearity  can become a problem. This can cause the coefficient estimates of the model to be unreliable and have high variance.
One way to get around this issue without completely removing some predictor variables from the model is to use a method known as <b>ridge regression</b>, which instead seeks to minimize the following:
<b>RSS + λΣβ<sub>j</sub><sup>2</sup></b>
where <em>j</em> ranges from 1 to <em>p</em> and λ ≥ 0.
This second term in the equation is known as a <em>shrinkage penalty</em>.
When λ = 0, this penalty term has no effect and ridge regression produces the same coefficient estimates as least squares. However, as λ approaches infinity, the shrinkage penalty becomes more influential and the ridge regression coefficient estimates approach zero.
In general, the predictor variables that are least influential in the model will shrink towards zero the fastest.
<h3>Why Use Ridge Regression?</h3>
The advantage of ridge regression compared to least squares regression lies in the  bias-variance tradeoff .
Recall that mean squared error (MSE) is a metric we can use to measure the accuracy of a given model and it is calculated as:
MSE = Var(<em>f<U+0302>(</em>x<sub>0</sub>)) + [Bias(<em>f<U+0302>(</em>x<sub>0</sub>))]<sup>2</sup> + Var(ε)
MSE = Variance + Bias<sup>2</sup> + Irreducible error
The basic idea of ridge regression is to introduce a little bias so that the variance can be substantially reduced, which leads to a lower overall MSE.
To illustrate this, consider the following chart:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridge1.png">
Notice that as λ increases, variance drops substantially with very little increase in bias. Beyond a certain point, though, variance decreases less rapidly and the shrinkage in the coefficients causes them to be significantly underestimated which results in a large increase in bias.
We can see from the chart that the test MSE is lowest when we choose a value for λ that produces an optimal tradeoff between bias and variance.
When λ = 0, the penalty term in ridge regression has no effect and thus it produces the same coefficient estimates as least squares. However, by increasing λ to a certain point we can reduce the overall test MSE.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridge2.png">
This means the model fit by ridge regression will produce smaller test errors than the model fit by least squares regression.
<h3>Steps to Perform Ridge Regression in Practice</h3>
The following steps can be used to perform ridge regression:
<b>Step 1: Calculate the correlation matrix and VIF values for the predictor variables.</b>
First, we should produce a  correlation matrix  and calculate the  VIF (variance inflation factor) values  for each predictor variable.
If we detect high correlation between predictor variables and high VIF values (some texts define a “high” VIF value as 5 while others use 10) then ridge regression is likely appropriate to use.
However, if there is no multicollinearity present in the data then there may be no need to perform ridge regression in the first place. Instead, we can perform ordinary least squares regression.
<b>Step 2: Standardize each predictor variable.</b>
Before performing ridge regression, we should scale the data such that each predictor variable has a mean of 0 and a standard deviation of 1. This ensures that no single predictor variable is overly influential when performing ridge regression.
<b>Step 3: Fit the ridge regression model and choose a value for λ.</b>
There is no exact formula we can use to determine which value to use for λ. In practice, there are two common ways that we choose λ:
<b>(1) Create a Ridge trace plot.</b> This is a plot that visualizes the values of the coefficient estimates as λ increases towards infinity. Typically we choose λ as the value where most of the coefficient estimates begin to stabilize.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/ridge3.png">
<b>(2) Calculate the test MSE for each value of λ.</b>
Another way to choose λ is to simply calculate the test MSE of each model with different values of λ and choose λ to be the value that produces the lowest test MSE.
<h3>Pros & Cons of Ridge Regression</h3>
The biggest <b>benefit</b> of ridge regression is its ability to produce a lower test mean squared error (MSE) compared to least squares regression when multicollinearity is present.
However, the biggest <b>drawback</b> of ridge regression is its inability to perform variable selection since it includes all predictor variables in the final model. Since some predictors will get shrunken very close to zero, this can make it hard to interpret the results of the model.
In practice, ridge regression has the potential to produce a model that can make better predictions compared to a least squares model but it is often harder to interpret the results of the model. 
Depending on whether model interpretation or prediction accuracy is more important to you, you may choose to use ordinary least squares or ridge regression in different scenarios.
<h3>Ridge Regression in R & Python</h3>
The following tutorials explain how to perform ridge regression in R and Python, the two most common languages used for fitting ridge regression models:
 Ridge Regression in R (Step-by-Step) 
 Ridge Regression in Python (Step-by-Step) 
<h2><span class="orange">How to Do a Right Join in R (With Examples)</span></h2>
There are two common ways to perform a right join in R:
<b>Method 1: Use Base R</b>
<b>merge(df1, df2, by='column_to_join_on', all.y=TRUE)
</b>
<b>Method 2: Use dplyr</b>
<b>library(dplyr)
right_join(df1, df2, by='column_to_join_on')</b>
Both methods will return all rows from <b>df2</b> and any rows with matching keys from <b>df1</b>.
It’s also worth noting that both methods will produce the same result, but the dplyr method will tend to work faster on extremely large datasets.
The following examples show how to use each of these functions in practice with the following data frames:
<b>#define first data frame
df1 = data.frame(team=c('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H'), points=c(18, 22, 19, 14, 14, 11, 20, 28))
df1
  team points
1    A     18
2    B     22
3    C     19
4    D     14
5    E     14
6    F     11
7    G     20
8    H     28
#define second data frame
df2 = data.frame(team=c('A', 'B', 'C', 'D', 'L', 'M'), assists=c(4, 9, 14, 13, 10, 8))
df2
  team assists
1    A       4
2    B       9
3    C      14
4    D      13
5    L      10
6    M       8</b>
<h3>Example 1: Right Join Using Base R</h3>
We can use the <b>merge()</b> function in base R to perform a right join, using the ‘team’ column as the column to join on:
<b>#perform right join using base R
df3 &lt;- merge(df1, df2, by='team', all.y=TRUE)
#view result
df3
  team points assists
1    A     18       4
2    B     22       9
3    C     19      14
4    D     14      13
5    L     NA      10
6    M     NA       8
</b>
Notice that all of the rows from <b>df2</b> were included in the final data frame, but only the rows from <b>df1</b> that had a matching team name were included in the final data frame.
<h3>Example 2: Right Join Using dplyr</h3>
We can use the <b>right_join()</b> function from the  dplyr  package to perform a right join, using the ‘team’ column as the column to join on:
<b>library(dplyr)
#perform right join using dplyr 
df3 &lt;- right_join(df1, df2, by='team')
#view result
df3
  team points assists
1    A     18       4
2    B     22       9
3    C     19      14
4    D     14      13
5    L     NA      10
6    M     NA       8</b>
Notice that this matches the result we obtained from using the <b>merge()</b> function in base R.
<h2><span class="orange">RMSE Calculator</span></h2>
The <b>root mean square error (RMSE)</b> is a metric that tells us how far apart our predicted values are from our observed values in a regression analysis, on average. It is calculated as:
<b>RMSE</b> = √[ Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n ]
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
To find the RMSE for a regression, simply enter a list of observed values and predicted values in the two boxes below, then click the “Calculate” button:
<b>Observed values:</b>
<textarea id="input_data_obs" name="x" rows="5" cols="40">34, 37, 44, 47, 48, 48, 46, 43, 32, 27, 26, 24</textarea>
<b>Predicted values:</b>
<textarea id="input_data_pred" name="x" rows="5" cols="40">37, 40, 46, 44, 46, 50, 45, 44, 34, 30, 22, 23</textarea>
<input type="button" id="button" onclick="calc()" value="Calculate">
<b>RMSE</b> = 2.43242
<script>
function calc() {
var obs = document.getElementById('input_data_obs').value.split(',').map(Number);
var pred = document.getElementById('input_data_pred').value.split(',').map(Number);
//check that both lists are equal length
if (obs.length - pred.length == 0) {
document.getElementById('error_msg').innerHTML = '';
//calculate RMSE
let error = 0
for (let i = 0; i < obs.length; i++) {
error += Math.pow((pred[i] - obs[i]), 2)
}
var RMSE = Math.sqrt(error / obs.length);
document.getElementById('RMSE').innerHTML = RMSE.toFixed(5);
}
else {
 document.getElementById('RMSE').innerHTML = '';
 document.getElementById('error_msg').innerHTML = 'The two lists must be of equal length.';
}
  
} //end calc function
</script>
<h2><span class="orange">How to Calculate RMSE in Python</span></h2>
The <b>root mean square error (RMSE) </b>is a metric that tells us how far apart our predicted values are from our observed values in a model, on average. It is calculated as:
<b>RMSE </b>= √[ Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n ]
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
This tutorial explains a simple method to calculate RMSE in Python.
<h3>Example: Calculate RMSE in Python</h3>
Suppose we have the following arrays of actual and predicted values:
<b>actual= [34, 37, 44, 47, 48, 48, 46, 43, 32, 27, 26, 24]
pred = [37, 40, 46, 44, 46, 50, 45, 44, 34, 30, 22, 23]
</b>
To calculate the RMSE between the actual and predicted values, we can simply take the square root of the  mean_squared_error()  function from the sklearn.metrics library:
<b>#import necessary libraries
from sklearn.metrics import mean_squared_error
from math import sqrt
#calculate RMSE
sqrt(mean_squared_error(actual, pred)) 
2.4324199198
</b>
The RMSE turns out to be <b>2.4324</b>.
<h3>How to Interpret RMSE</h3>
RMSE is a useful way to see how well a model is able to fit a dataset. The larger the RMSE, the larger the difference between the predicted and observed values, which means the worse a model fits the data. Conversely, the smaller the RMSE, the better a model is able to fit the data.
It can be particularly useful to compare the RMSE of two different models with each other to see which model fits the data better.
<h2><span class="orange">RMSE vs. R-Squared: Which Metric Should You Use?</span></h2>
Regression models are used to quantify the relationship between one or more predictor variables and a response variable.
Whenever we fit a regression model, we want to understand how well the model “fits” the data. In other words, how well is the model able to use the values of the predictor variables to predict the value of the  response variable ?
Two metrics that statisticians often use to quantify how well a model fits a dataset are the root mean squared error (RMSE) and the R-squared (R<sup>2</sup>), which are calculated as follows:
<b>RMSE</b>: A metric that tells us how far apart the predicted values are from the observed values in a dataset, on average. The lower the RMSE, the better a model fits a dataset.
It is calculated as:
RMSE = √Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n
where:
Σ is a symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation
n is the sample size
<b>R<sup>2</sup></b>: A metric that tells us the proportion of the variance in the response variable of a regression model that can be explained by the predictor variables. This value ranges from 0 to 1. The higher the R<sup>2</sup> value, the better a model fits a dataset.
It is calculated as:
R<sup>2</sup> = 1 – (RSS/TSS)
where:
RSS represents the sum of squares of residuals
TSS represents the total sum of squares
<h3>RMSE vs. R<sup>2</sup>: Which Metric Should You Use?</h3>
When assessing how well a model fits a dataset, it’s useful to calculate <em>both</em> the RMSE and the R<sup>2</sup> value because each metric tells us something different.
One one hand, RMSE tells us the typical distance between the predicted value made by the regression model and the actual value.
On the other hand, R<sup>2</sup> tells us how well the predictor variables can explain the variation in the response variable.
For example, suppose we have the following dataset that shows information about houses in a certain city:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/rmse_r2_1.png">
Now suppose we’d like to use square footage, number of bathrooms, and number of bedrooms to predict house price.
We can fit the following regression model:
Price = β<sub>0</sub> + β<sub>1</sub>(sq. footage) + β<sub>2</sub>(# bathrooms) + β<sub>3</sub>(# bedrooms)
Now suppose we fit this model and then calculate the following metrics to assess the goodness of fit of the model:
<b>RMSE</b>: 14,342
<b>R<sup>2</sup></b>: 0.856
The <b>RMSE</b> value tells us that the average deviation between the predicted house price made by the model and the actual house price is $14,342.
The <b>R<sup>2</sup></b> value tells us that the predictor variables in the model (square footage, # bathrooms, and # bedrooms) are able to explain 85.6% of the variation in the house prices.
When determining if these values are “good” or not, we can compare these metrics to alternative models.
For example, suppose we fit another regression model that uses a different set of predictor variables and calculate the following metrics for that model:
<b>RMSE</b>: 19,355
<b>R<sup>2</sup></b>: 0.765
We can see that the RMSE value for this model is greater than the previous model. We can also see that the R<sup>2</sup> value for this model is less than the previous model. This tells us that this model fits the data worse than the previous model.
<h3>Summary</h3>
Here are the main points made in this article:
Both RMSE and R<sup>2</sup> quantify how well a regression model fits a dataset.
The RMSE tells us how well a regression model can predict the value of the response variable in absolute terms while R<sup>2</sup> tells us how well a model can predict the value of the response variable in percentage terms.
It’s useful to calculate both the RMSE and R<sup>2</sup> for a given model because each metric gives us useful information.
<h2><span class="orange">How to Use the Equivalent of rnorm() in Python</span></h2>
In the R programming language, we can use the <b>rnorm()</b> function to generate a vector of random values that follow a  normal distribution  with a specific mean and standard deviation.
For example, the following code shows how to use <b>rnorm()</b> to create a vector of 8 random values that follow a normal distribution with a mean of 5 and standard deviation of 2:
<b>#make this example reproducible
set.seed(1)
#generate vector of 8 values that follow normal distribution with mean=5 and sd=2
rnorm(n=8, mean=5, sd=2)
[1] 3.747092 5.367287 3.328743 8.190562 5.659016 3.359063 5.974858 6.476649
</b>
The equivalent of the <b>rnorm()</b> function in Python is the <b>np.random.normal()</b> function, which uses the following basic syntax:
<b>np.random.normal(loc=0, scale=1, size=None)</b>
where:
<b>loc</b>: Mean of the distribution
<b>scale</b>: Standard deviation of the distribution
<b>size</b>: Sample size
The following example shows how to use this function in practice.
<h2>Example: Using the Equivalent of rnorm() in Python</h2>
The following code shows how to use the <b>np.random.normal()</b> function to generate an array of random values that follow a normal distribution with a specific mean and standard deviation.
<b>import numpy as np
#make this example reproducible
np.random.seed(1)
#generate array of 8 values that follow normal distribution with mean=5 and sd=2
np.random.normal(loc=5, scale=2, size=8)
array([8.24869073, 3.77648717, 3.9436565 , 2.85406276, 6.73081526,
       0.39692261, 8.48962353, 3.4775862 ])
</b>
The result is a NumPy array that contains 8 values generated from a normal distribution with a mean of 5 and a standard deviation of 2.
You can also create a histogram using Matplotlib to visualize a normal distribution generated by the <b>np.random.normal()</b> function:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#generate array of 200 values that follow normal distribution with mean=5 and sd=2
data = np.random.normal(loc=5, scale=2, size=200)
#create histogram to visualize distribution of values
plt.hist(data, bins=30, edgecolor='black')
</b>
<b> <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/norma1.jpg"497"></b>
We can see that the distribution of values is roughly bell-shaped with a mean located at 5 and a standard deviation of 2.
<b>Note</b>: You can find the complete documentation for the <b>np.random.normal()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in Python:
 How to Calculate & Plot the Normal CDF in Python 
 How to Plot a Normal Distribution in Python 
 How to Test for Normality in Python 
<h2><span class="orange">How to Perform Robust Regression in R (Step-by-Step)</span></h2>
<b>Robust regression</b> is a method we can use as an alternative to ordinary least squares regression when there are outliers or  influential observations  in the dataset we’re working with.
To perform robust regression in R, we can use the <b>rlm()</b> function from the <b>MASS</b> package, which uses the following syntax:
The following step-by-step example shows how to perform robust regression in R for a given dataset.
<h3>Step 1: Create the Data</h3>
First, let’s create a fake dataset to work with:
<b>#create data
df &lt;- data.frame(x1=c(1, 3, 3, 4, 4, 6, 6, 8, 9, 3,      11, 16, 16, 18, 19, 20, 23, 23, 24, 25), x2=c(7, 7, 4, 29, 13, 34, 17, 19, 20, 12,      25, 26, 26, 26, 27, 29, 30, 31, 31, 32),  y=c(17, 170, 19, 194, 24, 2, 25, 29, 30, 32,      44, 60, 61, 63, 63, 64, 61, 67, 59, 70))
#view first six rows of data
head(df)
  x1 x2   y
1  1  7  17
2  3  7 170
3  3  4  19
4  4 29 194
5  4 13  24
6  6 34   2
</b>
<h3>Step 2: Perform Ordinary Least Squares Regression</h3>
Next, let’s fit an ordinary least squares regression model and create a plot of the  standardized residuals .
In practice, we often consider any standardized residual with an absolute value greater than 3 to be an outlier.
<b>#fit ordinary least squares regression model
ols &lt;- lm(y~x1+x2, data=df)
#create plot of y-values vs. standardized residuals
plot(df$y, rstandard(ols), ylab='Standardized Residuals', xlab='y') 
abline(h=0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/robustRegR1.png">
From the plot we can see that there are two observations with standardized residuals around 3.
This is an indication that there are two potential outliers in the dataset and thus we may benefit from performing robust regression instead.
<h3>Step 3: Perform Robust Regression</h3>
Next, let’s use the <b>rlm()</b> function to fit a robust regression model:
<b>library(MASS)
#fit robust regression model
robust &lt;- rlm(y~x1+x2, data=df)</b>
To determine if this robust regression model offers a better fit to the data compared to the OLS model, we can calculate the residual standard error of each model.
The residual standard error (RSE) is a way to measure the standard deviation of the residuals in a regression model. The lower the value for RSE, the more closely a model is able to fit the data.
The following code shows how to calculate the RSE for each model:
<b>#find residual standard error of ols model
summary(ols)$sigma
[1] 49.41848
#find residual standard error of ols model
summary(robust)$sigma
[1] 9.369349
</b>
We can see that the RSE for the robust regression model is much lower than the ordinary least squares regression model, which tells us that the robust regression model offers a better fit to the data.
<h2><span class="orange">How to Calculate Robust Standard Errors in R</span></h2>
One of the  assumptions of linear regression  is that the  residuals  of the model are equally scattered at each level of the predictor variable.
When this assumption is violated, we say that  heteroscedasticity  is present in a regression model.
When this occurs, the standard errors for the regression coefficients in the model become untrustworthy.
To account for this, we can calculate <b>robust standard errors</b>, which are “robust” against heteroscedasticity and can give us a better idea of the true standard error values for the regression coefficients.
The following example shows how to calculate robust standard errors for a regression model in R.
<h2>Example: Calculating Robust Standard Errors in R</h2>
Suppose we have the following data frame in R that contains information on the hours studied and exam score received by 20 students in some class:
<b>#create data frame
df &lt;- data.frame(hours=c(1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4,         4, 5, 5, 5, 6, 6, 7, 7, 8), score=c(67, 68, 74, 70, 71, 75, 80, 70, 84, 72,         88, 75, 95, 75, 99, 78, 99, 65, 96, 70))
#view head of data frame
head(df)
  hours score
1     1    67
2     1    68
3     1    74
4     1    70
5     2    71
6     2    75
</b>
We can use the  lm()  function to fit a regression model in R that uses <b>hours</b> as the predictor variable and <b>score</b> as the response variable:
<b>#fit regression model
fit &lt;- lm(score ~ hours, data=df)
#view summary of model
summary(fit)
Call:
lm(formula = score ~ hours, data = df)
Residuals:
    Min      1Q  Median      3Q     Max 
-19.775  -5.298  -3.521   7.520  18.116 
Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   71.158      4.708   15.11 1.14e-11 ***
hours          1.945      1.075    1.81    0.087 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
Residual standard error: 10.48 on 18 degrees of freedom
Multiple R-squared:  0.154,Adjusted R-squared:  0.107 
F-statistic: 3.278 on 1 and 18 DF,  p-value: 0.08696</b>
The easiest way to visually check if heteroscedasticity is a problem in the regression model is to create a residual plot:
<b>#create residual vs. fitted plot
plot(fitted(fit), resid(fit))
#add a horizontal line at y=0 
abline(0,0)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/rob1.jpg"440">
The x-axis shows the fitted values of the response variable and the y-axis shows the corresponding residuals.
From the plot we can see that the variance in the residuals increases as the fitted values increase.
This is an indication that heteroscedasticity is likely a problem in the regression model and the standard errors from the model summary are untrustworthy.
To calculate robust standard errors, we can use the <b>coeftest()</b> function from the <b>lmtest</b> package and the <b>vcovHC()</b> function from the <b>sandwich</b> package as follows:
<b>library(lmtest)
library(sandwich)
#calculate robust standard errors for model coefficients
coeftest(fit, vcov = vcovHC(fit, type = 'HC0'))
t test of coefficients:
            Estimate Std. Error t value  Pr(>|t|)    
(Intercept)  71.1576     3.3072 21.5160 2.719e-14 ***
hours         1.9454     1.2072  1.6115    0.1245    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</b>
Notice that the standard error for the <b>hours</b> predictor variable increased from 1.075 in the previous model summary to 1.2072 in this model summary.
Since heteroscedasticity is present in the original regression model, this estimate for the standard error is more trustworthy and should be used when calculating a confidence interval for the <b>hours</b> predictor variable.
<b>Note</b>: The most common type of estimate to calculate in the <b>vcovHC()</b> function is ‘HC0’, but you can refer to the  documentation  to find other estimate types.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Perform White’s Test for Heteroscedasticity in R 
 How to Interpret Linear Regression Output in R 
 How to Create a Residual Plot in R 
<h2><span class="orange">How to Use Robust Standard Errors in Regression in Stata</span></h2>
 Multiple linear regression  is a method we can use to understand the relationship between several explanatory variables and a response variable. 
Unfortunately, one problem that often occurs in regression is known as  heteroscedasticity , in which there is a systematic change in the variance of residuals over a range of measured values.
This causes an increase in the variance of the regression coefficient estimates, but the regression model doesn’t pick up on this. This makes it much more likely for a regression model to declare that a term in the model is statistically significant, when in fact it is not.
One way to account for this problem is to use <b>robust standard errors</b>, which are more “robust” to the problem of heteroscedasticity and tend to provide a more accurate measure of the true standard error of a regression coefficient.
This tutorial explains how to use robust standard errors in regression analysis in Stata.
<h2>Example: Robust Standard Errors in Stata</h2>
We will use the built-in Stata dataset <em>auto </em>to illustrate how to use robust standard errors in regression.
<b>Step 1: Load and view the data.</b>
First, use the following command to load the data:
<b>sysuse auto</b>
Then, view the raw data by using the following command:
<b>br</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/robustErrorsStata1.png">
<b>Step 2: Perform multiple linear regression without robust standard errors.</b>
Next, we will type in the following command to perform a multiple linear regression using <em>price </em>as the response variable and <em>mpg </em>and <em>weight </em>as the explanatory variables:
<b>regress price mpg weight</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/multipleRegStata1.png">
<b>Step 3: Perform multiple linear regression using robust standard errors.</b>
Now we will perform the exact same multiple linear regression, but this time we’ll use the <b>vce(robust) </b>command so Stata knows to use robust standard errors:
<b>regress price mpg weight, vce(robust)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/robustErrorsStata2.png">
There are a few interesting things to note here:
<b>1. The coefficient estimates remained the same</b>. When we use robust standard errors, the coefficient estimates don’t change at all. Notice that the coefficient estimates for mpg, weight, and the constant are as follows for both regressions: 
<b>mpg: </b>-49.51222
<b>weight: </b>1.746559
<b>_cons: </b>1946.069
<b>2. The standard errors changed</b>. Notice that when we used robust standard errors, the standard errors for each of the coefficient estimates increased. 
<em><b>Note: </b>In most cases, robust standard errors will be larger than the normal standard errors, but in rare cases it is possible for the robust standard errors to actually be smaller.</em>
<b>3. The test statistic of each coefficient changed. </b>Notice that the absolute value of each test statistic<em>, t</em>, decreased. This is because the test statistic is calculated as the estimated coefficient divided by the standard error. Thus, the larger the standard error, the smaller the absolute value of the test statistic.
<b>4. The p-values changed</b>. Notice that the p-values for each variable also increased. This is because smaller test statistics are associated with larger p-values.
Although the p-values changed for our coefficients, the variable <em>mpg </em>is still not statistically significant at α = 0.05 and the variable <em>weight </em>is still statistically significant at α = 0.05.
<h2><span class="orange">How to Create a ROC Curve in Excel (Step-by-Step)</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the “true positive rate.”
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative. This is also called the “true negative rate.”
One way to visualize these two metrics is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve. This is a plot that displays the sensitivity and specificity of a logistic regression model.
The following step-by-step example shows how to create and interpret a ROC curve in Excel.
<h3>Step 1: Enter the Data</h3>
First, let’s enter some raw data:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/roc1.png">
<h3>Step 2: Calculate the Cumulative Data</h3>
Next, let’s use the following formula to calculate the cumulative values for the Pass and Fail categories:
Cumulative Pass values: <b>=SUM($B$3:B3)</b>
Cumulative Fail values: <b>=SUM($C$3:C3)</b>
We’ll then copy and paste these formulas down to every cell in column D and column E:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/roc2.png">
<h3>Step 3: Calculate False Positive Rate & True Positive Rate</h3>
Next, we’ll calculate the false positive rate (FPR), true positive rate (TPR), and the area under the curve AUC) using the following formulas:
FPR: <b>=1-D3/$D$14</b>
TPR: <b>=1-E3/$E$14</b>
AUC: <b>=(F3-F4)*G3</b>
We’ll then copy and paste these formulas down to every cell in columns F, G, and H:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/roc3.png">
<h3>Step 4: Create the ROC Curve</h3>
To create the ROC curve, we’ll highlight every value in the range <b>F3:G14</b>.
Then we’ll click the <b>Insert</b> tab along the top ribbon and then click <b>Insert Scatter(X, Y)</b> to create the following plot:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/roc4.png">
<h3>Step 5: Calculate the AUC</h3>
The more that the curve hugs the top left corner of the plot, the better the model does at classifying the data into categories.
As we can see from the plot above, this logistic regression model does a pretty good job of classifying the data into categories.
To quantify this, we can calculate the AUC (area under the curve) which tells us how much of the plot is located under the curve.
The closer AUC is to 1, the better the model. A model with an AUC equal to 0.5 is no better than a model that makes random classifications.
To calculate the AUC of the curve, we can simply take the sum of all of the values in column H:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/08/roc5.png">
The AUC turns out to be <b>0.802662</b>. This value is fairly high, which indicates that the model does a good job of classifying the data into ‘Pass’ and ‘Fail’ categories.
<h2><span class="orange">How to Plot a ROC Curve Using ggplot2 (With Examples)</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive.
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative.
One easy way to visualize these two metrics is by creating a <b>ROC curve</b>, which is a plot that displays the sensitivity and specificity of a logistic regression model.
This tutorial explains how to create and interpret a ROC curve in R using the ggplot2 visualization package.
<h3>Example: ROC Curve Using ggplot2</h3>
Suppose we fit the following logistic regression model in R:
<b>#load <em>Default</em> dataset from ISLR book
data &lt;- ISLR::Default
#divide dataset into training and test set
set.seed(1)
sample &lt;- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.7,0.3))
train &lt;- data[sample, ]
test &lt;- data[!sample, ]
#fit logistic regression model to training set
model &lt;- glm(default~student+balance+income, family="binomial", data=train)
#use model to make predictions on test set
predicted &lt;- predict(model, test, type="response")
</b>
To visualize how well the logistic regression model performs on the test set, we can create a ROC plot using the <b>ggroc()</b> function from the  pROC package :
<b>#load necessary packages
library(ggplot2)
library(pROC)
#define object to plot
rocobj &lt;- roc(test$default, predicted)
#create ROC plot
ggroc(rocobj)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/ROCggplot1.png">
The y-axis displays the sensitivity (the true positive rate) of the model and the x-axis displays the specificity (the true negative rate) of the model.
Note that we can add some styling to the plot and also provide a title that contains the AUC (area under the curve) for the plot:
<b>#load necessary packages
library(ggplot2)
library(pROC)
#define object to plot and calculate AUC
rocobj &lt;- roc(test$default, predicted)
auc &lt;- round(auc(test$default, predicted),4)
#create ROC plot
ggroc(rocobj, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')'))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/ROCggplot2.png">
Note that we can also modify the theme of the plot:
<b>#create ROC plot with minimal theme
ggroc(rocobj, colour = 'steelblue', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')')) +
  theme_minimal()</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/10/ROCggplot3.png">
Refer to  this article  for a guide to the best ggplot2 themes.
<h2><span class="orange">How to Create a ROC Curve in SAS</span></h2>
 Logistic regression  is a method we can use to fit a regression model when the response variable is binary.
To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive. This is also called the “true positive rate.”
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative. This is also called the “true negative rate.”
One way to visualize these two metrics is by creating a <b>ROC curve</b>, which stands for “receiver operating characteristic” curve. This is a plot that displays the sensitivity and specificity of a logistic regression model.
The following step-by-step example shows how to create and interpret a ROC curve in SAS.
<h3>Step 1: Create the Dataset</h3>
First, we’ll create a dataset that contains information on the following variables for 18 students:
Acceptance into a certain college (1 = yes, 0 = no)
GPA (scale of 1 to 4)
ACT score (scale of 1 to 36)
<b>/*create dataset*/
data my_data;
    input acceptance gpa act;
    datalines;
1 3 30
0 1 21
0 2 26
0 1 24
1 3 29
1 3 34
0 3 31
1 2 29
0 1 21
1 2 21
0 1 15
1 3 32
1 4 31
1 4 29
0 1 24
1 4 29
1 3 21
1 4 34
;
run;
</b>
<h3>Step 2: Fit the Logistic Regression Model & Create ROC Curve</h3>
Next, we’ll use <b>proc logistic</b> to fit the logistic regression model, using “acceptance” as the response variable and “gpa” and “act” as the predictor variables.
We will specify <b>descending</b> so SAS knows to predict the probability that the response variable will take on a value of 1.
We will also use <b>plots(only)=roc</b> to create the ROC curve for the model:
<b>/*fit logistic regression model & create ROC curve*/
proc logistic data=my_data descending plots(only)=roc;
  model acceptance = gpa act;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/rocsas1.jpg">
<h3>Step 3: Interpret the ROC Curve</h3>
The more that the ROC curve hugs the top left corner of the plot, the better the model does at predicting the value of the response values in the dataset.
From the plot above we can see that the blue ROC curve tends to hug the top left corner, which indicates the the logistic regression model does a good job of predicting the value of the response values.
To quantify how well the logistic regression model fits the data, we can calculate the <b>AUC</b> – area under the curve – which tells us how much of the plot is located under the curve.
The closer AUC is to 1, the better the model. A model with an AUC equal to 0.5 is no better than a model that makes random classifications.
Under the title of the plot above we can see that the AUC for this model is <b>0.9351</b>.
Since this value is close to one, it confirms that the model does a good job of predicting the value of the response values.
We can also use the AUC value to compare the fit of different logistic regression models.
For example, suppose we fit two different logistic regression models and calculate the AUC values for each:
AUC of model 1: <b>0.9351</b>
AUC of model 2: <b>0.8140</b>
Since the AUC of model 1 is greater, this tells us that model 1 does a better job of fitting the data than model 2.
<h2><span class="orange">How to Create and Interpret a ROC Curve in SPSS</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>The probability that the model predicts a positive outcome for an observation when indeed the outcome is positive.
<b>Specificity: </b>The probability that the model predicts a negative outcome for an observation when indeed the outcome is negative.
One easy way to visualize these two metrics is by creating a <b>ROC curve</b>, which is a plot that displays the sensitivity and specificity of a logistic regression model.
This tutorial explains how to create and interpret a ROC curve in SPSS.
<h3>Example: ROC Curve in SPSS</h3>
Suppose we have the following dataset that shows whether or not a basketball player got drafted into the NBA (0 = no, 1 = yes) along with their average points per game in college:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss1.png">
To create an ROC curve for this dataset, click the <b>Analyze </b>tab, then <b>Classify</b>, then <b>ROC Curve</b>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss2.png">
In the new window that pops up, drag the variable <b>draft </b>into the box labelled State Variable. Define the Value of the State Variable to be <b>1</b>. (This is the value that indicates a player got drafted). Drag the variable <b>points </b>into the box labelled Test Variable.
Check the boxes next to <b>With diagonal reference line </b>and <b>Coordinate points of the ROC Curve</b>. Then click <b>OK</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss3.png">
Here is how to interpret the output:
<b>Case Processing Summary:</b>
This table displays the total number of positive and negative cases in the dataset. In this example 8 players got drafted (positive result) and 6 players did not get drafted (negative result):
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss4.png">
<b>ROC Curve:</b>
The ROC (Receiver Operating Characteristic) curve is a plot of the values of sensitivity vs. 1-specificity as the value of the cut-off point moves from 0 to 1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss5.png">
A model with high sensitivity and high specificity will have a ROC curve that hugs the top left corner of the plot. A model with low sensitivity and low specificity will have a curve that is close to the 45-degree diagonal line.
We can see that the ROC curve (the blue line) in this example hugs the top left corner of the plot, which indicates that the model does a good job of predicting whether or not players will get drafted, based on their average points per game.
<b>Area Under the Curve:</b>
The Area Under the Curve gives us an idea of how well the model is able to distinguish between positive and negative outcomes. The AUC can range from 0 to 1. The higher the AUC, the better the model is at correctly classifying outcomes.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss6.png">
We can see that the AUC for this particular logistic regression model is <b>.948</b>, which is extremely high. This indicates that the model does a good job of predicting whether or not a player will get drafted.
<b>Coordinates of the Curve:</b>
This last table displays the sensitivity and 1 – specificity of the ROC curve for various cut-off points.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/06/rocspss7.png">
For example:
If we allow the cut-off point to be <b>8.50</b>, this means we predict that any player who scores less than 8.50 points per game to not get drafted, and any player who scores greater than 8.50 points per game to get drafted.
Using this as a cut off point, our <b>sensitivity </b>would be 100% (since each player that scored less than 8.50 points per game indeed did not get drafted) and our <b>1 – specificity</b> would be <b>66.7% </b>(since 8 out of 12 players who scored more than 8.50 points per game actually did get drafted).
The table above allows us to see the sensitivity and 1-specificity for every potential cut-off point.
<h2><span class="orange">How to Create and Interpret a ROC Curve in Stata</span></h2>
 Logistic Regression  is a statistical method that we use to fit a regression model when the response variable is binary. To assess how well a logistic regression model fits a dataset, we can look at the following two metrics:
<b>Sensitivity: </b>the probability that the model predicts a positive outcome for an observation when indeed the outcome is positive.
<b>Specificity: </b>the probability that the model predicts a negative outcome for an observation when indeed the outcome is negative.
One easy way to visualize these two metrics is by creating a <b>ROC curve</b>, which is a plot that displays the sensitivity and specificity of a logistic regression model.
This tutorial explains how to create and interpret a ROC curve in Stata.
<h2>Example: ROC Curve in Stata</h2>
For this example we will use a dataset called <em>lbw</em>, which contains the folllowing variables for 189 mothers:
<b>low</b> – whether or not the baby had a low birthweight. 1 = yes, 0 = no.
<b>age</b> – age of the mother.
<b>smoke</b> – whether or not the mother smoked during pregnancy. 1 = yes, 0 = no.
We will fit a logistic regression model to the data using age and smoking as explanatory variables and low birthweight as the response variable. Then we will create a ROC curve to analyze how well the model fits the data.
<b>Step 1: Load and view the data.</b>
Load the data using the following command:
<b>use http://www.stata-press.com/data/r13/lbw</b>
Gain a quick understanding of the dataset using the following command:
<b>summarize</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/logisticRegStata1.png"(max-width: 534px) 100vw, 534px">
There are 11 different variables in the dataset, but the only three that we care about are low, age, and smoke.
<b>Step 2: Fit the logistic regression model.</b>
Use the following command to fit the logistic regression model:
<b>logit low age smoke</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/logisticRegStata3.png"(max-width: 579px) 100vw, 579px">
<b>Step 3: Create the ROC curve.</b>
We can create the ROC curve for the model using the following command:
<b>lroc</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/03/ROCcurveStata1.png">
<b>Step 4: Interpret the ROC curve.</b>
When we fit a logistic regression model, it can be used to calculate the probability that a given observation has a positive outcome, based on the values of the predictor variables.
To determine if an observation should be classified as positive, we can choose a cut-point such that observations with a fitted probability above the cut-point are classified as positive and any observations with a fitted probability below the cut-point are classified as negative.
For example, suppose we choose the cut-point to be 0.5. This means that any observation with a fitted probability greater than 0.5 will be predicted to have a positive outcome, while any observation with a fitted probability less than or equal to 0.5 will be predicted to have a negative outcome.
The ROC curve shows us the values of sensitivity vs. 1-specificity as the value of the cut-off point moves from 0 to 1.  A model with high sensitivity and high specificity will have a ROC curve that hugs the top left corner of the plot. A model with low sensitivity and low specificity will have a curve that is close to the 45-degree diagonal line.
The <b>AUC</b> <b>(area under curve)</b> gives us an idea of how well the model is able to distinguish between positive and negative outcomes. The AUC can range from 0 to 1. The higher the AUC, the better the model is at correctly classifying outcomes. In our example, we can see that the AUC is <b>0.6111</b>.
We can use AUC to compare the performance of two or more models. The model with the higher AUC is the one that performs best.
<h2><span class="orange">How to Calculate a Rolling Average in R (With Example)</span></h2>
In time series analysis, a <b>rolling average</b> represents the average value of a certain number of previous periods.
The easiest way to calculate a rolling average in R is to use the <b>rollmean()</b> function from the <b>zoo</b> package:
<b>library(dplyr)
library(zoo)
#calculate 3-day rolling average
df %>%
  mutate(rolling_avg = rollmean(values, k=3, fill=NA, align='right'))</b>
This particular example calculates a <b>3</b>-day rolling average for the column titled <b>values</b>.
The following example shows how to use this function in practice.
<h2>Example: Calculate Rolling Average in R</h2>
Suppose we have the following data frame in R that shows the sales of some product during 10 consecutive days:
<b>#create data frame
df &lt;- data.frame(day=c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), sales=c(25, 20, 14, 16, 27, 20, 12, 15, 14, 19))
#view data frame
df
   day sales
1    1    25
2    2    20
3    3    14
4    4    16
5    5    27
6    6    20
7    7    12
8    8    15
9    9    14
10  10    19</b>
We can use the following syntax to create a new column called <b>avg_sales3</b> that displays the rolling average value of sales for the previous 3 days in each row of the data frame:
<b>library(dplyr)
library(zoo)
#calculate 3-day rolling average of sales
df %>%
  mutate(avg_sales3 = rollmean(sales, k=3, fill=NA, align='right'))
   day sales avg_sales3
1    1    25         NA
2    2    20         NA
3    3    14   19.66667
4    4    16   16.66667
5    5    27   19.00000
6    6    20   21.00000
7    7    12   19.66667
8    8    15   15.66667
9    9    14   13.66667
10  10    19   16.00000
</b>
<b>Note</b>: The value for <b>k</b> in the <b>rollmean()</b> function controls the number of previous periods used to calculate the rolling average.
The <b>avg_sales3</b> column shows the rolling average value of sales for the previous 3 periods.
For example, the first value of <b>19.66667</b> is calculated as:
3-Day Moving Average = (25 + 20 + 14) / 3 = <b>19.66667</b>
You can also calculate several rolling averages at once by using multiple <b>rollmean()</b> functions within the <b>mutate()</b> function.
For example, the following code shows how to calculate the 3-day and 4-day moving average of sales:
<b>library(dplyr)
library(zoo)
#calculate 3-day and 4-day rolling average of sales
df %>%
  mutate(avg_sales3 = rollmean(sales, k=3, fill=NA, align='right'),
         avg_sales4 = rollmean(sales, k=4, fill=NA, align='right'))
   day sales avg_sales3 avg_sales4
1    1    25         NA         NA
2    2    20         NA         NA
3    3    14   19.66667         NA
4    4    16   16.66667      18.75
5    5    27   19.00000      19.25
6    6    20   21.00000      19.25
7    7    12   19.66667      18.75
8    8    15   15.66667      18.50
9    9    14   13.66667      15.25
10  10    19   16.00000      15.00
</b>
The <b>avg_sales3</b> and <b>avg_sales4</b> columns show the 3-day and 4-day rolling average of sales, respectively.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Plot Multiple Columns in R 
 How to Average Across Columns in R 
 How to Calculate the Mean by Group in R 
<h2><span class="orange">How to Calculate Rolling Correlation in Excel</span></h2>
<b>Rolling correlations</b> are correlations between two time series on a rolling window. One benefit of this type of correlation is that you can visualize the correlation between two time series over time.
This tutorial explains how to calculate and visualize rolling correlations in Excel.
<h3>How to Calculate Rolling Correlations in Excel</h3>
Suppose we have the following two time series in Excel that display the total number of products sold for two different products during a 20-month period:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel1.png">
To calculate the <b>3-month rolling correlation </b>between the two time series, we can simply use the <b>CORREL() </b>function in Excel. For example, here’s how to calculate the first 3-month rolling correlation between the two time series:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel2.png">
We can then drag this formula down to the rest of the cells in the column:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel3.png">
Each cell in the column titled “Rolling 3-month correlation” tells us the correlation between the two product sales for the previous 3 months.
Note that we could use a longer rolling time frame if we’d like as well. For example, we could instead calculate the rolling 6-month correlation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel4.png">
<h3>How to Visualize Rolling Correlations in Excel</h3>
Once we’ve calculate a rolling correlation between two time series, we can visualization the rolling correlation using a simple line chart. Use the following steps to do so:
<b>Step 1: Highlight the rolling correlation values.</b>
First, highlight the values in the cell range D7:D21.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel5.png">
<b>Step 2: Insert a line chart.</b>
Next, click the <b>Insert </b>tab along the top ribbon in Excel. Within the <b>Charts</b> group, click on the first chart option in the <b>Line or Area Chart </b>section.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel6.png">
The following line chart will automatically appear:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/08/rollingCorrExcel7.png">
The y-axis displays the rolling 6-month correlation between the two time series and the x-axis displays the ending month for the rolling correlation.
Feel free to modify the title, axes labels, and colors to make the chart more aesthetically pleasing.
<h2><span class="orange">How to Calculate Rolling Correlation in R</span></h2>
<b>Rolling correlations</b> are correlations between two time series on a rolling window. One benefit of this type of correlation is that you can visualize the correlation between two time series over time.
This tutorial explains how to calculate rolling correlations in R.
<h3>How to Calculate Rolling Correlations in R</h3>
Suppose we have the following data frame that display the total number of products sold for two different products (<em>x</em> and <em>y</em>) during a 15-month period:
<b>#create data
data &lt;- data.frame(month=1:15,   x=c(13, 15, 16, 15, 17, 20, 22, 24, 25, 26, 23, 24, 23, 22, 20),   y=c(22, 24, 23, 27, 26, 26, 27, 30, 33, 32, 27, 25, 28, 26, 28))
#view first six rows
head(data)
  month  x  y
1     1 13 22
2     2 15 24
3     3 16 23
4     4 15 27
5     5 17 26
6     6 20 26</b>
To calculate a rolling correlation in R, we can use the  rollapply() function  from the <b>zoo</b> package.
This function uses the following syntax:
<b>rollapply(data, width, FUN, by.column=TRUE)</b>
where:
<b>data:</b> Name of the data frame
<b>width:</b> Integer specifying the window width for the rolling correlation
<b>FUN:</b> The function to be applied.
<b>by.column: </b>Specifies whether to apply the function to each column separately. This is TRUE by default, but to calculate a rolling correlation we need to specify this to be FALSE.
Here’s how to use this function to calculate the 3-month rolling correlation in sales between product <em>x</em> and product <em>y</em>:
<b>#calculate 3-month rolling correlation between sales for <em>x</em> and <em>y</em>
rollapply(data, width=3, function(x) cor(x[,2],x[,3]), by.column=FALSE)
 [1]  0.6546537 -0.6933752 -0.2401922 -0.8029551  0.8029551  0.9607689
 [7]  0.9819805  0.6546537  0.8824975  0.8170572 -0.9449112 -0.3273268
[13] -0.1889822
</b>
This function returns the correlation between the two product sales for the previous 3 months. For example:
The correlation in sales during months 1 through 3 was <b>0.6546537</b>.
The correlation in sales during months 2 through 4 was <b>-0.6933752.</b>
The correlation in sales during months 3 through 5 was <b>-0.2401922.</b>
And so on.
We can easily adjust this formula to calculate the rolling correlation for a different time period. For example, the following code shows how to calculate the 6-month rolling correlation in sales between the two products:
<b>#calculate 6-month rolling correlation between sales for <em>x</em> and <em>y</em>
rollapply(data, width=6, function(x) cor(x[,2],x[,3]), by.column=FALSE)
 [1] 0.5587415 0.4858553 0.6931033 0.7564756 0.8959291 0.9067715 0.7155418
 [8] 0.7173740 0.7684468 0.4541476
</b>
This function returns the correlation between the two product sales for the previous 6 months. For example:
The correlation in sales during months 1 through 6 was <b>0.5587415</b>.
The correlation in sales during months 2 through 7 was <b>0.4858553.</b>
The correlation in sales during months 3 through 8 was <b>0.6931033.</b>
And so on.
<h3>Notes</h3>
Keep the following in mind when using the rollapply() function:
The <b>width</b> (i.e. the rolling window) should be 3 or greater in order to calculate correlations.
In the formulas above, we used cor(x[,2],x[3]) because the two columns that we wanted to calculate correlations between were in position <b>2 </b>and <b>3</b>. Adjust these numbers if the columns you’re interested in are located in different positions.
<b>Related:</b>  How to Calculate Rolling Correlation in Excel 
<h2><span class="orange">How to Calculate Rolling Correlation in Pandas (With Examples)</span></h2>
<b>Rolling correlations</b> are correlations between two time series on a rolling window. One benefit of this type of correlation is that you can visualize the correlation between two time series over time.
This tutorial explains how to calculate and visualize rolling correlations for a pandas DataFrame in Python.
<h3>How to Calculate Rolling Correlations in Pandas</h3>
Suppose we have the following data frame that display the total number of products sold for two different products (<em>x</em> and <em>y</em>) during a 15-month period:
<b>import pandas as pd
import numpy as np
#create DataFrame
df = pd.DataFrame({'month': np.arange(1, 16),   'x': [13, 15, 16, 15, 17, 20, 22, 24, 25, 26, 23, 24, 23, 22, 20],   'y': [22, 24, 23, 27, 26, 26, 27, 30, 33, 32, 27, 25, 28, 26, 28]})
#view first six rows
df.head()
  month  x  y
1     1 13 22
2     2 15 24
3     3 16 23
4     4 15 27
5     5 17 26
6     6 20 26</b>
To calculate a rolling correlation in pandas, we can use the  rolling.corr() function .
This function uses the following syntax:
<b>df[‘x’].rolling(width).corr(df[‘y’])</b>
where:
<b>df:</b> Name of the data frame
<b>width:</b> Integer specifying the window width for the rolling correlation
<b>x, y:</b> The two column names to calculate the rolling correlation between
Here’s how to use this function to calculate the 3-month rolling correlation in sales between product <em>x</em> and product <em>y</em>:
<b>#calculate 3-month rolling correlation between sales for <em>x</em> and <em>y</em>
df['x'].rolling(3).corr(df['y'])
0          NaN
1          NaN
2     0.654654
3    -0.693375
4    -0.240192
5    -0.802955
6     0.802955
7     0.960769
8     0.981981
9     0.654654
10    0.882498
11    0.817057
12   -0.944911
13   -0.327327
14   -0.188982
dtype: float64
</b>
This function returns the correlation between the two product sales for the previous 3 months. For example:
The correlation in sales during months 1 through 3 was <b>0.654654</b>.
The correlation in sales during months 2 through 4 was <b>-0.693375.</b>
The correlation in sales during months 3 through 5 was <b>-0.240192.</b>
And so on.
We can easily adjust this formula to calculate the rolling correlation for a different time period. For example, the following code shows how to calculate the 6-month rolling correlation in sales between the two products:
<b>#calculate 6-month rolling correlation between sales for <em>x</em> and <em>y</em>
df['x'].rolling(6).corr(df['y']) 
0          NaN
1          NaN
2          NaN
3          NaN
4          NaN
5     0.558742
6     0.485855
7     0.693103
8     0.756476
9     0.895929
10    0.906772
11    0.715542
12    0.717374
13    0.768447
14    0.454148
dtype: float64
</b>
This function returns the correlation between the two product sales for the previous 6 months. For example:
The correlation in sales during months 1 through 6 was <b>0.558742</b>.
The correlation in sales during months 2 through 7 was <b>0.485855.</b>
The correlation in sales during months 3 through 8 was <b>0.693103.</b>
And so on.
<h3>Notes</h3>
Here are a few notes for the functions used in these examples:
The <b>width</b> (i.e. the rolling window) should be 3 or greater in order to calculate correlations.
You can find the full documentation for the rolling.corr() function  here .
<h2><span class="orange">How to Calculate a Rolling Mean in Pandas</span></h2>
A <b>rolling mean</b> is simply the mean of a certain number of previous periods in a time series.
To calculate the rolling mean for one or more columns in a pandas DataFrame, we can use the following syntax:
<b>df['column_name'].rolling(rolling_window).mean()
</b>
This tutorial provides several examples of how to use this function in practice.
<h3>Example: Calculate the Rolling Mean in Pandas</h3>
Suppose we have the following pandas DataFrame:
<b>import numpy as np
import pandas as pd
#make this example reproducible
np.random.seed(0)
#create dataset
period = np.arange(1, 101, 1)
leads = np.random.uniform(1, 20, 100)
sales = 60 + 2*period + np.random.normal(loc=0, scale=.5*period, size=100)
df = pd.DataFrame({'period': period, 'leads': leads, 'sales': sales})
#view first 10 rows
df.head(10)
   period    leads    sales
0111.42745761.417425
1214.58859864.900826
2312.45250466.698494
3411.35278064.927513
459.04944173.720630
5613.27198877.687668
679.31415778.125728
7817.94368775.280301
8919.30959273.181613
9108.28538985.272259
</b>
We can use the following syntax to create a new column that contains the rolling mean of ‘sales’ for the previous 5 periods:
<b>#find rolling mean of previous 5 sales periods
df['rolling_sales_5'] = df['sales'].rolling(5).mean()
#view first 10 rows
df.head(10)
period    leads    salesrolling_sales_5
0111.42745761.417425NaN
1214.58859864.900826NaN
2312.45250466.698494NaN
3411.35278064.927513NaN
459.04944173.72063066.332978
5613.27198877.68766869.587026
679.31415778.12572872.232007
7817.94368775.28030173.948368
8919.30959273.18161375.599188
9108.28538985.27225977.909514
</b>
We can manually verify that the rolling mean sales displayed for period 5 is the mean of the previous 5 periods:
Rolling mean at period 5: (61.417+64.900+66.698+64.927+73.720)/5 = <b>66.33</b>
We can use similar syntax to calculate the rolling mean of multiple columns:
<b>#find rolling mean of previous 5 leads periods 
df['rolling_leads_5'] = df['leads'].rolling(5).mean() 
#find rolling mean of previous 5 leads periods
df['rolling_sales_5'] = df['sales'].rolling(5).mean()
#view first 10 rows
df.head(10)
period    leads    salesrolling_sales_5 rolling_leads_5
0111.42745761.417425NaNNaN
1214.58859864.900826NaNNaN
2312.45250466.698494NaNNaN
3411.35278064.927513NaNNaN
459.04944173.72063066.332978 11.774156
5613.27198877.68766869.587026 12.143062
679.31415778.12572872.232007 11.088174
7817.94368775.28030173.948368 12.186411
8919.30959273.18161375.599188 13.777773
9108.28538985.27225977.909514 13.624963
</b>
We can also create a quick line plot using Matplotlib to visualize the raw sales compared to the rolling mean of sales:
<b>import matplotlib.pyplot as plt
plt.plot(df['rolling_sales_5'], label='Rolling Mean')
plt.plot(df['sales'], label='Raw Data')
plt.legend()
plt.ylabel('Sales')
plt.xlabel('Period')
plt.show()
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/12/rollingMeanPandas1.png">
The blue line displays the 5-period rolling mean of sales and the orange line displays the raw sales data.
<h2><span class="orange">How to Calculate Root Mean Square Error (RMSE) in Excel</span></h2>
In statistics,  regression analysis  is a technique we use to understand the relationship between a predictor variable, x, and a response variable, y. 
When we conduct regression analysis, we end up with a model that tells us the predicted value for the response variable based on the value of the predictor variable.
One way to assess how “good” our model fits a given dataset is to calculate the <b>root mean square error</b>, which is a metric that tells us how far apart our predicted values are from our observed values, on average.
The formula to find the root mean square error, more commonly referred to as <b>RMSE</b>, is as follows:
<b>RMSE </b>= √[ Σ(P<sub>i</sub> – O<sub>i</sub>)<sup>2</sup> / n ]
where:
Σ is a fancy symbol that means “sum”
P<sub>i</sub> is the predicted value for the i<sup>th</sup> observation in the dataset
O<sub>i</sub> is the observed value for the i<sup>th</sup> observation in the dataset
n is the sample size
<b>Technical</b> Notes:</b> 
The root mean square error can be calculated for any type of model that produces predicted values, which can then be compared to the observed values of a dataset.
The root mean square error is also sometimes called the root mean square deviation, which is often abbreviated as RMSD.
Next, let’s look at an example of how to calculate root mean square error in Excel.
<h2>How to Calculate Root Mean Square Error in Excel</h2>
There is no built-in function to calculate RMSE in Excel, but we can calculate it fairly easily with a single formula. We’ll show how to calculate RMSE for two different scenarios.
<h3>Scenario 1</h3>
In one scenario, you might have one column that contains the predicted values of your model and another column that contains the observed values. The image below shows an example of this scenario:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse1.png">
If this is the case, then you can calculate the RMSE by typing the following formula into any cell, and then clicking CTRL+SHIFT+ENTER:
<b>=SQRT(SUMSQ(A2:A21-B2:B21) / COUNTA(A2:A21))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse2.png">
This tells us that the root mean square error is <b>2.6646</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse3.png">
The formula might look a bit tricky, but it makes sense once you break it down:
<b>=SQRT(SUMSQ(A2:A21-B2:B21) / COUNTA(A2:A21))</b>
First, we calculate the sum of the squared differences between the predicted and observed values using the <b>SUMSQ() </b>function.
Next, we divide by the sample size of the dataset using <b>COUNTA()</b>, which counts the number of cells in a range that are not empty.
Lastly, we take the square root of the whole calculation using the <b>SQRT()</b> function.
<h3>Scenario 2</h3>
In another scenario, you may have already calculated the differences between the predicted and observed values. In this case, you will only have one column that displays the differences.
The image below shows an example of this scenario. The predicted values are displayed in column A, the observed values in column B, and the difference between the predicted and observed values in column D:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse4.png">
If this is the case, then you can calculate the RMSE by typing the following formula into any cell, and then clicking CTRL+SHIFT+ENTER:
<b>=SQRT(SUMSQ(D2:D21) / COUNTA(D2:D21))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse5.png">
This tells us that the root mean square error is <b>2.6646</b>, which matches the result that we got in the first scenario. This confirms that these two approaches to calculating RMSE are equivalent.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/02/rmse6.png">
The formula we used in this scenario is only slightly different than the one we used in the previous scenario:
<b>=SQRT(SUMSQ(D2:D21) / COUNTA(D2:D21))</b>
Since we already calculated the differences between the predicted and observed values in column D, we can calculate the sum of the squared differences by using the <b>SUMSQ() </b>function with just the values in column D.
Next, we divide by the sample size of the dataset using <b>COUNTA()</b>, which counts the number of cells in a range that are not empty.
Lastly, we take the square root of the whole calculation using the <b>SQRT()</b> function.
<h2>How to Interpret RMSE</h2>
As mentioned earlier, <b>RMSE </b>is a useful way to see how well a regression model (or any model that produces predicted values) is able to “fit” a dataset.
The larger the RMSE, the larger the difference between the predicted and observed values, which means the worse the regression model fits the data. Conversely, the smaller the RMSE, the better a model is able to fit the data.
It can be particularly useful to compare the RMSE of two different models with each other to see which model fits the data better.
<em><b>For more tutorials in Excel, be sure to check out our  Excel Guides Page , which lists every Excel tutorial on Statology.</b></em>
<h2><span class="orange">How to Rotate Axis Labels in ggplot2 (With Examples)</span></h2>
You can use the following syntax to rotate axis labels in a ggplot2 plot:
<b>p + theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))
</b>
The <b>angle</b> controls the angle of the text while <b>vjust</b> and <b>hjust</b> control the vertical and horizontal justification of the text.
The following step-by-step example shows how to use this syntax in practice.
<h3>Step 1: Create the Data Frame</h3>
First, let’s create a simple data frame:
<b>#create data frame
df = data.frame(team=c('The Amazing Amazon Anteaters',       'The Rowdy Racing Raccoons',       'The Crazy Camping Cobras'),
                points=c(14, 22, 11))
#view data frame
df
          team points
1 The Amazing Amazon Anteaters     14
2    The Rowdy Racing Raccoons     22
3     The Crazy Camping Cobras     11
</b>
<h3>Step 2: Create a Bar Plot</h3>
Next, let’s create a bar plot to visualize the points scored by each team:
<b>library(ggplot2)
#create bar plot
ggplot(data=df, aes(x=team, y=points)) +
  geom_bar(stat="identity")</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/rotate1.png">
<h3>Step 3: Rotate the Axis Labels of the Plot</h3>
We can use the following code to rotate the x-axis labels 90 degrees:
<b>library(ggplot2)
#create bar plot with axis labels rotated 90 degrees
ggplot(data=df, aes(x=team, y=points)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle=90, vjust=.5, hjust=1))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/rotate2.png">
Or we can use the following code to rotate the x-axis labels 45 degrees:
<b>library(ggplot2)
#create bar plot with axis labels rotated 90 degrees
ggplot(data=df, aes(x=team, y=points)) +
  geom_bar(stat="identity") +
  theme(axis.text.x = element_text(angle=45, vjust=1, hjust=1))</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/06/rotate3.png">
Depending on the <b>angle</b> you rotate the labels, you may need to adjust the <b>vjust</b> and <b>hjust</b> values to ensure that the labels are close enough to the plot.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in ggplot2:
 How to Set Axis Limits in ggplot2 
 How to Reverse Order of Axis in ggplot2 
 How to Remove Gridlines in ggplot2 
 How to Adjust Line Thickness in ggplot2 
<h2><span class="orange">How to Rotate Slices of a Pie Chart in Excel</span></h2>
A <b>pie chart</b> is a type of chart that is shaped like a circle and uses “slices” to represent proportions of a whole.
Fortunately it’s easy to create a pie chart in Excel, but occasionally you may want to rotate the individual slices of the chart in a certain direction. This tutorial explains how to do so.
<h3>Example: Rotate the Slices of a Pie Chart</h3>
Suppose we have the following pie chart in Excel that shows the total sales by quarter for a particular company:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotatePie1.png">
To rotate the slices in the chart, simply right click anywhere on the pie chart and then click <b>Format Data Series…</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotatePie2.png">
A new window will pop up on the right side of the screen with a slider bar that allows you to choose the <b>Angle of first slice</b>. By default, this is set to 0°.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotatePie3.png">
You can choose any angle to rotate the slices in the pie chart in a <b>clockwise rotation</b>. For example, we could rotate each of the slices 90° clockwise:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotatePie4.png">
This results in the following:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotate5.png">
Or we could rotate each of the slices 180° clockwise:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotate6.png">
And if we choose a full 360° then each of the slices in the chart will end up in the exact same positions they started in:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/rotatePie1.png">
<h2><span class="orange">How to Round Numbers in R (5 Examples)</span></h2>
You can use the following functions to round numbers in R: 
<b>round(x, digits = 0)</b>: Rounds values to specified number of decimal places.
<b>signif(x, digits = 6)</b>: Rounds values to specified number of significant digits.
<b>ceiling(x)</b>: Rounds values up to nearest integer.
<b>floor(x)</b>: Rounds values down to nearest integer.
<b>trunc(x)</b>: Truncates (cuts off) decimal places from values.
The following examples show how to use each of these functions in practice.
<h3>Example 1: round() Function in R</h3>
The following code shows how to use the <b>round()</b> function in R:
<b>#define vector of data
data &lt;- c(.3, 1.03, 2.67, 5, 8.91)
#round values to 1 decimal place
round(data, digits = 1)
[1] 0.3 1.0 2.7 5.0 8.9
</b>
<h3>Example 2: signif() Function in R</h3>
The following code shows how to use the <b>signif()</b> function to round values to a specific number of significant digits in R:
<b>#define vector of data
data &lt;- c(.3, 1.03, 2.67, 5, 8.91)
#round values to 3 significant digits
signif(data, digits = 3)
[1] 0.30 1.03 2.67 5.00 8.91</b>
<h3>Example 3: ceiling() Function in R</h3>
The following code shows how to use the <b>ceiling()</b> function to round values up to the nearest integer:
<b>#define vector of data
data &lt;- c(.3, 1.03, 2.67, 5, 8.91)
#round values up to nearest integer
ceiling(data)
[1] 1 2 3 5 9
</b>
<h3>Example 4: floor() Function in R</h3>
The following code shows how to use the <b>floor()</b> function to round values down to the nearest integer:
<b>#define vector of data
data &lt;- c(.3, 1.03, 2.67, 5, 8.91)
#round values down to nearest integer
floor(data)
[1] 0 1 2 5 8</b>
<h3>Example 5: trunc() Function in R</h3>
The following code shows how to use the <b>trunc()</b> function to truncate (cut off) decimal places from values:
<b>#define vector of data
data &lt;- c(.3, 1.03, 2.67, 5, 8.91)
#truncate decimal places from values
trunc(data)
[1] 0 1 2 5 8</b>
<h2><span class="orange">How to Calculate the Median Value of Rows in R</span></h2>
You can use the following methods to calculate the median value of rows in R:
<b>Method 1: Calculate Median of Rows Using Base R</b>
<b>df$row_median = apply(df, 1, median, na.rm=TRUE)
</b>
<b>Method 2: Calculate Median of Rows Using dplyr</b>
<b>library(dplyr) 
df %>%
  rowwise() %>%
  mutate(row_median = median(c_across(where(is.numeric)), na.rm=TRUE))
</b>
The following examples show how to use each method in practice.
<h2>Example 1: Calculate Median of Rows Using Base R</h2>
Suppose we have the following data frame in R that shows the points scored by various basketball players during three different games:
<b>#create data frame
df &lt;- data.frame(game1=c(10, 12, 14, 15, 16, 18, 19), game2=c(14, 19, 13, 8, 15, 15, 17), game3=c(9, NA, 15, 25, 26, 30, 19))
#view data frame
df
  game1 game2 game3
1    10    14     9
2    12    19    NA
3    14    13    15
4    15     8    25
5    16    15    26
6    18    15    30
7    19    17    19
</b>
We can use the <b>apply()</b> function from base R to create a new column that shows the median value of each row:
<b>#calculate median of each row
df$row_median = apply(df, 1, median, na.rm=TRUE)
#view updated data frame
df
  game1 game2 game3 row_median
1    10    14     9       10.0
2    12    19    NA       15.5
3    14    13    15       14.0
4    15     8    25       15.0
5    16    15    26       16.0
6    18    15    30       18.0
7    19    17    19       19.0
</b>
The new column called <b>row_median</b> contains the median value of each row in the data frame.
<h2>Example 2: Calculate Median of Rows Using dplyr</h2>
Suppose we have the following data frame in R that shows the points scored by various basketball players during three different games:
<b>#create data frame
df &lt;- data.frame(player=c('A', 'B', 'C', 'D', 'E', 'F', 'G'), game1=c(10, 12, 14, 15, 16, 18, 19), game2=c(14, 19, 13, 8, 15, 15, 17), game3=c(9, NA, 15, 25, 26, 30, 19))
#view data frame
df
  player game1 game2 game3
1      A    10    14     9
2      B    12    19    NA
3      C    14    13    15
4      D    15     8    25
5      E    16    15    26
6      F    18    15    30
7      G    19    17    19
</b>
We can use the <b>mutate()</b> function from the dplyr package to create a new column that shows the median value of each row for the numeric columns only:
<b>library(dplyr)
#calculate median of rows for numeric columns only
df %>%
  rowwise() %>%
  mutate(row_median = median(c_across(where(is.numeric)), na.rm=TRUE))
# A tibble: 7 x 5
# Rowwise: 
  player game1 game2 game3 row_median
            
1 A         10    14     9         10  
2 B         12    19    NA       15.5
3 C         14    13    15         14  
4 D         15     8    25         15  
5 E         16    15    26         16  
6 F         18    15    30         18  
7 G         19    17    19         19  </b>
The new column called <b>row_median</b> contains the median value of each row in the data frame for the numeric columns only.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in R:
 How to Replace NA with Median in R 
 How to Calculate a Trimmed Mean in R 
 How to Calculate a Weighted Mean in R 
<h2><span class="orange">How to Use rowMeans() Function in R</span></h2>
The <b>rowMeans()</b> function in R can be used to calculate the mean of several rows of a matrix or data frame in R.
This function uses the following basic syntax:
<b>#calculate row means of every column
rowMeans(df)
#calculate row means and exclude NA values
rowMeans(df, na.rm=T)
#calculate row means of specific rows
rowMeans(df[1:3, ])</b>
The following examples show how to use this syntax in practice.
<h3>Example 1: Calculate Mean of Every Row</h3>
The following code shows how to calculate the mean of every row in a data frame:
<b>#create data frame
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28), blocks=c(1, 4, 11, 0, 2))
#view data frame
df
  points assists rebounds blocks
1     99      33       30      1
2     91      28       28      4
3     86      31       24     11
4     88      39       24      0
5     95      34       28      2
#calculate row means
rowMeans(df)
[1] 40.75 37.75 38.00 37.75 39.75</b>
Here’s how to interpret the output:
The mean of values in the first row is <b>40.75</b>.
The mean of values in the second row is <b>37.75</b>.
And so on.
<h3>
<b>Example 2: Calculate Mean of Every Row & Exclude NA’s</b>
</h3>
The following code shows how to calculate the mean of every row and exclude NA values:
<b>#create data frame with some NA values
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, NA, 31, 39, 34), rebounds=c(30, 28, NA, NA, 28), blocks=c(1, 4, 11, 0, 2))
#view data frame
df
  points assists rebounds blocks
1     99      33       30      1
2     91      NA       28      4
3     86      31       NA     11
4     88      39       NA      0
5     95      34       28      2
#calculate row means
rowMeans(df, na.rm=T)
[1] 40.75000 41.00000 42.66667 42.33333 39.75000
</b>
<h3>Example 3: Calculate Mean of Specific Rows</h3>
The following code shows how to calculate the mean values of specific rows in the data frame:
<b>#create data frame
df &lt;- data.frame(points=c(99, 91, 86, 88, 95), assists=c(33, 28, 31, 39, 34), rebounds=c(30, 28, 24, 24, 28), blocks=c(1, 4, 11, 0, 2))
#calculate row means for first three rows only
rowMeans(df[1:3, ])
    1     2     3 
40.75 37.75 38.00  </b>
We can also use the <b>c()</b> syntax to select specific rows:
<b>#calculate row means for rows 1, 4, and 5
rowMeans(df[c(1, 4, 5), ])
    1     4     5 
40.75 37.75 39.75  </b>
<h2><span class="orange">How to Use rowSums() Function in R</span></h2>
The <b>rowSums()</b> function in R can be used to calculate the sum of the values in each row of a matrix or data frame in R.
This function uses the following basic syntax:
<b>rowSums(x, na.rm=FALSE)</b>
where:
<b>x</b>: Name of the matrix or data frame.
<b>na.rm</b>: Whether to ignore NA values. Default is FALSE.
The following examples show how to use this function in practice.
<h3>Example 1: Use rowSums() with Data Frame</h3>
The following code shows how to use <b>rowSums()</b> to find the sum of the values in each row of a data frame:
<b>#create data frame
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, 2, 5, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, 14, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3    2    3    1
3    3    5    6    2
4    4    3    6   14
5    5    2    8    9
#find sum of each row
rowSums(df)
[1] 12  9 16 27 24
</b>
<h3>Example 2: Use rowSums() with NA Values in Data Frame</h3>
The following code shows how to use <b>rowSums()</b> to find the sum of the values in each row of a data frame when there are NA values in some rows:
<b>#create data frame with some NA values
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, NA, NA, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, NA, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3   NA    3    1
3    3   NA    6    2
4    4    3    6   NA
5    5    2    8    9
#find sum of each row
rowSums(df, na.rm=TRUE)
[1] 12  7 11 13 24</b>
<h3>Example 3: Use rowSums() with Specific Rows</h3>
The following code shows how to use <b>rowSums()</b> to find the sum of the values in specific rows of a data frame:
<b>#create data frame with some NA values
df &lt;- data.frame(var1=c(1, 3, 3, 4, 5), var2=c(7, NA, NA, 3, 2), var3=c(3, 3, 6, 6, 8), var4=c(1, 1, 2, NA, 9))
#view data frame
df
  var1 var2 var3 var4
1    1    7    3    1
2    3   NA    3    1
3    3   NA    6    2
4    4    3    6   NA
5    5    2    8    9
#find sum of rows 1, 3, and 5
rowSums(df[c(1, 3, 5), ], na.rm=TRUE)
 1  3  5 
12 11 24</b>
<h2><span class="orange">How to Use the Equivalent of runif() in Python</span></h2>
In the R programming language, we can use the <b>runif()</b> function to generate a vector of random values that follow a  uniform distribution  with a specific minimum and maximum value.
For example, the following code shows how to use <b>runif()</b> to create a vector of 8 random values that follow a uniform distribution with a minimum value of 5 and maximum value of 10:
<b>#make this example reproducible
set.seed(1)
#generate vector of 8 values that follow uniform distribution with min=5 and max=10
runif(n=8, min=5, max=10)
[1] 6.327543 6.860619 7.864267 9.541039 6.008410 9.491948 9.723376 8.303989
</b>
The equivalent of the <b>runif()</b> function in Python is the <b>np.random.uniform()</b> function, which uses the following basic syntax:
<b>np.random.uniform(low=0, high=1, size=None)</b>
where:
<b>low</b>: Minimum value of the distribution
<b>high</b>: Maximum value of the distribution
<b>size</b>: Sample size
The following example shows how to use this function in practice.
<h2>Example: Using the Equivalent of runif() in Python</h2>
The following code shows how to use the <b>np.random.uniform()</b> function to generate an array of random values that follow a uniform distribution with a specific minimum and maximum value:
<b>import numpy as np
#make this example reproducible
np.random.seed(1)
#generate array of 8 values that follow uniform distribution with min=5 and max=10
np.random.uniform(low=5, high=10, size=8)
array([7.08511002, 8.60162247, 5.00057187, 6.51166286, 5.73377945,
       5.46169297, 5.93130106, 6.72780364])
</b>
The result is a NumPy array that contains 8 values generated from a uniform distribution with a minimum value of 5 and maximum value of 10.
You can also create a histogram using Matplotlib to visualize a normal distribution generated by the <b>np.random.uniform()</b> function:
<b>import numpy as np
import matplotlib.pyplot as plt
#make this example reproducible
np.random.seed(1)
#generate array of 200 values that follow uniform distribution with min=5 and max=10
data = np.random.uniform(low=5, high=10, size=200)
#create histogram to visualize distribution of values
plt.hist(data, bins=30, edgecolor='black')
</b>
<b> <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/10/runif1.jpg"553"></b>
The x-axis shows the values from the distribution and the y-axis shows the frequency of each value.
Notice that the x-axis ranges from 5 to 10 since these were the minimum and maximum values that we specified for the distribution.
<b>Note</b>: You can find the complete documentation for the <b>np.random.uniform()</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common operations in pandas:
 How to Create Pandas DataFrame with Random Data 
 How to Randomly Sample Rows in Pandas 
 How to Shuffle Rows in a Pandas DataFrame 
<h2><span class="orange">How to Use runif Function in R (4 Examples)</span></h2>
You can use the <b>runif()</b> function to generate random values from a  uniform distribution  in R.
This function uses the following syntax:
<b>runif(n, min=0, max=1)
</b>
where:
<b>n</b>: The number of random values to generate
<b>min</b>: The minimum value of the distribution (default is 0)
<b>max</b>: The maximum value of the distribution (default is 1)
The following examples show how to use the <b>runif()</b> function in different scenarios.
<h3>Example 1: Use runif() to Generate Random Values</h3>
The following code shows how to use the <b>runif()</b> function to generate 10 random values from a uniform distribution that ranges from 50 to 100:
<b>#make this example reproducible
set.seed(5)
#generate 10 random values from uniform distribution
runif(n=10, min=50, max=100)
[1] 60.01072 84.26093 95.84379 64.21997 55.23251 85.05287 76.39800 90.39676
[9] 97.82501 55.52265
</b>
Note that each of the 10 random values generated is between 50 and 100.
<h3>Example 2: Use runif() to Generate Random Values Rounded to Decimal Place</h3>
The following code shows how to use the <b>round()</b> function with the <b>runif()</b> function to generate 10 random values from a uniform distribution that ranges from 50 to 100, in which each value is rounded to one decimal place:
<b>#make this example reproducible
set.seed(5)
#generate 10 random values from uniform distribution rounded to one decimal place
round(runif(n=10, min=50, max=100), 1)
[1] 63.7 74.5 65.9 78.0 63.1 60.1 69.4 94.4 77.7 92.1
</b>
Note that each of the 10 random values generated is between 50 and 100 and is rounded to one decimal place.
<h3>Example 3: Use runif() to Generate Random Values Rounded to Whole Numbers</h3>
The following code shows how to use the <b>round()</b> function with the <b>runif()</b> function to generate 10 random values from a uniform distribution that ranges from 50 to 100, in which each value is rounded to a whole number:
<b>#make this example reproducible
set.seed(5)
#generate 10 random values from uniform distribution rounded to whole number
round(runif(n=10, min=50, max=100), 0)
[1] 64 75 66 78 63 60 69 94 78 92
</b>
Note that each of the 10 random values generated is between 50 and 100 and is rounded to a whole number.
<h3>Example 4: Use runif() to Create Histogram of Uniform Distribution</h3>
The following code shows how to use the <b>runif()</b> function to generate 1,000 random values from a uniform distribution that ranges from 50 to 100, and then use the <b>hist()</b> function to create a histogram that visualizes this distribution of values.
<b>#make this example reproducible
set.seed(5)
#generate 1,000 random values from uniform distribution
values &lt;- runif(n=1000, min=50, max=100)
#generate histogram to visualize these values
hist(values)</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/runif1.jpg">
The result is a histogram that displays the distribution of the 1,000 values generated from the uniform distribution.
<h2><span class="orange">How to Perform Runs Test in R</span></h2>
<b>Runs test</b> is a statistical test that is used to determine whether or not a dataset comes from a random process.
The  null and alternative hypotheses  of the test are as follows:
H<sub>0</sub> (null): The data was produced in a random manner.
H<sub>a</sub> (alternative): The data was <em>not </em>produced in a random manner.
This tutorial explains two methods you can use to perform Runs test in R. Note that both methods lead to the exam same results.
<h3>Method 1: Run’s Test Using the <em>snpar</em> Library</h3>
The first way you can perform Run’s test is with the <b>runs.test()</b> function from the <b>snpar </b>library, which uses the following syntax:
<b>runs.test(x, exact = FALSE, alternative = c(“two.sided”, “less”, “greater”))</b>
where:
<b>x: </b>A numeric vector of data values.
<b>exact: </b>Indicates whether an exact p-value should be calculated. This is FALSE by default. If the number of runs is fairly small, you can change this to TRUE.
<b>alternative: </b>Indicates the alternative hypothesis. The default is two.sided.
The following code shows how to perform Run’s test using this function in R:
<b>library(snpar)
#create dataset
data &lt;- c(12, 16, 16, 15, 14, 18, 19, 21, 13, 13)
#perform Run's test
runs.test(data)
Approximate runs rest
data:  data
Runs = 5, p-value = 0.5023
alternative hypothesis: two.sided
</b>
The p-value of the test is <b>0.5023</b>. Since this is not less than α = .05, we fail to reject the null hypothesis. We have sufficient evidence to say that the data was produced in a random manner.
<h3>Method 2: Run’s Test Using the <em>randtests</em> Library</h3>
The second way you can perform Run’s test is with the <b>runs.test()</b> function from the <b>randtests </b>library, which uses the following syntax:
<b>runs.test(x, alternative = c(“two.sided”, “less”, “greater”))</b>
where:
<b>x: </b>A numeric vector of data values.
<b>alternative: </b>Indicates the alternative hypothesis. The default is two.sided.
The following code shows how to perform Run’s test using this function in R:
<b>library(randtests)
#create dataset
data &lt;- c(12, 16, 16, 15, 14, 18, 19, 21, 13, 13)
#perform Run's test
runs.test(data)
Runs Test
data:  data
statistic = -0.67082, runs = 5, n1 = 5, n2 = 5, n = 10, p-value =
0.5023
alternative hypothesis: nonrandomness</b>
Once again the p-value of the test is <b>0.5023</b>. Since this is not less than α = .05, we fail to reject the null hypothesis. We have sufficient evidence to say that the data was produced in a random manner.
<h2><span class="orange">How to Perform Runs Test in Python</span></h2>
<b>Runs test</b> is a statistical test that is used to determine whether or not a dataset comes from a random process.
The  null and alternative hypotheses  of the test are as follows:
H<sub>0</sub> (null): The data was produced in a random manner.
H<sub>a</sub> (alternative): The data was <em>not </em>produced in a random manner.
This tutorial explains two methods you can use to perform Runs test in Python.
<h3>Example: Runs Test in Python</h3>
We can perform Runs test on a given dataset in Python by using the  runstest_1samp()  function from the <b>statsmodels</b> library, which uses the following syntax:
<b>runstest_1samp(x, cutoff=’mean’, correction=True) </b>
where:
<b>x: </b>Array of data values
<b>cutoff: </b>The cutoff to use to split the data into large and small values. Default is ‘mean’ but you can also specify ‘median’ as an alternative.
<b>correction: </b>For a sample size below 50, this function subtracts 0.5 as a correction. You can specify False to turn this correction off.
This function produces a z-test statistic and a corresponding p-value as the output.
The following code shows how to perform Run’s test using this function in Python:
<b>from statsmodels.sandbox.stats.runs import runstest_1samp 
#create dataset
data = [12, 16, 16, 15, 14, 18, 19, 21, 13, 13]
#Perform Runs test
runstest_1samp(data, correction=False)
(-0.6708203932499369, 0.5023349543605021)
</b>
The z-test statistic turns out to be <b>-0.67082 </b>and the corresponding p-value is <b>0.50233</b>. Since this p-value is not less than α = .05, we fail to reject the null hypothesis. We have sufficient evidence to say that the data was produced in a random manner.
<b>Note</b>: For this example we turned off the correction when calculating the test statistic. This matches the formula that is used to perform a  Runs test in R , which does not use a correction when performing the test.
<h2><span class="orange">How to Fix: runtimewarning: invalid value encountered in double_scalars</span></h2>
One error you may encounter in Python is:
<b>runtimewarning: invalid value encountered in double_scalars
</b>
This error occurs when you attempt to perform some mathematical operation that involves extremely small or extremely large numbers and Python simply outputs a NaN value as the result.
The following example shows how to fix this error in practice.
<h3>How to Reproduce the Error</h3>
Suppose we attempt to perform the following mathematical operation with two NumPy arrays:
<b>import numpy as np
#define two NumPy arrays
array1 = np.array([[1100, 1050]])
array2 = np.array([[1200, 4000]])
#perform complex mathematical operation
np.exp(-3*array1).sum() / np.exp(-3*array2).sum()
RuntimeWarning: invalid value encountered in double_scalars
</b>
We receive a <b>RuntimeWarning</b> because the result in the denominator is extremely close to zero.
This means the answer to the division problem will be extremely large and Python is unable to handle this large of a value.
<h3>How to Fix the Error</h3>
Typically the way to fix this type of error is to use a special function from another library in Python that is capable of handling extremely small or extremely large values in calculations.
In this case, we can use the <b>logsumexp()</b> function from the SciPy library:
<b>import numpy as np
from scipy.special import logsumexp
#define two NumPy arrays
array1 = np.array([[1100, 1050]])
array2 = np.array([[1200, 4000]])
#perform complex mathematical operation
np.exp(logsumexp(-3*array1) - logsumexp(-3*array2))
2.7071782767869983e+195
</b>
Notice that the result is extremely large but we don’t receive any error because we used a special mathematical function from the SciPy library that was designed to handle these types of numbers.
In many cases, it’s worth looking up special functions from the  SciPy library  that can handle extreme mathematical operations because these functions are designed specifically for scientific computing.
<b>Note</b>: You can find the complete online documentation for the <b>logsumexp()</b> function  here .
<h2><span class="orange">How to Fix: RuntimeWarning: overflow encountered in exp</span></h2>
One warning you may encounter in Python is:
<b>RuntimeWarning: overflow encountered in exp
</b>
This warning occurs when you use the NumPy <b>exp</b> function, but use a value that is too large for it to handle.
It’s important to note that this is simply a <b>warning</b> and that NumPy will still carry out the calculation you requested, but it provides the warning by default.
When you encounter this warning, you have two options:
<b>1.</b> Ignore it.
<b>2.</b> Suppress the warning entirely.
The following example shows how to address this warning in practice.
<h3>How to Reproduce the Warning</h3>
Suppose we perform the following calculation in Python:
<b>import numpy as np
#perform some calculation
print(1/(1+np.exp(1140)))
0.0
/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:3:
RuntimeWarning: overflow encountered in exp
</b>
Notice that NumPy performs the calculation (the result is 0.0) but it still prints the <b>RuntimeWarning</b>.
This warning is printed because the value np.exp(1140) represents e<sup>1140</sup>, which is a <em>massive</em> number.
We basically requested NumPy to perform the following calculation:
1 / (1 + massive number)
This can be reduced to:
1 / massive number
This is effectively 0, which is why NumPy calculated the result to be <b>0.0</b>.
<h3>How to Suppress the Warning</h3>
If we’d like, we can use the <b>warnings</b> package to suppress warnings as follows:
<b>import numpy as np
import warnings
#suppress warnings
warnings.filterwarnings('ignore')
#perform some calculation
print(1/(1+np.exp(1140)))
0.0</b>
Notice that NumPy performs the calculation and does not display a RuntimeWarning.
<b>Note</b>: In general, warnings can be helpful for identifying bits of code that take a long time to run so be highly selective when deciding to suppress warnings.
<h2><span class="orange">When to Use s / sqrt(n) in Statistics</span></h2>
In statistics, you will encounter the formula<b> s/√n </b>in different scenarios.
This formula is used to calculate the standard error of a sample mean.
In the formula, <b>s</b> represents the sample standard deviation and <b>n</b> represents the sample size.
This formula appears in the calculation of two statistical tests:
<b>1.</b> One sample t-test
<b>2.</b> Confidence interval for a population mean
The following examples show how to use <b>s/√n</b> in both of these scenarios.
<h2>Example 1: Using s / sqrt(n) in a One Sample t-test</h2>
A <b>one sample t-test</b> is used to test whether or not the mean of a  population  is equal to some value.
We use the following formula to calculate the test statistic t:
<b>t = (x – μ) / (s/√n)</b>
where:
<b>x: </b>sample mean
<b>μ<sub>0</sub>:</b> hypothesized population mean
<b>s: </b>sample standard deviation
<b>n: </b>sample size
For example, suppose we want to test whether or not the mean weight of turtles in some population is equal to 300 pounds.
We collect a  simple random sample  of turtles with the following information:
Sample size n = 40
Sample mean weight x = 300
Sample standard deviation s = 18.5
We will perform the one sample t-test with the following hypotheses:
<b>H<sub>0</sub>: </b>μ = 310 (population mean is equal to 310 pounds)
<b>H<sub>A</sub>: </b>μ ≠ 310 (population mean is not equal to 310 pounds)
First, we’ll calculate the test statistic:
t = (x – μ) / (s/√n) = (300-310) / (18.5/√40) = -3.4187
According to the  T Score to P Value Calculator , the p-value associated with t = -3.4817 and degrees of freedom = n-1 = 40-1 = 39 is 0.00149.
Since this p-value is less than 0.05, we reject the null hypothesis. We have sufficient evidence to say that the mean weight of this species of turtle is not equal to 310 pounds.
<h2>Example 2: Using s / sqrt(n) in a Confidence Interval for a Population Mean</h2>
A <b>confidence interval for a population mean</b> is a range of values that is likely to contain a population mean with a certain level of confidence.
We use the following formula to calculate a confidence interval for a mean:
<b>Confidence Interval = x  +/-  t<sub>n-1, 1-α/2</sub>*(s/√n)</b>
where:
<b>x: </b>sample mean
<b>t: </b>the t-critical value
<b>s: </b>sample standard deviation
<b>n: </b>sample size
For example, suppose we want to calculate a confidence interval for the true mean weight of turtles in a certain population.
We collect a simple random sample of turtles with the following information:
Sample size n = 40
Sample mean weight x = 300
Sample standard deviation s = 18.5
We can use the following formula to calculate a 95% confidence interval for the true population mean weight of turtles:
95% C.I. = x +/-  t<sub>n-1, 1-α/2</sub>*(s/√n)
95% C.I. = 300 +/- (2.022691) * (18.5/√40)
95% C.I. = [294.083, 305.917]
The 95% confidence interval for the true population mean weight of turtles is between 294.083 pounds and 305.917 pounds.
<h2>Additional Resources</h2>
The following tutorials explain how to calculate a standard error of a mean in different software:
 How to Calculate the Standard Error of the Mean in Excel 
 How to Calculate Standard Error of the Mean in R 
 How to Calculate the Standard Error of the Mean in Python 
<h2><span class="orange">How to Create a Sales Forecast in Excel (Step-by-Step)</span></h2>
The following step-by-step example shows how to create a sales forecast in Excel.
<h3>Step 1: Create the Data</h3>
First, let’s create a dataset that shows the total sales made by some company during 18 consecutive months:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast1.jpg"443">
<b>Note</b>: In order to use the forecast functionality in the next step, make sure that each of your dates are at evenly spaced intervals. For example, the dates in the dataset above are each one month apart.
<h3>Step 2: Create a Forecast</h3>
Next, highlight the cells in the range <b>A1:B19</b> and then click the <b>Data </b>tab along the top ribbon and click <b>Forecast Sheet</b> within the <b>Forecast</b> group:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast2.jpg"637">
In the new window that appears, click the <b>Options</b> dropdown arrow near the bottom to specify the <b>Forecast Start</b> and <b>Forecast End</b> dates, then click <b>Create</b> to automatically create a forecast:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast3.jpg"659">
A forecast will automatically be generated for the future dates that you specified, along with 95% confidence interval limits:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast4.jpg"651">
Here’s how to interpret the output:
The forecasted sales for <b>7/1/2021</b> are 172.518 and the 95% confidence interval for this forecast is [159.9, 185.14].
The forecasted sales for <b>8/1/2021</b> are 179.12 and the 95% confidence interval for this forecast is [162.14, 196.11].
And so on.
A line chart with the forecasted values is also produced automatically:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast5.jpg">
Here’s how to interpret the lines:
The dark blue line represents the historical sales values.
The dark orange line represents the forecasted sales values.
The light orange lines represent the 95% confidence limits for the forecasted sales values.
<h3>Step 3: Modify the Forecast (Optional)</h3>
When creating the forecast, you can choose to display the results in a bar graph instead of a line graph by clicking the bars icon in the top right corner:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/02/forecast6.jpg"616">
The blue bars represent the historical sales values and the orange bars represent the forecasted sales values.
Note that the confidence interval for each sales forecast will be represented with vertical error bars along the top of each of the orange bars.
<h2><span class="orange">Sample Mean vs. Population Mean: What’s the Difference?</span></h2>
Often in statistics we’re interested in answering questions like:
What is the mean household income in a certain city?
What is the mean weight of a certain species of turtle?
What is the mean attendance at college football games?
In each scenario, we are interested in answering some question about a  population , which represents every possible individual element that we’re interested in measuring.
However, instead of collecting data on every individual in a population we instead collect data on a sample of the population, which represents a portion of the total population.
For example, we might want to know the mean weight of a certain species of turtle that has a total population of 800 turtles.
Since it would take too long to locate and weigh every single turtle in the population, we instead collect a simple random sample of 30 turtles and measure their weights:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample2.png">
We could then use the mean weight of this sample of turtles to estimate the mean weight of all turtles in the population.
<h3>How to Calculate the Sample Mean</h3>
The formula to calculate the sample mean, often denoted x, is as follows:
<b>x = Σx<sub>i</sub> / n</b>
where:
<b>Σ:</b> A fancy Greek symbol that means “sum”
<b>x<sub>i</sub>:</b> The value of the ith observation in the dataset
<b>n:</b> The sample size
For example, suppose we collect a sample of 10 turtles with the following weights (in pounds):
70, 80, 80, 85, 90, 95, 110, 120, 140, 150
The sample mean would be calculated as:
<b>x</b> = (70+ 80+80+85+90+95+110+120+140+150) / 10 = <b>102</b>
<h3>Why the Sample Mean is Unbiased</h3>
In statistical jargon, we would say that the sample mean is a <b>statistic</b> while the population mean is a <b>parameter</b>.
Here’s the difference between the two terms:
A <b>statistic</b> is a number that describes some characteristic of a sample.
A <b>parameter</b> is a number that describes some characteristic of a population.
The parameter is the value that we’re actually interested in measuring, but the statistic is the value that we use to estimate the value of the parameter since the statistic is so much easier to obtain.
When we use a method like  simple random sampling  to obtain a sample, we say that the sample mean is an <b>unbiased estimator</b> of the population mean.
In other words, we have no reason to believe that the sample mean would underestimate or overestimate the true population mean.
The reason is because when we use a method like simple random sampling, every member in the population has an equal chance of being included in the sample, which means the sample is likely to be a “mini version” of the overall population.
We would say that the sample is  representative of the overall population , which means that the sample mean should be a good estimate of the population mean, assuming that the sample size is large enough.
<h3>On Using Confidence Intervals with the Sample Mean</h3>
Although the sample mean provides an unbiased estimate of the population mean, it’s unlikely that the sample mean will <em>exactly</em> match the population mean.
For example, if we want to use a sample of turtles to estimate the mean weight of a population of turtles, it’s possible that we might just happen to pick a sample full of low-weight turtles or perhaps a sample full of heavy turtles.
In order to capture this uncertainty around our estimate of the population mean, we can create a  confidence interval .
A confidence interval is a range of values that is likely to contain a population parameter with a certain level of confidence.
For example, we might collect a sample of 30 turtles and find that the mean weight of this sample is 102 pounds. If we then construct a 95% confidence interval, we might find that the interval is as follows:
<b>95% confidence interval = [98.5, 105.5]</b>
We would interpret this to mean there is a 95% chance that the confidence interval of [98.5, 105.5] contains the true population mean weight of turtles.
This confidence interval is more useful than just the sample mean because it gives us a <em>range</em> of values that the true population mean is likely to fall in.
<h2><span class="orange">How to Calculate Sample & Population Variance in Excel</span></h2>
The <b>variance </b>is a way to measure  the spread  of values in a dataset.
The formula to calculate <b>population variance</b> is:
<b>σ<sup>2</sup></b> = Σ (x<sub>i</sub> – μ)<sup>2</sup> / N
where:
<b>Σ</b>: A symbol that means “sum”
<b>μ</b>: Population mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the population
<b>N</b>: Population size
The formula to calculate <b>sample variance</b> is:
<b>s<sup>2</sup></b> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>x</b>: Sample mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the sample
<b>n</b>: Sample size
We can use the <b>VAR.S()</b> and <b>VAR.P() </b>formulas in Excel to quickly calculate the sample variance and population variance (respectively) for a given dataset.<b> </b>
The following examples show how to use each function in practice.
<h3>Example 1: Calculating Sample Variance in Excel</h3>
The following screenshot shows how to use the <b>VAR.S()</b> function to calculate the sample variance of the values in column A:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/varExcel2.png">
The sample variance turns out to be <b>35.2079</b>.
<h3>Example 2: Calculating Population Variance in Excel</h3>
The following screenshot shows how to use the <b>VAR.P()</b> function to calculate the population variance of the values in column A:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/varExcel3.png">
The population variance turns out to be <b>33.4475</b>.
<h3>Notes on Calculating Sample & Population Variance</h3>
Keep in mind the following when calculating the sample and population variance:
You should calculate the <b>population variance</b> when the dataset you’re working with represents an entire population, i.e. every value that you’re interested in.
You should calculate the <b>sample variance</b> when the dataset you’re working with represents a a sample taken from a larger population of interest.
The sample variance of a dataset will always be larger than the population variance for the same dataset because there is more uncertainty when calculating the sample variance, thus our estimate of the variance will be larger.
<h2><span class="orange">How to Calculate Sample & Population Variance in Python</span></h2>
The <b>variance </b>is a way to measure  the spread  of values in a dataset.
The formula to calculate <b>population variance</b> is:
<b>σ<sup>2</sup></b> = Σ (x<sub>i</sub> – μ)<sup>2</sup> / N
where:
<b>Σ</b>: A symbol that means “sum”
<b>μ</b>: Population mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the population
<b>N</b>: Population size
The formula to calculate <b>sample variance</b> is:
<b>s<sup>2</sup></b> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>x</b>: Sample mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the sample
<b>n</b>: Sample size
We can use the <b>variance</b> and <b>pvariance</b> functions from the  statistics  library in Python to quickly calculate the sample variance and population variance (respectively) for a given array.
<b>from statistics import variance, pvariance
#calculate sample variance
variance(x)
#calculate population variance
pvariance(x)
</b>
The following examples show how to use each function in practice.
<h3>Example 1: Calculating Sample Variance in Python</h3>
The following code shows how to calculate the sample variance of an array in Python:
<b>from statistics import variance 
#define data
data = [4, 8, 12, 15, 9, 6, 14, 18, 12, 9, 16, 17, 17, 20, 14]
#calculate sample variance
variance(data)
22.067
</b>
The sample variance turns out to be <b>22.067</b>.
<h3>Example 2: Calculating Population Variance in Python</h3>
The following code shows how to calculate the population variance of an array in Python:
<b>from statistics import pvariance 
#define data
data = [4, 8, 12, 15, 9, 6, 14, 18, 12, 9, 16, 17, 17, 20, 14]
#calculate sample variance
pvariance(data)
20.596</b>
The population variance turns out to be <b>20.596</b>.
<h3>Notes on Calculating Sample & Population Variance</h3>
Keep in mind the following when calculating the sample and population variance:
You should calculate the <b>population variance</b> when the dataset you’re working with represents an entire population, i.e. every value that you’re interested in.
You should calculate the <b>sample variance</b> when the dataset you’re working with represents a a sample taken from a larger population of interest.
The sample variance of a given array of data will always be larger than the population variance for the same array of a data because there is more uncertainty when calculating the sample variance, thus our estimate of the variance will be larger.
<h2><span class="orange">Sample Proportion vs. Sample Mean: The Difference</span></h2>
Two terms that are often used in statistics are <b>sample proportion</b> and <b>sample mean</b>.
Here’s the difference between the two terms:
<b>Sample proportion:</b> The proportion of  observations  in a sample with a certain characteristic.
Often denoted p<U+0302>, It is calculated as follows:
<b>p<U+0302> = x / n</b>
where:
<b>x:</b> The number of observations in the  sample  with a certain characteristic.
<b>n:</b> The total number of observations in the sample.
<b>Sample mean:</b> The average value in a sample.
Often denoted x, it is calculated as follows:
<b>x = Σx<sub>i</sub> / n</b>
where:
<b>Σ:</b> A symbol that means “sum”
<b>x<sub>i</sub>:</b> The value of the i<sup>th</sup> observation in the sample
<b>n:</b> The sample size
<h3>Sample Proportion vs. Sample Mean: When to Use Each</h3>
The sample proportion and sample mean are used for different reasons:
<b>Sample proportion:</b> Used to understand the proportion of observations in a sample that have a certain characteristic.
For example, we could use the sample proportion in each of the following scenarios:
<b>Politics:</b> Researchers might survey 500 individuals in a certain city to understand what proportion of residents support a certain candidate in an upcoming election.
<b>Biology:</b> Biologists may collect data on 100 sea turtles to understand what proportion of them have experienced damage from pollution.
<b>Sports:</b> A journalist may survey 1,000 college basketball players to understand what proportion of them shoot left-handed.
<b>Sample mean:</b> Used to understand the average value in a sample.
For example, we could use the sample mean in each of the following scenarios:
<b>Demographics:</b> Economists may collect data on 5,000 households in a certain city to estimate the average annual household income.
<b>Botany:</b> A botanist may take measurements on 50 plants from the same species to estimate the average height of the plant in inches.
<b>Nutrition:</b> A nutritionist may survey 100 people at a hospital to estimate the average number of calories that residents eat per day.
Depending on the question of interest, it might make more sense to use the sample proportion or the sample mean to answer the question.
<h3>Using the Sample Proportion & Sample Mean to Estimate Population Parameters</h3>
Both the sample proportion and the sample mean are used to estimate  population parameters .
<b>Sample Proportion as an Estimate</b>
We use the sample proportion to estimate a population proportion. For example, we might be interested in understanding what proportion of residents in a certain city support a new law.
Since it would be too costly and time-consuming to survey all 20,000 residents in the city, we instead survey 500 and calculate the proportion of residents in the sample who support the new law.
We then use this sample proportion as our best estimate of the proportion of residents in the entire city who suppose the new law. However, since it’s unlikely that our sample proportion exactly matches the population proportion, we often use a  confidence interval for a proportion  – a range of values that we believe contains the true population proportion with a certain level of confidence.
<b>Sample Mean as an Estimate</b>
We use the sample mean to estimate a population mean. For example, we might be interested in understanding the average height of a certain species of plants.
Since it would be too costly and time-consuming to measure the height of all 10,000 plants in a certain region, we instead measure the height of 150 plants and use the sample mean as our best estimate of the population mean.
However, since it’s unlikely that our sample mean exactly matches the population mean, we often use a  confidence interval for a mean – a range of values that we believe contains the true population mean with a certain level of confidence.
<h2><span class="orange">The Relationship Between Sample Size and Margin of Error</span></h2>
Often in statistics we’re interested in estimating the value of some  population parameter  such as a  population proportion  or a  population mean .
To estimate these values, we typically gather a  simple random sample  and calculate the sample proportion or the sample mean.
We then construct a  confidence interval  to capture our uncertainty around these estimates.
We use the following formula to calculate a confidence interval for a population proportion:
<b>Confidence Interval = p  ±  z*√p(1-p) / n</b>
where:
<b>p: </b>sample proportion
<b>z: </b>the chosen z-value
<b>n: </b>sample size
And we use the following formula to calculate a confidence interval for a population mean:
<b>Confidence Interval = x<U+0304>  ± z*(s/√n)</b>
where:
<b>x<U+0304>: </b>sample mean
<b>z: </b>the chosen z-value
<b>s</b>: sample standard deviation
<b>n: </b>sample size
<b>In both formulas, there is an inverse relationship between the sample size and the margin of error.</b>
The larger the sample size, the smaller the margin of error. Conversely, the smaller the sample size, the larger the margin of error.
Check out the following two examples to gain a better understanding of this.
<h3>Example 1: Sample Size and Margin of Error for a Population Proportion</h3>
We use the following formula to calculate a confidence interval for a population proportion:
<b>Confidence Interval = p  ±  z*√p(1-p) / n</b>
The portion in red is known as the <b>margin of error</b>:
<b>Confidence Interval = p  ±  z*√p(1-p) / n</b>
Notice that within the margin of error, we divide by n (the sample size).
Thus, when the sample size is large we divide by a large number, which makes the entire margin of error smaller. This leads to a narrower confidence interval.
For example, suppose we collect a simple random sample of data with the following information:
<b>p: </b>0.6
<b>n: </b>25
Here’s how to calculate a 95% confidence interval for the population proportion:
Confidence Interval = p  ±  z*√p(1-p) / n
Confidence Interval = .6 ±  1.96*√.6(1-.6) / 25
Confidence Interval = .6 ± 0.192
Confidence Interval = <b>[.408, .792]</b>
Now consider if we instead used a sample size of 200. Here’s how we would calculate the 95% confidence interval for the population proportion:
Confidence Interval = p  ±  z*√p(1-p) / n
Confidence Interval = .6 ±  1.96*√.6(1-.6) / 200
Confidence Interval = .6 ± 0.068
Confidence Interval = <b>[.532, .668]</b>
Notice that just by increasing the sample size we were able to reduce the margin of error and produce a much more narrow confidence interval.
<h3>Example 2: Sample Size and Margin of Error for a Population Mean</h3>
We use the following formula to calculate a confidence interval for a population mean:
<b>Confidence Interval = x<U+0304>  ± z*(s/√n)</b>
The portion in red is known as the <b>margin of error</b>:
<b>Confidence Interval = x<U+0304>  ± z*(s/√n)</b>
Notice that within the margin of error, we divide by n (the sample size).
Thus, when the sample size is large we divide by a large number, which makes the entire margin of error smaller. This leads to a narrower confidence interval.
For example, suppose we collect a simple random sample of data with the following information:
<b>x<U+0304>: </b>15
<b>s</b>: 4
<b>n: </b>25
Here’s how to calculate a 95% confidence interval for the population mean:
Confidence Interval = x<U+0304>  ± z*(s/√n)
Confidence Interval = 15 ±  1.96*(4/√25)
Confidence Interval = 15 ± 1.568
Confidence Interval = <b>[13.432, 16.568]</b>
Now consider if we instead used a sample size of 200. Here’s how we would calculate the 95% confidence interval for the population mean:
Confidence Interval = x<U+0304>  ± z*(s/√n)
Confidence Interval = 15 ±  1.96*(4/√200)
Confidence Interval = 15 ± 0.554
Confidence Interval = <b>[14.446, 15.554]</b>
Notice that just by increasing the sample size we were able to reduce the margin of error and produce a more narrow confidence interval.
 An Introduction to Confidence Intervals for a Proportion 
 Confidence Interval for Proportion Calculator 
The following tutorials provide additional information about confidence intervals for a mean:
 An Introduction to Confidence Intervals for a Mean 
 Confidence Interval for Mean Calculator 
<h2><span class="orange">Sample Size Calculator for a Mean</span></h2>
The sample size required to estimate a population mean with a certain level of confidence and a desired margin of error is calculated as:
Sample size =(z<sub>α/2</sub>σ/E)<sup>2</sup>
where:
z<sub>α/2</sub>: The z critical value 
E: The desired margin of error
σ: The population standard deviation
To find the sample size required to estimate a population mean, simply fill in the boxes below and then click the “Calculate” button.
<label for="z"><b>Confidence Level</b></label>
<input type="number" id="z" value="0.95">
<label for="E"><b>Desired Margin of Error</b></label>
<input type="number" id="E" value="4">
<label for="s"><b>Population standard deviation</b></label>
<input type="number" id="s" value="12">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Sample Size: </b> 35
<script>
function calc() {
//get input values
var z  = document.getElementById('z').value*1;
var s  = document.getElementById('s').value*1;
var E  = document.getElementById('E').value*1;
//find number of bins
var n = Math.ceil(Math.pow((Math.abs(jStat.normal.inv((1-z)/2, 0, 1))*s/E), 2));
//output
document.getElementById('n').innerHTML = n;
}
</script>
<h2><span class="orange">Sample Size Calculator for a Proportion</span></h2>
The sample size required to estimate a population proportion with a certain level of confidence and a desired margin of error is calculated as:
Sample size = p*(1-p)*(z<sub>α/2</sub>/E)<sup>2</sup>
where:
p: The expected proportion. If you’re unsure, leave this as 0.5.
z<sub>α/2</sub>: The z critical value 
E: The desired margin of error
To find the sample size required to estimate a population proportion, simply fill in the boxes below and then click the “Calculate” button.
<label for="z"><b>Confidence Level</b></label>
<input type="number" id="z" value="0.95">
<label for="E"><b>Desired Margin of Error</b></label>
<input type="number" id="E" value="0.03">
<label for="p"><b>Expected Proportion (leave as 0.5 if unsure)</b></label>
<input type="number" id="p" value="0.5">
<input type="button" id="button_calc" onclick="calc()" value="Calculate">
<b>Sample Size: </b> 1068
<script>
function calc() {
//get input values
var z  = document.getElementById('z').value*1;
var p  = document.getElementById('p').value*1;
var E  = document.getElementById('E').value*1;
//find number of bins
var n = Math.ceil(p*(1-p)*Math.pow((Math.abs(jStat.normal.inv((1-z)/2, 0, 1))/E), 2));
//output
document.getElementById('n').innerHTML = n;
}
</script>
<h2><span class="orange">What is a Sample Space? Definition & Examples</span></h2>
The <b>sample space</b> of an experiment is the set of all possible outcomes of the experiment.
For example, suppose we roll a dice one time. The sample space of possible outcomes includes:
Sample space = 1, 2, 3, 4, 5, 6
Using notation, we write the symbol for sample space as a cursive S and the outcomes in brackets as follows:
<b><em>S</em> = {1, 2, 3, 4, 5, 6}</b>
<h3>Examples of Sample Spaces</h3>
Here are a few more examples of sample spaces:
<b>Example 1: Coin Toss</b>
Suppose we toss a coin one time. If we let <em>H</em> = the coin lands on heads and <em>T</em> = the coin lands on tails, then the sample space for this coin toss is:
<em>S</em> = {H, T}
<b>Example 2: Marbles in a Bag</b>
Suppose we randomly select one marble from a bag that contains three marbles: a red marble, a green marble, and a blue marble. If we let <em>R</em> = red, <em>G</em> = green, and <em>B</em> = blue, then the sample space is:
<em>S</em> = {R, G, B}
<b>Example 3: Coin Toss & Dice Roll</b>
Suppose we toss a coin and roll a dice at the same time. If we let <em>H1</em> represent the result of a “Head” and a “1” then the sample space for the results is:
<em>S</em> = {H1, H2, H3, H4, H5, H6, T1, T2, T3, T4, T5, T6}
<h3>The Fundamental Counting Principle</h3>
The <b>fundamental counting principle</b> is a way to calculate the total number of potential outcomes of an experiment.
This principle states that if event A has <em>n</em> distinct outcomes and event B has <em>m</em> distinct outcomes, then the total number of potential outcomes can be calculated as:
Total outcomes = <em>m</em> * <em>n</em>
<b>Example 1: Coin Toss & Dice Roll</b>
For example, if we toss a coin and roll a dice at the same time, then the total number of outcomes in the sample space can be calculated as:
Total outcomes = (2 ways a coin can land) * (6 ways a dice can land) = <b>12</b> possible outcomes.
We wrote out these 12 outcomes in the previous example:
<em>S</em> = {H1, H2, H3, H4, H5, H6, T1, T2, T3, T4, T5, T6}
<b>Example 2: Counting Outfit Combinations</b>
This principle can also be used to calculate the total outcomes in a sample space for more than two events.
For example, suppose a random drawer contains 3 different shirts, 4 different pants, and 2 different socks. If we randomly select one article of clothing each without looking, the total number of possible outfits would be calculated as:
Total outfits = 3 * 4 * 2 = <b>24</b> possible outfits
<h3>Visualizing Sample Spaces with Tree Diagrams</h3>
When the number of outcomes in a sample space is large, it can be helpful to construct a <b>tree diagram</b> to visualize the different combinations of outcomes.
For example, suppose a closet contains 2 different shirts, 2 different pants, and 2 different socks. If we randomly select one article of clothing each without looking, the total number of possible outfits could be visualized as:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/sampleSpace1.png">
This diagram helps us visualize the eight different potential outcomes in the sample space.
We can also use the fundamental counting principle to confirm that there should be eight different outcomes:
Total outcomes = 2 shirts * 2 pants * 2 socks = 8 possible outfits
<h3>Calculating Probabilities of Outcomes in Sample Spaces</h3>
Once we have identified the sample space of some experiment, we can calculate the probability of some event <em>A</em> occurring by using the following formula:
P(A) = (Sample Space of A) / (Total Sample Space)
For example, suppose we roll a dice one time. The sample space can be written as:
<em>S</em> = {1, 2, 3, 4, 5, 6}
If we define event A as the dice landing on the number “2”, then the sample space of event A can be written as:
<em>S</em> = {2}
Thus, the probability of event A occurring can be calculated as:
P(A) = 1/6
If we define event A as the dice landing on an even number, then the sample space of event A can be written as:
<em>S</em> = {2, 4, 6}
Thus, the probability of event A occurring can be calculated as:
P(A) = 3/6
<h2><span class="orange">How to Find Sample Variance on a TI-84 Calculator</span></h2>
The <b>sample variance</b> tells us how spread out the values are in a given  sample .
Typically denoted as s<sup>2</sup>, it is calculated as:
s<sup>2</sup> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b><span>x</b>: sample mean
<b>x<sub>i</sub></b>: the i<sup>th</sup> value in the sample
<b> n</b>: the sample size
The following step-by-step example shows how to calculate the sample variance for the following sample:
<b>Sample:</b> 2, 4, 4, 7, 8, 12, 14, 15, 19, 22
<h3>Step 1: Enter the Data</h3>
First, we will enter the data values.
Press Stat, then press EDIT. Then enter the values of the sample in column L1:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sampleVar1.png">
<h3>Step 2: Find the Sample Variance</h3>
Next, press Stat and then scroll over to the right and press CALC.
Then press 1-Var Stats.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum2.png">
In the new screen that appears, press Enter.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sum3.png">
Once you press Enter, a list of summary statistics will appear.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/samplevar1-1.png">
The sample standard deviation is Sx = <b>6.783149056</b>.
To find the sample variance, we need to square this value. To do so, press VARS and then press 5:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sampleVar2.png">
In the new window that appears, press 3 to select the sample standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sampleVar3.png">
Lastly, press the x<sup>2</sup> button to square the sample standard deviation:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/04/sampleVar4.png">
The sample variance turns out to be <b>46.0111</b>.
<h2><span class="orange">Sample Variance vs. Population Variance: What’s the Difference?</span></h2>
The <b>variance</b> is a way to measure  the spread  of values in a dataset.
The formula to calculate <b>population variance</b> is:
<b>σ<sup>2</sup></b> = Σ (x<sub>i</sub> – μ)<sup>2</sup> / N
where:
<b>Σ</b>: A symbol that means “sum”
<b>μ</b>: Population mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the population
<b>N</b>: Population size
The formula to calculate <b>sample variance</b> is:
<b>s<sup>2</sup></b> = Σ (x<sub>i</sub> – x)<sup>2</sup> / (n-1)
where:
<b>x</b>: Sample mean
<b>x<sub>i</sub></b>: The i<sup>th</sup> element from the sample
<b>n</b>: Sample size
Notice that there’s only one tiny difference between the two formulas:
When we calculate population variance, we divide by <b>N</b> (the population size).
When we calculate sample variance, we divide by <b>n-1</b> (the sample size – 1).
When calculating the sample variance, we apply something known as  Bessel’s correction  – which is the act of dividing by n-1.
Without getting bogged down in the mathematical details, dividing by n-1 can be shown to provide an unbiased estimate of the population variance, which is the value we’re usually interested in anyway.
<h3>When to Calculate Sample Variance vs. Population Variance</h3>
If you’re unsure of whether you should calculate the sample variance or the population variance, keep this rule of thumb in mind:
You should calculate the <b>sample variance</b> when the dataset you’re working with represents a a sample taken from a larger population of interest.
You should calculate the <b>population variance</b> when the dataset you’re working with represents an entire population, i.e. every value that you’re interested in.
The following examples show different scenarios of when to calculate the sample variance vs. the population variance.
<h3>Example: Calculating Sample Variance</h3>
Suppose a botanist wants to calculate the variance in height of a certain species of plants. Because there are thousands of individual plants in one region, she decides to take a simple random sample of 20 plants and measure each of their heights.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/popsamp1.png">
In this scenario, the botanist should calculate the <b>sample variance</b> because she is interested in the variance of the entire population of plants but is simply using this sample to estimate the true population variance.
<h3>Example: Calculating Population Variance</h3>
Suppose a teacher wants to calculate the variance of exam scores for the 20 students in her class.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/09/varsamp.png">
In this scenario, the teacher should calculate the <b>population variance</b> because the dataset she’s working with (the 20 exam scores) represent the entire population that she is interested in.
<h2><span class="orange">Sampling Distribution Calculator</span></h2>
A <b> sampling distribution </b> is a probability distribution of a certain statistic based on many random samples from a single population.
This calculator finds the probability of obtaining a certain value for a sample mean, based on a population mean, population standard deviation, and sample size.
Simply enter the appropriate values for a given distribution below and then click the “Calculate” button.
<label for="n"><b>μ</b> (population mean)</label>
<input type="number" id="mean" value="5.3">
<label for="X"><b>σ</b> (population standard deviation)</label>
<input type="number" id="sd" value="9">
<label for="p"><b>n</b> (sample size)</label>
<input type="number" id="n" min="0" value="20">
<label for="X"><b>X</b> (random variable)</label>
<input type="number" id="X" min="0" value="6">
<input type="button" id="button" onclick="binomialCalc()" value="Calculate">
<div>
P(x ≤ 6): 0.63602
<div>
P(x ≥ 6): 0.36398
<script>
function binomialCalc() {
//get input values
var mean = document.getElementById('mean').value;
var sd = document.getElementById('sd').value;
var n = document.getElementById('n').value;
var X = document.getElementById('X').value;
var sd2 = sd/Math.sqrt(n);
//assign probabilities to variable names
var z_left = jStat.normal.cdf(X, mean, sd2);
var z_right = 1-z_left;
//output probabilities
document.getElementById('lessEqualProb').innerHTML = z_left.toFixed(5);
document.getElementById('greaterEqualProb').innerHTML = z_right.toFixed(5);
document.getElementById('left').innerHTML = X;
document.getElementById('right').innerHTML = X;
}
</script>
<h2><span class="orange">How to Calculate Sampling Distributions in Excel</span></h2>
A  sampling distribution  is a probability distribution of a certain  statistic  based on many random samples from a single  population .
This tutorial explains how to do the following with sampling distributions in Excel:
Generate a sampling distribution.
Visualize the sampling distribution.
Calculate the mean and standard deviation of the sampling distribution.
Calculate probabilities regarding the sampling distribution.
<h3>Generate a Sampling Distribution in Excel</h3>
Suppose we would like to generate a sampling distribution composed of <b>1,000</b> samples in which each sample size is <b>20</b> and comes from a normal distribution with a mean of <b>5.3</b> and a standard deviation of <b>9</b>.
We can easily do this by typing the following formula in cell A2 of our worksheet:
<b>=NORM.INV(RAND(), 5.3, 9)</b>
We can then hover over the bottom right corner of the cell until a tiny <b>+</b> appears and drag the formula to the right 20 cells and down 1,000 cells:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMeanExcel4.png">
Each row represents a sample of size 20 in which each value comes from a normal distribution with a mean of 5.3 and a standard deviation of 9.
<h3>Find the Mean & Standard Deviation</h3>
To find the mean and standard deviation of this sampling distribution of sample means, we can first find the mean of each sample by typing the following formula in cell U2 of our worksheet:
<b>=AVERAGE(A2:T2)</b>
We can then hover over the bottom right corner of the cell until a tiny <b>+</b> appears and double click to copy this formula to every other cell in column U:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMeanExcel5.png">
We can see that the first sample had a mean of 7.563684, the second sample had a mean of 10.97299, and so on.
We can then use the following formulas to calculate the mean and the standard deviation of the sample means:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMeanExcel6.png">
Theoretically the mean of the sampling distribution should be 5.3. We can see that the actual sampling mean in this example is <b>5.367869</b>, which is close to 5.3.
And theoretically the standard deviation of the sampling distribution should be equal to s/√n, which would be 9 / √20 = 2.012. We can see that the actual standard deviation of the sampling distribution is <b>2.075396</b>, which is close to 2.012.
<h3>Visualize the Sampling Distribution</h3>
We can also create a simple histogram to visualize the sampling distribution of sample means.
To do so, simply highlight all of the sample means in column U, click the <b>Insert</b> tab, then click the <b>Histogram</b> option under the <b>Charts</b> section.
This results in the following histogram:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMeanExcel3.png">
We can see that the sampling distribution is bell-shaped with a peak near the value 5.
From the tails of the distribution, however, we can see that some samples had means greater than 10 and some had means less than 0.
<h3>Calculate Probabilities</h3>
We can also calculate the probability of obtaining a certain value for a sample mean, based on a population mean, population standard deviation, and sample size.
For example, we can use the following formula to find the probability that the sample mean is less than or equal to 6, given that the population mean is 5.3, the population standard deviation is 9, and the sample size is:
<b>=COUNTIF(U2:U1001, "&lt;=6")/COUNT(U2:U1001)
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMeanExcel7.png">
We can see that the probability that the sample mean is less than or equal to 6 is <b>0.638.</b>
This is very close to the probability calculated by the  Sampling Distribution Calculator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMean2.png">
<h2><span class="orange">How to Calculate Sampling Distributions in R</span></h2>
A  sampling distribution  is a probability distribution of a certain  statistic  based on many random samples from a single population.
This tutorial explains how to do the following with sampling distributions in R:
Generate a sampling distribution.
Visualize the sampling distribution.
Calculate the mean and standard deviation of the sampling distribution.
Calculate probabilities regarding the sampling distribution.
<h3>Generate a Sampling Distribution in R</h3>
The following code shows how to generate a sampling distribution in R:
<b>#make this example reproducible
set.seed(0)
#define number of samples
n = 10000
#create empty vector of length n
sample_means = rep(NA, n)
#fill empty vector with means
for(i in 1:n){
  sample_means[i] = mean(rnorm(20, mean=5.3, sd=9))
}
#view first six sample means
head(sample_means)
[1] 5.283992 6.304845 4.259583 3.915274 7.756386 4.532656
</b>
In this example we used the  rnorm()  function to calculate the mean of 10,000 samples in which each sample size was 20 and was generated from a normal distribution with a mean of 5.3 and standard deviation of 9.
We can see that the first sample had a mean of 5.283992, the second sample had a mean of 6.304845, and so on.
<h3>Visualize the Sampling Distribution</h3>
The following code shows how to create a simple histogram to visualize the sampling distribution:
<b>#create histogram to visualize the sampling distribution
hist(sample_means, main = "", xlab = "Sample Means", col = "steelblue")
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMean1.png">
We can see that the sampling distribution is bell-shaped with a peak near the value 5.
From the tails of the distribution, however, we can see that some samples had means greater than 10 and some had means less than 0.
<h3>Find the Mean & Standard Deviation</h3>
The following code shows how to calculate the mean and standard deviation of the sampling distribution:
<b>#mean of sampling distribution
mean(sample_means)
[1] 5.287195
#standard deviation of sampling distribution
sd(sample_means)
[1] 2.00224</b>
Theoretically the mean of the sampling distribution should be 5.3. We can see that the actual sampling mean in this example is <b>5.287195</b>, which is close to 5.3.
And theoretically the standard deviation of the sampling distribution should be equal to s/√n, which would be 9 / √20 = 2.012. We can see that the actual standard deviation of the sampling distribution is <b>2.00224</b>, which is close to 2.012.
<h3>Calculate Probabilities</h3>
The following code shows how to calculate the probability of obtaining a certain value for a sample mean, based on a population mean, population standard deviation, and sample size.
<b>#calculate probability that sample mean is less than or equal to 6
sum(sample_means &lt;= 6) / length(sample_means)
</b>
In this particular example, we find the probability that the sample mean is less than or equal to 6, given that the population mean is 5.3, the population standard deviation is 9, and the sample size is 20 is <b>0.6417</b>.
This is very close to the probability calculated by the  Sampling Distribution Calculator :
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/samplingMean2.png">
<h3>The Complete Code</h3>
The complete R code used in this example is shown below:
<b>#make this example reproducible
set.seed(0)
#define number of samples
n = 10000
#create empty vector of length n
sample_means = rep(NA, n)
#fill empty vector with means
for(i in 1:n){
  sample_means[i] = mean(rnorm(20, mean=5.3, sd=9))
}
#view first six sample means
head(sample_means)
#create histogram to visualize the sampling distribution
hist(sample_means, main = "", xlab = "Sample Means", col = "steelblue")
#mean of sampling distribution
mean(sample_means)
#standard deviation of sampling distribution
sd(sample_means)
#calculate probability that sample mean is less than or equal to 6
sum(sample_means &lt;= 6) / length(sample_means)
</b>
<h2><span class="orange">What is a Sampling Distribution?</span></h2>
Imagine there exists a population of 10,000 dolphins and the mean weight of a dolphin in this population is 300 pounds.
If we take a  simple random sample  of 50 dolphins from this population, we might find that the mean weight of dolphins in this sample is 305 pounds.
Then if we take another simple random sample of 50 dolphins, we might find that the mean weight of dolphins in that sample is 295 pounds.
Each time we take a simple random sample of 50 dolphins, it’s likely that the mean weight of the dolphins in the sample will be close to the population mean of 300 pounds, but not exactly 300 pounds.
Imagine that we take 200 simple random samples of 50 dolphins from this population and make a histogram of the mean weight in each sample:
  
In most of the samples, the mean weight will be close to 300 pounds. In rare scenarios, we may happen to pick a sample full of small dolphins where the mean weight is only 250 pounds. Or we may happen to pick a sample full of large dolphins where the mean weight is 350 pounds. In general, the <em>distribution</em> of the sample means will be approximately normal with the center of the distribution located at the true center of the population. 
This distribution of sample means is known as the <b>sampling distribution of the mean </b>and has the following properties:
<b> μ<sub>x</sub></b>= μ 
where μ<sub>x</sub>  is the sample mean and μ is the population mean.
<b> σ<sub>x</sub></b>= σ/ √n 
where σ<sub>x</sub>  is the sample standard deviation, σ is the population standard deviation, and n is the sample size.
For example, in this population of dolphins we know that the mean weight is μ = 300. So the mean of the sampling distribution is <b>μ<sub>x</sub></b>= <b>300</b>.
Suppose we also know that the standard deviation of the population is 18 pounds. So the sample standard deviation is <b>σ<sub>x</sub></b>= 18/ √50 = <b>2.546</b>.
<h2>Sampling Distribution of the Proportion</h2>
Consider the same population of 10,000 dolphins. Suppose 10% of the dolphins are black and the rest are gray. Suppose we take a  simple random sample  of 50 dolphins and find that 14% of the dolphins in that sample are black. Then we take another simple random sample of 50 dolphins and find that 8% of the dolphins in that sample are black.
Imagine that we take 200 simple random samples of 50 dolphins from this population and make a histogram of the proportion of dolphins that are black in each sample:
  
In most of the samples, the proportion of dolphins that are black will be close to the true population of 10%. The <em>distribution</em> of the sample proportion of dolphins that are black will be approximately normal with the center of the distribution located at the true center of the population. 
This distribution of sample proportions is known as the <b>sampling distribution of the proportion </b>and has the following properties:
<b> μ<sub>p</sub> </b>= P 
where <em>p</em> is the sample proportion and <em>P</em> is the population proportion.
<b> σ<sub>p</sub> </b>= √(P)(1-P) / n
where P is the population proportion and n is the sample size.
For example, in this population of dolphins we know that the true proportion of dolphins that are black is 10% = 0.1. So the mean of the sampling distribution of the proportion is <b>μ<sub>p</sub> </b>=<b> 0.1</b>.
Suppose we also know that the standard deviation of the population is 18 pounds. So the sample standard deviation  is <b>σ<sub>p</sub> </b>= √(P)(1-P) / n = √(.1)(1-.1) / 50 = <b>.042</b>.
<h2>Establishing Normality</h2>
To use the formulas above, the sampling distribution needs to be normal.
According to the  <b>central limit theorem</b> , the sampling distribution of a sample mean is approximately normal if the sample size is large enough, <em>even if the population distribution is not normal</em>. In most cases, we consider a sample size of 30 or larger to be sufficiently large.
The sampling distribution of a sample proportion is approximately normal if the expected number of successes and failures are both at least 10.
<h2>Examples</h2>
We can use sampling distributions to calculate probabilities. 
<b>Example 1:</b> <em><b>A certain machine creates cookies. The distribution of the weight of these cookies is skewed to the right with a mean of 10 ounces and a standard deviation of 2 ounces. If we take a simple random sample of 100 cookies produced by this machine, what is the probability that the mean weight of the cookies in this sample is less than 9.8 ounces?</b></em>
<b>Step 1: Establish normality.</b>
We need to make sure that the sampling distribution of the sample mean is normal. Since our sample size is greater than or equal to 30, according to the central limit theorem we can assume that the sampling distribution of the sample mean is normal. 
<b>Step 2: Find the mean and standard deviation of the sampling distribution.</b>
<b>μ<sub>x</sub></b>= μ
<b> σ<sub>x</sub></b>= σ/ √n
<b>μ<sub>x</sub></b>= <b>10 ounces</b>
<b> σ<sub>x</sub></b>= 2/ √100 = 2/10 = <b>0.2 ounces</b>
<b>Step 3:</b> <b>Use the  Z Score Area Calculator  to find the probability that the mean weight of the cookies in this sample is less than 9.8 ounces.</b>
Enter the following numbers into the  Z Score Area Calculator . You can leave “Raw Score 2” blank since we’re only finding one number in this example.
  
Since we want to know the probability that the mean weight of the cookies in this sample is<em> less than</em> 9.8 ounces, we are interested in the area to the <em>left </em>of 9.8. The calculator tells us that this probability is <b>0.15866</b>.
<b>Example 2:</b> <em><b>According to a school-wide study, 87% of students in a particular school prefer pizza over ice cream. Suppose we take a simple random sample of 200 students. What is the probability that the proportion of students who prefer pizza is less than 85%?</b></em>
<b>Step 1: Establish normality.</b>
Recall that the sampling distribution of a sample proportion is approximately normal if the expected number of “successes” and “failures” are both at least 10.
In this case the expected number of students who will prefer pizza is 87% * 200 students = 174 students. The expected number of students who will not prefer pizza is 13% * 200 students = 26 students. Since both of these numbers are at least 10, we can assume that the sampling distribution of the sample proportion of students who will prefer pizza is approximately normal. 
<b>Step 2: Find the mean and standard deviation of the sampling distribution.</b>
<b> μ<sub>p</sub> </b>= P 
<b> σ<sub>p</sub> </b>= √(P)(1-P) / n
<b>μ<sub>p</sub> </b>= <b>0.87</b>
<b> σ<sub>p</sub> </b>= √(.87)(1-.87) / 200 = <b>.024</b>
<b>Step 3:</b> <b>Use the  Z Score Area Calculator  to find the probability that the proportion of students who prefer pizza is less than 85%.</b>
Enter the following numbers into the  Z Score Area Calculator . You can leave “Raw Score 2” blank since we’re only finding one number in this example.
  
Since we want to know the probability that the proportion of students who prefer pizza is less than 85%, we are interested in the area to the <em>left </em>of 0.85. The calculator tells us that this probability is <b>0.20233</b>.
<h3>Bonus: Video Explanation of Sampling Distributions</h3>
<iframe title="Sampling Distributions" src="https://www.youtube.com/embed/mnHBmrJSaYQ?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<h2><span class="orange">What is a Sampling Frame?</span></h2>
Often researchers are interested in answering some question about a population, such as:
What is the mean height of students at a certain school?
What is the mean household income in a certain city?
What is the mean square footage of houses in a certain country?
What proportion of residents in a certain county support a certain law?
The <b>target population </b>is the complete collection of items that researchers are interested in.
Since it’s often too time-consuming and expensive to go around and collect data on every individual in a target population, researchers will instead take a <b>sample </b>from the target population, which is simply a subset of the population.
The list of items from which a sample is obtained is known as the <b>sampling frame</b>. Ideally the sampling frame will be exactly equal to the target population, but this is rarely the case in practice.
<h3>Sampling Frame Example</h3>
Suppose researchers are interested in estimating the proportion of residents over 18 years old in a certain county that support a certain law.
The <b>target population </b>includes every resident in the city over 18 years old. For simplicity, let’s assume the city contains 100,000 such residents.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/samplingFrame1.png">
Ideally our <b>sampling frame </b>would contain all 100,000 such residents so that we could obtain a  sample that is representative  of the target population. However, in reality our sampling frame usually doesn’t match our target population perfectly for a list of reasons including:
Some residents may have moved since the most recent census.
Some residents may have turned 18 since the most recent census.
The city may not have complete information about each resident.
The city may not have means to contact certain residents.
For this reason, our sampling frame (the list of residents over 18 that we’re able to actually obtain information on) likely won’t match our target population perfectly.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/samplingFrame2.png">
Thus, when we go to collect a  random sample  of residents to survey it’s unlikely that our sample will be perfectly representative of the target population. This is known as <b>sampling frame error</b>.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/09/samplingFrame3.png">
While it’s typically impossible to get a sampling frame that perfectly matches the target population, researchers often try to make the sampling frame as similar to the target population as possible.
Thus, when they use data from the sample to draw inferences about the target population, they can be reasonably confident that their inferences will hold true.
<h2><span class="orange">Types of Sampling Methods (With Examples)</span></h2>
Researchers are often interested in answering questions about  populations  like:
What is the average height of a certain species of plant?
What is the average weight of a certain species of bird?
What percentage of citizens in a certain city support a certain law?
One way to answer these questions is to go around and collect data on every single individual in the population of interest.
However, this is typically too costly and time-consuming which is why researchers instead take a <b>sample </b>of the population and use the data from the sample to draw conclusions about the population as a whole.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/04/CIprop1.png">
There are many different methods researchers can potentially use to obtain individuals to be in a sample. These are known as <b>sampling methods</b>.
In this post we share the most commonly used sampling methods in statistics, including the benefits and drawbacks of the various methods.
<h2>Probability Sampling Methods</h2>
The first class of sampling methods is known as <b>probability sampling methods </b>because every member in a population has an equal probability of being selected to be in the sample.
<h3>Simple random sample</h3>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/sampleMethod-simple1.jpg"207"> 
<b>Definition:</b> Every member of a population has an equal chance of being selected to be in the sample. Randomly select members through the use of a random number generator or some means of random selection. 
<b>Example:</b> We put the names of every student in a class into a hat and randomly draw out names to get a sample of students.
<b>Benefit: </b>Simple random samples are usually  representative of the population  we’re interested in since every member has an equal chance of being included in the sample.
<h3>Stratified random sample</h3>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/sampleMethod-stratify.jpg"216"> 
<b>Definition:</b> Split a population into groups. Randomly select some members from each group to be in the sample.
<b>Example:</b> Split up all students in a school according to their grade – freshman, sophomores, juniors, and seniors. Ask 50 students from each grade to complete a survey about the school lunches.
<b>Benefit: </b>Stratified random samples ensure that members from each group in the population are included in the survey.
<h3>Cluster random sample</h3>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/sampleMethod-cluster.jpg"215"> 
<b>Definition:</b> Split a population into clusters. Randomly select some of the clusters and include all members from those clusters in the sample.
<b>Example:</b> A company that gives whale watching tours wants to survey its customers. Out of ten tours they give one day, they randomly select four tours and ask every customer about their experience.
<b>Benefit:</b> Cluster random samples get every member from some of the groups, which is useful when each group is reflective of the population as a whole.
<h3>Systematic random sample</h3>
 <img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2018/09/sampleMethod-systematic.jpg"356"> 
<b>Definition:</b> Put every member of a population into some order. Choosing a random starting point and select every n<sup>th</sup> member to be in the sample.
<b>Example:</b> A teacher puts students in alphabetical order according to their last name, randomly chooses a starting point, and picks every 5th student to be in the sample.
<b>Benefit:</b> Systematic random samples are usually  representative of the population  we’re interested in since every member has an equal chance of being included in the sample.
<h2>Non-probability Sampling Methods</h2>
Another class of sampling methods is known as <b>non-probability sampling methods </b>because not every member in a population has an equal probability of being selected to be in the sample.
This type of sampling method is sometimes used because it’s much cheaper and more convenient compared to probability sampling methods. It’s often used during exploratory analysis when researchers simply want to gain an initial understanding of a population.
However, the samples that result from these sampling methods cannot be used to draw inferences about the populations they came from because they typically aren’t representative samples.
<h3>Convenience sample</h3>
<b>Definition:</b> Choose members of a population that are readily available to be included in the sample.
<b>Example:</b> A researcher stands in front of a library during the day and polls people that happen to walk by.
<b>Drawback:</b> Location and time of day will affect the results. More than likely, the sample will suffer from  undercoverage bias  since certain people (e.g. those who work during the day) will not be represented as much in the sample.
<h3>Voluntary response sample</h3>
<b>Definition:</b> A researcher puts out a request for volunteers to be included in a study and members of a population voluntarily decide to be included in the sample or not. 
<b>Example:</b> A radio host asks listeners to go online and take a survey on his website.
<b>Drawback:</b> People who  voluntarily respond  will likely have stronger opinions (positive or negative) than the rest of the population, which makes them an unrepresentative sample. Using this sampling method, the sample is likely to suffer from  nonresponse bias  – certain groups of people are simply less likely to provide responses.
<h3>Snowball sample</h3>
<b>Definition:</b> Researchers recruit initial subjects to be in a study and then ask those initial subjects to recruit additional subjects to be in the study. Using this approach, the sample size “snowballs” bigger and bigger as each additional subject recruits more subjects.
<b>Example:</b> Researchers are conducting a study of individuals with rare diseases, but it’s difficult to find individuals who actually have the disease. However, if they can find just a few initial individuals to be in the study then they can ask them to recruit further individuals they may know through a private support group or through some other means.
<b>Drawback:</b> Sampling bias is likely to occur. Because initial subjects recruit additional subjects, it’s likely that many of the subjects will share similar traits or characteristics that might be unrepresentative of the larger population under study. Thus, findings from the sample can’t be extrapolated to the population.
<em> Read more about snowballing sampling here .</em>
<h3>Purposive sample</h3>
<b>Definition:</b> Researchers recruit individuals based on who they think will be most useful based on the purpose of their study.
<b>Example:</b> Researchers want to know about the opinions that individuals in a city have about a potential new rock climbing gym being placed in the city square so they purposely seek out individuals that hang out at other rock climbing gyms around the city.
<b>Drawback:</b> The individuals in the sample are unlikely to be representative of the overall population. Thus, findings from the sample can’t be extrapolated to the population.
<h2><span class="orange">What is Sampling Variability? Definition & Example</span></h2>
Often in statistics we’re interested in answering questions like:
What is the mean household income in a certain state?
What is the mean weight of a certain species of turtle?
What is the mean attendance at college football games?
In each scenario, we are interested in answering some question about a  population , which represents every possible individual element that we’re interested in measuring.
However, instead of collecting data on every individual in a population we instead collect data on a <b>sample</b> of the population, which represents a portion of the total population.
For example, we might want to know the mean weight of a certain species of turtle that has a total population of 800 turtles.
Since it would take too long to locate and weigh every single turtle in the population, we instead collect a  simple random sample  of 30 turtles and weigh them:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2020/11/pop_vs_sample2.png">
We could then use the mean weight of this sample of turtles to estimate the mean weight of all turtles in the population.
<b>Sampling variability</b> refers to the fact that the mean will vary from one sample to the next.
For example, in one random sample of 30 turtles the sample mean may turn out to be 350 pounds. In another random sample, the sample mean may be 345 pounds. In yet another sample, the sample mean may be 355 pounds.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/samplingVariability1.png">
There is <b>variability</b> among the sample means.
<h3>How to Measure Sampling Variability</h3>
In practice, we only collect one sample to estimate a population parameter. For example, we will only collect one sample of 30 sea turtles to estimate the mean weight for the entire population of turtles.
This means we’ll only calculate one sample mean (x) and use it to estimate the population mean (μ).
<b>Sample Mean = x</b>
But we know that the sample mean will vary from one sample to the next. So, to account for this variability we can use the following formula to estimate the standard deviation of the sample mean:
<b>Standard Deviation of Sample Mean = s / √n</b>
where:
<b>s:</b> The sample standard deviation
<b>n:</b> The sample size
For example, suppose we collect a sample of 30 sea turtles and find that the sample mean weight is 350 pounds and the sample standard deviation is 12 pounds. Based on these numbers, we would calculate:
<b>Sample Mean = </b>350 pounds
<b>Standard Deviation of Sample Mean = </b>12 / √30 = 2.19 pounds
This means that our best estimate for the true population mean weight of all turtles is 350 pounds, but that we should expect the mean from one sample to the next to vary with a standard deviation of about 2.19 pounds.
One interesting property of the standard deviation of the sample mean is that it naturally becomes smaller as we use larger and larger sample sizes.
For example, suppose we collect a sample of 100 sea turtles and find that the sample mean weight is 350 pounds and the sample standard deviation is 12 pounds. The standard deviation of the sample mean would then be calculated as:
<b>Standard Deviation of Sample Mean = </b>12 / √100 = 1.2 pounds
Our best estimate for the sample mean would still be 350 pounds, but we can expect the mean from one sample of 100 sea turtles to the next sample of 100 sea turtles to vary with a standard deviation of just 1.2 pounds.
In other words, there is less variability among sample means when the sample sizes are larger.
<h2><span class="orange">Sampling With Replacement vs. Without Replacement</span></h2>
Often in statistics we’re interested in collecting data so that we can answer some research question.
For example, we might want to answer the following questions:
<b>1.</b> What is the median household income in Cincinnati, Ohio?
<b>2.</b> What is the mean weight of a certain population of turtles?
<b>3.</b> What percentage of residents in a certain county support a certain law?
In each scenario, we are interested in answering some question about a  population , which represents every possible individual element that we’re interested in measuring.
However, instead of collecting data on every individual in a population we typically just collect data on a sample of the population, which represents a portion of the population.
There are two different ways to collect samples: <b>Sampling with replacement</b> and <b>sampling without replacement</b>.
This tutorial explains the difference between the two methods along with examples of when each is used in practice.
<h2>Sampling with Replacement</h2>
Suppose we have the names of 5 students in a hat:
Andy
Karl
Tyler
Becca
Jessica
Suppose we would like to take a sample of 2 students with replacement.
On the first random draw, we might select the name Tyler. We would then place his name back in the hat and draw again. On the second draw, we might select the name Tyler again. Thus our sample would be: {Tyler, Tyler}
This is an example of obtaining a sample with replacement because we replace the name we choose after each random draw.
When we sample with replacement, the items in the sample are <b>independent</b> because the outcome of one random draw is not affected by the previous draw.
For example, the probability of choosing the name Tyler is 1/5 on the first draw and 1/5 again on the second draw. The outcome of the first draw does not affect the probability of the outcome on the second draw.
Sampling with replacement is used in many different scenarios in statistics and machine learning, including:
 Bootstrapping 
 Bagging 
 A Simple Introduction to Boosting in Machine Learning 
 A Simple Introduction to Random Forests 
In each of these methods, sampling with replacement is used because it allows us to use the same dataset multiple times to build models as opposed to going out and gathering new data, which can be time-consuming and expensive.
<h2>Sampling without Replacement</h2>
Again, suppose we have the names of 5 students in a hat:
Andy
Karl
Tyler
Becca
Jessica
Suppose we would like to take a sample of 2 students without replacement.
On the first random draw, we might select the name Tyler. We would then leave his name out of the hat. On the second draw, we might select the name Andy. Thus our sample would be: {Tyler, Andy}
This is an example of obtaining a sample without replacement because we do not replace the name we choose after each random draw.
When we sample without replacement, the items in the sample are <b>dependent</b> because the outcome of one random draw is affected by the previous draw.
For example, the probability of choosing the name Tyler is 1/5 on the first draw and the probability of choosing the name Andy is 1/4 on the second draw. The outcome of the first draw affects the probability of the outcome on the second draw.
Sampling without replacement is the method we use when we want to select a  random sample  from a population.
For example, if we want to estimate the median household income in Cincinnati, Ohio there might be a total of 500,000 different households.
Thus, we might want to collect a random sample of 2,000 households but we don’t want the data for any given household to appear twice in the sample so we would sample without replacement.
In other words, once we’ve chosen a certain household to be included in the sample we don’t want there to be any chance of selecting that household to be included again.
<h2><span class="orange">How to Add Days to Date in SAS (With Example)</span></h2>
The easiest way to add days to a date variable in SAS is to use the <b>INTNX</b> function.
This function uses the following basic syntax:
<b>INTNX(interval, start_date, increment)</b>
where:
<b>interval</b>: The interval to add to date (day, week, month, year, etc.)
<b>start_date</b>: Variable that contains start dates
<b>increment</b>: The number of intervals to add
The following example shows how to use this syntax in practice.
<h2>Example: Add Days to Date in SAS</h2>
Suppose we have the following dataset in SAS that shows the total sales made on various days at some store:
<b>/*create dataset*/
data data1;
    input month day year sales;
    datalines;
10 15 2022 45
10 19 2022 50
10 25 2022 39
11 05 2022 14
12 19 2022 29
12 23 2022 40
;
run;
/*create second dataset with date formatted*/
data data2;
  set data1;
  date=mdy(month,day,year);
  format date mmddyy10.;
  drop month day year;
run;
/*view dataset*/
proc print data=data2;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/add_days1.jpg"192">
We can use the following code to create a new column called <b>date_plus5</b> that adds five days to the values in the <b>date</b> column:
<b>/*create new dataset with column that adds 5 days to date*/
data data3; 
  set data2; 
  date_plus5=intnx('day', date, 5); 
  format date_plus5 mmddyy10.;
run;
/*view dataset*/
proc print data=data3;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/add_days2.jpg">
Notice that the new column called <b>date_plus5</b> contains the values in the <b>date</b> column with fives days added to them.
Note that you can also subtract days by simply using a negative value in the <b>INTNX</b> function.
For example, we can use the following code to subtract five days from each value in the <b>date</b> column:
<b>/*create new dataset with column that subtracts 5 days to date*/
data data3; 
  set data2; 
  date_minus5=intnx('day', date, -5); 
  format date_minus5 mmddyy10.;
run;
/*view dataset*/
proc print data=data3;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/add_days3-1.jpg"305">
Notice that the new column called <b>dateminus5</b> contains the values in the <b>date</b> column with fives days subtracted from them.
<b>Note</b>: You can find the complete documentation for the SAS <b>INTNX</b> function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Normalize Data in SAS 
 How to Identify Outliers in SAS 
 How to Create Frequency Tables in SAS 
<h2><span class="orange">How to Add Row Numbers in SAS (With Examples)</span></h2>
You can use the following methods to add row numbers to a dataset in SAS:
<b>Method 1: Add Row Number</b>
<b>data my_data2;
    row_number = _N_;
    set my_data1;
run;</b>
<b>Method 2: Add Row Number by Group</b>
<b>/*sort original dataset by var1*/
proc sort data=my_data1;
    by var1;
run;
/*create new dataset that shows row number by var1*/
data my_data2;  
    set my_data1;
    by var1;
    if first.var1 then row_number=0;
    row_number+1;
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data my_data1;
    input team $ points;
    datalines;
Mavs 22
Mavs 40
Rockets 41
Rockets 29
Rockets 30
Spurs 18
Spurs 22
Spurs 27
Warriors 13
Warriors 19
;
run;
/*view dataset*/
proc print data=my_data1;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/row1.jpg"168">
<h3>Example 1: Add Row Number</h3>
The following code shows how to add a new column called <b>row_number</b> that contains the row number for each observation:
<b>/*create new dataset with column for row numbers*/
data my_data2;
    row_number = _N_;
    set my_data1;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/row2.jpg"268">
Notice that a new column called <b>row_number</b> has been added that contains the row number for each observation in the dataset.
<h3>Example 2: Add Row Number by Group</h3>
The following code shows how to add a row number by group:
<b>/*sort original dataset by team*/
proc sort data=my_data1;
    by var1;
run;
/*create new dataset that shows row number by team*/
data my_data2;  
    set my_data1;
    by var1;
    if first.var1 then row_number=0;
    row_number+1;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/row3.jpg"258">
The resulting table shows the row number by each team.
Notice that the row numbers start over for each new team.
<h2><span class="orange">How to Create Bar Charts in SAS (3 Examples)</span></h2>
You can use the following methods to create different types of bar charts in SAS:
<b>Method 1: Create One Bar Chart</b>
<b>proc sgplot data = my_data;
    vbar variable1;
run;
</b>
<b>Method 2: Create Stacked Bar Chart</b>
<b>proc sgplot data = my_data;
    vbar variable1 / group = variable2;
run;</b>
<b>Method 3: Create Clustered Bar Chart</b>
<b>proc sgplot data = my_data;
    vbar variable1 / group = variable2 groupdisplay = cluster;
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input team $ position $ points;
    datalines;
A Guard 8
A Guard 6
A Guard 6
A Forward 9
A Forward 14
A Forward 11
B Guard 10
B Guard 9
B Guard 5
B Forward 7
C Guard 10
C Forward 6
C Forward 8
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/bar1.jpg"218">
<h3>Example 1: <b>Create One Bar Chart</b></h3>
The following code shows how to create a bar chart to visualize the frequency of teams:
<b>/*create bar chart to visualize frequency of teams*/
title "Bar Chart of Team Frequency";
proc sgplot data = my_data;
    vbar team;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/bar2.jpg">
If you’d like a horizontal bar chart instead, simply use the <b>hbar</b> option:
<b>/*create horizontal bar chart to visualize frequency of teams*/
title "Bar Chart of Team Frequency";
proc sgplot data = my_data;
    hbar team;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/bar3.jpg">
<h3>Example 2: <b>Create Stacked Bar Chart</b></h3>
The following code shows how to create a stacked bar chart to visualize the frequency of both <b>team</b> and <b>position</b>:
<b>/*create stacked bar chart*/
title "Stacked Bar Chart of Team & Position";
proc sgplot data = my_data;
    vbar team / group = position;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/bar4.jpg">
This chart allows us to visualize the frequency of each team along with the frequency of positions within each team.
<h3>Example 3: Create Clustered Bar Chart</h3>
The following code shows how to create a clustered bar chart to visualize the frequency of both <b>team</b> and <b>position</b>:
<b>/*create clustered bar chart*/
title "Clustered Bar Chart of Team & Position";
proc sgplot data = my_data;
    vbar team / group = position groupdisplay = cluster;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/bar5.jpg">
This bar chart displays the same information as the previous bar chart, except the bars are “clustered” together instead of stacked on top of each other.
<h2><span class="orange">How to Create Boxplots by Group in SAS</span></h2>
 Boxplots  are useful for quickly visualizing the five-number summary of a dataset, which includes:
The minimum value
The first quartile
The median
The third quartile
The maximum value
The following example shows how to create grouped boxplots in SAS to visualize the distribution of several groups at once.
<h3>Example: Create Boxplots by Group in SAS</h3>
First, let’s create a dataset in SAS that contains three different groups:
<b>/*create dataset*/
data my_data;
    input Group $ Value;
    datalines;
A 7
A 8
A 9
A 12
A 14
B 5
B 6
B 6
B 8
B 11
C 8
C 9 
C 11
C 13
C 17
;
run;
</b>
Note that there are three different groups in this dataset: A, B, and C.
Next, we can use the following code to create boxplots by group:
<b>/*create boxplots by group*/
proc sgplot data=my_data;
   vbox Value / group=Group;
   keylegend / title="Group Name";
run; </b>
The result is three individual boxplots that display the distribution of data values for groups A, B, and C:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/sasGroup1.jpg">
<b>Related:</b>  How to Compare Box Plots 
Note that you can use the <b>hbox</b> function to create horizontal boxplots instead:
<b>/*create horizontal boxplots by group*/
proc sgplot data=my_data;
   hbox Value / group=Group;
   keylegend / title="Group Name";
run; </b>
The result is three horizontal boxplots:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/sasGroup2.jpg"568">
The legend at the bottom of the plot shows which color corresponds to each group.
<h2><span class="orange">How to Use a CASE WHEN Statement in SAS (With Examples)</span></h2>
We can use the <b>CASE</b> statement in SAS to create a new variable that uses case-when logic to determine the values to assign to the new variable.
This statement uses the following basic syntax:
<b>proc sql;
    select var1, case when var2 = 'A' then 'North'
                when var2 = 'B' then 'South'
                when var2 = 'C' then 'East' else 'West' end as variable_name
    from my_data;
quit;
</b>
The following example shows how to use the <b>CASE</b> statement in practice.
<h3>Example: Using the CASE Statement in SAS</h3>
Suppose we have the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input team $ points rebounds;
    datalines;
A 25 8
A 18 12
A 22 6
B 24 11
B 27 14
C 33 19
C 31 20
D 30 17
D 18 22
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/case11.jpg"251">
We can use the following <b>CASE</b> statement to create a new variable called <b>Division</b> whose values are based on the values of the <b>team</b> variable:
<b>/*create dataset*/
proc sql;
    select team, points, case when team = 'A' then 'North'        when team = 'B' then 'South'        when team = 'C' then 'East' else 'West'         end as Division
    from original_data;
quit;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/case12.jpg">
Note that a new variable <b>Division</b> was created whose values are based on the values for the <b>team</b> variable.
<h2><span class="orange">How to Check if Dataset Exists in SAS (With Example)</span></h2>
You can use the following macro in SAS to quickly check if a dataset exists:
<b>%macro check_exists(data);
   %if %sysfunc(exist(&data.)) %then %do;
      %put Dataset Exists;
   %end;
   %else %do;
      %put Dataset Does Not Exist;
   %end;
%mend check_exists;
</b>
When you run this macro, it will return “Dataset Exists” if a dataset exists.
Otherwise, it will return “Does Not Exist.”
The following example shows how to use this macro in practice.
<h2>Example: Check if Dataset Exists in SAS</h2>
Suppose we create the following dataset in SAS called <b>data1</b>:
<b>/*create dataset*/
data data1;
    input hours score;
    datalines;
1 64
2 66
4 76
5 73
5 74
6 81
6 83
7 82
8 80
10 88
;
run;
/*view dataset*/
proc print data=data1;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/macro1.jpg"167">
We can define the following macro to check if a dataset exists:
<b>%macro check_exists(data);
   %if %sysfunc(exist(&data.)) %then %do;
      %put Dataset Exists;
   %end;
   %else %do;
      %put Dataset Does Not Exist;
   %end;
%mend check_exists;</b>
We can then run this macro to check if the dataset called <b>data1</b> exists:
<b>/*check if dataset called data1 exists*/
%check_exists(data1);
</b>
When we view the log, we can see that the macro returns Does Exist since <b>data1</b> does indeed exist:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/macro2.jpg"503">
Now suppose we also run the macro to check if a dataset called <b>data2</b> exists:
<b>/*check if dataset called data2 exists*/
%check_exists(data2);</b>
When we view the log, we can see that the macro returns Does Not Exist since a dataset called <b>data2</b> has never been created.
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/macro3.jpg"503">
<b>Note</b>: You can find the complete documentation for the <b>EXIST</b> function in SAS  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Delete Datasets in SAS 
 How to Rename Variables in SAS 
 How to Create New Variables in SAS 
<h2><span class="orange">How to Use the COALESCE Function in SAS (With Examples)</span></h2>
You can use the <b>COALESCE </b>function in SAS to return the first non-missing value in each row of a dataset.
The following example shows how to use this function in practice.
<h3>Example: <b>How to Use COALESCE in SAS</b></h3>
Suppose we have the following dataset in SAS that contains some missing values:
<b>/*create dataset*/
data original_data;
    input team $ points rebounds assists;
    datalines;
Warriors 25 8 7
Wizards . 12 6
Rockets . . 5
Celtics 24 . 5
Thunder . 14 5
Spurs 33 19 .
Nets . . .
Mavericks . 8 10
Kings . . 9
Pelicans . 23 6
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/coa1.jpg"322">
We can use the <b>COALESCE</b> function to create a new column that returns the first non-missing value in each row among the points, rebounds, and assists columns:
<b>/*create new dataset*/
data new_data;
    set original_data;
    first_non_missing = coalesce(points, rebounds, assists);
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/coa2.jpg">
Here’s how the value in the <b>first_non_missing </b>column was chosen:
First row: The first non-missing value among points, rebounds, and assists was <b>25</b>.
Second row: The first non-missing value among points, rebounds, and assists was <b>12</b>.
Third row: The first non-missing value among points, rebounds, and assists was <b>5</b>.
And so on.
<b>Note #1</b>: If all values are missing (like in row 7) then the <b>COALESCE</b> function will simply return a missing value.
<b>Note #2</b>: The <b>COALESCE</b> function only works with numeric variables. If you’d instead like to return the first non-missing value among a list of character variables, use the <b>COALESCEC</b> function.
<h2><span class="orange">How to Use the COMPRESS Function in SAS (With Examples)</span></h2>
You can use the <b>COMPRESS </b>function in SAS to remove specific characters from a string.
This function uses the following basic syntax:
<b>COMPRESS(String, characters to be removed)</b>
where:
<b>String</b>: The string to analyze
<b>characters to be removed</b>: One or more specific characters to remove from string
Here are the four most common ways to use this function:
<b>Method 1: Remove All Blank Spaces from String</b>
<b>data new_data;
    set original_data;
    compressed_string = compress(string_variable);
run;
</b>
<b>Method 2:Remove Specific Characters from String</b>
<b>data new_data;
    set original_data;
    compressed_string = compress(string_variable, '!?@#');
run;</b>
<b>Method 3: Remove All Alphabetical Characters from String</b>
<b>data new_data;
    set original_data;
    compressed_string = compress(string_variable, '', 'a');
run;</b>
<b>Method 4: Remove All Numeric Values from String</b>
<b>data new_data;
    set original_data;
    compressed_string = compress(string_variable, '', 'd');
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input name $25.;
    datalines;
Andy Lincoln4 Bernard!
Barren Michael55 Smith!
Chad Simpson7 Arnolds?
Derrick Parson2 Henry
Eric Miller2 Johansen!
Frank Giovanni5 Goode
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compress1.jpg"219">
<h2>Example 1: Remove All Blank Spaces from String</h2>
The following code shows how to remove all blank spaces from each string in the <b>name</b> column:
<b>/*remove blank spaces from each string in name column*/
data new_data;
    set original_data;
    compressed_name = compress(name);
run;
/*view results*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compress2.jpg"399">
Notice that all blank spaces have been removed from each string in the new column called <b>compressed_name</b>.
<h2>Example 2: Remove Specific Characters from String</h2>
The following code shows how to remove all question marks and exclamation points from each string in the <b>name</b> column:
<b>/*remove question marks and exclamation points from each string in name column*/
data new_data;
    set original_data;
    compressed_name = compress(name, '?!');
run;
/*view results*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compress3.jpg"398">
Notice that all question marks and exclamation points have been removed from each string in the new column called <b>compressed_name</b>.
<h2>Example 3: Remove All Alphabetical Characters from String</h2>
The following code shows how to remove all alphabetical characters from each string in the <b>name</b> column:
<b>/*remove all alphabetical characters from each string in name column*/
data new_data;
    set original_data;
    compressed_name = compress(name, '', 'a');
run;
/*view results*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compress4.jpg"384">
Notice that all all alphabetical characters have been removed from each string in the new column called <b>compressed_name</b>.
<h2>Example 4: Remove All Numeric Values from String</h2>
The following code shows how to remove all numeric values from each string in the <b>name</b> column:
<b>/*remove all numeric values from each string in name column*/
data new_data;
    set original_data;
    compressed_name = compress(name, '', 'd');
run;
/*view results*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/compress5.jpg"397">
Notice that all all numeric values have been removed from each string in the new column called <b>compressed_name</b>.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Use the SUBSTR Function in SAS 
 How to Use the FIND Function in SAS 
 How to Use the COALESCE Function in SAS 
<h2><span class="orange">How to Concatenate Datasets in SAS (With Example)</span></h2>
You can use the following basic syntax to concatenate datasets in SAS:
<b>/*concatenate two datasets into one*/
data data3;
set data1 data2;
run;
</b>
The following example shows how to use this syntax in practice.
<h3>Example: Concatenate Datasets in SAS</h3>
Suppose we have the following two datasets in SAS:
<b>/*create first dataset*/
data data1;
    input firstName $ lastName $ points;
    datalines;
Austin Smith 15
Brad Stevens 31
Chad Miller 22
;
run;
/*create second dataset*/
data data2;
    input firstName $ lastName $ points;
    datalines;
Dave Michaelson 19
Eric Schmidt 29
Frank Wright 20
Greg Gunner 40
Harold Anderson 35
;
run;
/*view datasets*/
proc print data=data1;
proc print data=data2;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/concat1.jpg">
We can use the following code to concatenate these two datasets into one dataset:
<b>/*concatenate two datasets into one*/
data data3;
set data1 data2;
run;
/*view new dataset*/
proc print data=data3;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/concat2.jpg"266">
The resulting dataset contains all of the observations from the first two datasets.
<b>Note</b>: In this example we concatenated only two datasets into one. However, we can use similar syntax to concatenate as many datasets as we’d like. The only requirement is that each dataset contains the same variable names.
<h2><span class="orange">How to Concatenate Strings in SAS (With Examples)</span></h2>
You can use the following methods to quickly concatenate strings in SAS.
<b>Method 1: Concatenate Strings with Space in Between</b>
<b>new_variable = CAT(var1, var2);
</b>
<b>Method 2: Concatenate Strings with No Space in Between</b>
<b>new_variable = CATS(var1, var2);</b>
<b>Method 3: Concatenate Strings with Custom Delimiter</b>
<b>new_variable = CATX("-", var1, var2);</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data my_data1;
    input firstName $ lastName $ points;
    datalines;
Austin Smith 15
Brad Stevens 31
Chad Miller 22
Dave Michaelson 19
Eric Schmidt 29
Frank Wright 20
Greg Gunner 40
Harold Anderson 35
;
run;
/*view dataset*/
proc print data=my_data1;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/conc1.jpg"273">
<h3>Example 1: Concatenate Strings with <b>Space in Between</b></h3>
The following code shows how to create a new column called <b>fullName</b> that concatenates the <b>firstName</b> and <b>lastName</b> columns using a blank space as a delimiter:
<b>/*create new dataset with concatenated strings*/
data my_data2;
set my_data1;
fullName = CAT(firstName, lastName);
run;
/*view new dataset*/
proc print data=my_data2;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/conc2.jpg">
<h3>Example 2: Concatenate Strings with No Space in Between</h3>
The following code shows how to create a new column called <b>fullName</b> that concatenates the <b>firstName</b> and <b>lastName</b> columns using no space as a delimiter:
<b>/*create new dataset with concatenated strings*/
data my_data2;
set my_data1;
fullName = CATS(firstName, lastName);
run;
/*view new dataset*/
proc print data=my_data2;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/conc3.jpg"379">
<h3>Example 3: Concatenate Strings with Custom Delimiter</h3>
The following code shows how to create a new column called <b>fullName</b> that concatenates the <b>firstName</b> and <b>lastName</b> columns using a dash as a delimiter:
<b>/*create new dataset with concatenated strings*/
data my_data2;
set my_data1;
fullName = CATX("-", firstName, lastName);
run;
/*view new dataset*/
proc print data=my_data2;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/conc4.jpg"368">
<h2><span class="orange">SAS: Filter for Rows that Contain String</span></h2>
You can use the following methods to filter SAS datasets for rows that contain certain strings:
<b>Method 1: Filter Rows that Contain Specific String</b>
<b>/*filter rows where var1 contains "string1"*/
data specific_data;
    set original_data;
    where var1 contains 'string1';
run;</b>
<b>Method 2: Filter Row that Contain One of Several Strings</b>
<b>/*filter rows where var1 contains "string1", "string2", or "string3"*/
data specific_data;
    set original_data;
    where var1 in ('string1', 'string2', 'string3');
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data nba_data;
    input team $ points;
    datalines;
Mavs 95
Spurs 99
Warriors 104
Rockets 98
Heat 95
Nets 90
Magic 99
Cavs 106
;
run;
/*view dataset*/
proc print data=nba_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/contains1.jpg"181">
<h3>Method 1: Filter Rows that Contain Specific String</h3>
The following code shows how to filter the dataset for rows that contain the string “avs” in the <b>team</b> column:
<b>/*filter rows where team contains the string 'avs'*/
data specific_data;
    set nba_data;
    where team contains 'avs';
run;
/*view resulting rows*/
proc print data=specific_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/contains2.jpg"158">
The only two rows shown are the ones where the <b>team</b> column contains ‘avs’ in the name.
<h3>Method 2: Filter Rows that Contain One of Several Strings</h3>
The following code shows how to filter the dataset for rows that contain the strings “Mavs”, “Nets”, or “Rockets” in the <b>team</b> column:
<b>/*filter rows where team contains the string 'Mavs', 'Nets', or 'Rockets'*/
data specific_data;
    set nba_data;
    where team in ('Mavs', 'Nets', 'Rockets');
run;
/*view resulting rows*/
proc print data=specific_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/contains3.jpg"178">
The resulting dataset only shows the rows where the <b>team</b> column contains one of the three strings that we specified.
<h2><span class="orange">SAS: How to Convert Character Variable to Date</span></h2>
You can use the <b>input() </b>function in SAS to convert a character variable to a date variable format.
This function uses the following basic syntax:
<b>date_var = input(character_var, MMDDYY10.);
format date_var MMDDYY10.;
</b>
The following example shows how to use this function in practice.
<h3>Example: Convert Character Variable to Date in SAS</h3>
Suppose we have the following dataset in SAS that shows the total sales made by some store during six different days:
<b>/*create dataset*/
data original_data;
    input day $ sales;
    datalines;
01012022 15
01022022 19
01052022 22
01142022 11
01152022 26
01212022 28
;
run;
/*view dataset*/
proc print data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/date11.jpg"171">
We can see that <b>day</b> is a character variable, but it needs to be represented in a date format.
We can use the following code to create a new dataset in which we convert the <b>day</b> variable from a character to date format:
<b>/*create new dataset where 'day' is in date format*/
data new_data;
    set original_data;
    new_day = input(day, MMDDYY10.);
    format new_day MMDDYY10.;
    drop day;
run;
/*view new dataset*/
proc print data=new_data; </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/date12.jpg"181">
<b>Note</b>: We used the <b>drop</b> function to drop the original day variable from the dataset.
We can see that the new variable we created, <b>new_day</b>, is in a date format.
Note that <b>MMDDYY10.</b> is only one possible date format that we could have used. You can find a complete list of SAS date formats  here .
<h2><span class="orange">How to Convert Datetime to Date in SAS</span></h2>
The easiest way to convert a datetime to a date in SAS is to use the <b>DATEPART</b> function.
This function uses the following basic syntax:
<b>date = put(datepart(some_datetime), mmddyy10.);</b>
The argument <b>mmddyy10.</b> specifies that the date should be formatted like 10/15/2022.
The following example shows how to use this syntax in practice.
<h2>Example: Convert Datetime to Date in SAS</h2>
Suppose we have the following dataset in SAS that contains one column of datetimes:
<b>/*create dataset*/
data original_data;
    format some_datetime datetime23.;
    input some_datetime :datetime23.;
    datalines;
14OCT2022:0:0:0
09NOV2022:0:0:0
14NOV2022:0:0:0
15NOV2022:0:0:0
22DEC2022:0:0:0
24DEC2022:0:0:0
04JAN2023:0:0:0
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/datetime1.jpg"189">
The following code shows how to use the <b>DATEPART</b> function to create a new dataset in which the values in the datetime column are formatted as dates with various formats:
<b>/*create new dataset with datetime formatted as date*/
data new_data;
    set original_data;
    date_mmddyyyy = put(datepart(some_datetime), mmddyy10.);
    date_yyyymmdd = put(datepart(some_datetime), yymmdd10.);
    date_date9 = put(datepart(some_datetime), date9.);
    date_default = datepart(some_datetime);
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/datetime2.jpg"614">
Notice that the four new columns display the date from the original datetime column in various formats.
By default, the <b>DATEPART</b> function converts a datetime to the number of days since January 1, 1960.
Thus, the new column called <b>date_default</b> displays the number of days since January 1, 1960 for each datetime.
<b>Note</b>: You can find the complete documentation for the SAS <b>DATEPART </b>function  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Add Days to Date in SAS 
 How to Convert Numeric Variable to Date in SAS 
 How to Calculate Difference Between Two Dates in SAS 
<h2><span class="orange">SAS: Convert Numeric to Character with Leading Zeros</span></h2>
You can use the following basic syntax to convert a numeric variable to a character variable with a specific amount of leading zeros in SAS:
<b>data new_data;
    set original_data;
    employee_ID = put(employee_ID, z10.);
    format employee_ID z10.;
run;
</b>
This particular example converts the numeric variable called <b>employee_ID</b> into a character variable with enough leading zeros to make <b>employee_ID</b> have a length equal to 10.
The following example shows how to use this syntax in practice.
<h2>Example: Convert Numeric to Character with Leading Zeros in SAS</h2>
Suppose we have the following dataset in SAS that shows the total sales made by various employees at some company:
<b>/*create dataset*/
data original_data;
    input employee_ID sales;
    datalines;
4456 12
4330 18
2488 19
2504 11
2609 33
2614 30
2775 23
2849 14
;
/*view dataset*/
proc print data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/leadingz1.jpg"195">
Now suppose we would like to convert the variable called <b>employee_ID</b> to a character variable with enough leading zeros to make each value in the column have a length of 10.
We can use the following syntax to do so:
<b>/*create new dataset with employee_ID as character with leading zeros*/
data new_data;
    set original_data;
    employee_ID = put(employee_ID, z10.);
    format employee_ID z10.;
run;
/*view new dataset*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/leadingz2.jpg"195">
We can see that the <b>employee_ID</b> variable in the new dataset contains enough leading zeros to make each of the values have a length of 10.
To add a different number of leading zeros, simply change z10 to a different value.
For example, we could use z15 to add enough leading zeros to make each of the values in the <b>employee_ID</b> column have a length of 15:
<b>/*create new dataset with employee_ID as character with leading zeros*/
data new_data;
    set original_data;
    employee_ID = put(employee_ID, z15.);
    format employee_ID z15.;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/leadingz3.jpg"231">
Each of the values in the <b>employee_ID</b> column now have a length of 15.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 SAS: How to Rename Variables 
 SAS: How to Convert Numeric Variable to Date 
 SAS: How to Convert Character Variable to Numeric 
<h2><span class="orange">SAS: How to Convert Numeric Variable to Character</span></h2>
You can use the <b>put() </b>function in SAS to convert a numeric variable to a character variable.
This function uses the following basic syntax:
<b>character_var = put(numeric_var, 8.);
</b>
The following example shows how to use this function in practice.
<b>Related:</b>  How to Convert Character Variable to Numeric in SAS 
<h3>Example: Convert Numeric Variable to Character in SAS</h3>
Suppose we have the following dataset in SAS that shows the total sales made by some store during 10 consecutive days:
<b>/*create dataset*/
data original_data;
    input day sales;
    datalines;
1 7
2 12
3 15
4 14
5 13
6 11
7 10
8 16
9 18
10 24
;
run;
/*view dataset*/
proc print data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot1.jpg"132">
We can use<b> proc contents </b>to view the data type of each variable in the dataset:
<b>/*display data type for each variable*/
proc contents data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/num1.jpg"293">
We can see that <b>day</b> and <b>sales</b> are both numeric variables.
We can use the following code to create a new dataset in which we convert the <b>day</b> variable from numeric to character:
<b>/*create new dataset where 'day' is character*/
data new_data;
    set original_data;
    char_day = put(day, 8.);
    drop day;
run;
/*view new dataset*/
proc print data=new_data; </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/num2.jpg"169">
<b>Note</b>: We used the <b>drop</b> function to drop the original day variable from the dataset.
We can use <b>proc contents</b> once again to check the data type of each variable in the new dataset:
<b>/*display data type for each variable in new dataset*/
proc contents data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/num3.jpg"285">
We can see that the new variable we created, <b>char_day</b>, is a character variable.
<h2><span class="orange">SAS: How to Convert Numeric Variable to Date</span></h2>
You can use the following basic syntax to convert a numeric variable to a date variable in SAS:
<b>date_var = input(put(numeric_var, 8.), MMDDYY10.);
format date_var MMDDYY10.;
</b>
The following example shows how to use this function in practice.
<b>Related:</b>  How to Convert Numeric Variable to Character in SAS 
<h2>Example: Convert Numeric Variable to Date in SAS</h2>
Suppose we have the following dataset in SAS that shows the total sales made by some store during various different days:
<b>/*create dataset*/
data original_data;
    input day sales;
    datalines;
01012022 15
01022022 19
01052022 22
01142022 11
01152022 26
01212022 28
;
run;
/*view dataset*/
proc print data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/date11.jpg"180">
We can use<b> proc contents </b>to view the data type of each variable in the dataset:
<b>/*display data type for each variable*/
proc contents data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/num1.jpg"293">
We can see that <b>day</b> and <b>sales</b> are both numeric variables.
We can use the following code to create a new dataset in which we convert the <b>day</b> variable from numeric to date:
<b>/*create new dataset where 'day' is date*/
data new_data;
    set original_data;
    date_day = input(put(day, 8.), MMDDYY10.);
    format date_day MMDDYY10.;
    drop day;
run;
/*view new dataset*/
proc print data=new_data; </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/convert1.jpg"201">
<b>Note</b>: We used the <b>drop</b> function to drop the original day variable from the dataset.
We can see that the new variable we created, <b>date_day</b>, is in a date format.
Note that <b>MMDDYY10.</b> is only one possible date format that we could have used. You can find a complete list of SAS date formats  here .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 SAS: How to Convert Character Variable to Date 
 SAS: How to Convert Character Variable to Numeric 
 SAS: Convert Strings to Uppercase, Lowercase & Proper Case 
<h2><span class="orange">How to Count Observations by Group in SAS</span></h2>
You can use the following methods to count the total observations by group in SAS:
<b>Method 1: Count Observations by One Group</b>
<b>proc sql;
    select var1, count(*) as total_count
    from my_data
    group by var1;
quit;
</b>
<b>Method 2: Count Observations by Multiple Groups</b>
<b>proc sql;
    select var1, var2, count(*) as total_count
    from my_data
    group by var1, var2;
quit;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input team $ position $ points;
    datalines;
A Guard 15
A Guard 12
A Guard 29
A Forward 13
A Forward 9
A Forward 16
B Guard 25
B Guard 20
C Guard 34
C Forward 19
C Forward 3
C Forward 8
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/count11.jpg"231">
<h3>Example 1: Count Observations by One Group</h3>
The following code shows how to count the total number of observations by team:
<b>/*count observations by team*/
proc sql;
    select team, count(*) as total_count
    from my_data
    group by team;
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/count12.jpg"155">
From the output we can see that team A contains <b>6</b> observations, team B contains <b>2</b> observations, and team C contains <b>4</b> observations.
<h3>Example 2: Count Observations by Multiple Groups</h3>
The following code shows how to count the total number of observations, grouped by team and position:
<b>/*count observations by team and position*/
proc sql;
    select team, position, count(*) as total_count
    from my_data
    group by team, position;
quit;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/count13.jpg"222">
From the output table we can see:
A total of <b>3</b> players belong on team A and have a position of Forward.
A total of <b>3</b> players belong on team A and have a position of Guard.
A total of <b>2</b> players belong on team B and have a position of Guard.
A total of <b>3</b> players belong on team C and have a position of Forward.
A total of <b>1</b> player belongs on team A and has a position of Guard.
<h2><span class="orange">How to Count Distinct Values in SAS (With Examples)</span></h2>
You can use the following methods to count distinct values in a dataset in SAS:
<b>Method 1: Count Distinct Values in One Column</b>
<b>proc sql;
    select count(distinct var1) as distinct_var1
    from my_data;
quit;
</b>
<b>Method 2: Count Distinct Values by Group</b>
<b>proc sql;
    select var1, count(distinct var2) as distinct_var2
    from my_data
    group by var1;
quit;</b>
The following examples show how to use each method in practice with the following dataset:
<b>/*create dataset*/
data my_data;
    input team $ points;
    datalines;
Mavs 10
Mavs 13
Mavs 13
Mavs 15
Mavs 15
Rockets 9
Rockets 10
Rockets 10
Spurs 18
Spurs 19
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/distinct1.jpg"173">
<h3>Example 1: Count Distinct Values in One Column</h3>
The following code shows how to count the total distinct values in the team column:
<b>/*count distinct values in team column*/
proc sql;
    select count(distinct team) as distinct_teams
    from my_data;
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/distinct2.jpg"133">
From the output we can see that there are <b>3</b> distinct values in the team column.
We can confirm this manually by observing that there are three different teams: Mavs, Rockets, and Spurs.
<h3>Example 2: Count Distinct Values by Group</h3>
The following code shows how to count the distinct values in the <b>points</b> column, grouped by the <b>team</b> column:
<b>/*count distinct values in points column, grouped by team*/
proc sql;
    select team, count(distinct points) as distinct_points
    from my_data
    group by team;
quit;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/distinct3.jpg">
The resulting table shows the number of distinct values in the points column, grouped by each of the teams.
<h2><span class="orange">How to Count Missing Values in SAS (With Examples)</span></h2>
You can use the following methods to count the number of missing values in SAS:
<b>Method 1: Count Missing Values for Numeric Variables</b>
<b>proc means data=my_data
    NMISS;
run;</b>
<b>Method 2: Count Missing values for Character Variables</b>
<b>proc sql; 
    select nmiss(char1) as char1_miss, nmiss(char2) as char2_miss
    from my_data;
quit;</b>
The following examples show how to use each method in practice with the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input team $ pos $ rebounds assists;
    datalines;
A G 10 8
B F 4 .
. F 7 10
D C . 14
E F . 10
F G 12 7
G C . 11
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/countmiss1.jpg"263">
<h3>Example 1: Count Missing Values for Numeric Variables</h3>
We can use the following code to count the number of missing values for each of the numeric variables in the dataset:
<b>/*count missing values for each numeric variable*/
proc means data=my_data
    NMISS;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/countmiss2.jpg"178">
From the output we can see:
There are 3 total missing values in the <b>rebounds</b> column.
There is 1 total missing value in the <b>assists</b> column.
<h3>Example 2: Count Missing Values for Character Variables</h3>
We can use the following code to count the number of missing values for each of the character variables in the dataset:
<b>/*count missing for each character variable*/
proc sql; 
    select nmiss(team) as team_miss, nmiss(pos) as pos_miss
    from my_data; 
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/countmiss3.jpg"202">
From the output we can see:
There is 1 missing value in the <b>team </b>column.
There are 0 missing values in the <b>pos </b>column.
<b>Note</b>: You can find the complete documentation for the <b>NMISS</b> function  here .
<h2><span class="orange">How to Create an Empty Dataset in SAS</span></h2>
There are two common ways to create an empty dataset in SAS:
<b>Method 1: Create Empty Dataset from Scratch</b>
<b>data empty_data;
attrib 
    var1 length=8 format=best12. label="var1"
    var2 length=$30 format=$30. label="var2"
    var3 length=8 format=best12. label="var3"
stop;
run;
</b>
<b>Method 2: Create Empty Dataset from Existing Dataset</b>
<b>data empty_data;
    set existing_data;
    stop;
run;
</b>
In both methods, the <b>stop</b> statement prevents SAS from actually processing any rows.
This results in an empty dataset with variable names but no rows.
The following examples show how to use each method in practice.
<h2>Example 1: Create Empty Dataset from Scratch</h2>
We can use the following code to create an empty dataset called <b>empty_data</b> that contains four variables:
<b>/*create empty dataset*/
data empty_data;
    attrib 
    employee_ID length=8 format=best12. label="Employee ID"
    employee_Name length=$30 format=$30. label="Employee Name"
    sales length=8 format=best12. label="Sales"
    sales_date length=8 format=date9. label="Sales Date";
    stop;
run;</b>
We can then use<b> proc contents</b> to view the contents of the dataset:
<b>/*view contents of dataset*/
proc contents data=empty_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/empty1.jpg"637">
From the output we can see that the dataset has four variables but zero observations, i.e. zero rows.
At the bottom of the output we can also see the names of the four variables we created:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/empty2.jpg"399">
<h2>Example 2: Create Empty Dataset from Existing Dataset</h2>
We can use the following code to create an empty dataset called <b>empty_data</b> that is generated from an existing dataset called  Comet , which is a dataset built into SAS:
<b>/*create empty dataset from existing dataset*/
data empty_dat;
    set sashelp.Comet;
    stop;
run;</b>
We can then use<b> proc contents</b> to view the contents of the dataset:
<b>/*view contents of dataset*/
proc contents data=empty_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/empty3.jpg"644">
From the output we can see that the dataset has four variables but zero observations.
At the bottom of the output we can also see the names of the four variables created from the existing dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/empty4.jpg"492">
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Use Proc Summary in SAS 
 How to Rename Variables in SAS 
 How to Create New Variables in SAS 
<h2><span class="orange">How to Create New Variables in SAS (With Examples)</span></h2>
Here are the two most common ways to create new variables in SAS:
<b>Method 1: Create Variables from Scratch</b>
<b>data original_data;
    input var1 $ var2 var3;
    datalines;
A 12 6
B 19 5
C 23 4
D 40 4
;
run;
</b>
<b>Method 2: Create Variables from Existing Variables</b>
<b>data new_data;
    set original_data;
    new_var4 = var2 / 5;
    new_var5 = (var2 + var3) * 2;
run;</b>
The following examples show how to use each method in practice.
<b>Related:</b>  How to Rename Variables in SAS 
<h3>Example 1: <b>Create Variables from Scratch</b></h3>
The following code shows how to create a dataset with three variables: team, points, and rebounds:
<b>/*create dataset*/
data original_data;
    input team $ points rebounds;
    datalines;
Warriors 25 8
Wizards 18 12
Rockets 22 6
Celtics 24 11
Thunder 27 14
Spurs 33 19
Nets 31 20
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/create1.jpg"265">
Note that you can simply list the variable names after the <b>input</b> function and you can create their values from scratch after the <b>datalines</b> function. 
<b>Note</b>: SAS assumes each new variable is numeric. To create a character variable, simply type a dollar sign “<b>$</b>” after the variable name like we did for the <b>team</b> variable in this example.
<h3>Example 2: Create Variables from Existing Variables</h3>
The following code shows how to use the <b>set</b> function to create a new dataset whose variables are created from existing variables in another dataset:
<b>/*create new dataset*/
data new_data;
    set original_data;
    half_points = points / 2;
    avg_pts_rebs = (points + rebounds) / 2;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/create2.jpg"455">
We created the new variables <b>half_points</b> and <b>avg_pts_rebs</b> using variables that already existed in our original dataset.
<h2><span class="orange">How to Calculate a Cumulative Sum in SAS (With Example)</span></h2>
You can use the following basic syntax to calculate a cumulative sum in SAS:
<b>data new_data;
    set original_data;
    retain cum_sum;
    cum_sum+sales;
run;
</b>
This particular syntax creates a new dataset called <b>new_data</b> that contains a new column called <b>cum_sum</b> that contains the cumulative values of the column called <b>sales</b>.
The following example shows how to use this syntax in practice.
<h2>Example: Calculate a Cumulative Sum in SAS</h2>
Suppose we have the following dataset in SAS that shows the number of sales made by some store during 10 consecutive days:
<b>/*create dataset*/
data original_data;
    input day sales;
    datalines;
1 7
2 12
3 14
4 12
5 16
6 18
7 11
8 10
9 14
10 17
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/cumulative11.jpg"146">
The following code shows how to create a new dataset that calculates the cumulative sum of values in the sales column:
<b>/*calculate cumulative sum of sales*/
data new_data;
    set original_data;
    retain cum_sum;
    cum_sum+sales;
run;
/*view results*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/cumulative12.jpg"220">
The new column called <b>cum_sum</b> contains the cumulative sum of values in the <b>sales</b> column.
For example:
Cumulative Sum on Day 1: <b>7</b>
Cumulative Sum on Day 2: 7 + 12 = <b>19</b>
Cumulative Sum on Day 3: 7 + 12 + 14 = <b>33</b>
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Calculate the Sum by Group in SAS 
 How to Calculate the Mean by Group in SAS 
 How to Calculate a Moving Average in SAS 
<h2><span class="orange">SAS: How to Use Datalines Statement to Create a Dataset</span></h2>
You can use the <b>datalines</b> statement in SAS to quickly create a new dataset from scratch.
You can use the following basic syntax to do so:
<b>data original_data;
    input var1 $ var2;
    datalines;
A 12
B 19
C 23
D 40
;
run;
</b>
Here’s what each statement does:
<b> data</b>: The name of the dataset
<b>input</b>: The name and type of each variable in the dataset
<b>datalines</b>: The actual values in the dataset
Note that a dollar sign “<b>$</b>” following a variable name tells SAS that the variable is a character variable.
The following examples show how to use the <b>datalines</b> statement in practice.
<h3>Example 1: <b>Create Dataset with All Numeric Variables</b></h3>
The following code shows how to create a dataset with three numeric variables: points, assists, and rebounds:
<b>/*create dataset*/
data original_data;
    input points assists rebounds;
    datalines;
22 8 4
29 5 4
31 12 8
30 9 14
22 7 1
24 9 2
18 6 4
20 5 5
25 1 4
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datalines1.jpg"243">
The result is a dataset with three numeric variables.
<h3>
<b>Example 2: <b>Create Dataset with Character & Numeric Variables</b></b>
</h3>
The following code shows how to create a dataset with both character and numeric variables:
<b>/*create dataset*/
data original_data;
    input team $ position $ points assists;
    datalines;
A Guard 8 4
A Guard 5 4
A Forward 12 8
A Forward 9 14
A Forward 7 1
B Guard 9 2
B Guard 14 9
B Forward 15 8
B Forward 11 4
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datalines2.jpg"285">
We can use the <b>proc contents</b> function to check the type of each variable:
<b>proc contents data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datalines3.jpg"299">
From the output we can see that <b>team</b> and <b>position</b> are character variables while <b>points</b> and <b>assists</b> are numeric variables.
<h2><span class="orange">How to Use DAY, MONTH, and YEAR Functions in SAS</span></h2>
You can use the <b>DAY</b>, <b>MONTH</b>, and <b>YEAR</b> functions in SAS to extract the day, month, and year as numeric values from a date variable.
The following examples show how to use these functions in practice.
<h3>Example 1: Extract Day, Month, Year from Date in SAS</h3>
Suppose we have the following dataset in SAS that shows the birth date for seven individuals:
<b>/*create dataset*/
data original_data;
    format birth_date date9.;
    input birth_date :date9.;
    datalines;
01JAN2021
22FEB2022
14MAR2022
29MAY2022
14OCT2023
01NOV2024
26DEC2025
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/year1.jpg"139">
We can use the following code to create three new variables that show the day, month, and year of the birth date for each individual:
<b>/*create new dataset*/
data new_data;
    set original_data;
    day = DAY(birth_date);
    month = MONTH(birth_date);
    year = YEAR(birth_date);
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/year2.jpg">
The three new variables show the day, month, and year of the <b>birth_date</b> variable, respectively.
<h3>Example 2: Extract Only Month & Year from Date in SAS</h3>
The following code shows how to create a new variable that displays just the month and year of a date variable in SAS:
<b>/*create new dataset*/
data new_data;
    set original_data;
    month_year = birth_date;
    format month_year mmyyn6.;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/year3.jpg"251">
Notice that the new variable <b>month_year</b> contains only the month and year of the <b>birth_date</b> variable.
If you’d like the month to appear after the year, simply use a format of <b>yymmn6.</b> instead:
<b>/*create new dataset*/
data new_data;
    set original_data;
    month_year = birth_date;
    format month_year yymmn6.;
run;
/*view new dataset*/
proc print data=new_data;</b>
<b><img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/year4.jpg"241"></b>
<h2><span class="orange">How to Calculate Deciles in SAS (With Example)</span></h2>
In statistics, <b>deciles</b> are numbers that split a dataset into ten groups of equal frequency.
The first decile is the point where 10% of all data values lie below it.
The second decile is the point where 20% of all data values lie below it, and so forth.
You can use the following basic syntax to calculate the deciles for a dataset in SAS:
<b>/*calculate decile values for variable called var1*/
proc univariate data=original_data;
    var var1;
    output out=decile_data;
    pctlpts = 10 to 100 by 10
    pctlpre = D_;
run;</b>
<b>Note</b>: The <b>pctlpts</b> statement specifies which deciles to calculate and the <b>pctlpre</b> statement specifies the prefix to use for the deciles in the output.
The following example shows how to use this syntax in practice.
<h2>Example: How to Calculate Deciles in SAS</h2>
Suppose we have the following dataset in SAS that contains two variables:
<b>/*create dataset*/
data original_data;
    input team $ points;
    datalines;
A 12
A 15
A 16
A 21
A 22
A 25
A 29
A 31
B 16
B 22
B 25
B 29
B 30
B 31
B 33
B 38
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/percent0.jpg"143">
The following code shows how to calculate the deciles for the <b>points</b> variable in the dataset
<b>/*calculate decile values for points*/
proc univariate data=original_data;
    var points;
    output out=decile_data
    pctlpts = 10 to 100 by 10
    pctlpre = D_;
run;
/*view deciles for points*/
proc print data=decile_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/decile1.jpg"522">
Here’s how to interpret the output:
The value of the first decile is <b>15</b>.
The value of the second decile is <b>16</b>.
The value of the third decile is <b>21</b>.
The value of the fourth decile is <b>22</b>.
And so on.
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Calculate Percentiles in SAS 
 How to Calculate Quartiles in SAS 
 How to Use Proc Summary in SAS 
<h2><span class="orange">How to Delete Datasets in SAS (3 Examples)</span></h2>
Here are the three most common ways to delete datasets in SAS:
<b>Method 1: Delete One Dataset</b>
<b>proc datasets library=work nolist;
    delete data2;
quit;
</b>
<b>Method 2: Delete Multiple Datasets</b>
<b>proc datasets library=work nolist;
    delete data2 data3;
quit;
</b>
<b>Method 3: Delete All Datasets in Library</b>
<b>proc datasets library=work kill;</b>
The following examples show how to use each method using a <b>WORK</b> library that contains three datasets: <b>data1</b>, <b>data2</b>, and <b>data3</b>.
<h3>Example 1: Delete One Dataset</h3>
We can use the following code to delete only the dataset titled <b>data2</b> in our <b>WORK</b> library:
<b>/*delete data2 from work library*/
proc datasets library=work nolist;
    delete data2;
quit;</b>
We can then use the following code to list all remaining datasets in our <b>WORK</b> library:
<b>proc datasets library=work memtype=data;
run;
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/del1.jpg"655">
We can see that only <b>data1</b> and <b>data3</b> remain in our WORK library. The dataset called <b>data2</b> has been deleted.
<h3>Example 2: Delete Multiple Datasets</h3>
We can use the following code to delete the datasets titled <b>data2</b> and <b>data3</b> in our <b>WORK</b> library:
<b>/*delete data2 from work library*/
proc datasets library=work nolist;
    delete data2 data3;
quit;</b>
We can then use the following code to list all remaining datasets in our <b>WORK</b> library:
<b>/*view all remaining datasets in work library*/
proc datasets library=work memtype=data;
run;
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/del2.jpg"671">
We can see that only <b>data1</b> remains in our WORK library. The datasets <b>data2</b> and <b>data3</b> have been deleted.
<h3>Example 3: Delete All Datasets in Library</h3>
We can use the following code to delete all datasets in our <b>WORK</b> library:
<b>/*delete all datasets from work library*/
proc datasets library=work kill;
</b>
We can then use the following code to list all remaining datasets in our <b>WORK</b> library:
<b>/*view all remaining datasets in work library*/
proc datasets library=work memtype=data;
run;
quit;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/del3.jpg"672">
We can see that there are no remaining datasets in our <b>WORK</b> library since we used the <b>KILL</b> function to delete all datasets.
<h2><span class="orange">How to Delete Rows in SAS (3 Examples)</span></h2>
Here are the three most common ways to delete rows in SAS:
<b>Method 1: Delete Rows Based on One Condition</b>
<b>data new_data;
    set original_data;
    if var1 = "string" then delete;
run;
</b>
<b>Method 2: Delete Rows Based on Several Conditions</b>
<b>data new_data;
    set original_data;
    if var1 = "string" and var2 &lt; 10 then delete;
run;</b>
<b>Method 3: Delete Rows Based on One of Several Conditions</b>
<b>data new_data;
    set original_data;
    if var1 = "string" or var2 &lt; 10 then delete;
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input team $ position $ points;
    datalines;
A Guard 15
A Guard 19
A Guard 22
A Forward 25
A Forward 27
B Guard 11
B Guard 13
B Forward 19
B Forward 22
B Forward 26
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/delete1.jpg"228">
<h3>Example 1: Delete Rows Based on One Condition</h3>
The following code shows how to delete all rows from the dataset where <b>team</b> is equal to “A.”
<b>/*create new dataset*/
data new_data;
    set original_data;
    if team = "A" then delete;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/delete2.jpg"220">
Notice that all rows where <b>team</b> was equal to “A” have been deleted.
<h3>Example 2: Delete Rows Based on Several Conditions</h3>
The following code shows how to delete all rows from the dataset where <b>team</b> is equal to “A” <em>and</em> <b>points</b> is less than 20:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if team = "A" and points &lt; 20 then delete;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/delete3.jpg"222">
Notice that the two rows where <b>team</b> was equal to “A” <em>and</em> <b>points</b> was less than 20 have been deleted.
<h3>Example 3: Delete Rows Based on One of Several Conditions</h3>
The following code shows how to delete all rows from the dataset where <b>team</b> is equal to “A” <i>or </i><b>points</b> is less than 20:
<b>/*create new dataset*/
data new_data;
    set original_data;
    if team = "A" or points &lt; 20 then delete;
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/delete4.jpg"223">
Notice that the eight rows where <b>team</b> was equal to “A” <i>or </i><b>points</b> was less than 20 have been deleted.
<h2><span class="orange">How to Calculate Difference Between Two Dates in SAS</span></h2>
You can use the <b>INTCK </b>function in SAS to quickly calculate the difference between two dates in SAS.
This function uses the following basic syntax:
<b>INTCK(interval, start date, end data, method)</b>
where:
<b>interval</b>: Interval to calculate (day, week, month, year, etc.)
<b>start date</b>: The start date
<b>end date</b>: The end date
<b>method</b>: Whether to count complete intervals (‘D’ = No (Default), ‘C’ = Yes)
The following example shows how to use this function in practice.
<h3>Example : <b>Calculate Difference Between Dates in SAS</b></h3>
 Suppose we have the following dataset in SAS that contains two date variables:
<b>/*create dataset*/
data original_data;
    format start_date end_date date9.;
    input start_date :date9. end_date :date9.;
    datalines;
01JAN2022 09JAN2022
01FEB2022 22FEB2022 
14MAR2022 04APR2022
01MAY2022 14AUG2022
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datediffsas1.jpg"239">
We can use the following code to calculate the difference between the values in the <b>start_date</b> and <b>end_date</b> variables in days, weeks, and months:
<b>/*create new dataset*/
data new_data;
    set original_data;
    days_diff = intck('day', start_date, end_date);
    weeks_diff = intck('weeks', start_date, end_date);
    months_diff = intck('months', start_date, end_date);
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datediffsas2.jpg"469">
The three new variables show the difference between <b>start_date</b> and <b>end_date </b>in days, weeks, and months.
Note that we can use the ‘<b>c</b>‘ argument in the INTCK function to only calculate the difference in complete days, weeks, and months:
<b>/*create new dataset*/
data new_data;
    set original_data;
    days_diff = intck('day', start_date, end_date, 'c');
    weeks_diff = intck('weeks', start_date, end_date, 'c');
    months_diff = intck('months', start_date, end_date, 'c');
run;
/*view new dataset*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/datediffsas3.jpg"492">
Notice the difference between this table and the previous table.
In this table, the difference in weeks between Jan 1st and Jan 9th is calculated as <b>1</b> since only one whole week can fit between these dates.
However, in the previous table the difference in weeks was calculated as <b>2</b> since there were two partial weeks that fit between these two dates.
<h2><span class="orange">The Complete Guide to DO Loops in SAS</span></h2>
A <b>DO loop</b> in SAS can be used to <em>do</em> some action a certain number of times.
There are three basic DO loops in SAS:
<b>1. DO Loop</b>
<b>data data1;
x = 0;
do i = 1 to 10;
   x = i*4;
   output;
end;
run;</b>
<b>What It Does</b>: This loop performs 10 iterations, from i = 1 to 10, where the value in each row is equal to i multiplied by 4.
<b>When It Stops</b>: This loop only stops after 10 iterations have been performed.
<b>2. DO WHILE Loop</b>
<b>data data2;
x = 0;
do i = 1 to 10 while(x &lt; 20);
   x = i*4;
   output;
end;
run;</b>
<b>What It Does</b>: This loop will try to perform 10 iterations, from i = 1 to 10, where the value in each row is equal to i multiplied by 4.
<b>When It Stops</b>: This loop will stop when the value of x exceeds 20 or when 10 iterations have been performed, whichever comes first.
<b>3. DO UNTIL Loop</b>
<b>data data3;
x = 0;
do i = 1 to 10 until(x > 30);
   x = i*4;
   output;
end;
run;</b>
<b>What It Does</b>: This loop will try to perform 10 iterations, from i = 1 to 10, where the value in each row is equal to i multiplied by 4.
<b>When It Stops</b>: This loop will stop when the value of x exceeds 30 or when 10 iterations have been performed, whichever comes first.
The following examples show how to use each DO loop in practice.
<h3>Example 1: DO Loop</h3>
We can use the following <b>DO loop</b> to create a dataset with 10 rows:
<b>/*use DO loop to create dataset*/
data data1;
x = 0;
do i = 1 to 10;
   x = i*4;
   output;
end;
run;
/*view dataset*/
proc print data=data1;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/doloop1.jpg"138">
The result is a dataset that contains 10 rows where the values in column i range from 1 to 10 and the values in column x range from 4 to 40.
Note that you can use <b>drop i</b> to drop the index column from the dataset:
<b>/*use DO loop to create dataset*/
data data1;
x = 0;
do i = 1 to 10;
   x = i*4;
   output;
end;
drop i;
run;
/*view dataset*/
proc print data=data1;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/doloop2.jpg"102">
<h3>Example 2: DO WHILE Loop</h3>
We can use the following <b>DO WHILE  loop </b>to create a dataset with a variable i from i = 1 to 10, where the value in each row is equal to i multiplied by 4 <em>while</em> x is less than 20:
<b>/*use DO WHILE loop to create dataset*/
data data2;
x = 0;
do i = 1 to 10  while(x &lt; 20);
   x = i*4;
   output;
end;
run;
/*view dataset*/
proc print data=data2;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/doloop3.jpg"107">
Notice that the loop stopped creating rows once x reached 20.
<h3>Example 3: DO UNTIL Loop</h3>
We can use the following <b>DO UNTIL loop </b>to create a dataset with a variable i from i = 1 to 10, where the value in each row is equal to i multiplied by 4 <i>until </i>x is greater than 30:
<b>/*use DO UNTIL loop to create dataset*/
data data3;
x = 0;
do i = 1 to 10  until(x > 30);
   x = i*4;
   output;
end;
run;
/*view dataset*/
proc print data=data3;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/doloop4.jpg"104">
Notice that the loop stopped creating rows once x exceeded 30.
<h2><span class="orange">How to Create Dummy Variables in SAS (With Example)</span></h2>
A  dummy variable  is a type of variable that we create in regression analysis so that we can represent a categorical variable as a numerical variable that takes on one of two values: zero or one.
For example, suppose we have the following dataset and we would like to use <em>age</em> and <em>marital status </em>to predict <em>income</em>:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummy4.png">
To use <em>marital status</em> as a predictor variable in a regression model, we must convert it into a dummy variable.
Since it is currently a categorical variable that can take on three different values (“Single”, “Married”, or “Divorced”), we need to create <em>k</em>-1 = 3-1 = 2 dummy variables.
To create this dummy variable, we can let “Single” be our baseline value since it occurs most often. Thus, here’s how we would convert <em>marital status</em> into dummy variables:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/02/dummy6.png">
The following example shows how to create dummy variables for this exact dataset in SAS.
<h3>Example: Creating Dummy Variables in SAS</h3>
First, let’s create the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input income age status $;
    datalines;
45 23 single
48 25 single
54 24 single
57 29 single
65 38 married
69 36 single
78 40 married
83 59 divorced
98 56 divorced
104 64 married
107 53 married
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/dummysas1.jpg"219">
Next, we can use two IF-THEN-ELSE statements to create dummy variables for the <b>status</b> variable:
<b>/*create new dataset with dummy variables*/
data new_data;
set original_data;
if status = "married" then married = 1;
  else married = 0;
if status = "divorced" then divorced = 1;
  else divorced = 0;
run;
/*view new dataset*/
proc print data=new_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/dummysas2.jpg">
Notice that the values for the two dummy variables (<b>married</b> and <b>divorced</b>) match the values we calculated in the introductory example.
We could then use these dummy variables in a  regression model  if we’d like since they’re both numeric.
<h2><span class="orange">How to Export Data from SAS to CSV File (With Examples)</span></h2>
You can use <b>proc export</b> to quickly export data from SAS to a CSV file.
This procedure uses the following basic syntax:
<b>/*export data to file called data.csv*/
proc export data=my_data
    outfile="/home/u13181/data.csv"
    dbms=csv
    replace;
run;</b>
Here’s what each line does:
<b>data</b>: Name of dataset to export
<b>outfile</b>: Location to export CSV file
<b>dmbs</b>: File format to use for export
<b>replace</b>: Replace the file if it already exists
The following examples show how to use this function in practice.
<b>Related:</b>  How to Export Data from SAS to Excel 
<h3>Example 1: Export Dataset to CSV with Default Settings</h3>
Suppose we have the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input A B C;
    datalines;
1 4 76
2 3 49
2 3 85
4 5 88
2 2 90
4 6 78
5 9 80
;
run;
/*view dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/export11.jpg"143">
We can use the following code to export this dataset to a CSV file called <b>data.csv</b>:
<b>/*export dataset*/
proc export data=my_data
    outfile="/home/u13181/data.csv"
    dbms=csv
    replace;
run;</b>
I can then navigate to the location on my computer where I exported the file and view it:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/csv11.jpg"404">
The data in the CSV file matches the dataset from SAS.
<h3>Example 2: Export Dataset to CSV with Custom Settings</h3>
You can also use the <b>delimiter</b> and <b>putnames</b> arguments to change the delimiter that separates the values and remove the header row from the dataset.
For example, the following code shows how to export a SAS dataset to a CSV file using a semi-colon as the delimiter and no header row:
<b>/*export dataset*/
proc export data=my_data
    outfile="/home/u13181/data.csv"
    dbms=csv
    replace;
    delimiter=";";
    putnames=NO;
run;</b>
I can then navigate to the location on my computer where I exported the file and view it:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/csv12.jpg"422">
Notice that the header row has been removed and the values are separated by semi-colons instead of commas.
<h2><span class="orange">How to Export Data from SAS to Excel (With Examples)</span></h2>
You can use <b>proc export</b> to quickly export data from SAS to an Excel file.
This procedure uses the following basic syntax:
<b>/*export data to file called my_data.xlsx*/
proc export data=my_data
    outfile="/home/u13181/my_data.xlsx"
    dbms=xlsx
    replace;
    sheet="First Data";
run;</b>
Here’s what each line does:
<b>data</b>: Name of dataset to export
<b>outfile</b>: Location to export Excel file
<b>dmbs</b>: File format to use for export
<b>replace</b>: Replace the file if it already exists
<b>sheet</b>: Name to display on sheet in Excel workbook
The following examples show how to use this function in practice.
<h3>Example 1: Export One Dataset to One Excel Sheet</h3>
Suppose we have the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input A B C;
    datalines;
1 4 76
2 3 49
2 3 85
4 5 88
2 2 90
4 6 78
5 9 80
;
run;
/*view dataset*/
proc print data=my_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/export11.jpg"143">
We can use the following code to export this dataset to an Excel file called <b>my_data.xlsx</b>:
<b>/*export dataset*/
proc export data=my_data
    outfile="/home/u13181/my_data.xlsx"
    dbms=xlsx
    replace;
    sheet="First Data";
run;</b>
I can then navigate to the location on my computer where I exported the file and view it in Excel:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/export21.jpg"462">
The data in Excel matches the dataset from SAS and the sheet in the Excel workbook is called “First Data” just like I specified in the <b>proc export</b> statement.
<h3>Example 2: Export Multiple Datasets to Multiple Excel Sheets</h3>
Suppose we have two datasets in SAS:
<b>/*create first dataset*/
data my_data;
    input A B C;
    datalines;
1 4 76
2 3 49
2 3 85
4 5 88
2 2 90
4 6 78
5 9 80
;
run;
/*create second dataset*/
data my_data2;
    input D E F;
    datalines;
1 4 90
2 3 49
2 3 85
4 5 88
2 1 90
;
run;</b>
We can use the following code to export both datasets to the same Excel file in different sheets:
<b>/*export first dataset to first sheet in Excel*/
proc export data=my_data
    outfile="/home/u13181/my_data.xlsx"
    dbms=xlsx
    replace;
    sheet="First Data";
run;
/*export second dataset to second sheet in Excel*/
proc export data=my_data2
    outfile="/home/u13181/my_data.xlsx"
    dbms=xlsx
    replace;
    sheet="Second Data";
run;</b>
I can then navigate to the location on my computer where I exported the file and view it in Excel.
The first sheet titled “First Data” contains the first dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/export31.jpg"474">
And the second sheet titled “Second Data” contains the second dataset:
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/export41.jpg"479">
<h2><span class="orange">How to Extract Numbers from String in SAS</span></h2>
The easiest way to extract numbers from a string in SAS is to use the  COMPRESS  function with the ‘A’ modifier.
This function uses the following basic syntax:
<b>data new_data;
    set original_data;
    numbers_only = compress(some_string, '', 'A');
run;
</b>
The following example shows how to use this syntax in practice.
<h2>Example: Extract Numbers from String in SAS</h2>
Suppose we have the following dataset in SAS that shows the names of various college courses:
<b>/*create dataset*/
data original_data;
    input course $12.;
    datalines;
Stats101
Economics203
Business201
Botany411
Calculus101
English201
Chemistry402
Physics102
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/extract1.jpg"159">
We can use the following code to extract only the numbers from each course name:
<b>/*extract numbers from course column*/
data new_data;
    set original_data;
    course_number_only = compress(course, '', 'A');
run;
/*view results*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/extract2.jpg"307">
Notice that the new column called <b>course_number_only</b> contains only the numbers from the strings in the <b>course</b> column.
If you would instead like to only extract the characters in each string, you can use the <b>COMPRESS</b> function with the ‘d’ modifier instead:
<b>/*extract characters from course column*/
data new_data;
    set original_data;
    course_characters_only = compress(course, '', 'd');
run;
/*view results*/
proc print data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/09/extract3.jpg"336">
Notice that the new column called <b>course_characters_only</b> contains only the numbers from the strings in the <b>course</b> column.
<b>Note</b>: You can find a complete list of modifiers for the <b>COMPRESS</b> function on this  SAS documentation page .
<h2>Additional Resources</h2>
The following tutorials explain how to perform other common tasks in SAS:
 How to Normalize Data in SAS 
 How to Identify Outliers in SAS 
 How to Create Frequency Tables in SAS 
<h2><span class="orange">How to Use the FIND Function in SAS (With Examples)</span></h2>
You can use the <b>FIND</b> function in SAS to find the position of the first occurrence of some substring within a string.
Here are the two most common ways to use this function:
<b>Method 1: Find Position of First Occurrence of String</b>
<b>data new_data;
    set original_data;
    first_occurrence = find(variable_name, "string");
run;
</b>
<b>Method 2: Find Position of First Occurrence of String (Ignoring Case)</b>
<b>data new_data;
    set original_data;
    first_occurrence = find(variable_name, "string", "i");
run;</b>
The “i” argument tells SAS to ignore the case when searching for the substring.
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data original_data;
    input phrase $1-25;
    datalines;
The fox ran fast
That is a quick FOX
This fox is a slow fox
The zebra is cool
;
run;
/*view dataset*/
proc print data=original_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/findsas1.jpg"198">
<h3>Example 1: <b>Find Position of First Occurrence of String</b></h3>
The following code shows how to find the position of the first occurrence of “fox” in each string:
<b>data new_data;
    set original_data;
    first_fox = find(phrase, "fox");
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/findsas2.jpg"256">
Here’s how to interpret the output:
The <b>f</b>ox ran fast (First occurrence is at position <b>5</b>)
That is a quick FOX (The lowercase string “fox” never occurs)
This <b>f</b>ox is a slow fox (First occurrence is at position <b>6</b>)
The zebra is cool (The string “fox” never occurs)
<h3>Example 2: <b>Find Position of First Occurrence of String (Ignoring Case)</b></h3>
The following code shows how to find the position of the first case-insensitive occurrence of “fox” in each string:
<b>data new_data;
    set original_data;
    first_fox = find(phrase, "fox", "i");
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/findsas3.jpg"264">
Here’s how to interpret the output:
The <b>f</b>ox ran fast (First occurrence is at position <b>5</b>)
That is a quick <b>F</b>OX (First occurrence of “fox” is at position <b>17</b>)
This <b>f</b>ox is a slow fox (First occurrence is at position <b>6</b>)
The zebra is cool (The string “fox” never occurs)
<h2><span class="orange">How to Use FIRST. and LAST. Variables in SAS</span></h2>
You can use the <b>FIRST.</b> and <b>LAST.</b> functions in SAS to identify the first and last observations by group in a SAS dataset.
Here is what each function does in a nutshell:
<b>FIRST.variable_name</b> assigns a value of 1 to the <b>first</b> observation in a group and a value of 0 to every other observation in the group.
<b>LAST.variable_name</b> assigns a value of 1 to the <b>last </b>observation in a group and a value of 0 to every other observation in the group.
The following examples show how to use each function in practice with the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input team $ points rebounds;
    datalines;
Mavs 29 10
Mavs 13 6
Mavs 22 5
Mavs 20 9
Spurs 13 9
Spurs 15 10
Spurs 33 8
Spurs 27 11
Rockets 25 8
Rockets 14 4
Rockets 16 7
Rockets 12 4
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/firstlast1.jpg"249">
<h3>Example 1: <b>How to Use FIRST. in SAS</b></h3>
We can use the following <b>FIRST.</b> function in SAS to assign a value of <b>1</b> to the first observation for each team in the dataset:
<b>/*sort dataset by team*/
proc sort data=my_data;
    by team;
run;
/*create new dataset that labels first row for each team*/
data first_team;
    set my_data;
    by team;
    first_team=first.team;
run;
/*view dataset*/
proc print data=first_team;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/first1.jpg"334">
Notice that the <b>first_team</b> column assigns the first observation for each team a value of <b>1</b>. All other values are assigned a value of <b>0</b>.
You can also use the following code to create a new dataset that only contains the first observation for each team:
<b>/*sort dataset by team*/
proc sort data=my_data;
    by team;
run;
/*create new dataset only contains first row for each team*/
data first_team;
    set my_data;
    by team;
    if first.team;
run;
/*view dataset*/
proc print data=first_team;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/first2.jpg"260">
Notice that the dataset only contains the first observation for each team.
<h3>Example 2: <b>How to Use LAST. in SAS</b></h3>
We can use the following <b>LAST.</b> function in SAS to assign a value of <b>1</b> to the first observation for each team in the dataset:
<b>/*sort dataset by team*/
proc sort data=my_data;
    by team;
run;
/*create new dataset that labels last row for each team*/
data last_team;
    set my_data;
    by team;
    last_team=last.team;
run;
/*view dataset*/
proc print data=last_team;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/first3.jpg"333">
Notice that the <b>last_team</b> column assigns the last observation for each team a value of <b>1</b>. All other values are assigned a value of <b>0</b>.
You can also use the following code to create a new dataset that only contains the last observation for each team:
<b>/*sort dataset by team*/
proc sort data=my_data;
    by team;
run;
/*create new dataset only contains last row for each team*/
data last_team;
    set my_data;
    by team;
    if last.team;
run;
/*view dataset*/
proc print data=last_team;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/03/first4.jpg"255">
Notice that the dataset only contains the last observation for each team.
<h2><span class="orange">How to Create Frequency Tables in SAS (With Examples)</span></h2>
You can use <b>proc freq </b>in SAS to quickly create frequency tables for one or more variables in a dataset.
The following examples show how to use this procedure with the SAS built-in dataset called  BirthWgt , which contains various characteristics for 100,000 mothers that recently gave birth.
We can use <b>proc print</b> to view the first 10 observations from this dataset:
<b>/*view first 10 observations from <i>BirthWgt </i>dataset*/
proc print data=sashelp.BirthWgt (obs=10);
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/weight1.jpg"563">
<h3>Example 1: Frequency Table for One Variable</h3>
We can use the following code to create a frequency table for the Race variable:
<b>/*create frequency table for Race variable*/
proc freq data=sashelp.BirthWgt;
tables Race;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/weight2.jpg">
The output table contains four columns:
<b>Frequency</b>: The total number of observations that fell in a certain category.
<b>Percent</b>: The percentage of total observations that fell in a certain category.
<b>Cumulative Frequency</b>: The total number of observations that have been accounted for up to and including the current row.
<b>Cumulative Percent</b>: The cumulative percentage of total observations that have been accounted for up to and including the current row.
For example, from the output table we can see:
The total number of Hispanic mothers was <b>22,139</b>.
The percentage of total mothers who were Hispanic was <b>22.14%</b>.
The total number of mothers who were Asian, Black, or Hispanic was <b>41,496</b>.
The cumulative percentage of mothers who were Asian, Black, or Hispanic was <b>41.50%</b>.
<h3>Example 2: Frequency Table for One Variable (Sorted)</h3>
By default, frequency tables are sorted in alphabetical order based on the category names. However, we can use the <b>order</b> function to  sort the categories by frequency instead:
<b>/*create frequency table for Race variable, sorted by frequency*/
proc freq data=sashelp.BirthWgt order=freq;
tables Race;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/weight3.jpg"420">
Notice that the categories are now sorted based on frequency from highest to lowest.
<h3>Example 3: Frequency Table for One Variable (Include Missing Values)</h3>
By default, missing values are not included in frequency tables.
However, we can use the <b>missing</b> command to tell SAS to include a row to count the frequency of missing values:
<b>/*create frequency table for Race variable, sorted by frequency*/
proc freq data=sashelp.BirthWgt order=freq;
tables Race / missing;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/weight3.jpg"420">
Since no additional row was added to the frequency table, this tells us that there were no missing values for Race in the original dataset.
<h3>Example 4: Frequency Table for Multiple Variables</h3>
To create a frequency table for multiple variables at once, we can simply include multiple variable names in the <b>tables</b> argument.
For example, we can use the following code to create a frequency table for both Race and AgeGroup:
<b>/*create frequency table for Race and AgeGroup variables, both sorted by frequency*/
proc freq data=sashelp.BirthWgt order=freq;
tables Race AgeGroup;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2021/12/weight4.jpg"413">
We can see that a frequency table was created for both variables.
<h2><span class="orange">SAS: How to Specify Number of Bins in Histogram</span></h2>
You can use the <b>midpoints</b> statement to specify the number of bins that should be used in a histogram in SAS.
This statement uses the following basic syntax:
<b>proc univariate data=my_data;
    histogram my_variable / midpoints=(9 to 36 by 3);
run;</b>
This particular example creates a histogram with midpoints ranging from <b>9</b> to <b>36</b> at intervals of <b>3</b>.
The following example shows how to use this syntax in practice.
<h2>Example: How to Specify Number of Bins in Histogram in SAS</h2>
Suppose we have the following dataset in SAS that contains information about various basketball players:
<b>/*create dataset*/
data my_data;
    input team $ points rebounds;
    datalines;
A 29 8
A 23 6
A 20 6
A 21 9
A 33 14
A 35 11
A 31 10
B 21 9
B 14 5
B 15 7
B 11 10
B 12 6
B 10 8
B 15 10
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist1.jpg"208">
We can use the following syntax to create a histogram for the <b>points</b> variable:
<b>/*create histogram for points variable*/
proc univariate data=my_data;
    histogram points;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist2.jpg"568">
The x-axis displays the values for the <b>points</b> variable and the y-axis displays the percentage of observations in the dataset that fall into various values.
Notice that the midpoints in the histogram occur at intervals of <b>6</b>.
To increase the number of bins in the histogram, we can specify the midpoints to occur at intervals of <b>3</b> instead:
<b>/*create histogram for points variable with custom bins*/
proc univariate data=my_data;
    histogram points / midpoints=(9 to 36 by 3);
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/bins1.jpg">
Notice that this histogram has more total bins than the previous histogram since we made the intervals between the midpoints smaller.
To decrease the number of bins in the histogram, we can specify the midpoints to occur at intervals of <b>9</b> instead:
<b>/*create histogram for points variable with custom bins*/
proc univariate data=my_data;
    histogram points / midpoints=(9 to 36 by 9);
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/11/bins2.jpg"601">
Notice that this histogram has fewer total bins than the previous histogram since we made the intervals between the midpoints larger.
Feel free to play around with the values in the <b>midpoints</b> statement to increase or decrease the number of bins in your histogram.
<h2>Additional Resources</h2>
The following tutorials explain how to create other charts in SAS:
 How to Create Line Plots in SAS 
 How to Create Boxplots by Group in SAS 
 How to Create a Scatterplot with Regression Line in SAS 
<h2><span class="orange">How to Create Histograms in SAS (3 Examples)</span></h2>
You can use the following methods to create one or more histograms in SAS:
<b>Method 1: Create One Histogram</b>
<b>proc univariate data=my_data;
    var var1;
    histogram var1;
run;
</b>
<b>Method 2: Create Panel of Histograms</b>
<b>proc univariate data=my_data;
    class var2;
    var var1;
    histogram var1;
run;</b>
<b>Method 3: Overlay Histograms</b>
<b>proc univariate data=my_data;
    class var2;
    var var1;
    histogram var1 / overlay;
run;</b>
The following examples show how to use each method with the following dataset in SAS:
<b>/*create dataset*/
data my_data;
    input team $ points rebounds;
    datalines;
A 29 8
A 23 6
A 20 6
A 21 9
A 33 14
A 35 11
A 31 10
B 21 9
B 14 5
B 15 7
B 11 10
B 12 6
B 10 8
B 15 10
;
run;
/*view dataset*/
proc print data=my_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist1.jpg"208">
<h3>Example 1: <b>Create One Histogram</b></h3>
The following code shows how to create one histogram for the <b>points</b> variable:
<b>/*create histogram for points variable*/
proc univariate data=my_data;
    var points;
    histogram points;
run;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist2.jpg"568">
The x-axis displays the values for the <b>points</b> variable and the y-axis displays the percentage of observations in the dataset that fall into various values.
<h3>Example 2: <b>Create Panel of Histograms</b></h3>
The following code shows how to create a panel of histograms to visualize the distribution of values for the <b>points</b> variable grouped by the <b>team</b> variable:
<b>/*create histogram for points variable*/
proc univariate data=my_data;
    class team;
    var points;
    histogram points;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist3.jpg"584">
This plot allows us to quickly see that the players on team A tend to score more points than the players on team B.
Notice that the two histograms share an x-axis, which makes it easy to compare the points values between the two teams.
<h3>Method 3: Overlay Histograms</h3>
The following code shows how to overlay multiple histograms in one plot:
<b>/*create histogram for points variable*/
proc univariate data=my_data;
    class team;
    var points;
    histogram points / overlay;
run;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/hist4.jpg"590">
This type of plot is useful when you want to visualize multiple histograms in a single chart.
<h2><span class="orange">SAS: How to Convert Character Variable to Numeric</span></h2>
You can use the <b>input() </b>function in SAS to convert a character variable to a numeric variable.
This function uses the following basic syntax:
<b>numeric_var = input(character_var, comma9.);
</b>
The following example shows how to use this function in practice.
<b>Related:</b>  How to Convert Numeric Variable to Character in SAS 
<h3>Example: Convert Character Variable to Numeric in SAS</h3>
Suppose we have the following dataset in SAS that shows the total sales made by some store during 10 consecutive days:
<b>/*create dataset*/
data original_data;
    input day $ sales;
    datalines;
1 7
2 12
3 15
4 14
5 13
6 11
7 10
8 16
9 18
10 24
;
run;
/*view dataset*/
proc print data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/lineplot1.jpg"132">
We can use<b> proc contents </b>to view the data type of each variable in the dataset:
<b>/*display data type for each variable*/
proc contents data=original_data;
</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/char1.jpg"291">
We can see that <b>day</b> is a character variable and <b>sales</b> is a numeric variable.
We can use the following code to create a new dataset in which we convert the <b>day</b> variable from character to numeric:
<b>/*create new dataset where 'day' is numeric*/
data new_data;
    set original_data;
    numeric_day = input(day, comma9.);
    drop day;
run;
/*view new dataset*/
proc print data=new_data; </b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/char2.jpg"188">
<b>Note</b>: We used the <b>drop</b> function to drop the original day variable from the dataset.
We can use <b>proc contents</b> once again to check the data type of each variable in the new dataset:
<b>/*display data type for each variable in new dataset*/
proc contents data=new_data;</b>
<img class="lazy" data-src="https://www.statology.org/wp-content/uploads/2022/01/char3.jpg"304">
We can see that the new variable we created, <b>numeric_day</b>, is a numeric variable.

<script src='https://williamkpchan.github.io/LibDocs/readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
