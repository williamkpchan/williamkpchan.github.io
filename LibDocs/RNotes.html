<base target="_blank"><html><head><title>R Notes</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="stylesheet" href="https://williamkpchan.github.io/maincss.css">
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.5/jquery.js"></script>
<script src="../lazyload.min.js"></script>
<script type='text/javascript' src='../mainscript.js'></script>
<script src="D:/Dropbox/Public/commonfunctions.js"></script>
<script>
  var showTopicNumber = true;
  var bookid = "R Notes"
</script>
<style>
body{width:80%;margin-left: 10%; font-size:22px;}
h1, h2 {color: gold;}
strong {color: orange;}
img {max-width:90%; display: inline-block; margin-top: 2%;margin-bottom: 1%; border-radius:3px; background-color:#044;}
</style></head><body onkeypress="chkKey()"><center>
<h1>R Notes</h1>
<a href="#mustWatch" class="red goldbs" target="_self">Must Watch!</a>
<br><br>
<div id="toc"></div></center>
<br><br>
<div id="mustWatch"><center><span class="red">MustWatch</span></center><br>
<li>data visualization (ggplot2)</li>
<li>data manipulation (dplyr, lubridate, tidyr, stringr, readr, & forcats)</li>
<li>data analysis (combine ggplot2, dplyr to explore data and find insights)</li>
<a href="https://beckmw.wordpress.com/" class="whitebut ">R is my friend</a>

<pre>
<a href="R4DataScience.html" class="whitebut ">R4DataScience</a>

<a href="https://statisticsglobe.com/r-functions-list/" class="whitebut ">R Basic Commands of the</a>

<a href="Non-standard evaluation.html" class="whitebut ">Non-standard evaluation</a>
<a href="https://statisticsglobe.com" class="whitebut ">Statistics Globe</a>
<a href="http://www.datasciencemadesimple.com" class="whitebut ">DataScience Made Simple</a>

<a href="Libraries for Python & R.html">Libraries for Python & R</a>
<a href="sparklyr.html" class="redbut red blueblackgrad">sparklyr</a>

functions on non-tabular data
rlist is a set of tools for working with list objects.
<a href="RList Turorial.html" class="yellowbut gold purpleblackgrad">RList Turorial</a>
</pre>
<br>

</div>
<pre>
<br>
<br>
<br>
<br>
<a href="Libraries for Python & R.html">Libraries for Python & R</a>

<br>
<h2>free books for R</h2>
<a href="http://www.cookbook-r.com/" target="_blank">Cookbook for R</a>
<a href="RCookbook.html" target="_blank" class="orangesha">&diams;RCookbook</a>
<a href="https://bookdown.org" class="whitebut " target="_blank">bookdown R books</a>
<a href="https://bookdown.org/home/archive/" class="whitebut " target="_blank">bookdown all books</a>
<a href="https://bookdown.org/home/tags/r-programming/" class="whitebut " target="_blank">bookdown r-programming books</a>
<a href="https://bookdown.org/rdpeng/rprogdatascience/" class="whitebut " target="_blank">R Programming for Data Science</a>


<br>
<h2>Data Frame</h2>

Data Frame is a list of vectors of equal length

to create a dataframe:
n = c(2,3,5)
s = c('a','b','c')
b = c(TRUE, FALSE, FALSE)
df = data.frame(n,s,b)

Components of dataframe:
header, column names, data row, name of the row cell
single square bracket "[]", comma

Functions:
nrow(), ncol(), head()

Inport Data:
read.table("mydata.txt")
read.csv("mydata.csv")

retrieve the column vector by the double square bracket or the "$" operator
mtcars[[9]]
mtcars[["am"]]
mtcars$am
mtcars[,"am"]


retrieve a column slice with the single square bracket "[]"
mtcars[1]
mtcars["mpg"]
mtcars[c("mpg", "hp")]

Data frame Row Slice
mtcars[24,]
mtcars[c(3,24),]
mtcars["camaro z28",]
mtcars[c("datsun 710","camaro z28"),]

<h2># MLFundStat and Hangseng Fund Stat</h2>
#=================
MLFundStat.html
the computation is long, it is possible to cut time by adjusting the cutdate variable.
this should be modified to new version using r chart.

<h2># Start Of R</h2>
#=================
Sys.setlocale(category = 'LC_ALL', 'Chinese')

use the .Rprofile.site file to run R commands for all users when their R session starts.
D:\R-3.5.1\etc\Rprofile.site
See: Initialization at startup.

This command could be an environment set:
Sys.setenv(FAME="/opt/fame")

<a href="https://www.rdocumentation.org/packages/base/versions/3.5.0/topics/Startup">Start Of R Initialization</a>

<h2># Encoding Problems</h2>
<code>To write text UTF8 encoding on Windows</code>
Firstly, set encoding
options(encoding = "UTF-8")

To write text UTF8 encoding on Windows one has to use the <b class="gold embossts redbs borRad10">useBytes=T</b> options in functions like writeLines or readLines:

txt &lt;- "在"
<code>writeLines(txt, "test.txt", useBytes=T)

readLines("test.txt", encoding="UTF-8")</code>
[1] "在"

<code>writeLines(wholePage, theFilename, useBytes=T)</code>
The UTF-8 BOM is a sequence of bytes at the start of a text stream
(0xEF, 0xBB, 0xBF) that allows the reader to more reliably guess a file as being encoded in UTF-8.

Normally, the BOM is used to signal the endianness of an encoding, but since endianness is irrelevant to UTF-8, the BOM is unnecessary.

According to the Unicode standard, the BOM for UTF-8 files is not recommended

#=================
# Encoding Problems
Sys.getlocale()
getOption("encoding")
options(encoding = "UTF-8")
Encoding(txtstring) &lt;- "UTF-8"
Encoding(txtstring)
txtstring
Sys.setlocale
Sys.setlocale(category = 'LC_ALL', 'Chinese')
Sys.setlocale(category = "LC_ALL", locale = "chs") 
Sys.setlocale(category = "LC_ALL", locale = "cht") # fanti

Note: 
default: options("encoding" = "native.enc")
statTxtFile = "test.txt"
write("建设银行", statTxtFile, append=TRUE)
result file is ansi

add:
options("encoding" = "UTF-8")
write("建设银行", statTxtFile, append=TRUE)
result file is utf-8

mytext &lt;- "this is my text"
Encoding(mytext)

options(encoding = "UTF-8")
getOption("encoding")

options(encoding='native.enc')
getOption("encoding")


iconvlist()
theHeader = "http://qt.gtimg.cn/r=2&q=r_hk"
onecode = "02009"
con = url(paste0(theHeader,onecode), encoding = "GB2312")
thepage=readLines(con)
close(con)
Info=unlist(strsplit(thepage,"~"))
codename=Info[2]
codename
Encoding(codename)

==================
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
readLines(filename, encoding="UTF-8")
readLines(con = stdin(), n = -1L, ok = TRUE, warn = TRUE, encoding = "unknown", skipNul = FALSE)

<span class="redword"># note! the chiname encoding is ok inside R, but will be wrong when write to file by local pc locale, to solve the problem, set Sys.setlocale(category = 'LC_ALL', 'Chinese') </span>

readLines(con &lt;- file("Unicode.txt", encoding = "UCS-2LE"))
close(con)
unique(Encoding(A)) # will most likely be UTF-8
==================
guess_encoding(pageHeader)
pageHeader = repair_encoding(pageHeader, from="utf-8")
pageHeader = repair_encoding(pageHeader, "UTF-8")

iconv(pageHeader, to="UTF-8")
Encoding(pageHeader) &lt;- "UTF-8"

Sys.getlocale("LC_ALL")
https://rpubs.com/mauriciocramos/encoding
==================

Read text as UTF-8 encoding

the following reads in encoding twice and works but reasons unknown
readLines(textConnection("Z\u00FCrich", encoding="UTF-8"), encoding="UTF-8")
[1] "Zürich"

==================
the page source claim to be using UTF-8 encoding:
meta http-equiv="Content-Type" content="text/html; charset=utf-8"

So, the question is, are they really using a different enough encoding, 
or can we just convert to utf-8, guessing that any errors will be negligible?

A quick and dirty approach just force utf-8 using iconv:

TV_Audio_Video &lt;- read_html(iconv(page_source[[1]], to = "UTF-8"), encoding = "utf8")

In general, this is a bad idea - better to specify the encoding it's from.
In this case, maybe the error is theirs, so this quick and dirty approach might be ok.


<h2>to remove leading zeros</h2>
substr(t,regexpr("[^0]",t),nchar(t))

<h2>Pop up message in windows 8.1</h2>
use the tcl/tk package in R to create a messageBox. 
Here is a very simple example:

require(tcltk)
tkmessageBox(title = "Title of message box",
                       message = "Hello, world!", icon = "info", type = "ok")

library(tcltk)
tk_messageBox(type='ok',message='I am a tkMessageBox!')

different types of messagebox (yesno, okcancel, etc).
See ?tk_messageBox.


or
use cmd
system('CMD /C "ECHO The R process has finished running && PAUSE"', 

or
use hta

in one line:
mshta "about:&lt;script>alert('Hello, world!');close()&lt;/script>"
or
mshta "javascript:alert('message');close()"
or
mshta.exe vbscript:Execute("msgbox ""message"",0,""title"":close")


mshta "about:&lt;script src='file://%~f0'>&lt;/script>&lt;script>close()&lt;/script>" %*

msg = paste0(
'mshta ',
"\"about:&lt;script>alert('Hello, world!');close()&lt;/script>\""
)

to show web page, use script to create

#=================
Pop up message in windows 8.1
c.bat:  start MessageBox.vbs "This will be shown in a popup."

MessageBox.vbs :
Set objArgs = WScript.Arguments
messageText = objArgs(0)
MsgBox messageText

in fact, save a file named test.vbs with content:
MsgBox "some message"

double click the file will run directly


# options("scipen"=999)
# format(xx, scientific=F)
# options("scipen"=100, "digits"=4)
# getOption("scipen")
# or as.integer(functionResult);

df &lt;- data.frame(matrix(ncol = 10000, nrow = 0))
colnames(df) &lt;- c("a", "b," "c")
rm(list=ls())
Extracting a Single, Simple Table
The first step is to load the ¡§XML¡¨ package, 
then use the htmlParse() function to read the html document into an R object, 
and readHTMLTable() to read the table(s) in the document. 
The length() function indicates there is a single table in the document, simplifying our work.

The plot3d() function in the rgl package
library(rgl)
open3d()
attach(mtcars)
plot3d(disp,wt,mpg, col = rainbow(10))






<h2>library(stringr)</h2>
#============
library(stringr)
library(htmltools)
library(threejs)
data(mtcars)
data &lt;- mtcars[order(mtcars$cyl),]
uv &lt;- tabulate(mtcars$cyl)
col &lt;- c(rep("red",uv[4]),rep("yellow",uv[6]),rep("blue",uv[8]))
row.names(mtcars)
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")





<h2>scatterplot3js(data[,c(3,6,1)],</h2>
#============
scatterplot3js(data[,c(3,6,1)],
               labels=row.names(mtcars),
               size=mtcars$hp/100,
               flip.y=TRUE,
               color=col,renderer="canvas")
# Gumball machine
N &lt;- 100
i &lt;- sample(3, N, replace=TRUE)
x &lt;- matrix(rnorm(N*3),ncol=3)
lab &lt;- c("small", "bigger", "biggest")
scatterplot3js(x, color=rainbow(N), labels=lab[i],
               size=i, renderer="canvas")
# Example 1 from the scatterplot3d package (cf.)
z &lt;- seq(-10, 10, 0.1)
x &lt;- cos(z)
y &lt;- sin(z)
scatterplot3js(x,y,z, color=rainbow(length(z)),
   labels=sprintf("x=%.2f, y=%.2f, z=%.2f", x, y, z))
# Interesting 100,000 point cloud example, should run this with WebGL!
N1 &lt;- 10000
N2 &lt;- 90000
x &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
y &lt;- c(rnorm(N1, sd=0.5), rnorm(N2, sd=2))
z &lt;- c(rnorm(N1, sd=0.5), rpois(N2, lambda=20)-20)
col &lt;- c(rep("#ffff00",N1),rep("#0000ff",N2))
scatterplot3js(x,y,z, color=col, size=0.25)
cat("\014")	CLS Screen
#
match returns a vector of the positions
v1 &lt;- c("a","b","c","d")
v2 &lt;- c("g","x","d","e","f","a","c")
x &lt;- match(v1,v2)
6 NA  7  3
v1 %in% v2
TRUE FALSE  TRUE  TRUE
x &lt;- match(v1,v2,nomatch=-1)
6 -1  7  3
%in% returns a logical vector indicating if there is a match or not






<h2>this check whether an element is inside a group</h2>
#=============
this check whether an element is inside a group
v &lt;- c('a','b','c','e')
'b' %in% v





<h2>check vector includes in 31:37 %in% 0:36</h2>
#=============
31:37 %in% 0:36
#
dmInfo=data.matrix(Info)	# convert dataframe to matrix, but the row and column is exchanged
#
bob &lt;- data.frame(lapply(bob, as.character), stringsAsFactors=FALSE)	#Change numeric to characters
#
write.csv(Info,quote=FALSE, row.names = FALSE)	# write csv is the proper way to write the datafile
#

attach an excel file in R:
1: Install packages XLConnect and foreign and run both libraries
2: abcd &lt;- readWorksheet(loadWorkbook('file extension'),sheet=1)
#
allocate vector of size 1.7 Gb
Try memory.limit() for the current memory limit Use memory.limit (size=50000) to increase memory limit. Try using a cloud based environment, 
try using package slam
use factors 

Concatenate and Split Strings in R
==================================
use the paste() function to concatenate
strsplit() function to split
pangram &lt;- "The quick brown fox jumps over the lazy dog"
strsplit(pangram, " ")
"The"  "quick" "brown" "fox"  "jumps" "over" "the"  "lazy" "dog"

the unique elements
unique() function
unique(tolower(words))
"the"  "quick" "brown" "fox"  "jumps" "over" "lazy" "dog"

# <span class="gold">find duplicates</span>
# the intersect function is used for different set, not in inside a vector
# instead, use the duplicated function will be OK.

words = unlist(strsplit(pangram, " "))
words = tolower(words)
duplicated(words)
words[duplicated(words)]

arr = sample(1:36,6,replace=TRUE)
cat(arr, "\n")
arr[duplicated(arr)]


R split Function
================
split() function divides the data in a vector. 
unsplit() funtion do the reverse.
split(x, f, drop = FALSE, ...)
split(x, f, drop = FALSE, ...) &lt;- value
unsplit(value, f, drop = FALSE)
x: vector, data frame
f: indices
drop: discard non existing levels or not






<h2>x &lt;- read.csv("anova.csv",header=T,sep=",")</h2>
#=============
x &lt;- read.csv("anova.csv",header=T,sep=",")
Subtype,Gender,Expression
A,m,-0.54
A,m,-0.8
Split the "Expression" values into two groups based on "Gender" variable, 
"f" for female group, and 
"m" for male group:
>g &lt;- split(x$Expression, x$Gender)
>g
$f
  [1] -0.66 -1.15 -0.30 -0.40 -0.24 -0.92  0.48 -1.68 -0.80 -0.55 -0.11 -1.26
$m
  [1] -0.54 -0.80 -1.03 -0.41 -1.31 -0.43  1.01  0.14  1.42 -0.16  0.15 -0.62

Calculate the length, mean value of each group:
sapply(g,length)
  f   m 
135 146 
sapply(g,mean)
         f          m 
-0.3946667 -0.2227397

You may use lapply, return is a list:
lapply(g,mean)
unsplit() function combines the groups:
unsplit(g,x$Gender)

<h2><span class="blink red">Apply</span></h2>
=====
m &lt;- matrix(data=cbind(rnorm(30, 0), rnorm(30, 2), rnorm(30, 5)), nrow=30, ncol=3)
apply(m, 1, mean)
a 1 in the second argument, giving the mean of each row. 
apply(m, 2, mean)giving the mean of each column. 
apply(m, 2, function(x) length(x[x&lt;0]))	# count -ve values
apply(m, 2, function(x) is.matrix(x))
apply(m, 2, is.vector)
apply(m, 2, function(x) mean(x[x>0]))

#=========
ma &lt;- matrix(c(1:4, 1, 6:8), nrow = 2)

apply(ma, 1, table)

apply(ma, 1, stats::quantile)
apply(ma, 2, mean)

apply(m, 2, function(x) length(x[x&lt;0]))

sapply lapply rollapply
sapply(1:3, function(x) x^2)

lapply return a list:
lapply(1:3, function(x) x^2)
use unlist with lapply to get a vector

sapply(1:3, function(x, y) mean(y[,x]), y=m)

A&lt;-matrix(1:9, 3,3)
B&lt;-matrix(4:15, 4,3)
C&lt;-matrix(8:10, 3,2)
MyList&lt;-list(A,B,C)
Z=sapply(MyList,"[", 1,1 )

#==========
te=matrix(1:20,nrow=2)
sapply(te,mean)	# this is a vector, order arrange in matrix direction
matrix(sapply(te,mean),nrow=2)	# this is changed to matrix

subset()
apply()
sapply()
lapply()
tapply()
aggregate()
apply 	apply a function to the rows or columns of a matrix
M &lt;- matrix(seq(1,16), 4, 4)
apply(M, 1, min)
lapply 	apply a function to each element of a list in turn and get a list back
x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
sapply 	apply a function to each element of a list in turn, but you want a vector back
x &lt;- list(a = 1, b = 1:3, c = 10:100)
sapply(x, FUN = length)  
vapply 	squeeze some more speed out of sapply
x &lt;- list(a = 1, b = 1:3, c = 10:100)
vapply(x, FUN = length, FUN.VALUE = 0L) 

mapply 	apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in sapply

Note: 
mApply(X, INDEX, FUN, …, simplify=TRUE, keepmatrix=FALSE)
from Hmisc package

is different from 
mapply(FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)


Examples

#Sums the 1st elements, the 2nd elements, etc. 
mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15

mapply(rep, 1:4, 4:1)
mapply(rep, times = 1:4, x = 4:1)
mapply(rep, times = 1:4, MoreArgs = list(x = 42))
mapply(function(x, y) seq_len(x) + y,
       c(a =  1, b = 2, c = 3),  # names from first
       c(A = 10, B = 0, C = -10))
word &lt;- function(C, k) paste(rep.int(C, k), collapse = "")
utils::str(mapply(word, LETTERS[1:6], 6:1, SIMPLIFY = FALSE))

mapply(function(x,y){x^y},x=c(2,3),y=c(3,4))
8 81

values1 &lt;- list(a = c(1, 2, 3), b = c(4, 5, 6), c = c(7, 8, 9))
values2 &lt;- list(a = c(10, 11, 12), b = c(13, 14, 15), c = c(16, 17, 18)) 
mapply(function(num1, num2) max(c(num1, num2)), values1, values2)
 a  b  c 
12 15 18 



Map 	A wrapper to mapply with SIMPLIFY = FALSE, so it is guaranteed to return a list
rapply	For when you want to apply a function to each element of a nested list structure, recursively
tapply	For when you want to apply a function to subsets of a vector and the subsets are defined by some other vector, usually a factor
lapply is a list apply which acts on a list or vector and returns a list.
sapply is a simple lapply (function defaults to returning a vector or matrix when possible)
vapply is a verified apply (allows the return object type to be prespecified)
rapply is a recursive apply for nested lists, i.e. lists within lists
tapply is a tagged apply where the tags identify the subsets
apply is generic: applies a function to a matrix's rows or columns
by	a "wrapper" for tapply. The power of by arises when we want to compute a task that tapply can't handle
aggregate can be seen as another a different way of use tapply if we use it in such a way


xx = c(1,3,5,7,9,8,6,4,2,1,5)
duplicated(xx)
xx[duplicated(xx)]

Accessing dataframe by names:
mtcars["mpg"]
QueueNo = 12
mtcars[QueueNo,"mpg"]

some functions to remember
charToRaw(key)
as.raw(key)

A motion chart is a dynamic chart to explore several indicators over time. 
subset(airquality, Temp > 80, select = c(Ozone, Temp))
subset(airquality, Day == 1, select = -Temp)
subset(airquality, select = Ozone:Wind) with(airquality, subset(Ozone, Temp > 80))
 ## sometimes requiring a logical 'subset' argument is a nuisance nm &lt;- rownames(state.x77) start_with_M &lt;- nm %in% grep("^M", nm, value = TRUE)
subset(state.x77, start_with_M, Illiteracy:Murder) # but in recent versions of R this can simply be
subset(state.x77, grepl("^M", nm), Illiteracy:Murder)

join 3 dataframes
library("plyr")
join() function
names(gdp)[3] &lt;- "GDP"
names(life_expectancy)[3] = "LifeExpectancy"
names(population)[3] = "Population"
gdp_life_exp &lt;- join(gdp, life_expectancy)
development &lt;- join(gdp_life_exp, population)

subset() function
dev_2005 &lt;- subset(development, Year == 2005)
dev_2005_big &lt;- subset(dev_2005, GDP >= 30000)

development_motion &lt;- subset(development_complete, Country %in% selection)
library(googleVis)
gvisMotionChart() function
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year")
plot(motion_graph)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "GDP", yvar = "LifeExpectancy", sizevar = "Population")
development_motion$logGDP &lt;- log(development_motion$GDP)
motion_graph &lt;- gvisMotionChart(development_motion, idvar = "Country", timevar = "Year", xvar = "logGDP", yvar = "LifeExpectancy", sizevar = "Population")

my_list[[1]] extracts the first element of the list my_list, and my_list[["name"]] extracts the element in my_list that is called name. 
If the list is nested you can travel down the heirarchy by recursive subsetting. 
mylist[[1]][["name"]] is the element called name inside the first element of my_list.
A data frame is just a special kind of list, so you can use double bracket subsetting on data frames too. 
my_df[[1]] will extract the first column of a data frame and my_df[["name"]] will extract the column named name from the data frame.
names() and str() is a great way to explore the structure of a list.

i in 1:ncol(df)
This is a pretty common model for a sequence: a sequence of consecutive integers designed to index over one dimension of our data.
What might surprise you is that this isn't the best way to generate such a sequence, especially when you are using for loops inside your own functions. Let's look at an example where df is an empty data frame:
df &lt;- data.frame()
1:ncol(df)
for (i in 1:ncol(df)) {
  print(median(df[[i]]))
}
Our sequence is now the somewhat non-sensical: 1, 0. You might think you wouldn't be silly enough to use a for loop with an empty data frame, but once you start writing your own functions, there's no telling what the input will be.
A better method is to use the seq_along() function.
if you grow the for loop at each iteration (e.g. using c()), your for loop will be very slow.
A general way of creating an empty vector of given length is the vector() function. 
It has two arguments: the type of the vector ("logical", "integer", "double", "character", etc.) and the length of the vector.
Then, at each iteration of the loop you must store the output in the corresponding entry of the output vector, i.e. assign the result to output[[i]]. (You might ask why we are using double brackets here when output is a vector. It's primarily for generalizability: this subsetting will work whether output is a vector or a list.)

A time series can be thought of as a vector or matrix of numbers, 
along with some information about what times those numbers were recorded. This information is stored in a ts object in R.
read in some time series data from an xlsx file using read_excel(), 
a function from the readxl package, 
and store the data as a ts object.
Use the read_excel() function to read the data from "exercise1.xlsx" into mydata.
mydata &lt;- read_excel("exercise1.xlsx")
Create a ts object called myts using the ts() function. 
myts &lt;- ts(mydata[,2:4], start = c(1981, 1), frequency = 4)

The first step in any data analysis task is to plot the data. 
Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. 
The features that you see in the plots must then be incorporated into the forecasting methods that you use. 
Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.
You will use the autoplot() function to produce time plots of the data. 
In each plot, look out for outliers, seasonal patterns, and other interesting features.
Use which.max() to spot the outlier in the gold series. 

library("fpp2")
autoplot(a10)
ggseasonplot(a10)
An interesting variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. 
ggseasonplot(a10, polar = TRUE)
beer &lt;- window(a10, start=1992)
autoplot(beer)
ggseasonplot(beer)
Use the window() function to consider only the ausbeer data from 1992 and save this to beer. 
Set a keyword start to the appropriate year.

x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});





<h2>x &lt;- c(sort(sample(1:20, 9)), NA)</h2>
#===================
x &lt;- c(sort(sample(1:20, 9)), NA)
y &lt;- c(sort(sample(3:23, 7)), NA)
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

alist = readLines("alist.txt")
blist = readLines("blist.txt")
out = setdiff(blist, alist)

writeClipboard(out)





<h2># To skip 3rd iteration and go to next iteration</h2>
#===================
# To skip 3rd iteration and go to next iteration
for(n in 1:5) {
  if(n==3) next
  cat(n)
}





<h2>googleVis chart</h2>
#===================
googleVis chart
===============
library(googleVis)

Line chart
==========
df=data.frame(country=c("US", "GB", "BR"), 
              val1=c(10,13,14), val2=c(23,12,32))
Line &lt;- gvisLineChart(df)
plot(Line)

Scatter chart
=======================
# example 1
dat &lt;- data.frame(x=c(1,2,3,4,5), y1=c(0,3,7,5,2), y2=c(1,NA,0,3,2))
plot(gvisScatterChart(dat, options=list(lineWidth=2, pointSize=2, width=900, height=600)))

# example 2, women
Scatter &lt;- gvisScatterChart(women, 
               options=list(
                 legend="none", lineWidth=1, pointSize=2,
                 title="Women", vAxis="{title:'weight (lbs)'}",
                 hAxis="{title:'height (in)'}", width=900, height=600)
           )
plot(Scatter)

# example 3
ex3dat &lt;- data.frame(x=c(1,2,3,4,5,6,7,8), y1=c(0,3,7,5,2,0,8,6), y2=c(1,NA,0,3,2,6,4,2))
ex3 &lt;- gvisScatterChart(ex3dat, 
           options=list(
             legend="none", lineWidth=1, pointSize=2,
             title="ex3", vAxis="{title:'weight (lbs)'}",
             hAxis="{title:'height (in)'}", width=900, height=600)
       )
plot(ex3)
# Note: to plot timeline chart, arrange the time in x axis, beginning with -ve and the last is 1 to show the sequence


<h2>cat to a file using file(filename, open = "a")</h2>
cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "data.txt", sep = "\n")

<h3>cat append to a file, open file in "a" mode</h3>
#===================
textVector = c("First thing","Second thing","c")

catObj &lt;- file("theappend.txt", open = "a")
cat(textVector, file = catObj, sep="\n")
close(catObj)






<h2>install.packages("readr")</h2>
#===================
install.packages("readr")
library(readr)

to read rectangular data (like csv, tsv, and fwf)
readr is part of the core tidyverse
library(tidyverse)

readr supports seven file formats with seven read_ functions:

read_csv(): comma separated (CSV) files
read_tsv(): tab separated files
read_delim(): general delimited files
read_fwf(): fixed width files
read_table(): tabular files where columns are separated by white-space.
read_log(): web log files





<h2>iconv(keyword, "unknown", "GB2312")</h2>
#===================
iconv(keyword, "unknown", "GB2312")






<h2>Grabbing HTML Tags</h2>
#==========
Grabbing HTML Tags

\b[^>]*>(.*?) matches the opening and closing pair of a specific HTML tag. 

Anything between the tags is captured into the first backreference. 
The question mark in the regex makes the star lazy, to make sure it stops before the first closing tag rather than before the last, like a greedy star would do. 
This regex will not properly match tags nested inside themselves, like in one two one.

<([A-Z][A-Z0-9]*)\b[^>]*>(.*?)</\1> will match the opening and closing pair of any HTML tag. 
Be sure to turn off case sensitivity. 
The key in this solution is the use of the backreference \1 in the regex. 
Anything between the tags is captured into the second backreference. 
This solution will also not match tags nested in themselves






<h2>find the new item</h2>
#==========
find the new item

theList = c("00700","02318","02007")
newList=c("03333","01398","02007")

newList[!(newList %in% theList)]





<h2>formating numbers</h2>
#==========
formating numbers
a &lt;- seq(1,101,25)
sprintf("%03d", a)

format(round(a, 2), nsmall = 2)





<h2>the match function:</h2>
#==========
the match function:
match(x, table, nomatch = NA_integer_, incomparables = NULL)
%in%
match returns a vector of the positions of (first) matches of its first argument in its second.

Corpus&lt;- c('animalada', 'fe', 'fernandez', 'ladrillo')
Lexicon&lt;- c('animal', 'animalada', 'fe', 'fernandez', 'ladr', 'ladrillo')
Lexicon %in% Corpus

Lexicon[Lexicon %in% Corpus]





<h2>Machine Learning:</h2>
<a href="https://machinelearningmastery.com/machine-learning-in-r-step-by-step/">machine-learning-in-r-step-by-step</a>
<br>
<a href="https://lgatto.github.io/IntroMachineLearningWithR/index.html">An Introduction to Machine Learning with R</a>
<br>
<a href="https://www.r-bloggers.com/image-recognition-tutorial-in-r-using-deep-convolutional-neural-networks-mxnet-package/">mxnet</a>
<br>
<a href="https://blogs.rstudio.com/tensorflow/posts/2017-12-14-image-classification-on-small-datasets/">image classification</a>
<br>
#==========
Machine Learning:

The caret package

Caret contains wrapper functions that allow you to use the exact same functions for training and predicting with dozens of different algorithms. On top of that, it includes sophisticated built-in methods for evaluating the effectiveness of the predictions you get from the model. 

Use The Titanic dataset

Training a model
training a bunch of different decision trees and having them vote 
Random forests work pretty well in *lots* of different situations, so I often try them first.

Evaluating the model

Cross-validation is a way to evaluate the performance of a model without needing any other data than the training data. 

Making predictions on the test set

Improving the model







<h2>to handle error 404 when scraping: use tryCatch()</h2>
#==========
to handle error 404 when scraping: use tryCatch()

for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}
#==========
try(readLines(url), silent = TRUE)

tryCatch(readLines(url), error = function (e) conditionMessage(e))






<h2>write.table</h2>
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
#==========
write.table(matrixname, file = "outputname", append = FALSE, quote = FALSE, sep = "\t",
            eol = "\n", na = "NA", dec = ".", row.names = FALSE,
            col.names = FALSE, qmethod = c("escape", "double"),
            fileEncoding = "")

write.table(finalTableList, theOutputname, row.names=FALSE, col.names=FALSE, quote = FALSE, sep = "\t" )






<h2>Four normal distribution functions:</h2>
#==========
Four normal distribution functions:

<a href="https://www.r-bloggers.com/normal-distribution-functions/">Four normal distribution functions:</a>
<br>

RNORM	Generates random numbers from normal distribution	
rnorm(n, mean, sd)
rnorm(1000, 3, .25)	Generates 1000 numbers from a normal with mean 3 and sd=.25

DNORM	Probability Density Function(PDF)
dnorm(x, mean, sd)
dnorm(0, 0, .5)	Gives the density (height of the PDF) of the normal with mean=0 and sd=.5. 

PNORM	Cumulative Distribution Function
(CDF)	pnorm(q, mean, sd)
pnorm(1.96, 0, 1)	Gives the area under the standard normal curve to the left of 1.96, i.e. ~0.975

QNORM	Quantile Function – inverse of
pnorm	qnorm(p, mean, sd)
qnorm(0.975, 0, 1)	Gives the value at which the CDF of the standard normal is .975, i.e. ~1.96

Note that for all functions, leaving out the mean and standard deviation would result in default values of mean=0 and sd=1, a standard normal distribution.






<h2>pnorm students scoring higher than 84</h2>
#==========
pnorm students scoring higher than 84
> pnorm(84, mean=72, sd=15.2, lower.tail=FALSE) 
[1] 0.21492
Answer
The percentage of students scoring 84 or more in the college entrance exam is 21.5%.






<h2>plot a histogram of 1000</h2>
draws from a normal distribution with mean 10, standard deviation 2.
#==========
plot a histogram of 1000 draws from a normal distribution with mean 10, standard deviation 2.
set.seed(seed)
x = rnorm(1000, 10, 2)
plot(x)
hist(x)

Using a QQ plot. Assess the normality:
qqnorm(x)
qqline(x)

In statistics, a Q–Q (quantile-quantile) plot is a probability plot, 
which is a graphical method for comparing two probability distributions by plotting their quantiles against each other.
First, the set of intervals for the quantiles is chosen. 
A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). 
Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.






<h2>format leading zeros</h2>
#==========
format leading zeros

formatC(1, width = 2, format = "d", flag = "0")
"01"
formatC(125, width = 5, format = "d", flag = "0")
"00125"






<h2>library(pdftools)</h2>
#==========
setwd("C:/Users/User/Desktop")
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
writeClipboard(txt[1])

txt1 = gsub(".*ORIGINATOR", "", txt)
txt1 = gsub("          ", "", txt1)

list = c(13:16, 19:22, 25:28, 31:34, 37:42, 45:48, 52:58, 62:68, 71:75, 78:85, 88:95, 98:105, 108:115, 118:124, 127:133, 136:142, 145:156, 159:169, 173:202, 206:221, 225:240, 244:258, 261:274, 277:290, 294:298, 302:308, 312:318, 323:331, 334:345, 348:359)

txt1 = txt1[list]

writeClipboard(txt1)

pdf_info("a.pdf")
pdf_text("a.pdf")
pdf_fonts("a.pdf")
pdf_attachments("a.pdf")
pdf_toc("a.pdf")

toc = pdf_toc("a.pdf")
sink("test.txt")
print(toc)
sink()





<h2>library(pdftools)</h2>
#==========
library(pdftools)
txt &lt;- pdf_text("a.pdf")
str(txt)
txtList = unlist(strsplit(txt, "\\s{2,}"))

writeClipboard(txtList)






<h2>The name of the site environment variable R_ENVIRON</h2>
#==========
The name of the site environment variable R_ENVIRON
"R_HOME/etc/Renviron.site"

the default is "R_HOME/etc/Rprofile.site"

Sys.getenv("R_USER").

Examples

## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, 
so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, 
digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", 
"onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First &lt;- function() cat("\n   Welcome to R!\n\n")
.Last &lt;- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, 
set a CRAN mirror
  old &lt;- getOption("defaultPackages"); r &lt;- getOption("repos")
  r["CRAN"] &lt;- "http://my.local.cran"
  options(defaultPackages = c(old, 
"MASS"), 
repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols &lt;- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), 
"\n")
# coo\bardoh\exabc"def'







<h2>How to Convert Factor into Numerical?</h2>
#==========
How to Convert Factor into Numerical?

When you convert factors to numeric, 
first you should convert it into characters and then convert into numeric. 
as.numeric(as.character(X))

Df$column&lt;-as.numeric(as.factor(df$column)

as.integer(as.factor(region))





<h2>options(error=recover)</h2>
#==========
options(error=recover)

recover {utils}
Browsing after an Error

This function allows the user to browse directly on any of the currently active function calls, and is suitable as an error option.
The expression options(error = recover) will make this the error option.

Usage
recover()

When called, recover prints the list of current calls, and prompts the user to select one of them.
The standard R browser is then invoked from the corresponding environment;
the user can type ordinary R language expressions to be evaluated in that environment.

Turning off the options() debugging mode in R
options(error=NULL)





<h2>Extract hyperlink from Excel file in R</h2>

#==========

library(XML)

# rename file to .zip
my.zip.file &lt;- sub("xlsx", "zip", my.excel.file)
file.copy(from = my.excel.file, to = my.zip.file)

# unzip the file
unzip(my.zip.file)

# unzipping produces a bunch of files which we can read using the XML package
# assume sheet1 has our data
xml &lt;- xmlParse("xl/worksheets/sheet1.xml")

# finally grab the hyperlinks
hyperlinks &lt;- xpathApply(xml, "//x:hyperlink/@display", namespaces="x")


<span class="redword">To repair Hyperlink address corrupted:</span>
copy file to desk top and rename to zip file
open zip file and locate: <span class="redword">\xl\worksheets\_rels</span>
open the sheet1.xml.rels with editor
remove all text: D:\Users\Lawht\AppData\Roaming\Microsoft\Excel\




<h2>Extract part of a string</h2>

#==========
x &lt;- c("75 to 79", "80 to 84", "85 to 89")
substr(x, start = 1, stop = 2)

substr(x, start, stop)
x &lt;- "1234567890"
substr(x, 5, 7)
"567"

<br>
<h2>alter grades</h2>

#==========
alter grades

locate the word
get the line location
alter the score table
#==========

locate the word
v &lt;- c('a','b','c','e')
'b' %in% v ## returns TRUE
match('b',v) ## returns the first location of 'b', in this case: 2

subv &lt;- c('a', 'f')
subv %in% v ## returns a vector TRUE FALSE
is.element(subv, v) ## returns a vector TRUE FALSE

which()
which('a' == v) #[1] 2 4 For finding all occurances as vector of indices

grep() returns a vector of integers, which indicate where matches are.
yo &lt;- c("a", "a", "b", "b", "c", "c")
grep("b", yo) # [1] 3 4

ROC&lt;-"中華民國 – 維基百科，自由的百科全書"
grep("中華民國",ROC)

Partial String Matching
pmatch("med", c("mean", "median", "mode")) # returns 2


<br>
<br>
<h2>table, cut and barplot</h2>

atab=c(1,2,3,2,1,2,3,4,5,4)
table(atab)
cut(atab, 2)
table( cut(atab, 2))
counts = table( cut(atab, 4))
barplot(counts, main="Qty", xlab="grade")


<br>
<br>
<br>
<h2>non-paste answer to concatenate two strings</h2>

capture.output(cat(counts, sep = ","))


<br>
<h2>V8 is an R interface JavaScript engine. </h2>

This package helps us execute javascript code in R

#Loading both the required libraries
library(rvest)
library(V8)

#URL with js-rendered content to be scraped
link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'

#Read the html page content and extract all javascript codes that are inside a list

emailjs &lt;- read_html(link) %>% html_nodes('li') %>% html_nodes('script') %>% html_text()

# Create a new v8 context
ct &lt;- v8()

#parse the html content from the js output and print it as text
read_html(ct$eval(gsub('document.write','',emailjs))) %>%  html_text()

info@brewhemia.co.uk

Thus we have used rvest to extract the javascript code snippet from the desired location (that is coded in place of email ID) and used V8 to execute the javascript snippet (with slight code formatting) and output the actual email (that is hidden behind the javascript code). 

####################
Getting email address through rvest
You need a javascript engine here to process the js code.
R has got V8.

Modify your code after installing V8 package:
library(rvest)
library(V8)

link &lt;- 'https://food.list.co.uk/place/22191-brewhemia-edinburgh/'
page &lt;- read_html(link)
name_html &lt;- html_nodes(page,'.placeHeading')
business_adr &lt;- html_text(adr_html)
tel_html &lt;- html_nodes(page,'.value')
business_tel &lt;- html_text(tel_html)
emailjs &lt;- page %>% html_nodes('li') %>% html_nodes('script') %>% html_text()
ct &lt;- v8()
read_html(ct$eval(gsub('document.write','',emailjs))) %>% html_text()


<br>
<br>
<h2>extract protected pdf document</h2>

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("a.pdf")
str(txt)	# 361 pages
# copy page 1
writeClipboard(txt[1])
# copy page 2
writeClipboard(txt[2])
# copy page 3
writeClipboard(txt[3])

Convert unicode character to string format: remove "\u"

theStr = "\u9999\u6e2f\u98df\u54c1\u6295\u8d44"	#  "香港食品投资"

=============================

Sys.setlocale(category = 'LC_ALL', 'Chinese')

library(pdftools)
setwd("C:/Users/User/Desktop")
txt &lt;- pdf_text("45.pdf")
str(txt)

chi1 = gsub('\\u' , '&#x', txt[1])
chi2 = gsub('\\u' , '&#x', txt[2])
chi3 = gsub('\\u' , '&#x', txt[3])
sink("aaa.txt")
cat(chi1)
cat(chi2)
cat(chi3)
sink()



<br>
<br>
<h2>Writing an R package</h2>

  <a href="https://support.rstudio.com/hc/en-us/articles/200486488-Developing-Packages-with-RStudio">Develop Packages with RStudio</a>
<br>
<a href="http://web.mit.edu/insong/www/pdf/rpackage_instructions.pdf">rpackage_instructions.pdf</a>
<br>
<a href="https://hilaryparker.com/2014/04/29/writing-an-r-package-from-scratch/">Writing an R package from scratch</a>
<a href="Writing an R package.html"><span class="goldb">Writing an R package</span></a> 
<br>


<br>
<br>
<h2>table, cut and breaks</h2>

table(cut(as.numeric(resultTable[,3]), 10))
cut(as.numeric(resultTable[,3]),10)
breaks = c(seq(lower, 0, by = 5), 0, seq(0, upper, by = 5))

tableA = c(1,3,5,7,9)
tableB = c(1,3,5,7,2,4,6,8)
tableA = c(tableA, tableB)
tableA = sort(tableA)

table(tableA)

table(cut(tableA, 3))

breaks = c(seq(1, 3, by = 1), 4, seq(5, 9, by = 2))
table(cut(tableA, breaks))


<br>
<br>
<h2>List the Files in a Directory</h2>

List the Files in a Directory/Folder
list.files()

list.dirs(R.home("doc"))
list.dirs()

<br>
<br>
<h2>test url exist</h2>

library(httr)
http_error(theUrl)

<a href="https://stackoverflow.com/questions/18407177/load-image-from-website">Load image from website</a>
download.file("url", destfile="tmp.png", mode="wb")

<a href="https://jangorecki.gitlab.io/data.table/library/RCurl/html/url.exists.html">url.exists {RCurl}	</a> return true of false
<a href="https://stackoverflow.com/questions/31420210/r-check-existence-of-url-problems-with-httrget-and-url-exists">With httr use url_success()</a>
<br>

<br>
<br>
<h2>Passing arguments to R script</h2>

<a href="Passing arguments to R script.html"><span class="goldb">Passing arguments to R script</span></a> 

Rscript --vanilla testargument.R iris.txt newname

To avoid Rscript.exe loop forever for keyboard input:
use this:
cat("a string please: ");
a &lt;- readLines("stdin",n=1);


<br>
<br>
<h2>School Revision Papers</h2>

http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2018/
http://schoolsnetkenya.com/form-1-revision-papers-for-term-1-2017/
https://curriculum.gov.mt/en/Examination-Papers/Pages/list_secondary_papers.aspx
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F1-2ndTest-Eng.pdf
http://www2.hkedcity.net/sch_files/a/hf1/hf1-lin/visitor_cabinet/67726/F2-2ndTest-Eng.pdf
http://www.sttss.edu.hk/parents_corner/pastpaper.php

<br>
<br>
<h2>difference between 1L and 1</h2>

L specifies an integer type, rather than a double, it uses only 4 bytes per element
the function as.integer is simplified yb  "L " suffix

> str(1)
 num 1

> str(1L)
 int 1

<br>
<br>
<br>
<h2><span class="white goldbs">Datatable</span></h2>

<a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/datatable_Cheat_Sheet_R.pdf">Datatable Cheat Sheet</a>
library(data.table)

dt=data.table(read.table("wAveTable.txt", header=TRUE, colClasses=c('character', 'numeric', 'numeric')))
colnames(dt)
"Code"   "WAve5"  "WAve10"
dt[WAve5 > 5, ]
summary(dt[WAve5 = 5, ])
summary(dt[WAve5 %between% c(7,9), ])

data.table dt subset rows using i, and manipulate columns with j, grouped according to by	dt[i, j, by]
Create a data.table	data.table(a = c(1, 2), b = c("a", "b"))
convert a data frame or a list to a data.table	setDT(df) or as.data.table(df)
Subset data.table rows using i	dt[1:2, ]
subset data.table rows based on values in one or more columns	dt[a > 5, ]
data.table Logical Operators To Use In i	>,<,<=,>=, |, !,&, is.na(),!is.na(), %in%, %like%,  %between%
data.table extract column(s) by number. Prefix column numbers with “-” to drop	dt[, c(2)]
data.table extract column(s) by name	dt[, .(b, c)]
create a data.table with new columns based on the summarized values of rows	dt[, .(x = sum(a))]
compute a data.table column based on an expression	dt[, c := 1 + 2]
compute a data.table column based on an expression but only for a subset of rows	dt[a == 1, c := 1 + 2]
compute a data.table multiple columns based on separate expressions	dt[, `:=`(c = 1 , d = 2)]
delete a data.table column	dt[, c := NULL]
convert the type of a data.table column using as.integer(), as.numeric(), as.character(), as.Date(), etc..	dt[, b := as.integer(b)]
group data.table rows by values in specified column(s)	dt[, j, by = .(a)]
group data.table and simultaneously sort rows according to values in specified column(s)	dt[, j, keyby = .(a)]
summarize data.table rows within groups	dt[, .(c = sum(b)), by = a]
create a new data.table column and compute rows within groups	dt[, c := sum(b), by = a]
extract first data.table row of groups	dt[, .SD[1], by = a]
extract last data.table row of groups	dt[, .SD[.N], by = a]
perform a sequence of data.table operations by chaining multiple “[]”	dt[…][…]
reorder a data.table according to specified columns	setorder(dt, a, -b), “-” for descending
data.table’s functions prefixed with “set” and the operator “:=”	work without “&lt;-” to alter data without making copies in memory
df &lt;- as.data.table(df)	setDT(df)
extract unique data.table rows based on columns specified in “by”. Leave out “by” to use all columns	unique(dt, by = c("a", "b"))
return the number of unique data.table rows based on columns specified in “by”	uniqueN(dt, by = c("a", "b"))
rename data.table column(s)	setnames(dt, c("a", "b"), c("x", "y"))
data.table Syntax	DT[ i , j , by], i refers to rows. j refers to columns. by refers to adding a group
data.table Syntax arguments	DT[ i , j , by], with, which, allow.cartesian, roll, rollends, .SD, .SDcols, on, mult, nomatch
data.table fread() function	to read data, mydata = fread("https://github.com/flights_2014.csv")
data.table select only 'origin' column returns a vector	dat1 = mydata[ , origin]
data.table select only 'origin' column returns a data.table	dat1 = mydata[ , .(origin)] or dat1 = mydata[, c("origin"), with=FALSE]
data.table select column	dat2 =mydata[, 2, with=FALSE]
data.table select column Multiple Columns	dat3 = mydata[, .(origin, year, month, hour)], dat4 = mydata[, c(2:4), with=FALSE]
data.table Dropping Column	adding ! sign, dat5 = mydata[, !c("origin"), with=FALSE]
data.table Dropping Multiple Columns	dat6 = mydata[, !c("origin", "year", "month"), with=FALSE]
data.table select variables that contain 'dep'	use %like% operator, dat7 = mydata[,names(mydata) %like% "dep", with=FALSE]
data.table Rename Variables	setnames(mydata, c("dest"), c("Destination"))
data.table  rename multiple variables	setnames(mydata, c("dest","origin"), c("Destination", "origin.of.flight"))
data.table find all the flights whose origin is 'JFK'	dat8 = mydata[origin == "JFK"]
data.table Filter Multiple Values	dat9 = mydata[origin %in% c("JFK", "LGA")]
data.table selects not equal to 'JFK' and 'LGA'	dat10 = mydata[!origin %in% c("JFK", "LGA")]
data.table Filter Multiple variables	dat11 = mydata[origin == "JFK" & carrier == "AA"]
data.table Indexing Set Key	tells system that data is sorted by the key column
data.table setting 'origin' as a key	setkey(mydata, origin), 'origin' key is turned on. data12 = mydata[c("JFK", "LGA")]
data.table Indexing Multiple Columns	setkey(mydata, origin, dest), key is turned on. mydata[.("JFK", "MIA")] # First key 'origin' matches “JFK” second key 'dest' matches “MIA”
data.table Indexing Multiple Columns equivalent	mydata[origin == "JFK" & dest == "MIA"]
data.table  identify the column(s) indexed by	key(mydata)
data.table sort data using setorder()	mydata01 = setorder(mydata, origin)
data.table sorting on descending order	mydata02 = setorder(mydata, -origin)
data.table Sorting Data based on multiple variables	mydata03 = setorder(mydata, origin, -carrier)
data.table Adding Columns (Calculation on rows)	use := operator, mydata[, dep_sch:=dep_time - dep_delay]
data.table Adding Multiple Columns	mydata002 = mydata[, c("dep_sch","arr_sch"):=list(dep_time - dep_delay, arr_time - arr_delay)]
data.table IF THEN ELSE Method I	mydata[, flag:= 1*(min < 50)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table IF THEN ELSE Method II	mydata[, flag:= ifelse(min < 50, 1,0)] ,set flag= 1 if min is less than 50. Otherwise, set flag =0.
data.table build a chain	DT[ ] [ ] [ ], mydata[, dep_sch:=dep_time - dep_delay][,.(dep_time,dep_delay,dep_sch)]
data.table Aggregate Columns mean	mydata[, .(mean = mean(arr_delay, na.rm = TRUE),
data.table Aggregate Columns median	median = median(arr_delay, na.rm = TRUE),
data.table Aggregate Columns min	min = min(arr_delay, na.rm = TRUE),
data.table Aggregate Columns max	max = max(arr_delay, na.rm = TRUE))]
data.table Summarize Multiple Columns	all the summary function in a bracket, mydata[, .(mean(arr_delay), mean(dep_delay))]
data.table .SD operator	implies 'Subset of Data'
data.table .SD and .SDcols operators	calculate summary statistics for a larger list of variables
data.table calculates mean of two variables	mydata[, lapply(.SD, mean), .SDcols = c("arr_delay", "dep_delay")]
data.table Summarize all numeric Columns	mydata[, lapply(.SD, mean)]
data.table Summarize with multiple statistics	mydata[, sapply(.SD, function(x) c(mean=mean(x), median=median(x)))]
data.table Summarize by group 'origin	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = origin]
data.table Summary by group useing keyby= operator	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), keyby = origin]
data.table Summarize multiple variables by group 'origin'	mydata[, .(mean(arr_delay, na.rm = TRUE), mean(dep_delay, na.rm = TRUE)), by = origin], or mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin]
data.table remove non-unique / duplicate cases with unique()	setkey(mydata, "carrier"), unique(mydata)
data.table remove duplicated	setkey(mydata, NULL), unique(mydata), Note : Setting key to NULL is not required if no key is already set.
data.table Extract values within a group	mydata[, .SD[1:2], by=carrier], selects first and second values from a categorical variable carrier.
data.table Select LAST value from a group	mydata[, .SD[.N], by=carrier]
data.table window function frank()	dt = mydata[, rank:=frank(-distance,ties.method = "min"), by=carrier], calculating rank of variable 'distance' by 'carrier'. 
data.table cumulative sum cumsum()	dat = mydata[, cum:=cumsum(distance), by=carrier]
data.table lag and lead with shift()	shift(variable_name, number_of_lags, type=c("lag", "lead")), DT &lt;- data.table(A=1:5), DT[ , X := shift(A, 1, type="lag")], DT[ , Y := shift(A, 1, type="lead")]
data.table  %between% operator to define a range	DT = data.table(x=6:10), DT[x %between% c(7,9)]
data.table %like% to find all the values that matches a pattern	DT = data.table(Name=c("dep_time","dep_delay","arrival"), ID=c(2,3,4)), DT[Name %like% "dep"] 
data.table Inner Join	Sample Data: (dt1 &lt;- data.table(A = letters[rep(1:3, 2)], X = 1:6, key = "A")), (dt2 &lt;- data.table(A = letters[rep(2:4, 2)], Y = 6:1, key = "A")), merge(dt1, dt2, by="A")
data.table Left Join	merge(dt1, dt2, by="A", all.x = TRUE)
data.table Right Join	merge(dt1, dt2, by="A", all.y = TRUE)
data.table Full Join	merge(dt1, dt2, all=TRUE)
Convert a data.table to data.frame	setDF(mydata)
convert data frame to data table	setDT(), setDT(X, key = "A")
data.table Reshape Data	dcast.data.table() and melt.data.table()
data.table Calculate total number of rows by month and then sort on descending order	mydata[, .N, by = month] [order(-N)], The .N operator is used to find count.
data.table Find top 3 months with high mean arrival delay	mydata[, .(mean_arr_delay = mean(arr_delay, na.rm = TRUE)), by = month][order(-mean_arr_delay)][1:3]
data.table Find origin of flights having average total delay is greater than 20 minutes	mydata[, lapply(.SD, mean, na.rm = TRUE), .SDcols = c("arr_delay", "dep_delay"), by = origin][(arr_delay + dep_delay) > 20]
data.table Extract average of arrival and departure delays for carrier == 'DL' by 'origin' and 'dest' variables	mydata[carrier == "DL", lapply(.SD, mean, na.rm = TRUE), by = .(origin, dest), .SDcols = c("arr_delay", "dep_delay")]
data.table Pull first value of 'air_time' by 'origin' and then sum the returned values when it is greater than 300	mydata[, .SD[1], .SDcols="air_time", by=origin][air_time > 300, sum(air_time)]


<br>
<br>
<h2><span class="gold bordred1 blink">R Web Scraping</span></h2>
<a href="https://blog.scrapinghub.com/2016/10/27/an-introduction-to-xpath-with-examples" class="whitebut red bluebs blueblackgrad whitets blinkNmove">get started xpath selectors</a>

<a href="https://www.datacamp.com/community/tutorials/r-web-scraping-rvest">R Web Scraping Rvest</a>
<a href="http://www.programmingr.com/content/webscraping-using-readlines-and-rcurl/">webscraping-using-readlines-and-rcurl</a>
<a href="https://www.rdocumentation.org/packages/XML/versions/3.98-1.16/topics/xmlTreeParse">xmlTreeParse, htmlTreeParse</a>
<a href="http://www.cse.chalmers.se/~chrdimi/downloads/web/getting_web_data_r4_parsing_xml_html.pdf">getting web data parsing xml html</a>
<a href="https://stackoverflow.com/questions/35479549/error-in-r-no-applicable-method-for-xpathapply">error in r no applicable method for xpathapply</a>
<a href="https://blog.rstudio.com/2015/04/21/xml2/">Parse and process XML (and HTML) with xml2</a>
==================
web_page &lt;- readLines("http://www.interestingwebsite.com")
web_page &lt;- read.csv("http://www.programmingr.com/jan09rlist.html")

    # General-purpose data wrangling
    library(tidyverse)  

    # Parsing of HTML/XML files  
    library(rvest)    

    # String manipulation
    library(stringr)   

    # Verbose regular expressions
    library(rebus)     

    # Eases DateTime manipulation
    library(lubridate)

==================
install.packages("RCurl", dependencies = TRUE)
library("RCurl")
library("XML")

past &lt;- getURL("http://www.iciba.com/past", ssl.verifypeer = FALSE)	# getURL cannot work
webpage &lt;- read_html("http://www.iciba.com/past")	# getURL cannot work

jan09_parsed &lt;- htmlTreeParse(jan09)

==================
http://www.iciba.com/past
ul class="base-list switch_part" class

library('rvest')
library(tidyverse)
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
webpage &lt;- read_html(webpage)
grappedData &lt;- html_nodes(webpage,'.base-list switch_part')

parseData = htmlTreeParse(webpage)

rank_data &lt;- html_text(grappedData)

html_node("#mw-content-text > div > table:nth-child(18)")
html_table()

the function htmlParse() which is equivalent to xmlParse(file, isHTML = TRUE)
output = htmlParse(webpage)
class(output)

To parse content into an R structure :
htmlTreeParse() which is equivalent to htmlParse(file, useInternalNodes = FALSE)
output = htmlTreeParse(webpage)
class(output)

htmlTreeParse(file) especially suited for parsing HTML content
returns class "XMLDocumentContent" (R data structure)
equivalent to
xmlParse(file, isHTML = TRUE, useInternalNodes = FALSE)
htmlParse(file, useInternalNodes = FALSE)

root =xmlRoot(output)
xmlChildren(output)
xmlChildren(xmlRoot(output))
XMLNodeList

Functions for a given node
Function Description
xmlName() name of the node
xmlSize() number of subnodes
xmlAttrs() named character vector of all attributes
xmlGetAttr() value of a single attribute
xmlValue() contents of a leaf node
xmlParent() name of parent node
xmlAncestors() name of ancestor nodes
getSibling() siblings to the right or to the left
xmlNamespace() the namespace (if there’s one)

to parse HTML tables using R
sched &lt;- readHTMLTable(html, stringsAsFactors = FALSE)

The html.raw object is not immediately useful because it literally contains all of the raw HTML for the entire webpage. We can parse the raw code using the xpathApply function which parses HTML based on the path argument, which in this case specifies parsing of HTML using the paragraph tag.

html.raw&lt;-htmlTreeParse('http://www.dnr.state.mn.us/lakefind/showreport.html?downum=27013300',
    useInternalNodes=T    )
html.parse&lt;-xpathApply(html.raw, "//p", xmlValue)

# evaluate input and convert to text
txt &lt;- htmlToText(url)

==================
url &lt;- 'http://www.iciba.com/past'
webpage &lt;- readLines(url, warn=FALSE)
scraping_wiki &lt;- read_html(webpage)
scraping_wiki %>% html_nodes("h1") %>% html_text()

url &lt;- 'testvibrate.html'
webpage &lt;- readLines(url, warn=FALSE)
x &lt;- read_xml(webpage)
xml_name(x)
===========

This cannot work in office
library(rvest)
Sys.setlocale(category = 'LC_ALL', 'Chinese')
webpage &lt;- read_html("http://www.iciba.com/haunt")
ullist = webpage %>% html_nodes("ul")
content = ullist[2] %>% html_text()
content = gsub("n.| |\n|adj.|adv.|prep.|vt.|vi.|&","",content)
content = gsub("，|；"," ",content) %>%  strsplit(split = " ") %>% unlist() %>% sort() %>% unique()
paste0("past","\t",capture.output(cat(content)))


<br>
<br>
<h2>R scraping html text example</h2>

<a href="http://bradleyboehmke.github.io/2015/12/scraping-html-text.html">scraping-html-text</a>
library(rvest)

scraping_wiki &lt;- read_html("https://en.wikipedia.org/wiki/Web_scraping")

scraping_wiki %>% html_nodes("h1")
scraping_wiki %>% html_nodes("h2")
scraping_wiki %>% html_nodes("h1") %>% html_text()
scraping_wiki %>% html_nodes("h2") %>% html_text()
p_nodes &lt;- scraping_wiki %>% html_nodes("p")
length(p_nodes)
p_text &lt;- scraping_wiki %>% html_nodes("p") %>% html_text()
p_text[1]
p_text[5]

ul_text &lt;- scraping_wiki %>% html_nodes("ul") %>% html_text()
length(ul_text)
ul_text[1]
substr(ul_text[2], start = 1, stop = 200)

li_text &lt;- scraping_wiki %>% html_nodes("li") %>% html_text()
length(li_text)
li_text[1:8]
li_text[104:136]

all_text &lt;- scraping_wiki %>% html_nodes("div") %>%  html_text()

body_text &lt;- scraping_wiki %>% html_nodes("#mw-content-text") %>%  html_text()

# read the first 207 characters
substr(body_text, start = 1, stop = 207)

# read the last 73 characters
substr(body_text, start = nchar(body_text)-73, stop = nchar(body_text))

# Scraping a specific heading
scraping_wiki %>% html_nodes("#Techniques") %>%  html_text()
## [1] "Techniques"

# Scraping a specific paragraph
scraping_wiki %>% html_nodes("#mw-content-text > p:nth-child(20)") %>%  html_text()

# Scraping a specific list
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

# Scraping a specific reference list item
scraping_wiki %>% html_nodes("#cite_note-22") %>%  html_text()

# Cleaning up
library(magrittr)
scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text()

scraping_wiki %>% html_nodes("#mw-content-text > div:nth-child(22)") %>%  html_text() %>%  strsplit(split = "\n") %>% unlist() %>% .[. != ""]


library(stringr)

# read the last 700 characters
substr(body_text, start = nchar(body_text)-700, stop = nchar(body_text))

# clean up text
body_text %>% 
str_replace_all(pattern = "\n", replacement = " ") %>% 
str_replace_all(pattern = "[\\^]", replacement = " ") %>% 
str_replace_all(pattern = "\"", replacement = " ") %>% 
str_replace_all(pattern = "\\s+", replacement = " ") %>% 
str_trim(side = "both") %>% 
substr(start = nchar(body_text)-700, stop = nchar(body_text))

################
# rvest tutorials
https://blog.rstudio.com/2014/11/24/rvest-easy-web-scraping-with-r/
https://blog.gtwang.org/r/rvest-web-scraping-with-r/
https://www.rdocumentation.org/packages/rvest/versions/0.3.4
https://www.datacamp.com/community/tutorials/r-web-scraping-rvest
https://stat4701.github.io/edav/2015/04/02/rvest_tutorial/
https://lmyint.github.io/post/dnd-scraping-rvest-rselenium/

################
# parse guancha
library(rvest)
pageHeader="https://user.guancha.cn/main/content?id=181885"
pagesource &lt;- read_html(pageHeader)

################
# parse RTHK and metroradio
library(rvest)
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)
className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)

pageHeader = "http://www.metroradio.com.hk/MetroFinance/News/NewsLive.aspx"
pagesource &lt;- read_html(pageHeader)
className = ".n13newslist"
keywordList &lt;- html_nodes(pagesource, className)
className = "a"
keywordList &lt;- html_nodes(keywordList, className)
html_text(keywordList)

################
# parse xhamster
library(rvest)
pageHeader = "https://xhamster.com/users/fredlake/photos"
pagesource &lt;- read_html(pageHeader)
className = ".xh-paginator-button"
keywordList &lt;- html_nodes(pagesource, className)
html_text(keywordList)
html_name(keywordList)
html_attrs(keywordList)

thelist = unlist(html_attrs(keywordList))

length(keywordList)
as.numeric(html_text(keywordList[length(keywordList)]))

pagesource %>% html_nodes(className) %>% html_text() %>% as.numeric()

for ( i in keywordList ) { 
 qlink &lt;- html_nodes(s, ".gallery-thumb")
 cat("Title:", html_text(qlink), "\n")
 qviews &lt;- html_nodes(s, "name")
 cat("Views:", html_text(qviews), "\n")
}
################
# parse text and href
pageHeader = "http://news.rthk.hk/rthk/ch/latest-news.htm"
pagesource &lt;- read_html(pageHeader)

className = ".ns2-title"
keywordList &lt;- html_nodes(pagesource, className)

className = "a"
a &lt;- html_nodes(keywordList, className)

html_text(a)
html_attr(a, "href")

################
# extract huanqiu.com gallery

pageHeader = "https://china.huanqiu.com/gallery/9CaKrnQhXac"
pagesource &lt;- read_html(pageHeader)
className = "article"
keywordList &lt;- html_nodes(pagesource, className)

className = "img"
img &lt;- html_nodes(keywordList, className)
html_attr(img, "src")
html_attr(img, "data-alt")

################
# html_nodes samples
html_nodes(".a1.b1")
html_nodes(".b1:not(.a1)")  # <i>Select class contains b1 not a1:</i>
html_nodes(".content__info__item__value")
html_nodes("[class='b1']")
html_nodes("center")
html_nodes("font")
html_nodes(ateam, "center")
html_nodes(ateam, "center font")
html_nodes(ateam, "center font b")
html_nodes("table") %>% .[[3]] %>% html_table()
html_nodes("td")
html_nodes() returns all nodes
html_nodes(pagesource, className)
html_nodes(pg, "div > input:first-of-type"), "value")
html_nodes(s, ".gallery-thumb")
html_nodes(s, "name")
html_nodes(xpath = '//*[@id="a"]')

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")
td %>% html_nodes("font")

if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_node("font")
}

# To pick out an element at specified position, use magrittr::extract2
# which is an alias for [[
library(magrittr)
ateam %>% html_nodes("table") %>% extract2(1) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% `[[`(1) %>% html_nodes("img")

# Find all images contained in the first two tables
ateam %>% html_nodes("table") %>% `[`(1:2) %>% html_nodes("img")
ateam %>% html_nodes("table") %>% extract(1:2) %>% html_nodes("img")

# XPath selectors ---------------------------------------------
# If you prefer, you can use xpath selectors instead of css: 
html_nodes(doc, xpath = "//table//td")).

# chaining with XPath is a little trickier - you may need to vary
# the prefix you're using - // always selects from the root node
# regardless of where you currently are in the doc
ateam %>% html_nodes(xpath = "//center//font//b") %>% html_nodes(xpath = "//b")


read_html()
html_node()	# to find the first node
html_nodes(doc, "table td")	# to find the all node
html_nodes(doc, xpath = "//table//td"))

html_name()	# the name of the tag
html_tag()	# Extract the tag names
html_text()	# Extract all text inside the tag 

html_attr()	Extract the a single attribute
html_attrs()	Extract all the attributes

# html_attrs(keywordList) this cannot use id, just list all details
# html_attr(keywordList, "id") this select the ids
# html_attr(keywordList, "href") this select the hrefs

html_nodes("#titleCast .itemprop span")
html_nodes("#img_primary img")
html_nodes("div.name > strong > a")
html_attr("href")

html_text(keywordList, trim = FALSE)
html_name(keywordList)
html_children(keywordList)
html_attrs(keywordList)
html_attr(keywordList, "[href]", default = NA_character_)

parse with xml()
then extract components using 
xml_node()
xml_attr()
xml_attrs()
xml_text() and xml_name()

Parse tables into data frames with 
html_table().

Extract, modify and submit forms with 
html_form()
set_values()
submit_form().

Detect and repair encoding problems with 
guess_encoding()	Detect text encoding
repair_encoding()	repair text encoding

Navigate around a website as if you’re in a browser with 
html_session()
jump_to()
follow_link()
back()
forward()

Extract, modify and submit forms with 
html_form(), 
set_values() 
and submit_form()


The toString() function collapse the list of strings into one.

html_node(":not(#commentblock)")	# exclude tags

######### demos #########
# Inspired by https://github.com/notesofdabbler
library(rvest)
library(tidyr)

page &lt;- read_html("http://www.zillow.com/homes/for_sale/....")

houses &lt;- page %>% html_nodes(".photo-cards li article")
z_id &lt;- houses %>% html_attr("id")

address &lt;- houses %>% html_node(".zsg-photo-card-address") %>% html_text()

price &lt;- houses %>% html_node(".zsg-photo-card-price") %>% html_text() %>% readr::parse_number()

params &lt;- houses %>% html_node(".zsg-photo-card-info") %>% html_text() %>% strsplit("\u00b7")

beds &lt;- params %>% purrr::map_chr(1) %>% readr::parse_number()
baths &lt;- params %>% purrr::map_chr(2) %>% readr::parse_number()
house_area &lt;- params %>% purrr::map_chr(3) %>% readr::parse_number()

################
pagesource %>% html_nodes("table") %>% .[[3]] %>% html_table()

read_html(doc) %>% html_nodes(".b1:not(.a1)") # <i>Select class contains b1 not a1:</i>
# [1] <span class="b1"> text2 </span>

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")
# [1] <span class="b1"> text2 </span>

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
# [1] <span class="a1 b1"> text1 </span>

combine class and ID in CSS selector
div#content.sectionA  # <i>this is 'and' operation</i>

=====================
<i>select 2 classes in 1 tag</i>
Select class contains b1 not a1:
read_html(doc) %>% html_nodes(".b1:not(.a1)")

<i>use the attribute selector:</i>
read_html(doc) %>% html_nodes("[class='b1']")

<i>Select class contains both:</i>
read_html(doc) %>% html_nodes(".a1.b1")  # <i>this is 'and' operation</i>
=====================
standard CSS selector specify either or both

html_nodes(".content__info__item__value, skill")  # <i>the comma is 'or' operation</i>
{xml_nodeset (4)}
[1] <span class="content__info__item__value duration">5h 59m 42s</span>
[2] <span class="content__info__item__value skill">Beginner + Intermediate</span>
[3] <span class="content__info__item__value released">September 26, 2013</span>
[4] <span class="content__info__item__value viewers">82,552</span>

# has both classes in_learning_page
html_nodes(".content__info__item__value.skill")   # <i>this is 'and' operation</i>
{xml_nodeset (1)}
[1] <span class="content__info__item__value skill">Beginner + Intermediate</span>


in_learning_page %>%
  html_nodes(".content__info__item__value") %>% 
  str_subset(., "viewers")

h &lt;- read_html(text)

h %>% html_nodes(xpath = '//*[@id="a"]') %>% xml_attr("value")

html_attr(html_nodes(pg, "div > input:first-of-type"), "value")

ateam %>% html_nodes("center") %>% html_nodes("td")
ateam %>% html_nodes("center") %>% html_nodes("font")

td &lt;- ateam %>% html_nodes("center") %>% html_nodes("td")

# When applied to a list of nodes, html_nodes() returns all nodes,
# collapsing results into a new nodelist.
td %>% html_nodes("font")
# nodes, it returns a "missing" node
if (utils::packageVersion("xml2") > "0.1.2") {
  td %>% html_nodes("font")


<br>
<br>
<h2>sort() rank() order()</h2>

<strong>Rank</strong> references the position of the value in the sorted vector and is in the same order as the <strong>original </strong>sequence
<strong>Order</strong> returns the position of the original value and is in the order of <strong>sorted </strong>sequence
The graphic below helps tie together the values reported by rank and order with the positions from which they come.
<img class="lazy" data-src="https://cdn-images-1.medium.com/max/800/1*3KeaXU6luJDyoatWkEwdug.jpeg">
x = c(1, 8,9, 4)
sort(x)
1 4 8 9

# the original position in the sorted order
rank(x)
1 3 4 2

# the sorted position in the original position
order(x)
1 4 2 3

<br>
<br>
<h2>Bioinformatics</h2>

<a href="https://cran.r-project.org/doc/contrib/Krijnen-IntroBioInfStatistics.pdf">Bioinformatics using R</a>
<br>
<a href="https://www.bioconductor.org/">bioconductor</a>
<br>
<a href="https://www.r-exercises.com/product/introduction-to-bioconductor-annotation-and-analysis-of-genomes-and-genomic-assays/">Introduction to Bioconductor:Annotation and Analysis of Genomes and Genomics Assays</a>

<br>
<h2>a list of dataframes, 3D data arrangement</h2>

d1 &lt;- data.frame(y1=c(1,2,3),y2=c(4,5,6))
d2 &lt;- data.frame(y1=c(3,2,1),y2=c(6,5,4))
d3 &lt;- data.frame(y1=c(7,8,9),y2=c(5,2,6))
mylist &lt;- list(d1, d2, d3)
names(mylist) &lt;- c("List1","List2","List3")

mylist[1]	# same as mylist$List1

mylist[[2]][1,2]	# access an element inside a dataframe
mylist[[2]][2,2]	# same as mylist$List2[2,2]

to concate another dataframe:

d4 &lt;- data.frame(y1=c(2,5,8),y2=c(1,4,7))
mylist[[4]] &lt;- d4

to create an empty list:
data &lt;- list()

<br>
<br>
<h2>format time string</h2>

Sys.time()

sub(".* | .*", "", Sys.time())

format(Sys.time(), '%H:%M')

gsub(":", "", format(Sys.time(), '%H:%M'))


<br>
<h2>extract 5 digit from string</h2>

activityListCode = str_replace(activityListCode, ".*\\b(\\d{5})\\b.*", "\\1")


<br>
<h2>access Components of a Data Frame</h2>
<a href="https://www.datamentor.io/r-programming/data-frame/">access Components of a Data Frame</a>
<br>

Components of data frame can be accessed like a list or like a matrix.

<h3>Accessing like a list</h3>
We can use either <code>[</code>, <code>[[</code> or <code>$</code> operator to access columns of data frame.

<code>&gt; x["Name"]
Name
1 John
2 Dora
&gt; x$Name
[1] "John" "Dora"
&gt; x[["Name"]]
[1] "John" "Dora"
&gt; x[[3]]
[1] "John" "Dora"
</code>
Accessing with <code>[[</code> or <code>$</code> is similar. However, it differs for <code>[</code> in that, indexing with <code>[</code> <span class="redword">will return us a data frame</span> but the other two will <span class="redword">reduce it into a vector</span>.


<h3>Accessing like a matrix</h3>
Data frames can be accessed like a matrix by providing index for row and column.

To illustrate this, we use datasets already available in R. Datasets that are available can be listed with the command <code>library(help = "datasets")</code>.

We will use the <code>trees</code> dataset which contains <code>Girth</code>, <code>Height</code> and <code>Volume</code> for Black Cherry Trees.

A data frame can be examined using functions like <code>str()</code> and <code>head()</code>.

<code>&gt; str(trees)
'data.frame':   31 obs. of 3 variables:
$ Girth : num  8.3 8.6 8.8 10.5 10.7 10.8 11 11 11.1 11.2 ...
$ Height: num  70 65 63 72 81 83 66 75 80 75 ...
$ Volume: num  10.3 10.3 10.2 16.4 18.8 19.7 15.6 18.2 22.6 19.9 ...
&gt; head(trees,n=3)
Girth Height Volume
1   8.3     70   10.3
2   8.6     65   10.3
3   8.8     63   10.2
</code>
We can see that <code>trees</code> is a data frame with 31 rows and 3 columns. We also display the first 3 rows of the data frame.

Now we proceed to access the data frame like a matrix.

<code>&gt; trees[2:3,]    # select 2nd and 3rd row
Girth Height Volume
2   8.6     65   10.3
3   8.8     63   10.2
&gt; trees[trees$Height &gt; 82,]    # selects rows with Height greater than 82
Girth Height Volume
6   10.8     83   19.7
17  12.9     85   33.8
18  13.3     86   27.4
31  20.6     87   77.0
&gt; trees[10:12,2]
[1] 75 79 76
</code>
We can see in the last case that the returned type is a vector since we extracted data from a single column.

This behavior can be avoided by passing the argument <code>drop=FALSE</code> as follows.

<code>&gt; trees[10:12,2, drop = FALSE]
Height
10     75
11     79
12     76
</code>


# access first row by index, returns a data.frame
x[1,]

# access first row by "name", returns a data.frame
> x["1",]

# access first row returns a vector
use as.numeric
str(as.numeric(wAveTable["1",]))

unlist which keeps the names.
str(unlist(wAveTable["1",]))

use transpose and as.vector
str(as.vector(t(wAveTable["1",])[,1]))

use only as.vector cannot convert to vector
str(as.vector(wAveTable["1",]))

# convert dataframe to matrix
data.matrix(wAveTable)


<br>
<br>
<h2>read.csv as character</h2>

wAveTable = read.csv("wAveTable.txt", sep="\t", colClasses=c('character', 'character', 'character'))

<br>
<br>
<h2>frequency manipulation</h2>

grade = c("low", "high", "medium", "high", "low", "medium", "high")

# using factor to count the frequency
foodfac &lt;- factor(grade)
summary(foodfac)
max(summary(foodfac))
min(summary(foodfac))
levels(foodfac)
nlevels(foodfac)
summary(levels(foodfac))

# use of table to count frequency:
table(grade)
sort(table(grade))

table(grade)[1]
max(table(grade))
summary(table(grade))

# this locate the max item:
table(grade)[which(table(grade) == max(table(grade)))]

# change to dataframe and find the max item:
theTable = as.data.frame(table(grade))
theTable[which(theTable$Freq == max(theTable$Freq)),]

# use of the count function in plyr:
library(plyr)
count(grade)
count(mtcars, 'gear')

# use of the which function:
which(letters == "g")
x &lt;- c(1,5,8,4,6)
which(x == 5)
which(x != 5)


<br>
<br>
<h2>5 must have R programming tools</h2>

<h4>1) RStudio</h4>
<h4>2) lintr</h4>
If you come from the world of Python, you’ve probably heard of 
<a href="https://stackoverflow.com/questions/8503559/what-is-linting" data-href="https://stackoverflow.com/questions/8503559/what-is-linting" rel="nofollow noopener" target="_blank">linting</a>. 
Essentially, linting 
<a href="https://en.wikipedia.org/wiki/Lint_%28software%29" data-href="https://en.wikipedia.org/wiki/Lint_%28software%29" rel="nofollow noopener" target="_blank">analyzes</a> your code for readability. 
It makes sure you don’t produce code that looks like this:
# This is some bad R code
<br>if ( mean(x,na.rm=T)==1) { print(“This code is bad”); } # Still bad code because this line is SO long
There are 
<em>many</em> things wrong with this code. 
For starters, the code is too long. 
Nobody likes to read code with seemingly endless lines. 
There are also no spaces after the comma in the 
<code>mean()</code> function, or any spaces between the 
<code>==</code> operator. 
Oftentimes data science is done hastily, but linting your code is a good reminder for creating portable and understandable code. 
After all, if you can’t explain what you are doing or how you are doing it, your data science job is incomplete. 

<a href="https://cran.r-project.org/web/packages/lintr/index.html" data-href="https://cran.r-project.org/web/packages/lintr/index.html" rel="nofollow noopener" target="_blank">lintr</a> is an R package, growing in popularity, that allows you to lint your code. 
Once you install lintr, linting a file is as easy as 
<code>lint(&quot;filename.R&quot;)</code> .
<h4>3) Caret</h4>
<a href="http://topepo.github.io/caret/index.html" data-href="http://topepo.github.io/caret/index.html" rel="nofollow noopener" target="_blank">Caret</a>, which you can find on 
<a href="https://cran.r-project.org/web/packages/caret/caret.pdf" data-href="https://cran.r-project.org/web/packages/caret/caret.pdf" rel="nofollow noopener" target="_blank">CRAN</a>, is central to a data scientist’s toolbox in R. 
Caret allows one to quickly develop models, set cross-validation methods and analyze model performance all in one. 
Right out of the box, Caret abstracts the various interfaces to user-made algorithms and allows you to swiftly create models from averaged neural networks to boosted trees. 
It can even handle parallel processing. 
Some of the models caret includes are: AdaBoost, Decision Trees &amp; Random Forests, Neural Networks, Stochastic Gradient Boosting, nearest neighbors, support vector machines — among the most commonly used machine learning algorithms.
<h4>4) Tidyverse</h4>
You may not have heard of 
<code>tidyverse</code> as a whole, but chances are, you’ve used one of the packages in it. 
Tidyverse is a set of unified packages meant to make data science… 
<em>easyr</em> (classic R pun). 
These packages alleviate many of the problems a data scientist may run into when dealing with data, such as loading data into your workspace, manipulating data, tidying data or visualizing data. 
Undoubtedly, these packages make dealing with data in R more efficient.
It’s incredibly easy to get Tidyverse, you just run 
<code>install.packages(&quot;tidyverse&quot;)</code> and you get:
<a href="http://ggplot2.tidyverse.org/" data-href="http://ggplot2.tidyverse.org/" rel="nofollow noopener" target="_blank">ggplot2</a>: A popular R package for creating graphics
<a href="http://dplyr.tidyverse.org/" data-href="http://dplyr.tidyverse.org/" rel="nofollow noopener" target="_blank">dplyr</a>: A popular R package for efficiently manipulating data
tidyr: An R package for tidying up data sets
<a href="http://readr.tidyverse.org/" data-href="http://readr.tidyverse.org/" rel="nofollow noopener" target="_blank">readr</a>: An R package for reading in data
<a href="http://purrr.tidyverse.org/" data-href="http://purrr.tidyverse.org/" rel="nofollow noopener" target="_blank">purrr</a>: An R package which extends R’s functional programming toolkit
<a href="http://tibble.tidyverse.org/" data-href="http://tibble.tidyverse.org/" rel="nofollow noopener" target="_blank">tibble</a>: An R package which introduces the 
<em>tibble (tbl_df)</em>, an enhancement of the data frame
By and large, ggplot2 and dplyr are some of the most common packages in the R sphere today, and you’ll see countless posts on StackOverflow on how to use either package.

<em>(Fine Print: Keep in mind, you can’t just load everything with </em>
<code>
<em>library(tidyverse)</em></code>
<em> you must load each individually!)</em>
<h4>5) Jupyter Notebooks or R Notebooks</h4>
Data science 
<em>MUST</em> be transparent and reproducible. 
For this to happen, we have to see your code! The two most common ways to do this are through 
<a href="http://jupyter.org/" data-href="http://jupyter.org/" rel="nofollow noopener" target="_blank">Jupyter Notebooks</a> or 
<a href="http://rmarkdown.rstudio.com/r_notebooks.html" data-href="http://rmarkdown.rstudio.com/r_notebooks.html" rel="nofollow noopener" target="_blank">R Notebooks</a>.
Essentially, a notebook (of either kind) allows you to run R code block by block, and show output block my block. 
We can see on the left that we are summarizing the data, then checking the output. 
After, we plot the data, then view the plot. 
All of these actions take place within the notebook, and it makes analyzing both output and code a simultaneous process. 
This can help data scientists collaborate and ease the friction of having to open up someone’s code and understand what it does. 
Additionally, notebooks also make data science 
<em>reproducible</em>, which gives validity to whatever data science work you do!
<h4>Honorable Mention: Git</h4>
Last but not least, I want to mention Git. 
Git is a 
<em>version control</em> system. 
So why use it? Well, it’s in the name. 
Git allows you to keep versions of the code you are working on. 
It also allows multiple people to work on the same project and allows those changes to be attributed to certain contributors. 
You’ve probably heard of 
<a href="http://www.github.com" data-href="http://www.github.com" rel="nofollow noopener" target="_blank">Github</a>, undoubtedly one of the most popular git servers.
You can visit my website at 
<a href="http://www.peterxeno.com" data-href="http://www.peterxeno.com" rel="nofollow noopener" target="_blank">www.peterxeno.com</a> and my Github at 
<a href="https://github.com/peterxeno" data-href="https://github.com/peterxeno" rel="nofollow noopener" target="_blank">www.github.com/peterxeno</a>


<h2>R with Javascript</h2>
<a href="R and D3.html">R and D3</a>
<a href="https://www.opencpu.org/posts/js-release-0-1/">tools for working with JavaScript in R</a>
<a href="http://www.di.fc.ul.pt/~jpn/r/langs/javascript.html">R Connecting with Javascript</a>

<h2><span class="white bordgreen1">tryCatch</span></h2>
<a href="https://codeday.me/bug/20170502/13495.html">如何在R中写trycatch</a>
<a href="https://stackoverflow.com/questions/12193779/how-to-write-trycatch-in-r">write trycatch in R</a>
<a href="http://mazamascience.com/WorkingWithData/?p=912">error Handing with tryCatch()</a>

e.g.
readUrl = function(url) {
    out = tryCatch(
        {
            message("This is the 'try' part")
            readLines(con=url, warn=FALSE) 
        },
        error=function(cond) {
            message(paste("URL does not seem to exist:", url))
            message("Here's the original error message:")
            message(cond)
            # Choose a return value in case of error
            return(NA)
        },
        warning=function(cond) {
            message(paste("URL caused a warning:", url))
            message("Here's the original warning message:")
            message(cond)
            # Choose a return value in case of warning
            return(NULL)
        },
        finally={
        # Here goes everything that should be executed at the end,
        # regardless of success or error.
        # If you want more than one expression to be executed, then you 
        # need to wrap them in curly brackets ({...}); otherwise you could
        # just have written 'finally={expression}' 
            message(paste("Processed URL:", url))
            message("Some other message at the end")
        }
    )    
    return(out)
}

e.g.
x &lt;- tryCatch( readLines("wx.qq.com/"), warning=function(w){ return(paste( "Warning:", conditionMessage(w)));}, 
error = function(e) { return(paste( "this is Error:", conditionMessage(e)));}, 
finally={print("This is try-catch test. check the output.")});

e.g.
for (i in urls) {
    tmp &lt;- tryCatch(readLines(url(i), warn=F), error = function (e) NULL)
    if (is.null(tmp)) {
        next() # skip to the next url.
    }
}

a retry function:
retry &lt;- function(dothis, max = 10, init = 0){
	suppressWarnings( tryCatch({
		if(init&lt;max) dothis}, 
			error = function(e){retry(dothis, max, init = init+1)}
		)
	)
}
dothis &lt;- function(){do somthing}

<h2>Download Image</h2>
<a href="https://stackoverflow.com/questions/29110903/how-to-download-and-display-an-image-from-an-url-in-r">Download Image</a>
<br>
If I try your code it looks like the image is downloaded. However, when opened with windows image viewer it also says it is corrupt. The reason for this is that you don't have specified the mode in the download.file statement.

Try this:

download.file(y,'y.jpg', mode = 'wb')

download.file('http://78.media.tumblr.com/83a81c41926c1da585916a5c092b4789/tumblr_or0y0vdjOP1rttk8po1_1280.jpg','y.jpg', mode = 'wb')

To view the image in R, have a look at

library(jpeg)
jj &lt;- readJPEG("y.jpg",native=TRUE)
plot(0:1,0:1,type="n",ann=FALSE,axes=FALSE)
rasterImage(jj,0,0,1,1)

<h2>Download Something</h2>
Download Something
Download Something
Download Something

<h2>testShiny</h2>
setwd("D:/KPC/testShiny")
runApp("D:/KPC/testShiny")


<h2>Error in file(filename, "r", encoding = encoding)</h2>
The error indicate that either the file doesn't exist or the source() command an incorrect path. 

<h2>call a R program from another R program</h2>
source("program_B.R")

<h2>to view all the functions present in a package</h2>

To list all objects in the package use ls
ls("package:Hmisc")
Note that the package must be attached.

To list all strings
lsf.str("package:dplyr")
lsf.str("package:Hmisc")

To see the list of currently loaded packages use
search()

Alternatively calling the help would also do, even if the package is not attached:
help(package = dplyr)
help(package = Hmisc)

Finally, use RStudio which provides an autocomplete function.
So, for instance, typing Hmisc:: in the console or while editing a file will result in a popup list of all dplyr functions/objects.



<h2>cut2</h2>

Function like cut but left endpoints are inclusive.

install.packages("Hmisc")
library(Hmisc)

alist = c(-15,18,2,5,4,-7,-5,-3,-1,0,2,1,5,4,6)
breaks = c(-5,-3,-1,0,1,3,5)
table(cut2(alist, breaks))

<h2>Reference A Data Frame Column</h2>

with the double square bracket "[[]]" operator.
LastDayTable[["Vol"]] 
or
LastDayTable$Vol
or
<span class="cyanword">LastDayTable[,"Vol"] </span>

<h2>Writing data to a file</h2>
<h3 id="problem">Problem</h3>

You want to write data to a file.


<h3 id="solution">Solution</h3>

<h3 id="writing-to-a-delimited-text-file">Writing to a delimited text file</h3>

The easiest way to do this is to use <code>write.csv()</code>. By default, <code>write.csv()</code> includes row names, but these are usually unnecessary and may cause confusion.


<code># A sample data frame
data &lt;- read.table(header=TRUE, text='
 subject sex size
       1   M    7
       2   F    NA
       3   F    9
       4   M   11
 ')


# Write to a file, suppress row names
write.csv(data, "data.csv", row.names=FALSE)

# Same, except that instead of "NA", output blank cells
write.csv(data, "data.csv", row.names=FALSE, na="")

# Use tabs, suppress row names and column names
write.table(data, "data.csv", sep="\t", row.names=FALSE, col.names=FALSE) 
</code>

<h3>Saving in R data format</h3>

<code>write.csv()</code> and <code>write.table()</code> are best for interoperability with other data analysis programs. They will not, however, preserve special attributes of the data structures, such as whether a column is a character type or factor, or the order of levels in factors. In order to do that, it should be written out in a special format for R.


Below are are three primary ways of doing this:


The first method is to output R source code which, when run, will re-create the object. This should work for most data objects, but it may not be able to faithfully re-create some more complicated data objects.


<code># Save in a text format that can be easily loaded in R
dump("data", "data.Rdmpd")
# Can save multiple objects:
dump(c("data", "data1"), "data.Rdmpd")

# To load the data again: 
source("data.Rdmpd")
# When loaded, the original data names will automatically be used.
</code>

The next method is to write out individual data objects in RDS format. This format can be binary or ASCII. Binary is more compact, while ASCII will be more efficient with version control systems like Git.

<code># Save a single object in binary RDS format
saveRDS(data, "data.rds")
# Or, using ASCII format
saveRDS(data, "data.rds", ascii=TRUE)

# To load the data again:
data &lt;- readRDS("data.rds")
</code>

It’s also possible to save multiple objects into an single file, using the RData format.


<code># Saving multiple objects in binary RData format
save(data, file="data.RData")
# Or, using ASCII format
save(data, file="data.RData", ascii=TRUE)
# Can save multiple objects
save(data, data1, file="data.RData")

# To load the data again:
load("data.RData")
</code>
An important difference between <code>saveRDS()</code> and <code>save()</code> is that, with the former, when you <code>readRDS()</code> the data, you specify the name of the object, and with the latter, when you <code>load()</code> the data, the original object names are automatically used. Automatically using the original object names can sometimes simplify a workflow, but it can also be a drawback if the data object is meant to be distributed to others for use in a different environment.


<h2>Debugging a script or function</h2>
    <h3>Problem</h3>

You want to debug a script or function.


<h3>Solution</h3>

Insert this into your code at the place where you want to start debugging:


browser()

When the R interpreter reaches that line, it will pause your code and you will be able to look at and change variables.


In the browser, typing these letters will do things:


<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>c</td>
      <td>Continue</td>
    </tr>
    <tr>
      <td>n (or Return)</td>
      <td>Next step</td>
    </tr>
    <tr>
      <td>Q</td>
      <td>quit</td>
    </tr>
    <tr>
      <td>Ctrl-C</td>
      <td>go to top level</td>
    </tr>
  </tbody>
</table>

When in the browser, you can see what variables are in the current scope.


ls()

To pause and start a browser for every line in your function:


debug(myfunction)
myfunction(x)

<h3>Useful options</h3>

By default, every time you press Enter at the browser prompt, it runs the next step. This is equivalent to pressing <code class="highlighter-rouge">n</code> and then Enter. This can be annoying. To disable it use:

options(browserNLdisabled=TRUE)

To start debugging whenever an error is thrown, run this before your function which throws an error:


options(error=recover)

If you want these options to be set every time you start R, you can put them in your ~/.Rprofile file.

<h2>data.table vs data.frame</h2>
<a href="data.table vs data.frame.html">data.table vs data.frame</a>
<br>
<a href="Introduction to data.table.html">Introduction to data.table</a>
<br>
<a href="http://rstudio-pubs-static.s3.amazonaws.com/52230_5ae0d25125b544caab32f75f0360e775.html">JOINing data in R using data.table</a>
<br>
<a href="Advanced tips and tricks with data.table.html"><span class="goldwhiteb">&diams;Advanced tips and tricks with data.table</span></a>
<br>

X = data.table(a=1:5, b=6:10, c=c(5:1))

length(X[b %between% c(7,9)])
length(X[b %inrange% c(7,9)])

# inrange()
Y = data.table(a=c(8,3,10,7,-10), val=runif(5))
range = data.table(start = 1:5, end = 6:10)
Y[a %inrange% range]

https://stackoverflow.com/questions/16652533/insert-a-row-in-a-data-table
insert-a-row-in-a-data-table
dt1 &lt;- list(1,4,7)
rbind(dt1, X)

dt1 &lt;- data.table(1,4,7)
rbindlist(list(dt1, X))

===================
use data.frame
df &lt;- data.frame( name=c("John", "Adam"), date=c(3, 5) )

Extract exact matches:

subset(df, date==3)
nrow(subset(df, date==3))

Extract matches in range:

subset(df, date>4 & date&lt;6)

  name date
2 Adam    5



<a href="Data.Table Tutorial.html"><span class="silverredb">&diams;Data.Table Tutorial</span></a>
<br>

<a href="http://brooksandrew.github.io/simpleblog/articles/advanced-data-table/">Advanced tips and tricks with data.table package</a>
<br>

<br>
<h2>DiagrammeR</h2>
<a href="http://rich-iannone.github.io/DiagrammeR/graphviz_and_mermaid.html">DiagrammeR</a><br>
<a href="DiagrammeR Docs.html">DiagrammeR Docs</a>

<br>
<h2>ROC Curves</h2>
<a href="ROC Curves.html">ROC Curves</a>
<br>
<h2>capture.output to file</h2>
capture.output(options(), file="temp.txt")

<h2>writing functions</h2>

a simple function:

square = function(x){x*x}

to square a vector:

x = c(1,3,5)
square(x)

to square a matrix:
x = cbind(c(1,3),c(5,7))
square(x)

to return a list of objects, use list():
square = function(x){return(list(x*x,x*x*x))}
square(x)

using debug() to debug:
debug(square)
square(x-a)

using print to debug in function:
square = function(x){
print(x)
print(x*x)
x*x
}

using stop() and stopifnot() to write your own error msg:
squareRoot = function(x){
	if(x&lt;0){
		stop("cannot use negative number!")
	}
	sqrt(x)
}
squareRoot(-1)

good function practices:
keep short function
write comments
try with examples
use debug and error msg

<h2>For Loop in R with Examples</h2>
<a href="https://www.guru99.com/r-for-loop.html">For Loop in R with Examples</a>

<h2>case_when and switch</h2>

switch("shape", "color" = "red", "shape" = "square", "length" = 5)

library(dplyr)
Length=3.5
mode &lt;- case_when(
                (Length < 1) ~ "Walk",
                (1 <= Length & Length < 5) ~ "bike",
                (5 <= Length & Length < 10) ~ "drive",
                (Length >= 10) ~ "fly"
          )

<h2>Calling multiple external program from R</h2>

<a href="https://stackoverflow.com/questions/21966209/calling-external-program-from-r-with-multiple-commands-in-system">Calling multiple external program from R</a>
<br>
for(i in 1:10){
cmd=paste("export FOO=",i," ; echo \"$FOO\" ",sep='')
system(cmd)
}

<h2>rmItems</h2>
# rmItems(fmList, itemList) remove itemList from fmList
 rmItems &lt;- function(fmList, itemList){return(fmList [! fmList %in% itemList])}

fmList = 1:10
itemList = c(2,4,5)
rmItems(fmList, itemList)

# remove fraudSTK
 CodeTable = rmItems(CodeTable, fraudSTK)

milList8 = c("a","b","c","d")
milList20 = c("a","b","c","f")
setdiff(fmList,itemList)


<h2>R porjects</h2>
<a href="R porjects.html">R porjects</a>

<h2>call C from R</h2>
<a href="call C from R.html">call C from R</a>

<h2>Make R Beep</h2>

<a href="https://www.geoffchappell.com/studies/windows/win32/kernel32/api/index.htm">KERNEL32 Functions</a>
<a href="https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script">Make R Beep</a>

rundll32.exe Kernel32.dll,Beep 550,1000
rundll32.exe cmdext.dll,MessageBeepStub
rundll32 user32.dll,MessageBeep
BOOL Beep(
  DWORD dwFreq,
  DWORD dwDuration
);

C:\Windows\Media\Delta

install.packages("audio") 
library(audio)

# play(x, rate, ...)
# x = audioSample(sin(1:8000/10), 8000)
# play(x)
# 10000 is the set of numbers, 10 is the freq code

play(sin(c(2000:1000,1500:2000) / 3))

play(sin(1:10000/3))
Sys.sleep(1)
play(sin(1:10000/4))
Sys.sleep(1)
play(sin(1:10000/5))
Sys.sleep(1)
play(sin(1:10000/6))
Sys.sleep(1)
play(sin(1:10000/7))
Sys.sleep(1)
play(sin(1:10000/8))
Sys.sleep(1)
play(sin(1:10000/9))
Sys.sleep(1)
play(sin(1:10000/10))
Sys.sleep(1)
play(sin(1:10000/20))
Sys.sleep(1)
play(sin(1:10000/30))
Sys.sleep(1)

<h2>Play a random sound</h2>
# Update all packages and "ping" when it's ready
# danger! will take a long time and may get wrong result

library(beepr)
update.packages(ask=FALSE); beep()

#Play a fanfare instead of a "ping".
beep("fanfare")
#or
beep(3)

# Play a random sound
beep(0)

beep(sound = 1, expr = NULL)
Arguments
sound character string or number specifying what sound to be played by either specifying one of the built in sounds or specifying the path to a wav file. The default is 1.
Possible sounds are:
"ping" "coin" "fanfare" "complete" "treasure" "ready" "shotgun" "mario" "wilhelm" "facebook" "sword"
beep("shotgun")

<h2>read clipboard</h2>

simply use: readClipboard()

this gives too many columns:
read.table(file = "clipboard", sep = ",")

<h2>dplyr Data Manipulation</h2>
<a href="dplyr Data Manipulation.html">dplyr Data Manipulation</a>

<h2>Language Server Protocol</h2>
<a href="D:\R-3.4.3\bin\x64\R.exe">install.packages("languageserver") Language Server Protocol:</a>
<br>
Adding features like auto complete, go to definition, or documentation on hover for a programming language takes significant effort.

<h2>to run R by batch script</h2>
Rscript.exe  alert.r
Rscript.exe  something.r

Note: Rscript.exe cannot run with Chinese

calling chrome by batch script in sequence
can also call by R

<a href="https://kknews.cc/tech/92k2828.html">R与中文那些事 R script with Chinese</a>

<a href="https://stackoverflow.com/questions/31190468/integrating-r-and-its-graphics-with-existing-javascript-html-application"><strong>R and Javascript : Execution, Libraries, Integration</strong></a>
<br>
In today’s date, R is the megastar language for <a href="https://www.cuelogic.com/big-data-solution" data-href="https://www.cuelogic.com/big-data-solution" target="_blank">big data analytics</a>. 
In this article, I will talk about on coordination, visualization and execution of R and JavaScript. 
However, you may ask the question for what reason somebody might want to incorporate R into web applications?
There are quite a few reasons for this. 
When you add R to your solution, a vast opportunity of analytics opens up like statistics, predictive data modelling, forecasting, machine learning, visualization and much more.
R is developed by statisticians, scientists or professional analysts using the script but the reports and the results generated by them on the desktop can be easily emailed or presented in the form of presentation, but that is limiting the business use and other potential uses.
If R is incorporated with JavaScript, then web delivery can happen smoothly, and it can help in making efficient business decision making. 
Integrating R into web application naturally becomes quintessential.

<h2>Create Apps with Rt</h2>
<a href="https://www.r-bloggers.com/deploying-desktop-apps-with-r/">Create Apps with R</a>


<h2>Integrate R into JavaScript</h2>
There can be various ways through which you can integrate R with JavaScript. 
Here I am discussing the following methods that I prefer for Rand Javascript integration.
<strong>1. Deploy R open</strong>
Through Deploy R opens you can easily embed results of various R functions like- data and charts into any application. 
This specific structure is an open source server-based system planned especially for R, which makes it simple to call the R code at a real time.
The workflow for this is simple: first, the programmer develops R script which is then published on the Deploy R server. 
The published R script that can be executed from any standard application using DeployR API. 
Using client libraries JavaScript now can make calls to the server. 
The results returned by the call can be embedded into the displayed or processed according to the application.
<strong>2. Open CPU JavaScript API</strong>
This offers straightforward RPC and information input/Output through Ajax strategies that can be fused in JavaScript of your HTML page.

<h2>Visualization with R and JavaScript</h2>
You can make use of numerous JavaScript libraries that help in creating web functionality for dynamic data visualizations for R.
Here I will be elaborating some of those tools like D3, Highchart, and leaflet. 
You can quickly implement these tools in your R and program knowledge of JavaScript is not mandatory for this.
As I have already mentioned that R is an open source analytical software, it can create high dimensional data visualizations. 
Ggplot2 is a standout among the most downloaded bundle that has helped R to accomplish best quality level as a data visualization tool.
Javascript then again is a scripting dialect in which R can be consolidated to make data visualisation. 
Numerous javascript libraries can help in creating great intuitive plots, some of them are d3.Js, c3.js, vis.js, plotly.js, sigma.js, dygraphs.js.
HTM widgets act as a bridge between R and JavaScript. 
It is the principal support for building connectors between two languages. 
The flow of a program for HTM widgets r can be visualized as under:
• Information is perused into R
• Data is handled (and conceivably controlled) by R
• Data is changed over to JavaScript Object Notation (JSON) arrange
• Information is bound to JavaScript
• Information is prepared (and conceivably controlled) by JavaScript
• Information is mapped to plotting highlights and rendered
Now let us discuss some of the data visualization packages:
<strong>• r d3 package</strong>
Data-driven documents or d3 is one of the popular JavaScript visualization libraries. 
D3 can produce visualization for almost everything including choropleths, scatter plots, graphs, network visualizations and many more. 
Multiple R packages are using only D3 plotting methods. 
You can refer r d3 package tutorials to learn about this.
• <strong>ggplot2</strong> <br> <br> It is really very easy to create plots in R, but you may ask me whether it is same for creating custom plots, the answer is “yes”, and that is the primary motivation behind why ggplot came into existence. 
With ggplot, you can make complex multi-layered designs effectively.
Here you can start plotting with axes then add points and lines. 
But the only drawback that it has it is relatively slower than base R, and new developers might find it difficult to learn.
• <strong>Leaflet</strong>
The leaflet has found its profound use in GIS (mapping), this is an open source library. 
The R packages that backings this is composed and kept up by RStudio and ports. 
Using this developer can create pop up text, custom zoom levels, tiles, polygon, planning and many more.
The ggmap bundle of javaScript can be utilised for the estimation of the latitude and longitude.
• <strong>Lattice</strong>
Lattice helps in plotting visualized multivariate data. 
Here you can have tilled plots that help in comparing values or subgroups of a given variable. 
Here you will discover numerous lattice highlights has been acquired as utilizes grid package for its usage. 
The underlying logic used by lattice is very much similar to base R.
<strong>• visNetwork</strong>
For the graphical representation of nodes and edges, the visual network is referred. 
Vis.js is a standout amongst the most famous library among numerous that can do this sort of plotting. 
visNetwork is the related with R package for this.
Network plots ought to be finished remembering nodes and edges. 
For visNetwork, these two should be separated into two different data frames one for the nodes and the other
<strong>• Highcarter</strong>
This is another visualization tool which is very similar to D3. 
You can use this tool for a variety of plots like line, spline, arealinerange, column range, polar chart and many more. 
For the commercial use of Highcarter, you need to get a license while for the non-commercial you don’t need one.
Highcarter library can be accessed very easily using various chart () functions. 
Using this function, you can create a plot in a single task. 
This function is very much similar to qplot() of ggplot2 of D3. 
chart () can produce different types of scenarios depending on the data inputs and specifications.
<strong>• RColor Brewer</strong>
With this package, you can use color for your plots, graphs, and maps. 
This package works nicely with schemes.
<strong>• Plotly</strong>
It is a well distinguish podium for data visualization that works inordinately with R and Python notebook. 
It has similarity with the high career as both are known for interactive plotting. 
But here you get some extra as it offers something that most of the package don’t like contour plots, candlestick chart, and 3d charts.
• <strong>SunTrust</strong>
It is the way for representing data visualization as it nicely describes the sequence of events. 
The diagram that it produces speaks about itself. 
You don’t need an explanation for the chart as it is self-explanatory.
• <strong>RGL</strong>
For creating three-dimensional plots in R you should check out RGL. 
It has comparability with lattice, and on the off chance that you are an accomplished R developer you will think that its simple.
<strong>• Threejs</strong>
This is an R package and an HTML widget that helps in incorporating several data visualization from the JavaScript library.
Some of the visualization function three are as follows:
• Graphjs: this is used for implementing 3D interactive data visualization. 
This function accepts igraph as the first argument. 
This manages definition for nodes and edges.
• Scatterplot3js: this function is used for creating three dimensional scatter plot.
• Globejs: this function of JavaScript is used for plotting surface maps and data points on earth.
• <strong>Shiny</strong>
The most significant benefit of JavaScript visualization is it can be implanted voluntarily into the web application. 
They can be injected into several frameworks, one of such context of R development is shiny.
Shiny is created and maintained by R Studio. 
It is a <a href="https://www.cuelogic.com/custom-software-development" data-href="https://www.cuelogic.com/custom-software-development" target="_blank">software application development</a> instrument, to a great extent employed for making wise interfaces with R. 
R shiny tutorial will take in more about shiny.
Shiny is a podium for facilitating R web development.
Connecting R with javascript using libraries
Web scuffling has formed into an original piece of examination as through this movement you can pucker your required information. 
But the data should be extracted before any web developer start to insert javascript render content into the web page. 
To help in such situation R has an excellent package called V8 which acts as an interface to JavaScript. 
R v8 is the most generally utilized capacity utilized for interfacing r in javascript. 
You can undoubtedly implement JS code in R without parting the current session. 
The library function used for this is rvest().
To run the JavaScript in R, we need a context handler, within that context handler you can start programming. 
Then you can export the R data into JavaScript.
Some other JavaScript libraries that help in analytical programming such as Linear Regression, SVMs etc. 
are as follows:
• Brain.js()
• Mljs
• Webdnn
• Convnetjs

<h2>Conclusion:</h2>
R and Javascript can practically unlock innumerable possibility in Data Science and Analytics. 
Both technologies are working towards developing better integrations, knowledge repositories, libraries and use cases. 
It is a good time to use both of this together. 
The future looks bright.


<a href="https://hackernoon.com/r-and-javascript-execution-libraries-integration-40a30726f295">Integrating R and Javascript/HTML Application</a>
<br>
<h2>Rserve package</h2>
There is javascript implementation of Rserve client available rserve-js.
You can call R from javascript efficiently using Rserve package. 

<h2>FastRWeb</h2>
FastRWeb is an infrastructure that allows any webserver to use R scripts for generating content on the fly, such as web pages or graphics. 
URLs are mapped to scripts and can have optional arguments that are passed to the R function run from the script. 
For example http://my.server/cgi-bin/R/foo.png?n=100 would cause FastRWeb to look up a script foo.png.R, source it and call run(n="100"). 
So for example the script could be as simple as

run &lt;- function(n=10, ...) {
   p &lt;- WebPlot(800, 600)
   n &lt;- as.integer(n)
   plot(rnorm(n), rnorm(n), col=2, pch=19)
   p
}
This can potentially then be called using JavaScript to dynamically load images and display them.

<a href="https://stackoverflow.com/questions/22179512/suggestions-needed-for-building-r-server-rest-apis-that-i-can-call-from-externa/29537593#29537593">building R server REST API's that I can call from external app</a>
<br>
<h2>httpuv</h2>
You can use httpuv to fire up a basic server then handle the GET/POST requests. The following isn't "REST" per se, but it should provide the basic framework:

library(httpuv)
library(RCurl)
library(httr)

app &lt;- list(call=function(req) {

  query &lt;- req$QUERY_STRING
  qs &lt;- httr:::parse_query(gsub("^\\?", "", query))

  status &lt;- 200L
  headers &lt;- list('Content-Type' = 'text/html')

  if (!is.character(query) || identical(query, "")) {
    body &lt;- "\r\n<html><body></body></html>"
  } else {
    body &lt;- sprintf("\r\n<html><body>a=%s</body></html>", qs$a)
  }

  ret &lt;- list(status=status,
              headers=headers,
              body=body)

  return(ret)

})

message("Starting server...")

server &lt;- startServer("127.0.0.1", 8000, app=app)
on.exit(stopServer(server))

while(TRUE) {
  service()
  Sys.sleep(0.001)
}

stopServer(server)

<br>
<h2>Cucumber Selenium</h2>
<a href="https://www.guru99.com/using-cucumber-selenium.html">Cucumber Selenium</a>
<a href="https://www.youtube.com/watch?v=ZSfOEBh9BRM">Cucumber Selenium Tutorial</a>
<br>
<a href="https://blog.gtwang.org/r/rselenium-r-selenium-browser-web-scraping-tutorial/">RSelenium：R 使用 Selenium 操控瀏覽器下載網頁資料</a>
<br>

<h2>SQL databases and R</h2>
<a href="https://datacarpentry.org/R-ecology-lesson/05-r-and-databases.html">SQL databases and R</a>

<h2>SQLite</h2>
<a href="https://db.rstudio.com/databases/sqlite/">R SQLite</a>
<br>

install.packages("RSQLite")

Or install the latest development version from GitHub with:
# install.packages("devtools")
devtools::install_github("rstats-db/RSQLite")

To install from GitHub, you’ll need a development environment.

Basic usage
library(DBI)
# Create an ephemeral in-memory RSQLite database
con &lt;- dbConnect(RSQLite::SQLite(), ":memory:")

dbListTables(con)
## character(0)
dbWriteTable(con, "mtcars", mtcars)
dbListTables(con)
## [1] "mtcars"
dbListFields(con, "mtcars")
##  [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"
## [11] "carb"
dbReadTable(con, "mtcars")
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4
...

# You can fetch all results:
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
dbFetch(res)
##     mpg cyl  disp  hp drat    wt  qsec vs am gear carb
## 1  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1
...

dbClearResult(res)

# Or a chunk at a time
res &lt;- dbSendQuery(con, "SELECT * FROM mtcars WHERE cyl = 4")
while(!dbHasCompleted(res)){
  chunk &lt;- dbFetch(res, n = 5)
  print(nrow(chunk))
}
## [1] 5
## [1] 5
## [1] 1
# Clear the result
dbClearResult(res)

# Disconnect from the database
dbDisconnect(con)
Acknowledgements
Many thanks to Doug Bates, Seth Falcon, Detlef Groth, Ronggui Huang, Kurt Hornik, Uwe Ligges, Charles Loboz, Duncan Murdoch, and Brian D. 
Ripley for comments, suggestions, bug reports, and/or patches.


<br>

<h2>Invoking the Rstudio Viewer</h2>
viewer &lt;- getOption("viewer")
viewer("<a href="https://www.rt.com/")">viewer("C:/Users/User/Desktop/Debugging with RStudio.html")</a>

<h2>to sum only elements greater than 5</h2>
a&lt;-sample.int(10,20,replace=TRUE)
sum(a[a>5])

<h2>Customizing RStudio themes</h2>
<a href="https://www.r-bloggers.com/make-rstudio-look-the-way-you-want-because-beauty-matters/">Make RStudio Beauty</a>

D:\RStudio\www\rstudio\806BBC582D6B8DF91384AD7E3EFC9A52.cache.css

<a href="https://support.rstudio.com/hc/en-us/articles/200532307-Customizing-Fonts-and-Appearance">Customizing Fonts and Appearance</a>
<br>

<h2>table()</h2>
table()的输出可以看成是一个带名字的数字向量。
可以用names()和as.numeric()分别得到名称和频数：> 

x &lt;- sample(c("a", "b", "c"), 100, replace=TRUE)
tablex = table(x)

names(tablex)
[1] "a" "b" "c"

> as.numeric(tablex)
[1] 42 25 33

可以直接把输出结果转化为数据框，as.data.frame()：> 
as.data.frame(tablex)
  x Freq
1 a   42
2 b   25
3 c   33

<h2>with(data, expr, …)</h2>
applys an expression to a dataset.
eg
with(BOD,{BOD$demand &lt;- BOD$demand + 1; print(BOD$demand)})

<h2>R regular expression</h2>
<a href="https://blog.yjtseng.info/post/regexpr/">R regex</a>

<h2>R Operator Syntax</h2>
<a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/Syntax.html">R Operator Syntax and Precedence</a>

:: :::	access variables in a namespace
$ @	component / slot extraction
[ [[	indexing
^	exponentiation (right to left)
- +	unary minus and plus
:	sequence operator
%any%	special operators (including %% and %/%)
* /	multiply, divide
+ -	(binary) add, subtract
< > <= >= == !=	ordering and comparison
!	negation
& &&	and
| ||	or
~	as in formulae
-> ->>	rightwards assignment
&lt;- <&lt;-	assignment (right to left)
=	assignment (right to left)
?	help (unary and binary)

exampleRPackage
The exampleRPackage can be installed from github:

# install.packages("devtools")
devtools::install_github("mvuorre/exampleRPackage")

The file you are reading now is the package’s README, which describes how to create R packages with functions, data, and appropriate documentation. 


A Simple Example of Using replyr::gapply
It’s a common situation to have data from multiple processes in a “long” data format. 
It’s also natural to split that data apart to analyze or transform it, per-process — and then to bring the results of that data processing together, for comparison. 
Such a work pattern is called “Split-Apply-Combine”. 
A simple example of one such implementation, replyr::gapply, from package, replyr.

K-means clustering
K-means is a clustering techniques that subdivide the data sets into a set of k groups, where k is the number of groups pre-specified by the analyst.

Determining the optimal number of clusters: use factoextra::fviz_nbclust()


<h2>树状图</h2>
<a href="Dendrograms in R.html" target="_blank">Dendrograms in R</a>
<br>
<h2>shiny and rpanel - a quick comparison</h2>

Shiny is a package from RStudio that lets you produce interactive web pages. 
You build a page with some control widgets and a handler that does something dependent on the value of those widgets. 
You can build your interface programmatically or create a boilerplate html page that gets filled in by control and output widgets.

A conceptually similar pattern is implemented by the rpanel package, but this uses the tcltk toolkit. 
A panel is created, control widgets added, and callbacks on the controls can run R code to, for example, update a plot.

qq plot example
Here's the rpanel version:

require(rpanel)
# box-cox transform
bc.fn &lt;- function(y, lambda) {
    if (abs(lambda) < 0.001) 
        z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
}

# qqplot of transformed data
qq.draw &lt;- function(panel) {
    z &lt;- bc.fn(panel$y, panel$lambda)
    qqnorm(z, main = paste("lambda =", round(panel$lambda, 2)))
    panel
}

# create a new panel with some initial data
panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)

# add a slider that calls qq.draw on change
rp.slider(panel, lambda, -2, 2, qq.draw)

Run these functions and you should see a slider and a graphics window. 
Move the slider to modify the plot.
Note that this might not work too well under RStudio because of the way the embedded RStudio graphics device captures output.


And here is the shiny version, which comes in two files living in their own folder.  ui.R and server.R

First qqplot/ui.R:

library(shiny)
# this defines our page layout
shinyUI(pageWithSidebar(
  headerPanel("qqplot example"),
  sidebarPanel(
  # a slider called 'lambda':
    sliderInput("lambda", "Lambda value", min = -2, max = 2, step=0.01, value = 0)
  ),
  mainPanel(
    # the main panel is the plotted output from qqplot:
    plotOutput("qqPlot")
  )
))

and qqplot/server.R:

library(shiny)
shinyServer(function(input, output) {
    # b-c transform
    bc.fn &lt;- function(y, lambda) {
        if (abs(lambda) < 0.001) 
            z &lt;- log(y) else z &lt;- (y^lambda - 1)/lambda
    }
    # initial data
    y = exp(rnorm(50))
    # here's the qqplot method:
    output$qqPlot &lt;- reactivePlot(function() {
        z &lt;- bc.fn(y, input$lambda)
        qqnorm(z, main = paste("lambda =", round(input$lambda, 2)))
    })
})

With that done, launch the app with:

runApp("qqplot")
that should open up the page in your web browser. 
Hit break, stop, or control-C to quit.


Notes
The rpanel plot updates as you drag the slider, whereas shiny updates only when you let go of the slider.

I find that when I hit Control-C and break a running shiny app, then my tcltk windows go all unresponsive until I quit R and start again. 
Threading issues? This is on Linux. 
I've always had problems with tcltk widgets going unresponsive on me, or ending up unkillable.

The shiny UI looks, well, “shiny”, but the rpanel interface looks a bit old and not very exciting (if you can get excited by user interfaces).

Using the tkrplot package, you can build integrated rpanel packages with controls and plots in the same window. 
Without it, you are stuck with separate graphics and control windows.

Which should I use?
How do I know?! Shiny looks better, but I do like the update on drag of rpanel - it gives you much better feedback as you control the plot. 
Maybe this can be done in shiny with some additional work.

I don't really like the two-file method of shiny. 
Looking at the code I see the files just get sourced in, so conceivably it could be possible to run shiny apps just by specifying the shinyServer and shinyUI functions - but shiny monitors the server.R and ui.R file for changes and updates the application, which is quite nice.

So there's the basic existential dilemma. 
Choice. 
I can even throw some more things into the mix if you want - there' RServe, or RApache with gWidgetsWWW and probably many many more. 
I'm sure we can all agree that the days of needing Java and Apache Tomcat to deploy R applications to the web are now over (http://sysbio.mrc-bsu.cam.ac.uk/Rwui/tutorial/quick_tour.html).

I might try and implement some more of the rpanel examples in shiny shortly. 
Or why don't you have a go, and publish your works here?

<h2>R GUI 視窗程式設計</h2>
<a href="http://www.hmwu.idv.tw/web/R/F01-hmwu_R-GUI-Design.pdf">R GUI 視窗程式設計 tcltk/tcltk2, rpanel</a>
<a href="http://adrian.waddell.ch/EssentialSoftware/Rtcltk_geometry.pdf">Rtcltk_geometry</a>

<a href="http://rstudio-pubs-static.s3.amazonaws.com/2666_f0de0980ac9048d0a71d0f507cd83c3f.html">shiny and rpanel - a quick comparison</a>

<h3>rpanel sample</h3>
<a href="https://www.academia.edu/370260/rpanel_Simple_Interactive_Controls_for_R_Functions_Using_the_tcltk_Package_9999_">rpanel: Simple Interactive Controls for R Functions Using the tcltk Package </a>
<a href="https://www.academia.edu/attachments/1880059/download_file?st=MTU2NDAxOTg4MywxODIuMjM5LjExNS4zNg%3D%3D&s=swp-splash-header">download rpanel sample</a>
<a href="www.stats.gla.ac.uk/~adrian/rpanel">The `rpanel' package</a>
<a href="http://www.stats.gla.ac.uk/~adrian/rpanel/scripts/rpanel-paper-scripts.r">Simple Interactive Controls for R Functions scripts</a>

library(rpanel)
x11(width=4,height=4)
qq.draw &lt;- function(panel)
 { z &lt;- bc.fn(panel$y, panel$lambda)
   qqnorm(z, main = paste("lambda =",round(panel$lambda, 2)))
   panel
 }
 panel &lt;- rp.control(y = exp(rnorm(50)), lambda = 1)
 rp.slider(panel, lambda, -2, 2, qq.draw,showvalue = TRUE)


<h3>create a  matrix</h3>
A = matrix( 
c(2, 4, 3, 1, 5, 7), # the data elements 
nrow=2,              # number of rows 
ncol=3,              # number of columns 
byrow = TRUE)        # fill matrix by rows 


<h2>cross tabulations</h2>
<a href="Contingency Table.html" class="bordred2 borRad10 green whitebs">Contingency Table</a>  <a href="Xtabs exercises.html" class="bordred2 borRad10 white whitebs">Xtabs exercises</a> 

a chart is different from a table
a chart is a graphic representation
a table is a numeric representation

frequaency table is a single row table

cross tabulations, 列联表, contingency tables, 又称交互分类表 按两个或更多变量分类时所列出的频数表。

R provides many methods for creating frequency and contingency tables. 

generate frequency tables using the table( ) function, table( ) function can also create cross tab, table( ) can also generate multidimensional tables based on 3 or more categorical variables.

generate tables of proportions using the prop.table( ) function
generate marginal frequencies using margin.table( )

# 2-Way Frequency Table using table() function
attach(mtcars)
mytable &lt;- table(mtcars$gear,mtcars$cyl) # A will be rows, B will be columns 
mytable # print table 

# 2-Way Frequency Table using xtabs()
y = xtabs(~ cyl + gear, mtcars)	# xtabs gives row and col labels

margin.table(mytable, 1) # A frequencies (summed over B) 
margin.table(mytable, 2) # B frequencies (summed over A)

prop.table(mytable) # cell percentages
prop.table(mytable, 1) # row percentages 
prop.table(mytable, 2) # column percentages

# 3-Way Frequency Table 
mytable &lt;- table(A, B, C) 
mytable &lt;- table(mtcars$gear,mtcars$cyl,mtcars$mpg)
mytable

# 3-Way Frequency Table
mytable &lt;- xtabs(~A+B+c, data=mydata)
mytable &lt;- xtabs(~gear+cyl+mpg, mtcars)
summary(mytable) # chi-square test of indepedence

<a href="https://www.statmethods.net/stats/frequencies.html">Frequencies and Crosstabs</a>

<h2>parallel 平行計算</h2>
<a href="https://blog.gtwang.org/r/r-parallel-computing-module-tutorial/">R 的 parallel 平行計算套件使用教學與範例</a>
<br>
<a href="How-to go parallel in R.html">How-to go parallel in R</a>
<br>
<h2>edply</h2>
<a href="https://www.r-bloggers.com/edply-combining-plyr-and-expand-grid/">edply: combining plyr and expand.grid</a>

<h2>column merge two tables</h2>
lista = c(1:5)
listb = c(6:10)
listc = paste0(lista, "  ",listb)
lista
listb
listc

data from files:
lista = readLines("list1.txt")
listb = readLines("list2.txt")
listc = paste0(lista, "  ",listb)
sink("list3.txt")
cat(listc, sep="\n")
sink()

note: may use cbind in dataframe
lista = c(1:5)
listb = c(6:10)
listc = c(11:15)
MC = matrix()  # this is an empty matrix

MB = matrix( c(lista,listb,listc), nrow=5, ncol=3)  # a 3 column matrix
MC = cbind(MB[,1],MB[,3])   # now MC is a two column matrix

<h2>chop in blocks</h2>
groupPageNum = 7
theList = 1:78
if(length(theList)%%groupPageNum==0){
  pageNo = length(theList)%/%groupPageNum
}else{
  pageNo = length(theList)%/%groupPageNum +1
}
pageNo


for(page in 1:pageNo){
  if(length(theList) > groupPageNum){
	thepage= theList[1:groupPageNum]
	theList= theList[-(1:groupPageNum)]
     arrangePages(thepage)
	page = page + 1
  }else{
     arrangePages(theList)
  }
}


<h2>remove items</h2>

fmList=c('02917','01876','01960','03938','02951','02952','06820','06110','03601','01895')
itemList=c('02718','02696')

commons = fmList[fmList %in% itemList]
cat("\n\nnumber of Items to remove: ", length(commons), "\n")
for(item in commons){fmList = fmList[-(which(fmList == item))]}
fmList

<h2>extract chinanews images</h2>
http://www.chinanews.com/tp/hd2011/2019/10-20/909276.shtml
copy the thumb address and replace ending 320x300.jpg with 1000x2000

<h2>cut(x,breaks)</h2>
x = sort(rnorm(13,5,12))
x
-15.0 -11.3  -3.2   1.0   3.8   6.1   7.6   7.8  10.7 13.7  15.4  15.9  23.4

cut(x,5)
(-15.1,-7.36] (-15.1,-7.36] (-7.36,0.339] (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (0.339,8.03]  (8.03,15.7]   (8.03,15.7]   (8.03,15.7]   (15.7,23.5]   (15.7,23.5]  
Levels: (-15.1,-7.36] (-7.36,0.339] (0.339,8.03] (8.03,15.7] (15.7,23.5]

<h2>R GUI: RGtk or Tcl/Tk, gWidgets</h2>
<a href="http://www.ggobi.org/rgtk2/">RGtk2</a>

<a href="https://www.r-bloggers.com/playing-with-guis-in-r-with-rgtk2/">Playing with GUIs in R with RGtk2</a>

<a href="https://www.r-bloggers.com/gui-building-in-r-gwidgets-vs-deducer/">GUI building in R: gWidgets vs Deducer</a>

require("RGtk2")

window &lt;- gtkWindow()
window["title"] &lt;- "Calculator"

frame &lt;- gtkFrameNew("Calculate")
window$add(frame)

box1 &lt;- gtkVBoxNew()
box1$setBorderWidth(30)
frame$add(box1)   #add box1 to the frame

box2 &lt;- gtkHBoxNew(spacing= 10) #distance between elements
box2$setBorderWidth(24)

TextToCalculate&lt;- gtkEntryNew() #text field with expresion to calculate
TextToCalculate$setWidthChars(25)
box1$packStart(TextToCalculate)

label = gtkLabelNewWithMnemonic("Result") #text label
box1$packStart(label)

result&lt;- gtkEntryNew() #text field with result of our calculation
result$setWidthChars(25)
box1$packStart(result)

box2 &lt;- gtkHBoxNew(spacing= 10) # distance between elements
box2$setBorderWidth(24)
box1$packStart(box2)

Calculate &lt;- gtkButton("Calculate")
box2$packStart(Calculate,fill=F) #button which will start calculating

Sin &lt;- gtkButton("Sin") #button to paste sin() to TextToCalculate
box2$packStart(Sin,fill=F)

Cos &lt;- gtkButton("Cos") #button to paste cos() to TextToCalculate
box2$packStart(Cos,fill=F)

model&lt;-rGtkDataFrame(c("double","integer"))
combobox &lt;- gtkComboBox(model)
#combobox allowing to decide whether we want result as integer or double

crt &lt;- gtkCellRendererText()
combobox$packStart(crt)
combobox$addAttribute(crt, "text", 0)

gtkComboBoxSetActive(combobox,0)
box2$packStart(combobox)

DoCalculation&lt;-function(button)
{

  if ((TextToCalculate$getText())=="") return(invisible(NULL)) #if no text do nothing

   #display error if R fails at calculating
   tryCatch(
      if (gtkComboBoxGetActive(combobox)==0)
   result$setText((eval(parse(text=TextToCalculate$getText()))))
   else (result$setText(as.integer(eval(parse(text=TextToCalculate$getText()))))),
   error=function(e)
      {
      ErrorBox &lt;- gtkDialogNewWithButtons("Error",window, "modal","gtk-ok", GtkResponseType["ok"])
      box1 &lt;- gtkVBoxNew()
      box1$setBorderWidth(24)
      ErrorBox$getContentArea()$packStart(box1)

      box2 &lt;- gtkHBoxNew()
      box1$packStart(box2)

      ErrorLabel &lt;- gtkLabelNewWithMnemonic("There is something wrong with your text!")
      box2$packStart(ErrorLabel)
      response &lt;- ErrorBox$run()


      if (response == GtkResponseType["ok"])
         ErrorBox$destroy()

      }
   )

}


  PasteSin&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"sin()",sep=""))

}

PasteCos&lt;-function(button)
{
   TextToCalculate$setText(paste(TextToCalculate$getText(),"cos()",sep=""))

}

#however button variable was never used inside 
#functions, without it gSignalConnect would not work
gSignalConnect(Calculate, "clicked", DoCalculation)
gSignalConnect(Sin, "clicked", PasteSin)
gSignalConnect(Cos, "clicked", PasteCos)
Now it works like planned.



library(RGtk2)
createWindow &lt;- function()
{
    window &lt;- gtkWindow()
    label &lt;- gtkLabel("Hello World")
    window$add(label)
}
createWindow()
gtk.main() # this will create error

# using this will loop dead
gtkMain()


<h2>To keep the scripts and algorithm secret</h2>
by saving functions using save(). 
For example, here's a function f() you want to keep secret:

f &lt;- function(x, y) { return(x + y)}

Save it :
save(f, file = 'C:\\Users\\Joyce\\Documents\\R\\Secret.rda')

And when you want to use the function:
load("C:\\Users\\Joyce\\Documents\\R\\Secret.rda")

Save all functions in separate files, 
put them in a folder and have one plain old .R script
loading them all in and executing whatever.
Zip the whole thing up and distribute it to whoever.
Maybe even compile it into a package.
Effectively the whole thing would be read-only then.

This solution isn't that great though.
You can still see the function in R by typing the name of the function
so it's not hidden in that sense.
But if you open the .rda files their contents are all garbled.
It all depends really on how experienced the recipients of your code are with R.

One form of having encrypted code is implemented in the petals function in the TeachingDemos package.

it would only take intermediate level programing skills to find the hidden code,
however it does take deliberate effort and the user would not be able to claim having seen the code by accident.
You would then need some type of license agreement in place to enforce any no peeking agreements.

Well you are going to need R installed on the deployment machine.

<h2>Test if characters are in a string</h2>
grepl("abc", "abcde")
note: RE will be applied, take care of the expression

<h2>get password</h2>
install.packages("getPass")
pass = getPass::getPass(msg = "PASSWORD: ", noblank = FALSE, forcemask = FALSE)

<h2>tryCatch loop</h2>
  retrieveData &lt;- function(urlAddr){      
    retryCounter = 0
    while(retryCounter < 20) {
      cat("..",retryCounter," ") 
      retriveFile &lt;- tryCatch(readLines(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

  retrieveData &lt;- function(urlAddr){      
    retryCounter = 1
    while(retryCounter < 20) {
      cat("..try ",retryCounter," ") 
      retriveFile &lt;- tryCatch(read_html(urlAddr, warn=F), 
          warning = function(w){return("code param error")}, 
          error = function(e) {return("code param error")}
          )
      if (grepl("code param error", retriveFile)) {
        cat("Error in connection, try 5 secs later!\n")
        retryCounter &lt;- retryCounter + 1
        retriveFile = ""  # if end of loop this will be returned
      }else{
        retryCounter = 200  # to jump out of loop
      }
    }
    return(retriveFile)
  }

<h2>變異數分析, 方差分析 ANOVA </h2>
<a href="http://personality-project.org/r/r.guide/r.anova.html">r.anova</a>
<br>
<a href="https://alex59638.pixnet.net/blog/post/403137005-用r進行anova%28變方分析%29">ANOVA可分析多組間的差異 變異數分析 (ANOVA)</a>
<a href="http://programmermagazine.github.io/201310/htm/article3.html">主成分分析 Principle Component Analysis</a>
<br>
<h2>常用統計檢驗法簡介</h2>

T.test(又稱 T 檢定、T檢驗、t.test，以下簡稱T檢驗)
T檢驗主要用於檢定樣本的平均值，這是一項重點。

如果要看一個樣本的平均是否等於某值，要用 T 檢驗。

如果要看兩個樣本的平均是否相等，要用 T 檢驗。

T 檢驗分成三種類別
1.單樣本T檢驗(One smaple T test)
2.獨立雙樣本T檢驗
3.配對雙樣本T檢驗

要看 30 個男生的身高是否等於 180，用單樣本T檢驗。
[R語法:t.test(樣本,mu=平均)]

要看 A 班與 B 班男生身高是否相等，用獨立雙樣本T檢驗。
[R語法:t.test(A樣本,B樣本)]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A樣本,B樣本,paired=TURE)]

每一種 T 檢驗都還可以再分成雙尾與單尾檢驗。
[R語法:t.test(樣本,mu=平均,alternative= "two.sided")]

two.sided代表等於，就是雙尾的意思，也可以改成單尾的大於"greater"或是單尾的小於"less"。

重點只有"檢驗平均等於某值時"是雙尾，"檢驗平均小於某值時"是單尾，"檢驗平均大於某值時"是單尾。
請看到這裡後不要再講單尾或是雙尾了，一點意義也沒有，講大於等於小於就好了。
但Eecel沒有大於小於的選項，只有單尾雙尾，因此要自己判斷是大於還是小於(從樣本平均看即可)。
[Eecel語法:TTEST(A樣本,B樣本,2,2)]，2代表雙尾，改成1就變成單尾。

要看 30 個男生的身高是否大於 180，用單樣本T檢驗
[R語法:t.test(樣本,mu=180),alternative="greater"]

要看 A 班與 B 班男生身高差異是否小於 30，用獨立雙樣本T檢驗
[R語法:t.test(A,B,mu=30,alternative="less")]

要看 30 個男生吃藥前與吃藥後身高是否相等，用配對雙樣本T檢驗
[R語法:t.test(A,B,mu=30,paired=T)]
[Eecel語法:TTEST(A樣本,B樣本,2,1)]

其中獨立雙樣本T檢驗(Welch Two smaple T test)還細分成變異數相等或變異數不相等兩種，這要看你母體與取樣的方法，如果不確定，嚴格一點是認為不相等的。

變異數相等
[R語法:t.test(A,B,mu=0,var.equal=T)]

變異數不相等
[R語法:t.test(A,B,mu=0,var.equal=F)]

<h3>卡方檢定 chi-square test(以下簡稱卡方檢定)</h3>
卡方檢驗用於確認樣本是否符合某種分配
骰子丟一百次，每面的機率是否為1/6</a>)，
或是兩個屬性之間是否有所關聯(男生是否比較容易選擇藍色商品)。

這其實是一樣的概念，假設兩個屬性之間無關，其分佈上應該會呈現隨機;
如果兩個屬性有關，例如男生喜歡藍色商品，在同樣的其況下，男生買藍色商品的次數會比男生買紅色商品的次數多，也就是不符合隨機的分配(理論上無關的話次數會一樣多)。

卡方檢定分成三種
1.適合度檢定（Goodness of fit test）
2.獨立性檢定（Test of independence）
3.同質性檢定 (Test of Homogeneity)

其實獨立性與同質性檢定是同一個東西，只是問法不一樣而已卡方適合度檢定用來檢驗樣本是否服從某種分佈，這種分佈你的心裡要有底，比方隨機(丟骰子各面是1/6)，孟德爾的紅花白花是3:1等等，如果你不知道要選擇哪種分佈，那就不能用卡方適合度檢定。

紅花969株，白花360株，檢驗是否符合孟德爾3:1的分佈，用卡方適合度檢定
chisq.test(c(969,360),p=c(0.75,0.25))
#次數表放第一個變數,p後面接機率，機率合要等於1[R語法:chisq.test(次數表,p=機率)]

骰子1000次，檢驗每面是否為1/6的分佈，用卡方適合度檢定
x=ceiling(runif(1000)*6)#丟1000次骰子, ceiling是無條件進位，讓數值落在1~6的整數
table(x)
#這是卡方檢定的重點，必須輸入統計次數，知道骰出1的有幾次，2的有幾次
chisq.test(table(x),p=c(1/6,1/6,1/6,1/6,1/6,1/6))
#次數表放第一個，p後面接分佈的機率，本次是6個1/6。

[R語法:chisq.test(次數表,p=機率)]

<h3>費雪精確性檢定 Fisher's exact test</h3>
類似卡方檢定的小樣本方式，通常用於樣本小於20的狀況，案例是猜八杯茶是先加奶還是先加茶。
fisher.test(table(real,guess))

<h2>變異數分析 ANOVA</h2>
兩組資料連續看是否有差異，用t.test，兩組以上則用ANOVA，其虛無假說H0:u1=u2=u3=...un。
若p值小於0.05，則認為並非所有的資料來自同一個母體。

若要知道到底是哪組資料不同，可使用 
pairwise.t.test(Y, B, p.adjust.method="none")
其中Y為資料列，B為組別列，並且不調整p值。
雙因子變異數分析
aov(cardspent~factor(region)*factor(gender)
使用*符號而不是+


<h2>Logistic Regression</h2>
Logistic regression, also called a logit model, is used to model dichotomous outcome variables. 

Logistic regression is a method for fitting a regression curve, y = f(x), when y is a categorical variable. 
The typical use of this model is predicting y given a set of predictors x. The predictors can be continuous, categorical or a mix of both.

一般的線性迴歸都是連續數值，例如身高或體重。
但有些情況下的應變數為類別，例如生還與否(1或0)，就可以採用Logistic Regression。

Logistic Regression有幾項要點，
1.他需要應變數為類別變項
2.他會給出一個式子，帶入自變數後(可為連續變項或類別變項)，會得出一個值
3.這個值稱為勝算比


以鐵達尼號乘客名單的資料作為範例分析
model1&lt;-glm data="titanic_passenger," family="binomial(link=" formula="survival~fare," logit="" na.action="na.exclude)&lt;/p"&gt;summary(model1)
其中fare 對 survival 的對數機率為 0.013108
勝算比為exp(0.013108)=1.013085
多一英鎊，多1%生還率。
參考資料
<a href="https://sites.google.com/site/rlearningsite/catagory/logit" target="_blank">Logistic迴歸模型</a>
<a href="http://xn--r-vc8at2mlrkqvkh65cu2ccyjyqb/" target="_blank">R语言逻辑回归分析</a>
<a href="https://www.r-bloggers.com/how-to-perform-a-logistic-regression-in-r/" target="_blank">How to perform a Logistic Regression in R</a><!---glm-->


<a href="http://ccckmit.wikidot.com/r:optimize">一維空間優化方法：optimize()</a>
<br>
<a href="http://ccckmit.wikidot.com/r:main">R 統計軟體 作者：陳鍾誠</a>
<br>
<a href="http://programmermagazine.github.io/201309/htm/article3.html">R 統計軟體(6) – 迴歸分析</a>
<br>

<h2>Advanced Statistics Tree-Based Models</h2>
<a href="https://www.datacamp.com/community/tutorials/decision-trees-R">Decision Trees in R</a>
<br>
<a href="https://www.guru99.com/r-decision-trees.html">Decision Tree in R with Example</a>
<br>
<a href="https://www.statmethods.net/advstats/cart.html">Tree-Based Models</a>
<br>
<a href="http://www.di.fc.ul.pt/~jpn/r/tree/tree.html">Classification & Regression Trees</a>
<br>
<a href="https://blog.revolutionanalytics.com/2013/06/plotting-classification-and-regression-trees-with-plotrpart.html">Classification and Regression Trees with the rpart.plot package</a>
<br>

<h2>Grouping functions (tapply, by, aggregate) and the *apply family</h2>
R has many *apply functions.
Much of the functionality of the *apply family is covered by the extremely popular <code>plyr</code> package, the base functions remain useful and worth knowing.

<li><strong>apply</strong> - <em>When you want to apply a function to the rows or columns of a matrix (and higher-dimensional analogues); not generally advisable for data frames as it will coerce to a matrix first.</em>

<code># Two dimensional matrix
M &lt;- matrix(seq(1,16), 4, 4)

# apply min to rows
apply(M, 1, min)
[1] 1 2 3 4

# apply max to columns
apply(M, 2, max)
[1]  4  8 12 16

# 3 dimensional array
M &lt;- array( seq(32), dim = c(4,4,2))

# Apply sum across each M[*, , ] - i.e Sum across 2nd and 3rd dimension, look from top is an area
apply(M, 1, sum)
# Result is one-dimensional
[1] 120 128 136 144

# Apply sum across each M[*, *, ] - i.e Sum across 3rd dimension
apply(M, c(1,2), sum)
# Result is two-dimensional
     [,1] [,2] [,3] [,4]
[1,]   18   26   34   42
[2,]   20   28   36   44
[3,]   22   30   38   46
[4,]   24   32   40   48
</code>

If you want row/column means or sums for a 2D matrix, be sure to investigate the highly optimized, lightning-quick <code>colMeans</code>, <code>rowMeans</code>, <code>colSums</code>, <code>rowSums</code>.</li>
<li><strong>lapply</strong> - <em>When you want to apply a function to each element of a list in turn and get a list back.</em>

This is the workhorse of many of the other *apply functions. 
Peel back their code and you will often find <code>lapply</code> underneath.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100) 
lapply(x, FUN = length) 
$a 
[1] 1
$b 
[1] 3
$c 
[1] 91
lapply(x, FUN = sum) 
$a 
[1] 1
$b 
[1] 6
$c 
[1] 5005</code></li>
<li><strong>sapply</strong> - <em>When you want to apply a function to each element of a list in turn, but you want a <strong>vector</strong> back, rather than a list.</em>

If you find yourself typing <code>unlist(lapply(...))</code>, stop and consider <code>sapply</code>.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Compare with above; a named vector, not a list 
sapply(x, FUN = length)  
a  b  c   
1  3 91

sapply(x, FUN = sum)   
a    b    c    
1    6 5005 
</code>

In more advanced uses of <code>sapply</code> it will attempt to coerce the result to a multi-dimensional array, if appropriate. 
For example, if our function returns vectors of the same length, <code>sapply</code> will use them as columns of a matrix:

<code>sapply(1:5,function(x) rnorm(3,x))
</code>

If our function returns a 2 dimensional matrix, <code>sapply</code> will do essentially the same thing, treating each returned matrix as a single long vector:

<code>sapply(1:5,function(x) matrix(x,2,2))</code>

Unless we specify <code>simplify = "array"</code>, in which case it will use the individual matrices to build a multi-dimensional array:

<code>sapply(1:5,function(x) matrix(x,2,2), simplify = "array")</code>

Each of these behaviors is of course contingent on our function returning vectors or matrices of the same length or dimension.</li>
<li><strong>vapply</strong> - <em>When you want to use <code>sapply</code> but perhaps need to squeeze some more speed out of your code.</em>

For <code>vapply</code>, you basically give R an example of what sort of thing your function will return, which can save some time coercing returned values to fit in a single atomic vector.

<code>x &lt;- list(a = 1, b = 1:3, c = 10:100)
# Note that since the advantage here is mainly speed, this
# example is only for illustration. 
We're telling R that
# everything returned by length() should be an integer of length 1. 

vapply(x, FUN = length, FUN.VALUE = 0L) 
a  b  c  
1  3 91
</code></li>
<li><strong>mapply</strong> - <em>For when you have several data structures (e.g. 
vectors, lists) and you want to apply a function to the 1st elements of each, and then the 2nd elements of each, etc., coercing the result to a vector/array as in <code>sapply</code>.</em>

This is multivariate in the sense that your function must accept multiple arguments.

<code>#Sums the 1st elements, the 2nd elements, etc. 

mapply(sum, 1:5, 1:5, 1:5) 
[1]  3  6  9 12 15
#To do rep(1,4), rep(2,3), etc.
mapply(rep, 1:4, 4:1)   
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2

[[3]]
[1] 3 3

[[4]]
[1] 4
</code></li>
<li><strong>Map</strong> - <em>A wrapper to <code>mapply</code> with <code>SIMPLIFY = FALSE</code>, so it is guaranteed to return a list.</em>

<code>Map(sum, 1:5, 1:5, 1:5)
[[1]]
[1] 3

[[2]]
[1] 6

[[3]]
[1] 9

[[4]]
[1] 12

[[5]]
[1] 15
</code></li>
<li><strong>rapply</strong> - <em>For when you want to apply a function to each element of a <strong>nested list</strong> structure, recursively.</em>

To give you some idea of how uncommon <code>rapply</code> is, I forgot about it when first posting this answer! Obviously, I'm sure many people use it, but YMMV. 
<code>rapply</code> is best illustrated with a user-defined function to apply:

<code># Append ! to string, otherwise increment
myFun &lt;- function(x){
    if(is.character(x)){
      return(paste(x,"!",sep=""))
    }
    else{
      return(x + 1)
    }
}

#A nested list structure
l &lt;- list(a = list(a1 = "Boo", b1 = 2, c1 = "Eeek"), 
          b = 3, c = "Yikes", 
          d = list(a2 = 1, b2 = list(a3 = "Hey", b3 = 5)))


# Result is named vector, coerced to character          
rapply(l, myFun)

# Result is a nested list like l, with values altered
rapply(l, myFun, how="replace")
</code></li>
<li><strong>tapply</strong> - <em>For when you want to apply a function to <strong>subsets</strong> of a vector and the subsets are defined by some other vector, usually a factor.</em>

The black sheep of the *apply family, of sorts. 
The help file's use of the phrase "ragged array" can be a bit <a href="https://stackoverflow.com/questions/6297201/explain-r-tapply-description/6297396#6297396">confusing</a>, but it is actually quite simple.

A vector:

<code>x &lt;- 1:20</code>

A factor (of the same length!) defining groups:

<code>y &lt;- factor(rep(letters[1:5], each = 4))</code>

Add up the values in <code>x</code> within each subgroup defined by <code>y</code>:

<code>tapply(x, y, sum)  
 a  b  c  d  e  
10 26 42 58 74 
</code>

More complex examples can be handled where the subgroups are defined by the unique combinations of a list of several factors. 
<code>tapply</code> is similar in spirit to the split-apply-combine functions that are common in R (<code>aggregate</code>, <code>by</code>, <code>ave</code>, <code>ddply</code>, etc.) Hence its black sheep status.</li>

<b>Slice vector</b>
We can use lapply() or sapply() interchangeable to slice a data frame. 
We create a function, below_average(), that takes a vector of numerical values and returns a vector that only contains the values that are strictly above the average. 

below_ave &lt;- function(x) {  
    ave &lt;- mean(x) 
    return(x[x > ave])
}

Compare both results with the identical() function.
dataf_s&lt;- sapply(dataf, below_ave)
dataf_l&lt;- lapply(dataf, below_ave)
identical(dataf_s, dataf_l)


<h2>Principal Component Methods</h2>
<a href="Principal Component Methods.html">Principal Component Methods</a>


<h2>NLP techniques</h2>
<a href="NLP techniques.html">NLP techniques</a>
<br>


<h2>RMySQL R connect to MySQL</h2>
root
asdf1234
SHOW DATABASES

# 1. Library
library(RMySQL)

# 2. Settings
db_user &lt;- 'root'
db_password &lt;- 'asdf1234'
db_name &lt;- 'sampledb'
# db_table &lt;- 'example'
db_table &lt;- 'world'

db_host &lt;- '127.0.0.1' # for local access
db_port &lt;- 3306

# 3. Read data from db
mydb &lt;-  dbConnect(MySQL(), user = db_user, password = db_password,
         dbname = db_name, host = db_host, port = db_port)
s &lt;- paste0("select * from ", db_table)
rs &lt;- dbSendQuery(mydb, s)
df &lt;-  fetch(rs, n = -1)
on.exit(dbDisconnect(mydb))

<h2>convert R {xml_node} to plain text while preserving the tags</h2>
className = "#icnt"
keywordList &lt;- html_nodes(pagesource, className)
as.character(keywordList)

<h2>convert R objects into a binary format</h2>
x &lt;- list(1, 2, 3)
serialize(x, NULL)
The serialize() function is used to convert individual R objects into a binary format that can be communicated across an arbitrary connection. This may get sent to a file, but it could get sent over a network or other connection.

<h2>Convert an R Object to a Character String</h2>
x &lt;- c("a", "b", "aaaaaaaaaaa")
toString(x)
toString(x, width = 8)


<h2>html_node, html_nodes</h2>
html_node retrieves the first element it encounter, 
while html_nodes returns each matching element in the page as a list.

use html_nodes instead of html_node.

The toString() function collapse the list of strings into one.

library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#contentmiddle>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>Excluding Nodes in RVest</h2>
library(rvest)
pagesource &lt;- read_html("url")

testpost &lt;- pagesource %>% 
  html_nodes("#content>:not(#commentblock)") %>% 
  html_text %>%
  as.character %>%
  toString

<h2>xml_remove()</h2>
By using xml_remove(), you can literally remove any nodes

text &lt;- '
<table> <tr class="alt">
     <td>1</td>
     <td>2</td>
     <td class="hidden">3</td>
   </tr>
   <tr class="tr0 close notule">
     <td colspan="9">4</td> </tr>
</table>'

html_tree &lt;- read_html(text)

#select nodes you want to remove
hidden_nodes &lt;- html_tree %>% html_nodes(".hidden")
close_nodes &lt;- html_tree %>% html_nodes(".tr0.close.notule")

#remove those nodes
xml_remove(hidden_nodes)
xml_remove(close_nodes)

html_tree %>% html_table()


<h2>view all xml_nodeset class object (output of rvest::html_nodes)</h2>
print.AsIs(keywordList)

<h2>Install package loaclly</h2>
# 安装export包
if(!require(export)){
install.packages('export')
require(export)
}

下载安装包文件
打开git bash，执行命令：
git clone https://github.com/tomwenseleers/export.git

BUILD 安装包文件
R CMD BUILD export

安装包压缩文件
R CMD INSTALL

测试export包是否可以使用
require(export)

<h2>e1071 package Support vector machine</h2>
<a href="e1071 package SVM.html" class="whitebut ">e1071 package SVM</a>

<h2>substitute()</h2>
a &lt;- 1
b &lt;- 2
substitute(a + b + z) ## a + b + z

<h2>When to use CPUs vs GPUs vs TPUs?</h2>
Behind every machine learning algorithm is hardware crunching away at multiple gigahertz. 
You may have noticed several processor options when setting up Kaggle notebooks, but which one is best for you? In this blog post, we compare the relative advantages and disadvantages of using CPUs (<a href="https://www.intel.com/content/www/us/en/products/processors/xeon.html" target="_blank">Intel Xeon</a>*) vs GPUs (<a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a>) vs TPUs (<a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a>) for training machine learning models that were written using <a href="https://keras.io/" target="_blank">tf.keras</a> (Figure 1**). 
We’re hoping this will help you make sense of the options and select the right choice for your project.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*suXcuHEe29aKLPrQnXGBrg.png">

How we prepared the test
In order to compare the performance of CPUs vs GPUs vs TPUs for accomplishing common data science tasks, we used the <a href="https://www.tensorflow.org/datasets/catalog/tf_flowers" target="_blank">tf_flowers dataset</a> to train a convolutional neural network, and then the exact same code was run three times using the three different backends (CPUs vs GPUs vs TPUs; GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM). 
The accompanying <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">tutorial notebook</a> demonstrates a few best practices for getting the best performance out of your TPU.
For example:

Using a dataset of sharded files (<a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">e.g., .TFRecord</a>)
Using the <a href="https://www.tensorflow.org/guide/data" target="_blank">tf.data</a> API to pass the training data to the TPU
Using large batch sizes (e.g. 
batch_size=128)

By adding these precursory steps to your workflow, it is possible to avoid a common I/O bottleneck that otherwise prevents the TPU from operating at its full potential. 
You can find additional tips for optimizing your code to run on TPUs by visiting the official <a href="https://www.kaggle.com/docs/tpu" target="_blank">Kaggle TPU documentation</a>.
How the hardware performed
The most notable difference between the three hardware types that we tested was the speed that it took to train a model using <a href="https://keras.io/" target="_blank">tf.keras</a>. 
The tf.keras library is one of the most popular machine learning frameworks because tf.keras makes it easy to quickly experiment with new ideas. 
If you spend less time writing code then you have more time to perform your calculations, and if you spend less time waiting for your code to run, then you have more time to evaluate new ideas (Figure 2). 
tf.keras and TPUs are a powerful combination when participating in <a href="https://kaggle.com/c/flower-classification-with-tpus" target="_blank">machine learning competitions</a>!

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*bqmG-YzgJzVeLbQ5Ym1iFg.png">
For our first experiment, we used the same code (a modified version*** of the <a href="https://www.kaggle.com/mgornergoogle/flowers-with-keras-and-xception-fine-tuned-on-gpu" target="_blank">official tutorial notebook</a>) for all three hardware types, which required using a very small batch size of 16 in order to avoid out-of-memory errors from the CPU and GPU. 
Under these conditions, we observed that TPUs were responsible for a ~100x speedup as compared to CPUs and a ~3.5x speedup as compared to GPUs when training an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model (Figure 3). 
Because TPUs operate more efficiently with large batch sizes, we also tried increasing the batch size to 128 and this resulted in an additional ~2x speedup for TPUs and out-of-memory errors for GPUs and CPUs. 
Under these conditions, the TPU was able to train an <a href="https://keras.io/applications/#xception" target="_blank">Xception</a> model more than 7x as fast as the GPU from the previous experiment****.

<img class="lazy" data-src="https://miro.medium.com/max/1438/1*p2X9DQcq9K5Iu76Kk82vrg.png">
The observed speedups for model training varied according to the type of model, with Xception and Vgg16 performing better than ResNet50 (Figure 4). Model training was the only type of task where we observed the TPU to outperform the GPU by such a large margin. 
For example, we observed that in our hands the TPUs were ~3x faster than CPUs and ~3x slower than GPUs for performing a small number of predictions (TPUs perform exceptionally when making predictions in some situations such as when <a href="https://docs.google.com/presentation/d/1O49AkNyYV48n0X4nWr7KE-5aask88pz9gBSQ26ZG-5o/edit#slide=id.g50ce3d3866_0_1590" target="_blank">making predictions</a> on very large batches, which were not present in this experiment).

<img class="lazy" data-src="https://miro.medium.com/max/46/1*p7U2zlYn9O5Yvjluh2P-dg.png">
<img class="lazy" data-src="https://miro.medium.com/max/1466/1*p7U2zlYn9O5Yvjluh2P-dg.png">
To supplement these results, we note that <a href="https://arxiv.org/abs/1907.10701" target="_blank">Wang<em> et. 
al</em></a> have developed a rigorous benchmark called ParaDnn [1] that can be used to compare the performance of different hardware types for training machine learning models. 
By using this method Wang<em> et. 
al</em> were able to conclude that the performance benefit for parameterized models ranged from 1x to 10x, and the performance benefit for real models ranged from 3x to 6.8x when a TPU was used instead of a GPU (Figure 5). 
TPUs perform best when combined with sharded datasets, large batch sizes, and large models.

<img class="lazy" data-src="https://miro.medium.com/max/1466/1*QbP2CPDZH5BQWlnaTtW3oA.png">
Price considerations when training models
While our comparisons treated the hardware equally, there is a sizeable difference in pricing. TPUs are ~5x as expensive as GPUs (<a href="https://cloud.google.com/compute/gpus-pricing" target="_blank">$1.46/hr</a> for a <a href="https://www.nvidia.com/en-us/data-center/tesla-p100/" target="_blank">Nvidia Tesla P100</a> GPU vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$8.00/hr</a> for a <a href="https://cloud.google.com/tpu/" target="_blank">Google TPU v3</a> vs <a href="https://cloud.google.com/tpu/pricing" target="_blank">$4.50/hr</a> for the TPUv2 with “on-demand” access on <a href="https://cloud.google.com/pricing/" target="_blank">GCP</a> ). 
If you are trying to optimize for cost then it makes sense to use a TPU if it will train your model at least 5 times as fast as if you trained the same model using a GPU.
We consistently observed model training speedups on the order of ~5x when the data was stored in <a href="https://www.kaggle.com/paultimothymooney/convert-kaggle-dataset-to-gcs-bucket-of-tfrecords" target="_blank">a sharded format</a> in a <a href="https://www.kaggle.com/paultimothymooney/how-to-move-data-from-kaggle-to-gcs-and-back" target="_blank">GCS bucket</a> then passed to the TPU in large batch sizes, and therefore we recommend TPUs to cost-conscious consumers that are familiar with the <a href="http://tf.data" target="_blank">tf.data</a> API.
Some machine learning practitioners prioritize the reduction of model training time as opposed to prioritizing the reduction of model training costs. 
For someone that just wants to train their model as fast as possible, the TPU is the best choice. 
If you spend less time training your model, then you have more time to iterate upon new ideas. 
But don’t take our word for it — you can evaluate the performance benefits of CPUs, GPUs, and TPUs by running your own code in a <a href="https://www.kaggle.com/docs/kernels#the-kernels-environment" target="_blank">Kaggle Notebook</a>, free-of-charge. 
Kaggle users are already having a lot of fun and success experimenting with TPUs and text data: check out <a href="https://www.kaggle.com/c/tensorflow2-question-answering/discussion/127333" target="_blank">this forum post</a> that describes how TPUs were used to train a BERT transformer model to win $8,000 (2nd prize) in a recent <a href="https://www.kaggle.com/c/tensorflow2-question-answering" target="_blank">Kaggle competition</a>.
Which hardware option should you choose?
In summary, we recommend CPUs for their versatility and for their large memory capacity. 
GPUs are a great alternative to CPUs when you want to speed up a variety of data science workflows, and TPUs are best when you specifically want to train a machine learning model as fast as you possibly can.
You can get better results by optimizing your code for the specific hardware that you are using and we think it would be especially interesting to compare runtimes for code that has been optimized for a GPU to runtimes for code that has been optimized for a TPU. 
For example, it would be interesting to record the time that it takes to train a gradient-boosted model using a GPU-accelerated library such as <a href="https://rapids.ai/" target="_blank">RAPIDS.ai</a> and then to compare that to the time that it takes to train a deep learning model using a TPU-accelerated library such as <a href="https://keras.io/" target="_blank">tf.keras</a>.
What is the least amount of time that one can train an accurate machine learning model? How many different ideas can you evaluate in a single day? When used in combination with tf.keras, TPUs allow machine learning practitioners to spend less time writing code and less time waiting for their code to run — leaving more time to evaluate new ideas and improve one’s performance in <a href="http://kaggle.com/c/flower-classification-with-tpus" target="_blank">Kaggle Competitions</a>.

<h3>Footnotes</h3>* CPU types vary according to variability. 
In addition to the Intel Xeon CPUs, you can also get assigned to either Intel Skylake, Intel Broadwell, or Intel Haswell CPUs. 
GPUs were NVIDIA P100 with Intel Xeon 2GHz (2 core) CPU and 13GB RAM. 
TPUs were TPUv3 (8 core) with Intel Xeon 2GHz (4 core) CPU and 16GB RAM).
** Image for Figure 1 from <a href="https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference" target="_blank">https://cloud.google.com/blog/products/ai-machine-learning/cloud-tpu-breaks-scalability-records-for-ai-inference,</a> with permission.
*** The tutorial notebook was modified to keep the parameters (e.g. 
batch_size, learning_rate, etc) consistent between the three different backends.
**** CPU and GPU experiments used a batch size of 16 because it allowed the Kaggle notebooks to run from top to bottom without memory errors or 9-hr timeout errors. 
Only TPU-enabled notebooks were able to run successfully when the batch size was increased to 128.


<h2>Diff function – Difference between elements of vector</h2>
Differences between elements of a vector

diff(x, lag = 1, differences = 1)
x – numeric vector
lag-an integer indicating how many lags to use.
Difference- order of difference

# diff in r examples
> x=c(1,2,3,5,8,13,21)
> diff(x)
[1] 1 1 2 3 5 8

The diff function provides the option “lag”.
The default specification of this option is 1.

If we want to increase the size of the lag, we can specify the lag option within the diff command as follows:

x &lt;- c(5, 2, 10, 1, 3)
diff(x, lag = 2)                # Apply diff with lag
# 5 -1 -7

Example of difference function in R with lag 1 and differences 2:

#difference function in R with lag=1 and differences=2

diff(c(2,3,5,18,4,6,4),lag=1,differences=2)
First it is differenced with lag=1 and the result is again differenced with lag=1
So the output will be
[1]   1  11  -27   16   -4

ie. get the lag difference result, and then redo the difference again on the result:
2,3,5,18,4,6,4
  1,2,13,-14,2,-2
    1,11,-27,16,-4

<h2>cut2 function</h2>
cut2(x, cuts, m, g, levels.mean, digits, minmax=TRUE, oneval=TRUE)
Cut a Numeric Variable into Intervals
but left endpoints are inclusive and labels are of the form [lower, upper), except that last interval is [lower,upper].

x &lt;- runif(1000, 0, 100)
z &lt;- cut2(x, c(10,20,30))
table(z)
table(cut2(x, g=10))      # quantile groups
table(cut2(x, m=50))      # group x into intevals with at least 50 obs.

<h2>To clear up the memory</h2>
rm(list = ls())
.rs.restartR() # this will restart

memory.size(max=T) # gives the amount of memory obtained by the OS
memory.size(max=F) # gives the amount of memory being used
m = matrix(runif(10e7), 10000, 1000)
memory.size(max=F)

To clear up the memory
gc()
memory.size(max=F)
# still some memory being used

<h2>remove XML nodes</h2>
<a href="https://cran.r-project.org/web/packages/xml2/vignettes/modification.html" class="whitebut ">Node Modification</a>
<a href="https://cran.r-project.org/web/packages/XML/XML.pdf" class="whitebut ">Package XML</a>

#find parent nodes
parent&lt;- review %>% html_nodes("blockquote")

#find children nodes to exclude
toremove&lt;-parent %>% html_node("div.bbcode_container")

#remove nodes
xml_remove(toremove)

The xml_remove() can be used to remove a node (and it’s children) from a tree. 

library(XML)
r &lt;- xmlRoot(doc)
removeNodes(r[names(r) == "location"])

<h2>Comment out block of code</h2>

if(FALSE) {
  all your code
}


<h2>Reading XML data</h2>
Data in XML format are rarely organized in a way that would allow the xmlToDataFrame function to work. 
You're better off extracting everything in lists and then binding the lists together in a data frame:

require(XML)
data &lt;- xmlParse("http://forecast.weather.gov/MapClick.php?lat=29.803&lon=-82.411&FcstType=digitalDWML")

xml_data &lt;- xmlToList(data)

<code>&gt; install.packages("XML")</code>
<code>&gt; library(XML)
text = paste0("&lt;bookstore>&lt;book>","&lt;title>Everyday Italian&lt;/title>","&lt;author>Giada De Laurentiis&lt;/author>","&lt;year>2005&lt;/year>","&lt;/book>&lt;/bookstore>")
</code>
Parse the XML file
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]

xmlToDataFrame(nodes = getNodeSet(xmldoc, "//title"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//author"))
xmlToDataFrame(nodes = getNodeSet(xmldoc, "//book"))

newdf = xmlToDataFrame(getNodeSet(xmldoc, "//book"))
newdf = xmlToDataFrame(getNodeSet(xmldoc, "//title"))
</code>

Extract XML data:

<code>&gt; data &lt;- xmlSApply(rootNode,function(x) xmlSApply(x, xmlValue))</code>

text = paste0("&lt;CD>","&lt;TITLE>Empire Burlesque&lt;/TITLE>","&lt;ARTIST>Bob Dylan&lt;/ARTIST>","&lt;COUNTRY>USA&lt;/COUNTRY>","&lt;COMPANY>Columbia&lt;/COMPANY>","&lt;PRICE>10.90&lt;/PRICE>","&lt;YEAR>1985&lt;/YEAR>","&lt;/CD>")
<code>xmldoc &lt;- xmlParse(text)
rootNode &lt;- xmlRoot(xmldoc)
rootNode[1]</code>

Convert the extracted data into a data frame:

<code>&gt; cd.catalog &lt;- data.frame(t(data),row.names=NULL)</code>

Verify the results

The <code>xmlParse</code> function returns an object of the <code>XMLInternalDocument</code> class, which is a C-level internal data structure.
The <code>xmlRoot()</code> function gets access to the root node and its elements. 
We check the first element of the root node:

<code>&gt; rootNode[1]

$CD
&lt;CD&gt;
  &lt;TITLE&gt;Empire Burlesque&lt;/TITLE&gt;
  &lt;ARTIST&gt;Bob Dylan&lt;/ARTIST&gt;
  &lt;COUNTRY&gt;USA&lt;/COUNTRY&gt;
  &lt;COMPANY&gt;Columbia&lt;/COMPANY&gt;
  &lt;PRICE&gt;10.90&lt;/PRICE&gt;
  &lt;YEAR&gt;1985&lt;/YEAR&gt;
&lt;/CD&gt;
attr(,"class")
[1] "XMLInternalNodeList" "XMLNodeList"</code>
To extract data from the root node, we use the <code>xmlSApply()</code> function iteratively over all the children of the root node. 
The <code>xmlSApply</code> function returns a matrix.
To convert the preceding matrix into a data frame, we transpose the matrix using the <code>t()</code> function. 
We then extract the first two rows from the <code>cd.catalog</code> data frame:

<code>&gt; cd.catalog[1:2,]
             TITLE       ARTIST COUNTRY     COMPANY PRICE YEAR
1 Empire Burlesque    Bob Dylan     USA    Columbia 10.90 1985
2  Hide your heart Bonnie Tyler      UK CBS Records  9.90 1988</code>

XML data can be deeply nested and hence can become complex to extract. 
Knowledge of <code>XPath</code> will be helpful to access specific XML tags. 
R provides several functions such as <code>xpathSApply</code> and <code>getNodeSet</code> to locate specific elements.
<h4>Extracting HTML table data from a web page</h4>
Though it is possible to treat HTML data as a specialized form of XML, R provides specific functions to extract data from HTML tables as follows:

<code>&gt; url &lt;- "http://en.wikipedia.org/wiki/World_population"

webpage = read_html(url)
output = htmlParse(webpage)
tables = readHTMLTable(output)
world.pop = tables[[5]]

table.list = readHTMLTable(output, header=F)

u = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population"
webpage = read_html(u)
tables = readHTMLTable(webpage)
names(tables)
</code>
The <code>readHTMLTable()</code> function parses the web page and returns a <code>list</code> of all tables that are found on the page. 
For tables that have an <code>id</code> attribute, the function uses the <code>id</code> attribute as the name of that list element.
We are interested in extracting the "10 most populous countries," which is the fifth table; hence we use <code>tables[[5]]</code>.

<h4>Extracting a single HTML table from a web page</h4>

A single table can be extracted using the following command:

<code>&gt; table &lt;- readHTMLTable(url,which=5)</code>
Specify <code>which</code> to get data from a specific table. 
R returns a data frame.

<h2>use xpathSApply to extract html</h2>
library(RCulr)
library(XML)
 
html &lt;- read_html("http://tonybreyal.wordpress.com/2011/11/17/cool-hand-luke-aldwych-theatre-london-2011-production/", followlocation = TRUE)

doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//p", xmlValue)
cat(paste(plain.text, collapse = "\n"))

pageHeader = "http://www.hkej.com/template/dnews/jsp/toc_main.jsp"
html &lt;- read_html(pageHeader, followlocation = TRUE)
doc = htmlParse(html, asText=TRUE)
plain.text &lt;- xpathSApply(doc, "//a", xmlValue)
cat(paste(plain.text, collapse = "\n"))

<h2>reading XML using xml2</h2>
library(xml2)
library(purrr)

txt &lt;- '&lt;Doc name="Doc1">
    &lt;Lists Count="1">
        &lt;List Name="List1">
            &lt;Points Count="3">
                &lt;Point Id="1">
                    &lt;Tags Count ="1">"a"&lt;/Tags>
                    &lt;Point Position="1"  /> 
                &lt;/Point>
                &lt;Point Id="2">
                    &lt;Point Position="2"  /> 
                &lt;/Point>
                &lt;Point Id="3">
                    &lt;Tags Count="1">"c"&lt;/Tags>
                    &lt;Point Position="3"  /> 
                &lt;/Point>
            &lt;/Points>
        &lt;/List>
    &lt;/Lists>
&lt;/Doc>'

doc &lt;- read_xml(txt)
xml_find_all(doc, ".//Points/Point") %>% 
  map_df(function(x) {
    list(
      Point=xml_attr(x, "Id"),
      Tag=xml_find_first(x, ".//Tags") %>%  xml_text() %>%  gsub('^"|"$', "", .),
      Position=xml_find_first(x, ".//Point") %>% xml_attr("Position")
    )
  })



<h2>An Introduction to XPath: How to Get Started</h2>

XPath is a powerful language that is often used for scraping the web. 
It allows you to select nodes or compute values from an XML or HTML document and is actually one of the languages that you can use to extract web data using Scrapy. 
The other is CSS and while CSS selectors are a popular choice, XPath can actually allow you to do more.

With XPath, you can extract data based on text elements' contents, and not only on the page structure. 
So when you are scraping the web and you run into a hard-to-scrape website, XPath may just save the day (and a bunch of your time!).

This is an introductory tutorial that will walk you through the basic concepts of XPath, crucial to a good understanding of it, before diving into more complex use cases.
<h3>The basics</h3>
Consider this HTML document:

<code>&lt;html&gt;
  &lt;head&gt;
    &lt;title&gt;My page&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;h2&gt;Welcome to my &lt;a href="#"&gt;page&lt;/a&gt;&lt;/h2&gt;
    &lt;p&gt;This is the first paragraph.&lt;/p&gt;
    &lt;!-- this is the end --&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

XPath handles any XML/HTML document as a tree. 
This tree's root node is not part of the document itself. 
It is in fact the parent of the document element node (<code>&lt;html&gt;</code> in case of the HTML above). 
This is how the XPath tree for the HTML document looks like:

<img class="lazy" data-src="https://blog.scrapinghub.com/hs-fs/hubfs/tree-7.png" style="background-color: gray">

As you can see, there are many node types in an XPath tree:

<li><strong>Element node:</strong> represents an HTML element, a.k.a an HTML tag.</li>
<li><strong>Attribute node:</strong> represents an attribute from an element node, e.g. 
“href” attribute in <code>&lt;a href=”http://www.example.com”&gt;example&lt;/a&gt;</code>.</li>
<li><strong>Comment node:</strong> represents comments in the document (<code>&lt;!-- … --&gt;</code>).</li>
<li><strong>Text node:</strong> represents the text enclosed in an element node (<code>example</code> in <code>&lt;p&gt;example&lt;/p&gt;</code>).</li>
</ul>
Distinguishing between these different types is useful to understand how XPath expressions work. 
Now let's start digging into XPath.

Here is how we can select the title element from the page above using an XPath expression:

/html/head/title


This is what we call a location path. 
It allows us to specify the path from the <strong>context node</strong> (in this case the root of the tree) to the element we want to select, as we do when addressing files in a file system. 
The location path above has three location steps, separated by slashes. 
It roughly means: <em>start from the ‘html’ element, look for a ‘head’ element underneath, and a ‘title’ element underneath that ‘head’</em>. 
The context node changes in each step. 
For example, the <code>head</code> node is the context node when the last step is being evaluated.

However, we usually don't know or don’t care about the full explicit node-by-node path, we just care about the nodes with a given name. 
We can select them using:

//title


Which means:<em> look in the whole tree, starting from the root of the tree (<code>//</code>) and select only those nodes whose name matches <code>title</code></em>. 
In this example, <code>//</code> is the <strong>axis</strong> and <code>title</code> is the <strong>node test</strong>.

In fact, the expressions we've just seen are using XPath's abbreviated syntax. 
Translating <code>//title</code> to the full syntax we get:

/descendant-or-self::node()/child::title


So, <code>//</code> in the abbreviated syntax is short for <code>descendant-or-self</code>, which means <em>the current node or any node below it in the tree</em>. 
This part of the expression is called the <strong>axis</strong> and it specifies a set of nodes to select from, based on their direction on the tree from the current context (downwards, upwards, on the same tree level). 
Other examples of axes are: parent, child, ancestor, etc -- we’ll dig more into this later on.

The next part of the expression, <code>node()</code>, is called a <strong>node test</strong>, and it contains an expression that is evaluated to decide whether a given node should be selected or not. 
In this case, it selects nodes from all types. 
Then we have another axis,<code>child</code>, which means <em>go to the child nodes from the current context</em>, followed by another node test, which selects the nodes named as <code>title</code>.

<blockquote>
So, the <strong>axis</strong> defines where in the tree the <strong>node test</strong> should be applied and the nodes that match the node test will be returned as a <strong>result</strong>.

</blockquote>
You can test nodes against their name or against their type.

Here are some examples of name tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>/html</td>
<td>Selects the node named <code>html</code>, which is under the root.</td>
</tr>
<tr>
<td>/html/head</td>
<td>Selects the node named <code>head</code>, which is under the <code>html</code> node.</td>
</tr>
<tr>
<td>//title</td>
<td>Selects all the <code>title</code> nodes from the HTML tree.</td>
</tr>
<tr>
<td>//h2/a</td>
<td>Selects all <code>a</code> nodes which are directly under an <code>h2</code> node.</td>
</tr>
</tbody>
</table>
And here are some examples of node type tests:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//comment()</td>
<td>Selects only comment nodes.</td>
</tr>
<tr>
<td>//node()</td>
<td>Selects any kind of node in the tree.</td>
</tr>
<tr>
<td>//text()</td>
<td>Selects only text nodes, such as "This is the first paragraph".</td>
</tr>
<tr>
<td>//*</td>
<td>Selects all nodes, except comment and text nodes.</td>
</tr>
</tbody>
</table>
We can also combine name and node tests in the same expression. 
For example:

//p/text()


This expression selects the text nodes from inside <code>p</code> elements. 
In the HTML snippet shown above, it would select "This is the first paragraph.".

Now, <strong>let’s see how we can further filter and specify things</strong>. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li&gt;Quote 1&lt;/li&gt;
      &lt;li&gt;Quote 2 with &lt;a href="..."&gt;link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Quote 3 with &lt;a href="..."&gt;another link&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;h2&gt;Quote 4 title&lt;/h2&gt; ...&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the first <code>li</code> node from the snippet above. 
We can do this with:

//li[position() = 1]


The expression surrounded by square brackets is called a predicate and it filters the node set returned by <code>//li</code> (that is, all <code>li</code> nodes from the document) using the given condition. 
In this case it checks each node's position using the <code>position()</code> function, which returns the position of the current node in the resulting node set (notice that positions in XPath start at 1, not 0). 
We can abbreviate the expression above to:

//li[1]


Both XPath expressions above would select the following element:

    &lt;li class="quote"&gt;Quote 1&lt;/li&gt;


Check out a few more predicate examples:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//li[position()%2=0]</td>
<td>Selects the <code>li</code> elements at even positions.</td>
</tr>
<tr>
<td>//li[a]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element.</td>
</tr>
<tr>
<td>//li[a or h2]</td>
<td>Selects the <code>li</code> elements which enclose either an <code>a</code> or an <code>h2</code> element.</td>
</tr>
<tr>
<td>//li[ a [ text() = "link" ] ]</td>
<td>Selects the <code>li</code> elements which enclose an <code>a</code> element whose text is "link". 
Can also be written as <code>//li[ a/text()="link" ]</code>.</td>
</tr>
<tr>
<td>//li[last()]</td>
<td>Selects the last <code>li</code> element in the document.</td>
</tr>
</tbody>
</table>
So, a location path is basically composed by steps, which are separated by <code>/</code> and each step can have an axis, a node test and a predicate. 
Here we have an expression composed by two steps, each one with axis, node test and predicate:

&lt;span style="font-weight: 400;"&gt;//li[ 4 ]/h2[ text() = "Quote 4 title" ]&lt;/span&gt;


And here is the same expression, written using the non-abbreviated syntax:

/descendant-or-self::node()<br>
    /child::li[ position() = 4 ]<br>
        /child::h2[ text() = "Quote 4 title" ]


We can also <strong>combine</strong> multiple XPath expressions in a single one using the union operator <code>|</code>. 
For example, we can select all <code>a</code> and <code>h2</code> elements in the document above using this expression:

//a | //h2


Now, consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;ul&gt;
      &lt;li id="begin"&gt;&lt;a href="https://scrapy.org"&gt;Scrapy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://scrapinghub.com"&gt;Scrapinghub&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href="https://blog.scrapinghub.com"&gt;Scrapinghub Blog&lt;/a&gt;&lt;/li&gt;
      &lt;li id="end"&gt;&lt;a href="http://quotes.toscrape.com"&gt;Quotes To Scrape&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Say we want to select only the <code>a</code> elements whose link points to an HTTPS URL. 
We can do it by checking their <code>href</code> <strong>attribute</strong>:

//a[starts-with(@href, "https")]


This expression first selects all the <code>a</code> elements from the document and for each of those elements, it checks whether their <code>href</code> attribute starts with "https". 
We can access any node attribute using the <code>@attributename</code> syntax.

Here we have a few additional examples using attributes:

<table>
<tbody>
<tr>
<th>Expression</th>
<th>Meaning</th>
</tr>
<tr>
<td>//a[@href="https://scrapy.org"]</td>
<td>Selects the <code>a</code> elements pointing to https://scrapy.org.</td>
</tr>
<tr>
<td>//a/@href</td>
<td>Selects the value of the <code>href</code> attribute from all the <code>a</code> elements in the document.</td>
</tr>
<tr>
<td>//li[@id]</td>
<td>Selects only the <code>li</code> elements which have an <code>id</code> attribute.</td>
</tr>
</tbody>
</table>
<h3>More on Axes</h3>
We've seen only two types of axes so far:

<li>descendant-or-self</li>
<li>child</li>
</ul>
But there's plenty more where they came from and we'll see a few examples. 
Consider this HTML document:

<code>&lt;html&gt;
  &lt;body&gt;
    &lt;p&gt;Intro paragraph&lt;/p&gt;
    &lt;h1&gt;Title #1&lt;/h1&gt;
    &lt;p&gt;A random paragraph #1&lt;/p&gt;
    &lt;h1&gt;Title #2&lt;/h1&gt;
    &lt;p&gt;A random paragraph #2&lt;/p&gt;
    &lt;p&gt;Another one #2&lt;/p&gt;
    A single paragraph, with no markup
    &lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;</code>

Now we want to extract only the first paragraph after each of the titles. 
To do that, we can use the <code>following-sibling</code> axis, which selects all the siblings after the context node. 
Siblings are nodes who are children of the same parent, for example all children nodes of the <code>body</code> tag are siblings. 
This is the expression:

//h1/following-sibling::p[1]


In this example, the context node where the <code>following-sibling</code> axis is applied to is each of the <code>h1</code> nodes from the page.

What if we want to select only the text that is right before the <code>footer</code>? We can use the <code>preceding-sibling</code> axis:

//div[@id='footer']/preceding-sibling::text()[1]


In this case, we are selecting the first text node before the <code>div</code> footer (<em>"A single paragraph, with no markup"</em>).

XPath also allows us to select elements based on their text content. 
We can use such a feature, along with the <code>parent</code> axis, to select the parent of the <code>p</code> element whose text is "Footer text":

//p[ text()="Footer text" ]/..


The expression above selects <code>&lt;div id="footer"&gt;&lt;p&gt;Footer text&lt;/p&gt;&lt;/div&gt;</code>. 
As you may have noticed, we used <code>..</code> here as a shortcut to the <code>parent</code> axis.

As an alternative to the expression above, we could use:

//*[p/text()="Footer text"]


It selects, from all elements, the ones that have a <code>p</code> child which text is "Footer text", getting the same result as the previous expression.

You can find additional axes in the XPath specification: https://www.w3.org/TR/xpath/#axes

<h3>Wrap up</h3>
XPath is very powerful and this post is just an introduction to the basic concepts. 
If you want to learn more about it, check out these resources:

<li>http://zvon.org/comp/r/tut-XPath_1.html</li>
<li>http://fr.slideshare.net/scrapinghub/xpath-for-web-scraping</li>
<li>https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/</li>
</ul>
And <strong>stay tuned</strong>, because we will post a series with more XPath tips from the trenches in the following months.

<h2>to handle UTF</h2>
options("encoding" = "native.enc") # this is the natural environment
Sys.setlocale(category = 'LC_ALL', 'Chinese')	# to show chinese
# Sys.getlocale()
# options("encoding")

theNewsHeader = readLines("newsHeader.txt", encoding="UTF-8") # load UTF-8 file
options("encoding" = "UTF-8") # write UTF-8
sink("temp.html")

<h2><span class="gold embossts">R jsonlite to handle JSON</span></h2>
install.packages("jsonlite")
library(jsonlite)

# convert data frame to JSON array
my.json &lt;- toJSON(mtcars)

# convert JSON array to data frame
my.df &lt;- fromJSON(my.json)

# check data equality
all.equal(mtcars, my.df)
[1] TRUE

- set simplifyVector to FALSE, fromJSON will keep the raw JSON structure
ie, convert to list
fromJSON(json, simplifyVector = FALSE)

- fromJSON will convert multiple JSON structures to data frame
we may convert JSOn to data frame, and after fiddling, toJSON back to JSON.

- fromJSON will convert JSON matrix to R matrix

- higher order dimension JSON will be converted to R matrixs

<h2>Extract Components from Lists</h2>
Using [ ]
to extract a list components

Using [[ ]]
to extract only a single component

<h2>to view a list or dataframe</h2>
names(test), summary(test), head(test), tail(test), str(test)
typeof(test)

<h2>R function: cut</h2>
v &lt;- c( 8, 13, 19, 3, 14, 7, 6, 12, 18, 9, 7, 14, 2, 3, 8, 11, 17)
c &lt;- cut(v, c(0, 5, 10, 15, 20))
str(c)
 Factor w/ 4 levels "(0,5]","(5,10]",..: 2 3 4 1 3 2 2 3 4 2 ...

c # shows every element's category
#
#  [1] (5,10]  (10,15] (15,20] (0,5]   (10,15] (5,10]  (5,10]  (10,15] (15,20]
# [10] (5,10]  (5,10]  (10,15] (0,5]   (0,5]   (5,10]  (10,15] (15,20]

# Levels: (0,5] (5,10] (10,15] (15,20]

<h2>use cumsum() to create cumulative frequency graph</h2>

dataset = sample(1:20,100, replace= TRUE)
breaks = seq(0, 20, by=2) 
datasetCategory = cut(dataset, breaks, right=FALSE) 
dataset.freq = table(datasetCategory)

barplot(dataset.freq) # this show every category but not cumulative

We then compute its cumulative frequency with cumsum, add a starting zero element, and plot the graph.

cumfreq0 = c(0, cumsum(dataset.freq)) 
plot(breaks, cumfreq0,                 # plot the data 
 main="Old Faithful Eruptions",        # main title 
 xlab="dataset minutes",               # x−axis label 
 ylab="cumulative frequency graph")    # y−axis label 
lines(breaks, cumfreq0)                # join the points

<h2>to prevent scientific notation</h2>
Use a large positive value like 999 in options:
options(scipen=999)
to revert it back, the default scipen is 0

<h2>process daily data</h2>
# kline_dayqfq={"code":0,"msg":"","data":{"hk00700":{"qfqday":[["2020-01-14","410.000","400.400","413.000","396.600","26827634.000",{},"0.000","1086386.492"],

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/hkfqkline/get?_var=kline_dayqfq&param=hk00700,day,,,40,qfq"

my.json &lt;- readLines(urlAddr, warn=F)
my.json = gsub("kline_dayqfq=","",my.json) # remove the leading command

my.dataframe = fromJSON(my.json)
my.dataframe = my.dataframe[[3]][[1]][[1]] # 40 obs., list of list
# chr "2020-01-14"   Date   1
# chr "410.000"      open   2
# chr "400.400"      close  3
# chr "413.000"      high   4
# chr "396.600"      low    5
# chr "26827634.000" Qty    6
# Named list()              7
# chr "0.000"               8
# chr "1086386.492"  Amt    9

my.dataframe[[1]][1]  # date
my.dataframe[[1]][3]  # close

for (i in 1:40){      # remove column 7
  my.dataframe[[i]] = my.dataframe[[i]][-(7:8)]
}

dataMatrix = matrix(unlist(my.dataframe), nrow=40, ncol=7)  # convert to matrix

<h2>process minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00981"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only the third item is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec"
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, most recent day on top

datalist = unlist(my.list) # this is all strings in one vector


<h2>statistics of minute data</h2>
# {"code":0,"msg":"","data":{"hk00981":{"data":[{"date":"20200311","data":["0930 14.460 346508","0931 14.460 1564508",

library(jsonlite)
urlAddr = "http://web.ifzq.gtimg.cn/appstock/app/day/query?code=hk00388"
my.json &lt;- readLines(urlAddr, warn=F)
my.dataframe = fromJSON(my.json)
# str(my.dataframe), only list 3 is useful
# List of 3
#  $ code: int 0
#  $ msg : chr ""
#  $ data:List of 1

my.dataframe = my.dataframe[[3]][[1]][[1]] # 5 obs. of  3 variables:"date" "data" "prec", 5 obs for 5days
# names(my.dataframe)
my.list = my.dataframe[[2]] # this object is a list of five vectors, nearest day on top

datalist = unlist(my.list) # this is all strings in one vector

datalist = gsub("^.* ","",datalist) # this is the amount
datalist = round(as.numeric(datalist)/10000,0) # units in wan
datalist = sort(datalist)
datalist = datalist[-(1:20)]
datalist = datalist[-( (length(datalist)-20):length(datalist))] # remove the extremes

# max(datalist); min(datalist); length(datalist)

sections &lt;- cut(datalist, breaks = 100)
table(sections)
barplot(table(sections))

cumulative sums
plot(cumsum(table(sections)))

<h2>R examples</h2>
https://www.datamentor.io/r-programming/examples/
http://www.rexamples.com
https://www.guru99.com/r-tutorial.html
https://r4stats.com/examples/programming/
https://www.statmethods.net/r-tutorial/index.html
http://rprogramming.net

<h2>output text to the R console in color</h2>
library(crayon)
cat(blue("Hello", "world!\n"))

Genaral styles
reset, bold
blurred (usually called ‘dim’, renamed to avoid name clash)
italic (not widely supported)
underline, inverse, hidden
strikethrough (not widely supported)

Text colors
black, red, green, yellow, blue, magenta, cyan, white
silver (usually called ‘gray’, renamed to avoid name clash)

Background colors
bgBlack, bgRed, bgGreen, bgYellow, bgBlue, bgMagenta, bgCyan, bgWhite

Styling
The styling functions take any number of character vectors as arguments, and they concatenate and style them:

Crayon defines the %+% string concatenation operator, to make it easy to assemble stings with different styles.

cat("... to highlight the " %+%
    red("search term") %+%
    " in a block of text\n")

Styles can be combined using the $ operator:
  cat(yellow$bgMagenta$bold('Hello world!\n'))
See also combine_styles().

Styles can also be nested, and then inner style takes precedence:
  cat(green(
    'I am a green line ' %+%
    blue$underline$bold('with a blue substring') %+%
    ' that becomes green again!\n'
  ))

define your own themes:
  error &lt;- red $ bold
  warn &lt;- magenta $ underline
  note &lt;- cyan
  cat(error("Error: subscript out of bounds!\n"))
  cat(warn("Warning: shorter argument was recycled.\n"))
  cat(note("Note: no such directory.\n"))

See Also make_style() for using the 256 ANSI colors.

Examples
cat(blue("Hello", "world!"))
cat("... to highlight the " %+% red("search term") %+%
    " in a block of text")
cat(yellow$bgMagenta$bold('Hello world!'))
cat(green(
 'I am a green line ' %+%
 blue$underline$bold('with a blue substring') %+%
 ' that becomes green again!'
))
error &lt;- red $ bold
warn &lt;- magenta $ underline
note &lt;- cyan
cat(error("Error: subscript out of bounds!\n"))
cat(warn("Warning: shorter argument was recycled.\n"))
cat(note("Note: no such directory.\n"))

<h3>style - Add Style To A String</h3>
Usage
style(string, as = NULL, bg = NULL)
cat(style("I am pink\n", "pink"))
cat(style("#4682B433\n", "#4682B433"))
cat(style("#002050\n", "#002050"))


<h3>rgb()</h3>
To use the function:
rgb(red, green, blue, alpha) : quantity of red (between 0 and 1), of green and of blue, and finally transparency (alpha).
newcolor = rgb(0.5, 0.2, 0.1, 0.8)
newcolor
"#80331ACC"

cat(style("newcolor\n", newcolor))  # note, without quotation marks


<h3>make_style</h3>
pink &lt;- make_style("pink")
bgMaroon &lt;- make_style(rgb(0.93, 0.19, 0.65), bg = TRUE)
cat(bgMaroon(pink("pink style.\n")))

## Create a new style for pink and maroon background
make_style(pink = "pink")
make_style(bgMaroon = rgb(0.0, 0.3, 0.3), bg = TRUE)
"pink" %in% names(styles())
"bgMaroon" %in% names(styles())

cat(style("I am pink, too!\n", "pink"))
cat(style("I am pink, too!\n", "pink", bg = "blue")) # color will change
cat(style("I am pink, too!\n", "pink", bg = "bgMaroon"))
cat(style("I am pink, too!\n", "pink", bg = "cyan"))

<h2>print strings with wordwraps</h2>

strwrap(astring, width = 110, indent = 5, exdent = 2))
use writeLines to print it
note: control characters inside string will be ignored.

astring = "Substituted with the text matched by the capturing group that can be found by counting as many opening parentheses of named or numbered capturing groups as specified by the number from right to left starting at the backreference."

writeLines(strwrap(astring, width = 110, indent = 5, exdent = 2))

<h2>R.utils withTimeout()</h2>

withTimeout() from package R.utils, in concert with tryCatch(), might provide a cleaner solution.

For example:
require(R.utils)

for(i in 1:5) {
    tryCatch(
        expr = {
            withTimeout({Sys.sleep(i); cat(i, "\n")}, 
                         timeout = 3.1)
            }, 
        TimeoutException = function(ex) cat("Timeout. Skipping.\n")
    )
}

# 1 
# 2 
# 3 
# Timeout. Skipping.
# Timeout. Skipping.

In the artificial example above:

The first argument to withTimeout() contains the code to be evaluated within each loop.

The timeout argument to withTimeout() sets the time limit in seconds.

The TimeoutException argument to tryCatch() takes a function that is to be executed when an iteration of the loop is timed out.

<h2>drawing SVG</h2>
<a href="https://cran.r-project.org/web/packages/RIdeogram/vignettes/RIdeogram.html" class="whitebut ">RIdeogram: drawing SVG graphics</a>

<a href="https://cran.r-project.org/web/packages/magick/vignettes/intro.html" class="whitebut ">Magick: Advanced Image-Processing</a>

<a href="http://ralanbutler.com/blog/2016/03/31/animated-SVG-R" class="whitebut ">Animating an SVG</a>

svglite + ggsave function
Saving a plot as an SVG

sample code:
require("ggplot2")

#some sample data
head(diamonds) 

#to see actually what will be plotted and compare 
qplot(clarity, data=diamonds, fill=cut, geom="bar")

#save the plot in a variable image to be able to export to svg
image=qplot(clarity, data=diamonds, fill=cut, geom="bar")

#This actually save the plot in a image
ggsave(file="test.svg", plot=image, width=10, height=8)

<h2>Package ‘TTR’</h2>
Technical Trading Rules
x=c(1,2,4,3,5,6,5,4,5,6,7,9,10,11,10)

Usage
SMA(x, n = 4)
EMA(x, n = 4)
DEMA(x, n = 4)
WMA(x, n = 4, wts = 1:n)
EVWMA(price, volume, n = 4)
ZLEMA(x, n = 4, ratio = NULL)
VWAP(price, volume, n = 4)
VMA(x, w, ratio = 1)
HMA(x, n = 20)
ALMA(x, n = 9, offset = 0.85, sigma = 6)

<h3>Weighted moving average WMA</h3>
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Weighted_moving_average_weights_N%3D15.png/220px-Weighted_moving_average_weights_N%3D15.png">

<h3>Exponential moving average EMA</h3>
EMA is more exagerating
<img class="lazy" data-src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Exponential_moving_average_weights_N%3D15.png/220px-Exponential_moving_average_weights_N%3D15.png">

<h2>自然语言处理中的Transformer和BERT</h2>
2018年马上就要过去，回顾深度学习在今年的进展，让人印象最深刻的就是谷歌提出的应用于自然语言处理领域的BERT解决方案，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805">
https://arxiv.org/abs/1810.04805</a>）。
BERT解决方案刷新了各大NLP任务的榜单，在各种NLP任务上都做到state of the art。
这里我把BERT说成是解决方案，而不是一个算法，因为这篇文章并没有提出新的算法模型，还是沿用了之前已有的算法模型。
BERT最大的创新点，在于提出了一套完整的方案，利用之前最新的算法模型，去解决各种各样的NLP任务，因此BERT这篇论文对于算法模型完全不做介绍，以至于在我直接看这篇文章的时候感觉云里雾里。
但是本文中，我会从算法模型到解决方案，进行完整的诠释。
本文中我会分3个部分进行介绍，第一部分我会大概介绍一下NLP的发展，第二部分主要讲BERT用到的算法，最后一部分讲BERT具体是怎么操作的。

<h3>一，NLP的发展</h3>
要处理NLP问题，首先要解决文本的表示问题。
虽然我们人去看文本，能够清楚明白文本中的符号表达什么含义，但是计算机只能做数学计算，需要将文本表示成计算机可以处理的形式。
最开始的方法是采用one hot，比如，我们假设英文中常用的单词有3万个，那么我们就用一个3万维的向量表示这个词，所有位置都置0，当我们想表示apple这个词时，就在对应位置设置1，如图1.1所示。
这种表示方式存在的问题就是，高维稀疏，高维是指有多少个词，就需要多少个维度的向量，稀疏是指，每个向量中大部分值都是0。
另外一个不足是这个向量没有任何含义。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-597b011ddd148eb53b5a90730b6090ae_b.jpg">

<figcaption>图1.1</figcaption>
后来出现了词向量，word embedding，用一个低维稠密的向量去表示一个词，如图1.2所示。
通常这个向量的维度在几百到上千之间，相比one hot几千几万的维度就低了很多。
词与词之间可以通过相似度或者距离来表示关系，相关的词向量相似度比较高，或者距离比较近，不相关的词向量相似度低，或者距离比较远，这样词向量本身就有了含义。
文本的表示问题就得到了解决。
词向量可以通过一些无监督的方法学习得到，比如CBOW或者Skip-Gram等，可以预先在语料库上训练出词向量，以供后续的使用。
顺便提一句，在图像中就不存在表示方法的困扰，因为图像本身就是数值矩阵，计算机可以直接处理。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-840859265e735cce77233bb42a4bee6a_b.png">

<figcaption>图1.2</figcaption>
NLP中有各种各样的任务，比如分类（Classification），问答（QA），实体命名识别（NER）等。
对于这些不同的任务，最早的做法是根据每类任务定制不同的模型，输入预训练好的embedding，然后利用特定任务的数据集对模型进行训练，如图1.3所示。
这里存在的问题就是，不是每个特定任务都有大量的标签数据可供训练，对于那些数据集非常小的任务，恐怕就难以得到一个理想的模型。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-4546b7aa51af50d3ac0c7504f965cc70_b.jpg">

<figcaption>图1.3</figcaption>
我们看一下图像领域是如何解决这个问题的。
图像分类是计算机视觉中最基本的任务，当我要解决一个小数据集的图像分类任务时，该怎么做？CV领域已经有了一套成熟的解决方案。
我会用一个通用的网络模型，比如Vgg，ResNet或者GoogleNet，在ImageNet上做预训练（pre-training）。
ImageNet有1400万张有标注的图片，包含1000个类别，这样的数据规模足以训练出一个规模庞大的模型。
在训练过程中，模型会不断的学习如何提取特征，底层的CNN网络结构会提取边缘，角，点等通用特征，模型越往上走，提取的特征也越抽象，与特定的任务更加相关。
当完成预训练之后，根据我自己的分类任务，调整最上层的网络结构，然后在小数据集里对模型进行训练。
在训练时，可以固定住底层的模型参数只训练顶层的参数，也可以对整个模型进行训练，这个过程叫做微调（fine-tuning），最终得到一个可用的模型。
总结一下，整个过程包括两步，拿一个通用模型在ImageNet上做预训练（pre-training），然后针对特定任务进行微调（fine-tuning），完美解决了特定任务数据不足的问题。
还有一个好处是，对于各种各样的任务都不再需要从头开始训练网络，可以直接拿预训练好的结果进行微调，既减少了训练计算量的负担，也减少了人工标注数据的负担。

NLP领域也引入了这种做法，用一个通用模型，在非常大的语料库上进行预训练，然后在特定任务上进行微调，BERT就是这套方案的集大成者。
BERT不是第一个，但目前为止，是效果最好的方案。
BERT用了一个已有的模型结构，提出了一整套的预训练方法和微调方法，我们在后文中再进行详细的描述。

<h3>二，算法</h3>
BERT所采用的算法来自于2017年12月份的这篇文章，Attenion Is All You Need（<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762">
https://arxiv.org/abs/1706.03762</a>），同样来自于谷歌。
这篇文章要解决的是翻译问题，比如从中文翻译成英文。
这篇文章完全放弃了以往经常采用的RNN和CNN，提出了一种新的网络结构，即Transformer，其中包括encoder和decoder，我们只关注encoder。
这篇英文博客（<a href="https://link.zhihu.com/?target=https%3A//jalammar.github.io/illustrated-transformer/">
https://jalammar.github.io/illustrated-transformer/</a>）对Transformer介绍得非常详细，有兴趣的读者可以看一下，如果不想看英文博客也可以看本文，本文中的部分图片也截取自这篇博客。

<img class="lazy" data-src="https://pic4.zhimg.com/v2-393d5284f5132c3150b294cfc5e5218f_b.jpg">

<figcaption>图2.1</figcaption>
图2.1是Transformer encoder的结构，后文中我们都简称为Transformer。
首先是输入word embedding，这里是直接输入一整句话的所有embedding。
如图2.1所示，假设我们的输入是Thinking Machines，每个词对应一个embedding，就有2个embedding。
输入embedding需要加上位置编码（Positional Encoding），为什么要加位置编码，后文会做详细介绍。
然后经过一个Multi-Head Attention结构，这个结构是算法单元中最重要的部分，我们会在后边详细介绍。
之后是做了一个shortcut的处理，就是把输入和输出按照对应位置加起来，如果了解残差网络（ResNet）的同学，会对这个结构比较熟悉，这个操作有利于加速训练。
然后经过一个归一化normalization的操作。
接着经过一个两层的全连接网络，最后同样是shortcut和normalization的操作。
可以看到，除了Multi-Head Attention，都是常规操作，没有什么难理解的。
这里需要注意的是，每个小模块的输入和输出向量，维度都是相等的，比如，Multi-Head Attention的输入和输出向量维度是相等的，否则无法进行shortcut的操作；Feed Forward的输入和输出向量维度也是相等的；最终的输出和输入向量维度也是相等的。
但是Multi-Head Attention和Feed Forward内部，向量维度会发生变化。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-4019f1ffead184e3bc00aabb41e6b6b6_b.jpg">

<figcaption>图2.2</figcaption>
我们来详细看一下Multi-Head Attention的结构。
这个Multi-Head表示多头的意思，先从最简单的看起，看看单头Attention是如何操作的。
从图2.1的橙色方块可以看到，embedding在进入到Attention之前，有3个分叉，那表示说从1个向量，变成了3个向量。
具体是怎么算的呢？我们看图2.3，定义一个WQ矩阵（这个矩阵随机初始化，通过训练得到），将embedding和WQ矩阵做乘法，得到查询向量q，假设输入embedding是512维，在图3中我们用4个小方格表示，输出的查询向量是64维，图3中用3个小方格以示不同。
然后类似地，定义WK和WV矩阵，将embedding和WK做矩阵乘法，得到键向量k；将embeding和WV做矩阵乘法，得到值向量v。
对每一个embedding做同样的操作，那么每个输入就得到了3个向量，查询向量，键向量和值向量。
需要注意的是，查询向量和键向量要有相同的维度，值向量的维度可以相同，也可以不同，但一般也是相同的。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ac045486e0eff3b8a1eb27d2ae61a634_b.jpg">

<figcaption>图2.3</figcaption>
接下来我们计算每一个embedding的输出，以第一个词Thinking为例，参看图2.4。
用查询向量q1跟键向量k1和k2分别做点积，得到112和96两个数值。
这也是为什么前文提到查询向量和键向量的维度必须要一致，否则无法做点积。
然后除以常数8，得到14和12两个数值。
这个常数8是键向量的维度的开方，键向量和查询向量的维度都是64，开方后是8。
做这个尺度上的调整目的是为了易于训练。
然后把14和12丢到softmax函数中，得到一组加和为1的系数权重，算出来是大约是0.88和0.12。
将0.88和0.12对两个值向量v1和v2做加权求和，就得到了Thinking的输出向量z1。
类似的，可以算出Machines的输出z2。
如果一句话中包含更多的词，也是相同的计算方法。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-b25bb6a8f9b57a4831b485015080b8c1_b.jpg">

<figcaption>图2.4</figcaption>
通过这样一系列的计算，可以看到，现在每个词的输出向量z都包含了其他词的信息，每个词都不再是孤立的了。
而且每个位置中，词与词的相关程度，可以通过softmax输出的权重进行分析。
如图2.5所示，这是某一次计算的权重，其中线条颜色的深浅反映了权重的大小，可以看到it中权重最大的两个词是The和animal，表示it跟这两个词关联最大。
这就是attention的含义，输出跟哪个词关联比较强，就放比较多的注意力在上面。
上面我们把每一步计算都拆开了看，实际计算的时候，可以通过矩阵来计算，如图2.6所示。

<img class="lazy" data-src="https://pic2.zhimg.com/v2-2dbdd8dfb5088d22c7dd9d05a0e1035d_b.jpg">

<figcaption>图2.5</figcaption>

<img class="lazy" data-src="https://pic4.zhimg.com/v2-a02ab6ab4cad1f8fef307ced0a4cf9d3_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic2.zhimg.com/v2-d00785a9cfb835b5a345898e37b31be9_b.jpg">

<figcaption>图2.6</figcaption>
讲完了attention，再来讲Multi-Head。
对于同一组输入embedding，我们可以并行做若干组上面的操作，例如，我们可以进行8组这样的运算，每一组都有WQ，WK，WV矩阵，并且不同组的矩阵也不相同。
这样最终会计算出8组输出，我们把8组的输出连接起来，并且乘以矩阵WO做一次线性变换得到输出，WO也是随机初始化，通过训练得到，计算过程如图2.7所示。
这样的好处，一是多个组可以并行计算，二是不同的组可以捕获不同的子空间的信息。


<img class="lazy" data-src="https://pic3.zhimg.com/v2-2afc28e06f5550d5e20a2dc290f2224e_b.jpg">

<figcaption>图2.7</figcaption>
到这里就把Transformer的结构讲完了，同样都是做NLP任务，我们来和RNN做个对比。
图2.8是个最基本的RNN结构，还有计算公式。
当计算隐向量h4时，用到了输入x4，和上一步算出来的隐向量h3，h3包含了前面所有节点的信息。
h4中包含最多的信息是当前的输入x4，越往前的输入，随着距离的增加，信息衰减得越多。
对于每一个输出隐向量h都是如此，包含信息最多得是当前的输入，随着距离拉远，包含前面输入的信息越来越少。
但是Transformer这个结构就不存在这个问题，不管当前词和其他词的空间距离有多远，包含其他词的信息不取决于距离，而是取决于两者的相关性，这是Transformer的第一个优势。
第二个优势在于，对于Transformer来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。
而RNN只能用到前面的词，这并不是个严重的问题，因为这可以通过双向RNN来解决。
第三点，RNN是一个顺序的结构，必须要一步一步地计算，只有计算出h1，才能计算h2，再计算h3，隐向量无法同时并行计算，导致RNN的计算效率不高，这是RNN的固有结构所造成的，之前有一些工作就是在研究如何对RNN的计算并行化。
通过前文的介绍，可以看到Transformer不存在这个问题。
通过这里的比较，可以看到Transformer相对于RNN有巨大的优势，因此我看到有人说RNN以后会被取代。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-5bafe804c0dc77f945ade48561de63a0_b.jpg" data-caption="">

<img class="lazy" data-src="https://pic1.zhimg.com/v2-235854b916c55c54bbcad343443885c0_b.jpg">

<figcaption>图2.8</figcaption>
关于上面的第三点优势，可能有人会不认可，RNN的结构包含了序列的时序信息，而Transformer却完全把时序信息给丢掉了。
为了解决时序的问题，Transformer的作者用了一个绝妙的办法，这就是我在前文提到的位置编码（Positional Encoding）。
位置编码是和word embedding同样维度的向量，将位置embedding和词embedding加在一起，作为输入embedding，如图2.9所示。
位置编码可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数，这里不再多说。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-df4b9de6b9feb1971ab7225ebc4454d2_b.jpg">

<figcaption>图2.9</figcaption>
我们把图2.1的结构作为一个基本单元，把N个这样的基本单元顺序连起来，就是BERT的算法模型，如图2.10所示。
从前面的描述中可以看到，当输入有多少个embedding，那么输出也就有相同数量的embedding，可以采用和RNN采用相同的叫法，把输出叫做隐向量。
在做具体NLP任务的时候，只需要从中取对应的隐向量作为输出即可。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-81e63e36210c8e342d193be69c441e7c_b.jpg">

<figcaption>图2.10</figcaption>
<h3>三，BERT</h3>
在介绍BERT之前，我们先看看另外一套方案。
我在第一部分说过，BERT并不是第一个提出预训练加微调的方案，此前还有一套方案叫GPT，这也是BERT重点对比的方案，文章在这，Improving Language Understanding by Generative Pre-Training（<a href="https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</a>）。
GPT的模型结构和BERT是相同的，都是图2.10的结构，只是BERT的模型规模更加庞大。
GPT是这么预训练的，在一个8亿单词的语料库上做训练，给出前文，不断地预测下一个单词。
比如这句话，Winter is coming，当给出第一个词Winter之后，预测下一个词is，之后再预测下一个词coming。
不需要标注数据，通过这种无监督训练的方式，得到一个预训练模型。

我们再来看看BERT有什么不同。
BERT来自于Bidirectional Encoder Representations from Transformers首字母缩写，这里提到了一个双向（Bidirectional）的概念。
BERT在一个33亿单词的语料库上做预训练，语料库就要比GPT大了几倍。
预训练包括了两个任务，第一个任务是随机地扣掉15%的单词，用一个掩码MASK代替，让模型去猜测这个单词；第二个任务是，每个训练样本是一个上下句，有50%的样本，下句和上句是真实的，另外50%的样本，下句和上句是无关的，模型需要判断两句的关系。
这两个任务各有一个loss，将这两个loss加起来作为总的loss进行优化。
下面两行是一个小栗子，用括号标注的是扣掉的词，用[MASK]来代替。

<b>正样本：我[MASK]（是）个算法工程师，我服务于WiFi万能钥匙这家[MASK]（公司）。
</b>
<b>负样本：我[MASK]（是）个算法工程师，今天[MASK]（股票）又跌了。
</b>
我们来对比下GPT和BERT两种预训练方式的优劣。
GPT在预测词的时候，只预测下一个词，因此只能用到上文的信息，无法利用到下文的信息。
而BERT是预测文中扣掉的词，可以充分利用到上下文的信息，这使得模型有更强的表达能力，这也是BERT中Bidirectional的含义。
在一些NLP任务中需要判断句子关系，比如判断两句话是否有相同的含义。
BERT有了第二个任务，就能够很好的捕捉句子之间的关系。
图3.1是BERT原文中对另外两种方法的预训练对比，包括GPT和ELMo。
ELMo采用的还是LSTM，这里我们不多讲ELMo。
这里会有读者困惑，这里的结构图怎么跟图2.10不一样？如果熟悉LSTM的同学，看到最右边的ELMo，就会知道那些水平相连的LSTM其实只是一个LSTM单元。
左边的BERT和GPT也是一样，水平方向的Trm表示的是同一个单元，图中那些复杂的连线表示的是词与词之间的依赖关系，BERT中的依赖关系既有前文又有后文，而GPT的依赖关系只有前文。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-287ba1129d213df7e2ed5adb7c4a440e_b.jpg">

<figcaption>图3.1</figcaption>
讲完了这两个任务，我们再来看看，如何表达这么复杂的一个训练样本，让计算机能够明白。
图3.2表示“my dog is cute, he likes playing.”的输入形式。
每个符号的输入由3部分构成，一个是词本身的embedding；第二个是表示上下句的embedding，如果是上句，就用A embedding，如果是下句，就用B embedding；最后，根据Transformer模型的特点，还要加上位置embedding，这里的位置embedding是通过学习的方式得到的，BERT设计一个样本最多支持512个位置；将3个embedding相加，作为输入。
需要注意的是，在每个句子的开头，需要加一个Classification（CLS）符号，后文中会进行介绍，其他的一些小细节就不说了。

<img class="lazy" data-src="https://pic1.zhimg.com/v2-ec06762a57a7d7176747627dc3ee20b4_b.jpg">

<figcaption>图3.2</figcaption>
完成预训练之后，就要针对特定任务就行微调了，这里描述一下论文中的4个例子，看图3.4。
首先说下分类任务，分类任务包括对单句子的分类任务，比如判断电影评论是喜欢还是讨厌；多句子分类，比如判断两句话是否表示相同的含义。
图3.4（a）（b）是对这类任务的一个示例，左边表示两个句子的分类，右边是单句子分类。
在输出的隐向量中，取出CLS对应的向量C，加一层网络W，并丢给softmax进行分类，得到预测结果P，计算过程如图3.3中的计算公式。
在特定任务数据集中对Transformer模型的所有参数和网络W共同训练，直到收敛。
新增加的网络W是HxK维，H表示隐向量的维度，K表示分类数量，W的参数数量相比预训练模型的参数少得可怜。

<img class="lazy" data-src="https://pic3.zhimg.com/v2-61486f520243716de645f904e3a36ac2_b.jpg">

<figcaption>图3.3</figcaption>

<img class="lazy" data-src="https://pic3.zhimg.com/v2-42514100ab16b207d2732729c85fccaa_b.jpg">

<figcaption>图3.4</figcaption>
我们再来看问答任务，如图3.4（c），以SQuAD v1.1为例，给出一个问题Question，并且给出一个段落Paragraph，然后从段落中标出答案的具体位置。
需要学习一个开始向量S，维度和输出隐向量维度相同，然后和所有的隐向量做点积，取值最大的词作为开始位置；另外再学一个结束向量E，做同样的运算，得到结束位置。
附加一个条件，结束位置一定要大于开始位置。
最后再看NER任务，实体命名识别，比如给出一句话，对每个词进行标注，判断属于人名，地名，机构名，还是其他。
如图3.4（d）所示，加一层分类网络，对每个输出隐向量都做一次判断。
可以看到，这些任务，都只需要新增少量的参数，然后在特定数据集上进行训练即可。
从实验结果来看，即便是很小的数据集，也能取得不错的效果。

<h2>Delete Files unlink("data.txt")</h2>
Delete Files and Directories. unlink deletes the file(s) or directories specified by x .

Usage. unlink(x, recursive = FALSE, force = FALSE)

<h2>scan</h2>
Read data into a vector or list from the console or file.

cat("TITLE extra line", "2 3 5 7", "11 13 17", file = "ex.data", sep = "\n")
pp &lt;- scan("ex.data", skip = 1, quiet = TRUE)
scan("ex.data", skip = 1)
scan("ex.data", skip = 1, nlines = 1) # only 1 line after the skipped one
scan("ex.data", what = list("","","")) # flush is F -> read "7"
scan("ex.data", what = list("","",""), flush = TRUE)
unlink("ex.data") # tidy up

## "inline" usage
scan(text = "1 2 3")

<h2>Copy an R data.frame to an Excel spreadsheet</h2>
write.excel &lt;- function(x,row.names=FALSE,col.names=TRUE,...) {
  write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}

write.excel(my.df)

and finally Ctr+V in Excel :)

<h2>copy a table x to the clipboard preserving the table structure</h2>
write.table(x, "clipboard", sep="\t")

write.table(x, "clipboard", sep="\t", row.names=FALSE)
write.table(x, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

s = c('aa','gb','rc')
n = c('af','rd','ac')
df = data.frame(n,s)

write.table(df, "clipboard", sep="\t", row.names=FALSE, col.names=FALSE)

"af"	"aa"
"rd"	"gb"
"ac"	"rc"

<h2>read.table</h2>
reads a file into data frame in table format

x &lt;- read.table("tp.txt",header=T,sep="\t");

<h3>copy a table from the clipboard</h3>
x &lt;- read.table("clipboard",header=F,sep="\t");

<h2>distributed programming</h2>
reasons for distributed programming:

To speed up a process or piece of code
To scale up an interface or application for multiple users

<a href="http://spark.apache.org/docs/latest/sparkr.html" class="whitebut ">SparkR</a>: R on Apache Spark

SparkR provides an R frontend to Apache Spark and using Spark’s distributed computation engine allows us to run large scale data analysis from the R shell.

To get started you need to set up a Spark cluster. 
<a href="http://paxcel.net/blog/how-to-setup-apache-spark-standalone-cluster-on-multiple-machine/" class="whitebut ">SETUP APACHE SPARK STANDALONE CLUSTER ON MULTIPLE MACHINE</a>

The Spark documentation, without using Mesos or YARN as your cluster manager
<a href="http://spark.apache.org/docs/latest/spark-standalone.html" class="whitebut ">Spark Standalone Mode</a>

Once you have Spark set up, see <a href="https://rpubs.com/wendyu/sparkr" class="whitebut ">Wendy Yu's tutorial on SparkR</a>

She also shows how to integrate H20 with Spark which is referred to as 'Sparkling Water'.

R has been shipping with a base library parallel. 

In a nutshell, you can just do something like

mclapply(1:nCores, someFunction())
and the function someFunction() will be run in parallel over nCores. 
A default value of half your physical cores may be a good start.

<a href="https://cran.r-project.org/web/views/HighPerformanceComputing.html" class="whitebut ">High Performance Computing</a>

<h2>matrix operation</h2>
MatA &lt;- matrix(1:9, nrow = 3)  
MatB &lt;- matrix(9:1, nrow = 3)  
MatA + MatB

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Is Something a Matrix
> <span class="orange">is.matrix(A)</span>

[1] TRUE

> is.vector(A)

[1] FALSE
<span class="orange">Multiplication by a Scalar</span>
> c &lt;- 3
> c*A

     [,1] [,2]
[1,]    6    3
[2,]    9    6
[3,]   -6    6
<span class="orange">Matrix Addition & Subtraction</span>
> B &lt;- matrix(c(1,4,-2,1,2,1),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    4    2
[3,]   -2    1

> C &lt;- A + B
> C 

     [,1] [,2]
[1,]    3    2
[2,]    7    4
[3,]   -4    3

> D &lt;- A - B
> D

     [,1] [,2]
[1,]    1    0
[2,]   -1    0
[3,]    0    1

<span class="orange">Matrix Multiplication</span>
> D &lt;- matrix(c(2,-2,1,2,3,1),2,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3
[2,]   -2    2    1

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10
[2,]    0    4

> C &lt;- A %*% D
> C

     [,1] [,2] [,3]
[1,]    2    4    7
[2,]    2    7   11
[3,]   -8    2   -4

> D &lt;- matrix(c(2,1,3),1,3)
> D

     [,1] [,2] [,3]
[1,]    2    1    3

> C &lt;- D %*% A
> C

     [,1] [,2]
[1,]    1   10

> C &lt;- A %*% D

Error in A %*% D : non-conformable arguments
<span class="orange">Transpose of a Matrix</span>
> AT &lt;- t(A)
> AT

     [,1] [,2] [,3]
[1,]    2    3   -2
[2,]    1    2    2

> ATT &lt;- t(AT)
>ATT

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
Common Vectors
<span class="orange">Unit Vector</span>
> U &lt;- matrix(1,3,1)
> U

     [,1]
[1,]    1
[2,]    1
[3,]    1
<span class="orange">Zero Vector</span>
> Z &lt;- matrix(0,3,1)
> Z

     [,1]
[1,]    0
[2,]    0
[3,]    0
Common Matrices
<span class="orange">Unit Matrix</span>
> U &lt;- matrix(1,3,2)
> U

     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    1    1
<span class="orange">Zero Matrix</span>
> Z &lt;- matrix(0,3,2)
> Z

     [,1] [,2]
[1,]    0    0
[2,]    0    0
[3,]    0    0
<span class="orange">Diagonal Matrix</span>
> S &lt;- matrix(c(2,3,-2,1,2,2,4,2,3),3,3)
> S

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    2
[3,]   -2    2    3

> D &lt;- diag(S)
> D

[1] 2 2 3

> D &lt;- diag(diag(S))
> D

     [,1] [,2] [,3]
[1,]    2    0    0
[2,]    0    2    0
[3,]    0    0    3
<span class="orange">Identity Matrix</span>
> I &lt;- diag(c(1,1,1))
> I

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Symmetric Matrix</span>
> C &lt;- matrix(c(2,1,5,1,3,4,5,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2

> CT &lt;- t(C)
> CT

     [,1] [,2] [,3]
[1,]    2    1    5
[2,]    1    3    4
[3,]    5    4   -2
<span class="orange">Inverse of a Matrix</span>
> A &lt;- matrix(c(4,4,-2,2,6,2,2,8,4),3,3)
> A

     [,1] [,2] [,3]
[1,]    4    2    2
[2,]    4    6    8
[3,]   -2    2    4


> AI &lt;- solve(A)
> AI

     [,1] [,2] [,3]
[1,]  1.0 -0.5  0.5
[2,] -4.0  2.5 -3.0
[3,]  2.5 -1.5  2.0

> A %*% AI

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1

> AI %*% A

     [,1] [,2] [,3]
[1,]    1    0    0
[2,]    0    1    0
[3,]    0    0    1
<span class="orange">Inverse & Determinant of a Matrix</span>
> C &lt;- matrix(c(2,1,6,1,3,4,6,4,-2),3,3)
> C

     [,1] [,2] [,3]
[1,]    2    1    6
[2,]    1    3    4
[3,]    6    4   -2

> CI &lt;- solve(C)
CI

           [,1]        [,2]        [,3]
[1,]  0.2156863 -0.25490196  0.13725490
[2,] -0.2549020  0.39215686  0.01960784
[3,]  0.1372549  0.01960784 -0.04901961

> d &lt;- det(C)
> d

[1] -102
<span class="orange">Rank of a Matrix</span>
> A &lt;- matrix(c(2,3,-2,1,2,2,4,7,0),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    7
[3,]   -2    2    0

> matA &lt;- qr(A)
> matA$rank

[1] 3

> A &lt;- matrix(c(2,3,-2,1,2,2,4,6,-4),3,3)
> A

     [,1] [,2] [,3]
[1,]    2    1    4
[2,]    3    2    6
[3,]   -2    2   -4

> matA &lt;- qr(A)
> matA$rank

[1] 2

# note column 3 is 2 times column 1
Number of Rows & Columns
> X &lt;- matrix(c(3,2,4,3,2,-2,6,1),4,2)
> X

     [,1] [,2]
[1,]    3    2
[2,]    2   -2
[3,]    4    6
[4,]    3    1

> <span class="orange">dim(X)</span>

[1] 4 2

> r &lt;- nrow(X)
> r

[1] 4

> c &lt;- ncol(X)
> c

[1] 2
Computing Column & Row Sums
# note the uppercase S

> A &lt;- matrix(c(2,3,-2,1,2,2),3,2)
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> c &lt;- colSums(A)
> c

[1] 3 5

> r &lt;- rowSums(A)
> r

[1] 3 5 0

> a &lt;- sum(A)
> a

[1] 8
<span class="orange">Computing Column & Row Means</span>
# note the uppercase M

> cm &lt;- colMeans(A)
> cm

[1] 1.000000 1.666667

> rm &lt;- rowMeans(A)
> rm

[1] 1.5 2.5 0.0

> m &lt;- mean(A)
> m

[1] 1.333333
<span class="orange">Horizontal Concatenation</span>
> A

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2

> B &lt;- matrix(c(1,3,2,1,4,2),3,2)
> B

     [,1] [,2]
[1,]    1    1
[2,]    3    4
[3,]    2    2

> C &lt;- cbind(A,B)
> C

     [,1] [,2] [,3] [,4]
[1,]    2    1    1    1
[2,]    3    2    3    4
[3,]   -2    2    2    2
<span class="orange">Vertical Concatenation (Appending)</span>
> C &lt;- rbind(A,B)
> C

     [,1] [,2]
[1,]    2    1
[2,]    3    2
[3,]   -2    2
[4,]    1    1
[5,]    3    4
[6,]    2    2

<span class="orange">Matrix Operations in R</span>
A * B	Element-wise multiplication
A %*% B	Matrix multiplication
A %o% B	Outer product. AB'
crossprod(A,B)
crossprod(A)	A'B and A'A respectively.
t(A)	Transpose
diag(x)	Creates diagonal matrix with elements of x in the principal diagonal
diag(A)	Returns a vector containing the elements of the principal diagonal
diag(k)	If k is a scalar, this creates a k x k identity matrix. Go figure.
solve(A, b)	Returns vector x in the equation b = Ax (i.e., A-1b)
solve(A)	Inverse of A where A is a square matrix.
ginv(A)	Moore-Penrose Generalized Inverse of A.
ginv(A) requires loading the MASS package.
y&lt;-eigen(A)	y$val are the eigenvalues of A
y$vec are the eigenvectors of A
y&lt;-svd(A)	Single value decomposition of A.
y$d = vector containing the singular values of A
y$u = matrix with columns contain the left singular vectors of A
y$v = matrix with columns contain the right singular vectors of A
R &lt;- chol(A)	Choleski factorization of A. Returns the upper triangular factor, such that R'R = A.
y &lt;- qr(A)	QR decomposition of A.
y$qr has an upper triangle that contains the decomposition and a lower triangle that contains information on the Q decomposition.
y$rank is the rank of A.
y$qraux a vector which contains additional information on Q.
y$pivot contains information on the pivoting strategy used.
cbind(A,B,...)	Combine matrices(vectors) horizontally. Returns a matrix.
rbind(A,B,...)	Combine matrices(vectors) vertically. Returns a matrix.
rowMeans(A)	Returns vector of row means.
rowSums(A)	Returns vector of row sums.
colMeans(A)	Returns vector of column means.
colSums(A)	Returns vector of column sums.

<h2>UCLA stat</h2>
<a href="http://www.philender.com/courses/intro/" class="whitebut ">ucla.edu Introduction to Research Design and Statistics</a>
<a href="http://www.philender.com/courses/linearmodels/" class="whitebut ">Linear Statistical Models: Regression & Anova, Better Living Through Linear Models</a>
<a href="http://www.philender.com/courses/multivariate/" class="whitebut ">Multivariate Statistical Analysis</a>

<h2>R Linear Algebra</h2>
R is especially handy with linear algebra. 
Its built-in data types like vectors and matrices mesh well with built-in functions like eigenvalue and determinant solvers and dynamic indexing capabilities.

<h3>Vector Assignment</h3>
x &lt;- c(1, 2, 3, 4)
In most contexts, <code>&lt;-</code> can be switched with <code>=</code>.
The function <code>assign()</code> can also be used:
assign(&#x27;x&#x27;, c(1, 2, 3, 4))
Assignments can also be made in the other direction:
c(1, 2, 3, 4) -&gt; x

<h3>Vector Operations</h3>Vectors can also be used in a variety of ways.
The operation<code> y &lt;- c(x, 0, x)</code> would assign a vector <code>1, 2, 3, 4, 0, 1, 2, 3, 4 </code>to variable <code>y</code>.
Vectors can be freely multiplied and added by constants:
v &lt;- 2*x + y + 1
Note that this operation is valid even when <code>x</code> and <code>y</code> are different lengths. 
In this case, R will simply recycle x (sometimes fractionally) until it meets the length of y. 
Since y is 9 numbers long and x is 4 units long, x will be repeated 2.25 times to match the length of y.
The arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, and <code>^</code> can all be used. 
<code>log</code>, <code>exp</code>, <code>sin</code>, <code>cos</code>, <code>tan</code>, <code>sqrt</code>, and more can also be used. 
<code>max(x)</code> and <code>min(x)</code> represent the largest and smallest elements of a vector <code>x</code>, and <code>length(x)</code> is the number of elements in <code>x</code>. 
<code>sum(x)</code> gives the total of the elements in <code>x</code>, and <code>prod(x)</code> their product.
<code>mean(x)</code> calculates the sample mean, and <code>var(x)</code> returns the sample variance. 
<code>sort(x)</code> returns a vector of the same size as x with elements arranged in increasing order.

<h3>Generating Sequences</h3>R has many methods for generating sequences of numbers. 
<code>1:30</code> is the same as <code>c(1, 2, …, 29, 30)</code>. 
The colon as the highest priority in an expression, so<code>2*1:15</code> will return <code>c(2, 4, …, 28, 30)</code> instead of <code>c(2, 3, …, 14, 15)</code>.
30:1 may be used to generate the sequence backwards.
The <code>seq()</code> function can also be used to generate sequences. 
<code>seq(2,10) </code>returns the same vector as <code>2:10</code>. 
In <code>seq()</code>, one can also specify the length of the step in which to take: <code>seq(1,2,by=0.5)</code> returns <code>c(1, 1.5, 2)</code>.
A similar function is <code>rep()</code>, which replicates an object in various ways. 
For example, <code>rep(x, times=5)</code> will return five copies of <code>x</code> end-to-end.

<h3>Logical Vectors</h3>Logical values in R are TRUE, FALSE, and NA. 
Logical vectors are set by conditions. 
<code>val &lt;- x &gt; 13</code> sets <code>val</code> as a vector of the same length as <code>x</code> with values <code>TRUE</code> where the condition is met and <code>FALSE</code> where the condition is not.
The logical operators in r are <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, and <code>!=</code>, which mean less than, less than or equal to, greater than, greater than or equal to, equality, and inequality.

<h3>Missing Values</h3>The function <code>is.na(x)</code> returns a logical vector of the same size as <code>x</code> with <code>TRUE</code> if the corresponding element to <code>x</code> is <code>NA</code>.
<code>x == NA</code> is different from <code>is.na(x)</code> since <code>NA</code> is not a value but a marker for an unavailable quantity.
A second type of ‘missing value’ is that which is produced by numerical computation, such as <code>0/0</code>. 
In this case, <code>NaN</code> (Not a Number) values are treated as <code>NA</code> values; that is, <code>is.na(x)</code> will return <code>TRUE</code> for both <code>NA</code> and <code>NaN</code> values. 
<code>is.nan(x)</code> can be used only for identifying <code>NaN</code> values.

<h3>Indexing Vectors</h3>The first kind of indexing is through a logical vector. 
<code>y &lt;- x[!is.na(x)]</code> sets <code>y</code> to the values of <code>x</code> that are not equal to <code>NA</code> or <code>NaN</code>.
<code>(x+1)[(!is.na(x)) &amp; x&gt;0] -&gt; z</code> sets <code>z</code> to the values of <code>x+1</code> that are not <code>Na</code> or <code>NaN</code> and larger than 0.
A second method is with a vector of positive integral quantities. 
In this case, the values must be in the set <code>{1, 2, …, length(x)}</code>. 
The corresponding elements of the vector are selected and concatenated in that order to form a result. 
It is important to remember that unlike in other languages, the first index in R is 1 and not 0.
<code>x[1:10]</code> returns the first 10 elements of <code>x</code>, assuming <code>length(x)</code> is not less than 10. 
<code>c(‘x’, ‘y’)[rep(c(1,2,2,1), times=4)]</code> produces a character vector of length 16, where <code>‘x’, ‘y’, ‘y’, ‘x’</code> is repeated four times.
A vector of negative integral numbers specifies the values to be excluded rather than included. 
<code>y &lt;- x[-(1:5)]</code> sets <code>y</code> to all but the first five values of <code>x</code>.
Lastly, a vector of character strings can be used when an object has a names attribute to identify its components. 
With fruit <code>&lt;- c(1, 2, 3, 4)</code>, one can set the names of each index of the vector fruit with <code>names(fruit) &lt;- c(‘mango’, ‘apple’, ‘banana’, ‘orange’)</code>. 
Then, one can call the elements by name with <code>lunch &lt;- fruit[c(‘apple’, ‘orange’)]</code>.
The advantage of this is that alphanumeric names can sometimes be easier to remember than indices.
Note that an indexed expression can also appear on the receiving end of an assignment, in which the assignment is only performed on those elements of a vector. 
For example, <code>x[is.na(x)] &lt;- 0</code> replaces all <code>NA</code> and <code>NaN</code> values in vector <code>x</code> with the value <code>0</code>.
Another example: <code>y[y&lt;0] &lt;- -y[y&lt;0]</code> has the same effect as <code>y &lt;- abs(y)</code>. 
The code simply replaces all the values that are less than 0 with the negative of that value.
<h3>Arrays &amp; Matrices</h3>
<h3>Arrays</h3>An array is a subscripted collection of data entries, not necessarily numeric.
A dimension vector is a vector of non-negative integers. 
If the length is <em>k</em> then the array is <em>k</em>-dimensional. 
The dimensions are indexed from one up to the values given in the dimension vector.
A vector can be used by R as an array as its <code>dim </code>attribute. 
If <code>z</code> were a vector of 1500 elements, the assignment <code>dim(z) &lt;- c(100, 5, 3)</code> would mean <code>z</code> is now treated as a 100 by 5 by 3 array.

<h3>Array Indexing</h3>Individual elements of an array can be referenced by giving the name of the array followed by the subscripts in square brackets, separated by columns.
A 3 by 4 by 6 vector <code>a</code> could have its first value called via <code>a[1, 1, 1]</code> and its last value called via <code>a[3, 4, 6]</code>.
<code>a[,,]</code> represents the entire array; hence, <code>a[1,1,]</code> takes the first row of the first 2-dimensional cross-section in <code>a</code>.

<h3>Indexing Matrices</h3>The following code generates a 4 by 5 array: <code>x &lt;- array(1:20, dim = c(4,5))</code>.
Arrays are specified by a vector of values and the dimensions of the matrix. 
Values are calculated top-down first, left-right second.
<code>array(1:4, dim = c(2,2))</code> would return
1 3
2 4
and not
1 2
3 4
Negative indices are not allowed in index matrices. 
<code>NA</code> and zero values are allowed.

<h3>Outer Product of 2 Arrays</h3>An important operation on arrays is the outer product. 
If <code>a</code> and <code>b</code> are two numeric arrays, their outer product is an array whose dimension vector is obtained by concatenating the two dimension vectors and whose data vector is achieved by forming all possible products of elements of the data vector of <code>a</code> with those of <code>b</code>. 
The outer product is calculated with the operator <code>%o%</code>:
<code>ab &lt;- a %o% b</code>
Another way to achieve this is
<code>ab &lt;- outer(a, b, ‘*’)</code>
In fact, any function can be applied on two arrays using the outer() function. 
Suppose we define a function <code>f &lt;- function(x, y) cos(y)/(1+x²)</code>. 
The function could be applied to two vectors <code>x</code> and <code>y</code> via <code>z &lt;- outer(x, y, f)</code>.

<h3>Demonstration: All Possible Determinants of 2x2 Single-Digit Matrices</h3>Consider the determinants of 2 by 2 matrices [a, b; c, d] where each entry is a non-negative integer from 0 to 9. 
The problem is to find the determinants of all possible matrices in this form and represent the frequency of which the value occurs with a high density plot.
Rephrased, find the probability distribution of the determinant if each digit is chosen independently and uniformly at random.
One clever way of doing this uses the outer(0 function twice.
d &lt;- outer(0:9,0:9)
fr &lt;- table(outer(d, d, ‘-’))
plot(fr, xlab = ‘Determinant’, ylab = ‘Frequency’)
The first line assigns d to this matrix:
The second line uses the outer() function again to calculate all possible determinants, and the last line plots it.
Generalized Transpose of an Array</h3>The function <code>aperm(a, perm)</code> can be used to permute an array a. 
The argument perm must be the permutation of the integers {1,…, <em>k</em>} where <em>k</em> is the number of subscripts in <em>a</em>. 
The result of the function is an array of the same size as a but with the old dimension given by <code>perm[j]</code> becoming the new <code>j-th</code> dimension.
An easy way to think about it is a generalization of transposition for matrices. 
If <code>A</code> is a matrix, then <code>B</code> is simply the transpose of <code>A</code>:
B &lt;- aperm(A, c(2, 1))
In these special cases the function <code>t()</code> performs a transposition.

<h3>Matrix Multiplication</h3>The operator %*% is used for matrix multiplication. 
If <code>A</code> and <code>B</code> are square matrices of the same size, <code>A*B</code> is the element-wise product of the two matrices. 
<code>A %*% B</code> is the dot product (matrix product).
If x is a vector, then <code>x %*% A %*% x</code> is a quadratic form.
<code>crossprod()</code> performs cross-products; thus, <code>crossprod(X, y)</code> is the same as the operation <code>t(X) %*% y</code>, but more efficient.
<code>diag(v)</code>, where <code>v</code> is a vector, gives a diagonal matrix with elements of the vector as the diagonal entries. 
<code>diag(M)</code>, where <code>m</code> is a matrix, gives the vector of the main diagonal entries of <code>M</code> (the same convention as in Matlab). 
<code>diag(k)</code>, where <code>k</code> is a single numeric value, returns a <code>k</code> by <code>k</code> identity matrix.

<h3>Linear Equations and Inversion</h3>Solving linear equations is the inverse of matrix multiplication. 
When
b &lt;- A %*% x
with only <code>A</code> and <code>b</code> given, vector <code>x</code> is the solution of the linear equation system. 
This can be solved quickly in R with
solve(A, b)

<h3>Eigenvalues and Eigenvectors</h3>The function <code>eigen(Sm)</code> calculates the eigenvalues and eigenvectors of a symmetric matrix Sm. 
The result is a list, with the first element named values and the second named vectors. 
<code>ev &lt;- eigen(Sm)</code> assigns this list to <code>ev</code>.
<code>ev$val</code> is the vector of eigenvalues of <code>Sm</code> and <code>ev$vec</code> the matrix of corresponding eigenvectors.
For large matrices, it is better to avoid computing the eigenvectors if they are not needed by using the expression
evals &lt;- eigen(Sm, only.values = TRUE)$values

<h3>Singular Value Decomposition and Determinants</h3>The function <code>svd(m)</code> takes an arbitrary matrix argument, <code>m</code>, and calculates the singular value decomposition of <code>m</code>. 
This consists of a matrix of orthonormal columns <code>U</code> with the same column space as <code>m</code>, a second matrix of orthonormal columns <code>V</code> whose column space is the row space of <code>m</code> and a diagonal matrix of positive entries <code>D</code> such that
m = U %*% D %*% t(V)
<code>det(m)</code> can be used to calculate the determinant of a square matrix <code>m</code>.

<h3>Least Squares Fitting &amp; QR Decomposition</h3>The function <code>lsfit()</code> returns a list giving results of a least squares fitting procedure. 
An assignment like
ans &lt;- lsfit(X, y)
gives results of a least squares fit where y is the vector of observations and X is the design matrix.
<code>ls.diag()</code> can be used for regression diagnostics.
A closely related function is qr().
b &lt;- qr.coef(Xplus,y)
fit &lt;- qr.fitted(Xplus,y)
res &lt;- qr.resid(Xplus,y)
These compute the orthogonal projection of <code>y</code> onto the range of <code>X</code> in <code>fit</code>, the projection onto the orthogonal complement in <code>res</code> and the coefficient vector for the projection in <code>b</code>.

<h3>Forming Partitioned <code>Matrices</code></h3>Matrices can be built up from other vectors and matrices with the functions <code>cbind()</code> and <code>rbind()</code>.
<code>cbind()</code> forms matrices by binding matrices horizontally (column-wise), and <code>rbind()</code> binds matrices vertically (row-wise).
In the assignment <code>X &lt;- cbind(arg_1, arg_2, arg_3, …)</code> the arguments to <code>cbind()</code> must be either vectors of any length, or columns with the same column size (the same number of rows).
<code>rbind()</code> performs a corresponding operation for rows.

<h2>unable to install rvest package</h2>
Error: package or namespace load failed for ‘xml2’ in loadNamespace

install.packages("tidyverse")  # might need other dependencies installed in Rstudio

<h2>tcl/tk package to create messageBox</h2>
library(tcltk)

tkmessageBox(
 title = "Hello Friends Title",
 message = "Hello, world! message",
 icon = "warning",
 detail="This is the message details",
 type = "ok")

tk_messageBox(
message, 
icon = c("error", "info", "question", "warning")
type = c("ok", "okcancel", "yesno", "yesnocancel", "retrycancel", "abortretryignore"),
default = "", ...)

must be -default, -detail, -icon, -message, -parent, -title, or -type.

Arguments
title
character
string specifying title for dialog window

message
character
string specifying message displayed inside the alert extra arguments

A list of other arguments is shown here:
default character string specifying the default button of the dialog

detail
character string specifying a secondary message, usually displayed in a smaller font under the main message

parent
object of the class tkwin representing the window of the application for which this dialog is being posted.

type
character
string specifying predefined set of buttons to be displayed (askquestion only).

Possible values are:
 abortretryignore
  displays three buttons whose symbolic names are ‘abort’, ‘retry’ and ‘ignore’

 ok
  displays one button whose symbolic name is ‘ok’

 okcancel
  displays two buttons whose symbolic names are ‘ok’ and ‘cancel’

 retrycancel
  displays two buttons whose symbolic names are ‘retry’ and ‘cancel’

 yesno
  displays two buttons whose symbolic names are ‘yes’ and ‘no’

 yesnocancel displays three buttons whose symbolic names are ‘yes’, ‘no’ and ‘cancel’

<h2>Machine Learning in R for beginners</h2>
This small tutorial is meant to introduce you to the basics of machine learning in R: it will show you how to use R to work with KNN.

<h3>Introducing: Machine Learning in R</h3>

Machine learning is a branch in computer science that studies the design of algorithms that can learn. 
Typical machine learning tasks are concept learning, function learning or “predictive modeling”, clustering and finding predictive patterns. 
These tasks are learned through available data that were observed through experiences or instructions, for example. 
Machine learning hopes that including the experience into its tasks will eventually improve the learning. 
The ultimate goal is to improve the learning in such a way that it becomes automatic, so that humans like ourselves don’t need to interfere any more.

This small tutorial is meant to introduce you to the basics of machine learning in R: more specifically, it will show you how to use R to work with the well-known machine learning algorithm called “KNN” or <em>k</em>-nearest neighbors.


<h3>Using R For <em>k</em>-Nearest Neighbors (KNN)</h3>

The KNN or <em>k</em>-nearest neighbors algorithm is one of the simplest machine learning algorithms and is an example of instance-based learning, where new data are classified based on stored, labeled instances.

More specifically, the distance between the stored data and the new instance is calculated by means of some kind of a similarity measure. 
This similarity measure is typically expressed by a distance measure such as the Euclidean distance, cosine similarity or the Manhattan distance.

In other words, the similarity to the data that was already in the system is calculated for any new data point that you input into the system.

Then, you use this similarity value to perform predictive modeling. 
Predictive modeling is either classification, assigning a label or a class to the new instance, or regression, assigning a value to the new instance. 
Whether you classify or assign a value to the new instance depends of course on your how you compose your model with KNN.

The <em>k</em>-nearest neighbor algorithm adds to this basic algorithm that after the distance of the new point to all stored data points has been calculated, the distance values are sorted and the <em>k</em>-nearest neighbors are determined. 
The labels of these neighbors are gathered and a majority vote or weighted vote is used for classification or regression purposes.

In other words, the higher the score for a certain data point that was already stored, the more likely that the new instance will receive the same classification as that of the neighbor. 
In the case of regression, the value that will be assigned to the new data point is the mean of its <em>k</em> nearest neighbors.

<h3>Step One. Get Your Data</h3>

Machine learning usually starts from observed data. 
You can take your own data set or browse through other sources to find one.

<h3>Built-in Datasets of R</h3>

This tutorial uses the Iris data set, which is very well-known in the area of machine learning. 
This dataset is built into R, so you can take a look at this dataset by typing the following into your console:

iris

# Print first lines
head(iris)

<h3>Step Two. Know Your Data</h3>

Just looking or reading about your data is certainly not enough to get started!

You need to get your hands dirty, explore and visualize your data set and even gather some more domain knowledge if you feel the data is way over your head.

Probably you’ll already have the domain knowledge that you need, but just as a reminder, all flowers contain a sepal and a petal. 
The sepal encloses the petals and is typically green and leaf-like, while the petals are typically colored leaves. 
For the iris flowers, this is just a little bit different, as you can see in the following picture:

<img src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/iris-machinelearning.png" />

<h3>Initial Overview Of The Data Set</h3>

First, you can already try to get an idea of your data by making some graphs, such as histograms or boxplots. 
In this case, however, scatter plots can give you a great idea of what you’re dealing with: it can be interesting to see how much one variable is affected by another.

In other words, you want to see if there is any correlation between two variables.

You can make scatterplots with the <a href="http://www.rdocumentation.org/packages/ggvis"><code>ggvis</code> package</a>, for example.

<strong>Note</strong> that you first need to load the <code>ggvis</code> package:

<code># Load in `ggvis`
library(ggvis)

# Iris scatter plot
iris %&gt;% ggvis(~Sepal.Length, ~Sepal.Width, fill = ~Species) %&gt;% layer_points()</code>

<img alt="correlation iris" height="499" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_768312428.png" width="817" />

You see that there is a high correlation between the sepal length and the sepal width of the Setosa iris flowers, while the correlation is somewhat less high for the Virginica and Versicolor flowers: the data points are more spread out over the graph and don’t form a cluster like you can see in the case of the Setosa flowers.

The scatter plot that maps the petal length and the petal width tells a similar story:

<code>iris %&gt;% ggvis(~Petal.Length, ~Petal.Width, fill = ~Species) %&gt;% layer_points()</code>

<img alt="scatterplot iris" height="500" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/plot_675020181.png" width="817" />

You see that this graph indicates a positive correlation between the petal length and the petal width for all different species that are included into the Iris data set. 
Of course, you probably need to test this hypothesis a bit further if you want to be really sure of this:

# Overall correlation `Petal.Length` and `Petal.Width`
cor(iris$Petal.Length, iris$Petal.Width)

# Return values of `iris` levels 
x=levels(iris$Species)

# Print Setosa correlation matrix
print(x[1])
cor(iris[iris$Species==x[1],1:4])

# Print Versicolor correlation matrix
print(x[2])
cor(iris[iris$Species==x[2],1:4])

# Print Virginica correlation matrix
print(x[3])
cor(iris[iris$Species==x[3],1:4])

You see that when you combined all three species, the correlation was a bit stronger than it is when you look at the different species separately: the overall correlation is 0.96, while for Versicolor this is 0.79. 
Setosa and Virginica, on the other hand, have correlations of petal length and width at 0.31 and 0.32 when you round up the numbers.

<b>Tip</b>: are you curious about ggvis, graphs or histograms in particular? Check out our <a href="https://www.datacamp.com/community/tutorials/make-histogram-basic-r/">histogram tutorial</a> and/or <a href="https://www.datacamp.com/courses/ggvis-data-visualization-r-tutorial/">ggvis course</a>.

After a general visualized overview of the data, you can also view the data set by entering

# Return all `iris` data
iris

# Return first 5 lines of `iris`
head(iris)

# Return structure of `iris`
str(iris)

However, as you will see from the result of this command, this really isn’t the best way to inspect your data set thoroughly: the data set takes up a lot of space in the console, which will impede you from forming a clear idea about your data. 
It is therefore a better idea to inspect the data set by executing <code>head(iris)</code> or <code>str(iris)</code>.

Note that the last command will help you to clearly distinguish the data type <code>num</code> and the three levels of the <code>Species</code> attribute, which is a factor. 
This is very convenient, since many R machine learning classifiers require that the target feature is coded as a factor.

Remember that factor variables represent categorical variables in R. 
They can thus take on a limited number of different values.

A quick look at the <code>Species</code> attribute through tells you that the division of the species of flowers is 50-50-50. 
On the other hand, if you want to check the percentual division of the <code>Species</code> attribute, you can ask for a table of proportions:

# Division of `Species`
table(iris$Species) 

# Percentual division of `Species`
round(prop.table(table(iris$Species)) * 100, digits = 1)

<strong>Note</strong> that the <code>round</code> argument rounds the values of the first argument, <code>prop.table(table(iris$Species))*100</code> to the specified number of digits, which is one digit after the decimal point. 
You can easily adjust this by changing the value of the <code>digits</code> argument.

<h3>Profound Understanding Of Your Data</h3>

Let’s not remain on this high-level overview of the data! R gives you the opportunity to go more in-depth with the <code>summary()</code> function. 
This will give you the minimum value, first quantile, median, mean, third quantile and maximum value of the data set Iris for numeric data types. 
For the class variable, the count of factors will be returned:

# Summary overview of `iris`
summary(....) 

# Refined summary overview
summary(....[c("Petal.Width", "Sepal.Width")])

As you can see, the <code>c()</code> function is added to the original command: the columns <code>petal width</code> and <code>sepal width</code> are concatenated and a summary is then asked of just these two columns of the Iris data set.

<h3>Step Three. Where To Go Now?</h3>

After you have acquired a good understanding of your data, you have to decide on the use cases that would be relevant for your data set. 
In other words, you think about what your data set might teach you or what you think you can learn from your data. 
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.
From there on, you can think about what kind of algorithms you would be able to apply to your data set in order to get the results that you think you can obtain.

<strong>Tip</strong>: keep in mind that the more familiar you are with your data, the easier it will be to assess the use cases for your specific data set. 
The same also holds for finding the appropriate machine algorithm.

For this tutorial, the Iris data set will be used for classification, which is an example of predictive modeling. 
The last attribute of the data set, <code>Species</code>, will be the target variable or the variable that you want to predict in this example.

<strong>Note</strong> that you can also take one of the numerical classes as the target variable if you want to use KNN to do regression.

<h3>Step Four. Prepare Your Workspace</h3>

Many of the algorithms used in machine learning are not incorporated by default into R. 
You will most probably need to download the packages that you want to use when you want to get started with machine learning.

<b>Tip</b>: got an idea of which learning algorithm you may use, but not of which package you want or need? You can find a pretty complete overview of all the packages that are used in R <a href="http://www.rdocumentation.org/domains/MachineLearning">right here</a>.

To illustrate the KNN algorithm, this tutorial works with the package <code>class</code>:

library(.....)

If you don’t have this package yet, you can quickly and easily do so by typing the following line of code:

<code>install.packages(&quot;&lt;package name&gt;&quot;)</code>

<strong>Remember</strong> the nerd tip: if you’re not sure if you have this package, you can run the following command to find out!

<code>any(grepl(&quot;&lt;name of your package&gt;&quot;, installed.packages()))</code>

<h3>Step Five. Prepare Your Data</h3>

After exploring your data and preparing your workspace, you can finally focus back on the task ahead: making a machine learning model. 
However, before you can do this, it’s important to also prepare your data. 
The following section will outline two ways in which you can do this: by normalizing your data (if necessary) and by splitting your data in training and testing sets.

<h3>Normalization</h3>

As a part of your data preparation, you might need to normalize your data so that its consistent. 
For this introductory tutorial, just remember that normalization makes it easier for the KNN algorithm to learn. 
There are two types of normalization:

<ul><li>example normalization is the adjustment of each example individually, while</li><li>feature normalization indicates that you adjust each feature in the same way across all examples.</li>
</ul>

So when do you need to normalize your dataset?

In short: when you suspect that the data is not consistent.

You can easily see this when you go through the results of the <code>summary()</code> function. 
Look at the minimum and maximum values of all the (numerical) attributes. 
If you see that one attribute has a wide range of values, you will need to normalize your dataset, because this means that the distance will be dominated by this feature.

For example, if your dataset has just two attributes, X and Y, and X has values that range from 1 to 1000, while Y has values that only go from 1 to 100, then Y’s influence on the distance function will usually be overpowered by X’s influence.

When you normalize, you actually adjust the range of all features, so that distances between variables with larger ranges will not be over-emphasised.

<b>Tip</b>: go back to the result of <code>summary(iris)</code> and try to figure out if normalization is necessary.

The Iris data set doesn’t need to be normalized: the <code>Sepal.Length</code> attribute has values that go from 4.3 to 7.9 and <code>Sepal.Width</code> contains values from 2 to 4.4, while <code>Petal.Length</code>’s values range from 1 to 6.9 and <code>Petal.Width</code> goes from 0.1 to 2.5. 
All values of all attributes are contained within the range of 0.1 and 7.9, which you can consider acceptable.

Nevertheless, it’s still a good idea to study normalization and its effect, especially if you’re new to machine learning. 
You can perform feature normalization, for example, by first making your own <code>normalize()</code> function.

You can then use this argument in another command, where you put the results of the normalization in a data frame through <code>as.data.frame()</code> after the function <code>lapply()</code> returns a list of the same length as the data set that you give in. 
Each element of that list is the result of the application of the <code>normalize</code> argument to the data set that served as input:

<code>YourNormalizedDataSet &lt;- as.data.frame(lapply(YourDataSet, normalize))</code>

Test this in the DataCamp Light chunk below!

# Build your own `normalize()` function
normalize &lt;- function(x) {
num &lt;- x - min(x)
denom &lt;- max(x) - min(x)
return (num/denom)
}

# Normalize the `iris` data
iris_norm &lt;- .............(......(iris[1:4], normalize))

# Summarize `iris_norm`
summary(.........)

For the Iris dataset, you would have applied the <code>normalize</code> argument on the four numerical attributes of the Iris data set (<code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code>, <code>Petal.Width</code>) and put the results in a data frame.

<strong>Tip</strong>: to more thoroughly illustrate the effect of normalization on the data set, compare the following result to the summary of the Iris data set that was given in step two.

<h3>Training And Test Sets</h3>

In order to assess your model’s performance later, you will need to divide the data set into two parts: a training set and a test set.

The first is used to train the system, while the second is used to evaluate the learned or trained system. 
In practice, the division of your data set into a test and a training sets is disjoint: the most common splitting choice is to take 2/3 of your original data set as the training set, while the 1/3 that remains will compose the test set.

One last look on the data set teaches you that if you performed the division of both sets on the data set as is, you would get a training class with all species of “Setosa” and “Versicolor”, but none of “Virginica”. 
The model would therefore classify all unknown instances as either “Setosa” or “Versicolor”, as it would not be aware of the presence of a third species of flowers in the data.

In short, you would get incorrect predictions for the test set.

You thus need to make sure that all three classes of species are present in the training model. 
What’s more, the amount of instances of all three species needs to be more or less <em>equal</em> so that you do not favour one or the other class in your predictions.

To make your training and test sets, you first set a seed. 
This is a number of R’s random number generator. 
The major advantage of setting a seed is that you can get the same sequence of random numbers whenever you supply the same seed in the random number generator.

<code>set.seed(1234)</code>

Then, you want to make sure that your Iris data set is shuffled and that you have an equal amount of each species in your training and test sets.

You use the <code>sample()</code> function to take a sample with a size that is set as the number of rows of the Iris data set, or 150. 
You sample with replacement: you choose from a vector of 2 elements and assign either 1 or 2 to the 150 rows of the Iris data set. 
The assignment of the elements is subject to probability weights of 0.67 and 0.33.

<code>ind &lt;- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))</code>

<strong>Note</strong> that the <code>replace</code> argument is set to <code>TRUE</code>: this means that you assign a 1 or a 2 to a certain row and then reset the vector of 2 to its original state. 
This means that, for the next rows in your data set, you can either assign a 1 or a 2, each time again. 
The probability of choosing a 1 or a 2 should not be proportional to the weights amongst the remaining items, so you specify probability weights. 
Note also that, even though you don’t see it in the DataCamp Light chunk, the seed has still been set to <code>1234</code>.

<strong>Remember</strong> that you want your training set to be 2/3 of your original data set: that is why you assign “1” with a probability of 0.67 and the “2”s with a probability of 0.33 to the 150 sample rows.

You can then use the sample that is stored in the variable <code>ind</code> to define your training and test sets:

# Compose training set
iris.training &lt;- ....[ind==1, 1:4]

# Inspect training set
head(................)

# Compose test set
iris.test &lt;- ....[ind==2, 1:4]

# Inspect test set
head(...........)

<strong>Note</strong> that, in addition to the 2/3 and 1/3 proportions specified above, you don’t take into account all attributes to form the training and test sets. 
Specifically, you only take <code>Sepal.Length</code>, <code>Sepal.Width</code>, <code>Petal.Length</code> and <code>Petal.Width</code>. 
This is because you actually want to predict the fifth attribute, <code>Species</code>: it is your target variable. 
However, you do want to include it into the KNN algorithm, otherwise there will never be any prediction for it.

You therefore need to store the class labels in factor vectors and divide them over the training and test sets:
# Compose `iris` training labels
iris.trainLabels &lt;- iris[ind==1,5]

# Inspect result
print(iris.trainLabels)

# Compose `iris` test labels
iris.testLabels &lt;- iris[ind==2, 5]

# Inspect result
print(iris.testLabels)

<h3>Step Six. The Actual KNN Model</h3>

<h3>Building Your Classifier</h3>

After all these preparation steps, you have made sure that all your known (training) data is stored. 
No actual model or learning was performed up until this moment. 
Now, you want to find the <em>k</em> nearest neighbors of your training set.

An easy way to do these two steps is by using the <code>knn()</code> function, which uses the Euclidian distance measure in order to find the <em>k</em>-nearest neighbours to your new, unknown instance. 
Here, the <em>k</em> parameter is one that you set yourself.

As mentioned before, new instances are classified by looking at the majority vote or weighted vote. 
In case of classification, the data point with the highest score wins the battle and the unknown instance receives the label of that winning data point. 
If there is an equal amount of winners, the classification happens randomly.

<strong>Note</strong>: the <em>k</em> parameter is often an odd number to avoid ties in the voting scores.

# Build the model
iris_pred &lt;- ...(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)

# Inspect `iris_pred`
.........

You store into <code>iris_pred</code> the <code>knn()</code> function that takes as arguments the training set, the test set, the train labels and the amount of neighbours you want to find with this algorithm. 
The result of this function is a factor vector with the predicted classes for each row of the test data.

<strong>Note</strong> that you don’t want to insert the test labels: these will be used to see if your model is good at predicting the actual classes of your instances!

You see that when you inspect the the result, <code>iris_pred</code>, you’ll get back the factor vector with the predicted classes for each row of the test data.

<h3>Step Seven. Evaluation of Your Model</h3>

An essential next step in machine learning is the evaluation of your model’s performance. 
In other words, you want to analyze the degree of correctness of the model’s predictions.

For a more abstract view, you can just compare the results of <code>iris_pred</code> to the test labels that you had defined earlier:

# Put `iris.testLabels` in a data frame
irisTestLabels &lt;- data.frame(................)

# Merge `iris_pred` and `iris.testLabels` 
merge &lt;- data.frame(........., ...............)

# Specify column names for `merge`
names(.....) &lt;- c("Predicted Species", "Observed Species")

# Inspect `merge` 
merge


You see that the model makes reasonably accurate predictions, with the exception of one wrong classification in row 29, where “Versicolor” was predicted while the test label is “Virginica”.

This is already some indication of your model’s performance, but you might want to go even deeper into your analysis. 
For this purpose, you can import the package <code>gmodels</code>:

<code>install.packages(&quot;package name&quot;)</code>

However, if you have already installed this package, you can simply enter

<code>library(gmodels)</code>

Then you can make a cross tabulation or a contingency table. 
This type of table is often used to understand the relationship between two variables. 
In this case, you want to understand how the classes of your test data, stored in <code>iris.testLabels</code> relate to your model that is stored in <code>iris_pred</code>:

<code>CrossTable(x = iris.testLabels, y = iris_pred, prop.chisq=FALSE)</code>

<img alt="Crosstable iris knn" height="698" src="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Machine+Learning+R/Screenshot-2015-03-24-20.05.32.png" width="926" />

<strong>Note</strong> that the last argument <code>prop.chisq</code> indicates whether or not the chi-square contribution of each cell is included. 
The chi-square statistic is the sum of the contributions from each of the individual cells and is used to decide whether the difference between the observed and the expected values is significant.

From this table, you can derive the number of correct and incorrect predictions: one instance from the testing set was labeled <code>Versicolor</code> by the model, while it was actually a flower of species <code>Virginica</code>. 
You can see this in the first row of the “Virginica” species in the <code>iris.testLabels</code> column. 
In all other cases, correct predictions were made. 
You can conclude that the model’s performance is good enough and that you don’t need to improve the model!

<a href="https://www.datacamp.com/courses/" target="_blank"><img alt="Learn Python for Data Science With DataCamp" src="http://community.datacamp.com.s3.amazonaws.com/community/production/ckeditor_assets/pictures/293/content_blog_banner.png" /></a>

<h3>Machine Learning in R with <code>caret</code></h3>

In the previous sections, you have gotten started with supervised learning in R via the KNN algorithm. 
As you might not have seen above, machine learning in R can get really complex, as there are various algorithms with various syntax, different parameters, etc. 
Maybe you’ll agree with me when I say that remembering the different package names for each algorithm can get quite difficult or that applying the syntax for each specific algorithm is just too much.

That’s where the <code>caret</code> package can come in handy: it’s short for “Classification and Regression Training” and offers everything you need to know to solve supervised machine learning problems: it provides a uniform interface to a ton of machine learning algorithms. 
If you’re a bit familiar with Python machine learning, you might see similarities with <code>scikit-learn</code>!

In the following, you’ll go through the steps as they have been outlined above, but this time, you’ll make use of <code>caret</code> to classify your data. 
Note that you have already done a lot of work if you’ve followed the steps as they were outlined above: you already have a hold on your data, you have explored it, prepared your workspace, etc. 
Now it’s time to preprocess your data with <code>caret</code>!

As you have done before, you can study the effect of the normalization, but you’ll see this later on in the tutorial.

You already know what’s next! Let’s split up the data in a training and test set. 
In this case, though, you handle things a little bit differently: you split up the data based on the labels that you find in <code>iris$Species</code>. 
Also, the ratio is in this case set at 75-25 for the training and test sets.

# Create index to split based on labels  
index &lt;- createDataPartition(iris$Species, p=0.75, list=FALSE)

# Subset training set with index
iris.training &lt;- iris[.......,]

# Subset test set with index
iris.test &lt;- iris[-.........,]

You’re all set to go and train models now! But, as you might remember, <code>caret</code> is an extremely large project that includes a lot of algorithms. 
If you’re in doubt on what algorithms are included in the project, you can get a list of all of them. 
Pull up the list by running <code>names(getModelInfo())</code>, just like the code chunk below demonstrates. 
Next, pick an algorithm and train a model with the <code>train()</code> function:

# Overview of algos supported by caret
names(getModelInfo())

# Train a model
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn')

Note that making other models is extremely simple when you have gotten this far; You just have to change the <code>method</code> argument, just like in this example:

<code>model_cart &lt;- train(iris.training[, 1:4], iris.training[, 5], method='rpart2')</code>

Now that you have trained your model, it’s time to predict the labels of the test set that you have just made and evaluate how the model has done on your data:

# Predict the labels of the test set
predictions&lt;-predict(object=model_knn,iris.test[,1:4])

# Evaluate the predictions
table(predictions)

# Confusion matrix 
confusionMatrix(predictions,iris.test[,5])

Additionally, you can try to perform the same test as before, to examine the effect of preprocessing, such as scaling and centering, on your model. 
Run the following code chunk:


# Train the model with preprocessing
model_knn &lt;- train(iris.training[, 1:4], iris.training[, 5], method='knn', preProcess=c("center", "scale"))

# Predict values
predictions&lt;-predict.train(object=model_knn,iris.test[,1:4], type="raw")

# Confusion matrix
confusionMatrix(predictions,iris.test[,5])

<h3>Move On To Big Data</h3>

Congratulations! You’ve made it through this tutorial!

This tutorial was primarily concerned with performing basic machine learning algorithm KNN with the help of R. 
The Iris data set that was used was small and overviewable; Not only did you see how you can perform all of the steps by yourself, but you’ve also seen how you can easily make use of a uniform interface, such as the one that <code>caret</code> offers, to spark your machine learning.

But you can do so much more!

If you have experimented enough with the basics presented in this tutorial and other machine learning algorithms, you might want to find it interesting to go further into R and data analysis.

<h2>Machine Learning in R with Example</h2>
As a kid, you might have come across a picture of a fish and you would have been told by your kindergarten teachers or parents that this is a fish and it has some specific features associated with it like it has fins, gills, a pair of eyes, a tail and so on. 

Now, whenever your brain comes across an image with those set of features, it automatically registers it as a fish because your brain has <em>learned </em>that it is a fish.

That's how our brain functions but what about a machine? If the same image is fed to a machine, how will the machine identify it to be a fish?

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture6.png">

This is where M<em>achine Learning</em> comes in. 

We'll keep on feeding images of a fish to a computer with the tag "fish" until the <em>machine learns all the features associated</em> with a<em> fish. 
</em>

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture7.png">

Once the machine learns all the features associated with a fish, we will feed it new data to determine how much has it learned.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture8.png">

In other words,<em> Raw Data/Training Data </em>is given to the machine, so that it <em>learns </em>all the features associated with the <em>Training Data. 
</em>Once, the learning is done, it is given <em>New Data/Test Data </em>to determine how well the machine has learned.

Let us move ahead in this Machine Learning with R blog and understand about types of Machine Learning.
<h3><strong>Types of Machine Learning</strong></h3><h3><strong>Supervised Learning: </strong></h3>

Supervised Learning algorithm learns from a known data-set(Training Data) which has labels to make predictions.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture9.png">

Regression and Classification are some examples of Supervised Learning.
<h4><strong>#Classification:</strong></h4>
Classification determines to which set of categories does a new observation belongs i.e. 

a classification algorithm learns all the features and labels of the training data and when new data is given to it, it has to assign labels to the new observations depending on what it has learned from the training data.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture10.png">

For this example, if the first observation is given the label "Man" then it is rightly classified but if it is given the label "Woman", the classification is wrong. 

Similarly for the second observation, if the label given is "Woman", it is rightly classified, else the classification is wrong.
<h4><strong>#Regression: </strong></h4>
Regression is a supervised learning algorithm which helps in determining how does one variable influence another variable.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture11.png">

Over here, "living_area" is the independent variable and "price" is the dependent variable i.e. 

we are determining how does "price" vary with respect to "living_area".
<li><h3><strong>Unsupervised Learning:</strong></h3></li>

Unsupervised learning algorithm draws inferences from data which does not have labels.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture12.png">

<em>Clustering</em> is an example of unsupervised learning. 

"K-means", "Hierarchical", "Fuzzy C-Means" are some examples of clustering algorithms.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture13.png">

In this example, the set of observations is divided into two clusters. 

Clustering is done on the basis of similarity between the observations. 

There is a high intra-cluster similarity and low inter-cluster similarity i.e. 

there is a very high similarity between all the buses but low similarity between the buses and cars.
<li><h3><strong>Reinforcement Learning:</strong></h3></li>

Reinforcement Learning is a type of machine learning algorithm where the <em>machine/agent</em> in an <em>environment </em>learns ideal behavior in order to maximize its performance. 
Simple reward feedback is required for the agent to learn its behavior, this is known as the <em>reinforcement signal</em>.
<h3><strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/pacman.png">
</strong></h3>
Let's take <em>pacman</em> for example. 

As long as pacman keeps eating food, it earns points but when it crashes against a monster it loses it's life. 

Thus pacman learns that it needs to eat more food and avoid monsters so as to improve it's performance.
<h3><strong>Implementing Machine Learning with R:</strong></h3><h3><strong>Linear Regression:</strong></h3>
We'll be working with the diamonds data-set to implement linear regression algorithm:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond.png">

Description of the data-set:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/diamond_description.png">

Prior to building any model on the data, we are supposed to split the data into "train" and "test" sets. 

The model will be built on the "train" set and it's accuracy will be checked on the "test" set.

We need to load the "caTools" package to split the data into two sets.

<code>library(caTools)</code>
"caTools" package provides a function "sample.split()" which helps in splitting the data.

<code>sample.split(diamonds$price,SplitRatio = 0.65)-&gt;split_index</code>
65% of the observations from price column have been assigned the "true" label and the rest 35% have been assigned "false" label.

<code>subset(diamonds,split_index==T)-&gt;train
subset(diamonds,split_index==F)-&gt;test</code>
All the observations which have "true" label have been stored in the "<em>train" object</em> and those observations having "false" label have been assigned to the "test" set.

Now that the splitting is done and we have our "train" and "test" sets, it's time to build the linear regression model on the training set.

We'll be using the "lm()" function to build the linear regression model on the "train" data. 

We are determining the <em>price</em> of the diamonds with respect to all other variables of the data-set. 

The built model is stored in the object "mod_regress".

<code>lm(price~.,data = train)-&gt;mod_regress</code><em> </em>
Now, that we have built the model, we need to make predictions on the "test" set. 

"predict()" function is used to get predictions. 

It takes two arguments: the <em>built model</em> and the <em>test set. 
</em>The predicted results are stored in the "result_regress" object.

<code>predict(mod_regress,test)-&gt;result_regress</code>
Let's bind the actual price values from the "test" data-set and the predicted values into a single data-set using the "cbind()" function. 

The new data-frame is stored in "Final_Data"

<code>cbind(Actual=test$price,Predicted=result_regress)-&gt;Final_Data</code> 
<code>as.data.frame(Final_Data)-&gt;Final_Data</code>
A glance at the "Final_Data" which comprises of actual values and predicted values:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data.png">

Let's find the error by subtracting the predicted values from the actual values and add this error as a new column to the "Final_Data":

<code>(Final_Data$Actual- Final_Data$Predicted)-&gt;error</code>
<code>cbind(Final_Data,error)-&gt;Final_Data</code>
A glance at the "Final_Data" which also comprises of the error in prediction:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Now, we'll go ahead and calculate "<em>Root Mean Square Error" </em>which gives an aggregate error for all the predictions

<code>rmse1&lt;-sqrt(mean(Final_Data$error^2))</code> 
<code>rmse1</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse1.png">

Going ahead, let's build another model, so that we can compare the accuracy of both these models and determine which is a better one.

We'll build a new linear regression model on the "train" set but this time, we'll be dropping the &#8216;x' and &#8216;y' columns from the independent variables i.e. 

the "price" of the diamonds is determined by all the columns except &#8216;x' and &#8216;y'.

The model built is stored in "mod_regress2": 

<code>lm(price~.-y-z,data = train)-&gt;mod_regress2</code>
The predicted results are stored in "result_regress2"<br> 

<code>predict(mod_regress2,test)-&gt;result_regress2</code>
Actual and Predicted values are combined and stored in "Final_Data2":

<code>cbind(Actual=test$price,Predicted=result_regress2)-&gt;Final_Data2</code> 
<code>as.data.frame(Final_Data2)-&gt;Final_Data2</code>
Let's also add the error in prediction to "Final_Data2"

<code>(Final_Data2$Actual- Final_Data2$Predicted)-&gt;error2</code>
<code>cbind(Final_Data2,error2)-&gt;Final_Data2</code>
A glance at "Final_Data2":

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/final-data2.png">

Finding Root Mean Square Error to get the aggregate error:

<code>rmse2&lt;-sqrt(mean(Final_Data2$error^2))</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rmse2.png">

We see that "rmse2" is marginally less than "rmse1" and hence the second model is marginally better than the first model.
<h3><strong>Classification:</strong></h3>
We'll be working with the "car_purchase" data-set to implement <em>recursive partitioning </em>which is a classification algorithm.

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/car-1.png">

Let's split the data into "train" and "test" sets using "sample.split()" function from "caTools" package.

<code>library(caTools)</code>
65% of the observations from &#8216;Purchased' column will be assigned "TRUE" labels and the rest will be assigned "FALSE" labels.

<code>sample.split(car_purchase$Purchased,SplitRatio = 0.65)-&gt;split_values</code>
All those observations which have "TRUE" label will be stored into &#8216;train' data and those observations having "FALSE" label will be assigned to &#8216;test' data.

<code>subset(car_purchase,split_values==T)-&gt;train_data</code>
<code>subset(car_purchase,split_values==F)-&gt;test_data</code>
Time to build the Recursive Partitioning algorithm:

We'll start off by loading the &#8216;rpart' package:

<code>library(rpart)</code>
"Purchased" column will be the dependent variable and all other columns are the independent variables i.e. 

we are determining whether the person has bought the car or not with respect to all other columns. 

The model is built on the "train_data" and the result is stored in "mod1".

<code>rpart(Purchased~.,data = train_data)-&gt;mod1</code>
Let's plot the result:

<code>plot(mod1,margin = 0.1)</code> <code>text(mod1,pretty = T,cex=0.8)</code>
<strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/rplot.png">
</strong>

Now, let's go ahead and predict the results on "test_data". 

We are giving the built rpart model "mod1" as the first argument, the test set "test_data" as the second argument and prediction type as "class" for the third argument. 

The result is stored in &#8216;result1' object. 

<code>predict(mod1,test_data,type = "class")-&gt;result1</code>
Let's evaluate the accuracy of the model using "confusionMatrix()" function from caret package.

<code>library(caret)</code> <code></code><code>confusionMatrix(table(test_data$Purchased,result1))</code>
<strong><img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/confusion.png">
</strong>

The confusion matrix tells us that out of the 90 observations where the person did not buy the car, 79 observations have been rightly classified as "No" and 11 have been wrongly classified as "YES". 

Similarly, out of the 50 observations where the person actually bought the car, 47 have been rightly classified as "YES" and 3 have been wrongly classified as "NO".

We can find the accuracy of the model by dividing the correct predictions with total predictions i.e. 

(79+47)/(79+47+11+3).
<h3><strong>K-Means Clustering:</strong></h3>
We'll work with "iris" data-set to implement k-means clustering:

<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/iris.png">

Let's remove the "Species" column and create a new data-set which comprises only the first four columns from the &#8216;iris' data-set.

<code>iris[1:4]-&gt;iris_k</code>
Let us take the number of clusters to be 3. 

"Kmeans()" function takes the input data and the number of clusters in which the data is to be clustered. 

The syntax is : kmeans( data, k) where k is the number of cluster centers.

<code>kmeans(iris_k,3)-&gt;k1</code>
Analyzing the clustering:

<code>str(k1)</code>
<img src="https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2018/03/Picture15.png">

The str() function gives the structure of the kmeans which includes various parameters like withinss, betweenss, etc, analyzing which you can find out the performance of kmeans.

betweenss : Between sum of squares i.e. 

Intracluster similarity

withinss : Within sum of square i.e. 

Intercluster similarity

totwithinss : Sum of all the withinss of all the clusters i.e.Total intra-cluster similarity

A good clustering will have a lower value of "tot.withinss" and higher value of "betweenss" which depends on the number of clusters &lsquo;k&rsquo; chosen initially. 

The time is ripe to become an expert in Machine Learning to take advantage of new opportunities that come your way. 

This brings us to the end of this "<em><strong>Machine Learning with R</strong></em>" blog. 

I hope this blog was informative fruitful.

<h2>put the whole if else statement in one line</h2>
if (TRUE) 1 else 3

You have to use {} for allows the if statement to have more than one line. 

<h2>quit R</h2>
if logout, 
quit("yes")

<h2>Run R scripts from the Windows command line (CMD)</h2>
This use library RDCOMClient to send summary information to colleges with Microsoft Outlook

There are two ways to do that.

use batch file looks like this.
"C:\Program Files\R\R-3.4.3\bin\Rscript.exe" C:\Users\myusername\Documents\R\Send_Outlook_Email.R

The second one looks like this.
"C:\Program Files\R\R-3.4.3\bin\R.exe" CMD BATCH C:\Users\myusername\Documents\R\Send_Outlook_Email.R

Remember to use quotation marks when there is space in the file path.

<h2>Locate the position of patterns in a string</h2>
library("stringr")
fruit &lt;- "apple banana pear pineapple"
str_locate(fruit, "ea")
str_locate_all(fruit, "ea")

str_locate, an integer matrix.
First column gives start postion of match, and second column gives end position

str_locate_all a list of integer matrices.

<h3>trim string</h3>
x &lt;- "This string is moderately long"
strtrim(x, 20)

<h2>find the max length of string in array</h2>
setwd("D:/Dropbox/MyDocs/R misc Jobs/Learning Exercise/QuizData")
WordTableFIle <&lt;- readLines("EnglishWordList.txt", encoding="UTF-8", warn = FALSE)
WordTable <&lt;- matrix(unlist(strsplit(WordTableFIle, split = "\\t")), ncol=1, byrow=TRUE) # make it one column
maxNum = max(nchar(WordTable))
maxNum
WordTable[which(nchar(WordTable) == maxNum)]

<h2>Convert Character Vector between Encodings</h2>
iconv(x, from, to, sub=NA)

‘i’ stands for ‘internationalization’.

Usage
iconv(x, from, to, sub=NA)

Arguments
x	A character vector.
from	A character string describing the current encoding.

to	A character string describing the target encoding.

sub	character string. If not NA it is used to replace any non-convertible bytes in the input.
(This would normally be a single character, but can be more.
If "byte", the indication is "&lt;xx>" with the hex code of the byte.

Details
The names of encodings and which ones are available (and indeed, if any are) is platform-dependent. 
On systems that support R's iconv you can use "" for the encoding of the current locale, as well as "latin1" and "UTF-8".


iconvlist()
On many platforms iconvlist provides an alphabetical list of the supported encodings. 
On others, the information is on the man page for iconv(5) or elsewhere in the man pages (and beware that the system command iconv may not support the same set of encodings as the C functions R calls). 
Unfortunately, the names are rarely common across platforms.

Elements of x which cannot be converted (perhaps because they are invalid or because they cannot be represented in the target encoding) will be returned as NA unless sub is specified.

Some versions of iconv will allow transliteration by appending //TRANSLIT to the to encoding: see the examples.

Value
A character vector of the same length and the same attributes as x.

Note
Not all platforms support these functions. 

See Also
localeToCharset, file.

Examples
## Not run: 
iconvlist()

## convert from Latin-2 to UTF-8: two of the glibc iconv variants.
iconv(x, "ISO_8859-2", "UTF-8")
iconv(x, "LATIN2", "UTF-8")

## Both x below are in latin1 and will only display correctly in a
## latin1 locale.
(x &lt;- "fa\xE7ile")
charToRaw(xx &lt;- iconv(x, "latin1", "UTF-8"))
## in a UTF-8 locale, print(xx)

iconv(x, "latin1", "ASCII")          #   NA
iconv(x, "latin1", "ASCII", "?")     # "fa?ile"
iconv(x, "latin1", "ASCII", "")      # "faile"
iconv(x, "latin1", "ASCII", "byte")  # "faile"

# Extracts from R help files
(x &lt;- c("Ekstr\xf8m", "J\xf6reskog", "bi\xdfchen Z\xfcrcher"))
iconv(x, "latin1", "ASCII//TRANSLIT")
iconv(x, "latin1", "ASCII", sub="byte")
## End(Not run)

<h2>encoding error with read_html</h2>
<a href="https://stackoverflow.com/users/4350463/hodgenovice" class="whitebut ">hodgenovice R expert</a>
<a href="https://stackoverflow.com/questions/45290452/encoding-error-with-read-html" class="whitebut ">encoding error with read_html</a>
url = "http://www.chinanews.com/scroll-news/news1.html"
thekeyword = "新闻"
read_page = lapply(unique(iconvlist()), function(encoding_attempt) {

  # Optional print statement to show progress since this takes time
  # print(match(encoding_attempt, iconvlist()) / length(iconvlist()))

  read_attempt = tryCatch(expr=read_html(url, encoding=encoding_attempt),
                           error=function(condition) NA,
                           warning=function(condition) message(condition))
  read_attempt = as.character(read_attempt)
  fisrtLine = grep(thekeyword, read_attempt)
  if(length(fisrtLine)>0){
    cat(encoding_attempt, "\n")
    cat(read_attempt[fisrtLine], "\n")
  }
})

names(read_page) = unique(iconvlist())


# 2. See which encodings correctly display some complex characters
read_phrase = lapply(read_page, function(encoded_page) 
  if(!is.na(encoded_page))
    html_text(html_nodes(encoded_page, ".content_right")))

# ended up with encodings which could be sensible...
encoding_shortlist = names(read_phrase)[read_phrase == "新闻"]
encoding_shortlist

sink("testResult.txt")
print(read_page)
sink()

retriveFile &lt;- as.character(read_html(url, warn=F, encoding = "UTF-16"))
fisrtLine = grep(thekeyword, retriveFile)
fisrtLine

<h2>Object-oriented programming (OOP)</h2>
a programming paradigm based on the concept of "objects", 

which can contain data, in the form of fields (often known as attributes or properties), 

and code, in the form of procedures (often known as methods). 

A feature of objects is an object's procedures that can access and often modify the data fields of the object with which they are associated (objects have a notion of "this" or "self"). 

In OOP, computer programs are designed by making them out of objects that interact with one another.

OOP languages are diverse, but the most popular ones are class-based, meaning that objects are instances of classes, which also determine their types.

Many of the most widely used programming languages (such as C++, Java, Python, etc.) are multi-paradigm and they support object-oriented programming to a greater or lesser degree, typically in combination with imperative, procedural programming. 

Significant object-oriented languages include Java, C++, C#, Python, R, PHP, JavaScript, Ruby, Perl, Object Pascal, Objective-C, Dart, Swift, Scala, Kotlin, Common Lisp, MATLAB, and Smalltalk.


<h2>Reference classes</h2>
<!-- http://www.inside-r.org/r-doc/methods/ReferenceClasses -->
R has three object oriented (OO) systems: [[S3]], [[S4]] and Reference Classes (where the latter were for a while referred to as [[R5]], yet their official name is Reference Classes). 
This page describes this new reference-based class system.

Reference Classes (or refclasses) are new in R 2.12. 
They fill a long standing need for mutable objects that had previously been filled by non-core packages like R.oo, proto and mutatr. 
While the core functionality is solid, reference classes are still under active development and some details will change. 
The most up-to-date documentation for Reference Classes can always be found in ?ReferenceClasses.

There are two main differences between reference classes and S3 and S4:

Refclass objects use message-passing OO
Refclass objects are mutable: the usual R copy on modify semantics do not apply

These properties makes this object system behave much more like Java and C#. 
Surprisingly, the implementation of reference classes is almost entirely in R code - they are a combination of S4 methods and environments. 
This is a testament to the flexibility of S4.

Particularly suited for: simulations where you’re modelling complex state, GUIs.

Note that when using reference based classes we want to minimise side effects, and use them only where mutable state is absolutely required. 
The majority of functions should still be “functional”, and side effect free. 
This makes code easier to reason about (because you don’t need to worry about methods changing things in surprising ways), and easier for other R programmers to understand.

Limitations: can’t use enclosing environment - because that’s used for the object.

<h3>Classes and instances</h3>
Creating a new reference based class is straightforward: you use setRefClass. 
Unlike setClass from S4, you want to keep the results of that function around, because that’s what you use to create new objects of that type:

# Or keep reference to class around.
Person = setRefClass("Person")
Person$new()

A reference class has three main components, given by three arguments to setRefClass:

contains, the classes which the class inherits from. 
These should be other reference class objects:

setRefClass("Polygon")
setRefClass("Regular")

# Specify parent classes
setRefClass("Triangle", contains = "Polygon")
setRefClass("EquilateralTriangle", 
  contains = c("Triangle", "Regular"))

fields are the equivalent of slots in S4. 
They can be specified as a vector of field names, or a named list of field types:

setRefClass("Polygon", fields = c("sides"))
setRefClass("Polygon", fields = list(sides = "numeric"))

The most important property of refclass objects is that they are mutable, or equivalently they have reference semantics:

    Polygon = setRefClass("Polygon", fields = c("sides"))
    square = Polygon$new(sides = 4)
    
    triangle = square
    triangle$sides = 3
    
    square$sides        

methods are functions that operate within the context of the object and can modify its fields. 
These can also be added after object creation, as described below.

setRefClass("Dist")
setRefClass("DistUniform", c("a", "b"), "Dist", methods = list(
  mean = function() {
    (a + b) / 2
  }
))

You can also add methods after creation:

# Instead of creating a class all at once:
Person = setRefClass("Person", methods = list(
  say_hello = function() message("Hi!")
))

# You can build it up piece-by-piece
Person = setRefClass("Person")
Person$methods(say_hello = function() message("Hi!"))

It’s not currently possible to modify fields because adding fields would invalidate existing objects that didn’t have those fields.

The object returned by setRefClass (or retrieved later by getRefClass) is called a generator object. 
It has methods:

new for creating new objects of that class. 
The new method takes named arguments specifying initial values for the fields

methods for modifying existing or adding new methods

help for getting help about methods

fields to get a list of fields defined for class

lock locks the named fields so that their value can only be set once

accessors a convenience method that automatically sets up accessors of the form getXXX and setXXX.

<h3>Methods</h3>
Refclass methods are associated with objects, not with functions, and are called using the special syntax obj$method(arg1, arg2, ...). 
(You might recall we’ve seen this construction before when we called functions stored in a named list). 
Methods are also special because they can modify fields. 
This is different

We’ve also seen this construct before, when we used closures to create mutable state. 
Reference classes work in a similar manner but give us some extra functionality:

inheritance
a way of documenting methods
a way of specifying fields and their types

Modify fields with &lt;=. 
Will call accessor functions if defined.

Special fields: .self (Don’t use fields with names starting with . 
as these may be used for special purposes in future versions.)

initialize

<h3>Common methods</h3>
Because all refclass classes inherit from the same superclass, envRefClass, they a have common set of methods:

obj$callSuper:

obj$copy: creates a copy of the current object. 
This is necessary because Reference Classes classes don’t behave like most R objects, which are copied on assignment or modification.

obj$field: named access to fields. 
Equivalent to slots for S4. 
obj$field("xxx") the same as obj$xxx. 
obj$field("xxx", 5) the same as obj$xxx = 5

obj$import(x) coerces into this object, and obj$export(Class) coerces a copy of obj into that class. 
These should be super classes.

obj$initFields

<h1>R S3 Class</h1>
In this article, you will learn to work with S3 classes (one of the three class systems in R programming).

S3 class is the most popular and prevalent class in R programming language.

Most of the classes that come predefined in R are of this type. 
The fact that it is simple and easy to implement is the reason behind this.

<h2>How to define S3 class and create S3 objects?</h2>
S3 class has no formal, predefined definition.

Basically, a list with its class attribute set to some class name, is an S3 object. 
The components of the list become the member variables of the object.

Following is a simple example of how an S3 object of class student can be created.

> # create a list with required components
> s = list(name = "John", age = 21, GPA = 3.5)
> # name the class appropriately
> class(s) = "student"
> # That's it! we now have an object of class "student"
> s
$name
[1] "John"
$age
[1] 21
$GPA
[1] 3.5
attr(,"class")
[1] "student"

This might look awkward for programmers coming from C++, Python etc. 
where there are formal class definitions and objects have properly defined attributes and methods.

In R S3 system, it's pretty ad hoc. 
You can convert an object's class according to your will with objects of the same class looking completely different. 
It's all up to you.

<h2>How to use constructors to create objects?</h2>
It is a good practice to use a function with the same name as class (not a necessity) to create objects.

This will bring some uniformity in the creation of objects and make them look similar.

We can also add some integrity check on the member attributes. 
Here is an example. 
Note that in this example we use the attr() function to set the class attribute of the object.

# a constructor function for the "student" class
student = function(n,a,g) {
# we can add our own integrity checks
if(g>4 || g&lt;0)  stop("GPA must be between 0 and 4")
value = list(name = n, age = a, GPA = g)
# class can be set using class() or attr() function
attr(value, "class") = "student"
value
}

Here is a sample run where we create objects using this constructor.

> s = student("Paul", 26, 3.7)
> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"
> class(s)
[1] "student"
> s = student("Paul", 26, 5)
Error in student("Paul", 26, 5) : GPA must be between 0 and 4
> # these integrity check only work while creating the object using constructor
> s = student("Paul", 26, 2.5)
> # it's up to us to maintain it or not
> s$GPA = 5

<h2>Methods and Generic Functions</h2>
In the above example, when we simply write the name of the object, its internals get printed.

In interactive mode, writing the name alone will print it using the print() function.

> s
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7
attr(,"class")
[1] "student"

Furthermore, we can use print() with vectors, matrix, data frames, factors etc. 
and they get printed differently according to the class they belong to.

How does print() know how to print these variety of dissimilar looking object?

The answer is, print() is a generic function. 
Actually, it has a collection of a number of methods. 
You can check all these methods with methods(print).

> methods(print)
[1] print.acf*                                   
[2] print.anova*
...
[181] print.xngettext*                             
[182] print.xtabs*                                 
Non-visible functions are asterisked

We can see methods like print.data.frame and print.factor in the above list.

When we call print() on a data frame, it is dispatched to print.data.frame().

If we had done the same with a factor, the call would dispatch to print.factor(). 
Here, we can observe that the method names are in the form generic_name.class_name(). 
This is how R is able to figure out which method to call depending on the class.

Printing our object of class "student" looks for a method of the form print.student(), but there is no method of this form.

So, which method did our object of class "student" call?

It called print.default(). 
This is the fallback method which is called if no other match is found. 
Generic functions have a default method.

There are plenty of generic functions like print(). 
You can list them all with methods(class="default").

> methods(class="default")
[1] add1.default*            aggregate.default*      
[3] AIC.default*             all.equal.default
...

<h2>How to write your own method?</h2>
Now let us implement a method print.student() ourself.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now this method will be called whenever we print() an object of class "student".

In S3 system, methods do not belong to object or class, they belong to generic functions. 
This will work as long as the class of the object is set.

> # our above implemented method is called
> s
Paul 
26 years old
GPA: 3.7 
> # removing the class attribute will restore as previous
> unclass(s)
$name
[1] "Paul"
$age
[1] 26
$GPA
[1] 3.7

<h2>Writing Your Own Generic Function</h2>
It is possible to make our own generic function like print() or plot(). 
Let us first look at how these functions are implemented.

> print
function (x, ...) 
UseMethod("print")
&lt;bytecode: 0x0674e230>
&lt;environment: namespace:base>
> plot
function (x, y, ...) 
UseMethod("plot")
&lt;bytecode: 0x04fe6574>
&lt;environment: namespace:graphics>

We can see that they have a single call to UseMethod() with the name of the generic function passed to it. 
This is the dispatcher function which will handle all the background details. 
It is this simple to implement a generic function.

For the sake of example, we make a new generic function called grade.

grade = function(obj) {
UseMethod("grade")
}

A generic function is useless without any method. 
Let us implement the default method.

grade.default = function(obj) {
cat("This is a generic function\n")
}

Now let us make method for our class "student".

grade.student = function(obj) {
cat("Your grade is", obj$GPA, "\n")
}

A sample run.

> grade(s)
Your grade is 3.7

In this way, we implemented a generic function called grade and later a method for our class.

<h1>R Inheritance</h1>
In this article, you’ll learn everything about inheritance in R. 
More specifically, how to create inheritance in S3, S4 and Reference classes, and use them efficiently in your program.

Inheritance is one of the key features of object-oriented programming which allows us to define a new class out of existing classes.

This is to say, we can derive new classes from existing base classes and adding new features. 
We don’t have to write from scratch. 
Hence, inheritance provides reusability of code.

Inheritance forms a hierarchy of class just like a family tree. 
Important thing to note is that the attributes define for a base class will automatically be present in the derived class.

Moreover, the methods for the base class will work for the derived.

<img class="alignnone size-full wp-image-79" src="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg" alt="Inheritance in R Programming" srcset="https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance.jpg 740w, https://cdn.datamentor.io/wp-content/uploads/2017/11/r-inheritance-300x162.jpg 300w" sizes="(max-width: 740px) 100vw, 740px">

Below, we discuss how inheritance is carried out for the three different class systems in R programming language.

<h2>Inheritance in S3 Class</h2>
S3 classes do not have any fixed definition. 
Hence attributes of S3 objects can be arbitrary.

Derived class, however, inherit the methods defined for base class. 
Let us suppose we have a function that creates new objects of class student as follows.

student = function(n,a,g) {
value = list(name=n, age=a, GPA=g)
attr(value, "class") = "student"
value
}

Furthermore, we have a method defined for generic function print() as follows.

print.student = function(obj) {
cat(obj$name, "\n")
cat(obj$age, "years old\n")
cat("GPA:", obj$GPA, "\n")
}

Now we want to create an object of class InternationalStudent which inherits from student.

This is be done by assigning a character vector of class names like class(obj) = c(child, parent).

> # create a list
> s = list(name="John", age=21, GPA=3.5, country="France")
> # make it of the class InternationalStudent which is derived from the class student
> class(s) = c("InternationalStudent","student")
> # print it out
> s
John 
21 years old
GPA: 3.5

We can see above that, since we haven’t defined any method of the form print.InternationalStudent(), the method print.student() got called. 
This method of class student was inherited.

Now let us define print.InternationalStudent().

print.InternationalStudent = function(obj) {
cat(obj$name, "is from", obj$country, "\n")
}

This will overwrite the method defined for class student as shown below.

> s
John is from France

We can check for inheritance with functions like inherits() or is().

> inherits(s,"student")
[1] TRUE
> is(s,"student")
[1] TRUE

<h2>Inheritance in S4 Class</h2>
Since S4 classes have proper definition, derived classes will inherit both attributes and methods of the parent class.

Let us define a class student with a method for the generic function show().

# define a class called student
setClass("student",
slots=list(name="character", age="numeric", GPA="numeric")
)
# define class method for the show() generic function
setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Inheritance is done during the derived class definition with the argument contains as shown below.

# inherit from student
setClass("InternationalStudent",
slots=list(country="character"),
contains="student"
)

Here we have added an attribute country, rest will be inherited from the parent.

> s = new("InternationalStudent",name="John", age=21, GPA=3.5, country="France")
> show(s)
John 
21 years old
GPA: 3.5

We see that method define for class student got called when we did show(s).

We can define methods for the derived class which will overwrite methods of the base class, like in the case of S3 systems.

<h2>Inheritance in Reference Class</h2>
Inheritance in reference class is very much similar to that of the S4 class. 
We define in the contains argument, from which base class to derive from.

Here is an example of student reference class with two methods inc_age() and dec_age().

student = setRefClass("student",
fields=list(name="character", age="numeric", GPA="numeric"),
methods=list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

Now we will inherit from this class. 
We also overwrite dec_age() method to add an integrity check to make sure age is never negative.

InternationalStudent = setRefClass("InternationalStudent",
fields=list(country="character"),
contains="student",
methods=list(
dec_age = function(x) {
if((age - x)&lt;0)  stop("Age cannot be negative")
age &lt;= age - x
}
)
)

Let us put it to test.

> s = InternationalStudent(name="John", age=21, GPA=3.5, country="France")
> s$dec_age(5)
> s$age
[1] 16
> s$dec_age(20)
Error in s$dec_age(20) : Age cannot be negative
> s$age
[1] 16

In this way, we are able to inherit from the parent class.


<h2>R Reference Class</h2>
In this article, you will learn to work with reference classes in R programming which is one of the three class systems (other two are S3 and S4).

Reference class in R programming is similar to the object oriented programming we are used to seeing in common languages like C++, Java, Python etc.

Unlike <a title="R S3 Class" href="/r-programming/S3-class">S3</a> and <a title="R S4 class" href="/r-programming/S4-class">S4 classes</a>, methods belong to class rather than generic functions. 
Reference class are internally implemented as S4 classes with an environment added to it.

<h3>How to define a reference class?</h3>
Defining reference class is similar to defining a S4 class. 
Instead of setClass() we use the setRefClass() function.

> setRefClass("student")

Member variables of a class, if defined, need to be included in the class definition. 
Member variables of reference class are called fields (analogous to slots in S4 classes).

Following is an example to define a class called student with 3 fields, name, age and GPA.

> setRefClass("student", fields = list(name = "character", age = "numeric", GPA = "numeric"))

<h3>How to create a reference objects?</h3>
The function setRefClass() returns a generator function which is used to create objects of that class.

> student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"))
> # now student() is our generator function which can be used to create new objects
> s = student(name = "John", age = 21, GPA = 3.5)
> s
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>How to access and modify fields?</h3>
Fields of the object can be accessed using the $ operator.

> s$name
[1] "John"
> s$age
[1] 21
> s$GPA
[1] 3.5

Similarly, it is modified by reassignment.

> s$name = "Paul"
> s
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h4>Warning Note</h4>

In R programming, objects are copied when assigned to new variable or passed to a function (pass by value). 
For example.

> # create list a and assign to b
> a = list("x" = 1, "y" = 2)
> b = a
> # modify b
> b$y = 3
> # a remains unaffected
> a
$x
[1] 1
$y
[1] 2
> # only b is modified
> b
$x
[1] 1
$y
[1] 3

But this is not the case with reference objects. 
Only a single copy exist and all variables reference to the same copy. 
Hence the name, reference.

> # create reference object a and assign to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a
> # modify b
> b$name = "Paul"
> # a and b both are modified
> a
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

This can cause some unwanted change in values and be the source of strange bugs. 
We need to keep this in mind while working with reference objects. 
To make a copy, we can use the copy() method made availabe to us.

> # create reference object a and assign a’s copy to b
> a = student(name = "John", age = 21, GPA = 3.5)
> b = a$copy()
> # modify b
> b$name = "Paul"
> # a remains unaffected
> a
Reference class object of class "student"
Field "name":
[1] "John"
Field "age":
[1] 21
Field "GPA":
[1] 3.5
> # only b is modified
> b
Reference class object of class "student"
Field "name":
[1] "Paul"
Field "age":
[1] 21
Field "GPA":
[1] 3.5

<h3>Reference Methods</h3>
Methods are defined for a reference class and do not belong to generic functions as in S3 and S4 classes.

All reference class have some methods predefined because they all are inherited from the superclass envRefClass.

> student
Generator for class "student":
Class fields:
Name:       name       age       GPA
Class: character   numeric   numeric
Class Methods:  
"callSuper", "copy", "export", "field", "getClass", "getRefClass", 
"import", "initFields", "show", "trace", "untrace", "usingMethods"
Reference Superclasses:  
"envRefClass"

We can see class methods like copy(), field() and show() in the above list. 
We can create our own methods for the class.

This can be done during the class definition by passing a list of function definitions to methods argument of setRefClass().

student = setRefClass("student",
fields = list(name = "character", age = "numeric", GPA = "numeric"),
methods = list(
inc_age = function(x) {
age &lt;= age + x
},
dec_age = function(x) {
age &lt;= age - x
}
)
)

In the above section of our code, we defined two methods called inc_age() and dec_age(). 
These two method modify the field age.

Note that we have to use the non-local assignment operator &lt;= since age isn’t in the method’s local environment. 
This is important.

Using the simple assignment operator = would have created a local variable called age, which is not what we want. 
R will issue a warning in such case.

Here is a sample run where we use the above defined methods.

> s = student(name = "John", age = 21, GPA = 3.5)
> s$inc_age(5)
> s$age
[1] 26
> s$dec_age(10)
> s$age
[1] 16

<h2>R S4 Class</h2>
In this article, you’ll learn everything about S4 classes in R; how to define them, create them, access their slots, and use them efficiently in your program.

Unlike <a title="R S3 class" href="/r-programming/S3-class">S3 classes</a> and <a title="R object" href="/r-programming/object-class-introduction">objects</a> which lacks formal definition, we look at S4 class which is stricter in the sense that it has a formal definition and a uniform way to create objects.

This adds safety to our code and prevents us from accidentally making naive mistakes.

<h3>How to define S4 Class?</h3>
S4 class is defined using the setClass() function.

In R terminology, member variables are called slots. 
While defining a class, we need to set the name and the slots (along with class of the slot) it is going to have.

<h4>Example 1: Definition of S4 class</h4>
setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))

In the above example, we defined a new class called student along with three slots it’s going to have name, age and GPA.

There are other optional arguments of setClass() which you can explore in the help section with ?setClass.

<h3>How to create S4 objects?</h3>
S4 objects are created using the new() function.

<h4>Example 2: Creation of S4 object</h4>
> # create an object using new()
> # provide the class name and value for slots
> s = new("student",name="John", age=21, GPA=3.5)
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

We can check if an object is an S4 object through the function isS4().

> isS4(s)
[1] TRUE

The function setClass() returns a generator function.

This generator function (usually having same name as the class) can be used to create new objects. 
It acts as a constructor.

> student = setClass("student", slots=list(name="character", age="numeric", GPA="numeric"))
> student
class generator function for class “student” from package ‘.GlobalEnv’
function (...) 
new("student", ...)

Now we can use this constructor function to create new objects.

Note above that our constructor in turn uses the new() function to create objects. 
It is just a wrap around.

<h4>Example 3: Creation of S4 objects using generator function</h4>
> student(name="John", age=21, GPA=3.5)
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.5

<h3>How to access and modify slot?</h3>
Just as components of a <a title="R list" href="/r-programming/list">list</a> are accessed using $, slot of an object are accessed using @.

<h4>Accessing slot</h4>
> s@name
[1] "John"
> s@GPA
[1] 3.5
> s@age
[1] 21

<h4>Modifying slot directly</h4>
A slot can be modified through reassignment.

> # modify GPA
> s@GPA = 3.7
> s
An object of class "student"
Slot "name":
[1] "John"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h4>Modifying slots using slot() function</h4>
Similarly, slots can be access or modified using the slot() function.

> slot(s,"name")
[1] "John"
> slot(s,"name") = "Paul"
> s
An object of class "student"
Slot "name":
[1] "Paul"
Slot "age":
[1] 21
Slot "GPA":
[1] 3.7

<h3>Methods and Generic Functions</h3>
As in the case of S3 class, methods for S4 class also belong to generic functions rather than the class itself. 
Working with S4 generics is pretty much similar to S3 generics.

You can list all the S4 generic functions and methods available, using the function showMethods().

<h4>Example 4: List all generic functions</h4>
> showMethods()
Function: - (package base)
Function: != (package base)
...
Function: trigamma (package base)
Function: trunc (package base)

Writing the name of the object in interactive mode prints it. 
This is done using the S4 generic function show().

You can see this function in the above list. 
This function is the S4 analogy of the S3 print() function.

<h4>Example 5: Check if a function is a generic function</h4>
> isS4(print)
[1] FALSE
> isS4(show)
[1] TRUE

We can list all the methods of show generic function using showMethods(show).

<h4>Example 6: List all methods of a generic function</h4>
> showMethods(show)
Function: show (package methods)
object="ANY"
object="classGeneratorFunction"
...
object="standardGeneric"
(inherited from: object="genericFunction")
object="traceable"

<h3>How to write your own method?</h3>
We can write our own method using setMethod() helper function.

For example, we can implement our class method for the show() generic as follows.

setMethod("show",
"student",
function(object) {
cat(object@name, "\n")
cat(object@age, "years old\n")
cat("GPA:", object@GPA, "\n")
}
)

Now, if we write out the name of the object in interactive mode as before, the above code is executed.

> s = new("student",name="John", age=21, GPA=3.5)
> s    # this is same as show(s)
John 
21 years old
GPA: 3.5

In this way we can write our own S4 class methods for generic functions.

<h2>Write text to a file</h2>
fileConn<-file("output.txt")
writeLines(c("Hello","World"), fileConn)
close(fileConn)

sink("outfile.txt")
cat("hello")
cat("world")
sink()

cat("Hello",file="outfile.txt",sep="\n")
cat("World",file="outfile.txt",append=TRUE)
cat("hello","world",file="output.txt",sep="\n",append=TRUE)
file.show("outfile.txt")

txt <- "Hallo\nWorld"
writeLines(txt, "outfile.txt")

library(tidyverse)
c('Hello', 'World') %>% write_lines( "output.txt")

writeLines() with sink()
sink("tempsink", type="output")
writeLines("Hello\nWorld")
sink()
file.show("tempsink", delete.file=TRUE)

text = c("Hello", "World")
write.table(text, file = "output.txt", col.names = F, row.names = F, quote = F)

<h2>Play birthday music</h2>
<a href="https://stackoverflow.com/questions/31782580/how-can-i-play-birthday-music-using-r" class="whitebut ">play-birthday-music</a>
library("dplyr")
library("audio")
notes = c(A = 0, B = 2, C = 3, D = 5, E = 7, F = 8, G = 10)
pitch = "D D E D G F# D D E D A G D D D5 B G F# E C5 C5 B G A G"
duration = c( rep( c(0.75, 0.25, 1, 1, 1, 2), 2),
              0.75, 0.25, 1, 1, 1, 1, 1, 0.75, 0.25, 1, 1, 1, 2)
bday = data_frame(pitch = strsplit(pitch, " ")[[1]], duration = duration)

bday =
  bday %>%
  mutate(octave = substring(pitch, nchar(pitch)) %>%
          {suppressWarnings(as.numeric(.))} %>%
           ifelse(is.na(.), 4, .),
           note = notes[substr(pitch, 1, 1)],
           note = note + grepl("#", pitch) -
           grepl("b", pitch) + octave * 12 + 12 * (note < 3),
           freq = 2 ^ ((note - 60) / 12) * 440)

tempo = 120
sample_rate = 44100 # this is MP3 sample freq, the freq resolution is 40Hz
                    # the A4 freq is 440Hz
                    # the A#4 freq is 466Hz
                    # the Ab4 freq is 415Hz

# A3 (220) A4 (440) A5 (880) C6 (1046.502)

make_sine = function(freq, duration) {
  wave = sin( seq(0, duration /tempo *60, 1 /sample_rate) *freq *2 *pi)
  fade = seq(0, 1, 50 /sample_rate)
  wave * c(fade, rep(1, length(wave) - 2 * length(fade)), rev(fade))
}

bday_wave = mapply(make_sine, bday$freq, bday$duration) %>% do.call("c", .)

play(bday_wave)

There's a few points to note.
The default octave for the notes is octave 4, where A4 is at 440 Hz (the note used to tune the orchestra).
Octaves change over at C, so C3 is one semitone higher than B2.
The reason for the fade in make_sine is that without it there are audible pops when starting and stopping notes.

simple way:
library("audio")
bday = load.wave(bday_file)
play(bday)

<h2>S4 objects, slot</h2>
A slot can be seen as a part, element or a "property" of S4 objects. 
Say you have a car object, then you can have the slots "price", "number of doors", "type of engine", "mileage".

Slots can be accessed in numerous ways :
> aCar@price
> slot(aCar,"typeEngine")

<h2>Read a UTF-8 text file with BOM</h2>
library("data.table")
theName = "file_name.csv"
thetempData = fread(theName , encoding = "UTF-8", stringsAsFactors = F)

<h2>Data Cleanup: Remove NA</h2>
data = data[!is.na(data)]

<h3>Identifying missing values</h3>
We can test for the presence of missing values via the is.na() function.

<code># remove na in r - test for missing values (is.na example)
test &lt;- c(1,2,3,NA)
is.na(test)</code>

In the example above, is.na() will return a vector indicating which elements have a na value.

<h3>na.omit() &#8211; remove rows with na from a list</h3>

This is the easiest option. 
The na.omit() function returns a list without any rows that contain na values. 

try na.omit() or na.exclude()
max( na.omit(vec) )

<code># remove na in r - remove rows - na.omit function / option
ompleterecords &lt;- na.omit(datacollected) </code>

Passing your data frame through the na.omit() function is a simple way to purge incomplete records from your analysis. 
It is an efficient way to remove na values in r.

<h3>complete.cases() &#8211; returns vector of rows with na values</h3>

The na.omit() function relies on the sweeping assumption that the dropped rows (removed the na values) are similar to the typical member of the dataset. 

We accomplish this with the complete.cases() function. 
This r function will examine a dataframe and return a vector of the rows which contain missing values. 
We can examine the dropped records and purge them if we wish.

<code># na in R - complete.cases example
fullrecords &lt;-  collecteddata[!complete.cases(collecteddata)] droprecords &lt;-  collecteddata[complete.cases(collecteddata)] </code>

<h3>Fix in place using&nbsp;na.rm</h3>

For certain statistical functions in R, you can guide the calculation around a missing value through including the na.rm parameter (na.rm=True). 
The rows with na values are retained in the dataframe but excluded from the relevant calculations. 
Support for this parameter varies by package and function, so please check the documentation for your specific package.

This is often the best option if you find there are significant trends in the observations with na values. 
Use the na.rm parameter to guide your code around the missing values and proceed from there. 
We prepared a guide to using na.rm.

<h3>NA Values and regression analysis</h3>

Removal of missing values can distort a regression analysis. 
This is particularly true if you are working with higher order or more complicated models. 
Fortunately, there are several options in the common packages for working around these issues.

If you are using the lm function, it includes a na.action option. 
As part of defining your model, you can indicate how the regression function should handle missing values. 
Two possible choices are na.omit and na.exclude. 
na.omit will omit all rows from the calculations. 
The na.exclude option removes na values from the R calculations but makes an additional adjustment (padding out vectors with missing values) to maintain the integrity of the residual analytics and predictive calculations. 
This is often more effective that procedures that delete rows from the calculations.

You also have the option of attempting to &#8220;heal&#8221; the data using custom procedures. 
In this situation, map is.na against the data set to generate a logical vector that identifies which rows need to be adjusted. 
From there, you can build your own &#8220;healing&#8221; logic.

<h2>create a empty zero-length vector</h2>
numeric()
logical()
character()
integer()
double()
raw()
complex() 
vector('numeric')
vector('character')
vector('integer')
vector('double')
vector('raw')
vector('complex')
All return 0 length vectors of the appropriate atomic modes.


<h2>Export R tables to HTML</h2>
library(tableHTML)
#create an html table 
tableHTML(mtcars)

#and to export in a file
write_tableHTML(tableHTML(mtcars), file = 'myfile.html')

<h2>h2o max_depth</h2>
Available in: GBM, DRF, XGBoost, Isolation Forest

Hyperparameter: yes

Description
This specifies the maximum depth to which each tree will be built. 
A single tree will stop splitting when there are no more splits that satisfy the min_rows parameter, if it reaches max_depth, or if there are no splits that satisfy this min_split_improvement parameter.

In general, deeper trees can seem to provide better accuracy on a training set because deeper trees can overfit your model to your data. 
Also, the deeper the algorithm goes, the more computing time is required. 
This is especially true at depths greater than 10. 
At depth 4, 8 nodes, for example, you need 8 * 100 * 20 trials to complete this splitting for the layer.

One way to determine an appropriate value for max_depth is to run a quick Cartesian grid search. 
Each model in the grid search will use early stopping to tune the number of trees using the validation set AUC, as before. 
The examples below are also available in the GBM Tuning Tutorials folder on GitHub.

The max_depth default value varies depending on the algorithm.

library(h2o)
h2o.init()
# import the titanic dataset
df <- h2o.importFile(path = "http://s3.amazonaws.com/h2o-public-test-data/smalldata/gbm_test/titanic.csv")
dim(df)
head(df)
tail(df)
summary(df, exact_quantiles = TRUE)

# pick a response for the supervised problem
response <- "survived"

# the response variable is an integer.
# we will turn it into a categorical/factor for binary classification
df[[response]] <- as.factor(df[[response]])

# use all other columns (except for the name) as predictors
predictors <- setdiff(names(df), c(response, "name"))

# split the data for machine learning
splits <- h2o.splitFrame(data = df,
                         ratios = c(0.6, 0.2),
                         destination_frames = c("train", "valid", "test"),
                         seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

# Establish a baseline performance using a default GBM model trained on the 60% training split
# We only provide the required parameters, everything else is default
gbm <- h2o.gbm(x = predictors, y = response, training_frame = train)

# Get the AUC on the validation set
h2o.auc(h2o.performance(gbm, newdata = valid))
# The AUC is over 94%, so this model is highly predictive!
[1] 0.9480135

# Determine the best max_depth value to use during a hyper-parameter search.
# Depth 10 is usually plenty of depth for most datasets, but you never know
hyper_params = list( max_depth = seq(1, 29, 2) )
# or hyper_params = list( max_depth = c(4, 6, 8, 12, 16, 20) ), which is faster for larger datasets

grid <- h2o.grid(
  hyper_params = hyper_params,

  # full Cartesian hyper-parameter search
  search_criteria = list(strategy = "Cartesian"),

  # which algorithm to run
  algorithm = "gbm",

  # identifier for the grid, to later retrieve it
  grid_id = "depth_grid",

  # standard model parameters
  x = predictors,
  y = response,
  training_frame = train,
  validation_frame = valid,

  # more trees is better if the learning rate is small enough
  # here, use "more than enough" trees - we have early stopping
  ntrees = 10000,

  # smaller learning rate is better, but because we have learning_rate_annealing,
  # we can afford to start with a bigger learning rate
  learn_rate = 0.05,

  # learning rate annealing: learning_rate shrinks by 1% after every tree
  # (use 1.00 to disable, but then lower the learning_rate)
  learn_rate_annealing = 0.99,

  # sample 80% of rows per tree
  sample_rate = 0.8,

  # sample 80% of columns per split
  col_sample_rate = 0.8,

  # fix a random number generator seed for reproducibility
  seed = 1234,

  # early stopping once the validation AUC doesn't improve by at least
  # 0.01% for 5 consecutive scoring events
  stopping_rounds = 5,
  stopping_tolerance = 1e-4,
  stopping_metric = "AUC",

  # score every 10 trees to make early stopping reproducible
  # (it depends on the scoring interval)
  score_tree_interval = 10)

# by default, display the grid search results sorted by increasing logloss
# (because this is a classification task)
grid

# sort the grid models by decreasing AUC
sorted_grid <- h2o.getGrid("depth_grid", sort_by="auc", decreasing = TRUE)
sorted_grid

# find the range of max_depth for the top 5 models
top_depths = sortedGrid@summary_table$max_depth[1:5]
min_depth = min(as.numeric(top_depths))
max_depth = max(as.numeric(top_depths))

> sorted_grid
#H2O Grid Details
Grid ID: depth_grid
Used hyper parameters:
 -  max_depth
Number of models: 15
Number of failed models: 0
Hyper-Parameter Search Summary: ordered by decreasing auc
     max_depth           model_ids                auc
  1         13  depth_grid_model_6 0.9552831783601015
...
  15         1  depth_grid_model_0 0.9478162862778248

It appears that max_depth values of 9 to 27 are best suited for this dataset, which is unusually deep.

<h2>Median</h2>
The middle most value in a data series is called the median.
The median() function is used in R to calculate this value.

data = c( 1, 2, 2, 2,3,3, 4, 7, 9 )
median(data) # Find the median 3

<h2>find mode</h2>
the value that has highest number of occurrences in a dataset
R does not have a standard in-built mode function

data = c( 1, 2, 2, 2,3,3, 4, 7, 9 )
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
getmode(data) # 2

<h2>h2o Course Prerequisites</h2>
sample codes
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html
http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/max_depth.html.

familiar with pandas
http://pandas.pydata.org/pandas-docs/stable/10min.html

R and Python+Pandas
https://www.slideshare.net/ajayohri/python-for-r-users

Basic Stats
https://mathwithbaddrawings.com/2016/07/13/why-not-to-trust-statistics/
http://www.itl.nist.gov/div898/handbook/eda/section3/eda366.htm

most important to understand the normal distribution and standard deviation:
https://en.wikipedia.org/wiki/Standard_deviation


https://students.brown.edu/seeing-theory
linear regression

advice intermixed with xkcd cartoons on stats:
http://livefreeordichotomize.com/2016/12/15/hill-for-the-data-scientist-an-xkcd-story

Confusion Matrix
http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/


Bias/Variance
http://scott.fortmann-roe.com/docs/BiasVariance.html
https://elitedatascience.com/bias-variance-tradeoff
https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error

<h2>droplevels</h2>
removes unused levels of a factor. 
x <- factor(c(3, 4, 8, 1, 5, 4, 4, 5))        # Example factor vector
x <- x[- 1]                                   # Delete first entry

Our example vector consists of five factor levels: 1, 3, 4, 5, and 8. 
However, the vector itself does not include the value 3. 
The factor level 3 might therefore be dropped. 

x_drop <- droplevels(x)                       # Apply droplevels in R
x_drop
# 4 8 1 5 4 4 5
# Levels: 1 4 5 8

<h2>h2o samples</h2>
library(h2o)
h2o.init()

h2oiris <- as.h2o( droplevels(iris[1:100,]))

h2oiris
class(h2oiris)
h2o.levels(h2oiris, 5)

write.csv( mtcars, file = 'mtcars.csv') # create local data
h2omtcars <- h2o.importFile( path = 'mtcars.csv')
h2omtcars

h2obin <- h2o.importFile( path = 'https://stats.idre.ucla.edu/stat/data/binary.csv') # load online data

gbmModel <- h2o.gbm( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # train model use GBM

h2o.varimp(gbmModel) # find variable importance

xgBoostModel <- h2o.xgboost( x = c('Month', 'DayOfWeek', 'Distance'), y = 'IsDepDelayed', training_frame = airlinesTrainData) # xgb model

h2o.predict( gbmModel, airlinesTrainData) # predict

# https://stats.idre.ucla.edu/other/dae/
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html
# http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/gbm.html#defining-a-gbm-model
# https://dzone.com/articles/how-do-you-measure-if-your-customer-churn-predicti

<h2>Gradient Boosting Machine</h2>
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. 

H2O's Gradient Boosting Machine (GBM) offers a Stochastic GBM, which can increase performance quite a bit compared to the original GBM implementation.

Now we will train a basic GBM model

The GBM model will infer the response distribution from the response encoding if not specified explicitly through the "distribution" argument.
A seed is required for reproducibility.

gbm_fit1 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit1", seed = 1)

Next we will increase the number of trees used in the GBM by setting "ntrees=500".

The default number of trees in an H2O GBM is 50, so this GBM will trained using ten times the default.

Increasing the number of trees in a GBM is one way to increase performance of the model, however, you have to be careful not to overfit your model to the training data by using too many trees.

To automatically find the optimal number of trees, you must use H2O's early stopping functionality.

This example will not do that, however, the following example will.

gbm_fit2 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit2",
    #validation_frame = valid, only used if stopping_rounds > 0
  ntrees = 500, seed = 1)

We will again set "ntrees = 500", however, this time we will use early stopping in order to prevent overfitting (from too many trees).

All of H2O's algorithms have early stopping available, however early stopping is not enabled by default (with the exception of Deep Learning).

There are several parameters that should be used to control early stopping.

The three that are common to all the algorithms are: "stopping_rounds", "stopping_metric" and "stopping_tolerance".

The stopping metric is the metric by which you'd like to measure performance, and so we will choose AUC here.

The "score_tree_interval" is a parameter specific to the Random Forest model and the GBM.

Setting "score_tree_interval = 5" will score the model after every five trees.

The parameters we have set below specify that the model will stop training after there have been three scoring intervals where the AUC has not increased more than 0.0005.

Since we have specified a validation frame, the stopping tolerance will be computed on validation AUC rather than training AUC. 

gbm_fit3 <- h2o.gbm(
  x = x, y = y, training_frame = train, model_id = "gbm_fit3",
  validation_frame = valid,  #only used if stopping_rounds > 0
  ntrees = 500, score_tree_interval = 5,      #used for early stopping
                stopping_rounds = 3,          #used for early stopping
                stopping_metric = "AUC",      #used for early stopping
                stopping_tolerance = 0.0005,  #used for early stopping
                seed = 1)

Let's compare the performance of the two GBMs

gbm_perf1 <- h2o.performance(model = gbm_fit1, newdata = test)
gbm_perf2 <- h2o.performance(model = gbm_fit2, newdata = test)
gbm_perf3 <- h2o.performance(model = gbm_fit3, newdata = test)

# Print model performance
gbm_perf1
gbm_perf2
gbm_perf3

# Retreive test set AUC
h2o.auc(gbm_perf1)  # 0.682765594191
h2o.auc(gbm_perf2)  # 0.671854616713
h2o.auc(gbm_perf3)  # 0.68309902855


To examine the scoring history, use the "scoring_history" method on a trained model.
If "score_tree_interval" is not specified, it will score at various intervals, as we can see for "h2o.scoreHistory()" below.
However, regular 5-tree intervals are used for "h2o.scoreHistory()".
The "gbm_fit2" was trained only using a training set (no validation set), so the scoring history is calculated for training set performance metrics only.

h2o.scoreHistory(gbm_fit2)


When early stopping is used, we see that training stopped at 105 trees instead of the full 500.
Since we used a validation set in "gbm_fit3", both training and validation performance metrics are stored in the scoring history object.
Take a look at the validation AUC to observe that the correct stopping tolerance was enforced.

h2o.scoreHistory(gbm_fit3)

Look at scoring history for third GBM model

plot(gbm_fit3, timestep = "number_of_trees", metric = "AUC")
plot(gbm_fit3, timestep = "number_of_trees", metric = "logloss")

4. Deep Learning
H2O's Deep Learning algorithm is a multilayer feed-forward artificial neural network.
It can also be used to train an autoencoder.
In this example we will train a standard supervised prediction model.

Train a default DL
First we will train a basic DL model with default parameters.
The DL model will infer the response distribution from the response encoding if it is not specified explicitly through the "distribution" argument.
H2O's DL will not be reproducible if it is run on more than a single core, so in this example, the performance metrics below may vary slightly from what you see on your machine.
In H2O's DL, early stopping is enabled by default, so below, it will use the training set and default stopping parameters to perform early stopping.

dl_fit1 <- h2o.deeplearning(x = x,
     y = y,
     training_frame = train,
     model_id = "dl_fit1",
     seed = 1)

Train a DL with new architecture and more epochs.

Next we will increase the number of epochs used in the GBM by setting "epochs=20" (the default is 10).
Increasing the number of epochs in a deep neural net may increase performance of the model, however, you have to be careful not to overfit your model to your training data.
To automatically find the optimal number of epochs, you must use H2O's early stopping functionality.
Unlike the rest of the H2O algorithms, H2O's DL will use early stopping by default, so for comparison we will first turn off early stopping.
We do this in the next example by setting "stopping_rounds=0".

dl_fit2 <- h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit2",
     #validation_frame = valid, only used if stopping_rounds > 0
     epochs = 20, hidden= c(10,10),
     stopping_rounds = 0,  # disable early stopping
     seed = 1)
Train a DL with early stopping This example will use the same model parameters as "dl_fit2".
This time, we will turn on  early stopping and specify the stopping criterion.
We will also pass a validation set, as is recommended for early stopping.

dl_fit3 <- h2o.deeplearning(
  x = x, y = y, training_frame = train, model_id = "dl_fit3",
     validation_frame = valid,  #in DL, early stopping is on by default
     epochs = 20, hidden = c(10,10),
     score_interval = 1,           #used for early stopping
     stopping_rounds = 3,          #used for early stopping
     stopping_metric = "AUC",      #used for early stopping
     stopping_tolerance = 0.0005,  #used for early stopping
     seed = 1)


Let's compare the performance of the three DL models
dl_perf1 <- h2o.performance(model = dl_fit1, newdata = test)
dl_perf2 <- h2o.performance(model = dl_fit2, newdata = test)
dl_perf3 <- h2o.performance(model = dl_fit3, newdata = test)

Print model performance
dl_perf1
dl_perf2
dl_perf3

# Retreive test set AUC
h2o.auc(dl_perf1)  # 0.6774335
h2o.auc(dl_perf2)  # 0.678446
h2o.auc(dl_perf3)  # 0.6770498

# Scoring history
h2o.scoreHistory(dl_fit3)
# Scoring History: 
  timestamp   duration  training_speed   epochs
1 2016-05-03 10:33:29  0.000 sec                  0.00000
2 2016-05-03 10:33:29  0.347 sec 424697 rows/sec  0.86851
3 2016-05-03 10:33:30  1.356 sec 601925 rows/sec  6.09185
4 2016-05-03 10:33:31  2.348 sec 717617 rows/sec 13.05168
5 2016-05-03 10:33:32  3.281 sec 777538 rows/sec 20.00783
6 2016-05-03 10:33:32  3.345 sec 777275 rows/sec 20.00783

# iterations        samples training_MSE training_r2
1          0       0.000000  
2          1   99804.000000      0.14402     0.03691
3          7  700039.000000      0.14157     0.05333
4         15 1499821.000000      0.14033     0.06159
5         23 2299180.000000      0.14079     0.05853
6         23 2299180.000000      0.14157     0.05333
# training_logloss training_AUC training_lift
1                     
2          0.45930      0.66685       2.20727
3          0.45220      0.68133       2.59354
4          0.44710      0.67993       2.70390
5          0.45100      0.68192       2.81426
6          0.45220      0.68133       2.59354
# training_classification_error validation_MSE validation_r2
1             
2                       0.36145        0.14682       0.03426
3                       0.33647        0.14500       0.04619
4                       0.37126        0.14411       0.05204
5                       0.32868        0.14474       0.04793
6                       0.33647        0.14500       0.04619
# validation_logloss validation_AUC validation_lift
1    
2            0.46692        0.66582         2.53209
3            0.46256        0.67354         2.64124
4            0.45789        0.66986         2.44478
5            0.46292        0.67117         2.70672
6            0.46256        0.67354         2.64124
# validation_classification_error
1         
2  0.37197
3  0.34716
4  0.34385
5  0.36544
6  0.34716

# Look at scoring history for third DL model
plot(dl_fit3, timestep = "epochs", metric = "AUC")


5. Naive Bayes model
The Naive Bayes (NB) algorithm does not usually beat an algorithm like a Random Forest or GBM, however it is still a popular algorithm, especially in the text domain (when your input is text encoded as "Bag of Words", for example).
The Naive Bayes algorithm is for binary or multiclass classification problems only, not regression.
Therefore, your response must be a factor instead of a numeric.

First we will train a basic NB model with default parameters. 

nb_fit1 <- h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit1")

Train a NB model with Laplace Smoothing
One of the few tunable model parameters for the Naive Bayes algorithm is the amount of Laplace smoothing. 
The H2O Naive Bayes model will not use any Laplace smoothing by default.

nb_fit2 <- h2o.naiveBayes(
  x = x, y = y, training_frame = train, model_id = "nb_fit2", laplace = 6)

Let's compare the performance of the two NB models
nb_perf1 <- h2o.performance(model = nb_fit1, newdata = test)
nb_perf2 <- h2o.performance(model = nb_fit2, newdata = test)

# Print model performance
nb_perf1
nb_perf2

# Retreive test set AUC
h2o.auc(nb_perf1)  # 0.6488014
h2o.auc(nb_perf2)  # 0.6490678

<h2>Confusion Matrix</h2>
<a href="http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" class="whitebut ">the basic yes/no confusion matrix</a>

<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html" class="whitebut ">Bias/Variance</a>

<a href="https://elitedatascience.com/bias-variance-tradeoff" class="whitebut ">the Bias-Variance Tradeoff</a>

<a href="https://en.wikipedia.org/wiki/Bias_of_an_estimator#Bias.2C_variance_and_mean_squared_error" class="whitebut ">shows biased estimator can be better than perfectly unbiased estimator</a>

<a href="https://www.coursera.org/learn/machine-learning-h2o/home/welcome" class="whitebut ">Practical Machine Learning on H2O</a>

<a href="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-munging/merging-data.html" class="whitebut ">Merging Two Datasets</a>

<a href="https://www.h2o.ai/download/" class="whitebut ">Downloads</a>

<h2>Print Strings without Quotes</h2>
   print(resultTable, quote = FALSE, row.names = FALSE, col.names = FALSE)
   write.table(resultTable,quote = FALSE,  row.names = FALSE, sep = "\t")

<h2>The apply family</h2>
<a href="https://nicercode.github.io/guides/repeating-things/" class="whitebut ">repeating-things</a>

There are several related function in R which allow you to apply some function to a series of objects (eg. vectors, matrices, dataframes or files). They include:
lapply
sapply
tapply
aggregate
mapply
apply

Each repeats a function or operation on a series of elements, but they differ in the data types they accept and return. What they all in common is that <strong>order of iteration is not important</strong>.  This is crucial. If each each iteration is independent, then you can cycle through them in whatever order you like. Generally, we argue that you should only use the generic looping functions <code>for</code>, <code>while</code>, and <code>repeat</code> when the order or operations <strong>is</strong> important. Otherwise reach for one of the apply tools.

<h2>lapply and sapply</h2>

<code>lapply</code> applies a function to each element of a list (or vector), collecting results in a list.  <code>sapply</code> does the same, but will try to <em>simplify</em> the output if possible.

Lists are a very powerful and flexible data structure that few people seem to know about. Moreover, they are the building block for other data structures, like <code>data.frame</code> and <code>matrix</code>. To access elements of a list, you use the double square bracket, for example <code>X[[4]]</code> returns the fourth element of the list <code>X</code>. If you don’t know what a list is, we suggest you <a href="http://cran.r-project.org/doc/manuals/R-intro.html#Lists-and-data-frames">read more about them</a>, before you proceed.
<h3>Basic syntax</h3>
<code>result &lt;- lapply(a list or vector, a function, ...)</code>

This code will also return a list, stored in <code>result</code>, with same number of elements as <code>X</code>.

<h3>Usage</h3>
lapply is great for building analysis pipelines, where you want to repeat a series of steps on a large number of similar objects.  The way to do this is to have a series of lapply statements, with the output of one providing the input to another:
<code>first.step &lt;- lapply(X, first.function) second.step &lt;- lapply(first.step, next.function)</code>

The challenge is to identify the parts of your analysis that stay the same and those that differ for each call of the function. The trick to using <code>lapply</code> is to recognise that only one item can differ between different function calls.

It is possible to pass in a bunch of additional arguments to your function, but these must be the same for each call of your function. For example, let’s say we have a function <code>test</code> which takes the path of a file, loads the data, and tests it against some hypothesised value H0. We can run the function on the file
“myfile.csv” as follows.

<code>result &lt;- test(&quot;myfile.csv&quot;, H0=1)</code>

We could then run the test on a bunch of files using lapply:

<code>files &lt;- c(&quot;myfile1.csv&quot;, &quot;myfile2.csv&quot;, &quot;myfile3.csv&quot;) result &lt;- lapply(files, test, H0=1)</code>

But notice, that in this example, the <strong>only this that differs</strong> between the runs is a single number in the file name. So we could save ourselves typing these by adding an extra step to generate the file names

<code>files &lt;- lapply(1:10, function(x){paste0(&quot;myfile&quot;, x, &quot;.csv&quot;)}) result &lt;- lapply(files, test, H0=1)</code>

The nice things about that piece of code is that it would extend as long as we wanted, to 10000000 files, if needed.

<h3>Example - plotting temperature for many sites using open weather data</h3>

Let’s look at the weather in some eastern Australian cities over the last couple of days.  The website
<a href="http://openweathermap.org">openweathermap.com</a> provides access to all sorts of neat data, lots of it essentially real time.  We’ve parcelled up some on the nicercode website to use.  In theory, this sort of analysis script could use the weather data directly, but we don’t want to hammer their website too badly.  The code used to generate these files is <a href="https://gist.github.com/richfitz/5795029">here</a>.

We want to look at the temperatures over the last few days for the cities

<code>cities &lt;- c(&quot;Melbourne&quot;, &quot;Sydney&quot;, &quot;Brisbane&quot;, &quot;Cairns&quot;)</code>

The data are stored in a url scheme where the Sydney data is at
<a href="http://nicercode.github.io/guides/repeating-things/data/Sydney.csv">http://nicercode.github.io/guides/repeating-things/data/Sydney.csv</a> and so on.  

The URLs that we need are therefore:

<code>urls &lt;-
 sprintf(&quot;http://nicercode.github.io/guides/repeating-things/data/%s.csv&quot;,
         cities) urls</code>

<code>[1] "http://nicercode.github.io/guides/repeating-things/data/Melbourne.csv"
[2] "http://nicercode.github.io/guides/repeating-things/data/Sydney.csv"   
[3] "http://nicercode.github.io/guides/repeating-things/data/Brisbane.csv" 
[4] "http://nicercode.github.io/guides/repeating-things/data/Cairns.csv"   </code>

We can write a function to download a file if it does not exist:

<code>download.maybe &lt;- function(url, refetch=FALSE, path=&quot;.&quot;) {
 dest &lt;- file.path(path, basename(url))
 if (refetch || !file.exists(dest))
   download.file(url, dest)
 dest
}</code>
and then run that over the urls:

<code>path &lt;- &quot;data&quot; dir.create(path, showWarnings=FALSE) files &lt;- sapply(urls, download.maybe, path=path) names(files) &lt;- cities</code>

Notice that we never specify the order of which file is downloaded in which order; we just say “apply this function (<code>download.maybe</code>) to this list of urls.  We also pass the <code>path</code> argument to every function call.  So it was as if we’d written

<code>download.maybe(urls[[1]], path=path) download.maybe(urls[[2]], path=path) download.maybe(urls[[3]], path=path) download.maybe(urls[[4]], path=path)</code>
but much less boring, and scalable to more files.

The first column, <code>time</code> of each file is a string representing date and time, which needs processing into R’s native time format (dealing with times in R (or frankly, in any language) is a complete pain).  In a real case, there might be many steps involved in processing each file.  We can make a function like this:

<code>load.file &lt;- function(filename) {
 d &lt;- read.csv(filename, stringsAsFactors=FALSE)
 d$time &lt;- as.POSIXlt(d$time)
 d
}</code>
that reads in a file given a filename, and then apply that function to each filename using <code>lapply</code>:

<code>data &lt;- lapply(files, load.file) names(data) &lt;- cities</code>

We now have a <strong>list</strong>, where each element is a <code>data.frame</code> of weather data:

<code>head(data$Sydney)</code>

<code>             time  temp temp.min temp.max
1 2013-06-13 23:00:00 12.66     8.89    16.11
2 2013-06-14 00:00:00 15.90    12.22    20.00
3 2013-06-14 02:00:00 18.44    16.11    20.00
4 2013-06-14 03:00:00 18.68    16.67    20.56
5 2013-06-14 04:00:00 19.41    17.78    22.22
6 2013-06-14 05:00:00 19.10    17.78    22.22</code>

We can use <code>lapply</code> or <code>sapply</code> to easy ask the same question to each element of this list.  For example, how many rows of data are there?

<code>sapply(data, nrow)</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
      97        99        99        80 </code>

What is the hottest temperature recorded by city?

<code>sapply(data, function(x) max(x$temp))</code>

<code>Melbourne    Sydney  Brisbane    Cairns 
   12.85     19.41     22.00     31.67 </code>
or, estimate the autocorrelation function for each set:

<code>autocor &lt;- lapply(data, function(x) acf(x$temp, lag.max=24))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-221.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-222.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-223.png" alt="plot of chunk unnamed-chunk-22" /> <img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-224.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Sydney, main=&quot;Sydney&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-225.png" alt="plot of chunk unnamed-chunk-22" />

<code>plot(autocor$Cairns, main=&quot;Cairns&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-226.png" alt="plot of chunk unnamed-chunk-22" />

I find that for loops can be easier to plot data, partly because there is nothing to <em>collect</em> (or combine) at each iteration.

<code>xlim &lt;- range(sapply(data, function(x) range(x$time))) ylim &lt;- range(sapply(data, function(x) range(x[-1]))) plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type=&quot;n&quot;,
    xlab=&quot;Time&quot;, ylab=&quot;Temperature&quot;) cols &lt;- 1:4 for (i in seq_along(data))
 lines(data[[i]]$time, data[[i]]$temp, col=cols[i])</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-23.png" alt="plot of chunk unnamed-chunk-23" />

<code>plot(data[[1]]$time, data[[1]]$temp, ylim=ylim, type=&quot;n&quot;,
    xlab=&quot;Time&quot;, ylab=&quot;Temperature&quot;) mapply(function(x, col) lines(x$time, x$temp, col=col),
      data, cols)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-24.png" alt="plot of chunk unnamed-chunk-24" />

<code>$Melbourne
NULL

$Sydney
NULL

$Brisbane
NULL

$Cairns
NULL</code>

<h3>Parallelising your code</h3>

Another great feature of lapply is that is <strong>makes it really easy to parallelise your code</strong>. All computers now contain multiple CPUs, and these can all be put to work using the great <a href="http://www.rforge.net/multicore/">multicore package</a>.

<code>result &lt;- lapply(x, f)   #apply f to x using a single core and lapply
library(multicore) result &lt;- mclapply(x, f) #same thing using all the cores in your machine</code>

<h2>tapply and aggregate</h2>

In the case above, we had naturally “split” data; we had a vector of city names that led to a list of different data.frames of weather data.  Sometimes the “split” operation depends on a factor.  For example, you might have an experiment where you measured the size of plants at different levels of added fertiliser - you then want to know the mean height as a function of this treatment.

However, we’re actiually going to use some data on <a href="https://github.com/audy/smalldata">ratings of seinfeld episodes</a>, taken from the [Internet movie Database]
(http://www.reddit.com/r/dataisbeautiful/comments/1g7jw2/seinfeld_imdb_episode_ratings_oc/).

<code>library(downloader) if (!file.exists(&quot;seinfeld.csv&quot;))
 download(&quot;https://raw.github.com/audy/smalldata/master/seinfeld.csv&quot;,
          &quot;seinfeld.csv&quot;) dat &lt;- read.csv(&quot;seinfeld.csv&quot;, stringsAsFactors=FALSE)</code>

Columns are Season (number), Episode (number), Title (of the episode), Rating (according to IMDb) and Votes (to construct the rating).

<code>head(dat)</code>

<code>  Season Episode             Title Rating Votes
1      1       2      The Stakeout    7.8   649
2      1       3       The Robbery    7.7   565
3      1       4    Male Unbonding    7.6   561
4      1       5     The Stock Tip    7.8   541
5      2       1 The Ex-Girlfriend    7.7   529
6      2       1        The Statue    8.1   509</code>

Make sure it’s sorted sensibly

<code>dat &lt;- dat[order(dat$Season, dat$Episode),]</code>

Biologically, this could be Site / Individual / ID / Mean size /
Things measured.

Hypothesis: Seinfeld used to be funny, but got progressively less good as it became too mainstream.  Or, does the mean episode rating per season decrease?

Now, we want to calculate the average rating per season:

<code>mean(dat$Rating[dat$Season == 1])</code>

<code>[1] 7.725</code>

<code>mean(dat$Rating[dat$Season == 2])</code>

<code>[1] 8.158</code>
and so on until:

<code>mean(dat$Rating[dat$Season == 9])</code>

<code>[1] 8.323</code>

As with most things, we <em>could</em> automate this with a for loop:

<code>seasons &lt;- sort(unique(dat$Season)) rating  &lt;- numeric(length(seasons)) for (i in seq_along(seasons))
 rating[i] &lt;- mean(dat$Rating[dat$Season == seasons[i]])</code>

That’s actually not that horrible to do.  But we it could be nicer.  We first <strong>split</strong> the ratings by season:

<code>ratings.split &lt;- split(dat$Rating, dat$Season) head(ratings.split)</code>

<code>$`1`
[1] 7.8 7.7 7.6 7.8

$`2`
[1] 7.7 8.1 8.0 7.9 7.8 8.5 8.7 8.5 8.0 8.0 8.4 8.3

$`3`
[1] 8.3 7.5 7.8 8.1 8.3 7.3 8.7 8.5 8.5 8.6 8.1 8.4 8.5 8.7 8.6 7.8 8.3
[18] 8.6 8.7 8.6 8.0 8.5 8.6

$`4`
[1] 8.4 8.3 8.6 8.5 8.7 8.6 8.1 8.2 8.7 8.4 8.3 8.7 8.5 8.6 8.3 8.2 8.4
[18] 8.5 8.4 8.7 8.7 8.4 8.5

$`5`
[1] 8.6 8.4 8.4 8.4 8.3 8.2 8.1 8.5 8.5 8.3 8.0 8.1 8.6 8.3 8.4 8.5 7.9
[18] 8.0 8.5 8.7 8.5

$`6`
[1] 8.1 8.4 8.3 8.4 8.2 8.3 8.5 8.4 8.3 8.2 8.1 8.4 8.6 8.2 7.5 8.4 8.2
[18] 8.5 8.3 8.4 8.1 8.5 8.2</code>

Then use sapply to loop over this list, computing the mean

<code>rating &lt;- sapply(ratings.split, mean)</code>

Then if we wanted to apply a different function (say, compute the per-season standard error) we could just do:

<code>se &lt;- function(x)
 sqrt(var(x) / length(x)) rating.se &lt;- sapply(ratings.split, se)
plot(rating ~ seasons, ylim=c(7, 9), pch=19) arrows(seasons, rating - rating.se, seasons, rating + rating.se,
      code=3, angle=90, length=0.02)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-34.png" alt="plot of chunk unnamed-chunk-34" />

But there’s still repetition there.  Let’s abstract that away a bit.

Suppose we want a:
 1. response variable (like Rating was)
 2. grouping variable (like Season was)
 3. function to apply to each level

This just writes out <em>exactly</em> what we had before

<code>summarise.by.group &lt;- function(response, group, func) {
 response.split &lt;- split(response, group)
 sapply(response.split, func)
}</code>

We can compute the mean rating by season again:

<code>rating.new &lt;- summarise.by.group(dat$Rating, dat$Season, mean)</code>
which is the same as what we got before:

<code>identical(rating.new, rating)</code>

<code>[1] TRUE</code>

Of course, we’re not the first people to try this.  This is <strong>exactly</strong> what the <code>tapply</code> function does (but with a few bells and whistles, especially around missing values, factor levels, additional arguments and multiple grouping factors at once).

<code>tapply(dat$Rating, dat$Season, mean)</code>

<code>1     2     3     4     5     6     7     8     9 
7.725 8.158 8.304 8.465 8.343 8.283 8.441 8.423 8.323 </code>

So using <code>tapply</code>, you can do all the above manipulation in a single line.

There are a couple of limitations of <code>tapply</code>.

The first is that getting the season out of <code>tapply</code> is quite hard.  We could do:

<code>as.numeric(names(rating))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that’s quite ugly, not least because it involves the conversion numeric -&gt; string -&gt; numeric.

Better could be to use

<code>sort(unique(dat$Season))</code>

<code>[1] 1 2 3 4 5 6 7 8 9</code>

But that requires knowing what is going on inside of <code>tapply</code> (that unique levels are sorted and data are returned in that order).

I suspect that this approach:

<code>first &lt;- function(x) x[[1]] tapply(dat$Season, dat$Season, first)</code>

<code>1 2 3 4 5 6 7 8 9 
1 2 3 4 5 6 7 8 9 </code>
is probably the most fool-proof, but it’s certainly not pretty.

However, the returned format is extremely flexible.  If you do:

The <code>aggregate</code> function provides a simplfied interface to <code>tapply</code> that avoids this issue.  It has two interfaces: the first is similar to what we used before, but the grouping variable now must be a list or data frame:

<code>aggregate(dat$Rating, dat[&quot;Season&quot;], mean)</code>

<code>  Season     x
1      1 7.725
2      2 8.158
3      3 8.304
4      4 8.465
5      5 8.343
6      6 8.283
7      7 8.441
8      8 8.423
9      9 8.323</code>

(note that <code>dat["Season"]</code> returns a one-column data frame).  The column ‘x’ is our response variable, Rating, grouped by season.  We can get its name included in the column names here by specifying the first argument as a <code>data.frame</code> too:

<code>aggregate(dat[&quot;Rating&quot;], dat[&quot;Season&quot;], mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

The other interface is the formula interface, that will be familiar from fitting linear models:

<code>aggregate(Rating ~ Season, dat, mean)</code>

<code>  Season Rating
1      1  7.725
2      2  8.158
3      3  8.304
4      4  8.465
5      5  8.343
6      6  8.283
7      7  8.441
8      8  8.423
9      9  8.323</code>

This interface is really nice; we can get the number of votes here too.

<code>aggregate(cbind(Rating, Votes) ~ Season, dat, mean)</code>

<code>  Season Rating Votes
1      1  7.725 579.0
2      2  8.158 533.0
3      3  8.304 496.7
4      4  8.465 497.0
5      5  8.343 452.5
6      6  8.283 385.7
7      7  8.441 408.0
8      8  8.423 391.4
9      9  8.323 415.0</code>

If you have multiple grouping variables, you can write things like:
&lt;div class=&#8217;bogus-wrapper&#8217;&gt;&lt;figcaption&gt;&lt;/figcaption&gt;&lt;div class=&#8221;highlight&#8221;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#8221;gutter&#8221;&gt;&lt;pre class=&#8221;line-numbers&#8221;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#8217;code&#8217;&gt;&lt;pre&gt;<code>aggregate(response ~ factor1 + factor2, dat, function)</code>&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/div&gt;&lt;/div&gt;

to apply a function to each pair of levels of <code>factor1</code> and <code>factor2</code>.

<h2>replicate</h2>

This is great in Monte Carlo simulation situations.  For example.
Suppose that you flip a fair coin n times and count the number of heads:

<code>trial &lt;- function(n)
 sum(runif(n) &lt; 0.5) # could have done a binomial draw...</code>

You can run the trial a bunch of times:

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 4</code>

<code>trial(10)</code>

<code>[1] 6</code>
and get a feel for the results.  If you want to replicate the trial
100 times and look at the distribution of results, you could do:

<code>replicate(100, trial(10))</code>

<code>  [1] 4 4 5 6 8 5 5 7 3 5 6 4 4 3 5 3 6 7 2 6 6 4 5 4 4 4 4 5 6 5 4 2 6 5 6
[36] 5 6 8 5 6 4 5 4 5 5 5 4 7 3 5 5 6 4 6 4 6 4 4 4 6 3 5 5 7 6 7 5 3 4 4
[71] 5 6 8 5 6 2 5 7 6 3 5 9 3 7 6 4 5 3 7 3 3 7 6 8 5 4 6 7 4 3</code>
and then you could plot these:

<code>plot(table(replicate(10000, trial(50))))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-49.png" alt="plot of chunk unnamed-chunk-49" />

<h3>for loops</h3>

“<code>for</code>” loops shine where the output of one iteration depends on the result of the previous iteration.

Suppose you wanted to model random walk.  Every time step, with 50% probability move left or right.

Start at position 0

<code>x &lt;- 0</code>

Move left or right with probability p (0.5 = unbiased)

<code>p &lt;- 0.5</code>

Update the position

<code>x &lt;- x + if (runif(1) &lt; p) -1 else 1</code>

Let’s abstract the update into a function:

<code>step &lt;- function(x, p=0.5)
 x + if (runif(1) &lt; p) -1 else 1</code>

Repeat a bunch of times:

<code>x &lt;- step(x) x &lt;- step(x)</code>

To find out where we got to after 20 steps:

<code>for (i in 1:20)
 x &lt;- step(x)</code>

If we want to collect where we’re up to at the same time:

<code>nsteps &lt;- 200 x &lt;- numeric(nsteps + 1) x[1] &lt;- 0 # start at 0 for (i in seq_len(nsteps))
 x[i+1] &lt;- step(x[i]) plot(x, type=&quot;l&quot;)</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-56.png" alt="plot of chunk unnamed-chunk-56" />

Pulling <em>that</em> into a function:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5) {
 x &lt;- numeric(nsteps + 1)
 x[1] &lt;- x0
 for (i in seq_len(nsteps))
   x[i+1] &lt;- step(x[i])
 x
}</code>

We can then do 30 random walks:

<code>walks &lt;- replicate(30, random.walk(100)) matplot(walks, type=&quot;l&quot;, lty=1, col=rainbow(nrow(walks)))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-58.png" alt="plot of chunk unnamed-chunk-58" />

Of course, in this case, if we think in terms of vectors we can actually implement random walk using implicit vectorisation:

<code>random.walk &lt;- function(nsteps, x0=0, p=0.5)
 cumsum(c(x0, ifelse(runif(nsteps) &lt; p, -1, 1)))
walks &lt;- replicate(30, random.walk(100)) matplot(walks, type=&quot;l&quot;, lty=1, col=rainbow(nrow(walks)))</code>

<img class="lazy" data-src="https://nicercode.github.io/guides/repeating-things/img/unnamed-chunk-59.png" alt="plot of chunk unnamed-chunk-59" />

Which reinforces one of the advantages of thinking in terms of functions: you can change the implementation detail without the rest of the program changing.

<h2>increase the max print rows limit</h2>
getOption("max.print")
options(max.print=999999)

<h2>parallel processing: foreach package</h2>
loops are incredibly inefficient at processing data in R.

iters&lt;-10  #number of iterations in the loop
ls&lt;-vector('list',length=iters)  #vector for appending output
strt&lt;-Sys.time()   #start time
for(i in 1:iters){   #loop
    cat(i,'\n')    #counter
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    ls[[i]]&lt;-to.ls    #export
}

print(Sys.time()-strt)   #end time
# Time difference of 2.944168 secs

repeated the above code with an increasing number of iterations, 10 to 100 at intervals of 10.

iters&lt;-seq(10,100,by=10)   #iterations to time
times&lt;-numeric(length(iters))  #output time vector for  iteration sets
for(val in 1:length(iters)){   #loop over iteration sets
    cat(val,' of ', length(iters),'\n')
    to.iter&lt;-iters[val]
    ls&lt;-vector('list',length=to.iter)    #vector for appending output
    strt&lt;-Sys.time()    #start time
    for(i in 1:to.iter){    #same for loop as before
        cat(i,'\n')
        to.ls&lt;-rnorm(1e6)
        to.ls&lt;-summary(to.ls)
        ls[[i]]&lt;-to.ls          #export
    }
    times[val]&lt;-Sys.time()-strt    #end time
}

library(ggplot2)   #plot the times
to.plo&lt;-data.frame(iters,times)
ggplot(to.plo,aes(x=iters,y=times)) + 
    geom_point() +
    geom_smooth() + 
    theme_bw() + 
    scale_x_continuous('No. of loop iterations') + 
    scale_y_continuous ('Time in seconds')

<img src="https://beckmw.files.wordpress.com/2014/01/seq_time1.jpg">
Fig: Processing time as a function of number of iterations for a simple loop.

The processing time increases linearly with the number of iterations.  
Again, processing time is not extensive for the above example.  
Suppose we wanted to run the example with ten thousand iterations.  
We can predict how long that would take based on the linear relationship between time and iterations.

mod&lt;-lm(times~iters)    #predict times
predict(mod,newdata=data.frame(iters=1e4))/60
# 45.75964


This is all well and good if we want to wait around for 45 minutes.  
Running the loop in parallel would greatly decrease this time.  
I want to first illustrate the problem of running loops in sequence before I show how this can done using the foreach package.  
If the above code is run with <code>1e4</code> iterations, a quick look at the performance metrics in the task manager (Windows 7 OS) gives you an idea of how hard your computer is working to process the code.  
My machine has eight processors and you can see that only a fraction of them are working while the script is running.

<img src="https://beckmw.files.wordpress.com/2014/01/proc1.jpg">
Fig: Resources used during sequential processing of a <code>for</code> loop.

Running the code using foreach will make full use of the computer&#8217;s processors.
Individual chunks of the loop are sent to each processor so that the entire process can be run in parallel rather than in sequence.  
Here&#8217;s how to run the code with <code>1e4</code> iterations in parallel.
That is, each processor gets a finite set of the total number of iterations, i.e., iterations 1&#8211;100 goes to processor one, iterations 101&#8211;200 go to processor two, etc. The output from each processor is then comiled after the iterations are completed.  

#import packages
library(foreach)
library(doParallel)
iters&lt;-1e4   #number of iterations

#setup parallel backend to use 8 processors
cl&lt;-makeCluster(8)
registerDoParallel(cl)

#start time
strt&lt;-Sys.time()

#loop
ls&lt;-foreach(icount(iters)) %dopar% {
    to.ls&lt;-rnorm(1e6)
    to.ls&lt;-summary(to.ls)
    to.ls
    }

print(Sys.time()-strt)
stopCluster(cl)

#Time difference of 10.00242 mins

Running the loop in parallel decreased the processing time about four-fold.  
Although the loop generally looks the same as the sequential version, several parts of the code have changed.  
First, we are using the <code>foreach</code> function rather than <code>for</code> to define our loop.  
The syntax for specifying the iterator is slightly different with <code>foreach</code> as well, i.e., <code>icount(iters)</code> tells the function to repeat the loop a given number of times based on the value assigned to <code>iters</code>.  
Additionally, the convention <code>%dopar%</code> specifies that the code is to be processed in parallel if a backend has been registered (using <code>%do%</code> will run the loop sequentially).  
The functions <code>makeCluster</code> and <code>registerDoParallel</code> from the <a href="http://cran.r-project.org/web/packages/doParallel/index.html" title="doParallel">doParallel</a> package are used to create the parallel backend.  
Another important issue is the method for recombining the data after the chunks are processed. By default, <code>foreach</code> will append the output to a list which we&#8217;ve saved to an object.  
The default method for recombining output can be changed using the <code>.combine</code> argument.  
Also be aware that packages used in the evaluated expression must be included with the <code>.packages</code> argument.
The processors should be working at full capacity if the the loop is executed properly.  
Note the difference here compared to the first loop that was run in sequence.

<img src="https://beckmw.files.wordpress.com/2014/01/proc2.jpg">
Fig: Resources used during parallel processing of a <code>for</code> loop.

A few other issues are worth noting when using the foreach package.  
These are mainly issues I&#8217;ve encountered and I&#8217;m sure others could contribute to this list.  
The foreach package does not work with all types of loops.  
For example, I chose the above example to use a large number (<code>1e6</code>) of observations with the <code>rnorm</code> function.  
I can&#8217;t say for certain the exact type of data that works best, but I have found that functions     hat take a long time when run individually are generally handled very well.  
Interestingly, decreasing the number of observations and increasing the number of iterations may cause the processors to not run at maximum efficiency (try <code>rnorm(100)</code> with <code>1e5</code> iterations). I also haven&#8217;t had much success running repeated models in parallel.  
The functions work but the processors never seem to reach max efficiency.  
The system statistics should cue you off as to whether or not the functions are working.
I also find it bothersome that monitoring progress seems is an issue with parallel loops.  
A simple call using <code>cat</code> to return the iteration in the console does not work with parallel loops.  
The most practical solution I&#8217;ve found is described <a href="http://vikparuchuri.com/blog/monitoring-progress-inside-foreach-loop/" title="here">here</a>, which involves exporting information to a separate file that tells you how far the loop has progressed.  
Also, be very aware of your RAM when running processes in parallel.  
I&#8217;ve found that it&#8217;s incredibly easy to max out the memory, which not only causes the function to stop working correctly, but also makes your computer run like garbage.  
Finally, I&#8217;m a little concerned that I might be destroying my processors by running them at maximum capacity.  
The fan always runs at full blast leading me to believe that critical meltdown is imminent.  
I&#8217;d be pleased to know if this is an issue or not.
That&#8217;s it for now.  
I have to give credit to <a href="http://cran.r-project.org/web/packages/doParallel/vignettes/gettingstartedParallel.pdf" title="this tutorial">this tutorial</a> for a lot of the information in this post.  
<h3>Vectorised</h3>
E = sapply(1:10000, function(n) {max.eig(5, 1)})
summary(E)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
 0.7615  1.9150  2.2610  2.3160  2.6470  5.2800

Here eigenvalues are calculated from 10000 function calls, all of which use the same parameters. 
The distribution of the resulting eigenvalues is plotted in the histogram below. 
Generating these data took a couple of seconds on my middle-of-the-range laptop. 
Not a big wait. 
But it was only using one of the four cores on the machine, so in principle it could have gone faster.

We can make things more interesting by varying the dimensions of the matrix.
sapply(1:5, function(n) {max.eig(n, 1)})

Or changing both the dimensions (taking on integral values between 1 and 5) and the standard deviation (running through 1, 2 and 3).
sapply(1:5, function(n) {sapply(1:3, function(m) {max.eig(n, m)})})

The results are presented in an intuitive matrix. Everything up to this point is being done serially.
<h3>Enter foreach</h3>

library(foreach)

At first sight, the foreach library provides a slightly different interface for vectorisation. We’ll start off with simple repetition.
times(10) %do% max.eig(5, 1)

That just executes the function with the same arguments 10 times over. If we want to systematically vary the parameters, then instead of times() we use foreach().
foreach(n = 1:5) %do% max.eig(n, 1)

The results are returned as a list, which is actually more reminiscent of the behaviour of lapply() than sapply(). But we can get something more compact by using the .combine option.
foreach(n = 1:5, .combine = c) %do% max.eig(n, 1)


That’s better. Now, what about varying both the dimensions and standard deviation? We can string together multiple calls to foreach() using the %:% nesting operator.
foreach(n = 1:5) %:% foreach(m = 1:3) %do% max.eig(n, m)

I have omitted the output because it consists of nested lists: it’s long and somewhat ugly. But again we can use the .combine option to make it more compact.
foreach(n = 1:5, .combine = rbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

foreach(n = 1:5, .combine = cbind) %:% foreach(m = 1:3) %do% max.eig(n, m)

You can choose between combining using cbind() or rbind() depending on whether you want the output from the inner loop to form the columns or rows of the output. 
There’s lots more magic to be done with .combine. 
You can find the details in the informative article <a href="http://r.adu.org.za/web/packages/foreach/vignettes/foreach.pdf" rel="nofollow" target="_blank">Using The foreach Package</a> by Steve Weston.

You can also use foreach() to loop over multiple variables simultaneously.
foreach(n = 1:5, m = 1:5) %do% max.eig(n, m)

But this is still all serial…

<h3>Filtering</h3>
One final capability before we move on to parallel execution, is the ability to add in a filter within the foreach() statement.

library(numbers)
foreach(n = 1:100, .combine = c) %:% when (isPrime(n)) %do% n

Here we identify the prime numbers between 1 and 100 by simply looping through the entire sequence of values and selecting only those that satisfy the condition in the when() clause. Of course, there are more efficient ways to do this, but this notation is rather neat.
<h3>Going Parallel</h3>
Making the transition from serial to parallel is as simple as changing %do% to %dopar%.
foreach(n = 1:5) %dopar% max.eig(n, 1)

Warning message:
executing %dopar% sequentially: no parallel backend registered

The warning gives us pause for thought: maybe it was not quite that simple? Yes, indeed, there are additional requirements. You need first to choose a parallel backend. And here, again, there are a few options. We will start with the most accessible, which is the multicore backend.
<h3>Multicore</h3>
Multicore processing is provided by the doMC library. You need to load the library and tell it how many cores you want to use.
library(doMC)
registerDoMC(cores=4)

Let’s make a comparison between serial and parallel execution times.

library(rbenchmark)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

The overall execution time is reduced, but not by the factor of 4 that one might expect. 
This is due to the additional burden of having to distribute the job over the multiple cores. 
The tradeoff between communication and computation is one of the major limitations of parallel computing, but if computations are lengthy and there is not too much data to move around then the gains can be excellent.

On a single machine you are limited by the number of cores. But if you have access to a cluster then you can truly take things to another level.
<h3>Cluster</h3>
The foreach() functionality can be applied to a cluster using the doSNOW library. We will start by using doSNOW to create a collection of R instances on a single machine using a SOCK cluster.

library(doSNOW)
cluster = makeCluster(4, type = "SOCK")
registerDoSNOW(cluster)

benchmark(
+     foreach(n = 1:50) %do% max.eig(n, 1),
+     foreach(n = 1:50) %dopar% max.eig(n, 1)
+ )

stopCluster(cluster)

There is an improvement in execution time which is roughly comparable to what we got with the multicore implementation. Note that when you are done, you need to shut down the cluster.

Next we will create an <a href="https://en.wikipedia.org/wiki/Message_Passing_Interface" rel="nofollow" target="_blank">MPI</a> cluster consisting of 20 threads.
cluster = makeCluster(20, type = "MPI")
#
registerDoSNOW(cluster)
#
benchmark(
+     foreach(n = 1:100) %do% max.eig(n, 1),
+     foreach(n = 1:100) %dopar% max.eig(n, 1)
+ )


There is an improvement in performance, with the parallel job running roughly 3 times as quickly.

How about a slightly more complicated example? We will try running some bootstrap calculations. We start out with the serial implementation.
random.data <- matrix(rnorm(1000000), ncol = 1000)

bmed <- function(d, n) median(d[n])

library(boot)
#
sapply(1:100, function(n) {sd(boot(random.data[, n], bmed, R = 10000)$t)})

First we generated a big array of normally distributed random numbers. Then we used sapply to calculate bootstrap estimates for the standard deviation of the median for each columns of the matrix.

The parallel implementation requires a little more work: first we need to make the global data (the random matrix and the bootstrap function) available across the cluster.
clusterExport(cluster, c("random.data", "bmed"))

Then we spread the jobs out over the cluster nodes. We will do this first using clusterApply(), which is part of the snow library and is the cluster analogue of sapply(). It returns a list, so to get a nice compact representation we use unlist().
results = clusterApply(cluster, 1:100, function(n) {
+     library(boot)
+     sd(boot(random.data[, n], bmed, R = 10000)$t)
+ })
head(unlist(results))


The foreach implementation is a little neater.
results = foreach(n = 1:100, .combine = c) %dopar% {
    library(boot); sd(boot(random.data[, n], bmed, R = 10000)$t)
}
head(results)

stopCluster(cluster)

The key in both cases is that the boot library must be loaded on each of the cluster nodes as well so that its functionality is available. Simply loading the library on the root node is not enough!

<h2>repeating timer by r asynchronously</h2>
The future package:

library("future")
plan(multiprocess)

myfun <- function() {
  future(fun2())

  return(1+1)
}
Unless fun2() is function used purely for its side effects, you typically want to retrieve the value of that future expression, which you do as:

f <- future(fun2())
y <- fun3()
v <- value(f)
z <- v + y
An alternative is to use the %<-% operator as in:

v %<-% fun2()
y <- fun3()
z <- v + y
FYI, if you use

plan(cluster, workers = c("n1", "n3", "remote.server.org"))
then the future expression is resolved on one of those machines. Using

plan(future.BatchJobs::batchjobs_slurm)
will cause it to be resolved via a Slurm job scheduler queue.

<a href="https://rstudio.github.io/promises/articles/futures.html" class="whitebut ">Launching tasks with future</a>
<a href="https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html" class="whitebut ">A Future for R</a>
<a href="https://appsilon.com/an-example-of-how-to-use-the-new-r-promises-package/" class="whitebut ">example of new R promises package</a>

<h2>Set a timer in R to execute a program</h2>
executing same code block every 15 seconds:

interval = 15
x = data.frame()

repeat {
  startTime = Sys.time()
  x = rbind.data.frame(x, sum(data)) #replace this line with your code/functions
  sleepTime = startTime + interval - Sys.time()
  if (sleepTime > 0)
    Sys.sleep(sleepTime)
}

Or:
print_test<-function(x){
    if(condition)
    {
        Sys.sleep(x);
        cat("hello world");
        print_test(x);
    }
}
print_test(15)

<h2>What Is a Formula in R?</h2>
Formula allow you to capture two things:

An unevaluated expression
The context or environment in which the expression was created

In R the tilde operator ~ characterizes formulas With this operator, you say: "capture the meaning of this code, without evaluating it" You can think of a formula in R as a "quoting" operator

<code># A formula
d &lt;- y ~ x + b</code>

The variable on the left-hand side of a tilde (~) is called the "dependent variable", while the variables on the right-hand side are called the "independent variables" and are joined by plus signs +.

You can access the elements of a formula with the help of the square brackets: [[and ]].

<code>f &lt;- y ~ x + b 
# Retrieve the elements at index 1 and 2
f[[1]] ## "~"
f[[2]] ## y
f[[3]] ## x + b</code>

<h3>Why Use Formulae in R?</h3>
Formulas are powerful, general-purpose tools that allow you to capture the values of variables without evaluating them so that they can be interpreted by the function

Also, you use these R objects to express a relationship between variables.

For example, in the first line of code in the code chunk below, you say "y is a function of x, a, and b"

<code>y ~ x + a + b
## y ~ x + a + b</code>

More complex formulas like the code chunk below:

<code>Sepal.Width ~ Petal.Width | Species
## Sepal.Width ~ Petal.Width | Species</code>

Where you mean to say "the sepal width is a function of petal width, conditioned on species"

<h3>Using Formulas in R</h3>

<h2>How To Create a Formula in R</h2>
1.With the help of ~ operator 2.Some times you need or want to create a formula from an R object, such as a string. In such cases, you can use the formula or as.formula() function

<code>"y ~ x1 + x2"
## [1] "y ~ x1 + x2"
h &lt;- as.formula("y ~ x1 + x2")
h &lt;- formula("y ~ x1 + x2")</code>

<h2>How To Concatenate Formulae</h2>
To glue or bring multiple formulas together, you have two option:

Create separate variables for each formula and then use list()

<code># Create variables
i &lt;- y ~ x
j &lt;- y ~ x + x1
k &lt;- y ~ x + x1 + x2

# Concatentate
formulae &lt;- list(as.formula(i),as.formula(j),as.formula(k))</code>

Use the lapply() function, where you pass in a vector with all of your formulas as a first argument and as.formula as the function that you want to apply to each element of that vector

<code># Join all with "c()"
l &lt;- c(i, j, k)

# Apply "as.formula" to all elements of "f"
lapply(l, as.formula)
[[1]] ## y ~ x
[[2]] ## y ~ x + x1
[[3]] ## y ~ x + x1 + x2</code>

<h3>Formula Operators</h3>
"+" for joining
"-" for removing terms
":" for interaction
"*" for crossing
"%in%" for nesting
"^" for limit crossing to the specified degree

<code># Use multiple independent variables
y ~ x1 + x2 ## y ~ x1 + x2
# Ignore objects in an analysis
y ~ x1 - x2 ## y ~ x1 - x2</code>

What if you want to actually perform an arithmetic operation? you have a couple of solutions:

1.You can calculate and store all of the variables in advance 2.You use the I() or "as-is" operator: y ~ x + I(x^2)

<h3>How To Inspect Formulas in R</h3>
You saw functions such as attributes(), typeof(), class(), etc

To examine and compare different formulae, you can use the terms() function:

<code>m &lt;- formula("y ~ x1 + x2")
terms(m)
## y ~ x1 + x2
## attr(,"variables")
## list(y, x1, x2)
## attr(,"factors")
##    x1 x2
## y   0  0
## x1  1  0
## x2  0  1
## attr(,"term.labels")
## [1] "x1" "x2"
## attr(,"order")
## [1] 1 1
## attr(,"intercept")
## [1] 1
## attr(,"response")
## [1] 1
## attr(,".Environment")
## &lt;environment: R_GlobalEnv&gt;
class(m)
## [1] "formula"
typeof(m)
## [1] "language"
attributes(m)
## $class
## [1] "formula"
## 
## $.Environment
## &lt;environment: R_GlobalEnv&gt;</code>

If you want to know the names of the variables in the model, you can use all.vars.

<code>print(all.vars(m))
## [1] "y"  "x1" "x2"</code>

To modify formulae without converting them to character you can use the update() function:

<code>update(y ~ x1 + x2, ~. + x3)
## y ~ x1 + x2 + x3
y ~ x1 + x2 + x3
## y ~ x1 + x2 + x3</code>

Double check whether you variable is a formula by passing it to the is.formula() function.

<code># Load "plyr"
library(plyr)

# Check "m"
is.formula(m)
## [1] TRUE</code>

<h3>When To Use Formulas</h3>
1.Modeling Functions
2.Graphical Functions in R

<h3>R Formula Packages</h3>
1.Formula Package
2.formula.tools

<h2>dplyr samples</h2>
<a href="https://github.com/Apress/r-data-science-quick-reference" class="whitebut ">R data science quick reference</a>

library(tidyverse)
iris_df <- as_tibble(iris)
print(iris_df, n = 3)
head(iris_df$Species)

## ============
iris_df %>% select(Sepal.Length, Species) %>% print(n = 3)
iris_df %>% select(-Species) %>% print(n = 3)
iris_df %>% select(-Species, -Sepal.Length) %>% print(n = 3)

<h2>get rid of all non-ASCII characters.</h2>

Texts = c("Let the stormy clouds chase, everyone from the place ☁  ♪ ♬",
    "See you soon brother ☮ ",
    "A boring old-fashioned message" ) 

gsub("[^\x01-\x7F]", "", Texts)
[1] "Let the stormy clouds chase, everyone from the place    "
[2] "See you soon brother  "                                  
[3] "A boring old-fashioned message"

Details: You can specify character classes in regex's with [ ]. 
When the class description starts with ^ it means everything except these characters. 
Here, I have specified everything except characters 1-127, i.e. 
everything except standard ASCII and I have specified that they should be replaced with the empty string.

<h2>Display a popup from a batch file</h2>
The goal is to display a popup, the calling batch file must stop and wait the popup closing.

<h3>Using powershell</h3>
echo calling popup
powershell [Reflection.Assembly]::LoadWithPartialName("""System.Windows.Forms""");[Windows.Forms.MessageBox]::show("""rgagnon.com""", """HowTo""",0)>nul
echo we are back!

<h3>Using MHTA</h3>
echo calling popup
mshta javascript:alert("rgagnon.com\n\nHowTo!");close();
echo we are back!
Regular CMD
echo calling popup
START /WAIT CMD /C "ECHO rgagnon.com && ECHO HowTo && ECHO. && PAUSE"
echo we are back!

<h3>Using JScript</h3>
 @if (@x)==(@y) @end /***** jscript comment ******
     @echo off
     echo calling popup
     cscript //E:JScript //nologo "%~f0" "%~nx0" %*
     echo we are back!
     exit /b 0
 @if (@x)==(@y) @end ******  end comment *********/

var wshShell = WScript.CreateObject("WScript.Shell");
wshShell.Popup("HowTo", -1, "rgagnon.com", 16);

<h2>select after the nth word form a string</h2>
library(stringr)
a<-"starting anything goes from here now"
res <- gsub("^(\\w+\\s){1}","",a)
res
res <- gsub("^(\\w+\\s){3}","",a)
res
res <- gsub("^(\\w+\\s){4}","",a)
res

<h2>Significant network analysis packages</h2>
<a href="https://www.jessesadler.com/post/network-analysis-with-r/" class="redbut goldbs orangets">Introduction to Network Analysis with R</a>

<a href="http://statnet.org/" class="whitebut ">statnet</a>
<a href="https://igraph.org/" class="whitebut ">igraph</a>
<a href="https://www.data-imaginist.com/2017/introducing-tidygraph/" class="whitebut ">tidygraph</a>
<a href="https://www.data-imaginist.com/2017/ggraph-introduction-layouts/" class="whitebut ">ggraph</a>
<a href="http://www.viznetwork.com/" class="whitebut ">vizNetwork</a>
<a href="https://christophergandrud.github.io/networkD3/" class="whitebut ">networkD3</a>

<h3>Basic Managerial Applications of Network Analysis</h3>
Plans the projects by analyzing the project activities.

Projects are broken down to individual tasks or activities, which are arranged in logical sequence. 

It is also decided that which tasks will be performed simultaneously and which other sequentially.

A network diagram is prepared, which presents visually the relationship between all the activities involved and the cost for different activities. 

Network analysis helps designing, planning, coordi­nating, controlling and in decision-making in order to accom­plish the project economically in the minimum available time with the limited available resources. 

The network analysis fulfills the objectives of reducing total time, cost, idle resources, interruptions and conflicts. 

Managerial applications of network analysis are as follows:
Assembly line scheduling,
Research and development,
Inventory planning and control,
Shifting of manufacturing plant from one site to another,
Launching of new products and advertising campaigns,
Control of traffic flow in cities,
Budget and audit procedures,
Launching space programmes,
Installation of new equipments,
Long-range planning and developing staffing plans, etc.

Network techniques:
A number of network techniques:
PERT- Programme Evaluation and Review Technique
CPM- Critical Path Method
RAMS- Resource Allocation and Multi-project Scheduling
PEP- Programme Evolution Procedure
COPAC- Critical Operating Production Allocation Control
MAP- Manpower Allocation Procedure
RPSM- Resource Planning and Scheduling Method
LCS- Least Cost Scheduling
MOSS- Multi-Operation Scheduling System
PCS- Project Control System
GERT- Graphical Evaluation Review Technique.

<h2>shows exactly two decimal places for the number</h2>
format(round(x, 2), nsmall = 2)

<h2>R Customizing Startup</h2>
<a href="https://github.com/rstudio/rstudio/issues/5454" class="whitebut ">Tab for spaces setting</a>
<a href="https://github.com/rstudio/rstudio/issues/4448" class="whitebut ">Spaces per tab option</a>
<a href="https://www.statmethods.net/interface/customizing.html" class="whitebut ">R Customizing Startup</a>

<h2>R courses</h2>
<a href="https://www.datacamp.com/courses/free-introduction-to-r">free-introduction-to-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r">intermediate-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-the-tidyverse">introduction-to-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-dplyr">data-manipulation-with-dplyr</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-visualization-with-ggplot2">introduction-to-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/introduction-to-importing-data-in-r">introduction-to-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/cleaning-data-in-r">cleaning-data-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-dplyr">joining-data-with-dplyr</a>
<a href="https://www.datacamp.com/courses/intermediate-data-visualization-with-ggplot2">intermediate-data-visualization-with-ggplot2</a>
<a href="https://www.datacamp.com/courses/exploratory-data-analysis-in-r">exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/correlation-and-regression-in-r">correlation-and-regression-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-classification">supervised-learning-in-r-classification</a>
<a href="https://www.datacamp.com/courses/introduction-to-data-in-r">introduction-to-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-regression-in-r">introduction-to-regression-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-statistics-in-r">introduction-to-statistics-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-writing-functions-in-r">introduction-to-writing-functions-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-r-for-finance">introduction-to-r-for-finance</a>
<a href="https://www.datacamp.com/courses/case-study-exploratory-data-analysis-in-r">case-study-exploratory-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-importing-data-in-r">intermediate-importing-data-in-r</a>
<a href="https://www.datacamp.com/courses/multiple-and-logistic-regression-in-r">multiple-and-logistic-regression-in-r</a>
<a href="https://www.datacamp.com/courses/building-web-applications-with-shiny-in-r">building-web-applications-with-shiny-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-in-r">data-visualization-in-r</a>
<a href="https://www.datacamp.com/courses/supervised-learning-in-r-regression">supervised-learning-in-r-regression</a>
<a href="https://www.datacamp.com/courses/writing-efficient-r-code">writing-efficient-r-code</a>
<a href="https://www.datacamp.com/courses/working-with-dates-and-times-in-r">working-with-dates-and-times-in-r</a>
<a href="https://www.datacamp.com/courses/unsupervised-learning-in-r">unsupervised-learning-in-r</a>
<a href="https://www.datacamp.com/courses/machine-learning-with-caret-in-r">machine-learning-with-caret-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-analysis-in-r">time-series-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/manipulating-time-series-data-with-xts-and-zoo-in-r">manipulating-time-series-data-with-xts-and-zoo-in-r</a>
<a href="https://www.datacamp.com/courses/cluster-analysis-in-r">cluster-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/data-manipulation-with-datatable-in-r">data-manipulation-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/reporting-with-rmarkdown">reporting-with-rmarkdown</a>
<a href="https://www.datacamp.com/courses/working-with-data-in-the-tidyverse">working-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/forecasting-in-r">forecasting-in-r</a>
<a href="https://www.datacamp.com/courses/string-manipulation-with-stringr-in-r">string-manipulation-with-stringr-in-r</a>
<a href="https://www.datacamp.com/courses/fundamentals-of-bayesian-data-analysis-in-r">fundamentals-of-bayesian-data-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-r-for-finance">intermediate-r-for-finance</a>
<a href="https://www.datacamp.com/courses/introduction-to-portfolio-analysis-in-r">introduction-to-portfolio-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-probability-in-r">foundations-of-probability-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-inference-in-r">foundations-of-inference-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-text-analysis-in-r">introduction-to-text-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/joining-data-with-datatable-in-r">joining-data-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/credit-risk-modeling-in-r">credit-risk-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/modeling-with-data-in-the-tidyverse">modeling-with-data-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/working-with-web-data-in-r">working-with-web-data-in-r</a>
<a href="https://www.datacamp.com/courses/parallel-programming-in-r">parallel-programming-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-geospatial-data-in-r">visualizing-geospatial-data-in-r</a>
<a href="https://www.datacamp.com/courses/importing-and-managing-financial-data-in-r">importing-and-managing-financial-data-in-r</a>
<a href="https://www.datacamp.com/courses/linear-algebra-for-data-science-in-r">linear-algebra-for-data-science-in-r</a>
<a href="https://www.datacamp.com/courses/hierarchical-and-mixed-effects-models-in-r">hierarchical-and-mixed-effects-models-in-r</a>
<a href="https://www.datacamp.com/courses/case-study-exploring-baseball-pitching-data-in-r">case-study-exploring-baseball-pitching-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-manipulating-time-series-data-in-r">case-studies-manipulating-time-series-data-in-r</a>
<a href="https://www.datacamp.com/courses/differential-expression-analysis-with-limma-in-r">differential-expression-analysis-with-limma-in-r</a>
<a href="https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r">analyzing-election-and-polling-data-in-r</a>
<a href="https://www.datacamp.com/courses/introduction-to-tensorflow-in-r">introduction-to-tensorflow-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-statistical-modeling-in-r">intermediate-statistical-modeling-in-r</a>
<a href="https://www.datacamp.com/courses/structural-equation-modeling-with-lavaan-in-r">structural-equation-modeling-with-lavaan-in-r</a>
<a href="https://www.datacamp.com/courses/bond-valuation-and-analysis-in-r">bond-valuation-and-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/garch-models-in-r">garch-models-in-r</a>
<a href="https://www.datacamp.com/courses/foundations-of-functional-programming-with-purrr">foundations-of-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/anomaly-detection-in-r">anomaly-detection-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-interactive-data-visualization-with-plotly-in-r">intermediate-interactive-data-visualization-with-plotly-in-r</a>
<a href="https://www.datacamp.com/courses/network-analysis-in-the-tidyverse">network-analysis-in-the-tidyverse</a>
<a href="https://www.datacamp.com/courses/market-basket-analysis-in-r">market-basket-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/financial-analytics-in-r">financial-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/visualizing-big-data-with-trelliscope-in-r">visualizing-big-data-with-trelliscope-in-r</a>
<a href="https://www.datacamp.com/courses/choice-modeling-for-marketing-in-r">choice-modeling-for-marketing-in-r</a>
<a href="https://www.datacamp.com/courses/handling-missing-data-with-imputations-in-r">handling-missing-data-with-imputations-in-r</a>
<a href="https://www.datacamp.com/courses/forecasting-product-demand-in-r">forecasting-product-demand-in-r</a>
<a href="https://www.datacamp.com/courses/defensive-r-programming">defensive-r-programming</a>
<a href="https://www.datacamp.com/courses/intermediate-functional-programming-with-purrr">intermediate-functional-programming-with-purrr</a>
<a href="https://www.datacamp.com/courses/analyzing-us-census-data-in-r">analyzing-us-census-data-in-r</a>
<a href="https://www.datacamp.com/courses/life-insurance-products-valuation-in-r">life-insurance-products-valuation-in-r</a>
<a href="https://www.datacamp.com/courses/mixture-models-in-r">mixture-models-in-r</a>
<a href="https://www.datacamp.com/courses/data-visualization-with-lattice-in-r">data-visualization-with-lattice-in-r</a>
<a href="https://www.datacamp.com/courses/fraud-detection-in-r">fraud-detection-in-r</a>
<a href="https://www.datacamp.com/courses/designing-and-analyzing-clinical-trials-in-r">designing-and-analyzing-clinical-trials-in-r</a>
<a href="https://www.datacamp.com/courses/chip-seq-with-bioconductor-in-r">chip-seq-with-bioconductor-in-r</a>
<a href="https://www.datacamp.com/courses/intermediate-regular-expressions-in-r">intermediate-regular-expressions-in-r</a>
<a href="https://www.datacamp.com/courses/survey-and-measurement-development-in-r">survey-and-measurement-development-in-r</a>
<a href="https://www.datacamp.com/courses/feature-engineering-in-r">feature-engineering-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-exploring-employee-data-in-r">human-resources-analytics-exploring-employee-data-in-r</a>
<a href="https://www.datacamp.com/courses/scalable-data-processing-in-r">scalable-data-processing-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-statistics-interview-questions-in-r">practicing-statistics-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/practicing-machine-learning-interview-questions-in-r">practicing-machine-learning-interview-questions-in-r</a>
<a href="https://www.datacamp.com/courses/human-resources-analytics-predicting-employee-churn-in-r">human-resources-analytics-predicting-employee-churn-in-r</a>
<a href="https://www.datacamp.com/courses/optimizing-r-code-with-rcpp">optimizing-r-code-with-rcpp</a>
<a href="https://www.datacamp.com/courses/predictive-analytics-using-networked-data-in-r">predictive-analytics-using-networked-data-in-r</a>
<a href="https://www.datacamp.com/courses/case-studies-network-analysis-in-r">case-studies-network-analysis-in-r</a>
<a href="https://www.datacamp.com/courses/building-response-models-in-r">building-response-models-in-r</a>
<a href="https://www.datacamp.com/courses/business-process-analytics-in-r">business-process-analytics-in-r</a>
<a href="https://www.datacamp.com/courses/interactive-data-visualization-with-rbokeh">interactive-data-visualization-with-rbokeh</a>
<a href="https://www.datacamp.com/courses/r-for-sas-users">r-for-sas-users</a>
<a href="https://www.datacamp.com/courses/probability-puzzles-in-r">probability-puzzles-in-r</a>
<a href="https://www.datacamp.com/courses/time-series-with-datatable-in-r">time-series-with-datatable-in-r</a>
<a href="https://www.datacamp.com/courses/data-privacy-and-anonymization-in-r">data-privacy-and-anonymization-in-r</a>
<a href="https://www.datacamp.com/courses/course-creation-at-datacamp">course-creation-at-datacamp</a>
<a href="https://www.datacamp.com/projects/796">projects/796</a>
<a href="https://www.datacamp.com/projects/78">projects/78</a>
<a href="https://www.datacamp.com/projects/758">projects/758</a>
<a href="https://www.datacamp.com/projects/74">projects/74</a>
<a href="https://www.datacamp.com/projects/738">projects/738</a>
<a href="https://www.datacamp.com/projects/712">projects/712</a>
<a href="https://www.datacamp.com/projects/697">projects/697</a>
<a href="https://www.datacamp.com/projects/691">projects/691</a>
<a href="https://www.datacamp.com/projects/68">projects/68</a>
<a href="https://www.datacamp.com/projects/677">projects/677</a>
<a href="https://www.datacamp.com/projects/673">projects/673</a>
<a href="https://www.datacamp.com/projects/668">projects/668</a>
<a href="https://www.datacamp.com/projects/664">projects/664</a>
<a href="https://www.datacamp.com/projects/643">projects/643</a>
<a href="https://www.datacamp.com/projects/638">projects/638</a>
<a href="https://www.datacamp.com/projects/62">projects/62</a>
<a href="https://www.datacamp.com/projects/614">projects/614</a>
<a href="https://www.datacamp.com/projects/584">projects/584</a>
<a href="https://www.datacamp.com/projects/567">projects/567</a>
<a href="https://www.datacamp.com/projects/561">projects/561</a>
<a href="https://www.datacamp.com/projects/552">projects/552</a>
<a href="https://www.datacamp.com/projects/547">projects/547</a>
<a href="https://www.datacamp.com/projects/515">projects/515</a>
<a href="https://www.datacamp.com/projects/511">projects/511</a>
<a href="https://www.datacamp.com/projects/496">projects/496</a>
<a href="https://www.datacamp.com/projects/49">projects/49</a>
<a href="https://www.datacamp.com/projects/489">projects/489</a>
<a href="https://www.datacamp.com/projects/478">projects/478</a>
<a href="https://www.datacamp.com/projects/464">projects/464</a>
<a href="https://www.datacamp.com/projects/458">projects/458</a>
<a href="https://www.datacamp.com/projects/445">projects/445</a>
<a href="https://www.datacamp.com/projects/438">projects/438</a>
<a href="https://www.datacamp.com/projects/435">projects/435</a>
<a href="https://www.datacamp.com/projects/41">projects/41</a>
<a href="https://www.datacamp.com/projects/309">projects/309</a>
<a href="https://www.datacamp.com/projects/208">projects/208</a>
<a href="https://www.datacamp.com/projects/182">projects/182</a>
<a href="https://www.datacamp.com/projects/177">projects/177</a>
<a href="https://www.datacamp.com/projects/166">projects/166</a>
<a href="https://www.datacamp.com/projects/139">projects/139</a>

<h2>Customizing Startup the R environment</h2>
R will always source the Rprofile.site file first. 
On Windows, the file is in the C:\Program Files\R\R-n.n.n\etc directory. 

You can also place a .Rprofile file in any directory that you are going to run R from or in the user home directory.

At startup, R will source the Rprofile.site file. 
It will then look for a .Rprofile file to source in the current working directory. 
If it doesn't find it, it will look for one in the user's home directory. 

There are two special functions you can place in these files. 
.First( ) will be run at the start of the R session and .Last( ) will be run at the end of the session.

# Sample Rprofile.site file

# Things you might want to change
# options(papersize="a4")
# options(editor="notepad")
# options(pager="internal")

# R interactive prompt
# options(prompt="> ")
# options(continue="+ ")

# to prefer Compiled HTML
help options(chmhelp=TRUE)
# to prefer HTML help
# options(htmlhelp=TRUE)

# General options
options(tab.width = 2)
options(width = 130)
options(graphics.record=TRUE)

.First <- function(){
 library(Hmisc)
 library(R2HTML)
 cat("\nWelcome at", date(), "\n")
}

.Last <- function(){
 cat("\nGoodbye at ", date(), "\n")
}

<h2>Managing R</h2>
with .Rprofile, .Renviron, Rprofile.site, Renviron.site, rsession.conf, and repos.conf
Upon startup, R and RStudio look for a few different files you can use to control the behavior of your R session, for example by setting options or environment variables. 
In the context of RStudio Team, these settings are often used to set RStudio Server Pro to search for packages in an RStudio Package Manager repository.

This article is a practical guide to how to set particular options on R startup. 
General information on how to manage R package environments is available at <a href="https://environments.rstudio.com" target="_blank" rel="noopener">environments.rstudio.com</a> , and a deeper treatment of R process startup is available in <a href="https://rviews.rstudio.com/2017/04/19/r-for-enterprise-understanding-r-s-startup/" target="_blank" rel="noopener">this article</a>.&nbsp;

Here is a summary table of how to control R options and environment variables on startup. 
More details are below.

<table border="black">
<tbody>
<tr><td>File</td><td>Who Controls</td><td>Level</td><td>Limitations</td></tr>
<tr><td><code>.Rprofile</code></td><td>User or Admin</td><td>User or Project</td><td>None, sourced as R code.</td></tr>
<tr><td><code>.Renviron</code></td><td>User or Admin</td><td>User or Project</td><td>Set environment variables only.</td></tr>
<tr><td><code>Rprofile.site</code></td><td>Admin</td><td>Version of R</td><td>None, sourced as R code.</td></tr>
<tr><td><code>Renviron.site</code></td><td>Admin</td><td>Version of R</td><td>Set environment variables only.</td></tr>
<tr><td><code>rsession.conf</code></td><td>Admin</td><td>Server</td><td>Only RStudio settings, only single repository.</td></tr>
<tr><td><code>repos.conf</code></td><td>Admin</td><td>Server</td><td>Only for setting repositories.</td></tr>
</tbody>
</table>
<h3><code>.Rprofile</code></h3>

<code>.Rprofile</code> files are user-controllable files to set options and environment variables. 
<code>.Rprofile</code> files can be either at the user or project level. 
User-level <code>.Rprofile</code> files live in the base of the user's home directory, and project-level <code>.Rprofile</code> files live in the base of the project directory.&nbsp;

R will source only one <code>.Rprofile</code> file. 
So if you have both a project-specific <code>.Rprofile</code> file and a user <code>.Rprofile</code> file that you want to use, you explicitly source the user-level <code>.Rprofile</code> at the top of your project-level <code>.Rprofile</code> with <code>source("~/.Rprofile")</code>.

<code>.Rprofile</code> files are sourced as regular R code, so setting environment variables must be done inside a <code>Sys.setenv(key = "value")</code> call.&nbsp;

One easy way to edit your <code>.RProfile</code> file is to use the <code>usethis::edit_r_profile()</code> function from within an R session. 
You can specify whether you want to edit the user or project level <code>.Rprofile.</code>

<h3><code>.Renviron</code></h3>

<code>.Renviron</code> is a user-controllable file that can be used to create environment variables. 
This is especially useful to avoid including credentials like API keys inside R scripts. 
This file is written in a key-value format, so environment variables are created in the format:

Key1=value1
Key2=value2
...

And then <code>Sys.getenv("Key1")</code> will return <code>"value1"</code> in an R session.

Like with the <code>.Rprofile</code> file, <code>.Renviron</code> files can be at either the user or project level. 
If there is a project-level <code>.Renviron</code>, the user-level file will not be sourced. 
The <code>usethis</code> package includes a helper function for editing <code>.Renviron</code> files from an R session with <code>usethis::edit_r_environ()</code>.

<h3><code>Rprofile.site</code> and <code>Renviron.site</code></h3>

Both <code>.Rprofile</code> and <code>.Renviron</code> files have equivalents that apply server wide. 
<code>Rprofile.site</code>&nbsp;and<code>Renviron.site</code> (no leading dot) files are managed by admins on RStudio Server and are specific to a particular version of R.&nbsp;The most common settings for these&nbsp;files involve access to package repositories. 
For example, using the <a href="https://environments.rstudio.com/shared.html" target="_blank" rel="noopener">shared-baseline</a> package management strategy is generally done from an <code>Rprofile.site</code>.

Users can override settings in these files&nbsp;with their individual <code>.Rprofile</code>&nbsp;files.

These files are set for each version of R and should be located in <code>R_HOME/etc/</code>. 
You can find<code>R_HOME</code> by running the command&nbsp;<code>R.home(component
  = "home")</code> in a session of that version of R. 
So, for example, if you find that <code>R_HOME</code> is <code>/opt/R/3.6.2/lib/R</code>, the<code>Rprofile.site</code> for R 3.6.2 would go in <code>/opt/R/3.6.2/lib/R/etc/Rprofile.site</code>.

<h3><code>rsession.conf</code> and <code>repos.conf</code></h3>

RStudio Server allows server admins to configure particular server-wide R package repositories via the <code>rsession.conf</code> and <code>repos.conf</code> files. 
Only one repository can be configured in <code>rsession.conf</code>. 
If multiple repositories are needed, <code>repos.conf</code> should be used. 
Details on configuring RStudio Server with these files are in this <a href="https://support.rstudio.com/hc/en-us/articles/360009863114-Configuring-RStudio-Server-to-use-RStudio-Package-Manager" target="_blank" rel="noopener">support article</a>.

<h2>R startup mechanism is as follows</h2>
Unless --no-environ was given on the command line, R searches for site and user files to process for setting environment variables. 
The name of the site file is the one pointed to by the environment variable R_ENVIRON; if this is unset, ‘R_HOME/etc/Renviron.site’ is used (if it exists, which it does not in a ‘factory-fresh’ installation). 
The name of the user file can be specified by the R_ENVIRON_USER environment variable; if this is unset, the files searched for are ‘.Renviron’ in the current or in the user's home directory (in that order). 
See ‘Details’ for how the files are read.

Then R searches for the site-wide startup profile file of R code unless the command line option --no-site-file was given. 
The path of this file is taken from the value of the R_PROFILE environment variable (after tilde expansion). 
If this variable is unset, the default is ‘R_HOME/etc/Rprofile.site’, which is used if it exists (it contains settings from the installer in a ‘factory-fresh’ installation). 
This code is sourced into the base package. 
Users need to be careful not to unintentionally overwrite objects in base, and it is normally advisable to use local if code needs to be executed: see the examples.

Then, unless --no-init-file was given, R searches for a user profile, a file of R code. 
The path of this file can be specified by the R_PROFILE_USER environment variable (and tilde expansion will be performed). 
If this is unset, a file called ‘.Rprofile’ is searched for in the current directory or in the user's home directory (in that order). 
The user profile file is sourced into the workspace.

Note that when the site and user profile files are sourced only the base package is loaded, so objects in other packages need to be referred to by e.g. 
utils::dump.frames or after explicitly loading the package concerned.

R then loads a saved image of the user workspace from ‘.RData’ in the current directory if there is one (unless --no-restore-data or --no-restore was specified on the command line).

Next, if a function .First is found on the search path, it is executed as .First(). 
Finally, function .First.sys() in the base package is run. 
This calls require to attach the default packages specified by options("defaultPackages"). 
If the methods package is included, this will have been attached earlier (by function .OptRequireMethods()) so that namespace initializations such as those from the user workspace will proceed correctly.

A function .First (and .Last) can be defined in appropriate ‘.Rprofile’ or ‘Rprofile.site’ files or have been saved in ‘.RData’. 
If you want a different set of packages than the default ones when you start, insert a call to options in the ‘.Rprofile’ or ‘Rprofile.site’ file. 
For example, options(defaultPackages = character()) will attach no extra packages on startup (only the base package) (or set R_DEFAULT_PACKAGES=NULL as an environment variable before running R). 
Using options(defaultPackages = "") or R_DEFAULT_PACKAGES="" enforces the R system default.

On front-ends which support it, the commands history is read from the file specified by the environment variable R_HISTFILE (default ‘.Rhistory’ in the current directory) unless --no-restore-history or --no-restore was specified.

The command-line option --vanilla implies --no-site-file, --no-init-file, --no-environ and (except for R CMD) --no-restore Under Windows, it also implies --no-Rconsole, which prevents loading the ‘Rconsole’ file.

Details
Note that there are two sorts of files used in startup: environment files which contain lists of environment variables to be set, and profile files which contain R code.

Lines in a site or user environment file should be either comment lines starting with #, or lines of the form name=value. 
The latter sets the environmental variable name to value, overriding an existing value. 
If value contains an expression of the form ${foo-bar}, the value is that of the environmental variable foo if that exists and is set to a non-empty value, otherwise bar. 
(If it is of the form ${foo}, the default is "".) This construction can be nested, so bar can be of the same form (as in ${foo-${bar-blah}}). 
Note that the braces are essential: for example $HOME will not be interpreted.

Leading and trailing white space in value are stripped. 
value is then processed in a similar way to a Unix shell: in particular the outermost level of (single or double) quotes is stripped, and backslashes are removed except inside quotes.

On systems with sub-architectures (mainly Windows), the files ‘Renviron.site’ and ‘Rprofile.site’ are looked for first in architecture-specific directories, e.g. 
‘R_HOME/etc/i386/Renviron.site’. 
And e.g. 
‘.Renviron.i386’ will be used in preference to ‘.Renviron’.

Note
It is not intended that there be interaction with the user during startup code. 
Attempting to do so can crash the R process.

The startup options are for Rgui, Rterm and R but not for Rcmd: attempting to use e.g. 
--vanilla with the latter will give a warning or error.

Unix versions of R have a file ‘R_HOME/etc/Renviron’ which is read very early in the start-up processing. 
It contains environment variables set by R in the configure process, and is not used on R for Windows.

R CMD check and R CMD build do not always read the standard startup files, but they do always read specific Renviron files. 
The location of these can be controlled by the environment variables R_CHECK_ENVIRON and R_BUILD_ENVIRON. 
If these are set their value is used as the path for the Renviron file; otherwise, files ‘~/.R/check.Renviron’ or ‘~/.R/build.Renviron’ or sub-architecture-specific versions are employed.

If you want ‘~/.Renviron’ or ‘~/.Rprofile’ to be ignored by child R processes (such as those run by R CMD check and R CMD build), set the appropriate environment variable R_ENVIRON_USER or R_PROFILE_USER to (if possible, which it is not on Windows) "" or to the name of a non-existent file.

See Also
For the definition of the ‘home’ directory on Windows see the ‘rw-FAQ’ Q2.14. 
It can be found from a running R by Sys.getenv("R_USER").

.Last for final actions at the close of an R session. 
commandArgs for accessing the command line arguments.

There are examples of using startup files to set defaults for graphics devices in the help for windows.options.

An Introduction to R for more command-line options: those affecting memory management are covered in the help file for Memory.

readRenviron to read ‘.Renviron’ files.

For profiling code, see Rprof.

Examples
## Not run: 
## Example ~/.Renviron on Unix
R_LIBS=~/R/library
PAGER=/usr/local/bin/less

## Example .Renviron on Windows
R_LIBS=C:/R/library
MY_TCLTK="c:/Program Files/Tcl/bin"

## Example of setting R_DEFAULT_PACKAGES (from R CMD check)
R_DEFAULT_PACKAGES='utils,grDevices,graphics,stats'
# this loads the packages in the order given, so they appear on
# the search path in reverse order.

## Example of .Rprofile
options(width=65, digits=5)
options(show.signif.stars=FALSE)
setHook(packageEvent("grDevices", "onLoad"),
        function(...) grDevices::ps.options(horizontal=FALSE))
set.seed(1234)
.First <- function() cat("\n   Welcome to R!\n\n")
.Last <- function()  cat("\n   Goodbye!\n\n")

## Example of Rprofile.site
local({
  # add MASS to the default packages, set a CRAN mirror
  old <- getOption("defaultPackages"); r <- getOption("repos")
  r["CRAN"] <- "http://my.local.cran"
  options(defaultPackages = c(old, "MASS"), repos = r)
  ## (for Unix terminal users) set the width from COLUMNS if set
  cols <- Sys.getenv("COLUMNS")
  if(nzchar(cols)) options(width = as.integer(cols))
  # interactive sessions get a fortune cookie (needs fortunes package)
  if (interactive())
    fortunes::fortune()
})

## if .Renviron contains
FOOBAR="coo\bar"doh\ex"abc\"def'"

## then we get
# > cat(Sys.getenv("FOOBAR"), "\n")
# coo\bardoh\exabc"def'

## End(Not run)





<br>
<br>
<br>
<br>

<script type='text/javascript' src='readbook.js'></script>
<script>
var lazyLoadInstance = new LazyLoad({
    elements_selector: ".lazy"
    // ... more custom settings?
});
</script>
</body>
</html>
